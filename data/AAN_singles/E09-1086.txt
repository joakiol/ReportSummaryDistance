Proceedings of the 12th Conference of the European Chapter of the ACL, pages 754?762,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsUnsupervised Recognition of Literal and Non-Literal Useof Idiomatic ExpressionsCaroline Sporleder and Linlin LiSaarland UniversityPostfach 15 11 5066041 Saarbru?cken, Germany{csporled,linlin}@coli.uni-saarland.deAbstractWe propose an unsupervised method fordistinguishing literal and non-literal us-ages of idiomatic expressions.
Ourmethod determines how well a literal inter-pretation is linked to the overall cohesivestructure of the discourse.
If strong linkscan be found, the expression is classifiedas literal, otherwise as idiomatic.
We showthat this method can help to tell apart lit-eral and non-literal usages, even for id-ioms which occur in canonical form.1 IntroductionTexts frequently contain expressions whose mean-ing is not strictly literal, such as metaphors or id-ioms.
Non-literal expressions pose a major chal-lenge to natural language processing as they oftenexhibit lexical and syntactic idiosyncrasies.
Forexample, idioms can violate selectional restric-tions (as in push one?s luck under the assumptionthat only concrete things can normally be pushed),disobey typical subcategorisation constraints (e.g.,in line without a determiner before line), or changethe default assignments of semantic roles to syn-tactic categories (e.g., in break sth with X the ar-gument X would typically be an instrument but forthe idiom break the ice it is more likely to fill apatient role, as in break the ice with Russia).To avoid erroneous analyses, a natural languageprocessing system should recognise if an expres-sion is used non-literally.
While there has been alot of work on recognising idioms (see Section 2),most previous approaches have focused on a type-based classification, dividing expressions into ?id-iom?
or ?not an idiom?
irrespective of their actualuse in a discourse context.
However, while someexpressions, such as by and large, always have anon-compositional, idiomatic meaning, many id-ioms, such as break the ice or spill the beans, sharetheir linguistic form with perfectly literal expres-sions (see examples (1) and (2), respectively).
Forsome expressions, such as drop the ball, the lit-eral usage can even dominate in some domains.Hence, whether a potentially ambiguous expres-sion has literal or non-literal meaning has to beinferred from the discourse context.
(1) Dad had to break the ice on the chicken troughs sothat they could get water.
(2) Somehow I always end up spilling the beans allover the floor and looking foolish when the clerkcomes to sweep them up.Type-based idiom classification thus only ad-dresses part of the problem.
While it can au-tomatically compile lists of potentially idiomaticexpressions, it does not say anything about theidiomaticity of an expression in a particularcontext.
In this paper, we propose a novel,cohesion-based approach for detecting non-literalusages (token-based idiom classification).
Ourapproach is unsupervised and similar in spirit toHirst and St-Onge?s (1998) method for detectingmalapropisms.
Like them, we rely on the presenceor absence of cohesive links between the words ina text.
However, unlike Hirst and St-Onge we donot require a hand-crafted resource like WordNetor Roget?s Thesaurus; our approach is knowledge-lean.2 Related WorkMost studies on idiom classification focus on type-based classification; few researchers have workedon token-based approaches.
Type-based meth-ods frequently exploit the fact that idioms have754a number of properties which differentiate themfrom other expressions.
Apart from not having a(strictly) compositional meaning, they also exhibitsome degree of syntactic and lexical fixedness.
Forexample, some idioms do not allow internal modi-fiers (*shoot the long breeze) or passivisation (*thebucket was kicked).
They also typically only al-low very limited lexical variation (*kick the vessel,*strike the bucket).Many approaches for identifying idioms focuson one of these two aspects.
For instance, mea-sures that compute the association strength be-tween the elements of an expression have beenemployed to determine its degree of composition-ality (Lin, 1999; Fazly and Stevenson, 2006) (seealso Villavicencio et al (2007) for an overviewand a comparison of different measures).
Otherapproaches use Latent Semantic Analysis (LSA)to determine the similarity between a potential id-iom and its components (Baldwin et al, 2003).Low similarity is supposed to indicate low com-positionality.
Bannard (2007) proposes to iden-tify idiomatic expressions by looking at their syn-tactic fixedness, i.e., how likely they are to takemodifiers or be passivised, and comparing this towhat would be expected based on the observedbehaviour of the component words.
Fazly andStevenson (2006) combine information about syn-tactic and lexical fixedness (i.e., estimated degreeof compositionality) into one measure.The few token-based approaches include astudy by Katz and Giesbrecht (2006), who devisea supervised method in which they compute themeaning vectors for the literal and non-literal us-ages of a given expression in the training data.
Anunseen test instance of the same expression is thenlabelled by performing a nearest neighbour classi-fication.
They report an average accuracy of 72%,though their evaluation is fairly small scale, usingonly one expression and 67 instances.
Birke andSarkar (2006) model literal vs. non-literal classi-fication as a word sense disambiguation task anduse a clustering algorithm which compares test in-stances to two automatically constructed seed sets(one with literal and one with non-literal expres-sions), assigning the label of the closest set.
Whilethe seed sets are created without immediate humanintervention they do rely on manually created re-sources such as databases of known idioms.Cook et al (2007) and Fazly et al (To appear)propose an alternative method which crucially re-lies on the concept of canonical form (CForm).It is assumed that for each idiom there is a fixedform (or a small set of those) corresponding tothe syntactic pattern(s) in which the idiom nor-mally occurs (Riehemann, 2001).1 The canoni-cal form allows for inflectional variation of thehead verb but not for other variations (such asnominal inflection, choice of determiner etc.).
Ithas been observed that if an expression is usedidiomatically, it typically occurs in its canonicalform.
For example, Riehemann (2001, p. 34)found that for decomposable idioms 75% of theoccurrences are in canonical form, rising to 97%for non-decomposable idioms.2 Cook et al ex-ploit this behaviour and propose an unsupervisedmethod in which an expression is classified as id-iomatic if it occurs in canonical form and literalotherwise.
Canonical forms are determined auto-matically using a statistical, frequency-based mea-sure.
The authors report an average accuracy of72% for their classifier.3 Using Lexical Cohesion to IdentifyIdiomatic Expressions3.1 Lexical CohesionIn this paper we exploit lexical cohesion to detectidiomatic expressions.
Lexical cohesion is a prop-erty exhibited by coherent texts: concepts referredto in individual sentences are typically related toother concepts mentioned elsewhere (Halliday andHasan, 1976).
Such sequences of semantically re-lated concepts are called lexical chains.
Givena suitable measure of semantic relatedness, suchchains can be computed automatically and havebeen used successfully in a number of NLP appli-cations, starting with Hirst and St-Onge?s (1998)seminal work on detecting real-word spelling er-rors.
Their approach is based on the insight thatmisspelled words do not ?fit?
their context, i.e.,they do not normally participate in lexical chains.Content words which do not belong to any lexi-cal chain but which are orthographically close towords which do, are therefore good candidates forspelling errors.Idioms behave similarly to spelling errors inthat they typically also do not exhibit a high de-1This is also the form in which an idiom is usually listedin a dictionary.2Decomposable idioms are expressions such as spill thebeans which have a composite meaning whose parts can bemapped to the words of the expression (e.g., spill??reveal?,beans??secret?
).755gree of lexical cohesion with their context, at leastnot if one assumes a literal meaning for their com-ponent words.
Hence if the component words of apotentially idiomatic expression do not participatein any lexical chain, it is likely that the expressionis indeed used idiomatically, otherwise it is prob-ably used literally.
For instance, in example (3),where the expression play with fire is used in a lit-eral sense, the word fire does participate in a chain(shown in bold face) that also includes the wordsgrilling, dry-heat, cooking, and coals, while forthe non-literal usage in example (4) there are nochains which include fire.3(3) Grilling outdoors is much more than just an-other dry-heat cooking method.
It?s the chanceto play with fire, satisfying a primal urge to stiraround in coals .
(4) And PLO chairman Yasser Arafat has accused Is-rael of playing with fire by supporting HAMAS inits infancy.Unfortunately, there are also a few cases inwhich a cohesion-based approach fails.
Some-times an expression is used literally but does notfeature prominently enough in the discourse toparticipate in a chain, as in example (5) where themain focus of the discourse is on the use of mor-phine and not on children playing with fire.4 Theopposite case also exists: sometimes idiomatic us-ages do exhibit lexical cohesion on the componentword level.
This situation is often a consequenceof a deliberate ?play with words?, e.g.
the use ofseveral related idioms or metaphors (see example(6)).
However, we found that both cases are rel-atively rare.
For instance, in a study of 75 literalusages of various expressions, we only discoveredseven instances in which no relevant chain couldbe found, including some cases where the contextwas too short to establish the cohesive structure(e.g., because the expression occurred in a head-line).
(5) Chinamasa compared McGown?s attitude to mor-phine to a child?s attitude to playing with fire ?
alack of concern over the risks involved.
(6) Saying that the Americans were?playing with fire?
the official press specu-lated that the ?gunpowder barrel?
which is Taiwanmight well ?explode?
if Washington and Taipei donot put a stop to their ?incendiary gesticulations.
?3Idioms may, of course, link to the surrounding discoursewith their idiomatic meaning, i.e., for play with fire one mayexpect other words in the discourse which are related to theconcept ?danger?.4Though one could argue that there is a chain linking childand play which points to the literal usage here.3.2 Modelling Semantic RelatednessWhile a cohesion-based approach to token-basedidiom classification should be intuitively success-ful, its practical usefulness depends crucially onthe availability of a suitable method for computingsemantic relatedness.
This is currently an area ofactive research.
There are two main approaches.Methods based on manually built lexical knowl-edge bases, such as WordNet, model semantic re-latedness by computing the shortest path betweentwo concepts in the knowledge base and/or bylooking at word overlap in the glosses (see Budan-itsky and Hirst (2006) for an overview).
Distribu-tional approaches, on the other hand, rely on textcorpora, and model relatedness by comparing thecontexts in which two words occur, assuming thatrelated words occur in similar context (e.g., Hindle(1990), Lin (1998), Mohammad and Hirst (2006)).More recently, there has also been research on us-ing Wikipedia and related resources for modellingsemantic relatedness (Ponzetto and Strube, 2007;Zesch et al, 2008).All approaches have advantages and disadvan-tages.
WordNet-based approaches, for instance,typically have a low coverage and only workfor so-called ?classical relations?
like hypernymy,antonymy etc.
Distributional approaches usuallyconflate different word senses and may thereforelead to unintuitive results.
For our task, we need tomodel a wide range of semantic relations (Morrisand Hirst, 2004), for example, relations based onsome kind of functional or situational association,as between fire and coal in (3) or between ice andwater in example (1).
Likewise we also need tomodel relations between non-nouns, for instancebetween spill and sweep up in example (2).
Somerelations also require world-knowledge, as in ex-ample (7), where the literal usage of drop theball is not only indicated by the presence of goal-keeper but also by knowing that Wayne Rooneyand Kevin Campbell are both football players.
(7) When Rooney collided with the goalkeeper, caus-ing him to drop the ball, Kevin Campbell fol-lowed in.We thus decided against a WordNet-based mea-sure of semantic relatedness, opting instead for adistributional approach, Normalized Google Dis-tance (NGD, see Cilibrasi and Vitanyi (2007)),which computes relatedness on the basis of pagecounts returned by a search engine.
NGD is a mea-sure of association that quantifies the strength of a756relationship between two words.
It is defined asfollows:NGD(x, y) =max{log f(x), log f(y)} ?
log f(x, y)log M ?min{log f(x), log f(y)}(8)where x and y are the two words whose asso-ciation strength is computed (e.g., fire and coal),f(x) is the page count returned by the search en-gine for the term x (and likewise for f(y) and y),f(x, y) is the page count returned when queryingfor ?x AND y?
(i.e., the number of pages that con-tain both, x and y), and M is the number of webpages indexed by the search engine.
The basic ideais that the more often two terms occur together rel-ative to their overall occurrence the more closelythey are related.
For most pairs of search termsthe NGD falls between 0 and 1, though in a smallnumber of cases NGD can exceed 1 (see Cilibrasiand Vitanyi (2007) for a detailed discussion of themathematical properties of NGD).Using web counts rather than bi-gram countsfrom a corpus as the basis for computing semanticrelatedness was motivated by the fact that the webis a significantly larger database than any com-piled corpus, which makes it much more likelythat we can find information about the concepts weare looking for (thus alleviating data sparseness).The information is also more up-to-date, which isimportant for modelling the kind of world knowl-edge about named entities we need to resolve ex-amples like (7).
Furthermore, it has been shownthat web counts can be used as reliable proxies forcorpus-based counts and often lead to better sta-tistical models (Zhu and Rosenfeld, 2001; Lapataand Keller, 2005).To obtain the web counts we used Yahoo ratherthan Google because we found Yahoo gave usmore stable counts over time.
Both the Yahooand the Google API seemed to have problems withvery high frequency words, so we excluded thosecases.
Effectively, this amounted to filtering outfunction words.
As it is difficult to obtain reli-able figures for the number of pages indexed by asearch engine, we approximated this number (Min formula (8) above) by setting it to the numberof hits obtained for the word the, assuming thatthis word occurs in virtually all English languagepages (Lapata and Keller, 2005).
When generat-ing the queries we made sure that we queried forall combinations of inflected forms (for example?fire AND coal?
would be expanded to ?fire ANDcoal?, ?fires AND coal?, ?fire AND coals?, and?fires AND coals?).
The inflected forms were gen-erated by the morph tools developed at the Univer-sity of Sussex (Minnen et al, 2001).53.3 Cohesion-based ClassifiersWe implemented two cohesion-based classifiers:the first one computes the lexical chains for theinput text and classifies an expression as literal ornon-literal depending on whether its componentwords participate in any of the chains, the secondclassifier builds a cohesion graph and determineshow this graph changes when the expression is in-serted or left out.Chain-based classifier Various methods forbuilding lexical chains have been proposed in theliterature (Hirst and St-Onge, 1998; Barzilay andElhadad, 1997; Silber and McCoy, 2002) but thebasic idea is as follows: the content words of thetext are considered in sequence and for each wordit is determined whether it is similar enough to (thewords in) one of the existing chains to be placedin that chain, if not it is placed in a chain of itsown.
Depending on the chain building algorithmused, a word is placed in a chain if it is related toone other word in the chain or to all of them.
Thelatter strategy is more conservative and tends tolead to shorter but more reliable chains and it is themethod we adopted here.6 Note that the chainingalgorithm has a free parameter, namely a thresholdwhich has to be surpassed to consider two wordsrelated (relatedness threshold).On the basis of the computed chains, the classi-fier has to decide whether the target expression isused literally or not.
A simple strategy would clas-sify an expression as literal whenever one or moreof its component words participates in any chain.However, as the chains are potentially noisy, thismay not be the best strategy.
We therefore alsoevaluate the strength of the chain(s) in which theexpression participates.
If a component word ofthe expression participates in a long chain (and isrelated to all words in the chain, as we require)5The tools are available at: http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html.6If a WordNet-based relatedness measure is used, thechaining algorithm has to perform word sense disambigua-tion as well.
As we use a distributional relatedness measurewhich conflates different senses anyway, we do not have todisambiguate here.757then this is good evidence that the expression isindeed used in a literal sense.
For instance, in(3) the word fire belongs to the relatively longchain grilling ?
dry-heat ?
cooking ?
fire ?
coals,providing strong evidence of literal usage of playwith fire.
To determine the strength of the evi-dence in favour of a literal interpretation, we takethe longest chain in which any of the componentwords of the idiom participate7 and check whetherthis is above a predefined threshold (the classifi-cation threshold).
Both the relatedness thresholdand the classification threshold are set empiricallyby optimising on a manually annotated develop-ment set (see Section 4.2).Graph-based classifier The chain-based clas-sifier has two parameters which need to be op-timised on labelled data, making this methodweakly supervised.
To overcome this drawback,we designed a second classifier which does nothave free parameters and is thus fully unsuper-vised.
This classifier relies on cohesion graphs.The vertices of such a cohesion graph correspondto the (content) word tokens in the text, each pairof vertices is connected by an edge and the edgesare weighted by the semantic relatedness (i.e., theinverse NGD) between the two words.
The co-hesion graph for example (1) is shown in Figure 1(for expository reasons, edge weights are excludedfrom the figure).
Once we have built the cohe-sion graph we compute its connectivity (definedas the average edge weight) and compare it to theconnectivity of the graph that results from remov-ing the (component words of the) target expres-sion.
For instance in Figure 1, we would com-pare the connectivity of the graph as it is shownto the connectivity that results from removing thedashed edges.
If removing the idiom words fromthe graph leads to a higher connectivity, we as-sume that the idiom is used non-literally, other-wise we assume it is used literally.
In Figure 1,for example, most edges would have a relativelylow weight, indicating a weak relation between thewords they link.
The edge between ice and water,however, would have a higher weight.
Removingice from the graph would therefore lead to a de-creased connectivity and the classifier would pre-dict that break the ice is used in the literal sensein example (1).
Effectively, we replace the ex-7Note, that it is not only the noun that can participate in achain.
In example (2), the word spill can be linked to sweepup to provide evidence of literal usage.break icewatertroughschickenDadFigure 1: Cohesion graph for example (1)plicit thresholds of the lexical chain method byan implicit threshold (i.e., change in connectivity),which does not have to be optimised.4 Evaluating the Cohesion-BasedApproachWe tested our two cohesion-based classifiers aswell as a supervised classifier on a manually an-notated data set.
Section 4.2 gives details of theexperiments and results.
We start, however, by de-scribing the data used in the experiments.4.1 DataWe chose 17 idioms from the Oxford Dictionaryof Idiomatic English (Cowie et al, 1997) and otheridiom lists found on the internet.
The idioms weremore or less selected randomly, subject to twoconstraints: First, because the focus of the presentstudy is on distinguishing literal and non-literal us-age, we chose expressions for which we assumedthat the literal meaning was not too infrequent.
Wethus disregarded expressions like play the secondfiddle or sail under false colours.
Second, in linewith many previous approaches to idiom classifi-cation (Fazly et al, To appear; Cook et al, 2007;Katz and Giesbrecht, 2006), we focused mainly onexpressions of the form V+NP or V+PP as this isa fairly large group and many of these expressionscan be used literally as well, making them an idealtest set for our purpose.
However, our approachalso works for expressions which match a differ-ent syntactic pattern and to test the generality ofour method we included a couple of these in thedata set (e.g., get one?s feet wet).
For the same rea-son, we also included some expressions for whichwe could not find a literal use in the corpus (e.g.,back the wrong horse).For each of the 17 expressions shown in Ta-ble 1, we extracted all occurrences found in theGigaword corpus that were in canonical form (theforms listed in the table plus inflectional varia-758tions of the head verb).8 Hence, for rock the boatwe would extract rocked the boat and rocking theboat but not rock a boat, rock the boats or rockthe ship.
The motivation for this was two-fold.First, as was discussed in Section 2, the vast ma-jority of idiomatic usages are in canonical form.This is especially true for non-decomposable id-ioms (most of our 17 idioms), where only around3% of the idiomatic usages are not in canonicalform.
Second, we wanted to test whether our ap-proach would be able to detect literal usages in theset of canonical form expressions as this is pre-cisely the set of expressions that would be classi-fied as idiomatic by the unsupervised CForm clas-sifier (Cook et al (2007), Fazly et al (To appear)).While expressions in the canonical form are morelikely to be used idiomatically, it is still possibleto find literal usages as in examples (1) and (2).For some expressions, such as drop the ball theliteral usage even outweighs the non-literal usage.These literal usages would be mis-classified by theCForm classifier.In principle, though, our approach is very gen-eral and would also work on expressions that arenot in canonical form and expressions whose id-iomatic status is unclear, i.e., we do not necessar-ily require a predefined set of idioms but could runthe classifiers on any V+NP or V+PP chunk.For each extracted example, we included fiveparagraphs of context (the current paragraph plusthe two preceding and following ones).9 This wasthe context used by the classifiers.
The exampleswere then labelled as ?literal?
or ?non-literal?
byan experienced annotator.
If the distinction couldnot be made reliably, e.g., because the contextwas not long enough to disambiguate, the anno-tator was allowed to annotate ???.
These caseswere excluded from the data sets.
To estimatethe reliability of our annotation, a randomly se-lected sample (300 instances) was annotated inde-pendently by a second annotator.
The annotationsdeviated in eight cases from the original, amount-ing to an inter-annotator agreement of over 97%and a kappa score of 0.7 (Cohen, 1960).
All de-viations were cases in which one of the annotatorschose ??
?, often because there was not sufficientcontext and the annotation decision had to be madeon the basis of world knowledge.8The extraction was done via manually built regular ex-pressions.9Note that paragraphs tend to be rather short in newswire.For other genres it may be sufficient to extract one paragraph.expression literal non-literal allback the wrong horse 0 25 25bite off more than one can chew 2 142 144bite one?s tongue 16 150 166blow one?s own trumpet 0 9 9bounce off the wall* 39 7 46break the ice 20 521 541drop the ball* 688 215 903get one?s feet wet 17 140 157pass the buck 7 255 262play with fire 34 532 566pull the trigger* 11 4 15rock the boat 8 470 478set in stone 9 272 281spill the beans 3 172 175sweep under the carpet 0 9 9swim against the tide 1 125 126tear one?s hair out 7 54 61all 862 3102 3964Table 1: Idiom statistics (* indicates expressionsfor which the literal usage is more common thanthe non-literal one)4.2 Experimental Set-Up and ResultsFor the lexical chain classifier we ran two experi-ments.
In the first, we used the data for one expres-sion (break the ice) as a development set for opti-mising the two parameters (the relatedness thresh-old and the classification threshold).
To find goodthresholds, a simple hill-climbing search was im-plemented during which we increased the relat-edness threshold in steps of 0.02 and the classi-fication threshold (governing the minimum chainlength needed) in steps of 1.
We optimised the F-Score for the literal class, though we found that theselected parameters varied only minimally whenoptimising for accuracy.
We then used the param-eter values determined in this way and applied theclassifier to the remainder of the data.The results obtained in this way depend to someextent on the data set used for the parameter set-ting.10 To control this factor, we also ran anotherexperiment in which we used an oracle to set theparameters (i.e., the parameters were optimised forthe complete set).
While this is not a realistic sce-nario as it assumes that the labels of the test dataare known during parameter setting, it does pro-vide an upper bound for the lexical chain method.For comparison, we also implemented an in-formed baseline classifier, which employs a sim-ple model of cohesion, classifying expressions as10We also ran the experiment for different developmentsets and found that there was a relatively high degree of vari-ation in the parameters selected and in the results obtainedwith those settings.759literal if the noun inside the expression (e.g., icefor break the ice) is repeated elsewhere in the con-text, and non-literal otherwise.
One would expectthis classifier to have a high precision for literalexpressions but a low recall.Finally, we implemented a supervised classi-fier.
Supervised classifiers have been used be-fore for this task, notably by Katz and Giesbrecht(2006).
Our approach is slightly different: in-stead of creating meaning vectors we look at theword overlap11 of a test instance with the literaland non-literal instances in the training set (for thesame expression) and then assign the label of theclosest set.That such an approach might be promising be-comes clear when one looks at some examples ofliteral and non-literal usage.
For instance, non-literal examples of break the ice occur frequentlywith words such as diplomacy, relations, dialogueetc.
Effectively these words form lexical chainswith the idiomatic meaning of break the ice.
Theyare absent for literal usages.
A supervised classi-fier can learn which terms are indicative of whichusage.
Note that this information is expression-specific, i.e., it is not possible to train a classifierfor play with fire on labelled examples for breakthe ice.
This makes the supervised approach quiteexpensive in terms of annotation effort as data hasto be labelled for each expression.
Nonetheless, itis instructive to see how well one could do withthis approach.
In the experiments, we ran the su-pervised classifier in leave-one-out mode on eachexpression for which we had literal examples.Table 2 shows the results for the five classi-fiers discussed above: the informed baseline clas-sifier (Rep), the cohesion graph (Graph), the lexi-cal chain classifier with the parameters optimisedon break the ice (LC), the lexical chain classifierwith the parameters set by an oracle (LC-O), andthe supervised classifier (Super).
The table alsoshows the accuracy that would be obtained by aCForm classifier (Cook et al, 2007; Fazly et al,To appear) with gold standard canonical forms.This classifier would label all examples in our dataset as ?non-literal?
(it is thus equivalent to a ma-jority class baseline).
Since the majority of ex-amples is indeed used idiomatically, this classifierachieves a relatively high accuracy.
However, ac-curacy is not the best evaluation measure here be-11We used the Dice coefficient as implemented in Ted Ped-ersen?s Text::Similarity module: http://www.d.umn.edu/?tpederse/text-similarity.html.CForm Rep Graph LC LC-O SuperAcc 78.25 79.06 79.61 80.50 80.42 95.69Pl - 70.00 52.21 62.26 53.89 84.62Rl - 5.96 67.87 26.21 69.03 96.45Fl - 10.98 59.02 36.90 60.53 90.15Table 2: Accuracy, literal precision (Pl), recall(Rl), and F-Score (Fl) for the classifierscause we are interested in detecting literal usagesamong the canonical forms.
Therefore, we alsocomputed the precision (Pl), recall (Rl), and F-score (Fl) for the literal class.It can be seen that all classifiers obtain a rela-tively high accuracy but vary in precision, recalland F-Score.
For the CForm classifier, precision,recall, and F-Score are undefined as it does notlabel any examples as ?literal?.
As expected thebaseline classifier, which looks for repetitions ofthe component words of the target expression, hasa relatively high precision, showing that the ex-pression is typically used in the literal sense if partof it is repeated in the context.
The recall, though,is very low, indicating that lexical repetition is nota sufficient signal for literal usage.The graph-based classifier and the globally op-timised lexical chain classifier (LC-O) outperformthe other two unsupervised classifiers (CForm andRep), with an F-Score of around 60%.
For bothclassifiers recall is higher than precision.
Note,however, that this is an upper bound for the lexicalchain classifier that would not be obtained in a re-alistic scenario.
An example of the values that canbe expected in a realistic setting (with parameteroptimisation on a development set that is separatefrom the test set) is shown in column five (LC).Here the F-Score is much lower due to lower re-call.
This classifier is too conservative when cre-ating the chains and deciding how to interpret thechain structure; it thus only rarely outputs the lit-eral class.
The reason for this conservatism maybe that literal usages of break the ice (the develop-ment data) tend to have very strong chains, hencewhen optimising the parameters for this data set, itpays to be conservative.
It is positive to note thatthe (unsupervised) graph-based classifier performsjust as well as the (weakly supervised) chain-basedclassifier does under optimal circumstances.
Thismeans that one can by-pass the parameter settingand the need to label development data by employ-ing the graph-based method.Finally, as expected, the supervised classifier760outperforms all other classifiers.
It does so by alarge margin, which is surprising given that it isbased on relatively simplistic model.
This showsthat the context in which an expression occurscan really provide vital cues about its idiomatic-ity.
Note that our results are noticeably higher thanthose reported by Cook et al (2007), Fazly et al(To appear) and Katz and Giesbrecht (2006) forsimilar supervised classifiers.
We believe that thismay be partly explained by the size of our data setwhich is significantly larger than the ones used inthese studies.To assess how well our cohesion-based ap-proach works for different idioms, we also com-puted the accuracy of the graph-based classifier foreach expression individually (Table 3).
We reportaccuracy here rather than literal F-Score as the lat-ter is often undefined for the individual data sets(either because all examples of an expression arenon-literal or because the classifier only predictsnon-literal usages).
It can be seen that the perfor-mance of the classifier is generally relatively sta-ble, with accuracies above 50% for most idioms.12In particular, the classifier performs well on both,expressions with a dominant non-literal meaningand those with a dominant literal meaning; it is notbiased towards the non-literal class.
For expres-sions with a dominant literal meaning like drop theball, it correctly classifies more items as ?literal?
(530 items, 472 of which are correct) than as ?non-literal?
(373 items, 157 correct).5 ConclusionIn this paper, we described a novel method fortoken-based idiom classification.
Our approach isbased on the observation that literally used expres-sions typically exhibit cohesive ties with the sur-rounding discourse, while idiomatic expressionsdo not.
Hence idiomatic expressions can be de-tected by the absence of such ties.
We propose twomethods that exploit this behaviour, one based onlexical chains, the other based on cohesion graphs.We showed that a cohesion-based approach iswell suited for distinguishing literal and non-literal usages, even for expressions in canonicalform which tend to be largely idiomatic and wouldall be classified as non-literal by the previouslyproposed CForm classifier.
Moreover, our find-12Note that the data set for the worst performing idiom,blow one?s own trumpet only contained 9 instances.
Hence,the low performance for this idiom may well be accidental.expression Accuracyback the wrong horse 68.00bite off more than one can chew 79.17bite one?s tongue 37.35blow one?s own trumpet 11.11bounce off the wall* 47.82break the ice 85.03drop the ball* 69.66get one?s feet wet 64.33pass the buck 82.44play with fire 82.33pull the trigger* 60.00rock the boat 98.95set in stone 85.41spill the beans 83.43sweep under the carpet 88.89swim against the tide 93.65tear one?s hair out 49.18Table 3: Accuracies of the graph-based classifieron each of the expressions (* indicates a dominantliteral usage)ings suggest that the graph-based method per-forms nearly as well as the best performance to beexpected for the chain-based method.
This meansthat the task can be addressed in a completely un-supervised way.While our results are encouraging they are stillbelow the results obtained by a basic supervisedclassifier.
In future work we would like to explorewhether better performance can be achieved byadopting a bootstrapping strategy, in which we usethe examples about which the unsupervised clas-sifier is most confident (i.e., those with the largestdifference in connectivity in either direction) as in-put for a second stage supervised classifier.Another potential improvement has to do withthe way in which the cohesion graph is computed.Currently the graph includes all content words inthe context.
This means that the graph is rela-tively big and removing the potential idiom oftendoes not have a big effect on the connectivity; allchanges in connectivity are fairly close to zero.In future, we want to explore intelligent strategiesfor pruning the graph (e.g., by including a smallercontext).
We believe that this might result in morereliable classifications.AcknowledgmentsThis work was funded by the German ResearchFoundation DFG (under grant PI 154/9-3 and theMMCI Cluster of Excellence).
Thanks to AnnaMu?ndelein for her help with preparing the data andto Marco Pennacchiotti and Josef Ruppenhofer,for feedback and comments.761ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL 2003 workshop on Multiwordexpressions: analysis, acquisition and treatment,pages 89?96.Colin Bannard.
2007.
A measure of syntactic flex-ibility for automatically identifying multiword ex-pressions in corpora.
In Proceedings of the ACL-07Workshop on A Broader Perspective on MultiwordExpressions, pages 1?8.Regina Barzilay and Michael Elhadad.
1997.
Usinglexical chains for text summarization.
In Proceed-ings of the ACL-97 Intelligent Scalable Text Summa-rization Workshop (ISTS-97).Julia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for the nearly unsupervised recognition ofnonliteral language.
In Proceedings of EACL-06,pages 329?336.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of semantic dis-tance.
Computational Linguistics, 32(1):13?47.Rudi L. Cilibrasi and Paul M.B.
Vitanyi.
2007.
TheGoogle similarity distance.
IEEE Trans.
Knowledgeand Data Engineering, 19(3):370?383.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurements, 20:37?46.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntacticforms for the automatic identification of idiomaticexpressions in context.
In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multi-word Expressions, pages 41?48.A.P.
Cowie, R. Mackin, and I.R.
McCaig.
1997.
Ox-ford dictionary of English idioms.
Oxford Univer-sity Press.Afsaneh Fazly and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
In Proceedings of EACL-06.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
Toappear.
Unsupervised type and token identificationof idiomatic expressions.
Computational Linguis-tics.M.A.K.
Halliday and R. Hasan.
1976.
Cohesion inEnglish.
Longman House, New York.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In Proceedings ofACL-90, pages 268?275.Graeme Hirst and David St-Onge.
1998.
Lexicalchains as representations of context for the detec-tion and correction of malapropisms.
In ChristianeFellbaum, editor, WordNet: An electronic lexicaldatabase, pages 305?332.
The MIT Press.Graham Katz and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.
InProceedings of the ACL/COLING-06 Workshop onMultiword Expressions: Identifying and ExploitingUnderlying Properties, pages 12?19.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACMTransactions on Speech and Language Processing,2:1?31.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of ACL-98, pages768?774.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,pages 317?324.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Saif Mohammad and Graeme Hirst.
2006.
Distribu-tional measures of concept-distance: A task-orientedevaluation.
In Proceedings of EMNLP-06.Jane Morris and Graeme Hirst.
2004.
Non-classicallexical semantic relations.
In HLT-NAACL-04 Work-shop on Computational Lexical Semantics, pages46?51.Simone Paolo Ponzetto and Michael Strube.
2007.Knowledge derived from Wikipedia for computingsemantic relatedness.
Journal of Artificial Intelli-gence Research, 30:181?212.Susanne Riehemann.
2001.
A Constructional Ap-proach to Idioms and Word Formation.
Ph.D. thesis,Stanford University.H.
Gregory Silber and Kathleen F. McCoy.
2002.
Ef-ficiently computed lexical chains as an intermedi-ate representation for automatic text summarization.Computational Linguistics, 28(4):487?496.Aline Villavicencio, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In Proceedingsof EMNLP-07, pages 1034?1043.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Using wiktionary for computing semantic re-latedness.
In Proceedings of AAAI-08, pages 861?867.Xiaojin Zhu and Ronald Rosenfeld.
2001.
Improvingtrigram language modeling with the world wide web.In Proceedings of ICASSP-01.762
