Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPTransducing Logical Relations from Automatic and Manual GLARFAdam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,Mary Harper?, Ang Sun?, Wei Xu?
and Shasha Liao??
New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ.
of New York, ?JohnsHopkins Human Lang.
Tech.
Ctr.
of Excellence & U. of Maryland, College ParkAbstractGLARF relations are generated from tree-bank and parses for English, Chinese andJapanese.
Our evaluation of system out-put for these input types requires consid-eration of multiple correct answers.11 IntroductionSystems, such as treebank-based parsers (Char-niak, 2001; Collins, 1999) and semantic role la-belers (Gildea and Jurafsky, 2002; Xue, 2008), aretrained and tested on hand-annotated data.
Evalu-ation is based on differences between system out-put and test data.
Other systems use these pro-grams to perform tasks unrelated to the originalannotation.
For example, participating systems inCONLL (Surdeanu et al, 2008; Hajic?
et al, 2009),ACE and GALE tasks merged the results of sev-eral processors (parsers, named entity recognizers,etc.)
not initially designed for the task at hand.This paper discusses differences between hand-annotated data and automatically generated datawith respect to our GLARFers, systems for gen-erating Grammatical and Logical RepresentationFramework (GLARF) for English, Chinese andJapanese sentences.
The paper describes GLARF(Meyers et al, 2001; Meyers et al, 2009) andGLARFers and compares GLARF produced fromtreebank and parses.2 GLARFFigure 1 includes simplified GLARF analyses forEnglish, Chinese and Japanese sentences.
Foreach sentence, a GLARFer constructs both a Fea-ture Structure (FS) representing a constituencyanalysis and a set of 31-tuples, each representing1Support includes: NSF IIS-0534700 & IIS-0534325Structure Alignment-based MT; DARPA HR0011-06-C-0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.This work does not necessarily reflect views of sponsors.up to three dependency relations between pairs ofwords.
Due to space limitations, we will focus onthe 6 fields of the 31-tuple represented in Figure 1.These include: (1) a functor (func); (2) the de-pending argument (Arg); (3) a surface (Surf) la-bel based on the position in the parse tree with noregularizations; (4) a logic1 label (L?1) for a re-lation that reflects grammar-based regularizationsof the surface level.
This marks relations for fill-ing gaps in relative clauses or missing infinitivalsubjects, represents passives as paraphrases as ac-tives, etc.
While the general framework supportsmany regularizations, the relations actually repre-sented depends on the implemented grammar, e.g.,our current grammar of English regularizes acrosspassives and relative clauses, but our grammarsof Japanese and Chinese do not currently.
; (5) alogic2 label (L2) for Chinese and English, whichrepresents PropBank, NomBank and Penn Dis-course Treebank relations; and (6) Asterisks (*)indicate transparent relations, relations where thefunctor inherits semantic properties of certain spe-cial arguments (*CONJ, *OBJ, *PRD, *COMP).Figure 1 contains several transparent relations.The interpretation of the *CONJ relations in theJapanese example, include not only that the nouns[zaisan] (assets) and [seimei] (lives) are con-joined, but also that these two nouns, togetherform the object of the Japanese verb [mamoru](protect).
Thus, for example, semantic selectionpatterns should treat these nouns as possible ob-jects for this verb.
Transparent relations may serveto neutralize some of the problematic cases of at-tachment ambiguity.
For example, in the Englishsentence A number of phrases with modifiers arenot ambiguous, there is a transparent *COMP re-lation between numbers and of and a transpar-ent *OBJ relation between of and phrases.
Thus,high attachment of the PP with modifiers, wouldhave the same interpretation as low attachmentsince phrases is the underlying head of number of116Figure 1: GLARF 5-tuples for 3 languagesphrases.
In this same example, the adverb not canbe attached to either the copula are or the pred-icative adjective, with no discernible difference inmeaning?this factor is indicated by the transparentdesignation of the relations where the copula is afunctor.
Transparent features also provide us witha simple way of handling certain function words,such as the Chinese word De which inherits thefunction of its underlying head, connecting a vari-ety of such modifiers to head nouns (an adjectivein the Chinese example.).
For conjunction cases,the number of underlying relations would multi-ply, e.g., Mary and John bought and sold stockwould (underlyingly) have four subject relationsderived by pairing each of the underlying subjectnouns Mary and John with each of the underlyingmain predicate verbs bought and sold.3 Automatic vs. Manual AnnotationApart from accuracy, there are several other waysthat automatic and manual annotation differs.
ForPenn-treebank (PTB) parsing, for example, mostparsers (not all) leave out function tags and emptycategories.
Consistency is an important goal formanual annotation for many reasons including: (1)in the absence of a clear correct answer, consis-tency helps clarify measures of annotation quality(inter-annotator agreement scores); and (2) consis-tent annotation is better training data for machinelearning.
Thus, annotation specifications use de-faults to ensure the consistent handling of spuriousambiguity.
For example, given a sentence like Ibought three acres of land in California, the PP inCalifornia can be attached to either acres or landwith no difference in meaning.
While annotationguidelines may direct a human annotator to prefer,for example, high attachment, systems output mayhave other preferences, e.g., the probability thatland is modified by a PP (headed by in) versus theprobability that acres can be so modified.Even if the manual annotation for a particularcorpus is consistent when it comes to other factorssuch as tokenization or part of speech, developersof parsers sometimes change these guidelines tosuit their needs.
For example, users of the Char-niak parser (Charniak, 2001) should add the AUXcategory to the PTB parts of speech and adjusttheir systems to account for the conversion of theword ain?t into the tokens IS and n?t.
Similarly, to-kenization decisions with respect to hyphens varyamong different versions of the Penn Treebank, aswell as different parsers based on these treebanks.Thus if a system uses multiple parsers, such differ-ences must be accounted for.
Differences that arenot important for a particular application shouldbe ignored (e.g., by merging alternative analyses).For example, in the case of spurious attachmentambiguity, a system may need to either accept bothas right answers or derive a common representa-tion for both.
Of course, many of the particularproblems that result from spurious ambiguity canbe accounted for in hind sight.
Nevertheless, itis precisely this lack of a controlled environmentwhich adds elements of spurious ambiguity.
Us-ing new processors or training on new treebankscan bring new instances of spurious ambiguity.4 Experiments and EvaluationWe ran GLARFers on both manually created tree-banks and automatically produced parses for En-glish, Chinese and Japanese.
For each corpus, wecreated one or more answer keys by correcting117system output.
For this paper, we evaluate solelyon the logic1 relations (the second column in fig-ure 1.)
Figure 2 lists our results for all three lan-guages, based on treebank and parser input.As in (Meyers et al, 2009), we generated 4-tuples consisting of the following for each depen-dency: (A) the logic1 label (SBJ, OBJ, etc.
), (B)its transparency (True or False), (C) The functor (asingle word or a named entity); and (D) the argu-ment (a single word or a named entity).
In the caseof conjunction where there was no lexical con-junction word, we used either punctuation (com-mas or semi-colons) or the placeholder *NULL*.We then corrected these results by hand to producethe answer key?an answer was correct if all fourmembers of the tuple were correct and incorrectotherwise.
Table 2 provides the Precision, Recalland F-scores for our output.
The F-T columnsindicates a modified F-score derived by ignoringthe +/-Transparent distinction (resulting changesin precision, recall and F-score are the same).For English and Japanese, an expert nativespeaking linguist corrected the output.
For Chi-nese, several native speaking computational lin-guists shared the task.
By checking compatibil-ity of the answer keys with outputs derived fromdifferent sources (parser, treebank), we could de-tect errors and inconsistencies.
We processed thefollowing corpora.
English: 86 sentence article(wsj 2300) from the Wall Street Journal PTB testcorpus (WSJ); 46 sentence letter from Good Will(LET), the first 100 sentences of a switchboardtelephone transcript (TEL) and the first 100 sen-tences of a narrative from the Charlotte Narra-tive and Conversation (NAR).
These samples aretaken from the PTB WSJ Corpus and the SIGANNshared subcorpus of the OANC.
The filenames are:110CYL067, NapierDianne and sw2014.
Chi-nese: a 20 sentence sample of text from thePenn Chinese Treebank (CTB) (Xue et al, 2005).Japanese: 20 sentences from the Kyoto Corpus(KYO) (Kurohashi and Nagao, 1998)5 Running the GLARFer ProgramsWe use Charniak, UMD and KNP parsers (Char-niak, 2001; Huang and Harper, 2009; Kurohashiand Nagao, 1998), JET Named Entity tagger (Gr-ishman et al, 2005; Ji and Grishman, 2006)and other resources in conjunction with language-specific GLARFers that incorporate hand-writtenrules to convert output of these processors intoa final representation, including logic1 struc-ture, the focus of this paper.
English GLAR-Fer rules use Comlex (Macleod et al, 1998a)and the various NomBank lexicons (http://nlp.cs.nyu.edu/meyers/nombank/) forlexical lookup.
The GLARF rules implementedvary by language as follows.
English: cor-recting/standardizing phrase boundaries and partof speech (POS); recognizing multiword expres-sions; marking subconstituents; labeling rela-tions; incorporating NEs; regularizing infiniti-val, passives, relatives, VP deletion, predica-tive and numerous other constructions.
Chi-nese: correcting/standardizing phrase boundariesand POS, marking subconstituents, labeling rela-tions; regularizing copula constructions; incorpo-rating NEs; recognizing dates and number expres-sions.
Japanese: converting to PTB format; cor-recting/standardizing phrase boundaries and POS;labeling relations; processing NEs, double quoteconstructions, number phrases, common idioms,light verbs and copula constructions.6 DiscussionNaturally, the treebank-based system out-performed parse-based system.
The Charniakparser for English was trained on the Wall StreetJournal corpus and can achieve about 90% accu-racy on similar corpora, but lower accuracy onother genres.
Differences between treebank andparser results for English were higher for LET andNAR genres than for the TEL because the systemis not currently designed to handle TEL-specificfeatures like disfluencies.
All processors weretrained on or initially designed for news corpora.Thus corpora out of this domain usually producelower results.
LET was easier as it consistedmainly of short simple sentences.
In (Meyers etal., 2009), we evaluated our results on 40 Japanesesentences from the JENAAD corpus (Utiyamaand Isahara, 2003) and achieved a higher F-score(90.6%) relative to the Kyoto corpus, as JENAADtends to have fewer long complex sentences.By using our answer key for multiple inputs, wediscovered errors and consequently improved thequality of the answer keys.
However, at times wewere also compelled to fork the answer keys?givenmultiple correct answers, we needed to allow dif-ferent answer keys corresponding to different in-puts.
For English, these items represent approxi-mately 2% of the answer keys (there were a total118Treebank ParserID % Prec % Rec F F-T % Prec % Rec F F-TWSJ 12381491 = 83.012381471 = 84.2 83.6 87.111641452 = 80.211641475 = 78.9 79.5 81.8LET 419451 = 92.9419454 = 92.3 92.6 93.3390434 = 89.9390454 = 85.9 87.8 87.8TEL 478627 = 76.2478589 = 81.2 78.6 82.2439587 = 74.8439589 = 74.5 74.7 77.4NAR 8171013 = 80.7817973 =84.0 82.3 84.1724957 = 75.7724969 = 74.7 75.2 76.1CTB 351400 = 87.8351394 = 89.1 88.4 88.7352403 = 87.3352438 = 80.4 83.7 83.7KYO 525575 = 91.3525577 = 91.0 91.1 91.1493581 = 84.9493572 = 86.2 85.5 87.8Figure 2: Logic1 ScoresFigure 3: Examples of Answer Key Divergencesof 74 4-tuples out of a total of 3487).
Figure 3 listsexamples of answer key divergences that we havefound: (1) alternative tokenizations; (2) spuriousdifferences in attachment and conjunction scope;and (3) ambiguities specific to our framework.Examples 1 and 2 reflect different treatments ofhyphenation and contractions in treebank specifi-cations over time.
Parsers trained on different tree-banks will either keep hyphenated words togetheror separate more words at hyphens.
The Treebanktreatment of can?t regularizes so that (can neednot be differentiated from ca), whereas the parsertreatment makes maintaining character offsets eas-ier.
In example 3, the Japanese parser recognizesa single word whereas the treebank divides it intoa prefix plus stem.
Example 4 is a case of differ-ences in character encoding (zero).Example 5 is a common case of spurious attach-ment ambiguity for English, where a transparentnoun takes an of PP complement?nouns such asform, variety and thousands bear the feature trans-parent in the NOMLEX-PLUS dictionary (a Nom-Bank dictionary based on NOMLEX (Macleod etal., 1998b)).
The relative clause attaches eitherto the noun thousands or people and, therefore,the subject gap of the relative is filled by eitherthousands or people.
This ambiguity is spurioussince there is no meaningful distinction betweenthese two attachments.
Example 6 is a case ofattachment ambiguity due to a support construc-tion (Meyers et al, 2004).
The recipient of thegift will be Goodwill regardless of whether thePP is attached to give or gift.
Thus there is notmuch sense in marking one attachment more cor-rect than the other.
Example 7 is a case of conjunc-tion ambiguity?the context does not make it clearwhether or not the pearls are part of a necklace orjust the beads are.
The distinction is of little con-sequence to the understanding of the narrative.Example 8 is a case in which our grammar han-dles a case ambiguously: the prenominal adjectivecan be analyzed either as a simple noun plus ad-jective phrase meaning various businesses or as anoun plus relative clause meaning businesses thatare varied.
Example 9 is a common case in Chi-nese where the verb/noun distinction, while un-clear, is not crucial to the meaning of the phrase ?under either interpretation, 5 billion was exported.7 Concluding RemarksWe have discussed challenges of automatic an-notation when transducers of other annotationschemata are used as input.
Models underly-ing different transducers approximate the origi-nal annotation in different ways, as do transduc-ers trained on different corpora.
We have foundit necessary to allow for multiple correct answers,due to such differences, as well as, genuine andspurious ambiguities.
In the future, we intend toinvestigate automatic ways of identifying and han-dling spurious ambiguities which are predictable,including examples like 5,6 and 7 in figure 3 in-volving transparent functors.119ReferencesE.
Charniak.
2001.
Immediate-head parsing for lan-guage models.
In ACL 2001, pages 116?123.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.D.
Gildea and D. Jurafsky.
2002.
Automatic Label-ing of Semantic Roles.
Computational Linguistics,28:245?288.R.
Grishman, D. Westbrook, and A. Meyers.
2005.Nyu?s english ace 2005 system description.
In ACE2005 Evaluation Workshop.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,M.
A.
Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre,S.
Pado?, J.
?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,N.
Xue, and Y. Zhang.
2009.
The CoNLL-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In CoNLL-2009, Boulder, Col-orado, USA.Z.
Huang and M. Harper.
2009.
Self-training PCFGGrammars with Latent Annotations across Lan-guages.
In EMNLP 2009.H.
Ji and R. Grishman.
2006.
Analysis and Repair ofName Tagger Errors.
In COLING/ACL 2006, Syd-ney, Australia.S.
Kurohashi and M. Nagao.
1998.
Building aJapanese parsed corpus while improving the pars-ing system.
In Proceedings of The 1st InternationalConference on Language Resources & Evaluation,pages 719?724.C.
Macleod, R. Grishman, and A. Meyers.
1998a.COMLEX Syntax.
Computers and the Humanities,31:459?481.C.
Macleod, R. Grishman, A. Meyers, L. Barrett, andR.
Reeves.
1998b.
Nomlex: A lexicon of nominal-izations.
In Proceedings of Euralex98.A.
Meyers, M. Kosaka, S. Sekine, R. Grishman, andS.
Zhao.
2001.
Parsing and GLARFing.
In Pro-ceedings of RANLP-2001, Tzigov Chark, Bulgaria.A.
Meyers, R. Reeves, and Catherine Macleod.
2004.NP-External Arguments: A Study of ArgumentSharing in English.
In The ACL 2004 Workshopon Multiword Expressions: Integrating Processing,Barcelona, Spain.A.
Meyers, M. Kosaka, N. Xue, H. Ji, A.
Sun, S. Liao,and W. Xu.
2009.
Automatic Recognition of Log-ical Relations for English, Chinese and Japanese inthe GLARF Framework.
In SEW-2009 at NAACL-HLT-2009.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,and J. Nivre.
2008.
The CoNLL-2008 Shared Taskon Joint Parsing of Syntactic and Semantic Depen-dencies.
In Proceedings of the CoNLL-2008 SharedTask, Manchester, GB.M.
Utiyama and H. Isahara.
2003.
Reliable Mea-sures for Aligning Japanese-English News Articlesand Sentences.
In ACL-2003, pages 72?79.N.
Xue, F. Xia, F. Chiou, and M. Palmer.
2005.
ThePenn Chinese Treebank: Phrase Structure Annota-tion of a Large Corpus.
Natural Language Engi-neering.N.
Xue.
2008.
Labeling Chinese Predicates with Se-mantic roles.
Computational Linguistics, 34:225?255.120
