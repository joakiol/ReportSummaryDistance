Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29?32,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPA Novel Word Segmentation Approach forWritten Languages with Word Boundary MarkersHan-Cheol Cho?, Do-Gil Lee?, Jung-Tae Lee?, Pontus Stenetorp?, Jun?ichi Tsujii?and Hae-Chang Rim?
?Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan?Dept.
of Computer & Radio Communications Engineering, Korea University, Seoul, Korea{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.krAbstractMost NLP applications work under the as-sumption that a user input is error-free;thus, word segmentation (WS) for writtenlanguages that use word boundary mark-ers (WBMs), such as spaces, has been re-garded as a trivial issue.
However, noisyreal-world texts, such as blogs, e-mails,and SMS, may contain spacing errors thatrequire correction before further process-ing may take place.
For the Korean lan-guage, many researchers have adopted atraditional WS approach, which eliminatesall spaces in the user input and re-insertsproper word boundaries.
Unfortunately,such an approach often exacerbates theword spacing quality for user input, whichhas few or no spacing errors; such is thecase, because a perfect WS model doesnot exist.
In this paper, we propose anovel WS method that takes into consider-ation the initial word spacing informationof the user input.
Our method generatesa better output than the original user in-put, even if the user input has few spacingerrors.
Moreover, the proposed methodsignificantly outperforms a state-of-the-artKorean WS model when the user input ini-tially contains less than 10% spacing er-rors, and performs comparably for casescontaining more spacing errors.
We be-lieve that the proposed method will be avery practical pre-processing module.1 IntroductionWord segmentation (WS) has been a fundamen-tal research issue for languages that do not haveword boundary markers (WBMs); on the con-trary, other languages that do have WBMs have re-garded the issue as a trivial task.
Texts segmentedwith such WBMs, however, could contain a hu-man writer?s intentional or un-intentional spacingerrors; and even a few spacing errors can causeerror-propagation for further NLP stages.For written languages that have WBMs, such asfor the Korean language, the majority of recentresearch has been based on a traditional WS ap-proach (Nakagawa, 2004).
The first step of thetraditional approach is to eliminate all spaces inthe user input, and then re-locate the proper placesto insert WBMs.
One state-of-the-art Korean WSmodel (Lee et al, 2007) is known to achieve a per-formance of 90.31% word-unit precision, which iscomparable with other WS models for the Chineseor Japanese language.Still, there is a downside to the evaluationmethod.
If the user input has a few or no spac-ing errors, traditional WS models may cause morespacing errors than it correct because they producethe same output regardless the word spacing statesof the user input.In this paper, we propose a new WS method thattakes into account the word spacing informationfrom the user input.
Our proposed method firstgenerates the best word spacing states for the userinput by using a traditional WS model; howeverthe method does not immediately apply the out-put.
Secondly, the method estimates a thresholdbased on the word spacing quality of the user in-put.
Finally, the method uses the new word spac-ing states that have probabilities that are higherthan the threshold.The most important contribution of the pro-posed method is that, for most cases, the methodgenerates an output that is better than the user in-put.
The experimental results show that the pro-posed method produces a better output than theuser input even if the user input has less than 1%spacing errors in terms of the character-unit pre-cision.
Moreover, the proposed method outper-forms (Lee et al, 2007) significantly, when the29user input initially contains less than 10% spacingerrors, and even performs comparably, when theinput contains more than 10% errors.
Based onthese results, we believe that the proposed methodwould be a very practical pre-processing modulefor other NLP applications.The paper is organized as follows: Section 2 ex-plains the proposed method.
Section 3 shows theexperimental results.
Finally, the last section de-scribes the contributions of the proposed method.2 The Proposed MethodThe proposed method consists of three steps: abaseline WS model, confidence and threshold es-timation, and output optimization.
The followingsections will explain the steps in detail.2.1 Baseline Word Segmentation ModelWe use the tri-gram Hidden Markov Model(HMM) of (Lee et al, 2007) as the baseline WSmodel; however, we adopt the Maximum Like-lihood (ML) decoding strategy to independentlyfind the best word spacing states.
ML-decodingallows us to directly compare each output to thethreshold.
There is little discrepancy in accuracywhen using ML-decoding, as compared to Viterbi-decoding, as mentioned in (Merialdo, 1994).1Let o1,nbe a sequence of n-character user inputwithout WBMs, xtbe the best word spacing statefor otwhere 1 ?
t ?
n. Assume that xtis either 1(space after ot) or 0 (no space after ot).
Then eachbest word spacing state x?tfor all t can be found byusing Equation 1.x?t= argmaxi?
(0,1)P (xt= i|o1,n) (1)= argmaxi?
(0,1)P (o1,n, xt= i) (2)= argmaxi?
(0,1)?xt?2,xt?1P (xt= i|xt?2, ot?1, xt?1, ot)?
?xt?1P (ot+1|ot?1, xt?1, ot, xt= i)?
?xt+1P (ot+2|ot, xt= i, ot+1, xt+1) (3)Equation 2 is derived by applying the Bayes?rule and by eliminating the constant denominator.Moreover, the equation is simplified, as is Equa-tion 3, by using the Markov assumption, and by1In the preliminary experiment, Viterbi-decoding showeda 0.5% higher word-unit precision.eliminating the constant parts.
Every part of Equa-tion 3 can be calculated by adding the probabilitiesof all possible combinations of xt?2, xt?1, xt+1and xt+2values.The model is trained by using the relative fre-quency information of the training data, and asmoothing technique is applied to relieve the data-sparseness problem which is the linear interpola-tion of n-grams that are used in (Lee et al, 2007).2.2 Confidence and Threshold EstimationWe set a variable threshold that is proportional tothe word spacing quality of the user input, Confi-dence.
Formally, we can define the threshold T asa function of a confidence C, as in Equation 4.T = f(C) (4)Then, we define the confidence as is done inEquation 5.
Because calculating such a variableis impossible, we estimate the value by substi-tuting the word spacing states produced by thebaseline WS model, xWS1,n, with the correct wordspacing states, xcorrect1,n, as is done in Equation 6.This estimation is based on the assumption thatthe word spacing states of the WS model is suf-ficiently similar to the correct word spacing statesin the character-unit precision.2C =# of xinputtsame to xcorrectt# of xinputt(5)?# of xinputtsame to xWSt# of xinputt(6)?n???
?n?k=1P (xinputk|o1,n) (7)To handle the estimation error for short sen-tences, we use the probability generating wordspacing states of the user input with the length nor-malization as shown in Equation 7.Figure 1 shows that the estimated confidence ofEquation 7 is almost linearly proportional to thetrue confidence of Equation 5, thus suggesting thatthe threshold T can be defined as a function of theestimated confidence of Equation 7.32In the experiment with the development data, the base-line WS model shows about 97% character-unit precision.3The development data is generated by randomly intro-ducing spacing errors into correctly spaced sentences.
Wethink that this reflects various intentional and un-intentionalerror patterns of individuals.3020%30%40%50%60%70%80%90%100%100% 96% 92% 88% 84% 80%Estimated ConfidenceTrue ConfidenceFigure 1: The relationship between estimated con-fidence and true confidenceTo keep the focus on the research subject of thispaper, we simply assume f(x) = x as in Equation8, for the threshold function f .T ?
f(C) = C (8)In the experimental results, we confirm thateven this simple threshold function can be help-ful in improving the performance of the proposedmethod against traditional WS models.2.3 Output OptimizationAfter completing the two steps described in Sec-tion 2.1 and 2.2, we have acquired the new spacingstates for the user input generated by the baselineWS model, and the threshold measuring the wordspacing quality of the user input.The proposed method only applies a part of thenew word spacing states to the user input, whichhave probabilities that are higher than the thresh-old; further the method discards the other newword spacing states that have probabilities that arelower than the threshold.
By rejecting the unreli-able output of the baseline WS model in this way,the proposed method can effectively improve theperformance when the user input contains a rela-tively small number of spacing errors.3 Experimental ResultsTwo types of experiments have been performed.In the first experiment, we investigate the level ofperformance improvement based on different set-tings of the user input?s word spacing error rate.Because it is nearly impossible to obtain enoughtest data for any error rate, we generate pseudo testdata in the same way that we generate develop-ment data.4In the second experiment, we attempt4See Footnote 3.figuring out whether the proposed method reallyimproves the word spacing quality of the user in-put in a real-world setting.3.1 Performance Improvement according tothe Word Spacing Error Rate of UserInputFor the first experiment, we use the Sejong corpus5from 1998-1999 (1,000,000 Korean sentences) forthe training data, and ETRI corpus (30,000 sen-tences) for the test data (ETRI, 1999).
To gener-ate the test data that have spacing errors, we maketwenty one copies of the test data and randomlyinsert spacing errors from 0% to 20% in the sameway in which we made the development data.
Wefeel that this strategy can model both the inten-tional and un-intentional human error patterns.In Figure 2, the x-axis indicates the word spac-ing error rate of the user input in terms of thecharacter-unit precision, and the y-axis shows theword-unit precision of the output.
Each graph de-picts the word-unit precision of the test corpus,a state-of-the-art Korean WS model (Lee et al,2007), the baseline WS model, and the proposedmethod.Although Lee?s model is known to performcomparably with state-of-the-art Chinese andJapanese WS models, it does not necessarily sug-gest that the word spacing quality of the model?soutput is better than the user input.
In Figure 2,Lee?s model exacerbates the user input when it hasspacing errors that are lower than 3%.The proposed method, however, produces a bet-ter output, even if the user input has 1% spacing er-rors.
Moreover, the proposed method shows a con-siderably better performance within the 10% spac-ing error range, as compared to Lee?s model, al-though the baseline WS model itself does not out-performs Lee?s model.
The performance improve-ment in this error range is fairly significant be-cause we found that the spacing error rate of textscollected for the second experiment was about9.1%.3.2 Performance Comparison with Web Texthaving Usual Error RateIn the second experiment, we attempt finding outwhether the proposed method can be beneficial un-der real-world circumstances.
Web texts, whichconsist of 1,000 erroneous sentences from famous5Details available at: http://www.sejong.or.kr/eindex.php3184%86%88%90%92%94%96%98%100%0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%word-unitprecisionword spacing error rate of user input (in character-unit precision)Test corpus Lee's model Baseline WS model Proposed methodFigure 2: Performance improvement according to the word spacing error rate of user inputMethod Web TextTest Corpus 70.89%Lee?s Model 70.45%Baseline WS Model 69.13%Proposed Method 73.74%Table 1: Performance comparison with Web textWeb portals and personal blogs, were collectedand used as the test data.
Since the test data tendto have a similar error rate to the narrow standarddeviation, we computed the overall performanceover the average word spacing error rate, which is9.1%.
The baseline WS model is trained on theSejong corpus, described in Section 3.1.The test result is shown in Table 1.
Theoverall performance of Lee?s model, the baselineWS model and the proposed method decreasedby roughly 18%.
We hypothesize that the per-formance degradation probably results from thespelling errors of the test data, and the inconsis-tencies that exist between the training data and thetest data.
However, the proposed method still im-proves the word spacing quality of the user inputby 3%, while the two traditional WS models de-grades the quality.
Such a result indicates thatthe proposed method is effective for real-worldenvironments, as we had intended.
Furthermore,we also believe that the performance can be im-proved if a proper training corpus is provided, orif a spelling correction method is integrated.4 ConclusionIn this paper, we proposed a new WS method thatuses the word spacing information of the user in-put, for languages with WBMs.
By utilizing theuser input, the proposed method effectively refinesthe output of the baseline WS model and improvesthe overall performance.The most important contribution of this work isthat it produces an output that is better than theuser input even if it contains few spacing errors.Therefore, the proposed method can be applied asa pre-processing module for practical NLP appli-cations without introducing a risk that would gen-erate a worse output than the user input.
Moreover,the performance is notably better than a state-of-the-art Korean WS model (Lee et al, 2007) withinthe 10% spacing error range, which human writersseldom exceed.
It also performs comparably, evenif the user input contains more than 10% spacingerrors.5 AcknowledgmentThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan)and Special Coordination Funds for PromotingScience and Technology (MEXT, Japan).ReferencesETRI.
1999.
Pos-tag guidelines.
Technical report.Electronics and Telecomminications Research Insti-tute.Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.2007.
Automatic Word Spacing Using ProbabilisticModels Based on Character n-grams.
IEEE Intelli-gent Systems, 22(1):28?35.Bernard Merialdo.
1994.
Tagging English text with aprobabilistic model.
Comput.
Linguist., 20(2):155?171.Tetsuji Nakagawa.
2004.
Chinese and Japanese wordsegmentation using word-level and character-levelinformation.
In COLING ?04, page 466, Morris-town, NJ, USA.
Association for Computational Lin-guistics.32
