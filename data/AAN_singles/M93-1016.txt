NEW YORK UNIVERSITY :DESCRIPTION OF THE PROTEUS SYSTEMAS USED FOR MUC- 5Ralph Grishman and John SterlingThe Proteus ProjectComputer Science Departmen tNew York University715 Broadway, 7th FloorNew York, NY 1000 3{ grishman,sterling } @cs .nyu .eduTHE PROTEUS SYSTE MHistoryThe Proteus system which we have used for MUC-5 is largely unchanged from thatused for MUC-3 and MUC-4 .
It has three main components : a syntactic analyzer, asemantic analyzer, and a template generator .The Proteus syntactic analyzer was developed starting in the fall of 1984 as acommon base for all the applications of the Proteus Project .
Many aspects of it sdesign reflect its heritage in the Linguistic String Parser, previously developed an dstill in use at New York University .The current system, including the RestrictionLanguage compiler, the lexical analyzer, and the parser proper,compriseapproximately 4500 lines of Common Lisp .The semantic analyzer was initially developed in 1987 for the MUCK- I(RAINFORMs) application, extended for the MUCK-II (OPREPS) application, and ha sbeen incrementally revised since .
It currently consists of about 3000 lines o fCommon Lisp (excluding the domain-specific information) .The template generator was written from scratch for the MUC-5 joint ventur etask; it is about 1200 lines of Common Lisp .Stages of processingThe text goes through the five major stages of processing : lexical analysis ,syntactic analysis, semantic analysis, reference resolution, and template generation(see figure 1) .
In addition, some restructuring of the logical form is performed bot hafter semantic analysis and after reference resolution (only the restructuring afterreference resolution is shown in figure 1) .
Processing is basically sequential: eachsentence goes through lexical, syntactic, and semantic analysis and referenc eresolution ; the logical form for the entire message is then fed to template generation .However, semantic (selectional) checking is performed during syntactic analysis ,employing essentially the same code later used for semantic analysis .181Knowledge SourcesLexical AnalysisCGrammarSemantic ModelsSyntactic AnalysisSemantic AnalysisReference ResolutionMapping Rules)LF TransformationTemplate Generatio nTemplatesFigure 1 .
Proteus System Structure .182Lexical AnalysisDictionary Forma tOur dictionaries contain only syntactic information :the parts of speech for eachword, information about the complement structure of verbs, distributiona linformation (e .g., for adjectives and adverbs), etc .
We follow closely the set o fsyntactic features established for the NYU Linguistic String Parser .
This informatio nis entered in LISP form using noun, verb, adjective, and adverb macros for the open -class words, and a word macro for other parts of speech :(ADVERB "ABRUPTLY" :ATTRIBUTES (DSA) )(ADJECTIVE "ABRUPT" )(NOUN :ROOT "ABSCESS" :ATTRIBUTES (NCOUNT) )(VERB :ROOT "ABSCOND" :OBJLIST (NULLOBJ PN (PVAL (FROM WITH))) )The noun and verb macros automatically generate the regular inflectional forms .DictionaryFilesThe primary source of our dictionary information about open-class words (nouns ,verbs, adjectives, and adverbs) is the machine-readable version of the Oxfor dAdvanced Learner's Dictionary ("OALD") .
We have written programs which take th eSGML (Standard Generalized Markup Language) version of the dictionary, extrac tinformation on inflections, parts of speech, and verb subcategorization (includinginformation on adverbial particles and prepositions gleaned from the examples), an dgenerate the LISP-ified form shown above.
This is supplemented by a manually-codeddictionary (about 1500 lines, 900 entries) for closed-class words, words not adequatel ydefined in the OALD, and a few very common words .
In addition, we used severa lspecialized dictionaries for MUC-5, including a location dictionary (with al lcountries, continents, and major cities (CITY1 or , PORT1 in the gazetteer), a dictionaryof corporate designators, a dictionary of job titles, and a dictionary of currencies .Looku pThe text reader splits the input text into tokens and then attempts to assign to eac htoken (or sequence of tokens, in the case of an idiom) a definition (part of speech an dsyntactic attributes) .
The matching process proceeds in five steps : dictionarylookup, lexical pattern matching, spelling correction, prefix stripping, and defaul tdefinition assignment .
Dictionary lookup immediately retrieves definitions assigne dby any of the dictionaries (including inflected forms) .
The specialized dictionarie sare stored in memory, while the main dictionary is accessed from disk (using hashe dindex random access) .Lexical pattern matching is used to identify a variety of specialized patterns, suc has numbers, dates, times, and possessive forms .
The set of lexical patterns wassubstantially expanded for MUC-5 to include various forms of people's names ,company names, locations, and currencies .The lexical patterns are further discusse dbelow, in the "What's new for MUC-5" section .If neither dictionary lookup nor lexical pattern matching is successful, spellin gcorrection and prefix stripping are attempted.
For words of any length, we identifyan input token as a misspelled form of a dictionary entry if one of the two has asingle instance of a letter while the other has a doubled instance of the letter (e .g .
,"mispelled" and "misspelled") .
The prefix stripper attempts to identify the token as acombination of a prefix (e .g.,"un") and a word defined in the dictionary .183If all of these procedures fail, we assign a default definition .
In mixed case text ,undefined capitalized words are tagged as proper nouns ; undefined lower case wordsare tagged as common nouns .
In monocase text, all undefined words are tagged a sproper nouns .Syntactic AnalysisSyntactic analysis involves two stages of processing: parsing and syntacticregularization.
At the core of the system is an active chart parser .
The grammar i san augmented context-free grammar, consisting of BNF rules plus procedura lrestrictions which check grammatical constraints not easily captured in the BN Frules.
Most restrictions are stated in PROTEUS Restriction Language (a variant of th elanguage developed for the Linguistic String Parser) and translated into LISP ; a feware coded directly in LISP [1] .
For example, the count noun restriction (that singularcountable nouns have a determiner) is stated asWCOUNT = IN LNR AFTER NVAR :IF BOTH CORE Xcore IS NCOUNT AND Xcore IS SINGULARTHEN IN LN, TPOS IS NOT EMPTY .Associated with each BNF rule is a regularization rule, which computes th eregularized form of each node in the parse tree from the regularized forms of itsimmediate constituents.
These regularization rules are based on lambda-reduction, a sin GPSG.
The primary function of syntactic regularization is to reduce all clauses to astandard form consisting of aspect and tense markers, the operator (verb o radjective), and syntactically marked cases .
For example, the definition of assertion ,the basic S structure in our grammar, i s<assertion><sa> <subject> <sa> <verb> <sa> <object> <sa>:(s !
(<object> <subject> <verb> <sa*>)) .Here the portion after the single colon defines the regularized structure .Coordinate conjunction is introduced by a metarule (as in GPSG), which is applie dto the context-free components of the grammar prior to parsing .
The regularizationprocedure expands any conjunction into a conjuntion of clauses or of noun phrases .The output of the parser for the first sentence of 0592, "BRIDGESTONE SPORTS CO .SAID FRIDAY IT HAS SET UP A JOINT VENTURE IN TAIWAN WITH A LOCAL CONCERN AND AJAPANESE TRADING HOUSE TO PRODUCE GOLF CLUBS TO BE SHIPPED TO JAPAN . "
i s184(SENTENCE(CENTERS(CENTER(ASSERTION(SUBJECT(NSTG(LNR (NVAR (NAMESTG (LNAMER (N "BRIDGESTONE" "SPORTS" "CO" " ."))))))
)(VERB (LTVR (TV "SAID")) )(SA (SA-VAL (NSTGT (NSTG (LNR (NVAR (N "FRIDAY")))))) )(OBJECT(ASSERTION (SUBJECT (NSTG (LNR (NVAR (PRO "IT")))) )(VERB (LTVR (TV "HAS")) )(OBJECT(VENO (LVENR (VEN "SET" "up") )(OBJECT(NSTGO(NSTG(LNR (LN (TPOS (LTR (T "A")))) (NVAR (N "JOINT" "venture") )(RN(RN-VAL(PN (P "IN" )(NSTGO (NSTG (LNR (NVAR (NAMESTG (LNAMER (N "TAIWAN")))))))) )(MORE-RN(RN-VAL(PN (P "WITH" )(NSTGO(NSTG(LNR(LNR(LN (TPOS (LTR (T "A")) )(APOS (APOSVAR (AVAR (ADJ "LOCAL")))) )(NVAR (N "CONCERN")) )(CONJ-WORD ("AND" "AND") )(LNR(LN (TPOS (LTR (T "A")) )(APOS (APOSVAR (AVAR (ADJ "JAPANESE")))) )(NVAR (N "TRADING" "house"))))))) )(MORE-RN(RN-VAL(TOVO (TO ("TO" "TO")) (LVR (V "PRODUCE") )(OBJECT(NSTGO(NSTG(LNR (LN (NPOS (NPOSVAR (N "GOLF"))) )(NVAR (N "CLUBS"))))) )(SA(SA-VAL(TOVO (TO ("TO" "TO")) (LVR (V "BE") )(OBJECT(OBJECTBE(VENPASS (LVENR (VEN "SHIPPED") )(SA(SA-VAL(PN (P "TO" )(NSTGO(NSTG(LNR(NVAR(NAMESTG(LNAMER(N "JAPAN"))))))))))))))))))))))))))))))) )(ENDMARK (" ."
" ."))
)and the corresponding regularized structure i s185(S SAY (VTENSE PAST )(SUBJECT(NP A-COMPANY SINGULAR (NAMES ("BRIDGESTONE" "SPORTS" "CO")) (SN NP154)) )(OBJECT(S SET-UP (SUBJECT (NP IT SINGULAR (SN NP156)) )(OBJECT(NP JOINT-VENTURE SINGULAR (SN NP258) (T-POS A )(IN (NP A-COUNTRY SINGULAR (NAMES ("Taiwan")) (SN NP163)) )(WITH(AND (NP CONCERN SINGULAR (SN NP166) (T-POS A) (A-POS LOCAL) )(NP TRADING-HOUSE SINGULAR (SN NP171) (T-POS A) (A-POS JAPANESE))) )(RN-TOVO(S PRODUCE (SUBJECT ANYONE )(OBJECT(NP CLUB PLURAL (SN NP180) (N-POS (NP GOLF SINGULAR (SN NP170)))) )(IN-ORDER-TO(S SHIP (SUBJECT ANYONE) (OBJECT PRO )(TO (NP A-COUNTRY SINGULAR (NAMES ("Japan")) (SN NP177)))))))) )(ASPECT PERF) (VTENSE PRESENT)) )(TIMEPREP (NP FRIDAY SINGULAR (SN NP157))) )The system uses a chart parser operating top-down, left-to-right.
As edges arecompleted (i .e., as nodes of the parse tree are built), restrictions associated with thos eproductions are invoked to assign and test features of the parse tree nodes .If arestriction fails, that edge is not added to the chart .
When certain levels of the treeare complete (those producing noun phrase and clause structures), th eregularization rules are invoked to compute a regularized structure for the partia lparse, and selection is invoked to verify the semantic well-formedness of th estructure (as noted earlier, selection uses the same "semantic analysis" codesubsequently employed to translate the tree into logical form) .One unusual feature of the parser is its weighting capability .
Restrictions mayassign scores to nodes ; the parser will perform a best-first search for the parse treewith the highest score.
This scoring is used to implement various preferenc emechanisms :?
closest attachment of modifiers (we penalize each modifier by the number o fwords separating it from its head)?
preferred narrow conjoining for clauses (we penalize a conjoined claus estructure by the number of words it subsumes )?
preference semantics (selection does not reject a structure, but imposes a heav ypenalty if the structure does not match any lexico-semantic model, and a lesserpenalty if the structure matches a model but with some operands or modifiers leftover) [2,3 ]?
relaxation of certain syntactic constraints, such as the count noun constraint ,adverb position constraints, and comma constraints?
disfavoring (penalizing) headless noun phrases and headless relatives (this i simportant for parsing efficiency )The grammar is based on Harris's Linguistic String Theory and adapted from th elarger Linguistic String Project (LSP) grammar developed by Naomi Sager at NYU [4] .The grammar is gradually being enlarged to cover more of the LSP grammar .Thecurrent grammar is 1600 lines of BNF and Restriction Language plus 300 lines of Lisp ;it includes 186 non-terminals, 464 productions, and 132 restrictions .Over the course of the MUCs we have added several mechanisms for recoverin gfrom sentences the grammar cannot fully parse .
For MUC-5, we found that the mos t186effective was our "fitted parse" mechanism, which attempts to cover the sentenc ewith noun phrases and clauses, preferring the longest noun phrases or clauseswhich can be identifiedSemantic Analysis And Reference Resolutio nThe output of syntactic analysis goes through semantic analysis and referenc eresolution and is then added to the accumulating logical form for the message.Following both semantic analysis and reference resolution certain transformation sare performed to simplify the logical form.
All of this processing makes use of aconcept hierarchy which captures the class/subclass/instance relations in th edomain .Semantic analysis uses a set of lexico-semantic models to map the regularizedsyntactic analysis into a semantic representation .
Each model specifies a class o fverbs, adjectives, or nouns and a set of operands ; for each operand it indicates th epossible syntactic case markers, the semantic class of the operand, whether or no tthe operand is required, and the semantic case to be assigned to the operand in th eoutput representation.
For example, the model for "<entity> forms a joint venture wit h<entity>" i s(add-clause-model :id 'clause-form:parent 'clause-any:constraint 'W-form-venture:class 'C-form:adjuncts (list (make-specifier:marker 'subject:class 'C-muc5-entity:case :agent )(make-specifier:marker 'with:class 'C-muc5-entity:case :company-list-2 )(make-specifier:marker 'object:class 'C-joint-ventur e:essential-required 'require d:relaxable ni l:case :joint-venture)) )The models are arranged in a shallow hierarchy with inheritance, so tha targuments and modifiers which are shared by a class of verbs need only be statedonce.
The model above inherits only from the most general clause model, clause-any ,which includes general clausal modifiers such as negation, time, tense, modality, etc .The MUC-5 system has 61 clause models, 2 nominalization models, and 45 other nou nphrase models, a total of about 1700 lines .
The class C -mu c 5 -entity in the clause modelrefers to the concept in the concept hierarchy, whose entries have the form :(defconcept C-muc5-entity )(defconcept C-company :typeof C-muc5-entity )(defconcept C-government-or-country :typeof C-muc5-entity )(defconcept C-venture :typeof C-company )(defconcept C-joint-venture :typeof C-venture )This inheritance mechanism is also used to define word classes, such as the w -f or m -venture class :187(defconcept W-form-venture )(defconcept form :typeof W-form-venture )(defconcept establish :typeof W-form-venture )(defconcept expand :typeof W-form-venture )(defconcept launch :typeof W-form-venture )(defconcept set-up :typeof W-form-venture )There are currently a total of 154 concepts in the hierarchy .The output ofsemantic analysis is a nested set of entity and event structures, with argument slabeled by keywords primarily designating semantic roles .For the first sentence of0593, the output is188(EVENT:IDENTIFIER E00 1:TOP-LEVEL-FLAG T:PREDICATE C-SAY:TIME (ENTITY :SN NP15 7:MODEL-ID TIME-VALU E:IDENTIFIER N015:CLASS FRIDAY ):EVENT (EVEN T:IDENTIFIER E003:TOP-LEVEL-FLAG NI L:PREDICATE C-FORM:TENSE PRESEN T:ASPECT PERF:JOINT-VENTURE (ENTITY:ACTIVITY (EVENT :IDENTIFIER E01 0:TOP-LEVEL-FLAG NI L:PREDICATE C-PRODUCE:PATIENT (ENTITY :SN NP18 0:MODEL-ID NP-ANY:SET T:IDENTIFIER N014:CLASS CLUB ):AGENT NIL:MODEL-ID CLAUSE-PRODUCE ):AGENT (ENTITY :IDENTIFIER N009:CLASS C-MUC5-ENTIT Y:SET T:MEMBERS ((ENTITY :DETERMINER A:SN NP16 6:MODEL-ID NP-COMPANY- 2:IDENTIFIER N007:CLASS CONCERN )(ENTITY :NATIONALITY JAPANES E:DETERMINER A:SN NP17 1:MODEL-ID NP-COMPANY- 2:IDENTIFIER N008:CLASS TRADING-HOUSE)) ):NATIONALITY (ENTITY :SN NP163:NAMES ("Taiwan" ):MODEL-ID NP-A-COUNTRY:IDENTIFIER N00 6:CLASS C-COUNTRY ):DETERMINER A:SN NP25 8:MODEL-ID NP-JOINT-VENTURE- 1:IDENTIFIER N00 5:CLASS C-JOINT-VENTURE ):AGENT (ENTITY :SN NP15 6:MODEL-ID NP-ANY:IDENTIFIER N004:CLASS C-MUC5-ENTITY ):MODEL-ID CLAUSE-FORM ):AGENT (ENTITY :SN NP15 4:NAMES ("BRIDGESTONE" "SPORTS" "CO" ):MODEL-ID NP-COMPAN Y:IDENTIFIER N000000000 2:CLASS C-COMPANY ):TENSE PAS T:MODEL-ID CLAUSE-SAY)189Reference resolutio nReference resolution is applied to the output of semantic analysis in order t oreplace anaphoric noun phrases (representing either events or entities) b yappropriate antecedents .
Each potential anaphor is compared to prior entities orevents, looking for a suitable antecedent such that the class of the anaphor (in theconcept hierarchy) is equal to or more general than that of the antecedent, theanaphor and antecedent match in number, the restrictive modifiers in the anaphorhave corresponding arguments in the antecedent, and the non-restrictive modifier s(e .g., apposition) of the anaphor are not inconsistent with those of the antecedent .Special tests are provided for names, since people and companies may be referred to asubset of their full names .Logical form transformationsThe transformations which are applied after semantic analysis and afte rreference resolution simplify and regularize the logical form in various ways .
Thetransformations after semantic analysis primarily standardize the attribute structureof entities so that reference resolution will work properly .
The transformation safter reference resolution simplify the task of template generation by casting th eevents in a more uniform framework and performing a limited number ofinferences.
For example, we show here a rule which transforms the logical for mproduced from "X formed a joint venture with Y" into the equivalent for "X and Yformed a joint venture" :(((event :predicate ?predicat e:identifier ?id l:agent ?agent:joint-venture (entity.
?R1 )(entity :identifier ?id2 .
?R3 )(condition (isa '?predicate 'C-tie-up)) )-->((modify 1 (list :agent (conjoin-entitie s'?agent '?company-list-2)) )(modify 2 '( :agent nil :tied-up t))) )There are currently 32 such rules .
These transformations are written a sproductions and applied using a simple data-driven production system interprete rwhich is part of the Proteus system .Template generato rOnce all the sentences in an article have been processed through syntacti canalysis, semantic analysis, and the logical form transformations, the resultinglogical forms are sent to the template generator .
The logical form events and entitiesproduced by the transformations are in close correspondence to the template object sneeded for MUC-5, so the template generation is fairly straightforward .
The greates tcomplexity was involved in the procedures for accessing the two large data bases, thegazetteer (for normalizing locations) and the Standard Industrial Classification (forclassifying industries) .
:class C-joint-venture:tied-up ni l:agent ?company-list- 2:identifier ?id2.
?R2 )190OUR PERFORMANCE ON MUC- 5Overall Performanc eScore sOur overall scores on the final evaluation wereRecall 22Precision 59F (P&R) 32LearningError RateCurve80Figure 2 showns how our recall gradually improved over the development period .Precision remained within a fairly narrow range, from 47 to 63, throughout thetesting.
Five months were available for development (March - July) .
One person wasassigned full-time for the entire period ; a second person assisted, approximately 2/ 3time, for the last three months, for a total of about 7 person-months of effort (thi sexcludes time in August preparing for the conference) .
March and April weredevoted to getting an initial understanding of the fill rules, making minimal lexica lscanner additions so that we could parse the text, developing input code to handle thedifferent article formats, and developing some routines for larger-scale patternmatching (which were eventually not used) .
System integration and integratedsystem testing did not begin until mid-May, a couple of weeks before the dry run .Daily system testing began with a set of 25 articles, but shifted after the dry run t othe first 100 dry-run messages (with the second 100 dry-run messages being used onoccasion as a blind test) .Figure 2: Recall (Dry Run, part 1 )25 ?0	 I	 I	 I	 {	 I	 I	 I	 I	 IIMay- Jun- Jun- Jun- Jun- Jul-01 Jul-02 Jul-10 Jul-16 Jul-17 Jul-2 3261423252919 1In comparison with earlier MUCs, the overhead of getting started --understanding the fill rules, handling the different article formats, generating themore complex templates, and using the various data bases (gazetteer, SIC, currenc ytable, corporate designator table) -- was much greater than for prior MUCs, while th emanpower we had for the project was in fact somewhat less.
In consequence, oursystem is relatively less developed than our MUC-3 system, for example.
Inparticular, the attribute structure for the principal entity types (for MUC-5 ,companies) were less developed ;this adversely impacted the performance of ourreference resolution component and hence our event merging .This impact was evident in our performance on the walkthrough message, 0593 .We identified the primary constituent events (the joint venture and the associate downership relations), but we failed to identify several of the co-reference relations ,because of?
a bug in the handling of appositional names followed by relative clausesfailure to do spelling correction on names (we only correct spellings to matc hdictionary entries)shortcomings in the attribute structure of company entitie sBecause of these problems and a weak event merging rule (compared to the mor edetailed rules developed for MUC-4, for example), we generated two separate tie-upsfor the article, instead of one.The system was also not tuned to any significant degree to take advantage of th eMUC-5 scoring rules.
Based on a suggestion by Boyan Onyshkevych, we conducted asmall experiment after the conference.
Because one is told in advance that almostevery article in the corpus will have a reportable event, we modified the system t ogenerate a tie-up between a Japanese company and an Indonesian company (the tw omost frequent nationalities in the training corpus) whenever the text analysi scomponents were not able to find a tie up.
This simple strategy reduced our errorrate on the test corpus by 2% .WHAT'S NEW FOR MUC- 5Lexical AnalyzerThe Proteus system has a pattern matcher based on regular expressions wit hprovision for procedural tests, which is intended for identifying lexical units beforeparsing .Prior to MUC-5, the system employed a small number of patterns, fo rstructures such as dates, times, and numbers .The set of patterns was substantiall yenlarged for MUC-5, to include patterns for diffeent types of currencies, for companynames, for people's names, for locations, and for names of indeterminate type .
Inmixed-case text, we used capitalization as the primary indication of the beginning ofa name; in monocase text, we employed BBN's part-of-speech tagger and looked forproper noun tags.The lexical scanner and the constraints of the lexico-semantic models acted i nconcert to classify names.
If there was a clear lexical clue (a corporate designator atthe end of a name, a title ("Mr.", "President", .
.
.)
or middle initial in a personal name) ,the type was assigned by the lexical scanner .
If the type of a name could not bedetermined by the scanner, but the name occurred in a context where only one typ ewas allowed (e.g., as the object of "own"), the type would be assigned as a side effect ofapplying the lexico-semantic model .192Semantic Pattern and Similarity Acquisitio nWe have spent considerable time over the last two years building tools to acquir esemantic patterns and semantic word similarities from corpora [5, 6], and we hadhoped that these would be of significant benefit in our MUC-5 efforts, particularly i nbroadening our system's coverage.
However, we did not have much opportunity t ouse these tools, since so much of our time was consumed in building an initial syste mat some minimal performance level .Nested Semantic ModelsThe lexico-sematic models as used previously specified a single level in th eregularized parse tree structure : either a clause with its arguments and modifiers, oran NP with its modifiers .
We have found it increasingly valuable, however, to be abl eto specify ' larger patterns which involve several parse tree levels, such as "X signe dan agreement with Y to do Z", or "X formed a joint venture with Y to do Z" .
We havetherefore extended our system in order to allow for such larger patterns, and permitthe developer to specify the predicate structure into which this larger pattern shoul dbe mapped.Model BuilderOnce we began to allow these larger patterns, we found that the task of writin gsuch patterns correctly became quite challenging .
Our long-term goal is to enable auser to add such patterns, but we seemed (with the added complexity) to be movingfurther from this goal .
We therefore implemented a "model builder" interface whic hallows the developer to enter a prototype sentence and the corresponding predicatestructure which should be produced .
The interface then creates the required lexico-semantic patterns and mapping rules .For example, to handle constructs of the form "company signed an agreemen twith company to .
.
.
", the developer would enter the sentencecompanyl signed (an agreement with company2 to act3) .
(where the braces, which are optional, indicate the NP bracketing) and would givethe corresponding predicate(c-agree :agent companyl :co-agent company2 :event act3 )The system would then create models and mapping rules appropriate to a sentenc esuch as "IBM signed an agreement with Apple to form a joint venture."
Since theserules apply to the syntactically analyzed sentence, they would also handle syntacti cvariants such as "The agreement to create the new venture was signed last week b yIBM and Ford .
"REFERENCE S[1] Grishman, R. PROTEUS Parser Reference Manual .PROTEUS ProjectMemorandum #4-C, Computer Science Department, New York University, May 1990.
[2] Grishman, R ., and Sterling, J .
Preference Semantics for Messag eUnderstanding .
Proc.
DARPA Speech and Natural Language Workshop, MorganKaufman, 1990 (proceedings of the conference at Harwich Port, MA, Oct .
15-18, 1989) .
[3] Grishman, R ., and Sterling, J., Information Extraction and Semanti cConstraints.
Proc.
13th Int'l Conf Computational Linguistics (COLING 90), Helsinki ,August 20-25, 1990 .
[4] Sager, N. Natural Language Information Processing, Addison-Wesley, 1981 .193[5] Grishman, R., and Sterling, J .
Acquisition of Selectional Patterns .Proc.
14thIntl Conf.
Computational Linguistics (COLING 92), Nantes, France, July 23-28, 1992.
[6] Grishman, R ., and Sterling, J. Smoothing of Automatically GeneratedSelectional Constraints .
Proc.
ARPA Human Language Technology Workshop, March21-24, 1993 .SPONSORSHI PThe development of the entire PROTEUS system has been sponsored primarily b ythe Advanced Research Projects Agency as part of the Strategic Computing Program ,under Contract N00014-85-K-0163 and Grant N00014-90-J-1851 from the Office o fNaval Research.194
