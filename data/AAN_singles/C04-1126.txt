Information Extraction from Single and Multiple SentencesMark StevensonDepartment of Computer ScienceRegent Court, 211 Portobello Street,University of SheffieldSheffieldS1 4DP, UKmarks@dcs.shef.ac.ukAbstractSome Information Extraction (IE) systemsare limited to extracting events expressedin a single sentence.
It is not clear what ef-fect this has on the difficulty of the extrac-tion task.
This paper addresses the prob-lem by comparing a corpus which has beenannotated using two separate schemes: onewhich lists all events described in the textand another listing only those expressedwithin a single sentence.
It was found thatonly 40.6% of the events in the first anno-tation scheme were fully contained in thesecond.1 IntroductionInformation Extraction (IE) is the process ofidentifying specific pieces of information in text,for example, the movements of company execu-tives or the victims of terrorist attacks.
IE is acomplex task and a the description of an eventmay be spread across several sentences or para-graphs of a text.
For example, Figure 1 showstwo sentences from a text describing manage-ment succession events (i.e.
changes in corpo-rate executive management personnel).
It canbe seen that the fact that the executives areleaving and the name of the organisation arelisted in the first sentence.
However, the namesof the executives and their posts are listed in thesecond sentence although it does not mentionthe fact that the executives are leaving theseposts.
The succession events can only be fullyunderstood from a combination of the informa-tion contained in both sentences.Combining the required information acrosssentences is not a simple task since it is neces-sary to identify phrases which refer to the sameentities, ?two top executives?
and ?the execu-tives?
in the above example.
Additional diffi-culties occur because the same entity may bereferred to by a different linguistic unit.
For ex-ample, ?International Business Machines Ltd.?may be referred to by an abbreviation (?IBM?
),Pace American Group Inc. said it notifiedtwo top executives it intends to dismiss thembecause an internal investigation found ev-idence of ?self-dealing?
and ?undisclosed fi-nancial relationships.?
The executives areDon H. Pace, cofounder, president and chiefexecutive officer; and Greg S. Kaplan, seniorvice president and chief financial officer.Figure 1: Event descriptions spread across twosentencesnickname (?Big Blue?)
or anaphoric expressionsuch as ?it?
or ?the company?.
These complica-tions make it difficult to identify the correspon-dences between different portions of the text de-scribing an event.Traditionally IE systems have consisted ofseveral components with some being responsi-ble for carrying out the analysis of individualsentences and other modules which combine theevents they discover.
These systems were of-ten designed for a specific extraction task andcould only be modified by experts.
In an ef-fort to overcome this brittleness machine learn-ing methods have been applied to port sys-tems to new domains and extraction tasks withminimal manual intervention.
However, someIE systems using machine learning techniquesonly extract events which are described withina single sentence, examples include (Soderland,1999; Chieu and Ng, 2002; Zelenko et al, 2003).Presumably an assumption behind these ap-proaches is that many of the events describedin the text are expressed within a single sen-tence and there is little to be gained from theextra processing required to combine event de-scriptions.Systems which only attempt to extract eventsdescribed within a single sentence only reportresults across those events.
But the proportionof events described within a single sentence isnot known and this has made it difficult to com-pare the performance of those systems againstones which extract all events from text.
Thisquestion is addressed here by comparing twoversions of the same IE data set, the evaluationcorpus used in the Sixth Message Understand-ing Conference (MUC-6) (MUC, 1995).
Thecorpus produced for this exercise was annotatedwith all events in the corpus, including thosedescribed across multiple sentences.
An inde-pendent annotation of the same texts was car-ried out by Soderland (1999), although he onlyidentified events which were expressed within asingle sentence.
Directly comparing these datasets allows us to determine what proportion ofall the events in the corpus are described withina single sentence.The remainder of this paper is organised asfollows.
Section 2 describes the formats for rep-resenting events used in the MUC and Soder-land data sets.
Section 3 introduces a commonrepresentation scheme which allows events tobe compared, a method for classifying types ofevent matches and a procedure for comparingthe two data sets.
The results and implicationsof this experiment are presented in Section 4.Some related work is discussed in Section 5.2 Event Scope and RepresentationThe topic of the sixth MUC (MUC-6) wasmanagement succession events (Grishman andSundheim, 1996).
The MUC-6 data has beencommonly used to evaluate IE systems.
Thetest corpus consists of 100 Wall Street Jour-nal documents from the period January 1993to June 1994, 54 of which contained manage-ment succession events (Sundheim, 1995).
Theformat used to represent events in the MUC-6corpus is now described.2.1 MUC RepresentationEvents in the MUC-6 evaluation data arerecorded in a nested template structure.
Thisformat is useful for representing complex eventswhich have more than one participant, for ex-ample, when one executive leaves a post to bereplaced by another.
Figure 2 is a simplifiedevent from the the MUC-6 evaluation similarto one described by Grishman and Sundheim(1996).This template describes an event in which?John J. Dooner Jr.?
becomes chairman of thecompany ?McCann-Erickson?.
The MUC tem-plates are too complex to be described fully herebut some relevant features can be discussed.Each SUCCESSION EVENT contains the name of<SUCCESSION_EVENT-9402240133-2> :=SUCCESSION_ORG:<ORGANIZATION-9402240133-1>POST: "chairman"IN_AND_OUT: <IN_AND_OUT-9402240133-4>VACANCY_REASON: DEPART_WORKFORCE<IN_AND_OUT-9402240133-4> :=IO_PERSON: <PERSON-9402240133-1>NEW_STATUS: INON_THE_JOB: NOOTHER_ORG: <ORGANIZATION-9402240133-1>REL_OTHER_ORG: SAME_ORG<ORGANIZATION-9402240133-1> :=ORG_NAME: "McCann-Erickson"ORG_ALIAS: "McCann"ORG_TYPE: COMPANY<PERSON-9402240133-1> :=PER_NAME: "John J. Dooner Jr."PER_ALIAS: "John Dooner""Dooner"Figure 2: Example Succession event in MUCformatthe POST, organisation (SUCCESSION ORG) andreferences to at least one IN AND OUT sub-template, each of which records an event inwhich a person starts or leaves a job.
TheIN AND OUT sub-template contains details of thePERSON and the NEW STATUS field which recordswhether the person is starting a new job or leav-ing an old one.Several of the fields, including POST, PERSONand ORGANIZATION, may contain aliases whichare alternative descriptions of the field fillerand are listed when the relevant entity was de-scribed in different was in the text.
For ex-ample, the organisation in the above templatehas two descriptions: ?McCann-Erickson?
and?McCann?.
It should be noted that the MUCtemplate structure does not link the field fillersonto particular instances in the texts.
Conse-quently if the same entity description is usedmore than once then there is no simple way ofidentifying which instance corresponds to theevent description.The MUC templates were manually filled byannotators who read the texts and identified themanagement succession events they contained.The MUC organisers provided strict guidelinesabout what constituted a succession event andhow the templates should be filled which the an-notators sometimes found difficult to interpret(Sundheim, 1995).
Interannotator agreementwas measured on 30 texts which were examinedby two annotators.
It was found to be 83% whenone annotator?s templates were assumed to becorrect and compared with the other.2.2 Soderland?s RepresentationSoderland (1999) describes a supervised learn-ing system called WHISK which learned IErules from text with associated templates.WHISK was evaluated on the same texts fromthe MUC-6 data but the nested template struc-ture proved too complex for the system to learn.Consequently Soderland produced his own sim-pler structure to represent events which he de-scribed as ?case frames?.
This representationcould only be used to annotate events describedwithin a single sentence and this reduced thecomplexity of the IE rules which had to belearned.The succession event from the sentence?Daniel Glass was named president andchief executive officer of EMI RecordsGroup, a unit of London?s Thorn EMIPLC.?
would be represented as follows:1@@TAGS Succession{PersonIn DANIEL GLASS}{Post PRESIDENT AND CHIEF EXECUTIVE OFFICER}{Org EMI RECORDS GROUP}Events in this format consist of up to fourcomponents: PersonIn, PersonOut, Post andOrg.
An event may contain all four componentsalthough none are compulsory.
The minimumpossible set of components which can form anevent are (1) PersonIn, (2) PersonOut or (3)both Post and Org.
Therefore a sentence mustcontain a certain amount of information to belisted as an event in this data set: the nameof an organisation and post participating in amanagement succession event or the name of aperson changing position and the direction ofthat change.Soderland created this data from the MUC-6 evaluation texts without using any of theexisting annotations.
The texts were firstpre-processing using the University of Mas-sachusetts BADGER syntactic analyser (Fisheret al, 1995) to identify syntactic clauses and thenamed entities relevant to the management suc-cession task: people, posts and organisations.Each sentence containing relevant entities wasexamined and succession events manually iden-tified.1The representation has been simplified slightly forclarity.This format is more practical for machinelearning research since the entities which par-ticipate in the event are marked directly in thetext.
The learning task is simplified by the factthat the information which describes the eventis contained within a single sentence and so thefeature space used by a learning algorithm canbe safely limited to items within that context.3 Event Comparison3.1 Common Representation andTransformationThere are advantages and disadvantages to theevent representation schemes used by MUC andSoderland.
The MUC templates encode moreinformation about the events than Soderland?srepresentation but the nested template struc-ture can make them difficult to interpret man-ually.In order to allow comparison between eventseach data set was transformed into a com-mon format which contains the informationstored in both representations.
In this formateach event is represented as a single databaserecord with four fields: type, person, post andorganisation.
The type field can take thevalues person in, person out or, when the di-rection of the succession event is not known,person move.
The remaining fields take theperson, position and organisation names fromthe text.
These fields may contain alternativevalues which are separated by a vertical bar(?|?
).MUC events can be translated into thisformat in a straightforward way since eachIN AND OUT sub-template corresponds to a sin-gle event in the common representation.
TheMUC representation is more detailed thanthe one used by Soderland and so some in-formation is discarded from the MUC tem-plates.
For example, the VACANCY REASONfiled which lists the reason for the manage-ment succession event is not transfered tothe common format.
The event listed inFigure 2 would be represented as follows:type(person in)person(?John J. Dooner Jr.?|?John Dooner?|?Dooner?)org(?McCann-Erickson?|?McCann?
)post(chairman)Alternative fillers for the person and orgfields are listed here and these correspond to thePER NAME, PER ALIAS, ORG NAME and ORG ALIASfields in the MUC template.The Soderland succession event shownin Section 2.2 would be representedas follows in the common format.type(person in)person(?Daniel Glass?)post(?president?
)org(?EMI Records Group?
)type(person in)person(?Daniel Glass?
)post(?chief executive officer?
)org(?EMI Records Group?
)In order to carry out this transformation anevent has to be generated for each PersonIn andPersonOut mentioned in the Soderland event.Soderland?s format also lists conjunctions ofpost names as a single slot filler (?president andchief executive officer?
in this example).
Theseare treated as separate events in the MUC for-mat.
Consequently they are split into the sepa-rate post names and an event generated for eachin the common representation.It is possible for a Soderland event to consistof only a Post and Org slot (i.e.
there is nei-ther a PersonIn or PersonOut slot).
In thesecases an underspecified type, person move, isused and no person field listed.
Unlike MUCtemplates Soderland?s format does not containalternative names for field fillers and so thesenever occur when an event in Soderland?s for-mat is translated into the common format.3.2 MatchingThe MUC and Soderland data sets can be com-pared to determine how many of the eventsin the former are also contained in the latter.This provides an indication of the proportion ofevents in the MUC-6 domain which are express-ible within a single sentence.
Matches betweenSoderland and MUC events can be classified asfull, partial or nomatch.
Each of these possi-bilities may be described as follows:Full A pair of events can only be fully match-ing if they contain the same set of fields.
Inaddition there must be a common filler foreach field.
The following pair of events arean example of two which fully match.type(person in)person(?R.
Wayne Diesel?|?Diesel?
)org(?Mechanical Technology Inc.?|?Mechanical Technology?
)post(?chief executive officer?
)type(person in)person(?R.
Wayne Diesel?
)org(?Mechanical Technology?
)post(?chief executive officer?
)PartialA partial match occurs when one eventcontains a proper subset of the fields of an-other event.
Each field shared by the twoevents must also share at least one filler.The following event would partially matcheither of the above events; the org field isabsent therefore the matches would not befull.type(person in)person(?R.
Wayne Diesel?
)post(?chief executive officer?
)Nomatch A pair of events do not match if theconditions for a full or partial match are notmet.
This can occur if corresponding fieldsdo not share a filler or if the set of fieldsin the two events are not equivalent or onethe subset of the other.Matching between the two sets of events iscarried out by going through each MUC eventand comparing it with each Soderland event forthe same document.
The MUC event is firstcompared with each of the Soderland events tocheck whether there are any equal matches.
Ifone is found a note is made and the matchingprocess moves onto the next event in the MUCset.
If an equal match is not found the MUCevent is again compared with the same set ofSoderland events to see whether there are anypartial matches.
We allow more than one Soder-land event to partially match a MUC event sowhen one is found the matching process con-tinues through the remainder of the Soderlandevents to check for further partial matches.4 Results4.1 Event level analysisAfter transforming each data set into the com-mon format it was found that there were 276events listed in the MUC data and 248 in theSoderland set.
Table 1 shows the number ofmatches for each data set following the match-ing process described in Section 3.2.
The countsunder the ?MUC data?
and ?Soderland data?headings list the number of events which fallinto each category for the MUC and Soderlanddata sets respectively along with correspondingpercentages of that data set.
It can be seen that112 (40.6%) of the MUC events are fully cov-ered by the second data set, and 108 (39.1%)partially covered.Match MUC data Soderland dataType Count % Count %Full 112 40.6% 112 45.2%Partial 108 39.1% 118 47.6%Nomatch 56 20.3% 18 7.3%Total 276 248Table 1: Counts of matches between MUC andSoderland data.Table 1 shows that there are 108 events inthe MUC data set which partially match withthe Soderland data but that 118 events in theSoderland data set record partial matches withthe MUC data.
This occurs because the match-ing process allows more than one Soderlandevent to be partially matched onto a singleMUC event.
Further analysis showed that thedifference was caused by MUC events whichwere partially matched by two events in theSoderland data set.
In each case one eventcontained details of the move type, person in-volved and post title and another contained thesame information without the post title.
This iscaused by the style in which the newswire sto-ries which make up the MUC corpus are writ-ten where the same event may be mentioned inmore than one sentence but without the samelevel of detail.
For example, one text containsthe sentence ?Mr.
Diller, 50 years old, succeedsJoseph M. Segel, who has been named to thepost of chairman emeritus.?
which is later fol-lowed by ?At that time, it was announced thatDiller was in talks with the company on becom-ing its chairman and chief executive upon Mr.Segel?s scheduled retirement this month.
?Table 1 also shows that there are 56 events inthe MUC data which fall into the nomatch cat-egory.
Each of these corresponds to an event inone data set with no corresponding event in theother.
The majority of the unmatched MUCevents were expressed in such a way that therewas no corresponding event listed in the Soder-land data.
The events shown in Figure 1 areexamples of this.
As mentioned in Section 2.2,a sentence must contain a minimum amount ofinformation to be marked as an event in Soder-land?s data set, either name of an organisationand post or the name of a person changing po-sition and whether they are entering or leaving.In Figure 1 the first sentence lists the organisa-tion and the fact that executives were leaving.The second sentence lists the names of the exec-utives and their positions.
Neither of these sen-tences contains enough information to be listedas an event under Soderland?s representation,consequently the MUC events generated fromthese sentences fall into the nomatch category.It was found that there were eighteen eventsin the Soderland data set which were not in-cluded in the MUC version.
This is unexpectedsince the events in the Soderland corpus shouldbe a subset of those in the MUC corpus.
Anal-ysis showed that half of these corresponded tospurious events in the Soderland set which couldnot be matched onto events in the text.
Many ofthese were caused by problems with the BAD-GER syntactic analyser (Fisher et al, 1995)used to pre-process the texts before manualanalysis stage in which the events were identi-fied.
Mistakes in this pre-processing sometimescaused the texts to read as though the sentencecontained an event when it did not.
We exam-ined the MUC texts themselves to determinewhether there was an event rather than relyingon the pre-processed output.Of the remaining nine events it was foundthat the majority (eight) of these correspondedto events in the text which were not listed inthe MUC data set.
These were not identi-fied as events in the MUC data because of thethe strict guidelines, for example that historicalevents and non-permanent management movesshould not be annotated.
Examples of theseevent types include ?...
Jan Carlzon, who leftlast year after his plan for a merger with threeother European airlines failed.?
and ?CharlesT.
Young, chief financial officer, stepped downvoluntarily on a ?temporary basis pending con-clusion?
of the investigation.?
The analysis alsoidentified one event in the Soderland data whichappeared to correspond to an event in the textbut was not listed in the MUC scenario tem-plate for that document.
It could be arguedthat there nine events should be added to theset of MUC events and treated as fully matches.However, the MUC corpus is commonly used asa gold standard in IE evaluation and it was de-cided not to alter it.
Analysis indicated thatone of these nine events would have been a fullmatch and eight partial matches.It is worth commenting that the analysis car-ried out here found errors in both data sets.There appeared to be more of these in theSoderland data but this may be because theevent structures are much easier to interpretand so errors can be more readily identified.
It isalso difficult to interpret the MUC guidelines insome cases and it sometimes necessary to makea judgement over how they apply to a particularevent.4.2 Event Field AnalysisA more detailed analysis can be carried outexamining the matches between each of thefour fields in the event representation individu-ally.
There are 1,094 fields in the MUC data.Although there are 276 events in that dataset seven of them do not mention a post andthree omit the organisation name.
(Organisa-tion names are omitted from the template whenthe text mentions an organisation descriptionrather than its name.
)Table 4.2 lists the number of matches for eachof the four event fields across the two data sets.Each of the pairs of numbers in the main bodyof the table refers to the number of matching in-stances of the relevant field and the total num-ber of instances in the MUC data.The column headed ?Full match?
lists theMUC events which were fully matched againstthe Soderland data and, as would be expected,all fields are matched.
The column marked?Partial match?
lists the MUC events whichare matched onto Soderland fields via partiallymatching events.
The column headed ?No-match?
lists the event fields for the 56 MUCevents which are not represented at all in theSoderland data.Of the total 1,094 event fields in the MUCdata 727, 66.5%, can be found in the Soderlanddata.
The rightmost column lists the percent-ages of each field for which there was a match.The counts for the type and person fields are thesame since the type and person fields are com-bined in Soderland?s event representation andhence can only occur together.
These figuresalso show that there is a wide variation betweenthe proportion of matches for the different fieldswith 76.8% of the person and type fields be-ing matched but only 43.2% of the organisationfield.This difference between fields can be ex-plained by looking at the style in which the textsforming the MUC evaluation corpus are writ-ten.
It is very common for a text to introducea management succession event near the startof the newswire story and this event almost in-variably contains all four event fields.
For ex-ample, one story starts with the following sen-tence: ?Washington Post Co. said KatharineGraham stepped down after 20 years as chair-man, and will be succeeded by her son, Don-ald E. Graham, the company?s chief executiveofficer.?
Later in the story further successionevents may be mentioned but many of these usean anaphoric expression (e.g.
?the company?
)rather than explicitly mention the name of theorganisation in the event.
For example, this sen-tence appears later in the same story: ?Alan G.Spoon, 42, will succeed Mr. Graham as presi-dent of the company.?
Other stories again mayonly mention the name of the person in the suc-cession event.
For example, ?Mr.
Jones is suc-ceeded by Mr. Green?
and this explains whysome of the organisation fields are also absentfrom the partially matched events.4.3 DiscussionFrom some perspectives it is difficult to see whythere is such a difference between the amountof events which are listed when the entire textis viewed compared with considering single sen-tences.
After all a text comprises of an orderedlist of sentences and all of the information thetext contains must be in these.
Although, as wehave seen, it is possible for individual sentencesto contain information which is difficult to con-nect with the rest of the event description whena sentence is considered in isolation.The results presented here are, to some ex-tent, dependent on the choices made when rep-resenting events in the two data sets.
Theevents listed in Soderland?s data require a min-imal amount of information to be containedwithin a sentence for it to be marked as con-taining information about a management suc-cession event.
Although it is difficult to see howany less information could be viewed as repre-senting even part of a management successionevent.5 Related WorkHuttunen et al (2002) found that there is varia-tion between the complexity of IE tasks depend-ing upon how the event descriptions are spreadthrough the text and the ways in which they areencoded linguistically.
The analysis presentedhere is consistent with their finding as it hasFull match Partial match Nomatch TOTAL %Type 112 / 112 100 / 108 0 / 56 212 / 276 76.8%Person 112 / 112 100 / 108 0 / 56 212 / 276 76.8%Org 112 / 112 6 / 108 0 / 53 118 / 273 43.2%Post 111 / 111 74 / 108 0 / 50 185 / 269 68.8%Total 447 / 447 280 / 432 0 / 215 727 / 1094 66.5%Table 2: Matches between MUC and Soderland data at field levelbeen observed that the MUC texts are oftenwritten in such as way that the name of theorganisation in the event is in a different partof the text to the rest of the organisation de-scription and the entire event can only be con-structed by resolving anaphoric expressions inthe text.
The choice over which informationabout events should be extracted could have aneffect on the difficulty of the IE task.6 ConclusionsIt seems that the majority of events are not fullydescribed within a single sentence, at least forone of the most commonly used IE evaluationsets.
Only around 40% of events in the originalMUC data set were fully expressed within theSoderland data set.
It was also found that thereis a wide variation between different event fieldsand some information may be more difficult toextract from text when the possibility of eventsbeing described across multiple sentences is notconsidered.
This observation should be bornein mind when deciding which approach to usefor a particular IE task and should be used toput the results reported for IE systems whichextract from a single sentence into context.AcknowledgementsI am grateful to Stephen Soderland for allowingaccess to his version of the MUC-6 corpus andadvice on its construction.
Robert Gaizauskasand Beth Sundheim also provided advice on thedata used in the MUC evaluation.
Mark Hep-ple provided valuable comments on early draftsof this paper.
I am also grateful to an anony-mous reviewer who provided several useful sug-gestions.ReferencesH.
Chieu and H. Ng.
2002.
A MaximumEntroy Approach to Information Extractionfrom Semi-structured and Free Text.
In Pro-ceedings of the Eighteenth International Con-ference on Artificial Intelligence (AAAI-02),pages 768?791, Edmonton, Canada.D.
Fisher, S. Soderland, J. McCarthy, F. Feng,and W. Lehnert.
1995.
Description of theUMass system as used for MUC-6.
In Pro-ceedings of the Sixth Message Understand-ing Conference (MUC-6), pages 221?236, SanFrancisco, CA.R.
Grishman and B. Sundheim.
1996.
Mes-sage understanding conference - 6 : A briefhistory.
In Proceedings of the 16th Interna-tional Conference on Computational Linguis-tics (COLING-96), pages 466?470, Copen-hagen, Denmark.S.
Huttunen, R. Yangarber, and R. Grishman.2002.
Complexity of Event Structures in IEScenarios.
In Proceedings of the 19th Interna-tional Conference on Computational Linguis-tics (COLING-2002), pages 376?382, Taipei,Taiwan.MUC.
1995.
Proceedings of the Sixth Mes-sage Understanding Conference (MUC-6),San Mateo, CA.
Morgan Kaufmann.S.
Soderland.
1999.
Learning Information Ex-traction Rules for Semi-structured and freetext.
Machine Learning, 31(1-3):233?272.B.
Sundheim.
1995.
Overview of results ofthe MUC-6 evaluation.
In Proceedings ofthe Sixth Message Understanding Conference(MUC-6), pages 13?31, Columbia, MA.D.
Zelenko, C. Aone, and A. Richardella.
2003.Kernel methods for relation extraction.
Jour-nal of Machine Learning Research, 3:1083?1106.
