A New Taxonomy for the Quality of Telephone ServicesBased on Spoken Dialogue SystemsSebastian Mo?llerInstitute of Communication AcousticsRuhr?University BochumD?44780 Bochum, Germanymoeller@ika.ruhr-uni-bochum.deAbstractThis document proposes a new taxon-omy for describing the quality of serviceswhich are based on spoken dialogue sys-tems (SDSs), and operated via a telephoneinterface.
It is used to classify instru-mentally or expert?derived dialogue andsystem measures, as well as quality fea-tures perceived by the user of the service.A comparison is drawn to the quality ofhuman?to?human telephone services, andimplications for the development of evalu-ation frameworks such as PARADISE arediscussed.1 IntroductionTelephone services which rely on spoken dialoguesystems (SDSs) have now been introduced at a largescale.
For the human user, when dialing the num-ber it is often not completely clear that the agenton the other side will be a machine, and not a hu-man operator.
Because of this fact, and because theinteraction with the SDS is performed through thesame type of user interface (e.g.
the handset tele-phone), comparisons will automatically be drawn tothe quality of human?human communication overthe same channel, and sometimes with the same pur-pose.
Thus, while acknowledging the differences inbehaviors from both ?
human and machine ?
sides,it seems justified to take the human telephone inter-action (HHI) as one reference for telephone?basedhuman?machine interaction (HMI).The quality of interactions with spoken dialoguesystems is difficult to determine.
Whereas structuredapproaches have been documented on how to designspoken dialogue systems so that they adequatelymeet the requirements of their users (e.g.
by Bernsenet al, 1998), the quality which is perceived when in-teracting with SDSs is often addressed in an intuitiveway.
Hone and Graham (2001) describe efforts todetermine the underlying dimensions in user qualityjudgments, by performing a multidimensional anal-ysis on subjective ratings obtained on a large numberof different scales.
The problem obviously turnedout to be multi?dimensional.
Nevertheless, manyother researchers still try to estimate ?overall systemquality?, ?usability?
or ?user satisfaction?
by sim-ply calculating the arithmetic mean over several userratings on topics as different as perceived TTS qual-ity, perceived system understanding, and expectedfuture use of the system.
The reason is the lack ofan adequate description of quality dimensions, bothwith respect to the system design and to the percep-tion of the user.In this paper, an attempt is made to close thisgap.
A taxonomy is developed which allows qual-ity dimensions to be classified, and methods for theirmeasurement to be developed.
The starting point forthis taxonomy was a similar one which has fruitfullybeen used for the description of human?to?humanservices in telecommunication networks (e.g.
tra-ditional telephony, mobile telephony, or voice overIP), see Mo?ller (2000).
Such a taxonomy can behelpful in three respects: (1) system elements whichare in the hands of developers, and responsible forspecific user perceptions, can be identified, (2) thePhiladelphia, July 2002, pp.
142-153.
Association for Computational Linguistics.Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,dimensions underlying the overall impression of theuser can be described, together with adequate (sub-jective) measurement methods, and (3) predictionmodels can be developed to estimate quality ?
as itwould be perceived by the user ?
from purely instru-mental measurements.
While we are still far fromthe last point in HMI, examples will be presented ofthe first two issues.The next section will discuss what is understoodby the term ?quality?, and will present the taxon-omy for HMI.
In Section 3, quality features under-lying the aspects of the taxonomy are identified, anddialogue- and system-related measures for each as-pect are presented in Section 4, based on measureswhich are commonly documented in literature.
Sec-tion 5 shows the parallels to the original taxonomyfor HHI.
The outlook gives implications for the de-velopment of evaluation and prediction models, suchas the PARADISE framework.2 Quality of Service TaxonomyIt is obvious that quality is not an entity which couldbe measured in an easy way, e.g.
using a techni-cal instrument.
The quality of a service results fromthe perceptions of its user, in relation to what theyexpect or desire from the service.
In the following,it will thus be made use of the definition of qualitydeveloped by Jekosch (2000):?Quality is the result of the judgment ofa perceived constitution of an entity withregard to its desired constitution.
[...] Theperceived constitution contains the totalityof the features of an entity.
For the per-ceiving person, it is a characteristic of theidentity of the entity.
?The entity to be judged in our case is the servicethe user interacts with (through the telephone net-work), and which is based on a spoken dialogue sys-tem.
Its quality is a compromise between what s/heexpects or desires, and the characteristics s/he per-ceives while using the service.At this point, it is useful to differentiate betweenquality elements and quality features, as it was alsoproposed by Jekosch.
Whereas the former are sys-tem or service characteristics which are in the handsof the designer (and thus can be optimized to reachhigh quality), the latter are perceptive dimensionsforming the overall picture in the mind of the user.Generally, no stable relationship which would bevalid for all types of services, users and situationscan be established between the two.
Evaluationframeworks such as PARADISE establish a tem-porary relationship, and try to reach some cross?domain validity.
Due to the lack of quality elementswhich can really be manipulated in some way by thedesigner, however, the framework has to start mostlyfrom dialogue and system measures which cannot bedirectly controlled.
These measures will be listed inSection 4.The quality of a service (QoS) is often addressedonly from the designer side, e.g.
in the definitionused by the International Telecommunication Unionfor telephone services (ITU?T Rec.
E.800, 1994).It includes service support, operability, security andserveability.
Whereas these issues are necessary fora successful set?up of the service, they are not di-rectly perceived by the user.
In the following tax-onomy, the focus is therefore put on the user side.The overall picture is presented in Figure 1.
It il-lustrates the categories (white boxes) which can besub?divided into aspects (gray boxes), and their rela-tionships (arrows).
As the user is the decision pointfor each quality aspect, user factors have to be seenin a distributed way over the whole picture.
Thisfact has tentatively been illustrated by the gray canson the upper side of the taxonomy, but will not befurther addressed in this paper.
The remaining cate-gories are discussed in the following.Walker et al (1997) identified three factors whichcarry an influence on the performance of SDSs, andwhich therefore are thought to contribute to its qual-ity perceived by the user: agent factors (mainly re-lated to the dialogue and the system itself), task fac-tors (related to how the SDS captures the task it hasbeen developed for) and environmental factors (e.g.factors related to the acoustic environment and thetransmission channel).
Because the taxonomy refersto the service as a whole, a fourth point is addedhere, namely contextual factors such as costs, typeof access, or the availability.
All four types of factorssubsume quality elements which can be expected tocarry an influence on the quality perceived by theuser.
The corresponding quality features are sum-marized into aspects and categories in the followingqualityofserviceenvironmentalfactorsagentfactorstaskfactorscontextualfactorsspeechi/oqualitydialoguecooperativitydialoguesymmetrycommunicationefficiency comforttaskefficiencyusability serviceefficiencyeconomicalbenefitusersatisfactionutilityacceptabilityattitude experi-enceemotionsflexibility motivation,goalstask/domainknowledgetransm.channelbackgr.noiseroomacousticstaskcoveragedomaincov .taskflexibilitytaskdif ficultycostsavailabilityopeninghoursaccessintelligibilitynaturalnesslistening-effortsystemunderst.informativenesstruth&evidencerelevancemannerbackgr.know .meta-comm.handl.initiativeinteractioncontrolpartnerasymmetrypersonalitycognitivedemandtasksuccesstaskeaseeaseofuseenjoyabilityserviceadequacyaddedvaluefutureuseuserfactorslinguisticbackgr.speed/p acedialogueconcisenessdialoguesmoothnessvaluabilitysystemknowledgedialoguestrategydialogueflexibilityFigure 1: QoS schematic for task?oriented HCI.lower part of the picture.The agent factors carry an influence on three qual-ity categories.
On the speech level, input and outputquality will have a major influence.
Quality featuresfor speech output have been largely investigated inthe literature, and include e.g.
intelligibility, natu-ralness, or listening?effort.
They will depend on thewhole system set?up, and on the situation and taskthe user is confronted with.
Quality features relatedto the speech input from the user (and thus to thesystem?s recognition and understanding capabilities)are far less obvious.
They are, in addition, muchmore difficult to investigate, because the user onlyreceives an indirect feedback on the system?s capa-bilities, namely from the system reactions which areinfluences by the dialogue as a whole.
Both speechinput and output are highly influenced by the envi-ronmental factors.On the language and dialogue level, dialogue co-operativity has been identified as a key requirementfor high?quality services (Bernsen et al, 1998).
Theclassification of cooperativity into aspects whichwas proposed by Bernsen et al, and which is re-lated to Grice?s maxims (Grice, 1975) of cooperativebehavior in HHI, is mainly adopted here, with oneexception: we regard the partner asymmetry aspectunder a separate category called dialogue symme-try, together with the aspects initiative and interac-tion control.
Dialogue cooperativity will thus coverthe aspects informativeness, truth and evidence, rel-evance, manner, the user?s background knowledge,and meta?communication handling strategies.Adopting the notion of efficiency used by ETSIand ISO (ETSI Technical Report ETR 095, 1993),efficiency designates the effort and resources ex-panded in relation to the accuracy and complete-ness with which users can reach specified goals.
Itis proposed to differentiate three categories of effi-ciency.
Communication efficiency relates to the ef-ficiency of the dialogic interaction, and includes ?besides the aspects speed and conciseness ?
alsothe smoothness of the dialogue (which is sometimescalled ?dialogue quality?).
Note that this is a signif-icant difference to many other notions of efficiency,which only address the efforts and resources, but notthe accuracy and completeness of the goals to bereached.
Task efficiency is related to the success ofthe system in accomplishing the task; it covers tasksuccess as well as task ease.
Service efficiency isthe adequacy of the service as a whole for the pur-pose defined by the user.
It also includes the ?addedvalue?
which is contributed to the service, e.g.
incomparison to other means of information (compa-rable interfaces or human operators).In addition to efficiency aspects, other aspects ex-ist which relate to the agent itself, as well as itsperception by the user in the dialogic interaction.We subsume these aspects under the category ?com-fort?, although other terms might exist which bet-ter describe the according perceptions of the user.Comfort covers the agent?s ?social personality?
(per-ceived friendliness, politeness, etc.
), as well as thecognitive demand required from the user.Depending on the area of interest, several notionsof usability are common.
Here, we define usabil-ity as the suitability of a system or service to fulfillthe user?s requirements.
It considers mainly the easeof using the system and may result in user satisfac-tion.
It does, however, not cover service efficiency oreconomical benefit, which carry an influence on theutility (usability in relation to the financial costs andto other contextual factors) of the service.
Walkeret al (1998) also state that ?user satisfaction ratings[...] have frequently been used in the literature as anexternal indicator of the usability of an agent.?
AsKamm and Walker (1997), we assume that user sat-isfaction is predictive of other system designer ob-jectives, e.g.
the willingness to use or pay for a ser-vice.
Acceptability, which is commonly defined onthis more or less ?economic?
level, can therefore beseen in a relationship to usability and utility.
It isa multidimensional property of a service, describinghow readily a customer will use the service.
The ac-ceptability of a service (AoS) can be represented asthe ratio of potential users to the quantity of the tar-get user group, see definitions on AoS adopted byEURESCOM (EURESCOM Project P.807 Deliver-able 1, 1998).From the schematic, it can be seen that a largenumber of aspects contribute to what can be calledcommunication efficiency, usability or user satisfac-tion.
Several interrelations (and a certain degree ofinevitable overlap) exist between the categories andaspects, which are marked by arrows.
The interrela-tions will become more apparent by taking a closerlook to the underlying quality features which can beassociated with each aspect.
They will be presentedin the following section.3 Classification of Quality FeaturesIn Tables 1 and 2, an overview is given of the qual-ity features underlying each aspect of the QoS tax-onomy.
For the aspects related to dialogue coop-erativity, these aspects partly stem from the designguideline definitions given by Bernsen et al (1998).For the rest, quality features which have been usedin experimental investigations on different types ofdialogue systems have been classified.
They do notsolely refer to telephone?based services, but will bevalid for a broader class of systems and services.By definition, quality features are percepts of theusers.
They can consequently only be measuredby asking users in realistic scenarios, in a subjec-tive way.
Several studies with this aim are reportedin the literature.
The author analyzed 12 such in-vestigations and classified the questions which wereasked to the users (as far as they have been reported)according to the quality features.
For each aspectgiven in Tables 1 and 2, at least two questions couldbe identified which addressed this aspect.
This clas-sification cannot be reproduced here for space rea-sons.
Additional features of the questionnaires di-rectly address user satisfaction (e.g.
perceived sat-isfaction, degree of enjoyment, user happiness, sys-tem likability, degree of frustration or irritation) andacceptability (perceived acceptability, willingness touse the system in the future).From the classification, it seems that the taxon-omy adequately covers what researchers intuitivelywould include in questionnaires investigating usabil-ity, user satisfaction and acceptability.4 Classification of Dialogue and SystemMeasuresExperiments with human subjects are still the onlyway to investigate quality percepts.
They are, how-ever, time?consuming and expensive to carry out.For the developers of SDSs, it is therefore interestingto identify quality elements which are in their hands,and which can be used for enhancing the quality forthe user.
Unfortunately, only few such elements areknown, and their influence on service quality is onlypartly understood.
Word accuracy or word error rate,which are common measures to describe the perfor-mance of speech recognizers, can be taken as an ex-ample.
Although they can be measured partly in-strumentally (provided that an agreed?upon corpuswith reference transcriptions exists), and the systemdesigner can tune the system to increase the wordaccuracy, it cannot be determined beforehand howthis will affect system usability or user satisfaction.For filling this gap, dialogue?
and system?relatedmeasures have been developed.
They can be de-termined during the users?
experimental interactionwith the system or from log?files, either instrumen-tally (e.g.
dialogue duration) or by an expert eval-uator (e.g.
contextual appropriateness).
Althoughthey provide useful information on the perceivedquality of the service, there is no general relation-ship between one or several such measures, and spe-cific quality features.
The PARADISE framework(Walker et al, 1997) produces such a relationshipfor a specific scenario, using multivariate linear re-gression.
Some generalizablility can be reached, butthe exact form of the relationship and its constitut-ing input parameters have to be established for eachsystem anew.A generalization across systems and servicesmight be easier if a categorization of dialogue andsystem measures can be reached.
Tables 3 and 4 inthe Appendix report on the classification of 37 dif-ferent measures defined in literature into the QoStaxonomy.
No measures have been found so farwhich directly relate to speech output quality, agentpersonality, service efficiency, usability, or user sat-isfaction.
With the exception of the first aspect, itmay however be assumed that they will be addressedby a combination of the measures related to the un-derlying aspects.5 Comparison to Human-Human ServicesIt has been stated earlier that the QoS taxonomyfor telephone?based spoken dialogue services hasbeen derived from an earlier schematic address-ing human?to?human telephone services (Mo?ller,2000).
This schematic is depicted in Figure 2, withslight modifications on the labels of single cate-gories from the original version.In the HHI case, the focus is placed on the cate-gories of speech communication.
This category (re-Table 1: Dialogue?related quality features.Aspect Quality FeaturesDialogue Informativeness ?
Accuracy / Specificity of InformationCooperativity ?
Completeness of Information?
Clarity of Information?
Conciseness of Information?
System Feedback AdequacyTruth and ?
Credibility of InformationEvidence ?
Consistency of Information?
Reliability of Information?
Perceived System ReasoningRelevance ?
System Feedback Adequacy?
Perceived System Understanding?
Perceived System Reasoning?
Naturalness of InteractionManner ?
Clarity / Non?Ambiguity of Expression?
Consistency of Expression?
Conciseness of Expression?
Transparency of Interaction?
Order of InteractionBackground ?
Congruence with User?s Task/Domain Knowl.Knowledge ?
Congruence with User Experience?
Suitability of User Adaptation?
Inference Adequacy?
Interaction GuidanceMeta?Comm.
?
Repair Handling AdequacyHandling ?
Clarification Handling Adequacy?
Help Capability?
Repetition CapabilityDialogue Initiative ?
Flexibility of InteractionSymmetry ?
Interaction Guidance?
Naturalness of InteractionInteraction ?
Perceived Control CapabilityControl ?
Barge?In Capability?
Cancel CapabilityPartner ?
Transparency of InteractionAsymmetry ?
Transparency of Task / Domain Coverage?
Interaction Guidance?
Naturalness of Interaction?
Cognitive Demand Required from the User?
Respect of Natural Information PackagesSpeech I/O Speech Output ?
IntelligibilityQuality Quality ?
Naturalness of Speech?
Listening?Effort Required from the UserSpeech Input ?
Perceived System UnderstandingQuality ?
Perceived System ReasoningTable 2: Communication?, task?
and service?related quality features.Aspect Quality FeaturesCommunic.
Speed ?
Perceived Interaction PaceEfficiency ?
Perceived Response TimeConciseness ?
Perceived Interaction Length?
Perceived Interaction DurationSmoothness ?
System Feedback Adequacy?
Perceived System Understanding?
Perceived System Reasoning?
Repair Handling Adequacy?
Clarification Handling Adequacy?
Naturalness of Interaction?
Interaction Guidance?
Transparency of Interaction?
Congruence with User ExperienceComfort Agent ?
PolitenessPersonality ?
Friendliness?
Naturalness of BehaviorCognitive ?
Ease of CommunicationDemand ?
Concentration Required from the User?
Stress / FlusterTask Task Success ?
Adequacy of Task / Domain CoverageEfficiency ?
Validity of Task Results?
Precision of Task Results?
Reliability of Task ResultsTask Ease ?
Perceived Helpfulness?
Task Guidance?
Transparency of Task / Domain CoverageService Service ?
Access AdequacyEfficiency Adequacy ?
Availability?
Modality Adequacy?
Task Adequacy?
Perceived Service Functionality?
Perceived UsefulnessAdded Value ?
Service Improvement?
Comparable InterfaceUsability Ease of Use ?
Service Operability?
Service Understandability?
Service Learnabilityqualityofservicespeechcommunicationfactorsservicefactorscontextualfactorsvoicetransmissionqualityeaseofcommunicationconversationeffectivenesscommunicationefficiencyserviceefficiencyusabilityeconomicalbenefitusersatisfactionutilityacceptabilityloudnessratingsroomnoisecircuitnoiselistenersidetoneimpulsivenoiselistenerechofrequencydistortioncodecstransmissionerrorsinterruptionsfadinginvestmentcost soperationcost saccountconditionsuserfactorsattitude emotions experi-encemotivation,goalstypeofterminalergonomicsdesignavailabilityset-uptimeresponsetimereliabilitycompatibilitytalkerechotalkersidetonepuredelayFigure 2: QoS schematic for human?to?human telephone services.placing environmental and agent factors of the HMIcase) is divided into a one?way voice transmissioncategory, a conversational category (conversation ef-fectiveness), and a user?related category (ease ofcommunication; comparable to the category ?com-fort?
in the HMI case).
The task and service cate-gories of the interaction with the SDS are replacedby the service categories of the HHI schematic.
Therest of the schematic is congruent in both cases, al-though the single aspects which are covered by eachcategory obviously differ.The taxonomy of Figure 2 has fruitfully been usedto classify three types of entities:  quality elements which are used for the set?upand planning of telephone networks (some ofthese elements are given in the gray boxes ofFigure 2)  assessment methods commonly used for mea-suring quality features in telecommunications  quality prediction models which estimate sin-gle quality features from the results of instru-mental measurementsAlthough we seem to be far from reaching a compa-rable level in the assessment and prediction of HMIquality issues, it is hoped that the taxonomy of Fig-ure 1 can be equally useful with respect to telephoneservices based on SDSs.6 Discussion and ConclusionsThe new taxonomy was shown to be useful in clas-sifying quality features (dimensions of human qual-ity perception) as well as instrumentally or expert?derived measures which are related to service qual-ity, usability, and acceptability.
Nonetheless, in bothcases it has not been validated against experimen-tal (empirical) data.
Thus, one cannot guarantee thatthe space of quality dimensions is captured in an ac-curate and complete way.There are a number of facts reported in liter-ature, which make us confident that the taxon-omy nevertheless captures general assumptions andtrends.
First of all, in his review of both subjectiveevaluations as well as dialogue- or system-relatedmeasures, the author didn?t encounter items whichwould not be covered by the schematic.
This litera-ture review is still going on, and it is hoped that moredetailed data can be presented in the near future.As stated above, the separation of environmental,agent and task factors was motivated by Walker et al(1997).
The same categories appear in the character-ization of spoken dialogue systems given by Fraser(1997) (plus an additional user factor, which obvi-ously is nested in the quality aspects due to the factthat it is the user who decides on quality).
Thecontext factor is also recognized by Dybkj?r andBernsen (2000).
Dialogue cooperativity is a cat-egory which is based on a relatively sophisticatedtheoretical as well as empirical background.
It hasproven useful especially in the system design andset?up phase, and first results in evaluation havealso been reported (Bernsen et al, 1998).
The di-alogue symmetry category captures the remainingpartner asymmetry aspect, and has been designedseparately to additionally cover initiative and inter-action control aspects.
To the authors knowledge,no similar category has been reported.
The relation-ship between the different efficiency measures andusability, user satisfaction and utility was alreadydiscussed in Section 2.In the PARADISE framework, user satisfaction iscomposed of maximal task success and minimal di-alogue costs (Walker et al, 1997), ?
thus a typeof efficiency in the way it was defined here.
Thisconcept is still congruent with the proposed taxon-omy.
On the other hand, the separation into ?effi-ciency measures?
and ?quality measures?
(same fig-ure) does not seem to be fine?graded enough.
It isproposed that the taxonomy could be used to clas-sify different measures beforehand.
Based on thecategories, a multi?level prediction model could beenvisaged, first summarizing similar measures (be-longing to the same category) into intermediate in-dices, and then combining the contributions of dif-ferent indices into an estimation of user satisfaction.The reference for user satisfaction, however, cannotbe a simple arithmetic mean of the subjective ratingsin different categories.
Appropriate questionnairesstill have to be developed, and they will take profitof multidimensional analyses as reported by Honeand Graham (2001).ReferencesNiels Ole Bernsen, Hans Dybkj?r, and Laila Dybkj?r.1998.
Designing Interactive Speech Systems: FromFirst Ideas to User Testing.
Springer, D?Berlin.Morena Danieli and Elisabetta Gerbino.
1995.
Metricsfor evaluating dialogue strategies in a spoken languagesystem.
In: Empirical Methods in Discourse Inter-pretation and Generation.
Papers from the 1995 AAAISymposium, USA?Stanford CA, pages 34?39, AAAIPress, USA?Menlo Park CA.Laila Dybkj?r and Niels Ole Bernsen.
2000.
Usabilityissues in spoken dialogue systems.
Natural LanguageEngineering, 6(3-4):243?271.ETSI Technical Report ETR 095, 1993.
Human Factors(HF); Guide for Usability Evaluations of Telecommu-nication Systems and Services.
European Telecommu-nications Standards Institute, F?Sophia Antipolis.EURESCOM Project P.807 Deliverable 1, 1998.
JupiterII - Usability, Performability and Interoperability Tri-als in Europe.
European Institute for Researchand Strategic Studies in Telecommunications, D?Heidelberg.Norman Fraser.
1997.
Assessment of Interactive Sys-tems.
In: Handbook on Standards and Resources forSpoken Language Systems (D. Gibbon, R. Moore andR.
Winski, eds.
), pages 564?615, Mouton de Gruyter,D?Berlin.H.
Paul Grice, 1975.
Logic and Conversation, pages 41?58.
Syntax and Semantics, Vol.
3: Speech Acts (P.Cole and J. L. Morgan, eds.).
Academic Press, USA?New York (NY).Kate S. Hone and Robert Graham.
2001.
Subjective As-sessment of Speech?System Interface Usability.
Proc.7th Europ.
Conf.
on Speech Communication and Tech-nology (EUROSPEECH 2001 ?
Scandinavia), pages2083?2086, DK?Aalborg.ITU?T Rec.
E.800, 1994.
Terms and Definitions Relatedto Quality of Service and Network Performance In-cluding Dependability.
International Telecommunica-tion Union, CH?Geneva, August.Ute Jekosch.
2000.
Sprache ho?ren und beurteilen:Ein Ansatz zur Grundlegung der Sprachqualita?tsbe-urteilung.
Habilitation thesis (unpublished), Univer-sita?t/Gesamthochschule Essen, D?Essen.Candance A. Kamm and Marilyn A. Walker.
1997.Design and Evaluation of Spoken Dialogue Systems.Proc.
1997 IEEE Workshop on Automatic SpeechRecognition and Understanding, USA?Santa Barbara(CA), pages 14?17.Candance Kamm, Shrikanth Narayanan, Dawn Dutton,and Russell Ritenour.
1997.
Evaluating SpokenDialogue Systems for Telecommunication Services.Proc.
5th Europ.
Conf.
on Speech Communication andTechnology (EUROSPEECH?97), 4:2203?2206, GR?Rhodes.Diane J. Litman, Shimei Pan, and Marilyn A. Walker.1998.
Evaluating Response Strategies in a Web?Based Spoken Dialogue Agent.
Proc.
of the 36thAnn.
Meeting of the Assoc.
for Computational Linguis-tics and 17th Int.
Conf.
on Computational Linguistics(COLING-ACL 98), CAN-Montreal.Sebastian Mo?ller.
2000.
Assessment and Prediction ofSpeech Quality in Telecommunications.
Kluwer Aca-demic Publ., USA?Boston.Joseph Polifroni, Lynette Hirschman, Stephanie Sen-eff, and Victor Zue.
1992.
Experiments in Eval-uating Interactive Spoken Language Systems.
In:Proc.
DARPA Speech and Natural Language Work-shop, pages 28?33.Patti J.
Price, Lynette Hirschman, Elizabeth Shriberg, andElizabeth Wade.
1992.
Subject?Based EvaluationMeasures for Interactive Spoken Language Systems.In: Proc.
DARPA Speech and Natural Language Work-shop, pages 34?39.Andrew Simpson and Norman M. Fraser.
1993.
BlackBox and Glass Box Evaluation of the SUNDIAL Sys-tem.
Proc.
3rd Europ.
Conf.
on Speech Communi-cation and Technology (EUROSPEECH?93), 2:1423?1426, D?Berlin.Helmer Strik, Catia Cucchiarini, and Judith M. Kessens.2001.
Comparing the Performance of Two CSRs:How to Determine the Significance Level of the Dif-ferences.
Proc.
7th Europ.
Conf.
on Speech Communi-cation and Technology (EUROSPEECH 2001 ?
Scan-dinavia), pages 2091?2094, DK?Aalborg.Marilyn A. Walker, Diane J. Litman, Candance A.Kamm, and Alicia Abella.
1997.
PARADISE: AFramework for Evaluating Spoken Dialogue Agents.Proc.
of the ACL/EACL 35th Ann.
Meeting of the As-soc.
for Computational Linguistics, pages 271?280.Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,and Alicia Abella.
1998.
Evaluating Spoken DialogueAgents with PARADISE: Two Case Studies.
Com-puter Speech and Language, 12(3).A Classification of Dialogue and SystemMeasuresTable 3: Classification of measures (1).
#: average number of ... per dialogue.
For references, see cap-tion of Table 4.Aspect Dialogue / System MeasureDialogue ?
CA: contextual appropriateness (SF93, F97)CooperativityInformativeness ?
# user questions (P92)?
# help requests from the user (W98)Truth and ?
# questions correctly/incorrectly/partially/failed toEvidence be answered (P92)?
DARPA score, DARPA weighted error (P92)Relevance ?
# barge-in attempts from the user (W98)Manner ?
# system turns (W98)?
no.
of words per system turnBackground ?
# help requests (W98)Knowledge ?
# cancel attempts from the user (W98)?
# barge-in attempts from the user (W98)?
# time-out prompts (W98)Meta?Comm.
?
# system error messages (Pr92)Handling ?
# help requests (W98)?
# cancel attempts from the user (W98)?
CR: correction rate (SCR) (F97, SF93)?
IR: implicit recovery (DG95)Dialogue Initiative ?
# user questions (P92)Symmetry ?
# system questions?
CR: correction rate (SCR, UCR) (F97, SF93)Interaction ?
# barge-in attempts from the user (W98)Control ?
# help requests (W98)?
# cancel attempts from the user (W98)?
CR: correction rate (UCR) (F97, SF93)?
# time-out prompts (W98)Partner ?
# barge-in attempts from the user (W98)Asymmetry ?
# time-out prompts (W98)Speech I/O Speech Output ?Quality QualitySpeech Input ?
word accuracy, word error rate (SF93)Quality ?
sentence accuracy, sentence error rate (SF93)?
number or errors per sentence (S01)?
word error per sentence (S01)?,	,,(K97)?
UER: understanding error rate?
# ASR rejections (W98)?
IC: information content (SF93)?
# system error messages (Pr92)Table 4: Classification of measures (2).
#: average number of ... per dialogue.
References: DG95: Danieliand Gerbino (1995); F97: Fraser (1997); K97: Kamm et al (1997); P92: Polifroni et al (1992); Pr92: Priceet al (1992); SF93: Simpson and Fraser (1993); S01: Strik et al (2001); W98: Walker et al (1998).Aspect Dialogue / System MeasureCommunic.
Speed ?
TD: turn duration (STD, UTD) (F97)Efficiency ?
SRD: system response delay (Pr92)?
URD: user response delay (Pr92)?
# timeout prompts (W98)?
# barge-in attempts from the user (W98)Conciseness ?
DD: dialogue duration (F97, P92)(Litman et al, 1998: ?
# turns (# system turns, # user turns) (W98)dialogue efficiency)Smoothness ?
# system error messages (Pr92)(Litman et al, 1998: ?
# cancel attempts from the user (W98)dialogue quality) ?
# help requests (W98)?
# ASR rejections (W98)?
# barge-in attempts from the user (W98)?
# timeout prompts (W98)Comfort Agent ?PersonalityCognitive ?
# timeout prompts (W98)Demand ?
URD: user response delay (Pr92)Task Task Success ?
TS: task success (DG95, F97, SF93)Efficiency ?
 : kappa coefficient (W98)?
task solution (P92)?
solution correctness (P92)?
solution qualityTask Ease ?
# help requests (W98)Service Service ?Efficiency AdequacyAdded Value ?Usability Ease of Use ?UserSatisfaction ?
