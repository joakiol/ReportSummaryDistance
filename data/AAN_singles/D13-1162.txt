Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1557?1562,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsClassifying Message Board Posts with an Extracted Lexicon of PatientAttributesRuihong Huang and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{huangrh, riloff}@cs.utah.eduAbstractThe goal of our research is to distinguish vet-erinary message board posts that describe acase involving a specific patient from poststhat ask a general question.
We create a textclassifier that incorporates automatically gen-erated attribute lists for veterinary patients totackle this problem.
Using a small amount ofannotated data, we train an information extrac-tion (IE) system to identify veterinary patientattributes.
We then apply the IE system to alarge collection of unannotated texts to pro-duce a lexicon of veterinary patient attributeterms.
Our experimental results show that us-ing the learned attribute lists to encode pa-tient information in the text classifier yieldsimproved performance on this task.1 IntroductionOur research focuses on the problem of classify-ing message board posts in the domain of veterinarymedicine.
Most of the posts in our corpus discuss acase involving a specific patient, which we will callpatient-specific posts.
But there are also posts thatask a general question, for example to seek adviceabout different medications, information about newprocedures, or how to perform a test.
Our goal isto distinguish the patient-specific posts from generalposts so that they can be automatically routed to dif-ferent message board folders.Distinguishing patient-specific posts from generalposts is a challenging problem for two reasons.
First,virtually any medical topic can appear in either typeof post, so the vocabulary is very similar.
Second,a highly skewed distribution exists between patient-specific posts and general posts.
Almost 90% of theposts in our data are about specific patients.With such a highly skewed distribution, it wouldseem logical to focus on recognizing instances of theminority class.
But the distinguishing characteristicof a general post is the absence of a patient.
Twonearly identical posts belong in different categoriesif one mentions a patient and the other does not.Consequently, our aim is to create features that iden-tify references to a specific patient and use these tomore accurately distinguish the two types of posts.Our research explores the use of information ex-traction (IE) techniques to automatically identifycommon attributes of veterinary patients, which weuse to encode patient information in a text classifier.Our approach involves three phases.
First, we traina conditional random fields (CRF) tagger to iden-tify seven common types of attributes that are of-ten ascribed to veterinary patients: SPECIES/BREED,NAME, AGE, GENDER, WEIGHT, POSSESSOR, andDISEASE/SYMPTOM.
Second, we apply the CRFtagger to a large set of unannotated message boardposts, collect its extractions, and harvest the mostfrequently extracted terms to create a Veterinary Pa-tient Attribute (VPA) Lexicon.Finally, we define three types of features that ex-ploit the harvested VPA lexicon.
These features rep-resent the patient attribute terms, types, and com-binations of them to help the classifier determinewhether a post is discussing a specific patient.
Weconduct experiments which show that the extractedpatient attribute information improves text classifi-cation performance on this task.15572 Related WorkOur work demonstrates the use of information ex-traction techniques to benefit a text classification ap-plication.
There has been a great deal of research ontext classification (e.g., (Borko and Bernick, 1963;Hoyle, 1973; Joachims, 1998; Nigam et al 2000;Sebastiani, 2002)), which most commonly has usedbag-of-word features.
Researchers have also inves-tigated clustering (Baker and McCallum, 1998), La-tent Semantic Indexing (LSI) (Zelikovitz and Hirsh,2001), Latent Dirichlet Allocation (LDA) (Br et al2008) and string kernels (Lodhi et al 2001).
Infor-mation extraction techniques have been used previ-ously to create richer features for event-based textclassification (Riloff and Lehnert, 1994) and webpage classification (Furnkranz et al 1998).
Se-mantic information has also been incorporated fortext classification.
However, most previous work re-lies on existing semantic resources, such as Wordnet(Scott and Stan, 1998; Bloehdorn and Hotho, 2006)or Wikipedia (Wang et al 2009).There is also a rich history of automatic lexiconinduction from text corpora (e.g., (Roark and Char-niak, 1998; Riloff and Jones, 1999; McIntosh andCurran, 2009)), Wikipedia (e.g., (Vyas and Pantel,2009)), and the Web (e.g., (Etzioni et al 2005;Kozareva et al 2008; Carlson et al 2010)).
Thenovel aspects of our work are in using an IE taggerto harvest a domain-specific lexicon from unanno-tated texts, and using the induced lexicon to encodedomain-specific features for text classification.3 Text Classification with ExtractedPatient AttributesThis resesarch studies message board posts from theVeterinary Information Network (VIN), which is aweb site (www.vin.com) for professionals in veteri-nary medicine.
VIN hosts forums where veterinar-ians discuss medical issues, challenging cases, etc.We observed that patient-specific veterinary postsalmost always include some basic facts about thepatient, such as the animal?s breed, age, or gender.It is also common to mention the patient?s owner(e.g., ?a new client?s cat?)
or a disease or symptomthat the patient has (e.g., ?a diabetic cat?).
Generalposts almost never contain this information.Although some of these terms can be found inexisting resources such as Wordnet (Miller, 1990),our veterinary message board posts are filled withinformal and unconventional vocabulary.
For ex-ample, one might naively assume that ?male?
and?female?
are sufficient to identify gender.
But thegender of animals is often revealed by describingtheir spayed/neutered status, often indicated withshorthand notations.
For example, ?m/n?
meansmale and neutered, ?fs?
means female spayed, ?cas-trated?
means neutered and implies male.
Short-hand terms and informal jargon are also frequentlyused for breeds (e.g., ?doxy?
for dachsund, ?labx?for labrador cross, ?gshep?
for German Shepherd)and ages (e.g., ?3-yr-old?, ?3yo?, ?3mo?).
A par-ticularly creative age expression describes an animalas (say) ?a 1999 model?
(i.e., born in 1999).
To rec-ognize the idiosyncratic vocabulary in these texts,we use information extraction techniques to identifyterms corresponding to seven attributes of veterinarypatients: SPECIES/BREED, NAME, AGE, WEIGHT,GENDER, POSSESSOR, and DISEASE/SYMPTOM.Figure 1 illustrates our overall approach, whichconsists of three steps.
First, we train a sequentialIE tagger to label veterinary patient attributes usingsupervised learning.
Second, we apply the taggerto 10,000 unannotated message board posts to auto-matically create a Veterinary Patient Attribute (VPA)Lexicon.
Third, we use the VPA Lexicon to encodepatient attribute features in a document classifier.UnannotatedTextsPI SentenceClassifierVPA Tagger(CRF)VPALexiconStep 2PI SentenceClassifierVPA Tagger(CRF)AnnotatedTextsStep 1AnnotatedTextsVPALexiconDocumentClassifierStep 3Figure 1: Flowchart for Creating a Patient-Specific vs.General Document Classifier3.1 Patient Attribute TaggerThe first component of our system is a tagger thatlabels veterinary patient attributes.
To train the tag-ger, we need texts labeled with patient attributes.1558The message board posts can be long and tediousto read (i.e., they are often filled with medical his-tory and test results), so manually annotating everyword would be arduous.
However, the patient is usu-ally described at the beginning of a post, most com-monly in 1-2 ?introductory?
sentences.
Thereforewe adopted a two stage process, both for manual andautomatic tagging of patient attributes.First, we created annotation guidelines to iden-tify ?patient introductory?
(PI) sentences, which wedefined as sentences that introduce a patient to thereader by providing a general (non-medical) descrip-tion of the animal (e.g., ?I was presented with a m/nSiamese cat that is lethargic.?)
We randomly se-lected 300 posts from our text collection and askedtwo human annotators to manually identify the PIsentences.
We measured their inter-annotator agree-ment using Cohen?s kappa (?)
and their agreementwas ?=.93.
The two annotators then adjudicatedtheir differences to create our gold standard set of PIsentence annotations.
269 of the 300 posts containedat least one PI sentence , indicating that 89.7% of theposts mention a specific patient.
The remaining 31posts (10.3%) are general in nature.Second, the annotators manually labeled thewords in these PI sentences with respect to the 7 vet-erinary patient attributes.
On 50 randomly selectedtexts, the annotators achieved an inter-annotatoragreement of ?
= .89.
The remaining 250 posts werethen annotated with patient attributes (in the PI sen-tences), providing us with gold standard attribute an-notations for all 300 posts.
To illustrate, the sentencebelow would have the following labels:Daisyname is a 10yrage oldage labspeciesWe used these 300 annotated posts to train botha PI sentence classifier and a patient attribute tag-ger.
The PI sentence classifier is a support vectormachine (SVM) with a linear kernel (Keerthi andDeCoste, 2005), unigram and bigram features, andbinary feature values.
The PI sentences are the posi-tive training instances, and the sentences in the gen-eral posts are negative training instances.For the tagger, we trained a single conditional ran-dom fields (CRF) model to label all 7 types of pa-tient attributes using the CRF++ package (Laffertyet al 2001).
We defined features for the word stringand the part-of-speech tags of the targeted word, twowords on its left, and two words on its right.Given new texts to process, we first apply the PIsentence classifier to identify sentences that intro-duce a patient.
These sentences are given to the pa-tient attribute tagger, which labels the words in thosesentences for the 7 patient attribute categories.To evaluate the performance of the patient at-tribute tagger, we randomly sampled 200 of the 300annotated documents to use as training data and usedthe remaining 100 documents for testing.
For thisexperiment, we only applied the CRF tagger to thegold standard PI sentences, to eliminate any con-founding factors from the PI sentence classifier.
Ta-ble 1 shows the performance of the CRF tagger interms of Recall (%), Precision (%), and F Score (%).Its precision is consistently high, averaging 91%across all seven attributes.
But the average recall isonly 47%, with only one attribute (AGE) achievingrecall ?
80%.
Nevertheless, the CRF?s high preci-sion justifies our plan to use the CRF tagger to har-vest additional attribute terms from a large collectionof unannotated texts.
As we will see in Section 4,the additional terms harvested from the unannotatedtexts provide substantially more attribute informa-tion for the document classifier to use.Attribute Rec Prec FSPECIES/BREED 59 93 72NAME 62 100 76POSSESSOR 12 100 21AGE 80 91 85GENDER 59 81 68WEIGHT 19 100 32DISEASE/SYMPTOM 35 73 47Average 47 91 62Table 1: Patient Attribute Tagger Evaluation3.2 Creating a Veterinary Patient Attribute(VPA) LexiconThe patient attribute tagger was trained with super-vised learning, so its ability to recognize importantwords is limited by the scope of its training set.Since we had an additional 10,000 unannotated vet-erinary message board posts, we used the tagger toacquire a large lexicon of patient attribute terms.We applied the PI sentence classifier to all 10,000texts and then applied the patient attribute tagger toeach PI sentence.
The patient attribute tagger is not1559perfect, so we assumed that words tagged with thesame attribute value at least five times1 are mostlikely to be correct and harvested them to create aveterinary patient attribute (VPA) lexicon.
This pro-duced a VPA lexicon of 592 words.
Table 2 showsexamples of learned terms for each attribute, withthe total number of learned words in parentheses.Species/Breed (177): DSH, Schnauzer, kitty, Bengal,pug, Labrador, siamese, Shep, miniature, golden, lab,Spaniel, Westie, springer, Chow, cat, Beagle, Mix, ...Name (53): Lucky, Shadow, Toby, Ginger, Boo, Max,Baby, Buddy, Tucker, Gracie, Maggie, Willie, Tiger,Sasha, Rusty, Beau, Kiki, Oscar, Harley, Scooter, ...Age (59): #-year, adult, young, YO, y/o, model, wk,y.o., yr-old, yrs, y, #-yr, #-month, #m, mo, mth, ...Gender (39): F/s, speyed, neutered, spayed, N/M,FN, CM, F, mc, mn, SF, male, fs, M/N, Female,S, S/F, m/n, m/c, intact, M, NM, castrated, ...Weight (5): lb, lbs, pound, pounds, kgPossessor (7): my, owner, client, technician, ...Disease/Symptom (252): abscess, fever, edema,hepatic, inappetance, sneezing, blindness, pain,persistent, mass, insufficiency, acute, poor, ...Table 2: Examples from the Induced VPA Lexicon3.3 Text Classification with Patient AttributesOur ultimate goal is to incorporate patient attributeinformation into a text classifier to help it distinguishbetween patient-specific posts and general posts.
Wedesigned three sets of features:Attribute Types: We create one feature for eachattribute type, indicating whether a word of that at-tribute type appeared or not.Attribute Types with Neighbor: For each word la-beled as a patient attribute, we create two featuresby pairing its Attribute Type with a preceding or fol-lowing word.
For example, given the sentence: ?Thetiny Siamese kitten was lethargic.
?, if ?Siamese?
hasattribute type SPECIES then we create two features:<tiny, SPECIES> and <SPECIES, kitten>.Attribute Pairs: We create features for all pairs ofpatient attribute words that occur in the same sen-tence.
For each pair, we create one feature repre-1After our text classification experiments were done, we re-ran the experiments with the unigrams+lexicon classifier usingthresholds ranging from 1 to 10 for lexicon creation, just to seehow much difference this threshold made.
We found that values?
5 produced nearly identical classification results.senting the words themselves and one feature repre-senting the attribute types of the words.4 EvaluationTo create a blind test set for evaluation, our anno-tators labeled an additional 500 posts as patient-specific or general.
Specifically, they labeled those500 posts with PI sentences.
The absence of a PIsentence meant that the post was general.
Of the 500texts, 48 (9.6%) were labeled as general posts.
Weevaluated the performance of the PI sentence classi-fier on this test set and found that it achieved 88% ac-curacy at identifying patient introductory sentences.We then conducted a series of experiments for thedocument classification task: distinguishing patient-specific message board posts from general posts.All of our experiments used support vector machine(SVM) classifiers with a linear kernel, and ran 10-fold cross validation on our blind test set of 500posts.
We report Recall (%), Precision (%), and Fscore (%) results for the patient-specific posts andgeneral posts separately, and for the macro-averagedscore across both classes.
For the sake of complete-ness, we also show overall Accuracy (%) results.However, we will focus attention on the results forthe general posts, since our main goal is to improveperformance at recognizing this minority class.As a baseline, we created SVM classifiers usingunigram features.2 We tried binary, frequency, andtf-idf feature values.
The first three rows of Table 3show that binary feature values performed the best,yielding a macro-averaged F score of 81% but iden-tifying only 54% of the general posts.The middle section of Table 3 shows the perfor-mance of SVM classifiers using our patient attributefeatures.
We conducted three experiments: apply-ing the CRF tagger to PI sentences (per its design),and labeling words with the VPA lexicon either onall sentences or only on PI sentences (as identi-fied by the PI sentence classifier).
The CRF fea-tures produced extremely low recall and precisionon the general posts.
The VPA lexicon performedbest when applied only to PI sentences and pro-duced much higher recall than all of the other clas-sifiers, although with lower precision than the two2We also tried unigrams + bigrams, but they did not performbetter.1560Patient-Specific Posts General Posts Macro AvgMethod Rec Prec F Rec Prec F Rec Prec F AccUnigram FeaturesUnigrams (freq) 96 96 96 58 60 59 77 76 77 92Unigrams (tf-idf) 99 93 96 33 84 48 66 89 76 93Unigrams (binary) 98 95 97 54 79 64 76 87 81 94Patient Attribute FeaturesCRF Features (PI Sents) 99 91 95 02 25 04 51 58 54 90VPA Lexicon Features (All Sents) 96 96 96 60 63 62 78 79 79 93VPA Lexicon Features (PI Sents) 96 98 97 81 66 73 88 82 85 94Unigram & Patient Attribute FeaturesCRF Features (PI Sents) 97 96 97 60 71 65 79 83 81 94VPA Lexicon Features (PI Sents) 98 98 98 79 78 78 88 88 88 96Table 3: Experimental Resultsbest unigram-based SVMs.The bottom section of Table 3 shows results forclassifiers with both unigrams (binary) and patientattribute features.
Using the CRF features increasesrecall on the general posts from 54 ?
60, but de-creases precision from 79 ?
71.
Using the patientattribute features from the VPA lexicon yields a sub-stantial improvement.
Recall improves from 54 ?79 and precision is just one point lower.
Overall, themacro-averaged F score across the two categoriesjumps from 81% to 88%.We performed paired bootstrap testing (Berg-Kirkpatrick et al 2012)) to determine whether theSVM with unigrams and VPA lexicon features isstatistically significantly better than the best SVMwith only unigram features (binary).
The SVM withunigrams and VPA lexicon features produces sig-nificantly better F scores at the p < 0.05 level forgeneral post classification as well as the macro av-erage.
The F score for patient-specific classificationand overall accuracy are statistically significant atthe p < 0.10 level.Attribute CRF VPATagger LexiconSPECIES/BREED 270 1045NAME 36 43POSSESSOR 12 233AGE 545 1773GENDER 153 338WEIGHT 27 83DISEASE/SYMPTOM 220 2673Table 4: Number of Attributes Labeled in Test SetFinally, we did an analysis to understand why theVPA lexicon was so much more effective than theCRF tagger when used to create features for textclassification.
Table 4 shows the number of wordsin PI sentences (identified by the classifier) of thetest set that were labeled as patient attributes by theCRF tagger or the VPA lexicon.
The VPA lexiconclearly labeled many more terms, and the additionalcoverage made a big difference for the text classifier.5 ConclusionsThis work demonstrated how annotated data can beleveraged to automatically harvest a domain-specificlexicon from a large collection of unannotated texts.Our induced VPA lexicon was then used to createpatient attribute features that improved the ability ofa document classifier to distinguish between patient-specific message board posts and general posts.
Webelieve that this approach could also be used to cre-ate specialized lexicons for many other domains andapplications.
A key benefit of inducing lexiconsfrom unannotated texts is that they provide addi-tional vocabulary coverage beyond the terms foundin annotated data sets, which are usually small.6 AcknowledgementsThis material is based upon work supported bythe National Science Foundation under grant IIS-1018314.
We are very grateful to the Veterinary In-formation Network for providing us with samples oftheir data.1561ReferencesD.
Baker and A. McCallum.
1998.
Distributional cluster-ing of words for text classification.
In Proceedings ofthe 21st annual international ACM SIGIR conferenceon Research and development in information retrieval.T.
Berg-Kirkpatrick, D. Burkett, and D. Klein.
2012.
AnEmpirical Investigation of Statistical Significance inNLP.
In Proceedings of the 2012 Conference on Em-pirical Methods in Natural Language Processing.S.
Bloehdorn and A. Hotho.
2006.
Boosting for textclassification with semantic features.
In Advances inWeb mining and Web usage Analysis.H.
Borko and M. Bernick.
1963.
Automatic DocumentClassification.
J. ACM, 10(2):151?162.I.
Br, J. Szab, and A. Benczr.
2008.
Latent dirichlet al-cation in web spam filtering.
In Proceedings of the 4thinternational workshop on Adversarial information re-trieval on the web.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, R. Es-tevam, J. Hruschka, and T. Mitchell.
2010.
Towardan Architecture for Never-Ending Language Learning.In Proceedings of the Twenty-Fourth National Confer-ence on Artificial Intelligence.O.
Etzioni, M. Cafarella, A. Popescu, T. Shaked,S.
Soderland, D. Weld, and A. Yates.
2005.
Unsuper-vised Named-Entity Extraction from the Web: An Ex-perimental Study.
Artificial Intelligence, 165(1):91?134.J.
Furnkranz, T. Mitchell, and E. Riloff.
1998.
A CaseStudy in Using Linguistic Phrases for Text Catego-rization from the WWW.
In Working Notes of theAAAI/ICML Workshop on Learning for Text Catego-rization.W.
Hoyle.
1973.
Automatic Indexing and Generationof Classification Systems by Algorithm.
InformationStorage and Retrieval, 9(4):233?242.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In Proceedings of the European Conference on Ma-chine Learning (ECML).S.
Keerthi and D. DeCoste.
2005.
A Modified FiniteNewton Method for Fast Solution of Large Scale Lin-ear SVMs.
Journal of Machine Learning Research.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
SemanticClass Learning from the Web with Hyponym PatternLinkage Graphs.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-08).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceedingsof the Eighteenth International Conference on Ma-chine Learning.H.
Lodhi, J. Shawe-Taylor, N. Christianini, andC.
Watkins.
2001.
Text classification using string ker-nels.
In Advances in Neural Information ProcessingSystems (NIPS).T.
McIntosh and J. Curran.
2009.
Reducing SemanticDrift with Bagging and Distributional Similarity.
InProceedings of the 47th Annual Meeting of the Associ-ation for Computational Linguistics.G.
Miller.
1990.
Wordnet: An On-line Lexical Database.International Journal of Lexicography, 3(4).K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.2000.
Text Classification from Labeled and Unla-beled Documents using EM.
Machine Learning, 39(2-3):103?134, May.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence.E.
Riloff and W. Lehnert.
1994.
Information Ex-traction as a Basis for High-Precision Text Classifi-cation.
ACM Transactions on Information Systems,12(3):296?333, July.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic SemanticLexicon Construction.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics, pages 1110?1116.S.
Scott and M. Stan.
1998.
Text classification usingWordNet hypernyms.
In In Use of WordNet in Natu-ral Language Processing Systems: Proceedings of theConference.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
In ACM computing surveys (CSUR).V.
Vyas and P. Pantel.
2009.
Semi-automatic entity setrefinement.
In Proceedings of North American Asso-ciation for Computational Linguistics / Human Lan-guage Technology (NAACL/HLT-09).P.
Wang, J. Hu, H. Zeng, and Z. Chen.
2009.
UsingWikipedia knowledge to improve text classification.In Knowledge and Information Systems.S.
Zelikovitz and H. Hirsh.
2001.
Using LSI for textclassication in the presence of background text.
InProceedings of the 10th International Conference onInformation and Knowledge Management (CIKM).1562
