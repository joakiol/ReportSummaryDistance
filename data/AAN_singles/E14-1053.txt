Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501?510,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsTopical PageRank: A Model of Scientific Expertise for Bibliographic SearchJames Jardine Simone TeufelNatural Language and Information Processing GroupComputer LaboratoryCambridge University, CB3 0FD, UK{jgj29,sht25}@cam.ac.ukAbstractWe model scientific expertise as a mixtureof topics and authority.
Authority is calcu-lated based on the network properties of eachtopic network.
ThemedPageRank, our combi-nation of LDA-derived topics with PageRankdiffers from previous models in that topics in-fluence both the bias and transition probabili-ties of PageRank.
It also incorporates the ageof documents.
Our model is general in thatit can be applied to all tasks which require anestimate of document?document, document?query, document?topic and topic?query sim-ilarities.
We present two evaluations, oneon the task of restoring the reference lists of10,000 articles, the other on the task of au-tomatically creating reading lists that mimicreading lists created by experts.
In both eval-uations, our system beats state-of-the-art, aswell as Google Scholar and Google Search in-dexed againt the corpus.
Our experiments alsoallow us to quantify the beneficial effect of ourtwo proposed modifications to PageRank.1 IntroductionFor search, the presence of links in a documentcollection adds valuable information over that con-tained in the text of the documents alone.
Each actof linking can be interpreted as a latent judgement ofauthority or trust which is bestowed onto the linkeddocuments (Kleinberg, 1998).
This makes author-ity an objective measure of how important that pa-per is to a community who confer that authority.The citation count is the simplest of these, whichhas been used successfully for decades for biblio-metrics (Garfield, 1972) and for mapping out scien-tific fields via bibliometric coupling (Kessler, 1963)and co-citations (Small, 1978).
More recently, cita-tion counts have been shown to improve effective-ness of ad-hoc retrieval (Meij and De Rijke, 2007;Fujii, 2007).In science, the peer review process ensures thatthe right to cite is hard-earned, but on the web, hy-perlinking is infinitely cheap.
This means that thatthe authority of webpages cannot simply be approx-imated as the number of incoming links.
Algorith-mically more complex authority such as the random-surfer model PageRank (Brin and Page, 1998) or theauthorities/hub based algorithm HITS (Kleinberg,1998)) have spectacularly improved search results incomparison to standard IR models relying on simi-larity calculations based on the words in the text andother text-internal informatioh.Much recent work in bibliographic search hasbeen driven by the intuition that what works for theweb should also work for science, even though ci-tations are more comparable to each other in weightthan hyperlinks.
Case studies comparing PageRank-based authority measures against citation countsalone report some cases where PageRank is supe-rior (Chen et al., 2007; Ma et al., 2008), but exper-imental proof of standard PageRank outperformingcitation counts in a large-scale bibliographic searchexperiment is still outstanding.
In at least one suchexperiment, PageRank performed worse than cita-tion count (Bethard and Jurafsky, 2010).Straightforward PageRank calculations, when ap-plied to the scientific literature, are hampered by twofactors: on the one hand, the progression of time im-poses a directional structure on the citation network.Therefore, PageRank values of older papers are sys-tematically inflated as PageRank can only ever flowfrom newer to older papers (Walker et al., 2007).501Secondly, and more interestingly, researchers earntheir expertise in particular, well-defined scientificfields.
We propose that this requires a more fine-grained notion of specific ?
not global ?
expertise.Our solution is to use LDA-derived topics (Bleiet al., 2003) as approximations for scientific fields,and to model the importance of a paper as a mixtureof its relative expertise in each of the topics it cov-ers.
The second aspect of our solution, somewhatmore mundane but still necessary to adapt PageR-ank successfully to model scientific expertise, is toage-taper the resultant estimation.In this paper, we present ThemedPageRank(TPR), our model of topic-specific scientific exper-tise, which incorporates the two modifications, andprovide evidence that both are necessary for the ad-equate application of PageRank-style authority cal-culations to the scientific literature.
In two evalua-tions, our model beats standard PageRank and cita-tion counts by a large margin.
Previous models existwhich combine the idea of personalising PageRankby topics, but our manipulation of both PageRank?sbias and transition probabilities differs from these.Our experiments also support the claim of our sys-tem?s superiority over these models.We use two tasks to evaluate the system?s per-formance.
The first is the reintroduction of an ar-ticle?s reference items that have been artificially re-moved.
The assumption here is that a good modelof document?document similarity should be able toguess which articles any given paper would havecited.
The second task is the automatic creation ofreading lists, of the kind that an expert might pre-pare for their students.
We asked experts to create agold standard of such reading lists, and compare oursystem against the current de facto state-of-the-art insuch tasks, Google Scholar, and again find that oursystem beats it comfortably.This article is structured as follows: the next sec-tion describes our model, which section 3 contraststo related work.
The evaluations are described insections 4 and 5.
Section 6 concludes.2 Authority ModelOur model first determines an LDA space (Blei etal., 2003) representing the entire document collec-tion, which results in a set of topics describing theentirety of the field.
It then calculates an author-Figure 1: A High-level view of LDA.ity model for each topic based on a modificationof Personalised PageRank (Page et al., 1998).
De-pending on the search need, the input (one or morekeyword(s) or paper(s)) is converted into a topic dis-tribution, which we then use to linearly combine themultiple topic-specific expertise scores of our modelinto a unique authority score representing the fit be-tween search need and document.Latent Dirichlet Allocation (LDA) (Blei et al.,2003) is a Bayesian generative probabilistic modelfor collections of discrete data, which has becomepopular for the modelling of scientific text corpora(Wei and Croft, 2006; He et al., 2009; Blei andLafferty, 2006).
In LDA, a document in the cor-pus is modelled and explicitly represented as a fi-nite mixture over an underlying set of topics, whileeach topic is modelled as an infinite mixture overthe underlying set of words in the corpus.
We useLDA predominantly to produce the latent topics thatform a foundation for the relationships between pa-pers and technical terms in a corpus.Technical terms act as the terms in our model(rather than words), because technical terms are im-portant artefacts for formulating knowledge fromscientific texts (Ananiadou, 1994; Justeson andKatz, 1995), because descriptions of topics are bet-ter understandable using technical terms rather thanwords (Wallach, 2006; Wang et al., 2007); and tomake our model more scalable to large corpora.
Themethod we use to find technical terms is light-weightand requires little infrastructure, but does not repre-sent state-of-the-art in terminology detection (Lopezand Romary, 2010; Wang et al., 2007).
We collectall n-grams of words which appear in 2 or more titlesof all documents in the corpus, filter out all unigramsappearing in the Scrabble TWL98 word list, then alln-grams starting or ending in stop words.
To de-502cide whether a subsumed term should be removedif the subsuming term exists (?statistical machinetranslation?
subsumes both ?statistical machine?
and?machine translation?
), we remove those n-gramswhose frequency is lower than 25% of their subsum-ing terms.
Finally, only the most frequent 25% of theremaining unigrams and bigrams are retained.We then build a D ?
V matrix ?, which con-tains the counts of V technical-terms (the columns)in each of the D documents (the rows) in Fig.
1.
Ourown implementation of LDA (with LDA parameters?
= ?
= 0.01) is used to collapse matrix ?
into twodenser, smaller matrices ?
(containing the distribu-tion of documents over topics), and ?
(containingthe distribution of topics over technical-terms).To model topic-specific expertise in science, wemodify the original PageRank calculation of Page atal.
(1998) by adding a topic dimension to the scoreof both the bias and transition probabilities:TPR(t, d, k + 1) = ?B(t, d)+(1?
?)?d?
?li(d)T (t, d, d?
)TPR(t, d?, k)where TPR(t, d, k) is the topic-specific PageR-ank of topic t for paper d at iteration k; B(t, d) isthe probability that paper d is chosen at random fromthe corpus, given topic t, and T (t, d, d?)
is the tran-sition probability of reaching page d from page d?,given topic t. In our formula, the transition proba-bility T (t, d, d?)
takes into account the probabilitiesof topic t not only in documents d and d?, but also inthe other documents d??
referenced by document d?
:B(t, d) =P (t|d)?d?
?DP (t|d?)T?
(t, d, d?)
=?P (t|d?)?d?
?DP (t|d?
)P (t|d)?d???lo(d?
)P (t|d??
)T (t, d, d?)
=T?
(t, d, d?)?d??li(d)T?
(t, d, d?
)Here d is a document whose TPR is being calcu-lated, d?
is a document that refers to document d andwhose TPR score is being distributed during this it-eration of the algorithm, and d??
is a document thatis referred to by document d?.
The first term in thetransition function ensures that TPR scores are prop-agated only from citing documents that are highlyrelevant to topic t. The second term ensures that alarger proportion of a documents TPR score is prop-agated to cited documents that are highly relevant totopic t. The value P (t|d) can be read directly frommatrix ?
in Fig.
1.In a final step, we age-taper TPR by dividingTPR values by the age of the citation concerned inyears.
Experimentally, this achieved the best modelin comparison to more complex dampening methods(e.g., exponential).3 Related WorkOthers before us have observed that time effects biasPageRank if applied unmodified to the scientific lit-erature (Walker et al., 2007).
Walker et al.
?s Cit-eRank algorithm modifies the bias probabilities ofPageRank exponentially with age, favouring morerecent publications.We are also not the first to have combined a notionof topic-specification with Personalised PageRank.The idea goes back to the original PageRank paperby Page et al.
(1998), who discuss the personaliza-tion of PageRank by introducing a bias towards onlya set of trusted web sites W .
Page et al.
alter onlythe bias probability B, while leaving the transitionprobabilities T unchanged from global PageRank:B(t, d) ={1|W |if d ?W0 if d /?WT (t, d, d?)
=1|lo(d?
)|Richardson and Domingos (2002) first usedPageRank personalisation for specialisation atsearch time.
For query q with corresponding topict = q, they use the relevance of document d to queryq as a bias.
Haveliwalla (2003) calculates a Person-alised PageRank for each of a set of 16 manuallycreated topics t comprised of several documents byaltering only the Bias termB, using Page et al.
?s for-mula above.
This solution avoids the computationalscalability problem with Richardson and Domingos?approach, but is limited in applicability by requiringpredefined topics.
Several researchers followed Brinand Page and Haveliwala in altering only the bias503probabilities, including Wu et al.
(2006) and Goriand Pucci (2006).In contrast, Narayan et al.
(2003) and Pal andNarayan (2005) propose a model of personalisationthat alters the transition probabilities instead of thebias probabilities.
Under their model, the transitionprobability T (t, d) is proportional to the number ofwords in document d that are strongly present in thedocuments contained in topic t. Nie et al.
(2006)produce a more computationally scalable version ofthe ideas presented in Pal and Narayan (2005) by as-sociating a context vector with each document, witha fixed set of topics (12 in their case), for which theylearn these context vectors using a naive Bayes clas-sifier.
They then provide the possibility to alter boththe bias and transition probabilities of each webpageas follows:B(t, d) =1DCt(d)T (t, d, d?)
= ?1|lo(d?
)|+ (1?
?)?t?6=tCt?(d?)lo(d?
)where Ct(d) is the context vector score for topict associated with document d; the first term inT (t, d, d?)
corresponds to the probability of arrivingat page d from other pages in the same topic con-text; the second term is the probability of arriving atpage d from other pages in a different context; and?
is a factor that weights the influence of same-topicjumps over other-topic jumps.
Their results suggestthat ?
should be close to 1, indicating that distribut-ing PageRank within topics generates better Person-alised PageRank scores.Other than the fact that they treat bias and transi-tion probabilities differently to how we treat them,all personalisation methods discussed up to nowhave the disadvantage that they rely on a fixed listof manually selected topics, whereas our method of-fers adaptive specialisation to corpus or domain.The previous work closest to ours is Yang et al.
(2009), who were the first to use LDA to automat-ically discover abstract topic distributions in a cor-pus of scientific articles, and to combine them withPagerank by ?
in principle ?
altering both the biasand transition probabilities according to the follow-ing model:B(t, d) =1DP (t|d)T (t, d, d?)
= ?Ts t(t, d, d?)
+ (1?
?
)To t(t, d, d?
)Ts t(t, d, d?)
= P (d|d?, t)?=1|lo(d?
)|where T is the number of LDA topics, P (t|d) is aprobability of topic t given document d, which canbe read directly from the generated LDA probabili-ties, Ts tis the probability of arriving at page d fromother pages in the same topic context, whereas To ttreats the case of arriving at a different topic.
LikeNie et al., they achieve best results with ?
= 1, sothey ultimately only use bias probabilities, like themodels discussed above.
Crucially, their decisionthat P (d|d?, t) does not to involve any of the LDAtopic distributions is surprising.
Under their model,as in ours, when the reader randomly jumps to a newpaper, they will tend to favour papers that are closelyassociated with the topic.
However, when they fol-low a citation in Yang et.
al?s model, one is pickedwith equal probability.
In contrast, our model imple-ments the obvious intuition that if one follows cita-tions, one should also favour those that are closelyassociated with the topic.Let us now turn to the task of reference list rein-troduction (RLR), i.e., the prediction of which pa-pers a target papers originally cited, given only someinformation about the paper which stands in as asearch need ?
either its abstract, author names andother bibliometric information, and/or the full text ofa paper (with citation information redacted).
Evalu-ation of a search model by RLR is cheap because ofthe readily available gold standard, and it thus allowsfor experiments with large data sets.State-of-the-art solutions to RLR combine lexicalsimilarity (often via topic models), measures of au-thority over a citation graph, and information aboutsocial constructs and historic patterns of citation be-haviour.
Strohman et al.
(2007) perform RLR withthe paper text as a query to their recommendationsystem, using text similarity, citation counts, cita-tion coupling, author information, and the citationgraph.
Their model achieves a mean-average pre-cision of 0.102 against a corpus from the Rexa10database.
Bethard and Jurafsky (2010) improve onStrohman et al.
by the use of a SVM with 19 fea-tures from 6 broad categories: similar terms; citedby others; recency; cited using similar terms; simi-lar topics; and social habits.
They achieve a MAP of5040.279 against the ACL Anthology Reference Corpus(Bird et al., 2008), with the following features per-forming best: publication age, citation counts, theterms in citation sentences, and the LDA topics ofthe citing documents.
They also use (unchanged)PageRank authority counts as one of the features,but find that it provides little discriminative powerto the SVM.
A drawback of their method is the largeamount of information that has to be provided tocreate their SVM features, and the expensive train-ing routine, which is based on pairwise paper?papercomparisons in the corpus.Variations of the RLR tasks exist, which addi-tionally determine the position in the text of a pa-per where each recommended citation should occur(Tang and Zhang, 2009; He et al., 2011; Lu, 2011), atask which is typically solved by comparing a mov-ing window in the query paper against millions ofpreviously located citation contexts with.
The draw-back of this technique in contrast to ours is the factthat new papers, which have not collected sufficientcontexts in the literature, are severely disadvantagedand will never be recommended.We first create topics and then apply PageRankto find expertise within topical networks.
It is how-ever also possible to simultaneously model citationsand terms (Cohn and Hofmann, 2001; Mann et al.,2006).
Such models are not normally directly com-parable to ours; for instance Bharat and Henzinger?s(1998) model, a modified version of HITS (Klein-berg, 1998), is query-specific.There are numerous extensions to LDA that incor-porate external information in addition to the lex-ical information inside the documents in a corpus,via author-topic models and models of publicationvenues (Steyvers and Griffiths, 2007; Rosen-Zvi etal., 2010; Tang et al., 2008).
Erosheva et al.
(2004)model a corpus using a multinomial distribution si-multaneously over the citations and terms in eachdocument.
Topics (which they call aspects) are as-sociated with a list of the most likely words (inter-pretable as topics) and citations (interpretable as au-thorities) in that aspect.
Extensions of the model ex-ist (Nallapati and Cohen, 2008; Gruber et al., 2007;Chang and Blei, 2010; Kataria et al., 2010; Dietz etal., 2007).We avoid the tight coupling of topic discovery andcitation modeling that the above-mentioned worksfollow for several reasons.
Firstly, such models onlywork for papers and citations that were present dur-ing the learning stage, and there is no mechanismfor predicting influential citations for topics in gen-eral, or for combinations of topics.
The tight cou-pling might also result in overlooking some author-ities, namely those that are authoritative across sev-eral topics, which will be penalised via low jointdistribution probabilities in combined methods be-cause of the division of the probabilities across sev-eral topics.
Secondly, and more disturbingly, suchmodels will not locate topics that lack an authoritybecause the authority component of the joint distri-bution will be near-zero.
This rules out niches ina corpus where papers are equally relevant to eachother, or where the niches are so young that they donot yet have an established citation network.
Thereis also a scalability issue with joint models of top-ics and citations.
The evaluation data used in cou-pled models is generally small, with the number ofpapers ranging under around 2,000, the number ofcitations ranging under 10,000, and the number oftopics in their models ranging from eight to twenty.But LDA has been shown to scale to corpora of mil-lions of terms (Newman et al., 2006), and PageRankto billions (Page et al., 1998) of documents.
Ourmodel, which advocates a pipelined approach, ben-efits from the fact that separate topic modelling iscomputationally tractable using LDA, and the factthat citation graph modelling is cheap using Person-alised PageRank.4 Evaluation 1: RLRWe evaluate our authority-based search model us-ing the 2010 ACL Anthology Network (Radev et al.,2009).
We removed from it corrupted documents,i.e., those of less than 100 characters or contain-ing only control characters.
The ACL AnthologyNetwork provides external meta-data about the ar-ticles, which was manually curated.
We do not usethis meta-data because we wanted to build as systemthat can be applied to any large collection of arti-cles, where external meta-data would not normallyexist.
We therefore build an approximate citationgraph from the paper text itself, as a one-off taskwhen constructing the LDA space.
We extract titles,dates and full-text from every article and perform asearch of each articles title in the full-text of all other505Model MAP800 test papers, as in B&J (2010)B&J; best model 0.287TPR-NoDB 0.264TPR-NoAge 0.267TPR 0.30210,000 test papersA: NFIDF Cosine 0.062B: NFIDF + citation count 0.092C: NFIDF + global PageRank 0.099D: NFIDF LDA (KL divergence) 0.115E: TPR-NoDB 0.233F: TPR-NoAge 0.242G: TPR 0.268Figure 2: RLR resultsarticles (i.e., under the assumption that the referencelist is the (only) place where we will find such titles).Our system generates the RLR output (the recom-mended articles) for an article d by extracting tech-nical terms as described in section 2, examining thetopic distribution for that article ?d,t(i.e.
a ?iinFig.
1).
We use the topic distribution of article d inplace to generate the unique age-adjusted TPR tai-lored to the article, TPR(d, d?).
The 100 articlesd?
with the highest ThemedPageRanks are recom-mend as citations for article d. Results are reportedas mean average precision (MAP) of these 100 doc-uments against the actual citations in the article.We first compare our model to the state-of-the-art (Bethard and Jurafsky, 2010).
We emulate theirexperimental setup by including only the pre-2004articles in the corpus and testing only on the roughly800 2005/6 articles with more than 5 intra-corpuscitations in their reference list, for which we haveper-paper average precision scores.
The top part ofFig.
2 shows that our model (MAP=0.302) outper-forms their best model (MAP=0.287; difference at5% confidence with Wilcoxon Ranked Squares test),despite our model being a general, light-weight IRsystem, which relies on LDA and PageRank alone,and theirs is a specialised state-of-the art system,which relies on heavy-weight machine learning andon additional sociological features.The lower part of Fig.
2 compares the influenceof citation count, global PageRank, topic similar-ity, and combinations of topic similarity with ci-tation counts or global PageRank, and our model(TPR).
For these tests, we use the entire corpus of10,000 papers with more than 5 citations.
Over thebaseline (A), n-gram-frequency-inverse-document-frequency (NFIDF), both citation counts (B) andglobal PageRank (C) make a small improvement.Global LDA similarity scores (D) fare little better.As the performance of the full model (G;MAP=0.268) shows, the inclusion of topic modelslead to a large improvement over any of the above.This is, as far as we are aware, the first time that alarge-scale evaluation that finds significant improve-ments of a PageRank implementation over citationcounts in scientific search.We next consider our two modifications, age-adjusting (E) and double-biasing (F), in isolation.We use two versions of our system where weswitched off age-tapering and double-biasing (ie.,we only work with a change in the bias probabili-ties, as do Nie etal.
(2006), Havaliwala (2003) (al-though their models do not include automaticallygenerated topics) and Yang et al.
(2009)).
Ourmodel comfortably outperforms TPR-NoDB in boththe 800 and 10,000 paper experiment.
Similarly,the effect of age-tapering alone can be seen fromthe performance of TPR-NoAge (our model with-out age-adjusting), in the difference between 0.267and 0.302 and that between 0.242 and 0.268 (signif-icant at 99%).
This confirms our claim that a topic-specific age-tapered PageRank is superior to globalPageRank in scientific citation networks.5 Evaluation 2: Reading ListsThe aim of the second experiment is to test ourmodel against a much cleaner, albeit smaller goldstandard: on the task of reconstructing the mate-rial of expert-created reading lists.
We compare oursystem?s performance to three standard, commonlyused search engines: Lucene TFIDF, the Google-indexed ACL Anthology, and Google Scholar.
Wechose Google-index and Google Scholar becausethey represent commonly used state-of-the-art com-mercial search engines, and the Google-index iswhat is currently offered as the standard ACL An-thology search tool.
In contrast, Lucene TFIDFwas chosen to represent an easy-to-interpret, repro-ducible, out-of-the-box baseline implementing thesimplest kind of lexical similarity search withoutany notion of authority.
Of the three search engines,506we would predict Google Scholar to be the tough-est competitor to TPR, because it uses citation in-formation directly and it is reasonable to expect thatthe Google Scholar algorithm employs some domainadaptation to the scientific domain.We created gold standard expert-written readinglists using the following protocol.
Eight expertswere recruited from the computational linguisticsgroups of two universities (3 from one, 5 from theother).
All experts had a PhD in computational lin-guistics and several years of research experience.They were asked to choose a subject for an (imag-inary or existing) reading list for an MPhil student,concerning an area in which they know the litera-ture well.
We purposefully did not give them guid-ance as to the size of the reading list as we wantedto observe how experts create reading lists.
Duringthe interview, the experimenter documented the finallist chosen by the expert and made sure all paperschosen were present in the 2010 version of the ACLAnthology Network.This procedure resulted in reading lists of the fol-lowing topics and sizes: statistical parsing (22 pa-pers); parser evaluation (4); distributional semantics(14); domain adaptation for parsing (11); informa-tion extraction (9); lexical semantics (14); statisticalmachine translation models (5); and concept-to-textgeneration (16).In our retrieval model, which topic distribution ischosen for a query depends on whether the query isan exact match to one of the technical terms foundby our model.
If it is, then the topic distributionof the technical term is used directly as the querytopic distribution ?q, t (i.e.
a transposed renormal-ized ?
in Fig.
1).
If not, we perform a keyword-based search (using Lucene TFIDF), and use the av-erage topic distribution of the top 20 documents re-turned as the query topic distribution (i.e.
several ?iin Fig.
1).
The query topic distribution is then usedto linearly combine the topic-specific TPRs into aunique TPR tailored to the query.
The 20 documentswith the highest TPR are recommended.The three baselines are used as follows in theexperiment: The experiment is performed by issu-ing the topic of the reading list (exactly as givento us by the experts) as a key-word based query toeach system and recording the top 20 resulting pa-pers answers.
For Lucene TFIDF, we downloadedLucene.NET v2.9.2 and indexed our 2010 snapshotof the ACL Anthology using standard Lucene pa-rameters for the TFIDF model.
For the Google-indexed ACL Anthology (AAN), we use the in-terface provided on the ACL Anthology website.In order to provide an identical search ground, weautomatically exclude from the return lists papersadded after the creation of the AAN snapshot.
ForGoogle Scholar (GS), we use the interface providedat scholar.google.com, and parse returns to ex-clude non-AAN material semi-automatically.
Inthe case of Google Scholar, we restrict the searchground to the ACL Anthology by filtering the top200 return sets (which may lead to fewer than 20papers returned).We report FCSC, RCSC and F-score for each al-gorithm.
FCSC and RCSC are new metrics whichaddress the problem that F-score, being binary, doesnot support the notion of a ?close hit?, combinedwith the fact that we require a fine-grained compari-son of the quality of different systems retrieved listsdespite the small size of our gold standard.
Cita-tion Substitution Coefficient (FCSC), a new metricfor RLR, gives higher scores to papers closely re-lated to the target papers by citation distance.
TheFCSC of each expert paper is the inverse of the num-ber of nodes in the minimal citation graph connect-ing each expert paper to any system-retrieved pa-per (thus ranging between 0 and 1; non-connectedexpert papers receive a zero score).
We also in-troduce Reverse Citation Substitution Coefficient(RCSC), which measures the inverse of the num-ber of nodes in the minimal citation graph connect-ing each system-retrieved paper to any expert pa-per.
RCSC makes sure that systems cannot simplyincrease their FCSC values by returning many ir-relevant papers.
RCSC thus corresponds to preci-sion, while FCSC corresponds to recall.
The sys-tem RCSC and FCSC scores we report are the av-erage scores of all the system-retrieved and expertpapers, respectively.
Reporting both scores gives agood overall picture of system performance, partic-ularly when read together with the F-score.Fig.
3 shows that our model comfortably beats thecompetitor systems according to all metrics.
In par-ticular, our model > GS/AAN > Lucene TFIDF1.1For FCSC, the differences are statistically significant at507FCSC RCSC F-scoreAAN/Google 0.527 0.317 0.117GS 0.519 0.364 0.112Lucene TFIDF 0.412 0.330 0.040TPR 0.563 0.456 0.128Figure 3: Reading List Creation: Results.Concerning simpler methods of estimating author-ity, Fig.
4 shows that a multiplication of TFIDFby citation count (as Fujii (2007) does) results in aFCSC/RCSC of 0.419/0.359 (reported as TF-CC),and age-tapering of citation-count by dividing thecitation count by the age of the paper in years(reported as TF-CC-A) results in FCSC/RCSC of0.491/0.442.
We again compare different versionsof PageRank.
Global PageRank can be built intothe system by simple multiplication of PR scoresas above, with and without age-tapering (reportedas TF-PR and TF-PR-A, respectively).
We observea similar effect to the one reported by Bethard andJurafsky and seen in experiment 1, namely thatglobal PageRank only performs similar to citationcounts (0.450/0.360 vs 0.419/0.359).
With respectto double-biasing and age-tapering we see the sameeffect as in experiment 22.
In fact, we can see fromthese results that global PageRank barely improvesover standard TFIDF, while age-tapering even with-out topics already brings quite some improvement.Overall, these results confirms our claim of the su-periority of a topic-specific PageRank over globalPageRank in scientific citation networks.6 ConclusionsWe present here the first experiments that pinpointwhich modifications to PageRank are necessary to99% confidence via a two-tailed Wilcoxon Signed Ranks test,except that between GS and AAN (for which the confidence in-terval is only 96%) and that between Lucene and AAN, whereit is 98%.
Non-parametric paired tests such as the WilcoxonSigned Ranks test can be used on FCSC, but not on RCSC,as there are different sets of underlying system-retrieved pa-pers in each case.
For RCSC, differences between our modeland all others at 99% confidence interval, between GS andAAN/Lucene TFIDF at the 95% interval.
F-score is reportedfor completeness.2Wilcoxon Signed Rank test found all differences significantat the 99% level, except that between TF-PR and Lucene TFIDF(significant only at the 90% level), and the following equiva-lences: Lucene TFIDF = TF-CC; TF-PR = TF-CC; TF-CC-A =TF-PR-A; TF-CC-A = TF-PR.FCSC RCSCTF-CC 0.419 0.359TF-CC-A 0.491 0.442TF-PR 0.450 0.360TF-PR-A 0.512 0.407TPR-NoDB 0.541 0.440TPR-NoAge 0.526 0.436Figure 4: Citation counts and PageRank variants.adequately cater for the highly specialised situationwe encounter in science.
The modification we sug-gest are to use LDA-derived topics (Blei et al., 2003)as approximations for scientific fields, to calculateauthority in a topic-specific way, and to age-taperthe authority scores.
We present formulae wheretopics personalise both the bias and the transitionprobabilities.
This results in a general IR modelfor science incorporating a robust notion of author-ity.
Our implementation requires only minimal re-sources and relies only on LDA and PageRank cal-culation, which means that it is efficient during train-ing, retraining and at search time.We perform two evaluations.
In both, ourmodel significantly outperforms not only state-of-the-art, but also standard PageRank, non-age-tapered (but topical) PageRank, and non-topical (butage-tapered) PageRank.
Our model achieves itscompetitive performance by using only the raw textand citation links.
It requires no external informa-tion, neither explicit sociological information suchas past collaborations between authors, nor the ex-pertise and cooperation of like-minded readers, ascollaborative models do.
While successful applica-tions of collaborative filtering to bibliometric searchare rife (Goldberg et al., 2001; Agarwal et al., 2005;McNee et al., 2006; Torres et al., 2004), includingto reading list generation (Ekstrand et al., 2010), wewanted an entirely independent authority-based IRmodel similarity.
CF also suffers from a cold-startphenomenon, where recommendations are generallypoor where data is sparse, and has to wait for papersto be rated by a large number of authors (rather thancited) before it can rank them.Should the reader wish to evaluate the perfor-mance of TPR on their own PDF papers, it has beenincorporated into the Qiqqa reference managementsoftware 3.3Available at http://www.qiqqa.com508ReferencesN.
Agarwal, E. Haque, H. Liu, and L. Parsons.
2005.
Re-search paper recommender systems: A subspace clus-tering approach.
Advances in Web-Age InformationManagement.S.
Ananiadou.
1994.
A methodology for automatic termrecognition.
In Proceedings of COLING.S.K.
Pal B. Narayan, C. Murthy.
2003.
Topic continu-ity for web document categorization and ranking.
InIEEE/WIC International Conference on Web Intelli-gence.B.D.
Davison B. Wu, V. Goel.
2006.
Topical trustrank:Using topicality to combat web spam.
In Proceedingsof the 15th international conference on World WideWeb.S.
Bethard and D. Jurafsky.
2010. Who should i cite:learning literature search models from citation behav-ior.
In Proceedings of the 19th ACM InternationalConference on Information and Knowledge Manage-ment.K.
Bharat and M.R.
Henzinger.
1998.
Improved algo-rithms for topic distillation in a hyperlinked environ-ment.
In Proceedings of SIGIR.S.
Bird, R. Dale, B.J.
Dorr, B. Gibson, M.T.
Joseph, M.Y.Kan, D. Lee, B. Powley, D.R.
Radev, and Y.F.
Tan.2008.
The ACL anthology reference corpus: A ref-erence dataset for bibliographic research in computa-tional linguistics.
In Proc.
of LREC08.D.M.
Blei and J.D.
Lafferty.
2006.
Correlated TopicModels.
In Advances in Neural Information Process-ing Systems 18: Proceedings of the 2005 Conference,page 147.
Citeseer.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet allocation.
The Journal of Machine LearningResearch, 3:993?1022.J.
Boyd-Graber, D. Blei, and X. Zhu.
2007.
A topicmodel for word sense disambiguation.
In Proceedingsof EMNLP-CoNLL, pages 1024?1033.S.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual web search engine.
In Proceedings of the7th International World Wide Web Conference.J.
Chang and D.M.
Blei.
2010.
Hierarchical relationalmodels for document networks.
The Annals of AppliedStatistics, 4(1):124?150.P.
Chen, H. Xie, S. Maslov, and S. Redner.
2007.
Find-ing scientific gems with google?s pagerank algorithm.Journal of Infometrics, 1(1):8?15.D.
Cohn and T. Hofmann.
2001.
The missing link-aprobabilistic model of document content and hypertextconnectivity.
Advances in neural information process-ing systems, pages 430?436.L.
Dietz, S. Bickel, and T. Scheffer.
2007.
Unsuper-vised prediction of citation influences.
In Proceedingsof the 24th international conference on Machine learn-ing, page 240.
ACM.M.D.
Ekstrand, P. Kannan, J.A.
Stemper, J.T.
Butler,J.A.
Konstan, and J.T.
Riedl.
2010.
Automaticallybuilding research reading lists.
In Proceedings ofthe fourth ACM conference on Recommender systems,pages 159?166.
ACM.E.
Erosheva, S. Fienberg, and J. Lafferty.
2004.
Mixed-membership models of scientific publications.
Pro-ceedings of the National Academy of Sciences of theUnited States of America, 101(Suppl 1):5220.A.
Fujii.
2007.
Enhancing patent retrieval by citationanalysis.
In Proceedings of SIGIR.E.
Garfield.
1972.
Citation analysis as a tool in jour-nal evaluation.
American Association for the Advance-ment of Science.K.
Goldberg, T. Roeder, D. Gupta, and C. Perkins.
2001.Eigentaste: A constant time collaborative filtering al-gorithm.
Information Retrieval, 4(2):133?151.M.
Gori and A. Pucci.
2006.
Research paper rec-ommender systems: A random-walk based approach.IEEE Computer Society.A.
Gruber, M. Rosen-Zvi, and Y. Weiss.
2007.
Hiddentopic markov models.
In Proceedings of AISTATS.T.H.
Haveliwala.
2003.
Topic-sensitive pagerank: Acontext-sensitive ranking algorithm for web search.IEEE transactions on knowledge and data engineer-ing, pages 784?796.Q.
He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles.2009.
Detecting topic evolution in scientific literature:how can citations help?
In Proceeding of the 18thACM conference on Information and knowledge man-agement.Q.
He, D. Kifer, J. Pei, P. Mitra, and C.L.
Giles.
2011.Citation recommendation without author supervision.In Proceedings of the fourth ACM international con-ference on Web search and data mining.J.S.
Justeson and S.M.
Katz.
1995.
Technical terminol-ogy: some linguistic properties and an algorithm foridentification in text.
Natural Language Engineering,1(01):9?27.S.
Kataria, P. Mitra, and S. Bhatia.
2010.
Utilizing Con-text in Generative Bayesian Models for Linked Cor-pus.
In Proceedings of AAAI.M.M.
Kessler.
1963.
Bibliographic coupling be-tween scientific papers.
American Documentation,14(1):10?25.J.
Kleinberg.
1998.
Authoritative sources in a hy-perlinked environment.
In Proceedings of the 9thACM-SIAM Symposium on Discrete Algorithms.
Alsoavailable from http://www.cs.cornell.edu/home/kleinber/.509P.
Lopez and L. Romary.
2010.
HUMB: Automatic KeyTerm Extraction from Scientific Articles in GROBID.In SemEval 2010 Workshop.Y.
et al.
Lu.
2011.
Recommending citations with transla-tion model.
In Proceedings of the 20th ACM interna-tional conference on Information and knowledge man-agement.N.
Ma, J. Guan, and Y. Zhao.
2008.
Bringing pagerankto the citation analysis.
Information Processing andManagement, 44(2):800?810.G.S.
Mann, D. Mimno, and A. McCallum.
2006.
Biblio-metric impact measures leveraging topic analysis.
InProceedings of the 6th ACM/IEEE-CS joint conferenceon Digital libraries.S.M.
McNee, J. Riedl, and J.A.
Konstan.
2006.
Mak-ing recommendations better: an analytic model forhuman-recommender interaction.
In CHI?06 extendedabstracts on Human factors in computing systems.E.
Meij and M. De Rijke.
2007.
Using prior informationderived from citations in literature search.
In LargeScale Semantic Access to Content (Text, Image, Video,and Sound).R.
Nallapati and W. Cohen.
2008.
Link-plsa-lda: A newunsupervised model for topics and influence of blogs.In International Conference for Weblogs and SocialMedia.D.
Newman, P. Smyth, and M. Steyvers.
2006.
ScalableParallel Topic Models.
Journal of Intelligence Com-munity Research and Development.L.
Nie, B.D.
Davison, and X. Qi.
2006.
Topical linkanalysis for web search.
In Proceedings of SIGIR.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The pagerank citation ranking: Bringing order to theweb.
Stanford Digital Library Technologies Project.D.R.
Radev, P. Muthukrishnan, and V. Qazvinian.
2009.The ACL Anthology Network Corpus.
In Proceed-ings, ACL Workshop on NLP and IR for Digital Li-braries, Singapore.M.
Richardson and P Domingos.
2002.
The intelligentsurfer: Probabilistic combination of link and contentinformation in pagerank.
Advances in neural informa-tion processing systems, 14:1441?1448.M.
Rosen-Zvi, C. Chemudugunta, T. Griffiths, P. Smyth,and M. Steyvers.
2010.
Learning author-topic modelsfrom text corpora.
ACM Transactions on InformationSystems (TOIS), 28(1):1?38.B.
Narayan S.K.
Pal.
2005.
A web surfer model incorpo-rating topic continuity.
IEEE Transactions on Knowl-edge and Data Engineering, 17:726729.H.G.
Small.
1978.
Cited documents as concept symbols.Social Studies of Science, 8:327?340.M.
Steyvers and T. Griffiths.
2007.
Probabilistic topicmodels.
In T. Landauer, D. S. McNamara, S. Dennis,and W. Kintsch, editors, Handbook of latent semanticanalysis, page 427.
Erlbaum, Hillsdale, NJ.T.
Strohman, W.B.
Croft, and D. Jensen.
2007.
Recom-mending citations for academic papers.
In Proceed-ings of SIGIR.J.
Tang and J. Zhang.
2009.
A discriminative approachto Topic-Based citation recommendation.
Advances inKnowledge Discovery and Data Mining.J.
Tang, R. Jin, and J. Zhang.
2008.
A topic model-ing approach and its integration into the random walkframework for academic search.
In Eighth IEEE Inter-national Conference on Data Mining.R.
Torres, S.M.
McNee, M. Abel, J.A.
Konstan, andJ.
Riedl.
2004.
Enhancing digital libraries with Tech-Lens+.
In Proceedings of the 4th ACM/IEEE-CS jointconference on Digital libraries.D.
Walker, H. Xie, K.K.
Yan, and S. Maslov.
2007.Ranking scientific publications using a model of net-work traffic.
Journal of Statistical Mechanics: Theoryand Experiment, 2007:P06010.H.M.
Wallach.
2006.
Topic modeling: beyond bag-of-words (powerpoint).
In Proceedings of the 23rd inter-national conference on Machine learning.X.
Wang, A. McCallum, and X. Wei.
2007.
Topical n-grams: Phrase and topic discovery, with an applica-tion to information retrieval.
In Proceedings of the 7thIEEE international conference on data mining.X.
Wei and W.B.
Croft.
2006.
LDA-based documentmodels for ad-hoc retrieval.
In Proceedings of SIGIR.W.
Wong, W. Liu, and M. Bennamoun.
2009.
A proba-bilistic framework for automatic term recognition.
In-telligent Data Analysis, 13(4):499?539.Z.
Yang, J. Tang, J. Zhang, J. Li, and B. Gao.
2009.Topic-level random walk through probabilistic model.Advances in Data and Web Management.D.
Zhou, S. Zhu, K. Yu, X.
Song, B.L.
Tseng, H. Zha, andC.L.
Giles.
2008.
Learning multiple graphs for doc-ument recommendations.
In Proceeding of the 17thinternational conference on World Wide Web.510
