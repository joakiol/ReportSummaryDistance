Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 18?26,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPAnnotating and Recognising Named Entities in Clinical NotesYefeng WangSchool of Information TechnologyThe University of SydneyAustralia 2006ywang1@it.usyd.edu.auAbstractThis paper presents ongoing research inclinical information extraction.
This workintroduces a new genre of text which arenot well-written, noise prone, ungrammat-ical and with much cryptic content.
A cor-pus of clinical progress notes drawn forman Intensive Care Service has been manu-ally annotated with more than 15000 clin-ical named entities in 11 entity types.
Thispaper reports on the challenges involved increating the annotation schema, and recog-nising and annotating clinical named enti-ties.
The information extraction task hasinitially used two approaches: a rule basedsystem and a machine learning systemusing Conditional Random Fields (CRF).Different features are investigated to as-sess the interaction of feature sets and thesupervised learning approaches to estab-lish the combination best suited to thisdata set.
The rule based and CRF sys-tems achieved an F-score of 64.12% and81.48% respectively.1 IntroductionA substantial amount of clinical data is lockedaway in a non-standardised form of clinical lan-guage, which if standardised could be usefullymined to improve processes in the work of clin-ical wards, and to gain greater understanding ofpatient care as well as the progression of diseases.However in some clinical contexts these clinicalnotes, as written by a clinicians, are in a less struc-tured and often minimal grammatical form withidiosyncratic and cryptic shorthand.
Whilst thereis increasing interest in the automatic extractionof the contents of clinical text, this particular typeof notes cause significant difficulties for automaticextraction processes not present for well-writtenprose notes.The first step to the extraction of structured in-formation from these clinical notes is to achieveaccurate identification of clinical concepts ornamed entities.
An entity may refer to a concreteobject mentioned in the notes.
For example, thereare 3 named entities - CT, pituitary macroade-noma and suprasellar cisterns in the sentence:CT revealed pituitary macroadenoma in suprasel-lar cisterns.In recent years, the recognition of named en-tities from biomedical scientific literature has be-come the focus of much research, a large numberof systems have been built to recognise, classifyand map biomedical terms to ontologies.
How-ever, clinical terms such as findings, proceduresand drugs have received less attention.
Althoughdifferent approaches have been proposed to iden-tify clinical concepts and map them to terminolo-gies (Aronson, 2001; Hazlehurst et al, 2005;Friedman et al, 2004; Jimeno et al, 2008), mostof the approaches are language pattern based,which suffer from low recall.
The low recall rateis mainly due to the incompleteness of medicallexicon and expressive use of alternative lexico-grammatical structures by the writers.
However,only little work has used machine learning ap-proaches, because no training data has been avail-able, or the data are not available for clinicalnamed entity identification.There are semantically annotated corpora thathave been developed in biomedical domain in thepast few years, for example, the GENIA cor-pus of 2000 Medline abstracts has been annotatedwith biological entities (Kim et al, 2003); ThePennBioIE corpus of 2300 Medline abstracts an-notated with biomedical entities, part-of-speechtag and some Penn Treebank style syntactic struc-tures (Mandel, 2006) and LLL05 challenge taskcorpus (N?edellec, 2005).
However only a few cor-pora are available in the clinical domain.
Manycorpora are ad hoc annotations for evaluation, and18the size of the corpora are small which is not opti-mal for machine learning strategies.
The lack ofdata is due to the difficulty of getting access toclinical text for research purposes and clinical in-formation extraction is still a new area to explore.Many of the existing works focused only on clini-cal conditions or disease (Ogren et al, 2006; Pes-tian et al, 2007).
The only corpus that is anno-tated with a variety of clinical named entities isthe CLEF project (Roberts et al, 2007) .Most of the works mentioned above are anno-tated on formal clinical reports and scientific liter-ature abstracts, which generally conform to gram-matical conventions of structure and readability.The CLEF data, annotated on clinical narrative re-ports, still uses formal clinical reports.
The clini-cal notes presented in this work, is another genreof text, that is different from clinical reports, be-cause they are not well-written.
Notes writtenby clinicians and nurses are highly ungrammaticaland noise prone, which creates issues in the qualityof any text processing.
Examples of problems aris-ing from such texts are: firstly, variance in the rep-resentation of core medical concepts, whether un-consciously, such as typographical errors, or con-sciously, such as abbreviations and personal short-hand; secondly, the occurrences of different no-tations to signify the same concept.
The clinicalnotes contain a great deal of formal terminologybut used in an informal and unorderly manner, forexample, a study of 5000 instances of GlasgowComa Score (GCS) readings drawn from the cor-pus showed 321 patterns are used to denote thesame concept and over 60% of them are only usedonce.The clinical information extraction problem isaddressed in this work by applying machine learn-ing methods to a corpus annotated for clinicalnamed entities.
The data selection and annota-tion process is described in Section 3.
The initialapproaches to clinical concept identification usingboth a rule-based approach and machine learningapproach are described in Section 4 and Section 5respectively.
A Conditional Random Fields basedsystem was used to study and analyse the contri-bution of various feature types.
The results anddiscussion are presented in Section 6.2 Related WorkThere is a great deal of research addressing con-cept identification and concept mapping issues.The Unified Medical Language System Metathe-saurus (UMLS) (Lindberg et al, 1993) is theworld?s largest medical knowledge source and ithas been the focus of much research.
The sim-plest approaches to identifying medical conceptsin text is to maintain a lexicon of all the entitiesof interest and to systematically search throughthat lexicon for all phrases of any length.
Thiscan be done efficiently by using an appropriatedata structure such as a hash table.
Systems thatuse string matching techniques include SAPHIRE(Hersh and Hickam, 1995), IndexFinder (Zou etal., 2003), NIP (Huang et al, 2005) and Max-Matcher (Zhou et al, 2006).
With a large lexicon,high precision and acceptable recall were achievedby this approach in their experiments.
However,using these approaches out of box for our task isnot feasible, due to the high level of noise in theclinical notes, and the ad hoc variation of the ter-minology, will result in low precision and recall.A more sophisticated and promising approachis to make use of shallow parsing to identify allnoun phrases in a given text.
The advantage ofthis approach is that the concepts that do not existin the lexicon can be found.
MedLEE (Friedman,2000) is a system for information extraction inmedical discharge summaries.
This system uses alexicon for recognising concept semantic classes,word qualifiers, phrases, and parses the text usingits own grammar, and maps phrases to standardmedical vocabularies for clinical findings and dis-ease.
The MetaMap (Aronson, 2001) programuses a three step process started by parsing free-text into simple noun phrases using the Special-ist minimal commitment parser.
Then the phrasevariants are generated and mapping candidates aregenerated by looking at the UMLS source vocabu-lary.
Then a scoring mechanism is used to evaluatethe fit of each term from the source vocabulary, toreduce the potential matches (Brennan and Aron-son, 2003).
Unfortunately, the accurate identifica-tion of noun phrases is itself a difficult problem,especially for the clinical notes.
The ICU clin-ical notes are highly ungrammatical and containlarge number of sentence fragments and ad hocterminology.
Furthermore, highly stylised tokensof combinations of letters, digits and punctua-tion forming complex morphological tokens aboutclinical measurements in non-regular patterns addan extra load on morphological analysis, e.g.
?4-6ml+/hr?
means 4-6 millilitres or more secreted by19the patient per hour.
Parsers trained on generic textand MEDLINE abstracts have vocabularies andlanguage models that are inappropriate for suchungrammatical texts.Among the state-of-art systems for conceptidentification and named entity recognition arethose that utilize machine learning or statisticaltechniques.
Machine learners are widely used inbiomedical named entity recognition and have out-performed the rule based systems (Zhou et al,2004; Tsai et al, 2006; Yoshida and Tsujii, 2007).These systems typically involve using many fea-tures, such as word morphology or surroundingcontext and also extensive post-processing.
Astate-of-the-art biomedical named entity recog-nizer uses lexical features, orthographic features,semantic features and syntactic features, such aspart-of-speech and shallow parsing.Many sequential labeling machine learners havebeen used for experimentation, for example, Hid-den Markov Model(HMM) (Rabiner, 1989), Max-imum Entropy Markov Model (MEMM) (McCal-lum et al, 2000) and Conditional Random Fields(CRF) (Lafferty et al, 2001).
Conditional Ran-dom Fields have proven to be the best performinglearner for this task.
The benefit of using a ma-chine learner is that it can utilise both the infor-mation form of the concepts themselves and thecontextual information, and it is able to performprediction without seeing the entire length of theconcepts.
The machine learning based systems arealso good at concept disambiguation, in which astring of text may map to multiple concepts, andthis is a difficult task for rule based approaches.3 Annotation of Corpus3.1 The DataData were selected form a 60 million token cor-pus of Royal Prince Alfred Hospital (RPAH)?s In-tensive Care Service (ICS).
The collection con-sists of clinical notes of over 12000 patients ina 6 year time span.
It is composed of a vari-ety of different types of notes, for example, pa-tient admission notes, clinician notes, physiother-apy notes, echocardiogram reports, nursing notes,dietitian and operating theatre reports.
The corpusfor this study consists of 311 clinical notes drawnfrom patients who have stayed in ICS for morethan 3 days, with most frequent causes of admis-sion.
The patients were identified in the patientrecords using keywords such as cardiac disease,Category ExampleFINDING lung cancer; SOB; feverPROCEDURE chest X Ray;laparotomySUBSTANCE Ceftriaxone; CO2; plateletQUALIFIER left; right;elective; mildBODY renal artery; LAD; diaphragmBEHAVIOR smoker; heavy drinkerABNORMALITY tumor; lesion; granulomaORGANISM HCV; proteus; B streptococcusOBJECT epidural pump; larnygoscopeOCCUPATION cardiologist; psychiatristOBSERVABLE GCS; blood pressureTable 1: Concept categories and examples.liver disease, respiratory disease, cancer patient,patient underwent surgery etc.
Notes vary in size,from 100 words to 500 words.
Most of the notesconsist of content such as chief complaint, patientbackground, current condition, history of presentillness, laboratory test reports, medications, socialhistory, impression and further plans.
The varietyof content in the notes ensures completely differ-ent classes of concepts are covered by the corpus.The notes were anonymised, patient-specific iden-tifiers such as names, phone numbers, dates werereplaced by a like value.
All sensitive informationwas removed before annotation.3.2 Concept CategoryBased on the advice of one doctor and one clini-cian/terminologist, eleven concept categories weredefined in order to code the most frequently usedclinical concepts in ICS.
The eleven categorieswere derived from the SNOMED CT concept hier-archy.
The categories and examples are listed inTable 1.
Detailed explanation of these categoriescan be found in SNOMED CT Reference Guide13.3 Nested ConceptNested concepts are concepts containing otherconcepts and are annotated in the corpus.
They areof particular interest due to their compositional na-ture.
For example, the term left cavernous carotidaneurysm embolisation is the outermost concept,which belongs to PROCEDURE.
It contains sev-eral inner concepts: the QUALIFIER left and theterm cavernous carotid aneurysm as a FINDING,1SNOMED CTR?Technical Reference Guide - July 2008International Release.
http://www.ihtsdo.org/20which also contains cavernous carotid as BODYand aneurysm as ABNORMALITY.The recognition of nested concepts is crucial forother tasks that depend on it, such as coreferenceresolution, relation extraction, and ontology con-struction, since nested structures implicitly con-tain relations that may help improve their correctrecognition.
The above outermost concept may berepresented by embedded concepts and relation-ships as: left cavernous carotid aneurysm emboli-sation IS A embolisation which has LATERALITYleft, has ASSOCIATED MORPHOLOGY aneurysmand has PROCEDURE SITE cavernous carotid.3.4 Concept FrequencyThe frequency of annotation for each concept cat-egory are detailed in Table 2.
There are in total15704 annotated concepts in the corpus, 12688are outermost concepts and 3016 are inner con-cepts.
The nested concepts account for 19.21% ofall concepts in the corpus.
The corpus has 46992tokens, with 18907 tokens annotated as concepts,hence concept density is 40.23% of the tokens.This is higher than the density of the GENIA andMUC corpora.
The 12688 annotated outermostconcepts, results in an average length of 1.49 to-kens per concept which is less than those of theGENIA and MUC corpora.
These statistics suggestthat ICU staff tend to use shorter terms but moreextensively in their clinical notes which is in keep-ing with their principle of brevity.The highest frequency concepts are FIND-ING, SUBSTANCE, PROCEDURE, QUALIFIER andBODY, which account 86.35% of data.
The re-maining 13.65% concepts are distributed into 6rare categories.
The inner concepts are mainlyfrom QUALIFIER, BODY and ABNORMALITY, be-cause most of the long and complex FINDINGand PROCEDURE concepts contain BODY, AB-NORMALITY and QUALIFIER, such as the examplein Section 3.3.3.5 Annotation AgreementThe corpus had been tokenised using a white-space tokeniser.
Each note was annotated by twoannotators: the current author and a computationallinguist experienced with medical texts.
Annota-tion guidelines were developed jointly by the an-notators and the clinicians.
The guidelines wererefined and the annotators were trained using aniterative process.
At the end of each iteration, an-notation agreement was calculated and the anno-Category Outer Inner AllABNORMALITY 0 926 926BODY 735 1331 2066FINDING 4741 71 4812HEALTHPROFILE 399 0 399OBJECT 179 23 202OBSERVABLE 198 227 425OCCUPATION 139 0 139ORGANISM 36 17 53PROCEDURE 2353 39 2392QUALIFIER 1659 21 1680SUBSTANCE 2249 361 2610TOTAL 12688 3016 15704Table 2: Frequencies for nested and outermostconcept.tations were reviewed.
The guidelines were mod-ified if necessary.
This process was stopped un-til the agreement reached a threshold.
In total30 clinical notes were used in the developmentof guidelines.
Inter-Annotator Agreement (IAA)is reported as the F-score by holding one anno-tation as the standard.
F-score is commonly usedin information retrieval and information extractionevaluations, which calculates the harmonic meanof recall and precision as follows:F =2?
precision?
recallprecision+ recallThe IAA rate in the development cycle finallyreached 89.83.
The agreement rate between thetwo annotators for the whole corpus by exactmatching was 88.12, including the 30 develop-ment notes.
An exact match means both theboundaries and classes are exactly the same.
Theinstances where the annotators did not agree werereviewed and relabeled by a third annotator to gen-erate a single annotated gold standard corpus.
Thethird annotator is used to ensure every concept isagreed on by at least two annotators.Disagreements frequently occur at the bound-aries of a term.
Sometimes it is difficult to deter-mine whether a modifier should be included in theconcept: massive medial defect or medial defect,in which the latter one is a correct annotation andmassive is a severity modifier.
Mistakes in anno-tation also came from over annotation of a gen-eral term: anterior approach, which should notbe annotated.
Small disagreements were causedby ambiguities in the clinical notes: some medical21devices (OBJECT) are often annotated as PROCE-DURE, because the noun is used as a verb in thecontext.
Another source of disagreement is due tothe ambiguity in clinical knowledge: it was diffi-cult to annotate the man-made tissues as BODY orSUBSTANCE, such as bone graft or flap.4 Rule Based Concept Matcher4.1 Proofreading the CorpusBefore any other processing, the first step wasto resolve unknown tokens in the corpus.
Theunknown tokens are special orthographies or al-phabetic words that do not exist in any dic-tionary, terminologies or gazetteers.
Medicalwords were extracted from the UMLS lexicon andSNOMED CT (SNOMED International, 2009),and the MOBY (Ward, 1996) dictionary was usedas the standard English word list.
A list of abbrevi-ations were compiled from various resources.
Theabbreviations in the terminology were extractedusing pattern matching.
Lists of abbreviations andshorthand were obtained from the hospital, andwere manually compiled to resolve the meaning.Every alphabetic token was verified against thedictionary list, and classified into Ordinary En-glish Words, Medical Words, Abbreviations, andUnknown Words.An analysis of the corpus showed 31.8% ofthe total tokens are non-dictionary words, whichcontains 5% unknown alphabetic words.
Mostof these unknown alphabetic words are obviousspelling mistakes.
The spelling errors were cor-rected using a spelling corrector trained on the60 million token corpus, Abbreviations and short-hand were expanded, for example defib expandsto defibrillator.
Table 3 shows some unknown to-kens and their resolutions.
The proofreading re-quire considerable amount of human effort to buildthe dictionaries.4.2 Lexicon look-up Token MatcherThe lexicon look-up performed exact matching be-tween the concepts in the SNOMED CT terminol-ogy and the concepts in the notes.
A hash tabledata structure was implemented to index lexicalitems in the terminology.
This is an extension tothe algorithm described in (Patrick et al, 2006).
Atoken matching matrix run through the sentenceto find all candidate matches in the sentence tothe lexicon, including exact longest matches, par-tial matches, and overlapping between matches.unknown word examples resolutionCORRECT WORD bibasally bibasallyMISSING SPACE oliclinomel Oli ClinomelSPELLING ERROR dolaseteron dolasetronACRONYM BP blood pressureABBREVIATION N+V Nausea and vomitingSHORTHAND h?serous haemoserousMEASUREMENT e4v1m6 GCS measurementSLASHWORDS abg/ck/tropt ABG CK TroptREADINGS 7mg/hrTable 3: Unknown tokens and their resolutions.Then a Viterbi algorithm was used to find the bestsequence of non-overlapping concepts in a sen-tence that maximise the total similarity score.
Thismethod matches the term as it appears in the ter-minology so is not robust against term variationsthat have not been seen in the terminology, whichresults in an extremely low recall.
In addition, theprecision may be affected by ambiguous terms ornested terms.The exact lexicon look-up is likely to fail onmatching long and complex terms, as clinicians donot necessarily write the modifier of a concept ina strict order, and some descriptors are omitted.for example white blood cell count normal can bewritten as normal white cell count.
In order toincrease recall, partial matching is implemented.The partial matching tries to match the best se-quence, but penalise non-matching gaps betweentwo terms.
The above example will be found us-ing partial matching.5 CRF based Clinical Named EntityRecogniser5.1 Conditional Random FieldsThe concept identification task has been formu-lated as a named entity recognition task, whichcan be thought of as a sequential labeling problem:each word is a token in a sequence to be assigneda label, for example, B-FINDING, I-FINDING, B-PROCEDURE, I-PROCEDURE, B-SUBSTANCE, I-SUBSTANCE and so on.
Conditional RandomFields (CRF) are undirected statistical graphicalmodels, which is a linear chain of Maximum En-tropy Models that evaluate the conditional prob-ability on a sequence of states give a sequenceof observations.
Such models are suitable for se-quence analysis.
CRFs has been applied to the task22of recognition of biomedical named entities andhave outperformed other machine learning mod-els.
CRF++2is used for conditional random fieldslearning.5.2 Features for the LearnerThis section describes the various features used inthe CRF model.
Annotated concepts were con-verted into BIO notation, and feature vectors weregenerated for each token.Orthographic Features: Word formation wasgenaralised into orthographic classes.
The presentmodel uses 7 orthographic features to indicatewhether the words are captialised or upper case,whether they are alphanumeric or contains anyslashes, as many findings consist of captialisedwords; substances are followed by dosage, whichcan be captured by the orthography.
Word prefixesand suffixes of character length 4 were also usedas features, because some procedures, substancesand findings have special affixes, which are verydistinguishable from ordinary words.Lexical Features: Every token in the trainingdata was used as a feature.
Alphabetic wordsin the training data were converted to lowercase,spelling errors detected in proofreading stage werereplaced by the correct resolution.
Shorthand andabbreviations were expanded into bag of words(bow) features.
The left and right lexical bi-grams were also used as a feature, however it onlyyielded a slight improvement in performance.
Toutilise the context information, neighboring wordsin the window [?2,+2] are also added as features.Context window size of 2 is chosen because ityields the best performance.
The target and previ-ous labels are also used as features, and had beenshown to be very effective.Semantic Features: The output from thelexical-lookup system was used as features in theCRF model.
The identified concepts were addedto the feature set as semantic features, becausethe terminology can provide semantic knowledgeto the learner such as the category information ofthe term.
Moreover, many partially matched con-cepts from lexicon-lookup were counted as incor-rectly matching, however they are single term headnouns which are effective features in NER.Syntactic features were not used in this exper-iment as the texts have only a little grammaticalstructure.
Most of the texts appeared in fragmen-2http://crfpp.sourceforge.net/Experiment P R F-scoreno pruning 58.76 26.63 36.35exact matching 69.48 37.70 48.88+proofreading 74.81 52.42 61.65+partial matching 69.39 59.60 64.12Table 4: Lexical lookup Performance.tary sentences or single word or phrase bullet pointformat, which is difficult for generic parsers towork with correctly.6 EvaluationThis section presents experiment results for boththe rule-based system and machine learning basedsystem.
Only the 12688 outermost concepts areused in the experiments, because nested terms re-sult in multi-label for a single token.
Since thereis no outermost concepts in ABNORMALITY, theclassification was done on the remaining 10 cate-gories.
The performances were evaluated in termsof recall, precision and F-score.6.1 Token Matcher PerformanceThe lexical lookup performance is evaluated onthe whole corpus.
The first system uses only ex-act matching without any pre-processing of thelexicon.
The second experiment uses a prunedterminology with ambiguous categories and un-necessary categories removed, but without proof-reading of the corpus.
The concept will be re-moved if it belongs to a category that is not usedin the annotation.
The third experiment used theproofreaded corpus with all abbreviations anno-tated.
The fourth experiment was conducted onthe proofread corpus allowing both exact match-ing and partial matching.
The results are outlinedin Table 4.The lexicon lookup without pruning the ter-minologies achieved low precision and extremelylow recall.
This is mainly due to the ambiguousterms in the lexicon.
By removing unrelated termsand categories in the lexicon, both precision andrecall improved dramatically.
Proofreading, cor-recting a large number of unknown tokens such asspelling errors or irregular conventions further in-creased both precision and recall.
The 14.72 gainin recall mainly came from resolution and expan-sion of shorthand, abbreviations, and acronyms inthe notes.
This also suggest that this kind of clin-ical notes are very noisy, and require a consider-23able amount of effort in pre-processing.
Allow-ing partial matching increased recall by 7.18, butdecreased precision by 5.52, and gave the overallincrease of 2.47 F-score.
Partial matching discov-ered a larger number of matching candidates us-ing a looser matching criteria, therefore decreasedin precision with compensation of an increase inrecall.The highest precision achieved by exact match-ing is 74.81, confirming that the lexical lookupmethod is an effective means of identifying clin-ical concepts.
However, it requires extensive ef-fort on pre-processing both corpus and the termi-nology and is not easily adapted to other corpora.The lexical matching fails to identify long termsand has difficult in term disambiguation.
The lowrecall is caused by incompleteness of the terminol-ogy.
However, the benefit of using lexicon lookupis that the system is able to assign a concept iden-tifier to the identified concept if available.6.2 CRF Feature PerformanceThe CRF system has been evaluated using 10-foldcross validation on the data set.
The evaluationwas performed using the CoNLL shared task eval-uation script3.The CRF classifier experiment results areshown in Table 5.
A baseline system was builtusing only bag-of-word features from the trainingcorpus.
A context-window size of 2 and tag pre-diction of previous token were used in all experi-ments.
Without using any contextual features theperformance was 48.04% F-score.
The baselineperformance of 71.16% F-score outperformed thelexical-look up performance.
Clearly the contex-tual information surrounding the concepts gives astrong contribution in identification of concepts,while lexical-lookup hardly uses any contextualinformation.The full system is built using all features de-scribed in Section 5.2, and achieved the best resultof 81.48% F-score.
This is a significant improve-ment of 10.32% F-score over the baseline system.Further experimental analysis of the contributionof feature types was conducted by removing eachfeature type from the full system.
?bow meansbag-of-word features are removed from the fullsystem.
The results show only bow and lexical-lookup features make significant contribution tothe system, which are 5.49% and 4.40% sepa-3http://www.cnts.ua.ac.be/conll2002/ner/bin/Experiment P R F-scorebaseline 76.86 66.26 71.16+lexical-lookup 82.61 74.88 78.55full 84.22 78.90 81.48?bow 81.26 73.32 77.08?bigram 83.17 78.74 80.89?abbreviation 83.20 77.26 80.12?orthographic 83.67 78.24 80.87?affixes 83.16 77.01 79.97?lexical-lookup 79.06 73.15 75.99Table 5: Experiment on Feature Contribution forthe ICU corpus.rately.
Bigram, orthographic, affixes and abbrevi-ation features each makes around ?
1% contribu-tion to the F-score, which is individually insignif-icant, however the combination of them makes asignificant contribution, which is 4.83% F-score.The most effective feature in the system is theoutput from the lexical lookup system.
Anotherexperiment using only bow and lexical-lookup fea-tures showed a boost of 7.39% F-score.
This isproof of the hypothesis that using terminology in-formation in the machine learner would increaserecall.
In this corpus, about one third of the con-cepts has a frequency of only 1, from which thelearner as unable to learn anything from the train-ing data.
The gain in performance is due to theingestion of semantic domain knowledge which isprovided by the terminology.
This knowledge isuseful for determining the correct boundary of aconcept as well as the classification of the concept.6.3 Detailed CRF PerformanceThe detailed results of the CRF system are shownin Table 6.
Precision, Recall and F-score for eachclass are reported.
There is a consistent gap be-tween Recall and Precision across all categories.The best performing classes are among the mostfrequent categories.
This is an indication that suf-ficient training data is a crucial factor in achievinghigh performance.
SUBSTANCE, PROCEDURE andFINDING are the best three categories due to theirhigh frequency in the corpus.
However, QUALI-FIER achieved a lower F-score because qualifiersusually appear at the boundaries of two concepts,which is a source of error in boundary recognition.Low frequency categories generally achievedhigh precision and low recall.
The recall decreasesas the number of training instances decreases, be-24Class P R F-scoreBODY 72.00 64.29 67.92FINDING 83.17 78.74 80.89BEHAVIOR 83.87 72.22 77.61OBJECT 75.00 27.27 40.00OBSERVABLE 89.47 56.67 69.39ORGANISM 0.00 0.00 0.00PROCEDURE 87.63 81.09 84.24QUALIFIER 75.80 75.32 75.56OCCUPATION 87.50 41.18 56.00SUBSTANCE 91.90 88.53 90.19Table 6: Detailed Performance of the CRF system.cause there is not enough information in the train-ing data to learn the class profiles.
It is a chal-lenge to boost the recall of rare categories due tothe variability of the terms in the notes.
It is notlikely that the term would match to the terminol-ogy, and hence there would be no utilisation of thesemantic information.Another factor that causes recognition errors isthe nested concepts.
BODY achieved the least pre-cision because of the high frequency of nestedconcepts in its category.
The nested constructionalso causes boundary detection problems, for ex-ample C5/6 cervical discectomyPROCEDUREisannotated as C5/6BODYand cervical discectomyPROCEDURE.The results presented here are higher than thosereported in biomedical NER system.
Although itis difficult to compare with other work because ofthe different data set, but this task might be easierdue to the shorter length of the concepts and fewerlong concepts (avg.
1.49 in this corpus vs. avg.1.70 token per concept in GENIA).
Local featureswould be able to capture most of the useful infor-mation while not introducing ambiguity.7 Future Work and ConclusionThis paper presents a study of identification ofconcepts in progressive clinical notes, which isanother genre of text that hasn?t been studied todate.
This is the first step towards information ex-traction of free text clinical notes and knowledgerepresentation of patient cases.
Now that the cor-pus has been annotated with coarse grained con-cept categories in a reference terminology, a pos-sible improvement of the annotation is to reevalu-ate the concept categories and create fine grainedcategories by dividing top categories into smallerclasses along the terminology?s hierarchy.
For ex-ample, the FINDING class can be further dividedinto SYMPTOM/SIGN, DISORDER and EVALUA-TION RESULTS.
The aim would be to achieve bet-ter consistency, less ambiguity and greater cover-age of the concepts in the corpus.The nested concepts model the relations be-tween atomic concepts within the outermost con-cepts.
These structures represent important rela-tionships within this type of clinical concept.
Thenext piece of work could be the study of these rela-tionships.
They can be extended to represent rela-tionships between clinical concepts and allow forrepresenting new concepts using structured infor-mation.
The annotation of relations is under de-velopment.
The future work will move from con-cept identification to relation identification and au-tomatic ontology extension.Preliminary experiments in clinical named en-tity recognition using both rule-based and machinelearning approaches were performed on this cor-pus.
These experiments have achieved promisingresults and show that rule based lexicon lookup,with considerable effort on pre-processing andlexical verification, can significantly improve per-formance over a simple exact matching process.However, a machine learning system can achievegood results by simply adapting features frombiomedical NER systems, and produced a mean-ingful baseline for future research.
A directionto improve the recogniser is to add more syntac-tic features and semantic features by using depen-dency parsers and exploiting the unlabeled 60 mil-lion token corpus.In conclusion, this paper described a new anno-tated corpus in the clinical domain and presentedinitial approaches to clinical named entity recog-nition.
It has demonstrated that practical accept-able named entity recognizer can be trained on thecorpus with an F-score of 81.48%.
The challengein this task is to increase recall and identify rareentity classes as well as resolve ambiguities intro-duced by nested concepts.
The results should beimproved by using extensive knowledge resourceor by increasing the size and improving the qualityof the corpus.AcknowledgmentsThe author wish to thank the staff of the RoyalPrince Alfred Hospital, Sydney : Dr. StephenCrawshaw, Dr. Robert Herks and Dr Angela Ryan25for their support in this project.ReferencesR.
Aronson.
2001.
Effective mapping of biomedicaltext to the UMLS Metathesaurus: the MetaMap pro-gram.
In Proceeding of the AMIA Symposium,17?21.F.
Brennan and A. Aronson 2003.
Towards link-ing patients and clinical information: detectingUMLS concepts in e-mail.
Journal of BiomedicalInformatics,36(4/5),334?341.A.
C?ot?e and American Veterinary Medical Associa-tion and College of American Pathologists.
2009.Snomed International.
College of American Pathol-ogists.C.
Friedman 2000.
A broad coverage natural languageprocessing system.
In Proceedings of the AMIASymposium,270?274.C.
Friedman, L. Shagina, Y. Lussier, and G. Hripc-sak.
2004.
Automated Encoding of ClinicalDocuments Based on Natural Language Process-ing.
Journal of the American Medical InformaticsAssociation,11(5),392?402.B.
Hazlehurst, R. Frost, F. Sittig, and J. Stevens.
2005.MediClass: A System for Detecting and ClassifyingEncounter-based Clinical Events in Any ElectronicMedical Record.
Journal of the American MedicalInformatics Association,12(5),517?529.R.
Hersh, and D. Hickam.
1995.
Information re-trieval in medicine: The SAPHIRE experience.Journal of the American Society for InformationScience,46(10),743?747.Y.
Huang, J. Lowe, D. Klein, and J. Cucina.
2005.Improved Identification of Noun Phrases in Clini-cal Radiology Reports Using a High-PerformanceStatistical Natural Language Parser Augmented withthe UMLS Specialist Lexicon.
Journal of the Amer-ican Medical Informatics Association,12(3),275?285.A.
Jimeno, et al 2008.
Assessment of disease namedentity recognition on a corpus of annotated sen-tences.
BMC Bioinformatics,9(3).D.
Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.GENIA corpus - a semantically annotated cor-pus for bio-textmining.
Journal of Bioinformatics,19(1),180?182.J.
Lafferty et al 2001.
Conditional Random Fields:Probabilistic Models for Segmenting and Label-ing Sequence Data Machine learning-internationalworkshop then conference, 282?289.A.
Lindberg et al 1993.
The Unified Medical Lan-guage System.
Methods Inf Med.M.
Mandel 2006.
Integrated Annotation of Biomedi-cal Text: Creating the PennBioIE corpus.
Text Min-ing Ontologies and Natural Language Processing inBiomedicine, Manchester, UK.A.
McCallum, et al 2000.
Maximum entropy Markovmodels for information extraction and segmentationProc.
17th International Conf.
on Machine Learn-ing, 591?598.C.
N?edellec.
2005.
Learning Language in Logic -Genic Interaction Extraction Challenge.
Proceed-ings of the ICML05 Workshop on Learning Lan-guage in Logic, Bonn, 31?37.V.
Ogren, G. Savova, D. Buntrock, and G. Chute.2006.
Building and Evaluating Annotated Corporafor Medical NLP Systems.
AMIA Annu Symp Pro-ceeding..J. Patrick, Y. Wang, and P. Budd.
2006.
AutomaticMapping Clinical Notes to Medical TerminologiesIn Proceedings of Australasian Language Technol-ogy Workshop.P.
Pestian, C. Brew, P. Matykiewicz, J. Hovermale, N.Johnson, K. Cohen, and W. Duch.
2007.
A SharedTask Involving Multi-label Classification of ClinicalFree Text In Proceedings of BioNLP workshop.R.
Rabiner 1989.
A tutorial on hidden Markov modelsand selected applications inspeech recognition Pro-ceedings of the IEEE,77(2), 257?286.A.
Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,Y.
Guo, A. Setzer, I. and Roberts.
2007.
TheCLEF Corpus: Semantic Annotation of ClinicalText.
AMIA Annu Symp Proceeding., Oct 11:625?629.R.
Tsai, L. Sung, J. Dai, C. Hung, Y.
Sung, and L. Hsu.2006.
NERBio: using selected word conjunctions,term normalization, and global patterns to improvebiomedical named entity BMC Bioinformatics.G.
Ward 1996.
Moby thesaurus.http://etext.icewire.com/moby/.K.
Yoshida, and J. Tsujii.
2007.
Reranking forBiomedical Named-Entity Recognition Proceed-ings of BioNLP 2007: Biological, translational, andclinical language processing, 209?216.G.
Zhou, J. Zhang, J. Su, D. Shen, and L. Tan.
2004.Recognizing Names in Biomedical Texts: a MachineLearning Approach BioInformatics, 20(7) 1178?1190.X.
Zhou, et al 2006.
MaxMatcher: Biological ConceptExtraction Using Approximate Dictionary Lookup.Proc PRICAI,1145?1149.Q.
Zou.
2003.
IndexFinder: A Method of Extract-ing Key Concepts from Clinical Texts for Indexing.Proc AMIA Symp,763?767.26
