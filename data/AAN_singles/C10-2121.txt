Coling 2010: Poster Volume, pages 1050?1058,Beijing, August 2010Streaming Cross Document Entity Coreference ResolutionDelip Rao and Paul McNamee and Mark DredzeHuman Language Technology Center of ExcellenceCenter for Language and Speech ProcessingJohns Hopkins Universitydelip,mcnamee,mdredze@jhu.eduAbstractPrevious research in cross-document en-tity coreference has generally been re-stricted to the offline scenario where theset of documents is provided in advance.As a consequence, the dominant approachis based on greedy agglomerative cluster-ing techniques that utilize pairwise vec-tor comparisons and thus require O(n2)space and time.
In this paper we ex-plore identifying coreferent entity men-tions across documents in high-volumestreaming text, including methods for uti-lizing orthographic and contextual infor-mation.
We test our methods using severalcorpora to quantitatively measure both theefficacy and scalability of our streamingapproach.
We show that our approachscales to at least an order of magnitudelarger data than previous reported meth-ods.1 IntroductionA key capability for successful information ex-traction, topic detection and tracking, and ques-tion answering is the ability to identify equiva-lence classes of entity mentions.
An entity is areal-world person, place, organization, or object,such as the person who serves as the 44th pres-ident of the United States.
An entity mention isa string which refers to such an entity, such as?Barack Hussein Obama?, ?Senator Obama?
or?President Obama?.
The goal of coreference res-olution is to identify and connect all textual entitymentions that refer to the same entity.The first step towards this goal is to identify allreferences within the same document, or withindocument coreference resolution.
A document of-ten has a leading canonical reference to the entity(?Barack Obama?)
followed by additional expres-sions for the same entity (?President Obama.?
)An intra-document coreference system must firstidentify each reference, often relying on namedentity recognition, and then decide if these refer-ences refer to a single individual or multiple enti-ties, creating a coreference chain for each uniqueentity.
Feature representations include surfaceform similarity, lexical context of mentions, po-sition in the document and distance between ref-erences.
A variety of statistical learning meth-ods have been applied to this problem, includinguse of decision trees (Soon et al, 2001; Ng andCardie, 2002), graph partitioning (Nicolae andNicolae, 2006), maximum-entropy models (Luoet al, 2004), and conditional random fields (Choiand Cardie, 2007).Given pre-processed documents, in which enti-ties have been identified and entity mentions havebeen linked into chains, we seek to identify acrossan entire document collection all chains that re-fer to the same entity.
This task is called crossdocument coreference resolution (CDCR).
Sev-eral of the challenges associated with CDCR dif-fer from the within document task.
For example,it is unlikely that the same document will discussJohn Phillips the American football player andJohn Phillips the musician, but it is quite proba-ble that documents discussing each will appear inthe same collection.
Therefore, while matchingentities with the same mention string can workwell for within document coreference, more so-phisticated approaches are necessary for the crossdocument scenario where a one-entity-per-nameassumption is unreasonable.One of the most common approaches to bothwithin document and cross document corefer-ence resolution has been based on agglomerativeclustering, where vectors might be bag-of-wordcontexts (Bagga and Baldwin, 1998; Mann and1050Yarowsky, 2003; Gooi and Allan, 2004; Chenand Martin, 2007).
These algorithms creates aO(n2) dependence in the number of mentions ?for within document ?
and documents ?
for crossdocument.
This is a reasonable limitation forwithin document, since the number of referenceswill certainly be small; we are unlikely to en-counter a document with millions of references.In contrast to the small n encountered within adocument, we fully expect to run a CDCR sys-tem on hundreds of thousands or millions of doc-uments.
Most previous approaches cannot handlecollections of this size.In this work, we present a new method forcross document coreference resolution that scalesto very large corpora.
Our algorithm operates ina streaming setting, in which documents are pro-cessed one at a time and only a single time.
Thiscreates a linear (O(n)) dependence on the num-ber of documents in the collection, allowing usto scale to millions of documents and millionsof unique entities.
Our algorithm uses stream-ing clustering with common coreference similar-ity computations to achieve large scale.
Further-more, our method is designed to support bothname disambiguation and name variation.In the next section, we give a survey of relatedwork.
In Section 3 we detail our streaming setup,giving a description of the streaming algorithmand presenting efficient techniques for represent-ing clusters over streams and for computing simi-larity.
Section 4 describes the data sets on whichwe evaluate our methods and presents results.
Weconclude with a discussion and description of on-going work.2 Related WorkTraditional approaches to cross document coref-erence resolution have first constructed a vectorspace representation derived from local (or global)contexts of entity mentions in documents and thenperformed some form of clustering on these vec-tors.
This is a simple extension of Firth?s distribu-tional hypothesis applied to entities (Firth, 1957).We describe some of the seminal work in this area.Some of the earliest work in CDCR was byBagga and Baldwin (1998).
Key contributionsof their research include: promotion of a set-theoretic evaluation measure, B-CUBED; intro-duction of a data set based on 197 New YorkTimes articles which mention a person namedJohn Smith; and, use of TF/IDF weighted vec-tors and cosine similarity in single-link greedy ag-glomerative clustering.Mann and Yarowsky (2003) extended Baggaand Baldwin?s work and contributed several inno-vations, including: use of biographical attributes(e.g., year of birth, occupation), and evaluation us-ing pseudonames.
Pseudonames are sets of artifi-cially conflated names that are used as an efficientmethod for producing a set of gold-standard dis-ambiguations.1 Mann and Yarowsky used 4 pairsof conflated names in their evaluation.
Their sys-tem did not perform as well on named entities withlittle available biographic information.Gooi and Allan (2004) expanded on the useof pseudonames by semi-automatically creatinga much larger evaluation set, which they calledthe ?Person-X?
corpus.
They relied on automatednamed-entity tagging and domain-focused text re-trieval.
This data consisted of 34,404 documentswhere a single person mention in each documentwas rewritten as ?Person X?.
Besides their novelconstruction of a large-scale resource, they in-vestigated several minor variations in clustering,namely (a) use of Kullback-Leibler divergence asa distance measure, (b) use of 55-word snippetsaround entity mentions (vs. entire documents orextracted sentences), and (c) scoring clusters us-ing average-link instead of single- or complete-link.Finally, in more recent work, Chen and Martin(2007) explore the CDCR task in both English andChinese.
Their work focuses on use of both lo-cal, and document-level noun-phrases as featuresin their vector-space representation.There have been a number of open evaluationsof CDCR systems.
For example, the Web PeopleSearch (WePS) workshops (Artiles et al, 2008)have created a task for disambiguating personalnames from HTML pages.
A set of ambiguousnames is chosen and each is submitted to a popularweb search engine.
The top 100 pages are thenmanually clustered.We discuss several other data1See Sanderson (2000) for use of this technique in wordsense disambiguation.1051sets in Section 4.2All of the papers mentioned above focus on dis-ambiguating personal names.
In contrast, our sys-tem can also handle organizations and locations.Also, as was mentioned earlier, we are commit-ted to a scenario where documents are presentedin sequence and entities must be disambiguatedinstantly, without the benefit of observing the en-tire corpus.
We believe that such a system is bet-ter suited to highly dynamic environments such asdaily news feeds, blogs, and tweets.
Additionally,a streaming system exposes a set of known entityclusters after each document is processed insteadof waiting until the end of the stream.3 ApproachOur cross document coreference resolution sys-tem relies on a streaming clustering algorithmand efficient calculation of similarity scores.
Weassume that we receive a stream of corefer-ence chains, along with entity types, as theyare extracted from documents.
We use SERIF(Ramshaw and Weischedel, 2005), a state of theart document analysis system which performsintra-document coreference resolution.
BBN de-veloped SERIF to address information extractiontasks in the ACE program and it is further de-scribed in Pradhan et al (2007).Each unique entity is represented by an entitycluster c, comprised of entity chains from manydocuments that refer to the same entity.
Givenan entity coreference chain e, we identify the bestknown entity cluster c. If a suitable entity clusteris not found, a new entity cluster is formed.An entity cluster is selected for a given corefer-ence chain using several similarity scores, includ-ing document context, predicted entity type, andorthographic similarity between the entity men-tion and previously discovered references in theentity cluster.
An efficient implementation of thesimilarity score allows the system to identify thetop k most likely mentions without considering allm entity clusters.
The final output of our sys-tem is a collection of entity clusters, each con-taining a list of coreference chains and their doc-uments.
Additionally, due to its streaming nature,2We preferred other data sets to the WePS data in ourevaluation because it is not easily placed in temporal order.the system can be examined at any time to producethis information based on only the documents thathave been processed thus far.In the next sections, we describe both the clus-tering algorithm and efficient computation of theentity similarity scores.3.1 Clustering AlgorithmWe use a streaming clustering algorithm to cre-ate entity clusters as follows.
We observe a setof points from a potentially infinite set X , one ata time, and would like to maintain a fixed numberof clusters while minimizing the maximum clusterradius, defined as the radius of the smallest ballcontaining all points of the cluster.
This setup iswell known in the theory and information retrievalcommunity and is referred to as the dynamic clus-tering problem (Can and Ozkarahan, 1987).Others have attempted to use an incremen-tal clustering approach, such as Gooi and Al-lan (2004) (who eventually prefer a hierarchi-cal clustering approach), and Luo et al (2004),who use a Bell tree approach for incrementallyclustering within document entity mentions.
Ourwork closely follows the Doubling Algorithm ofCharikar et al (1997), which has better perfor-mance guarantees for streaming data.
Streamingclustering means potentially linear performance inthe number of observations since each documentneed only be examined a single time, as opposedto the quadratic cost of agglomerative clustering.3The Doubling Algorithm consists of two stages:update and merge.
Update adds points to existingclusters or creates new clusters while merge com-bines clusters to prevent the clusters from exceed-ing a fixed limit.
New clusters are created accord-ing to a threshold set using development data.
Weselected a threshold of 0.5 since it worked well inpreliminary experiments.
Since the number of en-tities grows with time, we have skipped the mergestep in our initial experiments so as not to limitcluster growth.We use a dynamic caching scheme which backsthe actual clusters in a disk based index, but re-3It is possible to implement hierarchical agglomerativeclustering in O(n logm) time where n is the number ofpoints and m in the number of clusters.
However this is stillsuperlinear and expensive in situations where m continuallyincreases like in streaming coreference resolution.10521 2 3 4 5 6log(Rank)0.50.00.51.01.52.02.53.03.54.0log(Frequency)PERLOCORGFigure 1: Frequency vs. rank for 567k people,136k organizations, and 25k locations in the NewYork Times Annotated Corpus (Sandhaus, 2008).tains basic cluster information in memory (see be-low).
Doing so improves paging performance asobserved in Omiecinski and Scheuermann (1984).Motivated by the Zipfian distribution of named en-tities in news sources (Figure 1), we organize ourcluster store using an LRU policy, which facili-tates easy access to named entities that were ob-served in the recent past.
We obtain additionalperformance gains by hashing the clusters basedon the constituent mention string (details below).This allows us to quickly retrieve a small but re-lated number of clusters, k. It is always the casethat k << m, the current number of clusters.3.2 Candidate Cluster SelectionAs part of any clustering algorithm, each new itemmust be compared against current clusters.
As wesee more documents, the number of unique clus-ters (entities) grows.
Therefore, we need efficientmethods to select candidate clusters.To select the top candidate clusters, we obtainthose that have high orthographic similarity withthe head name mention in the coreference chain e.We compute this similarity using the dice score oneither word unigrams or character skip bigrams.For each entity mention string associated with acluster c, we generate all possible n-grams usingone of the above two policies.
We then index thecluster by each of its n-grams in a hash maintainedin memory.
In addition, we keep the number of n-grams generated for each cluster.When given a new head mention e for a coref-erence chain, we generate all of the n-grams andlook up clusters that contain these n-grams usingthe hash.
We then compute the dice score:dice(e, c) = |{ngram(e)} ?
{ngram(c)}||{ngram(e)} ?
{ngram(c)}| ,where {ngram(e)} are the set of n-grams in entitymention e and {ngram(c)} are the set of n-gramsfor all entity mentions in cluster c. Note that wecan calculate the numerator (the intersection) bylooking up the n-grams of e in the hash and count-ing matches with c. The denominator is equivalentto the number of n-grams unique to e and to c plusthe number that are shared.
The number that areshared is the intersection.
The number unique toe is the total number of n-grams in e minus the in-tersection.
The final term, the number unique to c,is computed by taking the total number of n-gramsin c (a single integer stored in memory) minus theintersection.Through this strategy, we can select only thoseclusters that have the highest orthographic simi-larity to e without requiring the cluster contents,which may not be stored in memory.
In our exper-iments, we evaluate settings where we select allcandidates with non-zero score and a pruned set ofthe top k dice score candidates.
We also includein the n-gram list known aliases to facilitate or-thographically dissimilar, but reasonable matches(e.g., IBM or ?Big Blue?
for ?International Busi-ness Machines, Inc.?
).4For further efficiency, we keep separate cachesfor each named entity type.5 We then select theappropriate cache based on the automatically de-termined type of the named entity provided by thenamed entity tagger, which also prevents spuriousmatches of non-matching entity types.3.3 Similarity MetricAfter filtering by orthographic information toquickly obtain a small set of candidate clusters,a full similarity score is computed for the current4We generated alias lists for entities from Freebase.5Persons (PER), organizations (ORG), and locations(LOC).1053entity coreference chain and each retrieved can-didate cluster.
These computations require infor-mation about each cluster, so the cluster?s suffi-cient statistics are loaded using the LRU cache de-scribed above.We define several similarity metrics betweencoreference chains and clusters to deal with bothname variation and disambiguation.
For namevariation, we define an orthographic similaritymetric to match similar entity mention strings.
Asbefore, we use word unigrams and character skipbigrams.
For each of these methods, we computea similarity score as dice(e, c) and select the high-est scoring cluster.To address name disambiguation, we use twotypes of context from the document.
First, we uselexical features represented as TF/IDF weightedvectors.
Second, we consider topic features, inwhich each word in a document is replaced withthe topic inferred from a topic model.
This yieldsa distribution over topics for a given document.We use an LDA (Blei et al, 2003) model trainedon the New York Times Annotated Corpus (Sand-haus, 2008).
We note that LDA can be computedover streams (Yao et al, 2009).To compare context vectors we use cosine sim-ilarity, where the cluster vector is the average ofall document vectors assigned to the cluster.
Notethat the filtering step in Section 3.2 returns onlythose candidates with some orthographic similar-ity with the coreference chain, so a similarity met-ric that uses context only is still restricted to ortho-graphically similar entities.Finally, we consider a combination of ortho-graphic and context similarity as a linear combi-nation of the two metrics as:score(e, c) = ?
dice(e, c) + (1?
?
)cosine(e, c) .We set ?
= 0.8 based on initial experiments.4 EvaluationWe used several corpora to evaluate our meth-ods, including two data sets commonly used in thecoreference community.
We also created a newtest set using artificially conflated names.
And fi-nally to test scalability, we ran our algorithm overa large text collection that, while it did not haveAttribute smith nytac ace08 kbp09Total Documents 197 1.85M 10k 1.2MAnnotated Docs 197 19,360 415 **Annotated Entities 35 200 3,943 **Table 1: Data sets used in our experiments.
Forthe kbp09 data we did not have annotations.ground truth entity clusters, was useful for com-puting other performance statistics.
Properties foreach data set are given in Table 1.4.1 John Smith corpusBagga and Baldwin (1998) evaluated their disam-biguation system on a set of 197 articles from theNew York Times that mention a person named?John Smith?.
This data exhibits no name variantsand is strictly a disambiguation task.
We includethis data (smith) to allow comparison to previouswork.4.2 NYTAC Pseudo-name corpusTo study the effects of word sense ambiguityand disambiguation several researchers have ar-tificially conflated dissimilar words together andthen attempted to disambiguate them (Sanderson,2000).
The obvious advantage is cheaply obtainedground truth for disambiguation.The same trick has also been employed in per-son name disambiguation (Mann and Yarowsky,2003; Gooi and Allan, 2004).
We adopt the samemethod on a somewhat larger scale using annota-tions from the New York Times Annotated Corpus(NYTAC) (Sandhaus, 2008), which annotates doc-uments based on whether or not they mention anentity.
The NYTAC data contains documents from20 years of the New York Times and contains richmetadata and document-level annotations that in-dicate when an entity is mentioned in the docu-ment using a standard lexicon of entities.
(Notethat mention strings are not tagged.)
Using theseannotations we created a set of 100 pairs of con-flated person names.The names were selected to be medium fre-quency (i.e., occurring in between 50 and 200 ar-ticles) and each pair matches in gender.
The first50 pairs are for names that are topically similar,for example, Tim Robbins and Tom Hanks (bothactors); Barbara Boxer and Olympia Snowe (both1054smith nytac ace08Approach P R F P R F P R FBaseline 1.000 0.178 0.302 1.000 0.010 0.020 1.000 0.569 0.725ExactMatch 0.233 1.000 0.377 0.563 0.897 0.692 0.977 0.697 0.814Ortho 0.603 0.629 0.616 0.611 0.784 0.687 0.975 0.694 0.811BoW 0.956 0.367 0.530 0.930 0.249 0.349 0.989 0.589 0.738Topic 0.847 0.592 0.697 0.815 0.244 0.363 0.983 0.605 0.750Ortho+BoW 0.603 0.634 0.618 0.801 0.601 0.686 0.976 0.691 0.809Ortho+Topic 0.603 0.634 0.618 0.800 0.591 0.680 0.975 0.704 0.819Table 2: Best B3 performance on the smith, nytac, and ace08 test sets.US politicians).
We imagined that this would bea more challenging subset because of presumedlexical overlap.
The second set of 50 name pairswere arbitrarily conflated.
We sub-selected thedata to ensure that no two entities in our collec-tion co-occur in the same document and this leftus with 19,360 documents for which ground-truthwas known.
In each document we rewrote theconflated name mentions using a single gender-neutral name; any middle initials or names werediscarded.4.3 ACE 2008 corpusThe NIST ACE 2008 (ace08) evaluation studiedseveral related technologies for information ex-traction, including named-entity recognition, re-lation extraction, and cross-document coreferencefor person names in both English and Arabic.
Ap-proximately 10,000 documents from several gen-res (predominantly newswire) were given to par-ticipants, who were expected to cluster person andorganization entities across the entire collection.However, only a selected set of about 400 docu-ments were annotated and used to evaluate sys-tem performance.
Baron and Freedman (2008)describe their work in this evaluation, which in-cluded a separate task for within-document coref-erence.4.4 TAC-KBP 2009 corpusThe NIST TAC 2009 Knowledge Base Popula-tion track (kbp09) (McNamee and Dang, 2009)conducted an evaluation of a system?s ability tolink entity mentions to corresponding Wikipedia-derived knowledge base nodes.
The TAC-KBPtask focused on ambiguous person, organization,and geo-political entities mentioned in newswire,and required systems to cope with name variation(e.g., ?Osama Bin Laden?
/ ?Usama Bin Laden?or ?Mark Twain?
/ ?Samuel Clemens?)
as well asname disambiguation.
Furthermore, the task re-quired detection of when no appropriate KB entryexists, which is a departure from the conventionaldisambiguation problem.
The collection containsover 1.2 million documents, primarily newswire.Wikipedia was used as a surrogate knowledgebase, and it has been used in several previous stud-ies (e.g., Cucerzan (2007)).
This task is closely re-lated to CDCR, as mentions that are aligned to thesame knowledge base entry create a coreferencecluster.
However, there are no actual CDCR anno-tations for this corpus, though we used it nonethe-les as a benchmark corpus to evaluate speed andto demonstrate scalability.5 Discussion5.1 AccuracyIn Table 2 we report cross document coreferenceresolution performance for a variety of experi-mental conditions using the B3 method, whichincludes precision, recall, and calculated F?=1values.
For each of the three evaluation corpora(smith, nytac, and ace08) we report values for twobaseline methods and for similarity metrics us-ing different types of features.
The first baseline,called Baseline, places each coreference chain inits own cluster while the second baseline, calledExactMatch, merges all mentions that match ex-actly orthographically into the same cluster.Use of name similarity scores as features (in ad-dition to their use for candidate cluster selection)is indicated by rows labeled Ortho.
Use of lexi-cal features is indicated by BoW and use of topicmodel features by Topic.Using topic models as features was more help-ful than lexical contexts on the smith corpus.1055#coref chains Baseline ExactMatch Ortho BOW Topics Ortho+BOW Ortho+Topics1K10K20K30K40K50K60K70K80K90K100K110K120K130K140K1000 702 699 925 857 699 69710000 4563 4530 7964 7956 4514 451820000 8234 8202 15691 15073 8159 816330000 11745 11682 23138 21878 11608 1161140000 15041 14964 30900 28500 14869 1486350000 18110 18016 38248 34758 17910 1790360000 20450 20377 44735 40081 20241 2022870000 22845 22780 51190 45722 22615 2260380000 25062 25026 57440 51104 24832 2481890000 27389 27358 64140 56581 27145 27126100000 29797 29782 71034 62228 29546 29511110000 32147 32139 77705 67853 31882 31840120000 34567 34589 84309 73397 34284 34235130000 36817 36874 90465 78676 36543 36486140000 38826 38901 96225 83525 38539 38482037500750001125001500001K 20K 40K 60K 80K 100K 120K 140K#ofclustersproduced# of entity chains seenBaseline ExactMatchOrtho BOWTopics Ortho+BOWOrtho+TopicsFigure 2: Number of clusters produced vs. num-ber of entity chains observed in the stream.
Num-ber of entity chains is proportional to the numberof documents.When used alone topic beats BoW, but in com-bination with the ortho features performance isequivalent.
For both nytac and ace08 heavy re-liance on orthographic similarity proved hard tobeat.
On the ace08 corpus Baron and Freedman(2008) report B3 F-scores of 83.2 for persons and67.8 for organizations, and our streaming resultsappear to be comparable to their offline method.The cluster growth induced by the various mea-sures can be seen in Figure 2.
The two base-line methods, Baseline and ExactMatch, providebounds on the cluster growth with all other meth-ods falling in between.5.2 Hashing Strategies for CandidateSelectionTable 3 containsB3 F-scores when different hash-ing strategies are employed for candidate selec-tion.
The trend appears to be that stricter match-ing outperforms fuzzier matching; full mentionstended to beat words, which beat use of the char-acter bigrams.
This agrees with the results de-scribed in the previous section, which show heavyreliance on orthographic similarity.5.3 Timing ResultsFigure 3 shows how processing time increaseswith the number of entities observed in the ace08#chains 1 5 10 201000 2 0 0 12000 2 0 0 13000 2 0 0 14000 2 0 0 15000 2 2 0 46000 2 2 0 47000 2 2 0 48000 2 2 0 49000 2 2 0 410000 2 2 0 411000 2 2 0 412000 2 2 0 513000 2 2 0 614000 2 2 0 615000 2 2 0 616000 2 2 0 617000 2 2 0 618000 2 2 0 619000 2 2 1 720000 2 2 1 721000 2 2 2 722000 2 2 2 723000 2 2 3 724000 2 2 3 725000 2 2 3 726000 2 2 3 727000 2 2 4 828000 2 2 4 929000 2 2 5 1030000 2 2 8 1131000 2 2 8 1232000 2 2 8 1333000 2 2 8 1434000 2 2 8 1535000 2 2 8 1636000 2 2 8 1737000 2 2 8 1838000 2 2 9 1939000 2 2 9 2040000 2 2 9 2141000 2 2 10 2242000 2 2 10 2343000 2 2 11 2444000 2 2 12 2545000 2 2 12 2646000 2 2 13 2747000 2 3 14 2848000 2 3 15 2949000 2 4 16 3050000 2 5 17 3251000 2 6 18 3452000 2 7 19 3653000 2 7 20 3854000 2 7 21 4055000 2 8 22 4156000 2 8 23 4357000 2 9 24 4558000 2 10 25 4759000 2 10 26 4860000 2 11 27 5061000 2 12 28 5262000 2 13 29 5463000 2 13 30 5664000 2 14 31 5865000 2 15 32 6066000 3 16 33 6267000 3 17 34 6368000 3 18 35 6569000 3 19 36 6770000 3 19 37 6871000 4 20 38 7072000 4 21 39 7273000 4 22 40 7374000 5 23 41 7575000 6 24 42 7776000 6 25 43 7977000 6 26 44 8178000 6 27 45 8379000 7 28 46 8580000 7 29 47 8781000 8 30 48 8982000 8 31 49 9183000 9 32 50 9384000 9 33 51 9585000 9 34 52 9786000 9 35 53 9987000 10 36 54 10188000 10 37 55 10289000 10 38 56 10490000 11 39 57 10691000 13 40 58 10892000 14 41 59 11193000 15 42 60 11394000 15 43 61 11595000 16 44 62 11796000 17 45 63 11997000 17 46 64 12198000 18 47 65 12399000 19 48 66 125100000 20 49 67 127101000 21 50 68 129102000 21 51 69 131103000 22 52 70 133104000 23 53 71 136105000 24 54 72 138106000 25 55 73 140107000 25 56 74 142108000 26 57 75 144109000 27 58 76 146110000 28 59 77 148111000 28 60 78 150112000 29 61 79 153113000 30 62 80 156114000 31 63 81 158115000 32 64 83 161116000 33 66 84 163117000 34 67 85 165118000 35 68 86 167119000 36 69 87 169120000 37 70 88 171121000 38 71 90 174122000 39 72 92 177123000 40 73 93 179124000 41 74 94 181125000 42 75 95 183126000 43 76 96 186127000 44 77 99 188128000 45 78 100 191129000 46 79 101 196130000 47 80 102 198131000 48 81 103 201132000 49 82 104 204133000 50 83 105 207134000 51 84 106 210135000 52 85 107 214136000 53 86 109 217137000 54 87 110 219138000 55 89 112 222139000 56 90 113 225140000 57 91 115 228141000 58 92 117 231142000 59 93 118 234143000 62 94 120 237143442 62 94 121 23811010010001000 25000 49000 73000 97000 121000Time(secs)# of chains processed1 5 10 20Figure 3: Elapsed processing time as a function ofbounding the number of candidate clusters consid-ered for an entity.
When fewer candidates are con-sidered, clustering decisions can be made muchfaster.document stream.
We experimented with using anupper bound on the number of candidate clustersto consider for an entity.Figure 4 compares the efficiency of using threedifferent methods for candidate cluster identifica-tion.
The most restrictive hashing strategy, usingexact mention strings, is the most efficient, fol-lowed by the use of words, then the use of charac-ter skip bigrams.
This makes intuitive sense ?
thestrictest matching reduces the number of candi-date clusters that have to be considered when pro-cessing an entity.6The ace08 corpus contained over 10,000 doc-uments and is one of the largest CDCR test sets.In Figure 5 we show how processing time growswhen processing the kbp09 corpus.
Doubling thenumber of entities processed increases the runtimeby about a factor of 5.
The curve is not lineardue to the increasing number of entity cluster?sthat must be considered.
Future work will exam-ine how to keep the number of clusters consideredconstant over time, such as ignoring older entities.6 ConclusionWe have presented a new streaming cross doc-ument coreference resolution system.
Our ap-proach is substantially faster than previous sys-6In the limit, if names were unique, hashing on stringswould completely solve the CDCR problem and processingan entity would be O(1)1056smith nytac ace08Approach bigrams words mention bigrams words mention bigrams words mentionOrtho 0.382 0.553 0.616 0.120 0.695 0.687 0.540 0.797 0.811BoW 0.480 0.530 0.467 0.344 0.339 0.349 0.551 0.700 0.738Topic 0.697 0.661 0.579 0.071 0.620 0.363 0.544 0.685 0.750Ortho+BoW 0.389 0.554 0.618 0.340 0.691 0.686 0.519 0.783 0.809Ortho+Topic 0.398 0.555 0.618 0.120 0.477 0.680 0.520 0.776 0.819Table 3: B3 F-scores using different hashing strategies for candidate selection.
Name/cluster similaritycould be based on character skip bigrams, words appear in names, or exact matching of mention.#chains mention string word bigram1000 2 1 12000 2 2 53000 2 2 64000 2 4 75000 3 5 96000 4 6 117000 5 6 138000 5 7 159000 6 7 1710000 6 7 1911000 7 9 2112000 7 10 2313000 7 11 2514000 9 13 2715000 9 14 2916000 10 15 3117000 10 16 3318000 11 17 3619000 12 18 3920000 12 19 4221000 13 21 4422000 13 23 4723000 13 24 5024000 14 26 5325000 15 28 5626000 16 30 6027000 16 32 6428000 17 34 6829000 17 36 7130000 18 39 7531000 19 40 7932000 21 43 8333000 22 45 8834000 23 47 9235000 24 49 9636000 26 51 10037000 26 53 10438000 27 55 10839000 27 57 11240000 28 59 11741000 29 63 12142000 30 66 12543000 31 70 12944000 33 73 13345000 34 77 13746000 35 80 14247000 36 84 14648000 36 87 15049000 38 90 15450000 39 94 15851000 40 98 16252000 41 101 16753000 42 104 17154000 44 107 17555000 45 111 17956000 46 115 18357000 48 119 18758000 49 122 19259000 49 126 19660000 50 130 20061000 51 134 20462000 52 139 20863000 53 143 21264000 56 146 21765000 57 150 22266000 59 155 22767000 60 158 23268000 62 163 23769000 62 167 24270000 63 170 24771000 65 173 25272000 66 178 25773000 67 183 26274000 67 186 26775000 68 191 27376000 69 194 27777000 70 198 28278000 70 202 28779000 71 207 29280000 72 211 29781000 73 215 30282000 73 219 30783000 74 224 31284000 74 229 31785000 75 233 32286000 76 238 32887000 77 243 33388000 77 246 33889000 78 250 34390000 78 254 34891000 79 258 35392000 80 262 35893000 80 268 36394000 81 273 36895000 82 277 37396000 82 281 37897000 83 284 38498000 84 288 38999000 84 293 394100000 85 297 400101000 85 300 406102000 85 303 413103000 85 307 419104000 85 310 425105000 86 314 431106000 87 319 438107000 89 325 444108000 90 330 450109000 91 334 456110000 91 339 463111000 92 343 469112000 92 348 476113000 93 352 482114000 94 357 489115000 95 362 495116000 95 366 501117000 96 369 507118000 96 373 513119000 96 377 520120000 97 382 526121000 97 387 532122000 100 392 539123000 100 395 546124000 101 399 552125000 101 403 558126000 102 408 564127000 102 412 571128000 103 417 577129000 104 421 583130000 104 425 589131000 105 430 596132000 106 435 602133000 108 441 609134000 109 446 615135000 111 452 622136000 112 457 628137000 113 461 634138000 113 467 640139000 114 472 648140000 115 479 655141000 116 484 662142000 117 489 669143000 118 494 676143442 118 497 67911010010001000 25000 49000 73000 97000 121000Time(secs)# of coref chains processedmention string word bigramFigure 4: Comparison of three hashing strategiesfor identifying candidate clusters for a given en-tity.
The more restrictive strategies lead to fasterprocessing as fewer candidates are considered.tems, and our experiments have demonstratedscalability to an order of magnitude larger datathan previously published evaluations.
Despite itsspeed and simplicity, we still obtain competitiveresults on a variety of data sets as compared withbatch systems.
In future work, we plan to investi-gate additional similarity metrics that can be com-puted efficiently, as well as experiments on webscale corpora.ReferencesArtiles, Javier, Satoshi Sekine, and Julio Gonzalo.2008.
Web people search: results of the first evalua-tion and the plan for the second.
In World Wide Web(WWW).Bagga, Amit and Breck Baldwin.
1998.
Entity-based cross-document coreferencing using the vec-tor space model.
In Conference on ComputationalLinguistics (COLING).Baron, Alex and Marjorie Freedman.
2008.
Who#coref chains processed Time (secs)1K 1.5100K 10200K 40400K 120600K 700900K 9201.1M 12001101001000100001K 100K 200K 400K 600K 900K 1.1MTime(secs)# of coref chains processedFigure 5: The number of coreference chains pro-cessed over time in the kbp09 corpus.
The pro-cessing of over 1 million coreference chains is atleast an order of magnitude larger than previoussystems reported.is Who and What is What: Experiments in cross-document co-reference.
In Empirical Methods inNatural Language Processing (EMNLP).Blei, D.M., A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine LearningResearch (JMLR), 3:993?1022.Can, F. and E. Ozkarahan.
1987.
A dynamic clus-ter maintenance system for information retrieval.
InConference on Research and Development in Infor-mation Retrieval (SIGIR).Charikar, Moses, Chandra Chekuri, Toma?s Feder, andRajeev Motwani.
1997.
Incremental clustering anddynamic information retrieval.
In ACM Symposiumon Theory of Computing (STOC).Chen, Ying and James Martin.
2007.
Towards ro-bust unsupervised personal name disambiguation.In Empirical Methods in Natural Language Pro-cessing (EMNLP).Choi, Y. and C. Cardie.
2007.
Structured local trainingand biased potential functions for conditional ran-dom fields with application to coreference resolu-tion.
In North American Chapter of the Association1057for Computational Linguistics (NAACL), pages 65?72.Cucerzan, Silviu.
2007.
Large-scale named entitydisambiguation based on wikipedia data.
In Em-pirical Methods in Natural Language Processing(EMNLP), pages 708?716.Firth, J.R. 1957.
A synopsis of linguistic theory 1930-1955.
In Studies in Linguistic Analysis, pages 1?32.Oxford: Philological Society.Gooi, Chung Heong and James Allan.
2004.
Cross-document coreference on a large scale corpus.
InNorth American Chapter of the Association forComputational Linguistics (NAACL).Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous corefer-ence resolution algorithm based on the bell tree.
InAssociation for Computational Linguistics (ACL).Mann, Gideon S. and David Yarowsky.
2003.
Unsu-pervised personal name disambiguation.
In Confer-ence on Natural Language Learning (CONLL).McNamee, Paul and Hoa Dang.
2009.
Overview ofthe TAC 2009 knowledge base population track.
InText Analysis Conference (TAC).Ng, V. and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Asso-ciation for Computational Linguistics (ACL), pages104?111.Nicolae, C. and G. Nicolae.
2006.
Bestcut: Agraph algorithm for coreference resolution.
In Em-pirical Methods in Natural Language Processing(EMNLP), pages 275?283.
Association for Compu-tational Linguistics.Omiecinski, Edward and Peter Scheuermann.
1984.
Aglobal approach to record clustering and file reorga-nization.
In Conference on Research and Develop-ment in Information Retrieval (SIGIR).Pradhan, S.S., L. Ramshaw, R. Weischedel,J.
MacBride, and L. Micciulla.
2007.
Unre-stricted coreference: Identifying entities and eventsin ontonotes.
In International Conference onSemantic Computing (ICSC).Ramshaw, L. and R. Weischedel.
2005.
Informationextraction.
In IEEE ICASSP.Sanderson, Mark.
2000.
Retrieving with good sense.Information Retrieval, 2(1):45?65.Sandhaus, Evan.
2008.
The new york times annotatedcorpus.
Linguistic Data Consortium, Philadelphia.Soon, Wee Meng, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics.Yao, L., D. Mimno, and A. McCallum.
2009.
Effi-cient methods for topic model inference on stream-ing document collections.
In Knowledge discoveryand data mining (KDD).1058
