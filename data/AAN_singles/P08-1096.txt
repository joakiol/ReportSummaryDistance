Proceedings of ACL-08: HLT, pages 843?851,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAn Entity-Mention Model for Coreference Resolutionwith Inductive Logic ProgrammingXiaofeng Yang1 Jian Su1 Jun Lang2Chew Lim Tan3 Ting Liu2 Sheng Li21Institute for Infocomm Research{xiaofengy,sujian}@i2r.a-star.edu.sg2Harbin Institute of Technology{bill lang,tliu}@ir.hit.edu.cnlisheng@hit.edu.cn3National University of Singapore,tancl@comp.nus.edu.sgAbstractThe traditional mention-pair model for coref-erence resolution cannot capture informationbeyond mention pairs for both learning andtesting.
To deal with this problem, we presentan expressive entity-mention model that per-forms coreference resolution at an entity level.The model adopts the Inductive Logic Pro-gramming (ILP) algorithm, which provides arelational way to organize different knowledgeof entities and mentions.
The solution canexplicitly express relations between an entityand the contained mentions, and automaticallylearn first-order rules important for corefer-ence decision.
The evaluation on the ACE dataset shows that the ILP based entity-mentionmodel is effective for the coreference resolu-tion task.1 IntroductionCoreference resolution is the process of linking mul-tiple mentions that refer to the same entity.
Mostof previous work adopts the mention-pair model,which recasts coreference resolution to a binaryclassification problem of determining whether or nottwo mentions in a document are co-referring (e.g.Aone and Bennett (1995); McCarthy and Lehnert(1995); Soon et al (2001); Ng and Cardie (2002)).Although having achieved reasonable success, themention-pair model has a limitation that informa-tion beyond mention pairs is ignored for training andtesting.
As an individual mention usually lacks ad-equate descriptive information of the referred entity,it is often difficult to judge whether or not two men-tions are talking about the same entity simply fromthe pair alone.An alternative learning model that can overcomethis problem performs coreference resolution basedon entity-mention pairs (Luo et al, 2004; Yang etal., 2004b).
Compared with the traditional mention-pair counterpart, the entity-mention model aims tomake coreference decision at an entity level.
Classi-fication is done to determine whether a mention is areferent of a partially found entity.
A mention to beresolved (called active mention henceforth) is linkedto an appropriate entity chain (if any), based on clas-sification results.One problem that arises with the entity-mentionmodel is how to represent the knowledge related toan entity.
In a document, an entity may have morethan one mention.
It is impractical to enumerate allthe mentions in an entity and record their informa-tion in a single feature vector, as it would make thefeature space too large.
Even worse, the number ofmentions in an entity is not fixed, which would re-sult in variant-length feature vectors and make trou-ble for normal machine learning algorithms.
A solu-tion seen in previous work (Luo et al, 2004; Culottaet al, 2007) is to design a set of first-order featuressummarizing the information of the mentions in anentity, for example, ?whether the entity has any men-tion that is a name alias of the active mention??
or?whether most of the mentions in the entity have thesame head word as the active mention??
These fea-tures, nevertheless, are designed in an ad-hoc man-ner and lack the capability of describing each indi-vidual mention in an entity.In this paper, we present a more expressive entity-843mention model for coreference resolution.
Themodel employs Inductive Logic Programming (ILP)to represent the relational knowledge of an activemention, an entity, and the mentions in the entity.
Ontop of this, a set of first-order rules is automaticallylearned, which can capture the information of eachindividual mention in an entity, as well as the globalinformation of the entity, to make coreference deci-sion.
Hence, our model has a more powerful repre-sentation capability than the traditional mention-pairor entity-mention model.
And our experimental re-sults on the ACE data set shows the model is effec-tive for coreference resolution.2 Related WorkThere are plenty of learning-based coreference reso-lution systems that employ the mention-pair model.A typical one of them is presented by Soon et al(2001).
In the system, a training or testing instanceis formed for two mentions in question, with a fea-ture vector describing their properties and relation-ships.
At a testing time, an active mention is checkedagainst all its preceding mentions, and is linked withthe closest one that is classified as positive.
Thework is further enhanced by Ng and Cardie (2002)by expanding the feature set and adopting a ?best-first?
linking strategy.Recent years have seen some work on the entity-mention model.
Luo et al (2004) propose a systemthat performs coreference resolution by doing searchin a large space of entities.
They train a classifier thatcan determine the likelihood that an active mentionshould belong to an entity.
The entity-level featuresare calculated with an ?Any-X?
strategy: an entity-mention pair would be assigned a feature X, if anymention in the entity has the feature X with the ac-tive mention.Culotta et al (2007) present a system which usesan online learning approach to train a classifier tojudge whether two entities are coreferential or not.The features describing the relationships betweentwo entities are obtained based on the informationof every possible pair of mentions from the two en-tities.
Different from (Luo et al, 2004), the entity-level features are computed using a ?Most-X?
strat-egy, that is, two given entities would have a featureX, if most of the mention pairs from the two entitieshave the feature X.Yang et al (2004b) suggest an entity-based coref-erence resolution system.
The model adopted in thesystem is similar to the mention-pair model, exceptthat the entity information (e.g., the global num-ber/gender agreement) is considered as additionalfeatures of a mention in the entity.McCallum and Wellner (2003) propose severalgraphical models for coreference analysis.
Thesemodels aim to overcome the limitation that pair-wise coreference decisions are made independentlyof each other.
The simplest model conditions coref-erence on mention pairs, but enforces dependencyby calculating the distance of a node to a partition(i.e., the probability that an active mention belongsto an entity) based on the sum of its distances to allthe nodes in the partition (i.e., the sum of the prob-ability of the active mention co-referring with thementions in the entity).Inductive Logic Programming (ILP) has been ap-plied to some natural language processing tasks, in-cluding parsing (Mooney, 1997), POS disambigua-tion (Cussens, 1996), lexicon construction (Claveauet al, 2003), WSD (Specia et al, 2007), and so on.However, to our knowledge, our work is the first ef-fort to adopt this technique for the coreference reso-lution task.3 Modelling Coreference ResolutionSuppose we have a document containing n mentions{mj : 1 < j < n}, in which mj is the jth mentionoccurring in the document.
Let ei be the ith entity inthe document.
We defineP (L|ei,mj), (1)the probability that a mention belongs to an entity.Here the random variable L takes a binary value andis 1 if mj is a mention of ei.By assuming that mentions occurring after mjhave no influence on the decision of linking mj toan entity, we can approximate (1) as:P (L|ei,mj)?
P (L|{mk ?
ei, 1 ?
k ?
j ?
1},mj) (2)?
maxmk?ei,1?k?j?1P (L|mk,mj) (3)(3) further assumes that an entity-mention scorecan be computed by using the maximum mention-844[ Microsoft Corp. ]11 announced [ [ its ]12 new CEO ]23[ yesterday ]34.
[ The company ]15 said [ he ]26 will .
.
.Table 1: A sample textpair score.
Both (2) and (1) can be approximatedwith a machine learning method, leading to the tra-ditional mention-pair model and the entity-mentionmodel for coreference resolution, respectively.The two models will be described in the next sub-sections, with the sample text in Table 1 used fordemonstration.
In the table, a mention m is high-lighted as [ m ]eidmid, where mid and eid are the IDsfor the mention and the entity to which it belongs,respectively.
Three entity chains can be found in thetext, that is,e1 : Microsoft Corp. - its - The companye2 : its new CEO - hee3 : yesterday3.1 Mention-Pair ModelAs a baseline, we first describe a learning frameworkwith the mention-pair model as adopted in the workby Soon et al (2001) and Ng and Cardie (2002).In the learning framework, a training or testinginstance has the form of i{mk, mj}, in which mj isan active mention and mk is a preceding mention.An instance is associated with a vector of features,which is used to describe the properties of the twomentions as well as their relationships.
Table 2 sum-marizes the features used in our study.For training, given each encountered anaphoricmention mj in a document, one single positive train-ing instance is created for mj and its closest an-tecedent.
And a group of negative training in-stances is created for every intervening mentionsbetween mj and the antecedent.
Consider the ex-ample text in Table 1, for the pronoun ?he?, threeinstances are generated: i(?The company?,?he?),i(?yesterday?,?he?
), and i(?its new CEO?,?he?
).Among them, the first two are labelled as negativewhile the last one is labelled as positive.Based on the training instances, a binary classifiercan be generated using any discriminative learningalgorithm.
During resolution, an input document isprocessed from the first mention to the last.
For eachencountered mention mj , a test instance is formedfor each preceding mention, mk.
This instance ispresented to the classifier to determine the corefer-ence relationship.
mj is linked with the mention thatis classified as positive (if any) with the highest con-fidence value.3.2 Entity-Mention ModelThe mention-based solution has a limitation that in-formation beyond a mention pair cannot be captured.As an individual mention usually lacks complete de-scription about the referred entity, the coreferencerelationship between two mentions may be not clear,which would affect classifier learning.
Considera document with three coreferential mentions ?Mr.Powell?, ?he?, and ?Powell?, appearing in that or-der.
The positive training instance i(?he?, ?Powell?
)is not informative, as the pronoun ?he?
itself dis-closes nothing but the gender.
However, if the wholeentity is considered instead of only one mention, wecan know that ?he?
refers to a male person named?Powell?.
And consequently, the coreference rela-tionships between the mentions would become moreobvious.The mention-pair model would also cause errorsat a testing time.
Suppose we have three mentions?Mr.
Powell?, ?Powell?, and ?she?
in a document.The model tends to link ?she?
with ?Powell?
be-cause of their proximity.
This error can be avoided,if we know ?Powell?
belongs to the entity startingwith ?Mr.
Powell?, and therefore refers to a maleperson and cannot co-refer with ?she?.The entity-mention model based on Eq.
(2) per-forms coreference resolution at an entity-level.
Forsimplicity, the framework considered for the entity-mention model adopts similar training and testingprocedures as for the mention-pair model.
Specif-ically, a training or testing instance has the form ofi{ei, mj}, in which mj is an active mention and eiis a partial entity found before mj .
During train-ing, given each anaphoric mention mj , one singlepositive training instance is created for the entity towhich mj belongs.
And a group of negative train-ing instances is created for every partial entity whoselast mention occurs between mj and the closest an-tecedent of mj .See the sample in Table 1 again.
For the pronoun?he?, the following three instances are generated for845Features describing an active mention, mjdefNP mj 1 if mj is a definite description; else 0indefNP mj 1 if mj is an indefinite NP; else 0nameNP mj 1 if mj is a named-entity; else 0pron mj 1 if mj is a pronoun; else 0bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0Features describing a previous mention, mkdefNP mk 1 if mk is a definite description; else 0indefNP mk 1 if mk is an indefinite NP; else 0nameNP mk 1 if mk is a named-entity; else 0pron mk 1 if mk is a pronoun; else 0bareNP mk 1 if mk is a bare NP; else 0subject mk 1 if mk is an NP in a subject position; else 0Features describing the relationships between mk and mjsentDist sentence distance between two mentionsnumAgree 1 if two mentions match in the number agreement; else 0genderAgree 1 if two mentions match in the gender agreement; else 0parallelStruct 1 if two mentions have an identical collocation pattern; else 0semAgree 1 if two mentions have the same semantic category; else 0nameAlias 1 if two mentions are an alias of the other; else 0apposition 1 if two mentions are in an appositive structure; else 0predicative 1 if two mentions are in a predicative structure; else 0strMatch Head 1 if two mentions have the same head string; else 0strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0strMatch Contain 1 if the string of mj is fully contained in that of mk ; else 0Table 2: Feature set for coreference resolutionentity e1, e3 and e2:i({?Microsoft Corp.?, ?its?, ?The company?},?he?),i({?yesterday?},?he?
),i({?its new CEO?},?he?
).Among them, the first two are labelled as negative,while the last one is positive.The resolution is done using a greedy clusteringstrategy.
Given a test document, the mentions areprocessed one by one.
For each encountered men-tion mj , a test instance is formed for each partial en-tity found so far, ei.
This instance is presented to theclassifier.
mj is appended to the entity that is classi-fied as positive (if any) with the highest confidencevalue.
If no positive entity exists, the active mentionis deemed as non-anaphoric and forms a new entity.The process continues until the last mention of thedocument is reached.One potential problem with the entity-mentionmodel is how to represent the entity-level knowl-edge.
As an entity may contain more than one candi-date and the number is not fixed, it is impractical toenumerate all the mentions in an entity and put theirproperties into a single feature vector.
As a base-line, we follow the solution proposed in (Luo et al,2004) to design a set of first-order features.
The fea-tures are similar to those for the mention-pair modelas shown in Table 2, but their values are calculatedat an entity level.
Specifically, the lexical and gram-matical features are computed by testing any men-tion1 in the entity against the active mention, for ex-1Linguistically, pronouns usually have the most direct coref-ample, the feature nameAlias is assigned value 1 ifat least one mention in the entity is a name alias ofthe active mention.
The distance feature (i.e., sent-Dist) is the minimum distance between the mentionsin the entity and the active mention.The above entity-level features are designed in anad-hoc way.
They cannot capture the detailed infor-mation of each individual mention in an entity.
Inthe next section, we will present a more expressiveentity-mention model by using ILP.4 Entity-mention Model with ILP4.1 MotivationThe entity-mention model based on Eq.
(2) re-quires relational knowledge that involves informa-tion of an active mention (mj), an entity (ei), andthe mentions in the entity ({mk ?
ei}).
How-ever, normal machine learning algorithms work onattribute-value vectors, which only allows the repre-sentation of atomic proposition.
To learn from rela-tional knowledge, we need an algorithm that can ex-press first-order logic.
This requirement motivatesour use of Inductive Logic Programming (ILP), alearning algorithm capable of inferring logic pro-grams.
The relational nature of ILP makes it pos-sible to explicitly represent relations between an en-tity and its mentions, and thus provides a powerfulexpressiveness for the coreference resolution task.erence relationship with antecedents in a local discourse.Hence, if an active mention is a pronoun, we only consider thementions in its previous two sentences for feature computation.846ILP uses logic programming as a uniform repre-sentation for examples, background knowledge andhypotheses.
Given a set of positive and negative ex-ample E = E+ ?
E?, and a set of backgroundknowledge K of the domain, ILP tries to induce aset of hypotheses h that covers most of E+ with noE?, i.e., K ?
h |= E+ and K ?
h 6|= E?.In our study, we choose ALEPH2, an ILP imple-mentation by Srinivasan (2000) that has been provenwell suited to deal with a large amount of data inmultiple domains.
For its routine use, ALEPH fol-lows a simple procedure to induce rules.
It first se-lects an example and builds the most specific clausethat entertains the example.
Next, it tries to searchfor a clause more general than the bottom one.
Thebest clause is added to the current theory and all theexamples made redundant are removed.
The proce-dure repeats until all examples are processed.4.2 Apply ILP to coreference resolutionGiven a document, we encode a mention or a par-tial entity with a unique constant.
Specifically, mjrepresents the jth mention (e.g., m6 for the pronoun?he?).
ei j represents the partial entity i before thejth mention.
For example, e1 6 denotes the part ofe1 before m6, i.e., {?Microsoft Corp.?, ?its?, ?thecompany?
}, while e1 5 denotes the part of e1 be-fore m5 (?The company?
), i.e., {?Microsoft Corp.?,?its?
}.Training instances are created as described in Sec-tion 3.2 for the entity-mention model.
Each instanceis recorded with a predicate link(ei j , mj), where mjis an active mention and ei j is a partial entity.
Forexample, the three training instances formed by thepronoun ?he?
are represented as follows:link(e1 6,m6).link(e3 6,m6).link(e2 6,m6).The first two predicates are put into E?, while thelast one is put to E+.The background knowledge for an instancelink(ei j , mj) is also represented with predicates,which are divided into the following types:1.
Predicates describing the information related toei j and mj .
The properties of mj are pre-2http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/aleph toc.htmlsented with predicates like f (m, v), where fcorresponds to a feature in the first part of Ta-ble 2 (removing the suffix mj), and v is itsvalue.
For example, the pronoun ?he?
can bedescribed by the following predicates:defNP(m6, 0).
indefNP(m6, 0).nameNP(m6, 0).
pron(m6, 1).bareNP(m6, 0).The predicates for the relationships betweenei j and mj take a form of f (e, m, v).
In ourstudy, we consider the number agreement (ent-NumAgree) and the gender agreement (entGen-derAgree) between ei j and mj .
v is 1 if allof the mentions in ei j have consistent num-ber/gender agreement with mj , e.g,entNumAgree(e1 6,m6, 1).2.
Predicates describing the belonging relationsbetween ei j and its mentions.
A predicatehas mention(e, m) is used for each mention ine 3.
For example, the partial entity e1 6 hasthree mentions, m1, m2 and m5, which can bedescribed as follows:has mention(e1 6,m1).has mention(e1 6,m2).has mention(e1 6,m5).3.
Predicates describing the information related tomj and each mention mk in ei j .
The predi-cates for the properties of mk correspond to thefeatures in the second part of Table 2 (removingthe suffix mk), while the predicates for the re-lationships between mj and mk correspond tothe features in the third part of Table 2.
For ex-ample, given the two mentions m1 (?MicrosoftCorp.)
and m6 (?he), the following predicatescan be applied:nameNP(m1, 1).pron(m1, 0).. .
.nameAlias(m1,m6, 0).sentDist(m1,m6, 1).. .
.the last two predicates represent that m1 and3If an active mention mj is a pronoun, only the previousmentions in two sentences apart are recorded by has mention,while the farther ones are ignored as they have less impact onthe resolution of the pronoun.847m6 are not name alias, and are one sentenceapart.By using the three types of predicates, the dif-ferent knowledge related to entities and mentionsare integrated.
The predicate has mention acts asa bridge connecting the entity-mention knowledgeand the mention-pair knowledge.
As a result, whenevaluating the coreference relationship between anactive mention and an entity, we can make use ofthe ?global?
information about the entity, as well asthe ?local?
information of each individual mentionin the entity.From the training instances and the associatedbackground knowledge, a set of hypotheses can beautomatically learned by ILP.
Each hypothesis isoutput as a rule that may look like:link(A,B):-predi1, predi2, .
.
.
, has mention(A,C), .
.
.
, prediN.which corresponds to first-order logic?A,B(predi1 ?
predi2 ?
.
.
.?
?C(has mention(A,C) ?
.
.
.
?
prediN)?
link(A,B))Consider an example rule produced in our system:link(A,B) :-has mention(A,C), numAgree(B,C,1),strMatch Head(B,C,1), bareNP(C,1).Here, variables A and B stand for an entity and anactive mention in question.
The first-order logic isimplemented by using non-instantiated arguments Cin the predicate has mention.
This rule states that amention B should belong to an entity A, if there ex-ists a mention C in A such that C is a bare nounphrase with the same head string as B, and matchesin number with B.
In this way, the detailed informa-tion of each individual mention in an entity can becaptured for resolution.A rule is applicable to an instance link(e, m), ifthe background knowledge for the instance can bedescribed by the predicates in the body of the rule.Each rule is associated with a score, which is theaccuracy that the rule can produce for the traininginstances.The learned rules are applied to resolution in asimilar way as described in Section 3.2.
Given anactive mention m and a partial entity e, a test in-stance link(e, m) is formed and tested against everyrule in the rule set.
The confidence that m shouldTrain Test#entity #mention #entity #mentionNWire 1678 9861 411 2304NPaper 1528 10277 365 2290BNews 1695 8986 468 2493Table 3: statistics of entities (length > 1) and containedmentionsbelong to e is the maximal score of the applicablerules.
An active mention is linked to the entity withthe highest confidence value (above 0.5), if any.5 Experiments and Results5.1 Experimental SetupIn our study, we did evaluation on the ACE-2003corpus, which contains two data sets, training anddevtest, used for training and testing respectively.Each of these sets is further divided into three do-mains: newswire (NWire), newspaper (NPaper), andbroadcast news (BNews).
The number of entitieswith more than one mention, as well as the numberof the contained mentions, is summarized in Table 3.For both training and resolution, an input rawdocument was processed by a pipeline of NLPmodules including Tokenizer, Part-of-Speech tag-ger, NP Chunker and Named-Entity (NE) Recog-nizer.
Trained and tested on Penn WSJ TreeBank,the POS tagger could obtain an accuracy of 97% andthe NP chunker could produce an F-measure above94% (Zhou and Su, 2000).
Evaluated for the MUC-6 and MUC-7 Named-Entity task, the NER mod-ule (Zhou and Su, 2002) could provide an F-measureof 96.6% (MUC-6) and 94.1%(MUC-7).
For evalu-ation, Vilain et al (1995)?s scoring algorithm wasadopted to compute recall and precision rates.By default, the ALEPH algorithm only generatesrules that have 100% accuracy for the training data.And each rule contains at most three predicates.
Toaccommodate for coreference resolution, we loos-ened the restrictions to allow rules that have above50% accuracy and contain up to ten predicates.
De-fault parameters were applied for all the other set-tings in ALEPH as well as other learning algorithmsused in the experiments.5.2 Results and DiscussionsTable 4 lists the performance of different corefer-ence resolution systems.
For comparison, we first848NWire NPaper BNewsR P F R P F R P FC4.5- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4ILP- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5Table 4: Results of different systems for coreference resolutionexamined the C4.5 algorithm4 which is widely usedfor the coreference resolution task.
The first line ofthe table shows the baseline system that employs thetraditional mention-pair model (MP) as described inSection 3.1.
From the table, our baseline systemachieves a recall of around 66%-68% and a preci-sion of around 50%-60%.
The overall F-measurefor NWire, NPaper and BNews is 60.4%, 57.9% and62.9% respectively.
The results are comparable tothose reported in (Ng, 2005) which uses similar fea-tures and gets an F-measure ranging in 50-60% forthe same data set.
As our system relies only on sim-ple and knowledge-poor features, the achieved F-measure is around 2-4% lower than the state-of-the-art systems do, like (Ng, 2007) and (Yang and Su,2007) which utilized sophisticated semantic or real-world knowledge.
Since ILP has a strong capabilityin knowledge management, our system could be fur-ther improved if such helpful knowledge is incorpo-rated, which will be explored in our future work.The second line of Table 4 is for the systemthat employs the entity-mention model (EM) with?Any-X?
based entity features, as described in Sec-tion 3.2.
We can find that the EM model does notshow superiority over the baseline MP model.
Itachieves a higher precision (up to 2.6%), but a lowerrecall (2.9%), than MP.
As a result, we only see?0.4% difference between the F-measure.
The re-sults are consistent with the reports by Luo et al(2004) that the entity-mention model with the ?Any-X?
first-order features performs worse than the nor-mal mention-pair model.
In our study, we also testedthe ?Most-X?
strategy for the first-order features asin (Culotta et al, 2007), but got similar results with-out much difference (?0.5% F-measure) in perfor-4http://www.rulequest.com/see5-info.htmlmance.
Besides, as with our entity-mention predi-cates described in Section 4.2, we also tried the ?All-X?
strategy for the entity-level agreement features,that is, whether all mentions in a partial entity agreein number and gender with an active mention.
How-ever, we found this bring no improvement againstthe ?Any-X?
strategy.As described, given an active mention mj , the MPmodel only considers the mentions between mj andits closest antecedent.
By contrast, the EM modelconsiders not only these mentions, but also their an-tecedents in the same entity link.
We were interestedin examining what if the MP model utilizes all thementions in an entity as the EM model does.
Asshown in the third line of Table 4, such a solutiondamages the performance; while the recall is at thesame level, the precision drops significantly (up to12%) and as a result, the F-measure is even lowerthan the original MP model.
This should be becausea mention does not necessarily have direct corefer-ence relationships with all of its antecedents.
As theMP model treats each mention-pair as an indepen-dent instance, including all the antecedents wouldproduce many less-confident positive instances, andthus adversely affect training.The second block of the table summarizes the per-formance of the systems with ILP.
We were first con-cerned with how well ILP works for the mention-pair model, compared with the normally used algo-rithm C4.5.
From the results shown in the fourthline of Table 4, ILP exhibits the same capability inthe resolution; it tends to produce a slightly higherprecision but a lower recall than C4.5 does.
Overall,it performs better in F-measure (1.8%) for Npaper,while slightly worse (<1%) for Nwire and BNews.These results demonstrate that ILP could be used as849link(A,B) :-bareNP(B,0), has mention(A,C), appositive(C,1).link(A,B) :-has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).link(A,B) :-nameNP(B,0), has mention(A,C), predicative(C,1).link(A,B) :-has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).link(A,B) :-nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).link(A,B) :-pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),subject(D, 1)....Figure 1: Examples of rules produced by ILP (entity-mention model)a good classifier learner for the mention-pair model.The fifth line of Table 4 is for the ILP based entity-mention model (described in Section 4.2).
We canobserve that the model leads to a better performancethan all the other models.
Compared with the sys-tem with the MP model (under ILP), the EM versionis able to achieve a higher precision (up to 4.6% forBNews).
Although the recall drops slightly (up to1.8% for BNews), the gain in the precision couldcompensate it well; it beats the MP model in theoverall F-measure for all three domains (2.3% forNwire, 0.4% for Npaper, 1.4% for BNews).
Es-pecially, the improvement in NWire and BNews isstatistically significant under a 2-tailed t test (p <0.05).
Compared with the EM model with the man-ually designed first-order feature (the second line),the ILP-based EM solution also yields better perfor-mance in precision (with a slightly lower recall) aswell as the overall F-measure (1.0% - 1.8%).The improvement in precision against themention-pair model confirms that the global infor-mation beyond a single mention pair, when beingconsidered for training, can make coreference rela-tions clearer and help classifier learning.
The bet-ter performance against the EM model with heuristi-cally designed features also suggests that ILP is ableto learn effective first-order rules for the coreferenceresolution task.In Figure 1, we illustrate part of the rules pro-duced by ILP for the entity-mention model (NWiredomain), which shows how the relational knowledgeof entities and mentions is represented for decisionmaking.
An interesting finding, as shown in the lastrule of the table, is that multiple non-instantiated ar-guments (i.e.
C and D) could possibly appear inthe same rule.
According to this rule, a pronominalmention should be linked with a partial entity whichcontains a named-entity and contains an indefiniteNP in a subject position.
This supports the claimsin (Yang et al, 2004a) that coreferential informa-tion is an important factor to evaluate a candidate an-tecedent in pronoun resolution.
Such complex logicmakes it possible to capture information of multi-ple mentions in an entity at the same time, which isdifficult to implemented in the mention-pair modeland the ordinary entity-mention model with heuris-tic first-order features.6 ConclusionsThis paper presented an expressive entity-mentionmodel for coreference resolution by using InductiveLogic Programming.
In contrast to the traditionalmention-pair model, our model can capture infor-mation beyond single mention pairs for both trainingand testing.
The relational nature of ILP enables ourmodel to explicitly express the relations between anentity and its mentions, and to automatically learnthe first-order rules effective for the coreference res-olution task.
The evaluation on ACE data set showsthat the ILP based entity-model performs better thanthe mention-pair model (with up to 2.3% increase inF-measure), and also beats the entity-mention modelwith heuristically designed first-order features.Our current work focuses on the learning modelthat calculates the probability of a mention be-longing to an entity.
For simplicity, we just use agreedy clustering strategy for resolution, that is, amention is linked to the current best partial entity.In our future work, we would like to investigatemore sophisticated clustering methods that wouldlead to global optimization, e.g., by keeping a largesearch space (Luo et al, 2004) or using integerprogramming (Denis and Baldridge, 2007).Acknowledgements This research is supportedby a Specific Targeted Research Project (STREP)of the European Union?s 6th Framework Programmewithin IST call 4, Bootstrapping Of Ontologies andTerminologies STrategic REsearch Project (BOOT-Strep).850ReferencesC.
Aone and S. W. Bennett.
1995.
Evaluating automatedand manual acquisition of anaphora resolution strate-gies.
In Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 122?129.V.
Claveau, P. Sebillot, C. Fabre, and P. Bouillon.
2003.Learning semantic lexicons from a part-of-speech andsemantically tagged corpus using inductive logic pro-gramming.
Journal of Machine Learning Research,4:493?525.A.
Culotta, M. Wick, and A. McCallum.
2007.
First-order probabilistic models for coreference resolution.In Proceedings of the Annual Meeting of the NorthAmerica Chapter of the Association for ComputationalLinguistics (NAACL), pages 81?88.J.
Cussens.
1996.
Part-of-speech disambiguation usingilp.
Technical report, Oxford University ComputingLaboratory.P.
Denis and J. Baldridge.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In Proceedings of the Annual Meetingof the North America Chapter of the Association forComputational Linguistics (NAACL), pages 236?243.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous corefer-ence resolution algorithm based on the bell tree.
InProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages135?142.A.
McCallum and B. Wellner.
2003.
Toward condi-tional models of identity uncertainty with applicationto proper noun coreference.
In Proceedings of IJCAI-03 Workshop on Information Integration on the Web,pages 79?86.J.
McCarthy and W. Lehnert.
1995.
Using decisiontrees for coreference resolution.
In Proceedings ofthe 14th International Conference on Artificial Intel-ligences (IJCAI), pages 1050?1055.R.
Mooney.
1997.
Inductive logic programming for nat-ural language processing.
In Proceedings of the sixthInternational Inductive Logic Programming Work-shop, pages 3?24.V.
Ng and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 104?111,Philadelphia.V.
Ng.
2005.
Machine learning for coreference resolu-tion: From local classification to global ranking.
InProceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages157?164.V.
Ng.
2007.
Semantic class induction and coreferenceresolution.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 536?543.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learningapproach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.L.
Specia, M. Stevenson, and M. V. Nunes.
2007.
Learn-ing expressive models for words sense disambiguation.In Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages41?48.A.
Srinivasan.
2000.
The aleph manual.
Technical re-port, Oxford University Computing Laboratory.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the Sixth Mes-sage understanding Conference (MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.X.
Yang and J. Su.
2007.
Coreference resolution us-ing semantic relatedness information from automati-cally discovered patterns.
In Proceedings of the 45thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 528?535.X.
Yang, J. Su, G. Zhou, and C. Tan.
2004a.
Improv-ing pronoun resolution by incorporating coreferentialinformation of candidates.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 127?134, Barcelona.X.
Yang, J. Su, G. Zhou, and C. Tan.
2004b.
AnNP-cluster approach to coreference resolution.
InProceedings of the 20th International Conference onComputational Linguistics, pages 219?225, Geneva.G.
Zhou and J. Su.
2000.
Error-driven HMM-basedchunk tagger with context-dependent lexicon.
In Pro-ceedings of the Joint Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, pages 71?79, Hong Kong.G.
Zhou and J. Su.
2002.
Named Entity recognition us-ing a HMM-based chunk tagger.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 473?480, Philadel-phia.851
