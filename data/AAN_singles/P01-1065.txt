A Generic Approach to Parallel Chart Parsing with anApplication to LinGOMarcel van LohuizenFaculty of Information Technology and Systems Delft University of TechnologyDelft, The Netherlandsmpvl@acm.orgAbstractMulti-processor systems are becom-ing more commonplace and afford-able.
Based on analyses of ac-tual parsings, we argue that to ex-ploit the capabilities of such ma-chines, unification-based grammarparsers should distribute work at thelevel of individual unification oper-ations.
We present a generic ap-proach to parallel chart parsing thatmeets this requirement, and showthat an implementation of this tech-nique for LinGO achieves consider-able speedups.1 IntroductionThe increasing demand for accuracy and ro-bustness for today?s unification-based gram-mar parsers brings on an increasing demandfor computing power.
In addition, as thesesystems are increasingly used in applicationsthat require direct user interaction, e.g.
web-based applications, responsiveness is of majorconcern.
In the mean time, small-scale desk-top multiprocessor systems (e.g.
dual or evenquad Pentium machines) are becoming morecommonplace and affordable.
In this paperwe will show that exploiting the capabilitiesof these machines can speed up parsers con-siderably, and can be of major importance inachieving the required performance.There are certain requirements the designof a parallel parser should meet.
Over thepast years, many improvements to existingparsing techniques have boosted the perfor-mance of parsers by many factors (Oepen andCallmeier, 2000).
If a design of a parallelparser is tied too much to a particular ap-proach to parsing, it may be hard to incorpo-rate such improvements as they become avail-able.
For this reason, a solution to parallelparsing should be as general as possible.
Oneobvious way to ensure that optimizations forsequential parsers can be used in a parallelparser as well is to let a parallel parser mimica sequential parser as much as possible.
Thisis basically the approach we will take.The parser that we will present in this pa-per uses the LinGO grammar.
LinGO is anHPSG-based grammar which was developedat Stanford (Copestake, 2000).
It is currentlyused by many research institutions.
This al-lows our results to be compared with that ofother research groups.In Section 2, we explore the possibilities forparallelism in natural language parsing by an-alyzing the computational structure of pars-ings.
Section 3 and 4 discuss respectively thedesign and the performance of our system.Finally, we compare our work with other re-search on parallel parsing.2 Analysis of ParsingsTo analyze the possibilities for parallelism incomputations they are often represented astask graphs.
A task graph is a directed acyclicgraph, where the nodes represent some unitof computation, called a task, and the arcsrepresent the execution dependencies betweenthe tasks.
Task graphs can be used to an-alyze the critical path, which is the mini-mal time required to complete a computa-tion, given an infinite amount of processors.From Brent (1974) and Graham (1969) weknow that there exist P -processor schedulingswhere the execution time TP is bound as fol-lows:TP ?
T1/P + T?, (1)where T1 is the total work, or the executiontime for the one processor case, and T?
isthe critical path.
Furthermore, to effectivelyuse P processors, the average parallelism P?
=T1/T?
should be larger than P .The first step of the analysis is to find anappropriate graph representation for parsingcomputations.
According to Caroll (1994),performing a complexity analysis solely at thelevel of grammars and parsing schemata cangive a distorted image of the parsing pro-cess in practice.
For this reason, we basedour analysis on actual parsings.
The experi-ments were based on the fuse test suite, whichis a balanced extract from four appointmentscheduling (spoken) dialogue corpora (incl.VerbMobil).
Fuse contains over 2000 sen-tences with an average length of 11.6.We define a task graph for a single pars-ing computation as follows.
First, we distin-guish two types of tasks: unification tasks andmatch tasks.
A unification task executes asingle unification operation.
A match task isresponsible for all the actions that are takenwhen a unification succeeds: matching theresulting edge with other edges in the chartand putting resulting unification tasks on theagenda.
The match task is also responsiblefor applying filtering techniques like the quickcheck (Malouf et al, 2000).
The tasks areconnected by directed arcs that indicate theexecution dependencies.We define the cost of each unification taskas the number of nodes visited during theunification and successive copying operation.Unification operations are typically responsi-ble for over 90% of the total work.
In addi-tion, the cost of the match tasks are spreadout over succeeding unification tasks.
Wetherefore simply neglect the cost for match op-erations, and assume that this does not have asignificant impact on our measurements.
Thelength of a path in the graph can now be de-fined as the sum of the costs of all nodes onFigure 1: Task graphs for two different ap-proaches to parallel chart parsing.T1 T?
P?Type 1 1014247 3487 187Average type 2 1014247 11004 54Worst case type 2 1014247 69300 13Table 1: Critical path analysis for type 1 andtype 2 task graphs (average and worst case).the path.
The critical path length T?
can bedefined as the longest path between any twonodes in the graph.The presented model resembles a very fine-grained scheme for distributing work, whereeach single unification tasks to be scheduledindependently.
In a straightforward imple-mentation of such a scheme, the schedulingoverhead can become significant.
Limitingthe scheduling overhead is crucial in obtainingconsiderable speedup.
It might therefore betempting to group related tasks into a singleunit of execution to mitigate this overhead.For this reason we also analyzed a task graphrepresentation where only match tasks spawna new unit of execution.
The top graph inFigure 1 shows an example of a task graphfor the first approach.
The bottom graph ofFigure 1 shows the corresponding task graphfor the second approach.
Note that becausea unification task may depend on more thanone match task, a choice has to be made inwhich unit of execution the unification task isput.Table 1 shows the results of the critical pathanalysis of both approaches.
For the first ap-proach, the critical path is uniquely defined.For the second approach we show both theworst case, considering all possible schedul-ings, and an average case.
The results for T1,T?, and P?
are averaged over all sentences.1The results show that, using the first ap-proach, there is a considerable amount of par-allelism in the parsing computations.
The re-sults also show that a small change in the de-sign of a parallel parser can have a signifi-cant impact on the value for P?
.
To obtain aspeedup of P , in practice, there should be asafety margin between P and P?
.
This sug-gests that the first approach is a considerablysaver choice, especially when one is consider-ing using more than a dozen of processors.3 Design and ImplementationBased on the discussion in the preceding sec-tions, we can derive two requirements for thedesign of a parallel parser: it should be closein design to a sequential parser and it shouldallow each single unification operation to bescheduled dynamically.
The parallel parser wewill present in this section meets both require-ments.Let us first focus on how to meet the firstrequirement.
Basically, we let each processor,run a regular sequential parser augmentedwith a mechanism to combine the results ofthe different parsers.
Each sequential parsercomponent is contained in a different thread.By using threads, we allow each parser toshare the same memory space.
Initially, eachthread is assigned a different set of work, forexample, resembling a different part of the in-put string.
A thread will process the unifica-tion tasks on the agenda and, on success, willperform the resulting match task to match thenew edge with the edges on its chart.
Aftercompleting the work on its agenda, a threadwill match the edges on its chart with theedges derived so far by the other threads.
Thismay produce new unification tasks, which thethread puts on its agenda.
After the commu-nication phase is completed, it returns to nor-mal parsing mode to execute the work on itsagenda.
This process continues until all edges1Note that since?T1/?T?
6=?T1/T?, the re-sults for P?
turn out slightly lower than might havebeen expected from the values of T1 and T?.Figure 2: Architecture of MACAMBA.of all threads have been matched against eachother and all work has been completed.3.1 Data StructuresFigure 2 shows an outline of our approach interms of data structures.
Each thread con-tains an agenda, which can be seen as a queueof unification tasks, a chart, which stores thederived edges, and a heap, which is used tostore the typed-feature structures that are ref-erenced by the edges.
Each thread has full ac-cess to its own agenda, chart, and heap, andhas read-only access to the respective struc-tures of all other threads.
Grammars areread-only and can be read by all threads.In the communication phase, threads needread-only access to the edges derived by otherthreads.
This is especially problematic forthe feature structures.
Many unification al-gorithms need write access to scratch fieldsin the graph structures.
Such algorithms aretherefore not thread-safe.2 For this reason weuse the thread-safe unification algorithm pre-sented by Van Lohuizen (2000), which is com-parable in performance to Tomabechi?s algo-rithm (Tomabechi, 1991).Note that each thread also has its ownagenda.
Some parsing systems require strictcontrol over the order of evaluation of tasks.The distributed agendas that we use in ourapproach may make it hard to implement sucha strict control.
One solution to the problemwould be to use a centralized agenda.
The dis-advantage of such a solution is that it mightincrease the synchronization overhead.
Tech-niques to reduce the synchronization overhead2In this context, thread safe means that the samedata structure can be involved in more than one op-eration, of more than one thread, simultaneously.global shared NrThreadsIdle, Generation, IdleGenSched()var threadGen, newWork, isIdlethreadGen?Generation?Generation+1while NrThreadsIdle 6= P do1.
newWork?
not IsEmpty(agenda).2.
Process the agenda as in the sequentialcase.
In addition, stamp each newly de-rived I edge by setting I.generation to thecurrent value for threadGen and add I to thisthread?s edge list.3.
Examine all the other threads for newlyderived edges.
For each new edge I and foreach edge J on the chart for which holdsI.generation > J.generation, add the cor-responding task to the agenda if it passesthe filter.
If any edge was processed, setnewWork to true.4.
if not newWork thennewWork?Steal()5. lock GlobalLock6.
if newWork thenGeneration?
Generation + 1threadGen?
GenerationNrThreadsIdle?
07. elseif Generation 6= IdleGen thenisIdle?
falseGeneration?
Generation + 1threadGen?
IdleGen?
Generationelseif threadGen 6= IdleGen thenisIdle?
falsethreadGen?
IdleGenelseif not isIdle thenisIdle?
trueNrThreadsIdle?
NrThreadsIdle + 18. unlock GlobalLockFigure 3: Scheduling algorithm.in such a setup can be found in (Markatos andLeBlanc, 1992).3.2 Scheduling AlgorithmAt startup, each thread calls the schedulingalgorithm shown in Figure 3.
This algorithmcan be seen as a wrapper around an existingsequential parser that takes care of combin-ing the results of the individual threads.
Thefunctionality of the sequential parser is em-bedded in step 2.
After this step, the agendawill be empty.
The communication betweenthreads takes place in step 3.
Each time athread executes this step, it will proceed overall the newly derived edges of other threads(foreign edges) and match them with theedges on its own chart (local edges).
Checkingthe newly derived edges of other threads cansimply be done by proceeding over a linked listof derived edges maintained by the respectivethreads.
Threads record the last visited edgeof the list of each other thread.
This ensuresthat each newly derived item needs to be vis-ited only once by each thread.As a result of step 3, the agenda may be-come non-empty.
In this case, newWork willbe set and step 2 is executed again.
This cyclecontinues until all work is completed.The remaining steps serve several purposes:load balancing, preventing double work, anddetecting termination.
We will explain each ofthese aspects in the following sections.
Notethat step 6 and 7 are protected by a lock.This ensures that no two threads can executethis code simultaneously.
This is necessarybecause Step 6 and 7 write to variables thatare shared amongst threads.
The overheadincurred by this synchronization is minimal,as a thread typically iterates over this partonly a small number of times.
This is becausethe depth of the derivation graph of any edgeis limited (average 14, maximum 37 for thefuse test set).3.3 Work StealingIn the design as presented so far, each threadexclusively executes the unification tasks onits agenda.
Obviously, this violates the re-quirement that each unification task shouldbe scheduled dynamically.In (Blumofe and Leiserson, 1993), it isshown that for any multi-threaded compu-tation with work T1 and task graph depthT?, and for any number P of processors, ascheduling will achieve TP ?
T1/P +T?
if forthe scheduling holds that whenever there aremore than P tasks ready, all P threads areexecuting work.
In other words, as long asthere is work on any queue, no thread shouldbe idle.An effective technique to ensure the aboverequirement is met is work stealing (Frigo etal., 1998).
With this technique, a thread willfirst attempt to steal work from the queueof another thread before denouncing itself tobe idle.
If it succeeds, it will resume nor-mal execution as if the stolen tasks were itsown.
Work stealing incurs less synchroniza-tion overhead than, for example, a centralizedwork queue.In our implementation, a thread becomes athief by calling Steal, at step 4 of Sched.Steal allows stealing from two types ofqueues: the agendas, which contain outstand-ing unification tasks, and the unchecked for-eign edges, which resemble outstanding matchtasks between threads.A thief first picks a random victim to stealfrom.
It first attempts to steal the victim?smatch tasks.
If it succeeds, it will performthe matches and put any resulting unificationtasks on its own agenda.
If it cannot gainexclusive access to the lists of unchecked for-eign edges, or if there were no matches to beperformed, it will attempt to steal work fromthe victim?s agenda.
A thief will steal half ofthe work on the agenda.
This balances theload between the two threads and minimizesthe chance that either thread will have to callthe expensive steal operation soon thereafter.Note that idle threads will keep calling Stealuntil they either obtain new work or all otherthreads become idle.Obviously, stealing eliminates the exclusiveownership of the agenda and unchecked for-eign edge lists of the respective threads.
As aconsequence, a thread needs to lock its agendaand edge lists each time it needs to accessit.
We use an asymmetric mutual exclusionscheme, as presented in (Frigo et al, 1998), tominimize the cost of locking for normal pro-cessing and move more of the overhead to theside of the thief.3.4 Preventing Duplicate MatchesWhen two matching edges are stored on thecharts of two different threads, it should beprevented that both threads will perform thecorresponding match.
Failing to do so cancause the derivation of duplicate edges andeventually a combinatorial explosion of work.Our solution is based on a generation scheme.Each newly derived edge is stamped with thecurrent generation of the respective thread,threadGen (see step 2).
In addition, a threadwill only perform the match for two edges ifthe edge on its chart has a lower generationthan the foreign edge (see step 3).
Obviously,because the value of threadGen is unique forthe thread (see step 6), this scheme preventstwo edges from being matched twice.Sched also ensures that two matchingedges will always be matched by at least onethread.
After a thread completes step 3, itwill always raise its generation.
The new gen-eration will be greater than that of any for-eign edge processed before.
This ensures thatwhen an edge is put on the chart, no for-eign edge with a higher generation has beenmatched against the respective chart before.3.5 TerminationA thread may terminate when all work is com-pleted, that is, if and only if the followingconditions hold simultaneously: all agendasof all threads are empty, all possible matchesbetween edges have been processed, and allthreads are idle.
Step 7 of Sched enforcesthat these conditions hold before any threadleaves Sched.
Basically, each thread deter-mines for itself whether its queues are emptyand raises the global counter NrThreadsIdleaccordingly.
When all threads are idle simul-taneously, the parser is finished.A thread?s agenda is guaranteed to beempty whenever newWork is false at step 7.The same does not hold for the uncheckedforeign edges.
Whenever a thread derives anew edge, all other edges need to perform thecorresponding matches.
The following mecha-nism enforces this.
The first thread to becomeidle raises the global generation and recordsit in IdleGen.
Subsequent idle threads willadopt this as their idle generation.
When-ever a thread derives a new edge, it will raiseGeneration and reset NrThreadsIdle (step 6).This invalidates IdleGen which implicitly re-moves the idle status from all threads.
Notethat step 7 lets each thread perform an addi-tional iteration before raising NrThreadsIdle.This allows a thread to check for foreign edgesthat were derived after step 3 and before 7.Once all work is done, detecting terminationP TP (s) speedup1 1599.8 12 817.5 1.963 578.2 2.774 455.9 3.515 390.3 4.106 338.0 4.73Table 2: Execution times for the fuse testsuite for various number of processors.requires at most 2P synchronization steps.33.6 ImplementationThe implementation of the system con-sists of two parts: MACAMBA and CaLi.MACAMBA stands for Multi-threading Ar-chitecture for Chart And Memoization-BasedApplications.
The MACAMBA frameworkprovides a set of objects that implement thescheduling technique presented in the previ-ous section.
It also includes a set of sup-port objects like charts and a thread-safe uni-fication algorithm.
CaLi is an instance of aMACAMBA application that implements aChart parser for the LinGO grammar.
Thedesign of CaLi was based on PET (Callmeier,2000), one of the fastest parsers for LinGO.It implements the quick check (Malouf et al,2000), which, together with the rule check,takes care of filtering over 90% of the failingunification tasks before they are put on theagenda.
MACAMBA and CaLi were both im-plemented in Objective-C and currently runon Windows NT, Linux, and Solaris.4 Performance ResultsThe performance of the sequential version ofCaLi is comparable to that of PET.4 In ad-dition, for the single-processor parallel ver-sion of CaLi the total overhead incurred byscheduling is less than 1%.The first set of experiments consisted ofrunning the fuse test suite on a SUN UltraEnterprise with 8 nodes, each with a 400 MHz3No locking is required once a thread is idle.4Respectively, 1231s and 1339s on a 500MHz P-III,where both parsers used the same parsing schema.UltraSparc processor, for a varying number ofprocessors.
Table 2 shows the results of theseexperiments.5 The execution times for eachparse are measured in wall clock time.
Thetime measurement of a parse is started be-fore the first thread starts working and endsonly when all threads have stopped.
The fusetest suite contains a large number of smallsentences that are hard to parallelize.
Theseresults indicate that deploying multiple pro-cessors on all input sentences unconditionallystill gives a considerable overall speedup.The second set of experiments were run ona SUN Enterprise10000 with 64 250 MHz Ul-traSparc II processors.
To limit the amount ofdata generated by the experiments, and to in-crease the accuracy of the measurements, weselected a subset of the sentences in the fusesuite.
The parser is able to parse many sen-tences in the fuse suite in fewer than severalmilliseconds.
Measuring speedup is inaccu-rate in these cases.
We therefore eliminatedsuch sentences from the test suite.
From theremaining sentences we made a selection of500 sentences of various lengths.The results are shown in Figure 4.
The fig-ure includes a graph for the maximum, mini-mum, and average speedup obtained over allsentences.
The maximum speedup of 31.4 isobtained at 48 processors.
The overall peakis reached at 32 processors where the averagespeedup is 17.3.
One of the reasons for thedecline in speedup after 32 processors is theoverhead in the scheduling algorithm.
Mostnotably, the total number of top-level itera-tions of Sched increases for larger P .
Theminimum speedups of around 1 are obtainedfor, often small, sentences that contain too lit-tle inherent parallelism to be parallelized ef-fectively.Figure 4 shows a graph of the parallel ef-ficiency, which is defined as speedup dividedby the number of processors.
The average ef-ficiency remains close to 80% up till 16 pro-cessors.
Note that super linear speedup isachieved with up to 12 processors, repeat-edly for the same set of sentences.
Super lin-5Because the system was shared with other users,only 6 processors could be utilized.Figure 4: Average, maximum, and minimumspeedup and parallel efficiency based on wallclock time.ear speedup can occur because increasing thenumber of processors also reduces the amountof data handled by each node.
This reducesthe chance of cache misses.5 Related WorkParallel parsing for NLP has been researchedextensively.
For example, Thompson (1994)presented some implementations of parallelchart parsers.
Nijholt (1994) gives a more the-oretical overview of parallel chart parsers.
Asurvey of parallel processing in NLP is givenby Adriaens and Hahn (1994).Nevertheless, many of the presented solu-tions either did not yield acceptable speedupor were very specific to one application.
Re-cently, several NLP systems have been par-allelized successfully.
Pontelli et al (1998)show how two existing NLP applications weresuccessfully parallelized using the parallelProlog environment ACE.
The disadvantageof this approach, though, is that it can onlybe applied to parsers developed in Prolog.Manousopoulou et al (1997) discuss a par-allel parser generator based on the Eu-PAGEsystem.
This solution exploits coarse-grainedparallelism of the kind that is unusable formany parsing applications, including our own(see also Go?rz et.
al.
(1996)).Nurkkala et al (1994) presented a parallelparser for the UPenn TAG grammar, imple-mented on the nCUBE.
Although their bestresults were obtained with random grammars,speedups for the English grammar were alsoconsiderable.Yoshida et.
al.
(Yoshida et al, 1999) pre-sented a 2-phase parallel FB-LTAG parser,where the operations on feature structuresare all performed in the second phase.
Thespeedup ranged up to 8.8 for 20 processors,Parallelism is mainly thwarted by a lack ofparallelism in the first phase.Finally, Ninomiya et al (2001) developedan agent-based parallel parser that achievesspeedups of up to 13.2.
It is implementedin ABCL/f and LiLFeS.
They also provide ageneric solution that could be applied to manyparsers.
The main difference with our systemis the distribution of work.
This system usesa tabular chart like distribution of matchesand a randomized distribution of unificationtasks.
Experiments we conducted show thatthe choice of distribution scheme can have asignificant influence on the cache utilization.It should be mentioned, though, that it isin general hard to compare the performanceof systems when different grammars are used.On the scheduling side, our approach showsclose resemblance to the Cilk-5 system (Frigoet al, 1998).
It implements work stealingusing similar techniques.
An important dif-ference, though, is that our scheduler wasdesigned for chart parsers and tabular algo-rithms in general.
These types of applicationsfall outside the class of applications that Cilkis capable of handling efficiently.6 ConclusionsWe showed that there is sufficient parallelismin parsing computations and presented a par-allel chart parser for LinGO that can effec-tively exploit this parallelism by achievingconsiderable speedups.
Also, the presentedtechniques do not rely on a particular parsingschema or grammar formalism, and can there-fore be useful for other parsing applications.AcknowledgementsThanks to Makino Takaki and Takashi Ni-nomiya of the Department of Information Sci-ence, University of Tokyo, for running the1?64 processor experiments at their depart-ment?s computer.References[Adriaens and Hahn1994] Geert Adriaens and UdoHahn, editors.
1994.
Parallel Natural Lan-guage Processing.
Ablex Publishing Corpora-tion, Norwood, New Jersey.
[Blumofe and Leiserson1993] Robert D. Blumofeand Charles E. Leiserson.
1993.
Space-efficient scheduling of multithreaded computa-tions.
In Proceedings of the Twenty-Fifth An-nual ACM Symposium on the Theory of Com-puting (STOC ?93), pages 362?371, San Diego,CA, USA, May.
Also submitted to SIAM Jour-nal on Computing.
[Brent1974] Richard P. Brent.
1974.
The paral-lel evaluation of general arithmetic expressions.Journal of the ACM, 21(2):201?206, April.
[Callmeier2000] Ulrich Callmeier.
2000.
PET ?A platform for experimentation with efficientHPSG.
Natural Language Engineering, 6(1):1?18.
[Caroll1994] John Caroll.
1994.
Relating complex-ity to practical performance in parsing withwide-coverage unification grammars.
In Proc.of the 32nd Annual Meeting of the Associationfor Computational Linguistics, pages 287?294,Las Cruces, NM, June27?30.
[Copestake2000] Ann Copestake, 2000.
The (new)LKB system, version 5.2. from Stanford site.
[Frigo et al1998] Matteo Frigo, Charles E. Leiser-son, and Keigh H. Randall.
1998.
The im-plementation of the Cilk-5 multithreaded lan-guage.
ACM SIGPLAN Notices, 33(5):212?223, May.
[Go?rz et al1996] Gu?nther Go?rz, Marcus Kesseler,Jo?rg Spilker, and Hans Weber.
1996.
Researchon architectures for integrated speech/languagesystems in Verbmobil.
In The 16th Interna-tional Conference on Computational Linguis-tics, volume 1, pages 484?489, Copenhagen,Danmark, August5?9.
[Graham1969] R.L.
Graham.
1969.
Bounds onmultiprocessing timing anomalies.
SIAM J.Appl.
Math., 17(2):416?429.
[Malouf et al2000] Robert Malouf, John Carroll,and Ann Copestake.
2000.
Efficient featurestructure operations witout compilation.
Natu-ral Language Engineering, 6(1):1?18.
[Manousopoulou et al1997] A.G. Manousopoulou,G.
Manis, P. Tsanakas, and G. Papakonstanti-nou.
1997.
Automatic generation of portableparallel natural language parsers.
In Proceed-ings of the 9th Conference on Tools with Arti-ficial Intelligence (ICTAI ?97), pages 174?177.IEEE Computer Society Press.
[Markatos and LeBlanc1992] E. P. Markatos andT.
J. LeBlanc.
1992.
Using processor affinityin loop scheduling on shared-memory multipro-cessors.
In IEEE Computer Society.
TechnicalCommittee on Computer Architecture, editor,Proceedings, Supercomputing ?92: Minneapo-lis, Minnesota, November 16-20, 1992, pages104?113, 1109 Spring Street, Suite 300, SilverSpring, MD 20910, USA.
IEEE Computer So-ciety Press.
[Nijholt1994] Anton Nijholt.
1994.
Parallel ap-proaches to context-free language parsing.
InAdriaens and Hahn (1994).
[Ninomiya et al2001] Takashi Ninomiya, KentaroTorisawa, and Jun?ichi Tsujii.
2001.
An agent-based parallel HPSG parser for shared-memoryparallel machines.
Journal of Natural LanguageProcessing, 8(1), January.
[Nurkkala and Kumar1994] Tom Nurkkala andVipin Kumar.
1994.
A parallel parsing algo-rithm for natural language using tree adjoininggrammar.
In Howard Jay Siegel, editor, Pro-ceedings of the 8th International Symposiumon Parallel Processing, pages 820?829, LosAlamitos, CA, USA, April.
IEEE ComputerSociety Press.
[Oepen and Callmeier2000] Stephan Oepen andUlrich Callmeier.
2000.
Measure for mea-sure: Parser cross-fertilization.
In Proceedingssixth International Workshop on Parsing Tech-nologies (IWPT?2000), pages 183?194, Trento,Italy.
[Pontelli et al1998] Enrico Pontelli, Gopal Gupta,Janyce Wiebe, and David Farwell.
1998.
Natu-ral language multiprocessing: A case study.
InProceedings of the 15th National Conference onArtifical Intelligence (AAAI ?98), July.
[Thompson1994] Henry S. Thompson.
1994.
Par-allel parsers for context-free grammars?two ac-tual implementations compared.
In Adriaensand Hahn (1994).
[Tomabechi1991] H. Tomabechi.
1991.
Quasi-destructive graph unifications.
In Proceedingsof the 29th Annual Meeting of the ACL, Berke-ley, CA.
[van Lohuizen2000] Marcel P. van Lohuizen.2000.
Memory-efficient and thread-safe quasi-destructive graph unification.
In Proceedingsof the 38th Meeting of the Association forComputational Linguistics, Hong Kong, China.
[Yoshida et al1999] Minoru Yoshida, Takashi Ni-nomiya, Kentaro Torisawa, Takaki Makino, andJun?ichi Tsujii.
1999.
Proceedings of efficientFB-LTAG parser and its parallelization.
In Pro-ceedings of Pacific Association for Computa-tional Linguistics ?99, pages 90?103, Waterloo,Canada, August.
