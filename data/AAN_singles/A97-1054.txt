A Workbench for Finding Structure in TextsAndre i  M ikheevHCRC,  Language Technology Group,University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW, UK.Andre i .
Mikheev@ed.
ac.
ukS teven  FinchThomson Technical Labs,1375 Piccard Drive, Suite 250,Rockville Maryland,  20850s f inch@thomtech ,  comAbst rac tIn this paper we report on a set of compu-tational tools with (n)SGML pipeline dataflow for uncovering internal structure innatural language texts.
The main idea be-hind the workbench is the independence ofthe text representation a d text analysisphases.
At the representation phase thetext is converted from a sequence of char-acters to features of interest by means ofthe annotation tools.
At the analysis phasethose features are used by statistics gath-ering and inference tools for finding signifi-cant correlations in the texts.
The analysistools are independent of particular assump-tions about the nature of the feature-setand work on the abstract level of feature-elements represented asSGML items.1 Introduct ionThere is increasing agreement that progress in vari-ous areas of language ngineering needs large collec-tions of unconstrained language material.
Such cor-pora are emerging and are proving to be importantresearch tools in areas uch as lexicography, text un-derstanding and information extraction, spoken lan-guage understanding, the evaluation of parsers, theconstruction of large-scale xica, etc.The key idea of corpus oriented language analy-sis is to collect frequencies of "interesting" eventsand then run statistical inferences on the basis ofthose frequencies.
For instance, one might be inter-ested in frequencies of co-occurences of a word withother words and phrases (collocations) (Smadja,1993), or one might be interested in inducing word-classes from the text by collecting frequencies oftheleft and right context words for a word in focus(Finch&Chater, 1993).
Thus, the building blocksof the "interesting" events might be words, theirmorpho-syntactic properties (e.g.
part-of-speech,suffix, etc.
), phrases or their sub-phrases (e.g.
head-noun of a noun group), etc.
The "interesting" eventsusually also specify the relation between those build-ing blocks such as "the two words should occur nextto each other or in the same sentence".
In this paperwe describe a workbench for uncovering that kind ofinternal structure in natural language texts.2 Data Level IntegrationThe underlying idea behind our workbench is datalevel integration of abstract data processing tools bymeans of structured streams.
The idea of using anopen set of modular tools with stream input/output(IO) is akin to the philosophy behind UNIX.
Thisallows for localization of specific data processing ormanipulation tasks so we can use different combina-tions of the same tools in a pipeline for fulfilling dif-ferent asks.
Our architecture, however, imposes anadditional constraint on the IO streams: they shouldhave a common syntactic format which is realized asSGML markup (Goldfarb, 1990).
A detailed compar-ison of this SGML-oriented architecture with moretraditional data-base oriented architectures can befound in (McKelvie et al, 1997).As a markup device an SGML element has a label(L), a pre-specified set of attributes (attr) and canhave character data:<L attr=val .. attr=val>character data</L>SGML elements can also include other elements thusproducing tree-like structures.
For instance, a doc-ument can comprise sections which consist of a titleand a body-text and the body-text can consist ofsentences each of which has its number stated as anattribute, has its contents as character data and caninclude other marked elements such as pre-tokenizedphrases and dates as shown in Figure I.
Such struc-tures are described in Document Type Definition(DTD) files which are used to check whether anSGML document is syntactically correct, i.e.
whetherits SGML elements have only pre-specified attributesand include only the right kinds of other SGML ele-ments.
So, for instance, if in the document shown372in Figure 1 we had a header element (H) under anS element - this would be detected as a violation ofthe defined structure.An important property of SGML is that defining arigorous syntactic format does not set any assump-tions on the semantics of the data and it is up to atool to assign a specific interpretation to a particularSGML item or its attributes.
Thus a tool in our archi-tecture is a piece of software which uses an SGML-handling Application Programmer Interface (API)for all its data access to corpora and performs omeuseful task, whether exploiting markup which haspreviously been added by other tools, or itself addingnew markup to the stream(s) and without destroyingthe previously added markup.
This approach allowsus to remain entirely within the SGML paradigm forcorpus markup while allowing us to be very generalin designing our tools, each of which can be used formany purposes.
Furthermore, through the abilityto pipe data through processes, the UNIX operatingsystem itself provides the natural "glue" for inte-grating data-level applications together.The API  methodology is very widely used in thesoftware industry to integrate software componentsto form finished applications, often making use ofsome glue environment to stick the pieces together(e.g.
tcl/tk, Visual Basic, Delphi, etc.).
However,we choose to integrate our applications at the datalevel.
Rather than define a set of functions whichcan be called to perform tasks, we define a set ofrepresentations for how information which is typi-cally produced by the tasks is actually represented.For natural anguage processing, there are many ad-vantages to the data level integration approach.
Letus take the practical example of a tokenizer.
Ratherthan provide a set of functions which take stringsand return sets of tokens, we define a tokenizer tobe something which takes in a SGML stream and re-turns a SGML stream which has been marked up fortokens.
Firstly, there is no direct tie to the processwhich actually performed the markup; provided atokenizer adds a markup around what it tokenizes,it doesn't matter whether it is written in C or LISP,or whether it is based on a FSA or a neural net.Some tokenization can even be done by hand, andany downline application which uses tokenization iscompletely functionally isolated from the processesused to perform the tokenization.
Secondly, eachpart of the process has a well-defined image at thedata level, and a data-level semantics.
Thus a tok-enizer as part of a complex task has its own seman-tics, and furthermore its own image in the data.3 Queries and V iewsSGML markup (Goldfarb, 1990) represents a docu-ment in terms of embedded elements akin to a filestructure with directories, subdirectories and files.Thus in the example in Figure 1, the document com-prises a header and a body text and these mightrequire different strategies for processing 1.
TheSGML-handling API in our workbench is realizedby means of the LT NSL library (Thompson et al,1996) which can handle even the most complex doc-ument structures (DTDs).
It allows a tool to read,change or add attribute values and character data toSGML elements and address a particular element in anormalized 2 SGML (NSGML) stream using its partialdescription by means of nsl-queries.
Consider thesample text shown in Figure 1.
Given that text andits markup, we can refer to the second sentence un-der a BODY element which is under a DOC element:/DOC/BODY/S\[n=2\].
This will sequentially give usthe second sentences in all BODYs.
If we want to ad-dress only the sentence under the first BODY we canspecify that in the query: /DOC/BODY\[0\]/S\[n=2\].We can use wildcards in the queries.
For instance,the query .
*/S says "give me all sentences anywherein the document" and the wildcard ".
*" means " atany level of embedding".
Thus we can directly spec-ify which parts of the stream we want to process andwhich to skip.Using nsl-queries we can access required SGML el-ements in a document.
These elements can have,however, quite complex internal structures which wemight want to represent in a number of differentways according to a task at hand.
For instance, if wewant to count words in the corpus and these wordsare marked with their parts of speech and base formssuch as<W pos=VBD l=look>looked</W>we should be able to specify to a counting pro-gram which fields of the element it should con-sider.
We might be interested in counting onlyword-tokens themselves and in this case two word-tokens "looked" will be counted as one regardlesswhether they were past verbs or participles.
Usingthe same markup we can specify that the "pos" at-tribute should be considered as well, or we can countjust parts-of-speech or lemmas.
A special view pat-tern provides such information for a counting tool.A view pattern consists of names of the attributes toconsider with the symbol # representing the char-acter data field of the element.
For instance:?
{#} - this view pattern specifies that only thecharacter data field of the element should beconsidered ("looked");?
{#}{pos} - this view pattern says that the1For instance, unlike common texts, headers oftenhave capitalized words which are not proper nouns.2There are a number of constraints on SGML markupin its normalized version.373<DOC><H>This is a Title</H><BODY><S n=l>This is the first sentence with character data only</S><S n=2>There can be sub-elements such as <W pos=NN>noun groups</W> inside sentences.</S></BODY><H>This is another Title</H><BODY><S n=l>This is the first sentence of the second section</S><S n=2>Here is a marked date <D d=l m=11 y=1996>1st October 1996</D> in this sentence.</S></BODY></DOC>Figure 1: SGML marked text.CorpusViewersCORPORAX 2 test \[Retrieval ~ Indexing L '  ITools Tools i~iIINDEXING & RETRIEVALConvertto SGMLJ Convert I"q to NSGMI_JI rJ Element Count2ontingencyTableBuilderCOUNTING TOOLSLogisticRegressionI DendrogramBuilderI Tokenizer II\[ POS TaggerlIN.'
Lemmatizer IIIII!I " ' ?
? "
.
.
.
.
.
.
.
.
? '
"' Chunker I m j i , sgml trI I III .
.
.
.
?
.
.
.
.
.
, , , o .
.IA No  To s K  ' J iNsSGML - Record/FieldConverters( sgdelmarkup )_2_INFERENCE TOOLS UNIX UTILITIESFigure 2: Workbench Architecture.
Thick arrows represent NSGML "fat" data flow and thin arrows normal(record/field) ata flow.374character data and the value of the "pos" at-tr ibute should be considered ("looked/VBD");?
{1} - this view pattern says that only the lem-mas will be counted ("look");4 The  WorkbenchUsing the idea of data level integration the work-bench described in this paper promotes the idea ofindependence of the text representation a d the textanalysis phases.
At the representation phase thetext is converted from a sequence of characters tofeatures of interest by means of the annotation tools.At the analysis phases those features are used by thetools such as statistics gathering and inference toolsfor finding significant correlations in the texts.
Theanalysis tools are independent of particular assump-tions about the nature of the feature-set and workon the abstract level of feature-elements which arerepresented as SGML items.
Figure 2 shows the mainmodules and data flow between them.At the first phase documents are represented in anSGML format and then converted to the normalizedSGML (NSGML) markup.
Unfortunately there is nogeneral way to convert free text into SGML since it isnot trivial to recognize the layout of a text; however,there already is a large body of SGML-marked textssuch as, for instance, the British National Corpus.The widely used on WWW format - HTML - isbased on SGML and requires only a limited amount ofefforts to be converted to strict SGML.
Other markupformats uch as LATEX can be relatively easily con-verted to SGML using publicly available utilities.
Inmany cases one can write a perl script to converta text in a known layout, for example, Penn Tree-bank into SGML.
In the simplest case one can putall the text of a document as character data under,for instance, a D0C element.
Such conversions arerelatively easy to implement and they can be done"on the fly" (i.e.
in a pipe), thus without the needto keep versions of the same corpus in different for-mats.
The conversion from arbitrary SGML to NSGMLis well defined and is done by a special tool (nsgml)"on the fly".The NSGML stream is then sent to the annota-tion tools which convert the sequence of charactersin specified by the nsl-queries parts of the streaminto SGML elements.
At the annotation phase thetools mark up the text elements and their features:words, words with their part-of-speech, syntacticgroups, pairs of frequently co-occuring words, sen-tence length or any other features to be modelled.The annotated text can be used by other toolswhich rely on the existence of marked features ofinterest in the text.
For instance, the statistic gath-ering tools employ standard algorithms for countingfrequencies of the events and are not aware of thenature of these events.
They work with SGML ele-ments which represent features we want to accountfor in the text.
So these tools are called with thespecification of which SGML elements to consider,and what should be the relation between those el-ements.
Thus the same tools can count words andnoun-groups, collect contingency tables for a pairof words in the same sentence or for a pair of sen-tences in the same or different documents.
For in-stance, for automatic alignment we might be inter-ested in finding frequently co-occuring words in twosentences, one of which is in English and the otherone in French.
Then the collected statistics are usedwith the standard tools for statistical inferences toproduce desirable language models.
The importantpoint here is that neither statistics gathering nor in-ference tools are aware of the nature of the statistics- they work with abstract data (SGML elements) andthe semantics of the statistical experiments i con-trolled at the annotation phase where we enrich textswith the features to model.4.1 Text Annotat ion PhaseAt the text annotation phase the text as a sequenceof characters is converted into a set of SGML ele-ments which will later be used by other tools.
Theseelements can be words with their features, phrases,combinations of words, length of sentences, etc.
Theset of annotation tools is completely open and theonly requirement to the integration of a new toolis that it should be able to work with NSGML andpass through the information it is not supposed tochange.
An annotation tool takes a specification(nsl-query) of which part of the stream to annotateand all other parts of the stream are passed throughwithout modifications.
Here is the standard set ofthe annotators provided by our workbench:sgtoken - the tokenizer (marks word boundaries).Tokenization is at the base of many NLP applica-tions allowing the jump from the level of charactersto the level of words, sgtoken is built on a determin-istic FSA library similar to the Xerox FSA library,and as such provides the ability to define tokens asregular expressions.
It also provides the ability todefine a priority amongst competing token types soone doesn't need to ensure that the token types de-fined are entirely distinct.
However of greatest inter-est to us is how it interfaces with the NSGML stream.The arguments to sgtoken include a specification ofthe FSA to use for tokenization (there are severalpre-defined ones, or the user can define their own ina perl-like regular expression syntax), an nsl-querywhich syntactically specifies the part of the sourcestream to process, and specification of the markupto add.
The output of the process is an NSGMLdata stream which contains all the data of the inputstream (in the same order as it appears in the in-375put stream) together with additional markup whichtokenizes those parts of the input stream which arespecified by the nsl-query parameter.
A callsgtoken -q /DOC/BODY/S <== what to tokenize-s <== use stemdard tokenizer-m W <== markup tokens as Wcan produce an output like:<W>books</W><W>,</W> <W>for instance</W>This gives rise to a simple data-level semantics --"everything inside a <W> element is a token addedby sgtoken".
However, this semantics is flexible.For example, if W markup is used for some other pur-pose, this markup might be changed to T markup.itpos - a POS- tagger  (assigns a single part-of-speech (POS) to a word according to the context itwas used).
This is an HMM tagger akin to that de-scribed in (Kupiec, 1992).
It receives a tokenized NS-GML stream and instructions on what is the markupfor words (word element label), where to apply tag-ging (nsl-query), and how to output the assignedinformation (attribute to assign).
For instance, wemight want to tag only the body-text of a document,and if the tokenizer marked up words as W elementswe specify this to the tagger, together with the at-tr ibute that is to stand for the part-of-speech in theW element:itpos -q /DOC/BODY/.
*/W <== path to words-m pos <== attribute to set with tagresource <== resources spec.
fileThis call will produce, for instance,<W pos=NNS>books</W><W pos=CM>,</W><W pos=NNS>pens</W>Here the "pos" attributes of the word elements "W"are set by the tagger to pos-tags: NNS - pluralnoun and CM - -comma.
We can combine resultsproduced by different taggers in different attributeswhich is useful for their evaluation.Itlem - the lemmat izer  (finds the base form fora word).
The lemmatizer takes a stream withword elements together with their part-of-speechtags and further enriches the elements assigning apre-specified attribute with lemmas, such as:<W pos=NNS l=book>books</W><W pos=CM>,</W><W pos=NNS l=pen>pens</W>l t ch tmk - syntactic hunker which determines thestructural boundaries for syntactic groups such asnoun groups and verb groups as, for instance:<NG><W pos=DT>the</W><W pos=JJ>good</W><W pos--NNS l=man>men</W></NG>The chunker leaves all previously added informationin the text and creates a structural element whichincludes the words of the chunk.
The chunker it-self is a combination of a finite state transducer overSGML elements with a grammar for syntactic groupssimilar in spirit to that of Fidditch (Hindle, 1983).This grammar can employ all the fields of the SGMLelements.
For instance a rule can say:"If there isan element of type "W" with character data "book"and the "pos" attribute set to "NN" followed by zeroor more elements of type "W" with the "pos" at-tributes set to "NN" - create an "NG" element andput this sequence under it.
The transducer itselfis application independent - it rewrites the SGMLstreams according to a grammar stated in terms ofSGML elements.
It was, for instance, applied for theconversion of SGML markup into the LaTex markup.The presented annotation tools are quite fast: thewhole pipeline annotates at a speed of 1500-2000words per second.
Thus we never store the resultsof the annotation on disk, but annotate in the pipeshaping the annotation to the task in hand.
Al-though the tools presented are deterministic andalways produce a single annotation there is a wayto incorporate multiple analyses into SGML struc-tures using the hyperlinks described in (McKelvieet al, 1997).
This initial set of annotation toolsserves a wide range of tasks but one can imag-ine, for instance, a tool which adds the suffix ofa word as its attribute <W s=ed>looked</W> ornoun group length in words or characters <NG wl=3c l=10><W>.
.
.
</NG>.
Another point to mentionhere is that it is quite easy to integrate another tag-ger or chunker or other tools as long as they obey theNSGML input/output conventions of the workbenchor can be encased in a suitable "wrapper".4.2 Counting ToolsAfter the annotation tools have been used to con-vert the text from a sequence of characters into asequence of S(3ML elements, the counting tools cancount different phenomena in a uniform way.
Likethe annotation tools, the counting tools take a spec-ification of which part of the document o countthings from (nsl-query) and they also take a spec-ification of the view of an SGML element, i.e.
whichparts of those elements to consider for comparison.The element counting program sgcount  countsthe number of occurrences of certain SGML elementsin pre-specified fields of the stream according to acertain view.
For instance, the callsgcount -q /DOC/H/.
*/W -v {#}{pos}will count frequencies of words occurring only in thetitles of a document at any level of embedding (.
*)and considering their character fields and the part-of-speech information.
The call376sgcount -q /DOC/BODY/.
*/W -v {pos}will produce the distribution of parts of speech inthe BODY fields of documents.To count joint events such as co-occurences of aword with other words, there is a tool for build-ing contingency tables - sgcontin.
A contingencytable 3 records the number of times a joint event hap-pened and the number of times a corresponding sin-gle event happened.
For instance, if we are inter-ested in the association between some two words, inthe contingency table we will collect the frequencywhen these two words were seen together and whenonly one of them was seen.
For instance, the call:sgcontin -q /DOC/BODY/W -v {#}-q2 /DOC/H/W -v2 {#}will give us a table of the associations between wordsin the body text of a document with the words inthe title.
Here, the program takes the query andthe view for each element of a joint event.
We canalso specify the relative position of elements to eachother.
For instance, the callsgcontin -q /DOC/BODY/W -v {#}-q2 {q}/W\[-l\] -v2 {#}will build a contingency for a word with words tothe left of it.Both sgcount and sgcontin are extremely fast- they can process a million word corpus in a fewminutes, so it is cheap to collect statistics of differentsorts from the corpus.4.3 The  In fe rence  Too lsThe statistics gathered at the counting phase can beused for different kinds of statistical inferences.
Forinstance, using mutual information (MI) or X 2 teston a contingency table we can find "sticky" pairs ofitems.
Thus if we collected a contingency table ofwords in titles vs. words in body-texts we can inferwhich words in a title strongly imply certain wordsin a body text.Using a contingency table for collecting left andright contexts of words we can run tests on similarityof the context vectors such as Spearman Rank Cor-relation Coefficient, Manhattan Metric, EuclideanMetric, Divergence, etc, to see which two words havethe most similar context vectors and thus behavesimilarly in the corpus.
Such similarity would implycloseness of those words and the words can be clus-tered in one class.
The dendrogram tool then can beused to represent clustered words in a hierarchicalclassification.There is a wide body of publicly available statis-tical software (StatXact, Cytel Software, etc) which3Here we will talk only about two-way contingencytables but our tools can build n-way tables.can be used with the collected statistics for perform-ing different sorts of statistical inferences and onecan chose the test and package according to the task.4 .4  Index ing /Ret r ieva l  Too lsFor fast access to particular locations in an SGMLcorpus we can index the text by using features ofinterest.
Again, as in the case with the statisticaltools, the indexing tools work on the level of ab-stract SGML elements and take as arguments whichelement should be indexed and what are the index-ing units.
For instance, we can index documentsby word-tokens in their sentences, or by sentencelength, or we can index sentences themselves by theirtokens, or by tokens together with their parts-of-speech or by other features (marked by the anno-tation tools).
Then we can instantly retrieve onlythose documents or sentences which possess the setof features pecified by the user or another tool.If, for instance, we index sentences by their wordswe can collect the collocations for a particular wordor a set of words in seconds.
The mkindex pro-gram takes an annotated NSGML stream and in-dexes elements pecified by an nsl-query by theirsub-elements specified by another nsl-query:mkindex-dq .
*/BODY/S <== index all S in BODY-iq .
*/W <= by Ws in these Ss-v {#} <= using only character data of WThe "v" specifies the view of the indexing units.Such call will produce a table of indexing units(words in our case) with references (sequential num-bers) to the indexed elements (sentences) they werefound in.
For instance:book 23 78 96 584says that word "book" was found in the sentences23 78 96 and 584.Next we have to relate (hook) the indexed ele-ments (sentences) to the locations on disk.
Notehere that for the indexing itself we used sentenceswith annotations (they included W elements) butfor hooking of these sentences to their absolute lo-cations the annotation is not needed if we want toretrieve sentences as character data.
The call:MakeSGMLHook -dq .
*/BODY/S fi lename.sgmfinds all sentences in the BODY elements of thefile and stores their locations:0 12344i 33444So sentence 0, for instance, starts from the offset12344 in the file.
To retrieve a sentence with a cer-tain word (or set of words) we look up in which sen-tences this word was found and then look up thelocations of those sentences in the file.377In some cases when the corpus to index alreadyhas a required annotation there is no need for ourannotation tools.
An example of such case is theBritish National Corpus (BNC).
The BNC itselfis distributed in SGML format with annotation, thuswe used the indexing tools directly after the pipelineconversion into NSGML.4.5 Util it ies and FiltersOne of the attractive features of data-level integra-tion is the availability of a number of very flexi-ble data manipulation and filtering utilities.
Forthe record/field data format of the UNIX envi-ronment, such utilities include grep,  sed, count,awk, sor t ,  t r  and so on.
These utilities allow flex-ible and selective data-manipulation applications tobe built by "piping" data through several utility pro-cesses.
SGML is a more powerful data representa-tion language than the simple record/field formatassumed by many of these UNIX utilities, and con-sequently new utilities which exploit the additionalpower of the SGML representation format are re-quired.
We shall briefly describe the sggrep andsgdelmarkup utilities.sggrep is an NSGML-aware version of grep.
Thisutility selects parts of the NSGML stream accordingto whether or not a regular expression appears incharacter data at a specific place.
As arguments, ittakes two queries, the first (context query) tells itwhat parts of the stream to select or reject, and thesecond (content query) tells it where to look for theregular expression (within the context of the first).Optional flags tell the utility whether to include oromit parts of the stream falling outside the scopeof the first query, or whether to reverse the polarityof the decision to include or exclude (the "-v" flag).For instance, the call:sggrep -qx .
*/SECT\[t=SUBSECTION\] <== context-qt .
*/TITLE <== contentmiocar.+\[ \]+?nf <== regular expr.will produce a stream of those SECT elementswhose attribute "t" has the value SUBSECTIONand which contain somewhere an element called TI-TLE  with contiguous characters matched by the reg-ular expression "miocar.+\[ \]+inf" in the characterdata field.sgdelmarkup is a utility which converts SGML ele-ments into the record field format adopted by UNIXso the information can be further processed by thestandard UNIX utilities such as per l ,  awk, sed,t r ,  etc.
This tool takes an nsl-query as the specifi-cation which elements to convert and the view to theelements as the specification how to convert hem.For instance, the call:sgdelmarkup -q .
*/W\[pos=NN \] -v "{#} {i}"will convert all nouns into "word lemma" format:<W l=book pos=NN>books</W> ==> books bookAs an example of the combined functionality wecan first extract from an NSGML stream elements ofinterest by means of sggrep, then convert hem intothe record-field format using sgdelmarkup and thensort them using the standard UNIX utility sor t .5 Putting it all togetherHere we present a simple example of extracting andclustering the terminology from a medical corpus us-ing the tools from the workbench.
The PDS is a cor-pus of short Patient Discharge Summaries writtenby a doctor to another doctor.
Usually such a lettercomprises a structured header and a few paragraphsdescribing the admission, investigations, treatmentsand the discharge of the patient.
We easily convertedthes texts into SGML format with the structure PDS-HEADER-BODY writing a few lines of perl script.We did not keep a separate SGML version of the cor-pus and converted it "on the fly".
Then we appliedthe annotation as described in section 4.1 thus mark-ing up words with their lemmas and parts-of-speechand noun/verb group boundaries.
Putting in thisannotation is computationally cheap and we did itin the pipe rather than storing the annotated texton disk:cat *.pds i nawk- f  pds2sgml.awk I nsgml isgtoken -q ".
*/BODY" -m W Iitpos -q ".*/BODY/.
*/W" -mpos  \[itlem -q ".*/BODY/.
*/W" -m 1 litchunk -q ".
*/BODY/S -m NG ng-gram litchunk -q ".
*/BODY/S -m VG vg-gramWe annotated only the body-text of the sum-maries and as the result of the annotation phasewe obtained sentence elements "S" with noun-group("NG"), verb-group ("VG") and word ("W') ele-ments inside, such as:<S n=l> -- sentence N 1<NG> -- start of noun group<W pos=DT>This</W><W pos=CD>70</W><W pos=NN>year</W><W pos=JJ>old</W><W pos=NN>man</W></NG> -- end of noun group<VG> -- start of verb group<W pos=BED l=be>was</W><W pos=VBN l=admit>admitted</W></VG> -- end of verb group<W pos=IN>from</W>.......... -- other phrases and words<W pos=SENT>.
</W></S> -- end of sentence 1Following (Justeson&Katz, 1995) we extractedterminological multi-word phrases as frequent multi-378word noun groups.
We collected all noun-groupswith their frequencies of occurrences by running:sgdelmarkup -q ".
*/NG/W" -v {#} Isgcount -q /PDS/BODY/S/NG -v {#}The sgdelmarkup call substituted all W elementsin noun groups with their character data:<NG><W pos=DT>the</W> <W pos=NN>man</W></NG><NG>the man</NG>and sgcount counted all NGs  considering only theircharacter fields.
Here are the most frequent noungroups found in the corpus:463 cardiac atheterisation207 Happy Valley Hospital144 ischaemic heart disease114 Consultant Cardiologist111 Isosorbide Mononitrate108 the right coronary arteryThen we clustered the extracted terms by theirleft and right contexts.
Using the sgcontin tool wecollected frequencies ofthe two words on the left andtwo words on the right as the context.
We also im-posed a constraint that if there is a group rather thana word, we take only the head word: the last wordin a noun group or the last verb in a verb group.As the result of such clustering we obtained fourdistinctive clusters: body-parts ("the right coronaryartery"), patient-conditions ("70% severe stenosis" ),treatments ("coronary bypass operation") and inves-tigations ("cardiac atheterisation").
Unlike simpleword-level clustering this clustering revealed someinteresting details about the terms.
For instance,the terms "left coronary artery" and "right coronaryartery" were clustered together whereas "occludedcoronary artery" was clustered with "occlusion" and"stenosis" thus uncovering the fact that it is of the"patient-condition" type rather than of the "body-part".
A more detailed escription of the processfor uncovering corpus regularities can be found in(Mikheev&Finch, 1995).6 ConclusionIn this paper we outlined a workbench for investi-gating corpus regularities.
The important conceptof the workbench is the uniform representation fcorpus data by using SGML markup at the corpusannotation phase.
This allows us to use the samestatistics gathering and inference tools with differ-ent annotations for modelling on different features.The workbench is completely open to the integra-tion of new tools but imposes SGML requirementson their input/output interface.
The pipeline archi-tecture of our workbench is not particularly suitedfor nice GUIs but there are publicly available visualpipeline builders which can be used with our tools.The tools described in this paper and some othertools are available by contacting the authors.
Mostof the tools are implemented in c /c++ under theUNIX environment and now we are porting them tothe NT platform since it supports the pipes whichare essential to our architecture.7 AcknowledgementsThis work was carried out in the HCRC LanguageTechnology Group, partly with support from theUK EPSRC project "Text Tokenisation Tool".
TheHCRC is a UK ESRC funded institution.ReferencesS.
Finch and N. Chater 1993.
"Learning Syn-tactic Categories: a statistical approach."
InM.R.Oaksford & G.D.A.Brown (eds) Neurody-namics and Psychology, pp.
295-322.
London:Harcourt Brace&Co.C.F.
Goldfarb 1990.
"The SGML Handbook."
Ox-ford: Clarendon Press.D.
Hindle 1983.
"User manual for Fidditch.
"Naval Research Laboratory Technical Memoran-dum #7590-142J.S.
Justeson and S.M.
Katz 1995.
"Technical ter-minology: some linguistic properties and an algo-rithm for identification i text."
In Journal forNatural Language Engineering vol 1(i).
pp.9-27.Cambridge University Press.J.
Kupiec 1992.
"Robust Part-of-Speech TaggingUsing a Hidden Markov Model."
In ComputerSpeech and Language.
pp.225-241.
Academic PressLimited.D.
McKelvie, C. Brew and H. Thompson 1997.
"Us-ing SGML as a Basis for Data-Intensive NLP.
"In Proceedings of the 5th Applied Natural Lan-guage Processing Conference (ANLP'97), Wash-ington D.C., USA.
ACLA.
Mikheev and S. Finch 1995.
"Towards a Work-bench for Acquisition of Domain Knowledge fromNatural Language."
In Proceedings of the 7thConference of the European Chapter of the Asso-ciation for Computational Linguistics (EA CL '95),Dublin.
pp.194-201.
ACL (CMP-LG 9604026)F. Smadja 1993.
"Retrieving Collocations fromText: Xtract."
In Computational Linguistics, vol19(2), pp.
143-177.
ACLH.
Thompson, D. McKelvie and S. Finch 1996.
"The Normalised SGML Library LT NSL ver-sion 1.4.6."
Technical Report, LanguageTechnology Group, University of Edinburgh.http ://www.
itg.
ed.
ac .uk/software/nsl379
