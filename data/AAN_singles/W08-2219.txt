Open Knowledge Extractionthrough CompositionalLanguage ProcessingBenjamin Van DurmeLenhart SchubertUniversity of Rochester (USA)email: vandurme@cs.rochester.eduAbstractWe present results for a system designed to perform Open KnowledgeExtraction, based on a tradition of compositional language processing,as applied to a large collection of text derived from the Web.
Evaluationthrough manual assessment shows that well-formed propositions of rea-sonable quality, representing general world knowledge, given in a logicalform potentially usable for inference, may be extracted in high volumefrom arbitrary input sentences.
We compare these results with those ob-tained in recent work on Open Information Extraction, indicating withsome examples the quite different kinds of output obtained by the twoapproaches.
Finally, we observe that portions of the extracted knowledgeare comparable to results of recent work on class attribute extraction.239240 Van Durme and Schubert1 IntroductionSeveral early studies in large-scale text processing (Liakata and Pulman, 2002; Gildeaand Palmer, 2002; Schubert, 2002) showed that having access to a sentence?s syn-tax enabled credible, automated semantic analysis.
These studies suggest that theuse of increasingly sophisticated linguistic analysis tools could enable an explosionin available symbolic knowledge.
Nonetheless, much of the subsequent work in ex-traction has remained averse to the use of the linguistic deep structure of text; thisdecision is typically justified by a desire to keep the extraction system as computa-tionally lightweight as possible.The acquisition of background knowledge is not an activity that needs to occuronline; we argue that as long as the extractor will finish in a reasonable period oftime, the speed of such a system is an issue of secondary importance.
Accuracy andusefulness of knowledge should be of paramount concern, especially as the increasein available computational power makes such ?heavy?
processing less of an issue.The system explored in this paper is designed for Open Knowledge Extraction: theconversion of arbitrary input sentences into general world knowledge represented in alogical form possibly usable for inference.
Results show the feasibility of extractionvia the use of sophisticated natural language processing as applied to web texts.2 Previous WorkGiven that the concern here is with open knowledge extraction, the myriad projectsthat target a few prespecified types of relations occurring in a large corpus are setaside.Among early efforts, one might count work on deriving selectional preferences(e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicate-argument structure (e.g., Abney (1996)) as steps in the direction of open knowledgeextraction, though typically few of the tuples obtained (often a type of subject plus averb, or a verb plus a type of object) can be interpreted as complete items of worldknowledge.
Another somewhat relevant line of research was initiated by Zelle andMooney (1996), concerned with learning to map NL database queries into formal DBqueries (a kind of semantic interpretation).
This was pursued further, for instance,by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learninglog-linear models, or (in the latter case) synchronous CF grammars augmented withlambda operators, for mapping English queries to DB queries.
However, this approachrequires annotation of texts with logical forms, and extending this approach to gen-eral texts would seemingly require a massive corpus of hand-annotated text ?
and thelogical forms would have to cover far more phenomena than are found in DB queries(e.g., attitudes, generalized quantifiers, etc.
).Another line of relevant work is that on semantic role labelling.
One early examplewas MindNet (Richardson et al, 1998), which was based on collecting 24 semanticrole relations from MRDs such as the American Heritage Dictionary.
More recentrepresentative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer(2002), and Punyakanok et al (2008).
The relevance of this work comes from the factthat identifying the arguments of the verbs in a sentence is a first step towards formingpredications, and these may in many cases correspond to items of world knowledge.Open Knowledge Extraction through Compositional Language Processing 241Liakata and Pulman (2002) built a system for recovering Davidsonian predicate-argument structures from the Penn Treebank through the application of a small setof syntactic templates targeting head nodes of verb arguments.
The authors illustratetheir results for the sentence ?Apple II owners, for example, had to use their televisionsets as screens and stored data on audiocassettes?
(along with the Treebank anno-tations); they obtain the following QLF, where verb stems serve as predicates, andarguments are represented by the head words of the source phrases:have(e1,owner, (use(e3,owner,set), and as(e3,screen)),and (store(e2,owner,datum), and on(e2,audiocassette)))For a test set of 100 Treebank sentences, the authors report recall figures for vari-ous aspects of such QLFs ranging from 87% to 96%.
While a QLF like the one abovecannot in itself be regarded as world knowledge, one can readily imagine postprocess-ing steps that could in many cases obtain credible propositions from such QLFs.
Howaccurate the results would be with machine-parsed sentences is at this point unknown.In the same year, Schubert (2002) described a project aimed directly at the extrac-tion of general world knowledge from Treebank text, and Schubert and Tong (2003)provided the results of hand-assessment of the resulting propositions.
The Brown cor-pus yielded about 117,000 distinct simple propositions (somewhat more than 2 persentence, of variable quality).
Like Liakata and Pulman?s approach the method reliedon the computation of unscoped logical forms from Treebank trees, but it abstractedpropositional information along the way, typically discarding modifiers at deeper lev-els from LFs at higher levels, and also replacing NPs (including named entities) bytheir types as far as possible.
Judges found about 2/3 of the output propositions (whenautomatically verbalized in English) acceptable as general claims about the world.
Thenext section provides more detail on the extraction system, called KNEXT, employedin this work.Clark et al (2003), citing the 2002 work of Schubert, report undertaking a sim-ilar extraction effort for the 2003 Reuters corpus, based on parses produced by theBoeing parser, (see Holmback et al (2000)), and obtained 1.1 million subject-verb-object fragments.
Their goal was eventually to employ such tuples as common-senseexpectations to guide the interpretation of text and the retrieval of possibly relevantknowledge in question-answering.
This goal, unlike the goal of inferential use of ex-tracted knowledge, does not necessarily require the extracted information to be in theform of logical propositions.
Still, since many of their tuples were in a form that couldbe quite directly converted into propositional forms similar to those of Schubert, theirwork indicated the potential for scalability in parser-based approaches to informationextraction or knowledge extraction.A recent project aimed at large-scale, open extraction of tuples of text fragmentsrepresenting verbal predicates and their arguments is TextRunner (Banko et al, 2007).This systems does part-of-speech tagging of a corpus, identifies noun phrases with anoun phrase chunker, and then uses tuples of nearby noun phrases within sentences toform apparent relations, using intervening material to represent the relation.
Apparentmodifiers such as prepositional phrases after a noun or adverbs are dropped.
Everycandidate relational tuple is classified as trustworthy (or not) by a Bayesian classifier,using such features as parts of speech, number of relevant words between the noun242 Van Durme and Schubertphrases, etc.
The Bayesian classifier is obtained through training on a parsed corpus,where a set of heuristic rules determine the trustworthiness of apparent relations be-tween noun phrases in that corpus.
As a preview of an example we will discuss later,here are two relational tuples in the format extracted by TextRunner:1(the people) use (force),(the people) use (force) to impose (a government).No attempt is made to convert text fragments such as ?the people?
or ?use _ to impose?into logically formal terms or predicates.
Thus much like semantic role-labelling sys-tems, TextRunner is an information extraction system, under the terminology usedhere; however, it comes closer to knowledge extraction than the former, in that it oftenstrips away much of the modifying information of complex terms (e.g., leaving just ahead noun phrase).2.1 KNEXTKNEXT (Schubert, 2002) was originally designed for application to collections ofmanually annotated parse trees, such as the Brown corpus.
In order to extract knowl-edge from larger text collections, the system has been extended for processing arbi-trary text through the use of third-party parsers.
In addition, numerous improvementshave been made to the semantic interpretation rules, the filtering techniques, and othercomponents of the system.
The extraction procedure is as follows:1.
Parse each sentence using a Treebank-trained parser (Collins, 1997; Charniak,1999).2.
Preprocess the parse tree, for better interpretability (e.g., distinguish differenttypes of SBAR phrases and different types of PPs, identify temporal phrases,etc.).3.
Apply a set of 80 interpretive rules for computing unscoped logical forms (ULFs)of the sentence and all lower-level constituents in a bottom-up sweep; at thesame time, abstract and collect phrasal logical forms that promise to yieldstand-alone propositions (e.g., ULFs of clauses and of pre- or post-modifiednominals are prime candidates).
The ULFs are rendered in Episodic Logic(e.g., (Schubert and Hwang, 2000)), a highly expressive representation allowingfor generalized quantifiers, predicate modifiers, predicate and sentence reifica-tion operators, and other devices found in NL.
The abstraction process dropsmodifiers present in lower-level ULFs (e.g., adjectival premodifiers of nominalpredicates) in constructing higher-level ULFs (e.g., for clauses).
In addition,named entities are generalized as far as possible using several gazetteers (e.g.,for male and female given names, US states, world cities, actors, etc.)
and somemorphological processing.4.
Construct complete sentential ULFs from the phrasal ULFs collected in the pre-vious step; here some filtering is performed to exclude vacuous or ill-formedresults.1Boldface indicates items recognized as head nouns.Open Knowledge Extraction through Compositional Language Processing 2435.
Render the propositions from the previous step in (approximate) English; againsignificant heuristic filtering is done here.As an example of KNEXT output, the sentence:Cock fights, however, are still legal in six of the United States, perhapsbecause we still eat chicken regularly, but no-longer dogs.yields a pair of propositions expressed logically as:[(K (NN cock.n (PLUR fight.n))) legal.a],[(DET (PLUR person.n)) eat.v (K chicken.n)]and these are automatically rendered in approximate English as:COCK FIGHTS CAN BE LEGAL.PERSONS MAY EAT CHICKEN.As can be seen, KNEXT output does not conform to the ?relation, arg1, arg2, ...?, tuplestyle of knowledge representation favored in information extraction (stemming fromthat community?s roots in populating DB tables under a fixed schema).
This is furtherexemplified by the unscoped logical form:2[(DET (PLUR person.n)) want.v (Ka (rid.a (of.p (DET dictator.n))))]which is verbalized as PERSONS MAY WANT TO BE RID OF A DICTATOR and is sup-ported by the text fragment:... and that if the Spanish people wanted to be rid of Franco, they mustachieve this by ...Later examples will be translated into a more conventional logical form.One larger collection we have processed since the 2002-3 work on Treebank cor-pora is the British National Corpus (BNC), consisting of 100 million words of mixed-genre text passages.
The quality of resulting propositions has been assessed by thehand-judging methodology of Schubert and Tong (2003), yielding positive judge-ments almost as frequently as for the Brown Treebank corpus.
The next section,concerned with the web corpus collected and used by Banko et al (2007), containsa fuller description of the judging method.
The BNC-based KB, containing 6,205,877extracted propositions, is publicly searchable via a recently developed online knowl-edge browser.32Where Ka is an action/attribute reification operator.3http://www.cs.rochester.edu/u/vandurme/epik244 Van Durme and Schubert3 ExperimentsThe experiments reported here were aimed at a comparative assessment of linguisti-cally based knowledge extraction (by KNEXT), and pattern-based information extrac-tion (by TextRunner, and by another system, aimed at class attribute discovery).
Thegoal being to show that logically formal results (i.e.
knowledge) based on syntacticparsing may be obtained at a subjective level of accuracy similar to methods aimedexclusively at acquiring correspondences between string pairs based on shallow tech-niques.Dataset Experiments were based on sampling 1% of the sentences from each doc-ument contained within a corpus of 11,684,774 web pages harvested from 1,354,123unique top level domains.
The top five contributing domains made up 30% of thedocuments in the collection.4 There were 310,463,012 sentences in all, the samplecontaining 3,000,736.
Of these, 1,373 were longer than a preset limit of 100 tokens,and were discarded.5 Sentences containing individual tokens of length greater than500 characters were similarly removed.6As this corpus derives from the work of Banko et al (2007), each sentence in thecollection is paired with zero or more tuples as extracted by the TextRunner system.Note that while websites such as Wikipedia.org contain large quantities of (semi-)structured information stored in lists and tables, the focus here is entirely on naturallanguage sentences.
In addition, as the extraction methods discussed in this paper donot make use of intersentential features, the lack of sentence to sentence coherenceresulting from random sampling had no effect on the results.ExtractionSentences were processed using the syntactic parser of Charniak (1999).From the resultant trees, KNEXT extracted 7,406,371 propositions, giving a raw av-erage of 2.47 per sentence.
Of these, 4,151,779 were unique, so that the averageextraction frequency per sentence is 1.78 unique propositions.
Post-processing left3,975,197 items, giving a per sentence expectation of 1.32 unique, filtered proposi-tions.
Selected examples regarding knowledge about people appear in Table 1.For the same sample, TextRunner extracted 6,053,983 tuples, leading to a raw av-erage of 2.02 tuples per sentence.
As described by its designers, TextRunner is aninformation extraction system; one would be mistaken in using these results to saythat KNEXT ?wins?
in raw extraction volume, as these numbers are not in fact directlycomparable (see section on Comparison).Table 1: Verbalized propositions concerning the class PERSONA PERSON MAY...SING TO A GIRLFRIEND RECEIVE AN ORDER FROM A GENERAL KNOW STUFF PRESENT A PAPEREXPERIENCE A FEELING CARRY IMAGES OF A WOMAN BUY FOOD PICK_UP A PHONEWALK WITH A FRIEND CHAT WITH A MALE-INDIVIDUAL BURN A SAWMILL FEIGN A DISABILITYDOWNLOAD AN ALBUM MUSH A TEAM OF (SEASONED SLED DOGS) RESPOND TO A QUESTIONSING TO A GIRLFRIEND OBTAIN SOME_NUMBER_OF (PERCULA CLOWNFISH) LIKE (POP CULTURE)4en.wikipedia.org, www.answers.com, www.amazon.com, www.imdb.com, www.britannica.com5Typically enumerations, e.g., There have been 29 MET deployments in the city of Florida since theinception of the program : three in Ft. Pierce , Collier County , Opa Locka , ... .6For example, Kellnull phenotypes can occur through splice site and splice-site / frameshift muta-tions301,302 450039003[...]3000 premature stop codons and missense mutations.Open Knowledge Extraction through Compositional Language Processing 2451.
A REASONABLE GENERAL CLAIMe.g., A grand-jury may say a proposition2.
TRUE BUT TOO SPECIFIC TO BE USEFULe.g., Bunker walls may be decorated with seashells3.
TRUE BUT TOO GENERAL TO BE USEFULe.g., A person can be nearest an entity4.
SEEMS FALSEe.g., A square can be round5.
SOMETHING IS OBVIOUSLY MISSINGe.g., A person may ask6.
HARD TO JUDGEe.g., Supervision can be with a companyFigure 1: Instructions for categorical judgingEvaluation Extraction quality was determined through manual assessment of ver-balized propositions drawn randomly from the results.
Initial evaluation was doneusing the method proposed in Schubert and Tong (2003), in which judges were askedto label propositions according to their category of acceptability; abbreviated instruc-tions may be seen in Figure 1.7 Under this framework, category one corresponds toa strict assessment of acceptability, while an assignment to any of the categories be-tween one and three may be interpreted as a weaker level of acceptance.
As seen inTable 2, average acceptability was judged to be roughly 50 to 60%, with associatedKappa scores signalling fair (0.28) to moderate (0.48) agreement.Table 2: Percent propositions labeled under the given category(s), paired with Fleiss?Kappa scores.
Results are reported both for the authors (judges one and two), alongwith two volunteersCategory % Selected Kappa % Selected Kappa1 49% 0.4017 50% 0.28221, 2, or 3 54% 0.4766 60% 0.3360judges judges w/ volunteersJudgement categories at this level of specificity are useful both for system analysisat the development stage, as well as for training judges to recognize the disparate waysin which a proposition may not be acceptable.
However, due to the rates of agreementobserved, evaluation moved to the use of a five point sliding scale (Figure 2).
Thisscale allows for only a single axis of comparison, thus collapsing the various waysin which a proposition may or may not be flawed into a single, general notion ofacceptability.7Judges consisted of the authors and two volunteers, each with a background in linguistics and knowl-edge representation.246 Van Durme and SchubertTHE STATEMENT ABOVE IS A REASONABLYCLEAR, ENTIRELY PLAUSIBLE GENERALCLAIM AND SEEMS NEITHER TOO SPECIFICNOR TOO GENERAL OR VAGUE TO BE USEFUL:1.
I agree.2.
I lean towards agreement.3.
I?m not sure.4.
I lean towards disagreement.5.
I disagree.Figure 2: Instructions for scaled judgingThe authors judged 480 propositions sampled randomly from amongst bins corre-sponding to frequency of support (i.e., the number of times a given proposition wasextracted).
60 propositions were sampled from each of 8 such ranges.8 As seen inFigure 3, propositions that were extracted at least twice were judged to be more ac-ceptable than those extracted only once.
While this is to be expected, it is striking thatas frequency of support increased further, the level of judged acceptability remainedroughly the same.4 ComparisonTo highlight differences between an extraction system targeting knowledge (repre-sented as logical statements) as compared to information (represented as segmentedtext fragments), the output of KNEXT is compared to that of TextRunner for two selectinputs.4.1 BasicConsider the following sentence:A defining quote from the book, ?An armed society is a polite society?,is very popular with those in the United States who support the personalright to bear arms.From this sentence TextRunner extracts the tuples:9(A defining quote) is a (polite society ?
),(the personal right) to bear (arms).We might manually translate this into a crude sort of logical form:IS-A(A-DEFINING-QUOTE, POLITE-SOCIETY-?
),TO-BEAR(THE-PERSONAL-RIGHT, ARMS).8(0,20,21,23,24,26,28,210,212), i.e., (0,1], (1,2], (2,8], ... .9Tuple arguments are enclosed in parenthesis, with the items recognized as head given in bold.
Allnon-enclosed, conjoining text makes up the tuple predicate.Open Knowledge Extraction through Compositional Language Processing 2470 2 4 6 8 10 1254321NaturalNumber of Classes (lg scale)AverageAssessmentjudge 1judge 2Figure 3: As a function of frequency of support, average assessment for propositionsderived from natural sentencesBetter would be to consider only those terms classified as head, and make the assump-tion that each tuple argument implicitly introduces its own quantified variable:?x,y.
QUOTE(x) & SOCIETY(y) & IS-A(x,y),?x,y.
RIGHT(x) & ARMS(y) & TO-BEAR(x,y).Compare this to the output of KNEXT:10?x.
SOCIETY(x) & POLITE(x),?x,y,z.
THING-REFERRED-TO(x)& COUNTRY(y) & EXEMPLAR-OF(z,y) & IN(x,z),?x.
RIGHT(x) & PERSONAL(x),?x,y.
QUOTE(x) & BOOK(y) & FROM(x,y),?x.
SOCIETY(x) & ARMED(x),which is automatically verbalized as:A SOCIETY CAN BE POLITE,A THING-REFERRED-TO CAN BE IN AN EXEMPLAR-OF A COUNTRY,A RIGHT CAN BE PERSONAL,A QUOTE CAN BE FROM A BOOK,A SOCIETY CAN BE ARMED.10For expository reasons, scoped, simplified versions of KNEXT?s ULFs are shown.
More accuratelypropositions are viewed as weak generic conditionals, with a non-zero lower bound on conditional fre-quency, e.g., [?x.
QUOTE(x)] ?0.1 [?y.
BOOK(y) & FROM(x,y)], where x is dynamically bound in theconsequent.248 Van Durme and Schubert0 2 4 6 8 10 1254321CoreNumber of Classes (lg scale)AverageAssessmentjudge 1judge 2Figure 4: As a function of frequency of support, average assessment for propositionsderived from core sentences4.2 Extended TuplesWhile KNEXT uniquely recognizes, e.g., adjectival modification and various types ofpossessive constructions, TextRunner more aggressively captures constructions withextended cardinality.
For example, from the following:James Harrington in The Commonwealth of Oceana uses the term anarchy todescribe a situation where the people use force to impose a government on aneconomic base composed of either solitary land ownership, or land in the owner-ship of a few.TextRunner extracts 19 tuples, some with three or even four arguments, thus aimingbeyond the binary relations that most current systems are limited to.
That so manytuples were extracted for a single sentence is explained by the fact that for most tuplescontaining N > 2 arguments, TextRunner will also output the same tuple with N?
1arguments, such as:(the people) use (force),(the people) use (force) to impose (a government),(the people) use (force) to impose (a government) on (an economic base).In addition, tuples may overlap, without one being a proper subset of another:Open Knowledge Extraction through Compositional Language Processing 249(a situation) where (the people) use (force),(force) to impose (a government),(a government) on (an economic base) composed of(either solitary land ownership).This overlap raises the question of how to accurately quantify system performance.Whenmeasuring average extraction quality, should samples be drawn randomly acrosstuples, or from originating sentences?
If from tuples, then sample sets will be biased(for good or ill) towards fragments derived from complex syntactic constructions.
Ifsentence based, the system fails to be rewarded for extracting as much from an inputas possible, as it may conservatively target only those constructions most likely to becorrect.
With regards to volume, it is not clear whether adjuncts should each giverise to additional facts added to a final total; optimal would be the recognition of suchoptionality.
Failing this, perhaps a tally may be based on unique predicate head terms?As a point of merit according to its designers, TextRunner does not utilize a parser(though as mentioned it does part of speech tagging and noun phrase chunking).
Thisis said to be justified in view of the known difficulties in reliably parsing open domaintext as well as the additional computational costs.
However, a serious consequenceof ignoring syntactic structure is that incorrect bracketing across clausal boundariesbecomes all too likely, as seen for instance in the following tuple:(James Harrington) uses (the term anarchy) to describe (a situation)where (the people),or in the earlier example where from the book, ?An armed society appears to have beenerroneously treated as a post-nominal modifier, intervening between the first argumentand the is-a predicate.KNEXT extracted the following six propositions, the first of which was automati-cally filtered in post-processing for being overly vague:11?
A MALE-INDIVIDUAL CAN BE IN A NAMED-ENTITY OF A NAMED-ENTITY,A MALE-INDIVIDUAL MAY USE A (TERM ANARCHY),PERSONS MAY USE FORCE,A BASE MAY BE COMPOSED IN SOME WAY,A BASE CAN BE ECONOMIC,A (LAND OWNERSHIP) CAN BE SOLITARY.5 Extracting from Core SentencesWe have noted the common argument against the use of syntactic analysis when per-forming large-scale extraction viz.
that it is too time consuming to be worthwhile.We are skeptical of such a view, but decided to investigate whether an argument-bracketing system such as TextRunner might be used as an extraction preprocessor tolimit what needed to be parsed.For each TextRunner tuple extracted from the sampled corpus, core sentences wereconstructed from the predicate and noun phrase arguments,12 which were then used asinput to KNEXT for extraction.11The authors judge the third, fifth and sixth propositions to be both well-formed and useful.12Minor automated heuristics were used to recover, e.g., missing articles dropped during tupleconstruction.250 Van Durme and SchubertFrom 6,053,981 tuples came an equivalent number of core sentences.
Note thatsince TextRunner tuples may overlap, use of these reconstructed sentences may lead toskewed propositional frequencies relative to ?normal?
text.
This bias was very muchin evidence in the fact that of the 10,507,573 propositions extracted from the coresentences, only 3,787,701 remained after automatic postprocessing and eliminationof duplicates.
This gives a per-sentence average of 0.63, as compared to 1.32 for theoriginal text.While the raw number of propositions extracted for each version of the underlyingdata look similar, 3,975,197 (natural) vs. 3,787,701 (core), the actual overlap was lessthan would be expected.
Just 2,163,377 propositions were extracted jointly from bothnatural and core sentences, representing a percent overlap of 54% and 57% respec-tively.Table 3: Mean judgements (lower is better) on propositions sampled from those sup-ported either exclusively by natural or core sentences, or those supported by bothNatural Core Overlapjudge 1 3.35 3.85 2.96judge 2 2.95 3.59 2.55Quality was evaluated by each judge assessing 240 randomly sampled propositionsfor each of: those extracted exclusively from natural sentences, those extracted ex-clusively from core sentences, those extracted from both (Table 3).
Results show thatpropositions exclusively derived from core sentences were most likely to be judgedpoorly.
Propositions obtained both by KNEXT alone and by KNEXT- processing ofTextRunner-derived core sentences (the overlap set) were particularly likely to bejudged favorably.On the one hand, many sentential fragments ignored by TextRunner yield KNEXTpropositions; on the other, TextRunner?s output may be assembled to produce sen-tences yielding propositions that KNEXT otherwise would have missed.
Ad-hoc anal-ysis suggests these new propositions derived with the help of TextRunner are a mixof noise stemming from bad tuples (usually a result of the aforementioned incorrectclausal bracketing), along with genuinely useful propositions coming from sentenceswith constructions such as appositives or conjunctive enumerations where TextRun-ner outguessed the syntactic parser as to the correct argument layout.
Future workmay consider whether (syntactic) language models can be used to help prune coresentences before being given to KNEXT.Figure 4 differs from Figure 3 at low frequency of support.
This is the result of thepartially redundant tuples extracted by TextRunner for complex sentences; the coreverb-argument structures are those most likely to be correctly interpreted by KNEXT,while also being those most likely to be repeated across tuples for the same sentence.6 Class PropertiesWhile TextRunner is perhaps the extraction system most closely related to KNEXTin terms of generality, there is also significant overlap with work on class attributeOpen Knowledge Extraction through Compositional Language Processing 251Table 4: By frequency, the top ten attributes a class MAY HAVE.
Emphasis added toentries overlapping with those reported by Pas?ca and Van Durme.
Results for starredclasses were derived without the use of prespecified lists of instancesCOUNTRY government, war, team, history, rest, coast,census, economy, population, independenceDRUG*side effects, influence, uses, doses,manufacturer, efficacy, release, graduates,plasma levels, safetyCITY* makeup, heart, center, population, history,side, places, name, edge, areaPAINTER* works, art, brush, skill, lives, sons,friend, order quantity, muse, eyeCOMPANY windows, products, word, page, review, film,team, award, studio, directorextraction.
Pas?ca and Van Durme (2007) recently described this task, going on todetail an approach for collecting such attributes from search engine query logs.
As anexample, the search query ?president of Spain?
suggests that a Country may have apresident.If one were to consider attributes to correspond, at least in part, to things a classMAY HAVE, CAN BE, or MAY BE, then a subset of KNEXT?s results may be dis-cussed in terms of this specialized task.
For example, for the five classes used in thoseauthors?
experiments, Table 4 contains the top ten most frequently extracted thingseach class MAY HAVE, as determined by KNEXT, without any targeted filtering oradaptation to the task.Table 5: Mean assessed acceptability for properties occurring for a single class (1),and more than a single class (2+).
Final column contains Pearson correlation scores1 2+ 1 2+ corr.MAY HAVE 2.80 2.35 2.50 2.28 0.68MAY BE 3.20 2.85 2.35 2.13 0.59CAN BE 3.78 3.58 3.28 2.75 0.76judge 1 judge 2For each of these three types of attributive categories the authors judged 80 ran-domly drawn propositions, constrained such that half (40 for each) were supported bya single sentence, while the other half were required only to have been extracted atleast twice, but potentially many hundreds or even thousands of times.
As seen in Ta-ble 5, the judges were strongly correlated in their assessments, where for MAY HAVEand MAY BE they were lukewarm (3.0) or better on the majority of those seen.In a separate evaluation judges considered whether the number of classes sharing agiven attribute was indicative of its acceptability.
For each unique attributive propo-252 Van Durme and Schubert0 2 4 6 8 10 1254321Number of Classes (lg scale)AverageAssessmentjudge 1judge 2Figure 5: Mean quality of class attributes as a function of the number of classes sharinga given propertysition the class in ?subject?
position was removed, leaving fragments such as thatbracketed: A ROBOT [CAN BE SUBHUMAN].
These attribute fragments were talliedand binned by frequency,13 with 40 then sampled from each.
For a given attributeselected, a single attributive proposition matching that fragment was randomly drawn.For example, having selected the attribute CAN BE FROM A US-CITY, the propositionSOME_NUMBER_OF SHERIFFS CAN BE FROM A US-CITY was drawn from the390 classes sharing this property.
As seen in Figure 5, acceptability rose as a propertybecame more common.7 ConclusionsWork such as TextRunner (Banko et al, 2007) is pushing extraction researchers toconsider larger and larger datasets.
This represents significant progress towards thegreater community?s goal of having access to large, expansive stores of general worldknowledge.The results presented here support the position that advances made over decadesof research in parsing and semantic interpretation do have a role to play in large-scale knowledge acquisition from text.
The price paid for linguistic processing is notexcessive, and an advantage is the logical formality of the results, and their versatility,as indicated by the application to class attribute extraction.13Ranges: (0,20,21,23,26,?
)Open Knowledge Extraction through Compositional Language Processing 253Acknowledgements We are especially grateful to Michele Banko and her colleaguesfor generously sharing results, and to Daniel Gildea for helpful feedback.
This workwas supported by NSF grants IIS-0328849 and IIS-0535105.ReferencesAbney, S. (1996).
Partial Parsing via Finite-State Cascades.
Natural Language Engi-neering 2(4), 337?344.Banko, M., M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni (2007).
OpenInformation Extraction from the Web.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI-07), pp.
2670?2676.Charniak, E. (1999).
A Maximum-Entropy-Inspired Parser.
In Proceedings of the 1stConference of the North American Chapter of the Association for ComputationalLinguistics (NAACL 2000), pp.
132?139.Clark, P., P. Harrison, and J. Thompson (2003).
A Knowledge-Driven Approach toText Meaning Processing.
In Proceedings of the HLT-NAACL 2003 Workshop onText Meaning, pp.
1?6.Clark, S. and D. Weir (1999).
An iterative approach to estimating frequencies overa semantic hierarchy.
In Proceedings of the 1999 Joint SIGDAT Conference onEmpirical Methods in Natural Language Processing and Very Large Corpora(EMNLP/VLC-99), pp.
258?265.Collins, M. (1997).
Three Generative, Lexicalised Models for Statistical Parsing.
InProceedings of the 35th Annual Conference of the Association for ComputationalLinguistics (ACL-97), pp.
16?23.Gildea, D. and D. Jurafsky (2002).
Automatic labeling of semantic roles.
Computa-tional Linguistics 28(3), 245?288.Gildea, D. and M. Palmer (2002).
The necessity of syntactic parsing for predicateargument recognition.
In Proceedings of the 40th Annual Conference of the Asso-ciation for Computational Linguistics (ACL-02), Philadelphia, PA, pp.
239?246.Holmback, H., L. Duncan, and P. Harrison (2000).
A word sense checking applica-tion for Simplified English.
In Proceedings of the 3rd International Workshop onControlled Language Applications (CLAW00), pp.
120?133.Liakata, M. and S. Pulman (2002).
From Trees to Predicate Argument Structures.In Proceedings of the 19th International Conference on Computational Linguistics(COLING-02), pp.
563?569.Pas?ca, M. and B.
Van Durme (2007).
What You Seek is What You Get: Extraction ofClass Attributes from Query Logs.
In Proceedings of the 20th International JointConference on Artificial Intelligence (IJCAI-07), pp.
2832?2837.254 Van Durme and SchubertPunyakanok, V., D. Roth, andW.
tau Yih (2008).
The Importance of Syntactic Parsingand Inference in Semantic Role Labeling.
Computational Linguistics 34(2), 257?287.Resnik, P. (1993).
Semantic classes and syntactic ambiguity.
In Proceedings of ARPAWorkshop on Human Language Technology, pp.
278?283.Richardson, S. D., W. B. Dolan, and L. Vanderwende (1998).
MindNet: Acquiringand Structuring Semantic Information from Text.
In Proceedings of the 17th Inter-national Conference on Computational linguistics (COLING-98), pp.
1098?1102.Schubert, L. K. (2002).
Can we derive general world knowledge from texts?
InProceedings of the 2nd International Conference on Human Language TechnologyResearch (HLT 2002), pp.
94?97.Schubert, L. K. and C. H. Hwang (2000).
Episodic Logic meets Little Red RidingHood: A comprehensive, natural representation for language understanding.
InL.
Iwanska and S. Shapiro (Eds.
), Natural Language Processing and KnowledgeRepresentation: Language for Knowledge and Knowledge for Language, pp.
111?174.Schubert, L. K. and M. H. Tong (2003).
Extracting and evaluating general worldknowledge from the Brown corpus.
In Proceedings of the HLT-NAACL 2003Work-shop on Text Meaning, pp.
7?13.Wong, Y. W. and R. J. Mooney (2007).
Learning Synchronous Grammars for Seman-tic Parsing with Lambda Calculus.
In Proceedings of the 45th Annual Conferenceof the Association for Computational Linguistics (ACL-07), pp.
960?967.Zelle, J. M. and R. J. Mooney (1996).
Learning to Parse Database Queries usingInductive Logic Programming.
In Proceedings of the 13th National Conference onArtificial Intelligence (AAAI-96), pp.
1050?1055.Zernik, U.
(1992).
Closed yesterday and closed minds: Asking the right questionsof the corpus to distinguish thematic from sentential relations.
In Proceedings ofthe 19th International Conference on Computational Linguistics (COLING-02), pp.1305?1311.Zettlemoyer, L. and M. Collins (2005).
Learning to Map Sentences to Logical Form:Structured Classification with Probabilistic Categorial Grammars.
In Proceedingsof the 21st Conference on Uncertainty in Artificial Intelligence (UAI-05), pp.
658?666.
