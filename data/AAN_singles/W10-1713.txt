Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103?109,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe RALI Machine Translation System for WMT 2010Ste?phane Huet, Julien Bourdaillet, Alexandre Patry and Philippe LanglaisRALI - Universite?
de Montre?alC.P.
6128, succursale Centre-villeH3C 3J7, Montre?al, Que?bec, Canada{huetstep,bourdaij,patryale,felipe}@iro.umontreal.caAbstractWe describe our system for the translationtask of WMT 2010.
This system, devel-oped for the English-French and French-English directions, is based on Moses andwas trained using only the resources sup-plied for the workshop.
We report exper-iments to enhance it with out-of-domainparallel corpora sub-sampling, N-best listpost-processing and a French grammaticalchecker.1 IntroductionThis paper presents the phrase-based machinetranslation system developed at RALI in orderto participate in both the French-English andEnglish-French translation tasks.
In these twotasks, we used all the corpora supplied for the con-straint data condition apart from the LDC Giga-word corpora.We describe its different components in Sec-tion 2.
Section 3 reports our experiments to sub-sample the available out-of-domain corpora in or-der to adapt the translation models to the newsdomain.
Section 4, dedicated to post-processing,presents how N-best lists are reranked and how theFrench 1-best output is corrected by a grammaticalchecker.
Section 5 studies how the original sourcelanguage of news acts upon translation quality.
Weconclude in Section 6.2 System Architecture2.1 Pre-processingThe available corpora were pre-processed usingan in-house script that normalizes quotes, dashes,spaces and ligatures.
We also reaccentuatedFrench words starting with a capital letter.
Wesignificantly cleaned up the parallel Giga wordcorpus (noted as gw hereafter), keeping 18.1 Mof the original 22.5 M sentence pairs.
For exam-ple, sentence pairs with numerous numbers, non-alphanumeric characters or words starting withcapital letters were removed.Moreover, training material was tokenized withthe tool provided for the workshop and truecased,meaning that the words occuring after a strongpunctuation mark were lowercased when they be-longed to a dictionary of common all-lowercasedforms; the others were left unchanged.
In orderto reduce the number of words unknown to thetranslation models, all numbers were serialized,i.e.
mapped to a special unique token.
The origi-nal numbers are then placed back in the translationin the same order as they appear in the source sen-tence.
Since translations are mostly monotonic be-tween French and English, this simple algorithmworks well most of the time.2.2 Language ModelsWe trained Kneser-Ney discounted 5-gram lan-guage models (LMs) on each available corpus us-ing the SRILM toolkit (Stolcke, 2002).
TheseLMs were combined through linear interpola-tion: first, an out-of-domain LM was built fromEuroparl, UN and gw; then, this model wascombined with the two in-domain LMs trainedon news-commentary and news.shuffled, whichwill be referred to as nc and ns in the remainderof the article.
Weights were fixed by optimizingthe perplexity of a development corpus made ofnews-test2008 and news-syscomb2009 texts.In order to reduce the size of the LMs, welimited the vocabulary of our models to 1 Mwords for English and French.
The words ofthese vocabularies were selected from the com-putation of the number of their occurences us-ing the method proposed by Venkataraman andWang (2003).
The out-of-vocabulary rate mea-sured on news-test2009 and news-test2010with a so-built vocabulary varies between 0.6 %103and 0.8 % for both English and French, while itwas between 0.4 % and 0.7 % before the vocabu-lary was pruned.To train the LM on the 48 M-sentence Englishns corpus, 32 Gb RAM were required and up to16 Gb RAM, for the other corpora.
To reduce thememory needs during decoding, LMs were prunedusing the SRILM prune option.2.3 Alignment and Translation ModelsAll parallel corpora were aligned withGiza++ (Och and Ney, 2003).
Our transla-tion models are phrase-based models (PBMs)built with Moses (Koehn et al, 2007) with thefollowing non-default settings:?
maximum sentence length of 80 words,?
limit on the number of phrase translationsloaded for each phrase fixed to 30.Weights of LM, phrase table and lexicalizedreordering model scores were optimized on thedevelopment corpus thanks to the MERT algo-rithm (Och, 2003).2.4 ExperimentsThis section reports experiments done on thenews-test2009 corpus for testing various config-urations.
In these first experiments, we trainedLMs and translation models on the Europarl cor-pus.Case We tested two methods to handle case.
Thefirst one lowercases all training data and docu-ments to translate, while the second one normal-izes all training data and documents into their nat-ural case.
These two methods require a post-processing recapitalization but this last step ismore basic for the truecase method.
Training mod-els on lowercased material led to a 23.15 % case-insensitive BLEU and a 21.61 % case-sensitiveBLEU; from truecased corpora, we obtained a23.24 % case-insensitive BLEU and a 22.13 %case-sensitive BLEU.
As truecasing induces an in-crease of the two metrics, we built all our mod-els in truecase.
The results shown in the remain-der of this paper are reported in terms of case-insensitive BLEU which showed last year a bet-ter correlation with human judgments than case-sensitive BLEU for the two languages we con-sider (Callison-Burch et al, 2009).Tokenization Two tokenizers were tested: oneprovided for the workshop and another we devel-oped.
They differ mainly in the processing of com-pound words: our in-house tokenizer splits thesewords (e.g.
percentage-wise is turned into percent-age - wise), which improves the lexical coverage ofthe models trained on the corpus.
This featuredoes not exist in the WMT tool.
However, us-ing the WMT tokenizer, we measured a 23.24 %BLEU, while our in-house tokenizer yielded alower BLEU of 22.85 %.
Follow these resultsprompted us to use the WMT tokenizer.Serialization In order to test the effect of se-rialization, i.e.
the mapping of all numbers toa special unique token, we measured the BLEUscore obtained by a PBM trained on Europarl forEnglish-French, when numbers are left unchanged(Table 1, line 1) or serialized (line 2).
Theseresults exhibit a slight decrease of BLEU whenserialization is performed.
Moreover, if BLEUis computed using a serialized reference (line 3),which is equivalent to ignoring deserialization er-rors, a minor gain of BLEU is observed, whichvalidates our recovering method.
Since resortingto serialization/deserialization yields comparableperformance to a system not using it, while reduc-ing the model?s size, we chose to use it.BLEUno serialization 23.24corpus serialization 23.13corpus and reference serialization 23.27Table 1: BLEU measured for English-French onnews-test2009 when training on Europarl.LM Table 2 reports the perplexity measured onnews-test2009 for French (column 1) and En-glish (column 3) LMs learned on different cor-pora and interpolated using the development cor-pus.
We also provide the BLEU score (column 2)for English-French obtained from translation mod-els trained on Europarl and nc.
As expected, us-ing in-domain corpora (line 2) for English-Frenchled to better results than using out-of-domain data(line 3).
The best perplexities and BLEU scoreare obtained when LMs trained on all the availablecorpora are combined (line 4).
The last three linesexhibit how LMs perform when they are trained onin-domain corpora without pruning them.
Whilethe gzipped 5-gram LM (last line) obtained in104such a manner occupies 1.4 Gb on hard disk, thegzipped pruned 5-gram LM (line 4) trained usingall corpora occupies 0.9 Gb and yields the sameBLEU score.
This last LM was used in all the ex-periments reported in the subsequent sections.corporaFr Enppl BLEU pplnc 327 22.44 454nc + ns 125 25.69 166Europarl + UN + Gw 156 24.91 225all corpora 113 26.01 151nc + ns (3g, unpruned) 138 25.32 -nc + ns (4g, unpruned) 124 25.86 -nc + ns (5g, unpruned) 120 26.04 -Table 2: LMs perplexities and BLEU scores mea-sured on news-test2009.
Translation modelsused here were trained on nc and Europarl.3 Domain adaptationAs the only news parallel corpus provided forthe workshop contains 85k sentence pairs, wemust resort to other parallel out-of-domain cor-pora in order to build reliable translation models.If in-domain and out-of-domain LMs are usuallymixed with the well-studied interpolation tech-niques, training translation models from data ofdifferent domains has received less attention (Fos-ter and Kuhn, 2007; Bertoldi and Federico, 2009).Therefore, there is still no widely accepted tech-nique for this last purpose.3.1 Effects of the training data sizeWe investigated how increasing training data actsupon BLEU score.
Table 3 shows a high increaseof 2.7 points w.r.t.
the use of nc alone (line 1)when building the phrase table and the reorderingmodel from nc and either the 1.7 M-sentence-pairEuroparl (line 2) or a 1.7 M-sentence-pair cor-pus extracted from the 3 out-of-domain corpora:Europarl, UN and Gw (line 3).
Training a PBM onmerged parallel corpora is not necessarily the bestway to combine data from different domains.
Werepeated 20 times nc before adding it to Europarlso as to have the same amount of out-of-domainand in-domain material.
This method turned outto be less successful since it led to a minor 0.15BLEU decrease (line 4) w.r.t.
our previous system.Following the motto ?no data is better than morecorpora En?Fr Fr?Ennc 23.29 23.23nc + Europarl 26.01 -nc + 1.7 M random pairs 26.02 26.6820?nc + Europarl 25.86 -nc + 8.7 M pairs (part 0) 26.44 27.65nc + 8.7 M pairs (part 1) 26.68 27.46nc + 8.7 M pairs (part 2) 26.54 27.503 models merged 26.86 27.56Table 3: BLEU (in %) measured on news-test2009 for English-French and French-Englishwhen translations models and lexicalized reorder-ing models are built using various amount of datain addition to nc.data?, a PBM was built using all the parallel cor-pora at our disposal.
Since the overall parallel sen-tences were too numerous for our computationalresources to be simultaneously used, we randomlysplit out-of-domain corpora into 3 parts of 8.7 Msentence pairs each and then combined them withnc.
PBMs were trained on each of these parts(lines 5 to 7), which yields respectively 0.5 and0.8 BLEU gain for English-French and French-English w.r.t.
the use of 1.7 M out-of-domain sen-tence pairs.
The more significant improvement no-ticed for the French-English direction is probablyexplained by the fact that the French language ismorphologically richer than English.
The 3 PBMswere then combined by merging the 3 phrase ta-bles.
To do so, the 5 phrase table scores computedby Moses were mixed using the geometric averageand a 6th score was added, which counts the num-ber of phrase tables where the given phrase pairoccurs.
We ended up with a phrase table contain-ing 623 M entries, only 9 % and 4 % of them beingin 2 and 3 tables respectively.
The resulting phrasetable led to a slight improvement of BLEU scores(last line) w.r.t.
the previous models, except for themodel trained on part 0 for French-English.3.2 Corpus sub-samplingWhereas using all corpora improves translationquality, it requires a huge amount of memory anddisk space.
We investigate in this section ways toselect sentence pairs among large out-of-domaincorpora.Unknown words The main interest of addingnew training material relies on the finding ofwords missing in the phrase table.
According to105this principle, nc was extended with new sentencepairs containing an unknown word (Table 4, line 2)or a word that belongs to our LM vocabulary andthat occurs less than 3 times in the current cor-pus (line 3).
This resulted in adding 400 k pairsin the first case and 950 k in the second one, withBLEU scores close or even better than those ob-tained with 1.7 M.corpora En?Fr Fr?Ennc + 1.7 M random pairs 26.02 26.68nc + 400k pairs (occ = 1) 25.67 -nc + 950k pairs (occ = 3) 26.13 -nc + Joshua sub-sampling 26.98 27.68nc + IR (1-g q, w/ repet) 25.81 -nc + IR (1-g q, no repet) 26.56 27.54nc + IR (1,2-g q, w/ repet) 26.26 -nc + IR (1,2-g q, no repet) 26.53 -nc + 8.7 M pairs 26.68 27.65+ IR score (1g q, no repet) 26.93 27.653 large models merged 26.86 27.56+ IR score (1g q, no repet) 26.98 27.74Table 4: BLEU measured on news-test2009 forEnglish-French and French-English using transla-tion models trained on nc and a subset of out-of-domain corpora.Unknown n-grams We applied the sub-sampling method available in the Joshuatoolkit (Li et al, 2009).
This method adds anew sentence pair when it contains new n-grams(with 1 ?
n ?
12) occurring less than 20 times inthe current corpus, which led us to add 1.5 M pairsfor English-French and 1.4 M for French-English.A significant improvement of BLEU is observedusing this method (0.8 for English-French and1.0 for French-English) w.r.t.
the use of 1.7 Mrandomly selected pairs.
However, this methodhas the major drawback of needing to build a newphrase table for each document to translate.Information retrieval Information retrieval(IR) methods have been used in the past to sub-sample parallel corpora (Hildebrand et al, 2005;Lu?
et al, 2007).
These studies use sentencesbelonging to the development and test corpora asqueries to select the k most similar source sen-tences in an indexed parallel corpus.
The retrievedsentence pairs constitute a training corpus forthe translation models.
In order to alleviate thefact that a new PBM has to be learned for eachnew test corpus, we built queries using sentencescontained in the monolingual ns corpus, leadingto the selection of sentence pairs stylisticallyclose to those in the news domain.
The sourcesentences of the three out-of-domain corporawere indexed using Lemur.1 Two types of querieswere built from ns sentences after removing stopwords: the first one is limited to unigrams, thesecond one contains both unigrams and bigrams,with a weight for bigrams twice as high as forunigrams.
The interest of the latter query type isbased on the hypothesis that bigrams are moredomain-dependent than unigrams.
Another choicethat needs to be made when using IR methods isconcerning the retention of redundant sentencesin the final corpus.Lines 5 to 8 of Table 4 show the results obtainedwhen sentence pairs were gathered up to the sizeof Europarl, i.e.
1.7 M pairs.
10 sentences wereretrieved per query in various configurations: withor without bigrams inside queries, with or withoutduplicate sentence pairs in the training corpus.
Re-sults demonstrate the interest of the approach sincethe BLEU scores are close to those obtained us-ing the previous tested method based on n-gramsof the test data.
Taking bigrams into account doesnot improve results and adding only once new sen-tences is more relevant than duplicating them.Since using all data led to even better perfor-mances (see last line of Table 3), we used infor-mation provided by the IR method in the PBMstrained on nc + 8.7 M out-of-domain sentencepairs or taking into account all the training ma-terial.
To this end, we included a new score inthe phrase tables which is fixed to 1 for entriesthat are in the phrase table trained on sentencesretrieved with unigram queries without repetition(see line 6 of Table 4), and 0 otherwise.
Therefore,this score aims at boosting the weight of phrasesthat were found in sentences close to the news do-main.
The results reported in the 4 last lines of Ta-ble 4 show minor but consistent gains when addingthis score.
The outputs of the PBMs trained onall the training corpus and which obtained the bestBLEU scores on news-test2009 were submittedas contrastive runs.
The two first lines of Table 5report the results on this years?s test data, whenthe score related to the retrieved corpus is incor-porated or not.
These results still exhibit a minorimprovement when adding this score.1www.lemurproject.org106En?Fr Fr?EnBLEU BLEU-cased TER BLEU BLEU-cased TERPBM 27.5 26.5 62.2 27.8 26.9 61.2+IR score 27.7 26.6 62.1 28.0 27.0 61.0+N-best list reranking 27.9 26.8 62.1 28.0 27.0 61.2+grammatical checker 28.0 26.9 62.0 - - -Table 5: Official results of our system on news-test2010.4 Post-processing4.1 N-best List RerankingOur best PBM enhanced by IR methods was em-ployed to generate 500-best lists.
These lists werereranked combining the global decoder score withthe length ratio between source and target sen-tences, and the proportions of source sentence n-grams that are in the news monolingual corpora(with 1 ?
n ?
5).
Weights of these 7 scores areoptimized via MERT on news-test2009.
Lines 2and 3 of Table 5 provide the results obtained be-fore and after N-best list reranking.
They show atiny gain for all metrics for English-French, whilethe results remain constant for French-English.Nevertheless, we decided to use those translationsfor the French-English task as our primary run.4.2 Grammatical CheckerPBM outputs contain a significant number ofgrammatical errors, even when LMs are trainedon large data sets.
We tested the use of a gram-matical checker for the French language: AntidoteRX distributed by Druide informatique inc.2 Thissoftware was applied in a systematic way on thefirst translation generated after N-best reranking.Thus, as soon as the software suggests one or sev-eral choices that it considers as more correct thanthe original translation, the first proposal is kept.The checked translation is our first run for English-French.Antidote RX changed at least one word in26 % of the news-test2010 sentences.
The mostfrequent type of corrections are agreement errors,like in the following example where the agreementbetween the subject nombre (number) is correctlymade with the adjective coupe?
(cut), thanks to thefull syntactic parsing of the French sentence.Source: [...] the number of revaccinations could then becut [...]Reranking: [...] le nombre de revaccinations pourrait2www.druide.comalors e?tre coupe?es [...]+Grammatical checker: [...] le nombre de revacci-nations pourrait alors e?tre coupe?
[...]The example below exhibits a good decisionmade by the grammatical checker on the mood ofthe French verb e?tre (to be).Source: It will be a long time before anything else will beon offer in Iraq.Reranking: Il faudra beaucoup de temps avant que toutle reste sera offert en Irak.+Grammatical checker: Il faudra beaucoup de tempsavant que tout le reste soit offert en Irak.A last interesting type of corrected errors con-cerns negation.
Antidote has indeed the capacityto add the French particle ne when it is missing inthe expressions ne ... pas, ne ... plus, aucun ne, per-sonne ne or rien ne.
The results obtained using thegrammatical checker are reported in the last lineof Table 5.
The automatic evaluation shows only aminor improvement but we expect the changes in-duced by this tool to be more significant for humanannotators.5 Effects of the Original SourceLanguage of Articles on TranslationDuring our experiments, we found that translationquality is highly variable depending on the origi-nal source language of the news sentences.
Thisphenomenon is correlated to the previous work ofKurokawa et al (2009) that showed that whetheror not a piece of text is an original or a trans-lation has an impact on translation performance.The main reason that explains our observationsis probably that the topics and the vocabulary ofnews originally expressed in languages other thanFrench and English tend to differ more from thoseof the training materials used to train PBM mod-els for these two languages.
In order to take intoaccount this phenomenon, MERT tuning was re-peated for each original source language, using the107same PBM models trained on all parallel corporaand incorporating an IR score.Columns 1 and 3 of Table 5 display the BLEUmeasured using our previous global MERT op-timization made on 2553 sentence pairs, whilecolumns 2 and 4 show the results obtained whenrunning MERT on subsets of the development ma-terial, made of around 700 sentence pairs each.The BLEU measured on the whole 2010 test setis reported in the last line.
As expected, language-dependent MERT tends to increase the LM weightfor English and French.
However, an absolute0.35 % BLEU decrease is globally observed forEnglish-French using this approach and a 0.21 %improvement for French-English.En?Fr Fr?EnMERT global lang dep global lang depCz 21.95 21.45 21.84 21.85En 30.80 29.84 33.73 35.00Fr 37.59 36.96 31.59 32.62De 16.60 16.73 17.41 17.76Es 24.52 24.45 29.25 28.31total 27.64 27.39 27.99 28.20Table 6: BLEU scores measured on parts ofnews-test2010 according to the original sourcelanguage.6 ConclusionThis paper presented our statistical machine trans-lation system developed for the translation task us-ing Moses.
Our submitted runs were generatedfrom models trained on all the corpora made avail-able for the workshop, as this method had pro-vided the best results in our experiments.
Thissystem was enhanced using IR methods whichexploits news monolingual copora, N-best listreranking and a French grammatical checker.This was our first participation where such ahuge amount data was involved.
Training modelson so many sentences is challenging from an engi-neering point of view and requires important com-putational resources and storage capacities.
Thetime spent in handling voluminous data preventedus from testing more approaches.
We suggest thatthe next edition of the workshop could integratea task restraining the number of parameters in themodels trained.ReferencesNicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In 4th EACL Workshopon Statistical Machine Translation (WMT), Athens,Greece.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009workshop on statistical machine translation.
In 4thEACL Workshop on Statistical Machine Translation(WMT), Athens, Greece.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In 2nd ACL Workshopon Statistical Machine Translation (WMT), Prague,Czech Republic.Almut Silja Hildebrand, Matthias Eck, Stephan Vo-gel, and Alex Waibel.
2005.
Adaptation of thetranslation model for statistical machine translationbased on information retrieval.
In 10th conferenceof the European Association for Machine Transla-tion (EAMT), Budapest, Hungary.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In 45th Annual Meeting of the Association forComputational Linguistics (ACL), Companion Vol-ume, Prague, Czech Republic.David Kurokawa, Cyril Goutte, and Pierre Isabelle.2009.
Automatic detection of translated text andits impact on machine translation.
In 12th MachineTranslation Summit, Ottawa, Canada.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F.Zaidan.
2009.
Joshua: An open source toolkitfor parsing-based machine translation.
In 4thEACL Workshop on Statistical Machine Translation(WMT), Athens, Greece.Yajuan Lu?, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by train-ing data selection and optimization.
In Join Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), Prague, Czech Repub-lic.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In 41st Annual Meet-ing of the Association for Computational Linguistics(ACL), Sapporo, Japan.108Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In 7th International Con-ference on Spoken Language Processing (ICSLP),Denver, CO, USA.Arnand Venkataraman and Wen Wang.
2003.
Tech-niques for effective vocabulary selection.
In 8th Eu-ropean Conference on Speech Communication andTechnology (Eurospeech), Geneva, Switzerland.109
