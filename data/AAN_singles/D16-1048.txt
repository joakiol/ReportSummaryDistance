Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 501?510,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAutomatic Cross-Lingual Similarization of Dependency Grammarsfor Tree-based Machine TranslationWenbin Jiang 1 and Wen Zhang 1 and Jinan Xu 2 and Rangjia Cai 31Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences, China2School of Computer and Information Technology, Beijing Jiaotong University, China3Research Center of Tibetan Information, Qinghai Normal University, Chinajiangwenbin@ict.ac.cnAbstractStructural isomorphism between languagesbenefits the performance of cross-lingual ap-plications.
We propose an automatic al-gorithm for cross-lingual similarization ofdependency grammars, which automaticallylearns grammars with high cross-lingual sim-ilarity.
The algorithm similarizes the an-notation styles of the dependency grammarsfor two languages in the level of classifica-tion decisions, and gradually improves thecross-lingual similarity without losing linguis-tic knowledge resorting to iterative cross-lingual cooperative learning.
The dependencygrammars given by cross-lingual similariza-tion have much higher cross-lingual similar-ity while maintaining non-triviality.
As appli-cations, the cross-lingually similarized gram-mars significantly improve the performance ofdependency tree-based machine translation.1 IntroductionDue to the inherent syntactic regularity of eachlanguage and the discrepancy between annotationguidelines of linguists, there is not necessarily struc-tural isomorphism between grammars of differentlanguages.
For many cross-lingual scenarios suchas information retrieval and machine translation, re-lationships between linguistic units are expected tobe (at least roughly) consistent across languages(Hwa et al, 2002; Smith and Eisner, 2009).
Forcross-lingual applications, syntactic structures withhigh cross-lingual similarity facilitates knowledgeextraction, feature representation and classificationdecision.
The structural isomorphism between lan-guages, therefore, is an important aspect for the per-formance of cross-lingual applications such as ma-chine translation.To achieve effective cross-lingual similarizationfor two grammars in different languages, an ad-equate algorithm should both improve the cross-lingual similarity between two grammars and main-tain the non-triviality of each grammar, where non-triviality indicates that the resulted grammars shouldnot give flat or single-branched outputs.
Differ-ent from constituency structures, dependency struc-tures are lexicalized without specialized hierarchicalstructures.
Such concise structures depict the syn-tactic or semantic relationships between words, andthus have advantage on many cross-lingual scenar-ios.
It is worth to perform cross-lingual similariza-tion for dependency grammars, but the special prop-erty of dependency grammars makes it hard to di-rectly adopt the conventional structure transforma-tion methods resorting to hand-crafted rules or tem-plates.Both graph-based models (McDonald et al,2005) and transition-based models (Nivre et al,2006) factorize dependency parsing into fundamen-tal classification decisions, that is, the relation-ships between words or the actions applied to cur-rent states.
We assume that cross-lingual simi-larization can also be factorized into fundamen-tal classification decisions, and propose an au-tomatic cross-lingual similarization algorithm fordependency grammars according to this assump-tion.
The algorithm conducts cross-lingual sim-ilarization on the level of classification decisions501with simple blending operations rather than on thelevel of syntactic structures with complicated hand-crafted rules or templates, and adopts iterative cross-lingual collaborative learning to gradually improvethe cross-lingual similarity while maintaining thenon-triviality of grammars.We design an evaluation metric for the cross-lingual similarity of dependency grammars, whichcalculates the consistency degree of dependency re-lationships across languages.
We also propose aneffective method to measure the real performance ofthe cross-lingually similarized grammars based onthe transfer learning methodology (Pan and Yang,2010).
We validate the method on the dependencygrammar induction of Chinese and English, wheresignificant increment of cross-lingual similarity isachieved without losing non-triviality of the gram-mars.
As applications, the cross-lingually simi-larized grammars gain significant performance im-provement for the dependency tree-based machinetranslation by simply replacing the parser of thetranslator.2 Graph-based Dependency ParsingDependency parsing aims to link each word to itsarguments so as to form a directed graph spanningthe whole sentence.
Normally the directed graph isrestricted to a dependency tree where each word de-pends on exactly one parent, and all words find theirparents.
Given a sentence as a sequence n words:x = x1 x2 .. xndependency parsing finds a dependency tree y,where (i, j) ?
y is an edge from the head word xito the modifier word xj .
The root r ?
x in the treey has no head word, and each of the other words,j(j ?
x and j 6= r), depends on a head wordi(i ?
x and i 6= j).Following the edge-based factorization method(Eisner, 1996), the score of a dependency tree can befactorized into the dependency edges in the tree.
Thegraph-based method (McDonald et al, 2005) factor-izes the score of the tree as the sum of the scores ofall its edges, and the score of an edge is defined asthe inner product of the feature vector and the weightvector.
Given a sentence x, the parsing proceduresearches for the candidate dependency tree with themaximum score:y(x) = argmaxy?GEN(x)S(y)= argmaxy?GEN(x)?
(i,j)?yS(i, j) (1)Here, the function GEN indicates the enumer-ation of candidate trees.
The MIRA algorithm(Crammer et al, 2003) is used to train the parametervector.
A bottom-up dynamic programming algo-rithm is designed for projective parsing which givesprojective parsing trees, and the Chu-Liu-Edmondsalgorithm for non-projective parsing which givesnon-projective parsing trees.3 Cross-Lingual SimilarizationSince structural analysis can be factorized into fun-damental classification decisions, we assume thatthe adjustment of the analysis results can be fac-torized into the adjustment of the fundamental de-cisions.
The classification decision for graph-baseddependency parsing is to classify the dependency re-lationship between each pair of words, and we hopeit works well to conduct cross-lingual similariza-tion on the level of dependency relationship classifi-cation.
In this work, we investigate the automaticcross-lingual similarization for dependency gram-mars on the level of fundamental classification de-cisions, to avoid the difficulty of using hand-craftedtransformation rules or templates.In this section, we first introduce the evalua-tion metric for cross-lingual similarity, then describethe automatic cross-lingual similarization algorithm,and finally give a method to measure the real perfor-mance of the cross-lingually similarized grammars.3.1 Evaluation of Cross-Lingual SimilarityThe cross-lingual similarity between two depen-dency structures can be automatically evaluated.Dependency parsing is conducted on sentences, sowe take bilingual sentence pairs as the objects forevaluation.
The calculation of cross-lingual similar-ity needs the lexical alignment information betweentwo languages, which can be obtained by manual an-notation or unsupervised algorithms.Given a bilingual sentence pair x?
and x?
, theirdependency structures y?
and y?
, and the word502alignment probabilities A, the cross-lingual similar-ity can be calculated as below:d(y?, y?)
=?(i,j)?y??(i?,j?)?y?
Ai,i?Aj,j??(i,j)?y??i?,j??x?
Ai,i?Aj,j?
(2)The bracketed word pair indicates a dependencyedge.
The evaluation metric is a real number be-tween 0 and 1, indicating the degree of cross-lingual consistency between two dependency struc-tures.
For the cross-lingual similarity between bilin-gual paragraphs, we simply define it as the averageover the similarity between each sentence pairs.3.2 Factorized Cooperative SimilarizationThe fundamental decisions for graph-based depen-dency parsing are to evaluate the candidate depen-dency edges.
The cross-lingual similarization forfundamental decisions can be defined as some kindsof blending calculation on two evaluation scores,of which the one is directly given by the grammarof the current language (current grammar), and theother is bilingually projected from the grammar ofthe reference language (reference grammar).For the words i and j in the sentence x?
in thecurrent language, their evaluated score given by thecurrent grammar is S?
(i, j), which can be calculatedaccording to formula 1.
The score bilingually pro-jected from the reference grammar, S?
(i, j), can beobtained according to the translation sentence x?
inthe reference language and the word alignment be-tween two sentences:S?
(i, j) =?i?,j??x?Ai,i?Aj,j?S?
(i?, j?)
(3)where i?
and j?
are the corresponding words of i andj in the reference sentence x?
, Ai,j indicates theprobability that i aligns to j, and S?
(i?, j?)
is theevaluated score of the candidate edge (i?, j?)
givenby the reference grammar.Given the two evaluated scores, we simply adoptthe linear weighted summation:S??
(i, j) = (1?
?)S?
(i, j) + ?S?
(i, j) (4)where ?
is the relative weight to control the degreeof cross-lingual similarization, indicating to whichdegree we consider the decisions of the referencegrammar when adjusting the decisions of the currentgrammar.
We have to choose a value for ?
to achievean appropriate speed for effective cross-lingual sim-ilarization, in order to obtain similarized grammarswith high cross-lingual similarity while maintainingthe non-triviality of the grammars.In the re-evaluated full-connected graph, the de-coding algorithm searches for the cross-linguallysimilarized dependency structures, which are usedto re-train the dependency grammars.
Based on thecross-lingual similarization strategy, iterative coop-erative learning simultaneously similarizes the sen-tences in the current and reference languages, andgradually improves the cross-lingual similarity be-tween two grammars while maintaining the non-triviality of each monolingual grammar.
The wholetraining algorithm is shown in Algorithm 1.
Toreduce the computation complexity, we choose thesame ?
for the cross-lingual similarization for boththe current and the reference grammars.
Anotherhyper-parameter for the iterative cooperative learn-ing algorithm is the maximum training iteration,which can be determined according to the perfor-mance on the development sets.3.3 Evaluation of Similarized GrammarsThe real performance of a cross-lingually similar-ized grammar is hard to directly measured.
The ac-curacy on the standard testing sets no longer reflectsthe actrual accuracy, since cross-lingual similariza-tion leads to grammars with annotation styles differ-ent from those of the original treebanks.
We adoptthe transfer learning strategy to automatically adaptthe divergence between different annotation styles,and design a transfer classifier to transform the de-pendency regularities from one annotation style toanother.The training procedure of the transfer classifier isanalogous to the training of a normal classifier ex-cept for the features.
The transfer classifier adoptsguiding features where a guiding signal is attachedto the tail of each normal feature.
The guiding sig-nal is the dependency path between the pair of wordsin the source annotations, as shown in Figure 2.Thus, the transfer classifier learns the statistical reg-ularity of the transformation from the annotations ofthe cross-lingually similarized grammar to the an-notations of the original treebank.
Figure 1 shows503Algorithm 1 Cooperative cross-lingual similarization.1: function BISIMILARIZE(G?, G?
, ?, C) .
C includes a set of sentence pairs (x?, x?
)2: repeat3: T?,T?
?
BIANNOTATE(G?,G?, ?,C) .
it invokes BIPARSE to parse each (x?, x?
)4: G?
?
GRAMMARTRAIN(T?
)5: G?
?
GRAMMARTRAIN(T?
)6: until SIMILARITY(G?,G?)
converges .
according to formula 2, averaged across C7: return G?, G?8: function BIPARSE(G?, G?
, ?, x?, x?
, A)9: y?
?
argmaxy(1?
?)S?
(y) + ?S?
(y) .
according to formula 410: y?
?
argmaxy(1?
?)S?
(y) + ?S?
(y)11: return y?, y?source corpustrain withnormal featuressource classifiertrain withguiding featurestransfer classifiertarget corpus transformedtarget corpusFigure 1: The training procedure of the transfer classifier.the training pipeline for the transfer classifier, wheresource corpus and target corpus indicate the cross-lingually similarized treebank and the manually an-notated treebank, respectively.In decoding, a sentence is first parsed bythe cross-lingually similarized grammar, and thenparsed by the transfer classifier with the result ofthe similarized grammar as guiding signals to obtainthe final parsing results.
The improvement achievedby the transfer classifier against a normal classifiertrained only on the original treebank reflects thepromotion effect of the cross-lingually similarizedgrammar.
The accuracy of the transfer classifier,therefore, roughly indicates the real performance ofthe cross-lingually similarized grammar.Figure 2: The guiding signal for dependency parsing, wherepath(i, j) denotes the dependency path between i and j.
In thisexample, j is a son of the great-grandfather of i.4 Tree-based Machine TranslationSyntax-based machine translation investigates thehierarchical structures of natural languages, includ-ing formal structures (Chiang, 2005), constituencystructures (Galley et al, 2006; Liu et al, 2006;Huang et al, 2006; Mi et al, 2008) and dependencystructures (Lin, 2004; Quirk et al, 2005; Ding andPalmer, 2005; Xiong et al, 2007; Shen et al, 2008;Xie et al, 2011), so the performance is restricted tothe quality and suitability of the parsers.
Since thetrees for training follow an annotation style not nec-essarily isomorphic to that of the target language, itwould be not appropriate for syntax-based transla-tion to directly use the parsers trained on the origi-nal treebanks.
The cross-lingually similarized gram-mars, although performing poorly on a standard test-ing set, may be well suitable for syntax-based ma-chine translation.
In this work, we use the cross-lingually similarized dependency grammars in de-pendency tree-to-string machine translation (Xie etal., 2011), a state-of-the-art translation model resort-ing to dependency trees on the source side.504Treebank Train Develop Test1-270CTB 400-931 301-325 271-3001001-1151WSJ 02-21 22 23Table 1: Data partitioning for CTB and WSJ, in unit of section.5 Experiments and AnalysisWe first introduce the dependency parsing itself,then describe the cross-lingual similarization, andfinally show the application of cross-lingually simi-larized grammars in tree-based machine translation.For convenience of description, a grammar trainedby the conventional dependency model is namedas original grammar, a grammar after cross-lingualsimilarization is named as similarized grammar, andthe transferred version for a similarized grammar isnamed as adapted grammar.5.1 Dependency ParsingWe take Chinese dependency parsing as a case study,and experiment on Penn Chinese Treebank (CTB)(Xue et al, 2005).
The dependency structures areextracted from the original constituency trees ac-cording to the head-selection rules (Yamada andMatsumoto, 2003).
The partitioning of the datasetis listed in the Table 1, where we also give the parti-tioning of Wall Street Journal (WSJ) (Marcus et al,1993) used to train the English grammar.
The eval-uation metric for dependency parsing is unlabeledaccuracy, indicating the proportion of the words cor-rectly finding their parents.
The MIRA algorithm isused to train the classifiers.Figure 3 gives the performance curves on the de-velopment set with two searching modes, projectivesearching and non-projective searching.
The curvesshow that the non-projective searching mode fall be-hind of the projective one, this is because the depen-dency structures extracted from constituency treesare projective, and the projective search mode im-plies appropriate constraints on the searching space.Therefore, we use the projective searching mode forthe evaluation of the original grammar.
Table 2 liststhe performance of the original grammar on the CTBtesting set.1 2 3 4 5 6 7 8 9 10training iteration838485868788dependencyaccuracyprojective parsingnon-projective parsingFigure 3: The developing curves of Chinese dependency pars-ing with both projective and non-projective searching modes.5.2 Cross-Lingual SimilarizationThe experiments of cross-lingual similarization areconducted between Chinese and English, with FBISChinese-English dataset as bilingual corpus.
TheChinese sentences are segmented into words withthe character classification model (Ng and Low,2004), which is trained by MIRA on CTB.
The wordsequences of both languages are labeled with part-of-speech tags with the maximum entropy hiddenmarkov model (Ratnaparkhi and Adwait, 1996),which is reimplemented with MIRA and trained onCTB and WSJ.
The word alignment information isobtained by summing and normalizing the 10 bestcandidate word alignment results of GIZA++ (Ochand Ney, 2003).The upmost configuration for cross-lingual sim-ilarization is the searching mode.
On the Chineseside, both projective and non-projective modes canbe adopted.
For English, there is an additionalfixed mode besides the previous two.
In the fixedmode, the English dependency grammar remains un-changed during the whole learning procedure.
Thefixed mode on the English side means a degeneratedversion of cross-lingual similarization, where onlythe Chinese grammars are revolved during training.The combination of the searching modes for bothlanguages results in a total of 6 kinds of searchingconfigurations.
For each configuration, the learningalgorithm for cross-lingual similarization has twohyper-parameters, the coefficient ?
and maximum it-eration for iterative learning, which should be tunedfirst.5050 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.1354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.2354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.3354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.4354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.5354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.6354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.7354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.8354045505560650 1 2 3 4 5 6 7 8 9 105060708090lambda = 0.935404550556065Figure 4: The developing curves of cross-lingual similarization with projective searching on both languages.
X-axis: trainingiteration; Left Y-axis: parsing accuracy; Right Y-axis: cross-lingual similarity.
Thin dash-dotted line (gray): accuracy of thebaseline grammar; Thin dashed line (green): direct accuracy of cross-lingually similarized grammars; Thin solid line (red): adaptiveaccuracy of cross-lingually similarized grammars; Thick sold line (blue): the cross-lingual similarity of grammars.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.95060708090proj : fixed354045505560650.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.95060708090proj : proj354045505560650.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.95060708090proj : nonproj354045505560650.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.95060708090nonproj : fixed354045505560650.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.95060708090nonproj : proj354045505560650.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.95060708090nonproj : nonproj35404550556065Figure 5: The developing curves of cross-lingual similarization with all searching configurations.
X-axis: coefficient ?
; LeftY-axis: parsing accuracy; Right Y-axis: cross-lingual similarity.
The lines indicate the same as in Figure 4.5065.2.1 Determination of Hyper-ParametersWe select a subset of 40,000 sentence pairs outof the FBIS dataset, and use it as the smaller bilin-gual corpus to tune the hyper-parameters.
For thecoefficient ?
we try from 0.1 to 0.9 with step 0.1;and for the iterative learning we simply set the max-imum iteration as 10.
The developing procedureresults in a series of grammars.
For the configura-tion with projective searching modes on both sides,a total of 90 pairs of Chinese and English gram-mars are generated.
We use three indicators to vali-date each similarized grammar generated in the de-veloping procedure, including the performance onthe similarized grammar itself (direct accuracy), theperformance of the corresponding adapted grammar(adaptive accuracy), and the cross-lingual similar-ity between the similarized grammar and its Englishcounterpart.
Figure 4 shows the developing curvesfor the configuration with projective searching onboth sides.
With the fixed maximum iteration 10,we draw the developing curves for the other search-ing configurations with respect to the weight coeffi-cient, as shown in Figure 5.We find that the optimal performance is alsoachieved at 0.6 in most situations.
In all configu-rations, the training procedures increase the cross-lingual similarity of grammars.
Along with the in-crement of cross-lingual similarity, the direct accu-racy of the similarized grammars on the develop-ment set decreases, but the adaptive accuracy givenby the corresponding adapted grammars approach tothat of the original grammars.
Note that the projec-tive searching mode is adopted for the evaluation ofthe adapted grammar.5.2.2 Selection of Searching ModesWith the hyper-parameters given by the develop-ing procedures, cross-lingual similarization is con-ducted on the whole FBIS dataset.
All the searchingmode configurations are tried and 6 pairs of gram-mars are generated.
For each of the 6 Chinese de-pendency grammars, we also give the three indi-cators as described before.
Table 2 shows that,cross-lingual similarization results in grammars withmuch higher cross-lingual similarity, and the adap-tive accuracies given by the adapted grammars ap-proach to those of the original grammars.
It indi-cates that the proposed algorithm improve the cross-lingual similarity without losing syntactic knowl-edge.To determine the best searching mode for tree-based machine translation, we use the Chinese-English FBIS dataset as the small-scale bilingualcorpus.
A 4-gram language model is trained onthe Xinhua portion of the Gigaword corpus withthe SRILM toolkit (Stolcke and Andreas, 2002).For the analysis given by non-projective similarizedgrammars, The projective transformation should beconducted in order to produce projective depen-dency structures for rule extraction and translationdecoding.
In details, the projective transformationfirst traverses the non-projective dependency struc-tures just as they are projective, then adjusts the or-der of the nodes according to the traversed word se-quences.
We take NIST MT Evaluation testing set2002 (NIST 02) for developing , and use the case-sensitive BLEU (Papineni et al, 2002) to measurethe translation accuracy.The last column of Table 2 shows the perfor-mance of the grammars on machine translation.
Thecross-lingually similarized grammars correspond-ing to the configurations with projective searchingfor Chinese always improve the translation perfor-mance, while non-projective grammars always hurtthe performance.
It probably can be attributed tothe low performance of non-projective parsing aswell as the inappropriateness of the simple projec-tive transformation method.
In the final applicationin machine translation, we adopted the similarizedgrammar corresponding to the configuration withprojective searching on the source side and non-projective searching on the target side.5.3 Improving Tree-based TranslationOur large-scale bilingual corpus for machinetranslation consists of 1.5M sentence pairs fromLDC data, including LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T06.
The source sen-tences are parsed by the original grammar and theselected cross-lingually similarized grammar.
Thealignments are obtained by running GIZA++ on thecorpus in both directions and applying grow-diag-and refinement (Koehn et al, 2003).
The Englishlanguage model is trained on the Xinhua portion ofthe Gigaword corpus with the SRILM toolkit (Stol-507Grammar Similarity (%) Dep.
P (%) Ada.
P (%) BLEU-4 (%)baseline 34.2 84.5 84.5 24.6proj : fixed 46.3 54.1 82.3 25.8 (+1.2)proj : proj 63.2 72.2 84.6 26.1 (+1.5)proj : nonproj 64.3 74.6 84.7 26.2 (+1.6)nonproj : fixed 48.4 56.1 82.6 20.1 (?4.5)nonproj : proj 63.6 71.4 84.4 22.9 (?1.7)nonproj : nonproj 64.1 73.9 84.9 20.7 (?3.9)Table 2: The performance of cross-lingually similarized Chinese dependency grammars with different configurations.System NIST 04 NIST 05(Liu et al, 2006) 34.55 31.94(Chiang, 2007) 35.29 33.22(Xie et al, 2011) 35.82 33.62Original Grammar 35.44 33.08Similarized Grammar 36.78 35.12Table 3: The performance of the cross-lingually similarizedgrammar on dependency tree-based translation, compared withrelated work.cke and Andreas, 2002).
We use NIST 02 as thedevelopment set, and NIST 04 and NIST 05 as thetesting sets.
The quality of translations is evaluatedby the case insensitive NIST BLEU-4 metric.Table 3 shows the performance of the cross-lingually similarized grammar on dependency tree-based translation, compared with previous work(Xie et al, 2011).
We also give the performance ofconstituency tree-based translation (Liu et al, 2006)and formal syntax-based translation (Chiang, 2007).The original grammar performs slightly worse thanthe previous work in dependency tree-based trans-lation, this can ascribed to the difference betweenthe implementation of the original grammar and thedependency parser used in the previous work.
How-ever, the similarized grammar achieves very signif-icant improvement based on the original grammar,and also significant surpass the previous work.
Notethat there is no other modification on the translationmodel besides the replacement of the source parser.From the perspective of performance improve-ment, tree-to-tree translation would be a better sce-nario to verify the effectiveness of cross-lingualsimilarization.
This is because tree-to-tree transla-tion suffers from more serious non-isomorphism be-tween the source and the target syntax structures,and our method for cross-lingual similarization cansimultaneously similarize both the source and thetarget grammars.
For dependency-based translation,however, there are no available tree-to-tree modelsfor us to verify this assumption.
In the future, wewant to propose a specific tree-to-tree translationmethod to better utilize the isomorphism betweencross-lingually similarized grammars.6 Related WorkThere are some work devoted to adjusting the syn-tactic structures according to bilingual constraints toimprove constituency tree-based translation (Huangand Knight, 2006; Ambati and Lavie, 2008; Wanget al, 2010; Burkett and Klein, 2012; Liu etal., 2012).
These efforts concentrated on con-stituency structures, adopted hand-crafted transfor-mation templates or rules, and learnt the operationsequences of structure transformation on the bilin-gual corpora.
Such methods are hard to be di-rectly applied to dependency structures due to thegreat discrepancy between constituency and depen-dency grammars.
There are also work on automati-cally adjusting the syntactic structures for machinetranslation resorting to self-training (Morishita etal., 2015), where the parsed trees for self-trainingare selected according to translation performance.Our work focuses on the automatic cross-lingualsimilarization of dependency grammars, and learntgrammars with higher cross-lingual similarity whilemaintaining the non-triviality of the grammars.There are substantial efforts that have been madein recent years towards harmonizing syntactic repre-sentations across languages.
This includes the Ham-leDT project (Zeman et al, 2012; Zeman et al,2014), as well as the Universal Dependencies ini-tiative (Petrov et al, 2012; McDonald et al, 2013).508Our work aims to automatically harmonize the de-pendency representations resorting to bilingual cor-respondence, thus can be grouped into the build-ing strategies for harmonized or universal dependen-cies.
These existing annotated treebanks would alsopermit interesting control experiments, both for themeasurement of similarity and for parsing.7 Conclusion and Future WorkWe propose an automatic cross-lingual similariza-tion algorithm for dependency grammars, design anautomatic evaluation metric to measure the cross-lingual similarity between grammars, and use thesimilarized grammars to improve dependency tree-based machine translation.
Experiments show theefficacy of this method.
The cross-lingual similar-ization in this paper is still soft similarization, it isworth to investigate the hard similarization, wherethe syntactic structures are totally isomorphic be-tween two languages.
Of course, in such syntacticstructures, the syntactic nodes should be super-node,that is, a graph containing one or more basic syntac-tic nodes.
Hard similarization could be more suit-able for cross-lingual applications, and we leave thisaspect for future research.AcknowledgmentsThe authors are supported by National Natural Sci-ence Foundation of China (Contract 61379086 and61370130).
Jiang is also supported by Open-endFund of the Platform of Research Database and In-formation Standard of China (No.
qhkj2015-01).We sincerely thank the anonymous reviewers fortheir insightful comments.ReferencesVamshi Ambati and Alon Lavie.
2008.
Improving syntaxdriven translation models by re-structuring divergentand non-isomorphic parse tree structures.
In Proceed-ings of Student Research Workshop of AMTA.David Burkett and Dan Klein.
2012.
Transforming treesto improve syntactic convergence.
In Proceedings ofEMNLP-CNLL.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe ACL.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, pages 201?228.Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, andYoram Singer.
2003.
Online passive aggressive algo-rithms.
In Proceedings of NIPS.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of the ACL.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of COLING, pages 340?345.Michel Galley, Jonathan Graehl, Kevin Knight, andDaniel Marcu.
2006.
Scalable inference and trainingof context-rich syntactic translation models.
In Pro-ceedings of the COLING-ACL.Bryant Huang and Kevin Knight.
2006.
Relabeling syn-tax trees to improve syntax-based machine translationquality.
In Proceedings of NAACL.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the AMTA.Rebecca Hwa, Philip Resnik, Amy Weinberg, and OkanKolak.
2002.
Evaluating translational correspondenceusing annotation projection.
In Proceedings of theACL.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofNAACL.Dekang Lin.
2004.
A path-based transfer model for ma-chine translation.
In Proceedings of the COLING.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the ACL.Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou.
2012.Re-training monolingual parser bilingually for syntac-tic smt.
In Proceedings of EMNLP-CNLL.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
In ComputationalLinguistics.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL, pages 91?98.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundagez, Yoav Goldberg, Dipanjan Das, KuzmanGanchev, Keith Hall, Slav Petrov, Hao Zhang, OscarTa?ckstro?m, Claudia Bedini, Nuria Bertomeu Castello,and Jungmee Leez.
2013.
Universal dependency an-notation for multilingual parsing.
In Proceedings ofACL.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of the ACL.Makoto Morishita, Koichi Akabe, Yuto Hatakoshi, Gra-ham Neubig, Koichiro Yoshino, and Satoshi Naka-mura.
2015.
Parser self-training for syntax-based ma-chine translation.
In Proceedings of IWSLT.509Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Proceedings of EMNLP.Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,and Svetoslav Marinov.
2006.
Labeled pseudoprojec-tive dependency parsing with support vector machines.In Proceedings of CoNLL, pages 221?225.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
IEEE TKDE.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of theACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
Proceedings ofLREC.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of the ACL.Ratnaparkhi and Adwait.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of the EmpiricalMethods in Natural Language Processing Conference.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL.David Smith and Jason Eisner.
2009.
Parser adaptationand projection with quasi-synchronous grammar fea-tures.
In Proceedings of EMNLP.Stolcke and Andreas.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 311?318.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, re-labeling, and re-alignment for syntax-based machine translation.
Com-putational Linguistics.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A noveldependency-to-string model for statistical machinetranslation.
In Proceedings of EMNLP.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
A depen-dency treelet string correspondence model for statisti-cal machine translation.
In Proceedings of Workshopon SMT.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In Natural Lan-guage Engineering.H Yamada and Y Matsumoto.
2003.
Statistical depen-dency analysis using support vector machines.
In Pro-ceedings of IWPT.Daniel Zeman, David Marec?ek, Martin Popel, Lo-ganathan Ramasamy, Jan S?te?pa?nek, Jan Hajic?, andZdene?k Z?abokrtsky?.
2012.
Hamledt: To parse or notto parse?Daniel Zeman, Ondr?ej Dus?ek, David Marec?ek, MartinPopel, Loganathan Ramasamy, Jan S?te?pa?nek, Zdene?kZ?abokrtsky?, and Jan Hajic?.
2014.
Hamledt: Harmo-nized multi-language dependency treebank.
LanguageResources & Evaluation.510
