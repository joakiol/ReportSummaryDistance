Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503?1514,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsInducing Document Plans for Concept-to-text GenerationIoannis Konstas and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABikonstas@inf.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn a language generation system, a contentplanner selects which elements must be in-cluded in the output text and the ordering be-tween them.
Recent empirical approaches per-form content selection without any orderingand have thus no means to ensure that the out-put is coherent.
In this paper we focus onthe problem of generating text from a databaseand present a trainable end-to-end generationsystem that includes both content selectionand ordering.
Content plans are representedintuitively by a set of grammar rules that op-erate on the document level and are acquiredautomatically from training data.
We de-velop two approaches: the first one is inspiredfrom Rhetorical Structure Theory and repre-sents the document as a tree of discourse re-lations between database records; the secondone requires little linguistic sophistication anduses tree structures to represent global patternsof database record sequences within a doc-ument.
Experimental evaluation on two do-mains yields considerable improvements overthe state of the art for both approaches.1 IntroductionConcept-to-text generation broadly refers to the taskof automatically producing textual output from non-linguistic input (Reiter and Dale, 2000).
Depend-ing on the application and the domain at hand, theinput may assume various representations includingdatabases, expert system knowledge bases, simula-tions of physical systems, or formal meaning rep-resentations.
Generation systems typically followa pipeline architecture consisting of three compo-nents: content planning (selecting and ordering theparts of the input to be mentioned in the output text),sentence planning (determining the structure andlexical content of individual sentences), and surfacerealization (verbalizing the chosen content in natu-ral language).
Traditionally, these components arehand-engineered in order to ensure output of highquality.More recently there has been growing interestin the application of learning methods because oftheir promise to make generation more robust andadaptable.
Examples include learning which con-tent should be present in a document (Duboue andMcKeown, 2002; Barzilay and Lapata, 2005), how itshould be aligned to utterances (Liang et al 2009),and how to select a sentence plan among many al-ternatives (Stent et al 2004).
Beyond isolated com-ponents, a few approaches have emerged that tackleconcept-to-text generation end-to-end.
Due to thecomplexity of the task, most models simplify thegeneration process, e.g., by treating sentence plan-ning and surface realization as one component (An-geli et al 2010), by implementing content selectionwithout any document planning (Konstas and Lap-ata, 2012; Angeli et al 2010; Kim and Mooney,2010), or by eliminating content planning entirely(Belz, 2008; Wong and Mooney, 2007).In this paper we present a trainable end-to-endgeneration system that captures all components ofthe traditional pipeline, including document plan-ning.
Rather than breaking up the generation pro-cess into a sequence of local decisions, each learnedseparately (Reiter et al 2005; Belz, 2008; Chen andMooney, 2008; Kim and Mooney, 2010), our modelperforms content planning (i.e., document planningand content selection), sentence planning (i.e., lex-1503Database Recordstemp(time:6-21, min:9, mean:15, max:21)wind-spd(time:6-21, min:15, mean:20, max:30)sky-cover(time:6-9, percent:25-50)sky-cover(time:9-12, percent:50-75)wind-dir(time:6-21, mode:SSE)gust(time:6-21, min:20, mean:30, max:40)Output TextCloudy, with a high around 20.
South southeast windbetween 15 and 30 mph.
Gusts as high as 40 mph.
(a) WEATHERGOVDatabase Recordsdesktop(cmd:lclick, name:start, type:button)start(cmd:lclick, name:settings, type:button)start-target(cmd:lclick, name:control panel, type:button)win-target(cmd:dblclick, name:users and passwords, type:item)contMenu(cmd:lclick, name:advanced, type:tab)action-contMenu(cmd:lclick, name:advanced, type:button)Output TextClick start, point to settings, and then click control panel.
Double-click users and passwords.
On the advanced tab, click advanced.
(b) WINHELPFigure 1: Database records and corresponding text for (a) weather forecasting and (b) Windows trou-bleshooting.
Each record has a type (e.g., win-target), and a set of fields.
Each field has a value, whichcan be categorical (in typewriter), an integer (in bold), or a literal string (in italics).icalization of input entries), and surface realizationjointly.
We focus on the problem of generating textfrom a database.
The input to our model is a set ofdatabase records and collocated descriptions, exam-ples of which are shown in Figure 1.Given this input, we define a probabilisticcontext-free grammar (PCFG) that captures thestructure of the database and how it can be verbal-ized.
Specifically, we extend the model of Kon-stas and Lapata (2012) which also uses a PCFG toperform content selection and surface realization,but does not capture any aspect of document plan-ning.
We represent content plans with grammarrules which operate on the document level and areembedded on top of the original PCFG.
We essen-tially learn a discourse grammar following two ap-proaches.
The first one is linguistically naive butapplicable to multiple languages and domains; it ex-tracts rules representing global patterns of recordsequences within a sentence and among sentencesfrom a training corpus.
The second approach learnsdocument plans based on Rhetorical Structure The-ory (RST; Mann and Thomson, 1988); it thereforehas a solid linguistic foundation, but is resource in-tensive as it assumes access to a text-level discourseparser.We learn document plans automatically usingboth representations and develop a tractable decod-ing algorithm for finding the best output, i.e., deriva-tion in our grammar.
To the best of our knowledge,this is the first data-driven model to incorporate doc-ument planning in a joint end-to-end system.
Exper-imental evaluation on the WEATHERGOV (Liang etal., 2009) and WINHELP (Branavan et al 2009) do-mains shows that our approach improves over Kon-stas and Lapata (2012) by a wide margin.2 Related WorkContent planning is a fundamental component in anatural generation system.
Not only does it deter-mine which information-bearing units to talk about,but also arranges them into a structure that cre-ates coherent output.
It is therefore not surpris-ing that many content planners have been basedon theories of discourse coherence (Hovy, 1993;Scott and de Souza, 1990).
Other work has re-lied on generic planners (Dale, 1988) or schemas(Duboue and McKeown, 2002).
In all cases, con-tent plans are created manually, sometimes throughcorpus analysis.
A few researchers recognize thatthis top-down approach to planning is too inflexibleand adopt a generate-and-rank architecture instead(Mellish et al 1998; Karamanis, 2003; Kibble andPower, 2004).
The idea is to produce a large setof candidate plans and select the best one accordingto a ranking function.
The latter is typically devel-oped manually taking into account constraints relat-ing to discourse coherence and the semantics of thedomain.Duboue and McKeown (2001) present perhapsthe first empirical approach to content planning.They use techniques from computational biologyto learn the basic patterns contained within a planand the ordering among them.
Duboue and McK-eown (2002) learn a tree-like planner from analigned corpus of semantic inputs and correspond-ing human-authored outputs using evolutionary al-1504gorithms.
More recent data-driven work focuses onend-to-end systems rather than individual compo-nents, however without taking document planninginto account.
For example, Kim and Mooney (2010)first define a generative model similar to Liang etal.
(2009) that selects which database records totalk about and then use an existing surface real-izer (Wong and Mooney, 2007) to render the cho-sen records in natural language.
Their content plan-ner has no notion of coherence.
Angeli et al(2010)adopt a more unified approach that builds on top ofthe alignment model of Liang et al(2009).
Theybreak record selection into a series of locally coher-ent decisions, by first deciding on what records totalk about.
Each choice is based on a history ofprevious decisions, which is encoded in the formof discriminative features in a log-linear model.Analogously, they choose fields for each record,and finally verbalize the input using automaticallyextracted domain-specific templates from trainingdata.Konstas and Lapata (2012) propose a joint model,which recasts content selection and surface realiza-tion into a parsing problem.
Their model optimizesthe choice of records, fields and words simultane-ously, however they still select and order records lo-cally.
We replace their content selection mechanism(which is based on a simple markovized chaining ofrecords) with global document representations.
Aplan in our model is identified either as a sequenceof sentences, each containing a sequence of records,or as a tree where the internal nodes denote dis-course information and the leaf nodes correspond torecords.3 Problem FormulationThe generator takes as input a set of databaserecords d and outputs a text g that verbalizes someof these records.
Each record token ri ?
d, with1 ?
i ?
|d|, has a type ri.t and a set of fields f as-sociated with it.
Fields have different values f .v andtypes f .t (i.e., integer, categorical, or literal strings).For example, in Figure 1b, win-target is a recordtype with three fields: cmd (denotes the action theuser must perform on an object on their screen,e.g., left-click), name (denotes the name of the ob-ject), and type (denotes the type of the object).
Thevalues of these fields are dblclick, users and pass-words, and item; name is a literal string, the rest areGrammar Rules1.
S?
R(start)2.
R(ri.t)?
FS(r j,start) R(r j.t) | FS(r j,start)3.
FS(r,r.
fi)?
F(r,r.
f j) FS(r,r.
f j) | F(r,r.
f j)4.
F(r,r.
f )?W(r,r.
f ) F(r,r.
f ) |W(r,r.
f )5.
W(r,r.
f )?
?
| g( f .v)Figure 2: Grammar G of the original model.
Paren-theses denote features, and impose constraints on thegrammar.categorical.During training, our algorithm is given a corpusconsisting of several scenarios, i.e., database recordspaired with texts w (see Figure 1).
For each sce-nario, the model first decides on a global documentplan, i.e., it selects which types of records belong toeach sentence (or phrase) and how these sentences(or phrases) should be ordered.
Then it selects ap-propriate record tokens for each type and progres-sively chooses the most relevant fields; then, basedon the values of the fields, it generates the final text,word by word.4 Original ModelOur work builds on the model developed by Kon-stas and Lapata (2012).
The latter is essentiallya PCFG which captures both the structure of theinput database and the way it renders into naturallanguage.
This grammar-based approach lends it-self well to the incorporation of document planningwhich has traditionally assumed tree-like represen-tations.
We first briefly describe the original modeland then present our extensions in Section 5.Grammar Grammar G in Figure 2 defines a setof non-recursive CFG rewrite rules that capture thestructure of the database, i.e., the relationship be-tween records, records and fields, fields and words.These rules are domain-independent and could beapplied to any database provided it follows the samestructure.
Non-terminal symbols are in capitals, theterminal symbol ?
corresponds to the vocabulary ofthe training set and g( f .v) is a function which gener-ates integers given the field value f .v.
Note that allnon-terminals have features (in parentheses) which1505act as constraints and impose non-recursion (e.g., inrule (2) i 6= j, so that a record cannot emit itself).Rule (1) defines the expansion from the start sym-bol S to the first record R of type start.
The rulesin (2) implement content selection, by choosing ap-propriate records from the database and generatinga sequence.
R(ri.t) is the source record, R(r j.t) isthe target record and FS(r j.start) is a place-holdersymbol for the set of fields of record token r j. Thismethod is locally optimal, since it only keeps trackof the previous type of record for each re-write.
Therules in (3) conclude content selection on the fieldlevel, i.e., after we have chosen a record, we selectand order the corresponding fields.
Finally, the rulesin (4) and (5) correspond to surface realization.
Theformer rule binarizes the sequence of words emittedby a particular field r. f in an attempt to capture localdependencies between words, such as multi-wordexpressions (e.g., right click, radio button).
The lat-ter rule defines the emission of words and integernumbers1, given a field type and its value.
Note thatthe original model lexicalizes field values of cate-gorical and integer type only.Training The rules of grammar G are associatedwith weights that are learned using the EM algo-rithm (Dempster et al 1977).
During training, therecords, fields and values of database d and thewords w from the associated text are observed, andthe model learns the mapping between them.
Noticethat we use w to denote the gold-standard text andg to refer to the words generated by the model.
Themapping between the database and the observed textis unknown and thus the weights of the rules definea hidden correspondence h between records, fieldsand their values.Decoding Given a trained grammar G and an in-put scenario from a database d, the model generatestext by finding the most likely derivation, i.e., se-quence of rewrite rules for the input.
Although re-sembling parsing, the generation task is subtly dif-ferent.
In parsing, we observe a string of words andour goal is to find the most probable syntactic struc-ture, i.e., hidden correspondence h?.
In generation,1The function g( f .v) : Z?
Z, generates an integer in thefollowing six ways (Liang et al 2009): identical, roundingup/down to a multiple of 5, rounding off a multiple of 5 andadding or subtracting some noise modelled by a geometric dis-tribution.however, the string is not observed; instead, we mustfind the best text g?, by maximizing both over h and g,where g = g1 .
.
.gN is a sequence of words licensedby G. More formally:g?
= f(argmaxg,hP((g,h)))(1)where f is a function that takes as input a derivationtree (g,h) and returns g?.
Konstas and Lapata (2012)use a modified version of the CYK parser (Kasami,1965; Younger, 1967) to find g?.
Specifically, theyintersect grammar G with a n-gram language modeland calculate the most probable generation g?
as:g?
= f(argmaxg,hp(g) ?
p(g,h |d))(2)where p(g,h |d) is the decoding likelihood for a se-quence of words g = g1 .
.
.gN of length N and thehidden correspondence h that emits it, i.e., the likeli-hood of the grammar for a given database input sce-nario d. p(g) is a measure of the quality of each out-put and is provided by the n-gram language model.5 ExtensionsIn this section we extend the model of Konstas andLapata (2012) by developing two more sophisticatedcontent selection approaches which are informed bya global plan of the document to be generated.5.1 Planning with Record SequencesGrammar Our key idea is to replace the contentselection mechanism of the original model with adocument plan which essentially defines a gram-mar on record types.
We split a document intosentences, each terminated by a full-stop.
Then asentence is further split into a sequence of recordtypes.
Contrary to the original model, we observe acomplete sequence2 of record types, split into sen-tences.
This way we learn domain-specific pat-terns of frequently occurring record type sequencesamong the sentences of a document, as well as morelocal structures within a sentence.
We thus substituterules (1)?
(2) in Figure 2 with sub-grammar GRSEbased on record type sequences:Definition 1 (GRSE grammar)GRSE = {?R, NRSE , PRSE , D}2Note that a sequence is different from a permutation, as wemay allow repetitions or omissions of certain record types.1506where ?R is a set of terminal symbols R(r.t), andNRSE is a set of non-terminal symbols:NRSE = {D, SENT}where D represents the start symbol and SENT asequence of records.
PRSE is a set of production rulesof the form:(a) D?
SENT (ti, .
.
.
, t j) .
.
.
SENT (tl, .
.
.
, tm)(b) SENT (ti, .
.
.
, t j)?
R(ra.ti) .
.
.
R(rk.t j) ?where t is a record type, ti, t j, tl and tm may overlapand ra, rk are record tokens of type ti and t j respec-tively.
The corresponding weights for the productionrules PRSE are:Definition 2 (GRSE weights)(a) p(ti, .
.
.
, t j, .
.
.
tl, .
.
.
, tm | D)(b) p(ti) ?
... ?
p(t j) = 1|s(ti)| ?
.
.
.
?1|s(t j)|where s(t) is a function that returns the set of recordswith type t (Liang et al 2009).Rule (a) defines the expansion from the start sym-bol D to a sequence of sentences, each representedby the non-terminal SENT .
Similarly to the originalgrammar G, we employ the use of features (in paren-theses) to denote a sequence of record types.
Thesame record types may recur in different sentences,but not in the same one.
The weight of rule (a) issimply the joint probability of all the record typespresent, ordered and segmented appropriately intosentences in the document, given the start symbol.Once record types have been selected (on a persentence basis) we move on to rule (b) which de-scribes how each non-terminal SENT expands toan ordered sequence of records R, as they are ob-served within a sentence (see the terminal sym-bol ?.?
at the end of the rule).
Notice that a recordtype ti may correspond to several record tokens ra.Rules (3)?
(5) in grammar G make decisions on thesetokens based on the overall content of the databaseand the field/value selection.
The weight of thisrule is the product of the weights of each recordtype.
This is set to the uniform distribution over{1, ..., |s(t)|} for record type t, where |s(t)| is thenumber of records with that type.Figure 3d shows an example tree for the databaseinput in Figure 1b, using GRSE and assuming that thealignments between records and text are given.
Thetop level of the tree refers to the sequence of recordtypes as they are observed in the text.
The first sen-tence contains three records with types ?desktop?,?start?
and ?start-target?, each corresponding to thetextual segments click start, point to settings, andthen click control panel.
The next level on the tree,denotes the choice of record tokens for each sen-tence, provided that we have decided on the choiceand order of their types (see Figure 3b).
In Fig-ure 3d, the bottom-left sub-tree corresponds to thechoice of the first three records of Figure 1b.Training A straightforward way to train the ex-tended model would be to embed the parameters ofGRSE in the original model and then run the EM al-gorithm using inside-outside at the E-step.
Unfortu-nately, this method will induce a prohibitively largesearch space.
Rule (a) enumerates all possible com-binations of record type sequences and the numbergrows exponentially even for a few record types anda small sequence size.
To tackle this problem, we ex-tracted rules for GRSE from the training data, basedon the assumption that there will be far fewer uniquesequences of record types per dataset than exhaus-tively enumerating all possibilities.For each scenario, we obtain a word-by-wordalignment between the database records and the cor-responding text.
In our experiments we used Lianget als (2009) unsupervised model, however anyother semi- or fully supervised method could beused.
As we show in Section 7, the quality of thealignment inevitably correlates with the quality ofthe extracted grammar and the decoder?s output.
Wethen map the aligned record tokens to their corre-sponding types, merge adjacent words with the sametype and segment on punctuation (see Figure 3b).Next, we create the corresponding tree according toGRSE (Figure 3d) and binarize it.
We experimentedboth with left and right binarization and adhered tothe latter, as it obtained a more compact set of rules.Finally, we collectively count the rule weights on theresulting treebank and extract a rule set, discardingrules with frequency less than three.Using the extracted (weighted) GRSE rules, we runthe EM algorithm via inside-outside and learn theweights for the remaining rules in G. Decoding re-mains the same as in Konstas and Lapata (2012);the only requirement is that the extracted grammarremains binarized in order to guarantee the cubic1507desktop1Click start,start1point to settings,start-target1and then click control panel.win-target1Double-click users and passwords.contMenu1On the advanced tab ,paction-contMenu1click advanced.
(a) Record token alignments[desktop start start-target?win-target?contMenu action-contMenu?
](b) Record type segmentation[Click start,]desktop1.t [point to settings, ]start1.t [and thenclick control panel.
]start?target1.t [Double-click users andpasswords.
]win?target1.t [On the advanced tab,]contMenu1.t [clickadvanced.
]action?contMenu1.t(c) Segmentation of text into EDUsDSENT(c, a-c)R(a-c1.t)R(c1.t)SENT(w-t)R(w-t1.t)SENT(d, s, s-t)R(s-t1.t)R(s1.t)R(d1.t)(d) Document plan using the GRSE grammarDElaboration[N][S]Elaboration[N][S]R(a-c1.t)R(c1.t)Elaboration[N][S]R(w-t1.t)Elaboration[N][S]Joint[N][N]R(s-t1.t)R(s1.t)R(d1.t)(e) Document plan using the GRST grammarFigure 3: Grammar extraction example from the WINHELP domain using GRSE and GRST .
For GRSE , we takethe alignments of records on words and map them to their corresponding types (a); we then segment recordtypes into sentences (b); and finally, create a tree using grammar GRSE (c).
For GRST , we segment the textinto EDUs based on the records they align to (d) and output the discourse tree (omitted here for brevity?ssake); we build the document plan once we substitute the EDUs with their corresponding record types (e).bound of the Viterbi search algorithm.
Note that theoriginal grammar is limited to the generation of cat-egorical and integer values.
We extend it to supportthe generation of strings.
The following rule adds asimple verbatim lexicalization for string values:W(r,r.
f )?
gen str( f .v, i)gen str( f .v, i) : V ?V, f .v ?Vwhere V is the set of words for the fields of typestring, and gen str is a function that takes the valueof a string-typed field f .v, and the position i inthe string, and generates the corresponding word atthat position.
For example, gen str(users and pass-words, 3) = passwords.
The weight of this rule is setto 1.5.2 Planning with Rhetorical Structure TheoryGrammar RST (Mann and Thompson, 1988) is atheory of text organization which provides a frame-work for analyzing text.
A basic tenet of the the-ory is that a text consists of hierarchically organizedtext spans or elementary discourse units (EDUs) thatstand in various relations to one another (e.g., Elab-oration, Attribution).
These ?rhetorical relations?hold between two adjacent parts of the text, wheretypically, one part is ?nuclear?
and one a ?satellite?.An analysis of a text consists in identifying the re-lations holding between successively larger parts ofthe text, yielding a natural hierarchical descriptionof the rhetorical organization of the text.
From itsvery inception, RST was conceived as a way to char-acterize text and textual relations for the purpose oftext generation.In order to create a RST-inspired document planfor our input (i.e., database records paired withtexts), we make the following assumption: eachrecord corresponds to a unique non-overlappingspan in the collocated text, and can be thereforemapped to an EDU.
Assuming the text has been seg-mented and aligned to a sequence of records, wecan create a discourse tree with record types (inplace of their corresponding EDUs) as leaf nodes.Again, we define a sub-grammar GRST which re-places rules (1)?
(2) from Figure 2:1508Definition 3 (GRST grammar)GRST = {?R, NRST , PRST , D}where ?R is the alphabet of leaf nodes as de-fined in Section 5.1, NRST is a set of non-terminalscorresponding to rhetorical relations augmentedwith nucleus-satellite information (e.g., Elabora-tion[N][S] stands for the elaboration relation be-tween the nucleus EDU left-adjoining with the satel-lite EDU), PRST is the set of production rules of theform PRST ?
NRST ?
{NRST ??R}?
{NRST ?
?R} as-sociated with a weight for each rule, and D ?
NRSTis the root symbol.
Figure 3e gives the discourse treefor the database input of Figure 1b, using GRST .Training In order to obtain the weighted produc-tions of GRST , we use an existing state-of-the-art dis-course parser3 (Feng and Hirst, 2012) trained on theRST-DT corpus (Carlson et al 2001).
The lattercontains a selection of 385 Wall Street Journal arti-cles which have been annotated using the frameworkof RST and an inventory of 78 rhetorical relations,classified into 18 coarse-grained categories (Carl-son and Marcu, 2001).
Figure 4 gives a comparisonof the distribution of relations extracted for the twodatasets we used, against the gold-standard annota-tion of RST-DT.
The statistics for the RST-DT cor-pus are taken from Williams and Power (2008).
Therelative frequencies of relations on both datasets fol-low closely the distribution of those in RST-DT, thusempirically supporting the application of the RSTframework to our data.We segment each document in our training setinto EDUs based on the record-to-text alignmentsgiven by the model of Liang et al(2009) (see Fig-ure 3c).
We then run the discourse parser on theresulting EDUs, and retrieve the corresponding dis-course tree; the internal nodes are labelled withone of the RST relations.
Finally, we replace theleaf EDUs with their respective terminal symbolsR(r.t) ?
?R (Figure 3e) and collect the resultinggrammar productions; their weights are calculatedvia maximum likelihood estimation based on theircollective counts in the parse trees licensed by GRST .Training and decoding of the extended generationmodel (after we embed GRST in the original gram-mar G) is performed identically to Section 5.1.3Publicly available from http://www.cs.toronto.edu/?weifeng/software.html.6 Experimental DesignData Since our aim was to evaluate the planningcomponent of our model, we used datasets whosedocuments are at least a few sentences long.
Specif-ically, we generated weather forecasts and trou-bleshooting guides for an operating system.
Forthe first domain (henceforth WEATHERGOV) we usedthe dataset of Liang et al(2009), which consistsof 29,528 weather scenarios for 3,753 major UScities (collected over four days).
The database has12 record types, each scenario contains on average36 records, 5.8 out of which are mentioned in thetext.
A document has 29.3 words and is four sen-tences long.
The vocabulary is 345 words.
We used25,000 scenarios from WEATHERGOV for training,1,000 scenarios for development and 3,528 scenar-ios for testing.For the second domain (henceforth WINHELP) weused the dataset of Branavan et al(2009), whichconsists of 128 scenarios.
These are articles fromMicrosoft?s Help and Support website4 and containstep-by-step instructions on how to perform tasks onthe Windows 2000 operating system.
In its originalformat, the database provides a semantic representa-tion of the textual guide, i.e., it represents the user?sactions on the operating system?s UI.
We semi-automatically converted this representation into aschema of records, fields and values, following theconventions adopted in Branavan et al(2009).5 Thefinal database has 13 record types.
Each scenario has9.2 records and each document 51.92 words with 4.3sentences.
The vocabulary is 629 words.
We per-formed 10-fold cross-validation on the entire datasetfor training and testing.
Compared to WEATHER-GOV, WINHELP documents are longer with a largervocabulary.
More importantly, due to the nature ofthe domain, i.e., giving instructions, content selec-tion is critical not only in terms of what to say butalso in what order.Grammar Extraction and Parameter SettingWe obtained alignments between database recordsand textual segments for both domains and gram-mars (GRSE and GRST ) using the unsupervised modelof Liang et al(2009).
On WEATHERGOV, we ex-tracted a GRSE grammar with 663 rules (after bi-4support.microsoft.com5The dataset can be downloaded from http://homepages.inf.ed.ac.uk/ikonstas/index.php?page=resources1509ElaborationAttributionJointContrastExplanationBackgroundEnablementCauseEvaluationComparisonConditionTopic-CommentTemporalExplanationSummaryTopicChange0204060 RST-DTWEATHERGOVWINHELPFigure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power,2008).narization).
The WINHELP dataset is considerablysmaller, and as a result the procedure described inSection 5.1 yields a very sparse grammar.
To al-leviate this, we horizontally markovized the right-hand side of each rule (Collins, 1999; Klein andManning, 2003).6 After markovization, we obtaineda GRSE grammar with 516 rules.
On WEATHERGOV,we extracted 434 rules for GRST .
On WINHELP wecould not follow the horizontal markovization pro-cedure, since the discourse trees are already bina-rized.
Instead, we performed vertical markovization,i.e., annotated each non-terminal with their parentnode (Johnson, 1998) and obtained a GRST grammarwith 419 rules.
The model of Konstas and Lapata(2012) has two parameters, namely the number ofk-best lists to keep in each derivation, and the or-der of the language model.
We tuned k experimen-tally on the development set and obtained best re-sults with 60 for WEATHERGOV and 120 for WIN-HELP.
We used a trigram model for both domains,trained on each training set.Evaluation We compared two configurations ofour system, one with a content planning compo-nent based on record type sequences (GRSE) and6When horizontally markovizing, we can encode an arbi-trary amount of context in the intermediate non-terminals thatresult from this process; in our case we store h=1 horizontalsiblings plus the mother left-hand side (LHS) non-terminal, inorder to uniquely identify the Markov chain.
For example,A?
B C D becomes A?
B ?A .
.
.B?, ?A .
.
.B?
?
C ?A .
.
.C?,?A .
.
.C?
?
D.another one based on RST (GRST ).
In both casescontent plans were extracted from (noisy) unsuper-vised alignments.
As a baseline, we used the orig-inal model of Konstas and Lapata (2012).
We alsocompared our model to Angeli et als system (2010),which is state of the art on WEATHERGOV.System output was evaluated automatically, usingthe BLEU modified precision score (Papineni et al2002) with the human-written text as reference.
Inaddition, we evaluated the generated text by elicitinghuman judgments.
Participants were presented witha scenario and its corresponding verbalization andwere asked to rate the latter along three dimensions:fluency (is the text grammatical?
), semantic correct-ness (does the meaning conveyed by the text corre-spond to the database input?)
and coherence (is thetext comprehensible and logically structured?).
Par-ticipants used a five point rating scale where a highnumber indicates better performance.
We randomlyselected 12 documents from the test set (for each do-main) and produced output with the system of Kon-stas and Lapata (2012) (henceforth K&L), our twomodels using GRSE and GRST , respectively, and An-geli et al(2010) (henceforth ANGELI).
We also in-cluded the original text (HUMAN) as gold-standard.We obtained ratings for 60 (12 ?
5) scenario-textpairs for each domain.
Examples of the documentsshown to the participants are given in Table 1.The study was conducted over the Internet us-1510WEATHERGOV WINHELPGRSEShowers before noon.
Cloudy, with a high near38.
Southwest wind between 3 and 8 mph.Chance of precipitation is 55 %.Right-click my network places, and then click prop-erties.
Right-click local area connection, and clickproperties.
Click to select the file and printer sharingfor Microsoft networks, and then click ok.GRSTShowers likely.
Mostly cloudy, with a high around38.
South wind between 1 and 8 mph.
Chance ofprecipitation is 55 %.Right-click my network places, and then click proper-ties.
Right-click local area connection.
Click file andprinter sharing for Microsoft networks, and click ok.K&LA chance of showers.
Otherwise, cloudy, with ahigh near 38.
Southwest wind between 3 and 8mph.Right-click my network places, click properties.Right-click local area connection.
Click to select thefile and printer sharing for Microsoft networks, andthen click ok.ANGELI A chance of rain or drizzle after 9am.
Mostlycloudy, with a high near 38.
Southwest wind be-tween 3 and 8 mph.
Chance of precipitation is 50%.Right-click my network places, and then click prop-erties on the tools menu, and then click proper-ties.
Right-click local area connection, and then clickproperties.
Click file and printer sharing for Microsoftnetworks, and then click ok.HUMAN A 50 percent chance of showers.
Cloudy, with ahigh near 38.
Southwest wind between 3 and 6mph.Right-click my network places, and then click proper-ties.
Right-click local area connection, and then clickproperties.
Click to select the file and printer sharingfor Microsoft networks check box.
Click ok.Table 1: Human-authored text and system output on WEATHERGOV and WINHELP.ing Amazon Mechanical Turk7, and involved 200volunteers (100 for WEATHERGOV, and 100 forWINHELP), all self reported native English speak-ers.
For WINHELP, we made sure participants werecomputer-literate and familiar with the Windows op-erating system by administering a short question-naire prior to the experiment.7 ResultsThe results of the automatic evaluation are summa-rized in Table 2.
Overall, our models outperformK&L?s system by a wide margin on both datasets.The two content planners (GRSE and GRST ) performcomparably in terms of BLEU.
This suggests thatdocument plans induced solely from data are of sim-ilar quality to those informed by RST.
This is anencouraging result given that RST-style discourseparsers are currently available only for English.
AN-GELI performs better on WEATHERGOV possibly dueto better output quality on the surface level.
Theirsystem defines trigger patterns that specifically lexi-calize record fields containing numbers.
In contrast,on WINHELP it is difficult to explicitly specify suchpatterns, as none of the record fields are numeric; asa result their system performs poorly compared to7https://www.mturk.comthe other models.To assess the impact of the alignment on thecontent planner, we also extracted GRSE fromcleaner alignments which we obtained automat-ically via human-crafted heuristics for each do-main.
The heuristics performed mostly anchormatching between database records and words in thetext (e.g., the value Lkly of the field rainChance,matches with the string rain likely in the text).Using these alignments, GRSE obtained a BLEUscore of 39.23 on WEATHERGOV and 41.35 on WIN-HELP.
These results indicate that improved align-ments would lead to more accurate grammar rules.WEATHERGOV seems more sensitive to the align-ments than WINHELP.
This is probably becausethe dataset shows more structural variations in thechoice of record types at the document level, andtherefore the grammar extracted from the unsuper-vised alignments is noisier.
Unfortunately, perform-ing this kind of analysis for GRST would require goldstandard segmentation of our training corpus intoEDUs which we neither have nor can easily approx-imate via heuristics.The results of our human evaluation study areshown in Table 3.
We carried out an Analysis ofVariance (ANOVA) to examine the effect of system1511Model WEATHERGOV WINHELPGRSE 35.60 40.92GRST 36.54 40.65K&L 33.70 38.26ANGELI 38.40 32.21Table 2: Automatic evaluation of system output us-ing BLEU-4.WEATHERGOV WINHELPModel FL SC CO FL SC COGRSE 4.25 3.75 4.18 3.59 3.21 3.35GRST 4.10 3.68 4.10 3.45 3.29 3.22K&L 3.73 3.25 3.59 3.27 2.97 2.93ANGELI 3.90 3.44 3.82 3.44 2.79 2.97HUMAN 4.22 3.72 4.11 4.20 4.41 4.25Table 3: Mean ratings for fluency (FL), semanticcorrectness (SC) and coherence (CO) on system out-put elicited by humans.type (GRSE , GRST , K&L, ANGELI, and HUMAN) onfluency, semantic correctness and coherence ratings.Means differences of 0.2 or more are significant atthe 0.05 level using a post-hoc Tukey test.
Interest-ingly, we observe that document planning improvessystem output overall, not only in terms of coher-ence.
Across all dimensions our models are per-ceived better than K&L and ANGELI.
As far as co-herence is concerned, the two content planners arerated comparably (differences in the means are notsignificant).
Both GRSE and GRST are significantlybetter than the comparison systems (ANGELI andK&L).
Table 1 illustrates examples of system out-put along with the gold standard content selectionfor reference, for the WEATHERGOV and WINHELPdomains, respectively.In sum, we observe that integrating documentplanning either via GRSE or GRST boosts perfor-mance.
Document plans induced from recordsequences exhibit similar performance, comparedto those generated using expert-derived linguisticknowledge.
Our systems are consistently better thanK&L both in terms of automatic and human eval-uation and are close or better than the supervisedmodel of Angeli et al(2010).
We also show thatfeeding the system with a grammar of better qual-ity can achieve state-of-the-art performance, withoutfurther changes to the model.8 ConclusionsIn this paper, we have proposed an end-to-end sys-tem that generates text from database input and cap-tures all components of the traditional generationpipeline, including document planning.
Documentplans are induced automatically from training dataand are represented intuitively by PCFG rules cap-turing the structure of the database and the way itrenders to text.
We proposed two complementaryapproaches to inducing content planners.
In a firstlinguistically naive approach, a document is mod-elled as a sequence of sentences and each sentenceas a sequence of records.
Our second approachdraws inspiration from Rhetorical Structure Theory(Mann and Thomson, 1988) and represents a docu-ment as a tree with intermediate nodes correspond-ing to discourse relations, and leaf nodes to databaserecords.Experiments with both approaches demonstrateimprovements over models that do not incorporatedocument planning.
In the future, we would like totackle more challenging domains, such as NFL re-caps, financial articles and biographies (Howald etal., 2013; Schilder et al 2013).
Our models couldalso benefit from the development of more sophis-ticated planners either via grammar refinement ormore expressive grammar formalisms (Cohn et al2010).AcknowledgmentsWe are grateful to Percy Liang and Gabor Angelifor providing us with their code and data.
Thanks toGiorgio Satta and Charles Sutton for helpful com-ments and suggestions.
We also thank the membersof the Probabilistic Models reading group at the Uni-versity of Edinburgh for useful feedback.ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 502?512, Cambridge, MA.Regina Barzilay and Mirella Lapata.
2005.
Collec-tive content selection for concept-to-text generation.In Proceedings of Human Language Technology and1512Empirical Methods in Natural Language Processing,pages 331?338, Vancouver, British Columbia.Anja Belz.
2008.
Automatic generation ofweather forecast texts using comprehensive probabilis-tic generation-space models.
Natural Language Engi-neering, 14(4):431?455.S.R.K.
Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages82?90, Suntec, Singapore.L.
Carlson and D. Marcu.
2001.
Discourse tagging ref-erence manual.
Technical report, Univ.
of SouthernCalifornia / Information Sciences Institute.Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.2001.
Building a discourse-tagged corpus in theframework of rhetorical structure theory.
In Proceed-ings of the Second SIGdial Workshop on Discourseand Dialogue - Volume 16, SIGDIAL ?01, pages 1?10,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: A test of grounded language acqui-sition.
In Proceedings of International Conference onMachine Learning, pages 128?135, Helsinki, Finland.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journalof Machine Learning Research, 11(November):3053?3096.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.Robert Dale.
1988.
Generating referring expressions ina domain of objects and processes.
Ph.D. thesis, Uni-versity of Edinburgh.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Journal of the royal statistical society, se-ries B, 39(1):1?38.Pablo A. Duboue and Kathleen R. McKeown.
2001.
Em-pirically estimating order constraints for content plan-ning in generation.
In Proceedings of the 39th An-nual Meeting on Association for Computational Lin-guistics, pages 172?179.Pablo A. Duboue and Kathleen R. McKeown.
2002.Content planner construction via evolutionary algo-rithms and a corpus-based fitness function.
In Pro-ceedings of International Natural Language Genera-tion, pages 89?96, Ramapo Mountains, NY.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics, pages 60?68, JejuIsland, Korea.Eduard Hovy.
1993.
Automated discourse generationusing discourse structure relations.
Artificial Intelli-gence, 63:341?385.Blake Howald, Ravikumar Kondadadi, and FrankSchilder.
2013.
Domain adaptable semantic clusteringin statistical nlg.
In Proceedings of the 10th Interna-tional Conference on Computational Semantics (IWCS2013) ?
Long Papers, pages 143?154, Potsdam, Ger-many, March.
Association for Computational Linguis-tics.Mark Johnson.
1998.
Pcfg models of linguistic tree rep-resentations.
Computational Linguistics, 24(4):613?632, December.Nikiforos Karamanis.
2003.
Entity Coherence for De-scriptive Text Structuring.
Ph.D. thesis, University ofEdinburgh.Tadao Kasami.
1965.
An efficient recognition and syntaxanalysis algorithm for context-free languages.
Techni-cal Report AFCRL-65-758, Air Force Cambridge Re-search Lab, Bedford, MA.Rodger Kibble and Richard Power.
2004.
Optimisingreferential coherence in text generation.
Computa-tional Linguistics, 30(4):401?416.Joohyun Kim and Raymond Mooney.
2010.
Generativealignment and semantic parsing for learning from am-biguous supervision.
In Proceedings of the 23rd Con-ference on Computational Linguistics, pages 543?551,Beijing, China.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 423?430.
Association for Computa-tional Linguistics Morristown, NJ, USA.Ioannis Konstas and Mirella Lapata.
2012.
Unsupervisedconcept-to-text generation with hypergraphs.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 752?761, Montre?al, Canada.Percy Liang, Michael Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 91?99, Suntec, Singapore.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.William C. Mann and Sandra A. Thomson.
1988.Rhetorical structure theory.
Text, 8(3):243?281.1513Chris Mellish, Alisdair Knott, Jon Oberlander, and MickO?Donnell.
1998.
Experiments using stochasticsearch for text planning.
In Proceedings of Interna-tional Natural Language Generation, pages 98?107,New Brunswick, NJ.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
Cambridge UniversityPress, New York, NY.Ehud Reiter, Somayajulu Sripada, Jim Hunter, and IanDavy.
2005.
Choosing words in computer-generatedweather forecasts.
Artificial Intelligence, 167:137?169.Frank Schilder, Blake Howald, and Ravi Kondadadi.2013.
Gennext: A consolidated domain adaptable nlgsystem.
In Proceedings of the 14th European Work-shop on Natural Language Generation, pages 178?182, Sofia, Bulgaria, August.
Association for Compu-tational Linguistics.Donia Scott and Clarisse Sieckenius de Souza.
1990.Getting the message across in RST-based text gener-ation.
In Robert Dale, Chris Mellish, and MichaelZock, editors, Current Research in Natural LanguageGeneration, pages 47?73.
Academic Press, New York.Amanda Stent, Rashmi Prasad, and Marilyn Walker.2004.
Trainable sentence planning for complex infor-mation presentation in spoken dialog systems.
In Pro-ceedings of Association for Computational Linguis-tics, pages 79?86, Barcelona, Spain.Sandra Williams and Richard Power.
2008.
Derivingrhetorical complexity data from the rst-dt corpus.
InProceedings of the Sixth International Language Re-sources and Evaluation (LREC?08), May.Yuk Wah Wong and Raymond Mooney.
2007.
Gener-ation by inverting a semantic parser that uses statis-tical machine translation.
In Proceedings of the Hu-man Language Technology and the Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 172?179, Rochester, NY.Daniel H Younger.
1967.
Recognition and parsing forcontext-free languages in time n3.
Information andControl, 10(2):189?208.1514
