Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 980?989,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsData Driven Grammatical Error Detectionin Transcripts of Children?s SpeechEric MorleyCSLUOHSUPortland, OR 97239morleye@gmail.comAnna Eva HallinDepartment of CommunicativeSciences and DisordersNew York UniversityNew York, NYae.hallin@nyu.eduBrian RoarkGoogle ResearchNew York, NY 10011roarkbr@gmail.comAbstractWe investigate grammatical error detec-tion in spoken language, and present adata-driven method to train a dependencyparser to automatically identify and labelgrammatical errors.
This method is ag-nostic to the label set used, and the onlymanual annotations needed for training aregrammatical error labels.
We find that theproposed system is robust to disfluencies,so that a separate stage to elide disfluen-cies is not required.
The proposed systemoutperforms two baseline systems on twodifferent corpora that use different sets oferror tags.
It is able to identify utteranceswith grammatical errors with an F1-scoreas high as 0.623, as compared to a baselineF1 of 0.350 on the same data.1 IntroductionResearch into automatic grammatical error detec-tion has primarily been motivated by the task ofproviding feedback to writers, whether they be na-tive speakers of a language or second languagelearners.
Grammatical error detection, however, isalso useful in the clinical domain, for example, toassess a child?s ability to produce grammatical lan-guage.
At present, clinicians and researchers intochild language must manually identify and clas-sify particular kinds of grammatical errors in tran-scripts of children?s speech if they wish to assessparticular aspects of the child?s linguistic abilityfrom a sample of spoken language.
Such manualannotation, which is called language sample anal-ysis in the clinical field, is expensive, hinderingits widespread adoption.
Manual annotations mayalso be inconsistent, particularly between differentresearch groups, which may be investigating dif-ferent phenomena.
Automated grammatical errordetection has the potential to address both of theseissues, being both cheap and consistent.Aside from performance, there are at least twokey requirements for a grammatical error detectorto be useful in a clinical setting: 1) it must be ableto handle spoken language, and 2) it must be train-able.
Clinical data typically consists of transcriptsof spoken language, rather than formal written lan-guage.
As a result, a system must be preparedto handle disfluencies, utterance fragments, andother phenomena that are entirely grammatical inspeech, but not in writing.
On the other hand, asystem designed for transcripts of speech does notneed to identify errors specific to written languagesuch as punctuation or spelling mistakes.
Further-more, a system designed for clinical data must beable to handle language produced by children whomay have atypical language due to a developmen-tal disorder, and therefore may produce grammati-cal errors that would be unexpected in written lan-guage.
A grammatical error detector appropriatefor a clinical setting must also be trainable be-cause different groups of clinicians may wish toinvestigate different phenomena, and will there-fore prefer different annotation standards.
Thisis quite different from grammatical error detectorsfor written language, which may have models fordifferent domains, but which are not typically de-signed to enable the detection of novel error sets.We examine two baseline techniques for gram-matical error detection, then present a simple data-driven technique to turn a dependency parser into agrammatical error detector.
Interestingly, we findthat the dependency parser-based approach mas-sively outperforms the baseline systems in termsof identifying ungrammatical utterances.
Further-more, the proposed system is able to identify spe-cific error codes, which the baseline systems can-not do.
We find that disfluencies do not degradeperformance of the proposed detector, obviatingthe need (for this task) for explicit disfluency de-tection.
We also analyze the output of our systemto see which errors it finds, and which it misses.980Code Description Example[EO] Overgeneralization errors He falled [EO] .
[EW] Other word level errors He were [EW] looking .
[EU] Utterance level errors And they came to stopped .
[OM] Omitted bound morpheme He go [OM] .
[OW] Omitted word She [OW] running .Table 1: Error codes proposed in the SALT manual.
Note that in SALT annotated transcripts, [OM] and[OW] are actually indicated by ?*?
followed by the morpheme or word hypothesized to be omitted.When treating codes (other than [EU]) as tags, they are attached to the previous word in the string.Finally, we evaluate our detector on a second setof data with a different label set and annotationstandards.
Although our proposed system does notperform as well on the second data set, it still out-performs both baseline systems.
One interestingdifference between the two data sets, which doesappear to impact performance, is that the latter setmore strictly follows SALT guidelines (see Sec-tion 2.1) to collapse multiple errors into a singlelabel.
This yields transcripts with a granularity oflabeling somewhat less amenable to automation,to the extent that labels are fewer and can be re-liant on non-local context for aggregation.2 Background2.1 Systematic Analysis of LanguageTranscripts (SALT)The Systematic Analysis of Language Transcripts(SALT) is the de facto standard for clinicians look-ing to analyze samples of natural language.
TheSALT manual includes guidelines for transcrip-tion, as well as three types of annotations, ofwhich two are relevant here: maze annotations,and error codes.1Mazes are similar to what is referred to as ?dis-fluencies?
in the speech literature.
The SALTmanual defines mazes as ?filled pauses, falsestarts, repetitions, reformulations, and interjec-tions?
(Miller et al., 2011, p. 6), without definingany of these terms.
Partial words, which are in-cluded and marked in SALT-annotated transcripts,are also included in mazes.
Mazes are delimitedby parentheses, and have no internal structure, un-like disfluencies annotated following the Switch-board guidelines (Meteer et al., 1995), which arecommonly followed by the speech and language1SALT also prescribes annotation of bound morphemesand clitics, for example -ed in past tense verbs.
We preprocessall of the transcripts to remove bound morpheme and cliticannotations.processing communities.
An example maze anno-tation would be: ?He (can not) can not get up.
?The SALT manual proposes the set of errorcodes shown (with examples) in Table 1, but re-search groups may use a subset of these codes, oraugment them with additional codes.
For example,the SALT-annotated Edmonton Narrative NormsInstrument (ENNI) corpus (Schneider et al., 2006)rarely annotates omitted morphemes ([OM]), in-stead using the [EW] code.
Other SALT-annotatedcorpora include errors that are not described in theSALT manual.
For example the CSLU ADOS cor-pus (Van Santen et al., 2010) includes the [EX]tag for extraneous words, and the Narrative StoryRetell corpus (SALT Software, 2014b) uses thecode [EP] to indicate pronominal errors (albeitinconsistently, as many such errors are coded as[EW] in this corpus).
We note that the definitionsof certain SALT errors, notably [EW] and [EU],are open to interpretation, and that these codescapture a wide variety of errors.
For example,some of the errors captured by the [EW] code are:pronominal case and gender errors; verb tense er-rors; confusing ?a?
and ?an?
; and using the wrongpreposition.The SALT guidelines specify as a general rulethat annotators should not mark utterances withmore than two omissions ([OM] or [OW]) and/orword-level errors (ex [EW], [EP]) (SALT Soft-ware, 2014a).
Instead, annotators are instructedto code such utterances with an utterance-level er-ror ([EU]).
How strictly annotators adhere to thisrule affects the distribution of errors, reducing thenumber of word-level errors and increasing thenumber of utterance-level errors.
Following thisrule also increases the variety of errors capturedby the [EU] code.
The annotations in differentcorpora, including ENNI and NSR, vary in howstrictly they follow this rule, even though this isnot mentioned in the the published descriptions of981these corpora.2.2 Grammatical Error DetectionThe most visible fruits of research into grammati-cal error detection are the spellchecking and gram-mar checking tools commonly included with wordprocessors, for example Microsoft Word?s gram-mar checker.
Although developed for handlingwritten language, many of the techniques usedto address these tasks could still be applicable totranscripts of speech because many of the sameerrors can still occur.
The earliest grammatical-ity tools simply performed pattern matching (Mac-donald et al., 1982), but this approach is not robustenough to identify many types of errors, and pat-tern matching systems are not trainable, and there-fore cannot be adapted quickly to new label sets.Subsequent efforts to create grammaticality classi-fiers and detectors leveraged information extractedfrom parsers (Heidorn et al., 1982) and languagemodels (Atwell, 1987).
These systems, however,were developed for formal written English pro-duced by well-educated adults, as opposed to spo-ken English produced by young children, partic-ularly children with suspected developmental de-lays.There have been a few investigations into tech-niques to automatically identify particular con-structions in transcripts of spoken English.
Bow-den and Fox (2002) proposed a rule-based sys-tem to classify many types of errors made bylearners of English.
Although their system couldbe used on either transcripts of speech, or onwritten English, they did not evaluate their sys-tem in any way.
Caines and Buttery (2010) usea logistic regression model to identify the zero-auxiliary construction (e.g., ?you going home??
)with over 96% accuracy.
Even though the zero-auxilliary construction is not necessarily ungram-matical, identifying such constructions may beuseful as a preprocessing step to a grammatical-ity classifier.
Caines and Buttery also demonstratethat their detector can be integrated into a sta-tistical parser yielding improved performance, al-though they are vague about the nature of the parseimprovement (see Caines and Buttery, 2010, p. 6).Hassanali and Liu (2011) conducted the first in-vestigation into grammaticality detection and clas-sification in both speech of children, and speech ofchildren with language impairments.
They identi-fied 11 types of errors, and compared three typesof systems designed to identify the presence ofeach type of error: 1) rule based systems; 2) deci-sion trees that use rules as features; and 3) naiveBayes classifiers that use a variety of features.They were able to identify all error types well(F1 > 0.9 in all cases), and found that in generalthe statistical systems outperformed the rule basedsystems.
Hassanali and Liu?s system was designedfor transcripts of spoken language collected fromchildren with impaired language, and is able todetect the set of errors they defined very well.However, it cannot be straightforwardly adaptedto novel error sets.Morley et al.
(2013) evaluated how well thedetectors proposed by Hassanali and Liu couldidentify utterances with SALT error codes.
Theyfound that a simplified version of one of Has-sanali and Liu?s detectors was the most effective atidentifying utterances with any SALT error codes,although performance was very low (F1=0.18).Their system uses features extracted solely frompart of speech tags with the Bernoulli Naive Bayesclassifier in Scikit (Pedregosa et al., 2012).
Theirdetector may be adaptable to other annotationstandards, but it does not identify which errors arein each utterance; it only identifies which utter-ances have errors, and which do not.2.3 Redshift ParserWe perform our experiments with the redshiftparser2, which is an arc-eager transition-based de-pendency parser.
We selected redshift because ofits ability to perform disfluency detection and de-pendency parsing jointly.
Honnibal and Johnson(2014) demonstrate that this system achieves state-of-the-art performance on disfluency detection,even compared to single purpose systems such asthe one proposed by Qian and Liu (2013).
Ra-sooli and Tetreault (2014) have developed a sys-tem that performs disfluency detection and depen-dency parsing jointly, and with comparable perfor-mance to redshift, but it is not publicly available asof yet.Redshift uses an averaged perceptron learner,and implements several feature sets.
The first fea-ture set, which we will refer to as ZHANG is theone proposed by Zhang and Nivre (2011).
It in-cludes 73 templates that capture various aspectsof: the word at the top of the stack, along with its2Redshift is available at https://github.com/syllog1sm/redshift.
We use the version in theexperiment branch from May 15, 2014.982leftmost and rightmost children, parent and grand-parent; and the word on the buffer, along withits leftmost children; and the second and thirdwords on the buffer.
Redshift also includes fea-tures extracted from the Brown clustering algo-rithm (Brown et al., 1992).
Finally, redshift in-cludes features that are designed to help iden-tify disfluencies; these capture rough copies, ex-act copies, and whether neighboring words weremarked as disfluent.
We will refer to the featureset containing all of the features implemented inredshift as FULL.
We refer the reader to Honnibaland Johnson (2014) for more details.3 Data, Preprocessing, and EvaluationOur investigation into using a dependency parserto identify and label grammatical errors requirestraining data with two types of annotations: de-pendency labels, and grammatical error labels.
Weare not aware of any corpora of speech with bothof these annotations.
Therefore, we use two dif-ferent sets of training data: the Switchboard cor-pus, which contains syntactic parses; and SALTannotated corpora, which have grammatical errorannotations.3.1 SwitchboardThe Switchboard treebank (Godfrey et al., 1992)is a corpus of transcribed conversations that havebeen manually parsed.
These parses includeEDITED nodes, which span disfluencies.
We pre-process the Switchboard treebank by removing allpartial words as well as all words dominated byEDITED nodes, and converting all words to lower-case.
We then convert the phrase-structure trees todependencies using the Stanford dependency con-verter (De Marneffe et al., 2006) with the basic de-pendency scheme, which produces dependenciesthat are strictly projective.3.2 SALT Annotated CorporaWe perform two sets of experiments on the twoSALT-annotated corpora described in Table 2.
Wecarry out the first set of experiments on on the Ed-monton Narrative Norms Instrument (ENNI) cor-pus, which contains 377 transcripts collected fromchildren between the ages of 3 years 11 monthsand 10 years old.
The children all lived in Edmon-ton, Alberta, Canada, were typically developing,and were native speakers of English.After exploring various system configurations,ENNI NSRWords Utts Words UttsTrain 360,912 44,915 103,810 11,869Dev.
45,504 5,614 12,860 1,483Test 44,996 5,615 12,982 1,485% with error 13.2 14.3(a) Word and utterance countsENNI NSR[EP] 0 20[EO] 0 495[EW] 4,916 1,506[EU] 3,332 568[OM] 10 297[OW] 766 569Total 9,024 3,455(b) Error code countsTable 2: Summary of ENNI and NSR Corpora.There can be multiple errors per utterance.
Wordcounts include mazes.we evaluate how well our method works when itis applied to another corpus with different anno-tation standards.
Specifically, we train and testour technique on the Narrative Story Retell (NSR)corpus (SALT Software, 2014b), which contains496 transcripts collected from typically develop-ing children living in Wisconsin and Californiawho were between the ages of 4 years 4 monthsand 12 years 8 months old.
The ENNI and NSRcorpora were annotated by two different researchgroups, and as Table 2 illustrates, they containa different distribution of errors.
First, ENNIuses the [EW] (other word-level error) tag to codeboth overgeneralization errors instead of [EO], andomitted morphemes instead of [OM].
The [EU]code is also far more frequent in ENNI than NSR.Finally, the NSR corpus includes an error code thatdoes not appear in the ENNI corpus: [EP], whichindicates a pronominal error, for example usingthe wrong person or case.
[EP], however, is rarelyused.We preprocess the ENNI and NSR corpora toreconstruct surface forms from bound morphemeannotations (ex.
?go/3S?
becomes ?goes?
), partialwords, and non-speech sounds.
We also either ex-cise manually identified mazes or remove mazeannotations, depending upon the experiment.3.3 EvaluationEvaluating system performance in tagging taskson manually annotated data is typically straight-983Evaluation Level: ERROR UTTERANCEIndividual error codes Has error?Gold error codes: [EW] [EW] YesPredicted error codes: [EW] [OW] YesEvaluation: TP FN FP TPFigure 1: Illustration of UTTERANCE and ERROR level evaluationTP = true positive; FP = false positive; FN = false negativeforward: we simply compare system output to thegold standard.
Such evaluation assumes that thebest system is the one that most faithfully repro-duces the gold standard.
This is not necessarilythe case with applying SALT error codes for threereasons, and each of these reasons suggests a dif-ferent form of evaluation.First, automatically detecting SALT error codesis an important task because it can aid clini-cal investigations.
As Morley et al.
(2013) il-lustrated, even extremely coarse features derivedfrom SALT annotations, for example a binary fea-ture for each utterance indicating the presence ofany error codes, can be of immense utility for iden-tifying language impairments.
Therefore, we willevaluate our system as a binary tagger: each ut-terance, both in the manually annotated data andsystem output either contains an error code, or itdoes not.
We will label this form of evaluation asUTTERANCE level.Second, clinicians are not only interested inhow many utterances have an error, but also whichparticular errors appear in which utterances.
Toaddress this issue, we will compute precision, re-call, and F1 score from the counts of each er-ror code in each utterance.
We will label thisform of evaluation as ERROR level.
Figure 1 illus-trates both UTTERANCE and ERROR level evalua-tion.
Note that the utterance level error code [EU]is only allowed to appear once per utterance.
Asa result, we will ignore any predicted [EU] codesbeyond the first.Third, the quality of the SALT annotationsthemselves is unknown, and therefore evaluationin which we treat the manually annotated data as agold standard may not yield informative metrics.Morley et al.
(2014) found that there are likelyinconsistencies in maze annotations both withinand across corpora.
In light of that finding, it ispossible that error code annotations are somewhatinconsistent as well.
Furthermore, our approachhas a critical difference from manual annotation:we perform classification one utterance at a time,while manual annotators have access to the contextof an utterance.
Therefore certain types of errors,for example using a pronoun of the wrong gender,or responding ungrammatically to a question (ex.
?What are you doing??
?Eat.?)
will appear gram-matical to our system, but not to a human anno-tator.
We address both of these issues with an in-depth analysis of the output of one of our systems,which includes manually re-coding utterances outof context.4 Detecting Errors in ENNI4.1 BaselinesWe evaluate two existing systems to see how ef-fectively they can identify utterances with SALTerror codes: 1) Microsoft Word 2010?s gram-mar check, and 2) the simplified version of Has-sanali and Liu?s grammaticality detector (2011)proposed by Morley et al.
(2013) (mentioned inSection 2.2).
We configured Microsoft Word2010?s grammar check to look for the followingclasses of errors: negation, noun phrases, subject-verb agreement, and verb phrases (see http://bit.ly/1kphUHa).
Most error classes in gram-mar check are not relevant for transcribed speech,for example capitalization errors or confusing it?sand its; we selected classes of errors that wouldtypically be indicated by SALT error codes.Note that these baseline systems can only giveus an indication of whether there is an error inthe utterance or not; they do not provide the spe-cific error tags that mimic the SALT guidelines.Hence we evaluate just the UTTERANCE level per-formance of the baseline systems on the ENNI de-velopment and test sets.
These results are givenin the top two rows of each section of Table 3.We apply these systems to utterances in two condi-tions: with mazes (i.e., disfluencies) excised; andwith unannotated mazes left in the utterances.
Ascan be seen in Table 3, the performance MicrosoftWord?s grammar checker degrades severely when984(a)Him [EW] (can not) can not get up .
(b)ROOT him can not can not get up .nsubj+[EW]auxnegauxnegROOTprtPFigure 2: (a) SALT annotated utterance; mazes indicated by parentheses; (b) Dependency parse of sameutterance parsed with a grammar trained on the Switchboard corpus and augmented dependency labels.We use a corpus of parses with augmented labels to train our grammaticality detector.mazes are not excised, but this is not the case forthe Morley et al.
(2013) detector.4.2 Proposed SystemUsing the ENNI corpus, we now explore variousconfigurations of a system for grammatical errorcode detection.
All of our systems use redshiftto learn grammars and to parse.
First, we trainan initial grammar G0on the Switchboard tree-bank (Godfrey et al., 1992) (preprocessed as de-scribed in Section 3.1).
Redshift learns a model forpart of speech tagging concurrently with G0.
Weuse G0to parse the training portion of the ENNIcorpus.
Then, using the SALT annotations, weappend error codes to the dependency arc labelsin the parsed ENNI corpus, assigning each errorcode to the word it follows in the SALT annotateddata.
Figure 2 shows a SALT annotated utterance,as well as its dependency parse augmented witherror codes.
Finally, we train a grammar GErronthe parse of the ENNI training fold that includesthe augmented arc labels.
We can now use GErrto automatically apply SALT error codes: they aresimply encoded in the dependency labels.
We alsoapply the [EW] label to any word that is in a list ofovergeneralization errors3.We modify three variables in our initial trials onthe ENNI development set.
First, we change theproportion of utterances in the training data thatcontain an error by removing utterances.4Doingso allows us to alter the operating point of our sys-3The list of overgeneralization errors was generously pro-vided by Kyle Gorman4Of course, we never modify the development or test data.tem in terms of precision and recall.
Second, weagain train and test on two versions of the ENNIcorpus: one which has had mazes excised, and theother which has them present (but not annotated).Third, we evaluate two feature sets: ZHANG andFULL.The plots in Figure 3 show how the per-formances of our systems at different operatingpoints vary, while Table 3 shows the performanceof our best system configurations on the ENNI de-velopment and test sets.
Surprisingly, we see thatneither the choice of feature set, nor the presenceof mazes has much of an effect on system per-formance.
This is in strong contrast to MicrosoftWord?s grammar check, which is minimally effec-tive when mazes are included in the data.
TheMorley et al.
(2013) system is robust to mazes,but still performs substantially worse than our pro-posed system.4.3 Error AnalysisWe now examine the errors produced by our bestperforming system for data in which mazes arepresent.
As shown in Table 3, when we apply oursystem to ENNI-development, the UTTERANCEP/R/F1 is 0.831 / 0.502 / 0.626 and the ERRORP/R/F1is 0.759 / 0.434 / 0.552.
This system?s per-formance detecting specific error codes is shownin Table 4.
We see that the recall of [EU] errors isquite low compared with the recall for [EW] and[OW] errors.
This is not surprising, as human an-notators may need to leverage the context of an ut-terance to identify [EU] errors, while our systemmakes predictions for each utterance in isolation.985(a) UTTERANCE level evaluation (b) ERROR level evaluationFigure 3: SALT error code detection performance at various operating points on ENNI development setEval Mazes Excised Mazes PresentSystem type P R F1 P R F1DevelopmentMS Word UTT 0.843 0.245 0.380 0.127 0.063 0.084Morley et al.
(2013) UTT 0.407 0.349 0.376 0.343 0.321 0.332Current paperUTT 0.943 0.470 0.627 0.831 0.502 0.626ERR 0.895 0.412 0.564 0.759 0.434 0.552TestMS Word UTT 0.824 0.209 0.334 0.513 0.219 0.307Morley et al.
(2013) UTT 0.375 0.328 0.350 0.349 0.252 0.293Current PaperUTT 0.909 0.474 0.623 0.809 0.501 0.618ERR 0.682 0.338 0.452 0.608 0.360 0.452Table 3: Baseline and current paper systems?
performance on ENNI.
Evaluation is at the UTTERANCE(UTT) level except for the current paper?s system, which also presents evaluation at the ERROR (ERR)level.Error Code P R F1EU 0.639 0.193 0.297EW 0.832 0.582 0.685OW 0.680 0.548 0.607Table 4: ERROR level detection performance foreach code (system trained on ENNI; 30% errorutterances; ZHANG feature set; with mazes)We randomly sampled 200 utterances from thedevelopment set that have a manually annotatederror, are predicted by our system to have an er-ror, or both.
A speech-language pathologist whohas extensive experience with using SALT for re-search purposes in both clinical and typically de-veloping populations annotated the errors in eachutterance.
She annotated each utterance in isola-tion so as to ignore contextual errors.
We compareour annotations to the original annotations, andsystem performance using our annotations and theoriginal annotations as different gold standards.The results of this comparison are shown in Table5.Comparing our manual annotations to the orig-inal annotations, we notice some disagreements.We suspect there are two reasons for this.
First,unlike the original annotators, we annotate theseutterances out of context.
This may explain whywe identify far fewer utterance level error [EU]codes than the original annotators (20 comparedwith 67).
Second, we may be using different cri-teria for each error code than the original anno-tators.
This is an inevitable issue, as the SALTguidelines do not provide detailed definitions ofthe error codes, nor do individual groups of anno-tators.
To illustrate, the ?coding notes?
section of986Tag Gold Gold Count Disagreement P R F1[EU] Original 67 52 0.500 0.149 0.230Revised 20 0.450 0.333 0.383[EW] Original 137 27 0.859 0.533 0.658Revised 126 0.800 0.540 0.645[OW] Original 16 13 0.667 0.275 0.480Revised 15 0.444 0.267 0.333Table 5: System performance using ERROR level evaluation on 200 utterances selected from ENNI-devusing original and revised annotations as gold standardUTTERANCE level ERROR levelSystem P R F1 P R F1ENNI-trained 0.310 0.124 0.178 0.157 0.057 0.084NSR-trained 0.243 0.249 0.277 0.150 0.195 0.170MS Word 0.561 0.171 0.261 ?
?
?Morley et al.
(2013) 0.250 0.281 0.264 ?
?
?NSR ?
MS Word 0.291 0.447 0.353 ?
?
?NSR ?
Morley et al.
(2013) 0.297 0.387 0.336 ?
?
?All 3 0.330 0.498 0.397 ?
?
?Table 6: Error detection performance on NSR-development, mazes includedthe description of the ENNI corpus5only lists theerror codes that were used consistently, but doesnot describe how to apply them.
These findingsillustrate the importance of having a rapidly train-able error code detector: research groups will beinterested in different phenomena, and thereforewill likely have different annotation standards.5 Detecting Errors in NSRWe apply our system directly to the NSR corpuswith mazes included.
We use the same parametersset on the ENNI corpus in Section 4.2.
We applythe model trained on ENNI to NSR, but find that itdoes not perform very well as illustrated in Table6.
These results further underscore the need fora trainable error code detector in this domain, asopposed to the static error detectors that are morecommon in the grammatical error detection litera-ture.We see in Table 6 that retraining our modelon NSR data improves performance substantially(UTTERANCE F1 improves from 0.178 to 0.277),but not to the level we observed on the ENNI cor-pus.
The Morley et al.
(2013) system also per-forms worse when trained and tested on NSR, ascompared with ENNI.
When mazes are included,5http://www.saltsoftware.com/salt/databases/ENNIRDBDoc.pdfthe performance of Microsoft Word?s grammarcheck is higher on NSR than on ENNI (F1=0.261vs 0.084), but it it still yields the lowest perfor-mance of the three systems.
We find that combin-ing our proposed system with either or both of thebaseline systems further improves performance.The NSR corpus differs from ENNI in severalways: it is smaller, contains fewer errors, and usesa different set of tags with a different distributionfrom the ENNI corpus, as shown in Table 2.
Wefound that the smaller amount of training data isnot the only reason for the degradation in perfor-mance; we trained a model for ENNI with a set oftraining data that is the same size as the one forNSR, but did not observe a major drop in perfor-mance.
We found that UTTERANCE F1 drops from0.626 to 0.581, and ERROR F1 goes from 0.552 to0.380, not nearly the magnitude drop in accuracyobserved for NSR.We believe that a major reason for why our sys-tem performs worse on NSR than ENNI may bethat the ENNI annotations adhere less strictly tocertain SALT recommendations than do the onesin NSR.
The SALT guidelines suggest that utter-ances with two or more word-level [EW] and/oromitted word [OW] errors should only be taggedwith an utterance-level [EU] error (SALT Soft-ware, 2014a).
ENNI, however, has many utter-987ances with multiple [EW] and [OW] error codes,along with utterances containing all three errorcodes.
NSR has very few utterances with [EU] andother codes, or multiple [EW] and [OW] codes.The finer grained annotations in ENNI may sim-ply be easier to learn.6 Conclusion and Future DirectionsWe have proposed a very simple method to rapidlytrain a grammatical error detector and classifier.Our proposed system only requires training datawith error code annotations, and is agnostic as tothe nature of the specific error codes.
Furthermore,our system?s performance does not appear to beaffected by disfluencies, which reduces the burdenrequired to produce training data.There are several key areas we plan to inves-tigate in the future.
First, we would like to ex-plore different update functions for the parser; thepredicted error codes are a byproduct of parsing,but we do not care what the parse itself looks like.At present, the parser is updated whenever it pro-duces a parse that diverges from the gold stan-dard.
It may be better to update only when theerror codes predicted for an utterance differ fromthe gold standard.
Second, we hope to explore fea-tures that could be useful for identifying grammat-ical errors in multiple data sets.
Finally, we planto investigate why our system performed so muchbetter on ENNI than on NSR.AcknowledgmentsWe would like to thank the following people forvaluable input into this study: Joel Tetreault,Jan van Santen, Emily Prud?hommeaux, KyleGorman, Steven Bedrick, Alison Presmanes Hilland others in the CSLU Autism research groupat OHSU.
This material is based upon worksupported by the National Institute on Deafnessand Other Communication Disorders of the Na-tional Institutes of Health under award numberR21DC010033.
The content is solely the respon-sibility of the authors and does not necessarily rep-resent the official views of the National Institutesof Health.ReferencesEric Atwell.
1987.
How to detect grammatical er-rors in a text without parsing it.
In Bente Maegaard,editor, EACL, pages 38?45, Copenhagen, Denmark,April.
The Association for Computational Linguis-tics.Mari I Bowden and Richard K Fox.
2002.
A diagnos-tic approach to the detection of syntactic errors inenglish for non-native speakers.
The University ofTexas?Pan American Department of Computer Sci-ence Technical Report.Peter F. Brown, Vincent J. Della Pietra, Peter V.de Souza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Andrew Caines and Paula Buttery.
2010.
You talkingto me?
: A predictive model for zero auxiliary con-structions.
In Proceedings of the 2010 Workshop onNLP and Linguistics: Finding the Common Ground,pages 43?51.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.John J Godfrey, Edward C Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech cor-pus for research and development.
In IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing, volume 1, pages 517?520.Khairun-nisa Hassanali and Yang Liu.
2011.
Measur-ing language development in early childhood educa-tion: a case study of grammar checking in child lan-guage transcripts.
In Proceedings of the 6th Work-shop on Innovative Use of NLP for Building Educa-tional Applications, pages 87?95.George E. Heidorn, Karen Jensen, Lance A. Miller,Roy J. Byrd, and Martin S Chodorow.
1982.The EPISTLE text-critiquing system.
IBM SystemsJournal, 21(3):305?326.Matthew Honnibal and Mark Johnson.
2014.
Jointincremental disfluency detection and dependencyparsing.
TACL, 2:131?142.Nina H Macdonald, Lawrence T Frase, Patricia S Gin-grich, and Stacey A Keenan.
1982.
The writer?sworkbench: Computer aids for text analysis.
Edu-cational psychologist, 17(3):172?179.Marie W Meteer, Ann A Taylor, Robert MacIntyre,and Rukmini Iyer.
1995.
Dysfluency annotationstylebook for the switchboard corpus.
University ofPennsylvania.Jon F Miller, Karen Andriacchi, and Ann Nockerts.2011.
Assessing language production using SALTsoftware: A clinician?s guide to language sampleanalysis.
SALT Software, LLC.988Eric Morley, Brian Roark, and Jan van Santen.
2013.The utility of manual and automatic linguistic errorcodes for identifying neurodevelopmental disorders.In Proceedings of the Eighth Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pages 1?10, Atlanta, Georgia, June.
Associa-tion for Computational Linguistics.Eric Morley, Anna Eva Hallin, and Brian Roark.
2014.Challenges in automating maze detection.
In Pro-ceedings of the First Workshop on ComputationalLinguistics and Clinical Psychology, pages 69?77,Baltimore, Maryland, June.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake VanderPlas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and Edouard Duchesnay.
2012.Scikit-learn: Machine learning in python.
CoRR,abs/1201.0490.Xian Qian and Yang Liu.
2013.
Disfluency detectionusing multi-step stacked learning.
In Lucy Vander-wende, Hal Daum?e III, and Katrin Kirchhoff, edi-tors, HLT-NAACL, pages 820?825, Atlanta, Georgia,USA, June.
The Association for Computational Lin-guistics.Mohammad Sadegh Rasooli and Joel R. Tetreault.2014.
Non-monotonic parsing of fluent umm i meandisfluent sentences.
In Gosse Bouma and YannickParmentier, editors, EACL, pages 48?53, Gothen-burg, Sweden, April.
The Association for Compu-tational Linguistics.LLC SALT Software.
2014a.
Course1306: Transcription - Conventions Part 3.http://www.saltsoftware.com/onlinetraining/section-page?OnlineTrainingCourseSectionPageId=76.
[Online; accessed 29-May-2104].LLC SALT Software.
2014b.
NarrativeStory Retell Database.
http://www.saltsoftware.com/salt/databases/NarStoryRetellRDBDoc.pdf.
[Online;accessed 29-May-2104].Phyllis Schneider, Denyse Hayward, and Rita VisDub?e.
2006.
Storytelling from pictures usingthe edmonton narrative norms instrument.
Jour-nal of Speech Language Pathology and Audiology,30(4):224.Jan PH Van Santen, Emily T Prud?hommeaux, Lois MBlack, and Margaret Mitchell.
2010.
Com-putational prosodic markers for autism.
Autism,14(3):215?236.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InACL (Short Papers), pages 188?193, Portland, Ore-gon, USA, June.
The Association for ComputationalLinguistics.989
