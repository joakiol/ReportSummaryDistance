Design of a Hybrid Deterministic ParserKanaan A. FaisalInformation ,and Computer Science DepartmentKing Fahd University of Petroleum and Minerals TDhahran 31261Kingdom of Saudi Aa'abiaStart C. KwasnyCenter for Intelligent Computer Systems :IDepartment of Computer ScienceWashington UniversitySt.
Louis, MO 63130-4899U.S.A.1.
IntroductionA deterministic parser is under development whichrepresents a departure from traditional deterministicparsers in that it combines both symbolic and connec-tionist components.
The connectionist component istrained either from patterns derived from the rules of adeterministic grammar.
~The development and evolu-tion of such a hybrid architecture has lead to a parserwhich is superior to any known deterministic parser.Experiments are described and powerful training tech-niques are demonstrated that permit decision-makingby the connectionist component in the parsing process.This approach as permitted some simplifications to therules of other deterministic parsers, including the elimi-nation of rule packets and priorities.
Furthermore,parsing is performed more robustly and with moretolerance for error.
Data are presented which showhow a connectionist (neural) network trained withlinguistic rules can parse both expected (grammatical)sentences as well as some novel (ungrammatical or lex-ically ambiguous) entences.2.
Determinism and ParsingThe determinism hypothesis which forms the basis forPARSIFAL (Marcus, 1980) imposes important restric-tions on NatLu'al Language Processing.
It states (p.ll)that"Natural Language can be parsed by amechanism that operates 'strictlydeterministically' in that it does not simulate anondeterministic machine..."If we accept this hypothesis, it must follow that pro-cessing need not depend in any fundamental way onbacktracking.
As a further consequence, no partialstructures are produced during parsing which fail tobecome part of the final structure.
PARSIFAL was thefirst of a number of systems to demonstrate how deter-ministic parsing of Natural Language can be performedusing a rule-based grammar.
Extensions to PARSIFALhave been researched independently including the pars-ing of ungrammatical sentences in PARAGRAM(Charniak, 1983), the resolution of lexical ambiguitiesin ROBIE (Milne, 1986), and the acquiring of syntacticrules from examples in LPARSIFAL (Berwick, 1985).t The first author gratefldly ackmowledge the support of KhlgFahd University of Petroleum and Minerals.Traditional deterministic parsers process input sen-tences primarily left-to-right.
Determinism is accom-plished by permitting a lookahead of up to three consti-tuents with a constituent buffer designated for that pur-pose.
To permit embedded structures, a stack is alsopart of the architecture.
Rules are partitioned into rulepackets which dynamically become active or inactiveduring parsing, but are usually associated with thecurrent (top-level) node of the structure being built.
Asingle processing step consists of selecting a rule thatcan fire from an active rule packet, firing the rule, andperforming its action.
Conflicts are resolved withinpackets from the static ordering (priority) of rules.
Theaction effects changes to the stack and buffer.
After aseries of processing steps, a termination rule fires andprocessing ends.
The final structure is left on top of thestack.3.
Hybrid Deterministic ParsingOur parser takes the approach of deterministic parsingand combines it with connectionism.
McClelland andKawamoto (1986, p.317) first suggested the combina-tion of these ideas.
Deterministic parsing provides asetting in which no backtracking occurs while connec-tionism provides generalization and robustness.
Ourgoal is to combine the two in a way that enhances theiradvantages and minimizes their faults.
In simple terms,the rules of the deterministic parser are replaced by anetwork which is trained from training sequencesderived from the grammar ules.
The network embo-dies the decision-making component of the parser andmaps a state of the parser to an action.
Actions are per-formed in the traditional way by symbolically manipu-lating the stack and buffer contents.Parsing experiments are conducted to determine theeffectiveness of training by attempting to processungrammatical and lexically ambiguous sentenceforms.
The performance of our hybrid parser dependson the extent and nature of the training.
Once trained,the network is efficient, both in terms of representationand execution.3.1.
DesignSome sm~l modifications to deterministic grammarrules arc necessary to insure the suitability of each rulefor use with our "winner-take-all" network.
Many ofthese changes are simplifications that have been~: The sponsors of the Center are McDonnell DouglasCoq~oration and Southwestern Bell Telephone Company.1 11Rule Main-verbIF:THEN:in packet parse-vppriority: 10The first element in buffer is a verbRule Create._VPIF: current node is S nodeAttached is AUX nodefirst is a verbDEACTIVATE packet parse-vp THEN:if the active node is a major sentencethen ACTIVATE packet ss-finalelse if the active node is a secondary sentencethen ACTIVATE emb-s-final.CREATE a VP node.ATTACH a VP node to the S.ATTACH the first element in the buffer tothe active node as verb.ACTIVATE the clause level packet cpool THEN~if verb is labeled passivethen ACTIVATE the packet passiveand RUN the grammar rule passive next.CREATE VP nodeRule Main_verbIF: current node is VP nodeAttached is AUX nodefirst is a verbATTACH as MVBFigure I: PARSIFAL and Hybrid Parser Rules Comparedproposed by others and are not essential to the successof our approach.
All of these changes are madewithout substantially altering the capabilitiesrepresented in the original grammar ules.
Changesinclude: elimination of the packet system; removal ofattention-shifting rules; removal of rule priorities;reduction of lookahead to two positions instead ofthree; and revision of the rules so that a single action isperformed by each.As an example, consider part of one sample grammarrule from PARSIFAL and its reformulation in thehybrid parser.
Figure 1 shows the two styles side-by-side.
Rule actions are in capital etters; rule names arein bold.
In the PARSIFAL rule, a priority number isgiven explicitly and the rule contains multiple actionsand conditionals similar to a programming language.
Itexplicitly activates and deactivates rule packets, exe-cutes rules, creates new phrase structure nodes, andtests for complex properties of the elements in thebuffer.Rules in the hybrid parser eliminate many of thesedetails without substantially changing the capabilitiesof the grammar.
In the figure, two of several rulesderived from the Main-verb rule are shown.
In thefirst rule, a new VP active node is created on the stackand in the second rule the verb is attached as a mainverb to the active node (VP) on top of the stack.
Withthe elimination of rule packeting, no priorities norexplicit packet activations/deactivations are required.While this mechanism is precisely what is required forefficient design of a symbolic parser, priorities are atthe essence of what is learned when training the con-nectionist component of the hybrid.
Actions such ascreating and attaching or selecting the argument struc-ture of the verb are carried out symbolically in thehybrid parser.
Also, a symbolic lexicon is consulted todetermine the properties of words.
When a predicatesuch as a verb is encountered, the requirements orexpectations for its arguments are made part of thefeatures of the active VP node, thus affecting whichactions will be executed later on.3.2.
Evolutionary Steps from PARSIFALElimination of the packet system.
In PARSIFAL, rulesare organized into packets.
Only those rules in anactive packet are considered while processing.
Often,more than one packet is active.
For example, thepacket CPOOL, or clause level packet, is always active.Since the hybrid parser has no packets, every rule isconsidered in parallel with the situation dictating whichaction should be taken.Removal of attention-shifting rules.
PARSIFAL relieson attention-shifting rules to transparently build certainconstituents, particularly NPs, which begin in thesecond buffer position.
For example, in the sentencetaken from Marcus: Have the students who missed theexam taken the makeup today?, the subject-aux inver-sion mechanism (switch) must be deliberately post-poned until the NP starting in the second position isanalyzed as a complete constituent.
Only then can theinversion take place.
PARSIFAL solves this problemby temporarily shifting buffer positions so that theparser is viewing the buffer beginning in the secondposition.
The second lefunost complete constituent (theNP) is then reduced before the first element constituent.We follow the lead of Berwick (1985) and others in ourtreatment of such cases by using the parse stack as a"movement s ack" and stack the postponed item.
Twoactions, PUSH and DROP, are suitable for this purpose.In the example above, the end of the noun phrase, thestudents, can not be determined without applying therules to the embedded clause.
When complete, the NPis dropped into the buffer and the auxilim'y verb can bere-inserted into the buffer allowing the inversion cantake place.
Note that at no point is the "monotonic"property of determinism violated by undoing previousactions.Removal of rule priorities.
In PARSIFAL, rules areordered by priority.
In the hybrid parser, rules have nopriority.
They compete with each other and the mostrelevant rule, based on training, wins the competition.Only one action, corresponding to the Iiring of onesingle-action rule, will be performed on each process-ing step.
The current active node and its attachments12 2along with the contents of the two buffer cells is thebasis for this decision.
The rules are coded in such away that every rule has a unique left-hand side and isthus relevant o situations most similar to its left-handside pattern.Restriction of grammar rule format.
The format ofgrammar ules in the hybrid parser is different fromPARSIFAL in two ways.
First, grammar rules are for-bidden to have more than a single action which is per-formed on the first buffer cell only; and second, rulepatterns are defined to uniformly mention items in bothbuffer cells.Grammar actions.
The repertoire of rule actions isslightly different in the hybrid parser.
Actions such asACTIVATE and DEACTIVATE have been removed.The basic actions are:a) ATTACH as <node>: The first item in the buffer isattached through an intermediate descriptive<node> to the current active node.b) CREATE <type>: Generates a new node of type<type> and pushes it onto the parse stack as thecurrent active node.c) DROP: Pops a node or an item off the top of tilestack and inserts it into the buffer in the first bufferposition.
The previous contents of the buffer isshifted back by one position.d) INSERT <item>: Inserts the designated item intothe buffer in the first buffer position.
The previouscontents of the buffer is shifted back by one posi-tion.
In the general form, only a small number ofdesignated lexical items (you, to, be, wh-marker)can be inserted.
The special form INSERT TRACEinserts an (unbounded) NP trace.e) LABEL <feature>: Adds designated feature to thefirst buffer item.f) PUSH: Pushes an item onto the stack for temporarystorage whenever the parse stack is used as a move-ment stack.g) SWITCH: Exchanges the items in the first andsecond buffer positions.These are the only actions the grammar ules can per-form.
The buffer is manageA symbolically and if aposition is vacated, an item is taken from the inputstream to fill the position.
The connectionist com-ponent can only examine the current active node, itsimmediate attachments, and the features of the first twobuffer items.
Once a node is attached to its parent, itcan never again be examined.3.3.
The GrammarThe hybrid parser is capable of successfully processinga wide; variety of sentence forms such as simpledeclarative sentences, passives, imperatives, yes-noquestions, wh-questions, wh-clauses, and other embed-ted sentences.
The grammar to be learned by the sub-symbolic system, which has 73 rules, can be separatedinto base phrase structure rules and transformational-type rules.
The base structure system can be furtherbroken down into rules for NPs, VPs, auxiliaries, mainsentence, PPs, and embedded sentences.Transformational rules fall into two groups: simplelocal transformations (like subject-aux inversion) andmajor movement rules like wh movement.
In general,for each type of phrase, creation of the phrase (creatinga new node on the active node stack) and completion ofthe phrase (dropping it into the buffer) is carried out bya separate grammar rule action.The rules for analyzing verb phrases discriminateamong verbs that take different kinds of complements.For example, verbs that take a wh complement arediscriminated from ones that take a that complement.Verbs like want that take either a missing or lexicalsubject in embedded sentential complements areseparated from verbs like try or believe that do not takea lexical subject.
Verbs that take one NP object are dis-tinguished from ones that take two NP objects throughlexical features.4.
Architecture of the Hybrid ParserThe hybrid parser is composed of a connectionist net-work trained using backward propagation (Werbos1974; Rumelhart et al 1986) from rule templates which,are derived from the deterministic grammar.
Rule tem-plates are intermediate between symbolic rules and thetraining patterns required by the network.
Each ruletemplate typically represents a large number of pat-terns.
They serve to relate situations that occur duringparsing with the action deemed appropriate for thatsituation.
Actions in the hybrid parser are performedsymbolically on traditional data structures which arealso maintained symbolically.As Figure 2 illustrates, the hybrid parser is organizedinto a symbolic component and a connectionist com-ponent.
The latter component is implemented as anumeric simulation of an adaptive neural network.
Thesymbolic and connectionist components cooperate in atightly coupled manner since there are proven advan-tages to this type of organization (Kitzmiller andKowalik, 1987).
For the hybrid parser, the advantagesare performance and robustness.The symbolic component manages the input sentenceand the flow of constituents into the lookahead buffer,coding them as required for the input level of the net-work in the connectionist component.
On the returnside, it evaluates the activations of the output units,decides which action to perform, and performs thataction, potentially modifying the stack and buffer in theprocess.
The responsibility of the connectionist com-ponent, therefore, is to examine the contents of thebuffer and stack and yield a preference for a specificaction.
These preferences are garnered from manyiterations of back-propagation learning with instancesof the rule templates.
Learning itself occurs off-lineand is a time-consuming process, but once learned theprocessing times for file system are excellent.
Compu-tations need only flow in one direction in the network.The feed-forward multiplication of weights and compu-tation of activation levels for individual units producethe pattern of activation on the output level.
Activationof output units is interpreted in a winner-take-allmanner, with the highest activated unit determining theaction to be taken.3 13CONNECTIONIST  SYMBOLICCoded Actions?Coded Stack & BufferI/- .
.
.
.
.
.
.BufferI Joh  Isho  lStack<2riiiiiiikhave scheduled the meeting.Np~~ VPMVB NPFigure 2: System OverviewIn tile set of experiments described here, the networkhas a three-layer architecture, as illustrated, with 66input units, 40 hidden units, and 40 output units.
Eachinput pattern consists of two feature vectors from thebuffer items and one vector from the stack.
The firstvector activates 26 input units and the second vectoractivates 12 input units in a pattern vector epresentinga word or constituent of the sentence.
The stack vectoractivates 28 units representing the current node on thestack and its attachments.
One hidden layer has provensufficient in all of these experiments.
The output layerpermits the choice of one out of 40 possible actions thatcan be performed on a single iteration of processing.During sentence processing, the network is presentedwith encodings of the buffer and the top of the stack.What the model actually sees as input is not the rawsentence but a coded representation f each word in thesentence in a form that could be produced by a simplelexicon, although such a lexicon is not part of themodel in its present form.
The network produces theaction to be taken which is then performed.
If theaction creates a vacancy in the buffer and if more of thesentence is left to be processed then the next sentencecomponent is moved into the buffer.
The process thenrepeats until a stop action is performed, usually whenthe buffer becomes empty.
Iteration over the inputstream is achieved in this fashion.Figure 2 illustrates the nature of the processing,although it shows a composite of the initial and finalstates of the parser.
When a sentence form like "Johnshould have scheduled the meeting" appears in theinput stream, the first two constituents fill the buffer.These contents along with the contents of the top of thestack and its attachments are encoded and presented tothe network.
Coding is based on a simple scheme inwhich those features of the buffer and stack that areactually tested by grammar ules are represented (seeFaisal, 1990).
The network, in turn, produces a singleaction.
Specification of the action by the network isdone by activating one of the output units more than theothers thus determining the winner (called "winner-take-all").
This action is then executed symbolically,yielding changes in the buffer and stack.
The processrepeats until a stop action is performed at which timethe resultant parse structure is left on top of the stack asshown.4.1 .
Learn ing  a GrammarTraining of the hybrid parser proceeds by presentingpatterns to the network and teaching it to respond withan appropriate action.
The input patterns representencodings of the buffer positions and the top of theslick from the deterministic parser.
The output of thenetwork contains a series of units representing actionsto be performed during processing and judged in awinner-take-all fashion.
Network convergence isobserved once the network can achieve a perfect scoreon the training patterns themselves and the error meas-ure has decreased to an acceptable level (set as aparameter).
Once the network is trained, the weightsare stored in a file so that sentences can be parsed.
Asentence is parsed by iteratively presenting the networkwith coded inputs and performing the action specifiedby the network.Our neural network simulator features a logistic func-tion that computes values in the range of -1 to +1.Each grammar ule is coded as a training templatewhich is a list of feature values.
In general, each con-stituent is represented by an ordered feature vector inwhich one or more values is ON(+1) for features of theform and all other values are either OFF(-1) or DONOT CARE (?).
A rule template is inslintiated by ran-domly changing ?
to +1 or -1.
Thus, each templatecan be instantiated to give many training patterns andeach training epoch is slightly different.
It is obviouslyimpossible to test the performance of all these cases, sofor the purpose of judging convergence, a zero is sub-stituted for each ?
in the rule template to provide test-ing patterns.
For more discussion of the training pro-cess, see Faisal and Kwasny (1990).14 4TABLE 1Examples of Grammatical SentencesSentence Form(1) Scheduled a meeting for Monday.
(2) John has scheduled the meeting for Monday.
(3) The meeting seems to have been scheduled for Monday.
(4) The jar seems broken.
(5) I persuaded John to do it.
(6) I saw him do it.
(7) Ma131 wants John to have a party.
(8) Mary wants to have a party.
(9) What will the man put in the comer?
(10) What will the man put the book in?
(11) Who (lid John see?
(12) Who broke the jar?
(13) Who is carrying the baby?
(14) What is the baby carrying?
(15) What did Bob give Mary?
(16) The man who wanted to meet Mary has disappeared.
(17) The: man who hit Mary with a book has disappeared.
(18) The man whom Mary hit with a book has disappeared.
(19) I told that boy that boys should do it.
(20) That mouse that the cat chased had squeaked.
(21) I told Sue you would schedule the meeting.
(22) I told the girl that you would schedule the meeting.
(23) John is scheduling the meeting for Monday.5.
PerformanceFor testing purposes, several sentences are ceded thatwould parse correctly by the rules of the deterministicparser.
Additionally, severed mildly ungrammatical andlexical ambiguous entences are coded to determine ifthe network would generalize in any useful way.
Mostof these examples were drawn from work cited earlierby Chamiak and Milne.
The objective is to discoverexactly how syntactic ontext can aid in resolving suchproblems.
In previous work, a simpler (23-rule) gram-mar was tested with similar results (Kwasny andFaisal,1989).5.1.
Parsing Grammatical SentencesExperimentation with grammatical sentences confirmsthat indeed the rules from the grammar have beenlearned sufficiently to parse sentences.
When trainingwith the rule templates, testing for convergence is pos-sible by changing each ?
to a zero value.
Here the per-formance of the hybrid parser is examined with actualsentences and the claim that the parser simulates bothPARSIFAL and LPARSIFAL is substantiated.Gramrnatical sentences, by our definition, are thosewhich parse correctly in the rule-based grammar fromwhich the training set is derived.
Table 1 shows severalexamples of grammatical sentences which are parsedsuccessfully.
Parse trees are developed which areidentical with ones produced by other deterministicparsing systems.5.2.
Parsing Ungrammatical SentencesCapabilities described above only duplicate what canbe done rather comfortably symbolically.
Of course,the feedforward network in the hybrid parser allowsvery fast decision-making due to the nature of themodel.
But what other features does the model pos-sess?
Importantly, how robust is the processing?
As aTABLE 2Ungrammatical vs. Grammatical SentencesSentence Form Strength(la) *John have should scheduled the meeting for Monday.
14.4(lb) John should have scheduled the meeting for Monday.
56.9(2a) *Ilas John schedule the meeting for Monday?
32.3(2b) Itas John scheduled the meeting for Monday?
36.8(3a) *John is schedule the meeting for Monday.
9.5(3b) John is scheduling the meeting for Monday.
54.7(4a) *John is is scheduling the meeting for Monday.
7.2(4b) John is scheduling the meeting for Monday.
54.7(5a) *The boy did hitting Jack.
14.8(5b) The boy did hit Jack.
137.7(6a) *'llae meeting is been scheduled for Monday.
559.6(6b) The meeting has been scheduled for Monday.
565.5symbolic model, PARAGRAM extends PARSIFAL tohandle ungrammatical sentences.
This is accomplishedby considering all rules in parallel and scoring each testperformed on the left-hand side of a rule according topredefined weights.
The rule with the best score fires.In this way, processing will always have some rule tofire.
Reported experimentation with PARAGRAMshows this to be an effective method of extending theinherent capabilities of the grammar.To demonstrate its generalization capabilities, thehybrid parser is tested with several exmnples ofungrammatical sentences.
Its performance is strictlydependent upon its training experiences since no relax-ation rules (Kwasny and Sondheimer, 1981), meta-rules(Weischedel and Sondheimer, 1983), or other specialmechanisms were added to the original grammar rulesto handle ungrammatical cases.
In Table 2, ungram-matical sentences used in testing are shown along withtheir strengths.
These strengths are computed as thereciprocal of the average rror per processing step foreach sentence and reflect he certainty with which indi-vidual actions for building structures are being selected.Although there is no real meaning in the values of thesenumbers, they are a useful means of comparison.These examples produce reasonable structures whenpresented to our system.
Note that overall averagestrength is lower for ungrammatical sentences whencompared to similar grammatical ones.In sentence (la), for example, the structure producedwas identical to that produced while parsing sentence(lb).
The only difference is that the two auxiliaryverbs, have and should, were reversed in the parse tree.Sentence (2a) contains a disagreement between theauxiliary has and the main verb schedule and yet thecomparable grammatical sentence (2b) parsed identi-cally.
Sentences (3a) and (4a) parse comparable to sen-tence (3b).
Sentence (5a) is processed as if it were pro-gressive tense ('The boy is hitting Jack').
InPARAGRAM, a nonsensical parse structure is pro-duced for this sentence, as reported by Charniak (p.137).
It can be compared with sentence (5b), but thereis not one clear choice for how the sentence shouldappear if grammatical.
The problems with using asyntax-based approach to handling ungrammatical sen-tences are well-known (see, for example, Kwasny,1980).5 15TABLE 3Lexically Ambiguous vs.
Unambiguous SentencesSentence Form Strength(la) <Will> John schedule the meeting for Monday?
5.0(lb) Will(aux) John schedule the meeting for Monday?
57.46(2a) Tom <will> hit Mary.
29.8(2b) Tom will(aux) hit Mary.
125.8(3a) Tom <hit> Mary.
13.6(3b) Tom hit(v) Mary.
29.5(4a) The <will> gave the money to Mary.
16.6(4b) The will(noun) gave the money to Mary.
61.9(5a) They <can> fish(np).
20.6(5b) They can(v) fish(np).
30.0(6a) They can(aux) <fish>.
2.9(6b) They can(aux) fish(v).
6.35.3.
Lexical AmbiguityAs a further test of the generalization properties of thehybrid parser, sentences containing lexically ambigu-ous words are tested.
Some of these sentences areshown in Table 3.
Of course, ROBIE takes a symbolicapproach in extending PARSIFAL to address theseissues by requiring additional rules and lexical features.Note that in the deterministic approach, it is essentialfor lexical items to be properly disambiguated or back-tracking will be required.In testing the hybrid parser, normal sentences arepresented, except hat selected words are coded ambi-guously (here indicated by angle brackets < > aroundthe word).
Sentences containing words followed byparentheses are presented to the hybrid parser unambi-guously, even though these words have ambiguoususes.
The lexical choices are shown in parentheses.
Inthe cases shown, the lexically ambiguous words werecorrex:tly interpreted and reasonable structures resulted,although lower strengths were observed.
The hybridparser utilizes syntactic ontext o resolve these ambi-guities and automatically works to relate novel situa-tions to training cases through the generalization capa-bility of the network.
As before, no additional rules ormechanisms are required to provide this capability.Sentence (la) contains the word will coded ambigu-ously as an NP and an auxiliary, modal verb.
In thecontext of the sentence, it is clearly being used as amodal auxiliary and the parser treats it that way as (lb).A similar result was obtained for sentence (2a) whichparses as (2b).
In sentence (3a), hit is coded to beambiguous between an NP (as in a baseball hit) and averb.
The network correctly identifies it as the mainverb of the sentence as in sentence (3b).
Sentence (4a)is constructed as for sentence (4b).
Sentence (5a)presents can ambiguously as an auxiliary, modal, andmain verb, while fish is presented uniquely as an NP.Can is processed as the main verb of the sentence andresults in the same structure as sentence (5b).
Like-wise, sentence (6a), which contains fish coded ambigu-ously as a verb/NP and can coded uniquely as an auxi-liary verb, produces the same structure as sentence(6b).
In the cases shown, the lexically ambiguouswords were disambiguated and reasonable structuresresulted.
Note that the overall average strengths werelower than comparable grammatical sentences dis-cussed, as expected.6.
SummaryOur hybrid deterministic parser is based on a deter-ministic grammar modified slightly from that found intraditional systems.
Our grammar is derived from oneused by Marcus, but with much inspiration from thework of Milne, Berwick, and Chamiak.
The rules ofthe grammar are utilized in training a connectionistcomponent.
The result is a hybrid system which exhi-bits characteristics from several well-known extensionsof the basic deterministic parser.
In particular, someungrammatical nd lexically ambiguous inputs can besuccessfully processed although no special provisionsare made for them.
These extended properties comeessentially for free due to the coupling of a symboliccomponent with connectionism.ReferencesBerwick, R.C.
1985.
The Acquisition of SyntacticKnowledge.
MIT Press, Cambridge, MA.Charniak, E. 1983.
"A Parser with Something for Every-one."
In Parsing Natural Language, M. King, ed.
AcademicPress, New York, NY, 117-150.Faisal, K.A.
1990.
Cormectionist Deterministic Parsing.D.Sc.
Dissertation, Department of Computer Science, Wash-ington University, St. Louis, Missouri.Faisal, K.A.
and S.C. Kwasny.
1990.
Deductive and Induc-tive Learning in a Connectionist Deterministic Parser.
Proclntl Joint Conf Neural Networks, 1~471-474.Kitzmiller, C.T., and J.S.
Kowalik.
1987.
Coupling Symbolicand Numeric Computing in Knowledge-Based Systems.
AIMagazine 8, no.
2, 85-90.Kwasny, S.C. and K.A.
Faisal.
1989.
Competition andLearning in a Connectionist Deterministic Parser.
Proc llthConf Cog Sci Society, 690-697.Kwasny, S.C. and N.K.
Sondheimer.
1981.
Relaxation Tech-niques for Parsing Ill-Formed Input.
Am J Comp Ling 7, no.2, 99-108.Kwasny, S.C. 1980.
"Treatment of Ungrammatical andExtra-Grammatical Phenomena in Natural Language Under-standing Systems."
Indiana University Linguistics Club,Bloomington, Indiana.Marcus, M. P. 1980.
A Theory of Syntactic Recognition forNatural Language.
MIT Press, Cambridge, MA.McClelland, J. L., & A. H. Kawamoto.
1986.
"Mechanismsof Sentence Processing: Assigning Roles to Constituents ofSentences."
In Parallel Distributed Processing, D.E.Rumelhart and J.L.
McClelland, MIT Press, Cambridge, MA,272-325.Milne, R. 1986.
Resolving Lexical Ambiguity in a Deter-ministic Parser.
Comp Ling 12, No.
1, 1-12.Rumelhart, D. E., G. Hintoh, and R.J. Williams.
1986.
"Learning Internal Representations by Error Propagation.
"In Parallel Distributed Processing, D.E.
Rumelhart and J.L.McCMland, MIT Press, Cambridge, MA, 318-364.Weischedel, R.M.
and N.K.
Sondheimer.
1983.
Meta-Rulesas a Basis for Processing Ill-Formed Input.
Am J Comp Ling9, No.
3-4, 161-177.Werbos, P. 1974.
"Beyond Regression: New Tools for Pred-iction and Analysis in Behavioral Science."
Ph.D. Thesis.Harvard University, Cambridge, Ma.16 6
