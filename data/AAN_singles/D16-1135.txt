Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1275?1284,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsImproving Multilingual Named Entity Recognitionwith Wikipedia Entity Type MappingJian Ni and Radu FlorianIBM T. J. Watson Research Center1101 Kitchawan Road, Yorktown Heights, NY 10598, USA{nij, raduf}@us.ibm.comAbstractThe state-of-the-art named entity recognition(NER) systems are statistical machine learn-ing models that have strong generalization ca-pability (i.e., can recognize unseen entitiesthat do not appear in training data) basedon lexical and contextual information.
How-ever, such a model could still make mis-takes if its features favor a wrong entity type.In this paper, we utilize Wikipedia as anopen knowledge base to improve multilin-gual NER systems.
Central to our approachis the construction of high-accuracy, high-coverage multilingual Wikipedia entity typemappings.
These mappings are built fromweakly annotated data and can be extendedto new languages with no human annotationor language-dependent knowledge involved.Based on these mappings, we develop severalapproaches to improve an NER system.
Weevaluate the performance of the approachesvia experiments on NER systems trained for 6languages.
Experimental results show that theproposed approaches are effective in improv-ing the accuracy of such systems on unseenentities, especially when a system is applied toa new domain or it is trained with little train-ing data (up to 18.3 F1 score improvement).1 IntroductionNamed entity recognition (NER) is an importantNLP task that automatically detects entities in textand classifies them into pre-defined entity types suchas persons, organizations, geopolitical entities, lo-cations, events, etc.
NER is a fundamental compo-nent of many information extraction and knowledgediscovery applications, including relation extraction,entity linking, question answering and data mining.The state-of-the-art NER systems are usually sta-tistical machine learning models that are trainedwith human-annotated data.
Popular models in-clude maximum entropy Markov models (MEMM)(McCallum et al, 2000), conditional random fields(CRF) (Lafferty et al, 2001) and neural networks(Collobert et al, 2011; Lample et al, 2016).
Suchmodels have strong generalization capability to rec-ognize unseen entities1 based on lexical and contex-tual information (features).
However, a model couldstill make mistakes if its features favor a wrong en-tity type, which happens more frequently for unseenentities as we have observed in our experiments.Wikipedia is an open-access, free-content Inter-net encyclopedia, which has become the de factoon-line source for general reference.
A Wikipediapage about an entity normally includes both struc-tured information and unstructured text information,and such information can be used to help determinethe entity type of the referred entity.So far there are two classes of approaches thatexploit Wikipedia to improve NER.
The first classof approaches use Wikipedia to generate featuresfor NER systems, e.g., (Kazama and Torisawa,2007; Ratinov and Roth, 2009; Radford et al,2015).
Kazama and Torisawa (2007) try to find theWikipedia entity for each candidate word sequenceand then extract a category label from the first sen-tence of the Wikipedia entity page.
A part-of-speech(POS) tagger is used to extract the category label1An entity is an unseen entity if it does not appear in thetraining data used to train the NER model.1275features in the training and decoding phase.
Ratinovand Roth (2009) aggregate several Wikipedia cate-gories into higher-level concept and build a gazetteeron top of it.
The two approaches were shown tobe able to improve an English NER system.
Bothapproaches, however, are language-dependent be-cause (Kazama and Torisawa, 2007) requires a POStagger and (Ratinov and Roth, 2009) requires man-ual category aggregation by inspection of the anno-tation guidelines and the training set.
Radford etal.
(2015) assume that document-specific knowledgebase (e.g., Wikipedia) tags for each document areprovided, and they use those tags to build gazetteertype features for improving an English NER system.The second class of approaches use Wikipedia togenerate weakly annotated data for training multi-lingual NER systems, e.g., (Richman and Schone,2008; Nothman et al, 2013).
The motivation isthat annotating multilingual NER data by humanis both expensive and time-consuming.
Richmanand Schone (2008) utilize the category informa-tion of Wikipedia to determine the entity type ofan entity based on manually constructed rules (e.g.,category phrase ?Living People?
is mapped to en-tity type PERSON).
Such a rule-based entity typemapping is limited both in accuracy and cover-age, e.g., (Toral and Muoz, 2006).
Nothman etal.
(2013) train a Wikipedia entity type classifierusing human-annotated Wikipedia pages.
Such asupervised-learning based approach has better ac-curacy and coverage, e.g., (Dakka and Cucerzan,2008).
A number of heuristic rules are developedin both works to label the Wikipedia text to createweakly annotated NER training data.
The NER sys-tems trained with the weakly annotated data mayachieve similar accuracy compared with systemstrained with little human-annotated data (e.g., up to40K tokens as in (Richman and Schone, 2008)), butthey are still significantly worse than well-trainedsystems (e.g., a drop of 23.9 F1 score on the CoNLLdata and a drop of 19.6 F1 score on the BBN data asin (Nothman et al, 2013)).In this paper, we propose a new class of ap-proaches that utilize Wikipedia to improve multilin-gual NER systems.
Central to our approaches is theconstruction of high-accuracy, high-coverage mul-tilingual Wikipedia entity type mappings.
We useweakly annotated data to train an English Wikipediaentity type classifier, as opposed to using human-annotated data as in (Dakka and Cucerzan, 2008;Nothman et al, 2013).
The accuracy of the classi-fier is further improved via self-training.
We applythe classifier on all the English Wikipedia pages andconstruct an English Wikipedia entity type mappingthat includes entities with high classification confi-dence scores.
To build multilingual Wikipedia en-tity type mappings, we generate weakly annotatedclassifier training data for another language via pro-jection using the inter-language links of Wikipedia.This approach requires no human annotation orlanguage-dependent knowledge, and thus can beeasily applied to new languages.Our goal is to utilize the Wikipedia entity typemappings to improve NER systems.
A natural ap-proach is to use a mapping to create dictionary typefeatures for training an NER system.
In addition,we develop several other approaches.
The first ap-proach applies an entity type mapping as a decod-ing constraint for an NER system.
The second ap-proach uses a mapping to post-process the outputof an NER system.
We also design a robust jointapproach that combines the decoding constraint ap-proach and the post-processing approach in a smartway.
We evaluate the performance of the Wikipedia-based approaches on NER systems trained for 6 lan-guages.
We find that when a system is well trained(e.g., with 200K to 300K tokens of human-annotateddata), the dictionary feature approach achieves thebest improvement over the baseline system; whilewhen a system is trained with little human-annotatedtraining data (e.g., 20K to 30K tokens), a more ag-gressive decoding constraint approach achieves thebest improvement.
In both scenarios, the Wikipedia-based approaches are effective in improving the ac-curacy on unseen entities, especially when a systemis applied to a new domain (3.6 F1 score improve-ment on political party articles/English NER) or itis trained with little training data (18.3 F1 score im-provement on Japanese NER).We organize the paper as follows.
We describehow to build English Wikipedia entity type mappingin Section 2 and extend it to multilingual mappingsin Section 3.
We present several Wikipedia-basedapproaches for improving NER systems in Section4 and evaluate their performance in Section 5.
Weconclude the paper in Section 6.12762 English Wikipedia Entity Type MappingIn this section, we focus on English Wikipedia.
Wedivide Wikipedia pages into two types:?
Entity pages that describe an entity or object,either a named entity such as ?Michael Jordan?or a common entity such as ?Basketball.??
Non-entity pages that do not describe a certainentity, including disambiguation pages, redi-rection pages, list pages, etc.We have developed an in-house English NER sys-tem (Florian et al, 2004).
The system has 51 en-tity types, and the main motivation of deployingsuch a fine-grained entity type set is to build cog-nitive question answering applications on top of theNER system.
An important check for a question an-swering system is the capability to detect whethera particular answer matches the expected type de-rived from the question.
The entity type system usedin this paper has been engineered to cover manyof the frequent types that are targeted by naturally-phrased questions (such as PERSON, ORGANIZA-TION, GPE, TITLEWORK, FACILITY, EVENT,DATE, TIME, LOCATION, etc), and it was createdover a long period of time, being updated as moretypes were found to be useful for question answer-ing, and to improve inter-annotator consistency.We want to classify Wikipedia pages into one ofthe entity types used in the NER system.
For non-entity pages and entity pages describing commonentities, we assign them with a new type OTHER.2.1 Wikipedia Entity Type Classification2.1.1 FeaturesWe build maximum entropy classifiers (Nigam etal., 1999) for Wikipedia entity type classification.We use both structured information and unstructuredinformation of a Wikipedia page as features.Each Wikipedia page has a unique title.
The titleof an entity page is usually the name of the entity,and may include auxiliary information in a bracketto distinguish entities with the same name.
We useboth the entity name and auxiliary information in abracket (if any) of a Wikipedia title as features be-cause each could provide useful information for en-tity type classification.
For example, based on theword ?Prize?
in the title ?Nobel Prize?
or the word?Awards?
in the title ?Academy Awards?, one caninfer that the entity type is AWARD.
Likewise, theauxiliary information ?company?
in the title ?Jordan(company)?
indicates that the entity is an ORGA-NIZATION, and the auxiliary information ?film?
inthe title ?Alien (film)?
indicates that the entity is aTITLEWORK.The text in a Wikipedia page of an entity pro-vides rich information about the entity.
A personcan usually correctly infer the entity type by read-ing the first few sentences of the text in a Wikipediapage.
Using more sentences provides additional in-formation about the entity which might be helpful,but it is also more likely to introduce noisy informa-tion which could affect the classification accuracyadversely.
Therefore, we use the first 200 tokens ofthe text in a Wikipedia page and create n-gram wordfeatures out of them.
We have also found that in-cluding additional n-gram word features of the firstsentence in a Wikipedia page results in a better clas-sification accuracy.Most Wikipedia pages also have a structured tablecalled infobox, which is placed on the right top of apage.
An infobox contains attribute-value pairs, of-ten providing summary information about an entity.The attributes in an infobox could be particularlyuseful for entity type classification.
For example, theattribute ?Born?
in an infobox provides strong ev-idence that the corresponding entity is a PERSON;and the attribute ?Headquarters?
implies that the cor-responding entity is an ORGANIZATION.
We in-clude the infobox attributes as classifier features.2.1.2 Training and Test DataEntity linking (EL) or entity disambiguation is thetask of determining the identities of entities men-tioned in text, by linking each entity to an entry (ifexists) in an open knowledge base such as Wikipedia(Han et al, 2011; Hoffart et al, 2011).
We apply anEL system (Sil and Florian, 2014) to generate train-ing data for Wikipedia entity type classification asfollows: if a named entity in our NER training datawith entity type T is linked to a Wikipedia page, thatpage will be labeled with entity type T .
Similarly,we apply the EL system to generate a set of test databy linking named entities in our NER test data toWikipedia pages.
The English Wikipedia snapshot1277Features ALL PER ORG GPE TITL FACTitle 62.4 73.4 67.2 59.0 57.1 47.1Infobox 77.3 92.6 87.8 92.0 95.4 50.0Text 87.2 97.5 87.3 95.1 88.5 40.0All 90.1 96.1 92.5 95.1 96.9 75.0Table 1: F1 score of English Wikipedia entity type classifiers.was dumped in April 2014 which contains around4.6M pages.
Using this method we generate a train-ing data set with 4,699 English Wikipedia pages anda test set of 415 English Wikipedia pages.Notice that the automatically generated classifiertraining and test data are weakly labeled since theEL system may link an entity to a wrong Wikipediapage and thus the entity type assigned to that pagecould be wrong.
Since the test data is crucial forevaluating the classification accuracy, we manuallycorrected the output.2.1.3 Classifier PerformanceTo evaluate the prediction power of different typesof features, we train a number of classifiers usingonly title features, only infobox features, only textfeatures, and all features respectively.
We show theF1 score of the classifiers on different entity types inTable 1.
ALL is the overall performance, and PER(PERSON), ORG (ORGANIZATION), GPE, TITL(TITLEWORK), FAC (FACILITY) are the top fivemost frequently entity types in the test data.From Table 1, we can see that text features are themost important features for classifying Wikipediapages, since the classifier trained with only text fea-tures achieves an overall F1 score of 87.2, which isbetter than the classifier trained with either title orinfobox features alone.
Nevertheless, both infoboxand title features provide additional useful informa-tion for entity type classification, and the classifiertrained with all the features achieves an overall F1score of 90.1.2.1.4 Improvement via Self-TrainingSelf-training is a semi-supervised learning tech-nique that can be used in applications where thereis only a small number of labeled training examplesbut a large number of unlabeled examples.
Since ourweakly annotated classifier training data only cov-ers around 1% of all the Wikipedia pages, we aremotivated to use self-training to further improve theClassifier Train Size F1Original Classifier 4,699 90.1Self-Training (Standard) +2,352,836 91.1Self-Training (Sampling) +26,518 91.8Table 2: Improving classifier accuracy via self-training.classification accuracy.We first apply a standard self-training approach.The classifier trained with the initial training datais used to decode (i.e., classify) all the unla-beled Wikipedia pages to predict their entity typeswith confidence scores.
We add the self-decodedWikipedia pages with high confidence scores to thetraining data and train a new classifier.
Via exper-iments a threshold of 0.9 is used to sort out high-confident self-decoded examples.
The F1 score ofthe new classifier is improved to 91.1, as shown inTable 2.Under the standard approach, about 2.3M self-decoded examples are added, the size of which isabout 500 times of the size of the original trainingdata.
The errors of the original classifier could beamplified with such a big increase of the trainingsize with so many self-decoded examples.To address this issue, we have developed asampling-based self-training approach.
Instead ofadding all the self-decoded examples with confi-dence scores greater than or equal to 0.9, we do arandom sampling of those high-confident examples.We use a sampling probability p(e) = q ?c(e), whereq is a sampling ratio parameter and c(e) is the con-fidence score of example e. Under this approach,examples with higher confidence scores are morelikely to be selected, while the total number of se-lected examples is controlled by the sampling ratioq.
Via experiments we found that a small samplingratio like q = 0.01 yields good improvement (al-though the improvement is not sensitive to q).
Asshown in Table 2, the classification accuracy underthe sampling-based approach is further improved to91.8 F1 score (the improvement is calculated by av-eraging over 5 random samples with q = 0.01).2.2 Wikipedia Entity Type MappingWe construct an English Wikipedia entity typemapping by applying the English Wikipedia entitytype classifier on all the English Wikipedia pages1278(?4.6M).
Each entry of the mapping includes anentity name (which is extracted from the title of aWikipedia page) and the associated entity type withconfidence score (which is determined by the clas-sifier).
We denote the English Wikipedia entity typemapping that includes all the pages by English-Wiki-Mapping.To build a high-accuracy mapping, one maywant to include only entities with confidence scoresgreater than or equal to a threshold t in the mapping,and we denote such a mapping by English-Wiki-Mapping(t).
Notice that a mapping with a highert will have more accurate entity types for its enti-ties, but it will include fewer entities.
Therefore,there is a trade-off between accuracy and coverageof the mapping, which can be tuned by the confi-dence threshold t. There are about 2.9M entitiesin English-Wiki-Mapping(0.9), which covers about63% of all the English Wikipedia pages.We have also found that the length of an entityname (i.e., number of words in an entity name) alsoplays an important role for determining which enti-ties should be included in the mapping for improv-ing an NER system.
Therefore, we use English-Wiki-Mapping(t, i) to denote the English Wikipediaentity type mapping that includes all the entitieswith confidence scores greater than or equal to tand at least i words in their names.
English-Wiki-Mapping(0.9,2) covers about 55% of all the EnglishWikipedia pages, and English-Wiki-Mapping(0.9,3)covers about 25% of all the English Wikipediapages.3 Multilingual Wikipedia Entity TypeMappingBased on the English Wikipedia entity type map-ping, we want to build high-accuracy, high-coverageWikipedia entity type mappings for other languageswith minimum human annotation and language-dependent knowledge involved.
We utilize the inter-language links of Wikipedia, which are the linksbetween one entity?s pages in different languages.The inter-language links between English Wikipediapages and Wikipedia pages of another language pro-vide useful information for this task.Suppose we want to build a Wikipedia entity typemapping for a new language, and we use Portugueseas an example.
A direct approach is projection us-ing the inter-language links between English andPortuguese Wikipedia pages: for each PortugueseWikipedia page that has an inter-language link toan English Wikipedia page, we project the entitytype of the English Wikipedia page (determined bythe English entity type mapping) to the PortugueseWikipedia page.
The rationale is that both the En-glish and Portuguese pages are describing the sameentity, even probably with different spelling (e.g.,United States in English vs. Estados Unidos inPortuguese), the entity type of that entity does notchange from one language to another.However, the main limitation of the direct pro-jection approach is coverage.
Only a fraction of allthe Portuguese Wikipedia pages have inter-languagelinks to English Wikipedia pages, and among thosepages only a subset of them have classified en-tity types with confidence scores high enough (e.g.,at least 0.9).
For example, projecting English-Wiki-Mapping(0.9) to Portuguese Wikipedia returns143K pages, which covers only 15% of all the Por-tuguese Wikipedia pages (around 920K in total).We apply an alternative approach, which usesthe 143K Portuguese Wikipedia pages (acquiredby projection from English-Wiki-Mapping(0.9)) asweakly annotated training data to train a PortugueseWikipedia entity type classifier.
For feature en-gineering purpose, we also project the EnglishWikipedia entity type classifier training and testdata (as described in Section 2.1.2) to PortugueseWikipedia pages via inter-language links, and thisproduces 1,190 Portuguese Wikipedia pages whichare used as the test data.
Pages in the test data setare excluded from the 143K training data set.We use similar features (title, infobox and text)as for the English classifiers to train the Portugueseclassifiers.
Again we find that the classifier trainedwith all the features achieves the best accuracy of86.3 F1 score.
Notice that this is an approximatedevaluation because the pages in the test data set arelabeled via projection and not by human.We build Portuguese Wikipedia entity type map-pings by applying the Portuguese Wikipedia en-tity type classifier on all the Portuguese Wikipediapages.
We use Portuguese-Wiki-Mapping(t) to de-note the mapping that includes entities with con-fidence scores greater than or equal to a thresh-1279old t. There are 525K entities in Portuguese-Wiki-Mapping(0.9), which covers about 57% of all thePortuguese Wikipedia pages, a significant improve-ment of coverage compared to the direct projectionapproach (15%).The main advantage of our approach is that no hu-man annotation or language-dependent knowledgeis required, so it can be easily applied to a newlanguage.
We have applied this approach to buildhigh-accuracy, high-coverage Wikipedia entity typemappings for several new languages including Por-tuguese, Japanese, Spanish, Dutch and German.4 Improving NER SystemsWe have developed several approaches that utilizethe Wikipedia entity type mappings to improve NERsystems.
Let M be a Wikipedia entity type map-ping.
For an entity name x, let M(x) denote theset of possible entity types for x determined by themapping.
If an entity name x is in the mapping,then M(x) includes at least one entity type, i.e.,|M(x)| ?
1, where |M(x)| is the cardinality ofM(x).
Otherwise if an entity name x is not inthe mapping, then M(x) = ?
is the empty set and|M(x)| = 0.The first approach is to use a Wikipedia entitytype mapping M as a decoding constraint for anNER system.
Under this approach, the mapping isapplied as a constraint during the decoding proce-dure: if a sequence of words in the text form anentity name x that is included in the mapping, i.e.,|M(x)| ?
1, then the sequence of words will beidentified as an entity, and its entity type is deter-mined by the decoding algorithm while being con-strained to one of the entity types inM(x).The second approach is to use a Wikipedia entitytype mapping M to post-process the output of anNER system.
Under this approach, the mapping isapplied after the decoding procedure: if the nameof a system entity x is in the mapping and the en-tity type for that entity name is unique based on themapping, i.e., |M(x)| = 1, then its entity type willbe determined by the unique entity type inM(x).The decoding constraint approach is more aggres-sive than the post-processing approach, because itmay create new entities and change entity bound-aries.
This approach is more reliable for entitieswith longer names.
Via experiments we find thatusing Wiki-Mapping(0.9,2) or Wiki-Mapping(0.9,3)achieves the best improvement under the decodingconstraint approach.
Remember Wiki-Mapping(t, i)includes all the entities with confidence scores atleast t and at least i words in their names.In contrast, the post-processing approach is amore conservative approach since it relies on thesystem entity boundaries and only changes their en-tity types if determined by the mapping, so it will notcreate new entities.
Via experiments we find that us-ing Wiki-Mapping(0.9,2) achieves the best improve-ment under the post-processing approach.Based on the observation that the decoding con-straint approach is more reliable for longer enti-ties while the post-processing approach can betterhandle short entities, we have designed a joint ap-proach that combines the two approaches as fol-lows: it first applies Wiki-Mapping(0.9,3) as a de-coding constraint for an NER system to produce sys-tem entities, and then applies Wiki-Mapping(0.9,2)to post-process the system output.
The joint ap-proach combines the advantages of both approachesand achieves robust performance in our experiments.Finally, we can use a Wikipedia entity type map-ping to create dictionary features for training anNER system.
The idea of using Wikipedia to createtraining features was explored before, e.g., (Kazamaand Torisawa, 2007; Ratinov and Roth, 2009; Rad-ford et al, 2015).
The difference between ourapproach and the previous approaches is how thefeatures are created: we first build high-accuracy,high-coverage multilingual Wikipedia entity typemappings and then use the mappings to generatedictionary features.
Via experiments we find thatusing Wiki-Mapping(0.9,1) or Wiki-Mapping(0.9,2)achieves the best improvement under the dictionaryfeature approach.5 ExperimentsIn this section, we evaluate the effectiveness of theproposed Wikipedia-based approaches via experi-ments on NER systems trained for 6 languages:English, Portuguese, Japanese, Spanish, Dutch andGerman.
For each language, we compare the base-line NER system with the following approaches:?
DC(i): the decoding constraint approach with1280mapping Language-Wiki-Mapping(0.9,i).?
PP(i): the post-processing approach with map-ping Language-Wiki-Mapping(0.9,i).?
Joint: the joint approach that combines DC(3)and PP(2).?
DF(i): the dictionary feature approach withmapping Language-Wiki-Mapping(0.9,i).To evaluate the generalization capability of anNER system, we compute the F1 score on the un-seen entities (Unseen) as well as on all the entities(All) in a test data set.5.1 EnglishThe baseline English NER system is a CRF modeltrained with 328K tokens of human-annotated newsarticles.
It uses standard NER features in the litera-ture including n-gram word features, word type fea-tures, prefix and suffix features, Brown cluster typefeatures, gazetteer features, document-level cachefeatures, etc.We have two human-annotated test data sets: thefirst set, Test (News), consists of 40K tokens ofhuman-annotated news articles; and the second set,Test (Political), consists of 77K tokens of human-annotated political party articles from Wikipedia.The results are shown in Table 3.For Test (News) which is in the same domain asthe training data, the baseline system achieves 88.2F1 score on all the entities, and a relatively lowF1 score of 78.7 on the unseen entities (38% of allthe entities are unseen entities).
The dictionary fea-ture approach DF(2) achieves the highest F1 scoresamong the Wikipedia-based approaches.
It improvesthe baseline system by 1.2 F1 score on all the entitiesand by 3.1 F1 score on the unseen entities.
The jointapproach achieves the second highest F1 scores.
Itimproves the baseline by 0.7 F1 score on all the en-tities and by 2.0 F1 score on the unseen entities.For Test (Political) which is in a different domainfrom the training data, the fraction of unseen entitiesincreases to 84%.
In this case, the F1 score of thebaseline system drops to 64.1, and the Wikipedia-based approaches demonstrate larger improvements.For example, DF(2) improves the baseline system by2.7 F1 score on all the entities and by 3.6 F1 scoreon the unseen entities.NER Test (News) Test (Political)System All Unseen All Unseen100% 38% 100% 84%Baseline 88.2 78.7 64.1 60.9DC(2) 88.1 79.4 66.3 63.5DC(3) 88.7 80.2 65.8 62.9PP(2) 88.6 79.8 64.7 61.7Joint 88.9 80.7 66.3 63.6DF(1) 88.5 80.0 66.3 64.2DF(2) 89.4 81.8 66.8 64.5Table 3: Experimental results for English NER (the highest F1score among all approaches in a column is shown in bold).5.2 PortugueseFor Portuguese, we have applied a semi-supervisedlearning approach to build the baseline NER system.The training data set includes 31K tokens of human-annotated news articles, and 2M tokens of weaklyannotated data.
The weakly annotated data is gen-erated as follows.
We have a large number of paral-lel sentences between English and Portuguese newsarticles.
We apply the English NER system on theEnglish sentences and project the entity type tagsto the Portuguese sentences via alignments betweenthe English and Portuguese sentences.The baseline NER system is an MEMM model(CRF cannot handle such a big size of training data,since our NER system has 51 entity types, and thenumber of features and training time of CRF growat least quadratically in the number of entity types).The test data set consists of 34K tokens of human-annotated Portuguese news articles.The results are shown in Table 4.
Because thesystem is trained with little human-annotated train-ing data, the performance of the baseline systemachieves only 60.1 F1 score on all the entities and50.2 F1 score on the unseen entities (80% of all theentities).
In this case, the more aggressive decod-ing constraint approach DC(2) achieves the best im-provement among the Wikipedia-based approaches,which improves the baseline by 5.9 F1 score on allthe entities and by 8.6 F1 score on the unseen en-tities.
The joint approach improves the baseline by3.0 F1 score on all the entities and by 4.3 F1 scoreon the unseen entities.1281NER Test (News)System All Unseen100% 80%Baseline 60.1 50.2DC(2) 66.0 58.8DC(3) 62.2 53.4PP(2) 60.9 51.4Joint 63.1 54.5DF(1) 62.4 52.7DF(2) 61.3 51.9Table 4: Experimental results for Portuguese NER.NER Test (News)System All Unseen100% 59%Baseline 50.8 27.3DC(2) 59.8 45.6DC(3) 55.6 36.9PP(2) 50.8 27.3Joint 55.6 36.9DF(1) 52.9 29.0DF(2) 51.8 28.0Table 5: Experimental results for Japanese NER.5.3 JapaneseFor Japanese, the baseline NER system is anMEMM model trained with 20K tokens of human-annotated news articles and 2.1M tokens of weaklyannotated data.
The weakly annotated data was gen-erated using similar steps as for the Portuguese NERsystem.
The test data set consists of 22K tokens ofhuman-annotated Japanese news articles.The results are shown in Table 5.
Again, in thislow-resource case, DC(2) achieves the best improve-ment among the Wikipedia-based approaches.
It im-proves the baseline by 9.0 F1 score on all the entitiesand by 18.3 F1 score on the unseen entities (59% ofall the entities).
The joint approach improves thebaseline by 4.8 F1 score on all the entities and by9.6 F1 score on the unseen entities.5.4 Spanish, Dutch and GermanWe also evaluate the Wikipedia-based approacheson Spanish, Dutch and German NER systemstrained with the CoNLL data sets (Tjong Kim Sang,2002; Tjong Kim Sang and De Meulder, 2003).There are only 4 entity types in the CoNLL data:PER (person), ORG (organization), LOC (location),MISC (miscellaneous names).
Accordingly, wehave trained a CoNLL-style Wikipedia entity typeclassifier that produces the CoNLL entity types.
Thetraining data for the classifier is generated by usingthe CoNLL English training data set and the AIDA-YAGO2 data set that provides the Wikipedia titlesfor the named entities in the CoNLL English dataset (Hoffart et al, 2011).
Applying the classifieron all the English Wikipedia pages, we constructa CoNLL-style English Wikipedia entity type map-ping.
We then build CoNLL-style Wikipedia entitytype mappings for Spanish, Dutch and German us-ing steps as described in Section 3.For each of the three languages, the baselineNER system is a CRF model trained with human-annotated news data (?200K tokens), and there aretwo test data sets, TestA and TestB, that are alsohuman-annotated news data (ranging from 40K to70K tokens).
The results are shown in Table 6.For Dutch and German, DF(1) achieves the best im-provement among the Wikipedia-based approaches.For Spanish, the joint approach achieves the best im-provement among the Wikipedia-based approaches.Again, in all cases, the Wikipedia-based approachesdemonstrate larger improvements (ranging from 1.0to 3.4 F1 score) on the unseen entities.5.5 DiscussionFrom the experimental results, we have the follow-ing observations:?
NER systems are more likely to make mistakeson unseen entities.
In all cases, the F1 scoreof an NER system on all the entities is alwayshigher than the F1 score on the unseen entities.?
The Wikipedia-based approaches are effectivein improving the generalization capability ofNER systems (i.e., improving the accuracy onunseen entities), especially when a system isapplied to a new domain (3.6 F1 score improve-ment on political party articles/English NER)or it is trained with little human-annotatedtraining data (18.3 F1 score improvement onJapanese NER).?
In the low-resource scenario where an NER1282NER TestA TestBSystem All Unseen All UnseenSpanish 100% 47% 100% 38%Baseline 77.9 69.4 81.5 71.0DC(2) 77.9 69.7 81.4 71.0DC(3) 78.4 70.1 81.6 71.2PP(2) 78.2 70.1 82.0 72.1Joint 78.5 70.4 82.0 72.1DF(1) 77.7 69.6 82.0 71.6DF(2) 78.5 70.4 81.4 70.9Dutch 100% 60% 100% 54%Baseline 80.7 70.8 82.3 70.9DC(2) 80.8 71.3 82.8 71.9DC(3) 80.8 71.2 82.4 71.1PP(2) 81.2 71.6 83.2 72.5Joint 81.3 71.9 83.1 72.3DF(1) 82.3 73.2 84.5 74.3DF(2) 81.1 71.1 83.3 72.5German 100% 72% 100% 70%Baseline 69.6 63.0 70.3 63.0DC(2) 70.1 63.8 70.1 62.8DC(3) 69.9 63.5 70.4 63.1PP(2) 70.5 64.4 70.6 63.4Joint 70.8 64.8 70.6 63.4DF(1) 71.8 65.8 71.8 65.3DF(2) 71.2 65.4 70.5 63.6Table 6: Experimental results for Spanish, Dutch, and GermanNER.system is trained with little human-annotateddata (e.g., 20K-30K tokens of training data forthe Portuguese and Japanese systems), the de-coding constraint approach, which uses a high-accuracy, high-coverage Wikipedia entity typemapping to create constraints during the decod-ing phase, achieves the best improvement.?
In the rich-resource scenario where an NERsystem is well trained (e.g., 200K-300K tokensof training data for the English, Dutch and Ger-man systems), the dictionary feature approach,which uses a Wikipedia entity type mappingto create dictionary type features, achieves thebest improvement.?
In both scenarios, the joint approach, whichcombines the decoding constraint approach andthe post-processing approach in a smart way,achieves relatively robust performance amongthe Wikipedia-based approaches.6 ConclusionIn this paper, we proposed and evaluated several ap-proaches that utilize high-accuracy, high-coverageWikipedia entity type mappings to improve multi-lingual NER systems.
These mappings are builtfrom weakly annotated data, and can be easily ex-tended to new languages with no human annotationor language-dependent knowledge involved.Experimental results show that the Wikipedia-based approaches are effective in improving the gen-eralization capability of NER systems.
When a sys-tem is well trained, the dictionary feature approachachieves the best improvement over the baselinesystem; while when a system is trained with lit-tle human-annotated training data, a more aggres-sive decoding constraint approach achieves the bestimprovement.
The improvements are larger on un-seen entities, and the approaches are especially use-ful when a system is applied to a new domain or it istrained with little training data.AcknowledgmentsWe would like to thank Avirup Sil for helpful com-ments, and for collecting the Wikipedia data.
Wealso thank the anonymous reviewers for their sug-gestions.1283ReferencesRonan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537, November.Wisam Dakka and Silviu Cucerzan.
2008.
Augment-ing Wikipedia with named entity tags.
In Proceedingsof the 3rd International Joint Conference on NaturalLanguage Processing, pages 545?552, Hyderabad, In-dia.Radu Florian, Hany Hassan, Abe Ittycheriah, HongyanJing, Nanda Kambhatla, Xiaqiang Luo, Nicolas Ni-colov, and Salim Roukos.
2004.
A statistical modelfor multilingual entity detection and tracking.
In Pro-ceedings of the Human Language Technologies Con-ference 2004 (HLT-NAACL?04), pages 1?8, Boston,Massachusetts, USA, May.
Association for Computa-tional Linguistics.Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collec-tive entity linking in web text: A graph-based method.In Proceedings of the 34th International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, SIGIR ?11, pages 765?774, New York,NY, USA.
ACM.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,Hagen Fu?rstenau, Manfred Pinkal, Marc Spaniol,Bilyana Taneva, Stefan Thater, and Gerhard Weikum.2011.
Robust disambiguation of named entities in text.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 782?792, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing Wikipedia as external knowledge for named entityrecognition.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 698?707, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Guillaume Lample, Miguel Ballesteros, Sandeep Subra-manian, Kazuya Kawakami, and Chris Dyer.
2016.Neural architectures for named entity recognition.In Proceedings of NAACL-HLT (NAACL 2016), SanDiego, US.Andrew McCallum, Dayne Freitag, and Fernando C. N.Pereira.
2000.
Maximum entropy Markov models forinformation extraction and segmentation.
In Proceed-ings of the Seventeenth International Conference onMachine Learning, ICML ?00, pages 591?598, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Kamal Nigam, John Lafferty, and Andrew McCallum.1999.
Using maximum entropy for text classification.In In IJCAI-99 Workshop on Machine Learning for In-formation Filtering, pages 61?67.Joel Nothman, Nicky Ringland, Will Radford, Tara Mur-phy, and James R. Curran.
2013.
Learning multilin-gual named entity recognition from Wikipedia.
Jour-nal of Artificial Intelligence, 194:151?175, January.Will Radford, Xavier Carreras, and James Henderson.2015.
Named entity recognition with document-specific KB tag gazetteers.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 512?517, Lisbon, Por-tugal, September.
Association for Computational Lin-guistics.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL-2009),pages 147?155, Boulder, Colorado, June.
Associationfor Computational Linguistics.Alexander E. Richman and Patrick Schone.
2008.
Min-ing Wiki resources for multilingual named entityrecognition.
In Proceedings of ACL-08: HLT, pages1?9, Columbus, Ohio, June.
Association for Computa-tional Linguistics.Avirup Sil and Radu Florian.
2014.
The IBM systemsfor English entity discovery and linking and Spanishentity linking at TAC 2014.
In Text Analysis Confer-ence (TAC), Gaithersburg, Maryland, USA.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CONLL-2003 shared task:Language-independent named entity recognition.
InProceedings of the Seventh Conference on NaturalLanguage Learning at HLT-NAACL 2003 - Volume 4,CONLL ?03, pages 142?147, Stroudsburg, PA, USA.Association for Computational Linguistics.Erik F. Tjong Kim Sang.
2002.
Introduction tothe CONLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of the SixthConference on Natural Language Learning - Volume20, CONLL ?02, pages 1?4, Stroudsburg, PA, USA.Association for Computational Linguistics.Antonio Toral and Rafael Muoz.
2006.
A proposal toautomatically build and maintain gazetteers for namedentity recognition by using Wikipedia.
In EACL 2006.1284
