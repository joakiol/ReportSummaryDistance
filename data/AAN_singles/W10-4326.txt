Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139?146,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsThe Effects of Discourse Connectives Prediction on Implicit DiscourseRelation RecognitionZhi Min Zhou?, Man Lan?,?, Zheng Yu Niu?, Yu Xu?, Jian Su?
?East China Normal University, Shanghai, PRC.
?Baidu.com Inc., Beijing, PRC.
?Institute for Infocomm Research, Singapore.51091201052@ecnu.cn, lanman.sg@gmail.comAbstractImplicit discourse relation recognition isdifficult due to the absence of explicitdiscourse connectives between arbitraryspans of text.
In this paper, we use lan-guage models to predict the discourse con-nectives between the arguments pair.
Wepresent two methods to apply the pre-dicted connectives to implicit discourserelation recognition.
One is to use thesense frequency of the specific connec-tives in a supervised framework.
Theother is to directly use the presence of thepredicted connectives in an unsupervisedway.
Results on PDTB2 show that usinglanguage model to predict the connectivescan achieve comparable F-scores to theprevious state-of-art method.
Our methodis quite promising in that not only it hasa very small number of features but alsoonce a language model based on other re-sources is trained it can be more adaptiveto other languages and domains.1 IntroductionDiscourse relation analysis involves identifyingthe discourse relations (e.g., the comparison re-lation) between arbitrary spans of text, wherethe discourse connectives (e.g., ?however?, ?be-cause?)
may or may not explicitly exist in the text.This analysis is one important application both asan end in itself and as an intermediate step in var-ious downstream NLP applications, such as textsummarization, question answering etc.As discussed in (Pitler and Nenkova., 2009b),although explicit discourse connectives may havetwo types of ambiguity, i.e., one is discourse ornon-discourse usage (?once?
can be either a tem-poral connective or a word meaning ?formerly?
),the other is discourse relation sense ambiguity(?since?
can serve as either a temporal or causalconnective), their study shows that for explicitdiscourse relations in Penn Discourse Treebank(PDTB) corpus, the most general 4 senses, i.e.,Comparison (Comp.
), Contingency (Cont.
), Tem-poral (Temp.)
and Expansion (Exp.
), can be eas-ily addressed by the presence of discourse con-nectives and a simple method only considering thesense frequency of connectives can achieve morethan 93% accuracy.
This indicates the importanceof connectives for discourse relation recognition.However, with implicit discourse relationrecognition, there is no connective between thetextual arguments, which results in a very difficulttask.
In recent years, a multitude of efforts havebeen employed to solve this task.
One approachis to exploit various linguistically informed fea-tures extracted from human-annotated corpora ina supervised framework (Pitler et al, 2009a) and(Lin et al, 2009).
Another approach is to performrecognition without human-annotated corpora bycreating synthetic examples of implicit relations inan unsupervised way (Marcu and Echihabi, 2002).Moreover, our initial study on PDTB implicitrelation data shows that the averaged F-score forthe most general 4 senses can reach 91.8% whenwe obtain the sense of test examples by map-ping each implicit connective to its most frequentsense (i.e., sense recognition using gold-truth im-plicit connectives).
This high F-score performanceagain proves that the connectives are very crucialsource for implicit relation recognition.In this paper, we present a new method to ad-dress the problem of recognizing implicit dis-course relation.
This method is inspired by theabove observations, especially the two gold-truthresults, which reveals that discourse connectivesare very important signals for discourse relationrecognition.
Our basic idea is to recover the im-plicit connectives (not present in real text) be-tween two spans of text with the use of a language139model trained on large amount of raw data withoutany human-annotation.
Then we use these pre-dicted connectives to generate feature vectors intwo ways for implicit discourse relation recogni-tion.
One is to use the sense frequency of the spe-cific connectives in a supervised framework.
Theother is to directly use the presence of the pre-dicted connectives in an unsupervised way.We performed evaluation on explicit and im-plicit relation data sets in the PDTB 2 corpus.
Ex-perimental results showed that the two methodsachieved comparable F-scores to the state-of-artmethods.
It indicates that the method using lan-guage model to predict connectives is very usefulin solving this task.The rest of this paper is organized as follows.Section 2 reviews related work.
Section 3 de-scribes our methods for implicit discourse relationrecognition.
Section 4 presents experiments andresults.
Section 5 offers some conclusions.2 Related WorkExisting works on automatic recognition of im-plicit discourse relations fall into two categoriesaccording to whether the method is supervised orunsupervised.Some works perform relation recognition withsupervised methods on human-annotated corpora,for example, the RST Bank (Carlson et al, 2001)used by (Soricut and Marcu, 2003), adhoc anno-tations used by (Girju, 2003) and (Baldridge andLascarides, 2005), and the GraphBank (Wolf et al,2005) used by (Wellner et al, 2006).Recently the release of the Penn DiscourseTreeBank (PDTB) (Prasad et al, 2006) has sig-nificantly expanded the discourse-annotated cor-pora available to researchers, using a comprehen-sive scheme for both implicit and explicit rela-tions.
(Pitler et al, 2009a) performed implicit re-lation classification on the second version of thePDTB.
They used several linguistically informedfeatures, such as word polarity, verb classes, andword pairs, showing performance increases over arandom classification baseline.
(Lin et al, 2009)presented an implicit discourse relation classifierin PDTB with the use of contextual relations, con-stituent Parse Features, dependency parse featuresand cross-argument word pairs.
Although both oftwo methods achieved the state of the art perfor-mance for automatical recognition of implicit dis-course relations, due to lack of human-annotatedcorpora, their approaches are not very useful in thereal word.Another line of research is to use the unsuper-vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-terns to extract instances of discourse relationssuch as contrast and elaboration from unlabeledcorpora.
Then they used word-pairs between argu-ments as features for building classification mod-els and tested their model on artificial data for im-plicit relations.Subsequently other studies attempt to ex-tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered thatMarcu and Echihabi?s models do not perform aswell on implicit relations as one might expectfrom the test accuracy on synthetic data.
(Gold-ensohn, 2007) extended the work of (Marcu andEchihabi, 2002) by refining the training and clas-sification process using parameter optimization,topic segmentation and syntactic parsing.
(Saitoet al, 2006) followed the method of (Marcu andEchihabi, 2002) and conducted experiments witha combination of cross-argument word pairs andphrasal patterns as features to recognize implicitrelations between adjacent sentences in a Japanesecorpus.Previous work showed that with the use of somepatterns, structures, or the pairs of words, rela-tion classification can be performed using unsu-pervised methods.In contrast to existing work, we investigated anew knowledge source, i.e., implicit connectivespredicted using a language model, for implicit re-lation recognition.
Moreover, this method canbe applied in both supervised and unsupervisedways by generating features on labeled and unla-beled training data and then performing implicitdiscourse connectives recognition.3 Methodology3.1 Predicting implicit connectives via alanguage modelPrevious work (Pitler and Nenkova., 2009b)showed that with the presence of discourse con-nectives, explicit discourse relations in PDTB canbe easily identified with more than 90% F-score.Our initial study on PDTB human-annotated im-plicit relation data shows that the averaged F-scorefor the most general 4 senses can reach 91.8%when we simply map each implicit connective to140its most frequent sense.
These high F-scores indi-cate that the connectives are very crucial source ofinformation for both explicit and implicit relationrecognition.
However, for implicit relations, thereare no explicitly discourse connectives in real text.This built-in absence makes the implicit relationrecognition task quite difficult.
In this work weovercome this difficulty by inserting connectivesinto the two arguments with the use of a languagemodel.Following the annotation scheme of PDTB, weassume that each implicit connective takes twoarguments, denoted as Arg1 and Arg2.
Typi-cally, there are two possible positions for mostof implicit connectives, i.e., the position beforeArg1 and the position between Arg1 and Arg2.Given a set of implicit connectives {ci}, we gen-erate two synthetic sentences, ci+Arg1+Arg2 andArg1+ci+Arg2 for each ci, denoted as Sci,1 andSci,2.
Then we calculate the perplexity (an intrin-sic score) of these sentences with the use of a lan-guage model, denoted as Ppl(Sci,j).
According tothe value of Ppl(Sci,j) (the lower the better), wecan rank these sentences and select the connec-tives in top N sentences as implicit connectivesfor this argument pair.
Here the language modelmay be trained on any large amount of unanno-tated corpora that can be cheaply acquired.
Typi-cally, a large corpora with the same domain as thetest data will be used for training language model.Therefore, we chose news corpora, such as NorthAmerican News Corpora.After that, we use the top N predicted connec-tives to generate different feature vectors and per-form the classification in two ways.
One is to usethe sense frequency of predicted connectives in asupervised framework.
The other is to directly usethe presence of the predicted connectives in an un-supervised way.
The two approaches are describedas follows.3.2 Using sense frequency of predicteddiscourse connectives as featuresAfter the above procedure, we get a sorted set ofpredicted discourse connectives.
Due to the pres-ence of an implicit connective, the implicit dis-course relation recognition task can be addressedwith the methods for explicit relation recognition,e.g., sense classification based only on connectives(Pitler et al, 2009a).
Inspired by their work, thefirst approach is to use sense frequency of pre-dicted discourse connectives as features.
We takethe connective with the lowest perplexity value(i.e., top 1 connective) as the real connective forthe arguments pair.
Then we count the sensefrequency of this connective on the training set.Figure 1 illustrates the procedure of generatingpredicted discourse connective from a languagemodel and calculating its sense frequency fromtraining data.
Here the calculation of sense fre-quency of connective is based on the annotatedtraining data which has labeled discourse rela-tions, thus this method is a supervised one.Figure 1: Procedure of generating a predicted dis-course connective and its sense frequency from thetraining set and a language model.Then we can directly use the sense frequencyto generate a 4-feature vector to perform the clas-sification.
For example, the sense frequency ofthe connective but in the most general 4 sensescan be counted from training set as 691, 6, 49,2, respectively.
For a given pair of arguments,if but is predicted as the top 1 connective basedon a language model, a 4-dimension feature vec-tor (691, 6, 49, 2) is generated for this pair andused for training and test procedure.
Figure 2and 3 show the training and test procedure for thismethod.Figure 2: Training procedure for the first ap-proach.141Figure 3: Test procedure for the first approach.3.3 Using presence or absence of predicteddiscourse connective as features(Pitler et al, 2008) showed that most connectivesare unambiguous and it is possible to obtain high-accuracy in prediction of discourse senses due tothe simple mapping relation between connectivesand senses.
Given two examples:(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining isgetting heavier and heavier.The two connectives, i.e., but in E1 and becausein E2, convey the Comparison and Contingencysenses respectively.
In most cases, we can easilyrecognize the relation sense by the appearance ofa discourse connective since it can be interpretedin only one way.
That means the ambiguity ofthe mapping between sense and connective is quitelow.
Therefore, the second approach is to use onlythe presence of the top N predicted discourse con-nectives to generate a feature vector for a givenpair of arguments.4 Experiment4.1 Data setsWe used PDTB as our data set to perform the eval-uation of our methods.
The corpus contains anno-tations of explicit and implicit discourse relations.The first evaluation is performed on the annotatedimplicit data set.
Following the work of (Pitler etal., 2009a), we used sections 2-20 as the trainingset, sections 21-22 as the test set and sections 0-1 as the development set for parameter optimiza-tion (e.g., N value).
The second evaluation is per-formed on the annotated explicit data set.
We fol-low the method used in (Sporleder and Lascarides,2008) to remove the discourse connective from theexplicit instances and consider these processed in-stances as implicit ones.We constructed four binary classifiers to recog-nize each main senses (i.e., Cont., Cont., Exp.,Temp.)
from the rest.
For each sense we usedequal numbers of positive and negative instancesin training set.
The negative instances were cho-sen at random from the rest of training set.
Forboth evaluations all instances in sections 21-22were used as test set.
Table 1 lists the numbersof positive and negative instances for each sensein training, development and test sets of implicitand explicit relation data sets.4.2 Evaluation and classifierTo evaluate the performance of above systems, weused two widely-used measures, F-score ( i.e., F1)and accuracy.
In addition, in this work we usedthe LIBSVM toolkit to construct four linear SVMclassifiers for each sense.4.3 PreprocessingWe used the SRILM toolkit to build a languagemodel and calculated the perplexity value for eachtraining and test sample.
The steps are describedas follows.
First, since perplexity is an intrin-sic score to measure the similarity between train-ing and test samples, in order to fit the restric-tion of perplexity we chose 3 widely-used cor-pora in the Newswire domain to train the languagemodel, i.e., (1) the New York part of BLLIP NorthAmerican News Text (Complete), (2) the Xin and(3) the Ltw parts of the English Gigaword FourthEdition.
For the BLLIP corpus with 1,796,386automatically parsed English sentences, we con-verted the parsed sentences into original textualdata.
Some punctuation marks such as commas,periods, minuses, right/left parentheses are con-verted into their original form.
For the Xin andLtw parts, we only used the Sentence Detectortoolkit in OpenNLP to split each sentence.
Finallywe constructed 3-, 4- and 5-grams language mod-els from these three corpora.
Table 2 lists statis-tics of different n-grams in the different languagemodels and different corpora.Next, for each instance we combined its Arg1and Arg2 with connectives obtained from PDTB.There are two types of connectives, single con-nectives (e.g.
?because?
and ?but?)
and paral-142Table 1: Statistics of positive and negative instances for each sense in training, development and test setsof implicit and explicit relation data sets.Implicit ExplicitComp.
Cont.
Exp.
Temp.
Comp.
Cont.
Exp.
Temp.Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124Table 2: Statistics of different n-grams in the dif-ferent language models and different corpora.n-gram BLLIP - Gigaword- Gigaword-New York Xin Ltw1-gram 1638156 2068538 22764912-grams 26156851 23961796 335048733-grams 80876435 77799100 1018556394-grams 127142452 134410879 1597919165-grams 146454530 168166195 183794771lel connectives (such as ?not only .
.
.
, but also?
).Since discourse connectives may appear not onlyahead of the Arg1, but also between Arg1 andArg2, we considered this case.
Given a set ofpossible implicit connectives {ci}, for a singleconnective ci, we constructed two synthetic sen-tences, ci+Arg1+Arg2 and Arg1+ci+Arg2.
In caseof parallel connectives, we constructed one syn-thetic sentence like ci,1+Arg1+ci,2+Arg2.As a result, we obtain 198 synthetic sentences(|ci| ?
2 for single connective or |ci| for parallelconnective) for each pair of arguments.
Then weconverted all words to lower cases and used thelanguage model trained in the above step to calcu-late its perplexity (the lower the better) value onsentence level.
The sentences were ranked fromlow to high according to their perplexity scores.For example, given a sentence with arguments pairas follows:Arg1: it increased its loan-loss reserves by $93million after reviewing its loan portfolio,Arg2: before the loan-loss addition it had operat-ing profit of $10 million for the quarter.we got the perplexity (Ppl) values for this argu-ments pair in combination with two connectives(but and by comparison) in two positions as fol-lows:1. but + Arg1 + Arg2: Ppl= 349.6222.
Arg1 + but + Arg2: Ppl= 399.3393. by comparison + Arg1 + Arg2: Ppl= 472.2064.
Arg1 + by comparison + Arg2: Ppl= 543.051In our second approach described in Section3.3, we considered the combination of connectivesand their position as final features like mid but,first but, where the features are binary, that is,the presence or absence of the specific connective.According to the value of Ppl(Sci,j), we tried var-ious N values on development set to get the opti-mal N value.4.4 ResultsTable 3 summarizes the best performanceachieved using gold-truth implicit connectives,the previous state-of-art performance achievedby (Pitler et al, 2009a) and our approaches.The first line shows the result by mapping thegold-truth implicit connectives directly to therelation?s sense.
The second line presents the bestresult of (Pitler et al, 2009a).
One thing worthmentioning here is that for the Expansion relation,(Pitler et al, 2009a) expanded both training andtest sets by including EntRel relation as positiveexamples, which makes it impossible to performdirect comparison.
The third and fourth linesshow the best results using our first approach,where the sense frequency is counted on explicitand implicit training set respectively.
The last lineshows the best result of our second approach onlyconsidering the presence of top N connectives.Table 4 summarizes the best performance usinggold-truth explicit connectives reported in (Pitlerand Nenkova., 2009b) and our two approaches.Figure 4 shows the curves of averaged F-scoreson implicit connective classification with differ-ent n-gram language models.
From this figure wecan see that all 4-grams language models achievedaround 0.5% better averaged F-score than 3-gramsmodels.
And except for Ltw corpus, other 5-gramsmodels achieved lower averaged F-score than 4-grams models.
Specially the 5-grams result ofNew York corpus is much lower than its 3-gramsresult.Figure 5 shows the averaged F-scores of dif-ferent top N on the New York corpus with 3-,4- and 5-grams language models.
The essential143Table 3: Best result of implicit relations compared with state-of-art methods.System Comp.
vs. Not Cont.
vs. Other Exp.
vs. Other Temp.
vs. Other AveragedF1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)Sense recognition usinggold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)Best result in (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)Table 4: Best result of explicit relation conversion to implicit relation compared with results using thesame method.System Comp.
vs. Not Cont.
vs. Other Exp.
vs. Other Temp.
vs. Other AverageF1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)Sense recognition using gold-truthexplicit connectives in (Pitler et al, 2009a) N/A N/A N/A N/A N/A(93.67)Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)0 10 20 30 40 50 60 70 80 90 10011012013014015016017018019020030.030.531.031.532.032.533.033.534.034.5NY 3-gramNY 4-gramNY 5-gramTop N valueAveragedF-ScoreFigure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with differenttop N values.trend of these curves cannot be summarized inone sentence.
But we can see that the best aver-aged F-scores mostly appeared in the range from100 ?
160.
For 4-grams and 5-grams models, thesystem achieved the top averaged F-scores whenN = 20 as well.4.5 DiscussionExperimental results on PDTB showed that usingpredicted connectives achieved the comparable F-scores of the state-of-art method.From Table 3 we can find that our results areclosely to the best performance of previous state-of-art methods in terms of averaged F-score.
Onthe Comparison sense, our first approach has animprovement of more than 4% F-score on the pre-vious state-of-art method (Pitler et al, 2009a).
Aswe mentioned before, for the Expansion sense,they included EntRel relation to expand the train-ing set and test set, which makes it impossible toperform a direct comparison.
Since the positive in-stances size has been increased by 50%, they mayachieve a higher F-score than our approach.
Forother relations, our best performance is slightlylower than theirs.
While bearing in mind that ourapproach only uses a very small amount of fea-tures for implicit relation recognition.
Compared1443-gram 4-gram 5-gram31.031.231.431.631.832.032.232.432.6New YorkXinLtwn-gramAveragedF-scoreFigure 4: Curves of averaged F-score on implicitconnective classification with n-Gram languagemodel.with other approaches involving thousands of fea-tures, our method is quite promising.From Table 4 we observe comparable averagedF-score (39.98% F-score) on explicit relation dataset to that on implicit relation data set.
Previ-ously, (Sporleder and Lascarides, 2008) also usedthe same conversion method to perform implicitrelation recognition on different corpora and theirbest result is around 33.69% F-score.
Althoughthe two results cannot be compared directly due todifferent data sets, the magnitude of performancequantities is comparable and reliable.By comparing with the above different systems,we find several useful observations.
First, ourmethod using predicted implicit connectives via alanguage model can help the task of implicit dis-course relation recognition.
The results are com-parable to the previous state-of-art studies.
Sec-ond, our method has a lot of advantages, i.e., avery small amount of features (several or no morethan 200 vs. ten thousand), easy computation(only based on the trained language model vs. us-ing a lot of NLP tools to extract a large amount oflinguistically informed features) and fast running,which makes it more practical in real world appli-cation.
Furthermore, since the language model canbe trained on many corpora whether annotated orunannotated, this method is more adaptive to otherlanguages and domains.5 ConclusionsIn this paper we have presented an approach toimplicit discourse relation recognition using pre-dicted implicit connectives via a language model.The predicted connectives have been used for im-plicit relation recognition in two ways, i.e., super-vised and unsupervised framework.
Results on thePenn Discourse Treebank 2.0 show that the pre-dicted discourse connectives can help implicit re-lation recognition and the two algorithms achievecomparable F-scores with the state-of-art method.In addition, this method is quite promising due toits simple, easy to retrieve, fast run and increasedadaptivity to other languages and domains.AcknowledgmentsWe thank the reviewers for their helpful com-ments and Jonathan Ginzburg for his mentor-ing.
This work is supported by grants fromNational Natural Science Foundation of China(No.60903093), Shanghai Pujiang Talent Program(No.09PJ1404500) and Doctoral Fund of Ministryof Education of China (No.20090076120029).ReferencesJ.
Baldridge and A. Lascarides.
2005.
Probabilistichead-driven parsing for discourse structure.
Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning.L.
Carlson, D. Marcu, and Ma.
E. Okurowski.
2001.Building a discourse-tagged corpus in the frame-work of rhetorical structure theory.
Proceedings ofthe Second SIG dial Workshop on Discourse and Di-alogue.B.
Dorr.
LCS Verb Database.
Technical Report OnlineSoftware Database, University of Maryland, CollegePark, MD,2001.R.
Girju.
2003.
Automatic detection of causal relationsfor question answering.
In ACL 2003 Workshops.S.
Blair-Goldensohn.
2007.
Long-Answer Ques-tion Answering and Rhetorical-Semantic Relations.Ph.D.
thesis, Columbia Unviersity.M.
Lapata and A. Lascarides.
2004.
InferringSentence-internal Temporal Relations.
Proceedingsof the North American Chapter of the Assocation ofComputational Linguistics.Z.H.
Lin, M.Y.
Kan and H.T.
Ng.
2009.
RecognizingImplicit Discourse Relations in the Penn DiscourseTreebank.
Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing.D.
Marcu and A. Echihabi.
2002.
An UnsupervisedApproach to Recognizing Discourse Relations.
Pro-ceedings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics.145E.
Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.Lee, A. Joshi.
2008.
Easily Identifiable Dis-course Relations.
Coling 2008: Companion vol-ume: Posters.E.
Pitler, A. Louis, A. Nenkova.
2009.
Automaticsense prediction for implicit discourse relations intext.
Proceedings of the 47th Annual Meeting of theAssociation for Computational Linguistics.E.
Pitler and A. Nenkova.
2009.
Using Syntax to Dis-ambiguate Explicit Discourse Connectives in Text.Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers.M.
Porter.
An algorithm for suffix stripping.
In Pro-gram, vol.
14, no.
3, pp.130-137, 1980.R.
Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.2006.
Annotating attribution in the Penn DiscourseTreeBank.
Proceedings of the COLING/ACL Work-shop on Sentiment and Subjectivity in Text.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.Robaldo, A. Joshi, B. Webber.
2008.
The Penn Dis-course TreeBank 2.0.
Proceedings of LREC?08.M.
Saito, K.Yamamoto, S.Sekine.
2006.
UsingPhrasal Patterns to Identify Discourse Relations.Proceeding of the HLTCNA Chapter of the ACL.R.
Soricut and D. Marcu.
2003.
Sentence Level Dis-course Parsing using Syntactic and Lexical Informa-tion.
Proceedings of the Human Language Technol-ogy and North American Association for Computa-tional Linguistics Conference.C.
Sporleder and A. Lascarides.
2008.
Using automat-ically labelled examples to classify rhetorical rela-tions: an assessment.
Natural Language Engineer-ing, Volume 14, Issue 03.B.
Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.2006.
Classification of discourse coherence rela-tions: An exploratory study using multiple knowl-edge sources.
Proceedings of the 7th SIGDIALWorkshop on Discourse and Dialogue.F.
Wolf, E. Gibson, A. Fisher, M. Knight.
2005.The Discourse GraphBank: A database of texts an-notated with coherence relations.
Linguistic DataConsortium.146
