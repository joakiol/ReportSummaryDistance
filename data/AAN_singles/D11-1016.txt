Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 172?182,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsCompositional Matrix-Space Models for Sentiment AnalysisAinur YessenalinaDept.
of Computer ScienceCornell UniversityIthaca, NY, 14853ainur@cs.cornell.eduClaire CardieDept.
of Computer ScienceCornell UniversityIthaca, NY, 14853cardie@cs.cornell.eduAbstractWe present a general learning-based approachfor phrase-level sentiment analysis that adoptsan ordinal sentiment scale and is explicitlycompositional in nature.
Thus, we can modelthe compositional effects required for accu-rate assignment of phrase-level sentiment.
Forexample, combining an adverb (e.g., ?very?
)with a positive polar adjective (e.g., ?good?
)produces a phrase (?very good?)
with in-creased polarity over the adjective alone.
In-spired by recent work on distributional ap-proaches to compositionality, we model eachword as a matrix and combine words us-ing iterated matrix multiplication, which al-lows for the modeling of both additive andmultiplicative semantic effects.
Although themultiplication-based matrix-space frameworkhas been shown to be a theoretically ele-gant way to model composition (Rudolph andGiesbrecht, 2010), training such models hasto be done carefully: the optimization is non-convex and requires a good initial startingpoint.
This paper presents the first such al-gorithm for learning a matrix-space model forsemantic composition.
In the context of thephrase-level sentiment analysis task, our ex-perimental results show statistically signifi-cant improvements in performance over a bag-of-words model.1 IntroductionSentiment analysis has been an active research areain recent years.
Work in the area ranges from iden-tifying the sentiment of individual words to deter-mining the sentiment of phrases, sentences and doc-uments (see Pang and Lee (2008) for a survey).
Thebulk of previous research, however, models just pos-itive vs. negative sentiment, collapsing positive (ornegative) words, phrases and documents of differ-ing intensities into just one positive (or negative)class.
For word-level sentiment, therefore, thesemethods would not recognize a difference in senti-ment between words like ?good?
and ?great?, whichhave the same direction of polarity (i.e., positive)but different intensities.
At the phrase level, themethods will fail to register compositional effects insentiment brought about by intensifiers like ?very?,?absolutely?, ?extremely?, etc.
?Happy?
and ?veryhappy?, for example, will both be considered sim-ply ?positive?
in sentiment.
In real-world settings,on the other hand, sentiment values extend across apolarity spectrum ?
from very negative, to neutral,to very positive.
Recent research has shown, in par-ticular, that modeling intensity at the phrase level isimportant for real-world natural language process-ing tasks including question answering and textualentailment (de Marneffe et al, 2010).This paper describes a general approach forphrase-level sentiment analysis that takes these real-world requirements into account: we adopt a five-level ordinal sentiment scale and present a learning-based method that assigns ordinal sentiment scoresto phrases.Importantly, our approach will also be explicitlycompositional1 in nature so that it can accurately ac-count for critical interactions among the words in1The Principle of Compositionality asserts that the meaningof a complex expression is a function of the meanings of itsconstituent expressions and the rules used to combine them.172each sentiment-bearing phrase.
Consider, for exam-ple, combining an adverb like ?very?
with a polaradjective like ?good?.
?Good?
has an a priori posi-tive sentiment, so ?very good?
should be consideredmore positive even though ?very?, on its own, doesnot bear sentiment.
Combining ?very?
with a nega-tive adjective, like ?bad?, produces a phrase (?verybad?)
that should be characterized as more negativethan the original adjective.
Thus, it is convenientto think of the effect of combining an intensifyingadverb with a polar adjective as being multiplica-tive in nature, if we assume the adjectives (?good?and ?bad?)
to have positive and a negative sentimentscores, respectively.Next, let us consider adverbial negators like ?not?combined with polar adjectives.
When model-ing only positive and negative labels for sentiment,negators are generally treated as flipping the polar-ity of the adjective it modifies (Choi and Cardie,2008; Nakagawa et al, 2010).
However, recent work(Taboada et al, 2011; Liu and Seneff, 2009) sug-gests that the effect of the negator when ordinal sen-timent scores are employed is more akin to damp-ening the adjective?s polarity rather than flipping it.For example, if ?perfect?
has a strong positive sen-timent, then the phrase ?not perfect?
is still positive,though to a lesser degree.
And while ?not terrible?
isstill negative, it is less negative than ?terrible?.
Forthese cases, it is convenient to view ?not?
as shift-ing polarity to the opposite side of polarity scale bysome value.There are, of course, more interesting examples ofcompositional semantic effects on sentiment: e.g.,prevent cancer, ease the burden.
Here, the verbsprevent and ease act as content-word negators (Choiand Cardie, 2008) in that they modify the negativesentiment of their direct object arguments so that thephrase as a whole is perceived as somewhat positive.Nonetheless, the vast majority of methods forphrase- and sentence-level sentiment analysis do nottackle the task compositionally: they, instead, em-ploy a bag-of-words representation and, at best, in-corporate additional features to account for nega-tors, intensifiers, and for contextual valence shifters,which can change the sentiment over neighboringwords (e.g., Polanyi and Zaenen (2004), Wilson etal.
(2005) , Kennedy and Inkpen (2006), Shaikh etal.
(2007)).One notable exception is Moilanen and Pulman(2007), who propose a compositional semantic ap-proach to assign a positive or negative sentiment tonewspaper article titles.
However, their knowledge-based approach presupposes the existence of a sen-timent lexicon and a set of symbolic compositionalrules.But learning-based compositional approachesfor sentiment analyis also exist.
Choi andCardie (2008), for example, propose an algo-rithm for phrase-based sentiment analysis that learnsproper assignments of intermediate sentiment anal-ysis decision variables given the a priori (i.e., outof context) polarity of the words in the phrase andthe (correct) phrase-level polarity.
As in Moilianenand Pulman (2007), semantic inference is based on(a small set of) hand-written compositional rules.
Incontrast, Nakagawa et.
al (2010) use a dependencyparse tree to guide the learning of compositional ef-fects.
Each of the above, however, uses a binaryrather than an ordinal sentiment scale.In contrast, our proposed method for phrase-level sentiment analysis is inspired by recent workon distributional approaches to compositionality.In particular, Baroni and Zamparelli (2010) tackleadjective-noun compositions using a vector repre-sentation for nouns and learning a matrix represen-tation for each adjective.
The adjective matrices arethen applied as functions over the meanings of nouns?
via matrix-vector multiplication ?
to derive themeaning of adjective-noun combinations.
Rudolphand Giesbrecht (2010) show theoretically, that mul-tiplicative matrix-space models are a general caseof vector-space models and furthermore exhibit de-sirable properties for semantic analysis: they takeinto account word order and are algebraically, neuro-logically and psychologically plausible.
This work,however, does not present an algorithm for learningsuch models; nor does it provide empirical evidencein favor of matrix-space models over vector-spacemodels.In the sections below, we propose a learning-based approach to assign ordinal sentiment scores tosentiment-bearing phrases using a general composi-tional matrix-space model of language.
In contrastto previous work, all words are modeled as matri-ces, independent of their part-of-speech, and com-positional inference is uniformly modeled as ma-173trix multiplication.
To predict an ordinal scale sen-timent value, we employ Ordered Logistic Regres-sion, introducing a novel training algorithm to ac-commodate our compositional matrix-space repre-sentations (Section 2).
To our knowledge, this is thefirst such algorithm for learning matrix-space mod-els for semantic composition.
We evaluate the ap-proach on a standard sentiment corpus (Wiebe et al,2005) (Section 3), making use of its manually anno-tated phrase-level annotations for polarity and inten-sity, and compare our approach to the more com-monly employed bag-of-words model.
We show(Section 4) that our matrix-space model significantlyoutperforms a bag-of-words model for the ordinalscale sentiment prediction task.2 The Model for Ordinal Scale SentimentPredictionAs described above, our task is to predict an ordi-nal scale sentiment value for a phrase.
To this end,we employ a sentiment scale with five ordinal val-ues: VERY NEGATIVE, NEGATIVE, NEUTRAL, POS-ITIVE and VERY POSITIVE.
Given a set of phrase-level training examples with their gold-standard or-dinal sentiment value, we then use an Ordered Lo-gistic Regression (OLogReg) model for prediction.Unfortunately, our matrix-space representation pre-cludes doing this directly.We have chosen OLogReg, as opposed to sayPRanking (Crammer and Singer, 2001), because op-timization of the former is more attractive: the ob-jective (likelihood) is smooth and the gradients arecontinuous.
As will become clear shortly, learn-ing our models is not trivial and it is important touse sophisticated off-the-shelf optimizers such as L-BFGS.For a bag-of-words model, OLogReg learns oneweight for each word and a set of thresholds by max-imizing the likelihood of the training data.
Typically,this is accomplished by using an optimizer like L-BFGS whose interface needs the value and gradientof the likelihood with respect to the parameters attheir current values.
In the next subsections, we in-stantiate OLogReg for our sentiment prediction taskusing a matrix-space word model (2.1 and 2.2) anda bag-of-words model (2.3).
The learning formula-tion of bag-of-words OLogReg is convex thereforewe will get the global optimum; in contrast, the op-timization problem for matrix-space model is non-convex, it is important to initialize the model well.Initialization of the matrix-space model is discussedin Section 2.4.2.1 NotationIn the subsequent subsections we will use thefollowing notation.
Let n be the number of phrasesin the training set and let d be the number of wordsin the dictionary.
Let xi be the i-th phrase and yiwould be the label of xi, where yi takes r differentvalues yi ?
{0, .
.
.
, r ?
1}.
Then |xi| will denotethe length of the phrase xi, and the words in i-thphrase are: xi = xi1, xi2, .
.
.
, xi|xi|; xij , 1 ?
j ?
|xi|is the j-th word of i-th phrase; where xij is from thedictionary: 1 ?
xij ?
d.In the case of the bag-of-words model, ?
(xi) ?Rd is the representation of the i-th phrase.
?j(xi)counts the number of times the j-th word from thedictionary appears in the i-th phrase.
Given a w ?Rd it assigns a score ?i to a phrase xi by?i = wT?
(xi) =|xi|?j=1wxij (1)In the case of the matrix-space model the ?
(xi) ?R|xi|?d is the representation of the i-th phrase.
?jk(xi) is 1, if xij is the k-th word in the dictionary,and zero otherwise.
Given u, v ?
Rm and a set ofmatrices {Wp ?
Rm?m}dp=1, one for each word, itassigns a score ?i to a phrase xi by?i = uT??|xi|?j=1d?k=1Wk?jk(xi)??
v= uT??|xi|?j=1Wxij??
v (2)where ?|xi|j=1 Wxij = Wxi1Wxi2 ?
?
?Wxi|xi| in exactlythis order.
We choose to map matrices to the realnumbers by using vectors u and v from Rm?1; sothat ?
= uTMv, where M ?
Rm?m, which is sen-sitive to the order of matrices2 , i.e.
uTM1M2v 6=2Care must be taken in choosing way to map matrix to a real174uTM2M1v.Modeling composition.
Am?mmatrix, represent-ing a word, can be considered as a linear function,mapping from Rm to Rm.
Composition of words ismodeled by function composition, in our case com-position of linear functions, i.e.
matrix multipli-cation.
Note, that unlike bag-of-words model, thematrix-space model takes word order into account,since matrix multiplication is not commutative op-eration.2.2 Ordered Logistic RegressionNow we will describe our objective function forOLogReg and its derivatives.
OLogReg has r ?1 thresholds (?0, .
.
.
?r?2), so introducing ?
?1 =??
and ?r?1 = ?
leads to the unified expressionfor posterior probabilities for all values of k:P (yi = k|x) = P (?k?1 < ?i ?
?k)= F (?k ?
?i)?
F (?k?1 ?
?i)F (x) is an inverse-logit functionF (x) = ex1 + exthis is its derivative:dF (x)dx = F (x)(1?
F (x))Therefore the negative loglikelihood of the trainingdata will look like the following (Hardin and Hilbe,2007):L = ?n?i=1r?1?k=0ln(F (?k ?
?i)?
F (?k?1 ?
?i))I(yi = k)where r is the number of ordinal classes, ?i is thescore of i-th phrase, I is the indicator function thatis equal to 1 ?
when yi = k, and zero otherwise.
Weneed to minimize the objective L with respect to thefollowing constraints:?k?1 ?
?k, 1 ?
k ?
r ?
2 (3)number.
For example, one other way to map matrices to thereal numbers is to use the determinant of a matrix; however, thedeterminant is not sensitive to the word order: det(M1M2) =det(M1)det(M2) = det(M2M1); which is not desirable for amodel that needs to account for word order.
(The constraints are similar to the ones in PRank al-gorithm).
For ease of optimization we parametrizeour model via ?0, and ?j , 1 ?
j ?
r ?
2:?
?1 = ?
?,?0,?1 = ?0 + ?1,?2 = ?0 +?2j=1 ?j ,.
.
.
,?r?2 = ?0 +?r?2j=1 ?j?r?1 = ?,where ?1, .
.
., ?r?2 are non-negative values, that rep-resent how far the corresponding thresholds are fromeach other.
Then the constraints (3) would be:?j ?
0, 1 ?
j ?
r ?
2 (4)To simplify the equations we can rewrite the nega-tive loglikelihood as follows:L = ?n?i=1r?1?k=0ln(Aik ?Bik)I(yi = k) (5)whereAik ={F (?0 +?kj=1 ?j ?
?i), if k = 0, .
.
.
, r ?
21, if k = r ?
1Bik ={0, if k = 0F (?0 +?k?1j=1 ?j ?
?i), if k = 1, .
.
.
, r ?
1Let?s introduce Lik = ?
ln(Aik ?
Bik)I(yi = k)and then the derivative of Lik with respect to ?0 willbe:?Lik?
?0= ?
[Aik(1?Aik)?Bik(1?Bik)]Aik ?BikI(yi = k)= (Aik + Bik ?
1)I(yi = k)For j = yi:?Lik?
?j= ?Aik(1?Aik)Aik ?BikI(yi = k)For all j < yi:?Lik?
?j= (Aik + Bik ?
1)I(yi = k)For all j > yi: ?Lik?
?j = 0.The derivative with respect to the score ?i is:?Lik?
?i= (?Aik ?Bik + 1)I(yi = k) (6)1752.2.1 Matrix-Space Word ModelHere we show the derivatives with respect to aword.
For the OLogReg model with matrix-spaceword representations, we have:?L?Wxij= ?L??i?
?
?i?WxijThe expression for ?L?
?i is given in (6); we will derive?
?i?Wxijfrom (2).
In the case of the Matrix-Space wordmodel each word is represented as an m?m affinematrix W :W =(A b0 1)(7)We choose the class of affine matrices since foraffine matrices matrix multiplication represents bothoperations: linear transformation and translation.Linear transformation is important for modelingchanges in sentiment - translation is also useful (wemake use of a translation vector during initialization,see Section 2.4).
In this work we consider m ?
3since we want the matrix A from (7) to representrotation and scaling.
Applying the affine transfor-mation W to vector [x, 1]T is equivalent to applyinglinear transformation A and translation b to x.
3Though vectors u and v can be learned togetherwith word matrices Wj , we choose to fix u and v.The main intuition behind fixing u and v is to re-duce the degrees of freedom of the model: differ-ent assignments of u, v and Wj-s can lead to thesame score ?, i.e.
there exist u?
v?
and W?j-s dif-ferent from u, v and Wj-s respectively, such that?
(u, v,W ) would be equal to ?
(u?, v?, W?
).
43?A b0 1?
?x1?=?Ax+ b1?where A is a linear transformation, b is a translation vector.Also the product of affine matrices is an affine matrix.4The specific choice of u and v leads to an equivalent modelfor all u?
and v?
such that u?
= MTu, v?
= M?1v, where M isany invertible transformation (i.e.
u?, v?
are derived from u,v byapplying linear transformations MT , M?1 respectively):uTW1W2v = (uTM)(M?1W1M)(M?1W2M)(M?1v)= u?T W?1W?2v?The derivative of the phrase ?i with respect to j-thword Wj would be (for brevity we drop the phraseindex and Wj refers to Wxij and p refers to |xi|):?
?i?Wj=(?uTW1W2 .
.
.Wpv?Wj)=[(uTW1 .
.
.Wj?1)T (Wj+1 .
.
.Wpv)T]=[(W Tj?1 .
.
.W T1 )(uvT )(W Tp .
.
.W Tj+1)](see Peterson and Pederson(2008)).In case if a certain word appears multiple times inthe phrase, the derivative with respect to that wordwould be a sum of derivatives with respect to eachappearance of a word, while all other appearancesare fixed.
For example,(?uTWW1Wv?W)= u(W1Wv)T + (uTWW1)T vTwhere W is a representation of a word that is re-peated.So given the expression (6) for ?L?
?i , the derivativewith respect to each word can be computed.
Noticethat the update for the j-th word in a sentence de-pends on the order words, which is in line with ourdesire to account for word order.2.2.2 OptimizationThe goal of training procedure is for the i-thphrase with p words x1x2 .
.
.
xp to learn word ma-trices W1, W2, .
.
.
, Wp such that resulting ?i-s willlead to the lowest negative loglikelihood.
The goalof training procedure is to find word matrices W1,W2, .
.
.
Wp and thresholds ?0, ?1, .
.
.
?r?2 suchthat the negative loglikelihood is minimized.
So,given the negative loglikelihood and the derivativeswith respect ?0 and ?j-s and word matrices W , weoptimize objective (5) subject to ?j ?
0.
We use L-BFGS-B (Large-scale Bound-constrained Optimiza-tion) by Byrd et al (1995) as an optimizer.2.2.3 Regularization in Matrix-Space ModelIn order to make sure that the L-BFGS-B updatesdo not cause numerical issues we perform the fol-lowing regularization to the resulting matrices.
Anm by m matrix Wj that can be represented as:Wj =(A11 a12aT21 a22)176where A11 ?
Rm?1?m?1, a12, a21 ?
Rm?1?1,a22 ?
R. First make the matrix affine by updatingthe last row, then the updated matrix will look like:W?j =(A11 a120 1)It can be proven that such a projection returns theclosest affine matrix in Frobenius norm.However, we also want to regularize the model toavoid ill-conditioned matrices.
Ill-conditioned ma-trices represent transformations whose output is verysensitive to small changes in the input and thereforethey have a similar effect to having large weightsin a bag-of-words model.
To perform such a reg-ularization we ?shrink?
the singular values of A11towards one.
More specifically, we first use theSingular Value Decomposition (SVD) of the A11:U?V T = A11, where U and V are orthogonal ma-trices, ?
is a matrix with singular values on the diag-onal.
Then we update singular values in the follow-ing way to get ??
: ?
?ii = ?hii, where h is a parameterbetween 0 and 1.
If h = 1 then ?ii remains thesame.
In the extreme case h = 0 then ?hii = 1.
Forintermediate values of h the singular values of A11would be brought closer to one.
Finally, we recom-pute A?11: A?11 = U ?
?V T .
So, Wj would be :W?j =(A?11 a120 1)2.2.4 Learning in the Matrix-Space ModelWe use Algorithm 1 to learn the matrix-spacemodel.
What essentially happens is that we iter-ate two steps: optimizing the W matrices using L-BFGS-B and the projection step.
L-BFGS-B returnsa solution that is not necessarily an affine matrix.After projecting to the space of affine matrices westart L-BFGS-B from a better initial point.
In prac-tice, the first few iterations lead to large decrease innegative loglikelihood.2.3 Bag-Of-Words ModelIn the bag-of-words model the score of the i-thphrase is given in (1).
Therefore, the partial deriva-tive with respect to j-th word in i-th phrase ?
?i?wxijisequal to the number cj of times xji appears in xi, so:?L?wxij= ?L??i?
cjAlgorithm 1 Training Algorithm for Matrix-SpaceOLogReg1: Input: {(x1, y1), .
.
.
, (xn, yn)} //training data2: Input: h //projection parameter3: Input: T //number of iterations4: Input: W , ?0 and ?j //initial values5: for t = 1, .
.
.
, T do6: (W , ?0, ?j)=minimize L using L-BFGS-B7: for i = 1, .
.
.
, d do8: Wi=Project(Wi, h)9: end for10: end for11: Return W , ?0, ?jOptimization.
We minimize negative loglikelihoodusing L-BFGS-B subject to ?j ?
0.Regularization.
To prevent overfitting for bag-of-words model we regularize w. The L2-regularizednegative loglikelihood will consist of the expressionin (5) and an additional term ?2 ||w||22, where || ?
||2is the L2-norm of a vector.
The derivative of theadditional term with respect to w will be:?
?2 ||w||22?w = ?wHence the partial derivative with respect to wxij willhave an additional term ?wxij .2.4 InitializationInitialization of bag-of-words OLogReg.
We ini-tialize the weight for each word with zero and ?0with a random number and ?j-s with non-negativerandom numbers.
Since the learning problem forbag-of-words OLogReg is convex, we will get theglobal optimum.Better Initialization of Matrix-Space Model.
Pre-liminary experiments showed that the Matrix-Spacemodel needs a good initialization.
Initializing withdifferent random matrices reaches different localminima and the quality of local minima depends oninitialization.
Therefore, it is important to initializethe model with a good initial point.
One way to ini-tialize the Matrix-Space model is to use the weightslearned by the bag-of-words model.
We use thefollowing intuition for initializing the Matrix-Spacemodel.
As noted in Section 2.2.1 applying trans-formation A of affine matrix W can model a linear177transformation, while vector b represents a transla-tion.
Since matrix-space model can encode a vector-space model (Rudolph and Giesbrecht, 2010), wecan initialize the matrices to exactly mimic the bag-of-words model.
In order to do that we place theweight, learned by the bag-of-words model in thefirst component of b.
Let?s assume that wx1 and wx2are the weights learned for two distinct words x1 andx2 respectively.
To compute the polarity score ofa phrase x1, x2 the bag-of-words model sums theweights of these two words: wx1 and wx2 .
Now wewant to have the same effect in matrix-space model.Here we assume m = 3.Z =?
?1 0 wx10 1 00 0 1???
?1 0 wx20 1 00 0 1??=?
?1 0 wx1 + wx20 1 00 0 1?
?Finally, there is a step of mapping matrix Z to anumber using u and v, such that ?
(Z) = wx1 +wx2 .We also want vector u and v to be such that:uT?
?1 0 wx1 + wx20 1 00 0 1??
v = wx1 + wx2 (8)The last equation can help us construct u and v.We also set u and v to be orthogonal: uT v = 0.So, we arbitrarily choose two orthogonal vectors forwhich equation (8) holds: u = [1,?2, 1]T and v =[1,?
?2, 1]T .53 Experimental MethodologyFor experimental evaluation of the proposed methodwe use the publicly available Multi-PerspectiveQuestion Answering (MPQA)6 corpus (Wiebe et al,2005) version 1.2, which contains 535 newswiredocuments that are manually annotated with phrase-level subjectivity and intensity.
We use theexpression-level boundary markings in MPQA toextract phrases.
We evaluate on positive, negativeand neutral opinion expressions that have intensities5If m > 3, u and v can be set using the same intuition.6http://www.cs.pitt.edu/mpqa/Polarity Intensity Ordinallabelnegative high, extreme 0negative medium 1neutral high, extreme, medium 2positive medium 3positive high, extreme 4Table 1: Mapping of combination of polarities and inten-sities from MPQA dataset to our ordinal sentiment scale.
?medium?, ?high?
or ?extreme?.7 The schematicmapping of phrase polarity and intensity values onordinal sentimental scale is shown in Table 1.3.1 Training DetailsWe perform 10-fold cross-validation on phrases ex-tracted from the MPQA corpus: eight folds for train-ing; one as a validation set; and one as test set.
Intotal there were 8022 phrases.
Before training, weextract lemmas for each word.
For evaluation weuse Ranking Loss: 1n?i |y?i ?
yi|, where y?i is theprediction.Choice of dimensionality m. The reported ex-periments are done by setting m = 3.
Preliminaryexperiments with higher values of m (5, 20, 50), didnot lead to a better performance and increased thetraining time; therefore we did not use those valuesin our final experiments.3.2 MethodsPRank.
For each of the folds, we run 500 iterationsof PRank and choose an early stopping iteration us-ing a model that led to the lowest ranking loss on thevalidation set; afterwards report the average perfor-mance of on a test set.Bag-of-words OLogReg.
To prevent overfitting wesearch for the best regularization parameter amongthe following values of ?
: 10i, from 10?4 to 104.The lowest negative log-likelihood value on the val-idation set is attained for8 ?
= 0.1.
With this valueof ?
fixed, the final model is the one with the lowestnegative loglikelihood on the training set.7We ignored low-intensity phrases similar to (Choi andCardie, 2008; Nakagawa et al, 2010).8We pick single ?
that gives best average validation set per-formance, and then use it to compute the average test set perfor-mance.178Method Ranking lossPRank 0.7808Bag-of-words OLogReg 0.6665Matrix-space OLogReg+RandInit 0.7417Matrix-space OLogReg+BowInit 0.6375?Table 2: Ranking loss for vector-space Ordered LogisticRegression and Matrix-Space Logistic Regression.?
Stands for a significant difference w.r.t.
the Bag-Of-Words OLogReg model with p-value less than 0.001(p < 0.001)Matrix-space OLogReg+RandInit.
First, we ini-tialized matrices with with random numbers fromnormal distribution N(0, 0.1) and set u and v as insection 2.4, T is set to 25.
We run with two differentrandom seeds and three different values for the pa-rameter h: [0.1, 0.5, 0.9] and report the performanceof the model that had the lowest likelihood on thevalidation set.
The setting of h that lead to the bestmodel was 0.9.Matrix-space OLogReg+BowInit.
For the matrix-space models we initialize the model with the out-put of the regularized Bag-of-words OLogReg as de-scribed in Section 2.4, T is set to 25.
Then we usethe training procedure of Algorithm 1.
We considerthree different values for the parameter h [0.1, 0.5,0.9] and choose as the model with the lowest valida-tion set negative log-likelihood.
The best setting ofh was 0.1.4 Results and DiscussionWe report Ranking Loss for the four models in Ta-ble 2.
The worst performance (denoted by the high-est ranking loss value) is obtained by PRank, fol-lowed by matrix-space OLogReg with random ini-tialization.
Bag-of-words OLogReg obtains quitegood performance, and matrix-space OLogReg, ini-tialized using the bag-of-words model performs thebest, showing statistically significant improvementsover the bag-of-words OLogReg model according toa paired t-test.
.To see what the bag-of-word and matrix-spacemodels are learning we performed inference on afew examples.
In Table 3 we show the sentimentscores of the best performing bag-of-words OLo-gReg model and the best performing model basedPhrase Matrix-space Bag-of-wordsOLogReg+BowInit OLogRegnot -0.83 -0.42very 0.23 0.04good 2.81 1.51very good 3.53 1.55not good -0.16 1.09not very good 0.66 1.13bad -1.67 -1.42very bad -2.01 -1.38not bad -0.54 -1.85not very bad -1.36 -1.80Table 3: Phrase and the sentiment scores of the phrase for2 models Matrix-space OLogReg+BowInit and Bag-of-words OLogReg respectively.
Notice that relative rank-ing order what matterson matrices Matrix-space OLogReg+BowInit.
Bysentiment score, we mean equation (1) of Bag-of-words OLogReg and equation (2) of Matrix-spaceOLogReg+BowInit.Here we choose two popular adjectives like?good?
and ?bad?
that appeared in the training data,and examine the effect of applying the intensifier?very?
on the sentiment score.
As we can see,the matrix-space model learns a matrix that inten-sifies both ?bad?
and ?good?
in the correct sentimentscale, i.e., ?
(good) < ?
(very good) and ?
(bad) <?
(very bad), while the bag-of-words model gets thesentiment of ?very bad?
wrong: it is more positivethan ?bad?.
We also looked at the effect of combin-ing ?not?
with these adjectives.
The matrix-spacemodel correctly encodes the effect of the negatorfor both positive and negative adjectives, such that?
(not good) < ?
(good) and ?
(bad) < ?
(not bad).For the interesting case of applying a negator to aphrase with an intensifier, ?
(not good) should beless than ?
(not very good) and ?
(not very bad)should be less than ?
(not bad).9 As shown in Ta-ble 3, these are predicted correctly by the matrix-space model, which the matrix-space model getsright, but the bag-of-words model misses in the caseof ?bad?.Also notice that since in the matrix-space model9See the detailed discussion in Taboada et al (2011) and Liuand Seneff (2009).179each word is represented as a function, more specif-ically a linear operator, and the function composi-tion defined as matrix multiplication, we can thinkof ?not very?
being an operator itself, that is a com-position of operator ?not?
and operator ?very?.5 Related WorkSentiment Analysis.
There has been a lot ofresearch in determining the sentiment of wordsand constructing polarity dictionaries (Hatzivas-siloglou and McKeown, 1997; Wiebe, 2000; Raoand Ravichandran, 2009; Mohammad et al, 2009;Velikovich et al, 2010).
Some recent work is try-ing to identify the degree of sentiment of adjectivesand adverbs from text using co-occurrence statistics.Work by Taboada et.
al (2011) and Liu and Sen-eff (2009), suggest ways of computing the sentimentof adjectives from data, and computing the effectof combining adjective with adverb as multiplica-tive effect and combining adjective with negation asadditive effect.
However these models require theknowledge of a part of speech of given words andthe list of negators (since the negator is an adjectiveas well).
In our work we propose a single unifiedmodel for handling all words of any part of speech.On the other hand, there has been some researchin trying to model compositional effects for senti-ment at the phrase- and sentence-level.
Choi andCardie (2008) hand-code compositional rules in or-der to model compositional effects of combining dif-ferent words in the phrase.
The hand-coded rulesare based on domain knowledge and used to learnthe effects of combining words in the phrase.
An-other recent work that tries to model the compo-sitional semantics of combining different words isNakagawa et.
al.
(2010), which proposes a modelthat learns the effects of combining different wordsusing phrase/sentence dependency parse trees and aninitial polarity dictionary.
They present a learningmethod that employs hidden variables for sentimentclassification: given the polarity of a sentence andthe a priori polarities of its words, they learn howto model the interactions between words with head-modifier relations in the dependency tree.Some of the previous work looked at MPQAphrase-level classification.
Wilson et al (2004) tack-les the problem of classifying clauses according totheir subjective strength but not polarity; Wilson etal.
(2005) classifies phrases according to their po-larity/sentiment but not strength.
Our task is differ-ent: we classify phrases according to a single ordinalscale that combines both polarity and strength.Task of predicting document-level star ratings wasconsidered in (Pang and Lee, 2005; Goldberg andZhu, 2006).
In the current work we look at fine-grained sentiment analysis, more specifically westudy word representations for use in true compo-sitional semantic settings.Distributional Semantics and Compositional-ity.
Research in the area of distributional seman-tics in NLP and Cognitive Science has looked atdifferent word representations and different ways ofcombining words.
Mitchell and Lapata (2010) pro-pose a framework for vector-based semantic com-position.
They define composition as an additiveor multiplicative function of two vectors and showthat compositional approaches generally outperformnon-compositional approaches that treat the phraseas the union of single lexical items.Work by Baroni and Zamparelli (2010) modelsnouns as vectors in some semantic space and ad-jectives as matrices.
It shows that modeling adjec-tives as linear transformations and applying thoselinear transformations to nouns results in final vec-tors for adjective-noun compositions that are closein semantic space to other similar phrases.
Theauthors argue that modeling adjectives as a lineartransformation is a better idea than using additivevector-space models.
In this work, a separate ma-trix for each adjective is learned using the Par-tial Least Squares method in a completely unsuper-vised way.
The recent paper by Rudolph and Gies-brecht (2010), described in the introduction, arguesfor multiplicative matrix-space models.
In contrastto other work in this area, our work is concernedwith a specific dimension of word meaning ?
sen-timent.
Our techniques, however, are quite generaland should be applicable to other problems in lexicalsemantics.6 Conclusions and Future workIn the current work we present a novel matrix-spacemodel for ordinal scale sentiment prediction and analgorithm for learning such a model.
The proposed180model learns a matrix for each word; the composi-tion of words is modeled as iterated matrix multi-plication.
The matrix-space framework with iteratedmatrix multiplication defines an elegant frameworkfor modeling composition; it is also quite general.We use the matrix-space framework in the contextof sentiment prediction, a domain where interestingcompositional effects can be observed.
The main fo-cus of this work was to study word representations(represent as a single weight vs. as a matrix) for usein true compositional semantic settings.
One of thebenefits of the proposed approach is that by learn-ing matrices for words, the model can handle unseenword compositions (e.g.
unseen bigrams) when theunigrams involved have been seen.However, it is not trivial to learn a matrix-spacemodel.
Since the final optimization problem is non-convex, the initialization has to be done carefully.Here the weights learned in bag-of-words modelcome to rescue and provide good initial point for op-timization procedure.
The final model outperformsthe bag-of-words based model, which suggests thatthis research direction is very promising.Though in our model the order of composition isthe same as the word order, we believe that a linguis-tically informed order of composition can give usfurther performance gains.
For example, one can usethe output of a dependency parser to guide the orderof composition, similar to Nakagawa et al (2010).Another possibility for improvement is to use the in-formation about the scope of negation.
In the currentwork we assume the scope of negation to be the ex-pression following the negation; in reality, however,determining the scope of negation is a complex lin-guistic phenomenon (Moilanen and Pulman, 2007).So the proposed model can benefit from identify-ing the scope of negation, similar to (Councill et al,2010).Also we plan to consider other ways to initializethe matrix-space model.
One interesting direction toexplore might be to use non-negative matrix factor-ization (Lee and Seung, 2001), co-clustering tech-niques (Dhillon, 2001) to better initialize words thatshare similar contexts.
The other possible directionis to use existing sentiment lexicons and employ-ing a ?curriculum learning?
strategy (Bengio et al,2009; Kumar et al, 2010) for our learning problem.AcknowledgmentsThis work was supported in part by National ScienceFoundation Grants BCS-0904822, BCS-0624277,IIS-0968450; and by a gift from Google.
We thankthe anonymous reviewers, and David Bindel, NikosKarampatziakis, Lillian Lee and Cornell NLP groupfor useful suggestions and insightful discussions.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 1183?1193, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Yoshua Bengio, Je?ro?me Louradour, Ronan Collobert, andJason Weston.
2009.
Curriculum learning.
In Pro-ceedings of the 26th Annual International Conferenceon Machine Learning, ICML ?09.
ACM.R.
H. Byrd, P. Lu, and J. Nocedal.
1995.
A limitedmemory algorithm for bound constrained optimiza-tion.
SIAM Journal on Scientific and Statistical Com-puting, pages 1190?1208.Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Empirical Methods inNatural Language Processing (EMNLP).Isaac G. Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not: learn-ing to classify the scope of negation for improved sen-timent analysis.
In Proceedings of the Workshop onNegation and Speculation in Natural Language Pro-cessing, NeSp-NLP ?10, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Koby Crammer and Yoram Singer.
2001.
Pranking withranking.
In Advances in Neural Information Process-ing Systems 14, pages 641?647.
MIT Press.Marie-Catherine de Marneffe, Christopher D. Manning,and Christopher Potts.
2010.
Was it good?
It wasprovocative.
learning the meaning of scalar adjectives.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, Uppsala, Swe-den, July 11?16.
ACL.I.
S. Dhillon.
2001.
Co-clustering documents and wordsusing bipartite spectral graph partitioning.
In KDD.Andrew B. Goldberg and Jerry Zhu.
2006.
Seeingstars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization.
InHLT-NAACL Workshop on Textgraphs: Graph-basedAlgorithms for Natural Language Processing.181James W. Hardin and Joseph Hilbe.
2007.
GeneralizedLinear Models and Extensions.
Stata Press.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In EACL, pages 174?181.Alistair Kennedy and Diana Inkpen.
2006.
Sentimentclassification of movie reviews using contextual va-lence shifters.
Computational Intelligence, 22(2, Spe-cial Issue on Sentiment Analysis)):110?125.M.
Pawan Kumar, Benjamin Packer, and Daphne Koller.2010.
Self-paced learning for latent variable models.In Advances in Neural Information Processing Sys-tems 23.
NIPS.D.
Lee and H. Seung.
2001.
Algorithms for non-negativematrix factorization.
In NIPS.Jingjing Liu and Stephanie Seneff.
2009. Review sen-timent scoring via a parse-and-paraphrase paradigm.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages161?169, Singapore, August.
Association for Compu-tational Linguistics.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009.Generating high-coverage semantic orientation lexi-cons from overtly marked words and a thesaurus.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages599?608, Singapore, August.
Association for Compu-tational Linguistics.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.
In Proceedings of Recent Advances inNatural Language Processing (RANLP 2007), pages378?382, September 27-29.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classificationusing crfs with hidden variables.
In Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL).Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the ACL,pages 115?124.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.K.
B. Petersen and M. S. Pedersen.
?2008?.
The MatrixCookbook.
?Technical University of Denmark?, ?oct?.
?Version 20081110?.Livia Polanyi and Annie Zaenen.
2004.
Contextuallexical valence shifters.
In Proceedings of the AAAISpring Symposium on Exploring Attitude and Affect inText: Theories and Applications.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter of theACL (EACL 2009), pages 675?682, Athens, Greece,March.
Association for Computational Linguistics.Sebastian Rudolph and Eugenie Giesbrecht.
2010.
Com-positional matrix-space models of language.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, ACL ?10, pages 907?916, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mit-suru.
2007.
Assessing sentiment of text by semanticdependency and contextual valence analysis.Maite Taboada, Julian Brooke, Milan Tofiloskiy, andKimberly Vollz.
2011).
Lexicon-based methods forsentiment analysis.
In Computational Linguistics.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and RyanMcDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 777?785, Los Angeles, Cal-ifornia, June.
Association for Computational Linguis-tics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation (for-merly Computers and the Humanities), 39(2/3):164?210.Janyce M. Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In In AAAI, pages 735?740.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004.Just how mad are you?
In AAAI.
AAAI.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Empirical Methods in NaturalLanguage Processing (EMNLP).182
