Conversational Robots: Building Blocks for Grounding Word MeaningDeb RoyMIT Media Labdkroy@media.mit.eduKai-Yuh HsiaoMIT Media Labeepness@mit.eduNikolaos MavridisMIT Media Labnmav@media.mit.eduAbstractHow can we build robots that engage in fluidspoken conversations with people, moving be-yond canned responses to words and towardsactually understanding?
As a step towards ad-dressing this question, we introduce a roboticarchitecture that provides a basis for groundingword meanings.
The architecture provides per-ceptual, procedural, and affordance represen-tations for grounding words.
A perceptually-coupled on-line simulator enables sensory-motor representations that can shift points ofview.
Held together, we show that this archi-tecture provides a rich set of data structuresand procedures that provide the foundations forgrounding the meaning of certain classes ofwords.1 IntroductionLanguage enables people to talk about the world, past,present, and future, real and imagined.
For a robot todo the same, it must ground language in its world asmediated by its perceptual, motor, and cognitive capac-ities.
Many words that refer to entities in the world canbe grounded through sensory-motor associations.
For in-stance, the meaning of ball includes perceptual associ-ations that encode how balls look and predictive modelsof how balls behave.
The representation of touch must in-clude procedural associations that encode how to performthe action, and perceptual encodings to recognize the ac-tion in others.
In this view, words serve as labels for per-ceptual or action concepts.
When a word is uttered, theunderlying concept is communicated since the speakerand listener maintain similar associations.
This basicapproach underlies most work to date in building ma-chines that ground language (Bailey, 1997; Narayanan,1997; Regier and Carlson, 2001; Roy and Pentland, 2002;Siskind, 2001; Lammens, 1994; Steels, 2001).Not all words, however, can be grounded in termsof perceptual and procedural representations, even whenused in concrete situations.
In fact, in even the simplestconversations about everyday objects, events, and rela-tions, we run into problems.
Consider a person and arobot sitting across a table from each other, engaged incoordinated activity involving manipulation of objects.After some interaction, the person says to the robot:Touch the heavy blue thing that was on my left.To understand and act on this command in context,consider the range of knowledge representations that therobot must bind words of this utterance to.
Touch canbe grounded in a visually-guided motor program that en-ables the robot to move towards and touch objects.
Thisis an example of a procedural association which also crit-ically depends on perception to guide the action.
Heavyspecifies a property of objects which involves affordancesthat intertwine procedural representations with percep-tual expectations (Gibson, 1979).
Blue and left specifyvisual properties.
Thing must be grounded in terms ofboth perception and affordances (one can see an object,and expect to reach out and touch it).
Was triggers a ref-erence to the past.
My triggers a shift of perspective inspace.We have developed an architecture in which a physicalrobot is coupled with a physical simulator to provide thebasis for grounding each of these classes of lexical se-mantics1.
This workshop paper provides an abbreviated1We acknowledge that the words in this example, like mostwords, have numerous additional connotations that are not cap-tured by the representations that we have suggested.
For exam-ple, words such as touch, heavy and blue can be used metaphor-ically to refer to emotional actions and states.
Things are not al-ways physical perceivable objects, my usually indicates posses-sion, and so forth.
Barwise and Perry use the phrase ?efficiencyof language?
to highlight the situation-dependent reusability ofwords and utterances (Barwise and Perry, 1983).
However, forversion of a forthcoming paper (Roy et al, forthcoming2003).The robot, called Ripley, is driven by compliant actu-ators and is able to manipulate small objects.
Ripley hascameras, touch, and various other sensors on its ?head?.Force sensors in each actuated joint combined with po-sition sensors provide the robot with a sense of proprio-ception.
Ripley?s visual and proprioceptive systems drivea physical simulator that keeps a constructed version ofthe world (that includes Ripley?s own physical body) insynchronization with Ripley?s noisy perceptual input.
Anobject permanence module determines when to instanti-ate and destroy objects in the model based on perceptualevidence.
Once instantiated, perception can continue toinfluence the properties of an object in the model, butknowledge of physical world dynamics is built into thesimulator and counteracts ?unreasonable?
percepts.Language is grounded in terms of associations with el-ements of this perceptually driven world model, as wellas direct groundings in terms of sensory and motor rep-resentations.
Although the world model directly reflectsreality, the state of the model is the result of an interpre-tation process that compiles perceptual input into a sta-ble registration of the environment.
As opposed to directperception, the world model affords the ability to assumearbitrary points of view through the use of synthetic vi-sion which operates within the physical model, enablinga limited form of ?out of body experience?.
This abilityis essential to successfully differentiate the semantics ofmy left versus your left.
Non-linguistic cues such as thevisual location of the communication partners can be in-tegrated with linguistic input to context-appropriate per-spective shifts.
Shifts of perspective in time and spacemay be thought of as semantic modulation functions.
Al-though the meaning of ?left?
in one sense remains con-stant across usages, the words ?my?
and ?your?
modu-late the meaning by swapping frames of reference.
Wesuspect that successful use of language requires constantmodulations of meanings of this and related kinds.We describe the robot and simulator, and mechanismsfor real-time coupling.
We then discuss mechanismswithin this architecture designed for the purposes ofgrounding the semantics of situated, natural spoken con-versation.
Although no language understanding systemhas yet been constructed, we conclude by sketching howthe semantics of each of the words and the whole utter-ance discussed above can be grounded in the data struc-tures and processes provided by this architecture.
Thiswork represents steps towards our long term goal of de-veloping robots and other machines that use language inthe utterance and context that we have described, the ground-ings listed above play essential roles.
It may be argued thatother senses of words are often metaphoric extensions of theseembodied representations (Lakoff and Johnson, 1980).human-like ways by leveraging deep, grounded represen-tations of meaning that ?hook?
into the world throughmachine perception, action, and higher layers of cogni-tive processes.
The work has theoretical implications onhow language is represented and processed by machine,and also has practical applications where natural human-robot interaction is needed such as deep-sea robot con-trol, remote handling of hazardous materials by robots,and astronaut-robot communication in space.2 BackgroundAlthough robots, speech recognizers, and speech synthe-sizers can easily be connected in shallow ways, the re-sults are limited to canned behavior.
The proper inte-gration of language in a robot highlights deep theoret-ical issues that touch on virtually all aspects of artifi-cial intelligence (and cognitive science) including per-ception, action, memory, and planning.
Along with otherresearchers, we use the term grounding to refer to prob-lem of anchoring the meaning of words and utterances interms of non-linguistic representations that the languageuser comes to know through some combination of evolu-tionary and lifetime learning.A natural approach is to connect words to perceptualclassifiers so that the appearance of an object, event, orrelation in the environment can instantiate a correspond-ing word in the robot.
This basic idea has been appliedin many speech-controlled robots over the years (Brownet al, 1992; McGuire et al, 2002; Crangle and Suppes,1994).Detailed models have been suggested for sensory-motor representations underlying color (Lammens,1994), spatial relations (Regier, 1996; Regier and Carl-son, 2001).
Models for grounding verbs include ground-ing verb meanings in the perception of actions (Siskind,2001), and grounding in terms of motor control programs(Bailey, 1997; Narayanan, 1997).
Object shape is clearlyimportant when connection language to the world, but re-mains a challenging problem in computational models oflanguage grounding.
Landau and Jackendoff provide adetailed analysis of additional visual shape features thatplay a role in language (Landau and Jackendoff, 1993).In natural conversation, people speak and gesture tocoordinate joint actions (Clark, 1996).
Speakers and lis-teners use various aspects of their physical environmentto encode and decode utterance meanings.
Communica-tion partners are aware of each other?s gestures and fociof attention and integrate these source of information intothe conversational process.
Motivated by these factors,recent work on social robots have explored mechanismsthat provide visual awareness of human partners?
gazeand other facial cues relevant for interaction (Breazeal,2003; Scassellati, 2002).3 Ripley: An Interactive RobotRipley was designed specifically for the purposes of ex-ploring questions of grounded language, and interactivelanguage acquisition.
The robot has a range of motionsthat enables him to move objects around on a tabletopplaced in front of him.
Ripley can also look up and make?eye contact?
with a human partner.
Three primary con-siderations drove the design of the robot: (1) We are in-terested in the effects of changes of visual perspective andtheir effects on language and conversation, (2) Sensory-motor grounding of verbs.
(3) Human-directed trainingof motion.
For example, to teach Ripley the meaning of?touch?, we use ?show-and-tell?
training in which exem-plars of the word (in this case, motor actions) can be pre-sented by a human trainer in tandem with verbal descrip-tions of the action.To address the first consideration, Ripley has camerasplaced on its head so that all motions of the body lead tochanges of view point.
This design decision leads to chal-lenges in maintaining stable perspectives in a scene, butreflect the type of corrections that people must also con-stantly perform.
To support acquisition of verbs, Ripleyhas been designed with a ?mouth?
that can grasp objectsand enable manipulation.
As a result, the most naturalclass of verbs that Ripley will learn involve manual ac-tions such as touching, lifting, pushing, and giving.
Toaddress the third consideration, Ripley is actuated withcompliant joints, and has ?training handles?.
In spite ofthe fact that the resulting robot resembles an arm morethan a torso, it nonetheless serves our purposes as a vehi-cle for experiments in situated, embodied, conversation.In contrast, many humanoid robots are not actually ableto move their torso?s to a sufficient degree to obtain sig-nificant variance in visual perspectives, and grasping isoften not achieved in these robots due to additional com-plexities of control.
This section provides a description ofRipley?s hardware and low level sensory processing andmotor control software layers.3.1 Mechanical Structure and ActuationThe robot is essentially an actuated arm, but since cam-eras and other sensors are placed on the gripper, and therobot is able to make ?eye contact?, we often think ofthe gripper as the robot?s head.
The robot has seven de-grees of freedom (DOF?s) including a 2-DOF shoulder,1-DOF elbow, 3-DOF wrist / neck, and 1-DOF gripper/ mouth.
Each DOF other than the gripper is actuatedby series-elastic actuators (Pratt et al, 2002) in which allforce from electric motors are transferred through torsionsprings.
Compression sensors are placed on each springand used for force feedback to the low level motion con-troller.
The use of series-elastic actuators gives Ripley theability to precisely sense the amount of force that is beingapplied at each DOF, and leads to compliant motions.3.2 Motion ControlA position-derivative control loop is used to track targetpoints that are sequenced to transit smoothly from thestarting point of a motion gesture to the end point.
Nat-ural motion trajectories are learned from human teachersthrough manual demonstrations.The robot?s motion is controlled in a layered fashion.The lowest level is implemented in hardware and consistsof a continuous control loop between motor amplifiersand force sensors of each DOF.
At the next level of con-trol, a microcontroller implements a position-derivative(PD) control loop with a 5 millisecond cycle time.
Themicrocontroller accepts target positions from a mastercontroller and translates these targets into force com-mands via the PD control loop.
The resulting force com-mands are sent down stream to the motor amplifier con-trol loop.
The same force commands are also sent upstream back to the master controller, serving as dynamicproprioceptive force informationTo train motion trajectories, the robot is put in a grav-ity canceling motor control mode in which forces dueto gravity are estimated based on the robot?s joint po-sitions and counteracted through actuation.
While inthis mode, a human trainer can directly move the robotthrough desired motion trajectories.
Motion trajectoriesare recorded during training.
During playback, motiontrajectories can be interrupted and smoothly revised tofollow new trajectories as determined by higher level con-trol.
We have also implemented interpolative algorithmsthat blend trajectories to produce new motions that be-yond the training set.3.3 Sensory System and Visual ProcessingRipley?s perceptual system is based on several kinds ofsensors.
Two color video cameras, a three-axis tilt ac-celerometer (for sensing gravity), and two microphonesare mounted in the head.
Force sensitive resistors providea sense of touch on the inside and outside surfaces of thegripper fingers.
In the work reported here, we make use ofonly the visual, touch, and force sensors.
The remainingsensors will be integrated in the future.
The microphoneshave been used to achieve sound source localization andwill play a role in maintaining ?eye contact?
with com-munication partners.
The accelerometer will be used tohelp correct frames of reference of visual input.Complementing the motor system is the robot?s sensorsystem.
One of the most important sets of sensors is theactuator set itself; as discussed, the actuators are force-controlled, which means that the control loop adjusts theforce that is output by each actuator.
This in turn meansthat the amount of force being applied at each joint isknown.
Additionally, each DOF is equipped with abso-lute position sensors that are used for all levels of motioncontrol and for maintaining the zero-gravity mode.The vision system is responsible for detecting ob-jects in the robot?s field of view.
A mixture of Gaus-sians is used to model the background color and pro-vides foreground/background classification.
Connectedregions with uniform color are extracted from the fore-ground regions.
The three-dimensional shape of an objectis represented using histograms of local geometric fea-tures, each of which represents the silhouette of the ob-ject from a different viewpoint.
Three dimension shapesare represented in a view-based approach using sets ofhistograms.
The color of regions is represented using his-tograms of illumination-normalized RGB values.
Detailsof the shape and color representations can be found in(Roy et al, 1999).To enable grounding of spatial terms such as ?above?and ?left?, a set of spatial relations similar to (Regier,1996) is measured between pair of objects.
The first fea-ture is the angle (relative to the horizon) of the line con-necting the centers of area of an object pair.
The secondfeature is the shortest distance between the edges of theobjects.
The third spatial feature measures the angle ofthe line which connects the two most proximal points ofthe objects.The representations of shape, color, and spatial rela-tions described above can also be generated from virtualscenes based on Ripley?s mental model as described be-low.
Thus, the visual features can serve as a means toground words in either real time camera grounded visionor simulated synthetic vision.3.4 Visually-Guided ReachingRipley can reach out and touch objects by interpolatingbetween recorded motion trajectories.
A set of sampletrajectories are trained by placing objects on the tabletop,placing Ripley in a canonical position so that the tableis in view, and then manually guiding the robot until ittouches the object.
A motion trajectory library is col-lected in this way, with each trajectory indexed by theposition of the visual target.
To reach an object in an arbi-trary position, a linear interpolation between trajectoriesis computed.3.5 Encoding Environmental Affordances: ObjectWeight and ComplianceWords such as ?heavy?
and ?soft?
refer to properties ofobjects that cannot be passively perceived, but requireinteraction with the object.
Following Gibson (Gibson,1979), we refer to such properties of objects as affor-dances.
The word comes from considerations of whatan object affords to an agent who interacts with it.
Forinstance, a light object can be lifted with ease as opposedto a heavy object.
To assess the weight of an unknownobject, an agent must actually lift (or at least attemptto lift) it and gauge the level of effort required.
This isprecisely how Ripley perceives weight.
When an objectis placed in Ripley?s mouth, a motor routine is initiatedwhich tightly grasps the object and then lifts and low-ers the object three times.
While the motor program isrunning, the forces experienced in each DOF (Section3.2) are monitored.
In initial word learning experiments,Ripley is handed objects of various masses and providedword labels.
A simple Bayes classifier was trained to dis-tinguish the semantics of ?very light?, ?light?, ?heavy?,and ?very heavy?.
In a similar vein, we also groundedthe semantics of ?hard?
and ?soft?
in terms of graspingmotor routines that monitor pressure changes at each fin-gertip as a function of grip displacement.4 A Perceptually-Driven ?Mental Model?Ripley integrates real-time information from its visualand proprioceptive systems to construct an ?internalreplica?, or mental model of its environment that bestexplains the history of sensory data that Ripley has ob-served2.
The mental model is built upon the ODE rigidbody dynamics simulator (Smith, 2003).
ODE providesfacilities for modeling the dynamics of three dimensionalrigid objects based on Newtonian physics.
As Rip-ley?s physical environment (which includes Ripley?s ownbody) changes, perception of these changes drive the cre-ation, updating, and destruction of objects in the mentalmodel.
Although simulators are typically used in place ofphysical systems, we found physical simulation to be anideal substrate for implementing Ripley?s mental model(for coupled on-line simulation, see also (Cao and Shep-herd, 1989; Davis, 1998; Surdu, 2000)).The mental model mediates between perception of theobjective world on one hand, and the semantics of lan-guage on the other.
Although the mental model reflectsthe objective environment, it is biased as a result of a pro-jection through Ripley?s particular sensory complex.
Thefollowing sections describe the simulator, and algorithmsfor real-time coupling to Ripley?s visual and propriocep-tive systems.The ODE simulator provides an interface for creatingand destroying rigid objects with arbitrary polyhedron ge-ometries placed within a 3D virtual world.
Client pro-grams can apply forces to objects and update their proper-ties during simulation.
ODE computes basic Newtonianupdates of object positions at discrete time steps basedobject masses and applied forces.
Objects in ODE arecurrently restricted to two classes.
Objects in Ripley?sworkspace (the tabletop) are constrained to be spheres offixed size.
Ripley?s body is modeled within the simula-2Mental models have been proposed as a central mechanismin a broad range of cognitive capacities (Johnson-Laird, 1983).Figure 1: Ripley looks down at objects on a tabletop.tor as a configuration of seven connected cylindrical linksterminated with a rectangular head that approximate thedimensions and mass of the physical robot.
We introducethe following notation in order to describe the simulatorand its interaction with Ripley?s perceptual systems.4.1 Coupling Perception to the Mental ModelAn approximate physical model of Ripley?s body is builtinto the simulator.
The position sensors from the 7 DOFsare used to drive a PD control loop that controls the jointforces applied to the simulated robot.
As a result, motionsof the actual robot are followed by dampened motions ofthe simulated robot.A primary motivation for introducing the mental modelis to register, stabilize, and track visually observed ob-jects in Ripley?s environment.
An object permanencemodule, called the Objecter, has been developed as abridge between raw visual analysis and the physical sim-ulator.
When a visual region is found to stably exist for asustained period of time, an object is instantiated by theObjecter in the ODE physical simulator.
It is only at thispoint that Ripley becomes ?aware?
of the object and isable to talk about it.
Once objects are instantiated in themental model, they are never destroyed.
If Ripley looksaway from an object such that the object moves out ofview, a representation of the object persists in the mentalmodel.
Figure 1 shows an example of Ripley looking overthe workspace with four objects in view.
In Figure 2, theleft image shows the output from Ripley?s head-mountedcamera, and the right image shows corresponding simu-lated objects which have been registered and are beingtracked.The Objecter consists of two interconnected compo-nents.
The first component, the 2D-Objecter, tracks two-dimension visual regions generated by the vision sys-tem.
The 2D-Objecter also implements a hysteresis func-tion which detects visual regions that persist over time.Figure 2: Visual regions and corresponding simulated ob-jects in Ripley?s mental model corresponding to the viewfrom Figure 1Figure 3: By positioning a synthetic camera at the posi-tion approximating the human?s viewpoint, Ripley is ableto ?visualize?
the scene from the person?s point of viewwhich includes a partial view of Ripley.The second component, the 3D-Objecter, takes as in-put persistent visual regions from the 2D-Objecter, whichare brought into correspondence with a full three dimen-sional physical model which is held by ODE.
The 3D-Objecter performs projective geometry calculations to ap-proximate the position of objects in 3D based on 2D re-gion locations combined with the position of the sourcevideo camera (i.e., the position of Ripley?s head).
Eachtime Ripley moves (and thus changes his vantage point),the hysteresis functions in the 2D-Objecter are reset, andafter some delay, persistent regions are detected and sentto the 3D-Objecter.
No updates to the mental model areperformed while Ripley is in motion.
The key problem inboth the 2D- and 3D-Objecter is to maintain correspon-dence across time so that objects are tracked and persistin spite of perceptual gaps.
Details of the Objecter willbe described in (Roy et al, forthcoming 2003).4.2 Synthetic Vision and Imagined Changes ofPerspectiveThe ODE simulator is integrated with the OpenGL 3Dgraphics environment.
Within OpenGL, a 3D environ-ment may be rendered from an arbitrary viewpoint bypositioning and orienting a synthetic camera and render-Figure 4: Using virtual shifts of perspective, arbitraryvantage points may be taken.
The (fixed) location of thehuman partner is indicated by the figure on the left.ing the scene from the camera?s perspective.
We take ad-vantage of this OpenGL functionality to implement shiftsin perspective without physically moving Ripley?s pose.For example, to view the workspace from the human part-ner?s point of view, the synthetic camera is simply movedto the approximate position of the person?s head (whichis currently a fixed location).
Continuing our example,Figures 3 and 4 show examples of two synthetic viewsof the situation from Figures 1 and 2.
The visual analy-sis features described in Section 3.3 can be applied to theimages generated by synthetic vision.4.3 Event-Based MemoryA simple form of run length encoding is used to com-pactly represent mental model histories.
Each time an ob-ject changes a properties more than a set threshold usinga distance measure that combines color, size, and loca-tion disparities, an event is detected in the mental model.Thus stretches of time in which nothing in the environ-ment changes are represented by a single frame of dataand a duration parameter with a relatively large value.When an object changes properties, such as its position,an event is recorded that only retains the begin and endpoint of the trajectory but discards the actual path fol-lowed during the motion.
As a result, references to thepast are discretized in time along event boundaries.
Thereare many limitations to this approach to memory, but aswe shall see, it may nonetheless be useful in groundingpast tense references in natural language.5 Putting the Pieces TogetherWe began by asking how a robot might ground the mean-ing of the utterance, ?Touch the heavy blue thing that wason my left?.
We are now able to sketch an answer to thisquestion.
Ripley?s perceptual system, and motor controlsystem, and mental model each contribute elements forgrounding the meaning of this utterance.
In this section,we informally show how the various components of thearchitecture provide a basis for language grounding.The semantic grounding of each word in our ex-ample utterance is presented using algorithmic descrip-tions reminiscent of the procedural semantics developedby Winograd (Winograd, 1973) and Miller & Johnson(Miller and Johnson-Laird, 1976).
To begin with a simplecase, the word ?blue?
is a property that may be defined as:property Blue(x) {c?
GetColorModel(x)return fblue(c)}The function returns a scalar value that indicates howstrongly the color of object x matches the expected colormodel encoded in fblue.
The color model would be en-coded using the color histograms and histogram com-parison methods described in Section 3.3.
The functionGetColorModel() would retrieve the color model of xfrom memory, and if not found, call on motor proceduresto look at x and construct a model.?Touch?
can be grounded in the perceptually-guidedmotor procedure described in Section 3.4.
This reachinggesture terminates successfully when the touch sensorsare activated and the visual system reports that the targetx remains in view:procedure Touch(x) {repeatReach-towards(x)until touch sensor(s) activatedif x in view thenreturn successelsereturn failureend if}Along similar lines, it is also useful to define a weighprocedure (which has been implemented as described inSection 3.5):procedure Weigh(x) {Grasp(x)resistance?
0while Lift(x) doresistance?
resistance + joint forcesend whilereturn resistance}Weigh() monitors the forces on the robot?s joints as itlifts x.
The accumulated forces are returned by the func-tion.
This weighing procedure provides the basis forgrounding ?heavy?
:property Heavy(x) {w?
GetWeight(x)return fheavy(w)}Similar to GetColorModel(), GetWeight() would firstcheck if the weight of x is already known, and if not,then it would optionally call Weigh() to determine theweight.To define ?the?, ?my?, and ?was?, it is useful to intro-duce a data structure that encodes contextual factors thatare salient during language use:structure Context {Point-of-viewWorking memory}The point of view encodes the assumed perspective forinterpreting spatial language.
The contents of workingmemory would include, by default, objects currently inthe workspace and thus instantiated in Ripley?s mentalmodel of the workspace.
However, past tense markerssuch as ?was?
can serve as triggers for loading salient el-ements of Ripley?s event-based memory into the workingmodel.
To highlight its effect on the context data struc-ture, Was() is defined as a context-shift function:context-shift Was(context) {Working memory?
Salient events from mental modelhistory}?Was?
triggers a request from memory (Section 4.3) forobjects which are added to working memory, makingthem accessible to other processes.
The determiner ?the?indicates the selection of a single referent from workingmemory:determiner The(context) {Select most salient element from working memory}In the example, the semantics of ?my?
can be groundedin the synthetic visual perspective shift operation de-scribed in Section 4.2:context-shift My(context) {context.point-of-view?
GetPointOfView(speaker)}Where GetPointOfV iew(speaker) obtains the spatialposition and orientation of the speaker?s visual input.?Left?
is also grounded in a visual property modelwhich computes a geometric spatial function (Section3.3) relative to the assumed point of view:property Left(x, context) {trajector?
GetPosition(x)return fleft(trajector, context.point?
of ?
view)}GetPosition(), like GetColorModel() would use theleast effortful means for obtaining the position of x. Thefunction fleft evaluates how well the position of x fitsa spatial model relative to the point of view determinedfrom context.?Thing?
can be grounded as:object Thing(x) {if (IsTouchable(x) and IsViewable(x)) return true;else return false}This grounding makes explicit use of two affordances ofa thing, that it be touchable and viewable.
Touchabilitywould be grounded using Touch() and viewability basedon whether x has appeared in the mental model (which isconstructed based on visual perception).The final step in interpreting the utterance is to com-pose the semantics of the individual words in order toderive the semantics of the whole utterance.
We addressthe problem of grounded semantic composition in detailelsewhere (Gorniak and Roy, protectforthcoming 2003).For current purposes, we assume that a syntactic parseris able to parse the utterance and translate it into a nestedset of function calls:Touch(The(Left(My(Heavy(Blue(Thing(Was(context)))))))))The innermost argument, context, includes the as-sumed point of view and contents of working memory.Each nested function modifies the contents of contextby either shifting points of view, loading new contentsinto working memory, or sorting / highlighting contentsof working memory.
The Touch() procedure finally actson the specified argument.This concludes our sketch of how we envision the im-plemented robotic architecture would be used to groundthe semantics of the sample sentence.
Clearly many im-portant details have been left out of the discussion.
Ourintent here is to convey only an overall gist of how lan-guage would be coupled to Ripley.
Our current work isfocused on the realization of this approach using spokenlanguage input.ReferencesD.
Bailey.
1997.
When push comes to shove: A computa-tional model of the role of motor control in the acqui-sition of action verbs.
Ph.D. thesis, Computer sciencedivision, EECS Department, University of Californiaat Berkeley.Jon Barwise and John Perry.
1983.
Situations and Atti-tudes.
MIT-Bradford.Cynthia Breazeal.
2003.
Towards sociable robots.Robotics and Autonomous Systems, 42(3-4).Michael K. Brown, Bruce M. Buntschuh, and Jay G.Wilpon.
1992.
SAM: A perceptive spoken lan-guage understanding robot.
IEEE Transactions on Sys-tems, Man, and Cybernetics, 22 .
IEEE Transactions22:1390?1402.F.
Cao and B. Shepherd.
1989.
Mimic: a robot planningenvironment integrating real and simulated worlds.
InIEEE International Symposium on Intelligent Control,page 459464.Herbert Clark.
1996.
Using Language.
Cambridge Uni-versity Press.C.
Crangle and P. Suppes.
1994.
Language and Learningfor Robots.
CSLI Publications, Stanford, CA.W.
J. Davis.
1998.
On-line simulation: Need andevolving research requirements.
In J.
Banks, editor,Handbook of Simulation: Principles, Methodology,Advances, Applications and Practice.
Wiley.James J. Gibson.
1979.
The Ecological Approach to Vi-sual Perception.
Erlbaum.Peter Gorniak and Deb Roy.
forthcoming, 2003.Grounded semantic composition for visual scenes.P.N.
Johnson-Laird.
1983.
Mental Models: Towards aCognitive Science of Language, Inference, and Con-sciousness.
Cambridge University Press.George Lakoff and Mark Johnson.
1980.
Metaphors WeLive By.
University of Chicago Press, Chicago.Johan M. Lammens.
1994.
A computational model ofcolor perception and color naming.
Ph.D. thesis, StateUniversity of New York.Barbara Landau and Ray Jackendoff.
1993.
?What?
and?where?
in spatial language and spatial cognition.
Be-havioral and Brain Sciences, 16:217?265.P.
McGuire, J. Fritsch, J.J. Steil, F. Roethling, G.A.
Fink,S.
Wachsmuth, G. Sagerer, and H. Ritter.
2002.
Multi-modal human-machine communication for instructingrobot grasping tasks.
In Proceedings of the IEEE/RSJInternational Conference on Intelligent Robots andSystems (IROS).George Miller and Philip Johnson-Laird.
1976.
Lan-guage and Perception.
Harvard University Press.Srinivas Narayanan.
1997.
KARMA: Knowledge-basedactive representations for metaphor and aspect.
Ph.D.thesis, University of California Berkeley.J.
Pratt, B. Krupp B, and C. Morse.
2002.
Series elas-tic actuators for high fidelity force control.
IndustrialRobot, 29(3):234?241.T.
Regier and L. Carlson.
2001.
Grounding spatial lan-guage in perception: An empirical and computationalinvestigation.
Journal of Experimental Psychology,130(2):273?298.Terry Regier.
1996.
The human semantic potential.
MITPress, Cambridge, MA.Deb Roy and Alex Pentland.
2002.
Learning words fromsights and sounds: A computational model.
CognitiveScience, 26(1):113?146.Deb Roy, Bernt Schiele, and Alex Pentland.
1999.Learning audio-visual associations from sensory in-put.
In Proceedings of the International Conferenceof Computer Vision Workshop on the Integration ofSpeech and Image Understanding, Corfu, Greece.Deb Roy, Kai-Yuh Hsiao, and Nick Mavridis.
forthcom-ing, 2003.
Coupling robot perception and on-line sim-ulation: Towards grounding conversational semantics.Brian Scassellati.
2002.
Theory of mind for a humanoidrobot.
Autonomous Robots, 12:13?24.Jeffrey Siskind.
2001.
Grounding the Lexical Semanticsof Verbs in Visual Perception using Force Dynamicsand Event Logic.
Journal of Artificial Intelligence Re-search, 15:31?90.R Smith.
2003.
ODE: Open dynamics engine.Luc Steels.
2001.
Language games for autonomousrobots.
IEEE Intelligent Systems, 16(5):16?22.John R. Surdu.
2000.
Connecting simulation to themission operational environment.
Ph.D. thesis, TexasA&M.T.
Winograd, 1973.
A Process model of Language Un-derstanding, pages 152?186.
Freeman.
