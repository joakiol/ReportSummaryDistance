Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1435?1446, Dublin, Ireland, August 23-29 2014.Combining Supervised and Unsupervised Parsingfor Distributional SimilarityMartin Riedl, Irina Alles and Chris BiemannFG Language TechnologyComputer Science Department, Technische Universit?at DarmstadtHochschulstrasse 10, D-64289 Darmstadt, Germany{riedl,biem}@cs.tu-darmstadt.de, ialles@gmx.deAbstractIn this paper, we address the role of syntactic parsing for distributional similarity.
On the onehand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers.On the other hand, we explore whether single unsupervised parsers, or their combination, cancontribute to better distributional similarities, or even replace supervised parsing as a prepro-cessing step for word similarity.
We evaluate distributional thesauri against manually createdtaxonomies both for English and German for five unsupervised parsers.
While for English, asupervised parser is the best single parser in this evaluation, we find an unsupervised parser towork best for German.
For both languages, we show significant improvements in word similaritywhen combining features from supervised and unsupervised parsers.
To our knowledge, this isthe first work where unsupervised parsers are systematically evaluated extrinsically in a seman-tic task, and the first work to show that unsupervised parsing can complement and even replacesupervised parsing, when used as a pre-processing feature.1 IntroductionWhile the field has seen increased interest in automatically inducing syntactic structures from raw or part-of-speech (POS) tagged text, the evaluation of unsupervised data-driven parsers has almost exclusivelybeen conducted either by introspection or by automatic comparison to treebanks.
It might be due tocomparatively low scores on reproducing a treebank?s syntactic annotation that hardly anyone has yetattempted to use the output of unsupervised parsers for an NLP task other than parsing itself.A further complication with unsupervised parsers ?
be it dependency parsers, constituency parsersor combinatory categorial grammar parsers ?
is that the categories induced by such parsers cannot bestraightforwardly mapped to linguistically-inspired categories as defined in a treebank.
But also whenconsidering only unlabeled syntactic annotations, an unsupervised parser is hardly to blame if it does notadhere to sometimes arbitrary conventions: e.g.
for dependencies, it is not a priori clear how to connectauxiliary and main verbs, where to attach the complementizer of subordinate clauses, how to representa conjunction and its conjuncts, how to relate the preposition and the nominal in prepositional phrases,and how to handle punctuation, cf.
Nivre and K?ubler (2006), Schwartz et al.
(2011).When it comes to utilizing syntactic structures, however, it is more important that they are consistentacross different sentences than that they adhere to specific syntactic theories and conventions.
Here, wechoose a task that makes only intermediary use of syntactic structures: we employ unsupervised parsingfor preprocessing corpora for the purpose of computing distributional similarities.
Since it is generallyaccepted (e.g.
(Lin, 1997; Curran and Moens, 2002)), that syntactic preprocessing plays an importantrole for the quality of distributional thesauri, and comparing words along their syntactic contexts doesrely on the existence of such a structure rather than its actual representation, we believe that distribu-tional similarities are an excellent test bed for addressing the following two research questions: (1) Howdo unsupervised parsers compare to supervised parsers when used as feature providers for building Dis-This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footer areadded by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1435tributional Thesauri (DTs) in comparison to supervised parsers?
(2) Can the combination of syntacticparsers increase DT quality?2 Related Work2.1 Unsupervised Parser EvaluationAs with other unsupervised approaches, the premise of unsupervised induction of syntactic structureis to alleviate the bottleneck of expensive manual annotations for improving NLP applications.
Forgrammar induction, the potential is extremely high due to the complexity of the subject matter: treebanksbelong to the most work-intensive NLP datasets.
On the other hand, this complexity is hard to grasp forunsupervised systems, which is probably the reason why unsupervised parsing technology is still in itsinfancy, despite more than a decade of work on this topic.One of the early works inducing structure from raw sentences and yielding better performance thana random baseline was achieved by van Zaanen and of Leeds.
School of Computer Studies (2001),who used an Alignment Based Learning (ABL) approach.
This algorithm compares all sentences ofa given set and considers matching sequences as constituents.
Klein and Manning (2002) presentedanother approach focusing on constituent sequences called the Constituent-Context Model (CCM).
It isan EM-based iterative approach that makes use of the linguistic phenomenon that long constituents oftenhave shorter representations of the same grammatical function that occur in similar contexts.
A hybridapproach combining CCM with a dependency model, called Dependency Model with Valence (DMV),shows even better performance and is the first unsupervised system to outperform the right-branchingbaseline (Klein and Manning, 2004).
A great number of recent works are based on DMV, such as thesystem by Headden III et al.
(2009), who improved DMV by adding lexical information, and Gillenwateret al.
(2010) who added posterior regularization during the training process.
Bod (2007) takes a slightlydifferent direction by following an ?all subtrees approach?, where all possible binary trees are generatedfor each sentence.
It generates all possible binary trees for each sentence.
The parse of a new sentence isdetermined by selecting the most probable tree based on the previously accumulated subtree frequencies.Most of the evaluation of these parsers was performed against a treebank, offering manually annotatedand linguistically motivated parse trees.
Schwartz et al.
(2011) underline the fact that treebanks containlinguistically problematic annotations, cases without linguistic consensus, such as the decision on thehead of a verb phrase or a sequence of nouns.
They show that the neglectance of these cases has asignificant but unjustified negative influence on the evaluation outcomes and propose a new measure,Neutral Edge Direction (NED), which alleviates this problem.
Bod (2007) argues that parser evaluationagainst a treebank favors supervised approaches and therefore measures the parser quality on the outcomeof a syntax based Machine Translation (MT) task where the dependency parsers are evaluated as languagemodels.
In Motazedi et al.
(2012), a single unsupervised parser is evaluated in an extrinsic evaluationfor realisation ranking, and does not compare favorably against a supervised parser.
Other extrinsicevaluations with supervised dependency parsers have been performed in information extraction systems(Miyao et al., 2008; Buyko and Hahn, 2010) or semantic role labeling (Johansson and Nugues, 2008).2.2 Evaluating Distributional SimilarityDistributional thesauri have been evaluated both extrinsically and intrinsically.
Extrinsic evaluations havebeen performed e.g.
for automatic set expansion (Pantel et al., 2009) or phrase polarity identification(Goyal and Daum?e, 2011).
In this work, we will conduct an intrinsic evaluation, which is more commonfor the evaluation of DTs and lexical semantic similarity.
Lin (1997; 1998) introduced two measuresusing WordNet (Miller, 1995) and Roget?s Thesaurus.
Using WordNet, he defines context (synsets aword occurs in Wordnet or subsets when using Roget?s Thesaurus) and then builds a gold standardthesaurus using a similarity measure on these contexts.
Then he evaluates his automatically computedDistributional Thesaurus (DT) with respect to the gold standard thesauri.
Weeds et al.
(2004) evaluatevarious similarity measures based on 1000 frequent and 1000 infrequent target terms.
Curran (2004)created a gold standard thesaurus by manually extracting entries from several English thesauri for 70words.
His automatically generated DTs are evaluated against this gold standard thesaurus.
All these1436systems employ context representations based on syntactic parsing for computing word similarity.We are going to use a comparatively simple WordNet-based measure, which calculates the similaritybetween two terms using the WordNet::Similarity path measure (Pedersen et al., 2004), and averagespath scores between a target term and its n most similar terms.
The score between two terms is inverselyproportional to the shortest path between all the synsets of both terms.
If two terms share a synset,the highest possible score of one is assigned.
The score is 0.5 for terms that stand in a direct hypernymrelation, and so on.
While the absolute scores are hard to interpret due to inhomogeneity in the granularityof WordNet, they are well-suited for relative comparison when operating on the same set of target terms.The evaluation in this work is performed by comparing the average score of the top ten entries in theDT for each of the target terms and report separately on frequent and rare words.
Riedl and Biemann(2013) also show that the results, using the WordNet based approach, are highly correlated to the resultsobserved with Curran?s approach using a manually created thesaurus.
This justifies the usage of manuallycreated taxonomies for this evaluation.3 Methodology3.1 ParsersIn our evaluation, we use five unsupervised parsers, which we will describe briefly.
They have beenselected to span several paradigms of unsupervised syntax induction, and due to software availability.Gillenwater et al.
(2010)1use a model based on the DMV (Klein and Manning, 2004) and improveperformance by adding sparsity biases on dependency types.
They assume a corpus annotated with POStags.
The aim of this bias is to limit unique head-dependent tag pairs, which is achieved by a constrainton model posteriors during the learning process.The work of Marecek and Straka (2013)2is another enhancement of the DMV and is subsequentlyreferred to as Unsupervised Dependency Parser (UDP).
It additionally uses prior knowledge in the formof stop estimates that are computed on a large raw corpus using the reducibility principle: a sequenceof words is considered as reducible if a word can be removed from the phrase and the remaining partappears another time in the corpus.
The assumed property, that the first word of a reducible sequencedoes not have any left children and the last word of this sequence does not have any right children, isused for the calculation of such stop estimates.
The authors show that estimates computed on a largecorpus such as Wikipedia can be used for the parsing of new text.Bisk and Hockenmaier (2013) use an EM approach to induce a Combinatory Categorial Grammar(CCG), based on very general linguistic assumptions.
It creates a model that can be used to parse un-seen data.
The algorithm requires a corpus, previously assigned with POS tags, in order to be able todistinguish between word classes (mainly to find the verb), and employs general knowledge such as thatsentences are headed by verbs.
Further language-specific properties are induced from the training data.Seginer (2007)3takes an incremental parsing and learning approach.
It operates directly on the plaintext without the need for POS tags, by using Common Cover Links (CCL), which can be directly con-verted to dependency arcs.
This parser learns during parsing and can be used without a prior learningstep.
This should result in increased parsing quality towards later stages, which suggests several passesover the training data.
The obtained model can then be reused to parse unseen data.The approach of S?gaard (2012) is different from all other approaches discussed here: This algorithmdoes not require any training data and can operate with or without POS tags.
For this reason, it can beapplied to arbitrary amounts of data, since it operates sentence-wise without memorizing previous inputs,and produces non-projective dependency parses.
The words of a phrase are ordered by centrality and aparse is determined by the ranking of a parsing algorithm, which uses general linguistic knowledge forgrammar induction.
This knowledge is inspired by the rules of Naseem et al.
(2010), and the approachhas been tuned (once and for all, for all languages) on development data from the Penn Treebank.1http://code.google.com/p/pr-toolkit/2http://ufal.mff.cuni.cz/udp/3http://www.seggu.net/ccl/1437Figure 1: Parses for an example sentence for several parsers.
Here, Bisk?s parser looks most similar tothe parses extracted from the Stanford parser.
Gillenwater and UDP seem to have some problems withthe full stop.
S?gaards parser mostly connects neighbors.Baseline S?gaard Gillenwater UDP Bisk Seginer SeginerEnglish 53.2 59.9 64.4 55.4 70.3 55.6 (WSJ 40) 74.2 (WSJ 10)German 33.7 57.6 35.7 52.4 68.4 38.2 (Negra 40) 48.0 (Negra 10)Table 1: Unlabeled accuracy values of different unsupervised parsers based on the CoNLL-X shared task(Buchholz and Marsi, 2006).
Seginers results show F-measure values for the Negra and the WSJ corpus,used with maximum sentence of lengths of 10 and 40.An example sentence and the according parses coming from the 10M model, except for UDP, wherethe 1M model is used (cf.
Table 2 in Section 4.3.1), are shown in Figure 1.Table 1 reports the accuracy of four parsers for the English and the German treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006) predicting unlabeled dependency parses for sentences withlength equal and smaller than 10 tokens.
Seginer reports only F-scores for WSJ and Negra consideringsentences with a maximum length of 10 and 40.
The best baselines reported in Canisius et al.
(2006) area left branching method for English and a nearest neighbor branching method for German, which is acombination of left and right branching.3.2 Computing Distributional ThesauriThe extraction of context features, used to calculate similarities between terms, is performed in accor-dance with the generic scheme proposed in (Biemann and Riedl, 2013): A (typed or untyped) parserarc is split into term and context feature, which consists of the edge direction and label (if any), and theconnected term.
Similarity between terms is subsequently computed on the overlap of their most salientcontext features.
We represent the term t and the context feature c as a pair < t, c > and extract a depen-dency triple (or dependency pair, as most unsupervised dependency parsers do not label the edges).
Forthe dependency between I and gave (nsub;gave;I) in I gave her the book, terms and context featureswould look like <gave,(nsub,I,@)> and <I,(nsub,@,gave)>.
In this example, the term gaveis characterized by the context information that I is its nominal subject, and term I is characterized bybeing the subject of gave.
We build distributional thesauri using the JoBimText4open-source framework.This framework scales to large data and has proven to outperform other methods, when using large data(Riedl and Biemann, 2013).
The computation of the distributional thesaurus within this framework isfollowing the MapReduce paradigm and scales to very large corpora.
This is achieved by applying asignificance measure between term and context feature, retaining only the most salient 1000 context fea-tures per term, and computing the cardinality of the set overlap between the respective context features4www.jobimtext.org, (Biemann and Riedl, 2013)1438per term, which defines the similarity between terms.
Per term, the most similar terms are subsequentlyranked, resulting in a distributional thesaurus as introduced by Lin (1997).4 EvaluationWe report experimental results on German and English corpora.
Both corpora are compiled from 10million sentences (about 2 Gigawords) each from the Leipzig Corpora Collection5, randomly sampledfrom online newspapers.
The semantic similarity in English DTs is assessed using WordNet 3.1 as alexical resource, as proposed by Riedl and Biemann (2013).
For evaluating the German DTs, we replaceWordNet by its German counterpart, GermaNet 8 (Hamp and Feldweg, 1997).
We report results sepa-rately for frequent and infrequent targets and average the path scores for the most similar 10 words perentry.
The evaluation of the English DTs is performed using 1000 frequent and 1000 infrequent nouns,as previously employed by Weeds et al.
(2004).
These nouns are randomly sampled from the BritishNational Corpus (BNC) and all occur in WordNet.
For the evaluation of German DTs, we randomlyselected 1000 frequent and 1000 infrequent nouns from our German corpus that all occur in GermaNet.4.1 Experimental SettingsThe DTs are calculated using the dependencies from the unsupervised parsers, one at a time.
To showthe impact of corpus size, we down-sampled our corpora, and used 1 million (1M), 100,000 (100K) and10,000 (10K) sentences (raw or automatically POS-tagged with the TreeTagger6) for training/inducingthe parsers.
Not all parsers were able to deal with the large training sets in feasible runtime, which mighteither be due to their computational complexity or their implementation.
While it would be preferable tokeep the corpus size for DT computations constant, this was not possible since some of our unsupervisedparsers cannot be applied to unseen text.
Hence, we decided to report DT quality results for two setups:Setup A uses the same data for training the parsers and the DT computation.
Setup B uses the fullcorpus of 10M sentences for DT computation, parsed with unsupervised parsers induced on differentlysized corpus samples.
We feel that Setup B is better reflecting the possible utilization of unsupervisedparsers for semantic similarity, since DT quality is known to increase with corpus size.
However, westill wanted to assess the quality of parsers that cannot be operated on unseen text in their current stateof development.4.2 Four BaselinesWe compare the results of unsupervised parsers against four baselines.
As a lower-bound baseline, weuse a random dependency parser that connects each word in a sentence with a randomly chosen otherword.
As a supervised upper-bound baseline, we use Stanford collapsed dependencies (Marneffe et al.,2006) for the English data and dependencies coming from the Mate tools (Bohnet, 2010) for the Germancorpus.
Finally, to gauge whether the potential of unsupervised parsing to model long-range depen-dencies ?
as opposed to local n-gram contexts ?
lead to better distributional similarities, we use wordbigrams and trigrams as n-gram-based systems.
The bigram system simulates left- and right-branching.We characterize the word in the first and in the second position of two neighboring words, which re-sults to the following term feature pairs according to the example in Section 3.2: <I,(@,gave)> and<gave,(I,@)>.
Using the trigram, we characterize the word in the second position with the contextfeature formed by the pair of words in first and third position.
The term-feature pair for gave would be<gave, (I,@,her)>.While we expect the scores of any reasonable unsupervised parser to fall somewhere between thelower bound and the upper bound when compared in the same setting, the n-gram baselines serve as ameasure for whether it is worth the trouble to induce and run the unsupervised parser for our evaluationapplication, as opposed to relying on an arguably simpler system for this purpose.5corpora.uni-leipzig.de, (Richter et al., 2006)6www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/, (Schmid, 1997)14394.3 Results4.3.1 Single Parser Results for EnglishWe summarize the results for the English evaluation for Setup A and Setup B in Table 2.
All unsu-pervised parsers beat the random baseline in all setups, with higher improvements observed using moretraining data, which somewhat validates their approaches.
Also, more data for DT computation resultsin higher similarity scores, and rare words generally receive lower scores on average, which is expectedand validates the DT computation framework.10k 100k 1M 10MParser freq rare freq rare freq rare freq rareSetup ARandom 0.115 0.029 0.128 0.082 0.145 0.103 0.159 0.113Trigram 0.133 0.020 0.179 0.082 0.200 0.120 0.236 0.151Bigram 0.140 0.029 0.173 0.088 0.208 0.129 0.246 0.159Stanford 0.151 0.028 0.209 0.128 0.261 0.176 0.280 0.209Seginer 0.136 0.027 0.176 0.085 0.211 0.127 0.240 0.155Gillenwater 0.135 0.026 0.159 0.077 0.195 0.117 0.223 0.141S?gaard 0.120 0.027 0.147 0.083 0.185 0.117 0.227 0.144UDP 0.127 0.017 0.169 0.063 0.204 0.119 * *Bisk 0.118 0.017 * * * * * *Setup BSeginer 0.200 0.063 0.236 0.139 0.241 0.156 0.240 0.155Gillenwater 0.220 0.140 0.221 0.142 0.221 0.141 0.223 0.141S?gaard 0.227 0.144 0.227 0.144 0.227 0.144 0.227 0.144Bisk 0.220 0.139 * * * * * *Table 2: Setup A English: Parser induction and DT computation on the same corpus.
Wordnet pathscores averaged on top 10 similar words, for 1000 frequent and 1000 rare nouns.
A * denotes that theevaluation failed because of computational constraints.
Setup B English: Parser induction on differentcorpus sizes, and DT computation on 10M sentences.In comparison to the n-gram baselines, only the parser by Seginer yields a higher score for frequentwords and 1M sentences training in Setup A.
However, the difference is very small and is confirmed onthe 10M sentences only in comparison to the Trigram baseline.
It seems that Seginer?s training proceduresaturates somewhere between 100K and 1M sentences, and shows even slightly worse performance on10M sentences of training in Setup B.
All parsers do not seem to be particularly useful as preprocessingsteps for DT computation, since better similarity can consistently be reached by context features basedon bigram statistics.Comparing the unsupervised parsers, we note that Seginer?s approach consistently scores highest inSetup A, while UDP comes in second for frequent words but not for rare words.
While Gillenwater?sapproach reaches comparably high scores for small corpora in Setup A, it is beaten by S?gaard?s no-training approach for larger corpora: It seems that Gillenwater?s training procedure can hardly make useof additional training, which is shown in Setup B, where practically no differences are observed between10K and 10M sentences of parser training.
Differences in Setup A are thus solely due to increased corpussize for DT computation for the Gillenwater experiments.UDP did not finish parsing 10M sentences after running for 157 days, and it is not trivial to disableits update procedure, which is why we could not include UDP in Setup B. Bisk?s parser is a specialcase in this evaluation, since it only selects sentences shorter than 15 tokens for training, and hence waseffectively trained on a 5400 sentence subset of the 10K corpus.
While we did not manage to train it onlarger corpora, we could apply this model on 10M sentences in Setup B, where it lands slightly belowthe no-training S?gaard parser, but clearly above Seginer?s approach for 10K training.4.3.2 Single Parser Results for GermanA different picture is drawn for the German evaluation (see Setup A in Table 3).
Comparing the resultsof the unsupervised parsers, Seginer?s parser does not only outperform the trigram and bigram baselinefor frequent nouns but also the supervised Mate parser for all corpus sizes.
Yet, the improvements over1440the Mate parser are not significant for all results using a paired t-test7.
Also, S?gaards parser exceeds thetrigram and bigram baseline for 10 million sentences.
The remaining unsupervised parsers can beat therandom baseline for frequent nouns but none of the n-gram baselines.
Again we are not able to parse the10 million sentences using UDP and also Gillenwater?s parser failed, parsing this corpus.
Comparing thebaselines in Setup A (see Table 3), we observe a difference between the sophisticated baselines and therandom baseline only for frequent words.10k 100k 1M 10MParser freq rare freq rare freq rare freq rareSetup ARandom 0.097 0.002 0.108 0.010 0.123 0.051 0.143 0.077Trigram 0.102 0.002 0.130 0.014 0.159 0.067 0.179 0.086Bigram 0.112 0.003 0.130 0.009 0.163 0.053 0.192 0.082Mate 0.111 0.004 0.126 0.014 0.170 0.027 0.204 0.090Seginer 0.113?
0.002 0.137?
0.012 0.171 0.068 0.208 0.091Gillenwater 0.104 0.002 0.118 0.009 0.132 0.040 * *S?gaard 0.104 0.002 0.123 0.010 0.161 0.054 0.193 0.077UDP 0.107 0.001 0.129 0.004 0.151 0.021 * *Bisk 0.101 0.002 * * * * * *Setup BSeginer 0.153 0.004 0.186 0.021 0.200 0.092 0.208 0.091Gillenwater 0.189 0.080 0.190 0.082 0.189 0.080 * *S?gaard 0.193 0.077 0.193 0.077 0.193 0.077 0.193 0.077Bisk 0.185 0.069 * * * * * *Table 3: Setup A and B for German corpora.Furthermore, we see that the supervised Mate parser results in worse scores for the frequent nounsusing the 10k and 100k dataset in comparison to the bigram baseline.
This could be attributed to theheavier tail in German?s word frequency distribution, which results in sparser context features for smalldata8.
For the 1M and 10M datasets, the supervised parser yields the best similarities for frequent nouns.The results for Setup B for the German corpora are shown at the bottom in Table 3.
We observesimilar trends to the ones for the English data: using more data for the training does not seem to help theperformance of Gillenwater?s algorithm.
Noticeable is the increase of Seginer?s results for rare words astraining data size increases.
Seginer?s algorithm even beats both n-gram baselines for the 10M corpuswhen trained only on 1 million sentences.4.3.3 Combining Parsers for DT Quality ImprovementTo clarify the best practice for building a DT of high quality, we combine different parsers: the twobest-performing unsupervised parsers (S?gaard?s and Seginer?s), the baselines and the supervised parser.Additionally, these two parsers where the only ones which could be applied to the largest dataset for bothlanguages.For English (see Table 4), we observe a boost in performance when combining unsupervised parsers.Combining the supervised Stanford parser with the bigram and the trigram baselines also leads to a sig-nificant improvement (p < 0.01)9over the Stanford parser alone, which is about the same as combiningthe supervised parser with the two unsupervised parsers, and combining all five types of features forDT construction.
Overall, a relative improvement of 3.5% on the average WordNet::Path measure forfrequent words and a relative 4% improvement for rare words is obtained over the Stanford parser alone.The results for German (see Table 5) show a similar trend.
It is remarkable that merging the two un-supervised parsers already outperforms the supervised Mate parser significantly9with p < 0.01 (6.7%for frequent and 8% relative improvement for rare words).
The combination of the supervised and unsu-pervised parsers again leads to further improvement, which is also significant over the supervised parseralone, and again, adding the bigram and trigram baselines to the three parsers does not help.7Significant improvements (p < 0.01) against the Mate parser are marked with the symbol ?
in Table 3 for frequent nouns.8Within the 10M sentences, there are 22 million word types in the German corpus and 10 million word types in the Englishcorpus.9We use a paired t-test to compare the DTs built using the supervised parser and the combinations.1441Parser frequent rareStanford (supervised) 0.280 0.209Seginer 0.240 0.155S?gaard 0.227 0.144Seginer & S?gaard 0.248 0.162Stanford & Bigram & Trigram 0.290?
0.217?Stanford & Seginer & S?gaard 0.291?
0.217?Stanford & Seginer & S?gaard &Bigram & Trigram0.290?
0.218?Table 4: Combinations of different parsersfor computing English thesauri.
The cross (?
)indicates significant improvements over thesupervised parser.Parser frequent rareMate (supervised) 0.204 0.090Seginer 0.208 0.091S?gaard 0.193 0.077Seginer & S?gaard 0.218?
0.097?Mate & Bigram & Trigram 0.204 0.091Mate & Seginer & S?gaard 0.222?
0.100?Mate & Seginer & S?gaard &Bigram & Trigram0.222?
0.100?Table 5: Combinations of different parsersfor computing German thesauri4.3.4 DiscussionOverall, it is surprising how well S?gaard?s parser performs in comparison to others on this task, since itneither uses training nor relies on POS tags.
This hints at either unsupervised parsing being simpler thancommonly assumed or rather the immaturity of all unsupervised parsers tested.
Further, we would haveexpected that trained unsupervised parsers, as most unsupervised methods, would exhibit a better per-formance when trained on larger corpora.
This could not be confirmed for both systems that we trainedon various corpus sizes, i.e.
Seginer?s and Gillenwater?s approach.
The findings are only moderatelycorrelated with evaluations on treebanks, cf Table 1: Whereas Seginer?s and S?gaard?s parsers performfavorably in our evaluation, they are outperformed by Bisk?s parser on treebanks, which currently doesnot scale to large data.
Gillenwater?s parser seems to be overly tuned to English treebanks, but cannotcapitalize on this in our evaluation for English.POS information does not seem beneficial for unsupervised parser induction in noun similarity evalu-ation, since the highest-scoring approach by Seginer does not use POS tags and a version of S?gaard?sparser with POS tags scored slightly but consistently lower than the version without POS, as we foundin further experiments.
This is in line with the findings of Cramer (2007), who reports no benefit frommanually corrected or unsupervised POS tags for a range of unsupervised parsers.Comparing the results of previous intrinsic evaluations (see Table 1) and the results of our extrinsicevaluation (see Table 2 and 3), we observe that the ranking of parsers is only mildly correlated.
Thus,our proposed evaluation covers different aspects than the adherence to (partially arbitrary) conventionsof manually labeled dependency data.
Also, our current evaluation disregards all arcs that do not involvenouns.When combining parsers, we observe that we can improve the quality of DTs significantly.
This leadsus to conclude that unsupervised parsers should at least be used for generating features when computingdistributional thesauri of high quality.
In case no high-quality supervised parser is available for thelanguage or domain of interest, it might suffice to use combinations of unsupervised parsers.We also report the computation times of the different parsers, for the English dataset for Setup A (seeTable 6).
The results were computed on a server with 80 GB and 16 cores.
Whereas all parsers requiredifferent amounts of memory, all parsers are single-threaded10.
While S?gaard?s parser is the fastestfor small datasets, Seginer?s runs faster on 10 million sentences.
Whereas Gillenwater?s and Seginer?s10k 100k 1M 10MSeginer 210 224 261 508Gillenwater 3248 3248 3280 5546S?gaard 3 21 182 975UDP 183 1220 11316 -Table 6: Computation time in minutes for parsing the data according to the English corpora used in SetupA, cf.
Table 210As S?gaards algorithm parses sentence-wise without storing any information, it could be easily run multi-threaded.1442algorithm require almost the same time for parsing 10k, 100k or 1M sentences, the runtime of the UDPand S?gaard?s parser is linear in time with the number of sentences to be parsed.
We cannot report theparsing times for the Bisk algorithm, as the parsing was not performed by us.
Again it is noticeable thatthe best two parsers are also the two unsupervised parsers that run quickest.5 ConclusionThe contribution of this paper is two-fold: First, we have proposed and conducted a comparative extrin-sic evaluation procedure for unsupervised parsers based on noun similarity in DTs.
Second, we haveexplored how to improve DT quality by combining features from several parsers.
The transparency ofthis method with respect to the kind of induced structures (dependencies, constituent trees, combinatorycategorial grammar) and with respect to labels of nodes and edges in the parse graph makes it possible tocompare different unsupervised parsers without having to rely on treebanks.
Since semantic similarity,especially for nouns, is a building block for many NLP applications, we feel that removing the depen-dency on high-quality supervised parsers can give rise to semantic technologies in many languages.
Wehave conducted this evaluation with five different unsupervised parsers, and examined the influence ofcorpus size for parser training and for the similarity computation in a series of experiments.
Using estab-lished methods for evaluating distributional similarity against lexical semantic resources, we were ableto measure differences between parsers in this task that are not reflected by intrinsic evaluations thatcompare their induced structures to treebanks.
These include the influence of corpus size on the trainingprocedure and the consistency of parse fragments on ?frequent versus rare words?
as well as differentlanguages.
Further, we were able to pinpoint a crucial point in unsupervised parsers that has not receivedmuch attention: approaches that do not induce an actual parser that can be run on unseen sentences butmerely produce syntactic annotations for a given fixed training corpus are hardly useful in applications.Our evaluation results can be summarized as follows: For English, with its relatively fixed order,Seginer?s parser achieves very scarce to no improvements compared to a simple n-gram baseline whenused to compute distributional similarities.
But for German, Seginer?s parser outperforms all baselinesincluding a state-of-the-art supervised parser, and S?gaard?s simplistic approach compares favorably tothe n-gram baselines.
Furthermore, we demonstrate that the quality of noun similarity can be improvedsignificantly when combining the features from supervised and unsupervised parsers.While today?s unsupervised parsers might not be ready for their utilization for semantic similarity forthe English language, they can be applied to a large number of other languages where highly optimizedsupervised parsers are not available.
Additionally, we feel that our proposed evaluation method exhibitsenough sensitivity to be a meaningful test bed for future unsupervised parsers.6 OutlookWhere do we go from here?
We strongly argue that in times of availability of very large monolingualcorpora from the web, we should strive at unsupervised parser induction systems that can make use oflarge training data, as opposed to focussing our efforts on complex models that scale poorly, and thuscannot elevate to the performance levels needed in order to make unsupervised parsing a building blockin natural language processing applications.For further work, we want to proceed in several ways: we would like to extend our evaluation frame-work from nouns to other parts of speech.
Furthermore, we will explore whether unsupervised parserscan be tuned towards the task of computing a distributional thesaurus, e.g.
by using only assignmentswith a certain confidence, type, or from sentences with limited length.
Additionally, we would like to ex-plore the interaction of unsupervised POS induction and grammar induction (Headden, III et al., 2008),in order to entirely remove language-dependent preprocessing for the purpose of semantic similaritycomputations, while at the same time being able to leverage the advantages of structured representations,cf.
Erk and Pad?o (2008).
Finally, we would like to test whether we can also detect a different rankingfor different supervised parsers when comparing their scores in the normal treebank setting versus usingthem for building distributional thesauri.1443AcknowledgmentsThis work has been supported by the German Federal Ministry of Education and Research (BMBF)within the context of the Software Campus project LiCoRes under grant No.
01IS12054, by IBM undera Shared University Research Grant and by DFG under the SemSch project grant.ReferencesChris Biemann and Martin Riedl.
2013.
Text: Now in 2D!
A Framework for Lexical Expansion with ContextualSimilarity.
Journal of Language Modelling, 1(1):55?95.Yonatan Bisk and Julia Hockenmaier.
2013.
An HDP Model for Inducing Combinatory Categorial Grammars.
InTransactions of the Association for Computational Linguistics, pages 75?88, Atlanta, GA, USA.Rens Bod.
2007.
Is the end of supervised parsing in sight?
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 400?407, Prague, Czech Republic.Bernd Bohnet.
2010.
Very high accuracy and fast dependency parsing is not a contradiction.
In Proceedings ofthe 23rd International Conference on Computational Linguistics, COLING ?10, pages 89?97, Beijing, China.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x shared task on multilingual dependency parsing.
In Proceedingsof the Tenth Conference on Computational Natural Language Learning, CoNLL-X ?06, pages 149?164, NewYork City, New York.Ekaterina Buyko and Udo Hahn.
2010.
Evaluating the impact of alternative dependency graph encodings on solv-ing event extraction tasks.
In Proceedings of the 2010 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 982?992, Cambridge, Massachusetts.Sander Canisius, Toine Bogers, Antal van den Bosch, Jeroen Geertzen, and Erik Tjong Kim Sang.
2006.
Depen-dency parsing by inference over high-recall dependency predictions.
In Proceedings of the Tenth Conference onComputational Natural Language Learning, CoNLL-X ?06, pages 176?180, New York City, New York.Bart Cramer.
2007.
Limitations of Current Grammar Induction Algorithms.
In Proceedings of the ACL 2007Student Research Workshop, pages 43?48, Prague, Czech Republic.James R. Curran and Marc Moens.
2002.
Improvements in automatic thesaurus extraction.
In Proceedings ofthe ACL-02 workshop on Unsupervised lexical acquisition - Volume 9, ULA ?02, pages 59?66, Philadelphia,Pennsylvania, USA.James R. Curran.
2004.
From Distributional to Semantic Similarity.
Ph.D. thesis, University of Edinburgh.Katrin Erk and Sebastian Pad?o.
2008.
A structured vector space model for word meaning in context.
In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,Honolulu, Hawaii.Jennifer Gillenwater, Kuzman Ganchev, Jo?ao Grac?a, Fernando Pereira, and Ben Taskar.
2010.
Sparsity in depen-dency grammar induction.
In Proceedings of the ACL 2010 Conference Short Papers, pages 194?199, Uppsala,Sweden.Amit Goyal and Hal Daum?e, III.
2011.
Generating semantic orientation lexicon using large data and thesaurus.In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,WASSA ?11, pages 37?43, Portland, Oregon, USA.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet - a Lexical-Semantic Net for German.
In In Proceedings ofACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Appli-cations, pages 9?15, Madrid, Spain.William P. Headden, III, David McClosky, and Eugene Charniak.
2008.
Evaluating unsupervised part-of-speechtagging for grammar induction.
In Proceedings of the 22nd International Conference on Computational Lin-guistics - Volume 1, COLING ?08, pages 329?336, Manchester, United Kingdom.William P Headden III, Mark Johnson, and David McClosky.
2009.
Improving unsupervised dependency parsingwith richer contexts and smoothing.
In Proceedings of Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the Association for Computational Linguistics, pages 101?109,Boulder, CO, USA.1444Richard Johansson and Pierre Nugues.
2008.
The effect of syntactic representation on semantic role labeling.In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ?08,pages 393?400, Manchester, United Kingdom.Dan Klein and Christopher D Manning.
2002.
A generative constituent-context model for improved grammarinduction.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages128?135, Philadelphia, PA,USA.Dan Klein and Christopher D Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the 42nd Annual Meeting on Association for ComputationalLinguistics, pages 478?485, Barcelona, Spain.Dekang Lin.
1997.
Using syntactic dependency as local context to resolve word sense ambiguity.
In Proceedingsof the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of theEuropean Chapter of the Association for Computational Linguistics, ACL ?98, pages 64?71, Madrid, Spain.Dekang Lin.
1998.
Automatic retrieval and clustering of similar words.
In Proceedings of the 17th internationalconference on Computational linguistics - Volume 2, COLING ?98, pages 768?774, Montreal, Quebec, Canada.David Marecek and Milan Straka.
2013.
Stop-probability estimates computed on a large corpus improve Unsu-pervised Dependency Parsing.
In Proceedings of the 51st Annual Meeting of the Association for ComputationalLinguistics, pages 281?290, Sofia, Bulgaria.Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning.
2006.
Generating typed dependencyparses from phrase structure parses.
In Proceedings of the International Conference on Language Resourcesand Evaluation, LREC 2006, pages 449?454, Genova, Italy.George A. Miller.
1995.
WordNet: A Lexical Database for English.
Communications of the ACM, 38:39?41.Yusuke Miyao, Rune Stre, Kenji Sagae, Takuya Matsuzaki, and Jun?ichi Tsujii.
2008.
Task-oriented evaluationof syntactic parsers and their representations.
In Proceeding of the 46th Annual Meeting of the Association forComputational Linguistics: Human Language Technologies, pages 46?54, Columbus, Ohio.Yasaman Motazedi, Mark Dras, and Franc?ois Lareau.
2012.
Is bad structure better than no structure?
: Unsuper-vised parsing for realisation ranking.
In Proceedings of the 24th International Conference on ComputationalLinguistics, COLING ?12, pages 1811?1830, Mumbai,India.Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson.
2010.
Using universal linguistic knowledge toguide grammar induction.
In Proceedings of the 2010 Conference on Empirical Methods in Natural LanguageProcessing, pages 1234?1244, Cambridge, MA, USA.Joakim Nivre and Sandra K?ubler.
2006.
Dependency parsing.
In Tutorial at COLING-ACL, Sydney, Australia.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scale distri-butional similarity and entity set expansion.
In Proceedings of the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 2 - Volume 2, EMNLP ?09, pages 938?947, Singapore.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004.
WordNet::Similarity: measuring the relatednessof concepts.
In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL?Demonstrations ?04, pages 38?41,Boston, Massachusetts, USA.Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd?ottir, and Chris Biemann.
2006.
Exploiting the Leipzig CorporaCollection.
In Proceedings of the IS-LTC 2006, pages 68?73, Ljubljana, Slovenia.Martin Riedl and Chris Biemann.
2013.
Scaling to large3data: An efficient and effective method to computedistributional thesauri.
In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP 2013, pages 884?890, Seattle, WA, USA.Helmut Schmid.
1997.
Probabilistic part-of-speech tagging using decision trees.
In Daniel Jones and HaroldSomers, editors, New Methods in Language Processing, Studies in Computational Linguistics, pages 154?164.UCL Press, London, GB.Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport.
2011.
Neutralizing linguistically problematicannotations in unsupervised dependency parsing evaluation.
In Proceedings of the 49nd Annual Meeting of theAssociation for Computational Linguistics: Human Language Technologies, pages 663?672, Portland, Oregon,USA.1445Yoav Seginer.
2007.
Fast unsupervised incremental parsing.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 384?391, Prague, Czech Republic.Anders S?gaard.
2012.
Unsupervised dependency parsing without training.
Natural Language Engineering,18(02):187?203.Menno van Zaanen and University of Leeds.
School of Computer Studies.
2001.
Building Treebanks Using aGrammar Induction System.
Research report series.
University of Leeds, School of Computer Studies.Julie Weeds, David Weir, and Diana McCarthy.
2004.
Characterising measures of lexical distributional similarity.In Proceedings of the 20th international conference on Computational Linguistics, COLING ?04, pages 1015?1021, Geneva, Switzerland.1446
