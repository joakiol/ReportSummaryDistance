PART-OF-SPEECH TAGGING US INGA VARIABLE  MEMORY MARKOV MODELHinr ich  Sch i i t zeCenter for the Study ofLanguage and Informat ionStanford, CA 94305-4115Internet:  schuetze~csl i .stanford.eduYoram S ingerInst i tute  of Computer  Science andCenter for Neural Computat ionHebrew University, Jerusalem 91904Internet:  singer@cs.huji.ac.i lAbst rac tWe present anew approach to disambiguating syn-tactically ambiguous words in context, based onVariable Memory Markov (VMM) models.
In con-trast to fixed-length Markov models, which predictbased on fixed-length istories, variable memoryMarkov models dynamically adapt their historylength based on the training data, and hence mayuse fewer parameters.
In a test of a VMM basedtagger on the Brown corpus, 95.81% of tokens arecorrectly classified.INTRODUCTIONMany words in English have several parts of speech(POS).
For example "book" is used as a noun in"She read a book."
and as a verb in "She didn'tbook a trip."
Part-of-speech tagging is the prob-lem of determining the syntactic part of speech ofan occurrence of a word in context.
In any givenEnglish text, most tokens are syntactically am-biguous ince most of the high-frequency Englishwords have several parts of speech.
Therefore, acorrect syntactic lassification ofwords in contextis important for most syntactic and other higher-level processing of natural language text.Two stochastic methods have been widelyused for POS tagging: fixed order Markov modelsand Bidden Markov models.
Fixed order Markovmodels are used in (Church, 1989) and (Charniaket al, 1993).
Since the order of the model is as-sumed to be fixed, a short memory (small order) istypically used, since the number of possible combi-nations grows exponentially.
For example, assum-ing there are 184 different ags, as in the Browncorpus, there are 1843 = 6,229,504 different or-der 3 combinations of tags (of course not all ofthese will actually occur, see (Weischedel et al,1993)).
Because of the large number of param-eters higher-order fixed length models are hardto estimate.
(See (Brill, 1993) for a rule-basedapproach to incorporating higher-order informa-tion.)
In a Hidden iarkov Model (HMM) (Jelinek,1985; Kupiec, 1992), a different state is definedfor each POS tag and the transition probabilitiesand the output probabilities are estimated usingthe EM (Dempster et al, 1977) algorithm, whichguarantees convergence to.a local minimum (Wu,1983).
The advantage ofan HMM is that it can betrained using untagged text.
On the other hand,the training procedure is time consuming, and afixed model (topology) is assumed.
Another dis-advantage is due to the local convergence proper-ties of the EM algorithm.
The solution obtaineddepends on the initial setting of the model's pa-rameters, and different solutions are obtained fordifferent parameter initialization schemes.
Thisphenomenon discourages linguistic analysis basedon the output of the model.We present a new method based on vari-able memory Markov models (VMM) (Ron et al,1993; Ron et al, 1994).
The VMM is an approx-imation of an unlimited order Markov source.
Itcan incorporate both the static (order 0) and dy-namic (higher-order) information systematically,while keeping the ability to change the model dueto future observations.
This approach is easy toimplement, he learning algorithm and classifica-tion of new tags are computationally efficient, andthe results achieved, using simplified assumptionsfor the static tag probabilities, are encouraging.VARIABLE MEMORY MARKOVMODELSMarkov models are a natural candidate for lan-guage modeling and temporal pattern recognition,mostly due to their mathematical simplicity.
How-ever, it is obvious that finite memory Markov mod-els cannot capture the recursive nature of lan-guage, nor can they be trained effectively withlong memories.
The notion of variable contez~length also appears naturally in the context of uni-versal coding (Rissanen, 1978; Rissanen and Lang-don, 1981).
This information theoretic notion i snow known to be closely related to efficient mod-eling (Rissanen, 1988).
The natural measure that181appears in information theory is the descriptionlength, as measured by the statistical predictabil-ity via the Kullback-Leibler (KL) divergence.The VMM learning algorithm is based on min-imizing the statistical prediction error of a Markovmodel, measured by the instantaneous KL diver-gence of the following symbols, the current statisti-cal surprise of the model.
The memory is extendedprecisely when such a surprise is significant, untilthe overall statistical prediction of the stochasticmodel is sufficiently good.
For the sake of sim-plicity, a POS tag is termed a symbol and a se-quence of tags is called a string.
We now briefly de-scribe the algorithm for learning a variable mem-ory Markov model.
See (Ron et al, 1993; Ron etal., 1994) for a more detailed description of thealgorithm.We first introduce notational conventions anddefine some basic concepts.
Let \]E be a finite al-phabet.
Denote by \]~* the set of all strings over\]E. A string s, over L TM of length n, is denotedby s = s l s2 .
.
.
sn .
We denote by ?
the emptystring.
The length of a string s is denoted byIsl and the size of an alphabet \]~ is denoted by\[\]~1.
Let Pref ix(s)  = S lS2 .
.
.Sn_ l  denote thelongest prefix of a string s, and let Pref ix*(s)denote the set of all prefixes of s, including theempty string.
Similarly, Suf f ix (s )  = s2sz .
.
.
s ,and Suf f ix*  (s) is the set of all suffixes of s. A setof strings is called a suffix (prefix) free set if, V s ES:  SNSuf f i z* (s  ) = $ (SNPre f i z* (s )  = 0).We call a probability measure P, over the stringsin E* proper if P(o) = 1, and for every string s,Y~,er P(sa) = P(s).
Hence, for every prefix freeset S, ~'~,es P(s) < 1, and specifically for everyinteger n > O, ~'~se~, P(s) = 1.A prediction suffix tree T over \]E, is a treeof degree I~l.
The edges of the tree are labeledby symbols from ~E, such that from every internalnode there is at most one outgoing edge labeledby each symbol.
The nodes of the tree are labeledby pairs (s,%) where s is the string associatedwith the walk starting from that node and end-ing in the root of the tree, and 7s : ~ ---* \[0,1\]is the output probability function of s satisfying)"\]~o~ 7s (a) = 1.
A. prediction suffix, tree.
inducesprobabilities on arbitrarily long strings m the fol-lowing manner.
The probability that T gener-ates a string w = wtw2.
.
.wn  in E~, denoted byPT(w), is IIn=l%.i-,(Wi), where s o = e, and for1 < i < n - 1, s J is the string labeling the deep-est node reached by taking the walk correspondingto wl .
.
.w i  starting at the root of T. By defini-tion, a prediction suffix tree induces a proper mea-sure over E*, and hence for every prefix free setof strings {wX,... ,wm}, ~=~ PT(w i) < 1, andspecifically for n > 1, then ~,E~,  PT(S) = 1.A Probabilistic Finite Automaton (PFA) A isa 5-tuple (Q, E, r, 7, ~), where Q is a finite set ofn states, ~ is an alphabet of size k, v : Q x E --~ Qis the transition function, 7 : Q ?
E ~ \[0,1\] is theoutput probability function, and ~r : Q ~ \[0,1\] isthe probability distribution over the start states.The functions 3' and r must satisfy the followingrequirements: for every q E Q, )-'~oe~ 7(q, a) =1, and ~e~O rr(q) = 1.
The probability thatA generates  a str ing s = s l s2 .
.
.
s .
E En0 n is PA(s) = ~-~qoEq lr(q ) I-Ii=x 7(q i-1, sl), whereq i+ l  ~_ r(qi,si).
7" can be extended to be de-fined on Q x E* as follows: 7"(q, s ts2.
.
.s t )  =7"(7"(q, s t .
.
.
s t -x ) , s t )  = 7"(7"(q, Pref iz(s)) ,st) .The distribution over the states, 7r, can be re-placed by a single start state, denoted by e suchthat r(?, s) = 7r(q), where s is the label of the stateq.
Therefore, r(e) = 1 and r(q) = 0 if q # e.For POS tagging, we are interested in learninga sub-class of finite state machines which have thefollowing property.
Each state in a machine Mbelonging to this sub-class is labeled by a stringof length at most L over E, for some L _> O. Theset of strings labeling the states is suffix free.
Werequire that for every two states qX, q2 E Q andfor every symbol a E ~, if r(q 1,or) = q2 and qtis labeled by a string s 1, then q2 is labeled bya string s ~ which is a suffix of s 1 ?
or.
Since theset of strings labeling the states is suffix free, ifthere exists a string having this property then itis unique.
Thus, in order that r be well defined ona given set of string S, not only must the set besuffix free, but it must also have the property, thatfor every string s in the set and every symbol a,there exists a string which is a suffix of scr.
For ourconvenience, from this point on, if q is a state inQ then q will also denote the string labeling thatstate.A special case of these automata is the casein which Q includes all I~l L strings of length L.These automata re known as Markov processes oforder L. We are interested in learning automatafor which the number of states, n, is much smallerthan IEI L, which means that few states have longmemory and most states have a short one.
We re-fer to these automata s variable memory Markov(VMM) processes.
In the case of Markov processesof order L, the identity of the states (i.e.
the iden-tity of the strings labeling the states) is known andlearning such a process reduces to approximatingthe output probability function.Given a sample consisting of m POS tag se-quences of lengths Ix,12,..., l,~ we would like tofind a prediction suffix tree that will have thesame statistical properties as the sample and thuscan be used to predict the next outcome for se-c;uences generated by the same source.
At each182stage we can transform the tree into a variablememory Markov process.
The key idea is to iter-atively build a prediction tree whose probabilitymeasure quals the empirical probability measurecalculated from the sample.We start with a tree consisting of a singlenode and add nodes which we have reason to be-lieve should be in the tree.
A node as, must beadded to the tree if it statistically differs from itsparent node s. A natural measure to check thestatistical difference is the relative ntropy (alsoknown as the Kullback-Leibler (KL) divergence)(Kullback, 1959), between the conditional proba-bilities P(.Is) and P(.las).
Let X be an obser-vation space and P1, P2 be probability measuresover X then the KL divergence between P1 andP1 x P2 is, D L(PIlIP )= ?
Inour case, the KL divergence measures how muchadditional information is gained by using the suf-fix ~rs for prediction instead of the shorter suffix s.There are cases where the statistical difference islarge yet the probability of observing the suffix asitself is so small that we can neglect those cases.Hence we weigh the statistical error by the priorprobability of observing as.
The statistical errormeasure in our case is,Err(as, s)= P(crs)DgL (P(.las)llP(.ls))= P(as)  P(a' las) log: ~,0,~ P(asa')log p(P/s?
;p'()Therefore, a node as is added to the tree if the sta-tistical difference (defined by Err(as, s)) betweenthe node and its parrent s is larger than a prede-termined accuracy e. The tree is grown level bylevel, adding a son of a given leaf in the tree when-ever the statistical error is large.
The problem isthat the requirement that a node statistically dif-fers from its parent node is a necessary conditionfor belonging to the tree, but is not sufficient.
Theleaves of a prediction suffix tree must differ fromtheir parents (or they are redundant) but internalnodes might not have this property.
Therefore,we must continue testing further potential descen-dants of the leaves in the tree up to depth L. Inorder to avoid exponential grow in the number ofstrings tested, we do not test strings which belongto branches which are reached with small prob-ability.
The set of strings, tested at each step,is denoted by S, and can be viewed as a kind offrontier of the growing tree T.US ING A VMM FOR POSTAGGINGWe used a tagged corpus to train a VMM.
Thesyntactic information, i.e.
the probability of a spe-183cific word belonging to a tag class, was estimatedusing maximum likelihood estimation from the in-dividual word counts.
The states and the transi-tion probabilities of the Markov model were de-termined by the learning algorithm and tag out-put probabilities were estimated from word counts(the static information present in the training cor-pus).
The whole structure, for two states, is de-picted in Fig.
1.
Si and Si+l are strings of tags cor-responding to states of the automaton.
P(ti\[Si)is the probability that tag ti will be output bystate Si and P(ti+l\]Si+l) is the probability thatthe next tag ti+l is the output of state Si+l.P(Si+llSi)V 7P(TilSi) P(Ti+IlSi+I)Figure 1: The structure of the VMM based POStagger.When tagging a sequence of words Wl,,, wewant to find the tag sequence tl,n that is mostlikely for Wl,n.
We can maximize the joint proba-bility of wl,, and tl,n to find this sequence: 1T(Wl,n) = arg maxt,, P(tl,nlWl,n)P(t,..,~,,.)
= arg maxt~,.
P(wl,.
)= arg maxt~,.P(tl,.,wl,.
)P(tl,., Wl,.)
can be expressed as a product of con-ditional probabilities as follows:P(t l , .
,  Wl,.)
=P(ts)P(wl Itl)P(t~ltl, wl)e(w21tl,2, wl)... P(t. It 1,._ 1, Wl,.-1)P(w.
It1,., w l , .
-  1)= f i  P(tiltl,i-1, wl,i-1)P(wiltl,i, Wl,/-1)i=1With the simplifying assumption that the proba-bility of a tag only depends on previous tags andthat the probability of a word only depends on itstags, we get:P(tl,n, wl,.)
= fix P(tiltl,i-1) P(wilti)i=1Given a variable memory Markov model M,P(tilQ,i-1) is estimated by P(tilSi-l,M) where1 Part of the following derivation is adapted from(Charniak et al, 1993).Si = r(e, tx,i), since the dynamics of the sequenceare represented by the transition probabilities ofthe corresponding automaton.
The tags tl,n fora sequence of words wt,n are therefore chosen ac-cording to the following equation using the Viterbialgorithm:t%7-M(Wl,n) -- arg maxq.. H P(t i lS i - l '  M)P(wilt i)i=1We estimate P(wilti) indirectly from P(tilwi) us-ing Bayes' Theorem:P(wilti) = P(wi)P(ti lwi)P(ti)The terms P(wi) are constant for a given sequencewi and can therefore be omitted from the maxi-mization.
We perform a maximum likelihood es-timation for P(ti) by calculating the relative fre-quency of ti in the training corpus.
The estima-tion of the static parameters P(tilwi) is describedin the next section.We trained the variable memory Markovmodel on the Brown corpus (Francis and Ku~era,1982), with every tenth sentence removed (a totalof 1,022,462 tags).
The four stylistic tag modifiers"FW" (foreign word), "TL" (title), "NC" (citedword), and "HL" (headline) were ignored reduc-ing the complete set of 471 tags to 184 differenttags.The resulting automaton has 49 states: thenull state (e), 43 first order states (one symbollong) and 5 second order states (two symbolslong).
This means that 184-43=141 states werenot (statistically) different enough to be includedas separate states in the automaton.
An analy-sis reveals two possible reasons.
Frequent symbolssuch as "ABN" ("half", "all", "many" used as pre-quantifiers, e.g.
in "many a younger man") and"DTI" (determiners that can be singular or plu-ral, "any" and "some") were not included becausethey occur in a variety of diverse contexts or oftenprecede unambiguous words.
For example, whentagged as "ABN .... half", "all", and "many" tendto occur before the unambiguous determiners "a","an" and "the".Some rare tags were not included because theydid not improve the optimization criterion, min-imum description length (measured by the KL-divergence).
For example, "HVZ*" ("hasn't") isnot a state although a following "- ed" form is al-ways disambiguated as belonging to class "VBN"(past participle).
But since this is a rare event, de-scribing all "HVZ* VBN" sequences separately ischeaper than the added complexity of an automa-ton with state "HVZ*".
We in fact lost some ac-curacy in tagging because of the optimization cri-terion: Several "-ed" forms after forms of "have"were mistagged as "VBD" (past tense).transition to one-symbol two-symbolstate stateNN JJ: 0.45 AT JJ: 0.69IN JJ: 0.06 AT JJ: 0.004IN NN: 0.27 AT NN: 0.35NN: 0.14 AT NN: 0.10NNINNNJJVBVBNVBN: 0.08 AT VBN: 0.48VBN: 0.35 AT VBN: 0.003CC: 0.12 JJ CC: 0.04CC: 0.09 JJ CC: 0.58RB: 0.05 MD RB: 0.48RB: 0.08 MD RB: 0.0009Table 1: States for which the statistical predic-tion is significantly different when using a longersuffix for prediction.
Those states are identifiedautomatically b the VMM learning algorithm.
Abetter prediction and classification ofPOS-tags isachieved by adding those states with only a smallincrease in the computation time.The two-symbol states were "AT JJ", "ATNN", "AT VBN", "JJ CC", and "MD RB" (ar-ticle adjective, article noun, article past partici-ple, adjective conjunction, modal adverb).
Ta-ble 1 lists two of the largest differences in transi-tion probabilities for each state.
The varying tran-sition probabilities are based on differences be-tween the syntactic onstructions in which the twocompeting states occur.
For example, adjectivesafter articles ("AT JJ") are almost always usedattributively which makes a following prepositionimpossible and a following noun highly probable,whereas a predicative use favors modifying prepo-sitional phrases.
Similarly, an adverb preceded bya modal ("MD RB") is followed by an infinitive("VB") half the time, whereas other adverbs oc-cur less often in pre-infinitival position.
On theother hand, a past participle is virtually impossi-ble after "MD RB" whereas adverbs that are notpreceded by modals modify past participles quiteoften.While it is known that Markov models of order2 give a slight improvement over order-1 models(Charniak et al, 1993), the number of parametersin our model is much smaller than in a full order-2Markov model (49"184 = 9016 vs. 184"184"184 --6,229,504).EST IMATION OF THE STATICPARAMETERSWe have to estimate the conditional probabilitiesP(ti\[wJ), the probability that a given word ufi willappear with tag t i, in order to compute the staticparameters P(w j It/) used in the tagging equationsdescribed above.
A first approximation would be184to use the maximum likelihood estimator:p(ti\[w j) = C( ti, w i)c(w )where C(t i, w j) is the number of times ti is taggedas w~ in the training text and C(wJ) is the num-ber of times w/ occurs in the training text.
How-ever, some form of smoothing is necessary, sinceany new text will contain new words, for whichC(w j) is zero.
Also, words that are rare will onlyoccur with some of their possible parts of speechin the training text.
One solution to this problemis Good-Turing estimation:p(tilwj) _ C(t', wJ) + 1c(wJ) + Iwhere I is the number of tags, 184 in our case.It turns out that Good-Turing is not appropri-ate for our problem.
The reason is the distinctionbetween closed-class and open-class words.
Somesyntactic lasses like verbs and nouns are produc-tive, others like articles are not.
As a consequence,the probability that a new word is an article iszero, whereas it is high for verbs and nouns.
Weneed a smoothing scheme that takes this fact intoaccount.Extending an idea in (Charniak et al, 1993),we estimate the probability of tag conversion tofind an adequate smoothing scheme.
Open andclosed classes differ in that words often add a tagfrom an open class, but rarely from a closed class.For example, a word that is first used as a nounwill often be used as a verb subsequently, butclosed classes uch as possessive pronouns ("my","her", "his") are rarely used with new syntacticcategories after the first few thousand words of theBrown corpus.
We only have to take stock of these"tag conversions" to make informed predictions onnew tags when confronted with unseen text.
For-mally, let W\] ' '~ be the set of words that have beenseen with t i, but not with t k in the training text upto word wt.
Then we can estimate the probabilitythat a word with tag t i will later be seen with tagt ~ as the proportion of words allowing tag t i butnot t k that later add tk:P~m(i --* k) =I{n l l<n<m ^ i ~k , ~k wnEW I" OW,,- t ^t~=t~}liw~'.-klThis formula also applies to words we haven't seenso far, if we regard such words as having occurredwith a special tag "U" for "unseen".
(In this case,W~ '-'k is the set of words that haven't occurred upto l.) PI,n(U ---* k) then estimates the probabilitythat an unseen word has tag t k. Table 2 showsthe estimates of tag conversion we derived fromour training text for 1 = 1022462- 100000, m =1022462, where 1022462 is the number of words inthe training text.
To avoid sparse data problemswe assumed zero probability for types of tag con-version with less than 100 instances in the trainingset.tag conversionU --* NNU~J JU --~ NNSU --* NPU ~ VBDU ~ VBGU --~ VBNU --~ VBU---, RBU ~ VBZU --* NP$VBD -~ VBNVBN --* VBDVB --* NNNN ~ VBestimated probability0.290.130.120.080.070.070.060.050.050.010.010.090.050.050.01Table 2: Estimates for tag conversionOur smoothing scheme is then the followingheuristic modification of Good-Turing:C(t i, W j) -k ~k,ETi Rim(k1 --+ i)g(ti lwi) = C(wi) + Ek,ETi,k2E T Pam(kz --" ks)where Tj is the set of tags that w/has  in the train-ing set and T is the set of all tags.
This schemehas the following desirable properties:?
As with Good-Turing, smoothing has a small ef-fect on estimates that are based on large counts.?
The difference between closed-class and open-class words is respected: The probability forconversion to a closed class is zero and is notaffected by smoothing.?
Prior knowledge about the probabilities of con-version to different tag classes is incorporated.For example, an unseen word w i is five times aslikely to be a noun than an adverb.
Our esti-mate for P(ti\]w j) is correspondingly five timeshigher for "NN" than for "RB".ANALYS IS  OF  RESULTSOur result on the test set of 114392 words (thetenth of the Brown corpus not used for training)was 95.81%.
Table 3 shows the 20 most frequenterrors.Three typical examples for the most commonerror (tagging nouns as adjectives) are "Commu-nist", "public" and "homerun" in the followingsentences.185VMM:correct :NNVBDNNSVBNJJVB"'CS'NPINVBGRBQL\]1  JIVBNI NIVB?I INI ?sI259 102110632271651421949421911263103RPIQLI B1007176Table 3: Most common errors.VB I VBG69 66* the Cuban fiasco and the Communist militaryvictories in Laos?
to increase public awareness of the movement?
the best homerun hitterThe words "public" and "communist" can be usedas adjectives or nouns.
Since in the above sen-tences an adjective is syntactically more likely,this was the tagging chosen by the VMM.
Thenoun "homerun" didn't occur in the training set,therefore the priors for unknown words biased thetagging towards adjectives, again because the po-sition is more typical of an adjective than of anoun.Two examples of the second most common er-ror (tagging past tense forms ("VBD") as pastparticiples ("VBN")) are "called" and "elected"in the following sentences:?
the party called for government operation of allutilities?
When I come back here after the November elec-tion you'll think, you're my man - elected.Most of the VBD/VBN errors were caused bywords that have a higher prior for "VBN" so thatin a situation in which both forms are possible ac-cording to local syntactic ontext, "VBN" is cho-sen. More global syntactic context is necessaryto find the right tag "VBD" in the first sentence.The second sentence is an example for one of thetagging mistakes in the Brown corpus, "elected"is clearly used as a past participle, not as a pasttense form.Compar i son  w i th  o ther  Resu l tsCharniak et al's result of 95.97% (Charniak et al,1993) is slightly better than ours.
This differenceis probably due to the omission of rare tags thatpermit reliable prediction of the following tag (thecase of "HVZ."
for "hasn't").Kupiec achieves up to 96.36% correctness(Kupiec, 1992), without using a tagged corpus fortraining as we do.
But the results are not eas-ily comparable with ours since a lexicon is usedthat lists only possible tags.
This can result in in-creasing the error rate when tags are listed in thelexicon that do not occur in the corpus.
But it canalso decrease the error rate when errors due to badtags for rare words are avoided by looking them upin the lexicon.
Our error rate on words that do notoccur in the training text is 57%, since only thegeneral priors are used for these words in decod-ing.
This error rate could probably be reducedsubstantially by incorporating outside lexical in-formation.D ISCUSSIONWhile the learning algorithm of a VMM is efficientand the resulting tagging algorithm is very simple,the accuracy achieved is rather moderate.
This isdue to several reasons.
As mentioned in the intro-ductory sections, any finite memory Markov modelcannot capture the recursive nature of natural an-guage.
The VMM can accommodate longer sta-tistical dependencies than a traditional full-orderMarkov model, but due to its Markovian naturelong-distance statistical correlations are neglected.Therefore, a VMM based tagger can be used forpruning many of the tagging alternatives using itsprediction probability, but not as a complete tag-ging system.
Furthermore, the VMM power canbe better utilized in low level language process-ing tasks such as cleaning up corrupted text asdemonstrated in (Ron et al, 1993).We currently investigate other stochasticmodels that can accommodate long distance sta-tistical correlation (see (Singer and Tishby, 1994)for preliminary results).
However, there are theo-retical clues that those models are much harder tolearn (Kearns et al, 1993), including HMM basedmodels (Abe and Warmuth, 1992).186Another drawback of the current taggingscheme is the independence assumption of the un-derlying tags and the observed words, and the ad-hoc estimation of the static probabilities.
We arepursuing a systematic scheme to estimate thoseprobabilities based on Bayesian statistics, by as-signing a discrete probability distribution, such asthe Dirichlet distribution (Berger, 1985), to eachtag class.
The a-posteriori probability estimationof the individual words can be estimated from theword counts and the tag class priors.
Those priorscan be modeled as a mixture of Dirichlet distribu-tions (Antoniak, 1974), where each mixture com-ponent would correspond to a different ag class.Currently we estimate the state transition prob-abilities from the conditional counts assuming auniform prior.
The same technique can be used toestimate those parameters as well.ACKNOWLEDGMENTPart of this work was done while the second au-thor was visiting the Department of Computerand Information Sciences, University of California,Santa-Cruz, supported by NSF grant IRI-9123692.We would like to thank Jan Pedersen and Naf-tali Tishby for helpful suggestions and discussionsof this material.
Yoram Singer would like to thankthe Charles Clore foundation for supporting thisresearch.
We express our appreciation to facultyand students for the stimulating atmosphere atthe 1993 Connectionist Models Summer School atwhich the idea for this paper took shape.Re ferencesN.
Abe and M. Warmuth, On the computationalcomplexity of approximating distributionsbyprobabilistic automata, Machine Learning,Vol.
9, pp.
205-260, 1992.C.
Antoniak, Mixture of Dirichlet processes withapplications to Bayesian nonparametric prob-lems, Annals of Statistics, Vol.
2, pp.
1152-174, 1974.J.
Berger, Statistical decision theory and Bayesiananalysis, New-York: Springer-Verlag, 1985.E.
Brill.
Automatic grammar induction and pars-ing free text: A transformation-based ap-proach.
In Proceedings of ACL 31, pp.
259-265, 1993.E.
Charniak, Curtis Hendrickson, Neil Jacobson,and Mike Perkowitz, Equations for Part-of-Speech Tagging, Proceedings of the EleventhNational Conference on Artificial Intelligence,pp.
784-789, 1993.K.W.
Church, A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text,Proceedings of ICASSP, 1989.A.
Dempster, N. Laird, and D. Rubin, MaximumLikelihood estimation from Incomplete Datavia the EM algorithm, J. Roy.
Statist.
Soc.,Vol.
39(B), pp.
1-38, 1977.W.N.
Francis and F. Ku~era, Frequency Analysisof English Usage, Houghton Mifflin, BostonMA, 1982.F.
Jelinek, Robust part-of-speech tagging usinga hidden Markov model, IBM Tech.
Report,1985.M.
Kearns, Y. Mansour, D. Ron, R. Rubinfeld,R.
Schapire, L. Sellie, On the Learnability ofDiscrete Distributions, The 25th Annual ACMSymposium on Theory of Computing, 1994.S.
Kullback, Information Theory and Statistics,New-York: Wiley, 1959.J.
Kupiec, Robust part-of-speech tagging using ahidden Markov model, Computer Speech andLanguage, Vol.
6, pp.
225-242, 1992.L.R.
Rabiner and B. H. Juang, An Introductionto Hidden Markov Models, IEEE ASSP Mag-azine, Vol.
3, No.
1, pp.
4-16, 1986.J.
Rissanen, Modeling by shortest data discription,Automatica, Vol.
14, pp.
465-471, 1978.J.
Rissanen, Stochastic omplexity and modeling,The Annals of Statistics, Vol.
14, No.
3, pp.1080-1100, 1986.J.
Rissanen and G. G. Langdon, Universal model-ing and coding, IEEE Trans.
on Info.
Theory,IT-27, No.
3, pp.
12-23, 1981.D.
Ron, Y.
Singer, and N. Tishby, The powerof Amnesia, Advances in Neural InformationProcessing Systems 6, 1993.D.
Ron, Y.
Singer, and N. Tishby, LearningProbabilistic Automata with Variable MemoryLength, Proceedings of the 1994 Workshop onComputational Learning Theory, 1994.Y.
Singer and N. Tishby, Inferring Probabilis-tic Acyclic Automata Using the MinimumDescription Length Principle, Proceedings ofIEEE Intl.
Symp.
on Info.
Theory, 1994.R.
Weischedel, M. Meteer, R. Schwartz, L.Ramshaw, and :I. Palmucci.
Coping with am-biguity and unknown words through prob-abilistic models.
Computational Linguistics,19(2):359-382, 1993.J.
Wu, On the convergence properties of the EMalgorithm, Annals of Statistics, Vol.
11, pp.95-103, 1983.187
