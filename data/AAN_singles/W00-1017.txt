WIT: A Toolkit for Building Robust and Real-Time Spoken DialogueSystemsMikio Nakano* Noboru Miyazaki, Norihito Yasuda, Akira Sugiyama,Jun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki AikawaNTT Corporation3-1 Morinosato-WakamiyaAtsugi, Kanagawa 243-0198, JapanE-mail: nakano@atom.brl.ntt.co.jpAbstractThis paper describes WI'I; a toolkitfor building spoken dialogue systems.WIT features an incremental under-standing mechanism that enables ro-bust utterance understanding and real-time responses.
WIT's ability to com-pile domain-dependent system specifi-cations into internal knowledge sourcesmakes building spoken dialogue sys-tems much easier than :it is fromscratch.1 IntroductionThe recent great advances in speech and languagetechnologies have made it possible to build fullyimplemented spoken dialogue systems (Aust etal., 1995; Allen et al, 1996; Zue et al, 2000;Walker et al, 2000).
One of the next researchgoals is to make these systems task-portable, thatis, to simplify the process of porting to anothertask domain.To this end, several toolkits for building spo-ken dialogue systems have been developed (Bar-nett and Singh, 1997; Sasajima et al, 1999).One is the CSLU Toolkit (Sutton et al, 1998),which enables rapid prototyping of a spoken di-alogue system that incorporates a finite-state dia-logue model.
It decreases the amount of the ef-fort required in building a spoken dialogue sys-tem in a user-defined task domain.
However, itlimits system functions; it is not easy to employthe advanced language processing techniques de-veloped in the realm of computational linguis-tics.
Another is GALAXY-II (Seneffet al, 1998),*Mikio Nakano is currently a visiting scientist at MITLaboratory for Computer Science.which enables modules in a dialogue system tocommunicate with each other.
It consists of thehub and several servers, such as the speech recog-nition server and the natural language server, andthe hub communicates with these servers.
Al-though it requires more specifications than finite-state-model-based toolkits, it places less limita-tions on system functions.Our objective is to build robust and real-timespoken dialogue systems in different ask do-mains.
By robust we mean utterance understand-ing is robust enough to capture not only utter-ances including rammatical errors or self-repairsbut also utterances that are not clearly segmentedinto sentences by pauses.
Real time means thesystem can respond to the user in real time.
Thereason we focus on these features i  that they arecrucial to the usability of spoken dialogue sys-tems as well as to the accuracy of understand-ing and appropriateness of the content of the sys-tem utterance.
Robust understanding allows theuser to speak to the system in an unrestrictedway.
Responding in real time is important be-cause if a system response is delayed, the usermight think that his/her utterance was not recog-nized by the system and make another utterance,making the dialogue disorderly.
Systems havingthese features hould have several modules thatwork in parallel, and each module needs somedomain-dependent k owledge sources.
Creat-ing and maintaining these knowledge sources re-quire much effort, thus a toolkit would be help-ful.
Previous toolkits, however, do not allow us toachieve these features, or do not provide mecha-nisms that achieve these features without requir-ing excessive fforts by the developers.This paper presents WIT 1, which is a toolkitIWIT is an acronym of Workable spoken dialogue lnter-150for building spoken dialogue systems that inte-grate speech recognition, language understandingand generation, and speech output.
WIT featuresan incremental understanding method (Nakano etal., 1999b) that makes it possible to build a robustand real-time system.
In addition, WIT compilesdomain-dependent system specifications into in-ternal knowledge sources o that building systemsis easier.
Although WIT requires more domain-dependent specifications than finite-state-model-based toolkits, WIT-based systems are capableof taking full advantage of language processingtechnology.
WIT has been implemented and usedto build several spoken dialogue systems.In what follows, we overview WIT, explain itsarchitecture, domain-dependent system specifica-tions, and implementation, and then discuss itsadvantages and problems.2 OverviewA WIT-based spoken dialogue system has fourmain modules: the speech recognition module,the language understanding module, the lan-guage generation module, and the speech out-put module.
These modules exploit domain-dependent knowledge sources, which are auto-matically generated from the domain-dependentsystem specifications.
The relationship amongthe modules, knowledge sources, and specifica-tions are depicted in Figure 1.WIT can also display and move a human-face-like animated agent, which is controlled by thespeech output module, although this paper doesnot go into details because it focuses only on spo-ken dialogue.
We also omit the GUI facilities pro-vided by WIT.3 Architecture of  WIT-Based SpokenDialogue SystemsHere we explain how the modules in WIT workby exploiting domain-dependent k owledge andhow they interact with each other.3.1 Speech RecognitionThe speech recognition module is a phoneme-HMM-based speaker-independent continuousspeech recognizer that incrementally outputsface Toolldt.word hypotheses.
As the recogn/fion engine,either VoiceRex, developed by NTI" (Noda etal., 1998), or HTK from Entropic Research canbe used.
Acoustic models for HTK is trainedwith the continuous peech database of theAcoustical Society of Japan (Kobayashi et al,1992).
This recognizer incrementally outputsword hypotheses a soon as they are found in thebest-scored path in the forward search (Hirasawaet al, 1998) using the ISTAR (IncrementalStructure Transmitter And Receiver) protocol,which conveys word graph information as well asword hypotheses.
This incremental output allowsthe language understanding module to processrecognition results before the speech intervalends, and thus real-time responses are possible.This module continuously runs and outputsrecognition results when it detects a speechinterval.
This enables the language generationmodule to react immediately touser interruptionswhile the system is speaking.The language model for speech recognitionis a network (regular) grammar, and it allowseach speech interval to be an arbitrary numberof phrases.
A phrase is a sequence of words,which is to be defined in a domain-dependentway.
Sentences can be decomposed into a cou-ple of phrases.
The reason we use a repeti-tion of phrases instead of a sentence grammarfor the language model is that the speech recog-nition module of a robust spoken dialogue sys-tem sometimes has to recognize spontaneouslyspoken utterances, which include self-repairs andrepetition.
In Japanese, bunsetsu is appropriatefor defining phrases.
A bunsetsu consists of onecontent word and a number (possibly zero) offunction words.
In the meeting room reservationsystem we have developed, examples of definedphrases are bunsetsu to specify the room to be re-served and the time of the reservation and bun-setsu to express affirmation and negation.When the speech recognition module finds aphrase boundary, it sends the category of thephrase to the language understanding module,and this information is used in the parsing pro-cess.It is possible to hold multiple language mod-els and use any one of them when recogniz-ing a speech interval.
The language models are151Semantic \[I ~e  I/ specifications \[ r 1I R~ae I /L..___..--  - ' - ' -~ ' -  / Ph~e l/de~;,i~ions I /" "-.
I de~i*~._lI Feature I L._.___..~.. -~-'-'-'~ , ( "~ "'.L____i----',.,'-...
~ ~ "-..?
Surface- I de .~ons  \[:- ,, l ;~ : : - - ?Y \ l  Language II Language I ~Genera f io~n_ /  .
I , ~ ,  "., "~"----~__Z_--.--" M .
-.
i i . )
i .
.
.
.
.
.
.
l,,_J generaraon I ~ \  ,, ~ ~ unaerstanding I I generation IO  t procedures I TM / .
.
.
.
~ .
.
.
.
I\ I I I .
.
.
.
JI definitions I '\ \ I word I strings I hypothesea +I .
- -  I _ __~ '~seto f - " -L__~I  Speech i I~ ,~ L iT -s t  o f - - / L / i .
, i s to fI ~"  t I ~angu.~ge I- -I ~t io .
I I ou~u, r ' - - ' \ ]  pre-r~o.dedr'--\] pre-r~o~ed II d~f~i~23~_l t.models .....~1 I module I I ~oam~ I~Peech m~._J , I L_  juser utterance system utterancedomain-dependent:specification knowledge source moduleFigure 1: Architecture of WITswitched according to the requests from the lan-guage understanding module.
In this way, thespeech recognition success rate is increased byusing the context of the dialogue.Although the current version of WIT does notexploit probabilistic language models, such mod-els can be incorporated without changing the ba-sic WIT architecture.3.2 Language UnderstandingThe language understanding :module receivesword hypotheses from the speech recognitionmodule and incrementally understands the se-quence of the word hypotheses to update the di-alogue state, in which the resnlt of understand-ing and discourse information are representedby a frame (i.e., attribute-value pairs).
The un-derstanding module utilizes ISSS (IncrementalSignificant-utterance Sequence Search) (Nakanoet al, 1999b), which is an integrated parsing anddiscourse processing method.
ISSS enables theincremental understanding of user utterances thatare not segmented into sentences prior to pars-ing by incrementally finding the most plausiblesequence of sentences (or significant utterancesin the ISSS terms) out of the possible sentencesequences for the input word sequence.
ISSSalso makes it possible for the language generationmodule to respond in real time because it can out-put a partial result of understanding at any pointin time.The domain-dependent knowledge used in thismodule consists of a unification-based lexiconand phrase structure rules.
Disjunctive featuredescriptions are also possible; WIT incorporatesan efficient method for handling disjunctions(Nakano, 1991).
When a phrase boundary is de-tected, the feature structure for a phrase is com-puted using some built-in rules from the featurestructure rules for the words in the phrase.
Thephrase structure rules specify what kind of phrasesequences can be considered as sentences, andthey also enable computing the semantic repre-sentation for found sentences.
Two kinds of sen-tenees can be considered; domain-related onesthat express the user's intention about he reser-152vafion and dialogue-related ones that express theuser's attitude with respect to the progress of thedialogue, such as confirmation and denial.
Con-sidering the meeting room reservation system, ex-amples of domain-related sentences are "I need tobook Room 2 on Wednesday", I need to bookRoom 2", and "Room 2" and dialogue-relatedones are "yes", "no", and "Okay".The semantic representation for a sentence isa command for updatingthe dialogue state.
Thedialogue state is represented bya list of attribute-value pairs.
For example, attributes used in themeeting room reservation system include task-related attributes, such as the date and time ofthe reservation, as well as attributes that representdiscourse-related information, such as confirma-tion and grounding.3.3 Language GenerationHow the language generation module worksvaries depending on whether the user or systemhas the initiative of turn taking in the dialogue 2.Precisely speaking, the participant having the ini-tiative is the one the system assumes has it in thedialogue.The domain-dependent k owledge used by thelanguage generation module is generation proce-dures, which consist of a set of dialogue-phasedefinitions.
For each dialogue phase, an initialfunction, an action function, a time-out function,and a language model are assigned.
In addition,phase definitions designate whether the user orthe system has the initiative.
In the phases inwhich the system has the initiative, only the ini-tial function and the language model are assigned.The meeting room reservation system, for exam-ple, has three phases: the phase in which theuser tells the system his/her equest, he phase inwhich the system confirms it, and the phase inwhich the system tells the user the result of thedatabase access.
In the first two phases, the userholds the initiative, and in the last phase, the sys-tern holds the initiative.Functions defined here decide what stringshould be spoken and send that string to thespeech output module based on the current di-alogue state.
They can also shift the dialogue2The notion of the initiative inthis paper isdifferent fromthat of the dialogue initiative of Chu-Carroll (2000).phase and change the holder of the initiative aswell as change the dialogue state.
When the dia-logue phase shifts, the language model foi" speechrecognition is changed to get better speech recog-nition performance.
Typically, the language gen-eration module is responsible for database access.The language generation module works as fol-lows.
It first checks which dialogue participanthas the initiative.
If the initiative is held by theuser, it waits until the user's speech interval endsor a duration of silence after the end of a systemutterance is detected.
The action function in thedialogue phase at that point in time is executed inthe former case; the time-out function is executedin the latter case.
Then it goes back to the initialstage.
If the system holds the initiative, the mod-ule executes the initial function of the phase.
Intypical question-answer systems, the user has theinitiative when asking questions and the systemhas it when answering.Since the language generation module works inparallel with the language understanding module,utterance generation is possible even while thesystem is listening to user utterances and that ut-terance understanding is possible even while it isspeaking (Nakano et al, 1999a).
Thus the systemcan respond immediately after user pauses whenthe user has the initiative.
When the system holdsthe initiative, it can immediately react to an in-terruption by the user because user utterances areunderstood in an incremental way (Dohsaka ndShimazu, 1997).The time-out function is effective in movingthe dialogue forward when the dialogue getsstuck for some reason.
For example, the systemmay be able to repeat the same question with an-other expression and may also be able to ask theuser a more specific question.3.4 Speech OutputThe speech output module produces peech ac-cording to the requests from the language gener-ation module by using the correspondence tablebetween strings and pre-recorded speech data.
Italso notifies the language generation module thatspeech output has finished so that the languagegeneration module can take into account the tim-ing of the end of system utterance.
The meetingroom reservation system uses speech files of short153phrases.4 Building Spoken Dialo~te Systemswith WIT4.1 Domain-Dependent SystemSpecificationsSpoken dialogue systems can be built with WITby preparing several domain-dependent specifica-tions.
Below we explain the specifications.Feature Definitions: Feature definitions pec-ify the set of features used in the grammar for lan-guage understanding.
They also specify whethereach feature is a head feature or a foot feature(Pollard and Sag, 1994).
This information isusedwhen constructing feature structures for phrasesin a built-in process.The following is an example of a feature defini-tion.
Here we use examples from the specificationof the meeting room reservation system.
(case head)It means that the case feature is used and it is ahead feature 3.Lexieal Descriptions: Lexical descriptionsspecify both pronunciations and grammaticalfeatures for words.
Below is an example lexicalitem for the word 1-gatsu (January).
(l-gatsu ichigatsu month nil i)The first three elements are the identifier, the pro-nunciation, and the grammatical category of theword.
The remaining two elements are the caseand semantic feature values.Phrase Definitions: Phrase definitions pecifywhat kind of word sequence can be recognizedas a phrase.
Each definition is a pair compris-ing a phrase category name and a network ofword categories.
In the example below, month-phrase is the phrase category name and the re-maining part is the network of word categories.opt  means an option and or  means a disjunc-tion.
For instance, a word sequence that con-sists of a word in the month  category, such as 1-gatsu (January), and a word in the adraon ina l -par t i c le  category, such as no (of), forms aphrase in the month-phrase  category.3In this section, we use examples of different descriptionfrom the actual ones for simplicity.
Actual specifications arewritten in part in Japanese.
(month-phrase(month(opt(orexpression-following-subject(admoninal-particle(optsentence-final-particle))))))Network Definitions: Network definitionsspecify what kind of phrases can be included ineach language model.
Each definition is a paircomprising a network name and a set of phrasecategory names.Semantic-Frame Specifications: The result ofunderstanding and dialogue history can be storedin the dialogue state, which is represented by aflat frame structure, i.e., a set of attribute-valuepairs.
Semantic-frame specifications define theattributes used in the frame.
The meeting roomreservation system uses task-related attributes.Two are s tar t  and end, which represent theuser's intention about the start and end times ofthe reservation for some meeting room.
It alsohas attributes that represent discourse informa-tion.
One is conf i rmed,  whose value indicateswhether if the system has already made an utter-ance to confirm the content of the task-related at-tributes.Rule Definitions: Each rule has one of the fol-lowing two forms.
((rule name)(child feature structure)?
.
.
(child feature structure)=> (mother feature structu_e)(priority increase) )((role name)(child feature structure)?
.
.
(child feature structure)=> (flame operation command)(priority increase) )These roles are similar to DCG (Pereira nd War-ren, 1980) rules; they can include logical vari-ables and these variables can be bound whenthese rules are applied.
It is possible to add to therules constraints that stipulate relationships thatmust hold among variables (Nakano, 199 I), butwe do not explain these constraints indetail in this154paper.
The priorities are used for disambiguat-ing interpretation i  the incremental understand-ing method (Nakano et al, 1999b).When the command on the right-hand side ofthe arrow is a frame operation command, phrasesto which this rule can be applied can be consid-ered a sentence, and the sentence's semantic rep-resentation is the command for updating the dia-logue state.
The command is one of the follow-ing:?
A command to set the value of an attributeof the frame,?
A command to increase the priority,Conditional commands (If-then-else typecommand, the condition being whether thevalue of an attribute of the flame is or is notequal to a specified value, or a conjunctionor disjunction of the above condition), or?
A list of commands to be sequentially exe-cuted.Thanks to conditional commands, it is possibleto represent the semantics of sentences context-dependently.The following rule is an example.
( s ta r t -end- t imes-command( t ime-phrase  : f rom *start)( t ime-phrase  (:or :to nil) *end)=> (command (set :start *start)(set :end *end)))The name of this rule is s ta r t -end- t imes-command.
The second and third elementsare child feature structures.
In these elements,t ime-phrase  is a phrase category, : f rom and( : or : to n i l  ) are case feature values, and*s tar t  and *end are semantic feature val-ues.
Here :or means a disjunction, and sym-bols starting with an asterisk are variables.
Theright-hand side of the arrow is a command to up-date the frame.
The second element of the com-mand, (set :start  *start), changes the: s ta r t  atttribute value of the frame to the in-stance of *s tar t ,  which should be bound whenapplying this rule to the child feature structures.Phase Definitions: Each phase definition con-sists of a phase name, a network name, an ini-tiative holder specification, an initial function, anaction function, a maximum silence duration, anda time-out function.
The network name is theidentifier of the language model for the speechrecognition.
The maximum silence duration spec-ifies how long the generation module should waituntil the time-out function is invoked.Below is an example of a phase definition.The first element request  is the name of thisphase, " f ra r_ request"  is the name of thenetwork, and move- to - reques  t -phase  andrequest -phase-act ion  are the names ofthe initial and action functions.
In this phase,the maximum silence duration is ten seconds andthe name of the time-out function is request -phas e- t imeou t.(request " fmr_request"move-  to - reques  t -phaserequest -phase-act ion10.0request -phase-  t imeout  )For the definitions of these functions, WIT pro-vides functions for accessing the dialogue state,sending a request o speak to the speech out-put module, generating strings to be spoken us-ing surface generation templates, hifting the di-alogue phase, taking and releasing the initiative,and so on.
Functions are defined in terms of theCommon Lisp program.Surface-generation Templates: Surface-generation templates are used by the surfacegeneration library function, which convertsa list-structured semantic representation to asequence of strings.
Each string can be spoken,i.e., it is in the list of pre-recorded speech files.For example, let us consider the conversionof the semantic representation (date  (date -express ion  3 15) ) to strings using the fol-lowing template.
( (date(date -express ion  *month  *day))( (*month gatsu) (*day nichi)  ) )The surface generation library function matchesthe input semantic representation with the first el-ement of the template and checks if a sequences155of strings appear in the speech file list.
It re-turns ( '  '3gagsu l5n ich i ' ' )  (March 15th)if the string "3gatsul5nichi" s in the list ofpre-recorded speech files, and otherwise, returns( ' ' 3gatsu  .
.
.
.
15n ich i '  ' ) when thesestrings are in the list.List of Pre-recorded Speech Files: The list ofpre-recorded speech files should show the corre-spondence between strings and speech files to beplayed by the speech output module.4.2 Compiling System SpecificationsFrom the specifications explained above, domain-dependent knowledge sources are created as indi-cated by the dashed arrows in Figure 1.
When cre-ating the knowledge sources, WIT checks for sev-eral kinds of consistency.
For example, the set ofword categories appearing in the lexicon and theset of word categories appearing in phrase deft-nifions are compared.
This makes it easy to finderrors in the domain specifications.5 ImplementationWIT has been implemented in Common Lisp andC on UNIX, and we have built several experi-mental and demonstration dialogue systems usingit, including a meeting room reservation system(Nakano et al, 1999b), a video-recording pro-gramming system, a schedule management sys-tem (Nakano et al, 1999a), and a weather in-formation system (Dohsaka et al, 2000).
Themeeting room reservation system has vocabularyof about 140 words, around 40 phrase structurerules, nine attributes in the semantic frame, andaround 100 speech files.
A sample dialogue be-tween this system and a naive user is shownin Figure 2.
This system employs HTK as thespeech recognition engine.
The weather informa-tion system can answer the user's questions aboutweather forecasts in Japan.
The vocabulary sizeis around 500, and the number of phrase structurerules is 31.
The number of attributes in the se-mantic flame is 11, and the number of the files ofthe pre-recorded speech is about 13,000.6 DiscussionAs explained above, the architecture of WIT al-lows us to develop a system that can use utter-ances that are not clearly segmented into sen-tences by pauses and respond in real time.
Belowwe discuss other advantages and remaining prob-lems.6.1 Descriptive PowerWhereas previous finite-state-model-based tool-kits place many severe restrictions on domain de-scriptions, WIT has enough descriptive power tobuild a variety of dialogue systems.
Although thedialogue state is represented bya simple attribute-value matrix, since there is no limitation on thenumber of attributes, it can hold more compli-cated information.
For example, it is possible torepresent a discourse stack whose depth is lim-ited.
Recording some dialogue history is alsopossible.
Since the language understanding mod-ule utilizes unification, a wide variety of lin-guistic phenomena can be covered.
For exam-ple, speech repairs, particle omission, and fillerscan be dealt with in the framework of unifica-tion grammar (Nakano et al, 1994; Nakano andShimazu, 1999).
The language generation mod-ule features Common Lisp functions, so there isno limitation on the description.
Some of thesystems we have developed feature a generationmethod based on hierarchical planning (Dohsakaand Shirnazu, 1997).
It is also possible to build asimple finite-state-model-based dialogue systemusing WIT.
States can be represented bydialoguephases in WIT.6.2 ConsistencyIn an agglutinative language such as Japanese,there is no established definition of words, so dia-logue system developers must define words.
Thissometimes causes a problem in that the defini-tion of word, that is, the word boundaries, in thespeech recognition module are different from thatin the language understanding module.
In WIT,however, since the common lexicon is used inboth the speech recognition module and languageunderstanding module, the consistency betweenthem is maintained.6-3 Avoiding Information LossIn ordinary spoken language systems, the speechrecognition module sends just a word hypoth-esis to the language processing module, which156speaker start end utterancetime (s) time (s)system: 614.53 615.93user: 616.38 618.29system: 619.97 620.13user: 622.65 624.08system: 625.68 625.91user: 626.65 627.78system: 629.25 629.55user: 629.91 631.67system: 633.29 633.57user: 634.95 636.00system: 637.50 645.43user: 645.74 646.04system: 647.05 648.20Figure 2:donoy6na goy6ken desh6 ka (how may I help you?
)kaigishitsu oyoyaku shitai ndesu ga (I'd like to make a reserva-tion for a meeting room)hai (uh-huh)san-gatsujfini-nichi (on March 12th)hal (uh-huh)jayo-ji kara (from 14:00)hai (uh-huh)jashichi-ji sanjup-pun made (to 17:30)hai (uh-huh)dai-kaigishitsu (the large meeting room)san-gatsu jani-nichi, j~yo-ji kara, jashichi-ji sanjup-pun made,dai-kaigishitsu toyfi koto de yoroshf deshrka (on March 12th,from 14:00 to 17:30, the large meeting room, is that right?)
"hai (yes)kashikomarimashitd (allright)An example dialogue of an example systemmust disambiguate word meaning and find phraseboundaries by parsing.
In contrast, the speechrecognition module in WIT sends not only wordsbut also word categories, phrase boundaries, andphrase categories.
This leads to less expensiveand better language understanding.6.4 Problems and LimitationsSeveral problems remain with WIT.
One of themost significant is that he system developer mustwrite language generation functions.
If the gen-eration functions employ sophisticated dialoguestrategies, the system can perform complicateddialogues that are not just question answering.WIT, however, does not provide task-independentfacilities that make it easier to employ such dia-logue strategies.There have been several efforts aimed at de-veloping a domain-independent me hod for gen-erating responses from a frame representation fuser requests (Bobrow et al, 1977; Chu-CarroU,1999).
Incorporating such techniques would deocrease the system developer workload.
However,there has been no work on domain-independentresponse generation for robust spoken dialoguesystems that can deal with utterances that mightinclude pauses in the middle of a sentence, whichWIT handles well.
Therefore incorporating thosetechniques remains as a future work.Another limitation is that WIT cannot deal withmultiple speech recognition candidates such asthose in an N-best list.
Extending WIT to dealwith multiple recognition results would improvethe performance of the whole system.
The ISSSpreference mechanism is expected to play a rolein choosing the best recognition result.7 ConclusionThis paper described WIT, a toolkit for build-ing spoken dialogue systems.
Although it re-quires more system specifications than previousfinite-state-model-based toolkits, it enables oneto easily construct real-time, robust spoken dia-logue systems that incorporates advanced compu-tational linguistics technologies.AcknowledgementsThe authors thank Drs.
Ken'ichiro Ishii, Nori-hiro Hagita, and Takeshi Kawabata for their sup-port of this research.
Thanks also go to TetsuyaKubota, Ryoko Kima, and the members of theDialogue Understanding Research Group.
Weused the speech recognition engine VoiceRex de-veloped by NTT Cyber Space Laboratories andthank those who helped us use it.
Comments by157the anonymous reviewers were of' great help.ReferencesJames F. Allen, Bradford W. Miller, Eric K. Ringger,and Teresa Sikorski.
1996.
A robust system for nat-ural spoken dialogue.
In Proceedings of the 34thAnnual Meeting of the Association for Computa-tional Linguistics (A CL-96), pages 62-70.Harald Aust, Martin Oerder, Frank Seide, and VolkerSteinbiss.
1995.
The Philips automatic traintimetable information system.
Speech Communi-cation, 17:249--262.James Barnett and Mona Singh.
1997.
Designinga portable spoken language system.
In ElisabethMaier, Marion Mast, and Susann LuperFoy, editors,Dialogue Processing inSpoken Language Systems,pages 156--170.
Springer-Vedag.Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay,Dona!d A. Norman, Henry Thompson, and TerryWinograd.
1977.
GUS, a frame driven dialog sys-tem.
Arnficial Intelligence, 8:155-173.Jennifer Chu-Carroll.
1999.
Fo:rrn-based reason-ing for mixed-initiative dialogue management ininformation-query systems.
In Proceedings of theSixth European Conference on Speech Communica-tion and Technology (Eurospeech-99) , pages 1519-1522.Junnifer Chu-Carroll.
2000.
MIMIC: An adaptivemixed initiative spoken dialogue system for infor-mation queries.
In Proceedings of the 6th Con-f~rence on Applied Natural Language Processing(ANLP-O0), pages 97-104.Kohji Dohsaka nd Akira Shimazu.
1997.
System ar-chitecture for spoken utterance production in col-laborative dialogue.
In Working Notes of IJCAI1997 Workshop on Collaboration, Cooperation andConflict in Dialogue Systems.Kohji Dohsaka, Norihito Yasuda, Noboru Miyazaki,Mikio Nakano, and Kiyoaki AJkawa.
2000.
An ef-ficient dialogue control method under system's lim-ited knowledge.
In Proceedings of the Sixth Inter-national Conference on Spoken Language Process-ing (ICSLP-O0).Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano,and Takeshi Kawabata.
1998.
Implementationof coordinative nodding behavior on spoken dia-logue systems.
In Proceedings of the Fgth Interna-tional Conference on Spoken Language Processing(1CSLP-98), pages 2347-2350.Tetsunod Kobayashi, Shuichi Itahashi, SatoruHayamizu, and Toshiyuki Takezawa.
1992.
Asjcontinuous speech corpus for research.
The journalof th e Acoustical Society of Japan, 48(12): 888-893.Mikio Nakano and Akira Shimazu.
1999.
Pars-ing utterances including self-repairs.
In YorickWilks, editor, Machine Conversations, pages 99-112.
Kluwer Academic Publishers.Mikio Nakano, Aldra Shimazu, and Kiyoshi Kogure.1994.
A grammar and a parser for spontaneousspeech.
In Proceedings of the 15th Interna-tional Conference on Computational Linguistics(COLING-94), pages 1014-1020.Mildo Nakano, Kohji Dohsaka, Noboru Miyazald,Inn ichi Hirasawa, Masafiami Tamoto, MasahitoKawarnon, Akira Sugiyama, and Takeshi Kawa-bata.
1999a.
Handling rich turn-taking in spokendialogue systems.
In Proceedings of the Sixth Eu-ropean Conference on Speech Communication a dTechnology (Eurospeech-99), pages 1167-1170.Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,Kohji Dohsaka, and Takeshi Kawabata.
1999b.Understanding unsegmented user utterances in real-time spoken dialogue systems.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics (ACL-99), pages 200--207.Mikio Nakano.
1991.
Constraint projection: An ef-ficient treatment of disjunctive f ature descriptions.In Proceedings of the 29th Annual Meeting of theAssociation for Computational Linguistics (ACL-90, pages 307-314.Yoshiaki Noda, Yoshikazu Yamaguchi, TomokazuYamada, Akihiro Imamura, Satoshi Takahashi,Tomoko Matsui, and Kiyoaki Aikawa.
1998.
Thedevelopment of speech recognition engine REX.
InProceedings of the 1998 1EICE General Confer-ence D-14-9, page 220.
(in Japanese).Fernando C. N. Pereira and David H. D. Warren.1980.
Definite clause grammars for languageanalysis--a survey of the formalism and a compar-ison with augmented transition etworks.
ArtificialIntelligence, 13:231-278.Carl J. Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
CSLI, Stanford.Munehiko Sasajima, Yakehide Yano, and YasuyukiKono.
1999.
EUROPA: A genetic framework fordeveloping spoken dialogue systems.
In Proceed-ings of the Sixth European Conference on SpeechCommunication a d Technology (Eurospeech-99),pages 1163--1166.Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-fine Pao, Philipp Sehmid, and Victor Zue.
1998.GALAXY-H: A reference architecture for conver-sational system development.
In Proceedings of158the Fifth International Con l~rence on Spoken Lan-guage Processing (ICSLP-98).Stephen Sutton, Ronaid A. Cole, Jacques de Villiers,Johan SchMkwyk, Pieter Vermeulen, Michael W.Macon, Yonghong Yah, Edward Kaiser, Brian Run-die, K.haldoun Shobaki, Paul Hosom, Alex Kain,Johan Wouters, Dominic W. Massaro, and MichaelCohen.
1998.
Universal speech tools: TheCSLU toolkit.
In Proceedings of the Fifth Interna-tional Conference on Spoken Language Processing(1CSLP-98), pages 3221-3224.Marilyn Walker, Irene Langkilde, Jerry Wright, AllenGorin, and Diane Litman.
2000.
Learning to pre-dict problematic situations in a spoken dialoguesystem: Experiments with how may I help you?
InProceedings of the First Meeting of the North Amer-ican Chapter of the Association for ComputationalLinguistics (NAA CL-O0), pages 210--217.Victor Zue, Stephanie Seneff, James Glass, Joseph Po-lifroni, Christine Pao, Timothy J. Hazen, and LeeHe~erington.
2000.
Jupiter: A telephone-basedconversational interface for weather information.1EEE Transactions on Speech and Audio Process-ing, 8(1):85-96.159
