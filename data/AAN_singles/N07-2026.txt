Proceedings of NAACL HLT 2007, Companion Volume, pages 101?104,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsLook Who is Talking: Soundbite Speaker Name Recognition inBroadcast News SpeechFeifan Liu, Yang LiuDepartment of Computer ScienceThe University of Texas at Dallas, Richardson, TX{ffliu,yangl}@hlt.utdallas.eduAbstractSpeaker name recognition plays an importantrole in many spoken language applications,such as rich transcription, information extrac-tion, question answering, and opinion mining.In this paper, we developed an SVM-basedclassification framework to determine thespeaker names for those included speech seg-ments in broadcast news speech, called sound-bites.
We evaluated a variety of features withdifferent feature selection strategies.
Experi-ments on Mandarin broadcast news speechshow that using our proposed approach, thesoundbite speaker name recognition (SSNR)accuracy is 68.9% on our blind test set, an ab-solute 10% improvement compared to a base-line system, which chooses the person nameclosest to the soundbite.1 IntroductionBroadcast news (BN) speech often contains speech orinterview quotations from specific speakers other thanreporters and anchors in a show.
Identifying speakernames for these speech segmentations, called soundbites(Maskey and Hirschberg, 2006), is useful for manyspeech processing applications, e.g., question answering,opinion mining for a specific person.
This has recentlyreceived increasing attention in programs such as theDARPA GALE program, where one query template isabout a person?s opinion or statement.Previous work in this line includes speaker role de-tection (e.g., Liu, 2006; Maskey and Hirschberg, 2006)and speaker diarization (e.g., Canseco et al, 2005).
Inthis paper, we formulate the problem of SSNR as a tra-ditional classification task, and proposed an SVM-basedidentification framework to explore rich linguistic fea-tures.
Experiments on Mandarin BN speech have shownthat our proposed approach significantly outperformsthe baseline system, which chooses the closest name asthe speaker for a soundbite.2 Related WorkTo our knowledge, no research has yet been conductedon soundbite speaker name identification in MandarinBN domain.
However, this work is related to some ex-tent to speaker role identification, speaker diarization,and named entity recognition.Speaker role identification attempts to classify speechsegments based on the speakers?
role (anchor, reporter,or others).
Barzilay et al (2000) used BoosTexter andthe maximum entropy model for this task in English BNcorpus, obtaining a classification accuracy of about 80%compared to the chance of 35%.
Liu (2006) combined agenerative HMM approach with the conditional maxi-mum entropy method to detect speaker roles in Manda-rin BN, reporting a classification accuracy of 81.97%against the baseline of around 50%.
In Maskey andHirschberg (2006), the task is to recognize soundbites(which make up of a large portion of the ?other?
rolecategory in Liu (2006)).
They achieved a recognitionaccuracy of 67.4% in the English BN domain.
Differentfrom their work, our goal is to identify the person whospoke those soundbites, i.e., associate each soundbitewith a speaker name if any.Speaker diarization in BN aims to find speakerchanges, group the same speakers together, and recog-nize speaker names.
It is an important component forrich transcription (e.g., in the DARPA EARS program).So far most work in this area has only focused onspeaker segmentation and clustering, and not includedname recognition.
However, Canseco et al (2005) wereable to successfully use linguistic information (e.g.,related to person names) to improve performance of BNspeaker segmentation and clustering.This work is also related to named entity recognition(NER), especially person names.
There has been a largeamount of research efforts on NER; however, instead of101recognizing all the names in a document, our task is tofind the speaker for a particular speech segment.3 Framework for Soundbite SpeakerName Recognition (SSNR)Figure 1 shows our system diagram.
SSNR is conductedusing the speech transcripts, assuming the soundbitesegments are provided.
After running NER in the tran-scripts, we obtain candidate person names.
For a sound-bite, we use the name hypotheses from the region bothbefore and after the soundbite.
A ?region?
is definedbased on the turn and topic segmentation information.To determine which name among the candidates is thecorresponding speaker for the soundbite, we recast thisproblem as a binary classification problem for everycandidate name and the soundbite, which we call aninstance.
A positive tag for an instance means that thename is the soundbite speaker.
Each instance has anassociated feature vector, described further in the fol-lowing section.
Note that if a name occurs more thanonce, only one instance is created for it.Train Set Dev/Test SetPreprocessing (wordsegmentation, NER)Instance Creationfor SSNRFeature VectorRepresentationTrainedModelModel Trainingand OptimizingConflictResolutionTraining TestingOutputFigure 1.
System diagram for SSNR.Any classification approach can be used in this gen-eral framework for SSNR.
We choose to use an SVMclassifier in our experiments because of its superior per-formance in many classification tasks.3.1 FeaturesThe features that we have explored can be grouped intothree categories.Positional Features (PF)?
PF-1: the position of the candidate name relative tothe soundbite.
We hypothesize that names closer toa soundbite are more likely to be the soundbitespeaker.
This feature value can be ?last?, ?first?,?mid?, or ?unique?.
For example, ?last?
for a candi-date before a soundbite means that it is the closestname among the hypotheses before the soundbite.?Unique?
indicates that the candidate is the onlyperson name in the region before or after the sound-bite.
Note that if a candidate name occurs more thanonce, the PF-1 feature corresponds to the closestname to the soundbite.?
PF-2: the position of a name in its sentence.
Typi-cally a name appearing earlier in a sentence (e.g., asubject) is more likely to be quoted later.?
PF-3: an indicator feature to show where the namehas occurred, before, inside, or after the soundbite.We added this because it is rare that a name inside asoundbite is the speaker of that soundbite.?
PF-4: an indicator to denote if a candidate is in thelast sentence just before the soundbite turn, or is inthe first sentence just after the soundbite turn.Frequency Features (Freq)We hypothesize that a name with more occurrencesmight be an important subject and thus more likely to bethe speaker of the soundbite, therefore we include thefrequency of a candidate name in the feature set.Lexical Features (LF)In order to capture the cue words around the soundbitespeaker names in the transcripts, we included unigramfeatures.
For example, ?pre_word+1=?/said?
denotesthat the candidate name is followed by the word ?
?/said?, and that ?pre?
means this happens in the regionbefore the soundbite.3.2 Conflict ResolutionAnother component in the system diagram that is worthpointing out is ?conflict resolution?.
Since our approachtreats each candidate name as a separate classificationtask, we need to post-process the cases where there aremultiple or no positive hypotheses for a soundbite dur-ing testing.
To resolve this situation, we choose the in-stance with the best confidence value from the classifier.4 Experiments4.1 Experimental SetupWe use the TDT4 Mandarin broadcast news data in ourexperiment.
The data set consists of about 170 hours(336 shows) of news speech from different sources.Speaker turns and soundbite segment information wereannotated manually in the transcripts.
Our current study102only uses the soundbites that have a human-labeledspeaker name in the surrounding transcripts.
There are1292 such soundbites in our corpus.
We put aside 1/10of the data as the development set, another 1/10 as thetest set, and used the rest as our training set.
All thetranscripts were automatically tagged with named enti-ties using the NYU tagger (Ji and Grishman, 2005).
Forthe classifier, we used the libSVM toolkit (Chang andLin, 2001) and the RBF kernel in our experiments.A reasonable baseline for SSNR is to choose theclosest person name before a soundbite as its speaker.We will compare our system performance to this base-line approach.We used two performance metrics in our experi-ments.
First is the instance classification accuracy (CA)for the candidate names in the framework of the binaryclassification task.
Second, we compute name recogni-tion accuracy (RA) for the soundbites as follows:FilesinSoundbitesofNamesCorrectwithSoundbitesofRA##=4.2 Effects of Different Manually SelectedFeature SubsetsWe used 10-fold cross validation on the training set toevaluate the effect of different features and also for pa-rameter optimization.
Table 1 shows the instance classi-fication results.
?PF, Freq, LF?
are the featuresdescribed in Section 3.1.
?LF-before?
means the uni-gram features before the soundbites.
?All-before?
de-notes using all the features before the soundbites.Optimized Para.
FeatureSubsets C GCA(%)PF-1 0.125 2 83.48+PF-2 2048 1.22e-4 85.62+PF-3 2048 4.88e-4 85.79+PF-4 2 0.5 86.18+Freq 2 0.5 86.18+LF-before 32 7.81e-3 88.44+LF-afteri.e., All features 8 0.0313 88.44All-before 8 0.0313 88.03Table 1.
Instance classification accuracy (CA) usingdifferent feature sets.
C and G are the optimized pa-rameters in the SVM model.We notice that the system performance generallyimproves with incrementally expended feature sets,yielding an accuracy of 88.44% using all the features.Some features seem not helpful to system performance,such as ?Freq?
and ?LF-after?.
Using all the featuresbefore the soundbites achieves comparable performanceto using all the features, indicating that the region beforea soundbite contributes more than that after it.
This isexpected since the reporters typically have already men-tioned the person?s name before a soundbite.
In addition,we evaluated some compound features using our currentfeature definition, but adding those did not improve thesystem performance.4.3 Automatic Feature SelectionWe also performed automatic feature selection for theSVM model based on the F-score criterion (Chen andLin, 2006).
There are 6048 features in total in our sys-tem.
Figure 2 shows the classification performance inthe training set using different number of features viaautomatic feature selection.88.6190.7987.188.1288.4488.6188.7390.1488.44868788899091926048302418391512 756378189 94 47# of featuresCA(%)Figure 2.
Instance classification accuracy (CA) using F-score based feature selection.We can see that automatic feature selection further im-proves the classification performance (2.36% higheraccuracy than that in Table 1).
Table 2 lists some of thetop features based on their F-scores.
Consistent with ourexpectation, we observe that position related features, aswell as cue words, are good indicators for SSNR.Feature F-scoreJustbeforeturn (PF-4) 0.3543pre_contextpos=last (PF-1) 0.2857pre_senpos=unique (PF-2) 0.0631pre_word+1=???/morning?
(LF) 0.0475pre_word+1= ??/said?
(LF) 0.0399bool_pre=1 (PF-3) 0.0353Justafterturn (PF-4) 0.0349pre_contextpos=mid (PF-1) 0.0329post_contextpos=first (PF-1) 0.0323pre_word+1= ???/today?
(LF) 0.0288pre_word-1=???/reporter?
(LF) 0.0251pre_word+1=???/express?
(LF) 0.0246Table 2.
Top features ordered by F-score values.1034.4 Performance on Development SetUp to now our focus has been on feature selection basedon instance classification accuracy.
Since our ultimategoal is to identify soundbite speaker names, we choseseveral promising configurations based on the resultsabove to apply to the development set and evaluate thesoundbite name recognition accuracy.
Results using thetwo metrics are presented in Table 3.Feature Set CA (%) RA (%)Baseline 84.0 59.3PF 86.7 54.2PF+Freq 86.7 60.4PF+Freq+LF-before 87.8 63.5PF+Freq+LF-before+LF-after (ALL) 88.3 67.7Top 1512 by f-score 85.6 62.5Top 1839 by f-score 85.4 60.4Table 3.
Results on the dev set using two metrics: in-stance classification accuracy (CA), and soundbite namerecognition accuracy (RA).
The oracle RA is 79.1%.Table 3 shows that using all the features (ALL)performs the best, yielding an improvement of 4.3% and8.4% compared to the baseline in term of the CA and RArespectively.
However, using the automatically selectedfeature sets (the last two rows in Table 3) only slightlyoutperforms the baseline.
This suggests that the F-scorebased feature selection strategy on the training set maynot generalize well.
Interestingly, ?Freq?
and ?LF-after?features show some useful contribution (the 4th and 6throw in Table 3) respectively on the development set,different from the results on the training set using 10-fold cross validation.
The results using the two metricsalso show that they are not always correlated.Because of the possible NER errors, we also meas-ure the oracle RA, defined as the percent of the sound-bites for which the correct speaker name (based on NER)appears in the region surrounding the soundbite.
Theoracle RA on this data set is 79.1%.
We also notice that8.3% of the soundbites do not have the correct namehypothesis due to an NER boundary error, and that12.5% is because of missing errors.We used the method as described in Section 3.2 toresolve conflicts for the results shown in Table 3.
Inaddition, we evaluated another approach?we resort tothe baseline (i.e., chose the name that is closest to thesoundbite) for those soundbites that have multiple or nopositive hypothesis.
Our experiments on the develop-ment set showed this approach degrades system per-formance (e.g., RA of around 61% using all the features).4.5 Results on Blind Test SetFinally, we applied the all-feature configuration to ourblind test data and obtained the results as shown in Ta-ble 4.
Using all the features significantly outperformsthe baseline.
The gain is slightly better than that on thedevelopment set, although the oracle accuracy is alsohigher on the test set.CA (%) RA (oracle: 85.8%)Baseline 81.3 58.4All feature 85.1 68.9Table 4.
Results on the test set.5 ConclusionWe proposed an SVM-based approach for soundbitespeaker name recognition and examined various linguis-tic features.
Experiments in Mandarin BN corpus showthat our approach yields an identification accuracy of68.9%, significantly better than 58.4% from the baseline.Our future work will focus on exploring more usefulfeatures, such as part-of-speech and semantic features.In addition, we plan to test this framework using auto-matic speech recognition output, speaker segmentation,and soundbite segment detection.6 AcknowledgementWe thank Sameer Maskey, Julia Hirschberg, and MariOstendorf for useful discussions, and Heng Ji for shar-ing the Mandarin named entity tagger.
This work issupported by DARPA under Contract No.
HR0011-06-C-0023.
Any opinions expressed in this material arethose of the authors and do not necessarily reflect theviews of DARPA.ReferencesS.
Maskey and J. Hirschberg.
2006.
Soundbite Detec-tion in Broadcast News Domain.
In Proc.
of INTER-SPEECH2006.
pp: 1543-1546.Y.
Liu.
2006.
Initial Study on Automatic Identificationof Speaker Role in Broadcast News Speech.
In Proc.of HLT-NAACL.
pp: 81-84.R.
Barzilay, M. Collins, J. Hirschberg, and S. Whittaker.2000.
The Rules Behind Roles: Identifying SpeakerRole in Radio Broadcasts.
In Proc.
of AAAI.L.
Canseco, L. Lamel, and J.-L. Gauvain.
2005.
A Com-parative Study Using Manual and Automatic Tran-scriptions for Diarization.
In Proc.
of ASRU.H.
Ji and R. Grishman.
2005.
Improving Name Taggingby Reference Resolution and Relation Detection.
InProc.
of ACL.
pp: 411-418.Y.-W. Chen and C.-J.
Lin.
2006.
Combining SVMs withVarious Feature Selection Strategies.
Feature Extrac-tion, Foundations and Applications, Springer.C.
Chang and C. Lin.
2001.
LIBSVM: A Library forSupport Vector Machines.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvm.104
