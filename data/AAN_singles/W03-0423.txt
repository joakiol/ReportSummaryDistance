Named Entity Recognition with a Maximum Entropy ApproachHai Leong ChieuDSO National Laboratories20 Science Park DriveSingapore 118230chaileon@dso.org.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore3 Science Drive 2Singapore 117543nght@comp.nus.edu.sg1 IntroductionThe named entity recognition (NER) task involves iden-tifying noun phrases that are names, and assigning a classto each name.
This task has its origin from the MessageUnderstanding Conferences (MUC) in the 1990s, a seriesof conferences aimed at evaluating systems that extractinformation from natural language texts.
It became evi-dent that in order to achieve good performance in infor-mation extraction, a system needs to be able to recognizenames.
A separate subtask on NER was created in MUC-6 and MUC-7 (Chinchor, 1998).Much research has since been carried out on NER, us-ing both knowledge engineering and machine learningapproaches.
At the last CoNLL in 2002, a common NERtask was used to evaluate competing NER systems.
Inthis year?s CoNLL, the NER task is to tag noun phraseswith the following four classes: person (PER), organiza-tion (ORG), location (LOC), and miscellaneous (MISC).This paper presents a maximum entropy approach tothe NER task, where NER not only made use of localcontext within a sentence, but also made use of other oc-currences of each word within the same document to ex-tract useful features (global features).
Such global fea-tures enhance the performance of NER (Chieu and Ng,2002b).2 A Maximum Entropy ApproachThe maximum entropy framework estimates probabilitiesbased on the principle of making as few assumptions aspossible, other than the constraints imposed.
Such con-straints are derived from training data, expressing somerelationship between features and outcome.
The proba-bility distribution that satisfies the above property is theone with the highest entropy.
It is unique, agrees with themaximum-likelihood distribution, and has the exponen-tial form (Della Pietra et al, 1997):p(o|h) =1Z(h)k?j=1?fj(h,o)j ,where o refers to the outcome, h the history (or context),and Z(h) is a normalization function.
The features usedin the maximum entropy framework are binary.
An ex-ample of a feature function isfj(h, o) ={1 if o = org-B, word = PETER0 otherwiseThe parameters ?j are estimated by a procedure calledGeneralized Iterative Scaling (GIS) (Darroch and Rat-cliff, 1972).
This is an iterative procedure that improvesthe estimation of the parameters at each iteration.The maximum entropy classifier is used to classifyeach word as one of the following: the beginning of aNE (B tag), a word inside a NE (C tag), the last wordof a NE (L tag), or the unique word in a NE (U tag).During testing, it is possible that the classifier produces asequence of inadmissible classes (e.g., PER-B followedby LOC-L).
To eliminate such sequences, we define atransition probability between word classes P (ci|cj)to be equal to 1 if the sequence is admissible, and 0otherwise.
The probability of the classes c1, .
.
.
, cnassigned to the words in a sentence s in a document D isdefined as follows:P (c1, .
.
.
, cn|s,D) =n?i=1P (ci|s,D) ?
P (ci|ci?1),where P (ci|s,D) is determined by the maximum entropyclassifier.
The Viterbi algorithm is then used to select thesequence of word classes with the highest probability.3 Feature RepresentationWe present two systems: a system ME1 that does notmake use of any external knowledge base other than thetraining data, and a system ME2 that makes use of ad-ditional features derived from name lists.
ME1 is usedfor both English and German.
For German, however, forfeatures that made use of the word string, the lemma (pro-vided in the German training and test data) is used insteadof the actual word.3.1 Lists derived from training dataThe training data is first preprocessed to compile a num-ber of lists that are used by both ME1 and ME2.
Theselists are derived automatically from the training data.Frequent Word List (FWL) This list consists ofwords that occur in more than 5 different documents.Useful Unigrams (UNI) For each name class, wordsthat precede the name class are ranked using correlationmetric (Chieu and Ng, 2002a), and the top 20 are com-piled into a list.Useful Bigrams (UBI) This list consists of bigrams ofwords that precede a name class.
Examples are ?CITYOF?, ?ARRIVES IN?, etc.
The list is compiled by takingbigrams with higher probability to appear before a nameclass than the unigram itself (e.g., ?CITY OF?
has higherprobability to appear before a location than ?OF?).
A listis collected for each name class.
We have attempted touse bigrams that appear after a name class, but for Englishat least, we have been unable to compile any such mean-ingful bigrams.
A possible explanation is that in writing,people tend to explain with bigrams such as ?CITY OF?before mentioning the name itself.Useful Word Suffixes (SUF) For each word in a nameclass, three-letter suffixes with high correlation metricscore are collected.
This is especially important for theMISC class, where suffixes such as ?IAN?
and ?ISH?
of-ten appear.Useful Name Class Suffixes (NCS) A suffix list iscompiled for each name class.
These lists capture tokensthat frequently terminate a particular name class.
For ex-ample, the ORG class often terminates with tokens suchas INC and COMMITTEE, and the MISC class often ter-minates with CUP, OPEN, etc.Function Words (FUN) Lower case words that occurwithin a name class.
These include ?van der?, ?of?, etc.3.2 Local FeaturesThe basic features used by both ME1 and ME2 can bedivided into two classes: local and global (Chieu and Ng,2002b).
Local features of a token w are those that arederived from the sentence containing w. Global featuresare derived by looking up other occurrences of w withinthe same document.In this paper, w?i refers to the ith word before w, andw+i refers to the ith word after w. The features used aresimilar to those used in (Chieu and Ng, 2002b).
Localfeatures include:First Word, Case, and Zone For English, each doc-ument is segmented by simple rules into 4 zones: head-line (HL), author (AU), dateline (DL), and text (TXT).
Toidentify the zones, a DL sentence is first identified usinga regular expression.
The system then looks for an AUsentence that occurs before DL using another regular ex-pression.
All sentences other than AU that occur beforethe DL sentence are then taken to be in the HL zone.
Sen-tences after the DL sentence are taken to be in the TXTzone.
If no DL sentence can be found in a document, thenthe first sentence of the document is taken as HL, and therest as TXT.
For German, the first sentence of each docu-ment is taken as HL, and the rest as TXT.
Zone is used aspart of the following features:If w starts with a capital letter (i.e., initCaps), and it isthe first word of a sentence, a feature (firstword-initCaps,zone) is set to 1.
If it is initCaps but not the first word, afeature (initCaps, zone) is set to 1.
If it is the first wordbut not initCaps, (firstword-notInitCaps, zone) is set to 1.If it is made up of all capital letters, then (allCaps, zone)is set to 1.
If it starts with a lower case letter, and containsboth upper and lower case letters, then (mixedCaps, zone)is set to 1.
A token that is allCaps will also be initCaps.Case and Zone of w+1 and w?1 Similarly, if w+1(or w?1) is initCaps, a feature (initCaps, zone)NEXT (or(initCaps, zone)PREV ) is set to 1, etc.Case Sequence Suppose both w?1 and w+1 are init-Caps.
Then if w is initCaps, a feature I is set to 1, else afeature NI is set to 1.Token Information These features are based on thestring w, such as contains-digits, contains-dollar-sign, etc(Chieu and Ng, 2002b).Lexicon Feature The string of w is used as a feature.This group contains a large number of features (one foreach token string present in the training data).Lexicon Feature of Previous and Next Token Thestring of the previous token w?1 and the next token w+1is used with the initCaps information of w. If w has init-Caps, then a feature (initCaps, w+1)NEXT is set to 1.
Ifw is not initCaps, then (not-initCaps, w+1)NEXT is set to1.
Same for w?1.Hyphenated Words Hyphenated words w of the forms1-s2 have a feature U -U set to 1 if both s1 and s2 areinitCaps.
If s1 is initCaps but not s2, then the featuresU=s1, L=s2, and U -L are set to 1.
If s2 is initCaps butnot s1, then the features U=s2, L=s1, and L-U are set to1.Within Quotes/Brackets Sequences of tokens withinquotes or brackets have a feature to indicate that they arewithin quotes.
We found this feature useful for MISCclass, where names such as movie names often appearwithin quotes.Rare Words If w is not found in FWL, then this featureis set to 1.Bigrams If (w?2, w?1) is found in UBI for the nameclass nc, then the feature BI-nc is set to 1.Word Suffixes If w has a 3-letter suffix that can befound in SUF for the name class nc, then the featureSUF -nc is set to 1.Class Suffixes For w in a consecutive sequence ofinitCaps tokens (w,w+1, .
.
.
, w+n), if any of the tokensfrom w+1 to w+n is found in the NCS list of the nameclass nc, then the feature NCS-nc is set to 1.Function Words If w is part of a sequence found inFUN, then this feature is set to 1.3.3 Global FeaturesThe global features include:Unigrams If another occurrence of w in the samedocument has a previous word wp that can be foundin UNI, then these words are used as features Other-occurrence-prev=wp.Bigrams If another occurrence of w has the featureBI-nc set to 1, then w will have the feature OtherBI-nc set to 1.Class Suffixes If another occurrence of w has the fea-ture NCS-nc set to 1, then w will have the featureOtherNCS-nc set to 1.InitCaps of Other Occurrences This feature checksfor whether the first occurrence of the same word in anunambiguous position (non first-words in the TXT zone)in the same document is initCaps or not.
For a wordwhose initCaps might be due to its position rather thanits meaning (in headlines, first word of a sentence, etc),the case information of other occurrences might be moreaccurate than its own.Acronyms Words made up of all capitalized letters inthe text zone will be stored as acronyms (e.g., IBM).
Thesystem will then look for sequences of initial capitalizedwords that match the acronyms found in the whole doc-ument.
Such sequences are given additional features ofA begin, A continue, or A end, and the acronym is givena feature A unique.
For example, if FCC and FederalCommunications Commission are both found in a docu-ment, then Federal has A begin set to 1, Communicationshas A continue set to 1, Commission has A end set to 1,and FCC has A unique set to 1.Sequence of InitCaps In the sentence Even NewsBroadcasting Corp., noted for its accurate reporting,made the erroneous announcement., a NER may mistakeEven News Broadcasting Corp. as an organization name.However, it is unlikely that other occurrences of NewsBroadcasting Corp. in the same document also co-occurwith Even.
This group of features attempts to capturesuch information.
For every sequence of initial capital-ized words, its longest substring that occurs in the samedocument as a sequence of initCaps is identified.
For thisexample, since the sequence Even News BroadcastingCorp.
only appears once in the document, its longest sub-string that occurs in the same document is News Broad-casting Corp.
In this case, News has an additional featureof I begin set to 1, Broadcasting has an additional featureof I continue set to 1, and Corp. has an additional featureof I end set to 1.Name Class of Previous Occurrences The name classof previous occurrences of w is used as a feature, similarto (Zhou and Su, 2002).
We use the occurrence wherew is part of the longest name class phrase (name classwith the most number of tokens).
For example, if w is thesecond token in a person name class phrase of 5 tokens,then a feature 2Person5 is set to 1.
During training, thename classes are known.
During testing, the name classesare the ones already assigned to tokens in the sentencesalready processed.This last feature makes the order of processing impor-tant.
As HL sentences usually contain less context, theyare processed after the other sentences.3.4 Name ListIn additional to the above features used by both ME1 andME2, ME2 uses additional features derived from namelists compiled from a variety of sources.
These sourcesare the Internet and the list provided by the organizers ofthis shared task.
The list is a mapping of sequences ofwords to name classes.
An example of an entry in the listis ?JOHN KENNEDY : PERSON?.
Words that are part ofa sequence of words mapped to a name class nc will havea feature CLASS=nc set to 1.
Another list of weekdaysand month names is also used in the same way.
For ME2,we have also manually added additional entries into theautomatically compiled NCS lists.4 ExperimentsThe English training and test data are part of the ReutersCorpus, Volume 11.
The German training and test dataare part of the European Corpus Initiative, MultilingualCorpus 1.
The best results obtained on the developementand test sets of the 2 languages are as shown in Table 2.Results in Table 1 are obtained by applying ME1, withoutthe help of name lists, on the 2 languages.The best results for English are obtained using ME2,which made use of name lists compiled from the Inter-net and the list provided with the training set (See Sec-tion 3.4).
The best results on German are obtained byusing part-of-speech tags (provided in both training andtest data) as an additional feature to the features used byME1.For all experiments, features that occur only once inthe training data are not used, and the GIS algorithm isrun for 600 iterations.
Running more iterations does notbring about any significant improvement to the accuracy.Our system usually does well for the LOC and PERclass, but fails to do as well for the MISC and ORG class.The bad performance on the MISC class agrees with theobservations of (Carreras et al, 2002).
We felt that the1http://about.reuters.com/researchandstandards/corpus/English devel.
Precision Recall F?=1LOC 93.77% 94.23% 94.00MISC 89.20% 85.14% 87.13ORG 87.25% 85.76% 86.50PER 94.14% 95.98% 95.05Overall 91.76% 91.45% 91.60English test Precision Recall F?=1LOC 89.27% 90.29% 89.78MISC 80.38% 78.21% 79.28ORG 82.43% 82.18% 82.30PER 91.50% 91.84% 91.67Overall 86.83% 86.84% 86.84German devel.
Precision Recall F?=1LOC 74.42% 56.90% 64.49MISC 72.49% 33.66% 45.98ORG 81.00% 47.06% 59.53PER 84.34% 58.03% 68.75Overall 78.80% 49.84% 61.06German test Precision Recall F?=1LOC 72.08% 55.36% 62.62MISC 64.04% 34.03% 44.44ORG 75.95% 46.57% 57.74PER 87.87% 61.84% 72.59Overall 77.05% 51.73% 61.90Table 1: Results for development and test set for the twolanguages by ME1MISC class is particularly difficult due to its generality (itcan refer to anything from movie titles to sports events).Acknowledgements We would like to thank YoongKeok Lee for helping us to apply boosting and featureselection to the maximum entropy algorithm, althoughthese were not used in the final system.ReferencesXavier Carreras, Lluis Marquez, and Lluis Padro.
2002.Named Entity Extraction using AdaBoost.
In Pro-ceedings of the Sixth Conference on Natural LanguageLearning, pages 167?170.Hai Leong Chieu and Hwee Tou Ng.
2002a.
A Max-imum Entropy Approach to Information Extractionfrom Semi-Structured and Free Text.
In Proceedingsof the Eighteenth National Conference on Artificial In-telligence, pages 786?791.Hai Leong Chieu and Hwee Tou Ng.
2002b.
Named En-tity Recognition: A Maximum Entropy Approach Us-ing Global Information.
In Proceedings of the Nine-English devel.
Precision Recall F?=1LOC 95.39% 95.75% 95.57MISC 90.94% 86.01% 88.41ORG 89.12% 87.99% 88.56PER 94.85% 96.96% 95.89Overall 93.16% 92.86% 93.01English test Precision Recall F?=1LOC 90.88% 91.37% 91.12MISC 80.15% 78.21% 79.16ORG 83.82% 84.83% 84.32PER 93.07% 93.82% 93.44Overall 88.12% 88.51% 88.31German devel.
Precision Recall F?=1LOC 71.08% 65.96% 68.42MISC 72.23% 32.97% 45.28ORG 80.86% 48.67% 60.76PER 79.45% 65.95% 72.07Overall 76.15% 54.62% 63.61German test Precision Recall F?=1LOC 69.23% 59.13% 63.78MISC 62.05% 33.43% 43.45ORG 76.70% 48.12% 59.14PER 88.82% 75.15% 81.41Overall 76.83% 57.34% 65.67Table 2: Best results: For English, name lists are used.For German, part-of-speech tags are usedteenth International Conference on ComputationalLinguistics, pages 190?196.Nancy Chinchor.
1998.
MUC-7 Named Entity Task Def-inition, version 3.5.
In Proceedings of the SeventhMessage Understanding Conference.J.
N. Darroch and D. Ratcliff.
1972.
Generalized Itera-tive Scaling for Log-Linear Models.
Annals of Mathe-matical Statistics, 43(5):1470?1480.Stephen Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing Features of Random Fields.IEEE Transactions on Pattern Analysis and MachineIntelligence, 19(4):380?393.GuoDong Zhou and Jian Su.
2002.
Named EntityRecognition using an HMM-based Chunk Tagger.
InProceedings of the Fortieth Annual Meeting of the As-sociation for Computational Linguistics, pages 473?480.
