Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2270?2279, Dublin, Ireland, August 23-29 2014.Comparable Study of Event Extraction in Newswire and BiomedicalDomainsMakoto Miwa?,?Paul Thompson?Ioannis Korkontzelos?Sophia Ananiadou?
?National Centre for Text Mining and School of Computer Science,University of Manchester, United Kingdom?Graduate School of Engineering, Toyota Technological Institute, Japan{makoto.miwa, paul.thompson, ioannis.korkontzelos, sophia.ananiadou}@manchester.ac.ukAbstractEvent extraction is a popular research topic in natural language processing.
Several event extrac-tion tasks have been defined for both the newswire and biomedical domains.
In general, differentsystems have been developed for the two domains, despite the fact that the tasks in both domainsshare a number of characteristics.
In this paper, we analyse the commonalities and differencesbetween the tasks in the two domains.
Based on this analysis, we demonstrate how an eventextraction method originally designed for the biomedical domain can be adapted for applicationto the newswire domain.
The performance is state-of-the-art for both domains, with F-scores of52.7% for the biomedical domain and 52.1% for the newswire domain in terms of their primaryevaluation metrics.1 IntroductionResearch into event extraction was initially focussed on the general language domain, largely driven bythe Message Understanding Conferences (MUC) series (e.g., Chinchor (1998)) and the Automated Con-tent Extraction (ACE) evaluations1.
More recently, the focus of research has been widened to the biomed-ical domain, motivated by the ongoing series of biomedical natural language processing (BioNLP) sharedtasks (STs) (e.g., Kim et al.
(2013)).Although the textual characteristics and the types of relevant events to be extracted can vary consid-erably between domains, the same general features of events normally hold across domains.
An eventusually consists of a trigger and arguments (see Figures 1 and 2.)
A trigger is typically a verb or a nom-inalised verb that denotes the presence of the event in the text, while the arguments are usually entities.In general, arguments are assigned semantic roles that characterise their contribution towards the eventdescription.Until now, however, there has been little, if any, effort by researchers working on event extraction indifferent domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008))and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006))and relation extraction (e.g., (Qian and Zhou, 2012)).
This means that the potential to exploit cross-domain features of events to develop more adaptable event extraction systems is an under-studied area.Consequently, although there is a large number of published studies on event extraction, proposing manydifferent methods, no work has previously been reported that aims to adapt an event extraction methoddeveloped for one domain to a new domain.In response to the above, we have investigated the feasibility of adapting an event extraction methoddeveloped for the biomedical domain to the newswire domain.
To facilitate this, we firstly carry out adetailed static analysis of the differences that hold between event extraction tasks in the newswire andbiomedical domains.
Specifically, we consider the ACE 2005 event extraction task (Walker et al., 2006)for the newswire domain and the Genia Event Extraction task (GENIA) in BioNLP ST 2013 (Kim et al.,2013) for the biomedical domain.
Based on the results of this analysis, we adapt the biomedical eventThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1itl.nist.gov/iad/mig/tests/ace2270Jim McMahon was body slammed to the ground in the mid 80's about five seconds after he had released a pass.PER_Individual Conflict_Attack ??
timex2 PER_Individualtimex2Target Time-WithinTime-At-EndFigure 1: ACE 2005 event example (ID: MARKBACKER 20041220.0919)p300 immunoprecipitated Foxp3 when both proteins were overexpressed in HEK 293T cellsPro Binding Pro +Reg+RegGene expressionGene expressionTheme Theme2 CauseCauseThemeThemeThemeThemeFigure 2: GENIA event example (ID: PMC-1447668-08-Results)extraction method to the task of extracting events in the newswire domain, according to the specificationof the ACE 2005 event extraction task.
The original method consists of a classification pipeline that haspreviously been applied to extract events according to task descriptions that are similar to GENIA.
Inorder to address the differences between this task and the ACE task, we have made a number of changesto the original method, including modifications to the classification labels assigned, the pipeline itselfand the features used.
We retrained the model of the adapted system on the ACE task, compared theperformance, and empirically analysed the differences between the two tasks in terms of entity-relatedinformation.
We demonstrate that the resulting system achieves state-of-the-art performance for tasks inboth domains.2 Related WorkIn this section, we introduce the two domain specific event extraction tasks on which we will focus, i.e.,the ACE 2005 event extraction task, which concerns events in the newswire domain, and the GENIAevent task from the BioNLP ST 2013, which deals with biomedical event extraction.
We also examinestate-of-the-art systems that have been developed to address each task.2.1 Newswire Event ExtractionThe extraction of events from news-related texts has been widely researched, largely due to motivationfrom the various MUC and ACE shared tasks.
Whilst MUC focussed on filling a single event templateon a single topic by gathering information from different parts of a document, ACE defined a morecomprehensive task, involving the recognition of multiple fine-grained and diverse types of entities andassociated intra-sentential events within each document.A common approach to tackling the MUC template filling task has involved the employment ofpattern-based methods, e.g., Riloff (1996).
In contrast, supervised learning approaches have constituteda more popular means of approaching the ACE tasks2.
In this paper, we choose to focus on adaptingour biomedical-focussed event extraction method to the ACE 2005 task.
Our choice is based on the taskdefinition for ACE 2005 having more in common with the BioNLP 2013 GENIA ST definition than theMUC event template task definition.In terms of the characteristics of state-of-the-art event extraction systems designed according to theACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006).Grishman et al.
(2005) proposed a method that sequentially identifies textual spans of arguments, roletypes, and event triggers.
This pipeline approach has been further extended in several subsequent studies.For example, Liao et al.
(2010) investigated document-level cross-event consistency using co-occurrenceof events and event arguments, while Hong et al.
(2011) exploited information gathered from the web toensure cross-entity consistency.2Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) forthe ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper.2271Li et al.
(2013) recently proposed a joint detection method to detect both triggers and arguments(together with their role types) using a structured perceptron model.
The system outperformed the bestresults reported for the ACE 2005 task in the literature, without the use of any external resources.2.2 Biomedical Event ExtractionThe task of event extraction has received a large amount of attention from BioNLP researchers in recentyears.
Interest in this task was largely initiated by the BioNLP 2009 ST, and has been sustained throughthe organisation of further STs in 2011 and 2013.
The STs consist of a number of different sub-tasks, themajority of which concern the extraction of events from biomedical papers from the PubMed database.Events generally concern interactions between biomedical entities, such as proteins, cells and chemicals.Similarly to newswire event extraction systems, pipeline-based methods have constituted a popularapproach to extracting events in the biomedical domain (Bj?orne and Salakoski, 2013; Miwa et al., 2012).The pipeline developed by Miwa et al.
(2012) consists of a number of modules, which sequentiallydetect event triggers, event arguments, event structures and hedges (i.e., speculations and negations).The system has been applied to several event extraction tasks, and has achieved the best performance onmost of these, in comparison to other systems.
It should be noted that the ordering of the componentsin biomedical event extraction pipelines often differs from pipelines designed for news event extraction,e.g., Grishman et al.
(2005), which was described above.As in newswire event detection, some joint (non pipeline-based) approaches have also been proposedfor biomedical event extraction.
For example, McClosky et al.
(2012) used a stacking model to combinethe results of applying two different methods to event extraction.
The first method is a joint method,similar to Li et al.
(2013), that detects triggers, arguments and their roles.
However, in contrast tothe structured perceptron employed in Li et al.
(2013), McClosky et al.
(2012) use a dual-decompositionapproach for the detection.
The second method is based on dependency parsing and treats event structuresas dependency trees.3 Adaptation of Biomedical Event Extraction to Newswire Event ExtractionIn this section, we firstly analyse the differences between the domain-specific ACE 2005 and GENIAevent extraction tasks.
Based on our findings, we propose an approach to adapting an existing event ex-traction method, originally developed for biomedical event extraction, to the ACE 2005 task, by resolvingthe observed differences between the two task definitions.3.1 Differences in event extraction tasksBoth the ACE 2005 and GENIA tasks concern the task of event extraction, i.e., the identification ofrelationships between entities.
For both tasks, the requirement is to extract events from text that conformto the general event description introduced earlier, i.e., a trigger and its arguments, each of which isassigned a semantic role.
Despite this high-level similarity between the tasks, their finer-grained detailsdiverge in a number of ways.
Apart from the different textual domain, the tasks adopt varying annotationschemes.
The exact kinds of annotations provided at training time are also different, as are the evaluationsettings.Several variants of the official task setting for the ACE 2005 corpus have been defined.
This is partlydue to the demanding nature of the official task definition, which requires the detection of events fromscratch, including the recognition of named entities participating in events, together with the resolutionof coreferences.
Alternative task settings (such as Ji and Grishman (2008); Liao and Grishman (2010)))generally simplify the official task definition, e.g., by omitting the requirement to perform coreferenceresolution.
A further issue is that the test data sets for the official task setting have not been made publiclyavailable.
As a result of the multiple existing variations of the ACE 2005 task definition that have beenemployed by different research efforts, direct comparison of our results with those obtained by otherstate-of-the art systems is problematic.
The solution we have chosen is to adopt the same ACE 2005event extraction task specification that has been adopted in recent research, by Hong et al.
(2011) and Liet al.
(2013).
For GENIA, we follow the specification of the original GENIA event extraction task.2272ACE 2005 GENIA# of entity types 13 (type) / 53 (subtype) 2Argument Entity/Nominal/Value/Time Entity# of event types 8 (type) / 33 (subtype) 13# of argument role types 35 7Max # of arguments for an event 11 4Nested events None PossibleOverlaps of events None PossibleCorrespondences of arguments None PossibleEntity Available (Given) Available (Partially given)Entity attributes Available (Given) Not availableEvent attributes Available (Not given) Available (Not given)Entity coreference Available (Given) Available (Not given)Event coreference Available (Not given) Not availableEvaluation Trigger/Role EventTable 1: Comparison of event definitions and event extraction tasks.
?Available annotations?
are annota-tions available in the corresponding corpus, while ?Given annotations?
are annotations provided during(training and) prediction.
?Given annotations?
do not need to be predicted during event extraction.Event annotation examples for ACE 2005 and GENIA are shown in Figures 1 and 2, respectively.Table 1 summarises the following comparison between the two event extraction tasks.Semantic types There are more event, role and entity types and a greater potential number of argumentsin ACE 2005 events than in GENIA events.
There is also a hierarchy of event types and entity typesin ACE 2005.
For example, the Life event type has Be-Born, Marry, Divorce, Injure, Die eventsubtypes.
Some GENIA event types can also be arranged to have a hierarchy but they are limited.Events in ACE 2005 can take non-entity arguments, e.g., Time.Nested events/Overlapping events Event structures are flat in ACE 2005, but they can be nested inGENIA, i.e., an event can take other events as its arguments.
Events in GENIA can also be over-lapping, in the sense that a particular word or phrase can be a trigger for multiple events.
Figure 2illustrates both nesting and overlapping in GENIA events.
These properties of GENIA events arenot addressed by methods developed for event extraction according to the ACE 2005 specification,making direct application of these methods to the GENIA task impossible.Links amongst arguments A specific feature of the GENIA event extraction task, which is completelyabsent from the ACE 2005 task, is that links amongst arguments sometimes have to be identified.For example, the Binding event type in the GENIA task can take the following argument role types:Theme, Theme2, Site and Site2.
The number 2 is attached to differentiate specific linkages betweenarguments: Site is the location of Theme, while Site2 is the location of Theme2.Entities, events and their attributes Entities in ACE 2005 have rich attributes associated with them.For example, the Time entity type has an attribute to store a normalised temporal format (e.g., 2003-03-04 for entities ?20030304?, ?March 4?
and ?Tuesday?)
while the GPE (Geo-Political Entity)type has attributes such as subtypes (e.g., Nation), mention type (proper name, common noun orpronoun), roles (location of a group or person) and style (literal or metonymic).
In contrast, GENIAentities have no attributes3.
In ACE 2005, all entities are provided (gold) in the training and testdata and they do not need to be predicted.
In GENIA, some named entities (i.e., Proteins) are alsoprovided, but other types of named or non-named entities that can constitute event arguments, suchas locations and sites of proteins, are not provided in the test data and thus need to be predictedas part of the extraction process.
Events in both corpora also have associated attributes: modality,3Types are not counted as attributes in this paper.2273polarity, genericity and tense in ACE 2005 and negation and speculation in GENIA.
The GENIAtask definition requires event attributes to be predicted, but the ACE 2005 task definition does not.Coreference Both entity and event coreference are annotated in ACE 2005, but only entity coreference isannotated in GENIA.
Events in ACE 2005 can take non-entity mentions, such as pronouns, as theirarguments.
However, events in GENIA can take only entity mentions as arguments.
Thus, insteadof non-entity mentions, coreferent entity mentions that are the closest to triggers are annotated asarguments in GENIA.
For example, in Figure 2, ?p300?
and ?Foxp3?
are annotated as Themes ofGene expression events instead of ?both proteins?.Evaluation In ACE 2005, the accuracy of extracted events is evaluated at the level of individual ar-guments and their roles.
Completeness of events is not taken into consideration (Li et al., 2013),presumably because each event can take many arguments.
Evaluation is performed by taking intoaccount the 33 event subtypes, rather than the 8 coarser-grained event types.
In contrast, evaluationof events according to the GENIA specification considers only the correctness of complete events,after nested events have been broken down.In summary, the ACE 2005 task is in some respects more complex than the GENIA task, because itconcerns a greater number event types, whose arguments may constitute a greater range of entity types,and whose semantic roles are drawn from a larger set, some of which are specific to particular eventtypes and entities.
In other respects, the task is more straightforward than the GENIA task, because ofthe simpler nature of the event structures in ACE 2005, i.e., there are no nested or overlapping eventstructures.3.2 Adaptation of event extraction methodSince event structures are simpler in ACE 2005 than GENIA, we choose to adapt a biomedical eventextraction method to the ACE 2005 task rather than the other way around.
The inverse adaptation,starting from a newswire event extraction method, is considered more complex, since we would need toextend the method to capture the more complex event structures required in the GENIA task.
It wouldadditionally be inappropriate to employ domain adaptation methods (Daum?e III and Marcu, 2006; Panand Yang, 2010) to allow GENIA-trained models to be applied to the ACE 2005 tasks.
This is becausesuch methods require that there is at least a certain degree of overlap between the target informationtypes, which is not the case in this scenario.We employ the biomedical event extraction pipeline method described in Miwa et al.
(2012) as ourstarting point.
Our motivation is that, due to their modular nature, pipeline approaches are often easierto adapt to other task settings than joint approaches, e.g., (McClosky et al., 2012; Li et al., 2013).In addition, the method has previously been shown to achieve state-of-the-art performance in severalbiomedical event extraction tasks (Miwa et al., 2012).The pipeline consists of four detectors, i.e., trigger/entity, event role, event structure, and hedge de-tectors.
The trigger/entity detector finds triggers and entities in text.
The event role detector determineswhich triggers/entities constitute arguments of events, links them to the appropriate event trigger and as-signs semantic roles to the arguments.
The event structure detector merges trigger-argument pairs into allpossible complete event structures, and determines which of these structures constitute actual events.
Thesame detector determines links between arguments, such as Theme2 and Site2.
The hedge detector findsnegation and speculation information associated with events.
Each detector solves multi-label multi-class classification problems using lexical and syntactic features obtained from multiple parsers.
Thesefeatures include character n-grams, word n-grams, and shortest paths between triggers and participantswithin parse structures.
More detailed information can be found in Miwa et al.
(2012).We have updated the original method by simplifying the format of the classification labels used byboth the event role detector and event structure detector modules.
We refer to this method as BioEE,which we have applied to the GENIA task.
We use only the role types (e.g., Theme) as classificationlabels for instances in the event role detector, instead of the more complex labels used in the originalversion of the module, which combined event types, roles and semantic entity types of arguments (e.g.,2274Binding:Theme-Protein).
Similarly, in the event structure detector, we use only two labels (?EVENT?or ?NOT-EVENT?
), instead of the previously used composite labels, which consisted of the event type,together with the roles and semantic entity types of all arguments of the event (e.g., Regulation:Cause-Protein:Theme-Protein.)
We employed the simplified labels, since they increase the number of traininginstances for each label.
The use of such labels, compared to the more complex ones, could reduce thepotential of carrying out detailed modelling of specific aspects of the task.
However, this was found notto be an issue, since the use of the simplified labels improved the performance of the pipeline in detectingevents within the GENIA development data set (about 1% improvement in F-score).
The simplification ofthe set of classification labels was also vital to ensure the tractability of the classification problems withinthe context of the ACE 2005 task.
For example, using the same conventions to formulate classificationlabels as in the original system would result in 345 possible labels (compared to 91 in GENIA) to bepredicted by the event role detector (and an even greater number of labels for the event structure detector),based on event-role-semantic type combinations found in the ACE training/development sets.In order to adapt the system to extract events according to the ACE 2005 specification, we modifiedBioEE in several ways, making changes to both the pipeline itself and the features employed by thedifferent modules.
We refer to this method as Adapted BioEE, and we applied this method to the ACE2005 task.
These changes were made in an attempt to address the two major differences between theGENIA and ACE 2005 tasks, i.e., the simpler event structures and the availability of entity attribute andcoreference information in ACE.The pipeline-based modifications consisted of removing certain modules from the original pipeline,such that only two modules remained, i.e., the trigger/entity and event role detectors.
The other twomodules of the original pipeline, i.e., the event structure and hedge detectors, were designed to deal withproblems that do not exist in the ACE 2005 extraction task, and thus their usage would be redundant.Instead of using the event structure detector to piece the different elements of an event, we simply aggre-gate all the arguments of the same trigger into a single event structure, after the event role detector hasbeen applied.As mentioned above, the ACE 2005 task definition includes rich information about entities, includingattributes and coreference information.
Existing systems developed to address this task have exploitedthis information to generate rich feature sets for classification (Liao and Grishman, 2010; Li et al.,2013).
Based on the demonstrated utility of this information within the context of event extraction, wealso choose to use it, by adding binary feature that indicate the presence of base forms, entity subtypes,and attributes of the entities and their coreferent entities to features in both detectors above.
We chooseto use base forms, since surface forms of entities are not used by most biomedical event extractionsystems, including BioEE.
We also add the features for Brown clusters (Brown et al., 1992) following Liet al.
(2013).
Further details can be found in Li et al.
(2013).4 Evaluation4.1 Evaluation settingsTo assess the performance of Adapted BioEE on the ACE 2005 task, we followed the evaluation processand settings used in previously reported studies (Hong et al., 2011; Li et al., 2013).
ACE 2005 consistsof 599 documents.
In order to facilitate direct comparison with other systems trained on the same data,we conducted a blind test on the same 40 newswire documents that were used for evaluation in (Ji andGrishman, 2008; Li et al., 2013), and used the remaining documents as training/development sets.
Weuse precision (P), recall (R) and F-score (F) to report the performance of the adapted system in classifyingtriggers and argument roles.
We use the latter F-score as our primary metric for comparing our systemwith other systems, since this score better reflects the performance of the extraction of event structures.GENIA consists of 34 full paper articles (Kim et al., 2013).
To evaluate the performance of BioEEon the GENIA task, we followed the task setting in BioNLP ST 2013 and used the official evaluationsystems provided by the organisers.
We also used the same partitioning of data that was employed inthe official BioNLP ST 2013 evaluation, with 20 articles being used as the training/development set, andthe remaining 14 articles being held back as the test set.
For brevity, we show the only the primary P,2275Arg.
Role Decomposition Event DetectionP R F P R F (%)BioEE 71.76 47.44 57.12 64.36 44.62 52.71BioEE (+Entity) 69.47 46.94 56.02 61.81 44.11 51.48EVEX 64.30 48.51 55.30 58.03 45.44 50.97TEES-2.1 62.69 49.40 55.26 56.32 46.17 50.74Table 2: Overall performance of BioEE on the GENIA data setTrigger Classification Arg.
Role Classification Event DetectionP R F P R F P R F (%)Adapted BioEE 59.9 72.6 65.7 54.2 50.2 52.1 20.7 21.7 21.2Adapted BioEE (-Entity) 57.9 71.5 64.0 51.0 48.1 49.5 19.7 19.3 19.5Li et al.
(2013) 73.7 62.3 67.5 64.7 44.4 52.7 - - -Hong et al.
(2011) 72.9 64.3 68.3 51.6 45.5 48.4 - - -Table 3: Overall performance of Adapted BioEE on the ACE 2005 data setR and F scores in the shared task, i.e., the EVENT TOTAL results obtained using the approximate span& recursive evaluation method, as recommended by the organisers.
The method individually evaluateseach complete core event, i.e., event triggers with their Theme and/or Cause role arguments, with relaxedspan matching, after nested events have been broken down as explained in Section 3.1.
Note that thescores do not count the non-named entities, hedges, and links between arguments, since only core eventsare considered in the official evaluation.We applied both a deep parser, Enju (Miyao and Tsujii, 2008) and a dependency parser, ksdep (Sagaeand Tsujii, 2007) to generate features for the ACE 2005 task, and their bio-adapted versions for theGENIA task.
We also employed the GENIA sentence splitter (S?tre et al., 2007) for sentence splitting,and the snowball (Porter2) stemmer4for stemming.
We did not make use of any other external resources,such as dictionaries, since this would hinder direct comparison of the two versions of the system.4.2 Evaluation on GENIAThe ?Event Detection?
column in Table 2 shows evaluation results of BioEE on GENIA.
The effectson performance by including entity-related features, i.e., entity base forms and Brown clustering, asintroduced in Section 3.2, are shown as ?BioEE (+Entity)?.
The inclusion of these features slightlydegrades the performance.For completeness, we also show in Table 2 the best and second best performing systems that tookpart in the official BioNLP 2013 ST evaluation: EVEX (Hakala et al., 2013) and TEES-2.1 (Bj?orne andSalakoski, 2013).
TEES-2.1 consists of a modular pipeline similar to BioEE, but it uses a different setof features.
EVEX enhances the output of TEES-2.1, by using information obtained from the results oflarge-scale event extraction.
The comparison shows that BioEE achieves state-of-the-art event extractionperformance on the GENIA task.4.3 Evaluation on ACE 2005The ?Trigger Classification?
and ?Arg.
Role Classification?
columns of Table 3 summarise the evaluationresults of the Adapted BioEE system (as described in Section 3.2) on the ACE 2005 task.We analysed the effects of incorporating features based on entity-related information into the extrac-tion process, by repeating the experiments with such features omitted (-Entity).
As can be observed inTable 3, the removal of entity-related features led to 3% performance decrease in F-score.For completeness, Table 3 also illustrates the results of state-of-the-art systems that were specifi-cally developed for ACE 2005: the system based on a joint approach (Li et al., 2013) and the pipeline-based system enhanced with web-gathered information (Hong et al., 2011).
The difference between the4snowball.tartarus.org2276Adapted BioEE and the best system is small and insignificant and the Adapted BioEE achieved perfor-mance that is comparable to or better than these other systems, in terms of the F-scores in argument roleclassification.5 DiscussionTo further investigate the differences in performance of the BioEE and Adapted BioEE systems on thetwo tasks, we evaluate the scores achieved for each task using the evaluation criteria originally designedfor the other task.
Specifically, we apply the ACE 2005 argument role classification criteria to the out-put of GENIA task, and we apply the complete event-based evaluation, originally used to evaluate theGENIA task, to the events extracted for the ACE 2005 task.
The ?Arg.
Role Decomposition?
column ofTable 2 depicts the former evaluation, while the ?Event Detection?
column of Table 3 shows the latter.Table 2 also shows the performance of the other biomedical event extraction systems introduced abovein carrying out argument role classification, since such information was provided as ?Decomposition?within the results of the original task evaluation5.
Although the results shown for ?Arg.
Role Decompo-sition?
in Table 2 are not directly comparable to those shown for ?Arg.
Role Classification?
in Table 3(given the different characteristics of GENIA and ACE 2005 tasks), the scores are broadly comparable.This demonstrates that the task of argument role classifications is equally challenging for both tasks.The ?Event Detection?
column of Table 3 illustrates event-based evaluation scores on ACE 2005.The event structure detector was added to the pipeline to facilitate comparison of the results of the twodifferent tasks in a similar setting, and performance was evaluated according to the GENIA evaluationcriteria.
Evaluation scores on ACE 2005 are unexpectedly low compared to those in Table 2.
Consideringthat the performance of argument role classification is similar in both tasks, this low performance is likelyto be due to the large number of potential event arguments in ACE 2005.
This means that, in comparisonto GENIA events, which have a small number of possible argument types, there is a greater chance thatsome arguments of more complex ACE 2005 events will fail to be detected.
According to the GENIAevaluation criteria, even if the majority of arguments has been correctly identified, the complete eventstructure will still be evaluated as incorrect.
This helps to explain why such evaluation criteria may havebeen deemed inappropriate in the original ACE 2005 evaluations.Subsequently, we analysed the effects of utilising entity-related features.
We show the results obtainedby adding entity information (+Entity) in Table 2 and the results obtained by removing entity information(-Entity) in Table 3.
The positive or negative effect on performance of adding or removing these featuresis consistent across all subtask evaluations shown in the two tables, although the exact level of perfor-mance improvement or degradation depends on the subtask under evaluation.
Overall, the inclusion ofthe features degraded the performance of BioEE on the GENIA task, but improved the performance ofAdapted BioEE on the ACE 2005 task.
These differences may be due to the increased richness of en-tity information in the ACE 2005 corpus, suggesting that enriching entities in the GENIA corpus withattribute information could be a possible way to further improve the performance of the system on thistask.6 Conclusions and Future WorkIn this paper, we have described our adaptation of a biomedical event extraction method to the newswiredomain.
We firstly evaluated the method on a biomedical event extraction task (GENIA), and showedthat its performance was superior to other state-of-the-art systems designed for the task.
We then adaptedthe method to a newswire event extraction task (ACE 2005), by addressing the major differences betweenthe tasks.
With only a small number of adaptations, the resulting system was also able to achieve state-of-the-art performance on the newswire extraction task.
These results show that there is no need to developseparate systems for event extraction tasks in different domains, as long as the types of tasks beingaddressed exhibit domain-independent features.
However, further discussion and evaluation is needed tobetter understand how different potential methods for adapting such tools from one domain to anothercan be used and/or combined effectively.5bionlp-st.dbcls.jp/GE/2013/results2277As future work, we intend to further investigate the adaptation of alternative methods proposed foruse in one domain to another domain.
Several interesting approaches have been described, such as theutilisation of contextual information beyond the boundaries of individual sentences in the newswire do-main (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and joint approaches in thebiomedical domain (McClosky et al., 2012), but their adaptability to other domains has not yet beeninvestigated.
We also intend to investigate the possibility of discovering and utilising shared informationbetween the two domains (Goldwasser and Roth, 2013).
Encouraging greater levels of communicationbetween researchers working on NLP tasks in different domains will help to stimulate such new direc-tions of research, both for event extraction and for other related information extraction tasks, such asrelation extraction and coreference resolution.AcknowledgementsThis work was supported by the Arts and Humanities Research Council (AHRC) [grant numberAH/L00982X/1], the Medical Research Council [grant number MR/L01078X/1], the European Commu-nity?s Seventh Program (FP7/2007-2013) [grant number 318736 (OSSMETER)], and the JSPS Grant-in-Aid for Young Scientists (B) [grant number 25730129].ReferencesDavid Ahn.
2006.
The stages of event extraction.
In Proceedings of the Workshop on Annotating and Reasoningabout Time and Events, pages 1?8, Sydney, Australia, July.
ACL.Jari Bj?orne and Tapio Salakoski.
2013.
Tees 2.1: Automated annotation scheme learning in the bionlp 2013 sharedtask.
In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 16?25, Sofia, Bulgaria, August.
ACL.Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.
1992.
Class-basedn-gram models of natural language.
Computational linguistics, 18(4):467?479.Nancy A. Chinchor.
1998.
Overview of MUC-7/MET-2.
In Proceedings of the 7th Message UnderstandingConference (MUC-7/MET-2).Hal Daum?e III and Daniel Marcu.
2006.
Domain adaptation for statistical classifiers.
Journal of Artificial Intelli-gence Research, 26:101?126.Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006.
Simple information extraction (sie): A portableand effective ie system.
In Proceedings of the Workshop on Adaptive Text Extraction and Mining (ATEM 2006),pages 9?16, Trento, Italy, April.
Association for Computational Linguistics.Dan Goldwasser and Dan Roth.
2013.
Leveraging domain-independent information in semantic parsing.
InProceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: ShortPapers), pages 462?466, Sofia, Bulgaria, August.
Association for Computational Linguistics.Ralph Grishman, David Westbrook, and Adam Meyers.
2005.
NYU?s english ACE 2005 system description.
InProceedings of ACE 2005 Evaluation Workshop, Washington, US.Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer, and Filip Ginter.
2013.
Evex in st?13:Application of a large-scale text mining resource to event extraction and network construction.
In Proceedingsof the BioNLP Shared Task 2013 Workshop, pages 26?34, Sofia, Bulgaria, August.
ACL.Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, and Qiaoming Zhu.
2011.
Using cross-entity in-ference to improve event extraction.
In Proceedings of the 49th ACL-HLT, pages 1127?1136, Portland, Oregon,USA, June.
ACL.Heng Ji and Ralph Grishman.
2008.
Refining event extraction through cross-document inference.
In Proceedingsof ACL-08: HLT, pages 254?262, Columbus, Ohio, June.
ACL.Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori.
2013.
The genia event extraction shared task, 2013 edition- overview.
In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15, Sofia, Bulgaria, August.ACL.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint event extraction via structured prediction with global features.
InProceedings of the 51st ACL, pages 73?82, Sofia, Bulgaria, August.
ACL.2278Shasha Liao and Ralph Grishman.
2010.
Using document level cross-event inference to improve event extraction.In Proceedings of the 48th ACL, pages 789?797, Uppsala, Sweden, July.
ACL.Wei Lu and Dan Roth.
2012.
Automatic event extraction with structured preference modeling.
In Proceedingsof the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages835?844, Jeju Island, Korea, July.
Association for Computational Linguistics.David McClosky, Sebastian Riedel, Mihai Surdeanu, Andrew McCallum, and Christopher Manning.
2012.
Com-bining joint models for biomedical event extraction.
BMC Bioinformatics, 13(Suppl 11):S9.Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012.
Boosting automatic event extraction from theliterature using domain adaptation and coreference resolution.
Bioinformatics, 28(13):1759?1765.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature forest models for probabilistic HPSG parsing.
ComputationalLinguistics, 34(1):35?80, March.Sinno Jialin Pan and Qiang Yang.
2010.
A survey on transfer learning.
IEEE Transactions on Knowledge andData Engineering, 22(10):1345?1359.Longhua Qian and Guodong Zhou.
2012.
Tree kernel-based protein?protein interaction extraction from biomedi-cal literature.
Journal of biomedical informatics, 45(3):535?543.Ellen Riloff.
1996.
Automatically generating extraction patterns from untagged text.
In Proceedings of thenational conference on artificial intelligence, pages 1044?1049.Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, YusukeMiyao, Yuichiro Matsubayashi, and Tomoko Ohta.
2007.AKANE System: Protein-protein interaction pairs in BioCreAtIvE2 Challenge, PPI-IPS subtask.
In Proceed-ings of the Second BioCreative Challenge Evaluation Workshop, pages 209?212, CNIO, Madrid, Spain, April.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency parsing and domain adaptation with LR models and parserensembles.
In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,Prague, Czech Republic, June.
ACL.Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda.
2006.
Ace 2005 multilingual trainingcorpus.
Linguistic Data Consortium.2279
