PP-Attachment Disambiguation Boosted bya Gigantic Volume of Unambiguous ExamplesDaisuke Kawahara and Sadao KurohashiGraduate School of Information Science and Technology, University of Tokyo,7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan{kawahara, kuro}@kc.t.u-tokyo.ac.jpAbstract.
We present a PP-attachment disambiguation method basedon a gigantic volume of unambiguous examples extracted from raw cor-pus.
The unambiguous examples are utilized to acquire precise lexicalpreferences for PP-attachment disambiguation.
Attachment decisions aremade by a machine learning method that optimizes the use of the lexicalpreferences.
Our experiments indicate that the precise lexical preferenceswork effectively.1 IntroductionFor natural language processing (NLP), resolving various ambiguities is a fun-damental and important issue.
Prepositional phrase (PP) attachment ambigu-ity is one of the structural ambiguities.
Consider, for example, the followingsentences [1]:(1) a. Mary ate the salad with a fork.b.
Mary ate the salad with croutons.The prepositional phrase in (1a) ?with a fork?
modifies the verb ?ate?, because?with a fork?
describes how the salad is eaten.
The prepositional phrase in (1b)?with croutons?
modifies the noun ?the salad?, because ?with croutons?
de-scribes the salad.
To disambiguate such PP-attachment ambiguity, some kind ofworld knowledge is required.
However, it is currently difficult to give such worldknowledge to computers, and this situation makes PP-attachment disambigua-tion difficult.
Recent state-of-the-art parsers perform with the practical accuracy,but seem to suffer from the PP-attachment ambiguity [2, 3].For NLP tasks including PP-attachment disambiguation, corpus-based ap-proaches have been the dominant paradigm in recent years.
They can be dividedinto two classes: supervised and unsupervised.
Supervised methods automati-cally learn rules from tagged data, and achieve good performance for many NLPtasks, especially when lexical information, such as words, is given.
Such methods,however, cannot avoid the sparse data problem.
This is because tagged data arenot sufficient enough to discriminate a large variety of lexical information.
Todeal with this problem, many smoothing techniques have been proposed.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
188?198, 2005.c?
Springer-Verlag Berlin Heidelberg 2005PP-Attachment Disambiguation Boosted by Unambiguous Examples 189The other class for corpus-based approaches is unsupervised learning.
Unsu-pervised methods take advantage of a large number of data that are extractedfrom large raw corpora, and thus can alleviate the sparse data problem.
How-ever, the problem is their low performance compared with supervised methods,because of the use of unreliable information.For PP-attachment disambiguation, both supervised and unsupervised meth-ods have been proposed, and supervised methods have achieved better perfor-mance (e.g., 86.5% accuracy by [1]).
Previous unsupervised methods tried to ex-tract reliable information from large raw corpora, but the extraction heuristicsseem to be inaccurate [4, 5].
For example, Ratnaparkhi extracted unambiguousword triples of (verb, preposition, noun) or (noun, preposition, noun), and re-ported that their accuracy was 69% [4].
This means that the extracted triplesare not truly unambiguous, and this inaccurate treatment may have led to lowPP-attachment performance (81.9%).This paper proposes a PP-attachment disambiguation method based on anenormous amount of truly unambiguous examples.
The unambiguous examplesare extracted from raw corpus using some heuristics inspired by the followingexample sentences in [6]:(2) a.
She sent him into the nursery to gather up his toys.b.
The road to London is long and winding.In these sentences, the underlined PPs are unambiguously attached to thedouble-underlined verb or noun.
The extracted unambiguous examples are uti-lized to acquire precise lexical preferences for PP-attachment disambiguation.Attachment decisions are made by a machine learning technique that optimizesthe use of the lexical preferences.
The point of our work is to use a ?gigantic?volume of ?truly?
unambiguous examples.
The use of only truly unambiguousexamples leads to statistics of high-quality and good performance of disambigua-tion in spite of the learning from raw corpus.
Furthermore, by using a giganticvolume of data, we can alleviate the influence of the sparse data problem.The remainder of this paper is organized as follows.
Section 2 briefly describesthe globally used training and test set of PP-attachment.
Section 3 summarizesprevious work for PP-attachment.
Section 4 describes a method of calculatinglexical preference statistics from a gigantic volume of unambiguous examples.Section 5 is devoted to our PP-attachment disambiguation algorithm.
Section6 presents the experiments of our disambiguation method.
Section 7 gives theconclusions.2 Tagged Data for PP-AttachmentThe PP-attachment data with correct attachment site are available 1.
These datawere extracted from Penn Treebank [7] by the IBM research group [8].
Hereafter,we call these data ?IBM data?.
Some examples in the IBM data are shownin Table 1.1 Available at ftp://ftp.cis.upenn.edu/pub/adwait/PPattachData/190 D. Kawahara and S. KurohashiTable 1.
Some Examples of the IBM datav n1 p n2 attachjoin board as director Vis chairman of N.V. Nusing crocidolite in filters Vbring attention to problem Vis asbestos in product Nmaking paper for filters Nincluding three with cancer NTable 2.
Various Baselines and Upper Bounds of PP-Attachment Disambiguationmethod accuracyalways N 59.0%N if p is ?of?
; otherwise V 70.4%most likely for each preposition 72.2%average human (only quadruple) 88.2%average human (whole sentence) 93.2%The data consist of 20,801 training and 3,097 test tuples.
In addition, a de-velopment set of 4,039 tuples is provided.
Various baselines and upper bounds ofPP-Attachment disambiguation are shown in Table 2.
All the accuracies exceptthe human performances are on the IBM data.
The human performances werereported by [8].3 Related WorkThere have been lots of supervised approaches for PP-attachment disambigua-tion.
Most of them used the IBM data for their training and test data.Ratnaphakhi et al proposed a maximum entropy model considering wordsand semantic classes of quadruples, and performed with 81.6% accuracy [8].Brill and Resnik presented a transformation-based learning method [9].
Theyreported 81.8% accuracy, but they did not use the IBM data 2.
Collins andBrooks used a probabilistic model with backing-off to smooth the probabili-ties of unseen events, and its accuracy was 84.5% [10].
Stetina and Nagao useddecision trees combined with a semantic dictionary [11].
They achieved 88.1%accuracy, which is approaching the human accuracy of 88.2%.
This great per-formance is presumably indebted to the manually constructed semantic dictio-nary, which can be regarded as a part of world knowledge.
Zavrel et al em-ployed a nearest-neighbor method, and its accuracy was 84.4% [12].
Abney etal.
proposed a boosting approach, and yielded 84.6% accuracy [13].
Vanschoen-winkel and Manderick introduced a kernel method into PP-attachment disam-2 The accuracy on the IBM data was 81.9% [10].PP-Attachment Disambiguation Boosted by Unambiguous Examples 191biguation, and attained 84.8% accuracy [14].
Zhao and Lin proposed a nearest-neighbor method with contextually similar words learned from large raw corpus[1].
They achieved 86.5% accuracy, which is the best performance among previ-ous methods for PP-attachment disambiguation without manually constructedknowledge bases.There have been several unsupervised methods for PP-attachment disam-biguation.
Hindle and Rooth extracted over 200K (v, n1, p) triples with ambigu-ous attachment sites from 13M words of AP news stories [15].
Their disambigua-tion method used lexical association score, and performed at 75.8% accuracy ontheir own data set.
Ratnaparkhi collected 910K unique unambiguous triples (v,p, n2) or (n1, p, n2) from 970K sentences of Wall Street Journal, and pro-posed a probabilistic model based on cooccurrence values calculated from thecollected data [4].
He reported 81.9% accuracy.
As previously mentioned, theaccuracy was possibly lowered by the inaccurate (69% accuracy) extracted ex-amples.
Pantel and Lin extracted ambiguous 8,900K quadruples and unambigu-ous 4,400K triples from 125M word newspaper corpus [5].
They utilized scoresbased on cooccurrence values, and resulted in 84.3% accuracy.
The accuracy ofthe extracted unambiguous triples are unknown, but depends on the accuracy oftheir parser.There is a combined method of supervised and unsupervised approaches.Volk combined supervised and unsupervised methods for PP-attachment disam-biguation for German [16].
He extracted triples that are possibly unambiguousfrom 5.5M words of a science magazine corpus, but these triples were not trulyunambiguous.
His unsupervised method is based on cooccurrence probabilitieslearned from the extracted triples.
His supervised method adopted the backed-off model by Collins and Brooks.
This model is learned the model from 5,803quadruples.
Its accuracy on a test set of 4,469 quadruples was 73.98%, and wasboosted to 80.98% by the unsupervised cooccurrence scores.
However, his workwas constrained by the availability of only a small tagged corpus, and thus itis unknown whether such an improvement can be achieved if a larger size of atagged set like the IBM data is available.4 Acquiring Precise Lexical Preferences from RawCorpusWe acquire lexical preferences that are useful for PP-attachment disambiguationfrom a raw corpus.
As such lexical preferences, cooccurrence statistics betweenthe verb and the prepositional phrase or the noun and the prepositional phraseare used.
These cooccurrence statistics can be obtained from a large raw corpus,but the simple use of such a raw corpus possibly produces unreliable statistics.We extract only truly unambiguous examples from a huge raw corpus to acquireprecise preference statistics.This section first mentions the raw corpus, and then describes how to extracttruly unambiguous examples.
Finally, we explain our calculation method of thelexical preferences.192 D. Kawahara and S. Kurohashi4.1 Raw CorpusIn our approach, a large volume of raw corpus is required.
We extracted rawcorpus from 200M Web pages that had been collected by a Web crawler fora month [17].
To obtain the raw corpus, each Web page is processed by thefollowing tools:1. sentence extractingSentences are extracted from each Web page by a simple HTML parser.2.
tokenizingSentences are tokenized by a simple tokenizer.3.
part-of-speech taggingTokenized sentences are given part-of-speech tags by Brill tagger [18].4. chunkingTagged sentences are chunked by YamCha chunker [19].By the above procedure, we acquired 1,300M chunked sentences, which con-sist of 21G words, from the 200M Web pages.4.2 Extraction of Unambiguous ExamplesUnambiguous examples are extracted from the chunked sentences.
Our heuristicsto extract truly unambiguous examples were decided in the light of the followingtwo types of unambiguous examples in [6].
(3) a.
She sent him into the nursery to gather up his toys.b.
The road to London is long and winding.The prepositional phrase ?into the nursery?
in (3a) must attach to the verb?sent?, because attachment to a pronoun like ?him?
is not possible.
The prepo-sitional phrase ?to London?
in (3b) must attach to the noun ?road?, becausethere are no preceding possible heads.We use the following two heuristics to extract unambiguous examples likethe above.?
To extract an unambiguous triple (v, p, n2) like (3a), a verb followed by apronoun and a prepositional phrase is extracted.?
To extract an unambiguous triple (n1, p, n2) like (3b), a noun phrase followedby a prepositional phrase at the beginning of a sentence is extracted.4.3 Post-processing of Extracted ExamplesThe extracted examples are processed in the following way:?
For verbs (v):?
Verbs are reduced to their lemma.?
For nouns (n1, n2):?
4-digit numbers are replaced with <year>.PP-Attachment Disambiguation Boosted by Unambiguous Examples 193?
All other strings of numbers were replaced with <num>.?
All words at the beginning of a sentence are converted into lower case.?
All words starting with a capital letter followed by one or more lowercase letters were replaced with <name>.?
All other words are reduced to their singular form.?
For prepositions (p):?
Prepositions are converted into lower case.As a result, 21M (v, p, n2) triples and 147M (n, p, n2) triples,in total 168Mtriples, were acquired.4.4 Calculation of Lexical Preferences for PP-AttachmentFrom the extracted truly unambiguous examples, lexical preferences for PP-attachment are calculated.
As the lexical preferences, pointwise mutual informa-tion between v and ?p n2?
is calculated from cooccurrence counts of v and ?pn2?
as follows3:I(v, pn2) = logf(v,pn2)Nf(v)Nf(pn2)N(1)where N denotes the total number of the extracted examples (168M), f(v) andf(pn2) is the frequency of v and ?p n2?, respectively, and f(v, pn2) is the cooc-currence frequency of v and pn2.Similarly, pointwise mutual information between n1 and ?p n2?
is calculatedas follows:I(n1, pn2) = logf(n1,pn2)Nf(n1)Nf(pn2)N(2)The preference scores ignoring n2 are also calculated:I(v, p) = logf(v,p)Nf(v)Nf(p)N(3)I(n1, p) = logf(n1,p)Nf(n1)Nf(p)N(4)5 PP-Attachment Disambiguation MethodOur method for resolving PP-attachment ambiguity takes a quadruple (v, n1, p,n2) as input, and classifies it as V or N. The class V means that the prepositional3 As in previous work, simple probability ratios can be used, but a preliminary ex-periment on the development set shows their accuracy is worse than the mutualinformation by approximately 1%.194 D. Kawahara and S. Kurohashiphrase ?p n2?
modifies the verb v. The class N means that the prepositionalphrase modifies the noun n1.To solve this binary classification task, we employ Support Vector Machines(SVMs), which have been well-known for their good generalizationperformance [20].We consider the following features:?
LEX: word of each quadrupleTo reduce sparse data problems, all verbs and nouns are pre-processed usingthe method stated in Section 4.3.?
POS: part-of-speech information of v, n1 and n2POSs of v, n1 and n2 provide richer information than just verb or noun,such as inflectional information.The IBM data, which we use for our experiments, do not contain POS in-formation.
To obtain POS tags of a quadruple, we extracted the originalsentence of each quadruple from Penn Treebank, and applied the Brill tag-ger to it.
Instead of using the correct POS information in Penn Treebank,we use the POS information automatically generated by the Brill tagger tokeep the experimental environment realistic.?
LP: lexical preferencesGiven a quadruple (v, n1, p, n2), four statistics calculated in Section4.4,I(v, pn2), I(n1, pn2), I(v, p) and I(n1, p), are given to SVMs as features.6 Experiments and DiscussionsWe conducted experiments on the IBM data.
As an SVM implementation, we em-ployed SVMlight [21].
To determine parameters of SVMlight, we run our methodon the development data set of the IBM data.
As the result, parameter j, whichis used to make much account of training errors on either class [22], is set to0.65, and 3-degree polynomial kernel is chosen.
Table 3 shows the experimen-tal results for PP-attachment disambiguation.
For comparison, we conductedseveral experiments with different feature combinations in addition to our pro-posed method ?LEX+POS+LP?, which uses all of the three types of features.The proposed method ?LEX+POS+LP?
surpassed ?LEX?, which is the stan-dard supervised model, and furthermore, significantly outperformed all otherTable 3.
PP-Attachment AccuraciesLEX POS LP accuracy?85.34?
?85.05?83.73?
?84.66?
?86.44?
?
?87.25PP-Attachment Disambiguation Boosted by Unambiguous Examples 195Table 4.
Precision and Recall for Each Attachment Site (?LEX+POS+LP?
model)class precision recallV 1067/1258 (84.82%) 1067/1271 (83.95%)N 1635/1839 (88.91%) 1635/1826 (89.54%)Table 5.
PP-Attachment Accuracies of Previous Workmethod accuracyour method SVM 87.25%supervisedRatnaphakhi et al, 1994 ME 81.6%Brill and Resnik, 1994 TBL 81.9%Collins and Brooks, 1995 back-off 84.5%Zavrel et al, 1997 NN 84.4%Stetina and Nagao, 1997 DT 88.1%Abney et al, 1999 boosting 84.6%Vanschoenwinkel and Manderick, 2003 SVM 84.8%Zhao and Lin, 2004 NN 86.5%unsupervisedRatnaparkhi, 1998 - 81.9%Pantel and Lin, 2000 - 84.3%ME: Maximum Entropy, TBL: Transformation-Based Learning,DT: Decision Tree, NN: Nearest Neighborconfigurations (McNemar?s test; p < 0.05).
?LEX+POS?
model was a littleworse than ?LEX?, but ?LEX+POS+LP?
was better than ?LEX+LP?
(andalso ?POS+LP?
was better than ?LP?).
From these results, we can see that?LP?
worked effectively, and the combination of ?LEX+POS+LP?
was very ef-fective.
Table 4 shows the precision and recall of ?LEX+POS+LP?
model foreach class (N and V).Table 5 shows the accuracies achieved by previous methods.
Our performanceis higher than any other previous methods except [11].
The method of Stetinaand Nagao employed a manually constructed sense dictionary, and this conducesto good performance.Figure 1 shows the learning curve of ?LEX?
and ?LEX+POS+LP?
modelswhile changing the number of tagged data.
When using all the training data,?LEX+POS+LP?
was better than ?LEX?by approximately 2%.
Under the con-dition of small data set, ?LEX+POS+LP?
was better than ?LEX?by approxi-mately 5%.
In this situation, in particular, the lexical preferences worked moreeffectively.Figure 2 shows the learning curve of ?LEX+POS+LP?
model while changingthe number of used unambiguous examples.
The accuracy rises rapidly by 10Munambiguous examples, and then drops once, but after that rises slightly.
Thebest score 87.28% was achieved when using 77M unambiguous examples.196 D. Kawahara and S. Kurohashi767880828486880  5000  10000  15000  20000  25000Acc urac yNumber of Tagged Data"LEX+POS+LP""LEX"Fig.
1.
Learning Curve of PP-Attachment Disambiguation85.285.485.685.88686.286.486.686.88787.287.40  2e+07  4e+07  6e+07  8e+07  1e+08  1.2e+08  1.4e+08  1.6e+08  1.8e+08Acc urac yNumber of Used Unambiguous ExamplesFig.
2.
Learning Curve of PP-Attachment Disambiguation while changing the numberof used unambiguous examples7 ConclusionsThis paper has presented a corpus-based method for PP-attachment disam-biguation.
Our approach utilizes precise lexical preferences learned from a gi-gantic volume of truly unambiguous examples in raw corpus.
Attachment deci-sions are made using a machine learning method that incorporates these lexi-cal preferences.
Our experiments indicated that the precise lexical preferencesworked effectively.PP-Attachment Disambiguation Boosted by Unambiguous Examples 197In the future, we will investigate useful contextual features for PP-attachment, because human accuracy improves by around 5% when they seemore than just a quadruple.AcknowledgementsWe would like to thank Prof. Kenjiro Taura for allowing us to use an enormousvolume of Web corpus.
We also would like to express our thanks to TomohideShibata for his constructive and fruitful discussions.References1.
Zhao, S., Lin, D.: A nearest-neighbor method for resolving pp-attachment ambigu-ity.
In: Proceedings of the 1st International Joint Conference on Natural LanguageProcessing.
(2004) 428?4342.
Collins, M.: Head-Driven Statistical Models for Natural Language Parsing.
PhDthesis, University of Pennsylvania (1999)3.
Charniak, E.: A maximum-entropy-inspired parser.
In: Proceedings of the 1stMeeting of the North American Chapter of the Association for ComputationalLinguistics.
(2000) 132?1394.
Ratnaparkhi, A.: Statistical models for unsupervised prepositional phrase attach-ment.
In: Proceedings of the 17th International Conference on ComputationalLinguistics.
(1998) 1079?10855.
Pantel, P., Lin, D.: An unsupervised approach to prepositional phrase attachmentusing contextually similar words.
In: Proceedings of the 38th Annual Meeting ofthe Association for Computational Linguistics.
(2000) 101?1086.
Manning, C., Schu?tze, H.: Foundations of Statistical Natural Language Processing.MIT Press (1999)7.
Marcus, M., Santorini, B., Marcinkiewicz, M.: Building a large annotated corpusof English: the Penn Treebank.
Computational Linguistics 19 (1994) 313?3308.
Ratnaparkhi, A., Reynar, J., Roukos, S.: A maximum entropy model for preposi-tional phrase attachment.
In: Proceedings of the ARPA Human Language Tech-nology Workshop.
(1994) 250?2559.
Brill, E., Resnik, P.: A rule-based approach to prepositional phrase attachmentdisambiguation.
In: Proceedings of the 15th International Conference on Compu-tational Linguistics.
(1994) 1198?120410.
Collins, M., Brooks, J.: Prepositional phrase attachment through a backed-offmodel.
In: Proceedings of the 3rd Workhop on Very Large Corpora.
(1995) 27?3811.
Stetina, J., Nagao, M.: Corpus based pp attachment ambiguity resolution with asemantic dictionary.
In: Proceedings of the 5th Workhop on Very Large Corpora.
(1997) 66?8012.
Zavrel, J., Daelemans, W., Veenstra, J.: Resolving pp attachment ambiguitieswith memory-based learning.
In: Proceedings of the Workshop on ComputationalNatural Language Learning.
(1997) 136?14413.
Abney, S., Schapire, R., Singer, Y.: Boosting applied to tagging and pp attach-ment.
In: Proceedings of 1999 Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very Large Corpora.
(1999) 38?45198 D. Kawahara and S. Kurohashi14.
Vanschoenwinkel, B., Manderick, B.: A weighted polynomial information gainkernel for resolving pp attachment ambiguities with support vector machines.
In:Proceedings of the 18th International Joint Conference on Artificial Intelligence.
(2003) 133?13815.
Hindle, D., Rooth, M.: Structural ambiguity and lexical relations.
ComputationalLinguistics 19 (1993) 103?12016.
Volk, M.: Combining unsupervised and supervised methods for pp attachmentdisambiguation.
In: Proceedings of the 19th International Conference on Compu-tational Linguistics.
(2002) 1065?107117.
Takahashi, T., Soonsang, H., Taura, K., Yonezawa, A.: World wide web crawler.
In:Poster Proceedings of the 11th International World Wide Web Conference.
(2002)18.
Brill, E.: Transformation-based error-driven learning and natural language process-ing: A case study in part-of-speech tagging.
Computational Linguistics 21 (1995)543?56519.
Kudo, T., Matsumoto, Y.: Chunking with support vector machines.
In: Proceed-ings of the 2nd Meeting of the North American Chapter of the Association forComputational Linguistics.
(2001) 192?19920.
Vapnik, V.: The Nature of Statistical Learning Theory.
Springer (1995)21.
Joachims, T.: 11.
In: Making Large-Scale Support Vector Machine Learning Prac-tical, in Advances in Kernel Methods - Support Vector Learning.
MIT Press (1999)169?18422.
Morik, K., Brockhausen, P., Joachims, T.: Combining statistical learning with aknowledge-based approach ?
a case study in intensive care monitoring.
In: Proceed-ings of the 16th International Conference on Machine Learning.
(1999) 268?277
