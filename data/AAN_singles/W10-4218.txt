Generating Natural Language Descriptions of Z Test CasesMaximiliano Cristia?Flowgate Consulting and CIFASISRosario, Argentinamcristia@flowgate.netBrian Plu?ssCentre for Research in ComputingThe Open UniversityMilton Keynes, UKb.pluss@open.ac.ukAbstractCritical software most often requires anindependent validation and verification(IVV).
IVV is usually performed by do-main experts, who are not familiar withspecific, many times formal, developmenttechnologies.
In addition, model-basedtesting (MBT) is a promising testing tech-nique for the verification of critical soft-ware.
Test cases generated by MBT toolsare logical descriptions.
The problem is,then, to provide natural language (NL) de-scriptions of these test cases, making themaccessible to domain experts.
In this pa-per, we present ongoing research aimed atfinding a suitable method for generatingNL descriptions from test cases in a for-mal specification language.
A first proto-type has been developed and applied to areal-world project in the aerospace sector.1 IntroductionModel-based testing (MBT) is an active researcharea and a promising theory of software and hard-ware testing (Utting and Legeard, 2006; Hieronset al, 2009).
MBT approaches start with a formalmodel or specification of the software, from whichtest cases are generated.
These techniques havebeen developed and applied to models written indifferent formal notations, such as Z (Stocks andCarrington, 1996), finite state machines and theirextensions (Grieskamp et al, 2002), B (Legeard etal., 2002), algebraic specifications (Bernot et al,1991), and so on.The fundamental hypothesis behind MBT isthat, as a program is correct if it verifies its specifi-cation, then the specification is an excellent sourceof test cases.
Once test cases are derived from themodel, they are refined to the level of the imple-mentation language and executed.
The resultingoutput is then abstracted to the level of the speci-fication language, and the model is used again toverify if the test case has detected an error.The Test Template Framework (TTF) describedby Stocks and Carrington (1996) is a particularMBT theory specially well suited for unit testing.The TTF uses Z specifications (Spivey, 1989) asthe entry models and prescribes how to generatetest cases for each operation included in the model.Fastest (Cristia?
and Rodr?
?guez Monetti, 2009) im-plements the TTF allowing users to automaticallyproduce test cases for a given Z specification.
Re-cently, we used Fastest to test an on-board satellitesoftware for a major aerospace company in SouthAmerica.
Since Fastest uses models written in theZ specification language, test cases generated bythis tool are paragraphs of formal text (see Section2).
This description is suitable for the automatictasks involved in testing (e.g., automatic execu-tion, hyperlinking, traceability), but humans needto be able to read Z specifications in order to un-derstand what is being tested.
In projects whereindependent verification and validation (IVV) isrequired this might be a problem, as most stake-holders will not necessarily be fluent in Z.This is precisely the case in the project men-tioned above, where the aerospace company re-quested not only the test cases in Z, but also inEnglish.
As it can be expected, in a project withhundreds of test cases, manual translation wouldincrease the overall cost of testing and, most criti-cally, reduce its quality due to the introduction ofhuman errors.
Interestingly, this problem is op-posite to those in mainstream industrial practice,where test cases are described in natural languageand must be formalised, in order to augment thequality and, hopefully, reduce the costs of testing.Given the formal, structured nature of thesource text, natural language generation (NLG)techniques seem to be an appropriate approachto solving this problem.
In the rest of the pa-per, we give an example of a test case from theproject mentioned above (Section 2), describe atemplate-based method for generating NL descrip-tions (Section 3), and propose further work to-wards a more general NLG solution (Section 4).2 An Example from the AerospaceIndustryThe problem of generating NL descriptions ofspecifications in Z arises in the following scenario:a company developing the software for a satelliteneeds to verify that the implementation conformsto a certain aerospace standard (ECSS, 2003) de-scribing the basic functionality of any satellitesoftware.
We therefore started by modelling in Zthe services described by the standard and used theFastest tool to generate test cases.The model is a ?standard?
Z specification: ithas a schema box that defines the state space ofthe system and operations defining the transitionrelation between states1.
Each operation formal-izes one of the services described by the standard(e.g., memory dump, telecommand verification,enabling or disabling on-board monitoring, etc.
).Figure 1 shows one of the test cases generatedfor the operation DumpMemoryAbsAdd, that mod-els a remote request for the on-board software todump some portion of its memory.
In TTF andFastest, a Z test case is essentially a set of bind-ings between variables and values, and test casesare grouped according to the operation they test.Identifiers appearing in a test case are the inputand state variables from the definition of the oper-ation.
These are bound to certain values definingthe state in which the system must be tested andthe input given to the operation in each unique testcase.
In the example, input variables are those dec-orated with a question mark, while state variablesare plain identifiers.
All these variables are de-clared somewhere else in the specification, by us-ing a special schema box called valid input space,associated with each operation.For example, the Z schema in Figure 1 indicatesthat the implementation of the dump memory ser-vice must be tested in a state where the system isprocessing a telecommand (processingTC = yes),the telecommand is a request for a memory dump(srv = DMMA), the system has one memory block1The Z specification language is essentially typed first or-der logic, with syntactic sugar in the form of operators, thatserve as shortcuts for frequently used complex expressions.DumpMemoryAbsAdd SP 7 TCASEmid = mid0 ?
srv = DMAA ?
lengths = ?processingTC = yes ?
adds = ?blocks = {mid0 7?
{1 7?
byte0, 2 7?
byte1,3 7?
byte2, 4 7?
byte3}}m?
= mid0 ?
sa?
= ?1?
?
len?
= ?2?Figure 1: A test case described in Zwhich is four bytes long (blocks = {.
.
.
}), thereare no other pending requests (adds = lengths =?
); and the request is for a memory dump oflength two (len?
= ?2?)
starting at the first ad-dress (sa?
= ?1?)
of the available memory block(m?
= mid0 = mid).Fastest generated almost 200 test cases like theone depicted in Figure 1 from a model describ-ing a simplified version of five services listed inthe standard.
The customer requested to deliver anatural language description of each one of themand a model describing all the services would havethousands of test cases.
Clearly, trying to makethe translation by hand would have been not onlya source of errors, but also a technical retreat.3 A Template-Based NLG SolutionAs a first approach, we used a template-basedmethod.
We started by defining a grammar toexpress what we called NL test case templates(NLTCT).
It appears in Figure 22.
Each NLTCTspecifies how an NL description is generated forthe Z test cases of a given operation.
It starts withthe name of the operation.
Next follows a text sec-tion, intended as a parametrized NL description ofthe test case, where calls to translation rules canbe inserted as needed.
Finally, all necessary trans-lation rules are defined, by indicating what is writ-ten in replacement for a call when a certain vari-able in the formal description of a test case appearsbound to a specific value.
In this way, a differenttext is generated for each test case, according tothe binding between values and variables that de-fines the case.
The Appendix shows the NLTCTfor the operation DumpMemoryAbsAdd.We implemented a parser in awk that takes anNLTCT and a Z test case, and generates the NLdescription of each test case in the input.
Figure 3shows the result for the test case in Figure 1.This first prototype showed that NLTCTs tend2Fastest saves formal test cases in text files written in ZLa-TeX, an extension of the LATEX markup language, what ex-plains the use of this format in the NLTCT grammar.NLTCT ::= ?Operation?
eol?NLTCD?
eol?TCRule?
{, ?TCRule?
}Operation ::= operation =?identifier?NLTCD ::= \begin{tcase} eol?LaTeXText?
eol\end{tcase}LaTeXText ::= LaTeX | ?TCRuleCall?
| ?LaTeXText?TCRuleCall ::= & rule ?identifier?
&TCRule ::= \begin{trule}{?identifier?}
eolcase ?identifier?
[, ?identifier?]
eol?RuleDef ?
eol {, ?RuleDef ?
eol}endcase eol\end{trule}RuleDef ::= $?ZLaTeX?[?
| ?
?ZLaTeX?
| & ?ZLaTeX?
]: ?LaTeX?
eolLaTeX ::= free LATEX textZLaTeX ::= free Z LATEX textFigure 2: Grammar for NLTC templatesto be relatively small and simple, in spite of thelarge number of test cases.
This is due to test casescombining a small set of values in many differ-ent ways.
However, NLTCTs for large operationstend to become increasingly more complex, forthe number of combinations grows exponentially.As a consequence, these operations require a largenumber of cases within translation rules and some-times even more translation rules3.A thorough evaluation of this method is due.
Itssuitability must be measured from the perspectiveof two kinds of users: (a) the engineers who writethe formal models, generate the formal test casesand write the NLTCTs; and (b) other stakeholders(e.g., the customer, auditors, domain experts), whoread the descriptions of the test cases in naturallanguage.
For the engineers, applying the methodshould be more efficient, in terms of time and ef-fort, than writing the descriptions by hand.
For thereaders, success will be determined by the read-ability of the output and, more critically, by itsprecision with respect to the specification.
At themoment of writing, we are designing two empiri-cal studies aimed at obtaining these measures.4 Future and Related WorkThe solution presented above was successful ingenerating adequate NL descriptions of the test3This is because templates are written in terms of the val-ues bound to variables, and not in terms of the predicates sat-isfied by those values, which are nonetheless available as partof the MBT approach.Test case: DumpMemoryAbsAdd SP 7 TCASEService (6,5) will be tested in a situation that verifies that:?
the state is such that:?
the on-board system is currently processing atelecommand and has not answered it yet.?
the service type of the telecommand isDMAA.?
the set of sequences of available memorycells contains only one sequence, associatedto a memory ID, which has four differentbytes.?
the set of starting addresses of the chunksof memory that have been requested by theground is empty.?
the input memory ID to be dumped is the avail-able memory ID, the input set of start addressesof the memory regions to be dumped is the uni-tary sequence composed of 1, the set of numbers ofmemory cells to be dumped is the unitary sequencecomposed of 2.Figure 3: NL description of the test in Figure 1cases in one particular project.
However, the lim-itations mentioned in the previous section showthat this solution would not generalise well tospecifications in other domains.
Moreover, it re-quires defining a new template for each operation;a task of still considerable size for large systems.At the same time, Z specifications contain allthe information necessary to produce the tem-plates for the operations in the system, regardlessof its domain of application.
This information isstructured according to the syntax of the formallanguage.
Additionally, when formally specifyinga system, it is common practice to include associ-ations between the identifiers in the specification(new types, operations, state schemata, variables,constants, etc.)
and the elements they refer to inthe application domain (i.e., aerospace software).These associations are called designations (Jack-son, 1995), some of which, relevant to the test casein Figure 1, are shown in Figure 4.These considerations lead us to believe in thesrv?Service type of the telecommandDMAA?Dump memory using absolute addressesprocessingTC?The on-board system is currently processinga telecommand and has not answered it yetm?
?Memory ID to be dumpedsa?
?Start addresses of the memory regions to bedumpedlen?
?The number of memory cells to be dumpedfor each start addressFigure 4: Designations for the test in Figure 1possibility of generating NL descriptions of Z testcases automatically by using their definitions, thesystem specification and the designations of iden-tifiers.
Such a solution would be independent ofthe application domain and, more importantly, ofthe number of operations in the model.The linguistic properties of the target documentare relevant in devising an adequate treatment forthe input, but the overall structure of the output re-mains rigid and its content is determined by thedefinition of each test case.
The approach wouldstill be template-based, but in terms of the NLGarchitecture of Reiter and Dale (2000), templateswould be defined at the level of the documentstructure4, with minimal microplanning and sur-face strings generated according to the part of thetest case being processed and the designations ofthe identifiers5.
The next stages of our project willpoint in this direction, using techniques from NLGfor automating the definition of the templates pre-sented in the previous section.There have been efforts for producing nat-ural language versions of formal specificationsin the past.
Punshon et al (1997) use a casestudy to present the REVIEW system (Salek etal., 1994)6. REVIEW automatically paraphrasesspecifications developed with Metaview (Soren-son et al, 1988), an academic research metasys-tem that facilitates the construction of CASE envi-ronments to support software specification tasks.Coscoy (1997) describes a mechanism based onprogram extraction, for generating explanations offormal proofs in the Calculus of Inductive Con-structions, implemented in the Coq Proof Assis-tant (Bertot and Caste?ran, 2004).
Lavoie et al(1997) present MODEX, a tool that generates cus-tomizable descriptions of the relations betweenclasses in object-oriented models specified in theODL standard (Cattell and Barry, 1997).
Bertaniet al (1999) describe a controlled natural languageapproach to translating formal specifications writ-ten in an extension of TRIO (Ghezzi et al, 1990)by transforming syntactic trees in TRIO into syn-tactic trees of the controlled language.The solutions presented in the related workabove are highly dependant on particular aspects4Somewhat along the lines of what Wilcock (2005) de-scribes for XML-based NLG.5This approach is similar to the method proposed by Kit-tredge and Lavoie (1998) for generating weather forecasts.6Salek et al (1994) also give a comprehensive survey ofrelated work for generating NL explanations for particularspecification languages (most of which are now obsolete).of the source language and do not apply directlyto specifications written in Z.
To our knowledge,no work has been done towards producing NL de-scriptions of Z specifications.
The same holds fortest cases generated using the MBT approach.5 ConclusionIn this paper we presented a concrete NLG prob-lem in the area of software development involv-ing formal methods.
We focused the descriptionon the generation of NL descriptions of test cases,but nothing prevents us from extending the idea toentire system specifications.The development of a general technique for ver-balising formal specification would fill the com-munication gap between system designers andother stakeholders in the development process,while preserving the advantages associated to theuse of formal methods: precision, lack of ambigu-ity, formal proof of system properties, etc.Finally, we hope this paper draws attention fromNLG experts to an area which would benefit sub-stantially from their expertise.AcnowledgementsA substantial part of this research was funded byFlowgate Consulting.
We would also like to thankRichard Power from the NLG group at The OpenUniversity for help in finding financial support,Eva Banik for helpful comments on earlier ver-sions of this paper, and three anonymous reviewersfor useful feedback and suggestions.ReferencesG.
Bernot, M.C.
Gaudel, and B. Marre.
1991.
Soft-ware testing based on formal specifications: a the-ory and a tool.
Software Engineering Journal (SEJ),6(6):387?405.A.
Bertani, W. Castelnovo, E. Ciapessoni, and G.Mauri.
1999.
Natural language translations of for-mal specifications for complex industrial systems.In AI*IA 1992: Proceedings of the 6th Congressof the Italian Association for Artificial Intelligence,pages 185?194, Bologna, Italy.Y.
Bertot and P. Caste?ran.
2004.
Interactive TheoremProving and Program Development.
Coq?Art: TheCalculus of Inductive Constructions.
Texts in Theo-retical Computer Science.
Springer-Verlag.R.G.G.
Cattell and D.K.
Barry, editors.
1997.
The ob-ject database standard: ODMG 2.0.
Morgan Kauf-mann Publishers Inc., San Francisco, CA.Y.
Coscoy.
1997.
A natural language explanation forformal proofs.
In LACL ?96: Selected papers fromthe First International Conference on Logical As-pects of Computational Linguistics, pages 149?167,London, UK.
Springer-Verlag.M.
Cristia?
and P.
Rodr?
?guez Monetti.
2009.
Imple-menting and applying the Stocks-Carrington frame-work for model-based testing.
In Karin Breitmanand Ana Cavalcanti, editors, ICFEM, volume 5885of Lecture Notes in Computer Science, pages 167?185.
Springer-Verlag.ECSS.
2003.
Space Engineering ?
Ground Sys-tems and Operations: Telemetry and TelecommandPacket Utilization.
Technical Report ECSS-E-70-41A, European Space Agency.C.
Ghezzi, D. Mandrioli, and A. Morzenti.
1990.TRIO: A logic language for executable specifica-tions of real-time systems.
Journal of Systems andSoftware, 12(2):107?123.W.
Grieskamp, Y. Gurevich, W. Schulte, and M.Veanes.
2002.
Generating finite state machinesfrom abstract state machines.
In ISSTA ?02: Pro-ceedings of the 2002 ACM SIGSOFT InternationalSymposium on Software Testing and Analysis, pages112?122, Rome, Italy.R.M.
Hierons, K. Bogdanov, J.P. Bowen, R. Cleave-land, J. Derrick, et al 2009.
Using formal specifi-cations to support testing.
ACM Computing Surveys(CSUR), 41(2):9.M.
Jackson.
1995.
Software requirements & specifi-cations: a lexicon of practice, principles, and preju-dices.
Addison-Wesley.R.
Kittredge and B. Lavoie.
1998.
Meteocogent: Aknowledge-based tool for generating weather fore-cast texts.
In Proceedings of American Meteorolog-ical Society AI Conference (AMS-98), Phoenix, AZ.B.
Lavoie, O. Rambow, and E. Reiter.
1997.
Cus-tomizable descriptions of object-oriented models.
InProceedings of the Conference on Applied NaturalLanguage Processing (ANLP?97, pages 253?256,Washington, DC.B.
Legeard, F. Peureux, and M. Utting.
2002.
A Com-parison of the BTT and TTF Test-Generation Meth-ods.
In ZB ?02: Proceedings of the 2nd Interna-tional Conference of B and Z Users on Formal Spec-ification and Development in Z and B, pages 309?329, London, UK.
Springer-Verlag.J.M.
Punshon, J.P Tremblay, P.G.
Sorenson, and P.S.Findeisen.
1997.
From formal specifications to nat-ural language: A case study.
In 12th IEEE Interna-tional Conference Automated Software Engineering,pages 309?310.E.
Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press, Cambridge, UK.A.
Salek, P.G.
Sorenson, J.P. Tremblay, and J.M.
Pun-shon.
1994.
The REVIEW system: From formalspecifications to natural language.
In Proceedings ofthe First International Conference on RequirementsEngineering, pages 220?229.P.G.
Sorenson, J.P. Tremblay, and A.J.
McAllister.1988.
The Metaview system for many specificationenvironments.
IEEE Software, 5(2):30?38.J.M.
Spivey.
1989.
The Z Notation: A Reference Man-ual.
Prentice-Hall, Inc.P.
Stocks and D. Carrington.
1996.
A Framework forSpecification-Based Testing.
IEEE Transactions onSoftware Engineering, 22(11):777?793.M.
Utting and B. Legeard.
2006.
Practical Model-Based Testing: A Tools Approach.
Morgan Kauf-mann Publishers Inc., San Francisco, CA.G.
Wilcock.
2005.
An Overview of Shallow XML-based Natural Language Generation.
In Proceed-ings of the 2nd Baltic Conference on Human Lan-guage Technolgies, pages 67?78, Tallinn, Estonia.Appendix A. NLTCT for the ExampleNLTCT for DumpMemoryAbsAdd (some partswere replaced by [...] due to space restrictions):operation = DumpMemoryAbsAdd\begin{tcase}\centerline{{\bf Test case: \ltcaseid}}The service (6,5) will be tested in thesituation that verifies that:\begin{itemize}\item the state is such that:\begin{itemize}\item the on-board system is &trule PTCr&.\item the service type of the telecommandis &trule SRVr&.[...
]\item the set of starting addresses of thechunks of memory that have beenrequested by the ground is &truleADSr&.\end{itemize}[...]\end{itemize}\end{tcase}\begin{trule}{PTCr}case processingTC$yes :currently processing a telecommand andhas not answered it yet$no :not currently processing a telecommandendcase\end{trule}\begin{trule}{SRVr}case srv$* :*endcase\end{trule}\begin{trule}{ADSr}case adds$\emptyset :empty$\langle 0 \rangle :the unitary sequencecomposed of 0endcase\end{trule}[...]
