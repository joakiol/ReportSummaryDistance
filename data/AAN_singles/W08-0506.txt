Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 23?30,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSoftware testing and the naturally occurring dataassumption in natural language processing?K.
Bretonnel Cohen William A. Baumgartner, Jr. Lawrence HunterAbstractIt is a widely accepted belief in natural lan-guage processing research that naturally oc-curring data is the best (and perhaps the onlyappropriate) data for testing text mining sys-tems.
This paper compares code coverage us-ing a suite of functional tests and using a largecorpus and finds that higher class, line, andbranch coverage is achieved with structuredtests than with even a very large corpus.1 IntroductionIn 2006, Geoffrey Chang was a star of the proteincrystallography world.
That year, a crucial compo-nent of his code was discovered to have a simpleerror with large consequences for his research.
Thenature of the bug was to change the signs (positiveversus negative) of two columns of the output.
Theeffect of this was to reverse the predicted ?handed-ness?
of the structure of the molecule?an impor-tant feature in predicting its interactions with othermolecules.
The protein for his work on which Changwas best known is an important one in predictingthings like human response to anticancer drugs andthe likelihood of bacteria developing antibiotic re-sistance, so his work was quite influential and heav-ily cited.
The consequences for Chang were thewithdrawal of 5 papers in some of the most presti-gious journals in the world.
The consequences forthe rest of the scientific community have not been?K.
Bretonnel Cohen is with The MITRE Corporation.
Allthree co-authors are at the Center for Computational Pharma-cology in the University of Colorado School of Medicine.quantified, but were substantial: prior to the retrac-tions, publishing papers with results that did notjibe with his model?s predictions was difficult, andobtaining grants based on preliminary results thatseemed to contradict his published results was dif-ficult as well.
The Chang story (for a succinct dis-cussion, see (Miller, 2006), and see (Chang et al,2006) for the retractions) is an object illustration ofthe truth of Rob Knight?s observation that ?For sci-entific work, bugs don?t just mean unhappy userswho you?ll never actually meet: they mean retractedpublications and ended careers.
It is critical thatyour code be fully tested before you draw conclu-sions from results it produces?
(personal communi-cation).
Nonetheless, the subject of software testinghas been largely neglected in academic natural lan-guage processing.
This paper addresses one aspectof software testing: the monitoring of testing effortsvia code coverage.1.1 Code coverageCode coverage is a numerical assessment of theamount of code that is executed during the runningof a test suite (McConnell, 2004).
Although it isby no means a completely sufficient method for de-termining the completeness of a testing effort, it isnonetheless a helpful member of any suite of met-rics for assessing testing effort completeness.
Codecoverage is a metric in the range 0-1.0.
A value of0.86 indicates that 86% of the code was executedwhile running a given test suite.
100% coverage isdifficult to achieve for any nontrivial application, butin general, high degrees of ?uncovered?
code shouldlead one to suspect that there is a large amount of23code that might harbor undetected bugs simply dueto never having been executed.
A variety of codecoverage metrics exist.
Line coverage indicates theproportion of lines of code that have been executed.It is not the most revealing form of coverage assess-ment (Kaner et al, 1999, p. 43), but is a basic partof any coverage measurement assessment.
Branchcoverage indicates the proportion of branches withinconditionals that have been traversed (Marick, 1997,p.
145).
For example, for the conditional if $a&& $b, there are two possible branches?one is tra-versed if the expression evaluates to true, and theother if it evaluates to false.
It is more informativethan line coverage.
Logic coverage (also known asmulticondition coverage (Myers, 1979) and condi-tion coverage (Kaner et al, 1999, p. 44) indicates theproportion of sets of variable values that have beentried?a superset of the possible branches traversed.For example, for the conditional if $a || $b,the possible cases (assuming no short-circuit logic)are those of the standard (logical) truth table for thatconditional.
These coverage types are progressivelymore informative than line coverage.
Other types ofcoverage are less informative than line coverage.
Forexample, function coverage indicates the proportionof functions that are called.
There is no guaranteethat each line in a called function is executed, and allthe more so no guarantee that branch or logic cov-erage is achieved within it, so this type of coverageis weaker than line coverage.
With the advent ofobject-oriented programming, function coverage issometimes replaced by class coverage?a measureof the number of classes that are covered.We emphasize again that code coverage is nota sufficient metric for evaluating testing complete-ness in isolation?for example, it is by definitionunable to detect ?errors of omission,?
or bugs thatconsist of a failure to implement needed functional-ity.
Nonetheless, it remains a useful part of a largersuite of metrics, and one study found that testing inthe absence of concurrent assessment of code cov-erage typically results in only 50-60% of the codebeing executed ((McConnell, 2004, p. 526), citingWiegers 2002).We set out to question whether a dominant, if of-ten unspoken, assumption of much work in contem-porary NLP holds true: that feeding a program alarge corpus serves to exercise it adequately.
We be-gan with an information extraction application thathad been built for us by a series of contractors, withthe contractors receiving constant remote oversightand guidance but without ongoing monitoring of theactual code-writing.
The application had benefittedfrom no testing other than that done by the develop-ers.
We used a sort of ?translucent-box?
or ?gray-box?
paradigm, meaning in this case that we treatedthe program under test essentially as a black boxwhose internals were inaccessible to us, but with theexception that we inserted hooks to a coverage tool.We then monitored three types of coverage?linecoverage, branch coverage, and class coverage?under a variety of contrasting conditions:?
A set of developer-written functional tests ver-sus a large corpus with a set of semantic rulesoptimized for that corpus.?
Varying the size of the rule set.?
Varying the size of the corpus.We then looked for coverage differences betweenthe various combinations of input data and rule sets.In this case, the null hypothesis is that no differenceswould be observed.
In contrast with the null hypoth-esis, the unspoken assumption in much NLP workis that the null hypothesis does not hold, that theprimary determinant of coverage will be the size ofthe corpus, and that the observed pattern will be thatcoverage is higher with the large corpus than whenthe input is not a large corpus.2 Methods and materials2.1 The application under testThe application under test was an information ex-traction application known as OpenDMAP.
It is de-scribed in detail in (Hunter et al, 2008).
It achievedthe highest performance on one measure of theprotein-protein interaction task in the BioCreativeII shared task (Krallinger et al, 2007).
Its use inthat task is described specifically in (BaumgartnerJr.
et al, In press).
It contains 7,024 lines of codespread across three packages (see Table 1).
Onemajor package deals with representing the seman-tic grammar rules themselves, while the other dealswith applying the rules to and extracting data fromarbitrary textual input.
(A minor package deals with24Component Lines of codeParser 3,982Rule-handling 2,311Configuration 731Total 7,024Table 1: Distribution of lines of code in the application.the configuration files and is mostly not discussed inthis paper.
)The rules and patterns that the system uses aretypical ?semantic grammar?
rules in that they allowthe free mixing of literals and non-terminals, withthe non-terminals typically representing domain-specific types such as ?protein interaction verb.
?Non-terminals are represented as classes.
Thoseclasses are defined in a Prote?ge?
ontology.
Rules typ-ically contain at least one element known as a slot.Slot-fillers can be constrained by classes in the on-tology.
Input that matches a slot is extracted as oneof the participants in a relation.
A limited regularexpression language can operate over classes, liter-als, or slots.
The following is a representative rule.Square brackets indicate slots, curly braces indicatea class, the question-mark is a regular expression op-erator, and any other text is a literal.
{c-interact} := [interactor1]{c-interact-verb} the?
[interactor2]The input NEF binds PACS-2 (PMID 18296443)would match that rule.
The result would be therecognition of a protein interaction event, with in-teractor1 being NEF and interactor2 being PACS-2.Not all rules utilize all possibilities of the rule lan-guage, and we took this into account in one of ourexperiments; we discuss the rules further later in thepaper in the context of that experiment.2.2 MaterialsIn this work, we made use of the following sets ofmaterials.?
A large data set distributed as training data forpart of the BioCreative II shared task.
It is de-scribed in detail in (Krallinger et al, 2007).Briefly, its domain is molecular biology, andin particular protein-protein interactions?animportant topic of research in computationalTest type Number of testsBasic 85Pattern/rule 67Patterns only 90Slots 9Slot nesting 7Slot property 20Total 278Table 2: Distribution of functional tests.bioscience, with significance to a wide rangeof topics in biology, including understandingthe mechanisms of human diseases (Kann etal., 2006).
The corpus contained 3,947,200words, making it almost an order of mag-nitude larger than the most commonly usedbiomedical corpus (GENIA, at about 433Kwords).
This data set is publicly available viabiocreative.sourceforge.net.?
In conjunction with that data set: a set of 98rules written in a data-driven fashion by man-ually examining the BioCreative II data de-scribed just above.
These rules were used in theBioCreative II shared task, where they achievedthe highest score in one category.
The set ofrules is available on our SourceForge site atbionlp.sourceforge.net.?
A set of functional tests created by the primarydeveloper of the system.
Table 2 describes thebreakdown of the functional tests across vari-ous aspects of the design and functionality ofthe application.2.3 Assessing coverageWe used the open-source Cobertura tool(Mark Doliner, personal communication;cobertura.sourceforge.net) to mea-sure coverage.
Cobertura reports line coverage andbranch coverage on a per-package basis and, withineach package, on a per-class basis1.The architecture of the application is such thatCobertura?s per-package approach resulted in three1Cobertura is Java-specific.
PyDEV provides code coverageanalysis for Python, as does Coverage.py.25sets of coverage reports: for the configuration fileprocessing code, for the rule-processing code, andfor the parser code.
We report results for the appli-cation as a whole, for the parser code, and for therule-processing code.
We did note differences in theconfiguration code coverage for the various condi-tions, but it does not change the overall conclusionsof the paper and is omitted from most of the discus-sion due to considerations of space and of generalinterest.3 ResultsWe conducted three separate experiments.3.1 The most basic experiment: test suiteversus corpusIn the most basic experiment, we contrastedclass, line, and branch coverage when running thedeveloper-constructed test suite and when runningthe corpus and the corpus-based rules.
Tables 3 and4 show the resulting data.
As the first two linesof Table 3 show, for the entire application (parser,rule-handling, and configuration), line coverage washigher with the test suite?56% versus 41%?andbranch coverage was higher as well?41% versus28% (see the first two lines of Table 3).We give here a more detailed discussion of the re-sults for the entire code base.
(Detailed discussionsfor the parser and rule packages, including granularassessments of class coverage, follow.
)For the parser package:?
Class coverage was higher with the test suitethan with the corpus?88% (22/25) versus 80%(20/25).?
For the entire parser package, line coveragewas higher with the test suite than with thecorpus?55% versus 41%.?
For the entire parser package, branch cover-age was higher with the test suite than with thecorpus?57% versus 29%.Table 4 gives class-level data for the two mainpackages.
For the parser package:?
Within the 25 individual classes of the parserpackage, line coverage was equal or greaterwith the test suite for 21/25 classes; it was notjust equal but greater for 14/25 classes.?
Within those 21 of the 25 individual classesthat had branching logic, branch coverage wasequal or greater with the test suite for 19/21classes, and not just equal but greater for 18/21classes.For the rule-handling package:?
Class coverage was higher with the test suitethan with the corpus?100% (20/20) versus90% (18/20).?
For the entire rules package, line coverage washigher with the test suite than with the corpus?63% versus 42%.?
For the entire rules package, branch coveragewas higher with the test suite than with thecorpus?71% versus 24%.Table 4 gives the class-level data for the rulespackage:?
Within the 20 individual classes of the rulespackage, line coverage was equal or greaterwith the test suite for 19/20 classes, and not justequal but greater for 6/20 classes.?
Within those 11 of the 20 individual classesthat had branching logic, branch coverage wasequal or greater with the test suite for all11/11 classes, and not just equal but greater for(again) all 11/11 classes.3.2 The second experiment: Varying the size ofthe rule setPilot studies suggested (as later experiments veri-fied) that the size of the input corpus had a negligibleeffect on coverage.
This suggested that it would beworthwhile to assess the effect of the rule set on cov-erage independently.
We used simple ablation (dele-tion of portions of the rule set) to vary the size of therule set.We created two versions of the original rule set.We focussed only on the non-lexical, relational pat-tern rules, since they are completely dependent onthe lexical rules.
Each version was about half the26Metric Functional tests Corpus, all rules nominal rules verbal rulesOverall line coverage 56% 41% 41% 41%Overall branch coverage 41% 28% 28% 28%Parser line coverage 55% 41% 41% 41%Parser branch coverage 57% 29% 29% 29%Rules line coverage 63% 42% 42% 42%Rules branch coverage 71% 24% 24% 24%Parser class coverage 88% (22/25) 80% (20/25)Rules class coverage 100% (20/20) 90% (18/20)Table 3: Application and package-level coverage statistics using the developer?s functional tests, the full corpus withthe full set of rules, and the full corpus with two reduced sets of rules.
The highest value in a row is bolded.
The finalthree columns are intentionally identical (see explanation in text).Package Line coverage >= Line coverage > Branch coverage >= Branch coverage >Classes in parser package 21/25 14/25 19/21 18/21Classes in rules package 19/20 6/20 11/11 11/11Table 4: When individual classes were examined, both line and branch coverage were always higher with the functionaltests than with the corpus.
This table shows the magnitude of the differences.
>= indicates the number of classes thathad equal or greater coverage with the functional tests than with the corpus, and > indicates just the classes that hadgreater coverage with the functional tests than with the corpus.size of the original set.
The first consisted of thefirst half of the rule set, which happened to consistprimarily of verb-based patterns.
The second con-sisted of the second half of the rule set, which corre-sponded roughly to the nominalization rules.The last two columns of Table 3 show thepackage-level results.
Overall, on a per-package ba-sis, there were no differences in line or branch cov-erage when the data was run against the full rule setor either half of the rule set.
(The identity of the lastthree columns is due to this lack of difference in re-sults between the full rule set and the two reducedrule sets.)
On a per-class level, we did note minordifferences, but as Table 3 shows, they were withinrounding error on the package level.3.3 The third experiment: Coverage closureIn the third experiment, we looked at how cover-age varies as increasingly larger amounts of the cor-pus are processed.
This methodology is compara-ble to examining the closure properties of a corpusin a corpus linguistics study (see e.g.
Chapter 6 of(McEnery and Wilson, 2001)) (and as such may besensitive to the extent to which the contents of thecorpus do or do not fit the sublanguage model).
Wecounted cumulative line coverage as increasinglylarge amounts of the corpus were processed, rang-ing from 0 to 100% of its contents.
The results forline coverage are shown in Figure 1.
(The results forbranch coverage are quite similar, and the graph isnot shown.)
Line coverage for the entire applicationis indicated by the thick solid line.
Line coveragefor the parser package is indicated by the thin solidline.
Line coverage for the rules package is indi-cated by the light gray solid line.
The broken lineindicates the number of pattern matches?quantitiesshould be read off of the right y axis.The figure shows quite graphically the lack of ef-fect on coverage of increasing the size of the cor-pus.
For the entire application, the line coverage is27% when an empty document has been read in, and39% when a single sentence has been processed; itincreases by one to 40% when 51 sentences havebeen processed, and has grown as high as it everwill?41%?by the time 1,000 sentences have beenprocessed.
Coverage at 191,478 sentences?that is,3,947,200 words?is no higher than at 1,000 sen-tences, and barely higher, percentagewise, than at asingle sentence.An especially notable pattern is that the huge rise27Figure 1: Increase in percentage of line coverage as in-creasing amounts of the corpus are processed.
Left y axisis the percent coverage.
The x axis is the number of sen-tences.
Right y axis (scale 0-12,000) is the number ofrule matches.
The heavy solid line is coverage for the en-tire package, the thin solid line is coverage for the parserpackage, the light gray line is coverage for the rules pack-age, and the broken line is the number of pattern matches.in the number of matches to the rules (graphed bythe broken line) between 5,000 sentences and 191Ksentences has absolutely no effect on code coverage.4 DiscussionThe null hypothesis?that a synthetic test suiteand a naturalistic corpus provide the same codecoverage?is not supported by the data shown here.Furthermore, the widely, if implicitly, held assump-tion that a corpus would provide the best testing datacan be rejected, as well.
The results reported hereare consistent with the hypothesis that code cover-age for this application is not affected by the size ofthe corpus or by the size of the rule set, and that run-ning it on a large corpus does not guarantee thoroughtesting.
Rather, coverage is optimized by traditionalsoftware testing.4.1 Related workAlthough software testing is a first-class researchobject in computer science, it has received little at-tention in the natural language processing arena.
Anotable exception to this comes from the grammarengineering community.
This has produced a bodyof publications that includes Oepen?s work on testsuite design (Oepen et al, 1998), Volk?s work on testsuite encoding (Volk, 1998), Oepen et al?s work onthe Redwoods project (Oepen et al, 2002), Butt andKing?s discussion of the importance of testing (Buttand King, 2003), Flickinger et al?s work on ?seman-tics debugging?
with Redwoods data (Flickinger etal., 2005), and Bender et al?s recent work on testsuite generation (Bender et al, 2007).
Outside ofthe realm of grammar engineering, work on test-ing for NLP is quite limited.
(Cohen et al, 2004)describes a methodology for generating test suitesfor molecular biology named entity recognition sys-tems, and (Johnson et al, 2007) describes the de-velopment of a fault model for linguistically-basedontology mapping, alignment, and linking systems.However, when most researchers in the NLP com-munity refer in print to ?testing,?
they do not meanit in the sense in which that term is used in soft-ware engineering.
Some projects have publicized as-pects of their testing work, but have not published ontheir approaches: the NLTK project posts module-level line coverage statistics, having achieved me-dian coverage of 55% on 116 Python modules2 and38% coverage for the project as a whole; the MAL-LET project indicates on its web site that it en-courages the production of unit tests during devel-opment, but unfortunately does not go into detailsof their recommendations for unit-testing machinelearning code3.4.2 ConclusionsWe note a number of shortcomings of code cov-erage.
For example, poor coding conventionscan actually inflate your line coverage.
Con-sider a hypothetical application consisting onlyof the following, written as a single line of codewith no line breaks: if (myVariable ==1) doSomething elsif (myVariable== 2) doSomethingElse elsif(myVariable = 3) doYetAnotherThingand a poor test suite consisting only of inputs thatwill cause myVariable to ever have the value 1.The test suite will achieve 100% line coverage for2nltk.org/doc/guides/coverage3mallet.cs.umass.edu/index.php/Guidelines for writing unit tests28this application?and without even finding the errorthat sets myVariable to 3 if it is not valued 1or 2.
If the code were written with reasonable linebreaks, code coverage would be only 20%.
And,as has been noted by others, code coverage can notdetect ?sins of omission?
?bugs that consist of thefailure to write needed code (e.g.
for error-handlingor for input validation).
We do not claim that codecoverage is wholly sufficient for evaluating a testsuite; nonetheless, it is one of a number of metricsthat are helpful in judging the adequacy of a testingeffort.
Another very valuable one is the found/fixedor open/closed graph (Black, 1999; Baumgartner Jr.et al, 2007).While remaining aware of the potential shortcom-ings of code coverage, we also note that the datareported here supports its utility.
The developer-written functional tests were produced without mon-itoring code coverage; even though those tests rou-tinely produced higher coverage than a large corpusof naturalistic text, they achieved less than 60% cov-erage overall, as predicted by Wiegers?s work citedin the introduction.
We now have the opportunity toraise that coverage via structured testing performedby someone other than the developer.
In fact, ourfirst attempts to test the previously unexercised codeimmediately uncovered two showstopper bugs; thecoverage analysis also led us to the discovery thatthe application?s error-handling code was essentiallyuntested.Although we have explored a number of dimen-sions of the space of the coverage phenomenon, ad-ditional work could be done.
We used a relativelynaive approach to rule ablation in the second experi-ment; a more sophisticated approach would be to ab-late specific types of rules?for example, ones thatdo or don?t contain slots, ones that do or don?t con-tain regular expression operators, etc.
?and monitorthe coverage changes.
(We did run all three experi-ments on a separate, smaller corpus as a pilot study;we report the results for the BioCreative II data setin this paper since that is the data for which the ruleswere optimized.
Results in the pilot study were en-tirely comparable.
)In conclusion: natural language processing appli-cations are particularly susceptible to emergent phe-nomena, such as interactions between the contentsof a rule set and the contents of a corpus.
Theseare especially difficult to control when the evalua-tion corpus is naturalistic and the rule set is data-driven.
Structured testing does not eliminate thisemergent nature of the problem space, but it doesallow for controlled evaluation of the performanceof your system.
Corpora also are valuable evalua-tion resources: the combination of a structured testsuite and a naturalistic corpus provides a powerfulset of tools for finding bugs in NLP applications.AcknowledgmentsThe authors thank James Firby, who wrote the func-tional tests, and Helen L. Johnson, who wrote therules that were used for the BioCreative data.
SteveBethard and Aaron Cohen recommended Pythoncoverage tools.
We also thank the three anonymousreviewers.ReferencesWilliam A. Baumgartner Jr., K. Bretonnel Cohen, LynneFox, George K. Acquaah-Mensah, and LawrenceHunter.
2007.
Manual curation is not sufficientfor annotation of genomic databases.
Bioinformatics,23:i41?i48.William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-mann, Elizabeth K. White, Olga Medvedeva, K. Bre-tonnel Cohen, and Lawrence Hunter.
In press.
Con-cept recognition for extracting protein interaction rela-tions from biomedical text.
Genome Biology.Emily M. Bender, Laurie Poulson, Scott Drellishak, andChris Evans.
2007.
Validation and regression test-ing for a cross-linguistic grammar resource.
In ACL2007 Workshop on Deep Linguistic Processing, pages136?143, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Rex Black.
1999.
Managing the Testing Process.Miriam Butt and Tracy Holloway King.
2003.
Grammarwriting, testing and evaluation.
In Ali Farghaly, editor,A handbook for language engineers, pages 129?179.CSLI.Geoffrey Chang, Christopher R. Roth, Christopher L.Reyes, Owen Pornillos, Yen-Ju Chen, and Andy P.Chen.
2006. Letters: Retraction.
Science, 314:1875.K.
Bretonnel Cohen, Lorraine Tanabe, Shuhei Kinoshita,and Lawrence Hunter.
2004.
A resource for construct-ing customized test suites for molecular biology entityidentification systems.
In HLT-NAACL 2004 Work-shop: BioLINK 2004, Linking Biological Literature,Ontologies and Databases, pages 1?8.
Association forComputational Linguistics.29Dan Flickinger, Alexander Koller, and Stefan Thater.2005.
A new well-formedness criterion for semanticsdebugging.
In Proceedings of the HPSG05 Confer-ence.Lawrence Hunter, Zhiyong Lu, James Firby, WilliamA.
Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,and K. Bretonnel Cohen.
2008.
OpenDMAP: Anopen-source, ontology-driven concept analysis engine,with applications to capturing knowledge regardingprotein transport, protein interactions and cell-specificgene expression.
BMC Bioinformatics, 9(78).Helen L. Johnson, K. Bretonnel Cohen, and LawrenceHunter.
2007.
A fault model for ontology mapping,alignment, and linking systems.
In Pacific Sympo-sium on Biocomputing, pages 233?244.
World Scien-tific Publishing Company.Cem Kaner, Hung Quoc Nguyen, and Jack Falk.
1999.Testing computer software, 2nd edition.
John Wileyand Sons.Maricel Kann, Yanay Ofran, Marco Punta, and PredragRadivojac.
2006.
Protein interactions and disease.
InPacific Symposium on Biocomputing, pages 351?353.World Scientific Publishing Company.Martin Krallinger, Florian Leitner, and Alfonso Valen-cia.
2007.
Assessment of the second BioCreative PPItask: automatic extraction of protein-protein interac-tions.
In Proceedings of the Second BioCreative Chal-lenge Evaluation Workshop.Brian Marick.
1997.
The craft of software testing:subsystem testing including object-based and object-oriented testing.
Prentice Hall.Steve McConnell.
2004.
Code complete.
MicrosoftPress, 2nd edition.Tony McEnery and Andrew Wilson.
2001.
Corpus Lin-guistics.
Edinburgh University Press, 2nd edition.Greg Miller.
2006.
A scientist?s nightmare: softwareproblem leads to five retractions.
Science, 314:1856?1857.Glenford Myers.
1979.
The art of software testing.
JohnWiley and Sons.S.
Oepen, K. Netter, and J. Klein.
1998.
TSNLP - testsuites for natural language processing.
In John Ner-bonne, editor, Linguistic Databases, chapter 2, pages13?36.
CSLI Publications.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christopher Manning, Dan Flickinger, and ThorstenBrants.
2002.
The LinGO Redwoods treebank: mo-tivation and preliminary applications.
In Proceedingsof the 19th international conference on computationallinguistics, volume 2.Martin Volk.
1998.
Markup of a test suite with SGML.In John Nerbonne, editor, Linguistic databases, pages59?76.
CSLI Publications.30
