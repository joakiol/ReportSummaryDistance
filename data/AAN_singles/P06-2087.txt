Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 675?682,Sydney, July 2006. c?2006 Association for Computational LinguisticsArgumentative Feedback: A Linguistically-motivated TermExpansion for Information RetrievalPatrick Ruch, Imad Tbahriti, Julien GobeillMedical Informatics ServiceUniversity of Geneva24 Micheli du Crest1201 GenevaSwitzerland{patrick.ruch,julien.gobeill,imad.tbahriti}@hcuge.chAlan R. AronsonLister Hill CenterNational Library of Medicine8600 Rockville PikeBethesda, MD 20894USAalan@nlm.nih.govAbstractWe report on the development of a new au-tomatic feedback model to improve informa-tion retrieval in digital libraries.
Our hy-pothesis is that some particular sentences,selected based on argumentative criteria,can be more useful than others to performwell-known feedback information retrievaltasks.
The argumentative model we ex-plore is based on four disjunct classes, whichhas been very regularly observed in scien-tific reports: PURPOSE, METHODS, RE-SULTS, CONCLUSION.
To test this hy-pothesis, we use the Rocchio algorithm asbaseline.
While Rocchio selects the fea-tures to be added to the original querybased on statistical evidence, we proposeto base our feature selection also on argu-mentative criteria.
Thus, we restrict the ex-pansion on features appearing only in sen-tences classified into one of our argumen-tative categories.
Our results, obtained onthe OHSUMED collection, show a signifi-cant improvement when expansion is basedon PURPOSE (mean average precision =+23%) and CONCLUSION (mean averageprecision = +41%) contents rather than onother argumentative contents.
These resultssuggest that argumentation is an importantlinguistic dimension that could benefit in-formation retrieval.1 IntroductionInformation retrieval (IR) is a challenging en-deavor due to problems caused by the underly-ing expressiveness of all natural languages.
Oneof these problems, synonymy, is that authorsand users frequently employ different words orexpressions to refer to the same meaning (acci-dent may be expressed as event, incident, prob-lem, difficulty, unfortunate situation, the subjectof your last letter, what happened last week, etc.
)(Furnas et al, 1987).
Another problem is ambi-guity, where a specific term may have several(and sometimes contradictory) meanings andinterpretations (e.g., the word horse as in Tro-jan horse, light horse, to work like a horse, horseabout).
In order to obtain better meaning-basedmatches between queries and documents, vari-ous propositions have been suggested, usuallywithout giving any consideration to the under-lying domain.During our participation in different interna-tional evaluation campaigns such as the TRECGenomics track (Hersh, 2005), the BioCreativeinitiative (Hirschman et al, 2005), as well asin our attempts to deliver advanced searchtools for biologists (Ruch, 2006) and health-care providers (Ruch, 2002) (Ruch, 2004), wewere more concerned with domain-specific in-formation retrieval in which systems must re-turn a ranked list of MEDLINE records in re-sponse to an expert?s information request.
Thisinvolved a set of available queries describingtypical search interests, in which gene, pro-tein names, and diseases were often essentialfor an effective retrieval.
Biomedical publica-tions however tend to generate new informa-tion very rapidly and also use a wide varia-tion in terminology, thus leading to the cur-rent situation whereby a large number of names,symbols and synonyms are used to denote thesame concepts.
Current solutions to these issuescan be classified into domain-specific strate-gies, such as thesaurus-based expansion, anddomain-independent strategies, such as blind-feedback.
By proposing to explore a third typeof approach, which attempts to take advan-tage of argumentative specificities of scientificreports, our study initiates a new research di-rection for natural language processing appliedto information retrieval.The rest of this paper is organized as follows.Section 2 presents some related work in infor-mation retrieval and in argumentative parsing,while Section 3 depicts the main characteristicsof our test collection and the metrics used inour experiments.
Section 4 details the strategy675used to develop our improved feedback method.Section 5 reports on results obtained by varyingour model and Section 6 contains conclusions onour experiments.2 Related worksOur basic experimental hypothesis is that someparticular sentences, selected based on argu-mentative categories, can be more useful thanothers to support well-known feedback informa-tion retrieval tasks.
It means that selecting sen-tences based on argumentative categories canhelp focusing on content-bearing sections of sci-entific articles.2.1 ArgumentationOriginally inspired by corpus linguistics studies(Orasan, 2001), which suggests that scientificreports (in chemistry, linguistics, computer sci-ences, medicine...) exhibit a very regular logi-cal distribution -confirmed by studies conductedon biomedical corpora (Swales, 1990) and byANSI/ISO professional standards - the argu-mentative model we experiment is based on fourdisjunct classes: PURPOSE, METHODS, RE-SULTS, CONCLUSION.Argumentation belongs to discourse analy-sis1, with fairly complex computational mod-els such as the implementation of the rhetori-cal structure theory proposed by (Marcu, 1997),which proposes dozens of rhetorical classes.More recent advances were applied to docu-ment summarization.
Of particular interest forour approach, Teufel and Moens (Teufel andMoens, 1999) propose using a list of manuallycrafted triggers (using both words and expres-sions such as we argued, in this article, thepaper is an attempt to, we aim at, etc.)
toautomatically structure scientific articles intoa lighter model, with only seven categories:BACKGROUND, TOPIC, RELATED WORK,PURPOSE, METHOD, RESULT, and CON-CLUSION.More recently and for knowledge discovery inmolecular biology, more elaborated models wereproposed by (Mizuta and Collier, 2004) (Mizutaet al, 2005) and by (Lisacek et al, 2005) fornovelty-detection.
(McKnight and Srinivasan,2003) propose a model very similar to our four-class model but is inspired by clinical trials.Preliminary applications were proposed for bib-1After Aristotle, discourses structured following anappropriate argumentative distribution belong to logics,while ill-defined ones belong to rhetorics.liometrics and related-article search (Tbahritiet al, 2004) (Tbahriti et al, 2005), informa-tion extraction and passage retrieval (Ruch etal., 2005b).
In these studies, sentences were se-lected as the basic classification unit in orderto avoid as far as possible co-reference issues(Hirst, 1981), which hinder readibity of auto-matically generated and extracted sentences.2.2 Query expansionVarious query expansion techniques have beensuggested to provide a better match betweenuser information needs and documents, and toincrease retrieval effectiveness.
The generalprinciple is to expand the query using wordsor phrases having a similar or related meaningto those appearing in the original request.
Vari-ous empirical studies based on different IR mod-els or collections have shown that this type ofsearch strategy should usually be effective in en-hancing retrieval performance.
Scheme propo-sitions such as this should consider the variousrelationships between words as well as term se-lection mechanisms and term weighting schemes(Robertson, 1990).
The specific answers foundto these questions may vary; thus a varietyof query expansion approaches were suggested(Efthimiadis, 1996).In a first attempt to find related search terms,we might ask the user to select additional termsto be included in a new query, e.g.
(Velez etal., 1997).
This could be handled interactivelythrough displaying a ranked list of retrieveditems returned by the first query.
Voorhees(Voorhees, 1994) proposed basing a schemebased on the WordNet thesaurus.
The au-thor demonstrated that terms having a lexical-semantic relation with the original query words(extracted from a synonym relationship) pro-vided very little improvement (around 1% whencompared to the original unexpanded query).As a second strategy for expanding the orig-inal query, Rocchio (Rocchio, 1971) proposedaccounting for the relevance or irrelevance oftop-ranked documents, according to the user?smanual input.
In this case, a new query wasautomatically built in the form of a linear com-bination of the term included in the previousquery and terms automatically extracted fromboth the relevant documents (with a positiveweight) and non-relevant items (with a nega-tive weight).
Empirical studies (e.g., (Saltonand Buckley, 1990)) demonstrated that such anapproach is usually quite effective, and could676be used more than once per query (Aalbers-berg, 1992).
Buckley et al (Singhal et al,1996b) suggested that we could assume, with-out even looking at them or asking the user, thatthe top k ranked documents are relevant.
De-noted the pseudo-relevance feedback or blind-query expansion approach, this approach is usu-ally effective, at least when handling relativelylarge text collections.As a third source, we might use large textcorpora to derive various term-term relation-ships, using statistically or information-basedmeasures (Jones, 1971), (Manning and Schu?tze,2000).
For example, (Qiu and Frei, 1993)suggested that terms to be added to a newquery could be extracted from a similarity the-saurus automatically built through calculatingco-occurrence frequencies in the search collec-tion.
The underlying effect was to add idiosyn-cratic terms to the underlying document col-lection, related to the query terms by languageuse.
When using such query expansion ap-proaches, we can assume that the new terms aremore appropriate for the retrieval of pertinentitems than are lexically or semantically relatedterms provided by a general thesaurus or dic-tionary.
To complement this global documentanalysis, (Croft, 1998) suggested that text pas-sages (with a text window size of between 100to 300 words) be taken into account.
This localdocument analysis seemed to be more effectivethan a global term relationship generation.As a forth source of additional terms, wemight account for specific user informationneeds and/or the underlying domain.
In thisvein, (Liu and Chu, 2005) suggested that termsrelated to the user?s intention or scenario mightbe included.
In the medical domain, it was ob-served that users looking for information usu-ally have an underlying scenario in mind (ora typical medical task).
Knowing that thenumber of scenarios for a user is rather lim-ited (e.g., diagnosis, treatment, etiology), theauthors suggested automatically building a se-mantic network based on a domain-specific the-saurus (using the Unified Medical LanguageSystem (UMLS) in this case).
The effective-ness of this strategy would of course dependon the quality and completeness of domain-specific knowledge sources.
Using the well-known term frequency (tf)/inverse documentfrequency (idf) retrieval model, the domain-specific query-expansion scheme suggested byLiu and Chu (2005) produces better retrievalperformance than a scheme based on statis-tics (MAP: 0.408 without query expansion,0.433 using statistical methods and 0.452 withdomain-specific approaches).In these different query expansion ap-proaches, various underlying parameters mustbe specified, and generally there is no sin-gle theory able to help us find the most ap-propriate values.
Recent empirical studiesconducted in the context of the TREC Ge-nomics track, using the OHSUGEN collection(Hersh, 2005), show that neither blind expan-sion (Rocchio), nor domain-specific query ex-pansion (thesaurus-based Gene and Protein ex-pansion) seem appropriate to improve retrievaleffectiveness (Aronson et al, 2006) (Abdou etal., 2006).3 Data and metricsTo test our hypothesis, we used the OHSUMEDcollection (Hersh et al, 1994), originally devel-oped for the TREC topic detection track, whichis the most popular information retrieval collec-tion for evaluating information search in librarycorpora.
Alternative collections (cf.
(Savoy,2005)), such as the French Amaryllis collection,are usually smaller and/or not appropriate toevaluate our argumentative classifier, which canonly process English documents.
Other MED-LINE collections, which can be regarded as sim-ilar in size or larger, such as the TREC Ge-nomics 2004 and 2005 collections are unfortu-nately more domain-specific since informationrequests in these collection are usually target-ing a particular gene or gene product.Among the 348,566 MEDLINE citations ofthe OHSUMED collection, we use the 233,455records provided with an abstract.
An exam-ple of a MEDLINE citation is given in Table 1:only Title, Abstract, MeSH and Chemical (RN)fields of MEDLINE records were used for index-ing.
Out of the 105 queries of the OHSUMEDcollection, only 101 queries have at least onepositive relevance judgement, therefore we usedonly this subset for our experiments.
The sub-set has been randomly split into a training set(75 queries), which is used to select the differentparameters of our retrieval model, and a test set(26 queries), used for our final evaluation.As usual in information retrieval evaluations,the mean average precision, which computes theprecision of the engine at different levels (0%,10%, 20%... 100%) of recall, will be used in ourexperiments.
The precision of the top returned677Title: Computerized extraction of coded find-ings from free-text radiologic reports.
Work inprogress.Abstract: A computerized data acquisitiontool, the special purpose radiology understand-ing system (SPRUS), has been implemented asa module in the Health Evaluation through Log-ical Processing Hospital Information System.This tool uses semantic information from a di-agnostic expert system to parse free-text radi-ology reports and to extract and encode boththe findings and the radiologists?
interpreta-tions.
These coded findings and interpretationsare then stored in a clinical data base.
The sys-tem recognizes both radiologic findings and di-agnostic interpretations.
Initial tests showed atrue-positive rate of 87% for radiographic find-ings and a bad data rate of 5%.
Diagnostic in-terpretations are recognized at a rate of 95%with a bad data rate of 6%.
Testing suggeststhat these rates can be improved through en-hancements to the system?s thesaurus and thecomputerized medical knowledge that drives it.This system holds promise as a tool to obtaincoded radiologic data for research, medical au-dit, and patient care.MeSH Terms: Artificial Intelligence*; Deci-sion Support Techniques; Diagnosis, Computer-Assisted; Documentation; Expert Systems; Hos-pital Information Systems*; Human; NaturalLanguage Processing*; Online Systems; Radi-ology Information Systems*.Table 1: MEDLINE records with, title, abstractand keyword fields as provided by MEDLINElibrarians: major concepts are marked with *;Subheadings and checktags are removed.document, which is obviously of major impor-tance is also provided together with the totalnumber of relevant retrieved documents for eachevaluated run.4 MethodsTo test our experimental hypothesis, we use theRocchio algorithm as baseline.
In addition, wealso provide the score obtained by the enginebefore the feedback step.
This measure is nec-essary to verify that feedback is useful for query-ing the OHSUMED collection and to establish astrong baseline.
While Rocchio selects the fea-tures to be added to the original queries basedon pure statistical analysis, we propose to baseour feature expansion also on argumentative cri-teria.
That is, we overweight features appear-ing in sentences classified in a particular argu-mentative category by the argumentative cate-gorizer.4.1 Retrieval engine and indexing unitsThe easyIR system is a standard vector-spaceengine (Ruch, 2004), which computes state-of-the-art tf.idf and probabilistic weightingschema.
All experiments were conducted withpivoted normalization (Singhal et al, 1996a),which has recently shown some effectivenesson MEDLINE corpora (Aronson et al, 2006).Query and document weighings are provided inEquation (1): the dtu formula is applied to thedocuments, while the dtn formula is applied tothe query; t the number of indexing terms, dfjthe number of documents in which the term tj ;pivot and slope are constants (fixed at pivot =0.14, slope = 146).dtu: wij = (Ln(Ln(tfij)+1)+1)?idfj(1?slope)?pivot+slope?ntidtn: wij = idfj ?
(Ln(Ln(tfif ) + 1) + 1)(1)As already observed in several linguistically-motivated studies (Hull, 1996), we observe thatcommon stemming methods do not perform wellon MEDLINE collections (Abdou et al, 2006),therefore indexing units are stored in the in-verted file using a simple S-stemmer (Harman,1991), which basically handles most frequentplural forms and exceptions of the English lan-guage such as -ies, -es and -s and exclude end-ings such as -aies, -eies, -ss, etc.
This simplenormalization procedure performs better thanothers and better than no stemming.
We alsouse a slightly modified standard stopword list of544 items, where strings such as a, which standsfor alpha in chemistry and is relevant in biomed-ical expressions such as vitamin a.4.2 Argumentative categorizerThe argumentative classifier ranks and catego-rizes abstract sentences as to their argumenta-tive classes.
To implement our argumentativecategorizer, we rely on four binary Bayesianclassifiers, which use lexical features, and aMarkov model, which models the logical distri-bution of the argumentative classes in MED-LINE abstracts.
A comprehensive descriptionof the classifier with feature selection and com-parative evaluation can be found in (Ruch etal., 2005a)To train the classifier, we obtained 19,555 ex-plicitly structured abstracts from MEDLINE.
A678Abstract: PURPOSE: The overall prognosisfor patients with congestive heart failure is poor.Defining specific populations that might demon-strate improved survival has been difficult [...]PATIENTS AND METHODS: We identified 11patients with severe congestive heart failure (av-erage ejection fraction 21.9 +/- 4.23% (+/- SD)who developed spontaneous, marked improve-ment over a period of follow-up lasting 4.25 +/-1.49 years [...] RESULTS: During the follow-upperiod, the average ejection fraction improvedin 11 patients from 21.9 +/- 4.23% to 56.64+/- 10.22%.
Late follow-up indicates an aver-age ejection fraction of 52.6 +/- 8.55% for thegroup [...] CONCLUSIONS: We conclude thatselected patients with severe congestive heartfailure can markedly improve their left ventric-ular function in association with complete reso-lution of heart failure [...]Table 2: MEDLINE records with explicit ar-gumentative markers: PURPOSE, (PATIENTSand) METHODS, RESULTS and CONCLU-SION.Bayesian classifierPURP.
METH.
RESU.
CONC.PURP.
80.65 % 0 % 3.23 % 16 %METH.
8 % 78 % 8 % 6 %RESU.
18.58 % 5.31 % 52.21 % 23.89 %CONC.
18.18 % 0 % 2.27 % 79.55 %Bayesian classifier with Markov modelPURP.
METH.
RESU.
CONC.PURP.
93.35 % 0 % 3.23 % 3 %METH.
3 % 78 % 8 % 6 %RESU.
12.73 % 2.07 % 57.15 % 10.01 %CONC.
2.27 % 0 % 2.27 % 95.45 %Table 3: Confusion matrix for argumentativeclassification.
The harmonic means between re-call and precision score (or F-score) is in therange of 85% for the combined system.conjunctive query was used to combine the fol-lowing four strings: PURPOSE:, METHODS:,RESULTS:, CONCLUSION:.
From the originalset, we retained 12,000 abstracts used for train-ing our categorizer, and 1,200 were used for fine-tuning and evaluating the categorizer, followingremoval of explicit argumentative markers.
Anexample of an abstract, structured with explicitargumentative labels, is given in Table 2.
Theper-class performance of the categorizer is givenby a contingency matrix in Table 3.4.3 Rocchio feedbackVarious general query expansion approacheshave been suggested, and in this paper we com-pared ours with that of Rocchio.
In this lattercase, the system was allowed to add m terms ex-tracted from the k best-ranked abstracts fromthe original query.
Each new query was derivedby applying the following formula (Equation 2):Q?
= ?
?
Q + (?/k) ??
kj = 1wij (2), in whichQ?
denotes the new query built from the previ-ous query Q, and wij denotes the indexing termweight attached to the term tj in the documentDi.
By direct use of the training data, we de-termine the optimal values of our model: m =10, k = 15.
In our experiments, we fixed ?
=2.0, ?
= 0.75.
Without feedback the mean av-erage precision of the evaluation run is 0.3066,the Rocchio feedback (mean average precision =0.353) represents an improvement of about 15%(cf.
Table 5), which is statistically2 significant(p < 0.05).4.4 Argumentative selection forfeedbackTo apply our argumentation-driven feedbackstrategy, we first have to classify the top-rankedabstracts into our four argumentative moves:PURPOSE, METHODS, RESULTS, and CON-CLUSION.
For the argumentative feedback, dif-ferent m and k values are recomputed on thetraining queries, depending on the argumenta-tive category we want to over-weight.
The ba-sic segment is the sentence; therefore the ab-stract is split into a set of sentences before beingprocessed by the argumentative classifier.
Thesentence splitter simply applies as set of regu-lar expressions to locate sentence boundaries.The precision of this simple sentence splitterequals 97% on MEDLINE abstracts.
In thissetting only one argumentative category is at-tributed to each sentence, which makes the de-cision model binary.Table 4 shows the output of the argumenta-tive classifier when applied to an abstract.
Todetermine the respective value of each argumen-tative contents for feedback, the argumenta-tive categorizer parses each top-ranked abstract.These abstracts are then used to generate fourgroups of sentences.
Each group corresponds toa unique argumentative class.
Each argumenta-tive index contains sentences classified in one offour argumentative classes.
Because argumen-2Tests are computed using a non-parametric signedtest, cf.
(Zobel, 1998) for more details.679CONCLUSION (00160116) The highly favorable pathologic stage(RI-RII, 58%) and the fact that the majority of patients werealive and disease-free suggested a more favorable prognosisfor this type of renal cell carcinoma.METHODS (00160119) Tumors were classified according towell-established histologic criteria to determine stage ofdisease; the system proposed by Robson was used.METHODS (00162303) Of 250 renal cell carcinomas analyzed,36 were classified as chromophobe renal cell carcinoma,representing 14% of the group studied.PURPOSE (00156456) In this study, we analyzed 250 renal cellcarcinomas to a) determine frequency of CCRC at our Hospitaland b) analyze clinical and pathologic features of CCRCs.PURPOSE (00167817) Chromophobe renal cell carcinoma (CCRC)comprises 5% of neoplasms of renal tubular epithelium.
CCRCmay have a slightly better prognosis than clear cell carcinoma,but outcome data are limited.RESULTS (00155338) Robson staging was possible in all cases,and 10 patients were stage 1) 11 stage II; 10 stage III, andfive stage IV.Table 4: Output of the argumentative catego-rizer when applied to an argumentatively struc-tured abstract after removal of explicit mark-ers.
For each row, the attributed class is fol-lowed by the score for the class, followed by theextracted text segment.
The reader can com-pare this categorization with argumentative la-bels as provided in the original abstract (PMID12404725).tative classes are equally distributed in MED-LINE abstracts, each index contains approxi-mately a quarter of the top-ranked abstractscollection.5 Results and DiscussionAll results are computed using the treceval pro-gram, using the top 1000 retrieved documentsfor each evaluation query.
We mainly evaluatethe impact of varying the feedback category onthe retrieval effectiveness, so we separately ex-pand our queries based a single category.
Queryexpansion based on RESULTS or METHODSsentences does not result in any improvement.On the contrary, expansion based on PURPOSEsentences improve the Rocchio baseline by +23%, which is again significant (p < 0.05).
Butthe main improvement is observed when CON-CLUSION sentences are used to generate theexpansion, with a remarkable gain of 41% whencompared to Rocchio.
We also observe in Table5 that other measures (top precision) and num-ber of relevant retrieved articles do confirm thistrend.For the PURPOSE category, the optimal kparameter, computed on the test queries was11.
For the CONCLUSION category, the opti-mal k parameter, computed on the test querieswas 10.
The difference between the m values be-tween Rocchio feedback and the argumentativefeedback, respectively 15 vs. 11 and 10 for Roc-chio, PURPOSE, CONCLUSION sentences canNo feebackRelevant Top Mean averageretrieved precision precision1020 0.3871 0.3066Rocchio feedbackRelevant Top Mean averageretrieved precision precision1112 0.4020 0.353Argumentative feedback: PURPOSERelevant Top Mean averageretrieved precision precision1136 0.485 0.4353Argumentative feedback: CONCLUSIONRelevant Top Mean averageretrieved precision precision1143 0.550 0.4999Table 5: Results without feedback, with Roc-chio and with argumentative feedback appliedon PURPOSE and CONCLUSION sentences.The number of relevant document for all queriesis 1178.be explained by the fact that less textual mate-rial is available when a particular class of sen-tences is selected; therefore the number of wordsthat should be added to the original query ismore targeted.From a more general perspective, the impor-tance of CONCLUSION and PURPOSE sen-tences is consistent with other studies, whichaimed at selecting highly content bearing sen-tences for information extraction (Ruch et al,2005b).
This result is also consistent withthe state-of-the-art in automatic summariza-tion, which tends to prefer sentences appearingat the beginning or at the end of documents togenerate summaries.6 ConclusionWe have reported on the evaluation of anew linguistically-motivated feedback strategy,which selects highly-content bearing features forexpansion based on argumentative criteria.
Oursimple model is based on four classes, whichhave been reported very stable in scientific re-ports of all kinds.
Our results suggest thatargumentation-driven expansion can improveretrieval effectiveness of search engines by morethan 40%.
The proposed methods open newresearch directions and are generally promis-ing for natural language processing applied toinformation retrieval, whose positive impact isstill to be confirmed (Strzalkowski et al, 1998).Finally, the proposed methods are importantfrom a theoretical perspective, if we consider680that it initiates a genre-specific paradigm asopposed to the usual information retrieval ty-pology, which distinguishes between domain-specific and domain-independent approaches.AcknowledgementsThe first author was supported by a visitingfaculty grant (ORAU) at the Lister Hill Cen-ter of the National Library of Medicine in 2005.We would like to thank Dina Demner-Fushman,Susanne M. Humphrey, Jimmy Lin, HongfangLiu, Miguel E. Ruiz, Lawrence H. Smith, Lor-raine K. Tanabe, W. John Wilbur for the fruit-ful discussions we had during our weekly TRECmeetings at the NLM.
The study has also beenpartially supported by the Swiss National Foun-dation (Grant 3200-065228).ReferencesI Aalbersberg.
1992.
Incremental RelevanceFeedback.
In SIGIR, pages 11?22.S Abdou, P Ruch, and J Savoy.
2006.
Gen-eral vs.
Specific Blind Query Expansion forBiomedical Searches.
In TREC 2005.A Aronson, D Demner-Fushman, S Humphrey,J Lin, H Liu, P Ruch, M Ruiz, L Smith,L Tanabe, and J Wilbur.
2006.
Fusionof Knowledge-intensive and Statistical Ap-proaches for Retrieving and Annotating Tex-tual Genomics Documents.
In TREC 2005.J Xu B Croft.
1998.
Corpus-based stem-ming using cooccurrence of word variants.ACM-Transactions on Information Systems,16(1):61?81.E Efthimiadis.
1996.
Query expansion.
AnnualReview of Information Science and Technol-ogy, 31.G Furnas, T Landauer, L Gomez, and S Du-mais.
1987.
The vocabulary problem inhuman-system communication.
Communica-tions of the ACM, 30(11).D Harman.
1991.
How effective is suffixing ?JASIS, 42 (1):7?15.W Hersh, C Buckley, T Leone, and D Hickam.1994.
OHSUMED: An interactive retrievalevaluation and new large test collection forresearch.
In SIGIR, pages 192?201.W Hersh.
2005.
Report on the trec 2004 ge-nomics track.
pages 21?24.Lynette Hirschman, Alexander Yeh, Chris-tian Blaschke, and Alfonso Valencia.
2005.Overview of BioCreAtIvE: critical assessmentof information extraction for biology.
BMCBioinformatics, 6 (suppl.
1).G Hirst.
1981.
Anaphora in Natural LanguageUnderstanding: A Survey.
Lecture Notes inComputer Science 119 - Springer.D Hull.
1996.
Stemming algorithms: A casestudy for detailed evaluation.
Journal ofthe American Society of Information Science,47(1):70?84.K Sparck Jones.
1971.
Automatic KeywordClassification for Information Retrieval.
But-terworths.F Lisacek, C Chichester, A Kaplan, and San-dor.
2005.
Discovering Paradigm Shift Pat-terns in Biomedical Abstracts: Applicationto Neurodegenerative Diseases.
In Proceed-ings of the First International Symposium onSemantic Mining in Biomedicine (SMBM),pages 212?217.
Morgan Kaufmann.Z Liu and W Chu.
2005.
Knowledge-basedquery expansion to support scenario-specificretrieval of medical free text.
ACM-SAC In-formation Access and Retrieval Track, pages1076?1083.C Manning and H Schu?tze.
2000.
Foundationsof Statistical Natural Language Processing.MIT Press.D Marcu.
1997.
The Rhetorical Parsing of Nat-ural Language Texts.
pages 96?103.L McKnight and P Srinivasan.
2003.
Cate-gorization of sentence types in medical ab-stracts.
AMIA Annu Symp Proc., pages 440?444.Y Mizuta and N Collier.
2004.
Zone iden-tification in biology articles as a basis forinformation extraction.
Proceedings of thejoint NLPBA/BioNLP Workshop on NaturalLanguage for Biomedical Applications, pages119?125.Y Mizuta, A Korhonen, T Mullen, and N Col-lier.
2005.
Zone Analysis in Biology Articlesas a Basis for Information Extraction.
Inter-national Journal of Medical Informatics, toappear.C Orasan.
2001.
Patterns in Scientific Ab-stracts.
In Proceedings of Corpus Linguistics,pages 433?445.Y Qiu and H Frei.
1993.
Concept based queryexpansion.
ACM-SIGIR, pages 160?69.S Robertson.
1990.
On term selection forquery expansion.
Journal of Documentation,46(4):359?364.J Rocchio.
1971.
Relevance feedback in infor-mation retrieval in The SMART RetrievalSystem - Experiments in Automatic Docu-ment Processing.
Prentice-Hall.681P Ruch, R Baud, C Chichester, A Geissbu?hler,F Lisacek, J Marty, D Rebholz-Schuhmann,I Tbahriti, and AL Veuthey.
2005a.
Extract-ing Key Sentences with Latent Argumenta-tive Structuring.
In Medical Informatica Eu-rope (MIE), pages 835?40.P Ruch, L Perret, and J Savoy.
2005b.
FeaturesCombination for Extracting Gene Functionsfrom MEDLINE.
In European Colloquiumon Information Retrieval (ECIR), pages 112?126.P Ruch.
2002.
Using contextual spelling correc-tion to improve retrieval effectiveness in de-graded text collections.
COLING 2002.P Ruch.
2004.
Query translation by text cate-gorization.
COLING 2004.P Ruch.
2006.
Automatic Assignment ofBiomedical Categories: Toward a GenericApproach.
Bioinformatics, 6.G Salton and C Buckley.
1990.
Improving re-trieval performance by relevance feedback.Journal of the American Society for Informa-tion Science, 41(4).J Savoy.
2005.
Bibliographic database accessusing free-text and controlled vocabulary: Anevaluation.
Information Processing and Man-agement, 41(4):873?890.A Singhal, C Buckley, and M Mitra.
1996a.Pivoted document length normalization.ACM-SIGIR, pages 21?29.C Buckley A Singhal, M Mitra, and G Salton.1996b.
New retrieval approaches using smart.In Proceedings of TREC-4.T Strzalkowski, G Stein, G Bowden Wise,J Perez Carballo, P Tapanainen, T Jarvinen,A Voutilainen, and J Karlgren.
1998.
Natu-ral language information retrieval: TREC-7report.
In Text REtrieval Conference, pages164?173.J Swales.
1990.
Genre Analysis: English inAcademic and Research Settings.
CambridgeUniversity Press.I Tbahriti, C Chichester, F Lisacek, andP Ruch.
2004.
Using Argumention toRetrieve Articles with Similar Citationsfrom MEDLINE.
Proceedings of the jointNLPBA/BioNLP Workshop on Natural Lan-guage for Biomedical Applications.I Tbahriti, C Chichester, F Lisacek, andP Ruch.
2005.
Using Argumentation to Re-trieve Articles with Similar Citations: an In-quiry into Improving Related Articles Searchin the MEDLINE Digital Library.
Interna-tional Journal of Medical Informatics, to ap-pear.S Teufel and M Moens.
1999.
Argumenta-tive Classification of Extracted Sentences asa First Step Towards Flexible Abstracting.Advances in Automatic Text Summarization,MIT Press, pages 155?171.B Velez, R Weiss, M Sheldon, and D Gifford.1997.
Fast and effective query refinement.
InACM SIGIR, pages 6?15.E Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In ACM SIGIR,pages 61?69.J Zobel.
1998.
How reliable are large-scaleinformation retrieval experiments?
ACM-SIGIR, pages 307?314.682
