Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 67?74,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingA Character n-gram Based Approach for Improved Recallin Indian Language NERPraneeth M Shishtlapraneethms@students.iiit.ac.inPrasad Pingalipvvpr@iiit.ac.inVasudeva Varmavv@iiit.ac.inLanguage Technologies Research CentreInternational Institute of Information TechnologyHyderabad, IndiaAbstractNamed Entity Recognition (NER) is thetask of identifying and classifying all propernouns in a document as person names, or-ganization names, location names, date &time expressions and miscellaneous.
Previ-ous work (Cucerzan and Yarowsky, 1999)was done using the complete words as fea-tures which suffers from a low recall prob-lem.
Character n-gram based approach(Klein et al, 2003) using generative mod-els, was experimented on English languageand it proved to be useful over the wordbased models.
Applying the same techniqueon Indian Languages, we experimented withConditional Random Fields (CRFs), a dis-criminative model, and evaluated our sys-tem on two Indian Languages Telugu andHindi.
The character n-gram based modelsshowed considerable improvement over theword based models.
This paper describes thefeatures used and experiments to increasethe recall of Named Entity Recognition Sys-tems which is also language independent.1 IntroductionThe objective of NER is to classify all tokens in atext document into predefined classes such as per-son, organization, location, miscellaneous.
NER isa precursor to many language processing tasks.
Thecreation of a subtask for NER in Message Under-standing Conference (MUC) (Chinchor, 1997) re-flects the importance of NER in Information Extrac-tion (IE).
NER also finds aplication in question an-swering systems (Toral et al, 2005; Molla et al,2006), and machine translation (Babych and Hart-ley, 2003).
NER is an essential subtask in organizingand retrieving biomedical information (Tsai, 2006).NER can be treated as a two step process?
identification of proper nouns.?
classification of these identified proper nouns.Challenges in named entity recognition.Many named entities (NEs) occur rarely in corpusif at all.Ambiguity of NEs.
Ex Washington can be a per-son?s name or location.There are many ways of mentioning the sameNE.
Ex: Mahatma Gandhi, M.K.Gandhi, MohandasKaramchand Gandhi, Gandhi all refer to the sameperson.
New Jersey, NJ both refer to the same loca-tion.In English, the problem of identifying NEs is solvedto some extent by using the capitalization feature.Most of the named entities begin with a capital let-ter which is a discriminating feature for classifying atoken as named entity.
In addition to the above chal-lenges, the complexity of Indian Languages posefew more problems.
In case of Indian languagesthere is no concept of capitalization.
Ex: The per-son name Y.S.R (in english) is represented as ysr inthe Indian Languages.Agglutinative property of the Indian Languagesmakes the identification more difficult.
For exam-ple: hyderabad, hyderabad ki, hyderabadki, hyder-abadlo, hyderabad ni, hyderabad ko etc .. all referto the place Hyderabad.
where lo, ki, ni are all post-postion markers in Telugu and ko is a postposition67marker in Hindi.There are many ways of representing acronyms.The letters in acronyms could be the English alpha-bet or the native alphabet.
Ex: B.J.P and BaJaPaboth are acronyms of Bharatiya Janata Party.
In-dian Languages lack particular standard for formingacronyms.Due to these wide variations and the agglutina-tive nature of Indian languages, probabilistic graph-ical models result in very less recall.
If we are ableto identify the presence of a named entity with afairly good amount of accuracy, classification thencan be done efficiently.
But, when the machine failsto identify the presence of named entities, there isno chance of entity classification because we missmany of the named entities (less recall which resultsin less F-measure,F?=1).
So we focus mainly on theways to improve the recall of the system.
Also, In-dian Languages have a relatively free word order,i.e.
the words (named entities) can occupy any placein the sentence.
This change in the word position iscompensated using case markers.2 Related Work & Our ContributionsThe state-of-art techniques for Indic lan-guages(Telugu and Hindi) use word based modelswhich suffer from low recall, use gazetteers andare language dependent.
As such there is noNER system for Telugu.
Previously (Klein et al,2003) experimented with character-level modelsfor English using character based HMM which isa generative model.
We experimented using thediscriminative model for English, Hindi and Telugu.?
We propose an approach that increases the re-call of Indic languages (even the agglutinativelanguages).?
The model is language independent as none ofthe language resources is needed.3 Problem Statement3.1 NER as sequence labelling taskNamed entity recognition (NER) can be modelledas a sequence labelling task (Lafferty et al, 2001).Given an input sequence of words W n1 = w1w2w3...wn, the NER task is to construct a label sequenceLn1 = l1l2l3 ...ln , where label li either belongs tothe set of predefined classes for named entities oris none (representing words which are not propernouns).
The general label sequence ln1 has the high-est probability of occuring for the word sequenceW n1 among all possible label sequences, that is?Ln1 = argmax {Pr (Ln1 | W n1 ) }3.2 Tagging SchemeWe followed the IOB tagging scheme (Ramshawand Marcus, 1995) for all the three languages (En-glish, Hindi and Telugu).
In this scheme each linecontains a word at the beginning followed by itstag.
The tag encodes the type of named entityand whether the word is in the beginning or insidethe NE.
Empty lines represent sentence (document)boundaries.
An example of the IOB tagging schemeis given in Table 1.Words tagged with O are outside of named entitiesToken Named Entity TagDr.
B-PERTalcott I-PERled Oa Oteam Oof Oresearchers Ofrom Othe ONational B-ORGCancer I-ORGInstitute I-ORGTable 1: IOB tagging scheme.and the I-XXX tag is used for words inside a namedentity of type XXX.
Whenever two entities of typeXXX are immediately next to each other, the firstword of the second entity will be tagged B-XXX inorder to show that it starts another entity.
This tag-ging scheme is the IOB scheme originally put for-ward by Ramshaw and Marcus (Ramshaw and Mar-cus, 1995).4 Conditional Random FieldsConditional Random Fields (CRFs) (Wallach, 2004)are undirected graphical models used to calculate68the conditional probability of values on designatedoutput nodes given the values assigned to other des-ignated input nodes.
In the special case in whichthe output nodes of the graphical model are linkedby edges in a linear chain, CRFs make a first-orderMarkov independence assumption, and thus can beunderstood as conditionally-trained Finite State Ma-chines (FSMs).Let o = ?
O1,O2,...OT ?
be some observed inputdata sequence, such as a sequence of words in textin a document, (the values on n input nodes of thegraphical model).
Let S be a set of Finite State Ma-chine (FSM) states, each of which is associated witha label, l ?
L .Let s = ?
s1,s2,... sT ,?
be some sequence of states,(thevalues on T output nodes).
By the Hammersley-Clifford theorem CRFs define the conditional prob-ability of a state sequence given an input sequenceto beP(s|o) = 1Zo?
exp(T?t=1?k?k fk (st?1,st ,o, t))where Zo is a normalization factor over all statesequences, is an arbitrary feature function over its ar-guments, and ?k is a learned weight for each featurefunction.
A feature function may, for example, bedefined to have value 0 or 1.
Higher ?
weights maketheir corresponding FSM transitions more likely.CRFs define the conditional probability of a la-bel sequence based on total probability over the statesequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is thesequence of labels corresponding to the labels of thestates in sequence s. Note that the normalization fac-tor, Zo, (also known in statistical physics as the parti-tion function) is the sum of the scores of all possiblestate sequences,Zo = ?s?ST?exp(T?t=1?k?k fk (st?1,st ,o, t))and that the number of state sequences is expo-nential in the input sequence length, T. In arbitrarily-structured CRFs, calculating the partition function inclosed form is intractable, and approximation meth-ods such as Gibbs sampling, or loopy belief propa-gation must be used.5 FeaturesThere are many types of features used in NER sys-tems.Many systems use binary features i.e.
theword-internal features, which indicate the presenceor absence of particular property in the word.
(Mikheev, 1997; Wacholder et al, 1997; Bikel etal., 1997).
Following are examples of commonlyused binary features: All-Caps (IBM), internalcapitalization (eBay), initial capital (Abdul Kalam),uncapitalized word (can), 2-digit number (83, 73),4-digit number (1983, 2007), all digits (8, 28, 1273)etc.
The features that correspond to the capitaliza-tion are not applicable to Indian languages.
Also,we have not used any of the binary features in anyof our models.Dictionaries: Dictionaries are used to check if apart of the named entity is present in the dictionary.These dictionaries are called as gazetteers.
Theproblem with the Indian languages is that there areno proper gazetteers in Indian languages.Lexical features like a sliding window[w?2,w?1,wo,w1,w2] are used to create a lexi-cal history view.
Prefix and suffix tries were alsoused previously (Cucerzan and Yarowsky, 1999).Linguistics features like Part Of Speech, Chunk,etc are also used.
In our approach we don?t use anyof these language specific (linguistic) information.5.1 Our FeaturesIn our experiments, we considered and character n-grams (ASCII characters) as tokens.For example for the word Vivekananda, the 4-grammodel would result in 8 tokens namely Vive, ivek,veka, ekan, kana, anan, nand and anda.
If our cur-rent token (w0) is kanaFeature Examplecurrent token: w0 kanaprevious 3 tokens: w?3,w?2,w?1 ivek,veka,ekannext 3 tokens: w1,w2,w3 anan,nand,andacompound feature: w0 w1 kanaanancompound feature: w?1 w0 ekankanaIn Indian Languages suffixes and other inflectionsget attached to the words increasing the length of theword and reducing the number of occurences of thatword in the entire corpus.
The character n-grams69can capture these variations.
The compound featuresalso help in capturing such variations.
The slidingwindow feature helps in guessing the class of the en-tity using the context.
In total 9 features were usedin training and testing.
All the features are langugeindependent and no binary features are used.6 Experimental Setup6.1 CorpusWe conducted the experiments on three languagesnamely Telugu, Hindi and English.
We collected theTelugu corpus from Eenadu, a telugu daily news-paper.
The topics included politics, health andmedicine, sports, education, general issues etc.
Theannotated corpus had 45714 tokens, out of which4709 were named entities.
We collected the Englishcorpus from the Wall Street Journal (WSJ) news ar-ticles.
The corpus had 45870 tokens out of which4287 were named entities.
And we collected thehindi corpus from various sources.
The topics in thecorpus included social sciences, biological sciences,financial articles, religion, etc.
The hindi corpus isnot a news corpus.
The corpus had 45380 tokens outof which 3140 were named entities.
We evaluatedthe hand-annotated corpus once to check for any er-rors.6.2 ExperimentsWe conducted various experiments on Telugu andHindi.
Also, to verify the correctness of our modelfor other languages, we have conducted some ex-periments on English data also.
In this section wedescribe the various experiments conducted on theTelugu, Hindi and English data sets.We show the average performance of the systemin terms of precision, recall and F-measure for Tel-ugu, Hindi and English in Table 6 and then for theimpact of training data size on performance of thesystem in Table 7 (Telugu), Table 8 (English) andTable 9 (Hindi).
Here, precision measures the num-ber of correct Named Entities (NEs) in the machinetagged file over the total number of NEs in the ma-chine tagged file and the recall measures the numberof correct NEs in the machine tagged file over the to-tal number of NEs in the golden standard file whileF-measure is the weighted harmonic mean of preci-sion and recall:F =(?
2 + 1) RP?
2R + Pwith?
2 = 1where P is Precision, R is Recall and F is F-measure.Precision Recall F?=1words 89.66% 29.21% 44.07n=2 77.36% 46.07% 57.75n=3 85.45% 52.81% 65.28n=4 79.63% 48.31% 60.14n=5 74.47% 39.33% 51.47n=6 76.32% 32.58% 45.67Table 2: Precision,Recall and F?=1 measure for Date& Time expressions in Telugu.Precision Recall F?=1words 83.65% 28.71% 42.75n=2 80.29% 36.30% 50n=3 78.26% 35.64% 48.98n=4 81.03% 31.02% 44.87n=5 75.42% 29.37% 42.28n=6 53.21% 27.39% 36.17Table 3: Precision,Recall & F?=1 measure values forlocation names in Telugu.Precision Recall F?=1words 51.11% 18.70% 27.38n=2 53.41% 38.21% 44.55n=3 69.35% 34.96% 46.49n=4 69.35% 34.96% 46.49n=5 55.00% 26.83% 36.07n=6 50.98% 21.14% 29.89Table 4: Precision,Recall and F?=1 measure valuesfor organisation names in Telugu.Table:6 shows the average precison(P),recall(R)and F-measure(F) values for NEs in Telugu.Tables 2 to 5 show the P,R,F values for the indi-vidual categories of NEs in Telugu.
Interestingly,70Precision Recall F?=1words 57.32% 18.65% 28.14n=2 55.77% 34.52% 42.65n=3 61.04% 37.30% 46.31n=4 56.92% 29.37% 38.74n=5 60.50% 28.57% 38.81n=6 54.21% 23.02% 32.31Table 5: Precision,Recall and F?=1 measure valuesfor Person names in Telugu.though we have not used any of the features per-taining to years and numbers we have acheived anappreciable F-measure of 65.28 for date & time ex-pressions.In each table the model with the highest F-measure is higlighted in bold.
And, the tri-grammodel performed best in most of the cases exceptwith locations where bi-gram model performed well.But, even the tri-gram model (F?=1=48.98) per-formed close to the bi-gram model ((F?=1=50).For Hindi, the recall of the n-gram models(Table6) is more than the word based models but theamount of increase in recall and F-measure is less.On examining, we found that the average number ofnamed entities in the Hindi data were quite less.
Thisis because the articles for hindi were taken from gen-eral articles.
Whereas in case of English and Telugu,the corpus was collected from news articles, whichhad more probability of having new and more namedentities, which can occur in a similar repeating pat-tern.The character n-gram approach showed consider-able improvement in recall and F-measure (with adrop in precision) in Telugu and Hindi, which areagglutinative in nature.
In Telugu, there is a differ-ence of 14.19 and 14.02 in recall and F-measure re-spectively between the word based model and thebest performing n-gram model (n=3) of size 3.
InHindi, there is a difference of 2.34 and 2.33 in re-call and F-measure respectively between the wordbased model and the best performing n-gram model(n=5).
Even in case of non-agglutinative languagelike English there is a considerable improvement of1.48 and 1.91 in recall and F-measure respectivelybetween the word based model and best performingn-gram model (n=2) of size 2.In almost all the cases the character based modelsperformed better in terms of recall and F-measurethan the word based models.We also experimented changing the training datasize keeping the testing data size unchanged for Tel-ugu(Table 7) and English(Table 8) and Hindi(Table9).
From Table 7:All the models (words,charactern-gram models) are able to learn as we increase thetraining data size.
And the recall of the charactern-gram models is considerably more than recall ofthe word based model.
Also the 3-gram model per-formed well in almost all the runs.
The rate of learn-ing is more in case of 30K.From Table 8, in all the runs, the bi-gram char-acter model constantly performed the best.
Alsointerestingly the model is able to achieve a leastF-measure of 44.75 with just 10K words of train-ing data.
But, in case of Telugu,(Table 7) an F-measure of 44+ was reached with training data ofsize 35K i.e the learning rate for english is more forless amount of data.
This is due to the reason thatTelugu (Entropy=15.625 bits per character) (Bharatiet al, 1998) is comparitively a high entropy lan-guage than English (Brown and Pietra, 1992).
How-ever for Hindi, the relative jump in the performance(compared to Telugu and English)is less.
Even theentropy of Hindi (Entorpy=11.088) (Bharati et al,1998) is more than English.
This is also observedfrom the table (Table 10).
The numbers in the sec-ond, third and fourth columns are the number of fea-tures for English,Telugu and Hindi respectively.English Telugu Hindiwords 29145 320260 685032n=2 27707 267340 647109n=3 45580 680720 1403352n=4 64284 1162320 1830438n=5 65248 1359980 1735614n=6 57297 1278790 1433322Table 10: Number of features calculated in the wordbased model for English,Telugu and Hindi.7 Conclusion & Future WorkThe character based n-gram approach worked bet-ter than the word based approach even with agglu-tinative languages.
A considerably good NER for71Language English Telugu HindiPrecision Recall F?=1 Precision Recall F?=1 Precision Recall F?=1Words 92.42% 47.29% 62.56 70.38% 23.83% 35.6 51.66% 36.45% 42.74n=2 81.21% 68.77% 74.47 65.67% 37.11% 47.42 37.30% 36.06% 36.67n=3 88.37% 62.45% 73.18 71.39% 38.02% 49.62 54.89% 37.23% 44.37n=4 93.17% 59.19% 72.39 70.17% 33.07% 44.96 54.67% 37.62% 44.57n=5 90.71% 58.30% 70.98 66.57% 29.82% 41.19 53.78% 38.79% 45.07n=6 91.03% 56.14% 69.45 55.68% 25.52% 35 51.79% 36.65% 42.92Table 6: Average Precision, Recall and F?=1 measure for English, Telugu and Hindi ?n?
indicates the numberof n-gram charactersSize 10K 20K 30K 35KModel P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1words 58.04 8.46 14.77 56.54 14.06 22.52 67.90 21.48 32.64 71.03 23.31 35.1n=2 53.81 13.80 21.97 60.31 25.52 35.86 63.68 31.51 42.16 65.16 35.55 46n=3 68.07 14.71 24.2 64.71 24.35 35.38 70.22 32.55 44.48 71.79 37.11 48.93n=4 71.23 13.54 22.76 63.42 21.22 31.8 68.14 28.12 39.82 68.16 31.77 43.34n=4 69.92 11.20 19.3 61.20 19.92 30.06 63.90 26.04 37 66.96 29.30 40.76n=6 52.38 8.59 14.77 52.70 16.54 25.17 56.13 22.66 32.28 55.16 24.35 33.79Table 7: Effect of training data size on Average Precision,Recall and F?=1 measure for Telugu.Size 10K 20K 30K 35KModel P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1words 81.84 30.79 44.75 86.54 40.93 55.57 89.04 45.95 60.62 89.80 46.35 61.14n=2 71.49 42.00 52.92 74.80 58.40 65.59 75.46 61.03 67.49 76.63 61.87 68.46n=3 76.09 28.85 41.84 81.15 50.03 61.9 81.31 54.28 65.11 82.18 56.84 67.2n=4 83.42 25.75 39.36 83.35 42.93 56.67 88.01 48.70 62.7 87.40 50.25 63.81n=5 81.95 25.64 39.06 84.48 41.00 55.21 86.81 44.47 58.81 88.07 47.43 61.66n=6 79.24 26.89 40.16 83.31 38.18 52.36 89.34 42.88 57.95 87.71 44.32 58.88Table 8: Effect of training data size on Average Precision,Recall and F?=1 measure for English.Size 10K 20K 30K 35KModel P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1words 43.13 30.60 35.80 47.97 34.50 40.14 48.67 35.67 41.17 51.92 36.84 43.10n=2 39.29 30.41 34.29 40.73 34.70 37.47 37.58 36.26 36.90 37.91 36.06 36.96n=3 48.17 33.33 39.40 50.56 35.28 41.56 47.72 36.65 41.46 50.68 36.06 42.14n=4 49.18 35.09 40.96 49.21 36.26 41.75 52.14 35.67 42.36 54.87 38.40 45.18n=5 41.08 34.11 37.27 41.93 33.92 37.50 48.72 37.23 42.21 53.12 39.77 45.48n=6 41.43 31.58 35.84 44.59 33.72 38.40 46.35 35.87 40.44 50.67 36.84 42.66Table 9: Effect of training data size on Average Precision,Recall and F?=1 measure for Hindi.English can be built with less amount of data whenwe use character based models and for high entropylanguages large amount of training data is necessaryto build a considerably good NER.
We are able toachieve an F-measure (49.62 for Telugu and 45.07for Hindi) even without any extra features like regu-72lar expressions and gazetteer information.
The char-acter based n-gram models have worked well evenwith the discriminative models.
A total of 9 featureswere used in training and testing.
We have not usedany of the language dependent resources and any bi-nary features.
To improve the efficiency of the sys-tem we plan to experiment with language specificresources like Part Of Speech (POS) Taggers, Chun-kers, Morphological analyzers.. etc and also includesome regular expressions and gazetteer information.ReferencesBogdan Babych and Anthony Hartley.
2003.
Improv-ing machine translation quality with automatic namedentity recognition.Akshar Bharati, Prakash Rao K, Rajeev Sangal, andS.M.Bendre.
1998.
Basic statistical analysis of cor-pus and cross comparison among corpora.
Tech-nical report, International Institute of InformationTechnology-Hyderabad(IIIT-H).D.
Bikel, S. Miller, R. Schwartz, and R. Weischedel.1997.
Nymble: a high-performance learning name-finder.Peter E Brown and Vincent J. Della Pietra.
1992.
Anestimate of an upper bound for the entropy of english.Nancy Chinchor.
1997.
Muc-7 named entity task defini-tion.
Technical Report Version 3.5, Science Applica-tions International Corporation, Fairfax, Virginia.S.
Cucerzan and D. Yarowsky.
1999.
Language indepen-dent named entity recognition combining morphologi-cal and contextual evidence.D.
Klein, J. Smarr, H. Nguyen, and C. Manning.
2003.Named entity recognition with character-level models.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.
Morgan Kaufmann, San Francisco, CA.Andrei Mikheev.
1997.
Automatic rule inductionfor unknown-word guessing.
Comput.
Linguist.,23(3):405?423.Diego Molla, Menno van Zaanen, and Daniel Smith.2006.
Named entity recognition for question answer-ing.
In Proceedings of Australasian Language Tech-nology Workshop 2006, Sydney, Australia.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarovsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora, pages82?94, Somerset, New Jersey.
Association for Compu-tational Linguistics.Antonio Toral, Elisa Noguera, Fernando Llopis, andRafael Mun?oz.
2005.
Improving question answeringusing named entity recognition.
In Proceedings of the10th NLDB congress, Lecture notes in Computer Sci-ence, Alicante, Spain.
Springer-Verlag.Richard Tzong-Han Tsai.
2006.
A hybrid approach tobiomedical named entity recognition and semantic rolelabeling.
In Proceedings of the 2006 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy, pages 243?246, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.N.
Wacholder, Y. Ravin, and M. Choi.
1997.
Disam-biguation of proper names in text.Hanna M. Wallach.
2004.
Conditional random fields: Anintroduction.
Technical Report MS-CIS-04-21, Uni-versity of Pennsylvania, Department of Computer andInformation Science, University of Pennsylvania.7374
