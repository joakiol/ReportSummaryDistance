Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 78?86,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsNamed Entity Recognition for Dialectal ArabicAyah ZiriklyDepartment of Computer ScienceThe George Washington UniversityWashington DC, USAayaz@gwu.eduMona DiabDepartment of Computer ScienceThe George Washington UniversityWashington DC, USAmtdiab@gwu.eduAbstractTo date, majority of research for Ara-bic Named Entity Recognition (NER) ad-dresses the task for Modern Standard Ara-bic (MSA) and mainly focuses on thenewswire genre.
Despite some commoncharacteristics between MSA and Dialec-tal Arabic (DA), the significant differencesbetween the two language varieties hindersuch MSA specific systems from solvingNER for Dialectal Arabic.
In this paper,we present an NER system for DA specif-ically focusing on the Egyptian Dialect(EGY).
Our system delivers ?
16% im-provement in F1-score over state-of-the-art features.1 IntroductionNamed Entity Recognition (NER) aims to iden-tify predefined set of named entities types (e.g.Location, Person) in open-domain text (Nadeauand Sekine, 2007).
NER has proven to be an es-sential component in many Natural Language Pro-cessing (NLP) and Information Retrieval tasks.
In(Thompson and Dozier, 1997), the authors showthe significant impact NER imposes on the re-trieval performance, given the fact that names oc-cur with high frequency in text.
Moreover, inQuestion Answering, (Ferrndez et al., 2007) re-port that Questions on average contain ?
85%Named Entities.Although NER has been well studied in the liter-ature, but the majority of the work primarily fo-cuses on English in the newswire genre, with near-human performance (f-score?
93% in MUC-7).Arabic NER has gained significant attention in theNLP community with the increased availability ofannotated datasets.
However, due to the rich mor-phological and highly inflected nature of Arabiclanguage (Ryding, 2005), Arabic NER faces manychallenges (Abdul-Hamid and Darwish, 2010),that manifest in:?
Lack of capitalization: Unlike English (andother Latin-based languages), proper nounsare not capitalized, which renders the iden-tification of NER more complicated;?
Proper nouns can also represent regularwords (e.g.
jamilah, gmylp1?
which means?beautiful?
and can be a proper noun or an ad-jective;?
Agglutination: Since Arabic exhibits con-catenate morphology, we note the pervasivepresence of affixes agglutinating to propernouns as prefixes and suffixes (Shaalan,2014).
For instance: Determiners appear asprefixes as in Al (AlqAhrp ?Cairo?
), likewisewith affixival prepositions such as l meaning?for?
(ldm$q -?to/from Damascus?-), as wellas prefixed conjunctions such as w meaning?and?
(wAlqds -?and Jerusalem?-);?
Absence of Short Vowels (Diacritics): Writ-ten MSA, even in newswire, is undiacritized;resulting in ambiguity that can only be re-solved using contextual information (Bena-jiba et al., 2009).
Instances of such phe-nomena: mSr, which is underspecified forshort vowels, can refer to miSor ?Egypt?
ormuSir ?insistent?
; qTr may be ?Qatar?
ifqaTar, ?sugar syrup?
if qaTor, ?diameter?
ifquTor.Previously proposed Arabic NER systems (Be-najiba et al., 2007) and (Abdallah et al., 2012)were developed exclusively for MSA and primar-ily address the problem in the newswire genre.Nevertheless, with the extensive use of social net-working and web blogs, DA NLP is gaining more1The second form of the name is written in Buckwalterencoding http://www.qamus.org/transliteration.htm78attention, yielding a more urgent need for DANER systems.
Furthermore, applying NLP tools,such as NER, that are designed for MSA on DAresults in considerably low performance, thus theneed to build resources and tools that specificallytarget DA (Habash et al., 2012).In addition to the afore mentioned challenges forArabic NER in general compared to Latin basedlanguages, DA NER faces additional issues:?
Lack of annotated data for supervised NER;?
Lack of standard orthographies or languageacademics (Habash et al., 2013): UnlikeMSA, the same word in DA can be rewrittenin so many forms, e.g.
mAtEyT$, mtEyt$, mAtEyT$ ?do not cry?
are all acceptable variantssince there is no one standard;?
Lack of comprehensive enough Gazetteers:this is a problem facing all NER systemsfor all languages addressing NER in socialmedia text, since by definition such mediahas a ubiquitous presence of highly produc-tive names exemplified by the usage of nicknames, hence the PERSON class in socialmedia NER will always have a coverageproblem.In this paper, we propose a DA NER system ?using Egyptian Arabic (EGY) as an example di-alect.
Our contributions are as follows:?
Provide an annotated dataset for EGY NER;?
To the best of our knowledge, our system isone of the few systems that specifically tar-gets DA.2 Related WorkSignificant amount of work in the area of NERhas taken place.
In (Nadeau and Sekine, 2007),the authors survey the literature of NER andreport on the different set of used features suchas contextual and morphological.
Althoughmore research has been employed in the area ofEnglish NER, Arabic NER has been gaining moreattention recently.
Similar to other languages,several approaches have been used for ArabicNER: Rule-based methods, Statistical Learningmethods, and a hybrid of both.In (Shaalan and Raza, 2009), the authorspresent rule-based NER system for MSA thatcomprises gazetteers, local grammars in the formof regular expressions, and a filtering mechanismthat mainly focuses on rejecting incorrect NEsbased on a blacklist.
Their system yields a perfor-mance of 87.7% F1 measure for PER, 85.9% forLOC, and 83.15% for ORG when evaluated oncorpora built by the authors.
(Elsebai et al., 2009)proposed a rule-based system that is targeted forpersonal NEs in MSA and utilizes the BuckwalterArabic Morphological Analyser (BAMA) and aset of keywords used to introduce a PER NE.
Theproposed system yields an F-score of 89% whentested on a dataset of 700 news articles extractedfrom Aljazeera television website.
Although thisapproach proved to be successful, but most of therecent research focuses on Statistical Learningtechniques for NER (Nadeau and Sekine, 2007).In the area of Statistical Learning for NER,numerous research studies have been published.
(Benajiba et al., 2007) proposes a system (ANER-sys) based on n-grams and maximum entropy.
Theauthors also introduce ANERCorp corpora andANERGazet gazetteers.
(Benajiba and Rosso,2008) presents NER system (ANERsys) for MSAbased on CRF sequence labeling, where thesystem uses language independent features: POStags, Base Phrase Chunking (BPC), gazetteers,and nationality information.
The latter feature isincluded based on the observation that personalNEs come after mentioning the nationality, inparticular in newswire data.
In (Benajiba et al.,2008), a different classifier is built for each NEtype.
The authors study the effect of featureson each NE type, then the overall NER systemis a combination of the different classifiers thattarget each NE class label independently.
Theset of features used are a combination of generalfeatures as listed in (Benajiba and Rosso, 2008)and Arabic-dependent (morphological) features.Their system?s best performance was 83.5% forACE 2003, 76.7% for ACE 2004, and 81.31% forACE 2005, respectively.
(Benajiba et al., 2010)presents an Arabic NER system that incorporateslexical, syntactic, and morphological features andaugmenting the model with syntactic featuresderived from noisy data as projected from Arabic-English parallel corpora.
The system F-scoreperformance is 81.73%, 75.67%, 58.11% onACE2005 Broadcast News, Newswire, and Webblogs respectively.
The authors in (Abdul-Hamidand Darwish, 2010) suggest a number of features,that we incorporate a subset of in our DA NER79system, namely, the head and trailing bigrams(L2), trigrams (L3), and 4-grams (L4) characters.
(Shaalan and Oudah, 2014) presents a hybridapproach that targets MSA and produces state-of-the-art results.
However, due to the lackof availability of the used rules, it is hard toreplicate their results.
The rule-based componentis identical to their previous proposed rule-basedsystem in (Shaalan and Raza, 2009).
The featuresused are a combination of the rule-based featuresin addition to morphological, capitalization, POStag, word length, and dot (has an adjacent dot)features.
We reimplement their Machine Learningcomponent and present it as one of our baselines(BAS2).
(Abdul-Hamid and Darwish, 2010)produce near state-of-the-art results with the useof generic and language independent features thatwe use to generate baseline results (BAS1).
Theproposed system does not rely on any externalresources and the system outperforms (Benajibaand Rosso, 2008) performance with an F-score of81% on ANERCorp vs. the latter?s performanceof 72.68% F-score.
All the work mentioned hasfocused on MSA, albeit with variations in genresto the extent exemplified by the ACE data andauthor generated data.
However unlike the workmentioned above, (Darwish and Gao, 2014)proposed an NER system that specifically targetsmicroblogs as a genre, as opposed to newswiredata.
Their proposed language-independentsystem relies on set of features that are similarto (Abdul-Hamid and Darwish, 2010).
Theirdataset contains dialectal data, since it is collectedfrom Twitter.
However, the dataset containsEnglish and Arabic; in this work we only targetDialectal Arabic.
Their overall performance, ontheir proposed data, is 65.2% (LOC 76.7%, 55.6%ORG, 55.8% PER).3 ApproachIn this paper, we use a supervised machine learn-ing approach since it has been shown in the litera-ture that supervised typically outperform unsuper-vised approaches for the NER task (Nadeau et al.,2006).
We use Conditional Random Field (CRF)sequence labeling as described in (Lafferty et al.,2001).
Moreover, (Benajiba and Rosso, 2008)demonstrates that CRF yields better results overother supervised machine learning techniques.3.1 BaselineIn this paper, we introduce two baselines to com-pare our work against.
The first baseline (BAS1)is based on work reported in (Abdul-Hamid andDarwish, 2010).
We adopt their approach sinceit produces near state-of-the-art results.
Addition-ally, the features proposed are applicable to DA asthey do not rely on the availability of morphologi-cal or syntactical analyzers.
We reimplement theirlisted features that yield the highest performanceand report those results as our BAS1 system.
Thelist of features used are: previous and next word,in addition to the leading and trailing character bi-grams, trigrams, and 4-grams.The second baseline (BAS2) adopted is thework proposed in (Shaalan and Oudah, 2014).The authors present state-of-the-art results whenevaluated on ANERcorp (Benajiba and Rosso,2008) using the following features: Rule-basedfeatures, Morphological features generated byMADAMIRA (Pasha et al., 2014) presented inTable 1, targeted word POS tag, word length flagwhich is a binary feature that is true if the wordlength is?
3, a binary feature to represent whetherthe word has an adjacent dot, capitalization bi-nary feature which is dependent on the Englishgloss generated by MADAMIRA, nominal binaryfeature that is set to true if the POS tag is nounor proper noun, and binary features to representwhether the current, previous, or next word belongto the gazetteers.
We omit Rule-based features inour baseline since we do not have access to the ex-act rules used and their rules specifically targetedMSA, hence would not be directly applicable toDA.3.2 NER FeaturesIn our approach, we propose the following NERfeatures:?
Lexical Features: Similar to BAS1 (Darwishand Gao, 2014) character n-gram features,the head and trailing bigrams (L2), trigrams(L3), and 4-grams (L4) characters;?
Contextual Features (CTX): The surround-ing undiacritized lemmas and words of a con-text window = ?1; (LEM-1, LEM0, LEM1)and (W-1,W0,W1)?
Gazetteers (GAZ): We use two sets ofgazetteers.
The first set (ANERGaz) pro-posed by (Benajiba and Rosso, 2008), which80Feature Feature ValuesAspect Verb aspect: Command, Imperfective, Perfective, Not applicableCase Grammatical case: Nominative, Accusative, Genitive, Not applicable, UndefinedGender Nominal Gender: Feminine, Masculine, Not applicableMood Grammatical mood: Indicative, Jussive, Subjunctive, Not applicable, UndefinedNumber Grammatical number: Singular, Plural, Dual, Not applicable, UndefinedPerson Person Information: 1st, 2nd, 3rd, Not applicableState Grammatical state: Indefinite, Definite, Construct/Poss/Idafa, Not applicable, UndefinedVoice Verb voice: Active, Passive, Not applicable, UndefinedProclitic3 Question proclitic: No proclitic, Not applicable, Interrogative particleProclitic2 Conjunction proclitic: No proclitic, Not applicable, Conjunction fa, Connective particle fa, Response condi-tional fa, Subordinating conjunction fa, Conjunction wa, Particle wa, Subordinating conjunction waProclitic1 Preposition proclitic: No proclitic, Not applicable, Interrogative i$, Particle bi, Preposition bi, Progressive verbparticle bi, Preposition Ea, Preposition EalaY, Preposition fy, Demonstrative hA, Future marker Ha, Prepositionka, Emphatic particle la, Preposition la, Preposition li + preposition bi, Emphatic la + future marker Ha,Response conditional la + future marker Ha, Jussive li, Preposition li, Preposition min, Future marker sa,Preposition ta, Particle wa, Preposition wa, Vocative wA, vocative yAProclitic Article proclitic: No proclitic, Not applicable, Demonstrative particle Aa, Determiner, Determiner Al + negativeparticle mA, Negative particle lA, Negative particle mA, Negative particle mA, Particle mA, relative pronoun mAEnclitics Pronominals: No enclitic, Not applicable, 1st person plural/singular, 2nd person dual/plural, 2nd person fem-inine plural/singular, 2nd person masculine plural/singular, 3rd person dual/plural, 3rd person feminine plu-ral/singular, 3rd person masculine plural/singular, Vocative particle, Negative particle lA, Interrogative pronounmA, Interrogative pronoun mA, Interrogative pronoun man, Relative pronoun man, ma, mA, Subordinating con-junction ma, mA.Table 1: Morphological Featurescontains a total of 4893 names between Per-son (PER), Location (LOC), and Organiza-tion (ORG).
The second gazetteer is a largeWikipedia gazetteer (WikiGaz) from (Dar-wish and Gao, 2014); 50141 locations, 17092organizations, 65557 persons.
which repre-sents a significantly more extensive and com-prehensive list.
We introduce three methodsfor exploiting GAZ:?
Exact match (EM-GAZ): For moreefficient search, we use Aho-CorasickAlgorithm that has linear runningtime in terms of the input length plusthe number of matching entries ina gazetteer.
When a word sequencematches an entry in the gazetteer,EM-GAZ for the first word will takethe value ?B-<NE class>?
where<NE class>is one of the previouslydiscussed classes (PER, LOC, ORG),whereas the following words will beassigned I-<NE class>, where <NEclass>will be assigned the same valueof the matched sequence?s head;?
Partial match(PM-GAZ): This feature iscreated to handle the case of compoundgazetteer entries.
If the token is part ofthe compound name then this feature isset to true.
For example, if we have inthe gazetteer the compound name yAsrErfAt ?Yasser Arafat?
and the input textis yAsr BarakAt then PM-GAZ for thetoken yAsr will be set to true.
This isparticularly useful in the case of PER asit recovers a large list of first names incompounds;?
Levenshtein match (LVM-GAZ): Dueto the non-standard spelling of wordsin dialectal Arabic, we use Levenshteindistance (Levenshtein, 1966) to com-pare the similarity between the inputand a gazetteer entry;?
Morphological Features: The morphologi-cal features that we employ in our feature setare generated by MADAMIRA (Pasha et al.,2014):?
Gender (GEN): Since Arabic nounsare either masculine or feminine, webelieve that this information shouldhelp NER.
Moreover, instances of thesame name will share the same gender.MADAMIRA generates three valuesfor this feature: Feminine, Masculine,or Not Applicable (such as the case forprepositions, for instance);81?
Capitalization (CAPS): In order tocircumvent the lack of capitalization inArabic, we check the capitalization ofthe translated NE which could indicatethat a word is an NE (Benajiba et al.,2008).
This feature is dependent onthe English gloss that is generated byMADAMIRA;?
Part of Speech (POS) tags: We use POStags generated from MADAMIRA,where the POS tagger has a reportedaccuracy of 92.4% for DA;?
Distance from specific keywords within awindow (KEY): This feature captures certainpatterns in person names that are more com-monly used in DA (e.g.
using the nicknamepattern of Abw + proper noun instead of anactual name).
In this feature, if the distanceis set to one, the feature will be true if theprevious token equals an entry in a keywordslist, otherwise false.
Examples of keywords:Abw ?father of?, yA invocation particle, typ-ically used before names to call a person,terms of address, or honorifics, such as dk-twr/dktwrp ?doctor -masculine and feminine-?, and AstA*/AstA*p ?Mr/Mrs/Ms/teacher -masculine and feminine-?;?
Brown Clustering (BC): Brown clusteringas introduced in (Brown et al., 1992) is ahierarchical clustering approach that maxi-mizes the mutual information of word bi-grams.
Word representations, especiallyBrown Clustering, have been demonstratedto improve the performance of NER systemwhen added as a feature (Turian et al., 2010).In this work, we use Brown Clustering IDsof variable prefixes length (4,7,10,13) as fea-tures resulting in the following set of featuresBC4, BC7, BC10, BC13.
For example ifAmrykA ?America?
has the brown cluster ID11110010 then BC4 = 1111, BC7=1111001,whereas BC10 and BC13 are empty strings.This feature is based on the observation thatsemantically similar words will be groupedtogether in the same cluster and will have acommon prefix.4 Experiments & Discussion4.1 Datasets and ToolsEvaluation Data Due to the very limited re-sources in DA for NER, we manually annotate aportion of the DA data collected and provided bythe LDC from web blogs.2The annotated datawas chosen from a set of web blogs that are man-ually identified by LDC as Egyptian dialect andcontains nearly 40k tokens.
The data was anno-tated by one native Arabic speaker annotator whofollowed the Linguistics Data Consortium (LDC)guidelines for NE tagging.
Our dataset is rela-tively small and contains 285 PER, 153 LOC, and10 ORG instances.Brown Clustering Data In our work, we runBrown Clustering on BOLT Phase1 Egyptian Ara-bic Treebank (ARZ)3, where the chosen numberof clusters is 500.Parametric features values We use the follow-ing values for the parametric features:?
CTX features: we set context window = ?1for lemmas and tokens;?
Keyword distance: we set the distance fromthe token to a keyword to 1 and 2, namely,KEY1 and KEY2, respectively;?
LM-GAZ: The threshold of the number ofdeletion, insertion, or modification ?
2;?
BC: the length of the prefixes of the BrownClusters ID is set to 4,7,10,13;Tools In this work, we used the following tools:1.
MADAMIRA (Pasha et al., 2014): For tok-enization and other features such as lemmas,gender and Part of Speech (POS) tags, andother morphological features;2.
CRFSuite implementation (Okazaki, 2007).4.2 Evaluation MetricsWe choose precision (PREC), recall (REC), andharmonic F-measure (F1) metrics to evaluate theperformance of our NER system over accuracy.This decision is based on the observation that thebaseline accuracy on the token level in NER is not2GALE Arabic-Dialect/English Parallel TextLDC2012T093LDC2012E9882a fair assessment, since NER accuracy is alwayshigh as the majority of the tokens in free text arenot named entities.4.3 Results & DiscussionIn our NER system, we solely identify PER andLOC NE classes and omit the ORG class.
Thisis due to the small frequency (?
0.05%) of ORGinstances in our annotated data, which does notrepresent a fair training data to the system.
Thereported results are the average of 5-fold crossvalidation on the blog post level.
Also, it is worthmentioning that we use IOB tagging scheme;Inside I NE, Outside O, and Beginning B ofNE.
Table 2 depicts the two baselines discussedin 3.1.
BAS1 yields a weighted macro-averageF-score=54.762% using near state-of-the-artfeatures on our annotated data.
On the other hand,BAS2 F-score is 31%.
Although BAS2 presentsstate-of-the-art results, it actually produces lowerperformance than BAS1.
It should be noted thatour implementation of BAS2 does not incorpo-rate rule-based features (Shaalan and Oudah,2014).
However, by extrapolation using theirperformance improvement of ?
6% attributed torule-based features alone, such a relative gain inperformance for BAS2 in our setting would stillbe outperformed by both BAS1 and our currentsystem.In Table 3, we show our NER system perfor-mance using different permutations of featuresproposed in Section 3.2.
Additionally, in Table 3,we use the weighted macro-average (Overall) inorder to assess the system?s overall performance.We use the following abbreviation annotation:?
FEA1: includes n-gram characters and CTXon the word and lemma level features;?
FEA2: includes FEA1 in addition to KEYfeatures with distance 1&2;?
FEA3: includes FEA2 in addition to the mor-phological features (MORPH) and it is sub-categorized as follow: FEA3-GEN takes intoaccount the gender feature only, FEA3-POStakes into account POS tag (FEA2+POS),whereas FEA3-CAPS takes into account theuse of CAPS with FEA2;?
FEA4: shows the impact of adding EM-GAZfeatures (FEA3+EM-GAZ);?
FEA5: shows the impact of adding PM-GAZfeatures (FEA4+PM-GAZ);?
FEA6: shows the impact of adding LVM-GAZ features (FEA5+LM-GAZ);?
FEA7: shows the impact of adding BrownClustering (BC) features on the performance;The best results for precision, recall and F1-score are bolded in Table 3.
FEA6 delivers thebest NER performance of F1-score=70.305%Baseline PREC REC F1BAS1LOC 80 72.727 76.191PER 56.25 23.684 33.333AVG 68.125 48.201 54.762BAS2LOC 47.368 52.941 50PER 8.571 20 12AVG 27.97 36.471 31Table 2: Baseline NER performanceIn comparing FEA1, FEA2 results, we note thatKEY features increase the F1-score by 2% ab-solute.
This improvement mirrors the fact thatAbw+name, for example, is very commonly usedin dialects, where it represents ?
46% of PERnames.
The morphological features (GEN, POS,CAPS), produce the most significant improvement?
+9% absolute.
Although the gazetteers helpNER performance overall, the boost is not as sig-nificant as with using the MORPH features.
Like-wise, we note that LVM-GAZ using Levenshteindistance addresses the spelling variation challengethat DA pose and yields the best performance (F1-score=70.305%) when combining all features ex-cept the Brown clustering.
Unlike the BC effectnoted in English NER case studies, BC degradesthe performance of our DA NER system.
We fur-ther analyze this result by closely examining theclustering quality obtained on the dataset.
For ex-ample, the following instances of the LOC classfrom our dataset: mSr ?Egypt?, AmrykA ?Amer-ica?, and qtr ?Qatar?
; the cluster IDs assigned bythe Brown Clustering algorithm are 111101110,11110010, 00111000, respectively.
The commonprefix among the three instances is very short(1111 in case of Egypt and America and none withQatar), thus leading to poorer performance.Overall, we note more stable performance forLOC class in comparison to PER.
This is mainlydue to the high PER singleton instances frequen-cies which results in high unseen vocabulary in83FeaturesLOC PER OverallPREC REC F1 PREC REC F1 PREC REC F1FEA1={L2,L3,L4,W-1,W0,W1,LEM-1,LEM0,LEM1} 93.333 77.778 84.849 54.546 14.286 22.642 73.94 46.032 53.746FEA2={FEA1, KEY1, KEY2} 93.75 83.333 88.235 60 14.286 23.077 76.875 48.81 55.656FEA3-GEN={FEA2, GEN} 93.75 83.333 88.235 63.636 16.667 26.415 78.693 50 57.325FEA3-POS={FEA2, POS} 93.333 77.778 84.849 78.571 26.191 39.286 85.952 51.985 62.068FEA3-CAPS={FEA2, CAPS} 93.333 77.778 84.849 78.571 26.191 39.286 85.952 51.985 62.068FEA3={FEA2, MORPH} 94.118 88.889 91.429 83.333 23.81 37.037 88.7255 56.3495 64.233FEA4={FEA3, EM-GAZ} 94.118 88.889 91.429 72.222 30.952 43.333 83.17 59.9205 67.381FEA5={FEA4, PM-GAZ} 94.118 88.889 91.429 73.684 33.333 45.902 83.901 61.111 68.666FEA6={FEA5, LVM-GAZ} 94.118 88.889 91.429 78.947 35.714 49.18 86.533 62.302 70.305FEA7={FEA6, BC} 93.333 77.778 84.849 77.778 33.333 46.667 85.556 55.556 65.758Table 3: Dialectal Arabic NERthe test data.
In addition, LOC members, unlikePER, convey tag consistency, where most of thetime it will be tagged as NE.
For instance, mSr?Egypt?
occurred in the data 35 times and in all ofwhich it was assigned a LOC tag, unlike EAdl thatappears as an adjective ?fair/rightful?
and propername ?Adel?
in the same dataset.
The former rea-son explains why the GAZ helps PER class per-formance but does not affect LOC performance.If we discuss in more detail the MORPH featureset, we notice that CAPS and POS produce identi-cal results in terms of PREC, REC, and F-1 scoreon each of the NE classes.
However, CAPS andPOS help in PER class, whereas GEN helps in theLOC class.
For example in LOC class, the num-ber of false negatives, when POS is employed, ishigher as opposed to GEN.As mentioned earlier, LVM-GAZ produces thebest F-score.
However, LVM main contributionis on the PER class which is caused by the natureof Arabic names?
different spelling variations, es-pecially the last name (e.g.
with or without Al).5 Conclusion & Future WorkIn this paper we present Dialectal Arabic NERsystem using state-of-the-art features in addi-tion to proposing new features that improvethe performance.
We show that our proposedsystem improves over state-of-the-art featuresperformance.
Our contribution is not solelylimited to the NER system, but further includes,our manually annotated data.4In future work,we would like to annotate more data in morevariable genre and with more dialects includingcode switched data.4Please contact the authors for access to the annotateddata.6 AcknowledgmentThis work was supported by the Defense Ad-vanced Research Projects Agency (DARPA) Con-tract No.
HR0011-12-C-0014, the BOLT programwith subcontract from Raytheon BBN.84ReferencesSherief Abdallah, Khaled Shaalan, and MuhammadShoaib.
2012.
Integrating rule-based system withclassification for arabic named entity recognition.
InComputational Linguistics and Intelligent Text Pro-cessing, pages 311?322.
Springer.Ahmed Abdul-Hamid and Kareem Darwish.
2010.Simplified feature set for arabic named entity recog-nition.
In Proceedings of the 2010 Named EntitiesWorkshop, NEWS ?10, pages 110?115, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Yassine Benajiba and Paolo Rosso.
2008.
Arabicnamed entity recognition using conditional randomfields.
In Proc.
of Workshop on HLT & NLP withinthe Arabic World, LREC, volume 8, pages 143?153.Citeseer.Yassine Benajiba, Paolo Rosso, and Jos?e-MiguelBened??.
2007.
Anersys: An arabic named entityrecognition system based on maximum entropy.
InCICLing, pages 143?153.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2008.Arabic named entity recognition using optimizedfeature sets.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 284?293.
Association for ComputationalLinguistics.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2009.Arabic named entity recognition: A feature-drivenstudy.
Audio, Speech, and Language Processing,IEEE Transactions on, 17(5):926?934.Yassine Benajiba, Imed Zitouni, Mona Diab, and PaoloRosso.
2010.
Arabic named entity recognition:Using features extracted from noisy data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,ACLShort ?10, pages 281?285, Stroudsburg, PA,USA.
Association for Computational Linguistics.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Kareem Darwish and Wei Gao.
2014.
Simple effec-tive microblog named entity recognition: Arabic asan example.
In Proceedings of the Ninth Interna-tional Conference on Language Resources and Eval-uation (LREC-2014), Reykjavik, Iceland, May 26-31, 2014., pages 2513?2517.Ali Elsebai, Farid Meziane, and Fatma ZohraBelkredim.
2009.
A rule based persons namesarabic extraction system.
Communications of theIBIMA, 11(6):53?59.Sergio Ferrndez, Antonio Toral, scar Ferrndez, An-tonio Ferrndez, and Rafael Muoz.
2007.
Ap-plying wikipedias multilingual knowledge to cross-lingual question answering.
In In Zoubida Kedad,Nadira Lammari, Elisabeth Mtais, Farid Meziane,and Yacine Rezgui, editors, NLDB, volume 4592 ofLecture Notes in Computer Science.
Springer.Nizar Habash, Mona T Diab, and Owen Rambow.2012.
Conventional orthography for dialectal ara-bic.
In LREC, pages 711?718.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
Morphologi-cal analysis and disambiguation for dialectal arabic.In Human Language Technologies: Conference ofthe North American Chapter of the Association ofComputational Linguistics, Proceedings, June 9-14,2013, Westin Peachtree Plaza Hotel, Atlanta, Geor-gia, USA, pages 426?432.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.Vladimir I Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
InSoviet physics doklady, volume 10, page 707.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30(1):3?26.David Nadeau, Peter Turney, and Stan Matwin.
2006.Unsupervised named-entity recognition: Generatinggazetteers and resolving ambiguity.Naoaki Okazaki.
2007.
Crfsuite: A fast implementa-tion of conditional random fields (crfs).Arfath Pasha, Mohamed Al-Badrashiny, Ahmed ElKholy, Ramy Eskander, Mona Diab, Nizar Habash,Manoj Pooleery, Owen Rambow, and Ryan Roth.2014.
Madamira: A fast, comprehensive tool formorphological analysis and disambiguation of ara-bic.
In In Proceedings of the 9th InternationalConference on Language Resources and Evaluation,Reykjavik, Iceland.Karin C Ryding.
2005.
A Reference Grammar of Mod-ern Standard Arabic.
Cambridge University Press.Khaled Shaalan and Mai Oudah.
2014.
A hybrid ap-proach to arabic named entity recognition.
Journalof Information Science, 40(1):67?87.Khaled Shaalan and Hafsa Raza.
2009.
Nera: Namedentity recognition for arabic.
Journal of the Ameri-can Society for Information Science and Technology,60(8):1652?1663.Khaled Shaalan.
2014.
A survey of arabic namedentity recognition and classification.
Comput.
Lin-guist., 40(2):469?510, June.Paul Thompson and Christopher C. Dozier.
1997.Name searching and information retrieval.
In InProceedings of Second Conference on EmpiricalMethods in Natural Language Processing, pages134?140.85Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.86
