UNIVERSITY OF DURHAM:DESCRIPTION OF THE LOLITA SYSTEM AS USED IN MUC-7Roberto Garigliano, Agnieszka Urbanowicz and David J. Nettleton.Laboratory for Natural Language Engineering,Department of Computer Science,University of Durham,South Road,Durham.
DH1 3LEUnited Kingdomemail: R.Garigliano@durham.ac.ukINTRODUCTIONThis document describes the LOLITA system and how it was used to run the MUC tasks of named entity, co-reference and template element.
Details of the system?s performance are given for the walk-through articles as wellas overall performance.LOLITA has been designed in such a way that the code implementing the MUC tasks is only a small part of thewhole system.
A core system provides complex facilities with the MUC system being built so that it utilises thesefacilities.
Hence, after some background to the LOLITA project, the ?core?
of LOLITA is described.
This systemdescription is substantially similar to that given for MUC-6 [1].
However, some important changes to theunderlying core system have occurred since MUC-6 and these are presented in their own section.
Following this is adescription of changes that were required specifically for MUC-7.An analysis of the system?s performance for the walk-through articles is presented together with an overall viewof the system?s performance.
Prior to the conclusions a section on further and ongoing work is provided.
Thisincludes brief descriptions of some important work that is being undertaken, in particular: a re-engineering of thesystem which will result in a C++ version, and the addition of a large number of dictionary word definitions to theknowledge base.BACKGROUNDThe LOLITA (Large-scale, Object-based, Linguistic Interactor, Translator, and Analyser) system is designed asa general purpose Natural Language Processing (NLP) system and has been under development at the University ofDurham since 1986.
The system is designed to provide NLP capabilities to support many applications in multipledomains.
It attempts to do this by providing a core platform upon which different applications can be built.
Thiscore platform provides two main facilities: analysis, which converts text to a logical representation of its meaning,and generation, which expresses information represented in this logical form as text.
Unlike many of itscontemporary NLP systems, the LOLITA system is not designed as a framework that can be tailored to specificdomains, but as a system that brings its knowledge of specific domains to bear as and when appropriate.The Laboratory for Natural Language Engineering (LNLE) at the University of Durham is focussed ondeveloping this core.
Prototype applications have been built using the core facilities; some of them are listed below:?
Information extraction: production of summary and other templates.?
Simple meaning-based translation: currently Italian to English.?
Natural language query: supplying information to LOLITA and then asking questions about this information.?
Dialogue: a model of dialogue has been implemented.?
Chinese language tutoring: a mixed English and Chinese grammar allows detection of students of Chinese usingEnglish constructions, and diagnosis of problems.The MUC competitions have provided an opportunity for the Laboratory for Natural Language Engineering toevaluate the approach used in the LOLITA system on some very specific tasks as well as a chance to strengthen thesystem?s performance in the domain of newspaper articles.
Given the wider aims of the project, the approach takenwas to put minimal effort into the development of the new applications needed for the MUC tasks and maximumeffort into the development and improvement of the core system.The following three sections describe the details of the LOLITA core as used for MUC-7.
The description ofthe architecture in the following section is substantially similar to that for MUC-6 and readers familiar with this maychoose to skip over most of this section.
The section following ?Architecture?
describes some importantmodifications that have been carried out on the core since MUC-6.
The majority of the time spent in preparing forMUC-7 was spent enhancing these pieces of core functionality, and developing appropriate rules and knowledgebases for the tasks.
This approach follows from the design that was adopted, i.e., don?t tailor the system to theparticular domain, instead develop core functionality and use the available rules/knowledge where appropriate.ARCHITECTUREOverviewLOLITA is designed as a core system supplemented with a set of applications, the former supplying basic NLfacilities to the latter.
Figure 1 shows the MUC-relevant parts.
The most important part of the core is the largeknowledge base, which is called the ?Semantic Network?, SemNet or net, for short.
It is heavily used in most stagesof analysis, and the results of analysis are added to it, as a disambiguated logical representation of the input.
Theanalysis stages are fairly standard, and are arranged in a pipeline.
Each is implemented in a rule-based way.
Thesystem does not currently use any form of stochastic or adaptive techniques in the main system.Figure 1: Block diagram of the LOLITA core plus some applicationsThe applications can then read the results of the analysis from the SemNet, and generally interrogate thecontents of the SemNet.
Some central ?support?
facilities are provided to aid application writing, such as the generaltemplate mechanism and the NL generator - which translates pieces of the SemNet into English.
More detail on thearchitecture of LOLITA can be found elsewhere [2].The Semantic NetworkThe SemNet is a 100,000+ node, directed hyper-graph.
Each node has a set of links, plus a set of ?controlvariables?
(or controls).
Some nodes have an associated ?name?
: this is usually a single word which characterises themeaning of the node.
Each link has an arc and a set of targets.
Targets are other nodes, and the arc too is just anode.
Nodes correspond to concepts of entities or events.
Links correspond to relationships between nodes.
Sincean arc is also a node, the concepts of the different kinds of relationship possible between nodes can be represented inthe same formalism as more concrete concepts.
In this system, the ?meaning?
of any particular node is given by itsconnections, its relative position in the net.Controls indicate basic information about a node, such as its type (e.g., event, entity, relation), its family (e.g.,human, inanimate, food, organisation), its lexical type (e.g., noun, preposition, adverb) - as appropriate.
Animportant control is a node?s rank: this encodes quantification information.
Concepts of general sets have aUniversal rank, specifically named objects have a Named Individual rank, and general individuals an Individualrank.
There are several other less important ranks, used for things like encoding script-like information orexistential quantification.
Controls could be represented using links, but for efficiency reasons a more compact formis used.There are approximately 60 different arcs.
The arcs subj_, action_, and object_ are used to represent the basicroles of an event.
Events can have other arcs, such as those indicating temporal information, the status of theinformation (e.g., known fact, hypothesis, etc), or arcs that indicate the source of the information.
Most arcs alsohave inverses: e.g., the subject_ arc has the inverse subject_of_, which allows determination of the events in which aparticular concept played the subject role.Figure 2: Example piece of semantic net, for the sentence "John will retire as chairman".
It is givenhere as an example of SemNet structure, and its meaning is discussed in the section on the semanticnetwork.
The full structure is not shown, for reasons of space.Concepts are connected with arcs such as specialisation_ (and its inverse, generalisation_), or instance_ (inverseuniversal_).
Specialisation links a set to one possible subset; for example, in Figure 2, chairman[U] represents theset of all possible chairmen, and old_chairman[U] the set of all possible old chairmen.
Between the former and thelatter is a specialisation_ link, indicating that old chairmen are a subset of chairmen.
Conversely, the latter is linkedto the former with a generalisation_ link, representing a superset.
Using the specialisation_ link, hierarchies ofconcepts are specified.
The instance_ arc connects a concept to an instance of that concept: e.g., a particularchairman chairman1[I] would be linked to chairman[U] by an instance_ link.
Other links between concepts includesynonym_ and antonym_.The SemNet is used to hold several kinds of information:?
Concept hierarchies: built with arcs such as generalisation_, concept hierarchies encode knowledge like "manis_a mammal is_a vertebrate" etc.
They prevent duplication of information by allowing information to beinherited within the hierarchy.?
Lexical information: actual words are represented in the net, and their properties are stored in the net, asopposed to having a separate lexicon.
The lexical-level nodes are indexed via a simple dictionary: i.e., amapping from root words to all the senses of that word.
Note that the lexical forms are distinct from theconcepts: they are linked by a concept_ arc.
Concepts are linked to lexical forms by a link named after thelanguage of interest.
For example, dog[U] has a link english to the noun form of ?dog?, and a link italian to theItalian word ?cane?.?
Prototypical events: these define restrictions on events by providing  ?templates?
for events, e.g., by imposingselectional restrictions on the roles in an event.
"Human owners own things" says that only humans can take thesubject role in ?ownership?
events.?
General events: other kinds of information.
For example, the content of a MUC article would come in thisclass, when analysed.The bulk of the net (70%) comes from WordNet, a database containing lexical and semantic information aboutword forms in English [3].
More details about the formalism used in the net can be found in [4].Referring back to the Original TextBefore MUC-6, LOLITA did not have a method of referring back to its input: the previous orientation was tomove from language-dependent surface forms to a language-independent logical representation.
Therefore,information about the surface form was discarded.
Since the ability of reference has many uses outside of the MUCtasks, a more general mechanism was designed and added to the core.
It allows fine-grain connection of the analysisresults to the sections of the document giving rise to those results.
The system allocates new SemNet nodes tocomponents of the document (words, phrases, sentences, ...), which act as references into the document.
This iscalled the ?Textref?
system.Textrefs allow the document structure to be fully represented in the net, and represented uniformly with theother information in the system.
At the word level, a Textref signifies a specific occurrence of a word at a certainposition in the input, and is distinct from the nodes representing the lexical or semantic forms of its root form.
It isan instance_ of the universal concept of all occurrences of that word.
Concept nodes and Textref nodes are linkedby an event with the internal action words_used.
Two examples may be seen in Figure 2: single words are attachedto the ?key?
words of the sentence (only ?retire?
is shown), and all of the Textrefs in the sentence are attached to thenode representing the whole event.Text Pre-processingCore analysis of textual input starts from a LOLITA-specific SGML representation of the input (called anSGML tree).
Individual applications must convert from their own formats (e.g., plain text, MUC articles, LaTeX,HTML, ...) into this internal format.
The MUC converter is just a simple SGML parser.
The preprocessor then addsadditional structure to the internal SGML tree where necessary.
In particular the following structures are handled inthe order given: reported speech, paragraphs, sentences and words.
Markers for reported speech are distributed overall sentences inside the quotes.
Lastly, each word is allocated a Textref.MorphologyMorphology is applied to an SGML tree whose leaves are individual word tokens, and whose nodes representthe structure of the document.
A few transformations are done on this structure to unpack contractions (e.g., "I?ll"expanded to "I will"), expand monetary and numeric expressions (e.g., "$10 million" to "10 million dollars"), and totransform certain surface-level idiomatic phrases (e.g., "in charge of").
Some splitting of hyphenated words is alsodone.
Then, the basic morphology function is mapped on to all leaves (with additional treatment provided forsentence initial words).Lookups in the dictionary are done with the root forms suggested by affix stripping.
If successful, a word islinked to lexical and semantic nodes, allowing access to lexical and semantic information during the rest ofmorphology, parsing, and semantics.
Affix stripping loses information such as number and case, so this informationis represented using a Feature system.
Features are used in parsing (described below).
Other Features include wordclass (Noun, Verb, ...) and some semantic-based Features.
Finally, possible syntactic categories for a word aredetermined from the lexical (and sometimes semantic) node information.
Thus, each leaf is mapped to a set ofalternatives, varying in category and Features, which represent all possible interpretations of that leaf.ParsingThe parsing mechanism utilised in MUC-6 consisted of five stages:1.
A pre-parser which identifies and provides structure for monetary expressions.2.
Parsing of whole sentences using the Tomita algorithm [5].
The result of this stage is a "parse forest", adirected acyclic graph which indicates all possible parses.
Due to the complexity of the grammar, this forest isfrequently very large, implying many possible parses.3.
Decoding of the parse forest.
The forest is selectively explored from the topmost node, using heuristics such asFeature consistency and hand-assigned likelihoods of certain grammatical constructions.
Feature errors andunlikely pieces of grammar involve a cost: the aim of the search is to extract the set of lowest-cost trees.4.
Selection of best parse tree: subsequent analysis operates on a single tree.
The lowest cost set is ordered on thebasis of several heuristics on the form of the tree.
For example, preferring a deeper tree.5.
Normalisation: syntax-based, meaning-preserving transformations are applied to the trees to reduce the numberof cases required in semantics.
A prime example of this is passive to active, i.e., "I was bitten by a dog"changed to "A dog bit me".
Another class involves transformations such as "You are surprised" to"*SOMETHING* surprised you", which makes explicit the object doing the surprising.Although this mechanism remains at the core of the parsing for MUC-7 a number of additional strategies havesince been included.
These are described below in the section detailing the main changes to the system sinceMUC-6.An example parse is given in Figure 3.
Note that ?will?
and ?as?
are missing.
As so-called function words, theydon?t carry much inherent semantic meaning, so the tense information of ?will?
is transferred to the Features of themain verb, and the copula function of ?as?
is transformed into a syntactic construct.
This simplifies the semanticrules.Sen -- sentence branchfull_propernoun -- proper noun phrasepropernoun JOHN [Sexed]neg_copula -- copula verb phraseasCopulaN RETIRE [Fut]comnoun CHAIRMAN [Sing,Per3]Figure 3: Parse tree for "John will retire as chairman".Analysis of MeaningThis section describes how the parse tree is converted to a disambiguated piece of SemNet.
There are twostages to this ?semantic?
and ?pragmatic?.
The semantic analysis is generally compositional in nature: the meaning ofa tree is built from the meanings of its subtrees.
A mechanism goes through the parse tree in depth-first, post-ordertraversal, applying semantic rules mainly on the basis of the syntactic phrase type of the current tree node.
If themeaning of a particular subtree is unambiguous in role, the Textrefs for the text in that subtree are connected to thatmeaning.
Since the meanings can be nodes which already have Textrefs connected, then particular nodes can collectTextrefs for all occurrences of their mention.
This Textref handling is completely invisible to the semantic rules.A state value, the ?context?, is passed around during traversal: this holds possible referents in order ofoccurrence, and is used to resolve anaphoric expressions.
Use of this context prevents the semantics being purelycompositional.The ?meaning?
of most leaves is the semantic node associated with the word at the morphology stage.
The nodeis passed to the leaf?s parent in the form of a ?role?
structure, which indicates the role the node may play in thesemantics of the parent.
Often this is unknown, but in cases like verbs, it can be determined as the act.
The actualrole structure allows for representation of semantic ambiguity.The main task of the pragmatic stage is disambiguation and type checking.
Lexical ambiguities and anaphoraare resolved using a series of preference heuristics which are first applied to disambiguate the action of the event.Once the action is known, any knowledge available from the prototype event associated with that action can be usedto rule out pragmatically implausible readings, as well as to aid disambiguation of the remaining elements of theevent (in the spirit of [6]).The contents of the current context together with the topic of the text (the latter is given to the system inadvance) influence the choice of word senses: those meanings are preferred which are semantically closer to themeanings present in the context or the topic, where semantic closeness is computed on the basis of the distancebetween nodes in the network.
Other factors may cause one concept to be preferred over others, such as the amountof knowledge the system has about a given concept, or the concept?s frequency of use.Once an event is disambiguated, the system attempts to establish plausible connections between it and thepreviously processed discourse.Reference ResolutionAs the discourse is processed, the referents found in it are stored in the ?Context?
buffer.
Each time ananaphoric expression is identified in the incoming discourse, the system looks for a possible referent for thisexpression in the Context buffer (obeying matching rules dictated by the type of anaphoric expression).
If thesystem finds no match, it introduces a new entity into the Context.
If the system finds just one match, it unifies thetwo and adds the newly unified item into the Context.
If the system finds more than one match, it builds a specialstructure to represent the ambiguity and passes it onto the system of preference heuristics to decide between thepossibilities.The heuristics are loosely based on ideas from centering theory [7], psycholinguistic findings as well ascommon sense.
They assess the salience of the candidates based on grammatical and semantic features, as well theirposition in the sentence, recency of mention and relatedness to the topic of the text.
As in the whole of the LOLITAsystem, the algorithm relies heavily on a correct parsing and semantic analysis.Template SupportThe processes involved in producing templates can be generalised, hence the core contains a mechanism to helpwrite templates at an abstract level.
This mechanism handles search through the net, use of inference rules to deriveimplicit facts, and general output formatting.A template contains a predefined set of slots with associated fill-in rules that direct the search for appropriateinformation in the net.
The slot fill-in rules are predicates that check node controls, or use the inference functionsavailable in the core.
For more details see [1].Implementation and Operating DetailsLOLITA is written mostly in Haskell, a non-strict functional programming language [8].
Two resource-criticalsections are written in C - the parsing algorithm and the SemNet data structure and its access functions.
Haskell hassome similarity to LISP, such as building programs by writing functions, a garbage-collected heap, lists as a basictype, and full higher-order use of functions.
However, it provides excellent support for modern SoftwareEngineering, such as modularity, constrained polymorphism, a strong but flexible type system.
It also enforcesreferential transparency and allows coding in a ?lazy?
style, which means code is not executed unless needed.
Thus,whilst our system has the external appearance of a pipeline architecture, the evaluation of individual pieces of codeneed not occur in that strict order.IMPROVEMENTS CARRIED OUT SINCE MUC-6The system that entered the 6th Message Understanding Conference suffered from three major problems.
First,there was room for improvement in parsing.
Second, the named entity recognition rate was fairly low, as comparedwith other systems.
Third, the system contained a series of trivial errors in the code.
Altogether, these three majorshortcomings resulted in a considerable drop in performance.In the general approach adopted by the LOLITA project, every core component plays an important role in thefinal result.
Consequently, if any of the components is unsatisfactory, overall performance is affected.
This isespecially prominent if an early stage of analysis (e.g., parsing) is incorrect.Many of the problems encountered during MUC-6 have been addressed and several improvements to the systemhave been carried out since that time.
The most substantial of these improvements are discussed in the remainder ofthis section.Changes to the Grammar and Parsing ComponentsParsing can sometimes fail on very large forests: decoding these requires a lot of resources (time, memory).Rather than cause a crash due to overrunning limits, the parse is abandoned.
This is implemented by fixing a time-limit on the process - resource usage being proportional to time: the expiry of the time limit is referred to as a?timeout?.
It is also possible for parses to fail if the sentence can?t be analysed with the main grammar.
In the systemused for MUC-6 if the parse failed then analysis was discontinued on that sentence.
This meant that no semanticresult was produced and hence no information was available on NE?s etc in the sentence.
MUC-6 texts whichcontained sentences that timed out would therefore receive poor scores.
For MUC-7 a number of improvements tothe parsing mechanism have been adopted, including a recovery strategy for sentences that failed to parse.LOLITA?s grammar has been improved and expanded to allow for a better parsing of the materials used inMUC-6.
Furthermore, a new method for handling headlines in the articles has been added.
As well as devising aspecial grammar for them, the headlines are now analysed at the end of the article, using as context the initialsentences of the main body of the text.The parsing mechanism itself has been improved.
Island parsing, whereby easily recognisable noun phrases are?locked?
into units before being passed onto the parser, has been introduced.
This has improved the parsing successrate substantially.Moreover two extra passes have been added should parsing fail: a second pass using Brill?s tagger [9] and athird pass using a reduced grammar.
These are aimed at recovering constituents of complex sentences, if a full parseisn?t possible.Finally, in cases where all three parsing passes fail, a way of recovering most named entities, all the pronouns,possessive determiners and some noun phrases (particularly those related to the topic of the text, if the latter can beprovided in advance) has been devised.Changes to the NE Recognition ComponentsThe components responsible for named entity recognition have been revised and many new rules have beenadded.
A major change has been introduced to LOLITA?s morphology module, which allows the system to reusenames of entities previously recognised in the preceding text, rather than treat the entities in each sentence of theincoming text as new.
(Previously, the morphology module had no access to the results of the semantic andpragmatic analysis of the preceding text.
)A change in the treatment of unknown proper names that appear without clear designators (i.e., without Corp,Ltd, Mrs., etc) has been introduced.
In the MUC-6 system a decision as to what type of entity an unknown namestood for was made early and usually resulted in the conclusion that it must stand for an organisation.
The newimproved treatment, on the other hand, involves the introduction of the concept of human_or_organisation, the useof which allows for a delay in the decision, until some disambiguating information becomes available at thepragmatics stage.
For example, given the following first sentence of an article:Shortly after Fossett?s launching Monday his competitors sent him telegrams of congratulationThe system cannot decide what sort of entity Fosset is on the basis of this name itself.
However, the use of thepronoun his as well as the absence of any other possible referents, provide the disambiguating clues.Changes to the Semantic and Pragmatic ComponentsAt the semantic level, several new rules that had previously been missing and had been needed to handleexpressions common in MUC-6 articles have been added.
New rules were also needed due to the introduction ofnew constructions in the grammar.In the pragmatics component, the preference heuristics system has been substantially revised and expanded.
Inthe MUC-6 system the heuristics acted as filters and so rejected any non-preferred candidates.
This sometimesresulted in rejecting a candidate which didn?t match one of the heuristics that was applied at an early stage.
Thesame candidate could have been favoured by several later heuristics, but this had been ignored.Currently, the preference heuristics assign penalty points to non-preferred items and at the end of theirapplication, the candidate with least penalties is chosen as the referent.Increase in Basic DataSince the time of MUC-6, a lot of data concerning organisation names, corporate designators, personal namesand place names have been added to LOLITA?s knowledge base (SemNet).
The additions include names of majorUSA institutions and organisations (e.g., government departments), names of newspapers, names of majorgeographical locations in the USA, US states abbreviations and names of countries and nationalities of the world.Also, about 8000 new forenames have been added and all the existing forenames have been checked to ensure thatthey are marked correctly for gender.Text Output Errors CorrectedMinor coding errors in the ?Textref?
module of LOLITA resulted in the system occasionally inserting spuriousspace characters in some places, while deleting others.
This adversely affected the final result of the MUC-6evaluation, because the scoring software is sensitive to any misalignments between the answer keys and theresponses.
Many of these kinds of errors were corrected before participation in MUC-7.MUC-7 SPECIFIC CHANGESThe work carried out during the preparations for MUC-7 was concentrated in four main areas.
These arediscussed in this section.Addition of MUC-7 Specific DataOver three hundred airline names as well as some well known airport names have been added to the SemNet.Additionally, airline and aircraft specific artifacts, such as types of aircraft and most common aircraft models(including some military ones), have been added.
The area of the SemNet with knowledge relevant to the aircrashscenario has been checked and adjusted as necessary.Grammar ExpansionGrammar rules have been added to deal with constructions common in the training texts, such as references toaircraft and flights (Boeing 747, or Paris-bound Boeing 747, the TWA flight 800, etc).The MUC-7 corpus contains sentences which are, on average, much longer than the ones encountered in mostof our previous tests.
Sentences of around 40 words are not uncommon.
In view of this the island parsing and thethird pass parsing (of fragments of sentences, using a reduced grammar) proved particularly important, hence, areasonable amount of work was needed on the rules for the reduced grammar and the failure recovery mechanism.Revision of Pragmatic Disambiguation RulesIn order to deal with reported speech, commonly found in the training articles, it was necessary to improve therules and heuristics in LOLITA which deal with first person pronouns (i.e., ?I?
and ?we?).
However, the new rulesthat were introduced using the examples from the MUC-7 training corpus were not designed specifically for thoseexamples.
In line with the normal strategy of the LOLITA project, our intention was to make them as general aspossible.It was also found that some of our existing rules for noun phrase matching were not working well with theMUC-7 corpus.
The existing rules produced much better results for the MUC-6 corpus whose topic area generallyinvolved only companies and people.
The rules needed tightening, especially when dealing with references tolocations and aircraft related artifacts.A certain number of rules that we introduced were very MUC-7 specific and conflicted with LOLITA?s basicanalysis.
For example, in a sentence such as:The military version of the Boeing 737 that crashed in Croatia Wednesday was not equipped...Boeing was to be marked as ORGANIZATION, while in LOLITA?s analysis, it is an artifact.Rules to Handle ?Non-Natural?
LanguageSpecial treatment had to be devised for PREAMBLE and SLUG fields of the articles at the morphology andparsing levels.
Some of these fields contained strings which seemed more like a code (particular to the New YorkTimes News Service) than natural language, for example:<SLUG fv=ttj-z> BC-LUCID-STUCK-HNS </SLUG><SLUG fv=tia-z> BC-JAPAN-ROCKET-HUGHES-B </SLUG><SLUG fv=tia-z> BC-CLINTON-CHINA-SATELLI </SLUG><PREAMBLE>BC-WALSH-OBIT-NYT(Fla., Mass., N.J., Md., Colo., R.I. ATTN)JOHN PAUL WALSH, 78, FORMER SPACE SCIENTIST(lb)By WOLFGANG SAXONc.1996 N.Y. Times News Service</PREAMBLE>Additional morphology and grammar rules had to be written specifically to handle these.
The system processesthem at the end of the analysis of the main body of the text in the hope that the text can provide a useful context inwhich to deal with them.RESULTSThe Named Entity TaskThe system?s total score for the 100 articles of the formal run was:P&R   76.43 2P&R   77.31 P&2R   75.57This is an improvement on the scores for the named entity task that the system achieved during MUC-6.However, the score was a little disappointing, as during training the system consistently achieved scores in theregion of mid to high eighties.A shift in the topic from airlines and aircraft to satellite, rocket and missile launches explains some of theproblems that were encountered.
LOLITA?s data in the latter area was not strong.
Apart from the specific data,some basic data was found to be missing too, e.g., the names of the planets of the Solar system.
Furthermore, thesystem was not prepared to recognise space shuttle names (e.g., Endeavour, Columbia) or missile names like Scud,Patriot, etc.A number of company names in the satellite television market were also missing.
Several of them comewithout a clear designator and were not recognised by the system, e.g., BSkyB, SatelLife, Intelsat, Comsat, Canal-Plus, etc.
(NB.
BSkyB might have been resolved correctly in the presence of British Sky Broadcasting Group Plc inthe same article.
Unfortunately, the rules in the acronym matching algorithm didn?t handle this case correctly.
)Finally, a mistake was made in interpreting the guidelines of the task.
The names of newspapers were notmarked as ORGANIZATIONS and so this too contributed to a drop in scores.Walk-through articleThe score for the walk-through article was:P&R  75.57 2P&R  76.63 P&2R  74.55This is very slightly lower than the overall score for the formal run.
The worst scoring group of entities in thisarticle was ENAMEX PERSON, where out of 16 entities, just over half were marked correctly (R 56%, P 53%).This is different from the overall trend, where the system?s score for PERSON was a lot higher   (R 80%, P 74%).An example of an error that occurred in this text is for Llennel Evangelista.
The sentence:Llennel Evangelista, a spokesman for Intelsat, a global satellite consortium based inWashington, said the accident occurred at 2 p.m. EST Wednesday...was incorrectly analysed for two reasons.
First of all the name Llennel was not in the data base.
Secondly, a parsingerror resulted in the analysis of Llennel Evangelista as both a spokesman and a consortium.
The label ofORGANIZATION was then (incorrectly) chosen.Although it is helpful for the system to have the names of people in its database, it is not crucial.
For example,the name LaRae Marsik was also unknown to the system, but this was dealt with correctly, because here the correctparsing facilitated a correct analysis:A spokeswoman for Tele-Communications, LaRae Marsik, said the partners in the LatinAmerican venture intended to begin service by the end of 1996.So, because LaRae Marsik was understood to be a spokeswoman (a concept known to the system), it was possible toconclude that it must be a PERSON.A number of named entities in the SLUG and PREAMBLE fields were missed, e.g., three occurrences ofMURDOCH.
The strings in these fields appeared to be in some special format rather than natural language.
Therules required by the system for handling these fields were therefore rather specialised.
As these were MUC-7specific, relatively little effort was spent polishing them; the effort instead being expanded on the more generallyuseful core rule sets.The recognition of organisations was much better in this article: R 64%, P 80%.
This is higher than the overallresults in the whole test (the score there was R 63%, P 67%).
The most common reasons for errors appear to beproblems with tokenisation or parsing.The Co-reference TaskThe LOLITA system?s score for the 20 articles in this evaluation was:Recall  46.9% Precision  57.0% f-value  51.5%As with the named entity task, this was a much better score than the system achieved in MUC-6.One of the reasons leading to co-reference resolution errors was lack of full parsing.
The performance of theco-reference resolution task within our system is sensitive to the basic analysis being correct: possibly more so thanthe other two tasks in which the system was entered.
In many cases, the full parse was not available and the parsingrecovery mechanism didn?t always provide sufficient input to facilitate a good semantic analysis.
This led toproblems, whereby even seemingly easy co-reference links were sometimes lost.Another problem was the fact that due to lack of resources not enough time was devoted to dealing with co-references involving conjoined noun phrases.
In the previous MUC such co-references were excluded by the task,and although the LOLITA system has never explicitly excluded co-references involving conjoined noun phrases therules that were used required more thorough testing than resources allowed.Some trivial errors also contributed to a reduction in score.
For example, in the document 9601160264 thestring McDonald was consistently marked instead of McDonald?s.
Additionally, due to a text output error, a secondchain containing some occurrences of McDonald was built.
This lowered the score considerably.
Having correctedthe error, the score for this article increased by just over 10%.
This increase leads to a slightly better overall scoreof:Recall  48.0% Precision  58.6% f-value  52.8%In a number of articles the system scored particularly well: 6 of the articles scored well above the f-value of60% and one of them as high as 70.5%.
The problems that were encountered in other articles were typically due tothe lack or resources that were available for debugging and testing.
The developers believe that with a modestamount of further effort the system?s overall scores for this task would have been even higher.The problem such as the one illustrated by the McDonald example points to a difficulty in automatic scoringand evaluation.
On a semantic level, the system connected together the correct chain, however for scoring purposesthis constituted a spurious chain.
The resulting drop in score is treated the same as other spurious connections thatcould have been semantically incorrect.
A scorer which is able to include a semantic component would give a moreaccurate reflection as to the success of any ?deeper?
analysis that a system may have undertaken.Walk-through articleThe official score for the co-reference walk-through article is:Recall  45.6% Precision  57.1% f-value  50.7%This was slightly lower than our system?s overall score for this task.Although performing reasonably well on most of the smaller chains within the article, problems occurred withtwo of the longer chains.
The first involving Hughes and the second with Federal CommunicationsCommission/FCC.
In the case of Hughes, problems with the analysis of Hughes?
Galaxy VIII(I) led to losses in bothrecall and precision.
The string Hughes was not marked up at all, while Galaxy VIII(I) was split into two separateunits Galaxy VIII and I.We noticed also that in one case our system marked a larger maximal noun phrase than the key.
Havingchanged the key (according to what we believe is consistent with the task description) to include the following as anantecedent:... the Federal Communications Commission?s allocation of aswath of spectrum that will lettheir earth stations communicate with satellites in spacerather than just:... the Federal Communications Commission?s allocation of a swath of spectrumwe gain some extra points in the score: Recall  46.8%,  Precision  58.7%,  f-value  52.1%.Template Elements TaskThe overall score on this task was:P&R  66.75 2P&R  69.74 P&2R  64.01This result is probably the most satisfying of the three MUC tasks that the system entered.
Using LOLITA?stemplate support it was possible to produce very reasonable templates within a relatively short space of time.
Toprepare the system for this task took only about 10 person days.The best performing subtask was the entity slot, particularly where the task required the extraction oforganisations and persons.
The lowest score was obtained in the ENT_DESCRIPTOR category, which could havebeen increased, had the co-reference performance score been better.Walk-through articleThe walk-through article score was:P&R  76.92 2P&R  77.05 P&2R  76.80which is better than the system?s overall score.
The errors made were mainly due to tokenisation, for example, inInternational Technology Underwriters of Bethesda, Maryland the system didn?t take the strings as a full name ofcompany but split off Maryland into a separate entity.
There was a similar problem with Space TransportationAssociation of Arlington, Virginia.Also, some inaccurate co-reference resolution resulted in spurious ENT_DESCRIPTORS: for example, thecompany?s Washington headquarters for Bloomberg and Rupert Murdoch?s for News Corporation.
The latterdescriptor error is much less serious than the former, as it does actually make some sense.
The former descriptor,however, is erroneous.Other errors are data dependent: for example, the system didn?t have some basic geographical data for wellknown cities of Europe, resulting in Paris being classified as a region.Despite the above problems the majority of the underlying analysis for this text was correct.
The system wasthen able to successfully apply the higher level heuristics at the semantic and pragmatic levels.
This demonstratesthat given a correct underlying analysis the development of the high level template element application is relativelytrivial.
Results such as these provide further evidence to the developers that concentrating development on the coreanalysis is a strategy that will produce the best long term results.CURRENT ACTIVITIESRe-engineering of the SystemThe use of Haskell has proved very beneficial in the development of the LOLITA system (see [1] for moredetails).
Of particular benefit is the ability to quickly prototype complex algorithms.
More recently the need for alarge amount of prototyping has diminished, as the core parts of the system have become relatively stable.
In orderto improve the system?s performance a major effort is now underway to re-engineer large parts of the system in C++.This has also provided the developers with the opportunity to make some alterations to the structure of the overallsystem.The aim of the project is still to develop a powerful core set of tools (e.g., parser, etc) which can then be used bya number of high level applications.
However, the re-engineering process has also offered an opportunity for somemore fundamental alterations to the system.
An important one of these is the mechanism that is used for expressingthe system?s linguistic rules.
In the past rule sets have been written as pieces of code.
Although this allowed for agreat amount of control in the rule-writing process it had obvious limitations, e.g., having to recompile after eachchange, and the writers of the rules requiring the knowledge to translate them into pieces of code.
To avoid theseproblems a number of engines are now being implemented that are able to process sets of linguistic rules that havebeen written in a more appropriate language.
Linguists can now write and test rules without the need to be able tounderstand the underlying engine?s code.
The programmers can also now concentrate on optimising the engineswhich process these rule sets.Perhaps the clearest example of such an engine is the parser.
In the past grammatical rules were added as piecesof code and the system re-compiled.
This relatively inefficient grammar development process has now beensuperceded by ones in which the running system loads the grammar from appropriate files.
This allows thegrammarian to concentrate their effort on the grammatical rules and not the implementation of them.
Using C++ theprogrammers have also been able to develop some very low level optimisations that have greatly increased the speedof the parser whilst at the same time reducing its memory requirements.
The new parser is estimated to be some 10times faster and require a fifth less memory.
(This parser was not available in time for the MUC-7 evaluations.
)The developers view this re-engineering process as a natural progression of LOLITA from a research arena tothat of the commercial world.Addition of Dictionary Definitions to Knowledge BaseA natural language processing system requires knowledge at a number of important levels.
The requiredknowledge includes:Grammatical word informationknowledge about the structure of a word.Semantic word informationknowledge about the meaning of a word, e.g.,?
`soccer' is a game played on a pitch by two teams,?
`sell' involves a transfer of money from a buyer to a seller.World knowledgeknowledge such as:?
what particular objects are used for,?
why particular events happen.It is widely recognised that knowledge about words and their meanings (the first two types) is already availablein conventional dictionaries.
However, these are aimed at human readers.
For a computer to utilise knowledge in adictionary the definitions need to be processed to extract and represent the information in a suitable form.
A majorproject is currently underway which uses LOLITA to help process dictionary knowledge for computer use.
Thedictionary which has been selected for this project is the Cambridge International Dictionary of English (CIDE).The aim is to incorporate the knowledge contained in CIDE into SemNet.
It is estimated that this will increase thesize of SemNet from over 100,000 nodes to well over a million.To be able to carry out this process LOLITA is used to analyse as much of the definition as possible.
However,help is required in resolving various problems and ambiguities that occur in the original definition.
This help isgiven by the user in the form of a question-answering session (see [10] for more details).
The questions fall into anumber of different categories:choosing grammatical categories,picking word meanings,entering word information,solving structural ambiguities,finding referents for mentioned words,finding referents for implicit objects,making objects more specific,naming events and entities,naming relationships,confirming the analysis.The categories of questions in the above list also represents a rough estimate as to the order of the question-answering process (earlier questions such as the picking of word meanings may also occur in a different contextlater in the process).
In practice the interaction with the natural language system is via a Graphical User Interface.LOLITA processes as much of the definition as it is able and then typically presents the user with a question and alist of possible answers.
The user then simply uses the mouse to select the most appropriate answer.
Occasionally aquestion is asked that requires the name of an entity (or relationship) to be entered; a text entry box is then providedfor that purpose.
It is anticipated that it will take several person years to completely analyse the dictionary.
Thesystem has been designed so that it can be used, with little in the way of training, by people who are not specialistsin the area of NL.
This greatly increases the number of people that can be used to enter the knowledge and so willreduce the amount of time that will be required to complete this project.CONCLUSIONSThe LOLITA system is a natural language processor that has a core functionality on which a number ofdifferent applications can be built.
The system was not designed to compete solely in the MUC tasks and in fact theMUC specific code forms a very small part of the whole.
The first MUC evaluation to which LOLITA was appliedwas MUC-6.
Since then the system's core has been substantially improved and this has lead to a significantimprovement in the system's performance for the MUC-7 evaluation.
We are pleased with this improvement andbelieve that this further validates our method of development.Although pleased with the results there is certainly room for improvement.
In particular approximately 20% ofparses failed in the final evaluation.
This had a substantial impact on the system's performance as all of the taskapplications rely on a `good' core analysis of which parsing is a key component.
Although some recovery strategieswere used, these had a limited effect.
(Little effort was expanded in attempting to reduce the number of parsefailures as a new parser is under development.)
Of particular encouragement were the articles for which thesystem?s underlying analysis was correct.
In such cases the system was able to apply the higher level semantic andpragmatic rules to good effect.
The system also suffered a drop in scores due to the lack of data that was available.The issues of more robust parsing and data improvement are currently being addressed.
Major projects areunderway to re-engineer large parts of the system (the parser being one), and to add a large amount of data in theform of dictionary definitions.We feel that the MUC series of evaluations has been very useful to the development of the LOLITA system andhas allowed us to focus development on some key areas of the system?s core.
Now that MUC has reached an end weshall continue to devote our main development effort to the core issues, in particular those mentioned above, i.e., re-engineering of the system, and addition of dictionary definition data.
This engineering work, carried out by auniversity spin-off company, 3F Ltd, will allow us to reach our goal of bringing to the market, a number ofsophisticated and powerful natural language processing applications based on an underlying core system.REFERENCES[1] Morgan, R.G., et al, University of Durham: Description of the LOLITA system as used in MUC-6,In Proceedings of the Message Understanding Conference (MUC-6), 1995.
[2] Smith, M.H., Natural Language Generation in the LOLITA system: An Engineering Approach, PhD Thesis,Department of Computer Science, University of Durham, 1995.
[3] Miller, G., Wordnet: An online lexical database, International Journal of Lexicography, 3(4), 1990.
[4] Long, D. and Garigliano, R., Reasoning by Analogy and Causality: A Model and Application, Ellis Horwood,1994.
[5] Tomita, M., Efficient Parsing of NL: A Fast Algorithm for Practical Systems, Kluwer Academic Publishers,Boston, Ma, 1986.
[6] Wilks, Y., Preference Semantics, Formal Semantics of Natural Language, pp.
329--348, Cambridge UniversityPress, 1975.
[7] Grosz, B.J., Joshi, A.K.
and Weinstein, S., Towards a computational theory of discourse interpretation,Computational Linguistics, 21(2), 1986/1995.
[8] Hudak, P., Peyton Jones, S. and Wadler, P., Report on the functional programming language Haskell,Version 1.2, March 1992, URL ftp://ftp.dcs.gla.ac.uk/pub/haskell/report[9] Brill, E., Some advances in rule-based part of speech tagging, AAAI-94, 1994.
[10]Poria, S., A Natural Language Engineering Approach to Knowledge Acquisition by the Analysis of DictionaryDefinitions, PhD Thesis, Department of Computer Science, University of Durham, forthcoming.
