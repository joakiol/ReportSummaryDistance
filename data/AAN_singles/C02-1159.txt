Extending A Broad-Coverage Parser for a General NLP ToolkitHassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, FuadRahman, Yuliya Tarnikova, Timotius Tjahjadi and Che WilcoxBCL Technologies Inc.Santa Clara, CA 95050 U.S.A.fuad@bcltechnologies.comAbstractWith the rapid growth of real worldapplications for NLP systems, thereis a genuine demand for a generaltoolkit from which programmerswith no linguistic knowledge canbuild specific NLP systems.
Such atoolkit should have a parser that isgeneral enough to be used acrossdomains, and yet accurate enough foreach specific application.
In thispaper, we describe a parser thatextends a broad-coverage parser,Minipar (Lin, 2001), with anadaptable shallow parser so as toachieve both generality and accuracyin handling domain specific NLproblems.
We test this parser on ourcorpus and the results show that theaccuracy is significantly higher thana system that uses Minipar alone.1 IntroductionWith the improvement of natural languageprocessing (NLP) techniques, domains forNLP systems, especially those handling speechinput, are rapidly growing.
However, mostcomputer programmers do not have enoughlinguistic knowledge to develop NLP systems.There is a genuine demand for a general toolkitfrom which programmers with no linguisticknowledge can rapidly build NLP systems thathandle domain specific problems moreaccurately (Alam, 2000).
The toolkit will allowprogrammers to generate natural languagefront ends for new and existing applicationsusing, for example, a program-through-example method.
In this methodology, theprogrammer will specify a set of sample inputsentences or a domain corpus for each task.The toolkit will then organize the sentences bysimilarity and generate a large set of syntacticvariations of a given sentence.
It will alsogenerate the code that takes a user?s naturallanguage request and executes a command onan application.
Currently this is an activeresearch area, and the Advanced TechnologyProgram (ATP) of the National Institute ofStandards and Technology (NIST) is fundingpart of the work.In order to handle natural language input,an NLP toolkit must have a parser that maps asentence string to a syntactic structure.
Theparser must be both general and accurate.
Ithas to be general because programmers fromdifferent domains will use the toolkit togenerate their specific parsers.
It has to beaccurate because the toolkit targets commercialdomains, which usually require high accuracy.The accuracy of the parser directly affects theaccuracy of the generated NL interface.
In theprogram-through-example approach, thetoolkit should convert the example sentencesinto semantic representations so as to capturetheir meanings.
In a real world application, thisprocess will involve a large quantity of data.
Ifthe programmers have to check each syntacticor semantic form by hand in order to decide ifthe corresponding sentence is parsed correctly,they are likely to be overwhelmed by theworkload imposed by the large number ofsentences, not to mention that they do not havethe necessary linguistic knowledge to do this.Therefore the toolkit should have a broad-coverage parser that has the accuracy of aparser designed specifically for a domain.One solution is to use an existing parserwith relatively high accuracy.
Using existingparsers such as (Charniak, 2000; Collins,1999) would eliminate the need to build aparser from scratch.
However, there are twoproblems with such an approach.
First, manyparsers claim high precision in terms of thenumber of correctly parsed syntactic relationsrather than sentences, whereas in commercialapplications, the users are often concernedwith the number of complete sentences that areparsed correctly.
The precision might dropconsiderably using this standard.
In addition,although many parsers are domainindependent, they actually perform muchbetter in the domains they are trained on orimplemented in.
Therefore, relying solely on ageneral parser would not satisfy the accuracyneeds for a particular domain.Second, since each domain has its ownproblems, which cannot be foreseen in thedesign of the toolkit, customization of theparser might be needed.
Unfortunately, usingan existing parser does not normally allow thisoption.
One solution is to build another parseron top of the general parser that can becustomized to address domain specific parsingproblems such as ungrammatical sentences.This domain specific parser can be builtrelatively fast because it only needs to handle asmall set of natural language phenomena.
Inthis way, the toolkit will have a parser thatcovers wider applications and in the mean timecan be customized to handle domain specificphenomena with high accuracy.
In this paperwe adopt this methodology.The paper is organized into 6 sections.
InSection 2, we briefly describe the NLP toolkitfor which the parser is proposed andimplemented.
Section 3 introduces Minipar,the broad-coverage parser we choose for ourtoolkit, and the problems this parser has whenparsing a corpus we collected in an IT domain.In Section 4, we present the design of theshallow parser and its disadvantages.
Wedescribe how we combine the strength of thetwo parsers and the testing result in Section 5.Finally, in Section 6, we draw conclusions andpropose some future work.2 NLP ToolkitIn the previous section, we mentioned aNatural Language Processing Toolkit(NLPTK) that allows programmers with nolinguistic knowledge to rapidly develop naturallanguage user interfaces for their applications.The toolkit should incorporate the majorcomponents of an NLP system, such as a spellchecker, a parser and a semantic representationgenerator.
Using the toolkit, a softwareengineer will be able to create a system thatincorporates complex NLP techniques such assyntactic parsing and semantic understanding.In order to provide NL control to anapplication, the NLPTK needs to generatesemantic representations for input sentences.We refer to each of these semantic forms as aframe, which is basically a predicate-argumentrepresentation of a sentence.The NLPTK is implemented using thefollowing steps:1.
NLPTK begins to create an NLP front endby generating semantic representations ofsample input sentences provided by theprogrammer.2.
These representations are expanded usingsynonym sets and stored in a SemanticFrame Table (SFT), which becomes acomprehensive database of all thepossible commands a user could requestthe system to do.3.
The toolkit then creates methods forattaching the NLP front end to the backend applications.4.
When the NLP front end is released, a usermay enter an NL sentence, which istranslated into a semantic frame by thesystem.
The SFT is then searched for anequivalent frame.
If a match is found, theaction or command linked to this frame isexecuted.In order to generate semanticrepresentations in Step 1, the parser has toparse the input sentences into syntactic trees.During the process of building an NLP system,the programmer needs to customize the parserof the toolkit for their specific domain.
Forexample, the toolkit provides an interface tohighlight the domain specific words that arenot in the lexicon.
The toolkit then asks theprogrammer for information that helps thesystem insert the correct lexical item into thelexicon.
The NLPTK development team musthandle complicated customizations for theprogrammer.
For example, we might need tochange the rules behind the domain specificparser to handle certain natural language input.In Step 4, when the programmer finishesbuilding an NLP application, the system willimplement a domain specific parser.
Thetoolkit has been completely implemented andtested.We use a corpus of email messages fromour customers for developing the system.These emails contain questions, comments andgeneral inquiries regarding our document-conversion products.
We modified the rawemail programmatically to delete theattachments, HTML tags, headers and senderinformation.
In addition, we manually deletedsalutations, greetings and any information notdirectly related to customer support.
Thecorpus contains around 34,640 lines and170,000 words.
We constantly update it withnew emails from our customers.From this corpus, we created a test corpusof 1000 inquiries to test existing broad-coverage parsers and the parser of the toolkit.3 Minipar in NLPTKWe choose to use Minipar (Lin, 2001), awidely known parser in commercial domains,as the general parser of NLPTK.
It is worthpointing out that our methodology does notdepend on any individual parser, and we canuse any other available parser.3.1 Introduction to MiniparMinipar is a principle-based, broad-coverageparser for English (Lin, 2001).
It represents itsgrammar as a network of nodes and links,where the nodes represent grammaticalcategories and the links represent types ofdependency relationships.
The grammar ismanually constructed, based on the MinimalistProgram (Chomsky, 1995).Minipar constructs all possible parses of aninput sentence.
It makes use of the frequencycounts of the grammatical dependencyrelationships extracted by a collocationextractor (Lin, 1998b) from a 1GB corpusparsed with Minipar to resolve syntacticambiguities and rank candidate parse trees.The dependency tree with the highest rankingis returned as the parse of the sentence.The Minipar lexicon contains about130,000 entries, derived from WordNet(Fellbaum, 1998) with additional propernames.
The lexicon entry of a word lists allpossible parts of speech of the word and itssubcategorization frames (if any).Minipar achieves about 88% precision and80% recall with respect to dependencyrelationships (Lin, 1998a), evaluated on theSUSANNE corpus (Sampson, 1995), a subsetof the Brown Corpus of American English.3.2 Disadvantages of MiniparIn order to see how well Minipar performs inour domain, we tested it on 584 sentences fromour corpus.
Instead of checking the parse trees,we checked the frames corresponding to thesentences, since the accuracy of the frames iswhat we are most concerned with.
If any partof a frame was wrong, we treated it as an errorof the module that contributed to the error.
Wecounted all the errors caused by Minipar andits accuracy in terms of correctly parsedsentences is 77.6%.
Note that the accuracy isactually lower because later processes fix someerrors in order to generate correct frames.The majority of Minipar errors fall in thefollowing categories:1.
Tagging errors: some nouns are mis-tagged as verbs.
For example, in Can I geta copy of the batch product guide?, guideis tagged as a verb.2.
Attachment errors: some prepositionalphrases (PP) that should be attached totheir immediate preceding nouns areattached to the verbs.
For example, in CanDrake convert the PDF documents inJapanese?, in Japanese is attached toconvert.3.
Missing lexical entries: some domainspecific words such as download and theirusages are not in the Minipar lexicon.This introduces parsing errors becausesuch words are tagged as nouns bydefault.4.
Inability to handle ungrammaticalsentences: in a real world application, it isunrealistic to expect the user to enter onlygrammatical sentences.
Although Miniparstill produces a syntactic tree for anungrammatical sentence, the tree is illformed and cannot be used to extract thesemantic information being expressed.In addition, Minipar, like other broad-coverage parsers, cannot be adapted to specificapplications.
Its accuracy does not satisfy theneeds of our toolkit.
We have to build anotherparser on top of Minipar to enable domainspecific customizations to increase the parsingaccuracy.4 The Shallow ParserOur NLPTK maps input sentences to actionrequests.
In order to perform an accuratemapping the toolkit needs to get informationsuch as the sentence type, the main predicate,the arguments of the predicate, and themodifications of the predicate and argumentsfrom a sentence.
In other words, it mostlyneeds local dependency relationships.Therefore we decided to build a shallow parserinstead of a full parser.
A parser that capturesthe most frequent verb argument structures in adomain can be built relatively fast.
It takes lessspace, which can be an important issue forcertain applications.
For example, whenbuilding an NLP system for a handheldplatform, a light parser is needed because thememory cannot accommodate a full parser.4.1 IntroductionWe built a KWIC (keyword in context) verbshallow parser.
It captures only verb predicateswith their arguments, verb argument modifiersand verb adjuncts in a sentence.
The resultingtrees contain local and subjacent dependenciesbetween these elements.The shallow parser depends on three levelsof information processing: the verb list,subcategorization (in short, subcat) andsyntactic rules.
The verb subcat system isderived from Levin?s taxonomy of verbs andtheir classes (Levin, 1993).
We have 24 verbfiles containing 3200 verbs, which include allthe Levin verbs and the most frequent verbs inour corpus.
A verb is indexed to one or moresubcat files and each file represents a particularalternation semantico-syntactic sense.
We have272 syntactic subcat files derived from theLevin verb semantic classes.
The syntacticrules are marked for argument types andconstituency, using the Penn Treebank tagset(Marcus, 1993).
They contain both generalizedrules, e.g.,  .../NN, and specified rules, e.g.,purchase/VBP.
An example subcat rule for theverb purchase looks like this: .../DT .../JJ.../NN, .../DT .../NN from/RP .../NN for/RP.../NN.
The first element says that purchasetakes an NP argument, and the second says thatit takes an NP argument and two PP adjuncts.We also encoded specific PP head classinformation based on the WordNet concepts inthe rules for some attachment disambiguation.The shallow parser works like this: it firsttags an incoming sentence with Brill tagger(Brill, 1995) and matches verbs in the taggedsentence with the verb list.
If a match is found,the parser will open the subcat files indexed tothat verb and gather all the syntactic rules inthese specific subcat files.
It then matches theverb arguments with these syntactic rules andoutputs the results into a tree.
The parser cancontrol over-generation for any verb becausethe syntactic structures are limited to thatparticular verb's syntactic structure set fromthe Levin classes.4.2 Disadvantages of Shallow ParserThe disadvantages of the shallow parser aremainly due to its simplified design, including:1.
It cannot handle sentences whose mainverb is be or phrasal sentences without averb because the shallow parser mainlytargets command-and-control verbargument structures.2.
It cannot handle structures that appearbefore the verb.
Subjects will not appearin the parse tree even though it mightcontain important information.3.
It cannot detect sentence type, forexample, whether a sentence is a questionor a request.4.
It cannot handle negative or passivesentences.We tested the shallow parser on 500sentences from our corpus and compared theresults with the output of Minipar.
Weseparated the sentences into five sets of 100sentences.
After running the parser on each set,we fixed the problems that we could identify.This was our process of training the parser.Table 1 shows the data obtained from one suchcycle.
Since the shallow parser cannot handlesentences with the main verb be, thesesentences are excluded from the statistics.
Sothe test set actually contains 85 sentences.In Table 1, the first column and the firstrow show the statistics for the shallow parserand Minipar respectively.
The upper half of thetable is for the unseen data, where 55.3% ofthe sentences are parsed correctly and 11.8%incorrectly (judged by humans) by bothparsers.
18.9% of the sentences are parsedcorrectly by Minipar, but incorrectly by theshallow parser, and 14.1% vise versa.
Thelower half of the table shows the result afterfixing some shallow parser problems, forexample, adding a new syntactic rule.
Theaccuracy of the parser is significantlyimproved, from 69.4% to 81.2%.
This showsthe importance of adaptation to specificdomain needs, and that in our domain, theshallow parser outperforms Minipar.SP/MP Correct(74.1%)Wrong(25.9%)Correct (69.4%) 47 (55.3%) 12 (14.1%)Wrong (30.6%) 16 (18.9%) 10 (11.8%)SP/MP Correct(74.1%)Wrong(25.9%)Correct (81.2%) 53 (62.4%) 16 (18.8%)Wrong (18.8%) 10 (11.8%) 6 (7.1%)Table 1: Comparison of the shallowparser with Minipar on 85 sentencesThe parsers do not perform equally well onall sets of sentences.
For some sets, theaccuracies of Minipar and the shallow parserdrop to 60.9% and 67.8% respectively.5 Extending Minipar with theShallow ParserEach parser has pros and cons.
The advantageof Minipar is that it is a broad-coverage parserwith relatively high accuracy, and theadvantage of the shallow parser is that it isadaptable.
For this reason, we intend to useMinipar as our primary parser and the shallowparser a backup.
Table 1 shows only a smallpercentage of sentences parsed incorrectly byboth parsers (about 7%).
If we always choosethe correct tree between the two outputs, wewill have a parser with much higher accuracy.Therefore, combining the advantages of thetwo parsers will achieve better performance inboth coverage and accuracy.
Now the questionis how to decide if a tree is correct or not.5.1 Detecting Parsing ErrorsIn an ideal situation, each parser shouldprovide a confidence level for a tree that iscomparable to each other.
We would choosethe tree with higher confidence.
However, thisis not possible in our case because weightingsof the Minipar trees are not publicly available,and the shallow parser is a rule-based systemwithout confidence information.Instead, we use a few simple heuristics todecide if a tree is right or wrong, based on ananalysis of the trees generated for our testsentences.
For example, given a sentence, theMinipar tree is incorrect if it has more than onesubtree connected by a top-level node whosesyntactic category is U (unknown).
A shallowparser tree is wrong if there are unparsedwords at the end of the sentence after the mainverb (except for interjections).
We have threeheuristics identifying a wrong Minipar tree andtwo identifying a wrong shallow parser tree.
Ifa tree passes these heuristics, we must label thetree as a good parse.
This may not be true, butwe will compensate for this simplificationlater.
The module implementing theseheuristics is called the error detector.We tested the three heuristics for Minipartrees on a combination of 84 requestive,interrogative and declarative sentences.
Theresults are given in the upper part of Table 2.The table shows that 45 correct Minipar trees(judged by humans) are identified as correct bythe error detector and 18 wrong trees areidentified as wrong, so the accuracy is 75%.Tagging errors and some attachment errorscannot be detected.MP/ED Correct(76.2%)Wrong(23.8%)Correct (56%) 45 (53.6%) 2 (2.4%)Wrong (44%) 19 (22.6%) 18 (21.4%)SP/ED Correct(73%)Wrong(26%)Correct (59%) 58 (58%) 1 (1%)Wrong (40%) 15 (15%) 25 (25%)Table 2: The performance of the parsetree error detectorWe tested the two heuristics for shallowparser trees on 100 sentences from our corpusand the result is given in the lower part ofTable 2.
The accuracy is about 83%.
We didnot use the same set of sentences to test thetwo sets of heuristics because the coverage ofthe two parsers is different.5.2 Choosing the Better Parse TreesWe run the two parsers in parallel to generatetwo parse trees for an input sentence, but wecannot depend only on the error detector todecide which tree to choose because it is notaccurate enough.
Table 2 shows that the errordetector mistakenly judges some wrong treesas correct, but not the other way round.
Inother words, when the detector says a tree iswrong, we have high confidence that it isindeed wrong, but when it says a tree iscorrect, there is some chance that the tree isactually wrong.
This motivates us todistinguish three cases:1.
When only one of the two parse trees isdetected as wrong, we choose the correcttree, because no matter what the correcttree actually is, the other tree is definitelywrong so we cannot choose it.2.
When both trees are detected as wrong, wechoose the Minipar tree because it handlesmore syntactic structures.3.
When both trees are detected as correct,we need more analysis because eithermight be wrong.We have mentioned in the previous sectionsthe problems with both parsers.
By comparingtheir pros and cons, we come up withheuristics for determining which tree is betterfor the third case above.The decision flow for selecting the betterparse is given in Figure 1.
Since the shallowparser cannot handle negative and passivesentences as well as sentences with the mainverb be, we choose the Minipar trees for suchsentences.
The shallow parser outperformsMinipar on tagging and some PP attachmentbecause it checks the WordNet concepts.
So,when we detect differences concerning part-of-speech tags and PP attachment in the parsetrees, we choose the shallow parser tree as theoutput.
In addition, we prefer the parse withbigger NP chunks.We tested these heuristics on 200 sentencesand the result is shown in Table 3.
The firstrow specifies whether a Minipar tree or ashallow parser tree is chosen as the finaloutput.
The first column gives whether thefinal tree is correct or incorrect according tohuman judgment.
88% of the time, Minipartrees are chosen and they are 82.5% accurate.The overall contribution of Minipar to theaccuracy is 73.5%.
The improvement from justusing Minipar is about 7%, from about 75.5%to 82.5%.
This is a significant improvement.The main computational expense ofrunning two parsers in parallel is time.
Sinceour shallow parser has not been optimized, theextended parser is about 2.5 times slower thanMinipar alone.
We hope that with someoptimization, the speed of the system willincrease considerably.
Even in the current timeframe, it takes less than 0.6 second to parse a15 word sentence.Final tree MP tree(88%)SP tree(11%)Correct (82.5%) 73.5% 9%Wrong (16.5%) 14.5% 2%Table 3: Results for the extended parser6 Conclusions and Future WorkIn this paper we described a parser that extendsa broad-coverage parser, Minipar, with adomain adaptable shallow parser in order toachieve generality and higher accuracy at thesame time.
This parser is an importantcomponent of a general NLP Toolkit, whichhelps programmers quickly develop an NLPfront end that handles natural language inputfrom their end users.
We tested the parser on200 sentences from our corpus and the resultshows significant improvement over usingMinipar alone.Future work includes improving theefficiency and accuracy of the shallow parser.Also, we will test the parser on a differentdomain to see how much work is required toswitch to a new domain.ReferencesAlam H. (2000) Spoken Language GenericUser Interface (SLGUI).
Technical ReportAFRL-IF-RS-TR-2000-58, Air Force ResearchLaboratory, Rome.Brill E. (1992) A Simple Rule-based Part ofSpeech Tagger.
In Proceedings of the 3rdConference on Applied Natural LanguageProcessing.Charniak E. (2000) A Maximum-Entropy-Inspired Parser.
In Proceedings of the 1stMeeting of NAACL.
Washington.Chomsky N. (1995) Minimalist Program.MIT Press.Collins M. (1999) Head-Driven StatisticalModels for Natural Language Parsing.
PhDDissertation, University of Pennsylvania.Is the sentencepassive?Is the SP tree empty?Is the SP  tree correct?Adding the sentence typeand subject of the M P tree tothe SP treeNoNoYesAccept M P treeM inipar (M P) tree Shallow Parser (SP) treeFinal parse treeYesNoIs the sentencenegative?NoYesYesYesNoNoYesNoIs the M P tree correct?NoIs the size of the M Ptree bigger than orequal to that of theSP  tree?Is the length of anNP chunk in  the M Ptree longer?YesDoes the M P treehave less verb  tags?YesNoNoYesYesNoYes Does SP finds a verbwhen M P assigns asentence type as NP?Are the verb  tags inthe two treesinconsistent?Are there unequalnumber of verb tagsin the trees?Figure 1: Decision flow for parse tree selectionLevin B.
(1993) English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.Lin D. (1998a) Dependency-basedEvaluation of Minipar.
In Workshop on theEvaluation of Parsing Systems, Spain.Lin D. (1998b) Extracting Collocationsfrom Text Corpora.
In Workshop onComputational Terminology, Montreal,Canada, pp.
57-63.Lin D. (2001) Latat: Language and TextAnalysis Tools.
In Proceedings of HumanLanguage Technology Conference, CA, USA.Marcus M., Santorini B. and MarcinkiewiczM.
(1993) Building a Large Annotated Corpusof English: The Penn Treebank, ComputationalLinguistics, vol.
19, no.
2, pp.
313-330.Sampson G. (1995) English for theComputer.
Oxford University Press.
