Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 522?526,Dublin, Ireland, August 23-24, 2014.SAP-RI: Twitter Sentiment Analysis in Two DaysAkriti Vij1,2?, Nishtha Malhotra1,2?, Naveen Nandan1and Daniel Dahlmeier11SAP Research & Innovation, Singapore2Nanyang Technological University, Singapore{akriti.vij,nishtha.malhotra,naveen.nandan,d.dahlmeier}@sap.comAbstractWe describe the submission of the SAPResearch & Innovation team to the Se-mEval 2014 Task 9: Sentiment Analy-sis in Twitter.
We challenged ourselvesto develop a competitive sentiment anal-ysis system within a very limited timeframe.
Our submission was developedin less than two days and achieved anF1score of 77.26% for contextual polar-ity disambiguation and 55.47% for mes-sage polarity classification, which showsthat rapid prototyping of sentiment anal-ysis systems with reasonable accuracy ispossible.1 IntroductionMicroblogging platforms and social networkshave become increasingly popular for expressingopinions on a wide range of topics, hence mak-ing them valuable and ever-growing logs of pub-lic sentiment.
This has motivated the developmentof automatic natural language processing (NLP)methods to analyse the sentiment expressed inthese short, informal messages (Liu, 2012; Pangand Lee, 2008).In particular, the study of sentiment and opin-ions in messages from the Twitter microbloggingplatform has attracted a lot of interest (Jansen etal., 2009; Pak and Paroubek, 2010; Barbosa andFeng, 2010; O?Connor et al., 2010; Bifet et al.,2011).
However, comparative evaluations of senti-ment analysis of Twitter messages have previouslybeen hindered by the lack of a large benchmarkdata set.
The goal of the SemEval 2013 task 2:Sentiment Analysis in Twitter (Nakov et al., 2013)?The work was done during an internship at SAP.This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/and this year?s continuation in the SemEval 2014task 9: Sentiment Analysis in Twitter (Rosenthalet al., 2014) is to close this gap by hosting a sharedtask competition which provided a large corpus ofTwitter messages which are annotated with sen-timent polarity labels.
The task consists of twosubtasks: in subtask A contextual polarity disam-biguation, participants need to predict the polarityof a given word or phrase in the context of a tweetmessage, in subtask B message polarity classifica-tion, participants need to predict the dominatingsentiment of the complete message.
Both tasksconsider sentiment analysis to be a three-way clas-sification problem between positive, negative, andneutral sentiment.In this paper, we describe the submission of theSAP-RI team to the SemEval 2014 task 9.
Wechallenged ourselves to develop a competitive sen-timent analysis system within a very limited timeframe.
The complete system was implementedwithin only two days.
Our system is based onsupervised classification with support vector ma-chines with lexical and dictionary-based features.Our system achieved an F1score of 77.26% forcontextual polarity disambiguation and 55.47%for message polarity classification.
Although ourscores are about 10-20% behind the top-scoringsystems, we show that it is possible to developsentiment analysis systems via rapid prototypingwith reasonable accuracy in a very short amountof time.2 MethodsOur system is based on supervised classificationwith support vector machines and a variety of lex-ical and dictionary-based features.
From the be-ginning, we decided to restrict ourselves to super-vised classification and to focus on the constrainedsystem setting.
Experiments with more data orsemi-supervised learning would have required ad-ditional time and the results of last year?s task522did not show any convincing improvements usingfrom additional unconstrained data (Nakov et al.,2013).
We cast sentiment analysis as a multi-classclassification problem with three classes: positive,negative, and neutral.
For the features, we tried tore-implement most of the features from the NRC-Canada system (Mohammad et al., 2013) whichwas the best performing system in last year?s task.We describe the features in the following sections.2.1 Task A : FeaturesFor the contextual polarity disambiguation task,we extract features from the target phrase itselfand from a surrounding word window of fourwords before and after the target phrase.
To handlenegation, we append the suffix -neg to all wordsin a negated context.
A negated context includesany word in the target phrase or context that is fol-lowing a negation word1up to the next followingpunctuation symbol.?
Word N-grams: all lowercased unigramsand bigrams from the target phrase and thecontext.
We extract the lowercased full stringof the target phrase as an additional feature.?
Character N-grams: lowercased characterbigram and trigram prefixes and suffixes fromall words in the target phrase and the context.?
Elongations: binary feature that indicates thepresence of one or more words in the targetphrase or context that have a letter repeatedfor 3 for more times e.g., coool.?
Emoticons: two binary features that indicatethe presence of positive or negative emoti-cons in the target phrase or the context, re-spectively.
Two additional binary featuresindicate the presence of positive or negativeemoticons at the end of the target phrase orcontext2.?
Punctuation: three count features for thenumber of tokens that consist only of excla-mation marks, only of questions marks, ora mix of exclamation marks and questionsmarks, in the target phrase and context, re-ceptively.1http://sentiment.christopherpotts.net/lingstruc.html2positive emoticons: :-), :), :B, :-B, :3, =), <3, :D, :-D,=D, :?
), :d, ;), :}, :], :P, :-P, :-p, :p. negative emoticons: :-(,:/, :{, :[, -.-, - -, :O, :o, :?
(, :x, :X, v.v, ;(?
Casing: two binary features that indicate thepresence of at least one all upper-case wordand at least one title-cased word in the targetphrase or context, respectively.?
Stop words: a binary feature that indicates ifall the words in the target phrase or contextare stop words.
If so, an additional featureindicates the number of stop words: 1, 2, 3,or more stop words.?
Length: the number of tokens in the targetphrase and the context, plus a binary featurethat indicates the presence of any word withmore than three characters.?
Position: three binary features that indicatewhether a target phrase is at the beginning, inthe middle, or at the end of the tweet.?
Hashtags: all hashtags in the target phraseor the context.
To handle hashtags which areformed by concatenating words, e.g., #ihate-mondays, we additionally split hashtags us-ing a simple dictionary-based approach andadd each token of the segmented hashtag asan additional features.?
Twitter user: binary feature that indicateswhether the context or the target phrase con-tain a mention of a Twitter user.?
URL: binary feature that indicates whetherthe context or the target phrase contains aURL.?
Brown cluster: the word cluster index foreach word in the context.
Cluster indexes areobtained from the Brown word clusters of theARK Twitter tagger (Owoputi et al., 2013).?
Sentiment lexicons: we add the follow-ing sentiment dictionary features for the tar-get phrase and the context for four differ-ent sentiment lexicons (NRC sentiment lex-icon, NRC Hashtag lexicon (Mohammad etal., 2013), MPQA sentiment lexicon (Wilsonet al., 2005), and Bing Liu lexicon (Hu andLiu, 2004)):?
the count of words with positive senti-ment score.?
the sum of the sentiment scores for allwords.523?
the maximum non-negative sentimentscore for any word.?
the sentiment score of the last word withpositive sentiment score.We extract these features for both the targetphrase and the context.
For words that aremarked as negated, the sign of the sentimentscores flipped.
The MPQA lexicons requirespart of speech information.
We use the ARKTwitter part-of-speech tagger (Owoputi et al.,2013) to tag the input with part of speechtags.2.2 Task B : FeaturesFor the message polarity task, we extract featuresfrom the entire tweet message.
The features aresimilar to the features for phrase polarity disam-biguation.
As before we handle negation by ap-pending the suffix -neg to all words that appear ina negated context.?
Word N-grams: all lowercased N-grams forN=1, .
.
.
, 4 from the message.
We also in-clude ?skipgrams?
for each N-gram by re-placing each token in the N-gram with a as-terisk place holder, e.g., the cat ?
* cat,the *.?
Character N-grams: lowercased charac-ter level N-grams for N=3, .
.
.
, 5 for all thewords in the message.
Character N-grams donot cross word boundaries.?
Elongations: count of words in the messagewhich have a letter repeated for 3 for moretimes.?
Emoticons: similar to the contextual polaritydisambiguation task: two binary features forpresence of positive or negative emoticons inthe message and two binary features indicatethe presence of positive or negative emoti-cons at the end of the message.?
Punctuation: similar to the contextual polar-ity disambiguation task: three count featuresfor the number of tokens that consist only ofexclamation marks, only of questions marks,or a mix of exclamation marks and questionsmarks.?
Hashtags: all hashtags in the message.
Wedo not split concatonated hashtags.# Tokens # TweetsSubtask ATraining (SemEval 2014 train) 160,992 7,884Development (SemEval 2013 test) 76,409 3,710Subtask BTraining (SemEval 2014 train) 139,128 7,112Development (SemEval 2013 test) 47,052 2,405Table 1: Overview of the data sets.?
Casing: the count of all upper-case words inthe message.?
Brown cluster: similar to the contextual po-larity disambiguation task: the cluster indexfor each word in the message.3 Experiment and ResultsIn this section, we report experimental result forour method.
We used the scikit-learn Python ma-chine learning library (Pedregosa et al., 2011) toimplement the feature extraction pipeline and thesupport vector machine classifier.
We use a linearkernel for the support vector machine and fixed theSVM hyper-parameter C to 1.0.
We found thatscikit-learn allowed us to implement the systemfaster and resulted in much more compact codethan other machine learning tools we have workedwith in the past.We used the official training set provided for theSemEval 2014 task to train our system and evalu-ated on the test set of the SemEval 2013 task whichserved as development data for this year?s task3.Tweets in the training data that were not availableany more through the Twitter API were removedfrom the training set.
An overview of the data setsis shown in Table 1.
For the evaluation, we com-pute precision, recall and F1measure for the pos-itive, negative, and neutral sentiment tweets.
Fol-lowing the official evaluation metric, the overallprecision, recall, and F1measure of the system isthe average of the precision, recall, and F1mea-sures for positive and negative sentiment, respec-tively.Here, we report a feature ablation study: weomitted each individual feature category from thecomplete feature set to determine its influence onthe overall performance.
Table 2 summarizes theresults for subtask A and B.
Surprisingly many ofthe features do not result in a reduction of the F1score when removed, or even increase the score,3We also did some experiments with a 60:40 training/testsplit of the SemEval 2014 training data which showed com-parable results524Features Positive Negative Neutral OverallSubtask A P R F1P R F1P R F1P R F1All features 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82w/o Word N-grams 0.84 0.82 0.83 0.71 0.74 0.72 0.14 0.16 0.15 0.77 0.78 0.78w/o character N-grams 0.85 0.89 0.87 0.80 0.78 0.79 0.27 0.12 0.17 0.82 0.83 0.83w/o elongation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81w/o emoticons 0.85 0.87 0.86 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82w/o punctuation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.83 0.82w/o casing 0.86 0.87 0.87 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82w/o stop words 0.86 0.87 0.86 0.78 0.79 0.78 0.24 0.15 0.18 0.82 0.83 0.82w/o length 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.14 0.17 0.82 0.83 0.82w/o position 0.86 0.87 0.86 0.77 0.78 0.78 0.24 0.13 0.17 0.81 0.83 0.82w/o hashtags 0.86 0.87 0.87 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82w/o twitter user 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82w/o URL 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81w/o Brown cluster 0.86 0.88 0.87 0.78 0.80 0.79 0.25 0.13 0.17 0.82 0.84 0.83w/o Sentiment lexicon 0.81 0.84 0.82 0.70 0.68 0.69 0.16 0.09 0.11 0.75 0.76 0.76Subtask BAll features 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.54w/o word N-grams 0.73 0.59 0.65 0.52 0.46 0.49 0.61 0.75 0.67 0.62 0.52 0.57w/o character N-grams 0.80 0.49 0.61 0.65 0.23 0.34 0.56 0.90 0.69 0.72 0.36 0.48w/o elongation 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.55w/o emoticons 0.82 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.72 0.74 0.44 0.55w/o punctuation 0.81 0.54 0.65 0.66 0.34 0.45 0.59 0.89 0.71 0.74 0.44 0.55w/o casing 0.81 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.55w/o hashtags 0.82 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.54w/o Brown cluster 0.81 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.73 0.43 0.54Table 2: Experimental Results for feature ablation study.
Each row shows the precision, recall, and F1score for the positive, negative, and neutral class and the overall precision, recall, and F1score afterremoving the particular feature from the features set.although not significantly.
The most effective fea-tures are word N-grams and the sentiment lexi-cons.
It is interesting that the performance for theneutral class is very low for subtask A and highfor subtask B.
We can also see that for subtask B,our system clearly has a problem with recall forthe positive and negative sentiment.For the performance of our system in the Se-mEval 2014 shared task, we report the officialoverall F1scores of our system as released by theorganizers on the official test set in Table 3.
Thescores were reported separately for different testsets: the SemEval 2013 Twitter test set, a new Se-mEval 2014 Twitter test set, a new test set fromLiveJournal blogs, the SMS test set from the NUSSMS corpus (Chen and Kan, 2012), and a newtest set of sarcastic tweets.
We also include the F1score of the best participating system for each testset and the rank of our system among all partic-ipating systems.
The results of our system werefairly robust across different domains, with theexception of messages containing sarcasm whichshows understanding sarcasm requires a deeperand more subtle understanding of the text that isnot captured well in a simple linear model.Dataset Best score Our score RankSubtask ALiveJournal 2014 85.61 77.68 18 / 27SMS 2013 89.31 80.26 13 / 27Twitter 2013 90.14 80.32 17 / 27Twitter 2014 86.63 77.26 15 / 27Twitter 2014 Sarcasm 82.75 70.64 14 / 27Subtask BLiveJournal 2014 74.84 57.86 33 / 42SMS 2013 70.28 49.00 34 / 42Twitter 2013 72.12 50.18 37 / 42Twitter 2014 70.96 55.47 32 / 42Twitter 2014 Sarcasm 58.16 48.64 15 / 42Table 3: Official results for Semeval 2014 test set.Reported scores are overall F1scores.4 ConclusionIn this paper, we have described the submission ofthe SAP-RI team to the SemEval 2014 task 9.
Weshowed that is possible to develop sentiment anal-ysis systems via rapid prototyping with reasonableaccuracy within a couple of days.AcknowledgementThe research is partially funded by the EconomicDevelopment Board and the National ResearchFoundation of Singapore.525ReferencesLuciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on Twitter from biased and noisydata.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, pages 36?44.Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,and Ricard Gavalda.
2011.
Detecting sentimentchange in Twitter streaming data.
Journal of Ma-chine LearningResearch - Proceedings Track, 17:5?11.Tao Chen and Min-Yen Kan. 2012.
Creating a live,public short message service corpus: the NUS SMScorpus.
Language Resources and Evaluation, pages1?37.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 168?177.Bernhard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter power: Tweets aselectronic word of mouth.
J.
Am.
Soc Inf.
Sci.
Tech-nol., 60(11):2169?2188.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of Tweets.
In Proceed-ings of the 7th International Workshop on SemanticEvaluation, pages 321?327.Preslav Nakov, Zornitsa Kozareva, Alan Ritter, SaraRosenthal, Veselin Stoyanov, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysisin Twitter.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation, pages 312?320.Brendan O?Connor, Routledge Bryan R. Balasubra-manyan, Ramnath, and Noah A. Smith.
2010.
FromTweets to polls: Linking text sentiment to publicopinion time series.
In Proceedings of the Fourth In-ternational Conference on Weblogs and Social Me-dia (ICWSM ?10), pages 122?129.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), pages 380?390.Alexander Pak and Patrick Paroubek.
2010.
Twitterbased system: Using Twitter to disambiguating sen-timent ambiguous adjectives.
In Proceedings of the8th International Workshop on Semantic Evaluation,pages 436?439.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OliverGrisel, Mathieu Blondel, Peter.
Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Preslav Nakov andTorsten Zesch, editors, Proceedings of the 8th In-ternational Workshop on Semantic Evaluation, Se-mEval ?14, Dublin, Ireland.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HumanLanguage Technology and Empirical Methods inNatural Language Processing (HLT-EMNLP), pages307?314.526
