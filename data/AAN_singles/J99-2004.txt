Supertagging: An Approach to AlmostParsingSrinivas Bangalore*AT&T Labs - ResearchAravind K. Joshi tUniversity of PennsylvaniaIn this paper, we have proposed novel methods for robust parsing that integrate the flexibilityof linguistically motivated lexical descriptions with the robustness of statistical techniques.
Ourthesis is that the computation of linguistic structure can be localized iflexical items are associatedwith rich descriptions (supertags) that impose complex constraints in a local context.
The su-pertags are designed such that only those elements on which the lexical item imposes constraintsappear within a given supertag.
Further, each lexical item is associated with as many supertagsas the number of different syntactic ontexts in which the lexical item can appear.
This makesthe number of different descriptions for each lexical item much larger than when the descriptionsare less complex, thus increasing the local ambiguity for a parser.
But this local ambiguity canbe resolved by using statistical distributions of supertag co-occurrences collected from a corpusof parses.
We have explored these ideas in the context of the Lexicalized Tree-Adjoining Gram-mar (LTAG) framework.
The supertags in LTAG combine both phrase structure information anddependency information in a single representation.
Supertag disambiguation results in a repre-sentation that is effectively a parse (an almost parse), and the parser need "only" combine theindividual supertags.
This method of parsing can also be used to parse sentence fragments such asin spoken utterances where the disambiguated supertag sequence may not combine into a singlestructure.1.
IntroductionIn this paper, we present a robust parsing approach called supertagging that integratesthe flexibility of linguistically motivated lexical descriptions with the robustness ofstatistical techniques.
The idea underlying the approach is that the computation oflinguistic structure can be localized if lexical items are associated with rich descriptions(supertags) that impose complex constraints in a local context.
This makes the numberof different descriptions for each lexical item much larger than when the descriptionsare less complex, thus increasing the local ambiguity for a parser.
However, this localambiguity can be resolved by using statistical distributions of supertag co-occurrencescollected from a corpus of parses.
Supertag disambiguation results in a representationthat is effectively a parse (an almost parse).In the linguistic context, there can be many ways of increasing the complexity ofdescriptions of lexical items.
The idea is to associate l xical items with descriptions thatallow for all and only those elements on which the lexical item imposes constraints tobe within the same description.
Further, it is necessary to associate ach lexical itemwith as many descriptions as the number of different syntactic ontexts in which the* 180 Park Avenue, Florharn Park, NJ 07932.
E-mail: srini@research.att.comt Department of Computer and Information Sciences and Institute for Research inCognitive Science,University of Pennsylvania, Philadelphia, PA 19104.
E-maih joshi@linc.cis.upenn.edu(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 2lexical item can appear.
This, of course, increases the local ambiguity for the parser.The parser has to decide which complex description out of the set of descriptionsassociated with each lexical item is to be used for a given reading of a sentence, venbefore combining the descriptions together.
The obvious solution is to put the burdenof this job entirely on the parser.
The parser will eventually disambiguate all the de-scriptions and pick one per lexical item, for a given reading of the sentence.
However,there is an alternate method of parsing that reduces the amount of disambiguationdone by the parser.
The idea is to locally check the constraints that are associatedwith the descriptions of lexical items to filter out incompatible descriptions.
1 Duringthis disambiguation, the system can also exploit statistical information that can beassociated with the descriptions based on their distribution in a corpus of parses.We first employed these ideas in the context of Lexicalized Tree Adjoining gram-mars (LTAG) in Joshi and Srinivas (1994).
Although presented with respect o LTAG,these techniques are applicable to other lexicalized grammars as well.
In this paper, wepresent vastly improved supertag disambiguation results--from previously published68% accuracy to 92% accuracy using a larger training corpus and better smoothingtechniques.
The layout of the paper is as follows: In Section 2, we present an overviewof the robust parsing approaches.
A brief introduction to Lexicalized Tree Adjoininggrammars i presented in Section 3.
Section 4 illustrates the goal of supertag disam-biguation through an example.
Various methods and their performance r sults forsupertag disambiguation are discussed in detail in Section 5 and Section 6.
In Sec-tion 7, we discuss the efficiency gained in performing supertag disambiguation beforeparsing.
A robust and lightweight dependency analyzer that uses the supertag out-put is briefly presented in Section 8.
In Section 9, we will discuss the applicability ofsupertag disambiguation to other lexicalized grammars.2.
Related ApproachesIn recent years, there have been a number of attempts at robust parsing of natural lan-guage.
They can be broadly categorized under two paradigms--finite-state-grammar-based parsers and statistical parsers.
We briefly present hese two paradigms andsituate our approach to robust parsing relative to these paradigms.2.1 Finite-State-Grammar-based ParsersFinite-state-grammar-based pproaches toparsing are exemplified by the parsing sys-tems in Joshi, (1960), Abney (1990), Appelt et al (1993), Roche (1993), Grishman (1995),Hobbs et al (1997), Joshi and Hopely (1997), and Karttunen et al (1997).
These sys-tems use grammars that are represented as cascaded finite-state regular expressionrecognizers.
The regular expressions are usually hand-crafted.
Each recognizer in thecascade provides a locally optimal output.
The output of these systems i  mostly in theform of noun groups and verb groups rather than constituent s ructure, often calleda sha l low parse.
There are no clause-level attachments or modifier attachments in theshallow parse.
These parsers always produce one output, since they use the longest-match heuristic to resolve cases of ambiguity when more than one regular expression1 The use of descriptions for primitives tocapture constraints locally has a precursor inAI.
The Waltzalgorithm (Waltz 1975) for labeling vertices of polygonal solid objects can be thought of in these terms.Waltz made the description f vertices more complex by including information about the incidentedges, associated surfaces and other information.
This increases the local ambiguity but the localconstraints on the complex descriptions are strong enough to efficiently disambiguate th  descriptions.Of course, Waltz did not use statistical information for disambiguation.
Seealso Joshi (1998).238Bangalore and Joshi Supertaggingmatches the input string at a given position.
At present none of these systems useany statistical information to resolve ambiguity.
The grammar itself can be partitionedinto domain-independent a d domain-specific regular expressions, which implies thatporting to a new domain would involve rewriting the domain-dependent expressions.This approach has proved to be quite successful as a preprocessor in informationextraction systems (Hobbs et al 1995; Grishman 1995).2.2 Statistical ParsersPioneered by the IBM natural anguage group (Fujisaki et al 1989) and later pursuedby, for example, Schabes, Roth, and Osborne (1993), Jelinek et al (1994), Magerman(1995), Collins (1996), and Charniak (1997), this approach decouples the issue of well-formedness of an input string from the problem of assigning a structure to it.
Thesesystems attempt o assign some structure to every input string.
The rules to assign astructure to an input are extracted automatically from hand-annotated parses of largecorpora, which are then subjected to smoothing to obtain reasonable coverage of thelanguage.
The resultant set of rules are not linguistically transparent and are not easilymodifiable.
Lexical and structural ambiguity is resolved using probability informationthat is encoded in the rules.
This allows the system to assign the most-likely structureto each input.
The output of these systems consists of constituent analysis, the degreeof detail of which is dependent on the detail of annotation present in the treebank thatis used to train the system.There are also parsers that use probabilistic (weighting) information in conjunctionwith hand-crafted grammars, for example, Black et al (1993), Nagao (1994), Alshawiand Carter (1994), and Srinivas, Doran, and Kulick (1995).
In these cases the proba-bilistic information is primarily used to rank the parses produced by the parser andnot so much for the purpose of robustness of the system.3.
Lexicalized GrammarsLexicalized grammars are particularly well-suited for the specification of natural an-guage grammars.
The lexicon plays a central role in linguistic formalisms uch as LFG(Kaplan and Bresnan 1983), GPSG (Gazdar et al 1985), HPSG (Pollard and Sag 1987),CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991),Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992).Parsing, lexical semantics, and machine translation, to name a few areas, have all ben-efited from lexicalization.
Lexicalization provides a clean interface for combining thesyntactic and semantic information in the lexicon.
We discuss the merits of lexical-ization and other related issues in the context of partial parsing and briefly discussFeature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative ofthe class of lexicalized grammars.Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, andTakahashi 1975; Vijay-Shanker 1987; Schabes, AbeillG and Joshi 1988; Vijay-Shankerand Joshi 1991; Joshi and Schabes 1996) is a tree-rewriting rammar formalism unlikecontext-free grammars and head grammars, which are string-rewriting formalisms.The primitive elements of FB-LTAGs are called elementary trees.
Each elementary treeis associated with at least one lexical item on its frontier.
The lexical item associatedwith an elementary tree is called the anchor of that tree.
An elementary tree serves as acomplex description of the anchor and provides adomain of locality over which the an-chor can specify syntactic and semantic (predicate argument) constraints.
Elementarytrees are of two kinds: (a) initial trees and (b) auxiliary trees.
In an FB-LTAG gram-mar for natural language, initial trees are phrase structure trees of simple sentences239Computational Linguistics Volume 25, Number 2containing no recursion, while recursive structures are represented by auxiliary trees.Elementary trees are combined by substitution and adjunction operations.
The resultof combining the elementary trees is the derived tree and the process of combining theelementary trees to yield a parse of the sentence is represented by the derivation tree.The derivation tree can also be interpreted as a dependency tree with unlabeled arcsbetween words of the sentence.
A more detailed iscussion of LTAGs with an exampleand some of the key properties of elementary trees is presented in Appendix A.4.
SupertagsPart-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel etal.
1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce)the part-of-speech ambiguity.
The POS taggers are all local in the sense that they useinformation from a limited context in deciding which tag(s) to choose for each word.As is well known, these taggers are quite successful.In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG),each lexical item is associated with at least one elementary structure (tree).
The elemen-tary structures of LTAG localize dependencies, including long-distance dependencies,by requiring that all and only the dependent elements be present within the samestructure.
As a result of this localization, a lexical item may be (and, in general al-most always is) associated with more than one elementary structure.
We will call theseelementary structures upertags, in order to distinguish them from the standard part-of-speech tags.
Note that even when a word has a unique standard part of speech, saya verb (V), there will usually be more than one supertag associated with this word.Since there is only one supertag for each word (assuming there is no global ambiguity)when the parse is complete, an LTAG parser (Schabes, Abeill6, and Joshi 1988) needsto search a large space of supertags to select he right one for each word before com-bining them for the parse of a sentence.
It is this problem of supertag disambiguationthat we address in this paper.Since LTAGs are lexicalized, we are presented with a novel opportunity to elimi-nate or substantially reduce the supertag assignment ambiguity by using local informa-tion, such as local lexical dependencies, prior to parsing.
As in standard part-of-speechdisambiguation, we can use local statistical information in the form of n-gram modelsbased on the distribution of supertags in an LTAG parsed corpus.
Moreover, sincethe supertags encode dependency information, we can also use information about thedistribution of distances between a given supertag and its dependent supertags.Note that as in standard part-of-speech disambiguation, supertag disambiguationcould have been done by a parser.
However, carrying out part-of-speech disambigua-tion prior to parsing makes the job of the parser much easier and therefore speedsit up.
Supertag disambiguation reduces the work of the parser even further.
Aftersupertag disambiguation, we would have effectively completed the parse and theparser need "only" combine the individual structures; hence the term "almost pars-ing."
This method can also be used to associate a structure to sentence fragments andin cases where the supertag sequence after disambiguation may not combine into asingle structure.4.1 Example of SupertaggingLTAGs, by virtue of possessing the Extended Domain of Locality (EDL) property, 2 as-sociate with each lexical item, one elementary tree for each syntactic environment that2 EDL is described inAppendix B.240Bangalore and Joshi SupertaggingTable 1Examples of syntactic environments where the supertags shown in Figure 1 would be used.Supertag Construction Exampleal Nominal Predicativea2 Noun Phrasea 3 Topicalizationa4 Adjectival Predicativeas Noun Phrasefll Determinert2 Nominal Modifiera6 Nominal PredicativeSubject Extractiona7 Imperativet3 Determinert4 Adjectival Modifieras Nominal PredicativeSubject Extractiona9 Noun Phraseal0 Nominal Predicativec~11 Transitive Verba12 Adjectival PredicativeSubject Extractiona13 Noun Phrasethis is the purchasethe priceAlmost everything, the price includesthis is ancillarythe companythe companypurchase orderwhat is the priceinclude the share pricetwo hundred menancillary unitwhich are the companiespurchases have not increased.this is the pricethe price includes everythingwhat is ancillarycompanies have not been profitablethe lexical item may appear in.
As a result, each lexical item is invariably associatedwith more than one elementary tree.
We call the elementary structures associated witheach lexical item super parts-of-speech (super POS) or supertags.
3 Figure 1 illustratesa few elementary trees associated with each word of the sentence: the purchase priceincludes two ancillary companies.
Table 1 provides an example context in which eachsupertag shown in Figure 1 would be used.The example in Figure 2 illustrates the initial set of supertags assigned to eachword of the sentence: the purchase price includes two ancillary companies.
The order of thesupertags for each lexical item in the example is not relevant.
Figure 2 also showsthe final supertag sequence assigned by the supertagger, which picks the best su-pertag sequence using statistical information (described in Section 6) about individualsupertags and their dependencies on other supertags.
The chosen supertags are com-bined to derive a parse.
Without the supertagger, the parser would have to processcombinations of the entire set of trees (at least the 17 trees shown); with it the parserneed only process combinations of 7 trees.5.
Reducing Supertag Ambiguity Using Structural InformationThe structure of the supertag can be best seen as providing admissibility constraintson syntactic environments in which it may be used.
Some of these constraints can bechecked locally.
The following are a few constraints that can be used to determine theadmissibility of a syntactic environment for a supertag: 43 For the purpose of this paper, we suppress the features associated with the supertags.4 Mitch Marcus pointed out that these tests are similar to the generalized shaper tests used in theHarvard Predictive Analyzer (Kuno 1966).241Computational Linguistics Volume 25, Number 2Sr /NNPo~ VPV NP II INt?3qD NP*the purchase\]~1 ~2NP s, s, NPNB s, i NPoJ- VP~ veI " " ?
, Iprice I \] companies ~haea, ~n~y~2 0~3 Or4 assq/~  Sr Nr sqNPo VP I~ o VP\] ~ N~ / ~  A NI a NPo vp?
0 v ~ \[ / /  \ D DetP ;  I /~J I,, , Vl "* I " ;7\[ two ancillary ISqNP Sr S ~ NPI / x  .....
IN v Nr, ~ .L v'/"NA~ NI I I v NP,~ I II companies purchase p.~, ilKllld~ a~U)ary~9 O~lO Q:II ~12 ~13NPD NP*IthetheN~ NP s N r NPN Nf* N ~ D DetP/* A N f* Npurchase price includes two ancillary companiesf12 0?2 0tl l  /33 /~4 0?13purchase price inc ludes two anc i l la ry  compan ies .Figure 1A selection of the supertags associated with each word of the sentence: the purchase priceincludes two ancillary companies.Span of the supertag: Span of a supertag is the minimum number oflexical items that the supertag can coven Each substitution site of asupertag will cover at least one lexical item in the input.
A simple rulecan be used to eliminate supertags based on the span constraint: if thespan of a supertag is larger than the input string, then the supertagcannot be used in any parse of the input string.242Bangalore and Joshi SupertaggingSent: the purchase price includes two ancillary companies.Initial al OL2 0~3 a4 a5Assignment fll f12 (3?6 a7 f13 f14 a8O~9 OL10 C~?11 OL12 OL13FinalAssignment 131 f12 a2 au /33 f14 a13Figure 2Supertag disambiguation for the sentence: the purchase price includes two ancillarycompanies.Table 2Supertag ambiguity with and without he use of structural constraints.System Total # of Words Average # of Supertags/WordWithout Structural Constraints 48,783 47.0With Structural Constraints 48,783 25.0?
Left (Right) span constraint: If the span of the supertag to the left (right)of the anchor is larger than the length of the string to the left (right) ofthe word that anchors the supertag, then the supertag cannot be used inany parse of the input string.?
Lexical items in the supertag: A supertag can be eliminated if theterminals appearing on the frontier of the supertag do not appear in theinput string.Supertags with the built-in lexical item by, that represent passive constructions aretypically eliminated from being considered uring the parse of an active sentence.More generally, these constraints can be used to eliminate supertags that cannothave their features atisfied in the context of the input string.
An example of this isthe elimination of supertag that requires a wh+ NP when the input string does notcontain wh-words.Table 2 indicates the decrease in supertag ambiguity for 2,012 WSJ sentences(48,763 words) by using the structural constraints relative to the supertag ambigu-ity without the structural constraintsPThese filters prove to be very effective in reducing supertag ambiguity.
The graphin Figure 3 plots the number of supertags at the sentence level for sentences of length2 to 50 words with and without he filters.
As can be seen from the graph, the supertagambiguity is significantly lower when the filters are used.
The graph in Figure 4 showsthe percentage drop in supertag ambiguity due to filtering for sentences of length 2 to50 words.
As can be seen, the average reduction in supertag ambiguity is about 50%.This means that given a sentence, close to 50% of the supertags can be eliminatedeven before parsing begins by just using structural constraints of the supertags.
Thisreduction in supertag ambiguity speeds up the parser significantly.
In fact, the supertag5 WSJ Section 20 of the Penn Treebank.243Computational Linguistics Volume 25, Number 2# of Supertgas x 1032.802.602.402.202.001.801.601.401.201.000.800.600.400.200.00A,Ui....", .
.
. "
.
.
!
"V" " l .d  i -diI'V7:i- ,.,.
:" :" : " .
i  "" .
.
~ ..... i. , .
.
, .
:0.00 10.00 20.00 30.00 40.00 50.00'Without FiltersXViiii'i~i//~rs" .............Sentence LengthF igure 3Compar i son  of number  of supertags wi th  and  w i thout  f i ltering for sentences of length 2 to 50words.ambiguity in XTAG system is so large that the parser is prohibitively slow withoutthe use of these filters.Table 3 tabulates the reduction of supertag ambiguity due to the filters againstvarious parts of speech: Verbs in all their forms contribute most to the problem ofsupertag ambiguity and most of the supertag ambiguity for verbs is due to light verbsand verb particles.
The filters are very effective in eliminating over 50% of the verbanchored supertags.Even though structural constraints are effective in reducing supertag ambiguity,the search space for the parser is still sufficiently large.
In the next few sections, wepresent stochastic and rule-based approaches to supertag disambiguation.6 The description of the part-of-speech tags is provided in Marcus, Santorini, and Marcinkiewicz (1993).244Bangalore and Joshi SupertaggingPercentage80.00 \] ~ ~75.13070.0065.0060.00,~oo III ~ ~ t , /i ~ / / / /~,ii~o III / / / .
/ , \  .,,t,_ .. ?r-t,/ v I /~ i \ ,  f HI40.00 III l ,  / ~ '~I I lit~,oo II !
l i l yIII I,,, ~,.oo IIIIILl " "  ~o.oo i ~ I I I"??
II '~ I \ ]  10.00 \],.oo II IIIIo.oo II _ , .
IIIISentence Length10.00 20.00 30.00 40.00 50.00Figure 4Percentage drop in the number of supertags with and without filtering for sentences oflength2 to 50 words.6.
Models, Data, Experiments, and ResultsBefore proceeding to discuss the various models for supertag disambiguation, wewould like to trace the time course of development of this work.
We do this not onlyto show the improvements made to the early work reported in our 1994 paper (Joshiand Srinivas 1994), but also to explain the rationale for choosing certain models ofsupertag disambiguation ver others.
We summarize the early work in the followingsubsection.6.1 Early WorkAs reported in Joshi and Srinivas (1994), we experimented with a trigram model aswell as the dependency model for supertag disambiguation.
The trigram model thatwas trained on (part-of-speech, supertag) pairs, instead of (words, supertag) pairs,collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJsentences produced a correct supertag for 68% of the words in the test set.
We havesince significantly improved the performance of the trigram model by using a larger245Computational Linguistics Volume 25, Number 2Table 3The effect of filters on supertag ambiguity tabulated against part of speech.POS Average # of Supertags Average # of Supertags Percentage Dropwithout Filters with Filters in Supertag AmbiguityVBP 516.5 250.0 51.6VB 435.8 224.9 48.4VBD 209.0 100.7 51.8VBN 188.2 74.7 60.3MD 167.2 121.0 27.6VBZ 165.1 71.6 56.6VBG 100.7 49.8 50.5RP 34.5 30.9 10.5IN 24.3 20.9 14.0JJS 23.8 12.7 46.9WRB 23.1 14.3 38.2JJR 22.7 14.2 37.7JJ 21.7 13.5 37.920.0 10.7 46.6NN 19.8 10.7 46.0NNS 17.0 10.5 38.6NNP 15.0 10.2 31.9NNPS 15.0 10.2 32.1LS 15.0 15.0 0.0FW 15.0 15.0 0.0-RRB- 15.0 10.7 28.4-LRB- 15.0 12.3 18.0RBR 14.9 9.5 36.3RBS 14.9 6.1 59.2CC 14.8 3.4 76.9EX 14.0 5.8 58.7CD 13.3 9.9 25.8TO 11.3 10.8 4.5PRP 10.7 5.3 50.2UH 10.0 3.0 70.0RB 10.0 5.3 46.4" 6.0 3.2 46.7: 5.5 3.2 42.1PDT 5.4 4.9 9.0WP 4.6 2.9 35.8WP$ 4.0 1.8 56.2DT 3.9 3.1 21.8PRP$ 3.8 2.9 22.23.0 1.0 65.4POS 2.5 2.1 13.9WDT 1.2 1.1 5.5training set and incorporating smoothing techniques.
We present a detailed discussionof the model and its performance on a range of corpora in Section 6.5.
In Section 6.2,we briefly mention the dependency model of supertagging that was reported in theearlier work.6.2 Dependency ModelIn an n-gram model for disambiguating supertags, dependencies between supertagsthat appear beyond the n-word window cannot be incorporated.
This limitation canbe overcome if no a priori bound is set on the size of the window but instead a246Bangalore and Joshi Supertaggingprobability distribution of the distances of the dependent supertags for each supertagis maintained.
We define dependency between supertags in the obvious way: A su-pertag is dependent on another supertag if the former substitutes or adjoins into thelatter.
Thus, the substitution and the foot nodes of a supertag can be seen as specify-ing dependency requirements of the supertag.
The probability with which a supertagdepends on another supertag is collected from a corpus of sentences annotated withderivation structures.
Given a set of supertags for each word and the dependencyinformation between pairs of supertags, the objective of the dependency model is tocompute the most likely dependency linkage that spans the entire string.
The resultof producing the dependency linkage is a sequence of supertags, one for each wordof the sentence along with the dependency information.Since first reported in Joshi and Srinivas (1994), we have not continued experimentsusing this model of supertagging, primarily for two reasons.
We are restrained bythe lack of a large corpus of LTAG parsed derivation structures that is needed toreliably estimate the various parameters of this model.
We are currently in the processof collecting a large LTAG parsed WSJ corpus, with each sentence annotated withthe correct derivation.
A second reason for the disuse of the dependency model forsupertagging is that the objective of supertagging is to see how far local techniques canbe used to disambiguate supertags even before parsing begins.
The dependency model,in contrast, is too much like full parsing and is contrary to the spirit of supertagging.6.3 N-gram Models with SmoothingWe have improved the performance of the trigram model by incorporating smoothingtechniques into the model and training the model on a larger training corpus.
Wehave also proposed some new models for supertag disambiguation.
In this section,we discuss these developments in detail.Two sets of data are used for training and testing the models for supertag dis-ambiguation.
The first set has been collected by parsing the Wall Street Journal 7, IBMManual, and ATIS corpora using the wide-coverage English grammar being developedas part of the XTAG system (Doran et al 1994).
The correct derivation from all thederivations produced by the XTAG system was picked for each sentence from thesecorpora.The second and larger data set was collected by converting the Penn Treebankparses of the Wall Street Journal sentences.
The objective was to associate ach lexicalitem of a sentence with a supertag, given the phrase structure parse of the sentence.This process involved a number of heuristics based on local tree contexts.
The heuris-tics made use of information about the labels of a word's dominating nodes (parent,grandparent, and great-grandparent), labels of its siblings (left and right) and siblingsof its parent.
An example of the result of this conversion is shown in Figure 5.
Itmust be noted that this conversion is not perfect and is correct only to a first orderof approximation owing mostly to errors in conversion and lack of certain kinds ofinformation such as distinction between adjunct and argument preposition phrases,in the Penn Treebank parses.
Even though the converted supertag corpus can be re-fined further, the corpus in its present form has proved to be an invaluable resourcein improving the performance of the supertag models as is discussed in the followingsections.7 Sentences of length < 15 words.247Computational Linguistics Volume 25, Number 2( ("S"("NP-SBJ" ("NNP .... Mr.") ("NNP .... Vinken") )("VP" ("VBZ .... is")("NP-PRD"("NP" ("NN .... chairman") )("PP" ("IN .... of")("NP"("NP" ("NNP .... Elsevier") ("NNP .... N.V.") )( -  .... ,,)("NP" ("DT .... the") ("NNP .... Dutch") ("VBG"") )))))(,,.
.... .,,) ))"publishing") ("NN" "groupMr.//NNP//B_NnVinken//NNP//A_NXNis//VBZ//B_Vvxchairman//NN//A_nx0Nlof//IN//B nxPnxElsevier//NNP//B_NnN.V.//NNP//A_NXN,//,//B_nxPUnxputhe//DT//B_DnxDutch//NNP//B_Nnpublishing//VBG//B_Vngroup//NN//A_NXN.//.//B_sPU(noun modifier)(head noun)(auxiliary verb)(predicative noun)(noun-attached preposition)(noun modifier)(head noun)(appositive comma)(determiner)(noun modifier)(participle verb, nominal modifier)(head noun)(sentence punctuation)Figure 5The phrase structure tree and the supertags obtained from the phrase structure tree for theWSJ sentence: Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing roup.6.4 Unigram ModelUsing structural information to filter out supertags that cannot be used in any parseof the input string reduces the supertag ambiguity but obviously does not eliminateit completely.
One method of disambiguating the supertags assigned to each wordis to order the supertags by the lexical preference that the word has for them.
Thefrequency with which a certain supertag is associated with a word is a direct measureof its lexical preference for that supertag.
Associating frequencies with the supertagsand using them to associate a particular supertag with a word is clearly the simplestmeans of disambiguating supertags.
Therefore a unigram model is given by:whereSupertag(wi) -- tk 9 argmaxtkPr(tk I wi).
(1)frequency( tk, wi)Pr(tk l wi) = frequency(wi) (2)Thus, the most frequent supertag that a word is associated with in a trainingcorpus is selected as the supertag for the word according to the unigram model.
Forthe words that do not appear in the training corpus we back off to the part of speechof the word and use the most frequent supertag associated with that part of speechas the supertag for the word.248Bangalore and Joshi SupertaggingTable 4Results from the unigram supertag model.Data Set Training Set Test Set Top n Supertags % SuccessXTAG Parses 8,000 3,000 n = 1 73.4%n = 2 80.2%n = 3 80.8%Converted Penn Treebank Parses 1,000,000 47,000 n = I 77.2%n = 2 87.0%n = 3 91.5%6.4.1 Experiments and Results.
We tested the performance of the unigram model onthe previously discussed two sets of data.
The words are first assigned standard partsof speech using a conventional tagger (Church 1988) and then are assigned supertagsaccording to the unigram model.
A word in a sentence is considered correctly su-pertagged if it is assigned the same supertag as it is associated with in the correctparse of the sentence.
The results of these experiments are tabulated in Table 4.Although the performance of the unigram model for supertagging is significantlylower than the performance of the unigram model for part-of-speech tagging (91%accuracy), it performed much better than expected considering the size of the supertagset is much larger than the size of part-of-speech tag set.
One of the reasons for thishigh performance is that the most frequent supertag for the most frequent words--determiners, nouns, and auxiliary verbs--is the correct supertag most of the time.Also, backing off to the part of speech elps in supertagging unknown words, whichmost often are nouns.
The bulk of the errors committed by the unigram model isincorrectly tagged verbs (subcategorization and transformation), prepositions (nounattached vs. verb attached) and nouns (head vs. modifier noun).6.5 N-gram ModelWe first explored the use of trigram model of supertag disambiguation in Joshi andSrinivas (1994).
The trigram model was trained on (part-of-speech, supertag) pairscollected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJsentences.
It produced a correct supertag for 68% of the words in the test set.
A majordrawback of this early work was that it used no lexical information i the supertaggingprocess as the training material consisted of (part-of-speech, supertag) pairs.
Since thatearly work, we have improved the performance of the model by incorporating lexicalinformation and sophisticated smoothing techniques, as well as training on largertraining sets.
In this section, we present the details and the performance evaluation ofthis model.In a unigram model, a word is always associated with the supertag that is mostpreferred by the word, irrespective of the context in which the word appears.
Analternate method that is sensitive to context is the n-gram model.
The n-gram modeltakes into account the contextual dependency probabilities between supertags withina window of n words in associating supertags to words.
Thus, the most probablesupertag sequence for an n-word sentence is given by:= argmaxTPr(T1, T2.
.
.
.
.
TN) * Pr(W1, W2,..., WN I T1, T2 .
.
.
.
.
TN) (3)where Ti is the supertag for word Wi.249Computational Linguistics Volume 25, Number 2To compute this using only local information, we approximate, assuming that theprobability of a word depends only on its supertagNPr(W1, W2 .
.
.
.
.
WN I T1, T2 .
.
.
.
.
TN) ~ I I  Pr(Wi l Ti) (4)i=1and also use an n-gram (trigram, in this case) approximationNPr(T1, T2 .
.
.
.
.
TN) ,,~ 1-I Pr(Ti I Zi-2, Zi-1) (5)i=1The term Pr(Ti I Ti-2, Ti-1) is known as the contextual probability since it indicatesthe size of the context used in the model and the term Pr(Wi I Ti) is called the wordemit probability since it is the probability of emitting the word Wi given the tag Ti.These probabilities are estimated using a corpus where each word is tagged with itscorrect supertag.The contextual probabilities were estimated using the relative frequency estimatesof the contexts in the training corpus.
To estimate the probabilities for contexts thatdo not appear in the training corpus, we used the Good-Turing discounting technique(Good 1953) combined with Katz's back off model (Katz 1987).
The idea here is todiscount he frequencies of events that occur in the corpus by an amount related totheir frequencies and utilize this discounted probability mass in the back off model todistribute to unseen events.
Thus, the Good-Turing discounting technique stimatesthe frequency of unseen events based on the distribution of the frequency of the countsof observed events in the corpus.
If r is the observed frequency of an event, and Nris the number of events with the observed frequency r, and N is the total numberof events, then the probability of an unseen event is given by N1/N.
Furthermore,the frequencies of the observed events are adjusted so that the total probability of allevents ums to one.
The adjusted frequency for observed events, r*, is computed asNr+lr *=( r+ l ) *  Nr (6)Once the frequencies of the observed events are discounted and the frequenciesfor unseen events are estimated, Katz's back off model is used.
In this technique, if theobserved frequency of an < n-gram, supertag> sequence is zero then its probabilityis computed based on the observed frequency of an (n - 1)-gram sequence.
Thus,15r(T3IT1, T2) = Pr(T3\]T1, T2) if Pr(T31T1, T2) > 0= a(T1, T2) * Pr(T31T2) if Pr(T21T1) > 0= Pr(T31T2) otherwisePr(T2IT1) = Pr(T2IT1) if Pr(T2IT1) > 0= fl(T1) * Prl(T2) otherwisewhere a(Ti, Tj) and fl(Tk) are constants to ensure that the probabilities sum to one.The word emit probability for the (word, supertag) pairs that appear in the trainingcorpus is computed using the relative frequency estimates as shown in Equation 7.
Forthe (word, supertag) pairs that do not appear in the corpus, the word emit probabilityis estimated as shown in Equation 8.
Some of the word features used in our imple-250Bangalore and Joshi Supertaggingmentation i clude prefixes and suffixes of length less than or equal to three characters,capitalization, and digit features.N(Wi, Ti)Pr(WdTi) - N(Ti) if N(Wi, Ti) > 0 (7)= Pr(UNKITi) ?
Pr(word_features(Wi)\[Ti) otherwise (8)The counts for the (word, supertag) pairs for the words that do not appear in thecorpus is estimated using the leaving-one-out technique (Niesler and Woodland 1996;Ney, Essen, and Kneser 1995).
A token UNK is associated with each supertag and itscount NUN K is estimated by:NI(Tj) Pr(UNK\[Tj) - N(Tj) + ~\]Pr(UNKITj) ?
N(Tj)NUNK(Tj) = 1 -- PF(UNKITj)where NI(Tj) is the number of words that are associated with the supertag Tj thatappear in the corpus exactly once.
N(Tj) is the frequency of the supertag Tj andNUNK(Tj) is the estimated count of UNK in Tj.
The constant 7/is introduced so as toensure that the probability is not greater than one, especially for supertags that aresparsely represented in the corpus.We use word features imilar to the ones used in Weischedel et al (1993), suchas capitalization, hyphenation, and endings of words, for estimating the word emitprobability of unknown words.6.5.1 Experiments and Results.
We tested the performance of the trigram model onvarious domains uch as the Wall Street Journal (WSJ), the IBM Manual corpus and theATIS corpus.
For the IBM Manual corpus and the ATIS domains, a supertag annotatedcorpus was collected using the parses of the XTAG system (Doran et al 1994) andselecting the correct analysis for each sentence.
The corpus was then randomly splitinto training and test material.
Supertag performance is measured as the percentageof words that are correctly supertagged by a model when compared with the key forthe words in the test corpus.Experiment 1: (Performance on the Wall Street Journal corpus).
We used the two sets ofdata, from the XTAG parses and from the conversion of the Penn Treebank parses toevaluate the performance of the trigram model.
Table 5 shows the performance on thetwo sets of data.
The first data set, data collected from the XTAG parses, was splitinto 8,000 words of training and 3,000 words of test material.
The data collected fromconverting the Penn Treebank was used in two experiments differing in the size of thetraining corpus--200,000 words 8and 1,000,000 words9--and tested on 47,000 words 1?.A total of 300 different supertags were used in these experiments.Experiment 2: (Performance on the IBM Manual Corpus and ATIS).
For testing the perfor-mance of the trigram supertagger on the IBM Manual corpus, a set of 14,000 words8 Sentences in wsJ Sections 15 through 18 of Penn Treebank.9 Sentences in WSJ Sections 00 through 24, except Section 20 of Penn Treebank.10 Sentences in WSJ Section 20 of Penn Treebank.251Computational Linguistics Volume 25, Number 2Table 5Performance ofthe supertagger on the WSJ corpus.Data Set Size of Training Size ofTraining Set Test Set(Words) (Words)% CorrectXTAG Parses 8,000 Unigram 3,000 73.4%(Baseline)Trigram 3,000 86.0%Converted 200,000 UnigramPenn Treebank (Baseline) 47,000 75.3%Parses Trigram 47,000 90.9%1,000,000 Unigram(Baseline) 47,000 77.2%Trigram 47,000 92.2%Table 6Performance ofthe supertagger on the IBM Manual corpus and ATIS corpus.Corpus Size of Training Set (Words) Training Size of Test Set (Words) % CorrectIBM Manual 14,000 Unigram(Baseline) 1,000 77.8%Trigram 1,000 90.3%ATIS 1,500 Unigram(Baseline) 400 85.7%Trigram 400 93.8%correctly supertagged was used as the training corpus and a set of 1,000 words wasused as a test corpus.
The performance of the supertagger on this corpus is shownin Table 6.
Performance on the ATIS corpus was evaluated using a set of 1,500 wordscorrectly supertagged asthe training corpus and a set of 400 words as a test corpus.The performance of the supertagger on the ATIS corpus is also shown in Table 6.As expected, the performance on the ATIS corpus is higher than that of the WSJand the IBM Manual corpus despite the extremely small training corpus.
Also, theperformance of the IBM Manual corpus is better than the WSJ corpus when the sizeof the training corpus is taken into account.
The baseline for the ATIS domain isremarkably high due to the repetitive constructions and limited vocabulary in thatdomain.
This is also true for the IBM Manual corpus, although to a lesser extent.The trigram model of supertagging is attractive for limited domains ince it performsquite well with relatively insignificant amounts of training material.
The performanceof the supertagger can be improved in an iterative fashion by using the supertaggerto supertag larger amounts of training material, which can be quickly hand-correctedand used to train a better-performing supertagger.6.5.2 Effect of  Lexical  versus  Contextua l  In format ion .
Lexical information contributesmost to the performance of a POS tagger, since the baseline performance of assigningthe most likely POS for each word produces 91% accuracy (Brill 1993).
Contextualinformation contributes relatively a small amount owards the performance, improv-ing it from 91% to 96-97%, a 5.5% improvement.
In contrast, contextual informationhas greater effect on the performance of the supertagger.
As can be seen, from theabove experiments, the baseline performance of the supertagger is about 77% and theperformance improves to about 92% with the inclusion of contextual information, an252Bangalore and Joshi Supertaggingimprovement of 19.5%.
The relatively low baseline performance for the supertaggeris a direct consequence of the fact that there are many more supertags per word thanthere are POS tags.
Further, since many combinations of supertags are not possible,contextual information has a larger effect on the performance of the supertagger.6.6 Error-driven Transformation-based TaggerIn an error-driven transformation-based (EDTB) tagger (Brill 1993), a set of pattern-action templates that include predicates that test for features of words appearing inthe context of interest are defined.
These templates are then instantiated with the ap-propriate features to obtain transformation rules.
The effectiveness of a transformationrule to correct an error and the relative order of application of the rules are learnedusing a corpus.
The learning procedure takes a gold corpus in which the words havebeen correctly annotated and a training corpus that is derived from the gold corpus byremoving the annotations.
The objective in the learning phase is to learn the optimumordering of rule applications so as to minimize the number of tag mismatches betweenthe training and the reference corpus.6.6.1 Experiments and Results.
A EDTB model has been trained using templatesdefined on a three-word window.
We trained the templates on 200,000 words 11 andtested on 47,000 words 12 of the WSJ corpus.
The model performed at an accuracy of90%.
The EDTB model provides a great deal of flexibility to integrate domain-specificand linguistic information i to the model.
However, amajor drawback of this approachis that the training procedure is extremely slow, which prevented us from training onthe 1,000,000 word corpus.7.
Supertagging before ParsingThe output of the supertagger, an almost parse, has been used in a variety of applica-tions including information retrieval (Chandrasekar nd Srinivas 1997b, 1997c, 1997d)and information extraction (Doran et al 1997), text simplification (Chandrasekar, Do-ran, and Srinivas 1996, Chandrasekar nd Srinivas 1997a), and language modeling(Srinivas 1996) to illustrate that supertags provide an appropriate l vel of lexical de-scription eeded for most applications.The output of the supertagger has also been used as a front end to a lexicalizedgrammar parser.
As mentioned earlier, a lexicalized grammar parser can be conceptu-alized to consist of two stages (Schabes, AbeillG and Joshi 1988).
In the first stage, theparser looks up the lexicon and selects all the supertags associated with each word ofthe sentence to be parsed.
In the second stage, the parser searches the lattice of selectedsupertags in an attempt to combine them using substitution and adjunction operationsso as to yield a derivation that spans the input string.
At the end of the second stage,the parser would not only have parsed the input, but would have associated a smallset of (usually one) supertags with each word.The supertagger can be used as a front end to a lexicalized grammar parser soas to prune the search-space of the parser even before parsing begins.
It should beclear that by reducing the number of supertags that are selected in the first stage, thesearch-space for the second stage can be reduced significantly and hence the parsercan be made more efficient.
Supertag disambiguation techniques, as discussed in the11 WSJ Sections 15 to 18 of the Penn Treebank.12 WSJ Section 20 of the Penn Treebank.253Computational Linguistics Volume 25, Number 2Table 7Performance improvement of3-best supertagger over the 1-best supertagger on the WSJcorpus.Data Set Size of Size of Training % CorrectTest Set Training Set(Words) (Words)ConvertedPenn TreebankParses47,000 200,0001,000,000Trigram 90.9%(Best Supertag)Trigram 95.8%(3-Best Supertags)Trigram 92.2%(Best Supertag)Trigram 97.1%(3-Best Supertags)previous sections, attempt o disambiguate the supertags elected in the first pass,based on lexical preferences and local lexical dependencies, so as to ideally select onesupertag for each word.
Once the supertagger selects the appropriate supertag foreach word, the second stage of the parser is needed only to combine the individualsupertags to arrive at the parse of the input.
Tested on about 1,300 WSJ sentences witheach word in the sentence correctly supertagged, the LTAG parser took approximately4 seconds per sentence to yield a parse (combine the supertags and perform featureunification).
In contrast, the same 1,300 WSJ sentences without he supertag annotationtook nearly 120 seconds per sentence to yield a parse.
Thus the parsing speedup gainedby this integration is a factor of about 30.In the XTAG system, we have integrated the trigram supertagger as a front end toan LTAG parser to pick the appropriate supertag for each word even before parsingbegins.
However, a drawback of this approach is that the parser would fail completelyif any word of the input is incorrectly tagged by the supertagger.
This problem could becircumvented to an extent by extending the supertagger to produce n-best supertagsfor each word.
Although this extension would increase the load on the parser, itwould certainly improve the chances of arriving at a parse for a sentence.
In fact,Table 7 presents the performance of the supertagger that selects, at most, the top threesupertags for each word.
The optimum number of supertags to output to balancethe success rate of the parser against he efficiency of the parser must be determinedempirically.A more serious limitation of this approach is that it fails to parse ill-formed andextragrammatical strings such as those encountered in spoken utterances and unre-stricted texts.
This is due to the fact that the Earley-style LTAG parser attempts tocombine the supertags to construct a parse that spans the entire string.
In cases wherethe supertag sequence for a string cannot be combined into a unified structure, theparser fails completely.
One possible extension to account for ill-formed and extra-grammatical strings is to extend the Earley parser to produce partial parses for thefragments whose supertags can be combined.
An alternate method of computing de-pendency linkages robustly is presented in the next section.8.
Lightweight Dependency AnalyzerSupertagging associates each word with a unique supertag.
To establish the depen-dency links among the words of the sentence, we exploit he dependency requirements254Bangalore and Joshi Supertaggingencoded in the supertags.
Substitution nodes and foot nodes in supertags serve as slotsthat must be filled by the arguments of the anchor of the supertag.
A substitution slotof a supertag is filled by the complements of the anchor while the foot node of asupertag is filled by a word that is being modified by the supertag.
These argumentslots have a polarity value reflecting their orientation with respect o the anchor ofthe supertag.
Also associated with a supertag is a list of internal nodes (includingthe root node) that appear in the supertag.
Using the structural information coupledwith the argument requirements of a supertag, a simple heuristic-based, linear time,deterministic algorithm (which we call a lightweight dependency analyzer (LDA))produces dependency linkages not necessarily spanning the entire sentence.
The LDAcan produce a number of partial inkages, since it is driven primarily by the need tosatisfy local constraints without being driven to construct a single dependency link-age that spans the entire input.
This, in fact, contributes to the robustness of LDA andpromises to be a useful tool for parsing sentence fragments that are rampant in speechutterances, as exemplified by the Switchboard corpus.Tested on section 20 of the Wall Street Journal corpus, which contained 47,333dependency links in the gold standard, the LDA, trained on 200,000 words, produced38,480 dependency links correctly, resulting in a recall score of 82.3%.
Also, a total of41,009 dependency links were produced by the LDA, resulting in a precision score of93.8%.
A detailed evaluation of the LDA is presented in Srinivas (1997b).9.
Applicability of Supertagging to other Lexicalized GrammarsAlthough we have presented supertagging in the context of LTAG, it is applicable toother lexicalized grammar formalisms uch as CCG (Steedrnan 1997), HPSG (Pollardand Sag 1987), and LFG (Kaplan and Bresnan 1983).
We have implemented a broadcoverage CCG grammar (Doran and Srinivas 1994) containing about 80 categoriesbased on the XTAG English grammar.
These categories have been used to tag thesame training and test corpora used in the supertagging experiments discussed in thispaper and a supertagger to disambiguate he CCG categories has been developed.
Weare presently analyzing the performance of the supertagger using the LTAG trees andthe CCG categories.The idea of supertagging can also be applied to a grammar in HPSG formalismindirectl~ by compiling the HPSG grammar into an LTAG grammar (Kasper et al1995).
A more direct approach would be to tag words with feature structures thatrepresent supertags (Kempe 1994).
For LFG, the lexicalized subset of fragments usedin the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags.An approach that is closely related to supertagging is the reductionist approach toparsing that is being carried out under the Constraint Grammar framework (Karlssonet al 1994; Voutilainen 1994; Tapanainen and J/irvinen 1994).
In this framework, eachword is associated with the set of possible functional tags that it may be assignedin the language.
This constitutes the lexicon.
The grammar consists of a set of rulesthat eliminate functional tags for words based on the context of a sentence.
Parsinga sentence in this framework amounts to eliminating as many implausible functionaltags as possible for each word, given the context of the sentence.
The resultant out-put structure might contain significant syntactic ambiguity, which may not have beeneliminated by the rule applications, thus producing almost parses.
Thus, the reduc-tionist approach to parsing is similar to supertagging in that both view parsing astagging with rich descriptions.
However, the key difference is that the tagging is donein a probabilistic setting in the supertagging approach while it is rule based in theconstraint grammar approach.255Computational Linguistics Volume 25, Number 2We are currently developing supertaggers for other languages.
In collaborationwith Anne Abeill~ and Marie-Helene Candito of the University of Paris, using theirFrench TAG grammar, we have developed a supertagger for French.
We are currentlyworking on evaluating the performance of this supertagger.
Also, the annotated cor-pora necessary for training supertaggers for Korean and Chinese are under develop-ment at the University of Pennsylvania.A version of the supertagger t ained on the WSJ corpus is available under GNUPublic License from http: / / www.cis.upenn.edu / ~xtag / swrelease.html.10.
ConclusionsIn this paper, we have presented a novel approach to robust parsing distinguished fromthe previous approaches to robust parsing by integrating the flexibility of linguisticallymotivated lexical descriptions with the robustness of statistical techniques.
By associat-ing rich descriptions (supertags) that impose complex constraints in a local context, wehave been able to use local computational models for effective supertag disambigua-tion.
A trigram supertag disambiguation model, trained on 1,000,000 (word, supertag)pairs of the Wall Street Journal corpus, performs at an accuracy level of 92.2%.
Afterdisambiguation, we have effectively completed the parse of the sentence, creating analmost parse, in that the parser need only combine the selected structures to arrive ata parse for the sentence.
We have presented a lightweight dependency analyzer (LDA)that takes the output of the supertagger and uses the dependency requirements of thesupertags to produce a dependency linkage for a sentence.
This method can also serveto parse sentence fragments in cases where the supertag sequence after disambigua-tion may not combine to form a single structure.
This approach is applicable to alllexicalized grammar parsers.Appendix A: Feature-based Lexicalized Tree Adjoining GrammarFeature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) is a tree-rewriting gram-mar formalism, unlike context-free Grammars and head grammars, which are string-rewriting formalisms.
FB-LTAGs trace their lineage to Tree Adjunct Grammars (TAGs),which were first developed in Joshi, Lev36 and Takahashi (1975) and later extendedto include unification-based feature structures (Vijay-Shanker 1987; Vijay-Shanker andJoshi 1991) and lexicalization (Schabes, AbeillG and Joshi 1988).
For a more recent andcomprehensive r ference, see Joshi and Schabes (1996).The primitive elements of FB-LTAGs are called elementary trees.
Each elemen-tary tree is associated with at least one lexical item on its frontier.
The lexical itemassociated with an elementary tree is called the anchor of that tree.
An elementarytree serves as a complex description of the anchor and provides a domain of localityover which the anchor can specify syntactic and semantic (predicate argument) con-straints.
Elementary trees are of two kinds: (a) Initial Trees and (b) Auxiliary Trees.
Inan FB-LTAG grammar for natural anguage, initial trees are phrase structure trees ofsimple sentences containing no recursion, while recursive structures are representedby auxiliary trees.Examples of initial trees (c~s) and auxiliary trees (fls) are shown in Figure 6.
Nodeson the frontier of initial trees are marked as substitution sites by a "1", while exactlyone node on the frontier of an auxiliary tree, whose label matches the label of the rootof the tree, is marked as a foot node by a ",".
The other nodes on the frontier of anauxiliary tree are marked as substitution sites.Each node of an elementary tree is associated with two feature structures (FS),256Bangalore and Joshi SupertaggingDetl~\]ID \[agr: <I> \[\]\]\['":\[=: :""iIthem,\[\]\[agr: <1>\]DetP$ \[agr: <I> \[\]\] N \[agr: <l:qLnum : sing\]\]IcompanyS~ \[mode : iud~mplode : <4:~,x~p,$ \[agr: <3> \[I  VP \[.gr: <3, \]tmoa, : <4> \ [ \ ]agr : <1> \]ode : <2>JII agr: <'>\[\] /\[mode : <2> ppartJ\[mode: ppart\]Iacquiredo?2 ~3VPr\[\]~:<l>\[ l  \]ode : <2> \[\]Jv\[~,: <1> 1 vP.
\[\]\[mode: <2>j \[mode: ge~~gode : ind 1Inum: sing~\[3rdsing : +\]JIisw, \ [ \ ]agr: <1> \[\] \]ode : <2> \[\]\]V\[agr:<~> 1 VP* \[\]\[mode: <2> 1 \[mode: ppart\]\[mode" geqIbeingB1Figure 6Elementary trees for the sentence: the company is being acquired.B2the top and the bottom.
The bottom FS contains information relating to the subtreerooted at the node, and the top FS contains information relating to the supertree atthat node.
13 Features may get their values from three different sources:?
Morphology of anchor: from the morphological information of the lexicalitems that anchor the tree.?
Structural characteristics: from the structure of the tree itself (for13 Nodes  marked  for subst i tut ion are associated wi th  only the top FS.257Computational Linguistics Volume 25, Number 2(b)Figure 7Substitution and adjunction in LTAG.example, the mode = ind/imp feature on the root node in the c~3 tree inFigure 6).The derivation process: from unification with features from trees thatadjoin or substitute.Elementary trees are combined by substitution and adjunction operations.
Sub-stitution inserts elementary trees at the substitution odes of other elementary trees.Figure 7(a) shows two elementary trees and the tree resulting from the substitutionof one tree into the other.
In this operation, a node marked for substitution in anelementary tree is replaced by another elementary tree whose root label matches thelabel of the node.
The top FS of the resulting node is the result Of unification of thetop features of the two original nodes, while the bottom FS of the resulting node issimply the bottom features of the root node of the substituting tree.In an adjunction operation, an auxiliary tree is inserted into an elementary tree.Figure 7(b) shows an auxiliary tree adjoining into an elementary tree and the resultof the adjunction.
The root and foot nodes of the auxiliary tree must match the nodelabel at which the auxiliary tree adjoins.
The node being adjoined to splits, and its topFS unifies with the top FS of the root node of the auxiliary tree, while its bottom FSunifies with the bottom FS of the foot node of the auxiliary tree.
Figure 7(b) showsan auxiliary tree and an elementary tree, and the tree resulting from an adjunctionoperation.
For a parse to be well-formed, the top and bottom FS at each node shouldbe unified at the end of a parse.The result of combining the elementary trees shown in Figure 6 is the derivedtree, shown in Figure 8(a).
The process of combining the elementary trees to yield aparse of the sentence is represented by the derivation tree, shown in Figure 8(b).
Thenodes of the derivation tree are the tree names that are anchored by the appropriatelexical items.
The combining operation is indicated by the type of the arcs (a brokenline indicates ubstitution and a bold line indicates adjunction) while the address ofthe operation is indicated as part of the node label.
The derivation tree can also beinterpreted as a dependency tree with unlabeled arcs between words of the sentence,as shown in Figure 8(c).A broad-coverage rammar system, XTAG, has been implemented in the LTAGformalism.
In this section, we briefly discuss some aspects related to XTAG for thesake of completeness.
A more detailed report on XTAG can be found in XTAG-Group(1995).
The XTAG system consists of a morphological analyzer, a part-of-speech tag-ger, a wide-coverage LTAG English grammar, a predictive left-to-right Earley-styleparser for LTAG (Schabes 1990), and an X-windows interface for grammar develop-ment (Doran et al 1994).
The input sentence is subjected to morphological analysis258Bangalore and Joshi SupertaggingSr \[agr : <1> \[ 3rdsing : 4/ I "m:  singll\[mode : <2> lnd J\[agr: <1>\] llrPr \[agr: <1> 1\[mode : <2:>\]DeIP \[agr: <1>\] N \[agr: <1>\] V \[agr: <1> \] VP \[ :~, <.3> c~ 1\[mode : <2>J tmode : <4> ge~\[mode : <4>J \[mode : <6> ppartJ\[ i I\[mode : <6>JIacquired(~)~nxlV \[acquired\] acquiredaNXdxN \[company\] ~Vvx \[being\] company being!
I I I~DXD \[the\] fWvx \[is\] the is(b) (c)Figure 8(a) Derived tree, (b) derivation tree, and (c) dependency tree for the sentence: the company isbeing acquired.and is tagged with parts of speech before being sent to the parser.
The parser etrievesthe elementary trees that the words of the sentence anchor and combines them byadjunction and substitution operations to derive a parse of the sentence.
The gram-mar of XTAG has been used to parse sentences from ATIS, IBM Manual and WSJcorpora (TAG-Group 1995).
The resulting XTAG corpus contains entences from thesedomains along with all the derivations for each sentence.
The derivations provide259Computational Linguistics Volume 25, Number 2predicate argument relationships for the parsed sentences.Appendix B: Key Properties of LTAGsIn this section, we define the key properties of LTAGs: lexicalization, Extended Domainof Locality (EDL), and factoring of recursion from the domain of dependency (FRD),and discuss how these properties are realized in natural anguage grammars writtenin LTAGs.
A more detailed discussion about these properties is presented in Joshi(1985, 1987), Kroch and Joshi (1985), Schabes, AbeillG and Joshi (1988), and Joshi andSchabes (1996).DefinitionA grammar is lexicalized if it consists of:?
a finite set of elementary structures (strings, trees, directed acyclicgraphs, etc.
), each structure anchored on a lexical item.?
lexical items, each associated with at least one of the elementarystructures of the grammar?
a finite set of operations combining these structures.This property proves to be linguistically crucial since it establishes a direct linkbetween the lexicon and the syntactic structures defined in the grammar.
In fact, in lex-icalized grammars all we have is the lexicon, which projects the elementary structuresof each lexical item; there is no independent grammar.DefinitionThe Extended Domain of Locality (EDL) property has two parts:..Every elementary structure must contain all and only the arguments ofthe anchor in the same structure.For each lexical item, the grammar must contain an elementary structurefor each syntactic environment the lexical item might appear in.Part (1) of EDL allows the anchor to impose syntactic and semantic onstraints onits arguments directly since they appear in the same elementary structure that it an-chors.
Hence, all elements that appear within one elementary structure are consideredto be local.
This property also defines how large an elementary structure in a grammarcan be.
Figure 9 shows trees for the following example sentences:(1)(2)(3)John seems to like Mary.John hit Mary.Who did John hit?Figure 9(a) shows the elementary tree anchored by seem that is used to derive a raisinganalysis for sentence 1.
Notice that the elements appearing in the tree are only thosethat serve as arguments to the anchor and nothing else.
In particular, the subject NP260Bangalore and Joshi SupertaggingSqSr RI~ SrnV0~ VPV~n NTo~ VP~ v / " NP1V NPI$ 1~ V Vl~mf*I I Iseems hit hit E(a) (b) (c)Figure 9(a) Tree for raising analysis, anchored by seems; (b) transitive tree; (c) object extraction tree forthe verb hit.
(John in sentence 1) does not appear in the elementary tree for seem since it does notserve as an argument for seem.
Figure 9(b) shows the elementary tree anchored by thetransitive verb hit in which both the subject NP and object NP are realized within thesame elementary tree.LTAG is distinguished from other grammar formalisms by possessing part (2) ofthe EDL property.
In LTAGs, there is one elementary tree for every syntactic environ-ment that the anchor may appear in.
Each elementary tree encodes the linear orderof the arguments of the anchor in a particular syntactic environment.
For example, atransitive verb such as hit is associated with both the elementary tree shown in Fig-ure 9(b) for a declarative transitive sentence such as sentence 2, and the elementarytree shown in Figure 9(c) for an object extracted transitive sentence such as sentence 3.Notice that the object noun phrase is realized to the left of the subject noun phrase inthe object extraction tree.As a consequence of the fact that LTAGs possess the part (2) of the EDL property,the derivation structures in LTAGs contain the information of a dependency structure.Another aspect of EDL is that the arguments of the anchor can be filled in any order.This is possible because the elementary structures allocate a slot for each argument ofthe anchor in each syntactic environment that the anchor appears in.There can be many ways of constructing the elementary structures of a grammar soas to possess the EDL property.
However, by requiring that the constructed elementarystructures be "minimal," the third property of LTAGs namely, factoring of recursionfrom the domain of dependencies, follows as a corollary of EDL.Definit ionFactoring of recursion from the domain of dependencies (FRD): Recursion is factoredaway from the domain for the statement of dependencies.In LTAGs, recursive constructs are represented as auxiliary trees.
They combinewith elementary trees by the operation of adjunction.
Elementary trees define thedomain for stating dependencies such as agreement, subcategorization, and filler-gapdependencies.
Auxiliary trees, by adjunction to elementary trees, account for the long-distance behavior of these dependencies.261Computational Linguistics Volume 25, Number 2An additional advantage of a grammar possessing FRD and EDL properties i thatfeature structures in these grammars are extremely simple.
Since the recursion has beenfactored out of the domain of dependency, and since the domain is large enough foragreement, subcategorizafion, and filler-gap dependencies, feature structures in suchsystems do not involve any recursion.
In fact they reduce to typed terms that can becombined by simple term-like unification.AcknowledgmentsThis work was done when the first authorwas at the University of Pennsylvania.
Itwas partially supported by NSF grantNSF-STC SBR 8920230, ARPA grantN00014-94 and ARO grantDAAH04-94-G0426.
We would like to thankSteven Abney, Raman Chandrasekar,Christine Doran, Beth Ann Hockey, MarkLiberman, Mitch Marcus, and MarkSteedman for useful comments anddiscussions which have helped shape thiswork.
We also thank the reviewers for theirinsightful comments and suggestions toimprove an earlier version of this paper.ReferencesAbne~ Steven.
1990.
Rapid incrementalparsing with repair.
In Proceedings ofthe6th New OED Conference: Electronic TextResearch, pages 1-9, University ofWaterloo, Waterloo, Ontario, Canada.Alshawi, Hiyan and David Carter.
1994.Training and scaling preference functionsfor disambiguation.
ComputationalLinguistics, 20(4):635-648.Appelt, D., J. Hobbs, J.
Bear, D. J. Israel, andM.
Tyson.
1993.
FASTUS: A finite-stateprocessor for information extraction fromreal-world text.
In Proceedings oflJCAI-93,Charnbery, France, September.Black, Ezra, Fred Jelinek, John Lafferty,David M. Magerman, Robert Mercer, andSalim Roukos.
1993.
TowardsHistory-based Grammars: Using RicherModels for Probabilistic Parsing.
InProceedings ofthe 31st Ann ual Meeting,pages 31-37, Columbus, OH.
Associationfor Computational Linguistics.Bod, Rens and Ronald Kaplan.
1998.
Aprobabilistic corpus-driven model forlexical-functional analysis.
In Proceedingsof COLING-ACL "98: 36th Annual Meeting ofthe Association for Computational Linguisticsand 17th International Conference onComputational Linguistics, Montreal,Quebec, Canada, August.Brill, Eric.
1993.
Automatic grammarinduction and parsing free text: Atransformation-based approach.
InProceedings ofthe 31st Annual Meeting,Columbus, OH.
Association forComputational Linguistics.Chandrasekar, R., Christine Doran, and B.Srinivas.
1996.
Motivations and methodsfor text simplification.
In Proceedings ofthe16th International Conference onComputational Linguistics (COLING'96),Copenhagen, Denmark, August.Chandrasekar, R. and B. Srinivas.
1997a.Automatic induction of rules for textsimplification.
Knowledge-based Systems,10:183-190.Chandrasekar, R. and B. Srinivas.
1997b.Gleaning information from the web:Using syntax to filter out irrelevantinformation.
In Proceedings ofAAA11997Spring Symposium on NLP on the World WideWeb.Chandrasekar, R. and B. Srinivas.
1997c.Using supertags in document filtering:The effect of increased context oninformation retrieval effectiveness.
InProceedings ofRecent Advances in NLP(RANLP) '97, Tzigov Chark, Bulgaria,September.Chandrasekar, R. and B. Srinivas.
1997d.Using syntactic information i  documentfiltering: A comparative study ofpart-of-speech tagging and supertagging.In Proceedings ofRIAO'97, Montreal,Quebec, Canada, June.Charniak, Eugene.
1997.
Statistical parsingwith a context-free grammar and wordstatistics.
In Proceedings ofthe FourteenthNational Conference on Artificial IntelligenceAAA/, pages 47-66, Menlo Park, CA.Chomsk~ Noam.
1992.
A MinimalistApproach to Linguistic Theory.
MIT WorkingPapers in Linguistics, Occasional Papersin Linguistics, No.
1.Church, Kenneth Ward.
1988.
A stochasticparts program and noun phrase parserfor unrestricted text.
In 2nd AppliedNatural Language Processing Conference,pages 136-143, Austin, TX.Collins, Michael.
1996.
A new statisticalparser based on bigram lexicaldependencies.
In Proceedings ofthe 34thAnnual Meeting, Santa Cruz, CA.Association for ComputationalLinguistics.Doran, Christine, Dania Egedi, Beth Ann262Bangalore and Joshi SupertaggingHockey, B. Srinivas, and Martin Zaidel.1994.
XTAG System--A wide coveragegrammar for English.
In Proceedings ofthe17th International Conference onComputational Linguistics (COLING'94),Kyoto, Japan, August.Doran, Christine, Michael Niv, BreckenridgeBaldwin, Jeffrey Reynar, and B. Srinivas.1997.
Mother of Perl: A Multi-tier patterndescription language.
In Proceedings oftheInternational Workshop on Lexically DrivenInformation Extraction, Frascati, Italy, July.Doran, Christine and B. Srinivas.
1994.
Awide-coverage CCG parser.
In ProceedingsoJ: the 3rd TAG+ Conference, Paris, France.Fujisaki, T., F. Jelinek, J. Cocke, E. Blackand T. Nishino.
1989.
A probabilisticparsing method for sentencedisambiguation.
I  Proceedings ofthe 1stAnnual International Workshop of ParsingTechnologies, Pittsburgh, PA.Gazdar, G., E. Klein, G. Pullum, and I. Sag.1985.
Generalized Phrase Structure Grammar.Harvard University Press, Cambridge,MA.Good, I. J.
1953.
The population frequenceisof species and the estimation ofpopulation parameters.
Biometrika 40 (3and 4), pages 237-264.Grishman, Ralph.
1995.
Where's the syntax?The New York University MUC-6 System.In Proceedings ofthe Sixth MessageUnderstanding Conference, Columbia, MD.Gross, Maurice.
1984.
Lexicon-grammar andthe syntactic analysis of French.
InProceedings ofthe lOth InternationalConference on Computational Linguistics(COLING'84), Stanford, CA.Hobbs, Jerry R., Douglas Appelt, John Bear,David Israel Megumi Kameyama, MarkStickel, and Mabry Tyson.
1997.
FASTUS:A cascaded finite-state transducer forextracting information fromnatural-language text.
In E. Roche and Y.Schabes, editors, Finite State Devices forNatural Language Processing.
MIT Press,Cambridge, MA.Hobbs, Jerry R., Douglas E. Appelt, JohnBear, David Israel Andy Kehler, MegumiKamayama, David Martin, Karen Myers,and Mabry Tyson.
1995.
SRI InternationalFASTUS system MUC-6 test results andanalysis.
In Proceedings ofthe Sixth MessageUnderstanding Conference, Columbia, MD.Jelinek, Fred, John Lafferty, David M.Magerman, Robert Mercer, AdwaitRatnaparkhi, and Salim Roukos.
1994.Decision tree parsing using a hiddenderivation model.
In Proceedings from theARPA Workshop on Human LanguageTechnology Workshop, March.Joshi, Aravind K. 1960.
Computation ofsyntactic structure.
In Advances inDocumentation a d Library Science,volume III, Part 2.
Interscience Publishers,Inc., NY.Joshi, Aravind K. 1985.
Tree adjoininggrammars: How much context sensitivityis required to provide a reasonablestructural description?
In D. Dowty, I.Karttunen, and A. Zwicky, editors, NaturalLanguage Parsing.
Cambridge UniversityPress, Cambridge, U.K., pages 206-250.Joshi, Aravind K. 1987.
An introduction totree adjoining rammars.
In A. ManasterRamer, editor, Mathematics of Language.John Benjamins, Amsterdam.Joshi, Aravind K. 1998.
Role of constrainedcomputational systems in naturallanguage processing.
Artificial Intelligence,103:117-132.Joshi, Aravind K. and Philip Hopely.
1997.A parser from antiquity.
Natural LanguageEngineering, 2(4).Joshi, Aravind K., L. Levy, and M.Takahashi.
1975.
Tree adjunct grammars.Journal of Computer and System Sciences.Joshi, Aravind K. and Yves Schabes, 1996.Tree-adjoining grammars.
In Handbook ofFormal Languages and Automata.Springer-Verlag, Berlin.Joshi, Aravind K. and B. Srinivas.
1994.Disambiguation of super parts of speech(or supertags): Almost parsing.
InProceedings ofthe 15th InternationalConference on Computational Linguistics(COLING'94), Kyoto, Japan, August.Kaplan, Ronald and Joan Bresnan.
1983.Lexical-functional grammar: A formalsystem for grammatical representation.
IJ.
Bresnan, editor, The MentalRepresentation f Grammatical Relations.
MITPress, Cambridge, MA.Karlsson, F., A. Voutilainen, J. Heikkil~i, andA.
Anttila.
1994.
Constraint Grammar: ALanguage-Independent System for ParsingUnrestricted Text.
Mouton de Gruyter,Berlin and NY.Karttunen, L. J-P. Chanod, G. Grefenstette,and A. Schiller.
1997.
Regular expressionsfor language ngineering.
NaturalLanguage Engineering, 2(4).Kasper, Robert, Bernd Kiefer, Klaus Netter,and K. Vijay-Shanker.
1995.
Compilationof HPSG to TAG.
In Proceedings ofthe 33rdAnnual Meeting, Cambridge, MA.Association for ComputationalLinguistics.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on Acoustics,263Computational Linguistics Volume 25, Number 2Speech and SignalProcessing, 35(3):400-401.Kempe, Andre.
1994.
Probabilistic Taggingwith Feature Structures.
In Proceedings ofthe 15th International Conference onComputational Linguistics (COLING'94),Kyoto, Japan, August.Kroch, Anthony S. and Aravind K. Joshi.1985.
The linguistic relevance of treeadjoining grammars.
Technical ReportMS-CIS-85-16, Department of Computerand Information Science, University ofPennsylvania.Kuno, S. 1966.
Harvard predictive analyzer.In David G. Hays, editor, Readings inAutomatic Language Processing.
AmericanElsevier Pub.
Co., NY.Magerman, David M. 1995.
Statisticaldecision-tree models for parsing.
InProceedings ofthe 33rd Annual Meeting.Association for ComputationalLinguistics.Marcus, Mitchell M., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330.Nagao, Makoto.
1994.
Varieties of heuristicsin sentence processing.
In Current Issues inNatural Language Processing: In Honour ofDon Walker.
Giardini with Kluwer.Ney, Herman, Ute Essen, and ReinhardKneser.
1995.
On the estimation of 'small'probabilities by leaving-one-out.
IEEETransactions on Pattern Analysis and MachineIntelligence, 17(2).Niesler, T. R. and P. C. Woodland.
1996.
Avariable-length category-based n-gramlanguage model.
In Proceedings, IEEEICASSP.Pollard, Carl and Ivan A.
Sag.
1987.Information-Based Syntax and Semantics.Vol.
1: Fundamentals.
CSLI.Roche, Emmanuel.
1993.
Analyse syntaxiquetransformationelle du francais partransducteurs etlexique-grammaire.
Ph D.thesis, Universite Paris 7.Schabes, Yves.
1990.
Mathematical ndComputational Aspects of LexicalizedGrammars.
Ph.D. thesis, Computer ScienceDepartment, University of Pennsylvania.Schabes, Yves, Anne AbeillG and AravindK.
Joshi.
1988.
Parsing strategies with'lexicalized' grammars: Application toTree Adjoining Grammars.
In Proceedingsof the 12th International Conference onComputational Linguistics (COLING'88),Budapest, Hungary, August.Schabes, Yves and Aravind K. Joshi.
1991.Parsing with lexicalized tree adjoininggrammar.
In M. Tomita, editor, CurrentIssues in Parsing Technologies.
KluwerAcademic Publishers.Schabes, Y., M. Roth, and R. Osborne.
1993.Parsing the Wall Street Journal with theinside-outside algorithm.
In Proceedings ofthe European ACL.Sleator, Daniel and Davy Temperley.
1991.Parsing English with a Link Grammar.Technical Report CMU-CS-91-196,Department of Computer Sdence,Carnegie Mellon University.Srinivas, B.
1996.
"Almost parsing"technique for language modeling.
InProceedings oflCSLP96 Conference,Philadelphia, PA.Srinivas, B.
1997a.
Complexity of LexicalDescriptions and its Relevance to PartialParsing.
Ph.D. thesis, University ofPennsylvania.Srinivas, B.
1997b.
Performance evaluationof supertagging for partial parsing.
InProceedings ofthe International Workshop onParsing Technologies, September.Srinivas, B., Christine Doran, and SethKulick.
1995.
Heuristics and parseranking.
In Proceedings ofthe 4th AnnualInternational Workshop on ParsingTechnologies, Prague, September.Steedman, Mark.
1987.
Combinatorygrammars and parasitic gaps.
NaturalLanguage and Linguistic Theory, 5:403--439.Steedman, Mark editor.
1997.
The SyntacticInterface.
MIT Press, Cambridge, MA andLondon, England.Tapanainen, Pasi and Timo J~irvinen.
1994.Syntactic analysis of natural anguageusing linguistic rules and corpus-basedpatterns.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics (COLING'94), Kyoto, Japan,August.Vijay-Shanker, K. 1987.
A Study of TreeAdjoining Grammars.
Ph.D. thesis,Department ofComputer and InformationScience, University of Pennsylvania.Vijay-Shanker, K. and Aravind K. Joshi.1991.
Unification based tree adjoininggrammars.
In J. Wedekind, editor,Un~'cation-based Grammars.
MIT Press,Cambridge, MA.Voutilainen, Atro.
1994.
Designing aParsingGrammar.
Publications of the Departmentof General Linguistics, University ofHelsinki.Waltz, D. 1975.
Understanding linedrawings of scenes with shadows.
In P.Winston, editor, Psychology of ComputerVision, MIT Press.Weischedel, Ralph, Richard Schwartz, JeffPalmucci, Marie Meteer, and LanceRamshaw.
1993.
Coping with ambiguityand unknown words through264Bangalore and Joshi Supertaggingprobabilistic models.
ComputationalLinguistics, 19(2):359-382, June.XTAG-Group, The.
1995.
A lexicalized treeadjoining grammar for English.
TechnicalReport IRCS 95-03, University ofPennsylvania.265
