Mixing Weak Learners in Semantic ParsingRodney D. NielsenDept of Computer ScienceUniversity of Colorado, UCB-430Boulder, CO 80309-0430USARodney.Nielsen@Colorado.eduSameer PradhanCenter for Spoken Language ResearchUniversity of ColoradoBoulder, CO 80303USASameer.Pradhan@Colorado.eduAbstractWe apply a novel variant of Random Forests(Breiman, 2001) to the shallow semantic parsingproblem and show extremely promising results.The final system has a semantic role classificationaccuracy of 88.3% using PropBank gold-standardparses.
These results are better than all otherspublished except those of the Support Vector Ma-chine (SVM) approach implemented by Pradhanet al (2003) and Random Forests have numerousadvantages over SVMs including simplicity, fastertraining and classification, easier multi-class classi-fication, and easier problem-specific customization.We also present new features which result in a 1.1%gain in classification accuracy and describe a tech-nique that results in a 97% reduction in the featurespace with no significant degradation in accuracy.1 IntroductionShallow semantic parsing is the process of findingsentence constituents that play a semantic role rela-tive to a target predicate and then labeling those con-stituents according to their respective roles.
Speci-fying an event?s agent, patient, location, time of oc-currence, etc, can be useful for NLP tasks such asinformation extraction (c.f., Surdeanu et al, 2003),dialog understanding, question answering, text sum-marization, and machine translation.
Example 1 de-picts a semantic parse.
(1) [Agent She] [P bought] [Patient the vase][Locative in Egypt]We expand on previous semantic parsing work(Gildea and Jurafsky, 2002; Pradhan et al, 2003;Surdeanu et al, 2003) by presenting a novel algo-rithm worthy of further exploration, describing atechnique to drastically reduce feature space size,and presenting statistically significant new features.The accuracy of the final system is 88.3% on theclassification task using the PropBank (Kingsburyet al, 2002) corpus.
This is just 0.6% off the bestaccuracy reported in the literature.The classification algorithm used here is a vari-ant of Random Forests (RFs) (Breiman, 2001).This was motivated by Breiman?s empirical stud-ies of numerous datasets showing that RFs oftenhave lower generalize error than AdaBoost (Fre-und and Schapire, 1997), are less sensitive to noisein the training data, and learn well from weak in-puts, while taking much less time to train.
RFsare also simpler to understand and implement thanSVMs, leading to, among other things, easier in-terpretation of feature importance and interactions(c.f., Breiman, 2004), easier multi-class classifica-tion (requiring only a single training session versusone for each class), and easier problem-specific cus-tomization (e.g., by introducing prior knowledge).The algorithm described here is considerably differ-ent from those in (Breiman, 2001).
It was signifi-cantly revised to better handle high dimensional cat-egorical inputs and as a result provides much betteraccuracy on the shallow semantic parsing problem.The experiments reported here focus on the clas-sification task ?
given a parsed constituent knownto play a semantic role relative to a given predicate,decide which role is the appropriate one to assignto that constituent.
Gold-standard sentence parsesfor test and training are taken from the PropBankdataset.
We report results on two feature sets fromthe literature and a new feature set described here.In section 2, we describe the data used in the ex-periments.
Section 3 details the classification algo-rithm.
Section 4 presents the experimental resultsand describes each experiment?s feature set.
Sec-tion 5 provides a discussion and thoughts on futurework.2 The DataThe classifiers were trained on data derived fromthe PropBank corpus (Kingsbury et al, 2002).
Thesame observations and features are used as de-scribed by (Pradhan et al, 2003).
They acquiredthe original data from the July 15, 2002 releaseof PropBank, which the University of Pennsylva-nia created by manually labeling the constituentsSNP VPShe boughtNP PPthe vase in EgyptArg0 Predicate Arg1 ArgM-LocFigure 1: Syntactic parse of the sentence in (2)of the Penn TreeBank gold-standard parses (Marcuset al, 1994).
Predicate usages (at present, strictlyverbs) are hand annotated with 22 possible seman-tic roles plus the null role to indicate grammaticalconstituents that are not arguments of the predicate.The argument labels can have different meaningsdepending on their target predicate, but the anno-tation method attempted to assign consistent mean-ings to labels, especially when associated with sim-ilar verbs.
There are seven core roles or arguments,labeled ARG0-5 and ARG9.
ARG0 usually corre-sponds to the semantic agent and ARG1 to the entitymost affected by the action.
In addition to the corearguments, there are 15 adjunctive arguments, suchas ARGM-LOC which identifies locatives.
Thus ourprevious example, ?She bought the vase in Egypt?,would be parsed as shown in example 2.
Figure1 shows the associated syntactic parse without theparts of speech.
(2) [Arg0 She] [P bought] [Arg1 the vase][ArgM-Loc in Egypt]Development tuning is based on PropBank sec-tion 00 and final results are reported for section 23.We trained and tested on the same subset of obser-vations as did Pradhan et al (2003).
They indicatedthat a small number of sentences (less than 1%)were discarded due to manual tagging errors in theoriginal PropBank labeling process, (e.g., an emptyrole tag).
This one percent reduction applies to allsections of the corpus (training, development andtest).
They removed an additional 2% of the train-ing data due to issues involving the named entitytagger splitting corpus tokens into multiple words.However, where these issues occurred in tagging thesection 23 test sentences, they were manually cor-rected.
The size of the dataset is shown in Table 1.3 The Algorithm3.1 Random ForestsBreiman (2001) defines a random forest as ?a clas-sifier consisting of a collection of tree structuredclassifiers {h(x,?k), k=1, ...} where the {?k} areindependently identically distributed random [train-ing] vectors and each tree casts a unit vote forSection # sent # words # preds # argstraining 28 651 50 129development 1.2 28 2.2 5.7test 1.5 33 2.7 7.0Table 1: Number of sentences, words, marked pred-icates, and labeled arguments in thousandsthe most popular class at input x.?
Thus Bagging(Breiman, 1996) is a form of Random Forest, whereeach tree is grown based on the selection, with re-placement, of N random training examples, whereN is the number of total examples in the trainingset.Breiman (2001) describes two new subclasses ofRandom Forests, Forest-RI and Forest-RC.
In each,he combines Bagging, using the CART methodol-ogy to create trees, with random feature selection(Amit and Geman, 1997) at each node in the tree.That is, at each node he selects a different randomsubset of the input features and considers only thesein establishing the decision at that node.The big idea behind Random Forests is that by in-jecting randomness into the individual trees via ran-dom feature selection, the correlation between theirclassification results is minimized.
A lower correla-tion combined with reasonably good classificationaccuracy for individual trees leads to a much higheraccuracy for the composite forest.
In fact, Breimanshows that a theoretical upper bound can be estab-lished for the generalization error in terms of thestrength of the forest, s, and the mean value of theclassification correlation from individual trees, ?
?.The strength, s, is the expected margin over the in-put space, where the margin of an ensemble classi-fier is defined as the difference between the fractionof the ensemble members that vote for the correctclass versus the fraction voting for the most popularalternative class.
See (Breiman, 2001) for a detaileddescription of s and ??
and how they are calculated.The upper bound on the generalization error is givenby the following equation:E?
?
??(1?
s2)s2 (1)Breiman found that Forest-RI and Forest-RCcompare favorably to AdaBoost in general, are farless sensitive to noise in the training data, and canlearn well using weak inputs.3.2 Feature IssuesBefore describing the variant of Random Forests weuse here, it is helpful to discuss a couple of impor-tant issues related to the input features.
In the exper-iments here, the true input features to the algorithmare all categorical.
Breiman?s approach to handlingcategorical inputs is as follows.
He modifies theirselection probability such that they are V -1 times aslikely as a numeric input to be selected for evalu-ation at each node, where V is the number of val-ues the categorical feature can take.
Then when acategorical input is selected he randomly chooses asubset of the category values and converts the inputinto a binary-valued feature whose value is one ifthe training observation?s corresponding input valueis in the chosen subset and zero otherwise.In many machine learning approaches, a categor-ical feature having V different values would be con-verted to V (or V -1) separate binary-valued features(e.g., this is the case with SVMs).
Here, we processthem as categorical features, but conceptually thinkof them as separate binary-valued features.
In anattempt to minimize confusion, we will refer to thecategorical input features simply as inputs or as in-put features, the equivalent set of binary-valued fea-tures as the binary-valued features, and the featuresthat are randomly composed in the tree building pro-cess (via random category value subset selection) ascomposed features.3.3 Algorithm DescriptionTake any tree building algorithm (e.g., C5.0 (Quin-lan, 2002)) and modify it such that instead of exam-ining all of the input features at each node, it con-siders only a random subset of those features.
Con-struct a large number of trees using all of the train-ing data (we build 128 trees in each experiment).
Fi-nally, allow the trees to individually cast unit votesfor each test observation.
The majority vote deter-mines the classification and ties are broken in favorof the class that occurs most frequently in the train-ing set.Our implementation is the most similar to Forest-RI, but has several differences, some significant.These differences involve not using Bagging, theuse of a single forest rather than two competingforests, the assumed size of V?i (the number of rele-vant values for input i), the probability of selectingindividual inputs, how composed features are cre-ated, and the underlying tree building algorithm.
Wedelineate each of these differences in the followingparagraphs.Forest-RI combines random feature selectionwith Bagging.
Surprisingly, we found that, in ourexperiments, the use of Bagging was actually hurt-ing the classification accuracy of the forests and sowe removed this feature from the algorithm.
Thismeans that we use all training observations to con-struct each tree in the forest.
This is somewhatcounter-intuitive given that it should increase cor-relation in the outputs of the trees.
However, thestrength of the forest is based in part on the accu-racy of its trees, which will increase when utilizingmore training data.
We also hypothesize that, giventhe feature sets here, the correlation isn?t affectedsignificantly by the removal of Bagging.
The rea-son for this is the massive number of binary-valuedfeatures in the problem (577,710 in just the baselinefeature set).
Given this fact, using random featureselection alone might result in substantially uncor-related trees.
As seen in equation 1 and shown em-pirically in (Breiman, 2001), the lack of correlationproduced by random feature selection directly im-proves the error bound.Forest-RI involves growing two forests and se-lecting the one most likely to provide the best re-sults.
These two forests are constructed using dif-ferent values for F , the number of random featuresevaluated at each node.
The choice of which forestis more likely to provide the best results is based onestimates using the observations not included in thetraining data (the out-of-bag observations).
Sincewe did not use Bagging, all of our observations areused in the training of each tree and we could nottake this approach.
Additionally, it is not clear thatthis provided better results in (Breiman, 2001) andpreliminary experiments (not reported here) suggestthat it might be more effective to simply find a goodvalue for F .To create composed features, we randomly selecta number of the input?s category values, C, given bythe following equation:C = 1, V?
?
4C = ?1.5 + log2 V?
?, V?
> 4(2)where V?
is the number of category values still po-tentially relevant.
Random category value selec-tion is consistent with Breiman?s work, as noted insection 3.2.
This random selection method shouldact to further reduce the correlation between treesand Breiman notes that it gets around the problemcaused by categorical inputs with large numbers ofvalues.
However, he leaves the number of valueschosen unspecified.
There is also no indication ofwhat to do as the categorical input becomes moresparse near the leaves of the tree (e.g., if the algo-rithm sends every constituent whose head word is ina set ?
down the right branch of the node, what ef-fect does this have on future random value selectionin each branch).
This is the role of V?
in the aboveequation.A value is potentially relevant if it is not knownto have been effectively removed by a previous de-cision.
The decision at a given node typically sendsall of the observations whose input is in the se-lected category value subset down one branch, andthe remaining observations are sent down the other(boolean compositions would result in exceptions).The list of relevant category values for a given in-put is immediately updated when the decision hasobvious consequences (e.g., the values in ?
are re-moved from the list of relevant values used by theleft branch in the previous example and the list forthe right branch is set to ?).
However, a decisionbased on one input can also affect the remaining rel-evant category values of other inputs (e.g., supposethat at the node in our previous example, all prepo-sitional phrase (PP) constituents had the head wordwith and with was a member of ?, then the phrasetype PP would no longer be relevant to decisionsin the left branch, since all associated observationswere sent down the right branch).
Rather than up-date all of these lists at each node (a computation-ally expensive proposition), we only determine theunique category values when there are fewer than1000 observations left on the path, or the number ofobservations has been cut to less than half what itwas the last time unique values were determined.
Inearly experimentation, this reduced the accuracy byabout 0.4% relative to calculating the remaining cat-egory values after each decision.
So when speed isnot important, one should take the former approach.Breiman indicates that, when several of the in-puts are categorical, in order to increase strengthenough to obtain a good accuracy rate the numberof inputs evaluated at each node must be increasedto two-three times ?1 + log2 M?
(where M is thenumber of inputs).
It is not clear whether the inputselection process is with or without replacement.Some of the inputs in the semantic parsing prob-lem have five orders of magnitude more categoryvalues than others.
Given this issue, if the selec-tion is without replacement, it leads to evaluatingfeatures composed from each of our seven baselineinputs (figure 2) at each node.
This would likelyincrease correlation, since those inputs with a verysmall number of category values will almost alwaysbe the most informative near the root of the tree andwould be consistently used for the upper most deci-sions in the tree.
On the other hand, if selection iswith replacement, then using the Forest-RI methodfor calculating the input selection probability willresult in those inputs with few category values al-most never being chosen.
For example, the baselinefeature set has 577710 equivalent binary-valued fea-tures by the Forest-RI definition, including two truebinary inputs.
The probability of one of these twoinputs not being chosen in a given random draw ac-cording to the Forest-RI method is 577709/577710(see section 3.2 above).
With M=7 inputs, generat-ing 3?1 + log2 M?
= 9 random composed featuresresults in these two binary inputs having a selectionprobability of 1?
(577709/577710)9, or 0.000016.Our compromise is first to use C and V?
fromequation 2 to calculate a baseline number of com-posable features for each input i.
This quantity isthe total number of potentially relevant category val-ues divided by the number used to create a com-posed feature:fi =V?iCi(3)Second, given the large number of composable fea-tures fi, we also evaluate a larger number, F , ofrandom features at each node in the tree:F = max(?
?f?,min(f, ?1.5 + 3 log2(f)?))
(4)where f is the sum of fi over all inputs.
Finally,selection and feature composition is done with re-placement.
The final feature selection process has atleast two significant effects we find positive.
First,the number of composable features reflects the factthat several category values are considered simul-taneously, effectively splitting on Ci binary-valuedfeatures.
This has the effect of reducing the selec-tion probability of many-valued inputs and increas-ing the probability of selecting inputs with fewercategory values.
Using the baseline feature set asan example, the probability of evaluating one of thebinary-valued inputs at the root of the tree increasesfrom 0.000016 to 0.0058.
Second, as category val-ues are used they are periodically removed from theset under consideration, reducing the correspond-ing size of Vi, and the input selection probabilitiesare then adjusted accordingly.
This has the effectof continuously raising the selection probability forthose inputs that have not yet been utilized.Finally, we use ID3 to grow trees rather thanCART, which is the tree algorithm Forest-RI uses.We don?t believe this should have any significanteffect on the final results.
The choice was purelybased on already having an implementation of ID3.From a set of possible split decisions, ID3 choosesthe decision which leads to the minimum weightedaverage entropy among the training observations as-signed to each branch, as determined by class labels(Quinlan, 1986; Mitchell, 1997).These algorithm enhancements are appropriatefor any task with high dimensional categorical in-puts, which includes many NLP applications.PREDICATE: the lemma of the predicate whosearguments are to be classified ?
the infinitive formof marked verbs in the corpusCONSTITUENT PHRASE TYPE: the syntactic typeassigned to the constituent/argument being classi-fiedHEAD WORD (HW): the head word of the targetconstituentPARSE TREE PATH (PATH): the sequence of parsetree constituent labels from the argument to itspredicatePOSITION: a binary value indicating whether thetarget argument precedes or follows its predicateVOICE: a binary value indicating whether thepredicate was used in an active or passive phraseSUB-CATEGORIZATION: the parse tree expansionof the predicate?s grandparent constituentFigure 2: Baseline feature set of experiment 1, see(Gildea and Jurafsky, 2002) for details4 The ExperimentsFour experiments are reported: the first uses thebaseline features of Gildea and Jurafsky (2002); thesecond is composed of features proposed by Prad-han et al (2003) and Surdeanu et al (2003); thethird experiment evaluates a new feature set; and thefinal experiment addresses a method of reducing thefeature space.
The experiments all focus strictly onthe classification task ?
given a syntactic constituentknown to be an argument of a given predicate, de-cide which argument role is the appropriate one toassign to the constituent.4.1 Experiment 1: Baseline Feature SetThe first experiment compares the random for-est classifier to three other classifiers, a statisti-cal Bayesian approach with backoff (Gildea andPalmer, 2002), a decision tree classifier (Surdeanuet al, 2003), and a Support Vector Machine (SVM)(Pradhan et al, 2003).
The baseline feature set uti-lized in this experiment is described in Figure 2 (see(Gildea and Jurafsky, 2002) for details).Surdeanu et al omit theSUB-CATEGORIZATION feature, but add abinary-valued feature that indicates the governingcategory of noun-phrase argument constituents.This feature takes on the value S or VP dependingon which constituent type (sentence or verb phaserespectively) eventually dominates the argument inthe parse tree.
This generally indicates grammaticalsubjects versus objects, respectively.
They alsoused the predicate with its case and morphologyintact, in addition to using its lemma.
Surdeanuet al indicate that, due to memory limitations onClassifier AccuracyBayesian (Gildea and Palmer, 2002) 82.8Decision Tree (Surdeanu et al, 2003) 78.8SVM (Pradhan et al, 2003) 87.1First Tree 78.3Random Forest 84.6Table 2: Results of baseline feature set experimenttheir hardware, they trained on only 75 KB of thePropBank argument constituents ?
about 60% ofthe annotated data.Table 2 shows the results of experiment 1, com-paring the classifier accuracies as trained on thebaseline feature set.
Using a difference of two pro-portions test as described in (Dietterich, 1998), theaccuracy differences are all statistically significantat p=0.01.
The Random Forest approach outper-forms the Bayesian method and the Decision Treemethod.
However, it does not perform as well as theSVM classifier.
Interestingly, the classification ac-curacy of the first tree in the Random Forest, givenin row four, is almost as high as that of the C5 deci-sion trees (Quinlan, 2002) of Surdeanu et al4.2 Experiment 2: Extended Feature SetThe second experiment compares the random for-est classifier to the boosted decision tree and theSVM using all of the features reported by Pradhanet al The additional features used in this experi-ment are listed in Figure 3 (see sources for furtherdetails).
In addition to the extra features noted in theprevious experiment, Surdeanu et al report on fourmore features, not included here (content word partof speech (CW PoS)1, CW named entity class, andtwo phrasal verb collocation features).Table 3 shows the results of experiment 2, com-paring the classifier accuracies using the full featuresets reported in each source.
Surdeanu et al also ap-plied boosting in this experiment and chose the out-come of the boosting iteration that performed best.Using the difference of two proportions test, the ac-curacy differences are all statistically significant atp=0.01.
The Random Forest approach outperformsthe Boosted Decision Tree method by 3.5%, buttrails the SVM classifier by 2.3%.
In analyzing theperformance on individual argument classes usingMcNemar?s test, Random Forest performs signifi-cantly better on ARG0 (p=0.001) then the SVM, andthe SVM has significantly better results on ARG1(p=0.001).
The large number of degrees of freedom1We also tested the CW PoS, but it did not improve the de-velopment results and was omitted.NAMED ENTITIES: seven binary-valued fea-tures indicating whether specific named enti-ties (PERSON, ORGANIZATION, DATE, TIME,MONEY, LOCATION, and PERCENT) occurredanywhere in the target constituent (Surdeanu et al,2003)HW POS: the grammatical part of speech of thetarget constituent?s head word (Surdeanu et al,2003)CONTENT WORD (CW): ?lexicalized feature thatselects an informative word from the constituent,different from the head word?
(Surdeanu et al,2003)VERB CLUSTER: a generalization of the verbpredicate by clustering verbs into 64 classes(Pradhan et al, 2003)HALF PATH: the sequence of parse tree con-stituent labels from the argument to the lowestcommon ancestor of the predicate (Pradhan et al,2003)Figure 3: Additional features in experiment 2Classifier AccuracyBoosted Decision Tree (Surdeanu et al,2003)83.7Random Forest (trained with CW) 87.2SVM (Pradhan et al, 2003) 88.9Random Forest (trained without CW) 86.6Table 3: Results of experiment 2prevent significance at p=0.1 for any other argu-ments, but the SVM appears to perform much betteron ARG2 and ARG3.4.3 Experiment 3: New FeaturesWe evaluated several new features and report on themost significant here, as described in figure 4.2 Theresults are reported in table 4.
The accuracy im-provements relative to the results from experiment2 are all statistically significant at p=0.001 (McNe-mar?s test is used for all significance tests in this sec-tion).
Comparing the SVM results in experiment 2to the best results here shows statistical significance2Due to space, we cannot report all experiments; contact thefirst author for more information.
The other features we eval-uated involved: the phrase type of the parent constituent, thelist of phrase types encompassing the sentence fragment be-tween the target predicate and constituent, the prefix and suffixof the cw and hw, animacy, high frequency words precedingand following the predicate, and the morphological form of thepredicate.
All of these improved accuracy on the developmentset (some with statistical significance at p=0.01), but we sus-pect the development baseline was at a low point, since thesefeatures largely did not improve performance when combinedwith CW Base and GP.GOVERNING PREPOSITION (GP): if the con-stituent?s parent is a PP, this is the associatedpreposition (e.g., in ?made of [Arg2 gallium ar-senide]?, this feature is ?of?, since the Arg2-NP isgoverned by an ?of?-based PP)CW BASE: starting with the CW, convert it to itssingular form, remove any prefix, and convert dig-its to ?n?
(e.g., this results in the following CW ?CW Base mappings: accidents ?
accident, non-binding ?
binding, repayments ?
payment, and1012 ?
nnnn)Figure 4: Features in experiment 3Feature Set AccuracyExtended (see figures 2 & 3) 86.6Extended + CW BASE 87.4Extended + GOVERNING PREPOSITION 87.4Extended + CW BASE & GP 88.3Table 4: Results of experiment 2only at p=0.1.In analyzing the effect on individual argumentclasses, seven have high ?2 values (ARG2-4,ARGM-DIS (discourse), ARGM-LOC (locative),ARGM-MNR (manner), and ARGM-TMP (temporal)),but given the large number of degrees of free-dom, only ARGM-TMP is significant (p=0.05).
Ex-ample section-00 sentence fragments including thetarget predicate (P) and ARG2 role whose classi-fication was corrected by the GP feature include?
[P banned] to [everyday visitors]?, ?
[P consid-ered] as [an additional risk for the investor]?, and?
[P made] of [gallium arsenide]?.
Comparing theSVM results to the best results here, the Ran-dom Forest performs significantly better on Arg0(p=0.001), and the SVM is significantly better onArg1 (p=0.001).
Again the degrees of freedom pre-vent significance at p=0.1, but the Random Forestoutperforms the SVM with a fairly high ?2 value onARG4, ARGM-DIS, ARGM-LOC, and ARGM-TMP.4.4 Experiment 4: Dimensionality ReductionWe originally assumed we would be using binary-valued features with sparse matrices, much like inthe SVM approach.
Since many of the features havea very large number of values (e.g., the PATH fea-ture has over 540k values), we sought ways to re-duce the number of equivalent binary-valued fea-tures.
This section reports on one of these meth-ods, which should be of interest to others in resourceconstrained environments.In this experiment, we preprocess the baseline in-puts described in Figure 2 to reduce their numberof category values.
Specifically, for each originalcategory value, vi ?
V , we determine whether itoccurs in observations associated with one or morethan one semantic role label, R. If it is associatedwith more than one R, vi is left as is.
When vi mapsto only a single Rj , we replace vi with an arbitraryvalue, vk /?
V , which is the same for all such v oc-curring strictly in association with Rj .
The PATHinput starts with 540732 original feature values andhas only 1904 values after this process, while HEADWORD is reduced from 33977 values to 13208 andPHRASE TYPE is reduced from 62 to 44 values.The process has no effect on the other baseline inputfeatures.
The total reduction in equivalent binary-valued features is 97%.
We also test the effect ofdisregarding feature values during training if theyonly occur once in the training data.
This has amore modest effect, reducing PATH to 156788 val-ues and HEAD WORD to 29482 values, with no otherreductions.
The total reduction in equivalent binary-valued features is 67%.Training on the baseline feature set, the net effectof these two procedures was less than a 0.3% lossof accuracy on the development set.
The McNemartest indicates this is not significant at p=0.1.
In theend, our implementation used categorical features,rather than binary-valued features (e.g., rather thanuse 577710 binary-valued features to represent thebaseline inputs, we use 7 features which might takeon a large number of values ?
PATH has 540732 val-ues).
In this case, the method does not result in assignificant a reduction in the memory requirements.While we did not use this feature reduction in anyof the experiments reported previously, we see it asbeing very beneficial to others whose implementa-tion may be more resource constrained, particularlythose using a binary-valued feature representation.The method also reduced training time by 17%and should lead to much larger reductions for im-plementations using binary-valued features.
For ex-ample, the worst case training time for SVMs isquadratic in the number of features and this methodreduced the dimensionality to 3% of its originalsize.
Therefore, the method has the theoreticalpotential to reduce training time by up to 100(1-0.032) = 99.91%.
While it is unlikely to ap-proach this in practice, it should provide signifi-cant savings.
This may be especially helpful duringmodel selection or feature evaluation, after which,one could revert to the full dimensionality for fi-nal training to improve classification accuracy.
Theslight decrement in accuracy may also be overcomeby the ability to handle larger datasets.5 Discussion and Future ResearchThe version of Random Forests described here out-performs the Bayesian algorithm (Gildea and Juraf-sky, 2002; Gildea and Palmer, 2002) by 1.8% on thesame feature set and outperforms the boosted deci-sion tree classifier (Surdeanu et al, 2003) by 3.5%on the extended feature set with 5 fewer features.The SVM classifier (Pradhan et al, 2003) was 2.3%better training on the same data, but only 0.6% bet-ter than our best results.The Random Forest (RF) approach has advan-tages that might make it a better choice than anSVM in certain circumstances.
Conceptually, itis simpler to understand and can be implementedmore easily.
This also makes it easier to modifythe algorithm to evaluate new techniques.
RFs al-low one to more easily implement multi-class clas-sifiers.
The RFs here were implemented as a singleclassifier, rather than as the 22 one-against-all clas-sifiers required by the SVM approach.
Since RFsare not overly sensitive to noise in the training data(Breiman, 2001), it might be the case that they willnarrow the performance gap when training is basedon automatically parsed sentences.
Further researchis required in this area.
Additionally, RFs have anadvantage in training time.
It takes about 40% ofthe SVM time (8 versus 20 hours) to train on theextended feature set for the classification task andwe expect this time to be cut by up to a factor of 10in porting from MatLab to C. Classification time isgenerally faster for RFs as well, which is importantfor real-time tasks.In a class-by-class comparison, using the samefeatures, the RF performed significantly better thanthe SVM on Arg0 roles, the same or slightly betteron 12 of the other 21 arguments, and slightly bet-ter overall on the 14 adjunctive arguments (77.8%versus 77.3% accuracy on 1882 observations).
Re-viewing performance on data not seen during train-ing, both algorithms degraded to about 94% of theiraccuracy on seen data.The RF algorithm should be evaluated on theidentification task and on the combined identifica-tion and classification task.
This will provide addi-tional comparative evidence to contrast it with theSVM approach.
Further research is also required todetermine how RFs generalize to new genres.Another area for future research involves the es-timation of class probabilities.
MOB-ESP, a variantof Random Forests which outputs class probabilityestimates, has been shown to produce very good re-sults (Nielsen, 2004).
Preliminary experiments sug-gest that using these probability estimates in con-junction with an SVM classifier might be more ef-fective than estimating probabilities based on theexample?s distance from the decision surface as in(Platt, 2000).
Class probabilities are useful for sev-eral semantic parsing and more general NLP tasks,such as selective use of labeled examples duringtraining (c.f., Pradhan et al, 2003) and N-best listprocessing.6 ConclusionThe results documented in these experiments arevery promising and mandate further research.
Thefinal classification accuracy of the Random For-est was 88.3%, just 0.6% behind the SVM results(Pradhan et al, 2003) and 4.6% higher than the nextbest results (Surdeanu et al, 2003) ?
results thatwere based on a number of additional features.We defined several modifications to the RF algo-rithm that increased accuracy.
These improvementsare important for any application with high dimen-sional categorical inputs, which includes many NLPtasks.
We introduced new features which provideda 1.1% improvement in accuracy over the best re-sults using features from the literature.
We also in-troduced a technique to reduce the dimensionalityof the feature space, resulting in a reduction to just3% of the original feature space size.
This couldbe an important enabler for handling larger datasetsand improving the efficiency of feature and modelselection.AcknowledgementsWe thank Dan Jurafsky for miscellaneous supportand for valuable feedback on a draft of this paper.Thanks also go to the anonymous reviewers whosefeedback improved the paper.ReferencesYali Amit and Donald Geman.
1997.
Shape Quan-tization and Recognition with Randomized Trees.Neural Computation, 9:1545?1588.Leo Breiman.
2001.
Random Forests.
Journal ofMachine Learning, 45(1):5?32.Leo Breiman.
2004.
Random Forests.
http://stat-www.berkeley.edu/users/breiman/RandomForests/Leo Breiman.
1996.
Bagging Predictors.
MachineLearning, 26(2):123?140.Thomas G. Dietterich.
1998.
Approximate statis-tical tests for comparing supervised classifica-tion learning algorithms.
Neural Computation,10(7):1895?1924.Y.
Freund and R. E. Schapire.
1997.
A decision-theoretic generalization of on-line learning andan application to boosting.
Journal of Computerand Systems Sciences, 55(1):119?139.Daniel Gildea and Daniel Jurafsky.
2002.
Auto-matic Labeling of Semantic Roles.
Computa-tional Linguistics, 28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The Ne-cessity of Parsing for Predicate Argument Recog-nition.
Proceedings of ACL-02.Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the PennTreebank.
Proceedings of the HLT-02.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn TreeBank: Annotatingpredicate argument structure.Tom M. Mitchell.
1997.
Machine Learning.McGraw-Hill, Boston, MA.Rodney D. Nielsen.
2004.
MOB-ESP and other Im-provements in Probability Estimation.
Proceed-ings of the 20th Conference on Uncertainty in Ar-tificial Intelligence.John Platt.
2000.
Probabilities for Support VectorMachines.
In A. Smola, P. Bartlett, B. Scolkopf,and D. Schuurmans (Eds), Advances in LargeMargin Classifiers.
MIT Press, Cambridge, MA.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, Daniel Jurafsky.2003.
Shallow Semantic Parsing using SupportVector Machines.
University of Colorado Tech-nical Report: TR-CSLR-2003-03.J.
R. Quinlan.
1986.
Induction of decision trees.Machine Learning, 1:81?106.J.
R. Quinlan.
2002.
Data Mining Tools See5 andC5.0.
http://www.rulequest.com/see5-info.html.Mihai Surdeanu, Sanda Harabagiu, John Williamsand Paul Aarseth.
2003.
Using Predicate-Argument Structures for Information Extraction.Proceedings of ACL-03.
