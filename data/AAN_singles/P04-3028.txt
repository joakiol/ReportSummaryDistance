Co-training for Predicting Emotions with Spoken Dialogue DataBeatriz Maeireizo and Diane Litman and Rebecca HwaDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260, U.S.A.beamt@cs.pitt.edu, litman@cs.pitt.edu, hwa@cs.pitt.eduAbstractNatural Language Processing applicationsoften require large amounts of annotatedtraining data, which are expensive to obtain.In this paper we investigate the applicability ofCo-training to train classifiers that predictemotions in spoken dialogues.
In order to doso, we have first applied the wrapper approachwith Forward Selection and Na?ve Bayes, toreduce the dimensionality of our feature set.Our results show that Co-training can behighly effective when a good set of featuresare chosen.1 IntroductionIn this paper we investigate the automaticlabeling of spoken dialogue data, in order to train aclassifier that predicts students?
emotional states ina human-human speech-based tutoring corpus.Supervised training of classifiers requiresannotated data, which demands costly efforts fromhuman annotators.
One approach to minimize thiseffort is to use Co-training (Blum and Mitchell,1998), a semi-supervised algorithm in which twolearners are iteratively combining their outputs toincrease the training set used to re-train each otherand generate more labeled data automatically.
Themain focus of this paper is to explore how Co-training can be applied to annotate spokendialogues.
A major challenge to address is inreducing the dimensionality of the many featuresavailable to the learners.The motivation for our research arises from theneed to annotate a human-human speech corpus forthe ITSPOKE (Intelligent Tutoring SPOKEndialogue System) project (Litman and Silliman,2004).
Ongoing research in ITSPOKE aims torecognize emotional states of students in order tobuild a spoken dialogue tutoring system thatautomatically predicts and adapts to the student?semotions.
ITSPOKE uses supervised learning topredict emotions with spoken dialogue data.
Al-though a large set of dialogues have beencollected, only 8% of them have been annotated(10 dialogues with a total of 350 utterances), due tothe laborious annotation process.
We believe thatincreasing the size of the training set with moreannotated examples will increase the accuracy ofthe system?s predictions.
Therefore, we arelooking for a less labour-intensive approach to dataannotation.2 DataOur data consists of the student turns in a set of10 spoken dialogues randomly selected from acorpus of 128 qualitative physics tutoringdialogues between a human tutor and University ofPittsburgh undergraduates.
Prior to our study, the453 student turns in these 10 dialogues weremanually labeled by two annotators as either"Emotional" or "Non-Emotional" (Litman andForbes-Riley, 2004).
Perceived student emotions(e.g.
confidence, confusion, boredom, irritation,etc.)
were coded based on both what the studentsaid and how he or she said it.
For this study, weuse only the 350 turns where both annotatorsagreed on the emotion label.
51.71% of these turnswere labeled as Non-Emotional and the rest asEmotional.Also prior to our study, each annotated turn wasrepresented as a vector of 449 featureshypothesized to be relevant for emotion prediction(Forbes-Riley and Litman, 2004).
The featuresrepresent acoustic-prosodic (pitch, amplitude,temporal), lexical, and other linguisticcharacteristics of both the turn and its local andglobal dialogue context.3 Machine Learning TechniquesIn this section, we will briefly describe the ma-chine learning techniques used by our system.3.1 Co-trainingTo address the challenge of training classifierswhen only a small set of labeled examples isavailable, Blum and Mitchell (1998) proposed Co-training as a way to bootstrap classifiers from alarge set of unlabeled data.
Under this framework,two (or more) learners are trained iteratively intandem.
In each iteration, the learners classifymore unlabeled data to increase the training datafor each other.
In theory, the learners must havedistinct views of the data (i.e., their features areconditionally independent given the labelexample), but some studies suggest that Co-training can still be helpful even when theindependence assumption does not hold (Goldman,2000).To apply Co-training to our task, we developtwo high-precision learners: Emotional and Non-Emotional.
The learners use different featuresbecause each is maximizing the precision of itslabel (possibly with low recall).
While we havenot proved these two learners are conditionallyindependent, this division of expertise ensures thatthe learners are different.
The algorithm for ourCo-training system is shown in Figure 1.
Eachlearner selects the examples whose predictedlabeled corresponds to its expertise class with thehighest confidence.
The maximum number ofiterations and the number of examples added periteration are parameters of the system.While iteration < MAXITERATIONEmo_Learner.Train(train)NE_Learner.Train(train)emo_Predictions = Emo_Learner.Predict(predict)ne_Predictions = NE_Learner.Predict(predict)emo_sorted_Predictions = Sort_by_confidence(emo_Predictions)ne_sorted_Predictions = Sort_by_confidence(ne_Predictions)best_emo = Emo_Learner.select_best(emo_sorted_Predictions,NUM_SAMPLES_TO_ADD)best_ne = NE_Learner.select_best(ne_sorted_Predictions,NUM_SAMPLES_TO_ADD)train = train ?
best_emo ?
best_nepredict = predict ?
best_emo ?
best_neendFigure 1.
Algorithm for Co-training System3.2 Wrapper Approach with ForwardSelectionAs described in Section 2, 449 features havebeen currently extracted from each utterance of theITSPOKE corpus (where an utterance is astudent?s turn in a dialogue).
Unfortunately, highdimensionality, i.e.
large amount of input features,may lead to a large variance of estimates, noise,overfitting, and in general, higher complexity andinefficiencies in the learners.
Different approacheshave been proposed to address this problem.
Inthis work, we have used the Wrapper Approachwith Forward Selection.The Wrapper Approach, introduced by John etal.
(1994) and refined later by Kohavi and John(1997), is a method that searches for a good subsetof relevant features using an induction algorithm aspart of the evaluation function.
We can applydifferent search algorithms to find this set offeatures.Forward Selection is a greedy search algorithmthat begins with an empty set of features, andgreedily adds features to the set.
Figure 2 showsour algorithm implemented for the forwardwrapper approach.bestFeatures = []while dim(bestFeatures) < MINFEATURESfor iterations = 1: MAXITERATIONSsplit train into training/developmentparameters = computeParameters(training)for feature = 1:MAXFEATURESevaluate(parameters,development,[bestFeatures + feature])keep validation performanceendendaverage_performance and keep average_performanceendB = best average_performancebestFeatures  B ?
bestFeaturesendFigure 2.
Implemented algorithm for forwardwrapper approach.
The variables underlined arethe ones whose parameters we have changed inorder to test and improve the performance.We can use different criteria to select the featureto add, depending on the object of optimization.Earlier, we have explained the basis of the Co-training system.
When developing an expertlearner in one class, we want it to be correct mostof the time when it guesses that class.
That is, wewant the classifier to have high precision (possiblyat the cost of lower overall accuracy).
Therefore,we are interested in finding the best set of featuresfor precision in each class.
In this case, we arefocusing on Emotional and Non-Emotionalclassifiers.Figure 3 shows the formulas used for theoptimization criterion on each class.
For theEmotional Class, our optimization criterion was tomaximize the PPV (Positive Predictive Value), andfor the Non-Emotional Class our optimizationcriterion was to maximize the NPV (NegativePredictive Value).Figure 3.
Confusion Matrix, Positive PredictiveValue (Precision for Emotional) and NegativePredictive Value (Precision for Non-Emotional)4 ExperimentsFor the following experiments, we fixed the sizeof our training set to 175 examples (50%), and thesize of our test set to 140 examples (40%).
Theremaining 10% has been saved for laterexperiments.4.1 Selecting the featuresThe first task was to reduce the dimensionalityand find the best set of features for maximizing thePPV for Emotional class and NPV for Non-Emotional class.
We applied the WrapperApproach with Forward Selection as described insection 3.2, using Na?ve Bayes to evaluate eachsubset of features.We have used 175 examples for the training set(used to select the best features) and 140 for thetest set (used to measure the performance).
Thetraining set is randomly divided into two sets ineach iteration of the algorithm: One for trainingand the other for development (65% and 35%respectively).
We train the learners with thetraining set and we evaluate the performance topick the best feature with the development set.Number ofFeaturesNa?veBayesAdaBoost-j48Decision TreesAll Features 74.5 % 83.1 %3 best for PPV 92.9 % 92.9 %Table 1.
Precision of Emotional with all featuresand 3 best features for PPV using Na?ve Bayes(used for Feature Selection) and AdaBoost-j48Decision Trees (used for Co-training)The selected features that gave the best PPV forEmotional Class are 2 lexical features and oneacoustic-prosodic feature.
By using them weincreased the precision of Na?ve Bayes from 74.5%(using all 449 features) to 92.9%, and ofAdaBoost-j48 Decision Trees from 83.1% to92.9% (see Table 1).Number ofFeaturesNa?veBayesAdaBoost-j48Decision TreesAll Features 74.2  % 90.7 %1 best for NPV 100.0  % 100.0 %Table 2.
Precision of Non-Emotional with allfeatures and best feature for NPV using Na?veBayes  (used for Feature Selection) and AdaBoost-j48 Decision Trees (used for Co-training)For the Non-Emotional Class, we increased theNPV of Na?ve Bayes from 74.2% (with allfeatures) to 100% just by using one lexical feature,and the NPV of AdaBoost-j48 Decision Trees from90.7% to 100%.
This precision remained the samewith the set of 3 best features, one lexical and twonon-acoustic prosodic features (see Table 2).These two set of features for each learner aredisjoint.4.2 Co-training experimentsThe two learners are initialized with only 6labeled examples in the training set.
The Co-training system added examples from the 140?pseudo-labeled?
examples1 in the Prediction Set.The size of the training set increased in eachiteration by adding the 2 best examples (those withthe highest confidence scores) labeled by the twolearners.
The Emotional learner and the Non-Emotional learner were set to work with the set offeatures selected by the wrapper approach tooptimize the precision (PPV and NPV) asdescribed in section 4.1.We have applied Weka?s (Witten and Frank,2000) AdaBoost?s version of j48 decision trees (asused in Forbes-Riley and Litman, 2004) to the 140unseen examples of the test set for generating thelearning curve shown in figure 4.Figure 4 illustrates the learning curve of theaccuracy on the test set, taking the union of the setof features selected to label the examples.
Weused the 3 best features for PPV for the EmotionalLearner and the best feature for NPV for the Non-Emotional Learner (see Section 4.1).
The x-axisshows the number of training examples added; they-axis shows the accuracy of the classifier on testinstances.
We compare the learning curve fromCo-training with a baseline of majority class andan upper-bound, in which the classifiers are trainedon human-annotated data.
Post-hoc analysesreveal that four incorrectly labeled examples wereadded to the training set: example numbers 21, 22,45, and 51 (see the x-axis).
Shortly after theinclusion of example 21, the Co-training learningcurve diverges from the upper-bound.
All of themcorrespond to Non-Emotional examples that werelabeled as Emotional by the Emotional learner withthe highest confidence.The Co-training system stopped after adding 58examples to the initial 6 in the training set becausethe remaining data cannot be labeled by thelearners with high precision.
However, as we cansee, the training set generated by the Co-trainingtechnique can perform almost as well as the upper-bound, even if incorrectly labeled examples areincluded in the training set.1This means that although the example has beenlabeled, the label remains unseen to the learners.Learning Curve - Accuracy (features for Emotional/Non-Emotional Precision)00.10.20.30.40.50.60.70.80.911 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 103 109 115 121 127 133 139 145 151 157 163 169 175Majority Class Cotrain Upper-boundFigure 4.
Learning Curve of Accuracy using best features for Precision of Emotional/Non-Emotional5 ConclusionWe have shown Co-training to be a promisingapproach for predicting emotions with spokendialogue data.
We have given an algorithm thatincreased the size of the training set producingeven better accuracy than the manually labeledtraining set, until it fell behind due to its inabilityto add more than 58 examples.We have shown the positive effect of selectinga good set of features optimizing precision foreach learner and we have shown that the featurescan be identified with the Wrapper Approach.In the future, we will verify the generalizationof our results to other partitions of our data.
Wewill also try to address the limitation of noise inour Co-training System, and generalize oursolution to a corresponding corpus of human-computer data (Litman and Forbes-Riley, 2004).We will also conduct experiments comparing Co-training with other semi-supervised approachessuch as self-training and Active learning.6 AcknowledgementsThanks to R. Pelikan, T. Singliar and M.Hauskrecht for their contribution with FeatureSelection, and to the NLP group at University ofPittsburgh for their helpful comments.
Thisresearch is partially supported by NSF Grant No.0328431.ReferencesA.
Blum and T. Mitchell.
1998.
CombiningLabeled and Unlabeled Data with Co-training.Proceedings of the 11th Annual Conference onComputational Learning Theory: 92-100.K.
Forbes-Riley and D. Litman.
2004.
PredictingEmotion in Spoken Dialogue from MultipleKnowledge Sources.
Proceedings of HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (HLT/NAACL).S.
Goldman and Y. Zhou.
2000.
EnhancingSupervised Learning with Unlabeled Data.International Joint Conference on MachineLearning, 2000.G.
H. John, R. Kohavi and K. Pleger.
1994.Irrelevant Features and the Subset SelectionProblem.
Machine Learning: Proceedings of11th International Conference:121-129, MorganKaufmann Publishers, San Francisco, CA.R.
Kohavi and G. H. John.
1997.
Wrappers forFeature Subset Selection.
ArtificialIntelligence, Volume 97, Issue 1-2.D.
J. Litman and K. Forbes-Riley, 2004.Annotating Student Emotional States in SpokenTutoring Dialogues.
Proc.
5th Special InterestGroup on Discourse and Dialogue Workshopon Discourse and Dialogue (SIGdial).D.
J. Litman and S. Silliman, 2004.
ITSPOKE: AnIntelligent Tutoring Spoken Dialogue System.Companion Proceedings of Human LanguageTechnology conf.
of the North AmericanChapter of the Association for ComputationalLinguistics (HLT/NAACL).I.
H. Witten and E. Frank.
2000.
Data Mining:Practical Machine Learning Tools andTechniques with Java implementations.
MorganKaufmann, San Francisco.
