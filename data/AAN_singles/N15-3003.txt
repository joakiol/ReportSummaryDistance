Proceedings of NAACL-HLT 2015, pages 11?15,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational Linguisticshyp: A Toolkit for Representing, Manipulating, and Optimizing HypergraphsMarkus Dreyer?SDL Research6060 Center Drive Suite 150Los Angeles, CA 90045markus.dreyer@gmail.comJonathan GraehlSDL Research6060 Center Drive Suite 150Los Angeles, CA 90045graehl@sdl.comAbstractWe present hyp, an open-source toolkit forthe representation, manipulation, and opti-mization of weighted directed hypergraphs.hyp provides compose, project, in-vert functionality, k-best path algorithms,the inside and outside algorithms, and more.Finite-state machines are modeled as a spe-cial case of directed hypergraphs.
hyp con-sists of a C++ API, as well as a commandline tool, and is available for download atgithub.com/sdl-research/hyp.1 IntroductionWe present hyp, an open-source toolkit that pro-vides data structures and algorithms to processweighted directed hypergraphs.Such hypergraphs are important in natural lan-guage processing and machine learning, e.g., inparsing (Klein and Manning (2005), Huang andChiang (2005)), machine translation (Kumar et al,2009), as well as in logic (Gallo et al, 1993) andweighted logic programming (Eisner and Filardo,2011).The hyp toolkit enables representing and ma-nipulating weighted directed hypergraphs, pro-viding compose, project, invert functional-ity, k-best path algorithms, the inside and out-side algorithms, and more.
hyp also implementsa framework for estimating hypergraph featureweights by optimization on forests derived fromtraining data.
?Markus Dreyer is now at Amazon, Inc., Seattle, WA.tail1tail2tail3headwFigure 1: An arc leading from three tail states to a head state,with weight w .2 DefinitionsA weighted directed hypergraph (hereinafter hy-pergraph) is a pair H = ?V,E?, where V is a set ofvertices and E a set of edges.
Each edge (also calledhyperedge) is a triple e = ?T(e),h(e), w(e)?, whereT(e) is an ordered list of tails (i.e., source vertices),h(e) is the head (i.e., target vertex) and w(e) is thesemiring weight (see Section 3.4) of the edge (seeFigure 1).We regard hypergraphs as automata and call thevertices states and edges arcs.
We add an optionalstart state S ?V and a final state F ?V.Each state s has an input label i (s) ?
(??
{?
})and output label o(s) ?
(??
{?
}); if o(s) = ?
thenwe treat the state as having o(s) = i (s).
The labelalphabet ?
is divided into disjoint sets of nonter-minal, lexical, and special {?,?,?,?}
labels.
Theinput and output labels are analogous to those ofa finite-state transducer in some hyp operations(Section 3.3).The set of incoming arcs into a state s is calledthe Backward Star of s, or short, BS(s).
Formally,BS(s) = {a ?
E : h(a) = s}.
A path pi is a sequenceof arcs pi = (a1 .
.
.
ak ) ?
E?
such that ?a ?
pi,?t ?T(a), (?a?
?pi : h(a?
)= t )?BS(t )=;.
Each tail state11t of each arc on the path must be the head of somearc on the path, unless t is the start state or hasno incoming arcs and a terminal (lexical or spe-cial) input label, in which case we call t an axiom.The rationale is that each tail state of each arc onthe path must be derived, by traveling an arc thatleads to it, or given as an axiom.
If the hypergraphhas a start state, the first tail of the first arc of anypath must be the start state.
The head of the lastarc must always be the final state, h(ak ) = F. Pathscorrespond to trees, or proofs that the final statemay be reached from axioms.Hypergraph arcs have exactly one head; someauthors permit multiple heads and would call ourhypergraphs B-hypergraphs (Gallo et al, 1993).3 Representing hypergraphsText representation.
hyp uses a simple human-readable text format for hypergraphs.
For exam-ple, see the first two lines in Figure 2.
Each hyper-graph arc has the following format:head <- tail1 tail2 ... tailn / weightHead and tail states are non-negative integersfollowed by an optional label in parentheses (ora pair of (input output) labels).
If it is lexi-cal (i.e., a word), then it is double-quoted with theusual backslash-escapes; nonterminal and specialsymbols are unquoted.
Special symbols like ?, ?,?, ?
are written with brackets, as <eps>, <phi>,<rho>, <sigma>.
Each arc may optionally havea slash followed by a weight, which is typically anegative log probability (i.e., the cost of the arc).
Afinal state n is marked as FINAL <- n. Figure 2shows the text and visual representation of a hy-pergraph with only one arc; it represents and ac-cepts the string he eats rice.Visual representation.
A provided Draw com-mand can render hypergraphs using Graphviz(Gansner and North, 2000).
Small gray numbersindicate the order of arc tails.
Axiom nodes arefilled gray.1 The final state is drawn as a double cir-cle, following finite-state convention.1Gray is used analogously in graphical models for observednodes.0(S) <- 1("he") 2("eats") 3("rice") / 0.693FINAL <- 0(S)S"he" 1"eats" 2"rice"3 0.693Figure 2: The text and visual representation of a hypergraphwith a single arc, similar to Figure 1.
The visual representationleaves out the state IDs of labeled states.0(S) <- 1(NP) 2(VP)1(NP) <- 3(PRON)2(VP) <- 4(V) 5(NP) 6(PP)3(PRON) <- 10("He")4(V) <- 11("eats")5(NP) <- 7(N)6(PP) <- 8(PREP) 9(N)7(N) <- 12("rice")8(PREP) <- 13("with")9(N) <- 14("sticks")FINAL <- 0(S)# These added arcs# make it into a forest:15(NP) <- 7(N) 6(PP)2(VP) <- 4(V) 15(NP) S"h e1h ahts"1 e e"h ahh 2a" ehtrh e" aic3ii30.6ii93ii.ii6.6i"h aFigure 3: A packed forest.Reducing redundancy.
State labels need not berepeated at every mention of that state?s ID; if astate has a label anywhere it has it always.
For ex-ample, we write the label S for state 0 in Figure 2only once:0(S) <- 1("he") 2("eats") 3("rice") / 0.693FINAL <- 0Similarly, state IDs may be left out wherever alabel uniquely identifies a particular state:0(S) <- ("he") ("eats") ("rice") / 0.693FINAL <- 0hyp generates state IDs for these states automati-cally.3.1 Trees and forestsA forest is a hypergraph that contains a set of trees.A forest may be packed, in which case its treesshare substructure, like strings in a lattice.
An ex-ample forest in hyp format is shown in Figure 3.Any two or more arcs pointing into one state haveOR semantics; the depicted forest compactly rep-12S "he 1eats 2riceFigure 4: A one-sentence finite-state machine in OpenFst.START <- 01 <- 0 4("he")2 <- 1 5("eats")3 <- 2 6("rice")FINAL <- 3S " " " h " e1at1 h 1ts2r1 h 1ic3t1 hFigure 5: A one-sentence finite-state hypergraph in hyp.resents two interpretations of one sentence: (1) heeats rice using sticks OR he eats rice that has sticks.Hypergraphs can represent any context-free gram-mar, where the strings in the grammar are the lex-ical yield (i.e., leaves in order) of the hypergraphtrees.3.2 Strings, lattices, and general FSMsIn addition to trees and forests, hypergraphs canrepresent strings, lattices, and general finite-statemachines (FSMs) as a special case.
A standardfinite-state representation of a string would looklike Figure 4, which shows a left-recursive bracket-ing as (((he) eats) rice), i.e., we read ?he?,combine it with ?eats?, then combine the resultwith ?rice?
to accept the whole string (Allauzen etal., 2007).We can do something similar in hyp usinghypergraphs?see Figure 5.
The hypergraph canbe traversed bottom-up by first reading start state0 and the ?he?
axiom state, reaching state 1, thenreading the following words until finally arriving atthe final state 3.
The visual representation of thisleft-recursive hypergraph can be understood as anunusual way to draw an FSM, where each arc hasan auxiliary label state.
If a hypergraph has a startstate and all its arcs are finite-state arcs, hyp recog-nizes it as an FSM; some operations may require oroptimize for an FSM rather than a general hyper-graph.
A finite-state arc has two tails, where thefirst one is a structural state and the second one aterminal label state.2 Adding additional arcs to the2Some operations may efficiently transform a generaliza-tion of FSM that we call a ?graph?, where there are zero ormore label states following the structural or ?source?
state,simple sentence hypergraph of Figure 5, we couldarrive at a more interesting lattice or even an FSMwith cycles and so infinitely many paths.3.3 TransducersA leaf state s with an output label o(s) 6= i (s)rewrites the input label.
This applies to finite-stateas well as general hypergraphs.
The following arc,for example, reads ?eats?
and an NP and derives aVP; it also rewrites ?eats?
to ?ate?
:(V) <- ("eats" "ate") (NP)If a state has an output label, it must then have aninput label, though it may be <eps>.
The startstate conventionally has no label.3.4 Semirings and featuresEach hypergraph uses a particular semiring, whichspecifies the type of weights and defines howweights are added and multiplied.
hyp providesthe standard semirings (Mohri, 2009), as well asthe expectation semiring (Eisner, 2002), and a new?feature?
semiring.
The feature semiring pairs withtropical semiring elements a sparse feature vectorthat adds componentwise in the semiring prod-uct and follows the winning tropical element in thesemiring sum.
Features 0 and 8 fire with differentstrengths on this arc:(V) <- 11("eats" "ate") / 3.2[0=1.3,8=-0.5]By using the expectation or the feature semiring,we can keep track of what features fire on whatarcs when we perform compositions or other oper-ations.
Using standard algorithms that are imple-mented in hyp (e.g., the inside-outside algorithm,see below), it is possible to train arc feature weightsfrom data (see Section 6).4 Using the hyp executableThe hyp toolkit provides an executable thatimplements several commands to process andmanipulate hypergraphs.
It is generally calledas hyp <command> <options> <input-files>, where <command> may be Compose,Best, or others.
We now describe some of thesecommands.rather than exactly one.13Compose hyp Compose composes twosemiring-weighted hypergraphs.
Compositionis used to parse an input into a structure and/orrewrite its labels.
Composition can also rescorea weighted hypergraph by composing with afinite-state machine, e.g., a language model.Example call:$ hyp Compose cfg.hyp fsa.hypSince context-free grammars are not closed un-der composition, one of the two composition ar-guments must be finite-state (Section 3.2).
If bothstructures are finite-state, hyp uses a fast finite-state composition algorithm (Mohri, 2009).3 Oth-erwise, we use a generalization of the Earley al-gorithm (Earley (1970), Eisner et al (2005), Dyer(2010)).4Best and PruneToBest.
hyp Best prints the k-best entries from any hypergraph.
hyp Prune-ToBest removes structure not needed for the bestpath.Example calls:$ hyp Best --num-best=2 h.hyp > k.txt$ hyp PruneToBest h.hyp > best.hypFor acyclic finite-state hypergraphs, hyp usesthe Viterbi algorithm to find the best path; other-wise it uses a general best-tree algorithm for CFGs(Knuth (1977), Graehl (2005)).Other executables.
Overall, hyp provides morethan 20 commands that perform hypergraph op-erations.
They can be used to concatenate, in-vert, project, reverse, draw, sample paths, createunions, run the inside algorithm, etc.
A detaileddescription is provided in the 25-page hyp tutorialdocument (Dreyer and Graehl, 2015).5 Using the hyp C++ APIIn addition to the command line tools described,hyp includes an open-source C++ API for con-structing and processing hypergraphs, for maxi-3If the best path rather than the full composition is re-quested, that composition is lazy best-first and may, weightsdepending, avoid creating most of the composition.4In the current hyp version, the Earley-inspired algorithmcomputes the full composition and should therefore be usedwith smaller grammars.mum flexibility and performance.5 The followingcode snippet creates the hypergraph shown in Fig-ure 2:typedef ViterbiWeight Weight;typedef ArcTpl<Weight> Arc;MutableHypergraph<Arc> hyp;StateId s = hyp.addState(S);hyp.setFinal(s);hyp.addArc(new Arc(Head(s),Tails(hyp.addState(he),hyp.addState(eats),hyp.addState(rice)),Weight(0.693)));The code defines weight and arc types, thenconstructs a hypergraph and adds the final state,then adds an arc by specifying the head, tails,and the weight.
The variables S, he, eats,rice are symbol IDs obtained from a vocabulary(not shown here).
The constructed hypergraphhyp can then be manipulated using provided C++functions.
For example, callingreverse(hyp);reverses all paths in the hypergraph.
All other op-erations described in Section 4 can be called fromC++ as well.The hyp distribution includes additional C++example code and doxygen API documentation.6 Optimizing hypergraph feature weightshyp provides functionality to optimize hyper-graph feature weights from training data.
It trainsa regularized conditional log-linear model, alsoknown as conditional random field (CRF), with op-tional hidden derivations (Lafferty et al (2001),Quattoni et al (2007)).
The training data con-sist of observed input-output hypergraph pairs(x, y).
x and y are non-loopy hypergraphs andso may represent string, lattice, tree, or forest.A user-defined function, which is compiled andloaded as a shared object, defines the search spaceof all possible outputs given any input x, withtheir features.
hyp then computes the CRF func-tion value, feature expectations and gradients, andcalls gradient-based optimization methods like L-BFGS or Adagrad (Duchi et al, 2010).
This maybe used to experiment with and train sequence ortree-based models.
For details, we refer to the hyptutorial (Dreyer and Graehl, 2015).5Using the C++ API to perform a sequence of operations,one can keep intermediate hypergraphs in memory and soavoid the cost of disk write and read operations.147 ConclusionsWe have presentedhyp, an open-source toolkit forrepresenting and manipulating weighted directedhypergraphs, including functionality for learningarc feature weights from data.
The hyp toolkitprovides a C++ library and a command line ex-ecutable.
Since hyp seamlessly handles trees,forests, strings, lattices and finite-state transduc-ers and acceptors, it is well-suited for a wide rangeof practical problems in NLP (e.g., for implement-ing a parser or a machine translation pipeline) andrelated areas.
hyp is available for download atgithub.com/sdl-research/hyp.AcknowledgmentsWe thank Daniel Marcu and Mark Hopkins forguidance and advice; Kevin Knight for encour-aging an open-source release; Bill Byrne, Ab-dessamad Echihabi, Steve DeNeefe, Adria de Gis-pert, Gonzalo Iglesias, Jonathan May, and manyothers at SDL Research for contributions and earlyfeedback; the anonymous reviewers for commentsand suggestions.ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
Open-Fst: A general and efficient weighted finite-statetransducer library.
In Proceedings of the Ninth In-ternational Conference on Implementation and Ap-plication of Automata, (CIAA 2007), volume 4783of Lecture Notes in Computer Science, pages 11?23.Springer.Markus Dreyer and Jonathan Graehl.
2015.Tutorial: The hyp hypergraph toolkit.http://goo.gl/O2qpi2.J.
Duchi, E. Hazan, and Y.
Singer.
2010.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
Journal of Machine Learning Research,12:2121?2159.Christopher Dyer.
2010.
A formal model of ambigu-ity and its applications in machine translation.
Ph.D.thesis, University of Maryland.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Jason Eisner and Nathaniel W. Filardo.
2011.
Dyna: Ex-tending datalog for modern AI.
In Datalog Reloaded,pages 181?220.
Springer.Jason Eisner, Eric Goldlust, and Noah A. Smith.
2005.Compiling comp ling: Practical weighted dynamicprogramming and the Dyna language.
In In Ad-vances in Probabilistic and Other Parsing.Jason Eisner.
2002.
Parameter estimation for proba-bilistic finite-state transducers.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 1?8, Philadelphia,July.Giorgio Gallo, Giustino Longo, and Stefano Pallottino.1993.
Directed hypergraphs and applications.
Dis-crete Applied Mathematics, 42(2):177?201.Emden R. Gansner and Stephen C. North.
2000.
Anopen graph visualization system and its applicationsto software engineering.
Software: Practice and Ex-perience, 30(11):1203?1233.Jonathan Graehl.
2005.
Context-free algorithms.arXiv:1502.02328 [cs.FL].Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 53?64.
Asso-ciation for Computational Linguistics.Dan Klein and Christopher D. Manning.
2005.
Parsingand hypergraphs.
In New developments in parsingtechnology, pages 351?372.
Springer.Donald E. Knuth.
1977.
A generalization of Dijkstra?salgorithm.
Information Processing Letters, 6(1):1?5.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for transla-tion hypergraphs and lattices.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP: Vol-ume 1, pages 163?171.
Association for Computa-tional Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th InternationalConference on Machine Learning, pages 282?289.Morgan Kaufmann, San Francisco, CA.Mehryar Mohri.
2009.
Weighted automata algorithms.In Handbook of weighted automata, pages 213?254.Springer.A.
Quattoni, S. Wang, L. P. Morency, M. Collins, andT.
Darrell.
2007.
Hidden conditional random fields.IEEE Transactions on Pattern Analysis and MachineIntelligence, 29(10):1848?1852.15
