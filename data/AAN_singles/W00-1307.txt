A Uni form Method of Grammar Extract ionand Its Appl icat ionsFei  X ia  and Mar tha  Pa lmer  and Arav ind  Josh iDepartment  of Computer  and Information ScienceUniversity of PennsylvaniaPhi ladelphia PA 19104, USA{fxia, mpalmer, j oshi)@linc, cis.
upenn, eduAbst rac tGrammars are core elements of many NLP ap-plications.
In this paper, we present a systemthat automatically extracts lexicalized gram-mars from annotated corpora.
The data pro-duced by this system have been used in sev-eral tasks, such as training NLP tools (suchas Supertaggers) and estimating the coverageof harid-crafted grammars.
We report experi-mental results on two of those tasks and com-pare our approaches with related work.1 In t roduct ionThere are various grammar frameworks pro-posed for natural languages.
We take Lexi-calized Tree-adjoining Grammars (LTAGs) asrepresentative of a class of lexicalized gram-mars.
LTAGs (Joshi et al, 1975) are ap-pealing for representing various phenomenain natural anguages due to its linguistic andcomputational properties.
In the last decade,LTAG has been used in several aspects ofnatural language understanding (e.g., pars-ing (Schabes, 1990; Srinivas, 1997), semantics(Joshi and Vijay-Shanker, 1999; Kallmeyerand Joshi, 1999), and discourse (Webber andJoshi, 1998)) and a number of NLP applica-tions (e.g., machine translation (Palmer et al,1998), information retrieval (Chandrasekarand Srinivas, 1997), and generation (Stoneand Doran, 1997; McCoy et al, 1992).
Thispaper describes a system that extracts LTAGsfrom annotated corpora (i.e., Treebanks).There has been much work done on extract-ing Context-Free grammars (CFGs) (Shiraiet al, 1995; Charniak, 1996; Krotov et al,1998).
However, extracting LTAGs is morecomplicated than extracting CFGs becauseof the differences between LTAGs and CFGs.First, the primitive elements of an LTAG arelexicalized tree structures (called elementarytrees), not context-free rules (which can beseen.
as trees with depth one).
Therefore, anLTAG extraction algorithm needs to examinea larger portion of a phrase structure to buildan elementary tree.
Second, the compositionoperations in LTAG are substitution (sameas the one in a CFG) and adjunction.
It isthe operation of adjunction that distinguishesLTAG from all other formalisms.
Third, un-like in CFGs, the parse trees (also known asderived trees in the LTAG) and the derivationtrees (which describe how elementary treesare combined to form parse trees) are differ-ent in the LTAG formalism in the sense thata parse tree can be produced by several dis-tinct derivation trees.
Therefore, to providetraining data for statistical LTAG parsers, anLTAG extraction algorithm should also buildderivation trees.For each phrase structure in a Treebank,our system creates a fully bracketed phrasestructure, a set of elementary trees anda derivation tree.
The data produced byour system have been used in several NLPtasks.
We report experimental results on twoof those applications and compare our ap-proaches with related work.2 LTAG fo rmal i smThe primitive elements of an LTAG are ele-mentary trees (etrees).
Each etree is associ-ated with a lexical item (called the anchor ofthe tree) on its frontier.
We choose LTAGs asour target grammars (i.e., the grammars to beextracted) because LTAGs possess many de-sirable properties, such as the Extended Do-main of Locality, which allows the encapsula-tion of all arguments of \[he anchor associatedwith an etree.
There are two types of etrees:.initial trees and auxiliary trees.
An auxiliarytree represents recursive structure and has aunique leaf node, called the foot node, whichhas the same syntactic ategory as the rootnode.
Leaf nodes other than anchor nodesand foot nodes are substitutionnodes.
Etreesare combined by two operations: substitutionand adjunction, as in Figure 1 and 2.
The53resulting structure of the combined etrees iscalled a derived tree.
The combination pro-cess is expressed as a derivation tree.Figure 1: The substitution operation=>/_ _~Figure 2: The adjunction operationFigure 3 shows the etrees, the derived tree,and the derivation tree for the sentence un-derwriters stil l draft policies.
Foot and sub-stitution nodes are marked by .
,  and $, re-spectively.
The dashed and solid lines in thederivation tree are for adjunction and substi-tution operations, respectively.3 System Overv iewWe have built a system, called LexTract, forgrammar extraction.
The architecture ofLex-Tract is shown in Figure 4 (the parts that willbe discussed in this paper are in bold).
Thecore of LexTract is an extraction algorithmthat takes a Treebank sentence such as theone in Figure 5 and produces the trees (el-ementary trees, derived trees and derivationtrees) such as the ones in Figure 3.3.1 The Form of Target GrammarsWithout further constraints, the etrees in thetarget grammar could be of various shapes.#, .4" - - .
~ ,  #3.
~:l :  " VP -- ' S NP A "- ~I --  NPI ADVP VP* ".
NP I VP 's~s I " .
.
.
.
"~:~'-.
i Ntisxa v!31> NP t /  II t i ~ Iicll~i undenviStel'x st i l l  dr'aft Ix) '(a) err?
?adraft(#3)I ADVP VP underwd lers (#1 ) ", policies(#4)NNS l ~ s t i l l (#2)slill ~ NN$I(b) derived tree (c) derivation tr~Figure 3: Etrees, derived tree and derivationtree for underwriters till draft policiesi l.~xTract Syslcm i matde:di !
L'rAo~Treebonk ~peclflc tlerlvatimt ~ i implaulibleFigure 4: Architecture of LexTract((S (PP-LOC (IN at)(NP (NNP FNX)))(NP-SBJ-I (NNS underwriters))(ADVP (RiB still))(VP (VBP draft)(NP (NNS policies))(S-MNR(NP-SBJ (-NONE- *- 1))(VP (VBG using)(NP(NP (NN fountain) (NNS pens))(CC and)(NP (VBG blotting) (NN papers))))))))Figure 5: A Treebank exampleOur system recognizes three types of rela-tion (namely, predicate-argument, modifica-tion, and coordination relations) between theanchor of an etree and other nodes in the etree,and imposes the constraint that all the etreesto be extracted should fall into exactly one ofthe three patterns in Figure 6.?
The spine-etrees for predicate-argumentrelations.
X ?
is the head of X m and theanchor of the etree.
The etree is formedby a spine X m --+ X m-1 -~ .. --+ X ?
andthe arguments of X ?.?
The mod-etrees for modification rela-tions.
The root of the etree has two chil-dren, one is a foot node with the labelWq, and the other node X m is a modifierX mAx 'X ?
zP~,lexical itemX mWqAWq" X m ~.
.
cclt x ~xO zPl x o z~ Ilcxical ilerrl IIcxicali lem(a) spinc..etree (b) mod-etz.~ (c) conj-etreeFigure 6: Three types of elementary trees inthe target grammar54of the foot node.
X TM is further expandedinto a spine-etree whose head X ?
is theanchor of the whole mod-etree.?
The conj-etrees for coordination rela-tions.
In a conj-etree, the children of theroot are two conjoined constituents and anode for a coordination conjunction.
Oneconjoined constituent is marked as thefoot node, and the other is expanded intoa spine-etree whose head is the anchor ofthe whole tree.Spine-etrees are initial trees, whereas mod-etrees and conj-etrees are auxiliary trees.3.2 Treebank-spec i f i c  n format ionThe phrase structures in the Treebank (ttreesfor short) are partially bracketed in the sensethat arguments and modifiers are not struc-turally distinguished.
In order to constructthe etrees, which make such distinction, Lex-Tract requires its user to provide additionalinformation in the form of three tables: aHead Percolation Table, an Argument Table,and a Tagset Table.A Head Percolation Table has previ-ously been used in several statistical parsers(Magerman, 1995; Collins, 1997) to find headsof phrases.
Our strategy for choosing heads issimilar to the one in (Collins, 1997).
An Ar-gument Table informs LexTract what types ofarguments a head can take.
The Tagset Tablespecifies what function tags always mark ar-guments (adjuncts, heads, respectively).
Lex-Tract marks each sibling of a head as an argu-ment if the sibling can be an argument of thehead according to the Argument Table andnone of the function tags of the sister indi-cates that it is an adjunct.
For example, inFigure 5, the head of the root S is the verbdraft, and the verb has two siblings: the nounphrase policies is marked as an argument ofthe verb because from the Argument Table weknow that verbs in general can take an NP ob-ject; the clause is marked as a modifier of theverb because, although verbs in general cantake a sentential argument, the Tagset Tableinforms LexTract that the function tag -MNR(manner) always marks a modifier.3.3 Overv iew of  the  Ext ract ionA lgor i thmThe extraction process has three steps: First,LexTract fully brackets each ttree; Second,LexTract decomposes the fully bracketed ttree((S (PP-LOC (IN at)(NP (NNP FNX)))(S (NP-SBJ-I (NNS underwriters))(VP (ADVP (RB still))(VP (VP (VBP draft)(NP (NNS policies)))(S-MNR(NP-SBJ (-NONE- *-1))(VP (VBG using)(NP (NP (NN fountain)(NP (NNS pens)))(CC and)(NP (VBG blotting)(NP (NN papers))))))))Figure 7: The fully bracketed ttreeinto a set of etrees; Third, LexTract builds thederivation tree for the ttree.3.3.1 Ful ly  b racket ing  ttreesAs just mentioned, the ttrees in the Tree-bank do not explicitly distinguish argumentsand modifiers, whereas etrees do.
To accountfor this difference, we first fully bracket thettrees by adding intermediate nodes so thatat each level, one of the following relationsholds between the head and its siblings: (1)head-argument relation, (2) modification re-lation, and (3) coordination relation.
Lex-Tract achieves this by first choosing the head-child at each level and distinguishing argu-ments from adjuncts with the help of the threetables mentioned in Section 3.2, then addingintermediate nodes so that the modifiers andarguments of a head attach to different levels.Figure 7 shows the fully bracketed ttree.
Thenodes inserted by LexTract are in bold face.3.3.2 Bu i ld ing  etreesIn this step, LexTract removes recursive struc-tures - which will become mod-etrees or conj-etrees - from the fully bracketed ttrees andbuilds spine-etrees for the non-recursive struc-tures.
Starting from the root of a fully brack-eted ttree, LexTract first finds a unique pathfrom the root to its head.
It then checks eachnode e on the path.
If a sibling of e in the ttreeis marked as a modifier, LexTract marks e ande's parent, and builds a mod-etree (or a conj-etree if e has another sibling which is a con-junction) with e's parent as the root node, e asthe foot node, and e's siblings as the modifier.Next, LexTract creates a spine-etree with theremaining unmarked nodes on the path andtheir siblings.
Finally, LexTract repeats thisprocess for the nodes that are not on the path.In Figure 8, which is the same as the one inFigure 7 except hat some nodes are numberedand split into the top and bottom pairs, 1 the1When a pair of etrees are combined ur ing parsing,55#5s2.b -"----...L ?
"2" d I .#6Figure 8: The extracted et:rees can be seen asa decomposition of the fully bracketed ttree#1: #2: #3: #4: #5: #6:S NP NP VP S NPPP S" NNS ADVP VP = NPI VP NNS,s NP ~ { I RB VBP Na\[ FNX undcrwriten; ~ I pollciexstill draft a\[#7: #8: #9: #10: #I \[: #12:Vp NP NP NP "~ I cc NPv .
s NN N.  N~S I ~ VBG NP" NP" CCI NPNe VP \[ NNl~,t,lntaln bloitln8?~peruffm8Figure 9: The extracted etrees from the fullybracketed ttreepath from the root $1 to the head VBP is$1 ~ $2 ~ VP1 ~ VP2 --+ VP3 ~ VBP.Along the path the PP  ~ at FNX-  is amodifier of $2; therefore, Sl.b, S2.t, and thespine-etree rooted at PP  form a mod-etree#1.
Similarly, the ADVP still is a modifierof VP2 and $3 is a modifier of VP3, and thecorresponding structures form mod-etrees #4and #7.
On the path from the root to VBP,Sl .t  and S2.b are merged (and so are VPi .
tand VP3.b) to from the spine-etree #5.
Re-peating this process for other nodes will gen-erate other trees such as trees #2, #3 and #6.The whole ttree yields twelve etrees as shownin Figure 9.3.3.3 Bu i ld ing  der ivat ion  t reesThe fully bracketed ttree is in fact a derivedtree of the sentence if the sentence is parsedwith the etrees extracted by LexTract.
In ad-dition to these etrees and the derived tree, wethe root of one etree is merged with a node in the otheretree.
Splitting nodes into top and bottom pairs duringthe decomposition f the derived tree is the reverseprocess of merging nodes during parsing.
For the sakeof simplicity, we show the top and the bottom parts ofa node only when the two parts will end up in differentetrees.also need derivation trees to train statisticalLTAG parsers.
Recall that, in general, givena derived tree, the derivation tree that cangenerate the derived tree may not be unique.Nevertheless, given the fully bracketed ttree,the etrees, and the positions of the etrees inthe ttree (see Figure 8), the derivation treebecomes unique if we choose either one of thefollowing:?
We adopt the traditional definition ofderivation trees (which allows at most oneadjunction at any node) and add an ad-ditional constraint which says that no ad-junction operation is allowed at the footnode of any auxiliary tree.
2?
We adopt the definition of derivationtrees in (Schabes and Shieber, 1992)(which allows multiple adjunction at anynode) and require all mod-etrees adjointo the etree that they modify.The user of LexTract can choose either op-tion and inform LexTract about his choice bysetting a parameter.
3 Figure 10 shows thederivation tree based on the second option.draft (#5)a\[ (#1) underwriters(#3) ~i11(#4) policies(#6) using(#7)I IFNX(#2) pen(#9)fountain(#8) paper(#12)and(#10) bloldng(#l I)Figure 10: The derivation tree for the sentence3.4 Un iqueness  of  decompos i t ionTo summarize, LexTract is a language-independent grammar extraction system,which takes Treebank-specific information(see Section 3.2) and a ttree T, and creates2Without his additional constraint, the derivationtree sometimes i  not unique.
For example, in Figure8, both #4 and #7 modify the etree #5.
If adjunc-tion were allowed at foot nodes, ~4 could adjoin to~7 at VP2.b, and #7 would adjoin to #5 at VPs.b.An alternative isfor #4 to adjoin to #5 at VPs.b andfor ~7 to adjoin to ~4 at VP2.t.
The no-adjunction-at-foot-node constraint would rule out the latter al-ternative and make the derivation tree unique.
Notethat this constraint has been adopted by several hand-crafted grammars such as the XTAG grammar for En-glish (XTAG-Group, 1998), because it eliminates thissource of spurious ambiguity.SThis decision may affect parsing accuracy of anLTAG parser which uses the derivation trees for train-ing, but it will not affect he results reported in thispaper.56(1) a fully bracketed ttree T*, (2) a set Esetof etrees, and (3) a derivation tree D for T*.Furthermore, Eset  is the only tree set thatsatisfies all the following conditions:(C1)  Decompos i t ion :  The tree set is a de-composition of T*, that is, T* would begenerated if the trees in the set were com-bined via the substitution and adjunctionoperations.
(C2)  LTAG formal i sm:  Each tree in theset is a valid etree, according to the LTAGformalism.
For instance, each tree shouldbe lexicalized and the arguments of theanchor should be encapsulated in thesame etree.
(C3) Target  g rammar :  Each tree in theset falls into one of the three types asspecified in Section 3.1.
(C4)  T reebank-spec i f i c  in fo rmat ion :The head/argument/adjunct distinctionin the trees is made according to theTreebank-specific information providedby the user as specified in Section 3.2.SNP VP<1 IN VI tJohn left(T*)Ja,.
I IJohn left(E l) (E2)\[ &~m t lea lc~hn \[ Icft(E) (E,) (Es) (E6)Figure 11: Tree sets for a fully bracketed ttreeThis uniqueness of the tree set may be quitesurprising at first sight, considering that thenumber of possible decompositions of T* is~(2n), where n is the number of nodes in T*.
4Instead of giving a proof of the uniqueness,4Recall that the process of building etrees has twosteps.
First, LexTract treats each node as a pair ofthe top and bottom parts.
The ttree is cut into piecesalong the boundaries of the top and bottom parts ofsome nodes.
The top and the bottom parts of eachnode belong to either two distinct pieces or one piece,as a result, there are 2 ~ distinct partitions.
Second,some non-adjacent pieces in a partition can be gluedtogether to form a bigger piece.
Therefore, each par-tition will result in one or more decompositions of thettree.
In total, there are at least 2 n decompositions ofthe ttree.we use an example to illustrate how the con-ditions (C1)--(C4) rule out all the decompo-sitions except the one produced by LexTract.In Figure 11, the ttree T* has 5 nodes (i.e.,S, NP, N, VP, and V).
There are 32 distinctdecompositions for T*, 6 of which are shownin the same figure.
Out of these 32 decom-positions, only five (i.e., E2 - -  E6) are fullylexicalized - -  that is, each tree in these treesets is anchored by a lexical item.
The rest,including El, are not fully lexicalized, and aretherefore ruled out by the condition (C2).
Forthe remaining five etree sets, E2 - -  E4 areruled out by the condition (C3), because achof these tree sets has one tree that violates oneconstraint which says that in a spine-etree anargument of the anchor should be a substitu-tion node, rather than an internal node.
Forthe remaining two, E5 is ruled out by (C4)because according to the Head Table providedby the user, the head of the S node should beV, not N. Therefore, E6, the tree set that isproduced by LexTract, is the only etree set forT* that satisfies (C1)--(C4).3.5 The  Exper imentsWe have ran LexTract on the one-million-word English Penn Treebank (Marcus etal., 1993) and got two Treebank grammars.The first one, G1, uses the Treebank'stagset.
The second Treebank grammar,G2, uses a reduced tagset, where some tagsin the Treebank tagset are merged into asingle tag.
For example, the tags for verbs,MD/VB/VBP/VBZ/VBN/VBD/VBG,  aremerged into a single tag V. The reducedtagset is basically the same as the tagsetused in the XTAG grammar (XTAG-Group,1998).
G2 is built so that we can compareit with the XTAG grammar, as will bediscussed in the next section.
We also ran thesystem on the 100-thousand-word ChinesePenn Treebank (Xia et al, 2000b) and on a30-thousand-word Korean Penn Treebank.The sizes of extracted grammars are shown inTable 1.
(For more discussion on the Chineseand the Korean Treebanks and the compar-ison between these Treebank grammars, see(Xia et al, 2000a)).
The second column ofthe table lists the numbers of unique tem-plates in each grammar, where templates areetrees with the lexical items removed, s Thethird column shows the numbers of unique5For instance, #3, #6 and #9 in Figure 9 are threedifferent etrees but they share the same template.
Anetree can be seen as a (word, template) pair.57etrees.
The average numbers of etrees for eachword type in G1 and G2 are 2.67 and-2.38respectively.
Because frequent words oftenanchor many etrees, the numbers increase bymore than 10 times when we consider wordtoken, as shown in the fifth and sixth columnsof the table.
G3 and G4 are much smallerthan G1 and G2 because the Chinese and theKorean Treebanks are much smaller than theEnglish Treebank.In addition to LTAGs, by reading context-free rules off the etrees of a Treebank LTAG,LexTract also produces CFGs.
The numbersof unlexicalized context-free rules from G1--G4 are shown in the last column of Table 1.Comparing with other CFG extraction algo-rithms such as the one in (Krotov et al, 1998),the CFGs produced by LexTract have sev-eral good properties.
For example, they allowunary rules and epsilon rules, they are morecompact and the size of the grammar remainsmonotonic as the Treebank grows.Figure 12 shows the log frequency of tem-plates and the percentage of template tokenscovered by template types in G1.
6 In bothcases, template types are sorted according totheir frequencies and plotted on the X-axes.The figure indicates that a small portion oftemplate types, which can be seen as the coreof the grammar, cover majority of templatetokens in the Treebank.
For example, the first100 (500, 1000 and 1500, resp.)
templatescover 87.1% (96.6~o, 98.4% and 99.0% resp.
)of the tokens, whereas about half (3411) ofthe templates each occur only once, account-ing for only 0.29% of template tokens in total.4 App l i ca t ions  o f  LexTractIn addition to extract LTAGs and CFGs, Lex-Tract has been used to perform the followingtasks:?
We use the Treebank grammars producedby LexTract to evaluate the coverage ofhand-crafted grammars.?
We use the (word, template) sequenceproduced by LexTract to re-train Srini-vas' Supertaggers (Srinivas, 1997).?
The derivation trees created by LexTractare used to train a statistical LTAGparser (Sarkar, 2000).
LexTract outputhas also been used to train an LR LTAGparser (Prolo, 2000).6Similar results hold for G2, G3 and G4.?
We have used LexTract to retrieve thedata from Treebanks to test theoret-ical linguistic hypotheses uch as theTree-locality Hypothesis (Xia and Bleam,20O0).?
LexTract has a filter that checks theplausibility of extracted etrees by decom-posing each etree into substructures andchecking them.
Implausible trees are of-ten caused by Treebank annotation er-rors.
Because LexTract maintains themappings between etree nodes and ttreenodes, it can detect certain types of an-notation errors.
We have used LexTractfor the final cleanup of the Penn ChineseTreebank.Due to space limitation, in this paper wewill only discuss the first two tasks.4.1 Eva luat ing  the  coverage ofhand-c ra f ted  grammarsThe XTAG grammar (XTAG-Group, 1998)is a hand-crafted large-scale grammar for En-glish, which has been developed at Universityof Pennsylvania n the last decade.
It has beenused in many NLP applications uch as gen-eration (Stone and Doran, 1997).
Evaluatingthe coverage of such a grammar is importantfor both its developers and its users.Previous evaluations (Doran et al, 1994;Srinivas et al, 1998) of the XTAG grammaruse raw data (i.e., a set of sentences with-out syntactic bracketing).
The data are firstparsed by an LTAG parser and the coverageof the grammar is measured as the percent-age of sentences in the data that get at leastone parse, which is not necessarily the correctparse.
For more discussion on this approach,see (Prasad and Sarkar, 2000).We propose a new evaluation method thattakes advantage of Treebanks and LexTract.The idea is as follows: given a Treebank T anda hand-crafted grammar Gh, the coverage ofGh on T can be measured by the overlap of Ghand a Treebank grammar Gt that is producedby LexTract from T. In this case, we will esti-mate the coverage of the XTAG grammar onthe English Penn Treebank (PTB) using theTreebank grammar G2.There are obvious differences between thesetwo grammars.
For example, feature struc-tures and multi-anchor etrees are present onlyin the XTAG grammar, whereas frequency in-formation is available only in G2.
When wematch templates in two grammars, we disre-58template etreetypes typesEng G1 6926 131,397Eng G2 2920 117,356Ch G3 1140 21,125Kor G4 634 9,787wordtypes49,20649,20610,7726,747etree types etree types CFG rulesper word type i per word token (unlexicalized)2.67~ 34.68 15242.38 27.70 6751.96 9.13 5151.45 2.76 177Table 1: Grammars extracted from three Treebanks' ro.~o.e0.7o.eo.5o.4o~02o.~oT~ m T ~(a) Frequency of templates (b) Coverage of templatesFigure 12: Template types and template tokens in G1gard the type of information that is presentonly in one grammar.
As a result, the map-ping between two grammars is not one-to-one.XTAGG~~equencymatched unmatched totaltemplates templates497 507 1004215 2705 292082.1% I 17.9% \[100%Table 2: Matched templates in two grammarsTable 2 shows that 497 templates in theXTAG grammar and 215 templates in G2match, and the latter accounts for 82.1% ofthe template tokens in the PTB.
The remain-ing 17.9% template tokens in the PTB do notmatch any template in the XTAG grammarbecause of one of the following reasons:(T1) Incorrect templates in G2: These tem-plates result from Treebank annotation er-rors, and therefore, are not in XTAG.
(T2) Coordination in XTAG: the templatesfor coordinations in XTAG are generatedon the fly while parsing (Sarkar and Joshi,1996), and are not part of the 1004 templates.Therefore, the conj-etrees in G2, which ac-count for 3.4% of the template tokens in theTreebank, do not match any templates inXTAG.
(T3) Alternative analyses: XTAG and PTBsometimes choose different analyses for thesame phenomenon.
For example, the twogrammars treat reduced relative clauses dif-ferently.
As a result, the templates used tohandle those phenomena in these two gram-mars do not match according to our defini-tion.
(T4) Construct ions not covered by XTAG:Some of such constructions are the unlikecoordination phrase (UCP), parenthetical(PRN), and ellipsis.For (T1)--(T3),  the XTAG grammar canhandle the corresponding constructions al-though the templates used in two grammarslook very different.
To find out what construc-tions are not covered by XTAG, we manuallyclassify 289 of the most frequent unmatchedtemplates in G2 according to the reason whythey are absent from XTAG.
These 289 tem-plates account for 93.9% of all the unmatchedtemplate tokens in the Treebank.
The resultsare shown in Table 3, where the percentage iswith respect o all the tokens in the Treebank.From the table, it is clear that the most com-mon reason for mis-matches i (T3).
Combin-ing the results in Table 2 and 3, we concludethat 97.2% of template tokens in the Treebankare covered by XTAG, while another 1.7% arenot.
For the remaining 1.1% template tokens,we do not know whether or not they are cov-ered by XTAG because we have not checkedthe remaining 2416 unmatched templates inG2.
TTo summarize, we have just showed that,7The number 97.2% is the sum of two numbers:the first one is the percentage of matched template to-kens (82.1% from Table 2).
The secb-nd number is thepercentage oftemplate tokens which fall under (T1)--(T3), i.e., 16.8%-1.7%=15.1% from Table 3.59T1 T2 T3 T4 totaltype 51 52 ~93 93 289freq 1.1% 3.4% 10.6% 1.7% 16.8%Table 3: Classifications of 289 unmatchedtemplatesby comparing templates in the XTAG gram-mar with the 'IYeebank grammar produced byLexTract, we estimate that the XTAG gram-mar covers 97.2% of template tokens in theEnglish Treebank.
Comparing with previousevaluation approach, this :method has severaladvantages.
First, the whole process is semi-automatic and requires little human effort.Second, the coverage can be calculated at ei-ther sentence l vel or etree level, which is morefine-grained.
Third, the method provides alist of etrees that can be added to the gram-mar to improve its coverage.
Fourth, thereis no need to parse the whole corpus, whichcould have been very time-consuming.4.2 Training Super taggersA Supertagger (Joshi and Srinivas, 1994;Srinivas, 1997) assigns an etree template toeach word in a sentence.
The templatesare also called Supertags because they in-clude more information than Part-of-Speechtags.
Srinivas implemented the first Supertag-ger, and he also built a Lightweight Depen-dency Analyzer that assembles the Supertagsof words to create an almost-parse for the sen-tence.
Supertaggers have been found usefulfor several applications, such as informationretrieval (Chandrasekar nd Srinivas, 1997).To use a Treebank to train a Supertagger,the phrase structures in the Treebank have tobe converted into (word, Supertag) sequencesfirst.
Producing such sequences i  exactly oneof LexTract's main functions, as shown previ-ously in Section 3.3.2 and Figure 9.Besides LexTract, there are two other at-tempts in converting the English Penn Tree-bank to train a Supertagger.
Srinivas (1997)uses heuristics to map structural informationin the Treebank into Supertags.
His methodis different from LexTract in that the set ofSupertags in his method is chosen from thepre-existing XTAG grammar before the con-version starts, whereas LexTract extracts theSupertag set from Treebanks.
His conversionprogram is also designed for this particularSupertag set, and it is not very-easy to portit to another Supertag set.
A third differenceis that the Supertags in his converted ata donot always fit together, due to the discrep-ancy between the XTAG grammar and theTreebank annotation and the fact that theXTAG grammar does not cover all the tem-plates in the Treebank (see Section 4.1).
Inother words, even if the Supertagger is 100%accurate, it is possible that the correct parsefor a sentence can not be produced by com-bining those Supertags in the sentence.Another work in converting Treebanks intoLTAGs is described in (Chen and Vijay-Shanker, 2000).
The method is similar to oursin that both work use Head Percolation Tablesto find the head and both distinguish adjunctsfrom modifiers using syntactic tags and func-tional tags.
Nevertheless, there are severaldifferences: only LexTract explicitly createsfully bracketed ttrees, which are identical tothe derived trees for the sentences.
As a re-sult, building etrees can be seen as a task ofdecomposing the fully bracketed ttrees.
Themapping between the nodes in fully bracketedttrees and etrees makes LexTract a useful toolfor 'IYeebank annotation and error detection.The two approaches also differ in how theydistinguish arguments from adjuncts and howthey handle coordinations.Table 4 lists the tagging accuracy of thesame trigram Supertagger (Srinivas, 1997)trained and tested on the same original PTBdata.
s The difference in tagging accuracyis caused by different conversion algorithmsthat convert the original PTB data into the(word, template) sequences, which are fedto the Supertagger.
The results of Chen &Vijay-Shanker's method come from their pa-per (Chen and Vijay-Shanker, 2000).
Theybuilt eight grammars.
We just list two of themwhich seem to be most relevant: C4 uses a re-duced tagset while C3 uses the PTB tagset.As for Srinivas' results, we did not use the re-sults reported in (Srinivas, 1997) and (Chen etal., 1999) because they are based on differenttraining and testing data.
9 Instead, we re-ranSAll use Section 2-21 of the PTB for training, andSection 22 or 23 for testing.
We choose those sec-tions because several state-of-thwart parsers (Collins,1997; Ratnaparkhi, 1998; Charniak, 1997) are trainedon Section 2-21 and tested on Section 23.
We includethe results for Section 22 because (Chen and Vijay-Shanker, 2000) is tested on that section.
For Srinivas'and our grammars, the first line is the results tested onSection 23, and the second line is the one for Section22.
Chen & Vijay-Shauker's results~e for Section 22only.9He used Section 0-24 minus Section 20 for trainingand the Section 20 for testing.60his Supertagger using his data on the sectionsthat we have chosen.
1?
We have calculatedtwo baselines for each seg of data.
The firstone tags each word in testing data with themost common Supertag w.r.t the word in thetraining data.
For an unknown word, just useits most common Supertag.
For the secondbaseline, we use a trigram POS tagger to tagthe words first, and then for each word we usethe most common Supertag w.r.t, the (word,POS tag) pair.templatesSrinivas' 483our G2 2920our G1 6926Chen's 2366 - -(sect 22) - -  8996C4 4911C3 8623basel base272.59 74.2472.14 73.7471.45 74.1470.54 73.4169.70 71.8268.79 70.90acc85.7885.5384.4183.6082.2181.8877.8 - -- -  78.978.9078.O0Table 4: Supertagging results based on threedifferent conversion algorithmsA few observations are in order.
First, thebaselines for Supertagging are lower than theone for POS tagging, which is 91%, indicat-ing Supertagging is harder than POS tagging.Second, the second baseline is slightly bet-ter than the first baseline, indicating using~?Noticeably, the results we report on Srinivas' data,85.78% on Section 23 and 85.53% on Section 22, axelower than 92.2% reported in (Srinivas, 1997) and91.37% in (Chen et al, 1999).
There axe severalreasons for the difference.
First, the size of trainingdata in our report is smaller than the one for his pre-vious work; Second, we treat punctuation marks asnormal words during evaluation because, like otherwords, punctuation marks can anchor etrees, whereashe treats the Supertags for punctuation marks as al-ways correct.
Third, he used some equivalent classesduring evaluations.
If a word is mis-tagged as x, whilethe correct Supertag is y, he considers that not to bean error if x and y appear in the same equivalent class.We suspect that the reason that those Supertagging er-rors axe disregarded is that those errors might not af-fect parsing results when the Supertags are combined.For example, both adjectives and nouns can modifyother nouns.
The two templates (i.e.
Supertags) rep-resenting these modification relations look the sameexcept for the POS tags of the anchors.
If a wordwhich should be tagged with one Supertag is mis-tagged with the other Supertag, it is likely that thewrong Supertag can still fit with other Supertags inthe sentence and produce the right parse.
We did notuse these quivalent classes in this experiment becausewe are not aware of a systematic way to find all thecases in which Supertagging errors do not affect thefinal parsing results.POS tags may improve the Supertagging ac-curacy, n Third, the Supertagging accuracyusing G2 is 1.3-1.9% lower than the one usingSrinivas' data.
This is not surprising since thesize of G2 is 6 times that of Srinivas' grammar.Notice that G1 is twice the size of G2 andthe accuracy using G1 is 2% lower.
Fourth,higher Supertagging accuracy does not neces-sarily means the quality of converted ata arebetter since the underlying rammars differ alot with respect o the size and the coverage.A better measure will be the parsing accu-racy (i.e., the converted ata should be fed toa common LTAG parser and the evaluationsshould be based on parsing results).
We arecurrently working on that.
Nevertheless, theexperiments show that the (word, template)sequences produced by LexTract are useful fortraining Supertaggers.
Our results are slightlylower than the ones trained on Srinivas' data,but our conversion algorithm has several ap-pealing properties: LexTract does not use pre-existing Supertag set; LexTract is language-independent; he (word, Supertag) sequenceproduced by LexTract fit together.5 Conc lus ionWe have presented a system for grammar ex-traction that produces an LTAG from a Tree-bank.
The output produced by the systemhas been used in many NLP tasks, two ofwhich are discussed in the paper.
In the firsttask, by comparing the XTAG grammar witha Treebank grammar produced by LexTract,we estimate that the XTAG grammar covers97.2% of template tokens in the English Tree-bank.
We plan to use the Treebank grammarto improve the coverage of the XTAG gram-mar.
We have also found constructions thatare covered in the XTAG grammar but do notappear in the Treebank.
In the second task,LexTract converts the Treebank into a formatthat can be used to train Supertaggers, andthe Supertagging accuracy is compatible to, ifnot better than, the ones based on other con-version algorithms.
For future work, we planto use derivation trees to train LTAG parsersdirectly and use LexTract to add semantic in-formation to the Penn Treebank.Re ferencesR.
Chandrasekar nd B. Srinivas.
1997.
Glean-ing information from the Web: Using Syntaxto Filter out Irrelevant Information.
In Proc.
ofnThe baselines and results on Section 23 for (Chenand Vijay-Shanker, 2000) are not available to us.61AAAI 1997 Spring Symposium on NLP on theWorld Wide Web.Eugene Charniak.
1996.
Treebank Grammars.
InProc.
of AAAI-1996.Eugene Charniak.
1997.
Statistical Parsing witha Context-Free Grammar and Word Statistics.In Proc.
of AAAI-1997.John Chen and K. Vijay-Shanker.
2000.
Auto-mated Extraction of TAGs from the Penn Tree-bank.
In 6th International Workshop on Pars-ing Technologies (IWPT..2000), Italy.John Chen, Srinivas Bangalore, and K. Vijay-Shanker.
1999.
New Models for ImprovingSupertag Disambiguation.
In Proc.
of EACL-1999.Mike Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
In Proc.
of the35th ACL.C.
Doran, D. Egedi, B.
A. Hockey, B. Srinivas,and M. Zaidel.
1994.
XTAG System - A WideCoverage Grammar for English.
In Proc.
ofCOLING-1994, Kyoto, Japan.Aravind Joshi and B. Srinivas.
1994.
Disambigua-tion of Super Parts of Speech (or Supertags):Almost Parsing.
In Proc.
of COLING-1994.Aravind Joshi and K. Vijay-Shanker.
1999.
Com-positional Semantics with LTAG: How MuchUnderspecification Is Necessary?
In Proc.
of3nd International Workshop on ComputationalSemantics.Aravind K. Joshi, L. Levy, and M. Takahashi.1975.
Tree Adjunct Grammars.
Journal ofComputer and System Sciences.Laura Kallmeyer and Aravind Joshi.
1999.
Un-derspecified Semantics with LTAG.Alexander Krotov, Mark Hepple, RobertGalzauskas, and Yorick Wilks.
1998.
Compact-ing the Penn Treebank Grammar.
In Proc.
ofA CL- COLING.David M. Magerman.
1995.
Statistical Decision-Tree Models for Parsing.
In Proc.
of the 33rdACL.M.
Marcus, B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a LargeAnnotated Corpus of English: the PennTreebank.
Computational Lingustics.K.
F. McCoy, K. Vijay-Shanker, and G. Yang.1992.
A Functional Approach to Generationwith TAG.
In Proc.
of the 30th A CL.Martha Palmer, Owen Rainbow, and AlexisNasr.
1998.
Rapid Prototyping of Domain-Specific Machine Translation System.
In Proc.of AMTA-1998, Langhorne, PA.Rashmi Prasad and Anoop Sarkar.
2000.
Compar-ing Test-Suite Based Evaluation and Corpus-Based Evaluation of a Wide-Coverage Grammarfor English.
In Proc.
of LREC satellite work-shop Using Evaluation within HLT Programs:Results and Trends, Athen, Greece.Carlos A. Prolo.
2000.
An Efficient LR ParserGenerator for TAGs.
In 6th InternationalWorkshop on Parsing Technologies (IWPT2000), Italy.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Resolu-tion.
Ph.D. thesis, University of Pennsylvania.Anoop Sarkar and Aravind Joshi.
1996.
Coordi-nation in Tree Adjoining Grammars: Formaliza-tion and Implementation.
In Proc.
of the 18thCOLING, Copenhagen, Denmark.Anoop Sarkar.
2000.
Practical Experiments inParsing using Tree Adjoining Grammars.
InProc.
of 5th International Workshop on TAGand Related Frameworks (TAG+5).The XTAG-Group.
1998.
A Lexicalized Tree Ad-joining Grammar for English.
Technical ReportIRCS 98-18, University of Pennsylvania.Yves Schabes and Stuart Shieber.
1992.
An Al-ternative Conception of Tree-Adjoining Deriva-tion.
In Proc.
of the 20th Meeting of the Asso-ciation for Computational Linguistics.Yves Schabes.
1990.
Mathematical nd Computa-tional Aspects of Lexicalized Grammars.
Ph.D.thesis, University of Pennsylvania.Kiyoaki Shirai, Takenobu Tokunaga, and HozumiTanaka.
1995.
Automatic Extraction ofJapanese Grammar from a Bracketed Corpus.In Proc.
of Natural Language Processing PacificRim Symposium (NLPRS-1995).B.
Srinivas, Anoop Sarkar, Christine Doran, andBeth Ann Hockey.
1998.
Grammar and ParserEvaluation in the XTAG Project.
In Workshopon Evaluation of Parsing Systems, Granada,Spain.B.
Srinivas.
1997.
Complexity of Lexical De-scriptions and Its Relevance to Partial Parsing.Ph.D.
thesis, University of Pennsylvania.Matthew Stone and Christine Doran.
1997.
Sen-tence Planning as Description Using Tree Ad-joining Grammar.
In Proc.
of the 35th A CL.Bonnie Webber and Aravind Joshi.
1998.
Anchor-ing a Lexicalized Tree Adjoining Grammar forDiscourse.
In Proc.
of A CL-COLING Workshopon Discourse Relations and Discourse Markers.Fei Xia and Tonia Bleam.
2000.
A Corpus-BasedEvaluation of Syntactic Locality in TAGs.
InProc.
of 5th International Workshop on TAGand Related Frameworks (TAG+5).Fei Xia, Chunghye Han, Martha Palmer, andAravind Joshi.
2000a.
Comparing LexicalizedTreebank Grammars Extracted from Chinese,Korean, and English Corpora.
In Proc.
of the2nd GT~inese Language Processing Workshop,Hong Kong, China.Fei Xia, Martha Palmer, Nianwen Xue, Mary EllenOkurowski, John Kovarik, Shizhe Huang, TonyKroch, and Mitch Marcus.
2000b.
DevelopingGuidelines and Ensuring Consistency for Chi-nese Text Annotation.
In Proc.
of the 2nd In-ternational Conference on Language Resourcesand Evaluation (LREC-2000),-Athens, Greece.62
