A speech interface for open-domain question-answeringEdward Schofieldftw.
Telecommunications Research CenterVienna, AustriaDepartment of ComputingImperial College London, U.K.schofield@ftw.atZhiping ZhengDept.
of Computational LinguisticsSaarland UniversitySaarbru?cken, Germanyzheng@coli.uni-sb.deAbstractSpeech interfaces to question-answeringsystems offer significant potential for find-ing information with phones and mo-bile networked devices.
We describe ademonstration of spoken question answer-ing using a commercial dictation enginewhose language models we have cus-tomized to questions, a Web-based text-prediction interface allowing quick cor-rection of errors, and an open-domainquestion-answering system, AnswerBus,which is freely available on the Web.
Wedescribe a small evaluation of the effectof recognition errors on the precision ofthe answers returned and make some con-crete recommendations for modifying aquestion-answering system for improvingrobustness to spoken input.1 IntroductionThis paper demonstrates a multimodal interface forasking questions and retrieving a set of likely an-swers.
Such an interface is particularly appropri-ate for mobile networked devices with screens thatare too small to display general Web pages anddocuments.
Palm and Pocket PC devices, whosescreens commonly display 10?15 lines, are candi-dates.
Schofield and Kubin (2002) argue that forsuch devices question-answering is more appropri-ate than traditional document retrieval.
But untilrecently no method has existed for inputting ques-tions in a reasonable amount of time.
The studyof Schofield (2003) concludes that questions tendto have a limited lexical structure that can be ex-ploited for accurate speech recognition or text pre-diction.
In this demonstration we test whether thisresult can endow a real spoken question answeringsystem with acceptable precision.2 Related researchKupiec and others (1994) at Xerox labs built oneof the earliest spoken information retrieval systems,with a speaker-dependent isolated-word speech rec-ognizer and an electronic encyclopedia.
One rea-son they reported for the success of their systemwas their use of simple language models to exploitthe observation that pairs of words co-occurring ina document source are likely to be spoken togetheras keywords in a query.
Later research at CMUbuilt upon similar intuition by deriving the language-model of their Sphinx-II speech recognizer fromthe searched document source.
Colineau and others(1999) developed a system as a part of the THISLproject for retrieval from broadcast news to respondto news-related queries such as What do you have on.
.
.
?
and I am doing a report on .
.
.
?
can you helpme?
The queries the authors addressed had a sim-ple structure, and they successfully modelled themin two parts: a question-frame, for which they hand-wrote grammar rules; and a content-bearing stringof keywords, for which they fitted standard lexicallanguage-models from the news collection.Extensive research (Garofolo et al, 2000; Allan,2001) has concluded that spoken documents can beeffectively indexed and searched with word-errorrates as high as 30?40%.
One might expect a muchhigher sensitivity to recognition errors with a shortquery or natural-language question.
Two studies (etal., 1997; Crestani, 2002) have measured the detri-mental effect of speech recognition errors on the pre-cision of document retrieval and found that this taskcan be somewhat robust to 25% word-error rates forqueries of 2?8 words.Two recent systems are worthy of special men-tion.
First, Google Labs deployed a speaker-in-dependent system in late 2001 as a demo of atelephone-interface to its popular search engine.
(Itis still live as of April 2003.)
Second, Chang andothers (2002a; 2002b) have implemented systemsfor the Pocket PC that interpret queries spoken inEnglish or Chinese.
This last group appears to be atthe forefront of current research in spoken interfacesfor document retrieval.None of the above are question-answering sys-tems; they boil utterances down to strings of key-words, discarding any other information, and returnonly lists of matching documents.
To our knowledgeautomatic answering of spoken natural-languagequestions has not previously been attempted.3 System overviewOur demonstration system has three components: acommercial speaker-dependent dictation system, apredictive interface for typing or correcting natural-language questions, and a Web-based open-domainquestion-answering engine.
We describe these inturn.3.1 Speech recognizerThe dictation system is Dragon NaturallySpeaking6.1, whose language models we have customizedto a large corpus of questions.
We performed testswith a head-mounted microphone in a relativelyquiet acoustic environment.
(The Dragon AudioSetup Wizard identified the signal-to-noise ratio as22 dBs.)
We tested a male native speaker of En-glish and a female non-native speaker, requestingeach first to train the acoustic models with 5?10 min-utes of software-prompted dictation.We also trained the language models by present-ing the Vocabulary Wizard the corpus of 280,000questions described in (Schofield, 2003), of whichTable 1 contains a random sample.
The primaryfunction of this training feature in NaturallySpeak-ing is to add new words to the lexicon; the natureof the other adaptations is not clearly documented.New 2-grams and 3-grams also appear to be iden-tified, which one would expect to reduce the word-error rate by increasing the ?hit rate?
over the 30?50% of 3-grams in a new text for which a languagemodel typically has explicit frequency estimates.3.2 Predictive typing interfaceWe have designed a predictive typing interfacewhose purpose is to save keystrokes and time in edit-ing misrecognitions.
Such an interface is particu-larly applicable in a mobile context, in which textentry is slow and circumstances may prohibit speechaltogether.We fitted a 3-gram language model to the samecorpus as above using the CMU?Cambridge SLMToolkit (Clarkson and Rosenfeld, 1997).
The inter-face in our demo is a thin JavaScript client accessiblefrom a Web browser that intercepts each keystrokeand performs a CGI request for an updated list ofpredictions.
The predictions themselves appear ashyperlinks that modify the question when clicked.Figure 1 shows a screen-shot.3.3 Question-answering systemThe AnswerBus system (Zheng, 2002) has been run-ning on the Web since November 2001.
It servesthousands of users every day.
The original enginewas not designed for a spoken interface, and we haverecently made modifications in two respects.
We de-scribe these in turn.
Later we propose other modifi-cations that we believe would increase robustness toa speech interface.SpeedThe original engine took several seconds to an-swer each question, which may be too slow in a spo-ken interface or on a mobile device after factoringin the additional computational overhead of decod-ing the speech and the longer latency in mobile datanetworks.
We have now implemented a multi-levelcaching system to increase speed.Our cache system currently contains two levels.The first is a cache of recently asked questions.
Ifa question has been asked within a certain periodof time the system will fetch the answers directlyTable 1: A random sample of questions from the cor-pus.How many people take ibuprofenWhat are some work rulesDoes GE sell auto insuranceThe roxana video diazWhat is the shortest day of the yearWhere Can I find Frog T-ShirtsWhere can I find cheats for Soul Reaverfor the PCHow can I plug my electric blanket in tomy car cigarette lighterHow can I take home videos and put themon my computerWhat are squamous epithelial cellsfrom the cache.
The second level is a cache of semi-structured Web documents.
If a Web document is inthe cache and has not expired the system will use itinstead of connecting to the remote site.
By ?semi-structured?
we mean that we cache semi-parsed sen-tences rather than the original HTML document.
Wewill discuss some technical issues, like how and howoften to update the cache and how to use hash tablesfor fast access, in another paper.OutputThe original engine provided a list of sentences ashyperlinks to the source documents.
This is conve-nient for Web users but should be transformed forspoken output.
It now offers plain text as an alterna-tive to HTML for output.
1We have also made some cosmetic modificationsfor small-screen devices like shrinking the largelogo.4 EvaluationWe evaluated the accuracy of the system subjectto spoken input using 200 test questions from theTREC 2002 QA track (Voorhees, 2002).
AnswerBusreturns snippets from Web pages containing pos-sible answers; we compared these with the refer-1See http://www.answerbus.com/voice/Figure 1: The interface for rapidly typingquestions and correcting mistranscriptions fromspeech.
Available at speech.ftw.at/?ejs/answerbusTable 2: % of questions answered correctly fromperfect text versus misrecognized speech.Speaker 1 Speaker 2Misrecognized speech 39% 26%Verbatim typing 58% 60%ence answers used in the TREC competition, over-riding about 5 negative judgments when we feltthe answers were satisfactory but absent from theTREC scorecard.
For each of these 200 questionswe passed two strings to the AnswerBus engine,one typed verbatim, the other transcribed from thespeech of one of the people described above.
Theresults are in Tables 2 and 3.5 DiscussionWe currently perform no automatic checking or cor-rection of spelling and no morphological stemmingTable 3: # of answers degraded or improved by thedodgy input.Speaker 1 Speaker 2Degraded 12 34Improved 5 0of words in the questions.
Table 3 indicates thatthese features would improve robustness to errorsin speech recognition.
We now make some specificpoints regarding homographs, which are typicallytroublesome for speech recognizers.
QA systemscould relatively easily compensate for confusion intwo common classes of homograph:?
plural nouns ending ?s versus possessive nounsending ?
?s or ?s?.
Our system answered Q39Where is Devil?s tower?, but not the transcribedquestion Where is Devils tower??
written numbers versus numerals.
Our systemcould not answer What is slang for a 5 dol-lar bill?
although it could answer Q92 Whatis slang for a five dollar bill?.More extensive ?query expansion?
using syn-onyms or other orthographic forms would be trickierto implement but could also improve recall.
For ex-ample, Q245 What city in Australia has rain forests?it answered correctly, but the transcription What cityin Australia has rainforests (without a space), got noanswers.
Another example: Q35 Who won the No-bel Peace Prize in 1992?
got no answers, whereasWho was the winner .
.
.
?
would have found the rightanswer.6 ConclusionThis paper has described a multimodal interface to aquestion-answering system designed for rapid inputof questions and correction of speech recognitionerrors.
The interface for this demo is Web-based,but should scale to mobile devices.
We described asmall evaluation of the system?s accuracy given raw(uncorrected) transcribed questions from two speak-ers, which indicates that speech can be used for au-tomatic question-answering, but that an interface forcorrecting misrecognitions is probably necessary foracceptable accuracy.In the future we will continue tightening the inte-gration of the components of the system and port theinterface to phones and Palm or Pocket PC devices.AcknowledgementsThe authors would like to thank Stefan Ru?ger forhis suggestions and moral support.
Ed Schofield?sresearch is supported by a Marie Curie Fellowshipof the European Commission.ReferencesJ.
Allan.
2001.
Perspectives on information retrieval andspeech.
Lect.
Notes in Comp.
Sci., 2273:1.E.
Chang, Helen Meng, Yuk-chi Li, and Tien-ying Fung.2002a.
Efficient web search on mobile devices withmulti-modal input and intelligent text summarization.In The 11th Int.
WWW Conference, May.E.
Chang, F. Seide, H.M. Meng, Z. Chen, S. Yu, and Y.C.Li.
2002b.
A system for spoken query informationretrieval on mobile devices.
IEEE Trans.
Speech andAudio Processing, 10(8):531?541, nov.P.
R. Clarkson and R. Rosenfeld.
1997.
Statistical lan-guage modeling using the CMU?Cambridge toolkit.In Proc.
ESCA Eurospeech 1997.N.
Colineau and A. Halber.
1999.
A hybrid approach tospoken query processing in document retrieval system.In Proc.
ESCA Workshop on Accessing Information InSpoken Audio, pages 31?36.F.
Crestani.
2002.
Spoken query processing for interac-tive information retrieval.
Data & Knowledge Engi-neering, 41(1):105?124, apr.J.
Barnett et al 1997.
Experiments in spoken queries fordocument retrieval.
In Proc.
Eurospeech ?97, pages1323?1326, Rhodes, Greece.J.
S. Garofolo, C. G. P. Auzanne, and E. M. Voorhees.2000.
The TREC spoken document retrieval track: Asuccess story.
In Proc.
Content-Based Multimedia In-formation Access Conf., apr.J.
Kupiec, D. Kimber, and V. Balasubramanian.
1994.Speech-based retrieval using semantic co-occurrencefiltering.
In Proc.
ARPA Human Lang.
Tech.
Work-shop, Plainsboro, NJ, mar.E.
Schofield and G. Kubin.
2002.
On interfaces for mo-bile information retrieval.
In Proc.
4th Int.
Symp.
Hu-man Computer Interaction with Mobile Devices, pages383?387, sep.E.
Schofield.
2003.
Language models for questions.
InProc.
EACL Workshop on Language Modeling for TextEntry Methods, apr.E.M.
Voorhees.
2002.
Overview of the trec 2002 ques-tion answering track.
In The 11th Text Retrieval Conf.
(TREC 2002).
NIST Special Publication: SP 500-251.Z.
Zheng.
2002.
AnswerBus question answering system.In Human Lang.
Tech.
Conf., San Diego, CA., mar.
