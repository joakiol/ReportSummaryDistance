Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 277?280,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics?How was your day??
An affective companion ECA prototypeMarc CavazzaSchool of ComputingTeesside UniversityMiddlesbrough TS1 3BAm.o.cavazza@tees.ac.ukRa?l Santos de la C?maraTelef?nica I+DC/ Emilio Vargas 628043 Madride.rsai@tid.esMarkku TurunenUniversity of TampereKanslerinrinne 1FI-33014mturunen@cs.uta.fiJos?
Rela?o GilTelef?nica I+DC/ Emilio Vargas 628043 Madridjoserg@tid.esJaakko HakulinenUniversity of TampereKanslerinrinne 1FI-33014jh@cs.uta.fiNigel CrookOxford UniversityComputing LaboratoryOxford OX1 3QDnigc@comlab.ox.ac.ukDebora FieldComputer ScienceSheffield UniversitySheffield S1 4DPd.field@shef.ac.ukAbstractThis paper presents a dialogue system inthe form of an ECA that acts as a socia-ble and emotionally intelligent compan-ion for the user.
The system dialogue isnot task-driven but is social conversationin which the user talks about his/her dayat the office.
During conversations thesystem monitors the emotional state ofthe user and uses that information to in-form its dialogue turns.
The system isable to respond to spoken interruptionsby the user, for example, the user can in-terrupt to correct the system.
The systemis already fully implemented and aspectsof actual output will be used to illustrate.1 IntroductionHistorically, Embodied Conversational Agents(ECAs) have been used in research and industrymake information and complex tasks more ac-cessible to customers and users.
With the rise ofnew technologies in affective dialogue systems,we are beginning to see a future in which ECAdialogues are not all task-driven, but some willbe focused on the social aspects of conversation.We envisage the development of ECAs that en-hance the social quality of life of the user, andthat build deep relationships with their users overyears of use.
Our affective companion ECA isour first contribution to this emerging technol-ogy.Our emotionally intelligent dialogue systemrequires a high level of understanding of themeaning of the user turns, and so is quite unlikea ?chatbot?.
Affective aspects of the situation arealso key in influencing the course of the conver-sation.
The system attempts to balance both se-mantic and affective understanding in order tocreate an engaging and enjoyable companion forsocial conversation.2 Scenario descriptionThe system conversation is free-ranging and fol-lows the user?s initiative, rather than being task-driven.
We have for the time being restricted therange of conversation topics to things that hap-pen in the life of someone who works in an of-fice (the ?How was your day??
(HWYD) sce-nario).
The user ?offloads?
about his/her day atwork by chatting to the ECA, and the ECA triesto empathise with and advise the user.
The ECAasks questions to elicit information from the user,makes empathetic and sympathetic comments,and offers advice.The system is able to process long user turns,as well as to generate long system turns (around50 words).
If the user perhaps dislikes whathe/she hears, he/she can interrupt the system bybarging in, and the ECA will respond in a hu-man-like way.
The conversation style is thereforeconsiderably different from traditional task-baseddialogues that consist of short dialogue turns andin which the system takes all the initiative.The dialogue in Table 1 (actual output from auser session) illustrates some of the significantfeatures of the system, including long system277turns, user interruptions and emotional analysisof user input.User1 Worst day ever.
I arrived late be-cause of the traffic.System1 You have my sympathy.
Whathappened next?User2 I missed the meeting on projectresponsibilities; got more workdumped on me.
Just what I don?tneed.
I?m already likely to missthe deadline.
[ECA looks worriedwhen user?s negative tone is de-tected]System2 Sorry to hear that.
Who was at themeeting?User3 Only my team.
They gave me aset of PowerPoint slides from themeeting.
I spent the rest of the dayreading them but my computerkept crashing because of  updates.System3 Oh dear!
It is understandable howyou must be feeling.
It?s right tobe unhappy about such a bad situ-ation.
Try not to worry aboutmissing the deadline because ofthe technical problems.
[At thispoint user interrupts ECA bybeginning turn User4] It?s sad thatyour motivation could be affectedby them.
[System stops speaking]User4 No, I?m not worried about thetechnical problems, I?m worriedabout the amount of extra work onthis project and the deadline.System4 What is the project about?Table 1: Dialogue exampleA video demonstration of the system in actionis available at:http://www.youtube.com/watch?v=BmDMNguQUmM3 ArchitectureFigure 1 shows a screen shot taken at run-time ofactual system output.
The ECA is represented ona screen as a woman (waist up) who displaysnatural, human-like movements and performs awide range of complex facial expressions, bodilymovements, and hand and arm gestures.The screen also displays a transcript of theuser and system turns.
The user turns shown con-stitute the output of the Automatic Speech Rec-ogniser (ASR).
The system?s analysis of theuser?s emotional state is also shown.The right-most panel of the screen showsgraphics which convey real-time informationabout how the dialogue is being processed.
Itpresents a streamlined view of the softwaremodules that comprise the system.
Module activ-ity is visually represented at run-time by flashingcolours.
This ?glass-box?
approach enables de-tailed observation and analysis of systemprocedure at run-time.The system comprises a number of distinctmodules that are connected using Inamode, ahub-based message-passing framework usingXML formatted messages over plain text sock-ets.The system?s ASR is the NuanceTM dictationengine.
This is run in parallel with our own a-coustic analysis pipeline which extracts low level(pitch, tone) speech features and also high-levelfeatures such as emotional characteristics.Analysis of the emotions is currently carried outFigure 1: Screenshot of the prototype interface278by EmoVoice (Vogt et al (2008)).
The ASRoutput strings are analysed for sentiment by theAFFECTiS system (Moilanen and Pulman (2007,2009)) and classed as positive, neutral, or nega-tive.
This output is fused with the output fromEmoVoice to generate a value that represents theuser?s current emotional state, which is ex-pressed as a valence+arousal pairing (with fivepossible values).The ASR output goes to our own Natural Lan-guage Understanding (NLU) module which per-forms syntactic and semantic analysis of userutterances and derives noun phrases and verbgroups and associated arguments.
Events rele-vant to the scenario (e.g., promotions, redundan-cies, meetings, arguments, etc.)
are recognisedby the NLU and are used to populate an ontology(a model of the conversation content).
The sys-tem is currently able to recognize and respond tomore than 30 event types.The events recognised in a user turn arelabelled with the output of the Emotion Modulefor that turn; the result is a representation of boththe semantic and affective information that theuser might be trying to convey.Our own rule-based Dialogue Manager (DM)takes the affect-annotated semantic output of theNLU, and from that and its model of the conver-sation content determines the next system turn.
Itwill either ask a question about the events thatoccurred in the user?s day, express an opinion onthe events already described, or make empatheticcomments.
Whenever the system has gained suf-ficient understanding of a key event in the user?sday, it generates a complex long turn that encap-sulates comfort, opinion, warnings and advice tothe user.These long system turns are generated by ourown plan-based Affective Strategy Module thatmakes an appraisal of the user?s situation andgenerates an appropriate emotional strategy(Cavazza et al (2010)).
This strategy?expressedas an abstract, conceptual representation?is han-ded to our own Natural Language Generator(NLG) that maps it into a series of linguistic sur-face forms (usually 4 or 5 sentences).
We use astyle-controllable system using Tree-FurcatingGrammars (an extension of the Tree-AdjoiningGrammars formalism (Joshi et al (1997)).
Thisensures the generation of a large set of differentsurface forms from the same semantic input.The output of the NLG is passed to a modulethat adds this information to its system turninstructions for the ECA.
The ECA has been de-veloped around the HaptekTM toolkit and is con-trolled using an FML-like language (afterHern?ndez et al (2008)).
This 2-D embodimentproduces gestures, facial expressions, and bodymovements that convey the emotional state ofthe ECA.
Its movements and expressions enableit to visually display interest and enjoyment intalking to the user, and to display empathy withthe user.
The speech synthesis module is our ownemotion-focused extension of the LoquendoTMTTS system.
It includes paralinguistic elementssuch as exclamations and laughter, and emo-tional prosody generation for negative and posi-tive utterances.4 Special procedural featuresA significant processing design feature of thesystem is that there are two main processingloops from user input to system output; a ?longloop?
which passes through all the componentsof the system; and a ?short loop?
or ?feedbackloop?
which will now be discussed (the proce-dure already described in Section 3 is the longloop procedure).4.1 Feedback loopThe feedback loop (?short loop?)
bypasses manylinguistic components and generates immediatereactions to user activity.
The main function ofthe short loop is maintain user engagement bypreventing unnaturally long gaps of ECA inactiv-ity.
The feedback loop engages the acousticanalysis components, the TTS, and the ECA.
It isresponsible for the generation of real-time (< 500ms) reactions in the ECA in response to the emo-tional state of the user.
It attempts to align  bothverbal behaviour (backchannelling) and non-verbal behaviour (facial expressions, gestures,and general body language) to the emotions de-tected during most recent user turn.
In order toachieve a reasonable level of realism, these sys-tem reactions to the perceived emotional state ofthe user need to be perceptibly instantaneous.Using this short feedback loop that bypassesmany of the linguistic components ensures this.The feedback loop is also occasionally used tomake sympathetic comments immediately afterthe user stops speaking.
These act as acknowl-edgements of the emotion expressed by the user.An example can be seen in the System2 turn ofthe example dialogue in Table 1:1.?Sorry to hear that.
Who was at the meeting?
?Here, the first utterance was spoken by the sys-tem within a few tenths of a second after the end279of the previous user turn (User2).
The systemtried to identify the user?s emotion in the previ-ous turn and then to behave linguistically andvisually in an empathetic way.
The actual sympa-thetic utterance was randomly chosen from a setof ?negative emotion utterances?
(there are also?positive?
and ?neutral?
sets).The second half of the system turn in (1) wasderived by the system?s ?long loop?.
It is a ques-tion which refers to a meeting that the user men-tioned in the previous turn.
This ?meeting?
eventhas been heard by the ASR, understood by theNLU system, remembered by the DM, and isnow referred to by an appropriate definite nounphrase in the output of the NLG.The feedback and main loops run in parallel.However, the feedback loop generates its speechoutput almost immediately, giving time for themain dialogue loop to complete its more detailedanalysis of the user?s utterance.4.2 Handling user interruptionsThis system has a complex strategy for handlingsituations in which the user interrupts longsystem turns.
The system?s response to ?barge-in?
user interruptions is overseen by the Interrup-tion Manager (IM), which is alerted by theacoustic input modules whenever a genuine userinterruption (as opposed to, say, a backchannel)is detected during a long system utterance.
Whenalerted, the IM instructs the ECA to stop speak-ing when it reaches a natural stopping point in itscurrent turn (usually the end of the currentphrase).
The user?s interruption utterance isprocessed by the long loop.
Its progress istracked and controlled by the IM, for example, itmakes sure that the linguistic modules know thatthe current utterance is an interruption, whicmeans it requires special treatment.
The DM hasa range of strategies for system recoveries fromuser interruptions, including different ways ofcontinuing, replanning, and aborting.
An exam-ple of a user interruption is shown in Table 1.The user interrupts the long system utterance inthe System3 turn.
The system?s response to theinterruption is to stop the speech output from theECA, abort the long system turn altogether, andinstead to ask for more details about the projectthat the user has just mentioned during the inter-ruption.
(See (Crook et al (2010))  for a moredetailed description of the IM.
)AcknowledgementsThis work was funded by Companions, a Eu-ropean Commission Sixth Framework Pro-gramme Information Society Technologies Inte-grated Project (IST-34434).We would also like to thank the followingpeople for their valuable contributions to thework presented here: Stephen Pulman, RamonGranell, and Simon Dobnick (Oxford Univer-sity), Johan Boye (KTH Stockholm), CameronSmith and Daniel Charlton (Teesside Univer-sity), Roger Moore, WeiWei Cheng and Lei Ye(University of Sheffield), Morena Danieli andEnrico Zovato (Loquendo).ReferencesCavazza, M., Smith, C., Charlton, D., Crook, N.,Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-tos de la Camara, R., Turunen, M. 2010 PersuasiveDialogue based on a Narrative Theory: an ECAImplementation, Proc.
of the 5th Int.
Conf.
on Per-suasive Technology (Persuasive 2010), to appear2010.Crook, N., Smith, C., Cavazza, M., Pulman, S.,Moore, R., and Boye, J.
2010 Handling User Inter-ruptions in an Embodied Conversational Agent Inproc.
of AAMAS 2010.Hern?ndez, A., L?pez, B., Pardo, D., Santos, R.,Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C.
(2008) Modular definition of multimodal ECAcommunication acts to improve dialogue robust-ness and depth of intention.
In: Heylen, D., Kopp,S., Marsella, S., Pelachaud, C., and Vilhj?lmsson,H.
(Eds.
), AAMAS 2008 Workshop on FunctionalMarkup Language.Joshi, A.K.
& Schabes, Y.
(1997) Tree-adjoiningGrammars.
Handbook of formal languages, vol.
3:Beyond Words, Springer-Verlag New York, Inc.,New York, NY, 1997.Moilanen, K. and Pulman.
S. (2009).
Multi-entitySentiment Scoring.
Proc.
Recent Advances inNatural Language Processing (RANLP 2009).September 14-16, Borovets, Bulgaria.
pp.
258--263.Moilanen, K. and Pulman.
S. (2007).
Sentiment Com-position.
Proc.
Recent Advances in Natural Lan-guage Processing (RANLP 2007).
September 27-29, Borovets, Bulgaria.
pp.
378--382.Vogt, T., Andr?, E. and Bee, N. 2008.
EmoVoice ?
Aframework for online recognition of emotionsfrom voice.
Proc.
Workshop on Perception andInteractive Technologies for Speech-Based Sys-tems, Springer, Kloster Irsee, Germany, (June2008).280
