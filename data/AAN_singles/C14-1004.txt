Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 25?36, Dublin, Ireland, August 23-29 2014.A Context-Aware NLP Approach For Noteworthiness Detection inCellphone ConversationsFrancesca Bonin?Trinity College DublinDublin, Irelandboninf@tcd.ieJose San PedroTelefonica ResearchBarcelona, Spainjspw@tid.esNuria OliverTelefonica ResearchBarcelona, Spainnuriao@tid.esAbstractThis papers presents a context-aware NLP approach to automatically detect noteworthy infor-mation in spontaneous mobile phone conversations.
The proposed method uses a supervisedmodeling strategy which considers both features from the content of the conversation as wellas contextual information from the call.
We empirically analyze the predictive performance offeatures of different nature on a corpus of mobile phone conversations.
The results of this studyreveal that the context of the conversation plays a crucial role on boosting the predictive perfor-mance of the model.1 IntroductionMore than 6 billion people worldwide use their cellphones daily for a variety of purposes: contactingcolleagues, relatives or friends, doing business, getting help in emergency situations, etc.
Previous work(Carrascal et al., 2012) has shown that almost 40% of users frequently feel the need to recall bits ofinformation from their phone conversations and that 27% of the users consider the recall task to bedifficult, mainly because taking notes during a mobile phone call is not always possible (e.g.
hands notfree, lack of time or devices for note-taking).
In a related user study, Cycyl et al.
reveal that users areoften engaged in concurrent tasks during mobile phone conversations (e.g.
walking, jogging, driving,cooking, etc), which makes taking notes an unfeasible task (Cycil et al., 2013).In this setting, information extraction techniques could be applied to automatically detect noteworthyinformation from mobile phone conversations.
Related studies have focused on detecting noteworthinessfrom meeting transcripts (Banerjee and Rudnicky, 2009).
However, very little work has been done todate to identify this kind of information in other types of human communication, such as spontaneousphone conversations.In this paper, we present a data-driven information extraction approach aimed at automatically detect-ing fragments of phone conversations worth annotating for future recall, i.e.
noteworthy.
These call notescould then be presented to the users to enable fast browsing of their conversation history, and leveragedto design efficient information interaction techniques for supporting smart user interfaces.Given the particular characteristics of mobile phone calls, detecting noteworthiness in them is chal-lenging at many levels.
First, the audio is captured in a natural environment rather than in controlledsettings, which results in noisy signals, and consequently in noisy transcriptions.
Second, the conversa-tions are highly fragmented due to their spontaneous nature.
Finally, at a conceptual level, judging whichpieces of information are noteworthy is a very subjective task, as emerged in (Banerjee and Rudnicky,2009), who investigated the feasibility of the task by conducting a Wizard of Oz-based user study.Our noteworthiness modeling approach considers a supervised learning paradigm which takes intoaccount two types of information: (1) Contextual information both from the call (where, when, to whom,.
.
. )
and the users (gender, age, .
.
.
); and (2) Content information of the conversation.
The combination?
* The work was conducted while the author was intern at Telefonica Research, Barcelona, Spain.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/25of both sources of information enhances the flexibility of the model to accurately predict noteworthinessin different use scenarios.The main contributions of this paper are:i) We propose and evaluate a supervised machine learning model to automatically detect notewor-thy segments of phone conversations.
Our approach adopts a hybrid strategy to model conversationsexploiting both content and context-related information.ii) We propose a new set of content and context-based features specifically designed to detect note-worthy information in our corpus of real-world cellphone conversations, and compare their effectivenessiii) We provide a discussion of the results, derived from our quantitative and qualitative analyses.The paper is structured as follows.
Relevant previous work is presented in Section 2.
Section 3describes the corpus of phone conversations and the annotations provided by the participants.
In Section 4we describe in-depth the extracted features.
Our experimental validation and results are presented inSection 5.
Finally, Section 6 summarizes our findings and highlights some lines of future research.2 Related workNoteworthiness detection in conversations can be considered to be a particular form of summarization:the aim is to summarize the conversation by keeping only the relevant pieces of information that theuser would like to refer to at a later time.
Although related, the main distinction between automaticsummarization and detection of noteworthy information lays in the notion of relevance.
The former aimsat generating a comprehensive record of the conversation, while the latter considers only fragments worthregistering for future recall.Considerable research activity has recently been devoted to automatic text and speech summarization(Maskey and Hirschberg, 2003).
Many approaches have been proposed in the literature, including cluster(Zhang et al., 2005) and graph-based methods (Garg et al., 2009; Wang and Liu, 2011) and machinelearning techniques (Jian Zhang et al., 2007; Maskey and Hirschberg, 2006; Galley, 2006), where thetask is tackled as a binary classification problem considering whether the sentence is a good candidatefor a summary or not.
In addition, different types of features have been used, including lexical, acousticand structural characteristics (Xie et al., 2008; Maskey and Hirschberg, 2005).
Recent works have beenfocused on adapting summarization to the social context, exploiting user generated contents associatedwith the documents (Yang et al., 2011; Hu et al., 2012).
Implicit and explicit community feedback inonline collaborative websites have also been leveraged to detect highlights of media assets (San Pedro etal., 2009).However, few studies have focused on noteworthiness detection.
Banerjee et al.
investigate the fea-sibility of discovering noteworthy pieces of information in meetings by means of a Wizard of Oz-baseduser study where a human suggested notes to meeting participants during the meeting.
The authors foundthat the human annotator obtained a precision of 35% and a recall of 41.5%.
In the same work, Baner-jee et al.
reports a low inter annotator agreement (IAA) in noteworthiness discovery.
In a related work?probably the most relevant prior-art to our work, the authors apply extractive meeting summarizationtechniques to automatically detect noteworthy utterances in meetings (Banerjee and Rudnicky, 2008).They train a Decision Tree classifier over a collection of 5 meetings, obtaining an F-score of 0.14.
Thisresult highlights the difficulty of the task at hand and motivates to explore alternative approaches.To overcome the difficulties posed by this task we propose two main contributions: 1) the use ofnovel features engineered ad-hoc for this task, and 2) the use of contextual information.
While theformer adapts the document representation to the specific problem setting, the latter allows to enhance therepresentation with orthogonal information which many times provides a higher discriminative power.This approach has been used successfully in related fields; for instance, in information retrieval tasksrich multimodal queries have been shown to effectively boost the retrieval performance compared topure textual queries (Yeh et al., 2011).263 Corpus CollectionWe used a corpus of cellphone conversations collected in a previous study (Carrascal et al., 2012).
Inthis study, a large sample of mobile phone conversations was recorded, semi-automatically transcribed1and manually annotated for relevance by their participants.
Over 64 days, 796 mobile phone conversa-tions from 62 volunteering subjects (20 female) were recorded.
All the participants were Spanish nativespeakers, and the conversations were recorded and transcribed in Spanish.
Metadata about the call (e.g.duration, date, time) was also stored along with the actual conversation and its transcript.
More detailsabout the corpus collection process can be found in (Carrascal et al., 2012).All the participants were first asked to fill out a pre-study questionnaire where they provided somepersonal information, including gender, marital status, education and income.
Then they were asked toannotate what parts of their calls that they would like to take a note of: i.e.
noteworthy fragments ofconversations.
To this end, participants used a Web-based interface that gave them access to their callsand allowed them to highlight with the mouse the parts of the transcript that they considered to be worthkeeping for future reference.We used these annotations as the ground truth for the studies presented in this paper, considering themas the ideal noteworthy parts of the calls.
For privacy reasons, due to the sensible nature of the data(i.e.
private phone conversations) we could not consider alternative ground truth generation schemes, forinstance collecting annotations from users other than the callers themselves.2Finally, the participants were asked to fill out a questionnaire after annotating each call, which wasused to collect contextual information, including: location of the call (i.e at work, at home, while com-muting, while doing shopping, while exercising), and category of the call (i.e.
discuss a topic, taking anappointment, give/receive information, asking a favor, social).3.1 Characteristics of the CorpusThe original conversation collection consists of a total of 796 conversations, of an average length of 178seconds (s = 384 sec.).
We pre-filtered this original set to exclude calls with problems in the transcript(e.g.
empty transcript, only one speaker audible, etc).
Out of the entire corpus we finally selected 659conversations.
We denote this subset of the corpus as the G dataset.
The G dataset comprises 22, 474turns, with an average of 34.10 (s = 45) turns per conversation.
From these, only 671 are annotated asbeing noteworthy (2.98%), which represent an average of 1.02 turns (s = 1.803) per call.
Given thatthe vast majority of turns (97.2%) are not annotated, this can be considered a highly unbalanced dataset,which makes the automatic modeling problem more challenging.Hence, we considered a second dataset which included only the 295 calls from the G dataset containingat least one annotation.
This second subset, denoted as A amounts for approximately 45% of the Gdataset.
The A dataset features 10, 642 turns, with an average of 36.07 (s = 33) turns per conversation.From these, again 671 (6.3%) are annotated, which represent an average of 2.275 (s = 2.09) per call.The A dataset is still highly unbalanced but significantly less than the G dataset.
Table 1 summarizes thehigh level characteristics of each dataset.Turns Annotated Turns# Calls Total avg.
per call Total FractionG 659 22, 474 34.1 (s = 45) 671 2.9%A 295 10, 642 36 (s = 33) 671 6.3%Table 1: General statistics on G andA datasets.Class AnnotationsI We are in front of the fruit shopRoA Tomorrow we go to look for the swimsuitRI Are you coming to eat?
At what timeO Sure, it?s normalTable 2: Examples of annotations.Given the complexity of the modeling problem, we studied the note taking behavior of participantsto identify relevant patterns that would simplify the problem.
To this end, we conducted a quantitativeanalysis of the note taking behavior of participants.
We found that users tend to highlight complete1Participants were given the opportunity to revise transcriptions during the annotation phase.2Receivers of the calls were aware of the study and were given the possibility to not participate in the call, but were notdirected involved in the study.27turns as relevant, instead of parts of the turns.
On average, 66.57% (s = 35.87) of the words within anannotated turn are highlighted, with a median value of 80%.
Hence, we decided to use turns ?rather thanindividual words?
as the unit to be automatically detected as noteworthy.
Using this approach, a turn isconsidered to be noteworthy if it contains at least one annotated word.3.2 Qualitative Analysis of the CorpusSince our aim is to detect the noteworthy turns within a call, we conducted a preliminary qualitativeanalysis to understand the nature of the annotations entered by the participants in the study.
We distin-guished 4 types of annotations: Giving Information (I), Requesting Information (RI), Reporting on anAction (RoA) and Other (O).
Examples of these 4 types of annotations are presented in Table 2.
Wecollected annotations from three collaborators of our lab for a total of 54 randomly selected turns fromthe A dataset (IAA, Fleiss Kappa = 0.54 (Fleiss, 1971)).We found that 47% of the turns were classified as belonging to the Giving Information category, 22%of the turns to the Request Information category, 26% to the Other category, and only 3% were classifiedas Report on an Action.
Intuitively, we had expected the Giving Information category to be the mostcommon in the annotated turns.
However, the results obtained show that the other types of annotationsare also well represented in the data.Two main interesting aspects emerge.
First, while the vast majority of annotations correspond to turnswhere a piece of information is given (e.g.
We meet at 3pm), turns where information is requested arealso well represented in the sample.
There are plausible explanations for this behavior, such as userstrying to include more context in the annotations.
Second, more than 25% of this manually annotateddataset was marked under the Other category, which includes turns with very diverse functionalities(e.g.
greetings, statements of agreement).
This reveals that participants tend to annotate turns with verydiverse functional aspects, which poses a challenge to be added to the unbalanced nature of the dataset.4 Feature ExtractionWe follow a supervised machine learning approach to automatically detect noteworthy turns in conver-sations.
In this section we describe the features that we compute to represent conversations and whichhave been engineered to capture information relevant to the problem at hand.
We have divided the setof features into two categories: Content features, that we denote with the letter C, and conteXt features,that we denote with the letter X.4.1 Content FeaturesContent features are computed by analyzing the content of the conversations.
We use as input the textualinformation resulting from the semi-automatic transcription of the calls.
Note that we do not make use ofany conversational acoustic information.
While the analysis of the acoustic signal may reveal additionalcues useful for noteworthiness detection, it lies out of the scope of this work.In order to extract features from the transcript, we first pre-proces the datasets (split in turns, lemma-tized, PoS tagged).
Also, we extract and classify Named Entities (NEs).3We extract 42 content-basedfeatures which include both variations of features previously used in the meeting summarization litera-ture and novel features particularly adapted to our task.
However, in contrast to related work on meetingsummarization, we do not extract content features based on lexical similarity to the entire call or tothe main topic of the call, under the intuition that the notion of noteworthiness depends on the user?sneeds rather than on the main topic of the conversation.
In addition and for robusteness purposes, wedecided not to rely on long distance dependency information (e.g.
argument predicate relations) or deepsyntactical parsing, which are sensitive to the quality of the transcription.The resulting features are grouped into three main classes: Turn-Based (C-T), Dynamic (C-D), andConversational (C-C).
We compare them with a pure bag-of-words (BoW) representation.
Table 3aprovides a summary of all the content-based features used in our system.
Where applicable, we experi-3All pre-processing was performed using the Freeling Language Processing tools (Padro et al., 2010).28ment with two vector representations: binary and frequency-based.
We will refer to these two differentencoding schemes as Bin for the binary case, and Freq for the frequency case.CONTENT FEATURESC-BoW (Bag of Words)BoWBoW for all words (except hapax)C-T (Turn-based)NEPresence (or frequency) of NEs (Person, Location, Organization,Numbers, Dates, Misc.
)TLNTurn length in # words normalizedPoSPoS distributionTFMax and Mean term frequencyIDFMax and Mean inverse document frequencyC-D (Dynamic)RepRepetition between t and t-1,t+1,t-2,t+2IntPresence (or total amount) of Int.
pro./adj.
in t-1QPresence (or total amount) of question in t-1C-C (Conversational)DurDuration of the call (# turns and # words)CentConversation centralitySpkSpeakerDomSpeaker dominance(a) Content FeatureCONTEXT FEATURESX-C (Call-based)X-C-TTime of the callX-C-LocLocation of the callX-C-DayDay of the callX-C-ObjObjective of the callX-U (User-based)X-U-GGenderX-U-AAgeX-U -IIncomeX-U-EEducationX-U-MsMarital Status(b) Context FeatureTable 3: Content (a) and Context (b) based features.4.1.1 Turn-Based Content features (C-T)Turn-based content features take into account information related to individual turns.
We distinguishlexical and non-lexical C-T.Lexical content features: Lexical C-T features capture the lexical properties of a turn.
We includeNEs, such as Locations, Organizations, Persons, Miscs and Numbers, Dates, and temporal expressions.For each turn t, we detect the presence of any NE as well as the presence of individual classes of NEs.For each of these class of entities, we extract both a binary and a frequency feature vector.
In thetext summarization literature, the appearance of particular lexical phrases (e.g.
to summarize) has beenexploited to predict relevant sentences (Gupta and Lehal, 2010).
In our study, attention has been givento the presence of temporal expressions under the intuition that temporal cues are good indicators ofupcoming pieces of information (e.g.
The meeting is tomorrow).
We exploit temporal expressions, suchas today, tomorrow, etc.4Non-lexical content features: capture characteristics of the turn which do not involve lexical infor-mation, namely: turn length, Part-of-Speech (PoS) distributions and Tf-Idf descriptive statistics at theturn level.In meeting summarization, the average length of a turn has been found to be a good feature to automat-ically create a summary of a meeting (Xie et al., 2008).
In our dataset, preliminary analyses revealed thatannotated turns tend to be longer in average.
Hence, we include the turn length in the non-lexical contentfeature set.
The turn length is given by the number of tokens per turn normalized over the average turn4Note that, here and in the remainder of the paper, we report the English translations of the Spanish originals.29length (punctuation excluded).
To further gauge discourse characteristics, we detect the distribution ofPoS at the turn level: i.e.
for each turn, the frequency of nouns, pronouns, adjectives, adverbs, interjec-tions, verbs, prepositions and conjunctions is calculated.
Finally, we compute the term frequency (Tf)and inverse document frequency (Idf) measures.
In (Xie et al., 2008), authors report that Idf is amongthe most discriminative features in sentence selection for text summarization.
We compute maximumand mean Tf and Idf values for each turn.4.1.2 Dynamic content features (C-D)Dynamic content features are designed to capture the semantic relationships between each turn and itsprecedent and subsequent turns.
In particular we refer to relations such as lexical and topical cohesion,question-answer relationship, and the appearance of general cues that may anticipate relevant bits ofinformation in the subsequent turn.
We consider: 1) the lexical and topical cohesion among consecutiveturns (Repetitions); 2) the appearance of general cues that may anticipate relevant bits of informationin the subsequent turn (Interrogative Pronouns); 3) the question-answer relationship among consecutiveturns (Question).Repetitions: words repeated by different speakers in consecutive turns.
Participants of a conversationtend to align at several linguistic and paralinguistic levels in order to ease communication and increasemutual understanding (Pickering and Ferreira, 2008).
This phenomenon has been investigated in termsof prosody, lexicon and syntax (Levitan and Hirschberg, 2011; Brennan, 1996; Bonin et al., 2013; Brani-gan et al., 2010).
From a lexical point of view, the alignment mechanism, often referred to as priming, isrealized by means of word repetitions among speakers.
Many studies have investigated this phenomenonassessing correlation between priming and mutual understanding or dialogue success (Vogel, 2013; Re-itter and Moore, 2007).We exploit the priming phenomenon to detect concepts in the conversation that are considered impor-tant by both participants, relying on the fact that repeated words convey concepts that participants wantto make sure they have been successfully communicated to their interlocutor.
Given a dataset D, a turnin D, t ?
D, and t ?
i and t + i turns in the context of t, we calculate the amount of repeated lemmasbetween t and t?i, and t and t+i for 1?
i ?
2.
In order to consider semantically meaningful repetitions,we take into account only content words (nouns, adjectives, adverbs, verbs) when they activate one of theC-T features described above.
Being A the set of annotated turns, we noticed a significant difference inthe amount of repeated lemmas between t,t?
i for t ?
A rather than for t /?
A.
Find below an exampleof consecutive turns with repetitions:Turn Utterancet-1: Starting at half past four.t: Starting at half past four, yes.Interrogative pronouns and questions: We also exploit indicators of an upcoming giving infor-mation act.
As shown in Sec 3.2, 47% of the annotations were marked as giving information, whichmay have been triggered by a request of information in the precedent turn.
Hence, in order to capturethese cases, we identify linguistic elements that indicate a request of information in t?
1 (questions andinterrogative pronouns/adjectives).4.1.3 Conversational flow features (C-C)They are designed to model information about the conversation?s flow and speakers?
interaction.Centrality of the turn: Distance of a turn from the center of the conversation.
This feature is inspiredby the sentence location features used in text summarization (Chen et al., 2002).
Chen et al.
assigndifferent weights to sentences in the first, middle and final part of a paragraph, in order to favor sentencesthat are in the central part of the paragraph as they are considered to be more informative for a summary.In our corpus, we noticed the tendency of users to annotate turns that are in the central portion of theconversation.
Typically the first and the last quarters of the phone conversations are dedicated to socialtalk.
Hence, we introduce a temporal feature, referred to as conversation centrality, that captures thedistance of a turn from the center of the conversation.
This distance is measured in terms of number ofwords, excluding punctuation.30Speaker: Who is uttering the turn (caller vs callee).Conversation duration: Length of the conversation in number of turns and in number of words.
Thenumber of turns captures the dynamics of a dialogue (few longer turn vs a more dynamic exchange),while the number of words captures the overall duration.Speaker dominance: We consider whether the speaker is the dominant speaker of the conversation,defining dominance in terms of amount of productions during the call.
This is calculated by comparingthe number of turns of speaker a vs speaker b, normalized over the total amount of turns per call.4.1.4 Bag-of-Words (BoW)Finally, we explore the performance of a naive bag-of-words scheme to represent the content at the turnlevel.
Given the large vocabulary size of our corpus (10, 144 tokens) and the sparsity organic to bag-of-word representations, we decided to use a trivial dimensionality reduction strategy filtering out the termsthat appear only once in the corpus.
We decided not to apply a stop-list of functional words for furtherreducing the feature space.
This decision was based on the higher discrimintative power we observedwhen comparing classification accuracy with and without them.
We discarded the use of more aggressivefeature selection approaches (e.g.
mutual information) to allow for a fair comparison of accuracy withthe rest of feature representations described in the paper.
In total, our BoW representation had 5, 048dimensions in the G dataset, and 3, 219 dimensions in the A dataset.4.2 Context FeaturesContext features are introduced under the assumption that noteworthy information may depend on thecharacteristics of the user and on the situation in which the call takes place.
For example, people maynot need to annotate pieces of information that are part of their daily lives.
Whereas while taking anappointment, it is plausible the need to annotate the name of the doctor, in a social call with a friend, thename of the friend is part of the background knowledge of the user.
Therefore, while from a content (andan NLP) point of view both names are Person NEs and carry the same amount of information, from thepoint of view of the user they might have different weight (no need of taking note vs need of taking note).Also, the current situation or location of the user may influence the necessity of taking notes: a user in asupermarket will not need to annotate to buy milk, (s)he will rather take it directly from the shelf.
A userdriving to the supermarket will need to keep in his/her mind the need to buy milk for later recall.In line with this, we noted in Section 2 that pure NLP approaches applied to automatically detectingnoteworthy information in meetings are able to achieve an F-score of only 0.14.
This low F-score under-lines the complexity of the task and the limitations of a pure content-based approach.
Contextual cuesmay be used to increase the discriminative power of the classification model.Since we consider the specific scenario of cellphone conversations, we can exploit contextual informa-tion derived from the use of the mobile network, such as geo-location, and temporal information.
Othercontextual features that we use, gathered during the pre-study questionaire, are organically much morechallenging to infer.
We still decided to consider these as a way to assess the potential of several types ofcontextual information with respect to the discriminative power of the classifier.
We distinguish amongCall-based (X-C) and User-based (X-U) contextual features.
A schematic overview of these features isgiven in Table 3b.4.2.1 Call-Based Features (X-C)Call-based features are meant to capture contextual information at the call level.
In particular, X-Cfeatures include information about where, when and for what reason a call is made, under the intuitionthat calls made, for example, during working hours may have different noteworthy information than callsmade in the weekend.
We distinguish six location categories: home, work place, while commuting,while exercising, while shopping, other.
The location of the calls was provided by participants throughthe post-call questionnaire.
However location information is typically available from the mobile network.In terms of temporal features, we consider the actual time of the call (over 24 hours).
In addition, weclassify the time in two classes: working vs non working hours, and the day in also two classes: weekdayvs weekend.
Finally, we also consider the objective of the call as described in Section 3.
Note that,31although this information is not directly accessible from mobile data collected during the call, previousliterature on conversation classification supports the feasibility of inferring this information from thecontent of the conversation (Koc?o et al., 2012).4.2.2 User-Based Features (X-U)Finally, we introduce a set of features that feed the model with information about the user.
We exploitinformation that could be provided by users upon registration to such a note-taking service.
We captureage, gender, educational level, income and marital status.
Gender is represented as a binary feature, whileage is categorized in 5 groups: below 20 years old, between 20 and 30, between 30 and 40, between 40and 50 and above 50.
The education status is represented by the following categories: Primary education,Secondary education, Bachelor degree or a Postgraduate education (Master or PhD).
Yearly income iscategorized by: up to 10k, 20k, 30k, 40k and more than 40k.
Finally, marital status is categorized as:single, in a couple (married, with a stable partner), other.5 ExperimentsThe goal of our system is to automatically identify information annotated by users in terms of its potentialneed for future recall.
We frame this problem as a binary classification task (noteworthy or not) at theturn level.
This task presents two main challenges.
First, our dataset is extremely unbalanced, with lessthan 3% of the corpus labeled as relevant by the participants.
Second, the subjectivity of the task leadsto high variability of annotation behaviours, (see Sec.
3.2).
In this section we describe the experimentalsetting that we used to empirically evaluate the performance of different features sets and present theresults obtained using the ground truth data collected (Section 3) to provide classification performancescores.
In order to fully investigate the predictive performance of the different feature sets, we conductedour experiments using both the entire corpus G, which includes all the selected conversations, and itssubset A, which considers only the calls with at least one annotation.
Both sets are described in Sec.
3.We experimented using both encoding schemes described in Section 4: binary based (Bin) and frequencybased (Freq).We used Support Vector Machines (SVMs) with RBF kernel, as this classification approach yieldedthe most consistent results throughout all the evaluated configurations.
We used the same random split oftraining and test sets for all the experiments, accounting for 70% and 30% of the dataset respectively.
Wetune the hyperparameter C of the SVM model using a 3-fold cross-validation approach on the trainingdata only, where we chose F-score as the quality metric to optimize.
Given the nature of our task, recallis preferred to precision from a user-centric perspective: it is preferable to avoid missing any relevantinformation than to include some non-relevant fragments.
For this reason, we also report precision andrecall values.5.1 Classification ResultsThis section presents the results obtained in our binary classification task (turns being noteworthy or not).We study the performance of different combinations of features and present the results obtained usingonly content information (C), and the combination of content and context information (CX).5.1.1 Content featuresWe present a comparison of the different content feature sets using the naming scheme of Sec.4.
Weconsidered four classification scenarios: C-T only, C-D only, the combination of C-T and C-D (C-TD),and the combination of C-T, C-D and C-C (C-TDC).
The results of these feature sets are shown inTables 4a and 4b for the G and A collections, respectively.As shown in Table 4a, the maximum F-score for the G dataset is achieved for the combination ofall content features included the BoW.
The low score (F = 0.18) is a direct consequence of the lowprecision obtained (P = 0.11).
For the A dataset (Table 4b) we observe a better F-score (F = 0.296),still obtained by the combination of all content features, with a much higher precision (P = 0.18) due tothe significant amount of noise removed by considering only annotated calls.
Note that in both the G andthe A datasets the C-TDC feature set outperforms the pure BoW approach (F = 0.158 vs. F = 0.1432Features Precision Recall F-scoreEncoding Bin Freq Bin Freq Bin FreqBoW 0.081 0.083 0.730 0.720 0.150 0.150C-T 0.087 0.088 0.53 0.32 0.15 0.139C-D 0.03 0.26 0.26 0.12 0.15 0.05C-TD 0.087 0.09 0.754 0.33 0.1505 0.1419C-TDC 0.09 0.093 0.58 0.37 0.158 0.149C-TDC+BoW 0.11 0.11 0.52 0.51 0.18 0.18(a) Results for the G dataset.Features Precision Recall F-scoreEncoding Bin Freq Bin Freq Bin FreqBoW 0.14 0.14 0.54 0.54 0.22 0.22C-T 0.135 0.147 0.56 0.555 0.218 0.23C-D 0.149 0.11 0.24 0.15 0.18 0.12C-TD 0.143 0.143 0.506 0.546 0.223 0.227C-TDC 0.165 0.159 0.626 0.693 0.254 0.267C-TDC+BoW 0.188 0.1659 0.57 0.568 0.283 0.296(b) Results for the A dataset.Table 4: Classification performance of Content features, BoW and their combination.for G and F = 0.267 vs. F = 0.22 for A), using a fraction (about 1%) of the number of BoW features,which leads to a considerably simpler model.
On the other hand, the combination of C-TDC and BoWfeatures improves the results up to F = 0.18 for G (P = 0.11, R = 0.52) and F = 0.296 (P = 0.20,R = 0.57) for the A subset.
This result highlights how the lexical representation comprised by the BoWprovides the model with orthogonal information to the one provided by the C-TDC features set.To the best of our knowledge no previous work has been done in noteworthy detection from telephoneconversations.
For this reason, we report as a reference the results of the more similar prior art to ourwork, (Banerjee and Rudnicky, 2008), where the authors implement an SVM classifier for the detectionof noteworthy information in meetings.5Although aware of the different nature of the dataset, theseresults are reported to get a sense of the potentiality of the system.
The best performance of our modelon the A dataset improves in 15% the F-score of F = 0.14 reported in (Banerjee and Rudnicky, 2008).5.1.2 Combining Content and Context FeaturesIn this section we report the performance of the model trained using both content and context features.For simplicity, in the remainder of this section we refer to the entire set of content features, (C-TDC)as C, to the entire set of context features as X, and to their combination as CX.
When we test addingBoW features the +BoW naming is used.
The results are shown in Table 5 and Figure 1.
We observethat the fusion of content and context features (CX and CX+BoW) provides a noticeable overall increasein the F-score for both datasets.
This increase is particularly high for the G dataset, where the F-scoregets increased by almost a factor of 2, from F = 0.18 to F = 0.28.
On the A dataset, the combinationof content and context features improves the F-score from F = 0.29 to F = 0.32, given by a betterprecision (P = 0.24 vs P = 0.18) with similar recall.Features Precision Recall F-scoreRep.
B F B F B FC+BoW 0.11 0.11 0.52 0.51 0.18 0.18X 0.068 0.068 0.665 0.665 0.124 0.124CX 0.169 0.20 0.38 0.286 0.2354 0.2394CX+BoW 0.189 0.1919 0.524 0.5022 0.288 0.277(a) Results for the G dataset.Features Precision Recall F-scoreRep.
B F B F B FC+BoW 0.188 0.1659 0.57 0.568 0.283 0.296X 0.087 0.087 0.56 0.56 0.15 0.15CX 0.2075 0.212 0.5866 0.595 0.3066 0.3130CX+BoW 0.223 0.2455 0.573 0.426 0.3212 0.3116(b) Results for the A dataset.Table 5: Classification performance using the combination of context and context-based featuresBin Freq0.00.10.20.3X C+BoW CX CX+BoW X C+BoW CX CX+BoWF?score(a) Results for the G dataset.Bin Freq0.00.10.20.3X C+BoW CX CX+BoW X C+BoW CX CX+BoWF?score(b) Results for the A dataset.Figure 1: Classification performance using Content, Context features and their combinationThis result gives empirical evidence that these two sets of features convey complementary information5In their experimental settings all the meetings have at least one annotation as in our A scenario.33that is relevant for the task at hand.
That is, the same words can carry different relevance depending onthe contextual information of the conversation.Note that the BoW features add discriminative information in the G scenario, but have a minimaleffect in the less noisy A scenario where the combination of content and context features, without BoW,provides already an F-score of F = 0.31.An interesting remark about this combined model is that the difference in performance between theG and the A dataset is vastly reduced.
While in the pure content model the difference in F-score valuebetween both datasets was 0.10, in the combined model this difference is just 0.03.
This result showsthat the combination of content and context features boosts considerably the results in the more noisyand realistic dataset G, while its effect is weaker in the cleaner dataset.5.2 Qualitative AnalysisIn order to better understand the failure cases in our system, we carried out a qualitative analysis of bothfalse positives, i.e.
turns annotated by the system but not by the user, and false negatives, i.e.
turnsannotated by the user but not by the system.
Table 6 illustrates a few representative examples.
Note howthe proposed system does not perform well when detecting a request for information as something worthannotating (e.g.
What are you doing?).
We noticed that in these cases, the model tended to annotate turnswhere the information was actually provided (e.g.That is the package has arrived).
We can hypothesizethat users annotate the request for information to give context to the a-priori more relevant information,i.e.
the answer to the question.
However, in some cases, participants did not annotate the answer asrelevant.
This counter-intuitive observation reflects the subjectivity and variability of the task.False Positive False NegativeI am leaving soon, I start at 3 o?clock or [..] How are you?
Can you hear me?Let?s see if we can tell him.
What are you doing?That is, the package has arrived.
Did you buy beautiful things for me?Table 6: Examples of false positive and false negative turns.5.3 Comparative Analysis and DiscussionTo the best of our knowledge there are no previous works of similar nature to the study presented in thispaper.
Yet, it is important to give a sense of the merits and limitations of the proposed approach in thecontext of the state-of-the-art.
For this reason, we compare our results with (Banerjee and Rudnicky,2009), which is the most similar prior art to our work.
In (Banerjee and Rudnicky, 2009), Barnejee etal.
perform a Wizard-of-Oz experiment and report a performance of the human annotator of P = 0.35precision, R = 0.42 recall, leading to an F-score of F = 0.38.
This result highlights the difficulty ofthe task, even for a human annotator.
When comparing our proposed system with this Wizard-of-Ozexperiment, we obtain an F-score of F = 0.32 against the human annotator?s F-score of F = 0.38,with a significantly higher recall (0.57 vs 0.42) yet lower precision (0.245 vs 0.35).
Given this human-based prediction performance, the proposed approach represents a good first step towards realizing anintelligent annotation system for mobile phone conversations.6 Conclusions and Future WorkIn this paper we have proposed and empirically evaluated a machine learning-based approach to auto-matically detect noteworthy information in spontaneous mobile phone conversations.
The subjectivity ofthis task leads to a challenging classification problem even for human assessors.
Our approach adopts ahybrid strategy that exploits the content and the context of the conversation.
We have shown that infor-mation about the context of the conversation improves the predictive performance of the system over apure content based approach.In the future, we plan to extend the model by including acoustic features which could improve theperformance by adding orthogonal information to the current model.
To tackle the subjectivity of the taskwe also intend to investigate the performance of personalization techniques, creating individual modelsper user.
Finally, we plan to conduct a study to evaluate our system from a user-centric perspective.34AcknowledgmentsThis work was partially supported by the Innovation Bursary of Trinity College Dublin (project: ?Tech-nology for Harmonising Interpersonal Communication?
).ReferencesSatanjeev Banerjee and Alexander I. Rudnicky.
2008.
An extractive-summarization baseline for the automaticdetection of noteworthy utterances in multi-party human-human dialog.
In Proceeding of SLT, pages 177?180.Satanjeev Banerjee and Alexander Rudnicky.
2009.
Detecting the noteworthiness of utterances in human meet-ings.
In Proceedings of the SIGDIAL 2009 Conference, pages 71?78, London, UK, September.
Association forComputational Linguistics.Francesca Bonin, Celine De Looze, Sucheta Ghosh, Emer Gilmartin, Carl Vogel, Anna Polychroniou, HuguesSalamin, Alessandro Vinciarelli, and Nick Campbell.
2013.
Investigating fine temporal dynamics of prosodicand lexical accommodation.
In Proceedings of Interspeech 2013, Lyon, France, August.H.P.
Branigan, M.J. Pickering, J. Pearson, and J.F.
McLean.
2010.
Linguistic alignment between people andcomputers.
Journal of Pragmatics, 42(9):2355?2368.Susan E. Brennan.
1996.
Lexical entrainment in spontaneous dialog.
Proceedings of ISSD, pages 41?44.Juan Pablo Carrascal, Rodrigo de Oliveira, and Mauro Cherubini.
2012.
A note paper on note-taking: understand-ing annotations of mobile phone calls.
In Proceedings of the 14th international conference on Human-computerinteraction with mobile devices and services, MobileHCI ?12, pages 21?24, New York, NY, USA.
ACM.Fang Chen, Kesong Han, and Guilin Chen.
2002.
An approach to sentence-selection-based text summarization.In TENCON?02.
Proceedings.
2002 IEEE Region 10 Conference on Computers, Communications, Control andPower Engineering, volume 1, pages 489?493.
IEEE.Chandrika Cycil, Mark Perry, Eric Laurier, and Alex Taylor.
2013.
?eyes free?
in-car assistance: parent and childpassenger collaboration during phone calls.
In Proceedings of the 15th international conference on Human-computer interaction with mobile devices and services, MobileHCI ?13, pages 332?341, New York, NY, USA.ACM.Joseph L. Fleiss.
1971.
Measuring nominal scale agreement among many raters.
Psychological Bulletin,76(5):378?382.Michel Galley.
2006.
A skip-chain conditional random field for ranking meeting utterances by importance.
InProceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,pages 364?372, Stroudsburg, PA, USA.
Association for Computational Linguistics.Nikhil Garg, Benoit Favre, and Dilek Hakkani-T?ur.
2009.
Clusterrank: a graph based method for meeting sum-marization.
In Technical Report, IDIAP.Vishal Gupta and Gurpreet Singh Lehal.
2010.
A survey of text summarization extractive techniques.
Journal ofEmerging Technologies in Web Intelligence, 2(3):258?268.Po Hu, Dong-Hong Ji, Chong Teng, and Yujing Guo.
2012.
Context-enhanced personalized social summarization.In COLING, pages 1223?1238.J Jian Zhang, Ho Yin Chan, and Pascale Fung.
2007.
Improving lecture speech summarization using rhetoricalinformation.
In Automatic Speech Recognition & Understanding, 2007.
ASRU.
IEEE Workshop on, pages 195?200.
IEEE.Sokol Koc?o, C?ecile Capponi, and Fr?ed?eric B?echet.
2012.
Applying multiview learning algorithms to human-human conversation classification.
In INTERSPEECH.Rivka Levitan and Julia Hirschberg.
2011.
Measuring acoustic-prosodic entrainment with respect to multiplelevels and dimensions.
In Twelfth Annual Conference of the International Speech Communication Association.Sameer Maskey and Julia Hirschberg.
2003.
Automatic summarization of broadcast news using structural features.In INTERSPEECH.35Sameer Maskey and Julia Hirschberg.
2005.
Comparing lexical, acoustic/prosodic, structural and discourse fea-tures for speech summarization.
In INTERSPEECH, pages 621?624.Sameer Maskey and Julia Hirschberg.
2006.
Summarizing speech without text using hidden markov models.
InProceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,pages 89?92.
Association for Computational Linguistics.Lluis Padro, Samuel Reese, Eneko Agirre, and Aitor Soroa.
2010.
Semantic services in freeling 2.1: Wordnet andukb.
In Pushpak Bhattacharyya, Christiane Fellbaum, and Piek Vossen, editors, Principles, Construction, andApplication of Multilingual Wordnets, pages 99?105, Mumbai, India, February.
Global Wordnet Conference2010, Narosa Publishing House.Martin J. Pickering and Victor S. Ferreira.
2008.
Structural priming: a critical review.
Psychological bulletin,134(3):427.David Reitter and Johanna D Moore.
2007.
Predicting success in dialogue.
In Annual Meeting-Association forComputational Linguistics, volume 45, page 808.Jose San Pedro, Vaiva Kalnikaite, and Steve Whittaker.
2009.
You can play that again: Exploring social redun-dancy to derive highlight regions in videos.
In Proceedings of the 14th International Conference on IntelligentUser Interfaces, IUI ?09, pages 469?474, New York, NY, USA.
ACM.Carl Vogel.
2013.
Attribution of mutual understanding.
Journal of Law & Policy, pages 101?145.Dong Wang and Yang Liu.
2011.
A pilot study of opinion summarization in conversations.
In Annual Meeting-Association for Computational Linguistics, ACL, pages 331?339.Shasha Xie, Yang Liu, and Hui Lin.
2008.
Evaluating the effectiveness of features and sampling in extractivemeeting summarization.
In Spoken Language Technology Workshop, 2008.
SLT 2008.
IEEE, pages 157?160.IEEE.Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li.
2011.
Social context summarization.
InProceedings of the 34th International ACM SIGIR Conference on Research and Development in InformationRetrieval, SIGIR ?11, pages 255?264, New York, NY, USA.
ACM.Tom Yeh, Brandyn White, Jose San Pedro, Boriz Katz, and Larry S. Davis.
2011.
A case for query by image andtext content: Searching computer help using screenshots and keywords.
In Proceedings of the 20th InternationalConference on World Wide Web, WWW ?11, pages 775?784, New York, NY, USA.
ACM.Yongzheng Zhang, Nur Zincir-Heywood, and Evangelos Milios.
2005.
Narrative text classification for automatickey phrase extraction in web document corpora.
In Proceedings of the 7th annual ACM international workshopon Web information and data management, WIDM ?05, pages 51?58, New York, NY, USA.
ACM.36
