Adaptive Compression-based Approach for Chinese Pinyin InputJin Hu Huang and David PowersSchool of Informatics and EngineeringFlinders University of South AustraliaGPO Box 2100, SA 5001Australia{jin.huang,powers}@infoeng.flinders.edu.auAbstractThis article presents a compression-based adap-tive algorithm for Chinese Pinyin input.
Thereare many different input methods for Chinesecharacter text and the phonetic Pinyin in-put method is the one most commonly used.Compression by Partial Match (PPM) is anadaptive statistical modelling technique thatis widely used in the field of text compres-sion.
Compression-based approaches are able tobuild models very efficiently and incrementally.Experiments show that adaptive compression-based approach for Pinyin input outperformsmodified Kneser-Ney smoothing method im-plemented by SRILM language tools (Stolcke,2002).1 IntroductionChinese words comprise ideographic and picto-graphic characters.
Unlike English, these char-acters can?t be entered by keyboard directly.They have to be transliterated from keyboardinput based on different input methods.
Thereare two main approaches: phonetic-based inputmethods such as Pinyin input and structure-based input methods such as WBZX.
Pinyininput is the easiest to learn and most widelyused.
WBZX is more difficult as the user has toremember all the radical parts of each character,but it is faster.Early products using Pinyin input methodsare very slow because of the large number ofhomonyms in the Chinese language.
The userhas to choose the correct character after eachPinyin has been entered.
The situation in cur-rent products such as Microsoft IME for Chi-nese and Chinese Star has been improved withthe progress in language modelling (Goodman,2001) but users are still not satisfied.2 Statistical Language ModellingStatistical language modelling has been success-fully applied to Chinese Pinyin input (Gao etal., 2002).
The task of statistical language mod-elling is to determine the probability of a se-quence of words.P (w1 .
.
.
wi) = P (w1)?P (w2|w1)??
?
?
?P (wi|w1 .
.
.
wi?1)(1)Given the previous i-1 words, it is difficult tocompute the conditional probability if i is verylarge.
An n-gram Markov model approximatesthis probability by assuming that only wordsrelevant to predict are previous n-1 words.
Themost commonly used is trigram.P (wi|w1 .
.
.
wi?1) ?
P (wi|wi?2wi?1) (2)The key difficulty with using n-gram languagemodels is that of data sparsity.
One can neverhave enough training data to cover all the n-grams.
Therefore some mechanism for assigningnon-zero probability to novel n-grams is a keyissue in statistical language modelling.
Smooth-ing is used to adjust the probabilities andmake distributions more uniform.
Chen andGoodman (Chen and Goodman, 1999) made acomplete comparison of most smoothing tech-niques and found that the modified Kneser-Neysmoothing(equation 3) outperformed others.pKN (wi|wi?1i?n+1) =c(wi?1i?n+1)?D(c(wi?1i?n+1))?wi c(wi?1i?n+1)+?
(wi?1i?n+1)pKN (wi|wi?1i?n+2) (3)whereD(c) =??????
?0 if c = 0D1 if c = 1D2 if c = 2D3+ if c ?
3?
(wi?1i?n+1) =D1N1(wi?1i?n+1?)+D2N2(wi?1i?n+1?)+D3+N3+(wi?1i?n+1?)?wic(wi?1i?n+1)(4)N1(wi?1i?n+1?)
= | {wi : c(wi?1i?n+1wi) = 1} |(5)Y =n1n1 + 2n2D1 = 1?
2Yn2n1D2 = 1?
3Yn3n2D3+ = 1?
4Yn4n3(6)The process of Pinyin input can be formu-lated as follows.W = argmaxWPr(W |A) (7)W = argmaxWPr(A|W ) Pr(W ) (8)We assume each Chinese character has onlyone pronunciation in our experiments.Thus we can use the Viterbi algorithm to findthe word sequences to maximize the languagemodel according to Pinyin input.3 Prediction by Partial MatchingPrediction by Partial Matching (PPM)(Clearyand Witten, 1984; Bell et al, 1990) is a symbol-wise compression scheme for adaptive text com-pression.
PPM generates a prediction for eachinput character based on its preceding char-acters.
The prediction is encoded in form ofconditional probability, conditioned on previouscontext.
PPM maintains predictions, computedfrom the training data, for larger context as wellas all shorter con-texts.
If PPM cannot pre-dict the character from current context, it usesan escape probability to ?escape?
another con-text model, usually of length one shorter thanthe current context.
For novel characters thathave never seen before in any length model, thealgorithm escapes down to a default ?order-1?context model where all possible characters arepresent.PPM escape method can be considered as aninstance of Jelinek-Mercer smoothing.
It is de-fined recursively as a linear interpolation be-tween the nth-order maximum likelihood andthe (n-1)th-order smoothed model.
Variousmethods have been proposed for estimating theescape probability.
In the following descriptionof each method, e is the escape probability andp(?)
is the conditional probability for symbol ?, given a context.
c(?)
is the number of timesthe context was followed by the symbol ?
.
nis the number of tokens that have followed.
t isthe number of types.Method A works by allocating a count of oneto the escape symbol.e =1n+ 1(9)p(?)
=c(?
)n+ 1(10)Method B makes assumption that the firstoccurrence of a particular symbol in a particu-lar context may be taken as evidence of a novelsymbol appearing in the context, and thereforedoes not contribute towards the estimate of theprobability of the symbol which it occurred.e =tn(11)p(?)
=c(?)?
1n(12)Method C (Moffat, 1990) is similar to MethodB, with the distinction that the first observationof a particular symbol in a particular symbolin a particular context also contributes to theprobability estimate of the symbol itself.
Es-cape method C is called Witten-Bell smooth-ing in statistical language modelling.
Chen andGoodman (Chen and Goodman, 1999) reportedit is competitive on very large training data setscomparing with other smoothing techniques.e =tn+ t(13)p(?)
=c(?
)n+ t(14)Method D (Howard, 1993) is minor modifi-cation to method B.
Whenever a novel eventoccurs, rather than adding one to the symbol,half is added instead.e =t2n(15)p(?)
=2c(?)?
12n(16)To illustrate the PPM compression modellingtechnique, Table 1 shows the model after stringdealornodeal has been processed.
In this illus-tration the maximum order is 2 and each pre-diction has a count c and a prediction prob-ability p. The probability is determined fromOrder 2Prediction c pal ?
o 1 1/2?
Esc 1 1/2de ?
a 2 3/4?
Esc 1 1/4ea ?
l 2 3/4?
Esc 1 1/2lo ?
r 1 1/2?
Esc 1 1/2no ?
d 1 1/2?
Esc 1 1/2od ?
e 1 1/2?
Esc 1 1/2or ?
n 1 1/2?
Esc 1 1/2rn ?
o 1 1/2?
Esc 1 1/2Order 1Prediction c pa ?
l 2 3/4?
Esc 1 1/4d ?
e 2 3/4?
Esc 1 1/4e ?
a 2 3/4?
Esc 1 1/4l ?
o 1 1/2?
Esc 1 1/2n ?
o 1 1/2?
Esc 1 1/2o ?
d 1 1/4?
r 1 1/4?
Esc 2 1/2r ?
n 1 1/2?
Esc 1 1/2Order 0Prediction c p?
a 2 3/24?
d 2 3/24?
e 2 3/24?
l 2 3/24?
n 1 1/24?
o 2 3/24?
r 1 1/24?
Esc 7 7/24Order ?1Prediction c p?
A 1 1/|A|Table 1: PPM model after processing the stringdealornodealcounts associated with the prediction using es-cape method D(equation 16).
|A| is the size thealphabet which determines the probability foreach unseen character.Suppose the character following dealornodealis o.
Since the order-2 context is al and the up-coming symbol o has already seen in this con-text, the order-2 model is used to encode thesymbol.
The encoding probability is 1/2.
If thenext character were i instead of o, it has notbeen seen in the current order-2 context (al).Then an order-2 escape event is emitted with aprobability of 1/2 and the context truncated tol.
Checking the order-1 model, the upcomingcharacter i has not been seen in this context,so an order-1 escape event is emitted with aprobability of 1/2 and the context is truncatedto the null context, corresponding to the order-0 model.
As i has not appeared in the stringdealornodeal, a final level of escape is emittedwith a probability of 7/24 and the i will be pre-dicted with a probability of 1/256 in the order-?1, assuming that the alphabet size is 256 forASCII.
Thus i is encoded with a total probabil-ity of 12 ?12 ?724 ?1256 .In reality, the alphabet size in the order- ?1model may be reduced by the number of char-acters in the order-0 model as these characterswill never be predicted in the order- ?1 context.Thus it can be reduced to 249 in this case.
Simi-larly a character that occurs in the higher-ordermodel will never be encoded in the lower-ordermodels.
So it is not necessary to reserve theprobability space for the character in the lower-order models.
This is called ?exclusion?, whichcan greatly improve compression.Compression Method Size Compression RateEscape A(order 2) 434228 54.8%Escape B(order 2) 332278 41.9%Escape C(order 2) 333791 42.1%Escape D(order 2) 332829 42.0%Escape D(order 1) 345841 43.6%Escape D(order 3) 332932 42.0%gzip 434220 54.8%compress 514045 64.8%Table 2: Compression results for different com-pression methodsTable 2 shows the compression result for filePeople Daily (9101) with 792964 Bytes usingdifferent compression methods.
PPM compres-sion methods are significantly better than prac-tical compression utilities like Unix gzip andcompress except escape method A but they areslower during compression.
The compressionrates for escape method B and D are both higherthan escape method C. Order-2 model (trigram)is slightly better that order-1 and order-3 mod-els for escape method D.In our experiment we use escape method Dto calculate the escape probability as escapemethod D is slightly better than other escapemethods in compressing text although MethodB is the best here.
Teahan (Teahan et al, 2000)has successfully applied escape method D tosegment Chinese text.4 Experiment and ResultWe use 220MB People Daily (91-95) as thetraining corpus and 58M People Daily (96) andstories download from Internet (400K) as thetest corpus.We used SRILM language tools (Stolcke,2002) to collect trigram counts and appliedmodified Kneser-Ney smoothing method tobuild the language model.
Then we used disam-big to translate Pinyin to Chinese characters.In PPM model we used the same count datacollected by SRILM tools.
We chose a trie struc-ture to store the symbol and count.
AdaptivePPM model updates the counts during Pinyininput.
It is similar to a cache model (Kuhnand De Mori, 1990).
We tested both static andadaptive PPM models on test corpus.
PPMmodels run twice faster than SRILM tool dis-ambig.
It took 20 hours to translate Pinyin(People Daily 96) to character on a Sparc withtwo CPUs(900Mhz) using SRILM tools.
Thefollowing Table 3 shows the results in terms ofcharacter error rate.
People Daily(96) is thesame domain as the training corpus.
Results ob-tained testing on People Daily are consistentlymuch better than Stories.
Static PPM is a lit-tle worse than modified Kneser-Ney smoothingmethod.
Adaptive PPM model testing on largecorpus is better than small corpus as it takestime to adapt to the new model.People Daily(96) Storiesmodified Kneser-Ney 5.82% 14.48%Static PPM 6.00% 16.55%Adaptive PPM 4.98% 14.24%Table 3: Character Error Rates for Kneser-Ney,Static and Adaptive PPM5 ConclusionWe have introduced a method for Pinyin inputbased on an adaptive PPM model.
AdaptivePPM model outperforms both static PPM andmodified Kneser-Ney smoothing.ReferencesT.C.
Bell, J.G.
Cleary, and I.H.
Witten.
1990.Text Compression.
Prentice Hall.Stanly Chen and Joshua Goodman.
1999.
Anempirical study of smoothing techniques forlanguage modeling.
Computer Speech andLanguage, 13, 10.J.G.
Cleary and I.H.
Witten.
1984.
Data com-pression using adaptive coding and partialstring matching.
IEEE transactions on Com-munications, 32(4).Jianfeng Gao, Juashua Goodman, Mingjing Li,and Kai Fu Lee.
2002.
Toward a unified ap-proach to statistical language modeling forchinese.
ACM transaction on Asian Lan-guage information processing, 1(1), March.Joshua Goodman.
2001.
A bit of progress inlanguage modeling.
Technical Report MSR-TR-2001-72, Microsoft Research.P.G.
Howard.
1993.
The design and analysisof efficient lossless data compression systems.Ph.D.
thesis, Brown University, Providence,Rhode Island.R.
Kuhn and R. De Mori.
1990.
A cache-basednatural language model for speech reproduc-tion.
IEEE Transac-tion on Pattern Analysisand Machine Intelligence, 6.Alistair Moffat.
1990.
Implement the ppm datacom-pression scheme.
IEEE Transaction onCommunications, 38(11):1917?1921.A.
Stolcke.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
In Proc.
Intl.
Conf.on Spoken Lan-guage Processing, volume 2,pages 901?904, Denver.W.J.
Teahan, Yingying Wen, and I.H.
Wit-ten R. McNab.
2000.
A compression-based algorithm for chinese word segmenta-tion.
Computational Linguistics, 26(3):375?394, September.
