General Indexation of Weighted Automata ?Application to Spoken Utterance RetrievalCyril Allauzen and Mehryar Mohri and Murat SaraclarAT&T Labs - Research180 Park Avenue, Florham Park, NJ 07932{allauzen, mohri, murat}@research.att.comAbstractMuch of the massive quantities of digitizeddata widely available, e.g., text, speech, hand-written sequences, are either given directly,or, as a result of some prior processing, asweighted automata.
These are compact rep-resentations of a large number of alternativesequences and their weights reflecting the un-certainty or variability of the data.
Thus,the indexation of such data requires indexingweighted automata.We present a general algorithm for the index-ation of weighted automata.
The resulting in-dex is represented by a deterministic weightedtransducer that is optimal for search: the searchfor an input string takes time linear in the sumof the size of that string and the number ofindices of the weighted automata where it ap-pears.
We also introduce a general frameworkbased on weighted transducers that general-izes this indexation to enable the search formore complex patterns including syntactic in-formation or for different types of sequences,e.g., word sequences instead of phonemic se-quences.
The use of this framework is illus-trated with several examples.We applied our general indexation algorithmand framework to the problem of indexation ofspeech utterances and report the results of ourexperiments in several tasks demonstrating thatour techniques yield comparable results to pre-vious methods, while providing greater gener-ality, including the possibility of searching forarbitrary patterns represented by weighted au-tomata.1 MotivationMuch of the massive quantities of digitized data widelyavailable is highly variable or uncertain.
This uncertaintyaffects the interpretation of the data and its computationalprocessing at various levels, e.g., natural language textsare abundantly ambiguous, speech and hand-written se-quences are highly variable and hard to recognize in pres-ence of noise, biological sequences may be altered or in-complete.Searching or indexing such data requires dealing witha large number of ranked or weighted alternatives.
Thesemay be for example the different parses of an input text,the various responses to a search engine or informationextraction query, or the best hypotheses of a speech orhand-written recognition system.
In most cases, alterna-tive sequences can be compactly represented by weightedautomata.
The weights may be probabilities or someother weights used to rank different hypotheses.This motivates our study of the general problem ofindexation of weighted automata.
This is more generalthan the classical indexation problems since, typically,there are many distinct hypotheses or alternatives asso-ciated with the same index, e.g., a specific input speechor hand-written sequence may have a large number of dif-ferent transcriptions according to the system and modelsused.
Moreover, the problem requires taking into con-sideration the weight of each alternative, which does nothave a counterpart in classical indexation problems.We describe a general indexation algorithm forweighted automata.
The resulting index is representedby a deterministic weighted transducer that is optimal forsearch: the search for an input string takes time linearin the sum of the size of that string and the number ofindices of the weighted automata where it appears.In some cases, one may wish to search using sequencesin some level, e.g.
word sequences, different from thelevel of the sequences of the index, e.g.
phonemic se-quences.
One may also wish to search for complex se-quences including both words and parts-of-speech, or re-strict the search by either restricting the weights or proba-bilities or the lengths or types of sequences.
We describea general indexation framework covering all these cases.Our framework is based on the use of filtering weightedtransducers for restriction or other transducers mappingbetween distinct information levels or knowledge struc-tures.
We illustrate the use of this framework with sev-eral examples that demonstrate its relevance to a numberof indexation tasks.We applied our framework and algorithms to the par-ticular problem of speech indexation.
In recent years,spoken document retrieval systems have made largearchives of broadcast news searchable and browsable.Most of these systems use automatic speech recognitionto convert speech into text, which is then indexed us-ing standard methods.
When a user presents the systemwith a query, documents that are relevant to the query arefound using text-based information retrieval techniques.As speech indexation and retrieval systems move be-yond the domain of broadcast news to more challengingspoken communications, the importance for the indexedmaterial to contain more than just a simple text represen-tation of the communication is becoming clear.
Index-ation and retrieval techniques must be extended to han-dle more general representations including for examplesyntactic information.
In addition to the now familiar re-trieval systems or search engines, other applications suchas data mining systems can be used to automatically iden-tify useful patterns in large collections of spoken commu-nications.
Information extraction systems can be used togather high-level information such as named-entities.For a given input speech utterance, a large-vocabularyspeech recognition system often generates a lattice, aweighted automaton representing a range of alternativehypotheses with some associated weights or probabilitiesused to rank them.
When the accuracy of a system is rel-atively low as in many conversational speech recognitiontasks, it is not safe to rely only on the best hypothesis out-put by the system.
It is then preferable to use instead thefull lattice output by the recognizer.We report the results of our experiments in sev-eral tasks demonstrating that our techniques yieldcomparable results to the previous methods ofSaraclar and Sproat (2004), while providing greatergenerality, including the possibility of searching forarbitrary patterns represented by weighted automata.The paper is organized as follows.
Section 2 introducesthe notation and the definitions used in the rest of the pa-per.
Section 3 describes our general indexation algorithmfor weighted automata.
The algorithm for searching thatindex is presented in Section 4 and our general indexa-tion framework is described and illustrated in Section 5.Section 6 reports the results of our experiments in severaltasks.2 PreliminariesDefinition 1 A system (K,?,?, 0, 1) is a semiring(Kuich and Salomaa, 1986) if: (K,?, 0) is a commuta-tive monoid with identity element 0; (K,?, 1) is a monoidwith identity element 1; ?
distributes over ?
; and 0 is anannihilator for ?
: for all a ?
K, a?
0 = 0?
a = 0.Thus, a semiring is a ring that may lack negation.
Twosemirings often used in speech processing are: the logsemiring L = (R ?
{?
},?log,+,?, 0) (Mohri, 2002)which is isomorphic to the familiar real or probabilitysemiring (R+,+,?, 0, 1) via a log morphism with, forall a, b ?
R ?
{?
}:a?log b = ?
log(exp(?a) + exp(?b))and the convention that: exp(??)
= 0 and?
log(0) = ?, and the tropical semiring T = (R+ ?{?
},min,+,?, 0) which can be derived from the logsemiring using the Viterbi approximation.Definition 2 A weighted finite-state transducer T over asemiring K is an 8-tuple T = (?,?, Q, I, F,E, ?, ?
)where: ?
is the finite input alphabet of the transducer;?
is the finite output alphabet; Q is a finite set of states;I ?
Q the set of initial states; F ?
Q the set of finalstates; E ?
Q?
(??
{})?
(??
{})?K?Q a finiteset of transitions; ?
: I ?
K the initial weight function;and ?
: F ?
K the final weight function mapping F toK.A Weighted automaton A = (?, Q, I, F,E, ?, ?)
is de-fined in a similar way by simply omitting the output la-bels.
We denote by L(A) ?
??
the set of strings ac-cepted by an automaton A and similarly by L(X) thestrings described by a regular expression X .
We denoteby |A| = |Q|+ |E| the size of A.Given a transition e ?
E, we denote by i[e] its inputlabel, p[e] its origin or previous state and n[e] its desti-nation state or next state, w[e] its weight, o[e] its outputlabel (transducer case).
Given a state q ?
Q, we denoteby E[q] the set of transitions leaving q.A path pi = e1 ?
?
?
ek is an element of E?
with con-secutive transitions: n[ei?1] = p[ei], i = 2, .
.
.
, k. Weextend n and p to paths by setting: n[pi] = n[ek] andp[pi] = p[e1].
A cycle pi is a path whose origin anddestination states coincide: n[pi] = p[pi].
We denote byP (q, q?)
the set of paths from q to q?
and by P (q, x, q?
)and P (q, x, y, q?)
the set of paths from q to q?
with in-put label x ?
??
and output label y (transducer case).These definitions can be extended to subsets R,R?
?
Q,by: P (R, x,R?)
= ?q?R, q?
?R?P (q, x, q?).
The label-ing functions i (and similarly o) and the weight func-tion w can also be extended to paths by defining the la-bel of a path as the concatenation of the labels of itsconstituent transitions, and the weight of a path as the?-product of the weights of its constituent transitions:i[pi] = i[e1] ?
?
?
i[ek], w[pi] = w[e1] ?
?
?
?
?
w[ek].
Wealso extend w to any finite set of paths ?
by setting:w[?]
= ?pi??
w[pi].
The output weight associated byA to each input string x ?
??
is:[[A]](x) =?pi?P (I,x,F )?
(p[pi]) ?
w[pi]?
?
(n[pi])[[A]](x) is defined to be 0 when P (I, x, F ) = ?.
Simi-larly, the output weight associated by a transducer T to apair of input-output string (x, y) is:[[T ]](x, y) =?pi?P (I,x,y,F )?
(p[pi]) ?
w[pi]?
?
(n[pi])[[T ]](x, y) = 0 when P (I, x, y, F ) = ?.
A successfulpath in a weighted automaton or transducer M is a pathfrom an initial state to a final state.
M is unambiguous iffor any string x ?
??
there is at most one successful pathlabeled with x.
Thus, an unambiguous transducer definesa function.For any transducer T , denote by ?2(T ) the automatonobtained by projecting T on its output, that is by omittingits input labels.Note that the second operation of the tropical semiringand the log semiring as well as their identity elements areidentical.
Thus the weight of a path in an automaton Aover the tropical semiring does not change if A is viewedas a weighted automaton over the log semiring or vice-versa.Given two strings u and v in ?
?, v is a factor of u ifu = xvy for some x and y in ??
; if y =  then v is alsoa suffix of u.
More generally, v is a factor (resp.
suffix) ofL ?
??
if v is a suffix (resp.
factor) of some u ?
L. Wedenote by |x| the length of a string x ?
?
?.3 Indexation AlgorithmThis section presents an algorithm for the construction ofan efficient index for a large set of speech utterances.We assume that for each speech utterance ui of thedataset considered, i = 1, .
.
.
, n, a weighted automatonAi over the alphabet ?
and the log semiring, e.g., phoneor word lattice output by an automatic speech recognizer,is given.
The problem consists of creating a full index,that is one that can be used to search directly any factorof any string accepted by these automata.
Note that thisproblem crucially differs from classical indexation prob-lems in that the input data is uncertain.
Our algorithmmust make use of the weights associated to each stringby the input automata.The main idea behind the design of the algorithm de-scribed is that the full index can be represented by aweighted finite-state transducer T mapping each factorx to the set of indices of the automata in which x appearsand the negative log of the expected count of x. Moreprecisely, let Pi be the probability distribution defined bythe weighted automaton Ai over the set of strings ??
andlet Cx(u) denote the number of occurrences of a factorx in u, then, for any factor x ?
??
and automaton indexi ?
{1, .
.
.
, n}:[[T ]](x, i) = ?
log(EPi [Cx]) (1)Our algorithm for the construction of the index is simple,it is based on general weighted automata and transduceralgorithms.
We describe the consecutive stages of the al-gorithm.This algorithm can be seen as a generalization toweighted automata of the notion of suffix automaton andfactor automaton for strings.
The suffix (factor) automa-ton of a string u is the minimal deterministic finite au-tomata recognizing exactly the set of suffixes (resp.
fac-tors) of u (Blumer et al, 1985; Crochemore, 1986).
Thesize of both automata is linear in the length of u and bothcan be built in linear time.
These are classical repre-sentations used in text indexation (Blumer et al, 1987;Crochemore, 1986).3.1 PreprocessingWhen the automata Ai are word or phone lattices out-put by a speech recognition or other natural languageprocessing system, the path weights correspond to jointprobabilities.
We can apply to Ai a general weight-pushing algorithm in the log semiring (Mohri, 1997)which converts these weights into the desired (negativelog of) posterior probabilities.
More generally, the pathweights in the resulting automata can be interpreted aslog-likelihoods.
We denote by Pi the correspondingprobability distribution.
When the input automaton Ai isacyclic, the complexity of the weight-pushing algorithmis linear in its size (O(|Ai|)).
Figures 1(b)(d) illustratesthe application of the algorithm to the automata of Fig-ures 1(a)(c).3.2 Construction of Transducer Index TLet Bi = (?, Qi, Ii, Fi, Ei, ?i, ?i) denote the result ofthe application of the weight pushing algorithm to the au-tomaton Ai.
The weight associated by Bi to each stringit accepts can be interpreted as the log-likelihood of thatstring for the utterance ui given the models used to gen-erate the automata.
More generally, Bi defines a proba-bility distribution Pi over all strings x ?
??
which is justthe sum of the probability of all paths of Bi in which xappears.For each state q ?
Qi, denote by d[q] the shortest dis-tance from Ii to q (or -log of the forward probability) andby f [q] the shortest distance from q to F (or -log of thebackward probability):d[q] =?logpi?P (Ii,q)(?i(p[pi]) + w[pi]) (2)01a2bb3a 01a/0.52b/0.5b/13/1a/1(a) (b)01b/12a/2a/13/1b/1 01b/0.3332a/0.666a/13/1b/1(c) (d)Figure 1: Weighted automata over the real semiring (a) A1, (b) B1 obtained by applying weight pushing to A1, (c) A2and (d) B2 obtained by applying weight pushing to A2.f [q] =?logpi?P (q,Fi)(w[pi] + ?i(n[pi])) (3)The shortest distances d[q] and f [q] can be computed forall states q ?
Qi in linear time (O(|Bi|)) when Bi isacyclic (Mohri, 2002).
Then,?
log(EPi [Cx]) =?logi[pi]=xd[p[pi]] + w[pi] + f [n[pi]] (4)From the weighted automaton Bi, one can derive aweighted transducer Ti in two steps:1.
Factor Selection.
In the general case we select allthe factors to be indexed in the following way:?
Replace each transition (p, a, w, q) ?
Qi??
?R?Qi by (p, a, a, w, q) ?
Qi?????R?Qi;?
Create a new state s 6?
Qi and make s theunique initial state;?
Create a new state e 6?
Qi and make e theunique final state;?
Create a new transition (s, , , d[q], q) for eachstate q ?
Qi;?
Create a new transition (q, , i, f [q], e) for eachstate q ?
Qi;2.
Optimization.
The resulting transducer can be op-timized by applying weighted -removal, weighteddeterminization, and minimization over the logsemiring by viewing it as an acceptor, i.e., input-output labels are encoded a single labels.It is clear from Equation 4 that for any factor x ?
??
:[[Ti]](x, i) = ?
log(EPi [Cx]) (5)This construction is illustrated by Figures 2(a)(b).
Ourfull index transducer T is the constructed by?
taking the ?log-sum (or union) of all the transducersTi, i = 1, .
.
.
, n;?
defining T as the result of determinization (in thelog semiring) applied to that transducer.Figure 3 is illustrating this construction and optimization.01a:?/0.52b:?/0.55/1?:1/1b:?/1?:1/1?:1/13a:?/1?:1/14?:?/1?:?/0.5?:?/1?
:?/1(a)0 1/1?:1/3.52a:?/1.53b:?/1?
:1/1b:?/0.333 ?
:1/14a:?/1 ?
:1/1(b)Figure 2: Construction of T1 index of the weighted au-tomata B1 given Figure 1(b): (a) intermediary result afterfactor selection and (b) resulting weighted transducer T1.4 SearchThe full index represented by the weighted finite-statetransducer T is optimal.
Indeed, T contains no transi-tion with input  other than the final transitions labeledwith an output index and it is deterministic.
Thus, theset of indices Ix of the weighted automata containing afactor x can be obtained in O(|x|+ |Ix|) by reading in Tthe unique path with input label x and then the transitionswith input  which have each a distinct output label.The user?s query is typically an unweighted string, butit can be given as an arbitrary weighted automaton X .This covers the case of Boolean queries or regular expres-sions which can be compiled into automata.
The responseto a query X is computed using the general algorithm ofcomposition of weighted transducers (Mohri et al, 1996)followed by projection on the output:?2(X ?
T ) (6)which is then -removed and determinized to give di-rectly the list of all indices and their corresponding log-01a:?/2.52b:?/2.3333/1?:1/3.5?:2/3.333?:1/0.600?:2/0.4004b:?/0.600?:1/0.428?:2/0.5715a:?/0.571?:1/0.333?:2/0.6666a:?/0.333?:1/0.75?:2/0.257b:?/0.258/1?:1/19/1?
:2/1Figure 3: Weighted transducer T obtained by index-ing the weighted automata B1 and B2 given in Fig-ures 1(b)(d)likelihoods.
The final result can be pruned to include onlythe most likely responses.
The pruning threshold may beused to vary the number of responses.5 General Indexation FrameworkThe indexation technique just outlined can be easily ex-tended to include many of the techniques used for speechindexation.
This can be done by introducing a transducerF that converts between different levels of informationsources or structures, or that filters out or reweights indexentries.
The filter F can be applied (i) before, (ii) duringor (iii) after the construction of the index.
For case (i), thefilter is used directly on the input and the indexation algo-rithm is applied to the weighted automata (F ?Ai)1?i?n.For case (ii), filtering is done after the factor selectionstep of the algorithm and the filter applies to the factors,typically to restrict the factors that will be indexed.
Forcase (iii), the filter is applied to the index.
Obviouslydifferent filters can be used in combination at differentstages.When such a filter is used, the response to a query X isobtained using another transducer F ?
1 and the followingcomposition and projection:?2(X ?
F ?
?
T ) (7)Since composition is associative, it does not impose aspecific order to its application.
However, in practice,it is often advantageous to compute X ?
F ?
before appli-cation of T .
The following are examples of some filtertransducers that can be of interest in many applications.1In most cases, F ?
is the inverse of F .?
Pronunciation Dictionary: a pronunciation dic-tionary can be used to map word sequences intotheir phonemic transcriptions, thus transform wordlattices into equivalent phone lattices.
This map-ping can represented by a weighted transducer F .Using an index based on phone lattices allows auser to search for words that are not in the ASRvocabulary.
In this case, the inverse transduc-tion F ?
is a grapheme to phoneme converter, com-monly present in TTS front-ends.
Among others,Witbrock and Hauptmann (1997) present a systemwhere a phonetic transcript is obtained from theword transcript and retrieval is performed using bothword and phone indices.?
Vocabulary Restriction: in some cases using a fullindex can be prohibitive and unnecessary.
It mightbe desirable to do partial indexing by ignoring somewords (or phones) in the input.
For example, wemight wish to index only ?named entities?, or justthe consonants.
This is mostly motivated by thereduction of the size of the index while retainingthe necessary information.
A similar approach isto apply a many to one mapping to index groups ofphones, or metaphones (Amir et al, 2001), to over-come phonetic errors.?
Reweighting: a weighted transducer can be usedto emphasize some words in the input while de-emphasizing other.
The weights, for example mightcorrespond to TF-IDF weights.
Another reweight-ing method might involve edit distance or confusionstatistics.?
Classification: an extreme form of summarizing theinformation contained in the indexed material is toassign a class label, such as a topic label, to eachinput.
The query would also be classified and allanswers with the same class label would be returnedas relevant.?
Length Restriction: a common way of indexingphone strings is to index fixed length overlappingphone strings (Logan et al, 2002).
This results in apartial index with only fixed length strings.
Moregenerally a minimum and maximum string lengthmay be imposed on the index.
An example restric-tion automaton is given in Figure 4.
In this case,the filter applies to the factors and has to be appliedduring or after indexation.
The restricted index willbe smaller in size but contains less information andmay result in degradation in retrieval performance,especially for long queries.The length restriction filter requires a modification ofthe search procedure.
Assume a fixed ?
say r ?
lengthrestriction filter and a string query of length k. If k < r,0 1ab 2ab(a)01a:?/2.54b:?/2.3332b:?/0.6005a:?/0.5713/1?:1/0.333?:2/0.666?:1/0.75?
:2/0.25(b)Figure 4: (a) Filter F restricting to strings of length 2.
(b)Restricted index F ?
T , where T is the weighted trans-ducer given in Figure 3(b).then we need to pad the input to length r with ?r?k.
Ifk ?
r, then we must search for all substrings of length rin the index.
A string is present in a certain lattice if all itssubstrings are (and not vice versa).
So, the results of eachsubstring search must be intersected.
The probability ofeach substring xi+r?1i for i ?
{1, .
.
.
, k + 1 ?
r} is anupper bound on the probability of the string xk1 , and thecount of each substring is an upper bound on the count ofthe string, so for i ?
{1, .
.
.
, k + 1?
r}EP [C(xk1)] ?
EP [C(xi+r?1i )].Therefore, the intersection operation must use minimumfor combining the expected counts of substrings.
In otherwords, the expected count of the string is approximatedby the minimum of the probabilities of each of its sub-strings,EP [C(xk1)] ?
min1?i?k+1?r EP [C(xi+r?1i )].In addition to a filter transducer, pruning can be ap-plied at different stages of the algorithm to reduce thesize of the index.
Pruning eliminates least likely paths ina weighted automaton or transducer.
Applying pruningto Ai can be seen as part of the process that generates theuncertain input data.
When pruning is applied to Bi, onlythe more likely alternatives will be indexed.
If pruning isapplied to Ti, or to T , pruning takes the expected countsinto consideration and not the probabilities.
Note that thethreshold used for this type of pruning is directly compa-rable to the threshold used for pruning the search resultsin Section 4 since both are thresholds on expected counts.6 Experimental ResultsOur task is retrieving the utterances (or short audio seg-ments) that a given query appears in.
The experimentalsetup is identical to that of Saraclar and Sproat (2004).Since, we take the system described there as our base-line, we give a brief review of the basic indexation al-gorithm used there.
The algorithm uses the same pre-processing step.
For each label in ?, an index file isconstructed.
For each arc a that appears in the prepro-cessed weighted automaton Bi, the following informa-tion is stored: (i, p[a], n[a], d[p[a]], w[a]).
Since the pre-processing ensures that f [q] = 0 for all q in Bi, it is pos-sible to compute ?
log(EPi [Cx]) as in Equation 4 usingthe information stored in the index.6.1 Evaluation MetricsFor evaluating retrieval performance we use precisionand recall with respect to manual transcriptions.
LetCorrect(q) be the number of times the query q is foundcorrectly, Answer(q) be the number of answers to thequery q, and Reference(q) be the number of times q isfound in the reference.Precision(q) = Correct(q)Answer(q)Recall(q) = Correct(q)Reference(q)We compute precision and recall rates for each query andreport the average over all queries.
The set of queries Qincludes all the words seen in the reference except for astoplist of 100 most common words.Precision = 1|Q|?q?QPrecision(q)Recall = 1|Q|?q?QRecall(q)For lattice based retrieval methods, different operatingpoints can be obtained by changing the threshold.
Theprecision and recall at these operating points can be plot-ted as a curve.In addition to individual precision-recall values wealso compute the F-measure defined asF = 2?
Precision?
RecallPrecision + Recalland report the maximum F-measure (maxF) to summa-rize the information in a precision-recall curve.6.2 CorporaWe use three different corpora to assess the effectivenessof different retrieval techniques.The first corpus is the DARPA Broadcast News cor-pus consisting of excerpts from TV or radio programsincluding various acoustic conditions.
The test set isthe 1998 Hub-4 Broadcast News (hub4e98) evaluationtest set (available from LDC, Catalog no.
LDC2000S86)which is 3 hours long and was manually segmented into940 segments.
It contains 32411 word tokens and 4885word types.
For ASR we use a real-time system (Saraclaret al, 2002).
Since the system was designed for SDR,the recognition vocabulary of the system has over 200Kwords.The second corpus is the Switchboard corpus consist-ing of two party telephone conversations.
The test set isthe RT02 evaluation test set which is 5 hours long, has120 conversation sides and was manually segmented into6266 segments.
It contains 65255 word tokens and 3788word types.
For ASR we use the first pass of the evalua-tion system (Ljolje et al, 2002).
The recognition vocab-ulary of the system has over 45K words.The third corpus is named Teleconferences since it con-sists of multi-party teleconferences on various topics.
Atest set of six teleconferences (about 3.5 hours) was tran-scribed.
It contains 31106 word tokens and 2779 wordtypes.
Calls are automatically segmented into a total of1157 segments prior to ASR.
We again use the first passof the Switchboard evaluation system for ASR.We use the AT&T DCD Library (Allauzen et al, 2003)as our ASR decoder and our implementation of the algo-rithm is based on the AT&T FSM Library (Mohri et al,2000), both of which are available for download.6.3 ResultsWe implemented some of the proposed techniques andmade comparisons with the previous method used bySaraclar and Sproat (2004).
The full indexing methodconsumed too much time while indexing Broadcast Newslattices and used too much memory while indexing phonelattices for Teleconferences.
In the other cases, we con-firmed that the new method yields identical results.
InTable 1 we compare the index sizes for full indexing andpartial indexing with the previous method.
In both cases,the input lattices are pruned so that the cost (negative logprobability) difference between two paths is less than six.Although the new method results in much smaller indexsizes for the string case (i.e.
nbest=1), it can result in verylarge index sizes for full indexing of lattices (cost=6).However, partial indexing by length restriction solves thisproblem.
For the results reported in Table 1, the length ofthe word strings to be indexed was restricted to be lessthan or equal to four, and the length of the phone stringsto be indexed was restricted to be exactly four.In Saraclar and Sproat (2004), it was shown that usingword lattices yields a relative gain of 3-5% in maxF overusing best word hypotheses.
Furthermore, it was shownthat a ?search cascade?
strategy for using both word andphone indices increases the relative gain over the baselineto 8-12%.
In this strategy, we first search the word indexfor the given query, if no matches are found we searchthe phone index.
Using the partial indices, we obtaineda precision recall performance that is almost identical tothe one obtained with the previous method.
Comparisonof the maximum F-measure for both methods is given inTable 2.Task Previous Method Partial IndexBroadcast News 86.0 86.1Switchboard 60.5 60.8Teleconferences 52.8 52.7Table 2: Comparison of maximum F-measure for threecorpora.As an example, we used a filter that indexes only con-sonants (i.e.
maps the vowels to ).
The resulting indexwas used instead of the full phone index.
The size ofthe consonants only index was 370MB whereas the sizeof the full index was 431MB.
In Figure 5 we present theprecision recall performance of this consonant only in-dex.30 40 50 60 70 80304050607080PrecisionRecallWord IndexWord and Phone IndexWord and Phone (consonants only) IndexFigure 5: Comparison of Precision vs Recall Perfor-mance for Switchboard.7 ConclusionWe described a general framework for indexing uncer-tain input data represented as weighted automata.
Theindexation algorithm utilizes weighted finite-state algo-rithms to obtain an index represented as a weighted finite-state transducer.
We showed that many of the techniquesused for speech indexing can be implemented within thisframework.
We gave comparative results to a previousmethod for lattice indexing.The same idea and framework can be used for indexa-tion in natural language processing or other areas whereuncertain input data is given as weighted automata.
Thecomplexity of the index construction algorithm can beimproved in some general cases using techniques simi-lar to classical string matching ones (Blumer et al, 1985;Task Type Pruning Previous Method Full Index Partial IndexBroadcast News word nbest=1 29 2.7 ?Broadcast News word cost=6 91 ?
25Broadcast News phone cost=6 27 ?
14Switchboard word nbest=1 18 4.7 ?Switchboard word cost=6 90 99 88Switchboard phone cost=6 97 431 41Teleconferences word nbest=1 16 2.6 ?Teleconferences word cost=6 142 352 184Teleconferences phone cost=6 146 ?
69Table 1: Comparison of Index Sizes in MegaBytes.Crochemore, 1986; Blumer et al, 1987).
Various prun-ing techniques can be applied to reduce the size of theindex without significantly degrading performance.
Fi-nally, other types of filters that make use of the generalframework can be investigated.AcknowledgmentsWe wish to thank our colleague Richard Sproat for usefuldiscussions and the use of the lattice indexing software(lctools) used in our baseline experiments.ReferencesCyril Allauzen, Mehryar Mohri, and Michael Ri-ley.
2003.
DCD Library - Decoder Library.http://www.research.att.com/sw/tools/dcd.Arnon Amir, Alon Efrat, and Savitha Srinivasan.
2001.Advances in phonetic word spotting.
In Proceedingsof the Tenth International Conference on Informationand Knowledge Management, pages 580?582, Atlanta,Georgia, USA.Anselm Blumer, Janet Blumer, Andrzej Ehrenfeucht,David Haussler, and Joel Seiferas.
1985.
The smallestautomaton recognizing the subwords of a text.
Theo-retical Computer Science, 40(1):31?55.Anselm Blumer, Janet Blumer, David Haussler, Ross Mc-Connel, and Andrzej Ehrenfeucht.
1987.
Completeinverted files for efficient text retrieval and analysis.Journal of the ACM, 34(3):578?595.Maxime Crochemore.
1986.
Transducers and repeti-tions.
Theoretical Computer Science, 45(1):63?86.Werner Kuich and Arto Salomaa.
1986.
Semirings,Automata, Languages.
Number 5 in EATCS Mono-graphs on Theoretical Computer Science.
Springer-Verlag, Berlin, Germany.Andrej Ljolje, Murat Saraclar, Michiel Bacchiani,Michael Collins, and Brian Roark.
2002.
The AT&TRT-02 STT system.
In Proc.
RT02 Workshop, Vienna,Virginia.Beth Logan, Pedro Moreno, and Om Deshmukh.
2002.Word and sub-word indexing approaches for reducingthe effects of OOV queries on spoken audio.
In Proc.HLT.Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-ley.
1996.
Weighted Automata in Text and SpeechProcessing.
In Proceedings of the 12th biennial Euro-pean Conference on Artificial Intelligence (ECAI-96),Workshop on Extended finite state models of language,Budapest, Hungary.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
2000.
The Design Principles of aWeighted Finite-State Transducer Library.
The-oretical Computer Science, 231:17?32, January.http://www.research.att.com/sw/tools/fsm.Mehryar Mohri.
1997.
Finite-State Transducers in Lan-guage and Speech Processing.
Computational Lin-guistics, 23:2.Mehryar Mohri.
2002.
Semiring Frameworks and Algo-rithms for Shortest-Distance Problems.
Journal of Au-tomata, Languages and Combinatorics, 7(3):321?350.Murat Saraclar and Richard Sproat.
2004.
Lattice-basedsearch for spoken utterance retrieval.
In Proc.
HLT-NAACL.Murat Saraclar, Michael Riley, Enrico Bocchieri, andVincent Goffin.
2002.
Towards automatic closed cap-tioning: Low latency real-time broadcast news tran-scription.
In Proceedings of the International Confer-ence on Spoken Language Processing (ICSLP), Den-ver, Colorado, USA.Michael Witbrock and Alexander Hauptmann.
1997.
Us-ing words and phonetic strings for efficient informa-tion retrieval from imperfectly transcribed spoken doc-uments.
In 2nd ACM International Conference on Dig-ital Libraries (DL?97), pages 30?35, Philadelphia, PA,July.
