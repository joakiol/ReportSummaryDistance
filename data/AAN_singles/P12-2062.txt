Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 317?321,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAn Exploration of Forest-to-String Translation:Does Translation Help or Hurt Parsing?Hui ZhangUniversity of Southern CaliforniaDepartment of Computer Sciencehzhang@isi.eduDavid ChiangUniversity of Southern CaliforniaInformation Sciences Institutechiang@isi.eduAbstractSyntax-based translation models that operateon the output of a source-language parser havebeen shown to perform better if allowed tochoose from a set of possible parses.
In thispaper, we investigate whether this is because itallows the translation stage to overcome parsererrors or to override the syntactic structure it-self.
We find that it is primarily the latter, butthat under the right conditions, the transla-tion stage does correct parser errors, improv-ing parsing accuracy on the Chinese Treebank.1 IntroductionTree-to-string translation systems (Liu et al, 2006;Huang et al, 2006) typically employ a pipeline oftwo stages: a syntactic parser for the source lan-guage, and a decoder that translates source-languagetrees into target-language strings.
Originally, theoutput of the parser stage was a single parse tree, andthis type of system has been shown to outperformphrase-based translation on, for instance, Chinese-to-English translation (Liu et al, 2006).
More recentwork has shown that translation quality is improvedfurther if the parser outputs a weighted parse forest,that is, a representation of a whole distribution overpossible parse trees (Mi et al, 2008).
In this paper,we investigate two hypotheses to explain why.One hypothesis is that forest-to-string translationselects worse parses.
Although syntax often helpstranslation, there may be situations where syntax, orat least syntax in the way that our models use it, canimpose constraints that are too rigid for good-qualitytranslation (Liu et al, 2007; Zhang et al, 2008).For example, suppose that a tree-to-string systemencounters the following correct tree (only partialbracketing shown):(1) [NP j?
?ngj?`economyze?ngzha?ng]growthdeDEsu`du`rate?economic growth rate?Suppose further that the model has never seen thisphrase before, although it has seen the subphraseze?ngzha?ng de su`du` ?growth rate?.
Because this sub-phrase is not a syntactic unit in sentence (1), the sys-tem will be unable to translate it.
But a forest-to-string system would be free to choose another (in-correct but plausible) bracketing:(2) j?
?ngj?`economy[NP ze?ngzha?nggrowthdeDEsu`du`]rateand successfully translate it using rules learned fromobserved data.The other hypothesis is that forest-to-string trans-lation selects better parses.
For example, if a Chi-nese parser is given the input ca?njia?
bia?ojie?
de hu?nl?
?,it might consider two structures:(3) [VP ca?njia?attendbia?ojie?]cousindeDEhu?nl?
?wedding?wedding that attends a cousin?
(4) ca?njia?attend[NP bia?ojie?cousindeDEhu?nl??
]wedding?attend a cousin?s wedding?The two structures have two different translationsinto English, shown above.
While the parser prefersstructure (3), an n-gram language model would eas-ily prefer translation (4) and, therefore, its corre-sponding Chinese parse.317(a) f f fparser?????
.f f fdecoder??????
e e e esource source targetstring tree string(b) f f fparser?????
.f f fdecoder??????
e e e esource source targetstring forest stringFigure 1: (a) In tree-to-string translation, the parser gen-erates a single tree which the decoder must use to gen-erate a translation.
(b) In forest-to-string translation, theparser generates a forest of possible trees, any of whichthe decoder can use to generate a translation.Previous work has shown that an observed target-language translation can improve parsing of source-language text (Burkett and Klein, 2008; Huang et al,2009), but to our knowledge, only Chen et al (2011)have explored the case where the target-languagetranslation is unobserved.Below, we carry out experiments to test thesetwo hypotheses.
We measure the accuracy (usinglabeled-bracket F1) of the parses that the translationmodel selects, and find that they are worse than theparses selected by the parser.
Our basic conclusion,then, is that the parses that help translation (accord-ing to Bleu) are, on average, worse parses.
That is,forest-to-string translation hurts parsing.But there is a twist.
Neither labeled-bracket F1nor Bleu is a perfect metric of the phenomena it ismeant to measure, and our translation system is op-timized to maximize Bleu.
If we optimize our sys-tem to maximize labeled-bracket F1 instead, we findthat our translation system selects parses that scorehigher than the baseline parser?s.
That is, forest-to-string translation can help parsing.2 BackgroundWe provide here only a cursory overview of tree-to-string and forest-to-string translation.
For moredetails, the reader is referred to the original papersdescribing them (Liu et al, 2006; Mi et al, 2008).Figure 1a illustrates the tree-to-string transla-tion pipeline.
The parser stage can be any phrase-structure parser; it computes a parse for each source-language string.
The decoder stage translates thesource-language tree into a target-language string,using a synchronous tree-substitution grammar.In forest-to-string translation (Figure 1b), theparser outputs a forest of possible parses of eachsource-language string.
The decoder uses the samerules as in tree-to-string translation, but is free to se-lect any of the trees contained in the parse forest.3 Translation hurts parsingThe simplest experiment to carry out is to exam-ine the parses actually selected by the decoder, andsee whether they are better or worse than the parsesselected by the parser.
If they are worse, this sup-ports the hypothesis that syntax can hurt translation.If they are better, we can conclude that translationcan help parsing.
In this initial experiment, we findthat the former is the case.3.1 SetupThe baseline parser is the Charniak parser (Char-niak, 2000).
We trained it on the Chinese Treebank(CTB) 5.1, split as shown in Table 1, followingDuan et al (2007).1 The parser outputs a parse forestannotated with head words and other information.Since the decoder does not use these annotations,we use the max-rule algorithm (Petrov et al, 2006)to (approximately) sum them out.
As a side bene-fit, this improves parsing accuracy from 77.76% to78.42% F1.
The weight of a hyperedge in this for-est is its posterior probability, given the input string.We retain these weights as a feature in the translationmodel.The decoder stage is a forest-to-string system (Liuet al, 2006; Mi et al, 2008) for Chinese-to-Englishtranslation.
The datasets used are listed in Ta-ble 1.
We generated word alignments with GIZA++and symmetrized them using the grow-diag-final-and heuristic.
We parsed the Chinese side usingthe Charniak parser as described above, and per-formed forest-based rule extraction (Mi and Huang,2008) with a maximum height of 3 nodes.
We usedthe same features as Mi and Huang (2008).
Thelanguage model was a trigram model with modi-fied Kneser-Ney smoothing (Kneser and Ney, 1995;Chen and Goodman, 1998), trained on the target1The more common split, used by Bikel and Chiang (2000),has flaws that are described by Levy and Manning (2003).318Parsing TranslationTrain CTB 1?815 FBISCTB 1101?1136Dev CTB 900?931 NIST 2002CTB 1148?1151Test CTB 816?885 NIST 2003CTB 1137?1147Table 1: Data used for training and testing the parsing andtranslation models.Parsing TranslationSystem Objective F1% Bleu%Charniak n/a 78.42 n/atree-to-string max-Bleu 78.42 23.07forest-to-string max-Bleu 77.75 24.60forest-to-string max-F1 78.81 19.18Table 2: Forest-to-string translation outperforms tree-to-string translation according to Bleu, but the decreasesparsing accuracy according to labeled-bracket F1.
How-ever, when we train to maximize labeled-bracket F1,forest-to-string translation yields better parses than bothtree-to-string translation and the original parser.side of the training data.
We used minimum-error-rate (MER) training to optimize the feature weights(Och, 2003) to maximize Bleu.At decoding time, we select the best derivationand extract its source tree.
In principle, we oughtto sum over all derivations for each source tree; butthe approximations that we tried (n-best list crunch-ing, max-rule decoding, minimum Bayes risk) didnot appear to help.3.2 ResultsTable 2 shows the main results of our experiments.In the second and third line, we see that the forest-to-string system outperforms the tree-to-string sys-tem by 1.53 Bleu, consistent with previously pub-lished results (Mi et al, 2008; Zhang et al, 2009).However, we also find that the trees selected by theforest-to-string system score much lower accordingto labeled-bracket F1.
This suggests that the reasonthe forest-to-string system is able to generate bettertranslations is that it can soften the constraints im-posed by the syntax of the source language.4 Translation helps parsingWe have found that better translations can be ob-tained by settling for worse parses.
However, trans-lation accuracy is measured using Bleu and pars-ing accuracy is measured using labeled-bracket F1,and neither of these is a perfect metric of the phe-nomenon it is meant to measure.
Moreover, we op-timized the translation model in order to maximizeBleu.
It is known that when MER training is usedto optimize one translation metric, other translationmetrics suffer (Och, 2003); much more, then, canwe expect that optimizing Bleu will cause labeled-bracket F1 to suffer.
In this section, we try optimiz-ing labeled-bracket F1, and find that, in this case, thetranslation model does indeed select parses that arebetter on average.4.1 SetupMER training with labeled-bracket F1 as an objec-tive function is straightforward.
At each iteration ofMER training, we run the parser and decoder overthe CTB dev set to generate an n-best list of possibletranslation derivations (Huang and Chiang, 2005).For each derivation, we extract its Chinese parse treeand compute the number of brackets guessed andthe number matched against the gold-standard parsetree.
A trivial modification of the MER trainer thenoptimizes the feature weights to maximize labeled-bracket F1.A technical challenge that arises is ensuring di-versity in the n-best lists.
The MER trainer re-quires that each list contain enough unique transla-tions (when maximizing Bleu) or source trees (whenmaximizing labeled-bracket F1).
However, becauseone source tree may lead to many translation deriva-tions, the n-best list may contain only a few uniquesource trees, or in the extreme case, the derivationsmay all have the same source tree.
We use a variantof the n-best algorithm that allows efficient genera-tion of equivalence classes of derivations (Huang etal., 2006).
The standard algorithm works by gener-ating, at each node of the forest, a list of the bestsubderivations at that node; the variant drops a sub-derivation if it has the same source tree as a higher-scoring subderivation.319Maximumrule height F1%3 78.814 78.935 79.14LM data(lines) F1%none 78.78100 78.7930k 78.67300k 79.1413M 79.24Features F1%monolingual 78.89+ bilingual 79.24Parallel data(lines) F1%60k 78.00120k 78.16300k 79.24(a) (b) (c) (d)Table 3: Effect of variations on parsing performance.
(a) Increasing the maximum translation rule height increasesparsing accuracy further.
(b) Increasing/decreasing the language model size increases/decreases parsing accuracy.
(c) Decreasing the parallel text size decreases parsing accuracy.
(d) Removing all bilingual features decreases parsingaccuracy, but only slightly.4.2 ResultsThe last line of Table 2 shows the results of thissecond experiment.
The system trained to opti-mize labeled-bracket F1 (max-F1) obtains a muchlower Bleu score than the one trained to maximizeBleu (max-Bleu)?unsurprisingly, because a singlesource-side parse can yield many different transla-tions, but the objective function scores them equally.What is more interesting is that the max-F1 systemobtains a higher F1 score, not only compared withthe max-Bleu system but also the original parser.We then tried various settings to investigate whatfactors affect parsing performance.
First, we foundthat increasing the maximum rule height increasesF1 further (Table 3a).One of the motivations of our method is that bilin-gual information (especially the language model)can help disambiguate the source side structures.
Totest this, we varied the size of the corpus used to trainthe language model (keeping a maximum rule heightof 5 from the previous experiment).
The 13M-linelanguage model adds the Xinhua portion of Giga-word 3.
In Table 3b we see that the parsing perfor-mance does increase with the language model size,with the largest language model yielding a net im-provement of 0.82 over the baseline parser.To test further the importance of bilingual infor-mation, we compared against a system built onlyfrom the Chinese side of the parallel text (with eachword aligned to itself).
We removed all features thatuse bilingual information, retaining only the parserprobability and the phrase penalty.
In their placewe added a new feature, the probability of a rule?ssource side tree given its root label, which is essen-tially the same model used in Data-Oriented Parsing(Bod, 1992).
Table 3c shows that this system stilloutperforms the original parser.
In other words, partof the gain is not attributable to translation, but ad-ditional source-side context and data that the trans-lation model happens to capture.Finally, we varied the size of the parallel text(keeping a maximum rule height of 5 and the largestlanguage model) and found that, as expected, pars-ing performance correlates with parallel data size(Table 3d).5 ConclusionWe set out to investigate why forest-to-string trans-lation outperforms tree-to-string translation.
Bycomparing their performance as Chinese parsers, wefound that forest-to-string translation sacrifices pars-ing accuracy, suggesting that forest-to-string trans-lation works by overriding constraints imposed bysyntax.
But when we optimized the system to max-imize labeled-bracket F1, we found that, in fact,forest-to-string translation is able to achieve higheraccuracy, by 0.82 F1%, than the baseline Chineseparser, demonstrating that, to a certain extent, forest-to-string translation is able to correct parsing errors.AcknowledgementsWe are grateful to the anonymous reviewers fortheir helpful comments.
This research was sup-ported in part by DARPA under contract DOI-NBCD11AP00244.320ReferencesDaniel M. Bikel and David Chiang.
2000.
Two statis-tical parsing models applied to the Chinese Treebank.In Proc.
Second Chinese Language Processing Work-shop, pages 1?6.Rens Bod.
1992.
A computational model of languageperformance: Data Oriented Parsing.
In Proc.
COL-ING 1992, pages 855?859.David Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Proc.EMNLP 2008, pages 877?886.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
NAACL, pages 132?139.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Harvard UniversityCenter for Research in Computing Technology.Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-masa Tsuruoka, Yujie Zhang, Yiou Wang, KentaroTorisawa, and Haizhou Li.
2011.
SMT helps bitextdependency parsing.
In Proc.
EMNLP 2011, pages73?83.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Probabilis-tic models for action-based Chinese dependency pars-ing.
In Proc.
ECML 2007, pages 559?566.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
IWPT 2005, pages 53?64.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
AMTA 2006, pages 65?73.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proc.
EMNLP 2009, pages 1222?1231.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
In Proc.ICASSP 1995, pages 181?184.Roger Levy and Christopher D. Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank?
InProc.
ACL 2003, pages 439?446.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proc.
COLING-ACL 2006, pages 609?616.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007.Forest-to-string statistical translation rules.
In Proc.ACL 2007, pages 704?711.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proc.
EMNLP 2008, pages206?214.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
ACL-08: HLT, pages 192?199.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
ACL 2003,pages 160?167.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
COLING-ACL 2006,pages 433?440.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree translation model.In Proc.
ACL-08: HLT, pages 559?567.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, andChew Lim Tan.
2009.
Forest-based tree sequence tostring translation model.
In Proc.
ACL-IJCNLP 2009,pages 172?180.321
