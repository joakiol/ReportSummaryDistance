Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 84?89,Baltimore, Maryland, USA.
June 27, 2014. c?2014 Association for Computational LinguisticsOpinion Mining and Topic Categorization with Novel Term WeightingTatiana GasanovaInstitute of Communications Engineer-ing, Ulm University, Germanytatiana.gasanova@uni-ulm.deRoman SergienkoInstitute of Communications Engineer-ing, Ulm University, Germanyroman.sergienko@uni-ulm.deShakhnaz AkhmedovaInstitute of Computer Science andTelecommunications, Siberian StateAerospace University, Russiashahnaz@inbox.ruEugene SemenkinInstitute of Computer Science andTelecommunications, Siberian StateAerospace University, Russiaeugenesemenkin@yandex.comWolfgang MinkerInstitute of Communications Engineer-ing, Ulm University, Germanywolfgang.minker@uni-ulm.deAbstractIn this paper we investigate the efficiency ofthe novel term weighting algorithm for opin-ion mining and topic categorization of arti-cles from newspapers and Internet.
We com-pare the novel term weighting technique withexisting approaches such as TF-IDF andConfWeight.
The performance on the datafrom the text-mining campaigns DEFT?07and DEFT?08 shows that the proposed meth-od can compete with existing information re-trieval models in classification quality andthat it is computationally faster.
The pro-posed text preprocessing method can be ap-plied in large-scale information retrieval anddata mining problems and it can be easilytransported to different domains and differentlanguages since it does not require any do-main-related or linguistic information.1 IntroductionNowadays, Internet and social media generate ahuge amount of textual information.
It is in-creasingly important to develop methods of textprocessing such as text classification.
Text clas-sification is very important for such problemsas automatic opining mining (sentiment analy-sis) and topic categorization of different articlesfrom newspapers and Internet.Text classification can be considered to be apart of natural language understanding, wherethere is a set of predefined categories and thetask is to automatically assign new documentsto one of these categories.
The method of textpreprocessing and text representation influencesthe results that are obtained even with the sameclassification algorithms.The most popular model for text classifica-tion is vector space model.
In this case text cat-egorization may be considered as a machinelearning problem.
Complexity of text categori-zation with vector space model is compoundedby the need to extract the numerical data fromtext information before applying machine learn-ing methods.
Therefore text categorization con-sists of two parts: text preprocessing and classi-fication using obtained numerical data.All text preprocessing methods are based onthe idea that the category of the document de-pends on the words or phrases from this docu-ment.
The simplest approach is to take eachword of the document as a binary coordinateand the dimension of the feature space will bethe number of words in our dictionary.There exist more advanced approaches fortext preprocessing to overcome this problemsuch as TF-IDF (Salton and Buckley, 1988) andConfWeight methods (Soucy and Mineau,2005).
A novel term weighting method (Gasa-nova et al., 2013) is also considered, which has84some similarities with the ConfWeight method,but has improved computational efficiency.
It isimportant to notice that we use no morphologi-cal or stop-word filtering before text prepro-cessing.
It means that the text preprocessing canbe performed without expert or linguisticknowledge and that the text preprocessing islanguage-independent.In this paper we have used k-nearest neigh-bors algorithm, Bayes Classifier, support vectormachine (SVM) generated and optimized withCOBRA (Co-Operation of Biology Related Al-gorithms) which has been proposed byAkhmedova and Semenkin (2013), RocchioClassifier or Nearest Centroid Algorithm (Roc-chio, 1971) and Neural Network as classifica-tion methods.
RapidMiner and Microsoft VisualStudio C++ 2010 have been used as implemen-tation software.For the application of algorithms and com-parison of the results we have used the DEFT(?D?fi Fouille de Texte?)
Evaluation Package2008 (Proceedings of the 4th DEFT Workshop,2008) which has been provided by ELRA andpublically available corpora from DEFT?07(Proceedings of the 3rd DEFT Workshop,2007).The main aim of this work is to evaluate thecompetitiveness of the novel term weighting(Gasanova et al., 2013) in comparison with thestate-of-the-art techniques for opining miningand topic categorization.
The criteria using inthe evaluation are classification quality andcomputational efficiency.This paper is organized as follows: in Section2, we describe details of the corpora.
Section 3presents text preprocessing methods.
In Section4 we describe the classification algorithmswhich we have used to compare different textpreprocessing techniques.
Section 5 reports onthe experimental results.
Finally, we provideconcluding remarks in Section 6.2 Corpora DescriptionThe focus of DEFT 2007 campaign is the sen-timent analysis, also called opinion mining.
Wehave used 3 publically available corpora: re-views on books and movies (Books), reviews onvideo games (Games) and political debatesabout energy project (Debates).The topic of DEFT 2008 edition is related tothe text classification by categories and genres.The data consists of two corpora (T1 and T2)containing articles of two genres: articles ex-tracted from French daily newspaper Le Mondeand encyclopedic articles from Wikipedia inFrench language.
This paper reports on the re-sults obtained using both tasks of the campaignand focuses on detecting the category.Corpus Size ClassesBooks Train size = 2074Test size = 1386Vocabulary = 525070: negative,1: neutral,2: positiveGames Train size = 2537Test size = 1694Vocabulary = 631440: negative,1: neutral,2: positiveDebates Train size = 17299Test size = 11533Vocabulary = 596150: against,1: forTable 1.
Corpora description (DEFT?07)Corpus Size ClassesT1 Train size = 15223Test size = 10596Vocabulary = 2029790: Sport,1: Economy,2: Art,3: TelevisionT2 Train size = 23550Test size = 15693Vocabulary = 2624000: France,1: International,2: Literature,3: Science,4: SocietyTable 2.
Corpora description (DEFT?08)All databases are divided into a training(60% of the whole number of articles) and a testset (40%).
To apply our algorithms we extract-ed all words which appear in the training setregardless of the letter case and we also exclud-ed dots, commas and other punctual signs.
Wehave not used any additional filtering as exclud-ing the stop or ignore words.3 Text Preprocessing Methods3.1 Binary preprocessingWe take each word of the document as a binarycoordinate and the size of the feature space willbe the size of our vocabulary (?bag of words?
).3.2 TF-IDFTF-IDF is a well-known approach for text pre-processing based on multiplication of term fre-quency tfij (ratio between the number of timesthe ith word occurs in the jth document and thedocument size) and inverse document frequen-cy idfi.????
=????
?,  (1)85where tij is the number of times the ith word oc-curs in the jth document.
Tj is the document size(number of the words in the document).There are different ways to calculate theweight of each word.
In this paper we run clas-sification algorithms with the following vari-ants.1) TF-IDF 1????
= ???|?|?
?, (2)where |D| is the number of document in thetraining set and ??
is the number of documentsthat have the ith word.2) TF-IDF 2The formula is given by equation (2) except??
is calculated as the number of times ith wordappears in all documents from the training set.3) TF-IDF 3????
= ?|?|????,?
?
(0,1), (3)where ??
is calculated as in TF-IDF 1 and ?
isthe parameter (in this paper we have tested ?
=0.1, 0.5, 0.9).4) TF-IDF 4The formula is given by equation (3) except??
is calculated as in TF-IDF 4.3.3 ConfWeightMaximum Strength (Maxstr) is an alternativemethod to find the word weights.
This approachhas been proposed by Soucy and Mineau(2005).
It implicitly does feature selection sinceall frequent words have zero weights.
The mainidea of the method is that the feature f has anon-zero weight in class c only if the f frequen-cy in documents of the c class is greater thanthe f frequency in all other classes.The ConfWeight method uses Maxstr as ananalog of IDF:????????????
= ????????
+ 1?
?
??????(?
).Numerical experiments (Soucy and Mineau,2005) have shown that the ConfWeight methodcould be more effective than TF-IDF with SVMand k-NN as classification methods.
The maindrawback of the ConfWeight method is compu-tational complexity.
This method is more com-putationally demanding than TF-IDF methodbecause the ConfWeight method requires time-consuming statistical calculations such as Stu-dent distribution calculation and confidenceinterval definition for each word.3.4 Novel Term Weighting (TW)The main idea of the method (Gasanova et al.,2013) is similar to ConfWeight but it is not sotime-consuming.
The idea is that every wordthat appears in the article has to contributesome value to the certain class and the classwith the biggest value we define as a winner forthis article.For each term we assign a real number termrelevance that depends on the frequency in ut-terances.
Term weight is calculated using amodified formula of fuzzy rules relevance esti-mation for fuzzy classifiers (Ishibuchi et al.,1999).
Membership function has been replacedby word frequency in the current class.
The de-tails of the procedure are the following:Let L be the number of classes; ni is thenumber of articles which belong to the ith class;Nij is the number of the jth word occurrence inall articles from the ith class; Tij = Nij / ni is therelative frequency of the jth word occurrence inthe ith class.??
= max?
???
, ??
= arg (max?
???)
is thenumber of class which we assign to the jth word;The term relevance, Cj, is given by).11(111??
?==?
?=LSiiijjLijijjTLRTC(4)Cj is higher if the word occurs more often inone class than if it appears in many classes.
Weuse novel TW as an analog of IDF for text pre-processing.The learning phase consists of counting the Cvalues for each term; it means that thisalgorithm uses the statistical informationobtained from the training set.4 Classification MethodsWe have considered 11 different text prepro-cessing methods (4 modifications of TF-IDF,two of them with three different values of ?parameter, binary representation, ConfWeightand the novel TW method) and compared themusing different classification algorithms.
Themethods have been implemented usingRapidMiner (Shafait, 2010) and Microsoft Vis-ual Studio C++ 2010 for Rocchio classifier andSVM.
The classification methods are:- k-nearest neighbors algorithm with dis-tance weighting (we have varied k from 1 to15);- kernel Bayes classifier with Laplace cor-rection;- neural network with error back propaga-tion (standard setting in RapidMiner);- Rocchio classifier with different metricsand ?
parameter;86- support vector machine (SVM) generatedand optimized with Co-Operation of BiologyRelated Algorithms (COBRA).Rocchio classifier (Rocchio, 1971) is a well-known classifier based on the search of thenearest centroid.
For each category we calculatea weighted centroid:??
=1|??|?
?
?
?1???,???????????
?
?????,?
,where ??
is a set of documents which belong tothe class c; ??,??????
are k documents which do notbelong to the class c and which are close to thecentroid1|??|?
?;????
?
is parameter correspondsto relative importance of negative precedents.The given document is put to the class with thenearest centroid.
In this work we have appliedRocchio classifier with ?
?
(0.1; 0.9) and withthree different metrics: taxicab distance,Euclidean metric and cosine similarity.COBRA is a new meta-heuristic algorithmwhich has been proposed by Akhmedova andSemenkin (2013).
It is based on cooperation ofbiology inspired algorithms such as ParticleSwarm Optimization (Kennedy and Eberhart,1995), Wolf Pack Search Algorithm (Yang,2007), Firefly Algorithm (Yang, 2008), CuckooSearch Algorithm (Yang and Deb, 2009) andBat Algorithm (Yang, 2010).
For generatingSVM-machine the original COBRA is used:each individual in all populations represents aset of kernel function?s parameters .,, d?
?Then for each individual constrained modifica-tion of COBRA is applied for finding vector wand shift factor b.
And finally individual thatshowed the best classification rate is chosen asthe designed classifier.5 Experimental ResultsThe DEFT (?D?fi Fouille de Texte?)
EvaluationPackage 2008 and publically available corporafrom DEFT?07 (Books, Games and Debates)have been used for algorithms application andresults comparison.
In order to evaluate ob-tained results with the campaign participants wehave to use the same measure of classificationquality: precision, recall and F-score.Precision for each class i is calculated as thenumber of correctly classified articles for class idivided by the number of all articles which al-gorithm assigned for this class.
Recall is thenumber of correctly classified articles for class idivided by the number of articles that shouldhave been in this class.
Overall precision andrecall are calculated as the arithmetic mean ofthe precisions and recalls for all classes (macro-average).
F-score is calculated as the harmonicmean of precision and recall.Tables 3-7 present the F-scores obtained onthe test corpora.
The best values for each prob-lem are shown in bold.
Results of the all classi-fication algorithms are presented with the bestparameters.
We also present for each corpusonly the best TF-IDF modification.ClassificationalgorithmBinary  TF-IDFConfWeightNovelTWBayes 0.489 0.506 0.238 0.437k-NN 0.488  0.517 0.559 0.488Rocchio 0.479  0.498 0.557 0.537SVM (CO-BRA)0.558  0.580 0.588 0.619Neural network 0.475  0.505 0.570 0.493Table 3.
Classification results for BooksClassificationalgorithmBinary  TF-IDFConfWeightNovelTWBayes 0.653  0.652 0.210 0.675k-NN 0.703  0.701 0.720 0.700Rocchio 0.659  0.678 0.717 0.712SVM (CO-BRA)0.682  0.687 0.645 0.696Neural network 0.701  0.679 0.717 0.691Table 4.
Classification results for GamesClassificationalgorithmBinary  TF-IDFConfWeightNovelTWBayes 0.555  0.645 0.363 0.616k-NN 0.645  0.648 0.695 0.695Rocchio 0.636  0.646 0.697 0.696SVM (CO-BRA)0.673  0.669 0.714 0.700Neural network 0.656  0.647 0.705 0.697Table 5.
Classification results for DebatesClassificationalgorithmBinary  TF-IDFConfWeightNovelTWBayes 0.501  0.690 0.837 0.794k-NN 0.800  0.816 0.855 0.837Rocchio 0.794  0.825 0.853 0.838SVM (CO-BRA)0.788  0.827 0.840 0.856Neural network 0.783  0.830 0.853 0.854Table 6.
Classification results for T1ClassificationalgorithmBinary  TF-IDFConfWeightNovelTWBayes 0.569  0.728 0.712 0.746k-NN 0.728  0.786 0.785 0.811Rocchio 0.765  0.825 0.803 0.834SVM (CO-BRA)0.794  0.837 0.813 0.851Neural network 0.799  0.838 0.820 0.843Table 7.
Classification results for T287We can see from the Tables 3-7 that the bestF-scores have been obtained with eitherConfWeight or novel Term Weighting prepro-cessing.
The algorithm performances on theGames and Debates corpora achieved the bestresults with ConfWeight; however, we can seethat the F-scores obtained with novel TermWeighting preprocessing are very similar(0.712 and 0.720 for Games; 0.700 and 0.714for Debates).
Almost all best results have beenobtained with SVM except the Games databasewhere we achieved the highest F-score with k-NN algorithm.This paper focuses on the text preprocessingmethods which do not require language or do-main-related information; therefore, we havenot tried to achieve the best possible classifica-tion quality.
However, the result obtained onBooks corpus with novel TW preprocessing andSVM (generated using COBRA) as classifica-tion algorithm has reached 0.619 F-score whichis higher than the best known performance0.603 (Proceedings of the 3rd DEFT Workshop,2007).
Performances on other corpora haveachieved close F-score values to the best sub-missions of the DEFT?07 and DEFT?08 partici-pants.We have also measured computational effi-ciency of each text preprocessing technique.We have run each method 20 times using theBaden-W?rttemberg Grid (bwGRiD) ClusterUlm (Every blade comprehends two 4-CoreIntel Harpertown CPUs with 2.83 GHz and 16GByte RAM).
After that we calculated averagevalues and checked statistical significance ofthe results.Figure 1 and Figure 2 compare average com-putational time in minutes for different prepro-cessing methods applied on DEFT?07 andDEFT?08 corpora.Figure 1.
Computational efficiency of text pre-processing methods (DEFT?07)Figure 2.
Computational efficiency of text pre-processing methods (DEFT?08)The average value for all TF-IDF modifica-tions is presented because the time variation forthe modifications is not significant.We can see in Figure 1 and Figure 2 that TF-IDF and novel TW require almost the samecomputational time.
The most time-consumingmethod is ConfWeight (CW).
It requires ap-proximately six times more time than TF-IDFand novel TW for DEFT?08 corpora and aboutthree-four times more time than TF-IDF andnovel TW for DEFT?07 databases.6 ConclusionThis paper reported on text classification exper-iments on 5 different corpora of opinion miningand topic categorization using several classifi-cation methods with different text prepro-cessing.
We have used ?bag of words?, TF-IDFmodifications, ConfWeight and the novel termweighting approach as preprocessing tech-niques.
K-nearest neighbors algorithms, Bayesclassifier, Rocchio classifier, support vectormachine trained by COBRA and Neural Net-work have been applied as classification algo-rithms.The novel term weighting method gives simi-lar or better classification quality than theConfWeight method but it requires the sameamount of time as TF-IDF.
Almost all best re-sults have been obtained with SVM generatedand optimized with Co-Operation of BiologyRelated Algorithms (COBRA).We can conclude that numerical experimentshave shown computational and classificationefficiency of the proposed method (the novelTW) in comparison with existing text prepro-cessing techniques for opinion mining and topiccategorization.88ReferencesAkhmedova Sh.
and Semenkin E. 2013.
Co-Operation of Biology Related Algorithms.
Pro-ceedings of the IEEE Congress on EvolutionaryComputation (CEC 2013):2207-2214.Association Fran?aise d?Intelligence Artificielle.2007.
Proceedings of the 3rd DEFT Workshop.DEFT '07.
AFIA, Grenoble, France.Gasanova T., Sergienko R., Minker W., SemenkinE.
and Zhukov E. 2013.
A Semi-supervised Ap-proach for Natural Language Call Routing.
Pro-ceedings of the SIGDIAL 2013 Conference:344-348.Ishibuchi H., Nakashima T., and Murata T. 1999.Performance evaluation of fuzzy classifier sys-tems for multidimensional pattern classificationproblems.
IEEE Trans.
on Systems, Man, and Cy-bernetics, 29:601-618.Kennedy J. and Eberhart R. 1995.
Particle SwarmOptimization.
Proceedings of IEEE InternationalConference on Neural Networks:1942-1948.Le traitement automatique du langage naturel ou dela langue naturelle.
2008.
Proceedings of the 4thDEFT Workshop.
DEFT '08.
TALN, Avignon,France.Salton G. and Buckley C. 1988.
Term-WeightingApproaches in Automatic Text Retrieval.
Infor-mation Processing and Management:513-523.Shafait F., Reif M., Kofler C., and Breuel T. M.2010.
Pattern Recognition Engineering.RapidMiner Community Meeting and Conference,9.Soucy P. and Mineau G.W.
2005.
Beyond TFIDFWeighting for Text Categorization in the VectorSpace Model.
Proceedings of the 19th Interna-tional Joint Conference on Artificial Intelligence(IJCAI 2005):1130-1135.Rocchio J.
1971.
Relevance Feedback in Infor-mation Retrieval.
The SMART Retrieval System-Experiments in Automatic Document Processing,Prentice-Hall:313-323.Yang Ch.
2007.
Algorithm of Marriage in HoneyBees Optimization Based on the Wolf PackSearch.
Proceedings of International Conferenceon Intelligent Pervasive Computing:462-467.Yang X.S.
2008.
Nature-Inspired Metaheuristic Al-gorithms.Yang X.S.
and Deb S. 2009.
Cuckoo search viaLevy flights.
Proceedings of World Congress onNature & Biologically Inspired Computing:210-214.Yang X.S.
2010.
A New Metaheuristic Bat-InspiredAlgorithm.
Proceedings of Nature Inspired Co-operative Strategies for Optimization (NISCO2010):65-74.89
