Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 358?361,Prague, June 2007. c?2007 Association for Computational LinguisticsUBC-ZAS: A k-NN based Multiclassifier System to perform WSD in aReduced Dimensional Vector SpaceAna Zelaia, Olatz Arregi and Basilio SierraComputer Science FacultyUniversity of the Basque Countryana.zelaia@ehu.esAbstractIn this article a multiclassifier approach forword sense disambiguation (WSD) prob-lems is presented, where a set of k-NNclassifiers is used to predict the category(sense) of each word.
In order to combinethe predictions generated by the multiclas-sifier, Bayesian voting is applied.
Throughall the classification process, a reduced di-mensional vector representation obtained bySingular Value Decomposition (SVD) isused.
Each word is considered an indepen-dent classification problem, and so differ-ent parameter setting, selected after a tun-ing phase, is applied to each word.
The ap-proach has been applied to the lexical sam-ple WSD subtask of SemEval 2007 (task17).1 IntroductionWord Sense Disambiguation (WSD) is an impor-tant component in many information organizationand management tasks.
Both, word representationand classification method are crucial steps in theword sense disambiguation process.
In this articleboth issues are considered.
On the one hand, LatentSemantic Indexing (LSI) (Deerwester et al, 1990),which is a variant of the vector space model (VSM)(Salton and McGill, 1983), is used in order to ob-tain the vector representation of the correspondingword.
This technique compresses vectors represent-ing word related contexts into vectors of a lower-dimensional space.
LSI, which is based on Singu-lar Value Decomposition (SVD) (Berry and Browne,1999) of matrices, has shown to have the abilityto extract the relations among features representingwords by means of their context of use, and has beensuccessfully applied to Information Retrieval tasks.On the other hand, a multiclassifier (Ho et al,1994) which uses different training databases is con-structed.
These databases are obtained from theoriginal training set by random subsampling.
Theimplementation of this approach is made by a modelinspired in bagging (Breiman, 1996), and the k-NNclassification algorithm (Dasarathy, 1991) is used tomake the sense predictions for testing words.Our group (UBC-ZAS) has participated in the lex-ical sample subtask of SemEval-2007 for task 17,which consists on 100 different words for which atraining and testing database have been provided.The aim of this article is to give a brief descrip-tion of our approach to deal with the WSD task andto show the results obtained.
In Section 2, our ap-proach is presented.
In Section 3, the experimen-tal setup is introduced.
The experimental results arepresented and discussed in Section 4, and finally,Section 5 contains some conclusions and commentson future work.2 Proposed ApproachIn this article a multiclassifier based WSD systemwhich classifies word senses represented in a re-duced dimensional vector space is proposed.In Figure 1 an illustration of the experiment per-formed for each one of the 100 words can be seen.First, vectors in the VSM are projected to the re-duced space by using SVD.
Next, random subsam-pling is applied to the training database TD to obtain358different training databases TDi.
Afterwards the k-NN classifier is applied for each TDi to make senselabel predictions.
Finally, Bayesian voting schemeis used to combine predictions, and cj will be thefinal sense label prediction for testing word q.d2d4d1d7...q1 q2qqd34d23d135d509TD2TD1...k?NN k?NNd50d256d98d2787d33d1989d55d4612d9VSMVSMSVDk?NNRandomSubsamplingBayesian votingTDd2d1.. .d3d4d5d6d7d6 d5d3.
.
.d1 d2d61d778d638 d848TraindndndnTestTDiqn?PSfrag replacementsR m R mR p R p R pR pMMp = Up?pV Tpqp = qT Up?
?1pc1 c2 ci cjqpqpqpFigure 1: Proposed approach for WSD taskIn the rest of this section, the preprocessing ap-plied, the SVD dimensionality reduction technique,the k-NN algorithm and the combination of classi-fiers used are briefly reviewed.2.1 PreprocessingIn order to obtain the vector representation for eachof the word contexts (documents, cases) given by theorganizers of the SemEval-2007 task, we used thefeatures extracted by the UBC-ALM participatinggroup (Agirre and Lopez de Lacalle, 2007).
Thesefeatures are local collocations (bigrams and trigramsformed with the words around the target), syn-tactic dependencies (object, subject, noun-modifier,preposition, and sibling) and Bag-of-words features(basically lemmas of the content words in the wholecontext, and in a ?4-word window).2.2 The SVD Dimensionality ReductionTechniqueThe classical Vector Space Model (VSM) has beensuccessfully employed to represent documents intext categorization tasks.
The newer method ofLatent Semantic Indexing (LSI) 1 (Deerwester et1http://lsi.research.telcordia.com,http://www.cs.utk.edu/?lsial., 1990) is a variant of the VSM in which docu-ments are represented in a lower dimensional spacecreated from the input training dataset.
The SVDtechnique used by LSI consists in factoring term-document matrix M into the product of three ma-trices, M = U?V T where ?
is a diagonal matrix ofsingular values, and U and V are orthogonal matri-ces of singular vectors (term and document vectors,respectively).For classification purposes (Dumais, 2004), thetraining and testing documents are projected to thereduced dimensional space, qp = qT Up?
?1p , by us-ing p singular values and the cosine is usually calcu-lated to measure the similarity between training andtesting document vectors.2.3 The k-NN classification algorithmk-NN is a distance based classification approach.According to this approach, given an arbitrary test-ing case, the k-NN classifier ranks its nearest neigh-bors among the training word senses, and uses thesense of the k top-ranking neighbors to predict thecorresponding to the word which is being analyzed(Dasarathy, 1991).2.4 Combination of classifiersThe combination of multiple classifiers has been in-tensively studied with the aim of improving the ac-curacy of individual components (Ho et al, 1994).A widely used technique to implement this approachis bagging (Breiman, 1996), where a set of trainingdatabases TDi is generated by selecting n trainingcases drawn randomly with replacement from theoriginal training database TD of n cases.
When aset of n1 < n training cases is chosen from the orig-inal training collection, the bagging is said to be ap-plied by random subsampling.
In fact, this is theapproach used in our work and the n1 parameter hasbeen selected via tuning.According to the random subsampling, given atesting case q, the classifier will make a label predic-tion ci based on each one of the training databasesTDi.
One way to combine the predictions is byBayesian voting (Dietterich, 1998), where a con-fidence value cvicj is calculated for each trainingdatabase TDi and sense cj to be predicted.
Theseconfidence values have been calculated based on thetraining collection.
Confidence values are summed359by sense; the sense cj that gets the highest value isfinally proposed as a prediction for the testing exam-ples.3 Experimental SetupIn the approach proposed in this article there aresome decisions that need to be taken, because it isnot clear (1) how many examples should be selectedfrom the TD of each word in order to create each oneof the TDi; (2) which is the appropriate dimensionto be used in order to represent word related con-texts (cases) for each word database; (3) which isthe appropriate number of TDi that should be cre-ated (number of classifiers to be used) and (4) whichis the appropriate number of neighbors to be consid-ered by the k-NN algorithm.Therefore, a parameter tuning phase was carriedout in order to fix the parameters.
We decided toadjust them for each word independently.In the following, the parameters are introducedand the tuning process carried out is explained.
Fortwo of the parameters (the number of classifiers andthe number of neigbors for k-NN), the tuning phasewas performed based on our previous experimentson document categorization tasks.3.1 The size of each TDiAs it was mentioned, the multiclassifier is imple-mented by random subsampling, where a set of n1 <n vectors is chosen from the original training collec-tion of n examples for a given word (n is a differ-ent value for each one of the 100 words).
Conse-quently, the size of each TDi will vary dependingon the value of n1.
The selection of different num-bers of cases was experimented for each word in twodifferent ways:a) according to the following equation:n1 =s?i=1(2 + b tij c), j = 1, .
.
.
, 10where ti is the total number of training casesin the sense ci and s is the total number ofsenses for the given word.
By dividing param-eter ti by j, the number of cases selected fromeach sense preserves the proportion of cases persense in the original one.
However, it has to betaken into account that some of the senses havea very low number of cases assigned to them.By summing 2, at least 2 cases will be selectedfrom each sense.
In order to decide the optimalvalue for parameter j, the classification experi-ment was carried out varying j from 1 to 10 foreach word.b) selecting a fixed number of cases for each ofthe senses which appeared for the word in thetraining database.
Again, in the tuning phase,different numbers of cases (from 1 to 10) havebeen used for each of the 100 words in order toselect a value for each of the words.We optimized the size of each TDi for each word byselecting the number of cases sometimes by proce-dure a) and sometimes by b).3.2 The dimension of the reduced Vector SpaceModelTaking into account the wide differences among thetraining case numbers for different words, we de-cided to project vectors representing them to differ-ent reduced dimensional spaces.
The selection ofthose dimensions is based on the number of trainingcases available for each word, and limited to 500; theused dimensions vary from 19 (for the word grant)to 481 (for the word part).3.3 The number of classifiers (TDi)Based on previous experiments carried out for docu-ment categorization (Zelaia et al, 2006), we decidedto create 30 classifiers for some words and 50 forothers, i.e.
30 or 50 individual k-NN algorithms willbe used by the multiclassifier in order to combineopinions by Bayesian voting.3.4 Number of neigbors for k-NNBased on our previous experiments, we decided touse k = 1 and k = 5, and to select the best for eachof the words.
The cosine similarity measure is usedin order to find the nearest or the 5 nearests.4 Experimental ResultsThe experiment was conducted by considering theoptimal values for parameters tuned by using thetraining case set.360Results published in this section were calculatedby the SemEval-2007 organizers.
Table 1 shows ac-curacy rates obtained by the 13 participants in theSemEval-2007, 17 task, lexical sample WSD sub-task.System Accuracy System Accuracy1.
0.887 8.
0.8032.
0.869 9.
0.7993.
0.864 10.
0.7964.
0.857 11.
0.7435.
0.851 12.
0.5386.
0.851 13.
0.5217.
0.838Table 1: Accuracy rates obtained by the 13 partici-pants.
SemEval-2007, 17 task (Lexical Sample)The result obtained by our system is 0.799 (the9th among 13 participants), 1 point over the meanaccuracy (0.786).5 Conclusions and Future WorkResults obtained show that the construction of amulticlassifier, together with the use of Bayesianvoting to combine label predictions, plays an im-portant role in the improvement of results.
We alsowant to remark that we used the SVD dimensional-ity reduction technique in order to reduce the vectorrepresentation of cases.The approach presented in this paper was alreadyused in a document categorization task.
However,we never used it for WSD task.
Therefore, in orderto adapt the method to the new task, we fixed someparameters based on our previous experiments (30-50 classifiers, k = 1, 5 for the k-NN algorithm) andtuned some other parameters by experimenting quitea high number of TDi sizes and using different di-mensions for each word.
However, we noticed thatthe application of our approach to a different task isnot straightforward.
Greater effort will have to bemade in order to tune the different parameters to thisspecific task of WSD.One of the main difficulties we found was the dif-ference in the number of training cases, comparingwith the high number usually available in other taskslike text categorization.As future work, we can think of applying a newpreprocessing approach in order to extract better fea-tures from the training database which could helpthe SVD technique improving the accuracy aftera dimensionality reduction is applied.
The use ofWordnet may help.6 AcknowledgementsThis research was supported by the University ofthe Basque Country by the project ?ANHITZ 2006:Language Technologies for Multilingual Interactionin Intelligent Environments?, IE 06-185We wish to thank to the UBC-ALM group forhelping us extracting learning features.ReferencesE.
Agirre and O. Lopez de Lacalle.
2007.
Ubc-alm:Combining k-nn with svd for wsd.
submited for pub-lication to SemEval-2007.M.W.
Berry and M. Browne.
1999.
UnderstandingSearch Engines: Mathematical Modeling and Text Re-trieval.
SIAM Society for Industrial and AppliedMathematics, ISBN: 0-89871-437-0, Philadelphia.L.
Breiman.
1996.
Bagging predictors.
Machine Learn-ing, 24(2):123?140.B.V.
Dasarathy.
1991.
Nearest Neighbor (NN) Norms:NN Pattern Recognition Classification Techniques.IEEE Computer Society Press.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41:391?407.T.G.
Dietterich.
1998.
Machine learning research: Fourcurrent directions.
The AI Magazine, 18(4):97?136.S.
Dumais.
2004.
Latent semantic analysis.
In ARIST(Annual Review of Information Science Technology),volume 38, pages 189?230.T.K.
Ho, J.J.
Hull, and S.N.
Srihari.
1994.
Decision com-bination in multiple classifier systems.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,16(1):66?75.G.
Salton and M. McGill.
1983.
Introduction to ModernInformation Retrieval.
McGraw-Hill.A.
Zelaia, I. Alegria, O. Arregi, and B. Sierra.
2006.A multiclassifier based document categorization sys-tem: profiting from the singular value decompositiondimensionality reduction technique.
In Proceedings ofthe Workshop on Learning Structured Information inNatural Language Applications, pages 25?32.361
