Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 23?33,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsUtilizing Extra-sentential Context for ParsingJackie Chi Kit Cheung and Gerald PennDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canada{jcheung,gpenn}@cs.toronto.eduAbstractSyntactic consistency is the preference toreuse a syntactic construction shortly after itsappearance in a discourse.
We present an anal-ysis of the WSJ portion of the Penn Tree-bank, and show that syntactic consistency ispervasive across productions with various left-hand side nonterminals.
Then, we implementa reranking constituent parser that makes useof extra-sentential context in its feature set.Using a linear-chain conditional random field,we improve parsing accuracy over the gen-erative baseline parser on the Penn TreebankWSJ corpus, rivalling a similar model thatdoes not make use of context.
We show thatthe context-aware and the context-ignorantrerankers perform well on different subsets ofthe evaluation data, suggesting a combined ap-proach would provide further improvement.We also compare parses made by models, andsuggest that context can be useful for parsingby capturing structural dependencies betweensentences as opposed to lexically governed de-pendencies.1 IntroductionRecent corpus linguistics work has produced ev-idence of syntactic consistency, the preference toreuse a syntactic construction shortly after its ap-pearance in a discourse (Gries, 2005; Dubey et al,2005; Reitter, 2008).
In addition, experimental stud-ies have confirmed the existence of syntactic prim-ing, the psycholinguistic phenomenon of syntacticconsistency1.
Both types of studies, however, have1Whether or not corpus-based studies of consistency haveany bearing on syntactic priming as a reality in the human mindlimited the constructions that are examined to partic-ular syntactic constructions and alternations.
For in-stance, Bock (1986) and Gries (2005) examine spe-cific constructions such as the passive voice, dativealternation and particle placement in phrasal verbs,and Dubey et al (2005) deal with the internal struc-ture of noun phrases.
In this work, we extend theseresults and present an analysis of the distribution ofall syntactic productions in the Penn Treebank WSJcorpus.
We provide evidence that syntactic consis-tency is a widespread phenomenon across produc-tions of various types of LHS nonterminals, includ-ing all of the commonly occurring ones.Despite this growing evidence that the probabilityof syntactic constructions is not independent of theextra-sentential context, current high-performancestatistical parsers (e.g.
(Petrov and Klein, 2007; Mc-Closky et al, 2006; Finkel et al, 2008)) rely solelyon intra-sentential features, considering the partic-ular grammatical constructions and lexical itemswithin the sentence being parsed.
We address thisby implementing a reranking parser which takes ad-vantage of features based on the context surroundingthe sentence.
The reranker outperforms the genera-tive baseline parser, and rivals a similar model thatdoes not make use of context.
We show that thecontext-aware and the context-ignorant models per-form well on different subsets of the evaluation data,suggesting a feature set that combines the two mod-els would provide further improvement.
Analysis ofthe rerankings made provides cases where contex-tual information has clearly improved parsing per-is a subject of debate.
See (Pickering and Branigan, 1999) and(Gries, 2005) for opposing viewpoints.23prior denominatorprior numerator pos_adapt numeratorpos_adapt denominatorfp,tf?p,tf?p,?t fp,?t?p p?ttFigure 1: Visual representation of calculation of prior andpositive adaptation probabilities.
t represents the pres-ence of a construction in the target set.
p represents thepresence of the construction in the prime set.formance, indicating the potential of extra-sententialcontextual information to aid parsing, especially forstructural dependencies between sentences, such asparallelism effects.2 Syntactic Consistency in the PennTreebank WSJSyntactic consistency has been examined by Dubeyet al (2005) for several English corpora, includingthe WSJ, Brown, and Switchboard corpora.
Theyhave provided evidence that syntactic consistencyexists not only within coordinate structures, but alsoin a variety of other contexts, such as within sen-tences, between sentences, within documents, andbetween speaker turns in the Switchboard corpus.However, their analysis rests on a selected numberof constructions concerning the internal structure ofnoun phrases.
We extend their result here to arbi-trary syntactic productions.There have also been studies into syntactic con-sistency that consider all syntactic productions indialogue corpora (Reitter, 2008; Buch and Pietsch,2010).
These studies find an inverse correlation be-tween the probability of the appearance of a syn-less frequent        Production-type deciles        more frequentProportionofconsistent production-typesFigure 2: Production-types (singletons removed) catego-rized into deciles by frequency and the proportion of theproduction-types in that bin that is consistent to a signifi-cant degree.tactic structure and the distance since its last occur-rence, which indicates syntactic consistency.
Thesestudies, however, do not provide consistency resultson subsets of production-types, such as by produc-tion LHS as our study does, so the implications thatcan be drawn from them for improving parsing areless apparent.We adopt the measure used by Dubey et al (2005)to quantify syntactic consistency, adaptation prob-ability.
This measure originates in work on lexicalpriming (Church, 2000), and quantifies the probabil-ity of a target word or construction w appearing in a?primed?
context.
Specifically, four frequencies arecalculated, based on whether the target constructionappears in the previous context (the prime set), andwhether the construction appears after this context(the target set):fp,?t(w) = # of times w in prime set onlyf?p,t(w) = # of times w in target set onlyf?p,?t(w) = # of times w in neither setfp,t(w) = # of times w in both setsWe also define N to be the sum of the four fre-24LHS prior pos adapt ratio + > prior sig.
insig.
+ < prior sig.ADJP 0.03 0.05 1.96 26 251 0ADVP 0.21 0.24 1.15 26 122 0NP 0.17 0.22 1.27 281 2284 0PP 0.56 0.58 1.04 32 125 0PRN 0.01 0.03 4.60 12 82 0PRT 0.06 0.08 1.40 3 3 0QP 0.03 0.18 5.41 24 147 0S 0.30 0.34 1.13 42 689 1SBAR 0.15 0.20 1.31 13 68 0SINV 0.01 0.01 1.00 3 77 0VP 0.08 0.12 1.56 148 1459 0WHADVP 0.04 0.08 1.84 2 8 0WHNP 0.07 0.10 1.39 3 47 0WHPP 0.01 0.02 2.65 1 1 0Table 1: Weighted average by production frequency among non-singleton production-types of prior and positive adap-tation probabilities, and the ratio between them.
The columns on the right show the number of production-typesfor which the positive adaptation probability is significantly greater than, not different from, or less than the priorprobability.
We exclude LHSs with a weighted average prior of less than 0.005, due to the small sample size.quencies.
Then, we define the prior and the positiveadaptation probability of a construction as follows(See also Figure 1):prior(w) = fp,t(w) + f?p,t(w)Npos adapt(w) = fp,t(w)fp,t(w) + fp,?t(w)A positive adaptation probability that is greaterthan the prior probability would be interpreted asevidence for syntactic consistency for that construc-tion.
We conduct ?2 tests for statistical signif-icance testing.
We analyze the Penn TreebankWSJ corpus according this schema for all produc-tions that occur in sections 2 to 22.
These are thestandard training and development sets for train-ing parsers.
We did not analyze section 23 in or-der not to use its characteristics in designing ourreranking parser so that we can use this section asour evaluation test set.
Our analysis focuses on theconsistency of rules between sentences, so we takethe previous sentence within the same article as theprime set, and the current sentence as the target setin calculating the probabilities given above.
Theraw data from which we produced our analysis areavailable at http://www.cs.toronto.edu/?jcheung/wsj_parallelism_data.txt.We first present results for consistency in all theproduction-types2, grouped by the LHS of the pro-duction.
Table 1 shows the weighted average priorand positive adaptation probabilities for productionsby LHS, where the weighting is done by the num-ber of occurrence of that production.
Production-types that only occur once are removed.
It alsoshows the number of production-types in which thepositive adaptation probability is statistically signif-icantly greater than, not significantly different from,and significantly lower than the prior probability.Quite remarkably, very few production-types aresignificantly less likely to reoccur compared to theprior probability.
Also note the wide variety of LHSsfor which there is a large number of production-types that are consistent to a significant degree.While a large number of production-types appearsnot to be significantly more likely to occur in aprimed context, this is due to the large number ofproduction-types which only appear a few times.Frequently occurring production-types mostly ex-hibit syntactic consistency.We show this in Figure 2, in which we putnon-singleton production-types into ten bins by fre-2That is, all occurrences of a production with a particularLHS and RHS.25Ten most frequent production-typesproduction f?p,t fp,t fp,?t prior pos adapt ratioPP ?
IN NP 5624 26224 5793 0.80 0.82 1.02NP ?
NP PP 9033 12451 9388 0.54 0.57 1.05NP ?
DT NN 9198 10585 9172 0.50 0.54 1.07S ?
NP VP 8745 9897 9033 0.47 0.52 1.11S ?
NP VP .
8576 8501 8888 0.43 0.49 1.13S ?
VP 8717 7867 9042 0.42 0.47 1.11NP ?
PRP 7208 5309 7285 0.32 0.42 1.33ADVP ?
RB 7986 3949 7905 0.30 0.33 1.10NP ?
NN 7630 3390 7568 0.28 0.31 1.11VP ?
TO VP 7039 3552 7250 0.27 0.33 1.23Ten most consistent among 10% most frequent production-typesproduction f?p,t fp,t fp,?t prior pos adapt ratioQP ?
# CD CD 51 18 45 0.00 0.29 163.85NP ?
JJ NNPS 52 7 53 0.00 0.12 78.25NP ?
NP , ADVP 109 24 99 0.00 0.20 58.05NP ?
DT JJ CD NN 63 6 67 0.00 0.08 47.14PP ?
IN NP NP 83 10 87 0.00 0.10 43.86QP ?
IN $ CD 51 3 49 0.00 0.06 42.28NP ?
NP : NP .
237 128 216 0.01 0.37 40.34INTJ ?
UH 59 4 60 0.00 0.06 39.26ADVP ?
IN NP 108 11 83 0.00 0.12 38.91NP ?
CD CD 133 21 128 0.00 0.14 36.21Table 2: Some instances of consistency effects of productions.
All productions?
pos adapt probability is significantlygreater than its prior probability at p < 10?6.quency and calculated the proportion of production-types in that bin for which the positive adaptationprobability is significantly greater than the prior.
It isclear that the most frequently occurring production-types are also the ones most likely to exhibit evi-dence of syntactic consistency.Table 2 shows the breakdown of the prior andpositive adaptation calculation components for theten most frequent production-types and the ten mostconsistent (by the ratio pos adapt / prior) produc-tions among the top decile of production-types.
Notethat all of these production-types are consistent to astatistically significant degree.
Interestingly, manyof the most consistent production-types have NP asthe LHS, but overall, productions with many differ-ent LHS parents exhibit consistency.3 A Context-Aware RerankerHaving established evidence for widespread syntac-tic consistency in the WSJ corpus, we now investi-gate incorporating extra-sentential context into a sta-tistical parser.
The first decision to make is whetherto incorporate the context into a generative or a dis-criminative parsing model.Employing a generative model would allow us totrain the parser in one step, and one such parserwhich incorporates the previous context has beenimplemented by Dubey et al (2006).
They imple-ment a PCFG, learning the production probabilitiesby a variant of standard PCFG-MLE probability es-timation that conditions on whether a rule has re-cently occurred in the context or not:P (RHS|LHS,Prime) = c(LHS ?
RHS,Prime)c(LHS,Prime)LHS and RHS represent the left-hand side and26right-hand side of a production, respectively.
Primeis a binary variable which is True if and only ifthe current production has occurred in the prime set(the previous sentence).
c represents the frequencycount.The drawback of such a system is that it doublesthe state space of the model, and hence likely in-creases the amount of data needed to train the parserto a comparable level of performance as a more com-pact model, or would require elaborate smoothing.Dubey et al (2006) find that this system performsworse than the baseline PCFG-MLE model, drop-ping F1 from 73.3% to 71.6%3.We instead opt to incorporate the extra-sententialcontext into a discriminative reranking parser, whichnaturally allows additional features to be incorpo-rated into the statistical model.
Many discriminativemodels of constituent parsing have been proposed inrecent literature.
They can be divided into two broadcategories?those that rerank the N-best outputs of agenerative parser, and those that make all parsing de-cisions using the discriminative model.
We chooseto implement an N-best reranking parser so that wecan utilize state-of-the-art generative parsers to en-sure a good selection of candidate parses to feedinto our reranking module.
Also, fully discrimina-tive models tend to suffer from efficiency problems,though recent models have started to overcome thisproblem (Finkel et al, 2008).Our approach is similar to N-best rerankingparsers such as Charniak and Johnson (2005)and Collins and Koo (2005), which implement a va-riety of features to capture within-sentence lexicaland structural dependencies.
It is also similar towork which focuses on coordinate noun phrase pars-ing (e.g.
(Hogan, 2007; Ku?bler et al, 2009)) in thatwe also attempt to exploit syntactic parallelism, butin a between-sentence setting rather than in a within-sentence setting that only considers coordination.As evidence of the potential of an N-best rerank-ing approach with respect to extra-sentential con-text, we considered the 50-best parses in the devel-opment set produced by the generative parser, andcategorized each into one of nine bins dependingon whether this candidate parse exhibits more, less,3A similar model which conditions on whether productionshave previously occurred within the same sentence, however,improves F1 to 73.6%.Overlapless equal moreworse F1 32519 7224 17280(81.8%) (69.3%) (75.4%)equal F1 1023 1674 540(2.6%) (16.1%) (2.4%)better F1 6224 1527 5106(15.7%) (14.6%) (22.3%)Table 3: Correlation between rule overlap and F1 com-pared to the generative baseline for the 50-best parses inthe development set.or the same amount of rule overlap with the previ-ous correct parse than the generative baseline, andwhether the candidate parse has a better, worse, orthe same F1 measure than the generative baseline(Table 3).
We find that a larger percentage of candi-date parses which share more productions with theprevious parse are better than the generative base-line parse than for the other categories, and this dif-ference is statistically significant (?2 test).3.1 Conditional Random FieldsFor our statistical reranker, we implement a linear-chain conditional random field (CRF).
CRFs are avery flexible class of graphical models which havebeen used for various sequence and relational la-belling tasks (Lafferty et al, 2001).
They have beenused for tree labelling, in XML tree labelling (Jousseet al, 2006) and semantic role labelling tasks (Cohnand Blunsom, 2005).
They have also been used forshallow parsing (Sha and Pereira, 2003), and fullconstituent parsing (Finkel et al, 2008; Tsuruoka etal., 2009).
We exploit the flexibility of CRFs by in-corporating features that depend on extra-sententialcontext.In a linear-chain CRF, the conditional probabil-ity of a sequence of labels y = y{t=1...T} given a se-quence of observed output x = x{t=1...T} and weightvector ?
= ?
{k=1...K} is given as follows:P (y|x) = 1Zexp(T?t=1?k?kfk(yt?1, yt, x, t))27where Z is the partition function.
The feature func-tions fk(yt?1, yt, x, t) can depend on two neighbour-ing parses, the sentences in the sequence, and theposition of the sentence in the sequence.
Since ourfeature functions do not depend on the words orthe time-step within the sequence, however, we willwrite fk(yt?1, yt) from now on.We treat each document in the corpus as one CRFsequence, and each sentence as one time-step inthe sequence.
The label sequence then is the se-quence of parses, and the outputs are the sentencesin the document.
Since there is a large number ofparses possible for each sentence and correspond-ingly many possible states for each label variable,we restrict the possible label state-space by extract-ing the N-best parses from a generative parser, andrerank over the sequences of candidate parses thusprovided.
We use the generative parser of Petrovand Klein (2007), a state-splitting parser that uses anEM algorithm to find splits in the nonterminal sym-bols to maximize training data likelihood.
We usethe 20-best parses, with an oracle F1 of 94.96% onsection 23.To learn the weight vector, we employ a stochasticgradient ascent method on the conditional log like-lihood, which has been shown to perform well forparsing tasks (Finkel et al, 2008).
In standard gra-dient ascent, the conditional log likelihood with a L2regularization term for a Gaussian prior for a train-ing corpus of N sequences isL(?)
=N?i=1?t,k?kfk(y(i)t?1, y(i)t )?N?i=1log Z(i) ?
?k?2k2?2And the partial derivatives with respect to theweights are?L?
?k=N?i=1?tfk(y(i)t?1, y(i)t )?N?i=1?t?y,y?fk(y, y?
)P (y, y?|x(i))?
?k?k?2The first term is the feature counts in the train-ing data, and the second term is the feature expecta-tions according to the current weight vector.
Thethird term corresponds to the penalty to non-zeroweight values imposed by regularization.
The prob-abilities in the second term can be efficiently calcu-lated by the CRF-version of the forward-backwardalgorithm.In standard gradient ascent, we update the weightvector after iterating through the whole training cor-pus.
Because this is computationally expensive, weinstead use stochastic gradient ascent, which ap-proximates the true gradient by the gradient calcu-lated from a single sample from the training corpus.We thus do not have to sum over the training set inthe above expressions.
We also employ a learningrate multiplier on the gradient.
Thus, the weight up-date for the ith encountered training sequence duringtraining is?
= ?
+ ?i?Lstochastic(?
)?i = ??
?N?
?N + iThe learning rate function is modelled on the oneused by Finkel et al (2008).
It is designed such that?i is halved after ?
passes through the training set.We train the model by iterating through the train-ing set in a randomly permuted order, updating theweight vector after each sequence.
The parameters?, ?
, and ?
are tuned to the development set.
The fi-nal settings we use are ?
= 0.08, ?
= 5, and ?
= 50.We use sections 2?21 of the Penn Treebank WSJ fortraining, 22 for development, and 23 for testing.
Weconduct 20-fold cross validation to generate the N-best parses for the training set, as is standard for N-best rerankers.To rerank, we do inference with the linear-chainCRF for the most likely sequence of parses usingthe Viterbi algorithm.3.2 Feature FunctionsWe experiment with various feature functions thatdepend on the syntactic and lexical parallelism be-tween yt?1 and yt.
We use the occurrence of a rulein yt that occurred in yt?1 as a feature.
Based on theresults of the corpus analysis, the first representation28(1) (S (NP (DT NN)) (VP (VBD)))(2) (S (NP (NNS)) (VP (VBD)))Phrasal features:Template: (parent, childL, childR, repeated)(S, edge, NP, +), (S, NP, VP, +), (S, VP, edge, +), (NP, edge,NNS,?
), (NP, NNS, edge,?
), (VP, edge, VBD, +), (VP, VBD,edge, +)Lexical features:Template: (parent, POSL, POSR, repeated)(S, edge, NNS, ?
), (S, NNS, VBD, ?
), (S, VBD, edge, +),(NP, edge, NNS, ?
), (NP, NNS, edge, ?
), (VP, edge, VBD,+), (VP, VBD, edge, +)Figure 3: Example of features extracted from a parse se-quence specified down to the POS level.we tried was to simply enumerate the (non-lexical)productions in yt along with whether that productionis found in yt?1.
However, we found that our mostsuccessful feature function is to consider overlaps inpartial structures of productions.Specifically, we decompose a tree into all of thenonlexical vertically and horizontally markovizedsubtrees.
Each of the subtrees in yt marked bywhether that same subtree occurs in the previoustree is a feature.
The simple production represen-tation corresponds to a vertical markovization of 1and a horizontal markovization of infinite.
We foundthat a vertical markovization of 1 and a horizontalmarkovization of 2 produced the best results on ourdata.
We will call this model the phrasal model.This schema so far only considers local substruc-tures of parse trees, without being informed by thelexical information found in the leaves of the tree.We try another schema which considers the POS tagsequences found in each subtree.
A feature then isthe node label of the root of the subtree with the POStag sequence it dominates, again decomposed intosequences of length 2 by markovization.
We willcall this model the lexical model.To extract features from this sequence, we con-sider the substructures in the second parse, and markwhether they are found in the first parse as well.
Weadd edge markers to mark the beginning and end ofconstituents.
See Figure 3 for an example of featuresMethod F1 (%)Model-averaged 90.47Combined, jointly trained ?Context 90.33Combined, jointly trained 90.31Model-averaged ?Context 90.22lexical ?Context 90.21lexical 90.20phrasal 90.12phrasal ?Context 89.74Generative 89.70Table 4: Development set (section 22) results of variousmodels that we trained.
Italicized are the models we usefor the test set.extracted by the two models.We will consider various ways of combining thetwo schemata above in the next section.
In addition,we also add a feature corresponding to the scaled logprobability of a parse tree derived from the genera-tive parsing baseline.
Scaling is necessary becauseof the large differences in the magnitude of the logprobability for different sentences.
The scaling for-mula that we found to work best is to scale the max-imum log probability among the N-best candidateparses to be 1.0 and the minimum to be 0.0.3.3 ResultsWe train the two models which make use of extra-sentential context described in the previous section,and use the model to parse the development andtest set.
We also trained a model which combinesboth sets of features, but we found that we get betterperformance by training the two models separately,then averaging the models by computing the respec-tive averages of their features?
weights.
Thus, weuse the model-averaged version of the models thatconsider context in the test set experiments.
Thegenerative parser forms the first baseline methodto which we compare our results.
We also train areranker which makes use of the same features as wedescribed above, but without marking whether eachsubstructure occurs in the previous sentence.
This isthus a reranking method which does not make useof the previous context.
Again, we tried model aver-aging, but this produces less accurate parses on the29LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CBdevelopment set ?
length ?
40 development set ?
all sentencesGenerative 90.33 90.20 90.27 39.92 0.68 71.99 89.64 89.75 89.70 37.76 0.82 68.65+Context 91.25 90.71 90.98 41.25 0.61 73.45 90.62 90.33 90.47 38.88 0.74 70.47?Context 90.85 90.78 90.82 40.62 0.62 73.00 90.28 90.38 90.22 38.24 0.74 70.00Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB).
Generativeis the generative baseline of Petrov and Klein (2007), +Context is the best performing reranking model using previouscontext (model-averaged phrasal and lexical), ?Context is the best performing reranking model not usingprevious context (jointly trained phrasal and lexical).LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CBtest set ?
length ?
40 test set ?
all sentencesGenerative 90.04 89.84 89.94 38.31 0.80 68.33 89.60 89.35 89.47 36.05 0.94 65.81+Context 90.63 90.11 90.37 39.02 0.73 69.40 90.17 89.64 89.91 36.84 0.87 67.09?Context 90.64 90.43 90.54 38.62 0.72 69.84 90.20 89.97 90.08 36.47 0.85 67.55Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)development set, so we use the jointly trained modelon the test set.
We will refer to this model as thecontext-ignorant or ?Context model, as opposed tothe previous context-aware or +Context model.
Theresults of these experiments on the development setare shown in Table 4.PARSEVAL results4 on the development and testset are presented in Tables 5 and 6.
We see thatthe reranked models outperform the generative base-line model in terms of F1, and that the rerankedmodel that uses extra-sentential context outperformsthe version that does not use extra-sentential contextin the development set, but not in the test set.
Us-ing Bikel?s randomized parsing evaluation compara-tor5, we find that both reranking models outperformthe baseline generative model to statistical signifi-cance for recall and precision.
The context-ignorantreranker outperforms the context-aware reranker onrecall (p < 0.01), but not on precision (p = 0.42).However, the context-aware model has the highestexact match scores in both the development and thetest set.The F1 result suggests two possibilities?either thecontext-aware model captures the same informationas the context-ignorant model, but less effectively, orthe two models capture different information about4This evaluation ignores punctuation and corresponds to thenew.prm parameter setting on evalb.5http://www.cis.upenn.edu/?dbikel/software.htmlSec.
?Context better same +Context better22 157 1357 18623 258 1904 254Table 7: Context-aware vs. context-ignorant rerankingresults, by sentential F1.the parses.
Two pieces of evidence point to thelatter possibility.
First, if the context-aware modelwere truly inferior, then we would expect it to out-perform the context-ignorant model on almost nosentences.
Otherwise, we would expect them todo well on different sentences.
Table 7 shows thatthe context-aware model outperforms the context-ignorant model on nearly as many trees in the testsection as the reverse.
Second, if we hypotheti-cally had an oracle that could determine whether thecontext-ignorant or the context-aware model wouldbe more accurate on a sentence and if the two modelswere complementary to each other, we would expectto achieve a gain in F1 over the generative baselinewhich is roughly the sum of the gain achieved byeach model separately.
This is indeed the case, aswe are able to achieve F1s of 91.23% and 90.89%on sections 22 and 23 respectively, roughly twice theimprovement that the individual models obtain.To put our results in perspective, we now comparethe magnitude of the improvement in F1 our context-30System Baseline Best Imp.
(rel.
)Dubey et al (2006) 73.3 73.6 0.3 (1.1%)Hogan (2007) 89.4 89.6 0.2 (1.9%)This work 89.5 89.9 0.4 (3.8%)Table 8: A comparison of parsers specialized to exploitintra- or extra-sentential syntactic parallelism on section23 in terms of the generative baseline they compare them-selves against, the best F1 their non-baseline modelsachieve, and the absolute and relative improvements.aware model achieves over the generative baselineto that of other systems specialized to exploit intra-or extra-sentential parallelism.
We achieve a greaterimprovement despite the fact that our generativebaseline provides a higher level of performance, andis presumably thus more difficult to improve upon(Table 8).
These systems do not compare themselvesagainst a reranked model that does not use paral-lelism as we do in this work.During inference, the Viterbi algorithm recov-ers the most probable sequence of parses, and thismeans that we are relying on the generative parser toprovide the context (i.e.
the previous parses) whenanalyzing any given sentence.
We do another type oforacle analysis in which we provide the parser withthe correct, manually annotated parse tree of theprevious sentence when extracting features for thecurrent sentence during training and parsing.
This?perfect context?
model achieves F1s of 90.42% and90.00% on sections 22 and 23 respectively, which iscomparable to the best results of our reranking mod-els.
This indicates that the lack of perfect contextualinformation is not a major obstacle to further im-proving parsing performance.3.4 AnalysisWe now analyze several specific cases in the devel-opment set in which the reranker makes correct useof contextual information.
They concretely illustratehow context can improve parsing performance, andconfirm our initial intuition that extra-sentential con-text can be useful for parsing.
The sentence in (3)and (4) is one such case.
(3) Generative/Context-ignorant: (S (S A BMAspokesman said ?runaway medical costs?
havemade health insurance ?a significantchallenge) ,?
and (S margins also have beenpinched ...) (.
.
))(4) Context-aware: (S (NP A BMA spokesman)(VP said ?runaway medical costs?
have madehealth insurance ?a significant challenge,?
andmargins also have been pinched ...) (.
.
))The baseline and the context-ignorant modelsparse the sentence as a conjunction of two S clauses,misanalyzing the scope of what is said by the BMAspokesman to the first part of the conjunct.
By an-alyzing the features and feature weight values ex-tracted from the parse sequence, we determined thatthe context-aware reranker is able to correct theanalysis of the scoping due to a parallelism in thesyntactic structure.
Specifically, the substructureS ?
V P. is present in both this sentence and theprevious sentence of the reranked sequence, whichalso contains a reporting verb.
(5) (S (NP BMA Corp., Kansas City, Mo.,) (VPsaid it?s weighing ?strategic alternatives?
...and is contacting possible buyers ...) (.
.
))As a second example, consider the following sen-tence.
(6) Generative/Context-ignorant: To achievemaximum liquidity and minimize pricevolatility, (NP either all markets) (VP shouldbe open to trading or none).
(7) Context-aware: To achieve maximum liquidityand minimize price volatility, (CC either) (S(NP all markets) should be open to trading ornone).The original generative and context-ignorantparses posit that ?either all markets?
is a nounphrase, which is incorrect.
Syntactic parallelism cor-rects this for two reasons.
First, the reranker prefersa determiner to start an NP in a consistent context,as both surround sentences also contain this sub-structure.
Also, the previous sentence also containsa conjunction CC followed by a S node under a Snode, which the reranker prefers.While these examples show contextual features tobe useful for parsing coordinations, we also found31context-awareness to be useful for other types ofstructural ambiguity such as PP attachment ambi-guity.
Notice that the method we employ to cor-rect coordination errors is different from previousapproaches which usually rely on lexical or syntac-tic similarity between conjuncts rather than betweensentences.
Our approach can thus broaden the rangeof sentences that can be usefully reranked.
For ex-ample, there is little similarity between conjuncts toavail of in the second example (Sentences 6 and 7).Based on these analyses, it appears that con-text awareness provides a source of information forparsing which is not available to context-ignorantparsers.
We should thus consider integrating bothtypes of features into the reranking parser to buildon the advantages of each.
Specifically, within-sentence features are most appropriate for lexi-cal dependencies and some structural dependencies.Extra-sentential features, on the other hand, are ap-propriate for capturing the syntactic consistency ef-fects as we have demonstrated in this paper.4 ConclusionsIn this paper, we have examined evidence for syn-tactic consistency between neighbouring sentences.First, we conducted a corpus analysis of the PennTreebank WSJ, and shown that parallelism existsbetween sentences for productions with a varietyof LHS types, generalizing previous results fornoun phrase structure.
Then, we explored a novelsource of features for parsing informed by the extra-sentential context.
We improved on the parsing ac-curacy over a generative baseline parser, and rival asimilar reranking model that does not rely on extra-sentential context.
By examining the subsets ofthe evaluation data on which each model performsbest and also individual cases, we argue that con-text allows a type of structural ambiguity resolutionnot available to parsers which only rely on intra-sentential context.AcknowledgmentsWe would like to thank the anonymous reviewersand Timothy Fowler for their comments.
This workis supported in part by the Natural Sciences and En-gineering Research Council of Canada.ReferencesJ.K.
Bock.
1986.
Syntactic persistence in language pro-duction.
Cognitive Psychology, 18(3):355?387.A.
Buch and C. Pietsch.
2010.
Measuring syntacticpriming in dialog corpora.
In Proceedings of the Con-ference on Linguistic Evidence 2010: Empirical, The-oretical and Computational Perspectives.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and MaxEnt discriminative reranking.
In Pro-ceedings of the 43rd ACL, pages 173?180.
Associationfor Computational Linguistics.K.W.
Church.
2000.
Empirical estimates of adaptation:the chance of two Noriegas is closer to p/2 than p2.
InProceedings of 18th COLING, pages 180?186.
Asso-ciation for Computational Linguistics.T.
Cohn and P. Blunsom.
2005.
Semantic role labellingwith tree conditional random fields.
In Ninth Confer-ence on Computational Natural Language Learning,pages 169?172.M.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1):25?70.A.
Dubey, P. Sturt, and F. Keller.
2005.
Parallelism incoordination as an instance of syntactic priming: Evi-dence from corpus-based modeling.
In Proceedings ofHLT/EMNLP 2005, pages 827?834.A.
Dubey, F. Keller, and P. Sturt.
2006.
Integrating syn-tactic priming into an incremental probabilistic parser,with an application to psycholinguistic modeling.
InProceedings of the 21st COLING and the 44th ACL,pages 417?424.
Association for Computational Lin-guistics.J.R.
Finkel, A. Kleeman, and C.D.
Manning.
2008.
Effi-cient, feature-based, conditional random field parsing.Proceedings of ACL-08: HLT, pages 959?967.S.T.
Gries.
2005.
Syntactic priming: A corpus-basedapproach.
Journal of Psycholinguistic Research,34(4):365?399.D.
Hogan.
2007.
Coordinate noun phrase disambigua-tion in a generative parsing model.
In Proceedings of45th ACL, volume 45, pages 680?687.F.
Jousse, R. Gilleron, I. Tellier, and M. Tommasi.
2006.Conditional random fields for XML trees.
In ECMLWorkshop on Mining and Learning in Graphs.S.
Ku?bler, W. Maier, E. Hinrichs, and E. Klett.
2009.Parsing coordinations.
In Proceedings of the 12thEACL, pages 406?414.
Association for ComputationalLinguistics.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In InternationalConference on Machine Learning, pages 282?289.32D.
McClosky, E. Charniak, and M. Johnson.
2006.
Ef-fective self-training for parsing.
In Proceedings ofHLT-NAACL 2006.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proceedings of HLT-NAACL2007, pages 404?411.
Association for ComputationalLinguistics.M.J.
Pickering and H.P.
Branigan.
1999.
Syntactic prim-ing in language production.
Trends in Cognitive Sci-ences, 3(4):136?141.D.
Reitter.
2008.
Context Effects in Language Produc-tion: Models of Syntactic Priming in Dialogue Cor-pora.
Ph.D. thesis, University of Edinburgh.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proceedings of HLT-NAACL,pages 213?220.Y.
Tsuruoka, J. Tsujii, and S. Ananiadou.
2009.
Fast fullparsing by linear-chain conditional random fields.
InProceedings of the 12th EACL, pages 790?798.
Asso-ciation for Computational Linguistics.33
