REVERSIBLE  NLP BY DERIV ING THE GRAMMARSFROM THE KNOWLEDGE BASEDavid D. McDonaldContent Technologies, Inc.14 Brantwood Road, Arlington, MA 02174(617) 646-4124, MCDONALD@BRANDEIS.EDUABSTRACTWe present a new architecture for reversible NLP.Separate parsing and generation grammars areconstructed from the underlying application'ssemantic model and knowledge base.
By having twogrammars we are free to use process-specificrepresentations and control techniques, therebypermitting highly efficient processing.
The singlesemantic source ensures the parsimony ofdevelopment and matched competence that makereversible NLP attractive.INTRODUCTIONMost natural language processing systems areinitially built in a single direction only; most areparsers (understanding systems), afew are generators.These systems are often then embedded in full, bi-directional interfaces, whereupon a new, almost non-technical kind of problem arises if differences in thetwo uni-directional subsystems are not controlled.The full system ay not understand the same wordingor syntactic onstructions that it can generate; orgeneration and parsing development teams may bothhave to work on extensions and modifications totheirgrammars, with the likely result that still furtherdifferences will be introduced.These practical problems bolster an intuitionthat many have that knowledge of parsing andgeneration is the same knowledge in a person's mind,or at least that the two faculties draw on a singlerepresentation f their language ven if it is engagedin different ways.
This has led to the goal ofreversible NLP systems.
The common approach asbeen to take the computational artifact constructed byone of the single-direction projects, typically itsgrammar, and to aSapt it for use in the other direction.At ISI, for example, their massive systemicgrammar for generation, NIGEL, (Mann &Matthiessen 1985) has since been adapted for use as aparser (Casper 1989).
With the conceptual basis ofthe transformation i  place, the development offurther extensions and modifications i  done on thegeneration grammar, and then that grammar is re-transformed toyield the new parsing grammar.The other well-known approach to reversibleNLP is of course to use the very same computationalartifact in both processing dkections.
Thus far thisartifact has invariably been a grammar, typicallysome kind of specification of the text-stream --logical form relation that can be used as a transduceror can supply the data for it.Parsers and generators draw on their grammarsas their predominant knowledge source.
The grammarthus becomes a bottleneck for the processing if it isnot designed with efficiency of processing in mind.When virtually the same computational representationof the grammar is used in both processes and it isgiven an active role, e.g.
when the grammar iscouched in a unification formalism, this bottleneckcan be substantial since the "common denominator"processing architecture that must be employed inorder for the grammar to be literally usable by bothprocesses will be markedly less efficient thanarchitectures that work from single-directionrepresentations of the grammar.By their nature as information processingsystems, language understanding and generation arequite different kinds of processes.
Understandingproceeds from texts to intentions.
The "known" isthe wording of the text and its intonation.
Fromthese, the understanding process constructs anddeduces the propositional content conveyed by thetext and the probable intentions of the speaker inproducing it.
Its primary effort is to scan the wordsof the text in sequence, during which the form of thetext gradually unfolds.
This requirement to scanforces the adoption of algorithms based on themanagement of multiple hypotheses and predictionsthat feed a representation that must be expandeddynamically.
Major problems are caused by40ambiguity and under-specification (i.e.
the audiencetypically receives more information fromsituationally motivated inferences than is conveyed bythe actual text).In generation, information flows in the oppositedirection from understanding.
Generation proceedsfrom content to iform, from intentions andperspectives to linearly arrayed words and syntacticmarkers.
A generator's "known" is its awareness ofits intentions, its plgns, and the text it has alreadyproduced.
Coupled with a model of the audience, thesituation, and the discourse, this provides the basisfor making choices among the alternative wordingsand constructions that the language provides---theprincipal activity iff generation.
Most generationsystems do produce ;texts sequentially from left toright---just like an understanding system would scanit; but they do this only after having made decisionsabout the content and form of the text as a whole.Ambiguity in a generator's knowledge is not possible(indeed one of its problems is to notice that it hasinadvertently introduced an ambiguity into the text).And rather than under-specification, a generator'sproblem is to choose from its over-supply ofinformation what to include and what to omit so as toadequately signal its intended inferences to theaudience.Our concern with efficiency---optimizing thetwo processes to fit their differing informationprocessing characteristics---has led us to approachreversible NLP by al compilation-style route wherethe grammar that the processes use is not one artifactbut two, each with its own representation that isdeliberately tailored to the process that uses it.
Likethe system at ISI, our reversible knowledge source isgrounded in the generation process and then projected,via a compiler, to create the representation used bythe parser.
The difference is that while ISI projectedthe grammar that the generator used, i.e.
the set ofsystem networks that is the model of the linguisticresources provided by the language and theirdependencies, our system is a projection from theunderlying application's conceptual model.In generation ?he starts with a set of objectsrepresenting individuals, relations, propositions, etc.that have been selected from the application programas its representation f  the information it wants tocommunicate.
Accordingly, the kind of knowledgethat a generator must draw on most frequently is whatare the options for realizing those objectslinguistically.
In Order to make this look-upefficient, one is naturally led to an architecture whereIs stored directly with the definmons this knowledge " ~ .
.
.
.of the objects or their classes, in effect distributing ahighly lexicalized grammar over the knowledge base.A SIMPLE EXAMPLETo be concrete, consider the example in FigureOne below, a simple definition of the object class(category) for generic months.
This expression saysthat a month is a kind of time stuff that can beviewed either as a point or an interval; that it has aspecific number of days, a position within the year,and, especially, that it has a name and abbreviations--the primary options for realizing references to themonth in natural language.
(def-category month: special izest ime/ interval -or-point:slots((name (word proper-name:may-be-abbreviated) )(number-of-days number)(posit ion-in-the-yearnumber) ) )Figure OneIn our system, CTI-1, the evaluation of thisexpression causes a number of different hings to beconstructed: the object representing the category,indexing and printing functions for objects with thatcategory (instances of the class), and a defining form.One then uses the form to create objects for thetwelve months, as shown in Figure Two forDecember.
(define-month:name ~December": abbreviation "Dec":number-of-days 31:posit ion-in-the-year 12)#<month December: name #<word "December">: abbreviation #<word "Dec">:number-of-days #<number 31>: posit ion- in-the-year#<number 12>>Figure TwoAS a result of evaluating this form, we get theobject for December (Figure Two).
When referring tothis object in generation we will look it its namefield and use the word object there, or perhaps theabbreviated word.When parsing, we will see an instance of theword "December" or the phrase "Dec." and want toknow what object it the application program's modelit refers to.
In CTI-1 this is done by the phrasestructure rules in Figure Three.
These rules werewritten automatically (compiled) as one of the side-effects of defining the object for December; the codefor constructing the rules was incorporated into the41Define-month form by following the annotation inthe expression that defined the category month, thesame annotation that control where the the generatorlooks when it wants to realize a month object.#<context-free-rule:print-form Imonth -> "December"l:lefthand-side #<category month>:righthand-side (#<word ~December">):syntactic-form#<category proper-noun>:referent #<month December>>#<context-free-rule:print-form \[month -> "Dec."l:lefthand-side #<category month>:righthand-side ( #<word "Dec">#<word ".
"> ):syntactic-form#<category proper noun>:referent #<month December>>Figure ThreeThese parsing rules are part of CTI-I's semanticgrammar.
They are rewrite rules.
When the word"December" or the two word phrase "Dec ....
."
isscanned, the text segment is spanned with an edge ofthe chart (a parse node), and the edge receives a threepart label: (1) the category "month", whichparticipates in the semantic grammar, (2) the "form"category "proper-noun", which is available to thesyntactic grammar, and (3) the referent the edge picksout in the application model, i.e.
the very object#<month December> that was defined by the form inFigure Two.SUMMARY OF THE APPROACHBefore going into a more elaborate xample wecan briefly summarize the reversible NIP architecturewe have adopted.
The grammar is developed on thegeneration side by the linguist/semantic modeler aspart of defining the classes and individuals thatcomprise the application's domain model.
Theyinclude with the definitions annotations about howsuch objects can be realized in natural anguage.A side-effect of definition is the automaticinversion of the generation rules specified by theannotation to construct the equivalent set of parsingrules.
Parsimony and uniformity of coverage, thepractical goals of reversible systems, are achieved byhaving the parsing rammar constructed automaticallyfrom the original forms that the linguist enters ratherthan having them redundantly entered by hand.Note that what we are projecting from as weinvert "the generator's rules" is the generator'srepresentation f the form-meaning relationship---itsrules for mapping from specific objects in theunderlying application's domain model to their (setof) surface linguistic forms by warrant of how themodel has characterized them semantically.
This isnot the same as a representation f the principles thatconstrain the valid compositional forms of thelanguage: the constraints on how individual exicaiitems and syntactic onstructions can be combined,what elements are required if others are present, aformal vocabulary of linguistic ategories, and so on.That representation provides the framework in whichthe form-meaning relationship is couched, and it isdeveloped by hand.
For CTI-1 the design choices asto its categories and relations are taken from thetheory of Tree Adjoining Grammar (Joshi 1985).The simplicity and immediacy of the automaticinversion is possible because in our approach the taskof parsing (determining a text's form) has beenintegrated with the task of understanding/semanticinterpretation (determining the denotations of the textand its elements in some model).
This integration isbrought about by using a semantic grammar.
Asemantic grammar brings the categories of analysisused by the parser into the same realm as those usedby the generator, namely the categories of theapplication domain (in the present case personnelchanges), for example people, companies, dates, ages,job titles, relations uch as former, new, or has-title,and event ypes uch as appoint, succeed, retire, etc.If the parser had been intended only to producesyntactic structural descriptions of the text, thenprojecting its rules from the generator would havebeen either impossible or trivial.
An applicationsupports a potentially vast number of categories; thesyntactic ategories of natural languages are fixed andrelatively small.
Collapsing the different kinds ofthings that can be realized as noun phrases down tothat single category would lose the epistemologicalstructure of the application's model and provide onlyminimal information to constrain or define thegrammar.TREES FOR GENERATION, BINARYRULES FOR PARSINGConsider the definition of the event type"appoint-to-position, shown in Figure Four.
It'slinguistic annotation amounts to the specification ofa tree family in a TAG.
The features given in theannotation are consulted to establish what trees thefamily should contain, building on the basicsubcategorization frame of a verb that takes a subjectand two NP complements, e.g.
that it includes apassivized tree, one in participial form without itssubject, and so on.42(def-category appoint-to-posit ion:slots ((person person)(company company)(position title) ):tree-family( (personnel-changecompany !__  !
person !
tit le: verb ~appoinh"(subject -> company)(objectl -> pgrson)(object2 -> position): optional( (by-company -> company)(by-person -> new-person)): passivizes i: forms-participles ) ) )Figure FourThe annotation is equivalent o binding aspecific lexeme to the verb position in the trees of thefamily (this is a lexicalized TAG), as well asrestrictions on the de~otations of the phrases that willbe substituted for thel other constituents of the clause,e.g.
that the subjectlpicks out a company, the firstobject a person, etc.A tree family plus its bindings is how thisannotation looks from the generator's perspective.For the parser, this same information is representedquite differently, i.e.
as a set of binary phrasestructure rules.
Such rules are the more appropriaterepresentation for parsing (given the algorithm inCTI-1) since parsing is a process of serial scanningrather than the top-down refinement done ingeneration.
During the scan, constituents willemerge successively bottom up, and the parser's mostfrequent operation and reason for consulting thegrammar will be to judge whether two adjacentconstituents can compose to form a phrase.
(Therules are binary for efficiency concerns: CTI-1 doesthe multiplication operation for determining whethertwo adjacent constituents form a phrase in constanttime regardless of the size of the grammar.
)The tree family defines a set of rules that areapplicable to any verb and semantic bindings thatshare the same subCategodzation frame, such as"name" or "elect".
In projecting the annotation onthe definition of appoint-to-position i to parsingrules, the compilation!
process will create the rules ofthe family if it does ngt already exist, and also createa set of unary rules for the immediate non-terminalsof the verb, one for'k each different morphologicalvariant.
One of these' rules is shown in Figure Five,along with the general rule for the object-promotionaspect of passivizafion.#<context-free-rule: lefthand-side#<categorypc/company!
_!person!tit le>:righthand-side( #<word ~appointed"> ):form #<category main-verb/-ed>:referent#<category personnel-change/appoint-to-posit ion>># <context- f ree-ru le / form: right hand- side( #<category "be">#<category main-verb / -ed> ):head :second-constituent: revised-mapping((object -> subject)))Figure FiveThe first phrase structure rule, part of thesemantic grammar, ties the past participial form ofthe verb into the family of rules.
The long categoryname is a convenient mnemonic for the family ofrules, since it shows by its spelling what semanticcategories of constituents are expected as siblingconstituents in the clause as a whole.The object promotion rule is a syntactic("form") rule that makes reference to the form labelon an edge rather than theft semantic label.
The rulefor "appointed" has a form label showing that it is amain verb in past participle form, which is what thesyntactic rule is looking for.
When a segment like"was appointed" is scanned, the label "be" on the edgespanning "was" will be checked against he label"main-verb/-ed" on the edge over "appointed" and theresulting edge will carry the semantic label andreferent of the phrase's head, i.e.
the main verb.Figure Six shows some of the other ules in thefamily so that one can get an idea about how theparsing of the whole clause will be done.
The rulesare given just by their print forms.43I.2.3.4.pc/company!
!person!title-> appo in tedpc/company!
\[title-> pc/company!
!person!t i t lepersonpc/company!-> pc /company!title!
tit lepersonnel-change-> companypc / company !Figure SixSTATE OF DEVELOPMENTThe parsing side of this architecture forreversible NLP is implemented and running in anoperational system, CTI-1.
It has a mature domainmodel for personnel changes, and has been runningthe parsing grammar that is projected from that modelon hundreds of articles from the Wall Street Journal("Who's News").The generation side of the architecture is in itsinfancy, waiting on a suitable domain and task wherethe reasons for speaking and the situation models arerich enough to motivate subtle nuances in phrasing.By the same token, my prior experience withgeneration leads me to believe that the design of thelinguistic annotations i well-founded for generation,and that this side of the reversal will fall out once theopportunity for implementation arises.
When thishappens the "raw material" that the mappingsdiscussed here will supply will be fed to a textplanner like Meteer's RAVEL orchestrator in herSPOKESMAN system, and then drive a TAGrealization component along the lines of Mumble-86.REFERENCESJoshi, A.K.
(1985) How much context-sensitivity isrequked to provide reasonable structural descriptions:tree adjoining rammars, in Dowry et al (eds)Natural Language Processing, Cambridge UniversityPress.Mann, W.C. & C. Matthiessen (1985) Ademonstration f the Nigel text generation computerprogram, in Benson & Greaves (eds) SystemicPerspectives in Discourse, Benjamins,Amsterdam.44
