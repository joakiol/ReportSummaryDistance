Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201?204,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsGaussian Processes for Fast Policy Optimisation of POMDP-basedDialogue ManagersM.
Gas?ic?, F.
Jurc??
?c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. YoungCambridge University Engineering DepartmentTrumpington Street, Cambridge CB2 1PZ, UK{mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.ukAbstractModelling dialogue as a Partially Observ-able Markov Decision Process (POMDP)enables a dialogue policy robust to speechunderstanding errors to be learnt.
How-ever, a major challenge in POMDP pol-icy learning is to maintain tractability, sothe use of approximation is inevitable.We propose applying Gaussian Processesin Reinforcement learning of optimalPOMDP dialogue policies, in order (1) tomake the learning process faster and (2) toobtain an estimate of the uncertainty of theapproximation.
We first demonstrate theidea on a simple voice mail dialogue taskand then apply this method to a real-worldtourist information dialogue task.1 IntroductionOne of the main challenges in dialogue manage-ment is effective handling of speech understand-ing errors.
Instead of hand-crafting the error han-dler for each dialogue step, statistical approachesallow the optimal dialogue manager behaviourto be learnt automatically.
Reinforcement learn-ing (RL), in particular, enables the notion of plan-ning to be embedded in the dialogue managementcriteria.
The objective of the dialogue manager isfor each dialogue state to choose such an actionthat leads to the highest expected long-term re-ward, which is defined in this framework by the Q-function.
This is in contrast to Supervised learn-ing, which estimates a dialogue strategy in such away as to make it resemble the behaviour from agiven corpus, but without directly optimising over-all dialogue success.Modelling dialogue as a Partially ObservableMarkov Decision Process (POMDP) allows actionselection to be based on the differing levels of un-certainty in each dialogue state as well as the over-all reward.
This approach requires that a distribu-tion of states (belief state) is maintained at eachturn.
This explicit representation of uncertainty inthe POMDP gives it the potential to produce morerobust dialogue policies (Young et al, 2010).The main challenge in the POMDP approach isthe tractability of the learning process.
A dis-crete state space POMDP can be perceived as acontinuous space MDP where the state space con-sists of the belief states of the original POMDP.A grid-based approach to policy optimisation as-sumes discretisation of this space, allowing fordiscrete space MDP algorithms to be used forlearning (Brafman, 1997) and thus approximatingthe optimal Q-function.
Such an approach takesthe order of 100, 000 dialogues to train a real-world dialogue manager.
Therefore, the trainingnormally takes place in interaction with a simu-lated user, rather than real users.
This raises ques-tions regarding the quality of the approximationas well as the potential discrepancy between sim-ulated and real user behaviour.Gaussian Processes have been successfully usedin Reinforcement learning for continuous spaceMDPs, for both model-free approaches (Engel etal., 2005) and model-based approaches (Deisen-roth et al, 2009).
We propose using GP Rein-forcement learning in a POMDP dialogue man-ager to, firstly, speed up the learning process and,secondly, obtain the uncertainty of the approxima-tion.
We opt for the model-free approach since ithas the potential to allow the policy obtained ininteraction with the simulated user to be furtherrefined in interaction with real users.In the next section, the core idea of the method isexplained on a toy dialogue problem where differ-ent aspects of GP learning are examined.
Follow-ing that, in Section 3, it is demonstrated how thismethodology can be effectively applied to a realworld dialogue.
We conclude with Section 4.2 Gaussian Process RL on a Toy Problem2.1 Gaussian Process RLA Gaussian Process is a generative model ofBayesian inference that can be used for functionregression (Rasmussen and Williams, 2005).
AGaussian Process is fully defined by a mean and akernel function.
The kernel function defines priorfunction correlations, which is crucial for obtain-ing good posterior estimates with just a few ob-servations.
GP-Sarsa is an on-line reinforcementlearning algorithm for both continuous and dis-crete MDPs that incorporates GP regression (En-201gel et al, 2005).
Given the observation of rewards,it estimates the Q-function utilising its correlationsin different parts of the state and the action spacedefined by the kernel function.
It also gives a vari-ance of the estimate, thus modelling the uncer-tainty of the approximation.2.2 Voice Mail Dialogue TaskIn order to demonstrate how this methodologycan be applied to a dialogue system, we first ex-plain the idea on the voice mail dialogue prob-lem (Williams, 2006).The state space of this task consists of three states:the user asked for the message either to be savedor deleted, or the dialogue ended.
The systemcan take three actions: ask the user what to do,save or delete the message.
The observation ofwhat the user wants is corrupted with noise, there-fore we model this as a three-state POMDP.
ThisPOMDP can be viewed as a continuous MDP,where the MDP state is the POMDP belief state,a 3-dimensional vector of probabilities.
For bothlearning and evaluation, a simulated user is usedwhich makes an error with probability 0.3 and ter-minates the dialogue after at most 10 turns.
In thefinal state, it gives a positive reward of 10 or apenalty of ?100 depending on whether the systemperformed a correct action or not.
Each interme-diate state receives the penalty of ?1.
In order tokeep the problem simple, a model defining tran-sition and observation probabilities is assumed sothat the belief can be easily updated, but the policyoptimisation is performed in an on-line fashion.2.3 Kernel Choice for GP-SarsaThe choice of kernel function is very importantsince it defines the prior knowledge about the Q-function correlations.
They have to be defined onboth states and actions.
In the voice mail dialogueproblem the action space is discrete, so we opt fora simple ?
kernel over actions:k(a, a?)
= 1 ?
?a(a?
), (1)where ?a is the Kronecker delta function.
Thestate space is a 3-dimensional continuous spaceand the kernel functions over the state space thatwe explore are given in Table 1.
Each kernel func-kernel function expressionpolynomial k(x,x?)
= ?x,x?
?parametrised poly.
k(x,x?)
=PDi=1xix?ir2iGaussian k(x,x?)
= p2 exp ?
?x ?
x?
?22?2kscaled norm k(x,x?)
= 1 ?
?x ?
x??2?x?2?x?
?2Table 1: Kernel functionstion defines a different correlation.
The polyno-mial kernel views elements of the state vector asfeatures, the dot-product of which defines the cor-relation.
They can be given different relevance riin the parametrised version.
The Gaussian ker-nel accounts for smoothness, i.e., if two states areclose to each other the Q-function in these statesis correlated.
The scaled norm kernel defines posi-tive correlations in the points that are close to eachother and a negative correlation otherwise.
Thisis particularly useful for the voice mail problem,where, if two belief states are very different, tak-ing the same action in these states generates a neg-atively correlated reward.2.4 Optimisation of Kernel ParametersSome kernel functions are in a parametrisedform, such as Gaussian or parametrised polyno-mial kernel.
These parameters, also called thehyper-parameters, are estimated by maximisingthe marginal likelihood1 on a given corpus (Ras-mussen and Williams, 2005).
We adapted theavailable code (Rasmussen and Williams, 2005)for the Reinforcement learning framework to ob-tain the optimal hyper-parameters using a dialoguecorpus labelled with states, actions and rewards.2.5 Grid-based RL AlgorithmsTo assess the performance of GP-Sarsa, it wascompared with a standard grid-based algorithmused in (Young et al, 2010).
The grid-based ap-proach discretises the continuous space into re-gions with their representative points.
This thenallows discrete MDP algorithms to be used for pol-icy optimisation, in this case the Monte Carlo Con-trol (MCC) algorithm (Sutton and Barto, 1998).2.6 Optimal POMDP PolicyThe optimal POMDP policy was obtained us-ing the POMDP solver toolkit (Cassandra, 2005),which implements the Point Based Value Itera-tion algorithm to solve the POMDP off-line usingthe underlying transition and observation proba-bilities.
We used 300 sample dialogues betweenthe dialogue manager governed by this policy andthe simulated user as data for optimisation of thekernel hyper-parameters (see Section 2.4).2.7 Training set-up and EvaluationThe dialogue manager was trained in interactionwith the simulated user and the performance wascompared between the grid-based MCC algorithmand GP-Sarsa across different kernel functionsfrom Table 1.The intention was, not only to test which algo-rithm yields the best policy performance, but alsoto examine the speed of convergence to the opti-mal policy.
All the algorithms use an ?-greedyapproach where the exploration rate ?
was fixedat 0.1.
The learning process greatly depends on1Also called evidence maximisation in the literature.202the actions that are taken during exploration.
Ifearly on during the training, the systems discoversa path that generates high rewards due to a luckychoice of actions, then the convergence is faster.To alleviate this, we adopted the following proce-dure.
For every training set-up, exactly the sametraining iterations were performed using 1000 dif-ferent random generator seedings.
After every 20dialogues the resulting 1000 partially optimisedpolicies were evaluated.
Each of them was testedon 1000 dialogues.
The average reward of these1000 dialogues provides just one point in Fig.
1.20 60 100 140 180 220 260 300 340 380 420 460 500 540 580 620?50?45?40?35?30?25?20?15?10?50Training dialoguesAveragerewardpolynomial kernel ??
Gaussian kernel with learned hyper?parameters?
scaled norm kernelpolynomial kernel with learned hyper?parameters?Optimal POMDP PolicyGP?SarsaGrid?based Monte Carlo ControlFigure 1: Evaluation results on Voice Mail taskThe grid-based MCC algorithm used a Euclid-ian distance to generate the grid by adding everypoint that was further than 0.01 from other pointsas a representative of a new region.
As can beseen from Fig 1, the grid-Based MCC algorithmhas a relatively slow convergence rate.
GP-Sarsawith the polynomial kernel exhibited a learningrate similar to MCC in the first 300 training di-alogues, continuing with a more upward learningtrend.
The parametrised polynomial kernel per-forms slightly better.
The Gaussian kernel, how-ever, achieves a much faster learning rate.
Thescaled norm kernel achieved close to optimal per-formance in 400 dialogues, with a much higherconvergence rate then the other methods.3 Gaussian Process RL on a Real-worldTask3.1 HIS Dialogue Manager on CamInfoDomainWe investigate the use of GP-Sarsa in a real-world task by extending the Hidden InformationState (HIS) dialogue manager (Young et al, 2010).The application domain is tourist information forCambridge, whereby the user can ask for informa-tion about a restaurant, hotel, museum or anothertourist attraction in the local area.
The databaseconsists of more than 400 entities each of whichhas up to 10 attributes that the user can query.The HIS dialogue manager is a POMDP-based di-alogue manager that can tractably maintain beliefstates for large domains.
The key feature of thisapproach is the grouping of possible user goalsinto partitions, using relationships between differ-ent attributes from possible user goals.
Partitionsare combined with possible user dialogue actionsfrom the N-best user input as well as with the di-alogue history.
This combination forms the statespace ?
the set of hypotheses, the probability dis-tribution over which is maintained during the di-alogue.
Since the number of states for any real-world problem is too large, for tractable policylearning, both the state and the action space aremapped into smaller scale summary spaces.
Oncean adequate summary action is found in the sum-mary space, it is mapped back to form an action inthe original master space.3.2 Kernel Choice for GP-SarsaThe summary state in the HIS system is a four-dimensional space consisting of two elements thatare continuous (the probability of the top two hy-potheses) and two discrete elements (one relatingthe portion of the database entries that matches thetop partition and the other relating to the last useraction type).
The summary action space is discreteand consists of eleven elements.In order to apply the GP-Sarsa algorithm, a kernelfunction needs to be specified for both the sum-mary state space and the summary action space.The nature of this space is quite different from theone described in the toy problem.
Therefore, ap-plying a kernel that has negative correlations, suchas the scaled norm kernel (Table 1) might give un-expected results.
More specifically, for a givensummary action, the mapping procedure finds themost appropriate action to perform if such an ac-tion exists.
This can lead to a lower reward ifthe summary action is not adequate but wouldrarely lead to negatively correlated rewards.
Also,parametrised kernels could not be used for thistask, since there was no corpus available for hyper-parameter optimisation.
The polynomial kernel(Table 1) assumes that the elements of the spaceare features.
Due to the way the probability ismaintained over this very large state space, thecontinuous variables potentially encode more in-formation than in the simple toy problem.
There-fore, we used the polynomial kernel for the con-tinuous elements.
For discrete elements, we utilisethe ?-kernel (Eq.
2.3).3.3 Active Learning GP-SarsaThe GP RL framework enables modelling the un-certainty of the approximation.
The uncertaintyestimate can be used to decide which actionsto take during the exploration (Deisenroth et al,2032009).
In detail, instead of a random action, theaction in which the Q-function for the current statehas the highest variance is taken.3.4 Training Set-up and EvaluationPolicy optimisation is performed by interactingwith a simulated user on the dialogue act level.The simulated user gives a reward at the final stateof the dialogue, and that is 20 if the dialogue wassuccessful, 0 otherwise, less the number of turnstaken to fulfil the user goal.
The simulated usertakes a maximum of 100 turns in each dialogue,terminating it when all the necessary informationhas been obtained or if it looses patience.A grid-based MCC algorithm provides the base-line method.
The distance metric used ensuresthat the number of regions in the grid is smallenough for the learning to be tractable (Young etal., 2010).In order to measure how fast each algorithmlearns, a similar training set-up to the one pre-sented in Section 2.7 was adopted and the aver-aged results are plotted on the graph, Fig.
2.200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 300023456789Training dialoguesAveragereward?
Grid?based Monte Carlo Control?
GP?Sarsa with polynomial kernel?
Active learning GP?Sarsa with polynomial kernelFigure 2: Evaluation results on CamInfo taskThe results show that in the very early stage oflearning, i.e., during the first 400 dialogues, theGP-based method learns faster.
Also, the learningprocess can be accelerated by adopting the activelearning framework where the actions are selectedbased on the estimated uncertainty.After performing many iterations in an incremen-tal noise learning set-up (Young et al, 2010) boththe GP-Sarsa and the grid-based MCC algorithmsconverge to the same performance.4 ConclusionsThis paper has described how Gaussian Processesin Reinforcement learning can be successfully ap-plied to dialogue management.
We implementeda GP-Sarsa algorithm on a toy dialogue prob-lem, showing that with an appropriate kernel func-tion faster convergence can be achieved.
We alsodemonstrated how kernel parameters can be learntfrom a dialogue corpus, thus creating a bridgebetween Supervised and Reinforcement learningmethods in dialogue management.
We appliedGP-Sarsa to a real-world dialogue task showingthat, on average, this method can learn faster thana grid-based algorithm.
We also showed that thevariance that GP is estimating can be used in anActive learning setting to further accelerate policyoptimisation.Further research is needed in the area of kernelfunction selection.
The results here suggest thatthe GP framework can facilitate faster learning,which potentially allows the use of larger sum-mary spaces.
In addition, being able to learn ef-ficiently from a small number of dialogues offersthe potential for learning from direct interactionwith real users.AcknowledgementsThe authors would like to thank Carl Rasmussenfor valuable discussions.
This research was partlyfunded by the UK EPSRC under grant agreementEP/F013930/1 and by the EU FP7 Programme un-der grant agreement 216594 (CLASSiC project).ReferencesRI Brafman.
1997.
A Heuristic Variable Grid SolutionMethod for POMDPs.
In AAAI, Cambridge, MA.AR Cassandra.
2005.
POMDP solver.http://www.cassandra.org/pomdp/code/index.shtml.MP Deisenroth, CE Rasmussen, and J Peters.
2009.Gaussian Process Dynamic Programming.
Neuro-comput., 72(7-9):1508?1524.Y Engel, S Mannor, and R Meir.
2005.
Reinforcementlearning with Gaussian processes.
In ICML ?05:Proceedings of the 22nd international conference onMachine learning, pages 201?208, New York, NY.CE Rasmussen and CKI Williams.
2005.
GaussianProcesses for Machine Learning.
MIT Press, Cam-bridge, MA.RS Sutton and AG Barto.
1998.
Reinforcement Learn-ing: An Introduction.
Adaptive Computation andMachine Learning.
MIT Press, Cambridge, MA.JD Williams.
2006.
Partially Observable Markov De-cision Processes for Spoken Dialogue Management.Ph.D.
thesis, University of Cambridge.SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-mann, B Thomson, and K Yu.
2010.
The Hid-den Information State Model: a practical frame-work for POMDP-based spoken dialogue manage-ment.
Computer Speech and Language, 24(2):150?174.204
