Corpus Based Stat ist ical  Genera l izat ion Tree in Rule Opt imizat ion  *J oyce  Yue  Cha i  A lan  W.  B ie rm~n~Department of Computer ScienceBox 90129, Duke UniversityDurham, NC 27708-0129chai@cs.duke.edu awb@cs.duke.eduAbstractA corpus-based statistical Generalization Treemodel is described to achieve rule opthnizationfor the information extraction task.
First, theuser creates pecific rules for the target informa-tion from the sample articles through a traininginterface.
Second, WordNet is applied to gener-alize noun entities in the specific rules.
The de-gree of generalization is adjusted to fit the user'sneeds by use of the statistical Generalization Treemodel FinaUy, the optimally generalized rulesare applied to scan new information.
The resultsof experiments demonstrate he applicability ofour Generalization Tree method.In t roduct ionResearch on corpus-based natural language learningand processing is rapidly accelerating following the in-troduction of large on-line corpora, faster computers,and cheap storage devices.
Recent work involves novelways to employ annotated corpus in part of speech tag-ging (Church 1988) (Derose 1988) and the applicationof mutual information statistics on the corpora to un-cover lexical information (Church 1989).
The goal ofthe research is the construction of robust and portablenatural anguage processing systems.The wide range of topics available on the Internetcalls for an easily adaptable information extraction sys-tem for different domains.
Adapting an extraction sys-teem to a new domain is a tedious process.
In the tradi-tional customization process, the given corpus must bestudied carefully in order to get al the possible waysto express target information.
Many research groupsare implementing the efficient customization of infor-mation extraction systems, such as BBN (Weischedel1995), NYU (Grishman 1995), SRI (Appelt, Hobbs,et al1995), SRA (Krupka 1995), MITRE (Aberdeen,Burger, et al1995), and UMass (Fisher, Soderland, etal 1995).
"This work has been supported by a Fellowship fromIBM Corporation.We employ a rule optimization approach and imple-ment it in our tradable information extraction system.The system allows the user to train on a small amountof data in the domain and creates the specific rules.Then it automatically extracts a generalization fromthe tr~iui~g corpus and makes the rule general for thenew information, depending on the user's needs.
Inthis way, rule generali~.ation makes the customizationfor a new domain easier.This paper specifically describes the automated ruleoptimiT.ation method and the usage of WordNet (Miller1990).
A Generalization Tree (GT) model based on thetr~inlng corpus and WordNet is presented, as well ashow the GT model is used by our system to automat-ically learn and control the degree of generalizationaccording to the user's needs.Sys tem Overv iewThe system cont~i~.~ three major subsystems which,respectively~ address training, rule optlmi~ation, andthe scanning of new information.
The overall struc-ture of the system is shown in Figure 1.
First, eacharticle is partially parsed and segmented into NounPhrases, Verb Phrases and Prepositional Phrases.
AnIBM LanguageWare English Dictionary and Comput-ing Term Dictionary, a Partial Parser I, a Tokenizerand a Preprocessor are used in the parsing process.The Tokenizer and the Preprocessor are designed toidentify some special categories such as e-mail address,phone number, state and city etc.
In the training pro-cess, the user, with the help of a graphical user in-tefface(GUI) scans a parsed sample article and indi-cates a series of semantic net nodes and transitionsthat he or she would like to create to represent the in-formation of interest.
Specifically, the user designatesthose noun phrases in the article that are of interestand uses the interface commands to translate themIWe wish to thank Jerry Hobbs of SRI for providing uswith the finite-state rules for the parser.81\[ Training Article\](fL ~s=~ CL~'J(Rule Generator ~--~-I .
.
.
.
.Rule Optimization ProcessUnseen Article\[-t~Rule Matching Routines JFigure 1: System Overviewinto semantic net nodes.
Furthermore, the user des-ignates verb phrases and prepositions that relate thenoun phrases and uses commands to translate theminto semantic net transitions between nodes.
In theprocess, the user indicates the desired translation ofthe specific information of interest into semantic netform that can easily be processed by the machl-e. Foreach headword in a noun phrase, WordNet is used toprovide sense information.
Usually 90% of words inthe domain are used in sense one (the most frequentlyused sense) as defined in WordNet.
However, somewords might use other sense.
For example, "opening"often appears in the job advertisement domain.
Butinstead of using the first sense as {opening, gap}, ituses the fourth sense as {opportunity, chance}.
Basedon this scenario, for headwords with senses other thansense one, the user needs to identify the appropriatesenses, and the Sense Classifier will keep the record ofthese headwords and their most frequently used senses.When the user takes the action to create the semantictransitions, a Rule Generator keeps track of the user'smoves and creates the rules automatically.
These rulesare specific to the tralni~g articles and they need tobe generalized in order to be applied on other unseenarticles in the domain.
According to ditferent require-ments from the user, the Rule Optimization Engine,based on WordNet, generalizes the specific rules cre-ated in the training process and forms a set of opti-mi~.ed rules for processing new information.
This ruleoptimization process will be explained in the later sec-tions.
During the sc~nnlng of new information, withthe help of a rule matching routine, the system appliesthe optimized rules on a large number of unseen arti-cles from the domain.
For the most headwords in thephrases, if they are not in the Sense Classifier table,sense one in WordNet will be assigned; otherwise, theSense Classifier will provide the system their most fre-quently used senses in the domain.
The output of thesystem is a set of semantic transitions for each articlethat specifically extract information of interest to theuser.
Those transitions can then be used by a Post-processor to frill templates, answer queries, or generateabstracts (Bagga, Chai 1997).Rule OperationsOur trainable information extraction system is a rule-based system, which involves three aspects of role op-erations: rule creation, rule generalization and rule ap-plication.R ide  Creat ionIn a typical information extraction task, the most in-teresting part is the events and relationships holdingamong the events (Appelt, Hobbs, et al1995).
Theserelationships are usually specified by verbs and prepo-sitions.
Based on this observation, the left hand side(LHS) of our meaning extraction rules is made up ofthree entities.
The fn-st and the third entities are thetarget objects in the form of noun phrases, the secondentity is the verb or prepositional phrase indicatingthe zelationship between the two objects.
The righthand side (RHS) of the rule consists of the operationsIIIIII82Training Scntcncc:DCR Inc.G~ Commands: 1I ADD-NODE ISemantic Transition:is looking for ~ C progra ~ers.look forSpecific Rule Created by Rule Generator.\[DCR Inc., NG, I, company\], \[look.for, VG, I, other_type\], \[programmer, NG, I, other_type\]ADD_NODE(DCR Inc.), ADD_NODE(programmer), ADD_~LATION(Iook_.for, DCR Inc., programmer)Figure 2: The Rule Creation Processrequired to create a semantic transition-ADD..NODE,ADD.RELATION.For example, during the training process, as shownin Figure 2, the user tr~in~ on the sentence "DCR Inc.is looking for C programmers...', and would like todesignate the noun phrases(as found by the parser)to be semantic net nodes and the verb phrase to rep-resent a tr0n~ition between them.
The training inter-face provides the user ADD.NODE, ADD.RELATIONGUI commands to accomplish this.
ADD.NODEis to add an object in the semantic transition.ADD.RELATION is to add a relationship between twoobjects.
The specific rule is created automatically bythe rule generator according to the user's moves.Ru le  Genera l i za t ionThe rule created by the rule generator as shown in Fig-ure 2 is very specific, and can only be activated by thetraining sentence.
It will not be activated by other sen-tences uch as "IBM Corporation seeks job candidatesin Louisville...".
Semantically speaking, these two sen-tences are very much alike.
Both of them express thata company is looking for professional people.
However,without generalization, the second sentence will not beprocessed.
So the use of the specific rule is very Hrn-ited.
In order to make the specific rules applicable to alarge number of unseen articles in the domain, a com-prehensive generalization mechauism is necessary.
Weuse the power of WordNet to achieve generalization.Int roduct ion to VCbrdNet WordNet is a large-scale on-line dictionary developed by George Miller andcolleagues at Princeton University (Miller, et al1990a).The most useful feature of WordNet to the NaturalLanguage Processing community is its attempt o or-ganize lexical information in terms of word meanings,rather than word forms.
Each entry in WordNet is aconcept represented by the synset.
A synset is a listof synonyms, uch as {engineer, applied scientist, tech-nologist} .
The information is encoded in the form ofsemantic networks.
For instance, in the network fornouns, there are "part of", "is_a", "member of"..., re-lationships between concepts.
Philip Resnik wrote that"...it is d~mcult o ground taxonomic representationssuch as WordNet in precise formal terms, the use ofthe WordNet axonomy makes reasonably clear the na-ture of the relationships being represented..." (Remik1993).
The hierarchical organization of WordNet byword meanings (Miller 1990) provides the opportunityfor automated generalization.
With the large amountof information in semantic lassification and taxon-omy provided in WordNet, many ways of incorporat-ing WordNet semantic features with generalization areforeseeable.
At this stage, we only concentrate on theHypernym/Hyponym feature.A hyponym is defined in (Mitler, et al1990a) as fol-lows: "A  noun X is said to be a hyponym of a noun Yif we can say that X is a ldnd of Y.
This relation gen-erates a hierarchical tree structure, i.e., a taxonomy.A hyponym anywhere in the hierarchy can be said tobe "a kind of" all of its superordinateds .
.
.
."
If X is ahyponym of Y, then Y is a hypernym of X.General izat ion From the training process, the spe-cific rules contain three entities on the LHS as shown inFigure 3.
Each entity (sp) is a quadruple, in the formof (w,c,s,t), where w is the headword of the trainedphrase; c is the part of the speech of the word; s is thesense number epresenting the meaning of w; t is thesemantic type identified by the preprocessor for w.For each sp = (w,c,s,t) ,  if w exists in WordNet,then there is a corresponding synset in WordNet.
Thehyponym/hypernym hierarchical structure provides a831.
An Abstract Specific Rule:(wl, c~, s~, tl), (w2, c2, s~, t~),(~s, cs, ss, is)> ADD..NODE(wx), ADD_NODE(to2), ADD.RELATION(w2,  wx, ws)2.
A Generalized Rule:(W1, C1, Sx , T1) ?
Generalize( spl , hx ), (W2, Cz , S2, T2 ) e Generalize( st~z, hz ),(Ws, C3, Ss, Ts ) ~ Generalize( sI~, hs ), ) ADD.NODE(Wx), ADD_NODE(Ws), ADD_RELATION(W~,Wx, Ws)Figure 3: Sample Rulessp = (programmer.
NG, I, abet_type)various generalization degr~Genvralize(sp, I) = {engineer, applied scientist, technologist }Gvneralize(sp, 2) = {person, i dividual, someone,...}Generalize(sp, 3)= {life form, organism, being, ...}Generalize(sp, 4) = {chAry)Figure 4: Generalization for a Specific Conceptway of locating the superordinate concepts of sp.
Byfollowing additional Hypernymy, we will get more andmore generMi~ed concepts and eventually reach themost general concept, such as {entity).
As a result,for each concept, different degrees of generalizationcan be achieved by adjusting the distance between thisconcept and the most general concept in the WordNethierarchy (Chai, Bierm~nn 1997).The function to ac-complish this task is Generalize(sp, h), which returnsa hypernym h levels above the concept ~ in the hier-archy.
Generalize(sp, O) returns the synset of sp.
Forexample, in Figure 4, the concept {programmer} is gen-eralized at various levels based on Wordnet Hierarchy.WordNet is an acyclic structure, which suggests thata synset might have more than one hypernym.
How-ever, this situation doesn't happen often.
In case ithappens, the system selects the first hypernym path.The process of generMi~.ing rules consists of replac-ing each sp = (w,c,s,t) in the specific rules by a moregeneral superordinate synset from its hypernym hier-archy in WorclNet by performing the Generalize(sp, h)function.
The degree of generalization for rules varieswith the variation of h in Generalize(sp, h).Figure 3 shows an abstract generalized rule.
The Esymbol signifies the subsumption relationship.
There-fore, a E b signifies that a is subsumed by b, or conceptb is a superordinate concept of concept a.Opt~m~7.ation Rules with different degrees of gen-eralization on their different constituents will have adifferent behavior when processing new texts.
A setof generalized rules for one domain might be sufficient;but in another domain, they might not be.
Wit.hln aparticular rule, the user might expect one entity to berelatively specific and the other entity to be more gen-eral.
For example, if a user is interested in finding allDCR Inc. related jobs, he/she might want to hold thefirst entity as specific as that in Figure 2, and gener-M~ the third entity.
The rule optimization processis to automatically control the degree of generalizationin the generuli~d rules to meet user's different needs.Optimi~-ation will be described in later sections.Ru le  App l i ca t ionThe optimally generalized rules are applied to unseenarticles to achieve information extraction i  the form ofsemantic transitions.
The generaIi~.ed rule states thatthe RHS of the rule gets executed if a/l of the followingconditions atisfy:?
A sentence contains three phrases (not necessarilycontiguous) with headwords W1, W2, and Ws.?
The quadruples corresponding to these head-words are (Wl,C1,Sx,rl), (W2,C2,S2,r2), and(Ws, Cs,Ss, rs).?
The synsets, in WordNet, corresponding to thequadruples, are subsumed by Cenemlize(spl, hi),Gener~ize(s~, h2), and Gener~/ze(s~, hs)respec-tively.Figure 5 shows an example of rule matching and cre-ating a semantic transition for the new information.
In84Specific Rule:\[DCR Inc., NG, 1, company\], [look.for, VG, 1, other.type\], \[programmer, NG, 1, other.type\]= ADD_NODE(DCR Inc.), ADD.NODE(programmer), ADD.RELATION(look.for, DCR Inc., programmer)Generalized to the Most General Rule:(W~,C~,S~,T~) ~ {gr,~p,...}, (W2, C2, S2,T2) ?
{look.for,~eek, se.arch}, (Ws, Cs, Ss, Ts) E {entity}- ADD_NODE(Wx,  ADD_NODE(W3) ,  ADD..RELATION(W~, Wx,Ws)Unseen Sentence:The BiologyLab is searching forsubsumed to ~ subsumed to{group,...} {\]ook./er, seek/search}Execute the RHS of the Rule: search forthe mis~n~._~ogsubsumed to~entity}Figure 5: The Application of Gener~liT.ed Rulesthe example, the most general rule is created by gen-eralizing the first and the third entities in the specificrule to their top hypernyms in the hierarchy.
Sinceverbs usually have only one level in the hierarchy, theyare generalized to the syuset at the same level.Ru le  Opt imizat ionThe specific rule can be generalized to the most gen-eral rule as in Figure 5.
When we apply this mostgeneral rule again to the traLning corpus, a set of se-mantic transitions are created.
Some trausitions arerelevant, while the others are not.
Users are expectedto select he relevant transitions through a user inter-face.
We need a mechanism to determine the level ofgeneralization that can achieve best in extracting therelevant information and ignoring the irrelevant infor-mation.
Therefore, Generalization Tree (GT) is de-signed to accomplish this task.
While maintaining thesemantic relationship of objects as in WordNet, GTscollect the relevancy information of all activating ob-jects and automatically find the optimal level of gener-alization to fit the user's needs.
A database is used tomaintain the relevancy information for all the objectswhich activate ach most general concept in the mostgeneral rule.
This database is transformed to the GTstructure, which keeps the statistical information ofrelevancy for each activating object and the semanticrelations between the objects from WordNet.
The sysotern automatically adjusts the generalization degreesfor each noun entity in the rules to match the desiresof the user.
The idea of this optlmi~tion process isto first keep recall as high as possible by applying themost general rules, then adjust he precision by tuningthe rules based on the user's pecific inputs.An  Example  of GTSuppose we apply the most general rule in Figure 5 tothe training corpus, and the entity three in the rule isactivated by a set of objects hown in Table 1.
Froma user interface and a statistical classifier, the rele-vancy_rate(reLrate) for each object can be calculated.rel_rate(obj) = count of ob\] being relevanttotal count of  occurenoe of  objAs shown in Table 1,for example, rel3ate({analyst...}) = 80%, which in-dicates that when {entity} in the most general rule isactivated by analyst, 80% of time it hits relevant infor-mation and 20% of time it hits irrelevant information.On the other hand, it suggests that if {entity} is re-placed by the concept {analyst...}, a roughly 80% pre-cision could be achieved in extracting the relevant in-formation.
The corresponding GT for Table 1 is shownin Figure 6.In GT, each activating object is the leaf node in thetree, with an edge to its immediate hyperaym (par-ent).
For each hypernym list in the database, thereis a corresponding path in GT.
Besides the ordinaryhard edge represented by the solid line, there is a softedge represented by the dotted line.
Concepts con-nected by the soft edge are the same concepts.
Theonly difference between them is that the leaf node isthe actual activating object, while the internal node isthe hypernym for other activating objects.
Hard edgesand soft edges only have representational difference,as to the calculation, they are treated the same.
Each85o1~ject sense1 analystcandidateindividualparticipanthypemym list( anayst ) {ezpe,-t} {individual}{life form} =~, {entity}{candidate} ~ {ap1~licant} ~ {individual}{life form} m {entity}.
{ !ndividual }~ {life form} =~ .
{..entity }depth ?el_rate count4 i 80% 54 100%2 100%5 O%_professional 1 .
... 4 100%software 1 ... 5 0%=.o  !
.o .
.o .
o .
.
.
.
.Table 1: Sample Database for Objects Activating {entity}5121{En6ty} c= 17or .~.
.
.WbJ c=il~ .
.
.
}c : lrffiO4bl~oa , ,}?=1I~on .
- .
Ic f f i l~./= O~{life from.
m~mism.-} c = 16. ?
= 87.$~I{pe~.iadividmL.}
c=16.
r=87.5%{V, lffit..} IggL.}
{~,  ai~l..} {sppli?ffi~l { ig i~ lag}r = ~Oe~ r = lOOgb r = 0% r = lO0~ rd  = lO0%(aa'~yu} {l~fessi~al} |~- . )
{~ndkim)~ ffi $ count=2 I ?=1 ?oum = $rd  = 80e~ vd =/00~b I r f0~b rd  = 100e~count = 1rd=0~Figure 6: An Example of Generalization Treenode has two other fields counts of occurrence and rel-evancy_rate.
For the leaf nodes, those fields can befilled from the database directly.
For internal nodes,the instantiation ofthese fields depends on its children(hyponym) nodes.
The calculation will be describedlater.If the relevancy_rate for the root node {entity} is82.3%, it indicates that, with the probability 82.3%,objects which activate {entity} are relevant.
If theuser is satisfied with this rate, then it's not necessaryto alter the most general concept in the rule.
If theuser feels the estimated precision is too low, the sys-tem will go down the tree, and check the relevancy_ratein the next level.
For example, if 87.5% is good enough,then the concept {life form, organism...} will substi-tute {entity} in the most general rule.
If the preci-sion is still too low, the system will go down the tree,find {adult..}, {applicant..}, and replace the concept{entity} in the most general rule with the union ofthese two concepts.Genera l i za t ion  Tree  Mode lLet's suppose Zn is a noun entity in the most generalrule, and zn is activated by q concepts el ?, e~, ....
eq?
; thetimes of activation for each ei ?
are represented by c4.Since e~(i < q) activates zn, there exists a hypernyml ist .... z .
in WordNet ,  where isthe immediate hypernym of e~ - I .
The system main-tains a database of activation information as shown inTable 2, and transforms the database to a GT modelautomatically.GT is an n-ary branching tree structure with thefoUowing properties:?
Each node represents a concept, and each edge rep-resents the hypernym relationship between the con-cepts.
If ei is the immediate hypernym of ej, thenthere is an edge between ode ei and ej.
el is onelevel above ej in the tree.?
The root node zn is the most general concept fromthe most general rule.86activating objectse!'.
.
?
.sense I counts hypernym lists, e~ e y =r el ~ .... ~ z .s2 !
e~ ~md~.
.
.~z .?
.
?
.
.
.
.
.
.
.Sq !
e?
eq , ...depthdld2.
o?
.d~relevancy_raterlr2.
.orqTable 2: database of activating conceptsThe leaf nodes =o oo .o are the concepts which ~I ' ~ '  """~'qi activate zn.
The internal nodes are the concepts ej(i ~ 0 and 1 < j _~ q) from the hypernym paths forthe activating concepts.For a leaf node ei?
:~o~nt~(e~) =releva'ricy.rate(ei ?)
= ri* For an internal node e, if it has n hyponyms(ie, chil-dren) co, ...e~ then:11counts(e) = ~ eounts(ei)i.~.1nrelevancy.rate(e) = ~ P ( eO * relevancy_rate(w)i~ lwhereP(ei) = counts(ei)counts(e)Opt imized  Ru leFor each noun entity in the most general rule, the sys-tem keeps a GT  from the tra~in!ng set.
Depending onuser's di~Ibxent needs, a threshold 0 is pre-selected.
Foreach GT, the system will start from the root node,go dow~ the tree, and find all the nodes e~ such thatreIevan~,_rate(ei) _> O.
If a node relevancy_rate ishigher tl~an O, its children odes will be ignored.
In thisway, the\[system maintains a set of concepts whose re/e-san~y.r~te is higher than O, which is called Optimized-Concept~.
By substituting zn in the most general rulewith O p~!mized-Conc~pts, an optimized rule is createdto meet ;he user's needs.The se ~rehlng algorithm isbasically the breadth-firstsearch a~1.
Initial~selectrelevmthe pr,to extlnot ca, :follows:ze Optimal-Concep~ to be empty set.
Pre-flae threshold 0.
If the user wants to get the~t information and particularly cares about~c~ion, 0 should be set high; if the user wantsact as much as information possible and does:e about the precision, 0 should be set low.2.
Starting from the root node z, perform the.Recursive-Search algorithm, which is defined as thefollowing:Reenrsive-Search(concept z){ if (relevancy.rate(z) _>0) {put z into Optimal-Concepts set;ezit;}else {let m denote the number of children nodes o\[ z;let zi denote the child ol z (0 < i _< m);for ( i  = 1; i < m; i++)Recursive-S earch (zi ) ;);}Exper iment  and  D iscuss ionIn this section we present and discuss results from anexperiment.
The experimental domain is triangle.jobsUSENET newsgroup.
We trained our system on 24articles for the extraction of six facts of interests asfollows:Company Name.
Examples: IBM, Metro Informa-tion Services, DCR Inc.Position/Title.
Examples: programmer, financialanalyst, software ngineer.Experience/Skill.
Example: 5 years experience inOracle.?
Location.
Examples: Winston-Salem, North Car-olina.Benefit.
Examples: company matching funds~ com-prehensive health plan.Contact Info.
Examples: Fax is 919-660-6519, e-mailaddress.The testing set contained 162 articles from the samedomain as the system was trained on.
Out of 162articles, 21 articles were unrelated to the domaindue to the misplacement made by the person whoposted them.
Those unrelated articles were about jobs87location benefit contact info facts company position experiencetraining 62.5% 83.3% 91.7%testing 63.1% 90.8% 90.8%66.7% 25.0%62.4% 23.4%95.8%97.9%Table 3: Percentage ofFacts in Training and Testingwanted, questions answered, ads to web site etc.
First,we compared some of the statistics from the tr~nlngset and testing set.
The percentage of representationof each fact in the articles for both training and te -ing domain is shown in Table 3, which is the number ofarticles containing each fact out of the total number ofarticles.
The distribution of number of facts presentedin each article is shown in Figure 7.The mean number of facts in each article from thetra;nlng set is 4.39, the standard deviation is 1.2; themean number of facts in each article from the testingset is 4.35, the standard deviation is 1.
Although thesestatistics are not strong enough to indicate the train-ing set is absolutely the good trMn;ng corpora for thisinformation extraction task, it suggests that as far asthe facts of interest are concerned, the training set isa reasonable set to be trained and learned.J~toJ :  ~o10.90.80.7O.50.50.40.30.20.100Uainin 0tes~ng1 2 3 4 S 6number of facts in each a~cleFigure 7: Distribution of Number of Facts in EachArticleThe evaluation process consisted of the followingsteps: fn'st, each unseen article was studied to see ifthere was any fact of interest presented; second, thesemantic transitions produced by the system were ex-amined to see if they correctly extracted the fact of in-terest.
Precision is the number of transitions correctlyextracting facts of interest out of the total number oftransitions produced by the system; recall is the num-ber of facts which have been correctly extracted outof the total number of facts of interest.
The overallperformance of recall and precision is defined by theFomeasurement (Chinchor 1992), which is(~2 + 1.0) ?
P * R~.P+Rwhere P is precision, R is recall, 13 = 1 ff precision andrecall are equally important.First, we tested on single fact extraction, which wasposition~title fact.
The purpose of this experiment is totest whether the different 8 values will lead to the ex-pected recall, and precision statistics.
From the resultout of 141 related testing articles, the recall, precision,F-measurement curves are shown in Figure 8.
Recallis 51.6% when 8 = 1.0, which is lower than 75% at# = 0, however, precision is the highest at 84.7% when0 = 1.0.
The F-measurement achieves its highest valueat 64.1% when 0 = 1.0.QCR QCaI \[ \[ I i \[ i I l Iprecision - -0,.9 recall ~ "0.8 F -me~u~en~'~0.7 .
.
.
.
.
.
.
.
.
.
"~.2U0.40.30.20.100 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.5 0.9 IGT ~resholdF'tgure 8: Performance vs. GT  ThresholdAs mentioned earlier, 21 articles from the testingcorpus are unrelated to the job advertisement domain.The interesting question rising here is can we use GTrule optimization method to achieve the informationretrieval, in this particular case, to identify those unre-lated articles.
Certaln\]y, we would hope that optlrn;zedrules won't produce any trauqitions from the unrelatedarticles.
The result is shown in Figure 9.
The precisionof unrelated articles is the number of articles withoutany transitions created out of total 21 articles.
We cansee that, when 0 = 0.8, 1.0, precision is 95.7%.
Onlyone article out of 21 articles is mis-identified.
But when0 = 0, 0.2, the precision rate is very low, only 28.6%88and 38.1%.
If we use the traditional way of keywordmatching to do this information retrieval, the precisionwon't achieve as high as 95.7% since a few resume andjob wanted postings will succeed the keyword matchingand be mls-identitled asrelated articles.| | | I | | !
| 10.9O.80.70.6O.5O.40.30.20.100I !
I I I I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9GT thresholdFigure 9: Precision of Identifying Unrelated vs. GTThresholdThe system performance on extracting six facts isshown in Figure I0.
The overall performance F-measurement gets to its peak at 70.2% when 0 = 0.8.When 0 = 1.0, the precision does not get to what weexpected.
One explanation is that, Im!ike the extrac-tion of position/title fact, for extracting the six factsfrom the domain, the training data is quite small.
It isnot sufficient enough to support he user's requirementfor a strong estimate of precision.
0 = 0.8 is the bestchoice when the training corpus is small.oQ.1O.9O.80.70.60.50.40.30.20.100t t ' |  J i i J I |precisionrecall ----.F-measurement ....... .
.
.
.
.
-~ .t t !
s , I I I I0.1 0.2.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1GT lhresholdFigure I0: Performance of Extracting Six Facts vs. GTThresholdSome problems were also detected which preventbetter performance of the system.
The cu~ent do-main is a newsgroup, where anyone can post anythingwhich he/she believes is relevant to the newsgroup.
Itis inevitable that some typographical errors and someabbreviations occur in the articles.
And the format ofthe article sometimes is unpredictable.
If we can in-corporate into the system aspo\]llng checker, and builda database for the commonly used abbreviations, thesystem performance is expected to be enhanced.
Someproblems came for the use of WordNet as well.
Forexample, the sentence "DCR Inc. is looking for Q/Apeople" won't activate the most general rule in Fig-ure 5.
The reason for that is people subsumed to con-cept {group, grouping}, but not the concept {entity}.This problem can be fixed by adding one more rulewith {group, grouping} substituting {~nt/~} in mostgeneral rule in Figure 5.
WordNet has very refinedsenses for each concept, including some rarely usedones, which sometimes causes problems too.
This kindof problem certainly hurts the performance, but it'snot easy to correct because of the nature of WordNet.However, the use of WordNet generally provides agoodmethod to achieve generalization in this domain of jobadvertisement.Conclusion and Future WorkThis paper describes a rule 0ptlmizztion approach byusing Generalization Tree and WordNet.
Our informa-tion extraction system learns the necessary knowledgeby analyzing sample corpora through a training pro-cess.
The rule optimization makes it easier for theinformation extraction system to be customized to anew domain.
The Generalization Tree algorithm pro-rides a way to make the system adaptable to the user'sneeds.
The idea of first achieving the highest recallwith low precision, then adjusting precision to sat-isfy user's needs has been successful.
We are currentlystudying how to enhance the system performance byfurther efining the generalization approach.ReferencesAberdeen, John, John Burger, David Day, LynetteHirschman, Patricia Robinson, and Marc Vil~n 1995MITRE:  Description of the ALEMBIC System Usedfor MUC-6, Proceedings ofthe Sizth Message Under-standing Conference (MUC-6), pp.
141-155, Novem-ber 1995.Appelt, Douglas E., Jerry R. Hobbs, John Bear,David Israel, Megumi Kameymna, Andy Kehler,David Martin, Karen Myers, and Mabry Tyson 1995.SRI International: Description of the FASTUS Sys-tem Used for MUC-6, Proceedings ofthe Sizth Mes-sage Understanding Conference (MUC,-6), pp.
237-248, November 1995.89t , .
.Bagga, Amit, and Joyce Y. Chai 1997 A TrainableMessage Understanding System Computational Natu-ral Language Learning (CoNLL97),pp.
1-8, July 1997.Chai, Joyce Y. and Alan W. Biermann 1997 A Word-Net Based Rule Generalization Engine For Me~niugExtraction To appear at Tenth International Sympo-sium On Methodologies For Intelligent Systems, 1997.Chinchor, Nancy 1992.
MUC-4 Evaluation Metrics,Proceedings of the Fourth Message UnderstandingConference (MUC-4), June 1992, San Mateo: Mor-gan Kalrfm~nn.Church, Kenneth 1988 A Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text Pro-ceedings of the Second Conference on Applied NaturalLanguage Processing, ACL, 1988.Church, Kenneth, William Gale, Patrick Hauks, andDonald Hindle.
1989 Parsing, Word Associations andtypical Predicate-Argument Relations.
Proceedings ofthe International Workshop on Parsing Technologies,1989.Derose, S., 1988 Grammatical Category Disambigua-tion by Statistical Optimization Computational Lin-gu/sties, 14, 1988.Fisher, David, Stephen Soderland, Joseph McCarthy,Fangfang Feng and Wendy Lehnert.
1995.
Descriptionof the UMass System as Used for MUCC-6, Proceed-ings of the Sizth Message Understanding Conference(MUG-6), pp.
127-140, November 1995.Grishmau, Ralph 1995.
The NYU System for MUC-6or Where's the Syntax?
Proceedings ofthe Sizth Mes-sage Understanding Conference (MUC-6), pp.
167-175, November 1995.Krupka, George 1~.
1995.
Description of the SRA Sys-tem as Used for MUC-6, Proceedings ofthe Sixth Mes-sage Understanding Conference (MUG-6), pp.
221-235, November 1995.Miller, George A.
1990.
Introduction to WordNet: AnOn-Line Lexical Database.
WordNet Manuals, pp.
10-32, August 1993.Miller, George A., et al 1990a.
Five Papers on Word-Net, Cognitive Science Laboratory, Princeton Univer-sity, No.
43, July 1990.Resnik, Philip 1995a Using Information Con-tent to Evaluate Seantic Similsrity in a Taxon-omy.Proceedings of IJCAI-g5Resnik, Philip 1995b Disambiguating oun groupingswith respect o WordNet senses, Third Worshop onVery Large Corpora, 1995.Resuik, Philip 1993 Selection and Information: AClass Based Approach to Lexical Relationships, Ph.DDissertation, University of Pennsylvania, 1993.We~chedel, Ralph 1995.
BBN: Description of thePLUM System as Used for MUC-6, Proceedings ofthe Sixth Message Understanding Conference (MUG-6), pp.
55-69, November 1995.90
