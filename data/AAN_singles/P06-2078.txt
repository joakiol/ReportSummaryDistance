Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 603?610,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Automatic Method for Summary EvaluationUsing Multiple Evaluation Results by a Manual MethodHidetsugu NanbaFaculty of Information Sciences,Hiroshima City University3-4-1 Ozuka, Hiroshima, 731-3194 Japannanba@its.hiroshima-cu.ac.jpManabu OkumuraPrecision and Intelligence Laboratory,Tokyo Institute of Technology4259 Nagatsuta, Yokohama, 226-8503 Japanoku@pi.titech.ac.jpAbstractTo solve a problem of how to evaluatecomputer-produced summaries, a numberof automatic and manual methods havebeen proposed.
Manual methods evaluatesummaries correctly, because humansevaluate them, but are costly.
On theother hand, automatic methods, whichuse evaluation tools or programs, are lowcost, although these methods cannotevaluate summaries as accurately asmanual methods.
In this paper, weinvestigate an automatic evaluationmethod that can reduce the errors oftraditional automatic methods by usingseveral evaluation results obtainedmanually.
We conducted someexperiments using the data of the TextSummarization Challenge 2 (TSC-2).
Acomparison with conventional automaticmethods shows that our methodoutperforms other methods usually used.1 IntroductionRecently, the evaluation of computer-producedsummaries has become recognized as one of theproblem areas that must be addressed in the fieldof automatic summarization.
To solve thisproblem, a number of automatic (Donaway et al,2000, Hirao et al, 2005, Lin et al, 2003, Lin,2004, Hori et al, 2003) and manual methods(Nenkova et al, 2004, Teufel et al, 2004) havebeen proposed.
Manual methods evaluatesummaries correctly, because humans evaluatethem, but are costly.
On the other hand,automatic methods, which use evaluation tools orprograms, are low cost, although these methodscannot evaluate summaries as accurately asmanual methods.
In this paper, we investigate anautomatic method that can reduce the errors oftraditional automatic methods by using severalevaluation results obtained manually.
Unlikeother automatic methods, our method estimatesmanual evaluation scores.
Therefore, our methodmakes it possible to compare a new system withother systems that have been evaluated manually.There are two research studies related to ourwork (Kazawa et al, 2003, Yasuda et al, 2003).Kazawa et al (2003) proposed an automaticevaluation method using multiple evaluationresults from a manual method.
In the field ofmachine translation, Yasuda et al (2003)proposed an automatic method that gives anevaluation result of a translation system as ascore for the Test of English for InternationalCommunication (TOEIC).
Although theeffectiveness of both methods was confirmedexperimentally, further discussion of four points,which we describe in Section 3, is necessary fora more accurate summary evaluation.
In thispaper, we address three of these points based onKazawa?s and Yasuda?s methods.
We alsoinvestigate whether these methods canoutperform other automatic methods.The remainder of this paper is organized asfollows.
Section 2 describes related work.Section 3 describes our method.
To investigatethe effectiveness of our method, we conductedsome examinations and Section 4 reports onthese.
We present some conclusions in Section 5.2 Related WorkGenerally, similar summaries are considered toobtain similar evaluation results.
If there is a setof summaries (pooled summaries) produced froma document (or multiple documents) and if theseare evaluated manually, then we can estimate amanual evaluation score for any summary to beevaluated with the evaluation results for thosepooled summaries.
Based on this idea, Kazawa et603al.
(2003) proposed an automatic method usingmultiple evaluation results from a manualmethod.
First, n summaries for each document, m,were prepared.
A summarization systemgenerated summaries from m documents.
Here,we represent the ith summary for the jth documentand its evaluation score as xij and yij, respectively.The system was evaluated using Equation 1.?
?= =+=miijijnjj bxxSimywxscr1 1),()(  (1)The evaluation score of summary x wasobtained by summing parameter b for all thesubscores calculated for each pooled summary,xij.
A subscore was obtained by multiplying aparameter wj, by the evaluation score yij, and thesimilarity between x and xij.In the field of machine translation, there isanother related study.
Yasuda et al (2003)proposed an automatic method that gives anevaluation result of a translation system as ascore for TOEIC.
They prepared 29 humansubjects, whose TOEIC scores were from 300s to800s, and asked them to translate 23 Japaneseconversations into English.
They also generatedtranslations using a system for each conversation.Then, they evaluated both translations using anautomatic method, and obtained WH, whichindicated the ratio of system translations thatwere superior to human translations.
Yasuda et alcalculated WH for each subject and plotted thevalues along with their corresponding TOEICscores to produce a regression line.
Finally, theydefined a point where the regression line crossedWH = 0.5 to provide the TOEIC score for thesystem.Though, the effectiveness of Kazawa?s andYasuda?s methods were confirmedexperimentally, further discussions of four points,which we describe in the next section, arenecessary for a more accurate summaryevaluation.3 Investigation of an Automatic Methodusing Multiple Manual EvaluationResults3.1 Overview of Our Evaluation Methodand Essential Points to be DiscussedWe investigate an automatic method usingmultiple evaluation results by a manual methodbased on Kazawa?s and Yasuda?s method.
Theprocedure of our evaluation method is shown asfollows;(Step 1) Prepare summaries and theirevaluation results by a manual method(Step 2) Calculate the similarities between asummary to be evaluated and the pooledsummaries(Step 3) Combine manual scores of pooledsummaries in proportion to their similaritiesto the summary to be evaluatedFor each step, we need to discuss the followingpoints.
(Step 1)1.
How many summaries, and what type(variety) of summaries should be prepared?Kazawa et al prepared 6 summaries foreach document, and Yasuda et al prepared29 translations for each conversation.However, they did not examine about thenumber and the type of pooled summariesrequired to the evaluation.
(Step 2)2.
Which measure is better for calculating thesimilarities between a summary to beevaluated and the pooled summaries?Kazawa et al used Equation 2 to calculatesimilarities.|)||,min(|||),(xxxxxxSimijijij?=  (2)where xxij ?
indicates the number ofdiscourse units1 that appear in both xij and x,and | x | represents the number of words in x.However, there are many other measuresthat can be used to calculate the topicalsimilarities between two documents (orpassages).As well as Yasuda?s method does, usingWH is another way to calculate similaritiesbetween a summary to be evaluated andpooled summaries indirectly.
Yasuda et al(2003) tested DP matching (Su et al, 1992),BLEU (Papineni et al, 2002), and NIST2,for the calculation of WH.
However there aremany other measures for summaryevaluation.1 Rhetorical Structure Theory Discourse Treebank.www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002T07 Linguistic Data Consortium.2 http://www.nist.gov/speech/tests/mt/mt2001/resource/6043.
How many summaries should be used tocalculate the score of a summary to beevaluated?
Kazawa et al used all the pooledsummaries but this does not ensure the bestperformance of their evaluation method.
(Step 3)4.
How to combine the manual scores of thepooled summaries?
Kazawa et al calculatedthe score of a summary as a weighted linearsum of the manual scores.
Applyingregression analysis (Yasuda et al, 2003) isanother method of combining severalmanual scores.3.2 Three Points Addressed in Our StudyWe address the second, third and fourth points inSection 3.1.
(Point 2) A measure for calculatingsimilarities between a summary to beevaluated and pooled summaries:There are many measures that can calculate thetopical similarities between two documents (orpassages).
We tested several measures, such asROUGE (Lin, 2004) and the cosine distance.
Wedescribe these measures in detail in Section 4.2.
(Point 3) The number of summaries used tocalculate the score of a summary to beevaluated:We use summaries whose similarities to asummary to be evaluated are higher than athreshold value.
(Point 4) Combination of manual scores:We used both Kazawa?s and Yasuda?s methods.4 Experiments4.1 Experimental MethodsTo investigate the three points described inSection 3.2, we conducted the following fourexperiments.z Exp-1: We examined Points 2 and 3 basedon Kazawa?s method.
We tested thresholdvalues from 0 to 1 at 0.005 intervals.
Wealso tested several similarity measures, suchas cosine distance and 11 kinds of ROUGE.z Exp-2: In order to investigate whether theevaluation based on Kazawa?s method canoutperform other automatic methods, wecompared the evaluation with otherautomatic methods.
In this experiment, weused the similarity measure, which obtainthe best performance in Exp-1.z Exp-3: We also examined Point 2 based onYasuda?s method.
As a similarity measure,we tested cosine distance and 11 kinds ofROUGE.
Then, we examined Point 4 bycomparing the result of Yasuda?s methodwith that of Kazawa?s.z Exp-4: In the same way as Exp-2, wecompared the evaluation with otherautomatic methods, which we describe inthe next section, to investigate whether theevaluation based on Yasuda?s method canoutperform other automatic methods.4.2 Automatic Evaluation Methods Used inthe ExperimentsIn the following, we show the automaticevaluation methods used in our experiments.Content-based evaluation (Donaway et al,2000)This measure evaluates summaries by comparingtheir content words with those of the human-produced extracts.
The score of the content-based measure is obtained by computing thesimilarity between the term vector using tf*idfweighting of a computer-produced summary andthe term vector of a human-produced summaryby cosine distance.ROUGE-N (Lin, 2004)This measure compares n-grams of twosummaries, and counts the number of matches.The measure is defined by Equation 3.?
??
??
??
?=?RS SgramNRS SgramNmatchNNgramCountgramCountNROUGE)()((3)where Count(gramN) is the number of an N-gramand Countmatch(gramN) denotes the number of n-gram co-occurrences in two summaries.ROUGE-L (Lin, 2004)This measure evaluates summaries by longestcommon subsequence (LCS) defined byEquation 4.mCrLCSLROUGEuiii?=?=?
),((4)where LCSU(ri,C) is the LCS score of the union?slongest common subsequence between referencesentences ri and the summary to be evaluated,and m is the number of words contained in areference summary.605ROUGE-S (Lin, 2004)Skip-bigram is any pair of words in theirsentence order, allowing for arbitrary gaps.ROUGE-S measures the overlap of skip-bigramsin a candidate summary and a referencesummary.
Several variations of ROUGE-S arepossible by limiting the maximum skip distancebetween the two in-order words that are allowedto form a skip-bigram.
In the following,ROUGE-SN denotes ROUGE-S with maximumskip distance N.ROUGE-SU (Lin, 2004)This measure is an extension of ROUGE-S; itadds a unigram as a counting unit.
In thefollowing, ROUGE-SUN denotes ROUGE-SUwith maximum skip distance N.4.3 Evaluation MethodsIn the following, we elaborate on the evaluationmethods for each experiment.Exp-1: An experiment for Points 2 and 3based on Kazawa?s methodWe evaluated Kazawa?s method from theviewpoint of ?Gap?.
Differing from otherautomatic methods, the method uses multiplemanual evaluation results and estimates themanual scores of the summaries to be evaluatedor the summarization systems.
We thereforeevaluated the automatic methods using Gap,which manually indicates the difference betweenthe scores from a manual method and eachautomatic method that estimates the scores.
First,an arbitrary summary is selected from the 10summaries in a dataset, which we describe inSection 4.4, and an evaluation score is calculatedby Kazawa?s method using the other ninesummaries.
The score is compared with a manualscore of the summary by Gap, which is definedby Equation 5.nmyxscrGapmknlklkl??=?
?= =1 1|)('|(5)where xkl is the kth system?s lth summary, and yklis the score from a manual evaluation method forthe kth system?s lth summary.
To distinguish ourevaluation function from Kazawa?s, we denote itas scr?(x).
As a similarity measure in scr?
(x), wetested ROUGE and the cosine distance.We also tested the coverage of the automaticmethod.
The method cannot calculate scores ifthere are no similar summaries above a giventhreshold value.
Therefore, we checked thecoverage of the method, which is defined byEquation 6.summariesgivenofnumberThemethodthebyevaluatedsummariesofnumberTheCoverage =  (6)Exp-2: Comparison of Kazawa?s method withother automatic methodsTraditionally, automatic methods have beenevaluated by ?Ranking?.
This means thatsummarization systems are ranked based on theresults of the automatic and manual methods.Then, the effectiveness of the automatic methodis evaluated by the number of matches betweenboth rankings using Spearman?s rank correlationcoefficient and Pearson?s rank correlationcoefficient (Lin et al, 2003, Lin, 2004, Hirao etal., 2005).
However, we did not use bothcorrelation coefficients, because evaluationscores are not always calculated by a Kazawa-based method, which we described in Exp-1.Therefore, we ranked the summaries instead ofthe summarization systems.
Two arbitrarysummaries from the 10 summaries in a datasetwere selected and ranked by Kazawa?s method.Then, Kazawa?s method was evaluated using?Precision,?
which calculates the percentage ofcases where the order of the manual method ofthe two summaries matches the order of theirranks calculated by Kazawa?s method.
The twosummaries were also ranked by ROUGE and bycosine distance, and both Precision values werecalculated.
Finally, the Precision value ofKazawa?s method was compared with those ofROUGE and cosine distance.Exp-3: An experiment for Point 2 based onYasuda?s methodAn arbitrary system was selected from the 10systems, and Yasuda?s method estimated itsmanual score from the other nine systems.Yasuda?s method was evaluated by Gap, whichis defined by Equation 7.myxsGapmkkk?=?= 1|)(|(7)where xk is the kth system, s(xk) is a score of xk byYasuda?s method, and yk is the manual score forthe kth system.
Yasuda et al (2003) tested DPmatching (Su et al, 1992), BLEU (Papineni et al,2002), and NIST3, as automatic methods used intheir evaluation.
Instead of those methods, we3 http://www.nist.gov/speech/tests/mt/mt2001/resource/606tested ROUGE and cosine distance, both ofwhich have been used for summary evaluation.If a score by Yasuda?s method exceeds therange of the manual score, the score is modifiedto be within the range.
In our experiments, weused evaluation by revision (Fukushima et al,2002) as the manual evaluation method.
Therange of the score of this method is between zeroand 0.5.
If the score is less than zero, it ischanged to zero and if greater than 0.5 it ischanged to 0.5.Exp-4: Comparison of Yasuda?s method andother automatic methodsIn the same way as for the evaluation ofKazawa?s method in Exp-2, we evaluatedYasuda?s method by Precision.
Two arbitrarysummaries from the 10 summaries in a datasetwere selected, and ranked by Yasuda?s method.Then, Yasuda?s method was evaluated usingPrecision.
Two summaries were also ranked byROUGE and by cosine distance and bothPrecision values were calculated.
Finally, thePrecision value of Yasuda?s method wascompared with those of ROUGE and cosinedistance.4.4 The Data Used in Our ExperimentsWe used the TSC-2 data (Fukushima et al,2002) in our examinations.
The data consisted ofhuman-produced extracts (denoted as ?PART?
),human-produced abstracts (denoted as ?FREE?
),computer-produced summaries (eight systemsand a baseline system using the lead method(denoted as ?LEAD?))
4 , and their evaluationresults by two manual methods.
All thesummaries were derived from 30 newspaperarticles, written in Japanese, and were extractedfrom the Mainichi newspaper database for theyears 1998 and 1999.
Two tasks were conductedin TSC-2, and we used the data from a singledocument summarization task.
In this task,participants were asked to produce summaries inplain text in the ratios of 20% and 40%.Summaries were evaluated using a rankingevaluation method and the revision methodevaluation.
In our experiments, we used theresults of evaluation from the revision method.This method evaluates summaries by measuringthe degree to which computer-producedsummaries are revised.
The judges read the4 In Exp-2 and 4, we evaluated ?PART?, ?LEAD?,and eight systems (candidate summaries) byautomatic methods using ?FREE?
as the referencesummaries.original texts and revised the computer-producedsummaries in terms of their content andreadability.
The human revisions were made withonly three editing operations (insertion, deletion,replacement).
The degree of the human revision,called the ?edit distance,?
is computed from thenumber of revised characters divided by thenumber of characters in the original summary.
Ifthe summary?s quality was so low that a revisionof more than half of the original summary wasrequired, the judges stopped the revision and ascore of 0.5 was given.The effectiveness of evaluation by the revisionmethod was confirmed in our previous work(Nanba et al, 2004).
We compared evaluation byrevision with ranking evaluation.
We also testedother automatic methods: content-basedevaluation, BLEU (Papineni et al, 2001) andROUGE-1 (Lin, 2004), and compared theirresults with that of evaluation by revision asreference.
As a result, we found that evaluationby revision is effective for recognizing slightdifferences between computer-producedsummaries.4.5 Experimental Results and DiscussionExp-1: An experiment for Points 2 and 3based on Kazawa?s methodTo address Points 2 and 3, we evaluatedsummaries by the method based on Kazawa?smethod using 12 measures, described in Section4.4, as measures to calculate topical similaritiesbetween summaries, and compared thesemeasures by Gap.
The experimental results forsummarization ratios of 40% and 20% areshown in Tables 1 and 2, respectively.
Tablesshow the Gap values of 12 measures for eachCoverage value from 0.2 to 1.0 at 0.1 intervals.Average values of Gap for each measure are alsoshown in these tables.
As can be seen fromTables 1 and 2, the larger the threshold value,the smaller the value of Gap.
From the result, wecan conclude for Point 3 that more accurateevaluation is possible when we use similarpooled summaries (Point 2).
However, thenumber of summaries that can be evaluated bythis method was limited when the thresholdvalue was large.Of the 12 measures, unigram-based methods,such as cosine distance and ROUGE-1, producedgood results.
However, there were no significantdifferences between measures except for whenROUGE-L was used.607Table 1 Comparison of Gap values for several measures(ratio: 40%)CoverageMeasure1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 AverageR-1 0.080 0.070 0.067 0.057 0.064 0.062 0.058 0.045 0.041 0.062R-2 0.082 0.074 0.070 0.070 0.069 0.063 0.059 0.051 0.042 0.065R-3 0.083 0.074 0.075 0.071 0.069 0.063 0.059 0.051 0.045 0.066R-4 0.085 0.078 0.076 0.073 0.069 0.064 0.060 0.051 0.043 0.067R-L 0.102 0.100 0.097 0.094 0.091 0.090 0.089 0.082 0.078 0.091R-S 0.083 0.077 0.073 0.073 0.069 0.067 0.064 0.060 0.045 0.068R-S4 0.083 0.072 0.071 0.069 0.066 0.066 0.060 0.054 0.044 0.065R-S9 0.083 0.075 0.069 0.070 0.067 0.066 0.066 0.057 0.046 0.067R-SU 0.083 0.077 0.070 0.071 0.069 0.068 0.064 0.057 0.043 0.067R-SU4 0.082 0.073 0.069 0.069 0.065 0.068 0.063 0.051 0.043 0.065R-SU9 0.083 0.074 0.070 0.068 0.066 0.067 0.066 0.054 0.046 0.066Cosine 0.081 0.074 0.065 0.062 0.059 0.056 0.057 0.039 0.043 0.059Threshold Small                                                                                LargeTable 2 Comparison of Gap values for several measures(ratio: 20%)CoverageMeasure1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 AverageR-1 0.129 0.104 0.102 0.976 0.090 0.089 0.089 0.083 0.082 0.096R-2 0.132 0.115 0.107 0.109 0.096 0.093 0.079 0.081 0.082 0.099R-3 0.132 0.115 0.116 0.111 0.102 0.092 0.080 0.078 0.079 0.101R-4 0.134 0.121 0.121 0.112 0.103 0.090 0.080 0.080 0.078 0.102R-L 0.140 0.135 0.134 0.125 0.117 0.110 0.105 0.769 0.060 0.111R-S 0.130 0.119 0.113 0.106 0.098 0.099 0.089 0.089 0.087 0.103R-S4 0.130 0.114 0.109 0.105 0.102 0.092 0.085 0.088 0.085 0.101R-S9 0.130 0.119 0.113 0.105 0.095 0.097 0.095 0.085 0.084 0.103R-SU 0.130 0.118 0.109 0.109 0.097 0.098 0.088 0.089 0.079 0.102R-SU4 0.130 0.111 0.107 0.106 0.100 0.090 0.086 0.084 0.087 0.100R-SU9 0.130 0.116 0.108 0.105 0.096 0.090 0.085 0.085 0.082 0.099Cosine 0.128 0.106 0.102 0.094 0.091 0.090 0.079 0.080 0.057 0.092Threshold Small                                                                                LargeExp-2: Comparison of Kazawa?s method withother automatic methods (Point 2)In Exp-1, cosine distance outperformed the other11 measures.
We therefore used cosine distancein Kazawa?s method in Exp-2.
We rankedsummaries by Kazawa?s method, ROUGE andcosine distance, calculated using Precision.The results of the evaluation by Precision forsummarization ratios of 40% and 20% are shownin Figures 1 and 2, respectively.
We plotted thePrecision value of Kazawa?s method by changingthe threshold value from 0 to 1 at 0.05 intervals.We also plotted the Precision values of ROUGE-2 as dotted lines.
ROUGE-2 was superior to theother 11 measures in terms of Ranking.
The Xand Y axes in Figures 1 and 2 show the thresholdvalue of Kazawa?s method and the Precisionvalues, respectively.
From the result shown inFigure 1, we found that Kazawa?s methodoutperformed ROUGE-2, when the thresholdvalue was greater than 0.968.
The Coveragevalue of this point was 0.203.
In Figure 2, thePrecision curve of Kazawa?s method crossed thedotted line at a threshold value of 0.890.
TheCoverage value of this point was 0.405.To improve these Coverage values, we need toprepare more summaries and their manualevaluation results, because the Coverage iscritically dependent on the number and variety ofpooled summaries.
This is exactly the first pointin Section 3.1, which we do not address in thispaper.
We will investigate this point as the nextstep in our future work.60800.20.40.60.810 0.2 0.4 0.6 0.8 1threshold valueprecisionKazawa's method R-2Figure 1 Comparison of Kazawa?s method andROUGE-2 (ratio: 40%)00.20.40.60.810 0.2 0.4 0.6 0.8 1threshold valueprecisionKazawa's method R-2Figure 2 Comparison of Kazawa?s method andROUGE-2 (ratio: 20%)Exp-3: An experiment for Point 3 based onYasuda?s methodFor Point 2 in Section 3.2, we also examinedYasuda?s method.
The experimental result byGap is shown in Table 3.
When the ratio is 20%,ROUGE-SU4 is the best.
The N-gram and theskip-bigram are both useful when thesummarization ratio is low.For Point 4, we compared the result byYasuda?s method (Table 3) with that ofKazawa?s method (in Tables 1 and 2).
Yasuda?smethod could accurately estimate manual scores.In particular, the Gap values of 0.023 byROUGE-2 and by ROUGE-3 are smaller thanthose produced by Kazawa?s method with athreshold value of 0.9 (Tables 1 and 2).
Thisindicates that regression analysis used inYasuda?s method is superior to that used inKazawa?s method.Table 3 Gap between the manual method andYasuda?s methodRatio20% 40%AverageCosine 0.037 0.031 0.035R-1 0.033 0.022 0.028R-2 0.028 0.023 0.025R-3 0.028 0.023 0.025R-4 0.036 0.024 0.030R-L 0.040 0.038 0.039R-S(?)
0.051 0.060 0.055R-S4 0.025 0.040 0.033R-S9 0.042 0.052 0.047R-SU(?)
0.027 0.055 0.041R-SU4 0.022 0.037 0.029R-SU9 0.023 0.048 0.036Exp-4: Comparison of Yasuda?s method withother automatic methodsWe also evaluated Yasuda?s method bycomparison with other automatic methods interms of Ranking.
We evaluated 10 systems byYasuda?s method with ROUGE-3, whichproduced the best results in Exp-3.
We alsoevaluated the systems by ROUGE and cosinedistance, and compared the results.
The resultsare shown in Table 4.Table 4 Comparison between Yasuda?s method andautomatic methodsRatio20% 40%AverageYasuda 0.867 0.844 0.856Cosine 0.844 0.800 0.822R-1 0.822 0.778 0.800R-2 0.844 0.800 0.822R-3 0.822 0.800 0.811R-4 0.822 0.844 0.833R-L 0.822 0.800 0.811R-S(?)
0.667 0.689 0.678R-S4 0.800 0.756 0.778R-S9 0.733 0.689 0.711R-SU(?)
0.711 0.711 0.711R-SU4 0.800 0.822 0.811R-SU9 0.756 0.711 0.733As can be seen from Table 4, Yasuda?s methodproduced the best results for the ratios of 20%and 40%.
Of the automatic methods compared,ROUGE-4 was the best.609As evaluation scores by Yasuda?s methodwere calculated based on ROUGE-3, there wereno striking differences between Yasuda?s methodand the others except for the integration processof evaluation scores for each summary.
Yasuda?smethod uses a regression analysis, whereas theother methods average the scores for eachsummary.
Yasuda?s method using ROUGE-3outperformed the original ROUGE-3 for bothratios, 20% and 40%.5 ConclusionsWe have investigated an automatic method thatuses several evaluation results from a manualmethod based on Kazawa?s and Yasuda?smethods.
From the experimental results based onKazawa?s method, we found that limiting thenumber of pooled summaries could producebetter results than using all the pooled summaries.However, the number of summaries that can beevaluated by this method was limited.
Toimprove the Coverage of Kazawa?s method,more summaries and their evaluation results arerequired, because the Coverage is criticallydependent on the number and variety of pooledsummaries.We also investigated an automatic methodbased on Yasuda?s method and found that themethod using ROUGE-2 and -3 could accuratelyestimate manual scores, and could outperformKazawa?s method and the other automaticmethods tested.
From these results, we canconclude that the automatic method performedthe best when ROUGE-2 or 3 is used as asimilarity measure, and a regression analysis isused for combining manual method.ReferencesRobert L. Donaway, Kevin W. Drummey and LauraA.
Mather.
2000.
A Comparison of RankingsProduced by Summarization Evaluation Measures.Proceedings of the ANLP/NAACL 2000Workshop on Automatic Summarization: 69?78.Takahiro Fukushima and Manabu Okumura.
2001.Text Summarization Challenge/TextSummarization Evaluation at NTCIR Workshop2.Proceedings of the Second NTCIR Workshop onResearch in Chinese and Japanese Text Retrievaland Text Summarization: 45?51.Takahiro Fukushima, Manabu Okumura andHidetsugu Nanba.
2002.
Text SummarizationChallenge 2/Text Summarization Evaluation atNTCIR Workshop3.
Working Notes of the 3rdNTCIR Workshop Meeting, PART V: 1?7.Tsutomu Hirao, Manabu Okumura, and HidekiIsozaki.
2005.
Kernel-based Approach forAutomatic Evaluation of Natural LanguageGeneration Technologies: Application toAutomatic Summarization.
Proceedings of HLT-EMNLP 2005: 145?152.Chiori Hori, Takaaki Hori, and Sadaoki Furui.
2003.Evaluation Methods for Automatic SpeechSummarization.
Proceedings of Eurospeech 2003:2825?2828.Hideto Kazawa, Thomas Arrigan, Tsutomu Hirao andEisaku Maeda.
2003.
An Automatic EvaluationMethod of Machine-Generated Extracts.
IPSJ SIGTechnical Reports, 2003-NL-158: 25?30.
(inJapanese).Chin-Yew Lin and Eduard Hovy.
2003.
AutomaticEvaluation of Summaries Using N-gram Co-Occurrence Statistics.
Proceedings of the HumanLanguage Technology Conference 2003: 71?78.Chin-Yew Lin.
2004.
ROUGE: A Package forAutomatic Evaluation of Summaries.
Proceedingsof the ACL-04 Workshop ?Text SummarizationBranches Out?
: 74?81.Hidetsugu Nanba and Manabu Okumura.
2004.Comparison of Some Automatic and ManualMethods for Summary Evaluation Based on theText Summarization Challenge 2.
Proceedings ofthe Fourth International Conference on LanguageResources and Evaluation: 1029?1032.Ani Nenkova and Rebecca Passonneau, 2004.Evaluating Content Selection in Summarization:The Pyramid Method.
Proceedings of HLT-NAACL2004: 145?152.Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu.
2001.
BLEU: A Method forAutomatic Evaluation of Machine Translation.IBM Research Report, RC22176 (W0109-022).Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang.1992.
A New Quantitative Quality Measure forMachine Translation Systems.
Proceedings of the14th International Conference on ComputationalLinguistics: 433?439.Simone Teufel and Hans van Halteren.
2004.Evaluating Information Content by FactoidAnalysis: Human Annotation and Stability.Proceedings of EMNLP 2004: 419?426.Kenji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa,Seiichi Yamamoto and Masuzo Yanagida.
2003.Applications of Automatic Evaluation Methods toMeasuring a Capability of Speech TranslationSystem.
Proceedings of the Tenth Conference ofthe European Chapter of the Association forComputational Linguistics: 371?378.610
