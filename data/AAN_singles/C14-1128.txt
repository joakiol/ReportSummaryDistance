Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1350?1359, Dublin, Ireland, August 23-29 2014.Latent Dynamic Model with Category Transition Constraintfor Opinion ClassificationTakeshi S. KobayakawaNHK / 1-10-11 Kinuta, Setagaya-ku, Tokyo 157-8510, JAPANkobayakawa.t-ko@nhk.or.jpAbstractLatent models for opinion classification are studied.
Training a probabilistic model with a numberof latent variables is found unstable in some cases; thus this paper presents how to construct astable model for opinion classification by constraining classification transitions.
The baselinemodel is a CRF classification model with plural latent variables, dynamically constructed fromthe dependency parsed tree.
The aim of the baseline model is to have each latent variable conveya partial sentiment of the input sentence which is not explicitly given in the training data, andthe complete sentiment of the sentence is computed by summing up such partial sentiment wherethose latent variables hold.
Since such a conventional model has many degeneracies in principle,a model with a category transition constraint is proposed, which is expressed by a novel penaltyterm in the objective function for training the model.
The constraint is such that the sentimentof a partial sentence more likely propagates to the same sentiment of the complete sentence,rather than to another sentiment.
The effectiveness and the robustness of the proposed model areconfirmed by the experiments on binary as well as multi-class opinion classification task.1 IntroductionOpinion classification is a task to classify sentences into given categories, according to sentiment, evalu-ation, or some opinion-related points of view.
A practical implementation of opinion classification wouldbe very useful for managing customer relationships at contact centers, etc.
The classification problemmay be binary or sometimes multi-class.One of the simplest modeling process is to use explicit bag features, such as word surfaces, polarityinformation from the sentiment dictionary, etc.
Thanks to the good behavior of the Maximum Entropyor the Conditional Random Field (CRF) model, the maximum likelihood training is straight-forward,because the local optimum is always the global optimum.A challenge is to introduce into the model latent variables, which are not explicitly observable.
Theimplicit modeling here is supposed to express ambiguities of natural language; the partial sentiment ofthe sentence is not determined until the end of the sentence.
This paper presents in detail a probabilisticmodel with latent variables.
The baseline model is a CRF model which is constructed dynamicallyaccording to the dependency parsed tree and which contains latent variables on the nodes that correspondto the chunked expressions (Nakagawa et al., 2010).
The latent variables in the model are expected toconvey a partial sentiment of the sentence, such as the sentiment of the dependency-parsed-subtree itself,which is not explicitly observable.Although this idea is attractive, it actually suffers from numerical instability.
Our aim here is to finda way to deal with this problem.
We tried using a global optimizer and investigated the behavior of themodel, to ensure that this lack of stability comes from the degeneracy of the model.Our contribution to remedy this problem is as follows: We propose a model with a penalty on categorytransitions and compare several optimizers to train the model.
We also confirm the stability of the modelThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1350by applying it to the multi-class classification problem.
We also investigate the origin of the degeneracyand how regularizer works for this type of classification model, to see the robustness of the model.2 Related WorkStudies on opinion classification have more than a decade of history, including the pioneer work (Turney,2002; Pang et al., 2002), followed by (Takamura et al., 2005; Brun, 2012).
Our baseline model treatsthe sentiment of a partial sentence (Nakagawa et al., 2010); CRF with latent variables are constructeddynamically by the dependency structure tree of the input sentence.CRF model was first used in sequence labeling tasks (Lafferty, 2001).
The model does not sufferfrom the label-bias problem as does the Maximum Entropy model, and its parameter estimation is wellbehaved with the help of a convex loss function.
However, the convexity of the loss function of the CRFmodel does not hold anymore when there are unobserved data or latent variables (Sutton and McCallum,2007).Latent variables were first used with CRF for the purpose of noun coreference (McCallum and Well-ner, 2005), and object recognition (Quattoni et al., 2005).
Latent variables have been used to constructmeaning representations in a process called grounded language acquisition (Liang et al., 2009).
An-other approach with hidden variables has been used an recursive auto-encoder to reduce the reliance onsentiment dictionaries (Socher et al., 2011).Machine learning, used for training such models, is largely based on the concept of numerical opti-mization.
A general discussion of convex optimization can be found in (Boyd and Vandenberghe, 2004);it can be proven that the log-sum-exp type of a convex function is still a convex function.
This isthe situation with the CRF model without latent variables.
Since convexity holds, the best solution tothe problem would be a numerical local optimization.
A general discussion of local optimization can befound in the textbook (Nocedal and Wright, 2006), who is one of the authors of the Limited memoryBroyden-Fletcher-Goldfarb-Shanno (L-BFGS) optimizer program.
Many NLP application programs useprobabilistic models, including this optimizer or its derived work.
As the long name of the BFGS algo-rithm shows, the state-of-the-art local optimizer has a long history.
Another textbook (Press et al., 2007)covers a global optimization algorithm, Simulated Annealing, originating from (Kirkpatrick et al., 1983).The main idea comes from the statistical physics of equilibrium; when a material is warm, its energy isdistributed in excited states, whereas when it is cooled, it is very probable that the system will end in theground state, which corresponds to the global minimum point of the energy.Previous studies on CRF with latent variables were trained either by setting the initial parametersrandomly (Nakagawa et al., 2010), or by online training (McCallum and Wellner, 2005).
As far as theauthor knows, this is the first study to impose a penalty between latent variables in CRF model, and tocompare several optimization algorithms.3 The ModelThe model studied is a CRF model, which has set of conditional probabilities whose log is a linearcombination of model parameters associated with features given by (Lafferty, 2001):log p?
(y|x) ?
?v?V,k?kgk(v,y|v,x) +?e?E,k?kfk(e,y|e,x), (1)where x is an input vector and y is an output label sequence, and y|vand y|eare the vertex v and theedge e related to the component of y, respectively.
The model parameter vector ?
is estimated from thetraining data, whose components are the sum of the two sets: a vertex feature set (?1, ?2, .
.
. )
and anedge feature set (?1, ?2, .
.
.
).
The complete set of features are supposed to be enumerated and fixed, sothat each feature can be indicated by an index k in a rather relaxed way.
gkand fkare so-called featurefunctions, to indicate whether the feature in the argument appears in the input, or not.Nakagawa et al.
(2010) proposed a CRF model with latent variables that uses a dependency parsedstructure, where the latent variables are expected to convey the sentiment classifying the part underneaththe parsed tree.
We choose this model as a baseline and continue the same kind of treatment of the latent1351variables.
Section 3.1 and 3.2 briefly review this baseline model, then the proposed model follows inSection 3.3.3.1 Sentence Classification Model with Latent VariablesTo classify sentence x into a given set of a class, say C , the classification problem is formulated as:argmaxs0?Cp?
(s0|x), (2)where s0is a class label of the complete sentence and x is a given sentence such thatp?
(s0|x) =?sp?
(s0, s|x), (3)where s is the latent variables to be summed up, and ?
is the set of all parameters in the model.
Eachelement of s takes one of these class labels, corresponding to the partial sentiment of the sentence, and isto be summed up to construct the complete sentiment of the sentence.
A partial sentiment of a sentenceis sometimes ambiguous, so it is treated as such an unobserved variable.
The model parameters ?
areestimated from the training data; the sentiment label is only available for a whole sentence, not for a partof the sentence.3.2 Dependency Structure as a Graphical ModelThe given sentence x is parsed into a dependency structure of phrase chunks:xDependency Parsing????????????
G(x) = {V (x), E(x)}, (4)where V (x) are the set of chunks and E(x) is the set of dependency arcs.
The dependency structure isregarded as a graphical model, an example of which is shown in Figure 1.
The words are chunked upinto phrases, and the dependencies between those chunks are determined by dependency parsing.
Eachchunk corresponds to a variable that is supposed to convey a sentiment.Figure 1: Correspondence between dependencystructure and graphical representation of themodelFigure 2: Features attached to vertices and edgesVertex features Edge featuresword surface unigrams word surface unigrams of the parent vertexsucceeding word surface bigrams word surface unigrams of the child vertexsentiment information from a dictionary sentiment information from a dictionarynegation expression from a dictionary negation expression from a dictionarymeaning label of functional expression meaning label of functional expression of the parent vertexmeaning label of functional expression of the child vertexTable 1: Summary of features adopted in the model1352Every feature belongs to one of two types: vertex features or edge features.
Vertex features are thoselocally related to a vertex, and edge features are those that affect both sides of vertices of the dependency.The specific features adopted in the model are summarized in Table 1.A symbol ?
as a notation for model parameters related to vertex features.
It has two indices since theyare related to a vertex feature and a classification category.
As for the vertex feature fvrelated part, thelog probability that the vertex v has the category s is:log pfv(s) = ?fv,s.
(5)A symbol ?
as a notation for model parameters related to edge features.
It has three indices since theyare related to an edge feature and classification categories of both sides of vertices of the edge.As for the edge feature ferelated part, the log conditional probability that the target vertex takes thecategory s2is:log pfe(s2|s1) = ?fe,s2,s1, (6)given the category of the source vertex is s1.Multiple features can be attached either on a vertex or on a edge.
So, the whole vertex or edgeprobability is constructed as follows:log pv(s) =?fv?F(vertex)(v)log pfv(s) =?fv?F(vertex)(v)?fv,s, (7)log pe(s2|s1) =?fe?F(edge)(e)log pfe(s2|s1) =?fe?F(edge)(e)?fe,s2,s1, (8)where F(vertex)(v) is a set of features attached to a vertex v, and where F(edge)(e) is those attached to anedge e.Finally, the probability of a given sentence x is constructed as a log-linear model:log p(s0, s|x) =?v?V (x)log pv(s(v)) +?e?E(x)log pe(s(target(e))|s(source(e))), (9)where the set of vertices and edges are dynamically constructed from the dependency parsed tree of thesentence, i.e.
eq.
(4), and the notation source(e) and target(e) are the source and target vertex of the edgee, respectively (Figure 2.
)Care is necessary when assigning values to the latent variables in eq.
(9).
Because each latent variablewhich is assigned on a vertex and the connecting edges, share the same values, the latent variables mustbe summed up in such way; The summations can be done efficiently by using dynamic programming(a.k.a.
the factor graph in graphical model terminology.)
The tables of probabilities are constructed foreach vertex, and the tree is constructed in a bottom up manner.The sets1of all the vertex and edge features appearing in the training data D are denoted as V(D)andE(D)respectively:V(D)=?x?D?v?V (x)F(vertex)(v), E(D)=?x?D?e?E(x)F(edge)(e), (10)so that the complete set of parameters is:?
= {?v,c|v ?
V(D), c ?
C}+ {?e,c1,c2|e ?
E(D), c1, c2?
C},where the number of parameters is V(D)?
C + E(D)?
C ?
C .1Note that the following equations up to eq.
(11) are in the terminology of set theory; the addition is done with the eliminationof duplicated elements, and a product means a direct product, and a n-th power is an abbreviation of n direct products of theset.1353The log likelihood of the training data is given byL(?
;D) =?x?Dlog??
?s?C|V (x)|p?
(s0, s|x)?
?, (11)where |V (x)| is the number of vertices in a given sentence x, and where s0is the correct classificationlabel for x, and where s is the set of latent variables whose number is as many as |V (x)|.3.3 Category Transition PenaltyWe found that the classification accuracy of the trained model remained low, because little number ofparameters for edge features moved away from initial non-contributing values during the training.
Thefollowing form of regularizer2is used for the training:R(?)
= Cregularizer??????1n?
1????2.
(12)The constant Cregularizeris the strength of the regularizer, and no matter how strong, the classificationaccuracy did not improve in our preliminary experiments.
This phenomenon seems to be because of thedegeneracies the model has in principle.Degeneracy is a notion to explain the same probabilities of different configurations.
If the two differentconfigurations are preferably distinguished, an asymmetric treatment of them is required.In order to avoid such extra degeneracies, we introduce a novel constraint between latent variablesexpressed by the following penalty term:P(?)
= Cpenalty?fe?E(D)??
?s1=s2(log pfe(s2|s1)?
logCsame)2+?s16=s2(log pfe(s2|s1)?
logCdifferent)2?
?,(13)to satisfyCsame+ (n?
1)Cdifferent= 1, (14)where Cpenaltyis the weight of this penalty, and where Csameand Cdifferentare constant probabilities forthe following two cases; that is the categories connected to the other side of the edge should be the sameor different, respectively.
This term is incorporated into the objective function for training the model, toform a soft constraint that diminishes the change in the classification category.3.4 Model TrainingThe maximum likelihood training of a probabilistic model is a constrained optimization problem.
Aprobabilistic interpretation is possible if and only if 1) all probabilities are non-negative, and 2) the sumof the probabilities are one (or renormalizable to one).Using log probabilities almost automatically satisfies the first condition: Real number in log spacecorresponds to positive number in anti-log space, so that the only consideration needed is zero probability(which corresponds to negative infinity in the log space.)
In the experiments, overflow is checked thatnone occurred in the final results.
The model parameter to express zero probability could be finite butreasonably small, instead of zero.As for the second condition, instead of the strict constraint, we adopted a quadratic penalty in logspace:C(?)
= Cprob???fv?V(D)(log?s?Cpfv(s))2+?fe?E(D)?s1?C??log?s2?Cpfe(s2|s1)??2?
?, (15)2The offset1n?1 in the regularizer is so as to avoid singularity around zero probabilities in real space, which causes negativeinfinity in log space calculation.1354where the strictly normalized probabilities lead to zero penalty; otherwise, a quadratic penalty is givenaccording to the amount away from strictly normalized probabilities.
The weight of the penalty Cprobischosen to be heavy enough for the sum of the model probability to be adequately normalized.Finally, we adopted the probability calculation in the log space, with the quadratic penalty for normal-ization during the training.
The objective function for training the model is:O(?
;D) = L(?;D)?
R(?)?
P(?)?
C(?).
(16)4 ExperimentsExperiments are conducted to evaluate the effect of the proposed penalty term expressed by eq.
(13).4.1 Test SetsTwo kinds of test set in Japanese were used: Opinions in the Kyoto University and NTT Blog (KNB)Corpus3and Comments on an TV cultural program.
Both sets are balanced in the numbers sentencecategories.
The characteristics of each test set are shown in Table 2.The first test set, the KNB Corpus, is a collection of opinion sentences about Kyoto sightseeing spots,cellular phones, gourmet food, and sports.
The sentences are categorized in terms of many aspects, andwe used the sentences labeled with Evaluation+ or Evaluation?.
?Evaluation?
is a category of subjectivebut non-emotional opinions.The second test set is used for non-binary classification.
To make this set, viewers were asked tocomment (in Japanese natural language) on a certain TV program.
The comments are classified intocategories, i.e.
evaluations, impressions, requirements and questions.
The following four categories areused: positive and negative evaluations, and impressions of what the viewers learned from the program,and what they thought after watching the program.Name of Test Set # of Sentences CategoriesOpinions in KNB Corpus 328 2(Evaluation +/?
)Comments on TV cultural program 432 4(positive/negative evaluations,what viewers learned/think)Table 2: Characteristics of Test Set4.2 NLP ResourcesThe input sentence was processed by a morphological analyzer to split it into words, since Japanese isan agglutinative language.
The words were then chunked and the dependencies between those chunkswere determined.
Functional multi-word expressions were also detected by the analyzer we developed.We used a dictionary of sentiment expressions, and one of negation expressions, both of which weredistributed with the KNB Corpus.We did not prepare any special sentiment dictionary for the second test set because preparing such adictionary is too costly.
Furthermore, robustness can be estimated without a domain-adapted dictionary.The parameter values for training the models were tuned for the first test set, and the tuned values wereused without any extra tuning for the second test set.
In this situation, the first test set can be regarded asthe development test set, and the second as the evaluation test set.4.3 Latent Dynamic Model with Category Transition ConstraintExperiments on classifying opinions using the KNB Corpus are shown in Table 3.
The rightmost columnis a trivial baseline, where the classification category is decided by the majority occurrence of sentiment3The corpus is publicly available from http://nlp.ist.i.kyoto-u.ac.jp/kuntt/\#ga739fe2, and the de-tails of corpus are explained at http://alaginrc.nict.go.jp/opinion/index_e.html.
We excluded short sen-tences, made up of a fewwords, that were exclamations rather than natural complete sentences.
They do not form tree structures,which are not aimed to this study.
Accuracy of experiments conducted below are different from that by (Nakagawa et al., 2010)in that only the subset of the test set that satisfy the condition is used.1355words in the sentence.
This method needs no training, so only one figure is indicated.
The other columnsare figures for trained CRF; closed tests are the case where all the training data is used for the evaluationas well, shown in the upper row, while 10-folds open test are the case is 1:9 split of data for evaluationand training and the evaluation is done cyclically 10 times, shown in the lower row.
The leftmost columnis CRF without latent variables.
The 2nd left is a model with latent variables but without penalty, whichis the model in (Nakagawa et al., 2010).
The 3rd left is a model with latent variables that has a categorytransition penalty, which is the proposed model.
The proposed model performs the best accuracy amongall the models.Experiments on classifying opinions using the Comments on the TV cultural Program are shown inTable 4.
Majority voting is not possible because a suitable dictionary that has the same class polarity asthis classification problem does not exist.Trained CRF (no training)without latent variables with latent variables Majoritynon-penalized penalized Votingclosed test 95.12 95.73 95.7310-fold open test 61.59 63.72 65.55 64.79Table 3: Effect of Latent Variables and Penalized Model (Opinions in KNB Corpus)Trained CRF (no training)without latent variables with latent variables Majoritynon-penalized penalized Votingclosed test 99.54 99.77 99.3110-fold open test 60.42 60.42 64.81 N/ATable 4: Effect of Penalized Latent Dynamic Model (Comments on TV cultural program)4.4 Comparison of OptimizersThree optimization algorithms for model training were compared, two of which are local optimizers(BFGS and Steepest Descent), and one of which is a global optimizer (Simulated Annealing).BFGS was used as batch training where all of the training data were used during the training iteration.Two types of initial parameter configurations were tried for BFGS; initial parameters have the same fixedvalues, or were chosen randomly.
Steepest Descent (SD) was used as online training where some portion(i.e.
chunk) of the training data were used during an iteration.
Two types of chunk selection scheme weretried for Steepest Descent; chunks were fixed during the training, or chunks were randomly shuffled afterevery complete loop of the whole training data.Simulated Annealing was adopted as a global optimizer.
We implemented feature level granularity foracceptance or rejection: Every parameter corresponding to a feature was randomly moved, and decidedprobabilistically whether or not to accept according to the Boltzmann distribution under scheduled cool-ing down.
Although all of these methods utilize random variables, they were used in different ways.
Theaccuracy ranges of ten trials are shown in Table 5 and 6.The results show that the proposed model trained by a global optimizer outperforms models trained bythe other local optimizers (Steepest Descent and BFGS); The penalty in the proposed model works wellbecause the degeneracies in the penalized model seem to decrease, and the computation is noteworthystable.5 DiscussionThe degeneracy in the model is illustrated in Figure 3 and 4.Firstly, convergence for penalized model is quicker, as shown in Figure 3; The horizontal axis is thenumber of iterations, and the left vertical axis is the acceptance ratio, where the lower is well converged.1356Batch Training (BFGS) Online Training (SD) Simulatedparameter initialization chunked data selection Annealingrandom fixed shuffled fixedclosed test 89.33-82.32 87.80 76.83-72.56 76.83 95.73-95.7310-fold open test 62.80-58.54 59.15 66.46-65.55 65.55 65.55-64.63Table 5: Comparison of Optimizers (Opinions in KNB Corpus)Batch Training (BFGS) Online Training (SD) Simulatedparameter initialization chunked data selection Annealingrandom fixed shuffled fixedclosed test 85.65-35.65 88.19 47.69-45.37 45.60 99.31-99.3110-fold open test 58.56-34.72 59.95 38.89-36.81 37.50 64.81-64.58Table 6: Comparison of Optimizers (Comments on TV cultural program)The acceptance ratio remains high for the non-penalized model, while the penalized model quickly de-scends.
The dash line indicates the log likelihood of the training data, for penalized and non-penalizedmodels, which are almost identical.Secondly, in order to split degeneracy, only a small penalty is adequate, as illustrated in Figure 4; howless the penalty is, as long as it exists, the improvement remains.
The horizontal axis is the strength ofthe proposed penalty, Cpenaltyin eq.(13).
The vertical axis is the classification accuracy.
The dash line isa line fit for the accuracy by non-zero penalty.
In general, such an extra constraint term for the originalmodel may change the model itself, so, the less it is the better.
That is the reason for decreasing accuracywhen large penalty is used.
The significant jump in the accuracy between zero and non-zero penaltystrongly suggests the existence of degeneracies in the original model: Infinitesimally small penalty canlead to break those degeneracies.Local optima usually do not matter when regularizers are used in training.
However, according to ourexperiments in this type of models, the conventional regularizers are not able to avoid such local optimano matter how strong they are.
The reason the introduced penalty works well for this model is consideredthat the term works for excessive latent variables, which are not controlled by the ordinary regularizers.The ordinary regularizers only works for excessive number of explicitly observed parameters (i.e.
fea-tures).
If only a few latent variables are used, such a penalty is not necessary, just as a regularizer is notnecessary for a small number of features.
When the model is constructed dynamically and the number oflatent variables grows, there appear a number of latent variables having excessive freedom, which needto be controlled.1020304050607080901000  100  200  300  400  500  600  700  800  900  1000-7000-6500-6000-5500-5000-4500-4000-3500penalized modelnon-penalized modelacceptanceratio(%)number of iterationsloglikelihoodL(?
;D)Figure 3: Transition of the Acceptance Ratio0 1e-5 1e-3 0.1 1063.56464.56565.566finite penaltyzero penaltyAccuracy(%)Category Transit Penalty CpenaltyFigure 4: Effect of Weak Penalty13576 ConclusionA latent dynamic model with a category transition constraint is proposed for opinion classification task.The constraint is such that the sentiment of a partial sentence tends to propagate toward the completesentiment of the whole sentence, which is realized, in the objective function, by our novel term thatpenalizes, diminishing the change in the classification category.According to our experiments, the penalized latent dynamic model outperforms the conventionalmodel, not only in binary but also in multi-class opinion classification.The comparison of optimizers strongly suggests that the degeneracies in the conventional model dete-riorate the performance, and the proposed model solves such a defect.
The numerical stability of trainingthe proposed model is also confirmed.AcknowledgmentsThe author would like to thank to Jun?ichi Tsujii, Akiko Aizawa, Yusuke Miyao, Takuya Matsuzaki,Hideki Tanaka and Tadashi Kumano.
The author would also like to express the greatest gratitude for thediscussions with Akio Kobayashi, Hiroyuki Segi, Tsuneo Hirano, Clara K. Hirano, and Mariko Hirano.Mariko Hirano also helped some of the experiments.ReferencesStephen Boyd and Lieven Vandenberghe.
2004.
Convex Optimization.
Cambridge University Press.Caroline Brun.
2012.
Learning opinionated patterns for contextual opinion detection.
In Proceedings of COLING2012: Posters, pages 165?174, Mumbai, India, December.
The COLING 2012 Organizing Committee.S.
Kirkpatrick, C. D. Gelatt, and M. P. Vecchi.
1983.
Optimization by simulated annealing.
Science,220(4598):671?680.John Lafferty.
2001.
Conditional random fields: Probabilistic models for segmenting and labeling sequence data.In International Conference on Machine Learning (ICML), pages 282?289.
Morgan Kaufmann.Percy Liang, Michael Jordan, and Dan Klein.
2009.
Learning semantic correspondences with less supervision.
InProceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Con-ference on Natural Language Processing of the AFNLP, pages 91?99, Suntec, Singapore, August.
Associationfor Computational Linguistics.Andrew McCallum and Ben Wellner.
2005.
Conditional models of identity uncertainty with application to nouncoreference.
In Lawrence K. Saul, Yair Weiss, and Le?on Bottou, editors, Advances in Neural InformationProcessing Systems 17, pages 905?912.
MIT Press, Cambridge, MA.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010.
Dependency tree-based sentiment classificationusing crfs with hidden variables.
In Human Language Technologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics, pages 786?794, Los Angeles, California,June.
Association for Computational Linguistics.Jorge Nocedal and Stephen J. Wright.
2006.
Numerical Optimization.
Springer Verlag, 2nd edition.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proceedings of the 2002 Conference on Empirical Methods in Natural LanguageProcessing, pages 79?86.
Association for Computational Linguistics, July.William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery.
2007.
Numerical Recipes.Cambridge University Press, 3rd edition.Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2005.
Conditional random fields for object recognition.
InLawrence K. Saul, Yair Weiss, and Le?on Bottou, editors, Advances in Neural Information Processing Systems17, pages 1097?1104.
MIT Press, Cambridge, MA.Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning.
2011.
Semi-supervised recursive autoencoders for predicting sentiment distributions.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Processing, pages 151?161, Edinburgh, Scotland, UK., July.
Asso-ciation for Computational Linguistics.1358Charles Sutton and Andrew McCallum, 2007.
Introduction to Statistical Relational Learning, chapter 4 An Intro-duction to Conditional Random Fields.
The MIT Press.
also available on e-Print archive: arXiv:1011.4088v1.Hiroya Takamura, Takashi Inui, andManabuOkumura.
2005.
Extracting semantic orientations of words using spinmodel.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),pages 133?140, Ann Arbor, Michigan, June.
Association for Computational Linguistics.Peter Turney.
2002.
Thumbs up or thumbs down?
semantic orientation applied to unsupervised classificationof reviews.
In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages417?424, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.1359
