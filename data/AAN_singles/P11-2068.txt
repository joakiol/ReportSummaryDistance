Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 389?394,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsImproving On-line Handwritten Recognition using Translation Modelsin Multimodal Interactive Machine TranslationVicent Alabau, Alberto Sanchis, Francisco CasacubertaInstitut Tecnolo`gic d?Informa`ticaUniversitat Polite`cnica de Vale`nciaCam??
de Vera, s/n, Valencia, Spain{valabau,asanchis,fcn}@iti.upv.esAbstractIn interactive machine translation (IMT), a hu-man expert is integrated into the core of a ma-chine translation (MT) system.
The human ex-pert interacts with the IMT system by partiallycorrecting the errors of the system?s output.Then, the system proposes a new solution.This process is repeated until the output meetsthe desired quality.
In this scenario, the in-teraction is typically performed using the key-board and the mouse.
In this work, we presentan alternative modality to interact within IMTsystems by writing on a tactile display or us-ing an electronic pen.
An on-line handwrit-ten text recognition (HTR) system has beenspecifically designed to operate with IMT sys-tems.
Our HTR system improves previous ap-proaches in two main aspects.
First, HTR de-coding is tightly coupled with the IMT sys-tem.
Second, the language models proposedare context aware, in the sense that they takeinto account the partial corrections and thesource sentence by using a combination of n-grams and word-based IBM models.
The pro-posed system achieves an important boost inperformance with respect to previous work.1 IntroductionAlthough current state-of-the-art machine transla-tion (MT) systems have improved greatly in the lastten years, they are not able to provide the high qual-ity results that are needed for industrial and busi-ness purposes.
For that reason, a new interactiveparadigm has emerged recently.
In interactive ma-chine translation (IMT) (Foster et al, 1998; Bar-rachina et al, 2009; Koehn and Haddow, 2009) thesystem goal is not to produce ?perfect?
translationsin a completely automatic way, but to help the userbuild the translation with the least effort possible.A typical approach to IMT is shown in Fig.
1.
Asource sentence f is given to the IMT system.
First,the system outputs a translation hypothesis e?s in thetarget language, which would correspond to the out-put of fully automated MT system.
Next, the useranalyses the source sentence and the decoded hy-pothesis, and validates the longest error-free prefixep finding the first error.
The user, then, corrects theerroneous word by typing some keystrokes ?, andsends them along with ep to the system, as a new val-idated prefix ep, ?.
With that information, the sys-tem is able to produce a new, hopefully improved,suffix e?s that continues the previous validated pre-fix.
This process is repeated until the user agreeswith the quality of the resulting translation.systemuserf ?esep ,?Figure 1: Diagram of a typical approach to IMTThe usual way in which the user introduces thecorrections ?
is by means of the keyboard.
How-ever, other interaction modalities are also possible.For example, the use of speech interaction was stud-ied in (Vidal et al, 2006).
In that work, several sce-389narios were proposed, where the user was expectedto speak aloud parts of the current hypothesis andpossibly one or more corrections.
On-line HTR forinteractive systems was first explored for interactivetranscription of text images (Toselli et al, 2010).Later, we proposed an adaptation to IMT in (Alabauet al, 2010).
For both cases, the decoding of theon-line handwritten text is performed independentlyas a previous step of the suffix es decoding.
To ourknowledge, (Alabau et al, 2010) has been the firstand sole approach to the use of on-line handwritingin IMT so far.
However, that work did not exploitthe specific particularities of the MT scenario.The novelties of this paper with respect to previ-ous work are summarised in the following items:?
in previous formalisations of the problem, theHTR decoding and the IMT decoding were per-formed in two steps.
Here, a sound statisticalformalisation is presented where both systemsare tightly coupled.?
the use of specific language modelling for on-line HTR decoding that take into account theprevious validated prefix ep, ?, and the sourcesentence f .
A decreasing in error of 2% abso-lute has been achieved with respect to previouswork.?
additionally, a thorough study of the errorscommitted by the HTR subsystem is presented.The remainder of this paper is organised as fol-lows: The statistical framework for multimodal IMTand their alternatives will be studied in Sec.
2.
Sec-tion 3 is devoted to the evaluation of the proposedmodels.
Here, the results will be analysed and com-pared to previous approaches.
Finally, conclusionsand future work will be discussed in Sec.
4.2 Multimodal IMTIn the traditional IMT scenario, the user interactswith the system through a series of corrections intro-duced with the keyboard.
This iterative nature of theprocess is emphasised by the loop in Fig.
1, whichindicates that, for a source sentence to be translated,several interactions between the user and the systemshould be performed.
In each interaction, the systemproduces the most probable suffix e?s that completesthe prefix formed by concatenating the longest cor-rect prefix from the previous hypothesis ep and thekeyboard correction ?.
In addition, the concatena-tion of them, (ep, ?, e?s), must be a translation of f .Statistically, this problem can be formulated ase?s = argmaxesPr(es|ep, ?,f) (1)The multimodal IMT approach differs from Eq.
1in that the user introduces the correction using atouch-screen or an electronic pen, t. Then, Eq.
1can be rewritten ase?s = argmaxesPr(es|ep, t,f) (2)As t is a non-deterministic input (contrarily to ?
),t needs to be decoded in a word d of the vocabu-lary.
Thus, we must marginalise for every possibledecoding:e?s = argmaxes?dPr(es, d|ep, t,f) (3)Furthermore, by applying simple Bayes transfor-mations and making reasonable assumptions,e?s ?
argmaxesmaxdPr(t|d) Pr(d|ep,f)Pr(es|ep, d,f) (4)The first term in Eq.
4 is a morphological modeland it can be approximated with hidden Markovmodels (HMM).
The last term is an IMT modelas described in (Barrachina et al, 2009).
Finally,Pr(d|ep,f) is a constrained language model.
Notethat the language model is conditioned to the longestcorrect prefix, just as a regular language model.
Be-sides, it is also conditioned to the source sentence,since d should result of the translation of it.A typical session of the multimodal IMT is ex-emplified in Fig.
2.
First, the system starts withan empty prefix, so it proposes a full hypothesis.The output would be the same of a fully automatedsystem.
Then, the user corrects the first error, not,by writing on a touch-screen.
The HTR subsys-tem mistakenly recognises in.
Consequently, theuser falls back to the keyboard and types is.
Next,the system proposes a new suffix, in which the firstword, not, has been automatically corrected.
Theuser amends at by writing the word , which is cor-rectly recognised by the HTR subsystem.
Finally, asthe new proposed suffix is correct, the process ends.390SOURCE (f ): si alguna funcio?n no se encuentra disponible en su redTARGET (e): if any feature is not available in your networkITER-0 (ep)ITER-1(e?s) if any feature not is available on your network(ep) if any feature(t) if any feature(d?)
if any feature in(?)
if any feature isITER-2(e?s) if any feature is not available at your network(ep) if any feature is not available(t) if any feature is not available(d?)
if any feature is not available inFINAL(e?s) if any feature is not available in your network(ep ?
e) if any feature is not available in your networkFigure 2: Example of a multimodal IMT session for translating a Spanish sentence f from the Xerox corpus to anEnglish sentence e. If the decoding of the pen strokes d?
is correct, it is displayed in boldface.
On the contrary, if d?
isincorrect, it is shown crossed out.
In this case, the user amends the error with the keyboard ?
(in typewriter).2.1 Decoupled ApproachIn (Alabau et al, 2010) we proposed a decoupledapproach to Eq.
4, where the on-line HTR decod-ing was a separate problem from the IMT problem.From Eq.
4 a two step process can be performed.First, d?
is obtained,d?
?
argmaxdPr(t|d) Pr(d|ep,f) (5)Then, the most likely suffix is obtained as in Eq 1,but taking d?
as the corrected word instead of ?,e?s = argmaxesPr(es|ep, d?,f) (6)Finally, in that work, the terms of Eq.
5 were in-terpolated with a unigram in a log-linear model.2.2 Coupled ApproachThe formulation presented in Eq.
4 can be tackleddirectly to perform a coupled decoding.
The prob-lem resides in how to model the constrained lan-guage model.
A first approach is to drop either theep or f terms from the probability.
If f is dropped,then Pr(d|ep) can be modelled as a regular n-grammodel.
On the other hand, if ep is dropped, but theposition of d in the target sentence i = |ep| + 1 iskept, Pr(d|f , i) can be modelled as a word-basedtranslation model.
Let us introduce a hidden vari-able j that accounts for a position of a word in fwhich is a candidate translation of d. Then,Pr(d|f , i) =|f |?j=1Pr(d, j|f , i) (7)?|f |?j=1Pr(j|f , i)Pr(d|fj) (8)Both probabilities, Pr(j|f , i) and Pr(d|fj), canbe estimated using IBM models (Brown et al,1993).
The first term is an alignment probabilitywhile the second is a word dictionary.
Word dic-tionary probabilities can be directly estimated byIBM1 models.
However, word dictionaries are notsymmetric.
Alternatively, this probability can beestimated using the inverse dictionary to provide asmoothed dictionary,Pr(d|fj) =Pr(d) Pr(fj |d)?d?
Pr(d?)
Pr(fj |d?
)(9)Thus, four word-based translation models havebeen considered: direct IBM1 and IBM2 models,and inverse IBM1-inv and IBM2-inv models withthe inverse dictionary from Eq.
9.However, a more interesting set up than using lan-guage models or translation models alone is to com-bine both models.
Two schemes have been studied.391The most formal under a probabilistic point of viewis a linear interpolation of the models,Pr(d|ep,f) = ?Pr(d|ep) + (1?
?
)Pr(d|f , i)(10)However, a common approach to combine modelsnowadays is log-linear interpolation (Berger et al,1996; Papineni et al, 1998; Och and Ney, 2002),Pr(d|ep,f) =exp (?m ?mhm(d,f , ep))Z(11)?m being a scaling factor for model m, hm the log-probability of each model considered in the log-lineal interpolation and Z a normalisation factor.Finally, to balance the absolute values of the mor-phological model, the constrained language modeland the IMT model, these probabilities are com-bined in a log-linear manner regardless of the lan-guage modelling approach.3 ExperimentsThe Xerox corpus, created on the TT2project (SchulmbergerSema S.A. et al, 2001),was used for these experiments, since it has beenextensively used in the literature to obtain IMTresults.
The simplified English and Spanish versionswere used to estimate the IMT, IBM and languagemodels.
The corpus consists of 56k sentences oftraining and a development and test sets of 1.1ksentences.
Test perplexities for Spanish and Englishare 33 and 48, respectively.For on-line HTR, the on-line handwrittenUNIPEN corpus (Guyon et al, 1994) was used.The morphological models were represented by con-tinuous density left-to-right character HMMs withGaussian mixtures, as in speech recognition (Ra-biner, 1989), but with variable number of states percharacter.
Feature extraction consisted on speedand size normalisation of pen positions and veloc-ities, resulting in a sequence of vectors of six fea-tures (Toselli et al, 2007).The simulation of user interaction was performedin the following way.
First, the publicly availableIMT decoder Thot (Ortiz-Mart?
?nez et al, 2005) 1was used to run an off-line simulation for keyboard-based IMT.
As a result, a list of words the system1http://sourceforge.net/projects/thot/System Spanish Englishdev test dev testindependent HTR (?)
9.6 10.9 7.7 9.6decoupled (?)
9.5 10.8 7.2 9.6best coupled 6.7 8.9 5.5 7.2Table 1: Comparison of the CER with previous systems.In boldface the best system.
(?)
is an independent, con-text unaware system used as baseline.
(?)
is a modelequivalent to (Alabau et al, 2010).failed to predict was obtained.
Supposedly, this isthe list of words that the user would like to cor-rect with handwriting.
Then, from UNIPEN cor-pus, three users (separated from the training) wereselected to simulate user interaction.
For each user,the handwritten words were generated by concate-nating random character instances from the user?sdata to form a single stroke.
Finally, the generatedhandwritten words of the three users were decodedusing the corresponding constrained language modelwith a state-of-the-art HMM decoder, iAtros (Luja?n-Mares et al, 2008).3.1 ResultsResults are presented in classification error rate(CER), i.e.
the ratio between the errors committedby the on-line HTR decoder and the number of hand-written words introduced by the user.
All the resultshave been calculated as the average CER of the threeusers.Table 1 shows a comparison between the bestresults in this work and the approaches in previ-ous work.
The log-linear and linear weights wereobtained with the simplex algorithm (Nelder andMead, 1965) to optimise the development set.
Then,those weights were used for the test set.Two baseline models have been established forcomparison purposes.
On the one hand, (?)
is acompletely independent and context unaware sys-tem.
That would be the equivalent to decode thehandwritten text in a separate on-line HTR decoder.This system obtains the worst results of all.
Onthe other hand, (?)
is the most similar model to thebest system in (Alabau et al, 2010).
This systemis clearly outperformed by the proposed coupled ap-proach.A summary of the alternatives to language mod-392System Spanish Englishdev test dev test4gr 7.8 10.0 6.3 8.9IBM1 7.9 9.6 7.0 8.2IBM2 7.1 8.6 6.1 7.9IBM1-inv 8.4 9.5 7.5 9.2IBM2-inv 7.9 9.1 7.1 9.14gr+IBM2 (L-Linear) 7.0 9.1 6.0 7.94gr+IBM2 (Linear) 6.7 8.9 5.5 7.2Table 2: Summary of the CER results for various lan-guage modelling approaches.
In boldface the best sys-tem.elling is shown in Tab.
2.
Up to 5-grams were usedin the experiments.
However, the results did notshow significant differences between them, exceptfor the 1-gram.
Thus, context does not seem to im-prove much the performance.
This may be due tothe fact that the IMT and the on-line HTR systemsuse the same language models (5-gram in the caseof the IMT system).
Hence, if the IMT has failed topredict the correct word because of poor languagemodelling that will affect on-line HTR decoding aswell.
In fact, although language perplexities for thetest sets are quite low (33 for Spanish and 48 for En-glish), perplexities accounting only erroneous wordsincrease until 305 and 420, respectively.On the contrary, using IBM models provides asignificant boost in performance.
Although in-verse dictionaries have a better vocabulary coverage(4.7% vs 8.9% in English, 7.4% vs 10.4% in Span-ish), they tend to perform worse than their directdictionary counterparts.
Still, inverse IBM modelsperform better than the n-grams alone.
Log-linearmodels show a bit of improvement with respect toIBM models.
However, linear interpolated modelsperform the best.
In the Spanish test set the result isnot better that the IBM2 since the linear parametersare clearly over-fitted.
Other model combinations(including a combination of all models) were tested.Nevertheless, none of them outperformed the bestsystem in Table 2.3.2 Error AnalysisAn analysis of the results showed that 52.2% to61.7% of the recognition errors were produced bypunctuation and other symbols.
To circumvent thisproblem, we proposed a contextual menu in (Al-abau et al, 2010).
With such menu, errors wouldhave been reduced (best test result) to 4.1% in Span-ish and 2.8% in English.
Out-of-vocabulary (OOV)words also summed up a big percentage of the error(29.1% and 20.4%, respectively).
This differenceis due to the fact that Spanish is a more inflectedlanguage.
To solve this problem on-line learning al-gorithms or methods for dealing with OOV wordsshould be used.
Errors in gender, number and verbtenses, which rose up to 7.7% and 5.3% of the er-rors, could be tackled using linguistic informationfrom both source and target sentences.
Finally, therest of the errors were mostly due to one-to-threeletter words, which is basically a problem of hand-writing morphological modelling.4 ConclusionsIn this paper we have described a specific on-lineHTR system that can serve as an alternative interac-tion modality to IMT.
We have shown that a tight in-tegration of the HTR and IMT decoding process andthe use of the available information can produce sig-nificant HTR error reductions.
Finally, a study of thesystem?s errors has revealed the system weaknesses,and how they could be addressed in the future.5 AcknowledgmentsWork supported by the EC (FEDER/FSE) and theSpanish MEC/MICINN under the MIPRCV ?Con-solider Ingenio 2010?
program (CSD2007-00018),iTrans2 (TIN2009-14511).
Also supported bythe Spanish MITyC under the erudito.com (TSI-020110-2009-439) project and by the Generali-tat Valenciana under grant Prometeo/2009/014 andGV/2010/067, and by the ?Vicerrectorado de Inves-tigacio?n de la UPV?
under grant UPV/2009/2851.References[Alabau et al2010] V. Alabau, D.
Ortiz-Mart?
?nez, A. San-chis, and F. Casacuberta.
2010.
Multimodal in-teractive machine translation.
In Proceedings of the2010 International Conference on Multimodal Inter-faces (ICMI-MLMI?10), pages 46:1?4, Beijing, China,Nov.
[Barrachina et al2009] S. Barrachina, O. Bender,F.
Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. L.393Lagarda, H. Ney, J. Toma?s, E. Vidal, and J. M. Vilar.2009.
Statistical approaches to computer-assistedtranslation.
Computational Linguistics, 35(1):3?28.
[Berger et al1996] A. L. Berger, S. A. Della Pietra, andV.
J. Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
ComputationalLinguistics, 22:39?71.
[Brown et al1993] P. F. Brown, S. A. Della Pietra,V.
J. Della Pietra, and R. L. Mercer.
1993.
The math-ematics of machine translation.
19(2):263?311.
[Foster et al1998] G. Foster, P. Isabelle, and P. Plamon-don.
1998.
Target-text mediated interactive machinetranslation.
Machine Translation, 12:175?194.
[Guyon et al1994] Isabelle Guyon, Lambert Schomaker,Re?jean Plamondon, Mark Liberman, and Stan Janet.1994.
Unipen project of on-line data exchange andrecognizer benchmarks.
In Proceedings of Interna-tional Conference on Pattern Recognition, pages 29?33.
[Koehn and Haddow2009] P. Koehn and B. Haddow.2009.
Interactive assistance to human translators usingstatistical machine translation methods.
In Proceed-ings of MT Summit XII, pages 73?80, Ottawa, Canada.
[Luja?n-Mares et al2008] M?
?riam Luja?n-Mares, VicentTamarit, Vicent Alabau, Carlos D.
Mart?
?nez-Hinarejos, Moise?s Pastor i Gadea, Alberto Sanchis,and Alejandro H. Toselli.
2008. iATROS: A speechand handwritting recognition system.
In V Jornadasen Tecnolog?
?as del Habla (VJTH?2008), pages 75?78,Bilbao (Spain), Nov.[Nelder and Mead1965] J.
A. Nelder and R. Mead.
1965.A simplex method for function minimization.
Com-puter Journal, 7:308?313.
[Och and Ney2002] F. J. Och and H. Ney.
2002.
Dis-criminative training and maximum entropy models forstatistical machine translation.
In Proceedings of the40th ACL, pages 295?302, Philadelphia, PA, July.[Ortiz-Mart?
?nez et al2005] D.
Ortiz-Mart?
?nez, I.
Garc?
?a-Varea, and F. Casacuberta.
2005.
Thot: a toolkit totrain phrase-based statistical translation models.
InProceedings of the MT Summit X, pages 141?148.
[Papineni et al1998] K. A. Papineni, S. Roukos, and R. T.Ward.
1998.
Maximum likelihood and discriminativetraining of direct translation models.
In InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP?98), pages 189?192, Seattle, Washing-ton, USA, May.
[Rabiner1989] L. Rabiner.
1989.
A Tutorial of HiddenMarkov Models and Selected Application in SpeechRecognition.
Proceedings IEEE, 77:257?286.
[SchulmbergerSema S.A. et al2001] SchulmbergerSemaS.A., Celer Soluciones, Instituto Te?cnico de In-forma?tica, R.W.T.H.
Aachen - Lehrstuhl fu?r In-formatik VI, R.A.L.I.
Laboratory - University ofMontreal, Socie?te?
Gamma, and Xerox ResearchCentre Europe.
2001.
X.R.C.
: TT2.
TransType2- Computer assisted translation.
Project technicalannex.
[Toselli et al2007] Alejandro H. Toselli, Moise?s Pastori Gadea, and Enrique Vidal.
2007.
On-line handwrit-ing recognition system for tamil handwritten charac-ters.
In 3rd Iberian Conference on Pattern Recognitionand Image Analysis, pages 370?377.
Girona (Spain),June.
[Toselli et al2010] A. H. Toselli, V. Romero, M. Pastor,and E. Vidal.
2010.
Multimodal interactive transcrip-tion of text images.
Pattern Recognition, 43(5):1814?1825.
[Vidal et al2006] E. Vidal, F. Casacuberta, L.
Rodr??guez,J.
Civera, and C.
Mart??nez.
2006.
Computer-assistedtranslation using speech recognition.
IEEE Trans-action on Audio, Speech and Language Processing,14(3):941?951.394
