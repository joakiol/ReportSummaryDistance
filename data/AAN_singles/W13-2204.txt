Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsLIMSI @ WMT?13Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2Univ.
Paris-Sud1 and LIMSI-CNRS2rue John von Neumann, 91403 Orsay cedex, France{firstname.lastname}@limsi.frVietnamese Academy of Science and Technology3, Hanoi, Vietnamlehaison@ioit.ac.vnAbstractThis paper describes LIMSI?s submis-sions to the shared WMT?13 translationtask.
We report results for French-English,German-English and Spanish-English inboth directions.
Our submissions usen-code, an open source system based onbilingual n-grams, and continuous spacemodels in a post-processing step.
Themain novelties of this year?s participationare the following: our first participationto the Spanish-English task; experimentswith source pre-ordering; a tighter integra-tion of continuous space language mod-els using artificial text generation (for Ger-man); and the use of different tuning setsaccording to the original language of thetext to be translated.1 IntroductionThis paper describes LIMSI?s submissions to theshared translation task of the Eighth Workshop onStatistical Machine Translation.
LIMSI partici-pated in the French-English, German-English andSpanish-English tasks in both directions.
For thisevaluation, we used n-code, an open source in-house Statistical Machine Translation (SMT) sys-tem based on bilingual n-grams1, and continuousspace models in a post-processing step, both fortranslation and target language modeling.This paper is organized as follows.
Section 2contains an overview of the baseline systems builtwith n-code, including the continuous space mod-els.
As in our previous participations, severalsteps of data pre-processing, cleaning and filter-ing are applied, and their improvement took a non-negligible part of our work.
These steps are sum-marized in Section 3.
The rest of the paper is de-voted to the novelties of the systems submitted this1http://ncode.limsi.fr/year.
Section 4 describes the system developed forour first participation to the Spanish-English trans-lation task in both directions.
To translate fromGerman into English, the impact of source pre-ordering is investigated, and experimental resultsare reported in Section 5, while for the reverse di-rection, we explored a text sampling strategy us-ing a 10-gram SOUL model to allow a tighter in-tegration of continuous space models during thetranslation process (see Section 6).
A final sectiondiscusses the main lessons of this study.2 System overviewn-code implements the bilingual n-gram approachto SMT (Casacuberta and Vidal, 2004; Marin?oet al 2006; Crego and Marin?o, 2006).
In thisframework, translation is divided in two steps: asource reordering step and a (monotonic) transla-tion step.
Source reordering is based on a set oflearned rewrite rules that non-deterministically re-order the input words.
Applying these rules resultin a finite-state graph of possible source reorder-ings, which is then searched for the best possiblecandidate translation.2.1 FeaturesGiven a source sentence s of I words, the besttranslation hypothesis t?
is defined as the sequenceof J words that maximizes a linear combination offeature functions:t?
= argmaxt,a{ M?m=1?mhm(a, s, t)}(1)where ?m is the weight associated with featurefunction hm and a denotes an alignment betweensource and target phrases.
Among the featurefunctions, the peculiar form of the translationmodel constitutes one of the main difference be-tween the n-gram approach and standard phrase-based systems.62In addition to the translation model (TM), four-teen feature functions are combined: a target-language model; four lexicon models; six lexical-ized reordering models (Tillmann, 2004; Crego etal., 2011) aimed at predicting the orientation ofthe next translation unit; a ?weak?
distance-baseddistortion model; and finally a word-bonus modeland a tuple-bonus model which compensate for thesystem preference for short translations.
The fourlexicon models are similar to the ones used in stan-dard phrase-based systems: two scores correspondto the relative frequencies of the tuples and twolexical weights are estimated from the automaticword alignments.
The weight vector ?
is learnedusing the Minimum Error Rate Training frame-work (MERT) (Och, 2003) and BLEU (Papineniet al 2002) measured on nt09 (newstest2009) asthe optimization criteria.2.2 Translation InferenceDuring decoding, source sentences are representedin the form of word lattices containing the mostpromising reordering hypotheses, so as to repro-duce the word order modifications introduced dur-ing the tuple extraction process.
Hence, only thosereordering hypotheses are translated and are intro-duced using a set of reordering rules automaticallylearned from the word alignments.
Part-of-speech(POS) information is used to increase the gen-eralization power of these rules.
Hence, rewriterules are built using POS, rather than surface wordforms (Crego and Marin?o, 2006).2.3 SOUL rescoringNeural networks, working on top of conventionaln-gram back-off language models (BOLMs), havebeen introduced in (Bengio et al 2003; Schwenket al 2006) as a potential means to improve dis-crete language models (LMs).
As for our last yearparticipation (Le et al 2012c), we take advantageof the recent proposal of Le et al(2011).
Usinga specific neural network architecture (the Struc-tured OUtput Layer or SOUL model), it becomespossible to estimate n-gram models that use largevocabulary, thereby making the training of largeneural network LMs (NNLMs) feasible both fortarget language models and translation models (Leet al 2012a).
We use the same models as last year,meaning that the SOUL rescoring was used for allsystems, except for translating into Spanish.
Seesection 6 and (Le et al 2012c) for more details.3 Corpora and data pre-processingConcerning data pre-processing, we started fromour submissions from last year (Le et al 2012c)and mainly upgraded the corpora and the associ-ated language-dependent pre-processing routines.We used in-house text processing tools for the to-kenization and detokenization steps (De?chelotteet al 2008).
Previous experiments have demon-strated that better normalization tools provide bet-ter BLEU scores: all systems are thus built usingthe ?true-case?
scheme.As German is morphologically more complexthan English, the default policy which consists intreating each word form independently is plaguedwith data sparsity, which severely impacts bothtraining (alignment) and decoding (due to un-known forms).
When translating from Germaninto English, the German side is thus normalizedusing a specific pre-processing scheme (Allauzenet al 2010; Durgar El-Kahlout and Yvon, 2010)which aims at reducing the lexical redundancy by(i) normalizing the orthography, (ii) neutralizingmost inflections and (iii) splitting complex com-pounds.
All parallel corpora were POS-taggedwith the TreeTagger (Schmid, 1994); in addition,for German, fine-grained POS labels were alsoneeded for pre-processing and were obtained us-ing the RFTagger (Schmid and Laws, 2008).For Spanish, all the availaible data are tokenizedusing FreeLing2 toolkit (Padro?
and Stanilovsky,2012), with default settings and some added rules.Sentence splitting and morphological analysis aredisabled except for del ?
de el and al ?
a el.Moreover, a simple ?true-caser?
based on upper-case word frequency is used, and the specificSpanish punctuation signs ???
and ???
are removedand heuristically reintroduced in a post-processingstep.
All Spanish texts are POS-tagged also usingFreeling.
The EAGLES tag set is however sim-plified by truncating the category label to the firsttwo symbols, in order to reduce the sparsity of thereordering rules estimated by n-code.For the CommonCrawl corpus, we found thatmany sentences are not in the expected language.For example, in the French side of the French-English version, most of the first sentences arein English.
Therefore, foreign sentence pairs arefiltered out with a MaxEnt classifier that uses n-grams of characters as features (n is between 1and 4).
This filter discards approximatively 10%2http://nlp.lsi.upc.edu/freeling/63of the sentence pairs.
Moreover, we also observethat a lot of sentence pairs are not translation ofeach other.
Therefore, an extra sentence alignmentstep is carried out using an in-house implementa-tion of the tool described in (Moore, 2002).
Thislast step discards approximately 20% of the cor-pus.
For the Spanish-English task, the same filter-ing is applied to all the available corpora.4 System development for theSpanish-English taskThis is our first participation to the Spanish-English translation task in both directions.
Thissection provides details about the development ofn-code systems for this language pair.4.1 Data selection and filteringThe CommonCrawl and UN corpora can be con-sidered as very noisy and out-of-domain.
As de-scribed in (Allauzen et al 2011), to select a subsetof parallel sentences, trigram LMs were trained forboth Spanish and English languages on a subset ofthe available News data: the Spanish (resp.
En-glish) LM was used to rank the Spanish (resp.
En-glish) side of the corpus, and only those sentenceswith perplexity above a given threshold were se-lected.
Finally, the two selected sets were in-tersected.
In the following experiments, the fil-tered versions of these corpora are used to trainthe translation systems unless explicitly stated.4.2 Spanish language modelTo train the language models, we assumed that thetest set would consist in a selection of recent newstexts and all the available monolingual data forSpanish were used, including the Spanish Giga-word, Third Edition.
A vocabulary is first definedby including all tokens observed in the News-Commentary and Europarl corpora.
This vocab-ulary is then expanded with all words that occurmore than 10 times in the recent news texts (LDC-2007-2011 and news-crawl-2011-2012).
This pro-cedure results in a vocabulary containing 372kwords.
Then, the training data are divided into7 sets based on dates or genres.
On each set, astandard 4-gram LM is estimated from the vocab-ulary using absolute discounting interpolated withlower order models (Kneser and Ney, 1995; Chenand Goodman, 1998).
The resulting LMs are thenlinearly interpolated using coefficients chosen soCorpora BLEUdev nt11 test nt12es2en N,E 30.2 33.2N,E,C 30.6 33.7N,E,U 30.3 33.6N,E,C,U 30.6 33.7N,E,C,U (nf) 30.7 33.6en2es N,E 32.2 33.3N,E,C,U 32.3 33.6N,E,C,U (nf) 32.5 33.9Table 1: BLEU scores achieved with differentsets of parallel corpora.
All systems are base-line n-code with POS factor models.
The follow-ing shorthands are used to denote corpora, : ?N?stands for News-Commentary, ?E?
for Europarl,?C?
for CommonCrawl, ?U?
for UN and (nf) fornon filtered corpora.as to minimise the perplexity evaluated on the de-velopment set (nt08).4.3 ExperimentsAll reported results are averaged on 3 MERT runs.Table 1 shows the BLEU scores obtained with dif-ferent corpora setups.
We can observe that us-ing the CommonCrawl corpus improves the per-formances in both directions, while the impact ofthe UN data is less important, especially whencombined with CommonCrawl.
The filtering strat-egy described in Section 4.2 has a slightly posi-tive impact of +0.1 BLEU point for the Spanish-to-English direction but yields a 0.2 BLEU pointdecrease in the opposite direction.For the following experiments, all the availablecorpora are therefore used: News-Commentary,Europarl, filtered CommonCrawl and UN.
Foreach of these corpora, a bilingual n-gram modelis estimated and used by n-code as one individualmodel score.
An additionnal TM is trained on theconcatenation all these corpora, resulting in a to-tal of 5 TMs.
Moreover, n-code is able to handleadditional ?factored?
bilingual models where thesource side words are replaced by the correspond-ing lemma or even POS tag (Koehn and Hoang,2007).
Table 2 reports the scores obtained withdifferent settings.In Table 2, big denotes the use of a widercontext for n-gram TMs (n = 4, 5, 4 insteadof 3, 4, 3 respectively for word-based, POS-basedand lemma-based TMs).
Using POS factored64Condition BLEUdev nt11 test nt12es2en base 30.3 33.5pos 30.6 33.7big-pos 30.7 33.7big-pos-lem 30.7 33.8en2es base 32.0 33.4pos 32.3 33.6big-pos 32.3 33.8big-pos-pos+ 32.2 33.4Table 2: BLEU scores for different configurationof factored translation models.
The big prefix de-notes experiments with the larger context for n-gram translation models.models yields a significant BLEU improvement,as well as using a wider context for n-gram TMs.Since Spanish is morphologically richer than En-glish, lemmas are introduced only on the Span-ish side.
An additionnal BLEU improvement isachieved by adding factored models based on lem-mas when translating from Spanish to English,while in the opposite direction it does not seemto have any clear impact.For English to Spanish, we also experimentedwith a 5-gram target factored model, using thewhole morphosyntactic EAGLES tagset, (pos+ inTable 2), to add some syntactic information, butthis, in fact, proved harmful.As several tuning sets were available, experi-ments were carried out with the concatenation ofnt09 to nt11 as a tuning data set.
This yields an im-provement between 0.1 and 0.3 BLEU point whentesting on nt12 when translating from Spanish toEnglish.4.4 Submitted systemsFor both directions, the submitted systems aretrained on all the available training data, the cor-pora CommonCrawl and UN being filtered as de-scribed previously.
A word-based TM and a POSfactored TM are estimated for each training set.To translate from Spanish to English, the systemis tuned on the concatenation of the nt09 to nt11datasets with an additionnal 4-gram lemma-basedfactored model, while in the opposite direction, weonly use nt11.dev nt09 test nt11en2de 15.43 15.35en-mod2de 15.06 15.00Table 3: BLEU scores for pre-ordering experi-ments with a n-code system and the approach pro-posed by (Neubig et al 2012)5 Source pre-ordering for English toGerman translationWhile distorsion models can efficiently handleshort range reorderings, they are inadequate tocapture long-range reorderings, especially for lan-guage pairs that differ significantly in their syn-tax.
A promising workaround is the source pre-ordering method that can be considered similar,to some extent, to the reordering strategy imple-mented in n-code; the main difference is that thelatter uses one deterministic (long-range) reorder-ing on top of conventional distortion-based mod-els, while the former only considers one singlemodel delivering permutation lattices.
The pre-ordering approach is illustrated by the recent workof Neubig et al(2012), where the authors use adiscriminatively trained ITG parser to infer a sin-gle permutation of the source sentence.In this section, we investigate the use of thispre-ordering model in conjunction with the bilin-gual n-gram approach for translating English intoGerman (see (Collins et al 2005) for similar ex-periments with the reverse translation direction).Experiments are carried out with the same settingsas described in (Neubig et al 2012): given thesource side of the parallel data (en), the parser isestimated to modify the original word order and togenerate a new source side (en-mod); then a SMTsystem is built for the new language pair (en-mod?
de).
The same reordering model is used to re-order the test set, which is then translated with theen-mod?
de system.Results for these experiments are reported in Ta-ble 3, where nt09 and nt11 are respectively usedas development and test sets.
We can observe thatapplying pre-ordering on source sentences leads tosmall drops in performance for this language pair.To explain this degradation, the histogram of to-ken movements performed by the model on thepre-ordered training data is represented in Fig-ure 1.
We can observe that most of the movementsare in the range [?4,+6] (92% of the total occur-65Figure 1: Histogram of token movement size ver-sus its occurrences performed by the model Neu-big on the source english data.rences), which can be already taken into accountby the standard reordering model of the baselinesystem.
This is reflected also by the followingstatistics: surprisingly, only 16% of the total num-ber of sentences are changed by the pre-orderingmodel, and the average sentence-wise Kendall?s ?and the average displacement of these small partsof modified sentences are, respectively, 0.027 and3.5.
These numbers are striking for two reasons:first, English and German have in general quitedifferent word order, thus our experimental con-dition should be somehow similar to the English-Japanese scenario studied in (Neubig et al 2012);second, since the model is able to perform pre-ordering basically at any distance, it is surprisingthat a large part of the data remains unmodified.6 Artificial Text generation with SOULWhile the context size for BOLMs is limited (usu-ally up to 4-grams) because of sparsity issues,NNLMs can efficiently handle larger contexts upto 10-grams without a prohibitive increase of theoverall number of parameters (see for instance thestudy in (Le et al 2012b)).
However the majorbottleneck of NNLMs is the computation cost dur-ing both training and inference.
In fact, the pro-hibitive inference time usually implies to resort toa two-pass approach: the first pass uses a conven-tional BOLM to produce a k-best list (the k mostlikely translations); in the second pass, the prob-ability of a NNLM is computed for each hypoth-esis, which is then added as a new feature beforethe k-best list is reranked.
Note that to produce thek-best list, the decoder uses a beam search strategyto prune the search space.
Crucially, this pruningdoes not use the NNLMs scores and results in po-tentially sub-optimal k-best-lists.6.1 Sampling texts with SOULIn language modeling, a language is representedby a corpus that is approximated by a n-grammodel.
Following (Sutskever et al 2011; Deoraset al 2013), we propose an additionnal approxi-mation to allow a tighter integration of the NNLM:a 10-gram NNLM is first estimated on the trainingcorpus; texts then are sampled from this model tocreate an artificial training corpus; finally, this arti-ficial corpus is approximated by a 4-gram BOLM.The training procedure for the SOUL NNLM isthe same as the one described in (Le et al 2012c).To sample a sentence from the SOUL model, firstthe sentence length is randomly drawn from theempirical distribution, then each word of the sen-tence is sampled from the 10-gram distribution es-timated with the SOUL model.The convergence of this sampling strategy canbe evaluated by monitoring the perplexity evolu-tion vs. the number of sentences that are gener-ated.
Figure 2 depicts this evolution by measuringperplexity on the nt08 set with a step size of 400Msampled sentences.
The baseline BOLM (std) isestimated on all the available training data thatconsist of approximately 300M of running words.We can observe that the perplexity of the BOLMestimated on sampled texts (generated texts) de-creases when the number of sample sentences in-creases, and tends to reach slowly the perplex-ity of the baseline BOLM.
Moreover, when bothBOLMs are interpolated, an even lower perplex-ity is obtained, which further decreases with theamount of sampled training texts.6.2 Translation resultsExperiments are run for translation into German,which lacks a GigaWord corpus.
An artificial cor-pus containing 3 billions of running words is firstgenerated as described in Section 6.1.
This corpusis used to estimate a BOLM with standard settings,that is then used for decoding, thereby approxi-mating the use of a NNLM during the first pass.Results reported in Table 4 show that adding gen-erated texts improves the BLEU scores even whenthe SOUL model is added in a rescoring step.
Alsonote that using the LM trained on the sampled cor-pus yields the same BLEU score that using thestandard LM.66190 200 210 220 230 240 250 260270 2802  4  6  8  10  12ppx times 400M sampled sentencesartificial textsartificial texts+stdstdFigure 2: Perplexity measured on nt08 with thebaseline LM (std), with the LM estimated on thesampled texts (generated texts), and with the inter-polation of both.Therefore, to translate from English to German,the submitted system includes three BOLMs: onetrained on all the monolingual data, one on artifi-cial texts and a third one that uses the freely avail-able deWack corpus3 (1.7 billion words).target LM BLEUdev nt09 test nt10base 15.3 16.5+genText 15.5 16.8+SOUL 16.4 17.6+genText+SOUL 16.5 17.8Table 4: Impact of the use of sampled texts.7 Different tunings for different originallanguagesAs shown by Lembersky et al(2012), the originallanguage of a text can have a significant impact ontranslation performance.
In this section, this effectis assessed on the French to English translationtask.
Training one SMT system per original lan-guage is impractical, since the required informa-tion is not available for most of parallel corpora.However, metadata provided by the WMT evalua-tion allows us to split the development and test setsaccording to the original language of the text.
Toensure a sufficient amount of texts for each con-dition, we used the concatenation of newstest cor-pora for the years 2008, 2009, 2011, and 2012,leaving nt10 for testing purposes.Five different development sets have been cre-ated to tune five different systems.
Experimentalresults are reported in Table 7 and show a drastic3http://wacky.sslmit.unibo.it/doku.phpbaseline adaptedoriginal language tuningcz 22.31 23.83en 36.41 39.21fr 31.61 32.41de 18.46 18.49es 30.17 29.34all 29.43 30.12Table 5: BLEU scores for the French-to-Englishtranslation task measured on nt10 with systemstuned on development sets selected according totheir original language (adapted tuning).improvement in terms of BLEU score when trans-lating back to the original English and a significantincrease for original text in Czech and French.
Inthis year?s evaluation, Russian was introduced asa new language, so for sentences originally in thislanguage, the baseline system was used.
This sys-tem is used as our primary submission to the eval-uation, with additional SOUL rescoring step.8 ConclusionIn this paper, we have described our submis-sions to the translation task of WMT?13 forthe French-English, German-English and Spanish-English language pairs.
Similarly to last year?ssystems, our main submissions use n-code, andcontinuous space models are introduced in a post-processing step, both for translation and target lan-guage modeling.
To translate from English toGerman, we showed a slight improvement witha tighter integration of the continuous space lan-guage model using a text sampling strategy.
Ex-periments with pre-ordering were disappointing,and the reasons for this failure need to be betterunderstood.
We also explored the impact of usingdifferent tuning sets according to the original lan-guage of the text to be translated.
Even though thegain vanishes when adding the SOUL model in apost-processing step, it should be noted that due totime limitation this second step was not tuned ac-cordingly to the original language.
We thereforeplan to assess the impact of using different tuningsets on the post-processing step.AcknowledgmentsThis work was partially funded by the French Stateagency for innovation (OSEO), in the Quaero Pro-gramme.67ReferencesAlexandre Allauzen, Josep M. Crego, I?lknur Durgar El-Kahlout, and Franc?ois Yvon.
2010.
LIMSI?s statis-tical translation systems for WMT?10.
In Proc.
ofthe Joint Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 54?59, Uppsala, Swe-den.Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-Maynard, Josep M. Crego, Hai-Son Le, Aure?lienMax, Adrien Lardilleux, Thomas Lavergne, ArtemSokolov, Guillaume Wisniewski, and Franc?oisYvon.
2011.
LIMSI @ WMT11.
In Proceedings ofthe Sixth Workshop on Statistical Machine Transla-tion, pages 309?315, Edinburgh, Scotland, July.
As-sociation for Computational Linguistics.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for lan-guage modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard Un iversity.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL?05), pages 531?540, Ann Arbor,Michigan.Josep M. Crego and Jose?
B. Marin?o.
2006.
Improvingstatistical MT by coupling reordering and decoding.Machine Translation, 20(3):199?215.Josep M. Crego, Franois Yvon, and Jos B. Marin?o.2011.
N-code: an open-source Bilingual N-gramSMT Toolkit.
Prague Bulletin of Mathematical Lin-guistics, 96:49?58.Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,and Franois Yvon.
2008.
LIMSI?s statisticaltranslation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.Anoop Deoras, Toma?s?
Mikolov, Stefan Kombrink, andKenneth Church.
2013.
Approximate inference: Asampling based modeling technique to capture com-plex dependencies in a language model.
SpeechCommunication, 55(1):162 ?
177.Ilknur Durgar El-Kahlout and Franois Yvon.
2010.The pay-offs of preprocessing for German-EnglishStatistical Machine Translation.
In Marcello Fed-erico, Ian Lane, Michael Paul, and Franois Yvon, ed-itors, Proceedings of the seventh International Work-shop on Spoken Language Translation (IWSLT),pages 251?258.Reinhard Kneser and Herman Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing, ICASSP?95,pages 181?184, Detroit, MI.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 868?876.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
Structuredoutput layer neural network language model.
In Pro-ceedings of ICASSP?11, pages 5524?5527.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012a.
Continuous space translation models withneural networks.
In NAACL ?12: Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguisticson Human Language Technology.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012b.
Measuring the influence of long range de-pendencies with neural network language models.In Proceedings of the NAACL-HLT 2012 Workshop:Will We Ever Really Replace the N-gram Model?
Onthe Future of Language Modeling for HLT, pages 1?10, Montre?al, Canada.Hai-Son Le, Thomas Lavergne, Alexandre Al-lauzen, Marianna Apidianaki, Li Gong, Aure?lienMax, Artem Sokolov, Guillaume Wisniewski, andFranc?ois Yvon.
2012c.
Limsi @ wmt12.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 330?337, Montre?al,Canada.Gennadi Lembersky, Noam Ordan, and Shuly Wint-ner.
2012.
Language models for machine trans-lation: Original vs. translated texts.
Comput.
Lin-guist., 38(4):799?825, December.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego,Adria` de Gispert, Patrick Lambert, Jose?
A.R.
Fonol-losa, and Marta R. Costa-Jussa`.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549.Robert C. Moore.
2002.
Fast and accurate sen-tence alignment of bilingual corpora.
In Proceed-ings of the 5th Conference of the Association forMachine Translation in the Americas on MachineTranslation: From Research to Real Users, AMTA?02, pages 135?144, Tiburon, CA, USA.
Springer-Verlag.68Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a discriminative parser to optimizemachine translation reordering.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 843?853, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL ?03: Proc.
ofthe 41st Annual Meeting on Association for Compu-tational Linguistics, pages 160?167.Llu?
?s Padro?
and Evgeny Stanilovsky.
2012.
Freeling3.0: Towards wider multilinguality.
In Proceedingsof the Language Resources and Evaluation Confer-ence (LREC 2012), Istanbul, Turkey, May.
ELRA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In ACL ?02:Proc.
of the 40th Annual Meeting on Association forComputational Linguistics, pages 311?318.
Associ-ation for Computational Linguistics.Helmut Schmid and Florian Laws.
2008.
Estima-tion of conditional probabilities with decision treesand an application to fine-grained POS tagging.
InProceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages777?784, Manchester, UK, August.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proc.
of Interna-tional Conference on New Methods in LanguageProcessing, pages 44?49, Manchester, UK.Holger Schwenk, Daniel De?chelotte, and Jean-LucGauvain.
2006.
Continuous space language modelsfor statistical machine translation.
In Proc.
COL-ING/ACL?06, pages 723?730.Ilya Sutskever, James Martens, and Geoffrey Hinton.2011.
Generating text with recurrent neural net-works.
In Lise Getoor and Tobias Scheffer, editors,Proceedings of the 28th International Conferenceon Machine Learning (ICML-11), ICML ?11, pages1017?1024, New York, NY, USA, June.
ACM.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of HLT-NAACL 2004, pages 101?104.
As-sociation for Computational Linguistics.69
