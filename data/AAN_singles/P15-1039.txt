Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 397?407,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsGenerating High Quality Proposition Banksfor Multilingual Semantic Role LabelingAlan Akbik?Technische Universit?at Berlin, Germanyalan.akbik@tu-berlin.deLaura Chiticariu Marina Danilevsky Yunyao LiShivakumar Vaithyanathan Huaiyu ZhuIBM Research - Almaden650 Harry Road, San Jose, CA 95120, USA{chiti,mdanile,yunyaoli,vaithyan,huaiyu}@us.ibm.comAbstractSemantic role labeling (SRL) is crucial tonatural language understanding as it identi-fies the predicate-argument structure in textwith semantic labels.
Unfortunately, re-sources required to construct SRL modelsare expensive to obtain and simply do notexist for most languages.
In this paper, wepresent a two-stage method to enable theconstruction of SRL models for resource-poor languages by exploiting monolingualSRL and multilingual parallel data.
Exper-imental results show that our method out-performs existing methods.
We use ourmethod to generate Proposition Banks withhigh to reasonable quality for 7 languagesin three language families and release theseresources to the research community.1 IntroductionSemantic role labeling (SRL) is the task of automat-ically labeling predicates and arguments in a sen-tence with shallow semantic labels.
This level ofanalysis provides a more stable semantic representa-tion across syntactically different sentences, therebyenabling a range of NLP tasks such as informationextraction and question answering (Shen and Lap-ata, 2007; Maqsud et al, 2014).
Projects such as theProposition Bank (PropBank) (Palmer et al, 2005)spent considerable effort to annotate corpora withsemantic labels, in turn enabling supervised learn-ing of statistical SRL parsers for English.
Unfor-?This work was conducted at IBM.tunately, due to the high costs of manual annota-tion, comparable SRL resources do not exist formost other languages, with few exceptions (Haji?cet al, 2009; Erk et al, 2003; Zaghouani et al, 2010;Vaidya et al, 2011).As a cost-effective alternative to manual annota-tion, previous work has investigated the direct pro-jection of semantic labels from a resource rich lan-guage (English) to a resource poor target language(TL) in parallel corpora (Pado, 2007; Van der Plas etal., 2011).
The underlying assumption is that orig-inal and translated sentences in parallel corpora aresemantically broadly equivalent.
Hence, if Englishsentences of a parallel corpus are automatically la-beled using an SRL system, these labels can be pro-jected onto aligned words in the TL corpus, therebyautomatically labeling the TL corpus with seman-tic labels.
This way, PropBank-like resources canautomatically be created that enable the training ofstatistical SRL systems for new TLs.However, as noted in previous work (Pado, 2007;Van der Plas et al, 2011), aligned sentences in par-allel corpora often exibit issues such as translationWe need to hold people responsibleA0 need.01 A1A0 hold.01 A1 A2Il faut qu' il y desait responsablesneed.01 A1it needs exist those responsiblethat thereexist.01 A1need.01A1TLSLFigure 1: Pair of parallel sentences from Frenchgoldwith wordalignments (dotted lines), SRL labels for the English sentence,and gold SRL labels for the French sentence.
Only two of theseven English SRL labels should be projected here.397Figure 2: Overview of the proposed two-stage approach forprojecting English (EN) semantic role labels onto a TL corpus.shifts that go against this assumption.
For example,in Fig.
1, the English sentence ?We need to hold peo-ple responsible?
is translated into a French sentencethat literally reads as ?There need to exist those re-sponsible?.
Hence, the predicate label of the Englishword ?hold?
should not be projected onto the Frenchverb, which has a different meaning.
As the exam-ple in Fig.
1 shows, this means that only a subset ofall SL labels can be directly projected.In this paper, we aim to create PropBank-like re-sources for a range of languages from different lan-guage groups.
To this end, we propose a two-stageapproach to cross-lingual semantic labeling that ad-dresses such errors, shown in Fig.
2: Given a par-allel corpus in which the source language (SL) sideis automatically labeled with PropBank labels andthe TL side is syntactically parsed, we use a filteredprojection approach that allows the projection onlyof high-confidence SL labels.
This results in a TLcorpus with low recall but high precision.
In thesecond stage, we repeatedly sample a subset of com-plete TL sentences and train a classifier to iterativelyadd new labels, significantly increasing the recall inthe TL corpus while retaining the improvement inprecision.Our contributions are: (1) We propose filteredprojection focused specifically on raising the pre-cision of projected labels, based on a detailed anal-ysis of direct projection errors.
(2) We propose abootstrap learning approach to retrain the SRL toiteratively improve recall without a significant re-duction of precision, especially for arguments; (3)We demonstrate the effectiveness and generalizabil-ity of our approach via an extensive set of experi-ments over 7 different language pairs.
(4) We gen-erate PropBanks for each of these languages and re-lease them to the research community.12 Stage 1: Filtered Annotation ProjectionStage 1 of our approach (Fig.
2) is designed to createa TL corpus with high precision semantic labels.Direct Projection The idea of direct annotationprojection (Van der Plas et al, 2011) is to transfersemantic labels from SL sentences to TL sentencesaccording to word alignments.
Formally, for eachpair of sentences sSLand sTLin the parallel corpus,the word alignment produces alignment pairs (wSL,i,wTL,i?
), where wSL,iand wTL,i?are words from sSLandsTLrespectively.
Under direct projection, if lSL,iisa predicate label for wSL,iand (wSL,i, wTL,i?)
is analignment pair, then lSL,iis transferred to wTL,i?
; IflSL,jis a predicate-argument label for (wSL,i, wSL,j),and (wSL,i, wTL,i?)
and (wSL,j, wTL,j?)
are alignmentpairs, then lSL,jis transferred to (wTL,i?, wTL,j?
), asillustrated below.Filtered Projection As discussed earlier, directprojection is vulnerable to errors stemming fromissues such as translation shifts.
We propose fil-tered projection focused specifically on improvingthe precision of projected labels.
Specifically, for apair of sentences sSLand sTLin the parallel corpus,we retain the semantic label lSL,iprojected from wSL,ionto wTL,i?if and only if it satisfies the filtering poli-cies.
This results in a target corpus containing fewerlabels but of higher precision compared to that ob-tained via direct projection.In the rest of the section, we analyze typical errorsin direct projection (Sec.
2.2), present a set of filtersto handle such errors (Sec.
2.3), and experimentallyevaluate their effectiveness (Sec.
2.4).1The resources are available on request.398ERROR CLASS NUMBERTranslation Shift: Predicate Mismatch 37Translation Shift: Verb?Non-verb 36No English Equivalent 8Gold Data Errors 6SRL Errors 5Verb (near-)Synonyms 4Light Verb Construction 3Alignment Errors 1Total 100Table 1: Breakdown of error classes in predicate projection.2.1 Experimental SetupData For experiments in this section and Sec.
3, weused the gold data set compiled by (Van der Plaset al, 2011), referred to as Frenchgold.
It consistsof 1,000 sentence-pairs from the English-FrenchEuroparl corpus (Koehn, 2005) with French sen-tences manually labeled with predicate and argu-ment labels from the English Propbank.Evaluation In line with previous work (Van der Plaset al, 2010), we count synonymous predicate labelssharing the same VERBNET (Schuler, 2005) class astrue positives.2In addition, we exclude modal verbsfrom the evaluation due to inconsistent annotation.Source Language SRL Throughout the rest of thepaper, we use CLEARNLP (Choi and McCallum,2013), a state-of-the-art SRL system, to produce se-mantic labels for English text.2.2 Error AnalysisWe observe that direct projection labels have bothlow precision and low recall (see Tab.
3 (Direct)).Analysis of False Negatives The low recall of di-rect projection is not surprising; most semantic la-bels in the French sentences do not appear in thecorresponding English sentences at all.
Specifically,among 1,741 predicate labels in the French sen-tences, only 778 exist in the corresponding Englishsentences, imposing a 45% upper bound on the re-call for projected predicates.
Similarly, of the 5,061argument labels in the French sentences, only 1,757exist in the corresponding English sentences, result-ing in a 35% upper bound on recall for arguments.32For instance, the French verb sembler may be correctly la-beled as either of the synonyms: seem.01 or appear.02.3This upper bound is different from the one reportedin (Van der Plas et al, 2011) which corresponds to the inter-annotator agreement over manual annotation of 100 sentences.ERROR CLASS NUMBERNon-Argument Head 33SRL Errors 31No English Equivalent 12Gold Data Errors 11Translation Shift: Argument Function 6Parsing Errors 4Alignment Errors 3Total 100Table 2: Breakdown of error classes in argument projection.Analysis of False Positives While the recall pro-duced by direct projection is close to the theoreticalupper bound, the precision is far from the theoreticalupper bound of 100%.
To understand causes of falsepositives, we examine a random sample of 200 falsepositives, of which 100 are incorrect predicate la-bels, and 100 are incorrect argument labels belong-ing to correctly projected predicates.
Tab.
1 and 2show the detailed breakdown of errors for predicatesand arguments, respectively.
We first analyze themost common types of errors and discuss the resid-ual errors later in Sec.
2.5.?
Translation Shift: Predicate Mismatch Themost common predicate errors (37%) are translationshifts in which an English predicate is aligned to aFrench verb with a different meaning.
Fig.
1 illus-trates such a translation shift: label hold.01 of En-glish verb hold is wrongly projected onto the Frenchverb ait, which is labeled as exist.01 in Frenchgold.?
Translation Shift: Verb?Non-Verb is anothercommon predicate error (36%).
English verbs maybe aligned with TL words other than verbs, whichis often indicative of translation shifts.
For instance,in the following sentence pairsSLWe know what happenedsFROn connait la suiteWe know the resultthe English verb happen is aligned to the Frenchnoun suite (result), causing it to be wrongly pro-jected with the English predicate label happen.01.?
Non-Argument Head The most common argu-ment error (33%) is caused by the projection of ar-gument labels onto words other than the syntactichead of a target verb?s argument.
For example, inFig.
1 the label A1 on the English hold is wronglytransferred to the French ait, which is not the syn-tactic head of the complement.3992.3 FiltersWe consider the following filters to remove the mostcommon types of false positives.Verb Filter (VF) targets Verb?Non-Verb transla-tion shift errors (Van der Plas et al, 2011).
For-mally, if direct projection transfers predicate labellSL,ifrom wSL,ionto wTL,i?, retain lSL,ionly if bothwSL,iand wTL,i?are verbs.Translation Filter (TF) handles both PredicateMismatch and Verb?Non-Verb translation shift er-rors.
It makes use of a translation dictionary andallows projection only if the TL verb is a valid trans-lation of the SL verb.
In addition, in order to ensureconsistent predicate labels throughout the TL cor-pus, if a SL verb has several possible synonymoustranslations, it allows projection only for the mostcommonly observed translation.Formally, for an aligned pair (wSL,i, wTL,i?)
wherewSL,ihas predicate label lSL,i, if (wSL,i, wTL,i?)
is nota verb to verb translation from SL to TL, assign nolabel to wTL,i?.
Otherwise, split the set of SL trans-lations of wTL,i?into synonym sets S1, S2, .
.
.
; Foreach k, let Wkbe the subset of Skmost commonlyaligned with wTL,i?
; If wSL,iis in one of these Wk,assign label lSL,ito wTL,i?
; Otherwise assign no labelto wTL,i?.Reattachment Heuristic (RH) targets non-argument head errors that occur if a TL argumentis not the direct child of a verb in the dependencyparse tree of its sentence.4Assume direct projectiontransfers the predicate-argument label lSL,jfrom(wSL,i, wSL,j) onto (wTL,i?, wTL,j?).
Find the immedi-ate ancestor verb of wTL,j?in the dependency parsetree.
Denote as wTL,kits child that is an ancestor ofwTL,j?.
Assign the label lSL,jto (wTL,i?, wTL,k) insteadof (wTL,i?, wTL,j?).
An illustration is below:RH ensures that labels are always attached to thesyntactic heads of their respective arguments, as de-4In (Pad?o and Lapata, 2009), a similar filtering method isdefined over constituent-based trees to reduce the set of viablenodes for argument labels to all nodes that are not a child ofsome ancestor of the predicate.PREDICATE ARGUMENTPROJECTION P R F1 P R F1Direct 0.45 0.4 0.43 0.43 0.31 0.36VF 0.59 0.4 0.48 0.53 0.31 0.39TF 0.88 0.36 0.51 0.58 0.17 0.27VF+RH 0.59 0.4 0.48 0.68 0.35 0.46TF+RH 0.88 0.36 0.51 0.75 0.2 0.31Upper Bound 1 0.45 0.62 1 0.35 0.51Table 3: Quality of predicate and argument labels for differentprojection methods on Frenchgold, including upper bound.termined by the dependency tree.
An example ofsuch reattachment is illustrated in Fig.
1 (curved ar-row on TL sentence).2.4 Filter EffectivenessWe now present an initial validation on the effec-tiveness of the aforementioned filters by evaluatingtheir contribution to annotation projection qualityfor Frenchgold, as summarized in Tab.
3.VF reduces the number of wrongly projected predi-cate labels, resulting in an increase of predicate pre-cision to 59% (?14 pp), without impact to recall.
Asa side effect, argument precision also increases to53% (?10 pp), since, if a predicate label cannot beprojected, none of its arguments can be projected.TF5reduces the number of wrongly projected pred-icate labels even more significantly, increasing pred-icate precision to 88% (?43 pp), at a small cost to re-call.
Again, argument precision increases as a sideeffect.
However, as expected, argument recall de-creases significantly (?14 pp, to 17%), as many ar-guments can no longer be projected.RH targets argument labels directly (unlike VF andTF), significantly increasing argument precision andslightly increasing argument recall.In summary, initial experiments confirm that ourproposed filters are effective in improving preci-sion of projected labels at a small cost in recall.
Infact, TF+RH results in nearly 100% improvement inpredicate and argument labels precision with a muchsmaller drop in recall.2.5 Residual ErrorsFiltered projection removes the most common errorsdiscussed in Sec.
2.2.
Most of the remaining errors5In all experiments in this paper, we derived the trans-lation dictionaries from the WIKTIONARY project and usedVERBNET and WORDNET to find SL synonym groups.400come from the following sources.SRL Errors The most common residual errors inthe remaining projected labels, especially for argu-ment labels, are caused by mistakes made by the En-glish SRL system.
Any wrong label it assigns to anEnglish sentence may be projected onto the TL sen-tence, resulting in false positives.No English Equivalent A small number of errorsoccur due to French particularities that do not existin English.
Such errors include certain French verbsfor which no appropriate English PropBank labelsexists, and French-specific syntactic particularities.6Gold Data Errors Our evaluation so far relieson Frenchgoldas ground truth.
Unfortunately,Frenchgolddoes contain a small number of errors(e.g.
missing argument labels).
As a result, somecorrectly projected labels are being mistaken asfalse positives, causing a drop in both precision andrecall.
We therefore expect the true precision andrecall of the approach to be somewhat higher thanthe estimate based on Frenchgold.3 Stage 2: Bootstrapped Training of SRLAs discussed earlier, the TL corpus generated via fil-tered projection suffers from low recall.
We addressthis issue with the second stage of our method.Relabeling The idea of relabeling (Van der Plaset al, 2011) is to first train an SRL system over aTL corpus labeled using direct projection (with VFfilter) and then use this SRL to relabel the corpus,effectively overwriting the projected labels with po-tentially less noisy predicted labels.We first present an analysis on relabeling in con-cert with our proposed filters (Sec.
3.1), which mo-tivates our bootstrap algorithm (Sec.
3.2).3.1 Analysis of Relabeling ApproachWe use the same experimental setup as in Sec.
2, andproduce a labeled French corpus for each filtered an-notation method.
We then train an off-the-shelf SRLsystem (Bj?orkelund et al, 2009) on each generatedcorpus and use it to relabel the corpus.We measure precision and recall of each resultingTL corpus against Frenchgold(see Tab.
4).
Across all6French negations, for instance, are split into a particle anda connegative.
In the annotation scheme used in Frenchgold,particles and connegatives are labeled differently.PROJECTION PREDICATE ARGUMENTSRL training P R F1 P R F1DIRECT?
0.45 0.40 0.43 0.43 0.31 0.36relabel (SP) 0.49 0.57 0.53 0.52 0.43 0.47relabel (OW) 0.66 0.60 0.63 0.71 0.37 0.49VERB FILTER (VF)?
0.59 0.40 0.48 0.53 0.31 0.39relabel (SP) 0.57 0.55 0.56 0.61 0.42 0.50relabel (OW)0.56 0.55 0.56 0.69 0.31 0.43(Van der Plas et al, 2011)PROPOSED (TF+RH)?
0.88 0.36 0.51 0.75 0.20 0.31relabelfull data(SP) 0.83 0.58 0.68 0.75 0.41 0.53relabelfull data(OW) 0.78 0.51 0.62 0.73 0.35 0.47relabelcomp.
sent.
(SP) 0.80 0.64 0.71 0.68 0.48 0.56relabelcomp.
sent.
(OW) 0.62 0.60 0.61 0.55 0.40 0.47bootstrap (iter.
3) 0.78 0.68 0.73 0.71 0.55 0.62bootstrap (terminate)0.77 0.70 0.73 0.64 0.60 0.62Table 4: Experiments on Frenchgold, with different projectionand SRL training methods.
SP=Supplement; OW=Overwrite.experiments, relabeling consistently improves recallover projection.
The results also show how differentfactors affect the performance of relabeling.Supplement vs. Overwrite Projected LabelsThe labels produced by the trained SRL can be usedto either overwrite projected labels as in (Van derPlas et al, 2011), or to supplement them (supply-ing labels only for words w/o projected labels).Whether to overwrite or supplement depends onwhether labels produced by the trained SRL are ofhigher quality than the projected labels.
We find thatwhile predicted labels are of higher precision thandirectly projected labels, they are of lower precisionthan labels post filtered projection.
Therefore, forfiltered projection, it makes more sense to allow pre-dicted labels to only supplement projected labels.Impact of Sampling Method We are further in-terested in learning the impact of sampling the dataon the quality of relabeling.
For the best filter foundearlier (TF+RH), we compare SRL trained on theentire data set (full data) with SRL trained only on thesubset of completely annotated sentences (comp.
sent.
),where completeness is defined as:Definition 1.
A direct component of a labeled sen-tence sTLis either a verb in sTLor a syntactic depen-dent of a verb.
Then sTLis k-complete if sTLcontainsequal to or fewer than k unlabeled direct compo-401Algorithm 1 Bootstrap learning algorithmRequire: Corpus CTLwith initial set of labels LTL, and resam-pling threshold function k(i);for i = 1 to ?
doLet ki= k(i);Let CTLcomp= {w ?
CTL: w ?
sTL, sTLis ki-complete};Let LTLcompbe subset of LTLappearing on CTLcomp;Train an SRL on (CTLcomp, LTLcomp);Use the SRL to produce label set LTLnewon CTL;Let CTLno.lab= {w ?
CTL: w not labelled by LTL};Let LTLsupplbe subset of LTLnewappearing on CTLno.lab;if LTLsuppl= ?
thenReturn the SRL;end ifLet LTL= LTL?
LTLsuppl;end fornents.
0-complete is abbreviated as complete.We observe that for TF+RH, when new labelssupplement projected labels, relabeling over com-plete sentences results in better recall at slightly re-duced precision, while including incomplete sen-tences into the training data reduces recall, but im-proves precision.
While this finding may seemcounterintuitive, it can be explained by how statis-tical SRL works.
A densely labeled training data(such as comp.
sent.)
usually results in an SRL that gen-erates densely labeled sentences, resulting in betterrecall but poorer precision.
On the other hand, train-ing data that is sparsely labeled results in an SRLthat weighs the option of not assigning a label withhigher probability, resulting in better precision andpoorer recall.
In short, one can control the trade-off between precision and recall of SRL output bymanipulating the completeness of the training data.3.2 Bootstrap LearningBuilding on the observation that we can sample datain such a way as to either favor precision or re-call, we propose a bootstrapping algorithm to trainan SRL iteratively over k-complete subsets of thedata which are supplemented by high precision la-bels produced from previous iteration.
The detailedalgorithm is depicted in Algorithm 1.Resampling Threshold Our goal is to use bootstraplearning to improve recall without sacrificing toomuch precision.Proposition 1.
Under any resampling threshold,the set of labels LTLincreases monotonically in eachiteration of Algorithm 1.Figure 3: Values at each bootstrap iteration.Since Prop.
1 guarantees the increase of the setof labels, we need to select a resampling function tofavor precision while improving recall.
Specifically,we use the formula k(i) = max(k0?
i, 0), wherek0is sufficiently large.
Since the precision of labelsgenerated by the SRL is lower than the precision oflabels obtained from filtered projection, the preci-sion of the training data is expected to decrease withthe increase in recall.
Therefore, starting with a highk seeks to ensure high precision labels are added tothe training data in the first iterations.
Decreasing kin each iteration seeks to ensure that resampling isdone in an increasingly restrictive way to ensure thatonly high-quality annotated sentences are added tothe training data, thus maintaining a high confidencein the learned SRL model.3.3 Effectiveness of BootstrappingWe experimentally evaluate the effectiveness of ourmodel with k0= 9.7As shown in Tab 4, boot-strapping outperforms relabeling, producing labelswith best overall quality in terms of F1measure andrecall for both predicates and arguments, with a rel-atively small cost in precision.While Algorithm 1 guarantees the increase of re-call (Prop.
1), it provides no such guarantee on pre-cision.
Therefore, it is important to experimentallydecide an early termination cutoff before the SRLgets overtrained.
To do so, we evaluated the per-formance of the bootstrapping algorithm at each it-eration (Fig.
3).
We observe that for the first 3 it-erations, F1-measure for both predicates and argu-ments rises due to large increase in recall whichoffsets the smaller drop in precision.
Then F1-measure remains stable, with recall rising and pre-7We found that setting k0to larger values had little impacton the final results .402LANGUAGE DEP.
PARSER DATA SET #SENTENCEArabic STANFORD UN 481KChinese MATE-G UN 2,986KFrench MATE-T UN 2,542KGerman MATE-T Europarl 560KHindi MALT Hindencorp 54KRussian MALT UN 2,638KSpanish MATE-G UN 2,304KTable 5: Experimental setup .Dependency parsers: STANFORD: (Green and Manning, 2010), MATE-G:(Bohnet, 2010), MATE-T: (Bohnet and Nivre, 2012), MALT: (Nivre et al, 2006).Parallel corpora: UN: (Rafalovitch et al, 2009), Europarl: (Koehn, 2005),Hindencorp: (Bojar et al, 2014).
Word alignment: The UN corpus is alreadyword-aligned.
For others, we use the Berkeley Aligner (DeNero and Liang, 2007).cision falling slightly at each iteration until conver-gence.
To optimize precision and avoid overtrain-ing, we set an iteration cutoff of 3.
This combina-tion of TF+RH filters, bootstrapping with k0= 9and an iteration cutoff of 3 is used in the rest of ourevaluation (Sec.
4), denoted as FBbest.4 Multilingual ExperimentsWe use our method to generate Proposition Banksfor 7 languages and evaluate the generated re-sources.
We seek to answer the following ques-tions: (1) What is the estimated quality for the gen-erated PropBanks?
How well does the approachwork without language-specific adaptation?
(2) Arethere notable differences in quality from languageto language; if so, why?
We also present initial in-vestigations on how different factors affect the per-formance of our method.4.1 Experimental SetupData Tab.
5 lists the 7 different TLs and resourcesused in our experiments.8We chose these TLs be-cause (1) they are among top 10 most influential lan-guages in the world (Weber, 1997); and (2) we couldfind language experts to evaluate the results.
Englishis used as SL in all our experiments.Approach Tested For each TL, we used FBbest(Sec.
3.3) to generate a corpus with semantic la-bels.
From each TL corpus, we extracted all com-plete sentences to form the generated PropBanks.8From each parallel corpus, we only keep sentences that areconsidered well-formed based on a set of standard heuristics.For example, we require a well-formed sentence to end in punc-tuation and not to contain certain special characters.
For Ara-bic, as the dependency parser we use has relatively poor parsingaccuracy, we additionally require sentences to be shorter than100 characters.PREDICATE ARGUMENTLANG.
Match P R F1 P R F1 Agr ?Arabicpart.
0.97 0.89 0.93 0.86 0.69 0.77 0.92 0.87exact 0.97 0.89 0.93 0.67 0.63 0.65 0.85 0.77Chinesepart.
0.97 0.88 0.92 0.93 0.83 0.88 0.95 0.91exact 0.97 0.88 0.92 0.83 0.81 0.82 0.92 0.86Frenchpart.
0.95 0.92 0.94 0.92 0.76 0.83 0.97 0.95exact 0.95 0.92 0.94 0.86 0.74 0.8 0.95 0.91Germanpart.
0.96 0.92 0.94 0.95 0.73 0.83 0.95 0.91exact 0.96 0.92 0.94 0.91 0.73 0.81 0.92 0.86Hindipart.
0.91 0.68 0.78 0.93 0.66 0.77 0.94 0.88exact 0.91 0.68 0.78 0.58 0.54 0.56 0.81 0.69Russianpart.
0.96 0.94 0.95 0.91 0.68 0.78 0.97 0.94exact 0.96 0.94 0.95 0.79 0.65 0.72 0.93 0.89Spanishpart.
0.96 0.93 0.95 0.85 0.74 0.79 0.91 0.85exact 0.96 0.93 0.95 0.75 0.72 0.74 0.85 0.77Table 6: Estimated precision and recall over seven languages.Manual Evaluation While a gold annotated cor-pus for French (Frenchgold) was available for ourexperiments in the previous Sections, no such re-sources existed for the other TLs we wished to eval-uate.
We therefore chose to conduct a manual eval-uation for each TL, each executed identically: Foreach TL we randomly selected 100 complete sen-tences with their generated semantic labels and as-signed them to two language experts who were in-structed to evaluate the semantic labels (based ontheir English descriptions) for the predicates andtheir core arguments.
For each label, they wereasked to determine (1) whether the label is correct;(2) if yes, then whether the boundary of the labeledconstituent is correct: If also yes, mark the label asfully correct, otherwise as partially correct.Metrics We used the standard measures of preci-sion, recall, and F1 to measure the performance ofthe SRLs, with the following two schemes: (1) Ex-act: Only fully correct labels are considered as truepositives; (2) Partial: Both fully and partially cor-rect matches are considered as true positives.94.2 Experimental ResultsTab.
6 summarizes the estimated quality of seman-tic labels generated by our method for all seven TL.As can be seen, our method performed well for all9Note that since the manually evaluated semantic labels areonly a small fraction of the labels generated, the performancenumbers obtained from manual evaluation is only an estimateof the actual quality for the generated resources.Thus the num-bers obtained based on manual evaluation cannot be directlycompared against the numbers computed over Frenchgold.403PROPBANK #COMPLETE %COMPLETE #VERBSArabic 68.512 14% 330Chinese 419,140 14% 1,102French 248.256 10% 1145German 44.007 8% 537Hindi 1.623 3% 59Russian 496.033 19% 1.349Spanish 165.582 7% 909Table 7: Characteristics of the generated PropBanks.seven languages and generated high quality seman-tics labels across the board.
For predicate labels,the precision is over 95% and the recall is over 85%for all languages except for Hindi.
For argumentlabels, when considering partially correct matches,the precision is at least 85% (above 90% for mostlanguages) and the recall is between 66% to 83%for all the languages.
These encouraging resultsobtained from a diverse set of languages impliesthe generalizability of our method.
In addition, theinter-annotator agreement is very high for all thelanguages, indicating that the results obtained basedon manual evaluation are very reliable.In addition, we make a number of interesting ob-servations:Dependency Parsing Accuracy The precision forexact argument labels is significantly below partialmatches, particularly for Hindi (?35 pp) and Ara-bic (?19 pp).
Since argument boundaries are deter-mined syntactically, such errors are caused by de-pendency parsing.
The fact that Hindi and Arbicsuffer the most from this issue is consistent withthe poorer performance of their dependency parserscompared to other languages (Nivre et al, 2006;Green and Manning, 2010).Hindi as the Main Outlier The results for Hindiare much worse than the results for other languages.Besides the poorer dependency parser performance,the size of the parallel corpus used could be a fac-tor: Hindencorp is one to two orders of magni-tude smaller than the other corpora.
The qualityof the parallel corpus could be a reason as well:Hindencorp was collected from various sources,while both UN and Europarl were extracted fromgovernmental proceedings.Language-specific Errors Certain errors occurmore frequently in some languages than others.
Anexample are deverbal nouns in Chinese (Xue, 2006)in formal passive constructions with support verb?.
Since we currently only consider verbs for pred-PREDICATE ARGUMENTSAMPLE SIZE P R F1 P R F1100% 0.87 0.81 0.84 0.86 0.74 0.810% 0.88 0.8 0.84 0.87 0.72 0.791% 0.9 0.76 0.83 0.89 0.67 0.76Table 8: Estimated impact of downsampling parallel corpus.PREDICATE ARGUMENTHEURISTIC P R F1 P R F1none?0.87 0.81 0.84 0.86 0.74 0.8none?
?0.88 0.8 0.84 0.76 0.65 0.7customization?0.87 0.81 0.84 0.9 0.74 0.81Table 9: Impact of English SRLs (?=CLEARNLP,?
?=MATE-SRL) and language-spec.
customization (filter synt.
expletive).icate labels, predicate labels are projected onto thesupport verbs instead of the deverbal nouns.
Sucherrors appear for light verb constructions in all lan-guages, but particularly affect Chinese due to thehigh frequency of this passive construction in theUN corpus.Low Fraction of Complete Sentences As Tab.
7shows, the fraction of complete sentences in thegenerated PropBanks is rather low, indicating theimpact of moderate recall on the size of generatedPropBanks.
Especially for languages for which onlysmall parallel corpora are available, such as Hindi,this points to the need to address recall issues in fu-ture work.4.3 Additional ExperimentsThe observations made in Sec.
4.2 suggests a fewfactors that may potentially affect the performanceof our method.
To better understand their impact,we conducted the following initial investigation.SRL models produced in this set of experimentswere evaluated using Frenchgold, sampled and eval-uated in the same way as other experiments in thissection for comparability.Data Size We varied the data size for French bydownsampling the UN corpus.
As one can see fromTab.
8, downsampling the dataset by one order ofmagnitude (to 250k sentences) only slightly affectsprecision, while downsampling to 25k sentences hasa more pronounced but still small impact on recall.It appears that data size does not have significantimpact on the performance of our method.Language-specific Customizations While ourmethod is language-agnostic, intuitively language-specific customization can be helpful in address-404ing language-specific errors.
As an initial exper-iment, we added a simple heuristic to filter outFrench verbs that are commonly used for ?existen-tial there?
constructions, as one type of commonerrors for French involves the syntactic expletiveil (Danlos, 2005) in ?existential there?
constructionssuch as il faut (see Fig.
1 (TL sentence) for an ex-ample) wrongly labeled with with role information.As shown in Tab.
9, this simple customization re-sults in a small increase in precision, suggesting thatlanguage-specific customization can be helpful.Quality of English SRL As noted in Sec.
2.5, errorsmade by English SRL are often prorogated to the TLvia projection.
To assess the impact of English SRLquality, we used two different English SRL systems:CLEARNLP and MATE-SRL.
As can be seen fromTab.
9, the impact of English SRL quality is sub-stantial on argument labeling.4.4 Multilingual PropBanksTo facilitate future research on multilingual SRL,we release the created PropBanks for all 7 languagesto the research community to encourage further re-search.
Tab.
7 gives an overview over the resources.5 Related WorkAnnotation Projection in Parallel Corpora totrain monolingual tools for new languages was in-troduced in the context of learning a PoS tag-ger (Yarowsky et al, 2001).
Similar in spirit to ourapproach of using filters to increase the precisionof projected labels, recent work (T?ackstr?om et al,2013) uses token and type constraints to guide learn-ing in cross-lingual PoS tagging.Projection of Semantic Labels was considered forFrameNet (Baker et al, 1998) in (Pad?o and Lapata,2009; Basili et al, 2009).
Recently, however, mostwork in the area focuses on PropBank, which hasbeen identified as a more suitable annotation schemefor joint syntactic-semantics settings due to broadercoverage (Merlo and van der Plas, 2009), and wasshown to be usable for languages other than En-glish (Monachesi et al, 2007).Direct projection of PropBank annotations wasconsidered in (Van der Plas et al, 2011).
Our ap-proach significantly outperforms theirs in terms ofrecall and F1for both predicates and arguments(Section 3).
A approach was proposed in (Van derPlas et al, 2014) in which information is aggregatedat the corpus level, resulting in a significantly bet-ter SRL corpus for French.
However, this approachhas several practical limitations: (1) it does not con-sider the problem of argument identification of SRLsystems, treating arguments as already given; (2) itgenerates rules for the argument classification steppreferably from manually annotated data; (3) it hasbeen demonstrated for a single language (French),and was not applied to any other language.
In con-trast, our approach trains an SRL system for bothpredicate and argument labels, in a completely au-tomatic fashion.
Furthermore, we have applied ourapproach to generate PropBanks for 7 languages andconducted experiments that indicate a high F1mea-sure for all languages (Section 4).Other Related Work A number of approaches suchas model transfer (Kozhevnikov and Titov, 2013)and role induction (Titov and Klementiev, 2012)exist for the argument classification step in the SRLpipeline.
In contrast, our work addresses the fullSRL pipeline and seeks to generate SRL resourcesfor TLs with English PropBank labels.6 ConclusionWe proposed a two-staged method to construct mul-tilingual SRL resources using monolingual SRL andparallel data and showed that our method outper-forms previous approaches in both precision andrecall.
More importantly, through comprehensiveexperiments over seven languages from three lan-guage families, we show that our proposed methodworks well across different languages without anylanguage specific customization.
Preliminary re-sults from additional experiments indicate that bet-ter English SRL and language-specific customiza-tion can further improve the results, which we aimto investigate in future work.
A qualitative com-parison against existing or under-construction Prop-Banks for Chinese (Xue, 2008), Hindi (Vaidya et al,2011) or Arabic (Zaghouani et al, 2010) may be in-teresting, both for comparison of resources and fordefining language-specific customizations.
In ad-dition, we plan to expand our experiments both tomore languages as well as NomBank (Meyers et al,2004)-style noun labels.405References[Baker et al1998] Collin F Baker, Charles J Fillmore, andJohn B Lowe.
1998.
The berkeley framenet project.In Proceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics and 17th In-ternational Conference on Computational Linguistics-Volume 1, pages 86?90.
Association for Computa-tional Linguistics.
[Basili et al2009] Roberto Basili, Diego De Cao, DaniloCroce, Bonaventura Coppola, and Alessandro Mos-chitti.
2009.
Cross-language frame semantics trans-fer in bilingual corpora.
In Computational Linguis-tics and Intelligent Text Processing, pages 332?345.Springer.
[Bj?orkelund et al2009] Anders Bj?orkelund, LoveHafdell, and Pierre Nugues.
2009.
Multilingual se-mantic role labeling.
In Proceedings of the ThirteenthConference on Computational Natural LanguageLearning: Shared Task, pages 43?48.
Association forComputational Linguistics.
[Bohnet and Nivre2012] Bernd Bohnet and JoakimNivre.
2012.
A transition-based system for jointpart-of-speech tagging and labeled non-projectivedependency parsing.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 1455?1465.
Associationfor Computational Linguistics.
[Bohnet2010] Bernd Bohnet.
2010.
Very high accuracyand fast dependency parsing is not a contradiction.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 89?97.
Associationfor Computational Linguistics.
[Bojar et al2014] Ond?rej Bojar, Vojt?ech Diatka, PavelRychl`y, Pavel Stra?n?ak, V?
?t Suchomel, Ale?s Tamchyna,Daniel Zeman, et al 2014.
Hindencorp?hindi-englishand hindi-only corpus for machine translation.
InProceedings of the Ninth International Conference onLanguage Resources and Evaluation.
[Choi and McCallum2013] Jinho D. Choi and AndrewMcCallum.
2013.
Transition-based dependency pars-ing with selectional branching.
In Proceedings of the51st Annual Meeting of the Association for Computa-tional Linguistics.
[Danlos2005] Laurence Danlos.
2005.
Automatic recog-nition of french expletive pronoun occurrences.
InNatural language processing.
Proceedings of the 2ndInternational Joint Conference on Natural LanguageProcessing (IJCNLP-05), pages 73?78.
Citeseer.
[DeNero and Liang2007] John DeNero and Percy Liang.2007.
The Berkeley Aligner.
http://code.google.com/p/berkeleyaligner/.
[Erk et al2003] K. Erk, A. Kowalski, S. Pado, andS.
Pinkal.
2003.
Towards a resource for lexical se-mantics: A large german corpus with extensive se-mantic annotation.
In ACL.
[Green and Manning2010] Spence Green and Christo-pher D Manning.
2010.
Better arabic parsing: Base-lines, evaluations, and analysis.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, pages 394?402.
Association for Compu-tational Linguistics.
[Haji?c et al2009] Jan Haji?c, Massimiliano Cia-ramita, Richard Johansson, Daisuke Kawahara,Maria Ant`onia Mart?
?, Llu?
?s M`arquez, Adam Mey-ers, Joakim Nivre, Sebastian Pad?o, Jan?St?ep?anek,et al 2009.
The conll-2009 shared task: Syntacticand semantic dependencies in multiple languages.In Proceedings of the Thirteenth Conference onComputational Natural Language Learning: SharedTask, pages 1?18.
Association for ComputationalLinguistics.
[Koehn2005] Philipp Koehn.
2005.
Europarl: A paral-lel corpus for statistical machine translation.
In MTsummit, volume 5, pages 79?86.
[Kozhevnikov and Titov2013] Mikhail Kozhevnikov andIvan Titov.
2013.
Cross-lingual transfer of semanticrole labeling models.
In ACL (1), pages 1190?1200.
[Maqsud et al2014] Umar Maqsud, Sebastian Arnold,Michael H?ulfenhaus, and Alan Akbik.
2014.
Ner-dle: Topic-specific question answering using wikiaseeds.
In Lamia Tounsi and Rafal Rak, editors, COL-ING 2014, 25th International Conference on Compu-tational Linguistics, Proceedings of the ConferenceSystem Demonstrations, August 23-29, 2014, Dublin,Ireland, pages 81?85.
ACL.
[Merlo and van der Plas2009] Paola Merlo and Lonnekevan der Plas.
2009.
Abstraction and generalisation insemantic role labels: Propbank, verbnet or both?
InACL 2009, pages 288?296.
[Meyers et al2004] Adam Meyers, Ruth Reeves, Cather-ine Macleod, Rachel Szekely, Veronika Zielinska,Brian Young, and Ralph Grishman.
2004.
Annotat-ing noun argument structure for nombank.
In LREC,volume 4, pages 803?806.
[Monachesi et al2007] Paola Monachesi, GerwertStevens, and Jantine Trapman.
2007.
Adding seman-tic role annotation to a corpus of written dutch.
InProceedings of the Linguistic Annotation Workshop,LAW ?07, pages 77?84.
[Nivre et al2006] Joakim Nivre, Johan Hall, and JensNilsson.
2006.
Maltparser: A data-driven parser-generator for dependency parsing.
In Proceedings ofLREC, volume 6, pages 2216?2219.406[Pad?o and Lapata2009] Sebastian Pad?o and Mirella Lap-ata.
2009.
Cross-lingual annotation projection forsemantic roles.
Journal of Artificial Intelligence Re-search, 36(1):307?340.
[Pado2007] Sebastian Pado.
2007.
Cross-Lingual Anno-tation Projection Models for Role-Semantic Informa-tion.
Ph.D. thesis, Saarland University.
MP.
[Palmer et al2005] Martha Palmer, Daniel Gildea, andPaul Kingsbury.
2005.
The proposition bank: Anannotated corpus of semantic roles.
Computationallinguistics, 31(1):71?106.
[Rafalovitch et al2009] Alexandre Rafalovitch, RobertDale, et al 2009.
United nations general assemblyresolutions: A six-language parallel corpus.
In Pro-ceedings of the MT Summit, volume 12, pages 292?299.
[Schuler2005] Karin Kipper Schuler.
2005.
Verbnet: ABroad-coverage, Comprehensive Verb Lexicon.
Ph.D.thesis, University of Pennsylvania.
[Shen and Lapata2007] Dan Shen and Mirella Lapata.2007.
Using semantic roles to improve question an-swering.
In EMNLP-CoNLL, pages 12?21.
Citeseer.
[T?ackstr?om et al2013] Oscar T?ackstr?om, Dipanjan Das,Slav Petrov, Ryan McDonald, and Joakim Nivre.2013.
Token and type constraints for cross-lingualpart-of-speech tagging.
Transactions of the Associa-tion for Computational Linguistics, 1:1?12.
[Titov and Klementiev2012] Ivan Titov and AlexandreKlementiev.
2012.
Crosslingual induction of seman-tic roles.
In ACL, pages 647?656.
[Vaidya et al2011] Ashwini Vaidya, Jinho D Choi,Martha Palmer, and Bhuvana Narasimhan.
2011.Analysis of the hindi proposition bank using depen-dency structure.
In Proceedings of the 5th LinguisticAnnotation Workshop, pages 21?29.
Association forComputational Linguistics.
[Van der Plas et al2010] Lonneke Van der Plas, TanjaSamard?zi?c, and Paola Merlo.
2010.
Cross-lingual va-lidity of propbank in the manual annotation of french.In Proceedings of the Fourth Linguistic AnnotationWorkshop, pages 113?117.
Association for Computa-tional Linguistics.
[Van der Plas et al2011] Lonneke Van der Plas, PaolaMerlo, and James Henderson.
2011.
Scaling up auto-matic cross-lingual semantic role annotation.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers-Volume 2, pages 299?304.Association for Computational Linguistics.
[Van der Plas et al2014] Lonneke Van der Plas, Mari-anna Apidianaki, Rue John von Neumann, and Chen-hua Chen.
2014.
Global methods for cross-lingual se-mantic role and predicate labelling.
In Proceedings ofthe 25th International Conference on ComputationalLinguistics (COLING 2014), pages 1279?1290.
Asso-ciation for Computational Linguistics.
[Weber1997] George Weber.
1997.
Top languages: Theworld?s 10 most influential languages.
Language To-day, December.
[Xue2006] Nianwen Xue.
2006.
Semantic role labelingof nominalized predicates in chinese.
In Proceedingsof the main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 431?438.
Association for Computational Linguistics.
[Xue2008] Nianwen Xue.
2008.
Labeling chinese predi-cates with semantic roles.
Computational linguistics,34(2):225?255.
[Yarowsky et al2001] David Yarowsky, Grace Ngai, andRichard Wicentowski.
2001.
Inducing multilingualtext analysis tools via robust projection across alignedcorpora.
In Proceedings of the first internationalconference on Human language technology research,pages 1?8.
Association for Computational Linguistics.
[Zaghouani et al2010] Wajdi Zaghouani, Mona Diab,Aous Mansouri, Sameer Pradhan, and Martha Palmer.2010.
The revised arabic propbank.
In Proceedingsof the Fourth Linguistic Annotation Workshop, pages222?226.
Association for Computational Linguistics.407
