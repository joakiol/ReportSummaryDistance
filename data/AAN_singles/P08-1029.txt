Proceedings of ACL-08: HLT, pages 245?253,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExploiting Feature Hierarchy for Transfer Learning in Named EntityRecognitionAndrew Arnold, Ramesh Nallapati and William W. CohenMachine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA{aarnold, nmramesh, wcohen}@cs.cmu.eduAbstractWe present a novel hierarchical prior struc-ture for supervised transfer learning in namedentity recognition, motivated by the commonstructure of feature spaces for this task acrossnatural language data sets.
The problem oftransfer learning, where information gained inone learning task is used to improve perfor-mance in another related task, is an importantnew area of research.
In the subproblem of do-main adaptation, a model trained over a sourcedomain is generalized to perform well on a re-lated target domain, where the two domains?data are distributed similarly, but not identi-cally.
We introduce the concept of groupsof closely-related domains, called genres, andshow how inter-genre adaptation is related todomain adaptation.
We also examine multi-task learning, where two domains may be re-lated, but where the concept to be learned ineach case is distinct.
We show that our priorconveys useful information across domains,genres and tasks, while remaining robust tospurious signals not related to the target do-main and concept.
We further show that ourmodel generalizes a class of similar hierarchi-cal priors, smoothed to varying degrees, andlay the groundwork for future exploration inthis area.1 Introduction1.1 Problem definitionConsider the task of named entity recognition(NER).
Specifically, you are given a corpus of newsarticles in which all tokens have been labeled as ei-ther belonging to personal name mentions or not.The standard supervised machine learning problemis to learn a classifier over this training data that willsuccessfully label unseen test data drawn from thesame distribution as the training data, where ?samedistribution?
could mean anything from having thetrain and test articles written by the same author tohaving them written in the same language.
Havingsuccessfully trained a named entity classifier on thisnews data, now consider the problem of learning toclassify tokens as names in e-mail data.
An intuitivesolution might be to simply retrain the classifier, denovo, on the e-mail data.
Practically, however, large,labeled datasets are often expensive to build and thissolution would not scale across a large number ofdifferent datasets.Clearly the problems of identifying names innews articles and e-mails are closely related, andlearning to do well on one should help your per-formance on the other.
At the same time, however,there are serious differences between the two prob-lems that need to be addressed.
For instance, cap-italization, which will certainly be a useful featurein the news problem, may prove less informative inthe e-mail data since the rules of capitalization arefollowed less strictly in that domain.These are the problems we address in this paper.In particular, we develop a novel prior for namedentity recognition that exploits the hierarchical fea-ture space often found in natural language domains(?1.2) and allows for the transfer of informationfrom labeled datasets in other domains (?1.3).
?2introduces the maximum entropy (maxent) and con-ditional random field (CRF) learning techniques em-ployed, along with specifications for the design andtraining of our hierarchical prior.
Finally, in ?3 wepresent an empirical investigation of our prior?s per-formance against a number of baselines, demonstrat-ing both its effectiveness and robustness.1.2 Hierarchical feature treesIn many NER problems, features are often con-structed as a series of transformations of the inputtraining data, performed in sequence.
Thus, if ourtask is to identify tokens as either being (O)utside or(I)nside person names, and we are given the labeled245sample training sentence:O O O O O IGive the book to Professor Caldwell(1)one such useful feature might be: Is the token oneslot to the left of the current token Professor?We can represent this symbolically as L.1.Professorwhere we describe the whole space of useful featuresof this form as: {direction = (L)eft, (C)urrent,(R)ight}.
{distance = 1, 2, 3, ...}.
{value = Pro-fessor, book, ...}.
We can conceptualize this struc-ture as a tree, where each slot in the symbolic nameof a feature is a branch and each period between slotsrepresents another level, going from root to leaf asread left to right.
Thus a subsection of the entire fea-ture tree for the token Caldwell could be drawnas in Figure 1 (zoomed in on the section of the treewhere the L.1.Professor feature resides).directionLCRdistance1 2 ...... ...valueProfessorbook...... ...true false ...Figure 1: Graphical representation of a hierarchical fea-ture tree for token Caldwell in example Sentence 1.Representing feature spaces with this kind of tree,besides often coinciding with the explicit languageused by common natural language toolkits (Cohen,2004), has the added benefit of allowing a model toeasily back-off, or smooth, to decreasing levels ofspecificity.
For example, the leaf level of the fea-ture tree for our sample Sentence 1 tells us that theword Professor is important, with respect to la-beling person names, when located one slot to theleft of the current word being classified.
This maybe useful in the context of an academic corpus, butmight be less useful in a medical domain where theword Professor occurs less often.
Instead, wemight want to learn the related feature L.1.Dr.
Infact, it might be useful to generalize across multipledomains the fact that the word immediately preced-ing the current word is often important with respectLeftToken.*LeftToken.IsWord.*LeftToken.IsWord.IsTitle.*LeftToken.IsWord.IsTitle.equals.
*LeftToken.IsWord.IsTitle.equals.mrTable 1: A few examples of the feature hierarchyto the named entity status of the current word.
Thisis easily accomplished by backing up one level froma leaf in the tree structure to its parent, to representa class of features such as L.1.*.
It has been shownempirically that, while the significance of particularfeatures might vary between domains and tasks, cer-tain generalized classes of features retain their im-portance across domains (Minkov et al, 2005).
Bybacking-off in this way, we can use the feature hier-archy as a prior for transferring beliefs about the sig-nificance of entire classes of features across domainsand tasks.
Some examples illustrating this idea areshown in table 1.1.3 Transfer learningWhen only the type of data being examined is al-lowed to vary (from news articles to e-mails, forexample), the problem is called domain adapta-tion (Daume?
III and Marcu, 2006).
When the taskbeing learned varies (say, from identifying personnames to identifying protein names), the problemis called multi-task learning (Caruana, 1997).
Bothof these are considered specific types of the over-arching transfer learning problem, and both seemto require a way of altering the classifier learnedon the first problem (called the source domain, orsource task) to fit the specifics of the second prob-lem (called the target domain, or target task).More formally, given an example x and a classlabel y, the standard statistical classification taskis to assign a probability, p(y|x), to x of belong-ing to class y.
In the binary classification case thelabels are Y ?
{0, 1}.
In the case we examine,each example xi is represented as a vector of bi-nary features (f1(xi), ?
?
?
, fF (xi)) where F is thenumber of features.
The data consists of two dis-joint subsets: the training set (Xtrain, Ytrain) ={(x1, y1) ?
?
?
, (xN , yN )}, available to the model forits training and the test set Xtest = (x1, ?
?
?
, xM ),upon which we want to use our trained classifier tomake predictions.246In the paradigm of inductive learning,(Xtrain, Ytrain) are known, while both Xtest andYtest are completely hidden during training time.
Inthis cases Xtest and Xtrain are both assumed to havebeen drawn from the same distribution, D. In thesetting of transfer learning, however, we would liketo apply our trained classifier to examples drawnfrom a distribution different from the one uponwhich it was trained.
We therefore assume thereare two different distributions, Dsource and Dtarget,from which data may be drawn.
Given this notationwe can then precisely state the transfer learningproblem as trying to assign labels Y targettest to testdata Xtargettest drawn from Dtarget, given trainingdata (Xsourcetrain , Y sourcetrain ) drawn from Dsource.In this paper we focus on two subproblems oftransfer learning:?
domain adaptation, where we assume Y (the setof possible labels) is the same for both Dsourceand Dtarget, while Dsource and Dtarget them-selves are allowed to vary between domains.?
multi-task learning (Ando and Zhang, 2005;Caruana, 1997; Sutton and McCallum, 2005;Zhang et al, 2005) in which the task (and labelset) is allowed to vary from source to target.Domain adaptation can be further distinguished bythe degree of relatedness between the source and tar-get domains.
For example, in this work we groupdata collected in the same medium (e.g., all anno-tated e-mails or all annotated news articles) as be-longing to the same genre.
Although the specificboundary between domain and genre for a particu-lar set of data is often subjective, it is nevertheless auseful distinction to draw.One common way of addressing the transferlearning problem is to use a prior which, in conjunc-tion with a probabilistic model, allows one to spec-ify a priori beliefs about a distribution, thus bias-ing the results a learning algorithm would have pro-duced had it only been allowed to see the trainingdata (Raina et al, 2006).
In the example from ?1.1,our belief that capitalization is less strict in e-mailsthan in news articles could be encoded in a prior thatbiased the importance of the capitalizationfeature to be lower for e-mails than news articles.In the next section we address the problem of howto come up with a suitable prior for transfer learningacross named entity recognition problems.2 Models considered2.1 Basic Conditional Random FieldsIn this work, we will base our work on Condi-tional Random Fields (CRF?s) (Lafferty et al, 2001),which are now one of the most preferred sequentialmodels for many natural language processing tasks.The parametric form of the CRF for a sentence oflength n is given as follows:p?
(Y = y|x) =1Z(x) exp(n?i=1F?j=1fj(x, yi)?j)(2)where Z(x) is the normalization term.
CRF learns amodel consisting of a set of weights ?
= {?1...?F }over the features so as to maximize the conditionallikelihood of the training data, p(Ytrain|Xtrain),given the model p?.2.2 CRF with Gaussian priorsTo avoid overfitting the training data, these ?
?s areoften further constrained by the use of a Gaussianprior (Chen and Rosenfeld, 1999) with diagonal co-variance, N (?, ?2), which tries to maximize:argmax?N?k=1(log p?(yk|xk))?
?F?j(?j ?
?j)22?2jwhere ?
> 0 is a parameter controlling the amountof regularization, and N is the number of sentencesin the training set.2.3 Source trained priorsOne recently proposed method (Chelba and Acero,2004) for transfer learning in Maximum Entropymodels 1 involves modifying the ?
?s of this Gaussianprior.
First a model of the source domain, ?source,is learned by training on {Xsourcetrain , Y sourcetrain }.
Then amodel of the target domain is trained over a limitedset of labeled target data{Xtargettrain , Ytargettrain}, but in-stead of regularizing this ?target to be near zero (i.e.setting ?
= 0), ?target is instead regularized to-wards the previously learned source values ?source(by setting ?
= ?source, while ?2 remains 1) andthus minimizing (?target ?
?source)2.1Maximum Entropy models are special cases of CRFs thatuse the I.I.D.
assumption.
The method under discussion canalso be extended to CRF directly.247Note that, since this model requires Y targettrain in or-der to learn ?target, it, in effect, requires two distinctlabeled training datasets: one on which to train theprior, and another on which to learn the model?s fi-nal weights (which we call tuning), using the previ-ously trained prior for regularization.
If we are un-able to find a match between features in the trainingand tuning datasets (for instance, if a word appearsin the tuning corpus but not the training), we back-off to a standard N (0, 1) prior for that feature.3yx ii(1)(1)(1)Mw (1)1yx ii(Myx ii(M(2)2)(2)(3)3)(3)w w (1) w (1) w1 w w w1 w(1)2 3 4 (2) (2) (2)2 3 (3) (3)2z zz1 2Figure 2: Graphical representation of the hierarchicaltransfer model.2.4 New model: Hierarchical prior modelIn this section, we will present a new model thatlearns simultaneously from multiple domains, bytaking advantage of our feature hierarchy.We will assume that there are D domains onwhich we are learning simultaneously.
Let there beMd training data in each domain d. For our experi-ments with non-identically distributed, independentdata, we use conditional random fields (cf.
?2.1).However, this model can be extended to any dis-criminative probabilistic model such as the MaxEntmodel.
Let ?
(d) = (?
(d)1 , ?
?
?
, ?
(d)Fd ) be the param-eters of the discriminative model in the domain dwhere Fd represents the number of features in thedomain d.Further, we will also assume that the features ofdifferent domains share a common hierarchy repre-sented by a tree T , whose leaf nodes are the featuresthemselves (cf.
Figure 1).
The model parameters?
(d), then, form the parameters of the leaves of thishierarchy.
Each non-leaf node n ?
non-leaf(T ) ofthe tree is also associated with a hyper-parameter zn.Note that since the hierarchy is a tree, each node nhas only one parent, represented by pa(n).
Simi-larly, we represent the set of children nodes of a noden as ch(n).The entire graphical model for an example con-sisting of three domains is shown in Figure 2.The conditional likelihood of the entire trainingdata (y,x) = {(y(d)1 ,x(d)1 ), ?
?
?
, (y(d)Md ,x(d)Md)}Dd=1 isgiven by:P (y|x,w, z) ={ D?d=1Md?k=1P (y(d)k |x(d)k ,?(d))}???
?D?d=1Fd?f=1N (?
(d)f |zpa(f (d)), 1)???????
?n?TnonleafN (zn|zpa(n), 1)???
(3)where the terms in the first line of eq.
(3) representthe likelihood of data in each domain given their cor-responding model parameters, the second line repre-sents the likelihood of each model parameter in eachdomain given the hyper-parameter of its parent in thetree hierarchy of features and the last term goes overthe entire tree T except the leaf nodes.
Note that inthe last term, the hyper-parameters are shared acrossthe domains, so there is no product over d.We perform a MAP estimation for each model pa-rameter as well as the hyper-parameters.
Accord-ingly, the estimates are given as follows:?
(d)f =Md?i=1???
(d)f(logP (ydi |x(d)i ,?
(d)))+ zpa(f (d))zn =zpa(n) +?i?ch(n)(?|z)i1 + |ch(n)| (4)where we used the notation (?|z)i because node i,the child node of n, could be a parameter node ora hyper-parameter node depending on the positionof the node n in the hierarchy.
Essentially, in thismodel, the weights of the leaf nodes (model param-eters) depend on the log-likelihood as well as theprior weight of its parent.
Additionally, the weight248of each hyper-parameter node in the tree is com-puted as the average of all its children nodes and itsparent, resulting in a smoothing effect, both up anddown the tree.2.5 An approximate Hierarchical prior modelThe Hierarchical prior model is a theoretically wellfounded model for transfer learning through featureheirarchy.
However, our preliminary experimentsindicated that its performance on real-life data sets isnot as good as expected.
Although a more thoroughinvestigation needs to be carried out, our analysis in-dicates that the main reason for this phenomenon isover-smoothing.
In other words, by letting the infor-mation propagate from the leaf nodes in the hierar-chy all the way to the root node, the model loses itsability to discriminate between its features.As a solution to this problem, we propose anapproximate version of this model that weds ideasfrom the exact heirarchical prior model and theChelba model.As with the Chelba prior method in ?2.3, this ap-proximate hierarchical method also requires two dis-tinct data sets, one for training the prior and anotherfor tuning the final weights.
Unlike Chelba, wesmooth the weights of the priors using the feature-tree hierarchy presented in ?1.1, like the hierarchicalprior model.For smoothing of each feature weight, we chose toback-off in the tree as little as possible until we had alarge enough sample of prior data (measured as M ,the number of subtrees below the current node) onwhich to form a reliable estimate of the mean andvariance of each feature or class of features.
Forexample, if the tuning data set is as in Sentence1, but the prior contains no instances of the wordProfessor, then we would back-off and computethe prior mean and variance on the next higher levelin the tree.
Thus the prior for L.1.Professor wouldbe N (mean(L.1.
*), variance(L.1.
*)), where mean()and variance() of L.1.
* are the sample mean andvariance of all the features in the prior dataset thatmatch the pattern L.1.
* ?
or, put another way, all thesiblings of L.1.Professor in the feature tree.
If fewerthan M such siblings exist, we continue backing-off,up the tree, until an ancestor with sufficient descen-dants is found.
A detailed description of the approx-imate hierarchical algorithm is shown in table 2.Input: Dsource = (Xsourcetrain , Y sourcetrain )Dtarget = (Xtargettrain , Ytargettrain );Feature sets Fsource, F target;Feature HierarchiesHsource,HtargetMinimum membership size MTrain CRF using Dsource to obtainfeature weights ?sourceFor each feature f ?
F targetInitialize: node n = fWhile (n /?
Hsourceor |Leaves(Hsource(n))| ?M)and n 6= root(Htarget)n?
Pa(Htarget(n))Compute ?f and ?f using the sample{?sourcei | i ?
Leaves(Hsource(n))}Train Gaussian prior CRF using Dtarget as dataand {?f} and {?f} as Gaussian prior parameters.Output:Parameters of the new CRF ?target.Table 2: Algorithm for approximate hierarchical prior:Pa(Hsource(n)) is the parent of node n in feature hierar-chy Hsource; |Leaves(Hsource(n))| indicates the num-ber of leaf nodes (basic features) under a node n in thehierarchyHsource.It is important to note that this smoothed tree isan approximation of the exact model presented in?2.4 and thus an important parameter of this methodin practice is the degree to which one chooses tosmooth up or down the tree.
One of the benefitsof this model is that the semantics of the hierarchy(how to define a feature, a parent, how and whento back-off and up the tree, etc.)
can be specifiedby the user, in reference to the specific datasets andtasks under consideration.
For our experiments, thesemantics of the tree are as presented in ?1.1.The Chelba method can be thought of as a hier-archical prior in which no smoothing is performedon the tree at all.
Only the leaf nodes of theprior?s feature tree are considered, and, if no matchcan be found between the tuning and prior?s train-ing datasets?
features, a N (0, 1) prior is used in-stead.
However, in the new approximate hierarchicalmodel, even if a certain feature in the tuning datasetdoes not have an analog in the training dataset, wecan always back-off until an appropriate match isfound, even to the level of the root.Henceforth, we will use only the approximate hi-erarchical model in our experiments and discussion.249Table 3: Summary of data used in experimentsCorpus Genre TaskUTexas Bio ProteinYapex Bio ProteinMUC6 News PersonMUC7 News PersonCSPACE E-mail Person3 Investigation3.1 Data, domains and tasksFor our experiments, we have chosen five differ-ent corpora (summarized in Table 3).
Althougheach corpus can be considered its own domain (dueto variations in annotation standards, specific task,date of collection, etc), they can also be roughlygrouped into three different genres.
These are: ab-stracts from biological journals [UT (Bunescu et al,2004), Yapex (Franze?n et al, 2002)]; news articles[MUC6 (Fisher et al, 1995), MUC7 (Borthwick etal., 1998)]; and personal e-mails [CSPACE (Krautet al, 2004)].
Each corpus, depending on its genre,is labeled with one of two name-finding tasks:?
protein names in biological abstracts?
person names in news articles and e-mailsWe chose this array of corpora so that we couldevaluate our hierarchical prior?s ability to generalizeacross and incorporate information from a variety ofdomains, genres and tasks.In each case, each item (abstract, article or e-mail)was tokenized and each token was hand-labeled aseither being part of a name (protein or person) ornot, respectively.
We used a standard natural lan-guage toolkit (Cohen, 2004) to compute tens ofthousands of binary features on each of these to-kens, encoding such information as capitalizationpatterns and contextual information from surround-ing words.
This toolkit produces features of the typedescribed in ?1.2 and thus was amenable to our hi-erarchical prior model.
In particular, we chose touse the simplest default, out-of-the-box feature gen-erator and purposefully did not use specifically en-gineered features, dictionaries, or other techniquescommonly employed to boost performance on suchtasks.
The goal of our experiments was to see towhat degree named entity recognition problems nat-urally conformed to hierarchical methods, and notjust to achieve the highest performance possible.0.10.20.30.40.50.60.70 20 40 60 80 100F1Percent of target-domain data used for tuningIntra-genre transfer performance evaluated on MUC6(a) GAUSS: tuned on MUC6(b) CAT: tuned on MUC6+7(c) HIER: MUC6+7 prior, tuned on MUC6(d) CHELBA: MUC6+7 prior, tuned on MUC6Figure 3: Adding a relevant HIER prior helps comparedto the GAUSS baseline ((c) > (a)), while simply CAT?ingor using CHELBA can hurt ((d) ?
(b) < (a), except withvery little data), and never beats HIER ((c) > (b) ?
(d)).3.2 Experiments & resultsWe evaluated the performance of various transferlearning methods on the data and tasks describedin ?3.1.
Specifically, we compared our approximatehierarchical prior model (HIER), implemented as aCRF, against three baselines:?
GAUSS: CRF model tuned on a single domain?sdata, using a standard N (0, 1) prior?
CAT: CRF model tuned on a concatenation ofmultiple domains?
data, using a N (0, 1) prior?
CHELBA: CRF model tuned on one domain?sdata, using a prior trained on a different, relateddomain?s data (cf.
?2.3)We use token-level F1 as our main evaluation mea-sure, combining precision and recall into one metric.3.2.1 Intra-genre, same-task transfer learningFigure 3 shows the results of an experiment inlearning to recognize person names in MUC6 newsarticles.
In this experiment we examined the effectof adding extra data from a different, but related do-main from the same genre, namely, MUC7.
Linea shows the F1 performance of a CRF model tunedonly on the target MUC6 domain (GAUSS) across arange of tuning data sizes.
Line b shows the sameexperiment, but this time the CRF model has beentuned on a dataset comprised of a simple concate-nation of the training MUC6 data from (a), alongwith a different training set from MUC7 (CAT).
Wecan see that adding extra data in this way, though2500.10.20.30.40.50.60.70.80.910 20 40 60 80 100F1Percent of target-domain data used for tuningInter-genre transfer performance evaluated on MUC6(e) HIER: MUC6+7 prior, tuned on MUC6(f) CAT: tuned on all domains(g) HIER: all domains prior, tuned on MUC6(h) CHELBA: all domains prior, tuned on MUC6Figure 4: Transfer aware priors CHELBA and HIER ef-fectively filter irrelevant data.
Adding more irrelevantdata to the priors doesn?t hurt ((e) ?
(g) ?
(h)), whilesimply CAT?ing it, in this case, is disastrous ((f) << (e).the data is closely related both in domain and task,has actually hurt the performance of our recognizerfor training sizes of moderate to large size.
This ismost likely because, although the MUC6 and MUC7datasets are closely related, they are still drawn fromdifferent distributions and thus cannot be intermin-gled indiscriminately.
Line c shows the same com-bination of MUC6 and MUC7, only this time thedatasets have been combined using the HIER prior.In this case, the performance actually does improve,both with respect to the single-dataset trained base-line (a) and the naively trained double-dataset (b).Finally, line d shows the results of the CHELBAprior.
Curiously, though the domains are closely re-lated, it does more poorly than even the non-transferGAUSS.
One possible explanation is that, althoughmuch of the vocabulary is shared across domains,the interpretation of the features of these words maydiffer.
Since CHELBA doesn?t model the hierarchyamong features like HIER, it is unable to smoothaway these discrepancies.
In contrast, we see thatour HIER prior is able to successfully combine therelevant parts of data across domains while filteringthe irrelevant, and possibly detrimental, ones.
Thisexperiment was repeated for other sets of intra-genretasks, and the results are summarized in ?3.2.3.3.2.2 Inter-genre, multi-task transfer learningIn Figure 4 we see that the properties of the hi-erarchical prior hold even when transferring acrosstasks.
Here again we are trying to learn to recognizeperson names in MUC6 e-mails, but this time, in-stead of adding only other datasets similarly labeledwith person names, we are additionally adding bi-ological corpora (UT & YAPEX), labeled not withperson names but with protein names instead, alongwith the CSPACE e-mail and MUC7 news articlecorpora.
The robustness of our prior prevents amodel trained on all five domains (g) from degradingaway from the intra-genre, same-task baseline (e),unlike the model trained on concatenated data (f ).CHELBA (h) performs similarly well in this case,perhaps because the domains are so different that al-most none of the features match between prior andtuning data, and thus CHELBA backs-off to a stan-dard N (0, 1) prior.This robustness in the face of less similarly relateddata is very important since these types of transfermethods are most useful when one possesses onlyvery little target domain data.
In this situation, itis often difficult to accurately estimate performanceand so one would like assurance than any transfermethod being applied will not have negative effects.3.2.3 Comparison of HIER prior to baselinesEach scatter plot in Figure 5 shows the relativeperformance of a baseline method against HIER.Each point represents the results of two experi-ments: the y-coordinate is the F1 score of the base-line method (shown on the y-axis), while the x-coordinate represents the score of the HIER methodin the same experiment.
Thus, points lying be-low the y = x line represent experiments for whichHIER received a higher F1 value than did the base-line.
While all three plots show HIER outperform-ing each of the three baselines, not surprisingly,the non-transfer GAUSS method suffers the worst,followed by the naive concatenation (CAT) base-line.
Both methods fail to make any explicit dis-tinction between the source and target domains andthus suffer when the domains differ even slightlyfrom each other.
Although the differences aremore subtle, the right-most plot of Figure 5 sug-gests HIER is likewise able to outperform the non-hierarchical CHELBA prior in certain transfer sce-narios.
CHELBA is able to avoid suffering as muchas the other baselines when faced with large differ-ence between domains, but is still unable to capture2510.2.4.6.810 .2 .4 .6 .8 1GAUSS(F1)HIER (F1)0.2.4.6.810 .2 .4 .6 .8 1CAT(F1)HIER (F1).4.6.8.4 .6 .8CHELBA(F1)HIER (F1)?y = xMUC6@3%MUC6@6%MUC6@13%MUC6@25%MUC6@50%MUC6@100%CSPACE@3%CSPACE@6%CSPACE@13%CSPACE@25%CSPACE@50%CSPACE@100%Figure 5: Comparative performance of baseline methods (GAUSS, CAT, CHELBA) vs. HIER prior, as trained on nineprior datasets (both pure and concatenated) of various sample sizes, evaluated on MUC6 and CSPACE datasets.
Pointsbelow the y = x line indicate HIER outperforming baselines.as many dependencies between domains as HIER.4 Conclusions, related & future workIn this work we have introduced hierarchical featuretree priors for use in transfer learning on named en-tity extraction tasks.
We have provided evidence thatmotivates these models on intuitive, theoretical andempirical grounds, and have gone on to demonstratetheir effectiveness in relation to other, competitivetransfer methods.
Specifically, we have shown thathierarchical priors allow the user enough flexibil-ity to customize their semantics to a specific prob-lem, while providing enough structure to resist un-intended negative effects when used inappropriately.Thus hierarchical priors seem a natural, effectiveand robust choice for transferring learning acrossNER datasets and tasks.Some of the first formulations of the transferlearning problem were presented over 10 yearsago (Thrun, 1996; Baxter, 1997).
Other techniqueshave tried to quantify the generalizability of cer-tain features across domains (Daume?
III and Marcu,2006; Jiang and Zhai, 2006), or tried to exploit thecommon structure of related problems (Ben-Davidet al, 2007; Scho?lkopf et al, 2005).
Most ofthis prior work deals with supervised transfer learn-ing, and thus requires labeled source domain data,though there are examples of unsupervised (Arnoldet al, 2007), semi-supervised (Grandvalet and Ben-gio, 2005; Blitzer et al, 2006), and transductive ap-proaches (Taskar et al, 2003).Recent work using so-called meta-level priors totransfer information across tasks (Lee et al, 2007),while related, does not take into explicit account thehierarchical structure of these meta-level features of-ten found in NLP tasks.
Daume?
allows an extra de-gree of freedom among the features of his domains,implicitly creating a two-level feature hierarchy withone branch for general features, and another for do-main specific ones, but does not extend his hierar-chy further (Daume?
III, 2007)).
Similarly, work onhierarchical penalization (Szafranski et al, 2007) intwo-level trees tries to produce models that rely onlyon a relatively small number of groups of variable,as structured by the tree, as opposed to transferringknowledge between branches themselves.Our future work is focused on designing an al-gorithm to optimally choose a smoothing regimefor the learned feature trees so as to better exploitthe similarities between domains while neutralizingtheir differences.
Along these lines, we are workingon methods to reduce the amount of labeled targetdomain data needed to tune the prior-based mod-els, looking forward to semi-supervised and unsu-pervised transfer methods.252ReferencesRie K. Ando and Tong Zhang.
2005.
A framework forlearning predictive structures from multiple tasks andunlabeled data.
In JMLR 6, pages 1817 ?
1853.Andrew Arnold, Ramesh Nallapati, and William W. Co-hen.
2007.
A comparative study of methods for trans-ductive transfer learning.
In Proceedings of the IEEEInternational Conference on Data Mining (ICDM)2007 Workshop on Mining and Management of Bio-logical Data.Jonathan Baxter.
1997.
A Bayesian/information theo-retic model of learning to learn via multiple task sam-pling.
Machine Learning, 28(1):7?39.Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2007.
Analysis of representations fordomain adaptation.
In NIPS 20, Cambridge, MA.
MITPress.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP, Sydney, Australia.A.
Borthwick, J.
Sterling, E. Agichtein, and R. Grishman.1998.
NYU: Description of the MENE named entitysystem as used in MUC-7.R.
Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney,A.
Ramani, and Y. Wong.
2004.
Comparative experi-ments on learning information extractors for proteinsand their interactions.
In Journal of AI in Medicine.Data from ftp://ftp.cs.utexas.edu/pub/mooney/bio-data/proteins.tar.gz.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28(1):41?75.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
In Dekang Lin and Dekai Wu, editors, EMNLP2004, pages 285?292.
ACL.S.
Chen and R. Rosenfeld.
1999.
A gaussian prior forsmoothing maximum entropy models.William W. Cohen.
2004.
Minorthird: Methods foridentifying names and ontological relations in textusing heuristics for inducing regularities from data.http://minorthird.sourceforge.net.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
In Journal of ArtificialIntelligence Research 26, pages 101?126.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.David Fisher, Stephen Soderland, Joseph McCarthy,Fangfang Feng, and Wendy Lehnert.
1995.
Descrip-tion of the UMass system as used for MUC-6.Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, LarsAsker, Per Lidn, and Joakim Co?ster.
2002.
Proteinnames and how to find them.
In International Journalof Medical Informatics.Yves Grandvalet and Yoshua Bengio.
2005.
Semi-supervised learning by entropy minimization.
In CAP,Nice, France.Jing Jiang and ChengXiang Zhai.
2006.
Exploiting do-main structure for named entity recognition.
In Hu-man Language Technology Conference, pages 74 ?
81.R.
Kraut, S. Fussell, F. Lerch, and J. Espinosa.
2004.
Co-ordination in teams: evidence from a simulated man-agement game.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.
Morgan Kaufmann, San Francisco, CA.S.-I.
Lee, V. Chatalbashev, D. Vickrey, and D. Koller.2007.
Learning a meta-level prior for feature relevancefrom multiple related tasks.
In Proceedings of Interna-tional Conference on Machine Learning (ICML).Einat Minkov, Richard C. Wang, and William W. Cohen.2005.
Extracting personal names from email: Ap-plying named entity recognition to informal text.
InHLT/EMNLP.Rajat Raina, Andrew Y. Ng, and Daphne Koller.
2006.Transfer learning by constructing informative priors.In ICML 22.Bernhard Scho?lkopf, Florian Steinke, and Volker Blanz.2005.
Object correspondence as a machine learningproblem.
In ICML ?05: Proceedings of the 22nd inter-national conference on Machine learning, pages 776?783, New York, NY, USA.
ACM.Charles Sutton and Andrew McCallum.
2005.
Composi-tion of conditional random fields for transfer learning.In HLT/EMLNLP.M.
Szafranski, Y. Grandvalet, and P. Morizet-Mahoudeaux.
2007.
Hierarchical penalization.In Advances in Neural Information ProcessingSystems 20.
MIT press.B.
Taskar, M.-F. Wong, and D. Koller.
2003.
Learn-ing on the test data: Leveraging ?unseen?
features.
InProc.
Twentieth International Conference on MachineLearning (ICML).Sebastian Thrun.
1996.
Is learning the n-th thing anyeasier than learning the first?
In NIPS, volume 8,pages 640?646.
MIT.J.
Zhang, Z. Ghahramani, and Y. Yang.
2005.
Learningmultiple related tasks using latent independent compo-nent analysis.253
