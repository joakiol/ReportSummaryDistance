Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 46?53,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsTowards the Interpretation of Utterance Sequences in a Dialogue SystemIngrid Zukerman and Patrick Ye and Kapil Kumar Gupta and Enes MakalicFaculty of Information TechnologyMonash UniversityClayton, VICTORIA 3800, Australiaingrid@infotech.monash.edu.au, {ye.patrick,kapil.k.gupta,emakalic}@gmail.comAbstractThis paper describes a probabilistic mech-anism for the interpretation of sentence se-quences developed for a spoken dialoguesystem mounted on a robotic agent.
Themechanism receives as input a sequence ofsentences, and produces an interpretationwhich integrates the interpretations of in-dividual sentences.
For our evaluation, wecollected a corpus of hypothetical requeststo a robot.
Our mechanism exhibits goodperformance for sentence pairs, but re-quires further improvements for sentencesequences.1 IntroductionDORIS (Dialogue Oriented Roaming InteractiveSystem) is a spoken dialogue system under devel-opment, which will eventually be mounted on ahousehold robot.
The focus of our current work ison DORIS?s language interpretation module calledScusi?.
In this paper, we consider the interpreta-tion of a sequence of sentences.People often utter several separate sentences toconvey their wishes, rather than producing a sin-gle sentence that contains all the relevant informa-tion (Zweig et al, 2008).
For instance, people arelikely to say ?Go to my office.
Get my mug.
It ison the table.
?, instead of ?Get my mug on the tablein my office?.
This observation, which was val-idated in our corpus study (Section 4), motivatesthe mechanism for the interpretation of a sequenceof sentences presented in this paper.
Our mecha-nism extends our probabilistic process for inter-preting single spoken utterances (Zukerman et al,2008) in that (1) it determines which sentences ina sequence are related, and if so, combines theminto an integrated interpretation; and (2) it pro-vides a formulation for estimating the probabilityof an interpretation of a sentence sequence, whichsupports the selection of the most probable inter-pretation.
Our evaluation demonstrates that ourmechanism performs well in understanding textualsentence pairs of different length and level of com-plexity, and highlights particular aspects of our al-gorithms that require further improvements (Sec-tion 4).In the next section, we describe our mechanismfor interpreting a sentence sequence.
In Section 3,we present our formalism for assessing the prob-ability of an interpretation.
The performance ofour system is evaluated in Section 4, followed byrelated research and concluding remarks.2 Interpreting a Sequence of UtterancesScusi?
employs an anytime algorithm to interpreta sequence of sentences (Algorithm 1).
The algo-rithm generates interpretations until time runs out(in our case, until a certain number of iterationshas been executed).
In Steps 1?5, Algorithm 1processes each sentence separately according tothe interpretation process for single sentences de-scribed in (Zukerman et al, 2008).1 Charniak?sprobabilistic parser2 is applied to generate parsetrees for each sentence in the sequence.
The parserproduces up to N (= 50) parse trees for each sen-tence, associating each parse tree with a probabil-ity.
The parse trees for each sentence are then it-eratively considered in descending order of proba-bility, and algorithmically mapped into Uninstan-tiated Concept Graphs (UCGs) ?
a representa-1Although DORIS is a spoken dialogue system, our cur-rent results pertain to textual input only.
Hence, we omit theaspects of our work pertaining to spoken input.2ftp://ftp.cs.brown.edu/pub/nlparser/46Algorithm 1 Interpret a sentence sequenceRequire: Sentences T1, .
.
.
, Tn{ Interpret Sentences }1: for all sentences Ti do2: Generate parse trees {Pi}, and UCGs {Ui}3: Generate candidate modes {Mi}4: For each identifier j in Ti, generate candi-date referents {Rij}5: end for{ Combine UCGs }6: while there is time do7: Get {(U1,M1, R1), .
.
.
, (Un,Mn, Rn)} ?a sequence of tuples (one tuple per sen-tence)8: Generate {UD}, a sequence of declara-tive UCGs, by merging the declarativeUCGs in {(Ui,Mi, Ri)} as specified bytheir identifier-referent pairs and modes9: Generate {U I}, a sequence of imperativeUCGs, by merging each imperative UCGin {(Ui,Mi, Ri)} with declarative UCGsas specified by their identifier-referent pairsand modes10: Generate candidate ICG sequences {IIj } forthe sequence {U I}11: Select the best sequence of ICGs {II?
}12: end whiletion based on Concept Graphs (Sowa, 1984) ?one parse tree yielding one UCG (but several parsetrees may produce the same UCG).
UCGs rep-resent syntactic information, where the conceptscorrespond to the words in the parent parse tree,and the relations are derived from syntactic in-formation in the parse tree and prepositions (Fig-ure 1(a) illustrates UCGs UD and U I generatedfrom the sentences ?The mug is on the table.
Cleanit.?
).Our algorithm requires sentence mode (declar-ative, imperative or interrogative3), and resolvedreferences to determine how to combine the sen-tences in a sequence.
Sentence mode is obtainedusing a classifier trained on part of our corpus(Section 2.2).
The probability distribution for thereferents of each identifier is obtained from thecorpus and from rules derived from (Lappin andLeass, 1994; Ng et al, 2005) (Section 2.3).At this point, for each sentence Ti in a sequence,we have a list of UCGs, a list of modes, and lists3Interrogatives are treated as imperatives at present, so inthe remainder of the paper we do not mention interrogatives.clean0mug03table01OnPatientonmugDEFDEFtableI1{U       , R="the table"}1clean0table02PatientobjectcleanitIUobjectcleanDEFtablecleanobjectonmug DEFtable DEFDUI1{U       , R="the mug"}2I1}1I{ I1}2I{DECLARATIVEThe mug is on the table.
Clean it.IMPERATIVE(b) Merged UCGs       (c) Candidate ICGs(a) Declarative andimperative UCGsFigure 1: Combining two sentencesof referents (one list for each identifier in the sen-tence).
In Step 7, Algorithm 1 generates a tu-ple (Ui,Mi, Ri) for each sentence Ti by selectingfrom these lists a UCG, a mode and a referent foreach identifier (yielding a list of identifier-referentpairs).
Each element in each (U,M,R) tuple is it-eratively selected by traversing the appropriate listin descending order of probability.
For instance,given sentences T1, T2, T3, the top UCG for T1 ispicked first, together with the top mode and thetop identifier-referent pairs for that sentence (like-wise for T2 and T3); next the second-top UCG ischosen for T1, but the other elements remain thesame; and so on.Once the (U,M,R) tuples have been deter-mined, the UCGs for the declarative sentencesare merged in the order they were given (Step 8).This is done by first merging a pair of declara-tive UCGs, then merging the resultant UCG withthe next declarative UCG, and so on.
The idea isthat if the declarative sentences have co-referents,then the information about these co-referents canbe combined into one representation.
For exam-ple, consider the sequence ?The mug is on the ta-ble.
It is blue.
Find it.
The mug is near the phone.Bring it to me.?
Some of the UCG sequences ob-tained from the declarative sentences (first, secondand fourth) are:{UD1 }1={mug(CLR blue)-(on-table & near-phone)}{UD1 }2={mug-(on-table(CLR blue) &near-phone)}{UD1 , UD2 }3={mug(CLR blue)-on-table,mug-near-phone}.44The different notations are because colour (and size) areproperties of objects, while prepositions indicate relations.47The first two sequences contain one declarativemerged UCG, and the third contains two UCGs.In Step 9, Algorithm 1 considers a UCG foreach imperative sentence in turn, and merges itwith declarative UCGs (which may have resultedfrom a merger), as specified by the modes andidentifier-referent pairs of the sentences in ques-tion.
For example, consider the sentence sequence?Find my mug.
It is in my office.
Bring it.?
One ofthe (U,M,R)-tuple sequences for this instructionset is{(find-obj-mug-owner-me, imperative, NIL),(it1-in-office-owner-me, declarative, it1-mug),(bring-obj-it2, imperative, it2-mug)}.After merging the first two UCGs (imperative-declarative), and then the second and third UCGs(declarative-imperative), we obtain the imperativeUCG sequence{U I1,U I2 }:U I1=find-obj-mug-(owner-me &in-office-owner-me)U I2=bring-obj-mug-(in-office-owner-me).This process enables Scusi?
to iteratively mergeever-expanding UCGs with subsequent UCGs,eventually yielding UCG sequences which containdetailed UCGs that specify an action or object.
Alimitation of this merging process is that the infor-mation about the objects specified in an impera-tive UCG is not aggregated with the informationabout these objects in other imperative UCGs, andthis sometimes can cause the merged imperativeUCGs to be under-specified.
This limitation willbe addressed in the immediate future.After a sequence of imperative UCGs has beengenerated, candidate Instantiated Concept Graphs(ICGs) are proposed for each imperative UCG,and the most probable ICG sequence is selected(Steps 10?11 of Algorithm 1).
We focus on im-perative UCGs because they contain the actionsthat the robot is required to perform; these actionsincorporate relevant information from declarativeUCGs.
ICGs are generated by nominating dif-ferent instantiated concepts and relations from thesystem?s knowledge base as potential realizationsfor each concept and relation in a UCG (Zukermanet al, 2008); each UCG can generate many ICGs.Since this paper focuses on the generation of UCGsequences, the generation of ICGs will not be dis-cussed further.2.1 Merging UCGsGiven tuples (Ui,Mi, Ri) and (Uj ,Mj , Rj) wherej > i, pronouns and one-anaphora in Uj are re-placed with their referent in Ui on the basis of theset of identifier-referent pairs in Rj (if there is noreferent in Ui for an identifier in Uj , the identifieris left untouched).
Ui and Uj are then merged intoa UCG Um by first finding a node n that is com-mon to Ui and Uj , and then copying the sub-tree ofUj whose root is n into a copy of Ui.
If more thanone node can be merged, the node (head noun) thatis highest in the Uj structure is used.
If one UCGis declarative and the other imperative, we swapthem if necessary, so that Ui is imperative and Ujdeclarative.For instance, given the sentences ?The mug ison the table.
Clean it.?
in Figure 1, Step 4 ofAlgorithm 1 produces the identifier-referent pairs{(it, mug), (it, table)}, yielding two intermedi-ate UCGs for the imperative sentence: (1) clean-object-mug, and (2) clean-object-table.
The firstUCG is merged with a UCG for the declarativesentence using mug as root node, and the secondUCG is merged using table as root node.
Thisresults in merged UCG sequences (of length 1)corresponding to ?Clean the table?
and ?Clean themug on the table?
({U I1 }1 and {U I1 }2 respectivelyin Figure 1(b), which in turn produce ICG se-quences {II1}1 and {II1}2 in Figure 1(c), amongothers).2.2 Determining modesWe use the MaxEnt classifier5 to determine themode of a sentence.
The input features to the clas-sifier (obtained from the highest probability parsetree for this sentence) are: (1) top parse-tree node;(2) position and type of the top level phrases underthe top parse-tree node, e.g., (0, NP), (1, VP), (2,PP); (3) top phrases under the top parse-tree nodereduced to a regular expression, e.g., VP-NP+ torepresent, say, VP NP NP; (4) top VP head ?
thehead word of the first top level VP; (5) top NP head?
the head word of the first top level NP; (6) firstthree tokens in the sentence; and (7) last token inthe sentence.
Using leave-one-out cross valida-tion, this classifier has an accuracy of 97.8% onthe test data ?
a 30% improvement over the ma-jority class (imperative) baseline.2.3 Resolving referencesScusi?
handles pronouns, one-anaphora and NPidentifiers (e.g., ?the book?).
At present, we con-sider only precise matches between NP identifiers5http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html48and referents, e.g., ?the cup?
does not match ?thedish?.
In the future, we will incorporate similar-ity scores based on WordNet, e.g., Leacock andChodorow?s (1998) scores for approximate lexicalmatches; such matches occurred in 4% of our cor-pus (Section 4).To reduce the complexity of reference reso-lution across a sequence of sentences, and theamount of data required to reliably estimate prob-abilities (Section 3), we separate our problem intotwo parts: (1) identifying the sentence being re-ferred to, and (2) determining the referent withinthat sentence.Identifying a sentence.
Most referents in ourcorpus appear in the current, previous or first sen-tence in a sequence, with a few referents appear-ing in other sentences (Section 4).
Hence, wehave chosen the sentence classes {current, previ-ous, first, other}.
The probability of referring toa sentence of a particular class from a sentencein position i is estimated from our corpus, wherei = 1, .
.
.
, 5, > 5 (there are only 13 sequenceswith more than 5 sentences).
We estimate this dis-tribution for each leave-one-out cross-validationfold in our evaluation (Section 4).Determining a referent.
We use heuristicsbased on those described in (Lappin and Leass,1994) to classify pronouns (an example of a non-pronoun usage is ?It is ModalAdjective that S?
),and heuristics based on the results obtained in (Nget al, 2005) to classify one-anaphora (an exam-ple of a high-performing feature pattern is ?one ashead-noun with NN or CD as Part-of-speech andno attached of PP?).
If a term is classified as a pro-noun or one-anaphor, then a list of potential ref-erents is constructed using the head nouns in thetarget sentence.
We use the values in (Lappin andLeass, 1994) to assign a score to each anaphor-referent pair according to the grammatical role ofthe referent in the target UCG (obtained from thehighest probability parse tree that is a parent of thisUCG).
These scores are then converted to proba-bilities using a linear mapping function.3 Estimating the Probability of a MergedInterpretationWe now present our formulation for estimating theprobability of a sequence of UCGs, which sup-ports the selection of the most probable sequence.One sentence.
The probability of a UCG gener-ated from a sentence T is estimated as describedin (Zukerman et al, 2008), resulting inPr(U |T ) ?
?P Pr(P |T )?Pr(U |P ) (1)where T , P and U denote text, parse tree and UCGrespectively.
The summation is taken over all pos-sible parse trees from the text to the UCG, be-cause a UCG can have more than one ancestor.
Asmentioned above, the parser returns an estimate ofPr(P |T ); and Pr(U |P ) = 1, since the process ofgenerating a UCG from a parse tree is determinis-tic.A sentence sequence.
The probability of an in-terpretation of a sequence of sentences T1, .
.
.
, TnisPr(U1, .
.
.
, Um|T1, .
.
.
, Tn) =Pr(U1, .
.
.,Un,M1, .
.
.,Mn,R1, .
.
.,Rn|T1, .
.
.,Tn)where m is the number of UCGs in a merged se-quence.By making judicious conditional independenceassumptions, and incorporating parse trees into theformulation, we obtainPr(U1, .
.
.
, Um|T1, .
.
.
, Tn) =n?i=1Pr(Ui|Ti)?Pr(Mi|Pi, Ti)?Pr(Ri|P1, .
.
.
, Pi)This formulation is independent of the num-ber of UCGs in a merged sequence generatedby Algorithm 1, thereby supporting the compari-son of UCG sequences of different lengths (pro-duced when different numbers of mergers are per-formed).Pr(Ui|Ti) is calculated using Equation 1, andPr(Mi|Pi, Ti) is obtained as described in Sec-tion 2.2 (recall that the input features to the clas-sifier depend on the parse tree and the sentence).In principle, Pr(Mi|Pi, Ti) and Pr(Ri|P1, .
.
.
, Pi)could be obtained by summing over all parse trees,as done in Equation 1.
However, at present we usethe highest-probability parse tree to simplify ourcalculations.To estimate Pr(Ri|P1, .
.
.
, Pi) we assume con-ditional independence between the identifiers in asentence, yieldingPr(Ri|P1, .
.
.
, Pi) =ki?j=1Pr(Rij |P1, .
.
.
, Pi)where ki is the number of identifiers in sentencei, and Rij is the referent for identifier j in sen-tence i.
As mentioned in Section 2.3, this factor is49separated into determining a sentence, and deter-mining a referent in that sentence.
We also includein our formulation the Type of the identifier (pro-noun, one-anaphor or NP) and sentence position i,yieldingPr(Rij |P1, .
.
.
, Pi) =Pr(Rij ref NPa in sent b, Type(Rij)|i, P1, .
.
.
, Pi)After additional conditionalization we obtainPr(Rij |P1, .
.
.
, Pi) =Pr(Rij ref NPa|Rij ref sent b,Type(Rij),Pi,Pb)?Pr(Rij ref sent b|Type(Rij), i)?Pr(Type(Rij)|Pi)As seen in Section 2.3, Pr(Type(Rij)|Pi) andPr(Rij ref NPa|Rij ref sent b,Type(Rij),Pi,Pb)are estimated in a rule-based manner, andPr(Rij ref sent b|Type(Rij), i) is estimated fromthe corpus (recall that we distinguish betweensentence classes, rather than specific sentences).4 EvaluationWe first describe our experimental set-up, fol-lowed by our results.4.1 Experimental set-upWe conducted a web-based survey to collect a cor-pus comprising multi-sentence requests.
To thiseffect, we presented participants with a scenariowhere they are in a meeting room, and they aska robot to fetch something from their office.
Theidea is that if people cannot see a scene, their in-structions will be more segmented than if they canview the scene.
The participants were free to de-cide which object to fetch, and what was in theoffice.
There were no restrictions on vocabularyor grammatical form for the requests.We collected 115 sets of instructions mostlyfrom different participants (a few people did thesurvey more than once).6 The sentence sequencesin our corpus contain between 1 and 9 sentences,with 74% of the sequences comprising 1 to 3 sen-tences.
Many of the sentences had grammaticalrequirements which exceeded the capabilities ofour system.
To be able to use these instructionsets in our evaluation, we made systematic manualchanges to produce sentences that meet our sys-tem?s grammatical restrictions (in the future, we6We acknowledge the modest size of our corpus comparedto that of some publicly available corpora, e.g., ATIS.
How-ever, we must generate our own corpus since our task differsin nature from the tasks where these large corpora are used.SMALL OFFICEMAIN OFFICEPRINTER TABLECHAIRBOOKCASEWINDOWSIDEDESKFILINGCABINETGLASSMAIN DESKCABINETBOOKCASEJOE?S DESKFigure 2: Our virtual environment (top view)will relax these restrictions, as required by a de-ployable system).
Below are the main types ofchanges we made.?
Indirect Speech Acts in the form of questionswere changed to imperatives.
For instance,?Can you get my tea??
was changed to ?Getmy tea?.?
Conjoined verb phrases or sentences were sep-arated into individual sentences.?
Composite verbs were simplified, e.g., ?I thinkI left it on?
was changed to ?it is on?, and out-of-vocabulary composite nouns were replacedby simple nouns or adjectives, e.g., ?the diaryis A4 size?
to ?the diary is big?.?
Conditional sentences were removed.Table 1 shows two original texts compared withthe corresponding modified texts (the changedportions in the originals have been italicized).Our evaluation consists of two experiments:(1) ICGs for sentence pairs, and (2) UCGs for sen-tence sequences.Experiment 1.
We extracted 106 sentence pairsfrom our corpus ?
each pair containing onedeclarative and one imperative sentence.
To eval-uate the ICGs, we constructed a virtual environ-ment comprising a main office and a small office(Figure 2).
Furniture and objects were placed ina manner compatible with what was mentioned inthe requests in our corpus; distractors were alsoplaced in the virtual space.
In total, our environ-ment contains 183 instantiated concepts (109 of-fice and household objects, 43 actions and 31 re-lations).
The (x, y, z) coordinates, colour and di-mensions of these objects were stored in a knowl-edge base.
Since we have two sentences and theirmode is known, no corpus-based information isused for this experiment, and hence no training isrequired.50Original Get my book ?The Wizard of Oz?
from my office.
It?s green and yellow.
It has a pictureof a dog and a girl on it.
It?s in my desk drawer on the right side of my desk, the seconddrawer down.
If it?s not there, it?s somewhere on my shelves that are on the left side of myoffice as you face the window.Modified Get my book from my office.
It?s green.
It?s in my drawer on the right of my desk.Original DORIS, I left my mug in my office and I want a coffee.
Can you go into my office and getmy mug.
It is on top of the cabinet that is on the left side of my desk.Modified My mug is in my office.
Go into my office.
Get my mug.
It is on top of the cabinet on theleft of my desk.Table 1: Original and modified textExperiment 2.
Since UCGs contain only syn-tactic information, no additional setup was re-quired.
However, for this experiment we need totrain our mode classifier (Section 2.2), and esti-mate the probability distribution of referring to aparticular sentence in a sequence (Section 2.3).Owing to the small size of our corpus, we useleave-one-out cross validation.For both experiments, Scusi?
was set to gener-ate up to 300 sub-interpretations (including parsetrees, UCGs and ICGs) for each sentence in thetest-set; on average, it took less than 1 secondto go from a text to a UCG.
An interpretationwas deemed successful if it correctly representedthe speaker?s intention, which was represented byan imperative Gold ICG for the first experiment,and a sequence of imperative Gold UCGs for thesecond experiment.
These Gold interpretationswere manually constructed by the authors throughconsensus-based annotation (Ang et al, 2002).
Asmentioned in Section 2, we evaluated only imper-ative ICGs and UCGs, as they contain the actionsthe robot is expected to perform.4.2 ResultsTable 2 summarizes our results.
Column 1 showsthe type of outcome being evaluated (ICGs in Ex-periment 1, and UCG sequences and individualUCGs in Experiment 2).
The next two columnsdisplay how many sentences had Gold interpreta-tions whose probability was among the top-1 andtop-3 probabilities.
The average rank of the Goldinterpretation appears in Column 4 (?not found?Gold interpretations are excluded from this rank).The rank of an interpretation is its position in alist sorted in descending order of probability (start-ing from position 0), such that all equiprobable in-terpretations have the same position.
Columns 5and 6 respectively show the median and 75%-ilerank of the Gold interpretation.
The number ofGold interpretations that were not found appears inColumn 7, and the total number of requests/UCGsis shown in the last column.Experiment 1.
As seen in the first row of Ta-ble 2, the Gold ICG was top ranked in 75.5% ofthe cases, and top-3 ranked in 85.8%.
The aver-age rank of 2.17 is mainly due to 7 outliers, whichtogether with the ?not-found?
Gold ICG, are dueto PP-attachment issues, e.g., for the sentence pair?Fetch my phone from my desk.
It is near the key-board.
?, the top parses and resultant UCGs have?near the keyboard?
attached to ?the desk?
(in-stead of ?the phone?).
Nonetheless, the top-rankedinterpretation correctly identified the intended ob-ject and action in 5 of these 7 cases.
Medianand 75%-ile results confirm that most of the GoldICGs are top ranked.Experiment 2.
As seen in the second row of Ta-ble 2, the Gold UCG sequence was top ranked for51.3% of the requests, and top-3 ranked for 53.0%of the requests.
The third row shows that 62.4%of the individual Gold UCGs were top-ranked,and 65.4% were top-3 ranked.
This indicates thatwhen Scusi?
cannot fully interpret a request, itcan often generate a partially correct interpreta-tion.
As for Experiment 1, the average rank of3.14 for the Gold UCG sequences is due to out-liers, several of which were ranked above 30.
Themedian and 75%-ile results show that when Scusi?generates the correct interpretation, it tends to behighly ranked.Unlike Experiment 1, in Experiment 2 there islittle difference between the top-1 and top-3 re-sults.
A possible explanation is that in Experi-ment 1, the top-ranked UCG may yield severalprobable ICGs, such that the Gold ICG is not topranked ?
a phenomenon that is not observable atthe UCG stage.Even though Experiment 2 reaches only the51Table 2: Scusi?
?s interpretation performance# Gold interps.
with prob.
in Average Median 75%-ile Not Totaltop 1 top 3 rank rank rank found #ICGs 80 (75.5%) 91 (85.8%) 2.17 0 0 1 (0.9%) 106 reqs.UCG seqs.
59 (51.3%) 61 (53.0%) 3.14 0 1 36 (31.3%) 115 reqs.UCGs 146 (62.4%) 153 (65.4%) NA NA NA 55 (23.5%) 234 UCGsUCG stage, Scusi?
?s performance for this exper-iment is worse than for Experiment 1, as thereare more grounds for uncertainty.
Table 2 showsthat 31.3% of Gold UCG sequences and 23.5% ofGold UCGs were not found.
Most of these cases(as well as the poorly ranked UCG sequencesand UCGs) were due to (1) imperatives withobject specifications (19 sequences), (2) wronganaphora resolution (6 sequences), and (3) wrongPP-attachment (6 sequences).
In the near future,we will refine the merging process to address thefirst problem.
The second problem occurs mainlywhen there are multiple anaphoric references in asequence.
We propose to include this factor in ourestimation of the probability of referring to a sen-tence.
We intend to alleviate the PP-attachmentproblem, which also occurred in Experiment 1,by interleaving semantic and pragmatic interpreta-tion of prepositional phrases as done in (Brick andScheutz, 2007).
The expectation is that this willimprove the rank of candidates which are pragmat-ically more plausible.5 Related ResearchThis research extends our mechanism for inter-preting stand-alone utterances (Zukerman et al,2008) to the interpretation of sentence sequences.Our approach may be viewed as an informationstate approach (Larsson and Traum, 2000; Beckeret al, 2006), in the sense that sentences may up-date different informational aspects of other sen-tences, without requiring a particular ?legal?
set ofdialogue acts.
However, unlike these informationstate approaches, ours is probabilistic.Several researchers have investigated proba-bilistic approaches to the interpretation of spo-ken utterances in dialogue systems, e.g., (Pflegeret al, 2003; Higashinaka et al, 2003; He andYoung, 2003; Gorniak and Roy, 2005; Hu?wel andWrede, 2006).
Pfleger et al (2003) and Hu?weland Wrede (2006) employ modality fusion to com-bine hypotheses from different analyzers (linguis-tic, visual and gesture), and apply a scoring mech-anism to rank the resultant hypotheses.
They dis-ambiguate referring expressions by choosing thefirst object that satisfies a ?differentiation crite-rion?, hence their system does not handle situa-tions where more than one object satisfies this cri-terion.
He and Young (2003) and Gorniak andRoy (2005) use Hidden Markov Models for theASR stage.
However, these systems do not han-dle utterance sequences.
Like Scusi?, the systemdeveloped by Higashinaka et al (2003) maintainsmultiple interpretations, but with respect to dia-logue acts, rather than the propositional content ofsentences.
All the above systems employ seman-tic grammars, while Scusi?
uses generic, statisti-cal tools, and incorporates semantic- and domain-related information only in the final stage of theinterpretation process.
This approach is supportedby the findings reported in (Knight et al, 2001) forrelatively unconstrained utterances by users unfa-miliar with the system, such as those expected byDORIS.Our mechanism is also well suited for process-ing replies to clarification questions (Horvitz andPaek, 2000; Bohus and Rudnicky, 2005), since areply can be considered an additional sentence tobe incorporated into top-ranked UCG sequences.Further, our probabilistic output can be used by autility-based dialogue manager (Horvitz and Paek,2000).6 ConclusionWe have extended Scusi?, our spoken languageinterpretation system, to interpret sentence se-quences.
Specifically, we have offered a procedurethat combines the interpretations of the sentencesin a sequence, and presented a formalism for es-timating the probability of the merged interpre-tation.
This formalism supports the comparisonof interpretations comprising different numbers ofUCGs obtained from different mergers.Our empirical evaluation shows that Scusi?
per-forms well for textual input corresponding to(modified) sentence pairs.
However, we still need52to address some issues pertaining to the integra-tion of UCGs for sentence sequences of arbitrarylength.
Thereafter, we propose to investigate theinfluence of speech recognition performance onScusi?
?s performance.
In the future, we intend toexpand Scusi?
?s grammatical capabilities.AcknowledgmentsThis research was supported in part by grantDP0878195 from the Australian Research Coun-cil.ReferencesJ.
Ang, R. Dhillon, A. Krupski, E. Shriberg, andA.
Stolcke.
2002.
Prosody-based automatic de-tection of annoyance and frustration in human-computer dialog.
In ICSLP?2002 ?
Proceedings ofthe 7th International Conference on Spoken Lan-guage Processing, pages 2037?2040, Denver, Col-orado.T.
Becker, P. Poller, J. Schehl, N. Blaylock, C. Ger-stenberger, and I. Kruijff-Korbayova?.
2006.
TheSAMMIE system: Multimodal in-car dialogue.
InProceedings of the COLING/ACL 2006 InteractivePresentation Sessions, pages 57?60, Sydney, Aus-tralia.D.
Bohus and A. Rudnicky.
2005.
Constructing accu-rate beliefs in spoken dialog systems.
In ASRU?05?
Proceedings of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 272?277, San Juan, Puerto Rico.T.
Brick and M. Scheutz.
2007.
Incremental naturallanguage processing for HRI.
In HRI 2007 ?
Pro-ceedings of the 2nd ACM/IEEE International Con-ference on Human-Robot Interaction, pages 263?270, Washington, D.C.P.
Gorniak and D. Roy.
2005.
Probabilistic groundingof situated speech using plan recognition and refer-ence resolution.
In ICMI?05 ?
Proceedings of the7th International Conference on Multimodal Inter-faces, pages 138?143, Trento, Italy.Y.
He and S. Young.
2003.
A data-driven spo-ken language understanding system.
In ASRU?03?
Proceedings of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 583?588, St. Thomas, US Virgin Islands.R.
Higashinaka, M. Nakano, and K. Aikawa.
2003.Corpus-Based discourse understanding in spoken di-alogue systems.
In ACL-2003 ?
Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 240?247, Sapporo,Japan.E.
Horvitz and T. Paek.
2000.
DeepListener: Har-nessing expected utility to guide clarification dialogin spoken language systems.
In ICSLP?2000 ?
Pro-ceedings of the 6th International Conference on Spo-ken Language Processing, pages 229?229, Beijing,China.S.
Hu?wel and B. Wrede.
2006.
Spontaneous speechunderstanding for robust multi-modal human-robotcommunication.
In Proceedings of the COL-ING/ACL Main Conference Poster Sessions, pages391?398, Sydney, Australia.S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, and I. Lewin.
2001.
Comparing grammar-basedand robust approaches to speech understanding: Acase study.
In Proceedings of Eurospeech 2001,pages 1779?1782, Aalborg, Denmark.S.
Lappin and H.J.
Leass.
1994.
An algorithm forpronominal anaphora resolution.
ComputationalLinguistics, 20:535?561.S.
Larsson and D. Traum.
2000.
Information stateand dialogue management in the TRINDI dialoguemove engine toolkit.
Natural Language Engineer-ing, 6:323?340.C.
Leacock and M. Chodorow.
1998.
Combining lo-cal context and WordNet similarity for word senseidentification.
In C. Fellbaum, editor, WordNet: AnElectronic Lexical Database, pages 265?285.
MITPress.H.T.
Ng, Y. Zhou, R. Dale, and M. Gardiner.
2005.A machine learning approach to identification andresolution of one-anaphora.
In IJCAI-05 ?
Proceed-ings of the 19th International Joint Conference onArtificial Intelligence, pages 1105?1110, Edinburgh,Scotland.N.
Pfleger, R. Engel, and J. Alexandersson.
2003.
Ro-bust multimodal discourse processing.
In Proceed-ings of the 7th Workshop on the Semantics and Prag-matics of Dialogue, pages 107?114, Saarbru?cken,Germany.J.F.
Sowa.
1984.
Conceptual Structures: InformationProcessing in Mind and Machine.
Addison-Wesley,Reading, MA.I.
Zukerman, E. Makalic, M. Niemann, and S. George.2008.
A probabilistic approach to the interpreta-tion of spoken utterances.
In PRICAI 2008 ?
Pro-ceedings of the 10th Pacific Rim International Con-ference on Artificial Intelligence, pages 581?592,Hanoi, Vietnam.G.
Zweig, D. Bohus, X. Li, and P. Nguyen.
2008.Structured models for joint decoding of repeated ut-terances.
In Proceedings of Interspeech 2008, pages1157?1160, Brisbane, Australia.53
