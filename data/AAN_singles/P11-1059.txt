Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 581?589,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemantic Representation of Negation Using Focus DetectionEduardo Blanco and Dan MoldovanHuman Language Technology Research InstituteThe University of Texas at DallasRichardson, TX 75080 USA{eduardo,moldovan}@hlt.utdallas.eduAbstractNegation is present in all human languagesand it is used to reverse the polarity of partof statements that are otherwise affirmative bydefault.
A negated statement often carries pos-itive implicit meaning, but to pinpoint the pos-itive part from the negative part is rather dif-ficult.
This paper aims at thoroughly repre-senting the semantics of negation by revealingimplicit positive meaning.
The proposed rep-resentation relies on focus of negation detec-tion.
For this, new annotation over PropBankand a learning algorithm are proposed.1 IntroductionUnderstanding the meaning of text is a long termgoal in the natural language processing commu-nity.
Whereas philosophers and linguists have pro-posed several theories, along with models to rep-resent the meaning of text, the field of computa-tional linguistics is still far from doing this automati-cally.
The ambiguity of language, the need to detectimplicit knowledge, and the demand for common-sense knowledge and reasoning are a few of the dif-ficulties to overcome.
Substantial progress has beenmade, though, especially on detection of semanticrelations, ontologies and reasoning methods.Negation is present in all languages and it is al-ways the case that statements are affirmative bydefault.
Negation is marked and it typically sig-nals something unusual or an exception.
It maybe present in all units of language, e.g., words(incredible), clauses (He doesn?t have friends).Negation and its correlates (truth values, lying,irony, false or contradictory statements) are exclu-sive characteristics of humans (Horn, 1989; Hornand Kato, 2000).Negation is fairly well-understood in grammars;the valid ways to express a negation are documented.However, there has not been extensive research ondetecting it, and more importantly, on representingthe semantics of negation.
Negation has been largelyignored within the area of semantic relations.At first glance, one would think that interpretingnegation could be reduced to finding negative key-words, detect their scope using syntactic analysisand reverse its polarity.
Actually, it is more com-plex.
Negation plays a remarkable role in text un-derstanding and it poses considerable challenges.Detecting the scope of negation in itself is chal-lenging: All vegetarians do not eat meat means thatvegetarians do not eat meat and yet All that glittersis not gold means that it is not the case that all thatglitters is gold (so out of all things that glitter, someare gold and some are not).
In the former example,the universal quantifier all has scope over the nega-tion; in the latter, the negation has scope over all.In logic, two negatives always cancel each otherout.
On the other hand, in language this is only theo-retically the case: she is not unhappy does not meanthat she is happy; it means that she is not fully un-happy, but she is not happy either.Some negated statements carry a positive implicitmeaning.
For example, cows do not eat meat impliesthat cows eat something other than meat.
Otherwise,the speaker would have stated cows do not eat.
Aclearer example is the correct and yet puzzling state-ment tables do not eat meat.
This sentence sounds581unnatural because of the underlying positive state-ment (i.e., tables eat something other than meat).Negation can express less than or in betweenwhen used in a scalar context.
For example, Johndoes not have three children probably means that hehas either one or two children.
Contrasts may usenegation to disagree about a statement and not tonegate it, e.g., That place is not big, it is massivedefines the place as massive, and therefore, big.2 Related WorkNegation has been widely studied outside of com-putational linguistics.
In logic, negation is usu-ally the simplest unary operator and it reverses thetruth value.
The seminal work by Horn (1989)presents the main thoughts in philosophy and psy-chology.
Linguists have found negation a complexphenomenon; Huddleston and Pullum (2002) ded-icate over 60 pages to it.
Negation interacts withquantifiers and anaphora (Hintikka, 2002), and in-fluences reasoning (Dowty, 1994; Sa?nchez Valencia,1991).
Zeijlstra (2007) analyzes the position andform of negative elements and negative concords.Rooth (1985) presented a theory of focus in hisdissertation and posterior publications (e.g., Rooth(1992)).
In this paper, we follow the insights onscope and focus of negation by Huddleston and Pul-lum (2002) rather than Rooth?s (1985).Within natural language processing, negationhas drawn attention mainly in sentiment analysis(Wilson et al, 2009; Wiegand et al, 2010) andthe biomedical domain.
Recently, the Negationand Speculation in NLP Workshop (Morante andSporleder, 2010) and the CoNLL-2010 Shared Task(Farkas et al, 2010) targeted negation mostly onthose subfields.
Morante and Daelemans (2009) and?Ozgu?r and Radev (2009) propose scope detectorsusing the BioScope corpus.
Councill et al (2010)present a supervised scope detector using their ownannotation.
Some NLP applications deal indirectlywith negation, e.g., machine translation (van Mun-ster, 1988), text classification (Rose et al, 2003) andrecognizing entailments (Bos and Markert, 2005).Regarding corpora, the BioScope corpus anno-tates negation marks and linguistic scopes exclu-sively on biomedical texts.
It does not annotate fo-cus and it purposely ignores negations such as (talk-ing about the reaction of certain elements) in NK3.3cells is not always identical (Vincze et al, 2008),which carry the kind of positive meaning this workaims at extracting (in NK3.3 cells is often identi-cal).
PropBank (Palmer et al, 2005) only indicatesthe verb to which a negation mark attaches; it doesnot provide any information about the scope or fo-cus.
FrameNet (Baker et al, 1998) does not con-sider negation and FactBank (Saur??
and Pustejovsky,2009) only annotates degrees of factuality for events.None of the above references aim at detecting orannotating the focus of negation in natural language.Neither do they aim at carefully representing themeaning of negated statements nor extracting im-plicit positive meaning from them.3 Negation in Natural LanguageSimply put, negation is a process that turns a state-ment into its opposite.
Unlike affirmative state-ments, negation is marked by words (e.g., not, no,never) or affixes (e.g., -n?t, un-).
Negation can inter-act with other words in special ways.
For example,negated clauses use different connective adjunctsthat positive clauses do: neither, nor instead of ei-ther, or.
The so-called negatively-oriented polarity-sensitive items (Huddleston and Pullum, 2002) in-clude, among many others, words starting with any-(anybody, anyone, anywhere, etc.
), the modal aux-iliaries dare and need and the grammatical units atall, much and till.
Negation in verbs usually requiresan auxiliary; if none is present, the auxiliary do is in-serted (I read the paper vs.
I didn?t read the paper).3.1 Meaning of Negated StatementsState-of-the-art semantic role labelers (e.g., the onestrained over PropBank) do not completely repre-sent the meaning of negated statements.
GivenJohn didn?t build a house to impress Mary, they en-code AGENT(John, build ), THEME(a house, build ),PURPOSE(to impress Mary, build ), NEGATION(n?t,build ).
This representation corresponds to the inter-pretation it is not the case that John built a houseto impress Mary, ignoring that it is implicitly statedthat John did build a house.Several examples are shown Table 1.
For all state-ments s, current role labelers would only encode itis not the case that s. However, examples (1?7)582Statement Interpretation1 John didn?t build a house:to:::::::impress::::Mary.
John built a house for other purpose.2 I don?t have a watch:::with:::me.
I have a watch, but it is not with me.3 We don?t have an evacuation plan:::for:::::::flooding.
We have an evacuation plan for something else (e.g., fire).4 They didn?t release the UFO files::::until::::2008.
They released the UFO files in 2008.5 John doesn?t know:::::exactly how they met.
John knows how they met, but not exactly.6 His new job doesn?t require:::::driving.
His new job has requirements, but it does not require driving.7 His new job doesn?t require driving::yet.
His new job requires driving in the future.8 His new job doesn?t::::::require anything.
His new job has no requirements.9 A panic on Wall Street doesn?t exactly:::::inspire confidence.
A panic on Wall Streen discourages confidence.Table 1: Examples of negated statements and their interpretations considering underlying positive meaning.
A wavyunderline indicates the focus of negation (Section 3.3); examples (8, 9) do not carry any positive meaning.carry positive meaning underneath the direct mean-ing.
Regarding (4), encoding that the UFO fileswere released in 2008 is crucial to fully interpretthe statement.
(6?8) show that different verb argu-ments modify the interpretation and even signal theexistence of positive meaning.
Examples (5, 9) fur-ther illustrate the difficulty of the task; they are verysimilar (both have AGENT, THEME and MANNER)and their interpretation is altogether different.
Notethat (8, 9) do not carry any positive meaning; eventhough their interpretations do not contain a verbalnegation, the meaning remains negative.
Some ex-amples could be interpreted differently dependingon the context (Section 4.2.1).This paper aims at thoroughly representing the se-mantics of negation by revealing implicit positivemeaning.
The main contributions are: (1) interpre-tation of negation using focus detection; (2) focus ofnegation annotation over all PropBank negated sen-tences1; (3) feature set to detect the focus of nega-tion; and (4) model to semantically represent nega-tion and reveal its underlying positive meaning.3.2 Negation TypesHuddleston and Pullum (2002) distinguish four con-trasts for negation:?
Verbal if the marker of negation is grammati-cally associated with the verb (I did not see any-thing at all); non-verbal if it is associated with adependent of the verb (I saw nothing at all).?
Analytic if the sole function of the negatedmark is to mark negation (Bill did not go);synthetic if it has some other function as well([Nobody]AGENT went to the meeting).1Annotation will be available on the author?s website?
Clausal if the negation yields a negative clause(She didn?t have a large income); subclausal oth-erwise (She had a not inconsiderable income).?
Ordinary if it indicates that something is not thecase, e.g., (1) She didn?t have lunch with myold man: he couldn?t make it; metalinguistic ifit does not dispute the truth but rather reformu-lates a statement, e.g., (2) She didn?t have lunchwith your ?old man?
: she had lunch with your fa-ther.
Note that in (1) the lunch never took place,whereas in (2) a lunch did take place.In this paper, we focus on verbal, analytic, clausal,and both metalinguistic and ordinary negation.3.3 Scope and FocusNegation has both scope and focus and they are ex-tremely important to capture its semantics.
Scope isthe part of the meaning that is negated.
Focus is thatpart of the scope that is most prominently or explic-itly negated (Huddleston and Pullum, 2002).Both concepts are tightly connected.
Scope corre-sponds to all elements any of whose individual fal-sity would make the negated statement true.
Focusis the element of the scope that is intended to be in-terpreted as false to make the overall negative true.Consider (1) Cows don?t eat meat and its positivecounterpart (2) Cows eat meat.
The truth conditionsof (2) are: (a) somebody eats something; (b) cowsare the ones who eat; and (c) meat is what is eaten.In order for (2) to be true, (a?c) have to be true.And the falsity of any of them is sufficient to make(1) true.
In other words, (1) would be true if nobodyeats, cows don?t eat or meat is not eaten.
Therefore,all three statements (a?c) are inside the scope of (1).The focus is more difficult to identify, especially5831 AGENT(the cow, didn?t eat) THEME(grass, didn?t eat) INSTRUMENT(with a fork, didn?t eat)2 NOT[AGENT(the cow, ate) THEME(grass, ate) INSTRUMENT(with a fork, ate)]3 NOT[AGENT(the cow, ate)] THEME(grass, ate) INSTRUMENT(with a fork, ate)4 AGENT(the cow, ate) NOT[THEME(grass, ate)] INSTRUMENT(with a fork, ate)5 AGENT(the cow, ate) THEME(grass, ate) NOT[INSTRUMENT(with a fork, ate)]Table 2: Possible semantic representations for The cow didn?t eat grass with a fork.without knowing stress or intonation.
Text under-standing is needed and context plays an importantrole.
The most probable focus for (1) is meat, whichcorresponds to the interpretation cows eat somethingelse than meat.
Another possible focus is cows,which yields someone eats meat, but not cows.Both scope and focus are primarily semantic,highly ambiguous and context-dependent.
More ex-amples can be found in Tables 1 and 3 and (Huddle-ston and Pullum, 2002, Chap.
9).4 Approach to Semantic Representation ofNegationNegation does not stand on its own.
To be useful, itshould be added as part of another existing knowl-edge representation.
In this Section, we outline howto incorporate negation into semantic relations.4.1 Semantic RelationsSemantic relations capture connections betweenconcepts and label them according to their nature.It is out of the scope of this paper to define themin depth, establish a set to consider or discuss theirdetection.
Instead, we use generic semantic roles.Given s: The cow didn?t eat grass with a fork,typical semantic roles encode AGENT(the cow, eat),THEME(grass, eat), INSTRUMENT(with a fork, eat)and NEGATION(n?t, eat).
This representation onlydiffers on the last relation from the positive counter-part.
Its interpretation is it is not the case that s.Several options arise to thoroughly represent s.First, we find it useful to consider the seman-tic representation of the affirmative counterpart:AGENT(the cow, ate), THEME(grass, ate), and IN-STRUMENT(with a fork, ate).
Second, we believedetecting the focus of negation is useful.
Eventhough it is open to discussion, the focus corre-sponds to INSTRUMENT(with a fork, ate) Thus, thenegated statement should be interpreted as the cowate grass, but it did not do so using a fork.Table 2 depicts five different possible semanticrepresentations.
Option (1) does not incorporate anyexplicit representation of negation.
It attaches thenegated mark and auxiliary to eat; the negation ispart of the relation arguments.
This option failsto detect any underlying positive meaning and cor-responds to the interpretation the cow did not eat,grass was not eaten and a fork was not used to eat.Options (2?5) embody negation into the represen-tation with the pseudo-relation NOT.
NOT takes as itsargument an instantiated relation or set of relationsand indicates that they do not hold.Option (2) includes all the scope as the argumentof NOT and corresponds to the interpretation it is notthe case that the cow ate grass with a fork.
Like typi-cal semantic roles, option (2) does not reveal the im-plicit positive meaning carried by statement s. Op-tions (3?5) encode different interpretations:?
(3) negates the AGENT; it corresponds to the cowdidn?t eat, but grass was eaten with a fork.?
(4) applies NOT to the THEME; it corresponds tothe cow ate something with a fork, but not grass.?
(5) denies the INSTRUMENT, encoding the mean-ing the cow ate grass, but it did not use a fork.Option (5) is preferred since it captures the bestimplicit positive meaning.
It corresponds to the se-mantic representation of the affirmative counterpartafter applying the pseudo-relation NOT over the fo-cus of the negation.
This fact justifies and motivatesthe detection of the focus of negation.4.2 Annotating the Focus of NegationDue to the lack of corpora containing annotation forfocus of negation, new annotation is needed.
An ob-vious option is to add it to any text collection.
How-ever, building on top of publicly available resourcesis a better approach: they are known by the commu-nity, they contain useful information for detectingthe focus of negation and tools have already beendeveloped to predict their annotation.584Statement V A0 A1 A2 A4 TMPMNRADVLOCPNCEXTDISMOD1 Even if [that deal]A1 isn?t [::::::revived]V, NBC hopes to find another.?
Even if that deal is suppressed, NBC hopes to find another one.
?
- + - - - - - - - - - -2 [He]A0 [simply]MDIS [ca]MMODn?t [stomach]V [:::the::::taste:::of:::::Heinz]A1 , she says.?
He simply can stomach any ketchup but Heinz?s.
+ + ?
- - - - - - - - + +3 [A decision]A1 isn?t [expected]V [ ::::until:::::some::::time:::::next ::::year]MTMP .?
A decision is expected at some time next year.
+ - + - - ?
- - - - - - -4 [.
.
. ]
it told the SEC [it]A0 [could]MMODn?t [provide]V [financial statements]A1 [by the end of its firstextension]MTMP ?
[:::::::without::::::::::::unreasonable:::::::burden ::or::::::::expense]MMNR?.?
It could provide them by that time with a huge overhead.
+ + + - - + ?
- - - - - +5 [For example]MDIS, [P&G]A0 [up until now]MTMP hasn?t [sold]V [coffee]A1 [::to:::::::airlines]A2 and does only limitedbusiness with hotels and large restaurant chains.?
Up until now, P&G has sold coffee, but not to airlines.
+ + + ?
- + - - - - - + -6 [Decent life .
.
.
]A1 [wo]MMODn?t be [restored]V [:::::unless:::the:::::::::::government::::::::reclaims:::the::::::streets:::::from:::the::::::gangs]MADV .?
It will be restored if the government reclaims the streets from the gangs.
+ - + - - - - ?
- - - - +7 But [::::quite::a:::few:::::::money:::::::::managers]A0 aren?t [buying]V [it]A1 .?
Very little managers are buying it.
+ ?
+ - - - - - - - - - -8 [When]MTMP [she]A0 isn?t [performing]V [ ::for:::an::::::::audience]MPNC , she prepares for a song by removing the wad ofgum from her mouth, and indicates that she?s finished by sticking the gum back in.?
She prepares in that way when she is performing, but not for an audience.
+ + - - - + - - - ?
- - -9 [The company?s net worth]A1 [can]MMODnot [fall]V [::::::below:::::$185 ::::::million]A4 [after the dividends are issued]MTMP .?
It can fall after the dividends are issued, but not below $185 million.
+ - + - ?
+ - - - - - - +10 Mario Gabelli, an expert at spotting takeover candidates, says that [takeovers]A1 aren?t [::::::totally]MEXT [gone]V.?
Mario Gabelli says that takeovers are partially gone.
+ - + - - - - - - - ?
- -Table 3: Negated statements from PropBank and their interpretation considering underlying positive meaning.
Focusis underlined; ?+?
indicates that the role is present, ?-?
that it is not and ???
that it corresponds to the focus of negation.We decided to work over PropBank.
Unlike otherresources (e.g., FrameNet), gold syntactic trees areavailable.
Compared to the BioScope corpus, Prop-Bank provides semantic annotation and is not lim-ited to the biomedical domain.
On top of that, therehas been active research on predicting PropBankroles for years.
The additional annotation can bereadily used by any system trained with PropBank,quickly incorporating interpretation of negation.4.2.1 Annotation GuidelinesThe focus of a negation involving verb v is resolvedas:?
If it cannot be inferred that an action v oc-curred, focus is role MNEG.?
Otherwise, focus is the role that is most promi-nently negated.All decisions are made considering as context theprevious and next sentence.
The mark -NOT is usedto indicate the focus.
Consider the following state-ment (file wsj 2282, sentence 16).
[While profitable]MADV1,2 , [it]A11 ,A02 ?was[n?t]MNEG1[growing]v1 and was[n?t]MNEG2 [providing]v2 [a sat-isfactory return on invested capital]A12 ,?
he says.The previous sentence is Applied, then a closelyheld company, was stagnating under the manage-ment of its controlling family.
Regarding the firstverb (growing), one cannot infer that anything wasgrowing, so focus is MNEG.
For the second verb(providing), it is implicitly stated that the companywas providing a not satisfactory return on invest-ment, therefore, focus is A1.The guidelines assume that the focus correspondsto a single role or the verb.
In cases where more thanone role could be selected, the most likely focus ischosen; context and text understanding are key.
Wedefine the most likely focus as the one that yields themost meaningful implicit information.For example, in (Table 3, example 2) [He]A0could be chosen as focus, yielding someone canstomach the taste of Heinz, but not him.
However,given the previous sentence ([.
.
. ]
her husband is585While profitableMADV55MADV**itA155A0**was n?tMNEG-NOT!
!growing and was n?tMNEG<<providing a satisfacory return .
.
.A1-NOTuuFigure 1: Example of focus annotation (marked with -NOT).
Its interpretation is explained in Section 4.2.2.adamant about eating only Hunt?s ketchup), it isclear that the best option is A1.
Example (5) has asimilar ambiguity between A0 and A2, example (9)between MTMP and A4, etc.
The role that yields themost useful positive implicit information given thecontext is always chosen as focus.Table 3 provides several examples having as theirfocus different roles.
Example (1) does not carryany positive meaning, the focus is V. In (2?10) theverb must be interpreted as affirmative, as well asall roles except the one marked with ???
(i.e., thefocus).
For each example, we provide PropBank an-notation (top), the new annotation (i.e., the focus,bottom right) and its interpretation (bottom left).4.2.2 Interpretation of -NOTThe mark -NOT is interpreted as follows:?
If MNEG-NOT(x, y), then verb y must benegated; the statement does not carry positivemeaning.?
If any other role is marked with -NOT, ROLE-NOT(x, y) must be interpreted as it is not thecase that x is ROLE of y.Unmarked roles are interpreted positive; they cor-respond to implicit positive meaning.
Role labels(A0, MTMP, etc.)
maintain the same meaning fromPropBank (Palmer et al, 2005).
MNEG can be ig-nored since it is overwritten by -NOT.The new annotation for the example (Figure 1)must be interpreted as: While profitable, it (the com-pany) was not growing and was providing a not sat-isfactory return on investment.
Paraphrasing, Whileprofitable, it was shrinking or idle and was providingan unsatisfactory return on investment.
We discoveran entailment and an implicature respectively.4.3 Annotation ProcessWe annotated the 3,993 verbal negations signaledwith MNEG in PropBank.
Before annotation began,all semantic information was removed by mappingall role labels to ARG.
This step is necessary to en-sure that focus selection is not biased by the seman-Role #Inst.
Focus# ?
%A1 2,930 1,194 ?
40.75MNEG 3,196 1,109 ?
34.70MTMP 609 246 ?
40.39MMNR 250 190 ?
76.00A2 501 179 ?
35.73MADV 466 94 ?
20.17A0 2,163 73 ?
3.37MLOC 114 22 ?
19.30MEXT 25 22 ?
88.00A4 26 22 ?
84.62A3 48 18 ?
37.50MDIR 35 13 ?
37.14MPNC 87 9 ?
10.34MDIS 287 6 ?
2.09Table 4: Roles, total instantiations and counts corre-sponding to focus over training and held-out instances.tic labels provided by PropBank.As annotation tool, we use Jubilee (Choi et al,2010).
For each instance, annotators decide the fo-cus given the full syntactic tree, as well as the previ-ous and next sentence.
A post-processing step incor-porates focus annotation to the original PropBank byadding -NOT to the corresponding role.In a first round, 50% of instances were annotatedtwice.
Inter-annotator agreement was 0.72.
Aftercareful examination of the disagreements, they wereresolved and annotators were given clearer instruc-tions.
The main point of conflict was selecting a fo-cus that yields valid implicit meaning, but not themost valuable (Section 4.2.1).
Due to space con-straints, we cannot elaborate more on this issue.
Theremaining instances were annotated once.
Table 4depicts counts for each role.5 Learning AlgorithmWe propose a supervised learning approach.
Eachsentence from PropBank containing a verbal nega-tion becomes an instance.
The decision to be madeis to choose the role that corresponds to the focus.586No.
Feature Values Explanation1 role-present {y, n} is role present?2 role-f-pos {DT, NNP, .
.
.}
First POS tag of role3 role-f-word {This, to, overseas, .
.
. }
First word of role4 role-length N number fo words in role5 role-posit N position within the set of roles6 A1-top {NP, SBAR, PP, .
.
.}
syntactic node of A17 A1-postag {y, n} does A1 contain the tag postag?8 A1-keyword {y, n} does A1 cotain the word keyword?9 first-role {A1, MLOC, .
.
.}
label of the first role10 last-role {A1, MLOC, .
.
.}
label of the last role11 verb-word {appear, describe, .
.
. }
main verb12 verb-postag {VBN, VBZ, .
.
.}
POS tag main verb13 VP-words {were-n?t, be-quickly, .
.
. }
sequence of words of VP until verb14 VP-postags {VBP-RB-RB-VBG, VBN-VBG, .
.
.}
sequence of POS tags of VP until verb15 VP-has-CC {y, n} does the VP contain a CC?16 VP-has-RB {y, n} does the VP contain a RB?17 predicate {rule-out, come-up, .
.
. }
predicate18 them-role-A0 {preparer, assigner, .
.
. }
thematic role for A019 them-role-A1 {effort, container, .
.
. }
thematic role for A120 them-role-A2 {audience, loaner, .
.
. }
thematic role for A221 them-role-A3 {intensifier, collateral, .
.
. }
thematic role for A322 them-role-A4 {beneficiary, end point, .
.
. }
thematic role for A4Table 5: Full set of features.
Features (1?5) are extracted for all roles, (7, 8) for all POS tags and keywords detected.The 3,993 annotated instances are divided intotraining (70%), held-out (10%) and test (20%).
Theheld-out portion is used to tune the feature set andresults are reported for the test split only, i.e., us-ing unseen instances.
Because PropBank adds se-mantic role annotation on top of the Penn TreeBank,we have available syntactic annotation and semanticrole labels for all instances.5.1 BaselinesWe implemented four baselines to measure the diffi-culty of the task:?
A1: select A1, if not present then MNEG.?
FIRST: select first role.?
LAST: select last role.?
BASIC: same than FOC-DET but only using fea-tures last role and flags indicating the presenceof roles.5.2 Selecting FeaturesThe BASIC baseline obtains a respectable accuracyof 61.38 (Table 6).
Most errors correspond to in-stances having as focus the two most likely foci: A1and MNEG (Table 4).
We improve BASIC with anextended feature set which targets especially A1 andthe verb (Table 5).Features (1?5) are extracted for each role andcapture their presence, first POS tag and word,length and position within the roles present forthat instance.
Features (6?8) further characterizeA1.
A1-postag is extracted for the followingPOS tags: DT, JJ, PRP, CD, RB, VB and WP;A1-keyword for the following words: any, any-body, anymore, anyone, anything, anytime, any-where, certain, enough, full, many, much, other,some, specifics, too and until.
These lists of POStags and keywords were extracted after manual ex-amination of training examples and aim at signalingwhether this role correspond to the focus.
Examplesof A1 corresponding to the focus and including oneof the POS tags or keywords are:?
[Apparently]MADV , [the respondents]A0 do n?tthink [::::that:::an::::::::::economic::::::::::slowdown::::::would::::::harm:::the::::::major:::::::::::investment::::::::markets:::::::veryRB::::::much]A1.
(i.e., the responders think it would harm the in-vestements little).587?
[The oil company]A0 does n?t anticipate[::::::::::anykeyword::::::::::::additional::::::::charges]A1 (i.e., thecompany anticipates no additional charges).?
[Money managers and other bond buyers]A0haven?t [shown]V [ ::::::::::::muchkeyword::::::::interest ::in::::the::::::::Refcorp::::::bonds]A1 (i.e., they have shown littleinterest in the bonds).?
He concedes H&R Block is well-entrenchedand a great company, but says ?
[it]A1 doesn?t[grow]V [::::fast::::::::::::::enoughkeyword::::for::us]A1?
(i.e., itis growing too slow for us).?
[We]A0 don?t [see]V [:a::::::::::domestic :::::::source::::for::::::::::::somekeyword:::of::::our::::::::HDTV:::::::::::::requirements ]A1,and that?s a source of concern [.
.
. ]
(i.e., we seea domestic source for some other of our HDTVrequirements)Features (11?16) correspond to the main verb.VP-words (VP-postag) captures the full se-quence of words (POS tags) from the beginning ofthe VP until the main verb.
Features (15?16) checkfor POS tags as the presence of certain tags usuallysignal that the verb is not the focus of negation (e.g.,[Thus]MDIS, he asserts, [Lloyd?s]A0 [[ca]MMODn?t[react]v [::::::::::quicklyRB ]MMNR [to competition]A1]VP).Features (17?22) tackle the predicate, which in-cludes the main verb and may include other words(typically prepositions).
We consider the words inthe predicate, as well as the specific thematic rolesfor each numbered argument.
This is useful sincePropBank uses different numbered arguments forthe same thematic role depending on the frame (e.g.,A3 is used as PURPOSE in authorize.01 and as IN-STRUMENT in avert.01).6 Experiments and ResultsAs a learning algorithm, we use bagging with C4.5decision trees.
This combination is fast to train andtest, and typically provides good performance.
Morefeatures than the ones depicted were tried, but weonly report the final set.
For example, the parentnode for all roles was considered and discarded.
Wename the model considering all features and trainedusing bagging with C4.5 trees FOC-DET.Results over the test split are depicted in Table 6.Simply choosing A1 as the focus yields an accuracyof 42.11.
A better baseline is to always pick the lastrole (58.39 accuracy).
Feeding the learning algo-System AccuracyA1 42.11FIRST 7.00LAST 58.39BASIC 61.38FOC-DET 65.50Table 6: Accuracies over test split.rithm exclusively the label corresponding to the lastrole and flags indicating the presence of roles yields61.38 accuracy (BASIC baseline).Having an agreement of 0.72, there is still roomfor improvement.
The full set of features yields65.50 accuracy.
The difference in accuracy betweenBASIC and FOC-DET (4.12) is statistically significant(Z-value = 1.71).
We test the significance of the dif-ference in performance between two systems i and jon a set of ins instances with the Z-score test, wherez = abs(erri,errj)?d , errk is the error made using set kand ?d =?erri(1?erri)ins +errj(1?errj)ins .7 ConclusionsIn this paper, we present a novel way to semanticallyrepresent negation using focus detection.
Implicitpositive meaning is identified, giving a thorough in-terpretation of negated statements.Due to the lack of corpora annotating the focus ofnegation, we have added this information to all thenegations marked with MNEG in PropBank.
A setof features is depicted and a supervised model pro-posed.
The task is highly ambiguous and semanticfeatures have proven helpful.A verbal negation is interpreted by considering allroles positive except the one corresponding to thefocus.
This has proven useful as shown in severalexamples.
In some cases, though, it is not easy toobtain the meaning of a negated role.Consider (Table 3, example 5) P&G hasn?t soldcoffee::to::::::::airlines.
The proposed representation en-codes P&G has sold coffee, but not to airlines.
How-ever, it is not said that the buyers are likely to havebeen other kinds of companies.
Even without fullyidentifying the buyer, we believe it is of utmost im-portance to detect that P&G has sold coffee.
Empir-ical data (Table 4) shows that over 65% of negationsin PropBank carry implicit positive meaning.588ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 17th international conference on Computa-tional Linguistics, Montreal, Canada.Johan Bos and Katja Markert.
2005.
Recognising Tex-tual Entailment with Logical Inference.
In Proceed-ings of Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, pages 628?635, Vancouver, BritishColumbia, Canada.Jinho D. Choi, Claire Bonial, and Martha Palmer.
2010.Propbank Instance Annotation Guidelines Using aDedicated Editor, Jubilee.
In Proceedings of the Sev-enth conference on International Language Resourcesand Evaluation (LREC?10), Valletta, Malta.Isaac Councill, Ryan McDonald, and Leonid Velikovich.2010.
What?s great and what?s not: learning to clas-sify the scope of negation for improved sentiment anal-ysis.
In Proceedings of the Workshop on Negation andSpeculation in Natural Language Processing, pages51?59, Uppsala, Sweden.David Dowty.
1994.
The Role of Negative Polarityand Concord Marking in Natural Language Reason-ing.
In Proceedings of Semantics and Linguistics The-ory (SALT) 4, pages 114?144.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010Shared Task: Learning to Detect Hedges and theirScope in Natural Language Text.
In Proceedings ofthe Fourteenth Conference on Computational NaturalLanguage Learning, pages 1?12, Uppsala, Sweden.Jaakko Hintikka.
2002.
Negation in Logic and in NaturalLanguage.
Linguistics and Philosophy, 25(5/6).Laurence R. Horn and Yasuhiko Kato, editors.
2000.Negation and Polarity - Syntactic and Semantic Per-spectives (Oxford Linguistics).
Oxford UniversityPress, USA.Laurence R. Horn.
1989.
A Natural History of Negation.University Of Chicago Press.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press.Roser Morante and Walter Daelemans.
2009.
Learningthe Scope of Hedge Cues in Biomedical Texts.
In Pro-ceedings of the BioNLP 2009 Workshop, pages 28?36,Boulder, Colorado.Roser Morante and Caroline Sporleder, editors.
2010.Proceedings of the Workshop on Negation and Specu-lation in Natural Language Processing.
University ofAntwerp, Uppsala, Sweden.Arzucan ?Ozgu?r and Dragomir R. Radev.
2009.
Detect-ing Speculations and their Scopes in Scientific Text.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1398?1407, Singapore.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Mats Rooth.
1985.
Association with Focus.
Ph.D. thesis,Univeristy of Massachusetts, Amherst.Mats Rooth.
1992.
A Theory of Focus Interpretation.Natural Language Semantics, 1:75?116.Carolyn P. Rose, Antonio Roque, Dumisizwe Bhembe,and Kurt Vanlehn.
2003.
A Hybrid Text ClassificationApproach for Analysis of Student Essays.
In In Build-ing Educational Applications Using Natural LanguageProcessing, pages 68?75.Victor Sa?nchez Valencia.
1991.
Studies on Natural Logicand Categorial Grammar.
Ph.D. thesis, University ofAmsterdam.Roser Saur??
and James Pustejovsky.
2009.
FactBank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Elly van Munster.
1988.
The treatment of Scope andNegation in Rosetta.
In Proceedings of the 12th In-ternational Conference on Computational Linguistics,Budapest, Hungary.Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-orgy Mora, and Janos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9+.Michael Wiegand, Alexandra Balahur, Benjamin Roth,Dietrich Klakow, and Andre?s Montoyo.
2010.
A sur-vey on the role of negation in sentiment analysis.
InProceedings of the Workshop on Negation and Specu-lation in Natural Language Processing, pages 60?68,Uppsala, Sweden, July.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing Contextual Polarity: An Explo-ration of Features for Phrase-Level Sentiment Analy-sis.
Computational Linguistics, 35(3):399?433.H.
Zeijlstra.
2007.
Negation in Natural Language: Onthe Form and Meaning of Negative Elements.
Lan-guage and Linguistics Compass, 1(5):498?518.589
