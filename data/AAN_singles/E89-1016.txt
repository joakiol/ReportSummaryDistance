User studies and the design of Natural Language SystemsSteve Whi t taker  and Phi l  StentonHewlett -Packard Laborator iesF i l ton Road,  Bristol BS12 6QZ, UK.email: s jw~hplb .hp l .hp .comAbstractThis paper presents a critical discussion of the vari-ous approaches that have been used in the evaluationof Natural Language systems.
We conclude that pre-vious approaches have neglected to evaluate systemsin the context of their use, e.g.
solving a task requir-ing data retrieval.
This raises questions about thevalidity of such approaches.
In the second half of thepaper, we report a laboratory study using the Wizardof Oz technique to identify NL requirements for carry-ing out this task.
We evaluate the demands that taskdialogues collected using this technique, place upona prototype Natural Language system.
We identifythree important requirements which arose from thetask that we gave our subjects: operators specific tothe task of database access, complex contextual refer-ence and reference to the structure of the informationsource.
We discuss how these might be satisfied byfuture Natural Language systems.1 Int roduct ion1.1  Approaches  to  the  eva luat ion  o fNL  sys temsIt is clear that a number of different criteria mightbe employed in the evaluation of Natural Language(NL) systems.
It is also clear that there is noconsensus on how evaluation should be carried out\[RQR*88, GM84\].
Among the different criteria thathave been suggested are (a) Coverage; (b) Learnabil-ity; (c) General software requirements; (d) Compar-ison with other interface media.
Coverage is con-cerned with the set of inputs which the system shouldbe capable of handling and one issue we will discussis how this set should be identified.
Learnabi l i ty ispremised on the fact that complete coverage is notforseeable in the near future.
As a consequence, anyNL system will have limitations and one problem forusers will be to learn to communicate within suchlimitations.
Learnability is measured by the easewith which new users are able to identify these cov-erage limitations, and exploit what coverage is avail-able to carry out their task.
The general softwarecriteria of importance are speed, size, modifiabil-ity and installation and maintenance osts.
Com-parison studies have mainly required users to per-form the same task using either a formal query lan-guage such as SQL or a restricted natural anguageand evaluated one against he other on such param-eters as time to solution or number of queries pertask\[SW83, JTS*85\].
Our discussion will mainly ad-dress the problem of coverage: we shall not discussthese other issues further.Our concern here will be with interactive NL in-terfaces and not other applications of NL technologysuch as MT or messaging systems.
Interactive inter-faces are not designed to be used in isolation, rather,they are intended to be connected to some sort ofbackend system, to improve access to that system.Our view is that NL systems should be evaluated withthis in mind: the aim will be to identi fy the NL in-puts which a typical  user would want to enterin order to uti l lse that  backend system to carryout  a representat ive task.
By representative taskwe mean the class of task that the back-end systemwas designed to carry out.
In the case of databases,this would be accessing or updating information.
Forexpert systems it might involve identifying or diag-nosing faults.
?I .
I .
I  Tes t  su i tesOne method that is often used in computer science forthe evaluation of systems i the use of test suites.
ForNL systems the idea is to generate a corpus of sen-tences which contains the major set of syntactic, se-- 116-mantic and pragmatic phenomena the system shouldcover \[BB84, FNSW87\].
One problem with this ap-proach is how we determine whether the test set iscomplete.
Do we have a clear notion of what consti-tute the major phenomena of language so that we cangenerate test sentences which identify whether thesehave been analysed correctly?
Theories of syntax arewell developed and may provide us with a good tax-onomy of syntactic phenomena, but we do not havesimilar classifications of key pragmatic requirements.There are two reasons why current approaches mayfail to identify the key phenomena.
Current est setsare organised on a single-utterance basis, with certainexceptions such as intersentential anaphora nd ellip-sis.
Now it may be that more complex discourse phe-nomena such as reference to dialogue structure arisewhen systems are being used to carry out tasks, be-cause of the need to construct and manipulate sets ofinformation \[McK84\].
In addition, context may con-tribute to inputs being fragmentary or telegraphicin style.
Unless we investigate systems being usedto carry out tasks, such phenomena will continue tohe omitted from our test suites and NL systems willhave to be substantially modified when they are con-nected to their backend systems.
Thus we are notarguing against he use of test suites in principle butrather are attempting to determine what methodol-ogy should be used to design such test suites.1.1.2 Field studiesIn field studies, subjects are given the NL  inter-face connected to some application and encouragedto make use of it.
It would seem that these stud-ies would offer vital information about target re-quirements.
Despite arguments that such studies arehighly necessary \[Ten79\], few systematic studies havebeen conducted \[Dam81, JTS*85, Kra80\].
The prob-lem here may be with finding committed users whoare prepared to make serious use of a fragile system.A major problem with such studies concerns therobustness of the systems which were tested and thisleads to difficulties in the interpretation of the results.This is because a fragile system necessarily imposeslimitations on the ways that a user can interact withit.
We cannot therefore infer that the set of sentencesthat users input when they have adjusted to a frag-ile system, reflects the set of inputs that they wouldwish to enter given a system with fewer limitations.In other words we cannot infer that such inputs repre-sent the way that users would ideally wish to interactusing NL.
The users may well have been employingstrategies to communicate within the limitations ofthe system and they may therefore have been usinga highly restricted form of English.
Indeed the exis-tence of strategies such as paraphrasing and syntaxsimplification when a query failed, and repetition ofinput syntax when a query succeeded has been doc-umented \[ThoS0, WW89\].Since we cannot currently envisage a system with-out limitations, we may want to exploit this ability tolearn system limitations, nevertheless the existence ofsuch user strategies does not give us a clear view ofwhat language might have been used in the absenceof these limitations.1.1.3 Pen  and paper tasksOne technique which overcomes some of the prob-lems of robustness has been to use pen and papertasks.
Here we do not use a system at all but rathergive subjects what is essentially a translation task\[JTS*85, Mil81\].
This technique has also been em-ployed to evaluate formal query languages such asSQL.
The subjects of the study are given a sampletask: A list of alumni in the state of California hasbeen requested.
The request applies to those alumniwhose last name starts with an S. Obtain such a listcontaining last names and first names.
When thesubjects have generated their natural language query,it is evaluated by judges to determine whether itwould have successfully elicited the information fromthe system.This approach avoids the problem of using fragilesystems, but it is susceptible to the same objectionsas were levelled at test suites: a potential drawbackwith the approach concerns the representativeness ofthe set of tasks the users are required to do whenthey carry out the translation tasks.
For the tasksdescribed by Reisner, for example, the queries are allone shot, i.e.
they are attempts to complete a taskin a single query \[Rei77\].
As a result the translationproblems may fail to test the system's coverage ofdiscourse phenomena.1.1.4 Wizard of  OzA similar technique to pen and paper tasks has beenthe use of a method called the "Wizard of Or" (hence-forth WOZ)  which also avoids the problem of thefragility of current systems by simulating the opera-tion of the system rather than using the system itself.- 117-In these studies, subjects are told that they are in-teracting with the computer when in reality they arelinked to the Wizard, a person simulating the opera-tion of the system, over a computer network.In Guindon's study using the WOZ technique,subjects were told they were using an NL  front-end to a knowledge-based statistics advisory package\[GSBC86\].
The main result is a counterintuitive one.These studies suggest that people produce "simplelanguage" when they believe that they are using anNL  interface.
Guindon has compared the WOZ dia-logues of users interacting with the statistics package,to informal speech, and likened them to the simplifiedregister of "baby talk" \[SF7?\].
In comparison withinformal speech, the dialogues have few passives, fewpronouns and few examples of fragmentary speech.One problem with the research is that it hasbeen descriptive: It has chiefly been concerned withdemonstrating the fact that the language observedis "simple" relative to norms gathered for informaland written speech and the results are expressed attoo general a level to be useful for system design.It is not enough to know, for example, that thereare fewer fragments observed in WOZ type dialoguesthan in informal speech: it is necessary to know theprecise characteristics of such fragments if we are todesign a system to analyse these when they occur.Despite this, our view is that WOZ represents themost promising technique for identifying the targetrequirements of an NL interface.
However, to avoidthe problem of precision described above, we modifiedthe technique in one significant respect.
Having usedthe WOZ technique to generate a set of sentences thatusers ideally require to carry out a database retrievaltask, we then input these sentences into a NL systemlinked to the database.
The target requirements aretherefore valuated against a version of a real systemand we can observe the ways in which the systemsatisfies, or fails to satisfy, user requirements.We discuss semantics and pragmatics only insofar asthey are reflected in individual lexical items.
Thisis of some importance, given the lexical basis of theHPNL  system.
It must also be noted that the evalua-tion took place against a prototype version of HPNL.Many of the lexical errors we encountered could beremoved with a trivial amount of effort.
Our inter-est was not therefore in the absolute number of sucherrors, but rather with the general classes of lexicalerrors which arose.
We present a classification of sucherrors below.The task we investigated was database retrieval.This was predominantly because this has been a typ-ical application for NL  interfaces.
Our initial inter-est was in the target requirements for an NL  system,i.e.
what set of sentences users would enter if theywere given no constraints on the types of sentencesthat they could input.
The Wizard was therefore in-structed to answer all questions (subject to the limi-tation given below).
We ensured that this person hadsufficient information to answer questions about thedatabase, and so in principle, the system was capableof handling all inputs.The subjects were asked to access information fromthe "database" about a set of paintings which pos-sessed certain characteristics.
The database con-tained information about Van Gogh's paintings in-cluding their age, theme, medium, and location.
Thesubjects had to find a set of paintings which togethersatisfied a series of requirements, and they did thisby typing English sentences into the machine.
Theywere not told exactly what information the databasecontained, nor about the set of inputs the NaturalLanguage interface might be capable of processing.2 Method1.2 The  cur rent  s tudyThe  current study therefore has two components: thefirst is a WOZ study of dialogues involved in databaseretrieval tasks.
We then take the recorded ialoguesand map them onto the capabilities of an existingsystem, HPNL \[NP88\] to look at where the languagethat the users produce goes beyond the capabilitiesof this system.
The results we present concern thefirst phase of such an analysis in which we discussthe set of words that the system failed to analyse.2.1 Sub jec tsThe 12 subjects were all familiar with using com-puters insofar as they had used word processors andelectronic mail.
A further 5 of them had used omceapplications such as spreadsheets or graphics pack-ages.
Of the remainder, 4 had some experience withusing databases and one of these had participated inthe design a database.
None of them was familiarwith the current state of NL  technology.- 118-2.2 Procedure hard copy.The experimenter told the subjects that he was in-terested in evaluating the efficiency of English as amedium for communicating with computers.
He toldthem that an English interface to a database was run-ning on the machine and that the database containedinformation about paintings by Van Gogh and otherartists.
In fact this was not true: the information thatthe subjects typed into the terminal was transmittedto a person (The Wizard) at another terminal whoanswered the subject's requests by consulting papercopies of the database tables.The experimenter then gave the details of the twotasks.
Subjects were told that they had to find a set ofpaintings which satisfied several requirements, wherea requirement might be for example that (a) all thepaintings must come from different cities; or (b) theymust all have different themes.
Having found this set,they had then to access particular information aboutthe set of pictures that they had chosen, e.g the paintmedium for each of the pictures chosen.3 Resu l ts3.1 Preliminary analysis and filteringThis analysis is concerned with user input and so theWizard's responses are not considered here.
We be-gan by taking all the 384 subject utterances, enteringthem into the NL prototype and observing what anal-ysis the system produced.
We found that by far thelargest category of errors was unknown words, so webegan by analysing the total of 401 instances of 104unknown words.Our interest here lay in the influence of the taskon language use so we focus on 3 classes of unknownwords which demonstrate this in different ways: thesewere operators and explicit reference to set proper-ties; references to context; and references to the in-formation source.Our interest was in the target set of queries inputby people who wanted to use the system for databaseaccess.
We therefore gave the Wizard instructions toanswer all queries regardless of linguistic omplexity.There was however one exception to this rule: eachtask was expressed as a series of requirements and onepossible strategy for the task was to enter all theserequirements a  one long query.
If the Wizard had an-swered this query then the dialogue would have beenextremely short, i.e it would have been one query anda response which was the answer to the whole task.To prevent this, the .Wizard was told to reply to suchlong queries by saying Too much information to pro-cess.
There were no other constraints on the typeof input that the Wizard could process and answerswere given to all other types of query.Subject and Wizard both used HP-Unix Worksta-tions and communicated by writing in networked Xwindows.
The inputs of both subject and Wizardwere displayed in a single window on each of the ma-chines with the subject's entries presented in lowercase and the Wizard's in upper case, so the con-tents of the display windows on both machines wereidentical.
To avoid teaching the subjects kills likescrolling, we also provided them with hard copy out-put of the whole of the interaction by printing thecontents of the windows to a printer next to the sub-jeet's machine.
If they wanted to refer back to muchearlier in the dialogue, the subjects could consult he3.1.1 Operators and the explicit specificationof set propert iesThe task of database access involves the constructionand manipulation of answer sets with various prop-erties.The unknown words that were used for set con-struction and manipulation were mainly verbs.
Thesewe called operators.
They can be further subclassi-fled into verbs which were used to select sets, thosewhich were used to permute  already constructedsets and those which operate over a set of queries.The majority of operators invoked simple set selec-tion: these included for example, state and tell.
Therewere also instances of indirect requests for selection,e.g.
need and want.
Subjects tried to permute thepresentation of sets by using words like arrange.
Fi-nally queries uch as All the conditions from now onwill apply to .
.
.
show there were verbs which oper-ated over sets of queries.A second way in which these set manipulation op-erations appeared was in the subjects' explicit ref-erence to the fact that they were constructing setswith specific properties.
Find paintings that satisfythe following criteria .
.
.
was an example of this.Altogether operators and explicit reference to set- 119-properties occurred on 102 occasions which accountedfor 25% of the unknown words.3.1.2 References to contextThe task could not be accomplished in one query sowe expected that this would necessitate our subjectsmaking reference to previous queries.
We thereforewent on to analyse those unknown words that re-quired information from outside the current queryfor their interpretation.
Among the unknown wordswhich relied upon context, we distinguished betweenwhat we called pointers (N = 42 instances) and ex-clusion operators (N = 21 instances).
Togetherthey accounted for 16% of unknown words.Pointers ignalled to the listener that the referenceset lay outside the current utterance.
These could befurther subdivided according to whether or not theypointed forwards, e.g.
Give me the dates of the fol-lowing paintings .
.
.
or backwards in the dialogue,e.g.
previous and above.
There were two instances offorwards pointers following and now on.The backwards pointers could be subclassified ac-cording to how many previous answer sets they re-ferred to.
The majority referred to a single answerset and this was most often the one generated by theimmediately prior query.
Other pointers referred toa number of prior answer sets, which could scope asfar back as the beginning of the current subdialogue,or even the beginning of the whole dialogue.Exclusion operators applied to sets created ear-lier in the dialogue.
They served to exclude lementsof these sets from the current query.
The simplest ex-amples of this occurred when people had (a) identifieda set previously; (b) they had then selected a subsetof this original set; and (c) they wanted all or part ofthe set of the original set which had not been selectedby the second opergtion.
These included words likeanother and more, as in Give me I0 more Van Goghpaintings.A more complex instance of this type of exclusionwas when the word was used, not to exclude sub-sets from sets already identified, but to exclude theattributes of the items in the excluded subsets, e.g.Find me a painting with a theme that is differentfrom those already mentioned.
Here the system hasfirst to generate the set of paintings already men-tioned, then it has to generate their themes and thenfinally it has to find a painting whose theme is differ-ent from the set of themes already identified.3.1.3 References to the informat ion sourceOur subjects believed that they were interacting witha real information source, in this case a database, alsoseemed to affect heir language use.
We found 19 (5%of all unknown words) which seemed to refer to thedatabase and its structure directly.There were words which seemed to refer to fieldnames in the database, e.g.
categories and infor-mation, e.g.
What in format ion on each painting isthere?
There were also words which seemed to referto values within a field, e.g.
types as in List the me-dia types.
In addition, there were references to theordering of entities, e.g.
first or second, as in Whatis the first painting in your list?.
Finally, there werewords which referred to the general scope or prop-erties of the database: e.g.
database and represented,e.g.
What different paint media are represented?.There were also 3 occasions on which referenceis made both to database structure and to context.These are the instances of next being used to accessentities in a column but also referring to context.
Theutterance List next 10 paintings, references 10 itemsin the sequence that they appear in the database,but excludes the 10 items already chosen.
Finallythere was one instance of a question which wouldhave required inferencing based on the structure ofthe information source, Is a portrait the same as aself portrait?.
Here the question was about the typerelation.4 Conclus ionsThis paper had two objectives: the first was to eval-uate the use of the WOZ technique for assessing NLsystems and the second was to investigate the effectof task on language use.One criticism we made of both test suites and tasksusing pen and paper, was that they may attempt toevaluate systems against inadequate criteria.
Specif-ically they may not evaluate the adequacy of NL sys-tems when users are carrying out tasks with specificsoftware systems.
The unknown words analysis eemsto bear this out: we found 3 classes of unknown wordswhich occurred only because our users were doing atask.
Firstly our users wanted to carry out operations- 120  -involving the selection and permutation of answersets and make explicit reference to their properties.Secondly, we found that our subjects wanted to usecomplex reference to refer back to previous queries inorder to refine those queries, or to exclude answersto previous queries from their current query.
Finally,we found that users attempted touse the structure ofthe information source, in this case the database, inorder to access information.
Together these 3 classesaccounted for 45% of all unknown words.
We be-lieve that whatever the task and software, there willalways be instances of operators, context use and ref-erence to the information source.
It would thereforeseem that coverage of these 3 sets of phenomena is animportant requirement for any NL interface to an ap-plication.
The fact that other evaluation techniquesmay not have detected this requirement is, we be-lieve, a vindication of our approach.
An exception tothis is the work of Cohen et al \[CPA82\] who pointto the need for retaining and tracking context in thistype of application.Of course there are still problems with the WOZtechnique.
One such problem concerns the task rep-resentativeness and a difficulty in designing this studylay in the selection of a task which we felt to be typi-cal of database access.
Clearly more information fromfield studies would be useful in helping to identifyprototypical database access tasks.A second problem lies in the interpretation f theresults with respect to the classification and fre-quency of the unknown word errors: how frequentlymust an error occur if it is to warrant system modi-fication?
For example, references to the informationsource accounted for only 5% of the errors and yetwe believe this is an interesting class of error becauseexploiting the structure of the database was a usefulretrieval tactic for some users.
The frequency prob-lem is not specific to this study, but is an instance ofa general problem in computational linguistics con-cerning the coverage and the range of phenomena towhich we address our research.
In the past, the fieldhas focussed on the explanation oftheoretically inter-esting phenomena without much attention to theirfrequency in naturally occurring speech or text.
Iti s  clear, however, that if we are to be successful indesigning working systems, then we cannot afford toignore frequently occurring but theoretically uninter-esting phenomena such as punctuation ordates.
Thisis because such phenomena will probably have to betreated in whatever application we design.
Frequencydata may also be of real use in determining prioritiesfor system improvement.As a result of using our technique, we have iden-tified a number of unknown words.
How shouldthese words be treated?
Some of the unknown wordsare synonyms of words already in the system.
Herethe obvious trategy is to modify the NL system byadding these.
In other cases, system modificationmay not be possible because linguistic theory doesnot have a treatment of these words.h In these cir-cumstances, there are three possible strategies for fi-nessing the problem.
The first two involve encour-aging users to avoid these words, either by gener-ating co-operative error messages to enable the userto rephrase the query and so avoid the use of theproblematic word \[Adg88, Ste88\] or by user training.The third strategy for finessing the analysis of suchwords is to supplement the NL interface with anothermedium such as graphics, and we will describe an ex-ample of this below.We believe that the use of such finessing strategieswill be important if NL systems are to be usable inthe short term.
Our data suggests that certain wordsare used frequently by subjects in doing this task.It is also clear that computational linguistics has notreatment ofthese words.
If we wish to build a systemwhich will enable our users to carry out the task, wemust be able to respond in some way to such inputs.The above techniques may provide the means to dothis, although the use of such strategies i still anunder-researched area.For the unknown words encountered in this study,of the operators, many can be dealt with by sim-ple system modification because they are synonymsof list or show.
Within the class of operators, how-ever, it would seem that new semantic interpretationprocedures would have to be defined for verbs like ar-range or order.
These would involve two operations,the first would be the generation ofa set, and the sec-ond the sorting of that set in terms of some attributesuch as age or date.
The unknown words relating toexplicit reference to set properties would not be dif-ficult to add to the system, given that they can beparaphrased asrelative clauses.
For example, the sen-tence Find Van Gogh paintings to include four dif-ferent themes can be paraphrased asFind Van Goghpaintings that have different hemes.The context words present a much more seriousproblem.
Current linguistic theory does not havetreatments of words like previously or already, interms of how these scope in dialogues.
On some oc-casions, these are used to refer to the immediatelyprior query only, whereas on other occasions they- 121  -might scope back to the beginning of the dialogue.In addition, words like more or another present newproblems for discourse theory in that they require x-tensional representations of answers: Given the queryGive me 10 paintingsfollowedby Now give me 5 morepaintings, the system has to retain an extensional rep-resentation of the answer set generated to the firstquery, if it is to respond appropriately to the secondone.
Otherwise it will not have a record of preciselywhich 10 paintings were originally selected, so thatthese can be excluded from the second set.
This ex-tensional record would have to be incorporated intothe discourse model.One solution to the dual problems presented bycontext words is again to either finesse the use of suchwords or to use a mixed media interface of NL andgraphics.
If users had the answers to previous queriespresented on screen, then the problems of determin-ing the reference set for phrases like the paintings al.ready mentioned could be solved by allowing the usersto click on previous answer sets using a mouse, thusavoiding the need for reference resolution.For the references to the information source,it would not be difficult to modify the system soit could analyse the majority of the the specific in-stances recorded here, but it is not clear that all ofthem could have been solved in this way, especiallythose that require some form of inferencing based onthe database structure.There are also a number of unknown words in thedata that have not been discussed here, because thesedid not directly arise from the fact that our users werecarrying out a task.
Nevertheless, the set of strate-gies given above is also relevant o these.
Just aswith the task specific words, there are a number ofwords which can be added to the system with rel-atively little effort.
The system can be modified tocope with the majority of the open class unknownwords, e.g.
common nouns, adjectives, and verbs,many of which are simple omissions from the domain-specific lexicon.
Some of the closed class words suchas prepositions and personal pronouns may also provestraightforward to add.There are also a number of these words which didnot arise from the task, which are more difficult toadd to the system.
This is true for a few the openclass words domain-independent words, including ad-jectives like same and different.
The majority of theclosed class words, may also be difficult to add tothe system, including superlatives and various logi-cal connectives, then, neither, some quantifiers, e.g.only, as well as words which relate to the control ofdialogue such as right and o.k..
These words indi-cate genuine gaps in the coverage of the system.
Forthese difficult words, it might necessary to finesse theproblem of direct analysis.In conclusion, the WOZ technique proved success-ful for NL evaluation.
We identified 3 classes oftask based language use which have been neglectedby other evaluation methodologies.
We believe thatthese classes exist across applications and tasks: Forany combination ofapplication and task, specific op-erators will emerge, and support will have to be pro-vided to enable reference to context and informationstructure.
In addition, we were able to suggest anum-ber of strategies for dealing with unknown words.
Forcertain words, NL system modification can be easilyachieved.
For others, different strategies have to beemployed which avoid direct analysis of these words.These finessing strategies are important if NL sys-tems are to usable in the short term.5 AcknowledgementsThanks to Lyn Walker, Derek Proudian, and DavidAdger for critical comments.References\[AdgS8\]\[BB84\]\[CPA82\]\[Dam81\]David Adger.
Heuristic input redaction.Technical Report, Hewlett-Packard Lab-oratories, Bristol, 1988.Madeleine Bates and Robert J. Bobrow.What's here, what's coming, and whoneeds it.
In Walter Reitman, editor, Ar-tificial Intelligence Applications for Busi-ness, pages 179-194, Ablex PublishingCorp, Norwood, N.J., 1984.Phillip R. Cohen, C. Raymond Perrault,and James F. Allen.
Beyond questionanswering.
In Wendy Lehnert and Mar-tin Ringle, editors, Strategies for Natu-ral Language Processing, pages 245-274,Lawrence Erlbaum Ass.
Inc, Hillsdale,N.J., 1982.Fred J. Damerau.
Operating statistics forthe transformational question answeringsystem.
American Journal of Computa-tional Linguistics, 7:30-42, 1981.- 122 -\[FNSW87\]\[CM84\]\[aSBC86\]\[JTS*85\]\[Kra80\]\[McK84\]\[MilS1\]\[NP88\]\[Rei77\]\[RQR*88\]Daniel Fliekinger, John Nerbonne, IvanSag, and Thomas Wasow.
Towards eval-uation of nip systems.
1987.
Presentedto the 25th Annual Meeting of the Asso-ciation for Computational Linguistics.G.
Guida and G. Mauri.
A formal ba-sis for performance evaluation of naturallanguage understanding systems.
Amer-ican Journal of Computational Linguis-tics, 10:15-29, 1984.Raymonde Guindon, P. Sladky, H. Brun-ner, and J. Conner.
The structure ofuser-adviser dialogues: is there methodin their madness?
In Proc.
24st An-nual Meeting of the ACL, Associationof Computational Linguistics, pages 224-230, 1986.Matthias Jarke, Jon A. Turner, Ed-ward A. Stohr, Yannis Vassiliou, Nor-man H. White, and Ken Michielsen.
Afield evaluation of natural anguage fordata retrieval.
IEEE Transactions onSoftware Engineering, SE-11, No.1:97-113, 1985.Jurgen Kranse.
Natural anguage accessto information systems: an evaluationstudy of its acceptance byend users.
In-formation Systems, 5:297-318, 1980.Kathleen R. MeKeown.
Natural languagefor expert systems: comparisons withdatabase systems.
In COLING84: Proc.lOth International Conference on Compu-tational Linguistics.
Stanford University,Stanford, Ca., pages 190-193, 1984.L.
A. Miller.
Natural anguage program-ming: styles, strategies and contrasts.IBM Systems Journal, 20:184- 215, 1981.John Nerbonne and Derek Proudian.
TheHPNL System Report.
Technical Re-port STL-88-11, Hewlett-Packard Labo-ratories, Palo Alto, 1988.Phyllis Reisner.
Use of psychological ex-perimentation as an aid to developmentof a query language.
IEEE Transactionson Software Engineering, SE-3, No.3:219-229, 1977.W.
Read, A. Quiliei, J. Reeves, M. Dyer,and E. Baker.
Evaluating natural an-guage systems: a sourcebook approach.\[SF77\]\[Ste88\]\[sw83\]\[Ten79\]\[Tho80\]\[ww891In COLING88: Proc.
l~th InternationalConference on Computational Linguis-tics, Budapest, Hungary, 1988.C.
Snow and C. A. Ferguson.
Talkingto children.
Cambridge University Press,1977.Phil Stenton.
Natural Language: asnapshot aken from ISC January 1988.Technical Report HPL-BRC-TR-88-022,Hewlett-Packard Laboratories, Bristol,U.K., 1988.Duane W. Small and Linda J. Weldon.An experimental comparison of naturaland structured query languages.
HumanFactors, 25(3):253-263, 1983.Harry R. Tennant.
Evaluation of NaturalLanguage Processors.
PhD thesis, Uni-versity of Illinois Urbana, 1979.Bozena Henisz Thompson.
Linguis-tic analysis of natural language com-munication with computers.
In COL-ING80: Proc.
8th International Con-ference on Computational Linguistics.Tokyo, pages 190-201, 1980.Marilyn Walker and Steve Whittaker.When Natural Language is Better thanMenus: A Field Study.
Technical Re-port, Hewlett Packard Laboratories, Bris-tol, England, 1989.- 123-
