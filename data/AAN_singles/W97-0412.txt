Six Issues in Speech Translat ionMark SeligmanUniversit4 Joseph FourierGETA, CLIPS, IMAG-campus, BP 53150, rue de la Chimie38041 Grenoble Cedex 9, Francese l igman@cer  f. netAbstractThis position paper sketches theauthor's research in six areas related tospeech translation: interactive disambig-uation; system architecture; the interfacebetween speech recognition and analysis;the use of natural pauses for segmentingutterances; dialogue acts; and the trackingof lexical co-occurrences.IntroductionThis position paper reviews some aspects of myresearch in speech translation since 1992.
Since thepurpose is to prompt discussion, the treatment isinformal and speculative, with frequent reference towork in progress.The paper sketches work in six areas: interactivedisambiguation; system architecture; the interfacebetween speech recognition and analysis; the use ofnatural pauses for segmenting utterances; dialogueacts; and the tracking of lexical co-occurrences.There is no attempt to provide a balanced survey ofthe speech translation scene.
However, I havetouched upon a number of research areas whichseem to me of particular interest.1 Interactive DisambiguationAt the present state of the art, several stages ofspeech translation leave ambiguities which currenttechniques cannot yet resolve correctly andautomatically.
Such residual ambiguity plaguesspeech recognition, analysis, transfer, andgeneration alike.Since users generally can resolve theseambiguities, it seems reasonable to incorporatefacilities for interactive disambiguation i to speechtranslation systems, especially those aiming forbroad coverage.
A good idea of the range of work inthis area can be gained by browsing the proceedingsof MIDDIM-96 (the International Seminar onMultimodal Interactive Disambiguation, Col dePorte, France, August 11 - 15, 1996).In fact, (Seligman 1997) suggests that, bystressing such interactive disambiguation - -  forinstance, by using isolated-word ictation ratherthan connected speech for input, and by adaptingexisting techniques for interactive disambiguation ftext translation (Boitet 1996, Blanchon 1996) - -practically usable speech translation systems maybe constructable in the near term.
The paper alsoreports on an Internet-based demo along these lines(Kowalski, Rosenberg, and Krause 1995).
In such"quick and dirty" or "low road" speech translationsystems, user interaction is substituted for systemintegration.
For example, the interface betweenspeech recognition and analysis can be suppliedentirely by the user, who can correct SR resultsbefore passing them to translation components,thus bypassing any attempt at effectivecommunication r feedback between SR and MT.The argument, however, is not that the "high road"toward integrated and maximally automatic systemsshould be abandoned.
Rather, it is that the "lowroad" of forgoing integration and embracinginteraction may offer the quickest route towidespread usability, and that experience with realuse is vital for progress.
Clearly, the "high road" isthe most desirable for the longer term: integrationof knowledge sources is a fundamental issue forboth cognitive and computer science, andmaximally automatic use is intrinsically desirable.The suggestion, then, is that the low and high roadsbe traveled in tandem; and that even systems aimingfor full automaticity recognize the need forinteractive resolution when automatic resolution isinsufficient.2 System ArchitectureAn ideal architecture for "high road" or integration-intensive speech translation systems would allowglobal coordination of, cooperation between, andfeedback among, components (speech recognition,analysis, transfer, etc.
), thus moving away fromlinear or pipeline arrangements.
For instance,83speech recognition, as it moves through anutterance, should be able to benefit frompreliminary analysis results for segments earlier inthe utterance.
The architecture should also bemodular, so that a variety of configurations can betried: it should be possible, for  instance, toexchange competing speech recognitioncomponents; and it should be possible to combinecomponents not explicitly intended for worktogether, even if these are written in differentlanguages or running on different machines.Blackboard architectures have been proposed(Erman and Lesser, 1980) to permit cooperationamong components.
In such systems, allparticipating components read from and write to acentral set of data structures - -  the blackboard.
Toshare this common area.
however, the componentsmust all "speak a common (software) language".Modularity thus suffers, since it is difficult toassemble a system from components developedseparately.
Further, blackboard systems are widelyseen as difficult to debug, since control is typicallydistributed, with each component determiningindependently when to act and what actions to take.In order to maintain the cooperative benefits of ablackboard system while enhancing modularity andfacilitating central coordination or control ofcomponents, (Seligman and Boitet 1994 and Boitetand Seligman 1994) proposed and demonstrated a"whiteboard" architecture for speech translation.
Asin the blackboard architecture, a central datastructure is maintained which contains selectedresults of all components.
However, thecomponents do not access this "whiteboa.d"directly.
Instead, only a privileged program calledthe Coordinator can read from it and write to it.Each component communicates with theCoordinator and the whiteboard via a go-betweenprogram called a manager, which handles messagesto and from the Coordinator in a set of mailboxfiles.
Because files are used as data holding areas inthis way, components (and their managers) can befreely distributed across many machines.
Managersare not only mailmen, but interpreters: theytranslate between the reserved language of thewhiteboard and the native languages of thecomponents, which are thus free to differ.
In ourdemo, the whiteboard was maintained in acommercial Lisp-based object-oriented language,while components included independently.developedspeech recognition, analysis, and word-lookupcomponents written in C. Overall, the whiteboardarchitecture can be seen as an adaptation ofblackboard architectures for client-server operations:the Coordinator becomes the main client for severalcomponents behaving as servers.Since the Coordinator surveys the whiteboard, inwhich are assembled the selected results of allcomponents, all represented in a single softwareinterlingua, it is indeed well situated to providecentral or global coordination.
However, any de~'eeof distributed control can also be achieved byproviding appropriate programs alongside theCoordinator which represent the components fromthe whiteboard side.
That is, to dilute theCoordinator's omnipotence, a number of demi-godscan be created.
In one possible partly-distributedcontrol structure, the Coordinator would oversee aset of agendas, one or more for each component.A closely-related effort to create a modular"agent-based" (client-server-style) architecture with acentral data structure, usable for many sorts ofsystems including speech translation, is described in(Julia et al1997).
Lacking a central board but stillaiming in a similar spirit for modularity in varioussorts of translation applications is the projectdescribed in (Zajac and Casper 1997).3 Interface between SpeechRecognition and MT AnalysisIn a certain sense, speech recognition and analysisfor MT are comparable problems.
Both require therecognition of the most probable sequences ofelements.
In speech recognition, sequences of shortspeech segments must be recognized as phones, andsequences of phones must be recognized as words.In analysis, sequences of words must be recognizedas phrases, sentences, and utterances.Despite this similarity, current speech translationsystems use quite different echniques for phone,word, and syntactic recognition.
Phone recognitionis generally handled using hidden Markov models(HMMs); word recognition is often handled usingViturbi-style search for the best paths in phonelattices; and sentence recognition is handled througha variety of parsing techniques.It can be argued that these differences are justifiedby differences of scale, perplexity, andmeaningfulness.
On the other hand, they introducethe need for interfaces between processing levels.The processors may thus become black boxes toeach other, when seamless connection and easycommunication might well be preferable.
Inparticular, word recognition and syntactic analysis(of phrases, sentences, and utterances) hould have alot to say to each other: the probability of a wordshould depend on its place in the top-down contextof surrounding words, just as the probability of aphrase or larger syntactic unit should depend on thebottom-up information of the words which itcontains.84To integrate speech recognition and analysismore tightly, it is possible to employ a singlegrammar for both processes, one whose terminalsare phones and whose non-terminals are words,phrases, sentences, etc.'
This phone-groundedstrategy was used to good effect e.g.
in the HMM-LR speech recognition component of the ASURAspeech translation system (Morimotb et al1993), inwhich an LR parser extended a parse phone byphone left to right while building a full syntactictree.
: The technique worked well for scriptedexamples.
For spontaneous examples, however,performance was unsatisfactory, because of thegaps, repairs, and other noise common inspontaneous speech.
To deal with such structuralproblems, an island-driven parsing style might wellbe preferable.
An island-based chart parser, like thatof (Stock et al1989), would be a good candidate.However, chart initialization presents sometechnical problems.
There is no difficulty incomputing a lattice from spotted phones, giveninformation regarding the maximum gap andoverlap of phones.
But it is not trivial to convertthat lattice into a "chart" (i.e.
multi-path finite stateautomaton) without introducing spurious extrapaths.
The author has implemented a Common Lispprogram which does so correctly, based on analgorithm by Christian Boitet.
Experiments withbottom-up island-driven chart parsing from chartsinitialized with phones are anticipated.4 Use of Pauses for SegmentationIt is widely believed that prosody can prove crucialfor speech recognition and analysis of spontaneousspeech, but effective demonstrations have been few.Several aspects of prosody might be exploited: pitchcontours, rhythm, volume modulation, etc.However, (Seligman, Hosaka, and Singer 1996)propose focusing on natural pauses as an aspect ofprosody which is both important and relatively easyto detect automatically.Given the frequency of utterances in spontaneousspeech which are not fully well-formed - -  whichcontain repairs, hesitations, and fragments - -strategies for dividing and conquering utterancesInclusion of other levels is also possible.
At thelower limit, assuming the grammar were stochastic,one could even use sub-phone speech segments asgrammar terminals, thus subsuming even HMM-basedphone recognition in the parsing regime.
At anintermediate level between phones and words,syllables could be used.
"- The parse tree was not used for analysis, however.Instead.
it was discarded, and a unification-based parserbegan a new parse for MT purposes on a text stringpassed from speech recognition.would be quite useful.
The suggestion is thatnatural pauses can play a part in such a strategy:that pause units, or segments within utterancesbounded by natural pauses, can provide chunkswhich (1) are reliably shorter and less variable inlength than entire utterances and (2) are relativelywell-behaved internally from the syntacticviewpoint, though analysis of the relationshipsamong them appears more problematic.Four specific questions are addressed: (1) Arepause units reliably shorter than whole utterances?If they were not, they could hardly be useful insimplifying analysis.
It was found however, that inthe corpus investigated (Loken-Kim, Yato,Kurihara, Fais, and Furukawa 1993; Furukawa,Yato, and Loken-Kim 1993), pause units are in factabout 60% the length of entire utterances, on theaverage, when measured in Japanese morphemes.The average length of pause units was 5.89morphemes, as compared with 9.39 for wholeutterances.
Further, pause units are less variable inlength than entire utterances: the standard eviationis 5.79 as compared with 12.97.
(2) Wouldhesitations give even shorter, and thus perhaps evenmore manageable, segments if used as alternate oradditional boundaries?
The answer seems to be thatbecause hesitations o often coincide with pauseboundaries, the segments they mark out are nearlythe same as the segments marked by pauses alone.No combination of expressions was found whichgave segments as much as one morpheme shorterthan pause units on average.
(3) Is the syntaxwithin pause units relatively manageable?
A manualsurvey showed that, once hesitation expressions axefiltered from them, some 90% of the pause unitsstudied can be parsed using standard Japanesegrammars; a variety of special problems appear inthe remaining 10%.
(4) Is translation of isolatedpause units a possibility?
We found that a majorityof the pause units in four dialogues gaveunderstandable translations into English whentranslated by hand.The study provided encouragement for a "divideand conquer" analysis trategy, in which parsing andperhaps translation of pause units is carried outbefore, or even without, attempts to create coherentanalyses of entire utterances.As mentioned, parsability of spontaneousutterances can be enhanced by filtering hesitationexpressions from them in preprocessing.
Researchon spotting techniques for such expressions wouldthus seem to be worthwhile.
Researchers canexploit speakers' tendency to lengthen hesitations,and to use them just before or after natural pauses.855 Communicat ive ActsSpeech act analysis (Searle 1969) - -  analysis interms of illocutionary acts like INFORM, WH-QUESTION, REQUEST, etc.
- -  can be useful forspeech translation in numerous ways.
Six uses,three related to translation and three to speechprocessing, will be mentioned here.
Concerningtranslation, it is necessary to:eldentify the speech acts of the current utterance.Speech act analysis of the current utterance isnecessary for translation.
For instance, theEnglish pattern "can you (VP, bare infinitive)?
"may express either an ACTION-REQUEST ora YN-QUESTION (yes/no-question).
Resolu-tion of this ambiguity will be crucial fortranslation.oldenti~, related utterances.
Utterances in dia-logues are often closely related: for instance,one utterance may be a prompt and anotherutterance may be its response; and the propertranslation of a response often depends onidentification and analysis of its prompt.
Forexample, Japanese hai can be translated as yesif it is the response to a YN-QUESTION, butas all right if it is the response to an ACTION-REQUEST.
Further, the syntax of a promptmay become a factor in the final translation.Thus, in a responding utterance hai, sou desu(meaning literally "yes, that's right"), thesegment sou desu may be most naturallytranslated as he can, you will, she does, etc.,depending on the structure and content of theprompting question.
The recognition of suchprompt-response relationships will requireanalysis of typical speech act sequences.oAnalyze relationships among segments andfragments.
Early processing of utterances mayyield fragments which must later be assembledto form the global interpretation for anutterance.
Speech act sequence analysis shouldhelp fit fragments together, since we hope tolearn about ypical act groupings.Concerning speech processing, it is necessary to:oPredict speech acts to aid speech recognition.
Ifwe can predict he coming speech acts, we canpartly predict their surface patterns.
Thisprediction can be used to constrain speechrecognition.
For example, in recognizingspoken Japanese, if we can predict he relativeprobability that the current utterance is a YN-QUESTION as opposed to an INFORM, wemay be able to differentiate utterance-final ka (aquestion particle) and utterance-final ga (aconjunction or politeness particle), which areoften very similar phonetically.?
Provide conventions for prosody recognition.Once spontaneous data is labeled, speechrecognition researchers can try to recognizeprosodic ues to aid in speech act recognitionand disambiguation.
For instance, they can tryto distinguish segments expressing INFORMsand YN-QUESTIONs according to the F0curves associated with them - -  a distinctionwhich would be especially useful forrecognizing YN-QUESTIONs with no mor-phological or syntactic markings.eProvide conventions for speech synthesis.Similarly, speech synthesis researchers can tryto provide more natural prosody by exploitingspeech act information.
Once relations betweenprosody and speech acts have been extractedfrom corpora labeled with speech actinformation, researchers can attempt to supplynatural prosody for synthesized utterancesaccording to the specified speech acts.
Forinstance, more natural pronunciations can beattempted for YN-QUESTIONs, or forCONFIRMATION-QUESTIONs (including tagquestions in English, as in The train goes east,doesn't it?
).While a well-founded set of speech act labelswould be useful, it has not been clear what thetheoretical foundation should be.
As a result, nospeech act set has yet become standard.
Labels areproposed intuitively or by trial and error.Speakers' goals can certainly be analyzed inmany ways.
However, (Seligman, Fais, andTomokiyo 1995) hypothesize that only a limited setof goals is conventionally expressed in a givenlanguage.
For just these goals, relatively fixedexpressive patterns are learned by speakers whenthey learn the language.
In English, for instance, itis conventional to express certain invitations usingthe patterns "Lets *" or "Shall we *?"
In Japanese,one conventionally expresses similar goals via thepatterns "(V, combining stem)mashou" or "(V,combining stem)masen ka?
"The proposal is to focus on discovery andexploitation of these conventionally-expressiblespeech acts, or Communicative Acts.
The relevantexpressive patterns and the contexts within whichthey are found have the great virtue of beingobjectively observable; and assuming the use ofthese patterns is common to all native speakers, itshould be possible to reach a consensusclassification of the patterns according to theircontextualized meaning and use.
This functionalclassification should yield a set of language-specificspeech act labels which can help to put speech actanalysis for speech translation on a firmerfoundation.86The first reason to analyze speech acts in termsof obse~able linguistic patterns, then, is themeasure of objectivity thus gained: the discoveryprocess is to some degree mpirical, data-driven, orcorpus-based.
A second reason is that on-lineanalysis, being shallow or surface-bound, should berelatively quick as opposed to plan-based analysis.Plan-based analysis may well proCe necessary forcertain purposes, but it is quite expensive.
Forapplications like speech translation which must becarried out in nearly real time, it seems wise toexploit shallow analysis as far as possible.With these advantages of cue-based processing - -empirical grounding and speed - -  come certainlimitations.
When analyzing in terms of CAs, wecannot expect o recognize all communicative goals.Instead, we restrict our attention to communicativegoals which can be expressed using conventionallinguistic cue patterns.
Communicative goals whichcannot be described as Communicative Acts includeutterance goals which are expressed non-conventionally (compare the non-conventionalwarning May I call your attention to a potentiallydangerous dog to the conventional WARNINGLook out for the dog.t); or goals which are expressedonly implicitly (It's cold outside as an implicitrequest o shut the window); or goals which canonly be defined in terms of relations betweenutterances.
(While speakers often repeat aninterlocutor's utterance to confirm it, we do not usea REPEAT-TO-CONFIRM CA, since it is appar-ently signaled by no cue patterns, and thus couldonly be recognized by noting inter-utterancerepetition.
)Given that the aim is to classify expressivepatterns according to their meaning and function,how should this be done?
The paper describes aparaphrase-based approach: native speakers arepolled as to the essential equivalence of expressivepatterns in specified discourse contexts.
If byconsensus everal patterns can yield paraphraseswhich are judged equivalent in context, and if theresulting pattern set is not identical to anycompeting pattern set, then it can be considered todefine a Communicative Act.Communicative Acts are defined in terms ofmonolingual conventions for expressing certaincommunicative goals using certain cue patterns.
Fortranslation purposes, however, it will be necessaryto compare the conventions in language A withthose in language B.
With this goal in mind, thediscovery procedure was applied to twin corpora ofJapanese-Japanese and English-English spontaneousdialogues concerning transportation directions andhotel accommodations (Loken-Kim et al 1993).CAs were first identified according to monolingualcriteria.
Then, by observing translation relationsamong the English and Japanese cue patterns, theresulting Eriglish and Japanese CAs were compared.Interestingly, it was found that most of theproposed CAs seem valid for both English andJapanese: only two out of 27 CAs seem to bemonolingual for the corpus in question.6 Tracking Lexical Co-occurrencesIn the processing of spontaneous language, the needfor predictions at the morphological or lexical levelis clear.
For bottom-up arsing based on phones orsyllables, the number of lexical candidates isexplosive.
It is crucial to predict whichmorphological or lexical items are likely so thatcandidates can be weighted appropriately.
(Comparesuch lexical prediction with the CommunicativeAct-based predictions discussed above.
In general, itis hoped that by predicting CAs we can in turnpredict he structural elements of their cue patterns.We are now shifting the discussion to the predictionof open-class elements instead.
The hope is that thetwo sorts of prediction will prove complementary.
)N-grams provide such predictions only at veryshort ranges.
To support bottom-up parsing ofnoisy material containing gaps and fragments,longer-range predictions are needed as well.
Someresearchers have proposed investigation ofassociations beyond the n-gram range, but theproposed associations remain relatively short-range(about five words).
While stochastic grammars canprovide somewhat longer-range predictions than n-grams, they predict only within utterances.
Ourinterest, however, extends to predictions on thescale of several utterances.Thus (Seligman 1994) proposes to permit thedefinition of windows in a transcribed corpus withinwhich co-occurrences of morphological or lexicalelements can be examined.
A window is defined as asequence of minimal segments, where a segment istypically a turn, but can also be a block delimitedby suitable markers in the transcript.
A flexible setof facilities (CO-OC) has been implemented inCommon Lisp to aid collection of such discourse-range co-occurrence information and to providequick access to the statistics for on-line use.Sparse data is somewhat less problematic forlong-range than for short-range predictions, since itis in general easier to predict what is coming "soon"than what is coming next.
Even so, there is neverquite enough data; so smoothing will remainimportant.
CO-OC can support various statisticalsmoothing measures.
However, since thesetechniques are likely to remain insufficient, a newtechnique for semantic smoothing is proposed andsupported: researchers can track co-occurrences ofsemantic tokens associated with words or morphs in87addition to co-occurrences of the words or morphsthemsel~,es.
The semantic tokens are obtained fromstandard on-line thesaura.
The benefits of suchsemantic smoothing appear especially in thepossibility of retrieving reasonable semantically-mediated associations for morphs which are rare orabsent in a training corpus.A weighted co-occurrence b tweerf morphemes orlexemes can be viewed as an association betweenthese itemsi so the set of co-occurrences which CO-OC discovers can be viewed as an associative orsemantic network.
Spreading activation within suchnetworks is often proposed as a method of lexicaidisambiguation.
(For example, if the conceptMONEY has been observed, then the lexical itembank has the meaning closest to MONEY in thenetwork: "savings institution" rather than "edge ofriver", etc.)
Thus disambiguation becomes a secondpossible application of CO-OC's results, beyond theabovementioned primary use for constraining speechrecognition.
A third possible use is in the discoveryof topic transitions: we can hypothesize that a spanwithin a dialogue where few co-occurrencepredictions are fulfilled is a topic boundary.
Oncethe new topic is determined, appropriate constraintscan be exploited, e.g.
by selecting a relevant sub-grammar.Preliminary tests of CO-OC were carried out on acorpus of Japanese-Japanese dialogues concerningstreet directions and hotel arrangements at ATRInterpreting Telecommunications Laboratories.However, further testing is necessary to demonstratethe reliability and usefulness of the approach.
Aprinciple aim would be to determine how large thecorpus must be before consistent co-occurrencepredictions are obtained.Conclusionsuseful for speech recognition, lexicaldisambiguation, and topic boundary recognition.AcknowledgementsWork on all six of the issues discussed here beganat ATR Interpreting Telecommunications Labora-tories in Kyoto, Japan.
I am very grateful for thesupport and stimulation I received there.ReferencesBlanchon, Herv6.
1996.
"A CustomizableInteractive Disambiguation Methodology andTwo Implementations to Disambiguate Frenchand English Input."
In Proceedings of MIDDIM-96 (International Seminar on MultimodalInteractive Disambiguation), Col de Porte,France, August 11 - 15, 1996.Boitet, Christian.
1996.
"Dialogue-based MachineTranslation for Monolinguals and Future Self-explaining Documents."
In Proceedings ofMIDDIM-96 (International Seminar onMultimodal Interactive Disambiguation), Col dePorte, France, August 11 - 15, 1996.Boitet, Christian and M. Seligman.
1994.
"The'Whiteboard' Architecture: A Way to IntegrateHeterogeneous Components of NLP Systems.
"In Proceedings ofCOLING-94, Kyoto, Aug. 5 -9, 1994.Erman, L.D.
and V.R.
Lesser.
1980.
"The Hearsay-1/Speech Understanding System: A Tutorial."
InTrends in Speech Recognition, W.A.
Lea, ed.,Prentice-Hall, 361-381.The six areas of research just examined suggest asix-item wish list for an experimental speechtranslation system.
(1) The system would includefacilities for interactive disambiguation of bothspeech and translation candidates.
(2) Its architecturewould allow modular reconfiguration and globalcoordination of components.
(3) The system wouldemploy a grammar whose terminals were phones,recognizing both words and syntactic structures in auniform and integrated manner, e.g.
via island-driven chart parsing.
(4) Natural pauses and otheraspects of prosody would be used to segmentutterances and otherwise aid analysis.
(5) Speech ordialogue acts would be defined in terms of their cuepatterns, and analyses based upon them would beexploited for speech recognition and analysis.
(6)Semantically-smoothed tracking of lexical co-occurrences would provide a network of associationsFurukawa, R., F. Yato, and K. Loken-Kim.
1993.Analysis of Telephone and MultimediaDialogues.
Technical Report TR-IT-0020, ATRInterpreting Telecommunications Laboratories,Kyoto.
(In Japanese)Julia, L., L. Neumeyer, M. Charafeddine, A.Cheyer, and J. Dowding.
1997.
"HTTP://WWW.SPEECH.SRI.COM/DEMOS/ATIS.HTML."
InWorking Notes, Natural LanguagePro-cessing for the Worm Wide Web.
AAAI-97Spring Symposium, Stanford University, March24-26, 1997.Kowalski, Piotr, Burton Rosenberg, and JefferyKrause.
1995.
Information Transcript.
Biennalede Lyon d'Art Contemporain.
December 20,1995 to February 18, 1996.
Lyon, France.88Loken-Kim, K., F. Yato, K. Kurihara, L. Fais, andR.
Furukawa.
1993.
EMMI-ATR Environmentfor Multi-modal Interaction.
Technical ReportTR-IT-0018, ATR Interpreting Telecommuni-cations Laboratories, Kyoto.
(In Japanese).Morimoto.
T.. T. Takezawa, F. Yato, et al 1993.
"ATR's Speech Translation Sys~m: ASURA.
"In Proceedings of Eurospeech-93, Vol.
2, pages1291-1294.Searle, J.
1969.
Speech Acts.
Cambridge:Cambridge University Press, 1969.Seligman, Mark.
1994.
CO-OC: Semi-automaticProduction of Resources for Tracking Morph-ological and Semantic Co-occurrences inSpontaneous Dialogues.
Technical Report TR-IT-0084, ATR Interpreting TelecommunicationsLaboratories, Kyoto.Seligman, Mark.
1997.
"Interactive Real-timeTranslation via the Internet."
lnWorking Notes,Natural Language Processing for the World WideWeb.
AAAI-97 Spring Symposium, StanfordUniversity.
March 24-26, 1997.Seligman, Mark and C. Boitet.
1994.
"A'Whiteboard' Architecture for Automatic SpeechTranslation."
In Proceedings of the InternationalSymposium on Spoken Dialogue, ISSD-93,Waseda University, Tokyo, Nov. 10 - 12, 1993.Seligman, Mark, Laurel Fais, and MutsukoTomokiyo.
1995.
A bilingual set of com-municative act labels for spontaneous dialogues.Technical Report TR-IT-0081, ATR InterpretingTelecommunications Laboratories, Kyoto.Seligman, Mark, Junko Hosaka, and Harald Singer.1996.
"'Pause Units' and Analysis ofSpontaneous Japanese Dialogues: PreliminaryStudies."
In Notes of the ECAI-96 Workshopon Dialogue Processing in Spoken LanguageSystems, August 12, 1996, Budapest, Hungary.
(Also to appear in Springer Series: LNAI -Lecture Notes in Artificial Intelligence.
)Stock, Oliviero, Rino Falcone and PatriziaInsinnamo.
1989.
"Bi-directional Charts: APotential Technique For Parsing Spoken NaturalLanguage Sentences."
Computer Science andLanguage (1989) 3,219-237.Zajac, Remy and Mark Casper.
1997.
"The TempleWeb Translator."
In Working Notes, NaturalLanguage Processing for the World Wide Web.AAAI-97 Spring Symposium, Stanford University,March 24-26, 1997,8990Posters91
