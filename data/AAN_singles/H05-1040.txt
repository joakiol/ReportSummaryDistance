Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 315?322, Vancouver, October 2005. c?2005 Association for Computational LinguisticsEnhanced Answer Type Inference from Questions using Sequential ModelsVijay Krishnan and Sujatha Das and Soumen Chakrabarti?Computer Science and Engineering Department, IIT Bombay, IndiaAbstractQuestion classification is an importantstep in factual question answering (QA)and other dialog systems.
Several at-tempts have been made to apply statisticalmachine learning approaches, includingSupport Vector Machines (SVMs) withsophisticated features and kernels.
Curi-ously, the payoff beyond a simple bag-of-words representation has been small.
Weshow that most questions reveal their classthrough a short contiguous token subse-quence, which we call its informer span.Perfect knowledge of informer spans canenhance accuracy from 79.4% to 88%using linear SVMs on standard bench-marks.
In contrast, standard heuristicsbased on shallow pattern-matching giveonly a 3% improvement, showing that thenotion of an informer is non-trivial.
Us-ing a novel multi-resolution encoding ofthe question?s parse tree, we induce a Con-ditional Random Field (CRF) to identifyinformer spans with about 85% accuracy.Then we build a meta-classifier using alinear SVM on the CRF output, enhancingaccuracy to 86.2%, which is better than allpublished numbers.1 IntroductionAn important step in factual question answering(QA) and other dialog systems is to classify thequestion (e.g., Who painted Olympia?)
to the antic-ipated type of the answer (e.g., person).
This stepis called ?question classification?
or ?answer typeidentification?.The answer type is picked from a hand-built tax-onomy having dozens to hundreds of answer types(Harabagiu et al, 2000; Hovy et al, 2001; Kwok etal., 2001; Zheng, 2002; Dumais et al, 2002).
QA?
soumen@cse.iitb.ac.insystems can use the answer type to short-list answertokens from passages retrieved by an information re-trieval (IR) subsystem, or use the type together withother question words to inject IR queries.Early successful QA systems used manually-constructed sets of rules to map a question to atype, exploiting clues such as the wh-word (who,where, when, how many) and the head of nounphrases associated with the main verb (what is thetallest mountain in .
.
.
).With the increasing popularity of statistical NLP,Li and Roth (2002), Hacioglu and Ward (2003) andZhang and Lee (2003) used supervised learning forquestion classification on a data set from UIUC thatis now standard1.
It has 6 coarse and 50 fine answertypes in a two-level taxonomy, together with 5500training and 500 test questions.
Webclopedia (Hovyet al, 2001) has also published its taxonomy withover 140 types.The promise of a machine learning approach isthat the QA system builder can now focus on de-signing features and providing labeled data, ratherthan coding and maintaining complex heuristic rule-bases.
The data sets and learning systems quotedabove have made question classification a well-defined and non-trivial subtask of QA for which al-gorithms can be evaluated precisely, isolating morecomplex factors at work in a complete QA system.Prior work: Compared to human performance,the accuracy of question classifiers is not high.
In allstudies, surprisingly slim gains have resulted fromsophisticated design of features and kernels.Li and Roth (2002) used a Sparse Network ofWinnows (SNoW) (Khardon et al, 1999).
Their fea-tures included tokens, parts of speech (POS), chunks(non-overlapping phrases) and named entity (NE)tags.
They achieved 78.8% accuracy for 50 classes,which improved to 84.2% on using an (unpublished,to our knowledge) hand-built dictionary of ?seman-tically related words?.1http://l2r.cs.uiuc.edu/?cogcomp/Data/QA/QC/315Hacioglu and Ward (2003) used linear supportvector machines (SVMs) with question word 2-grams and error-correcting output codes (ECOC)?but no NE tagger or related word dictionary?to get80.2?82% accuracy.Zhang and Lee (2003) used linear SVMs withall possible question word q-grams, and obtained79.2% accuracy.
They went on to design an inge-nious kernel on question parse trees, which yieldedvisible gains for the 6 coarse labels, but only ?slight?gains for the 50 fine classes, because ?the syntactictree does not normally contain the information re-quired to distinguish between the various fine cate-gories within a coarse category?.Algorithm 6-class 50-classLi and Roth, SNoW (1) 78.8(2)Hacioglu et al, SVM+ECOC ?
80.2?82Zhang & Lee, LinearSVMq 87.4 79.2Zhang & Lee, TreeSVM 90 ?SVM, ?perfect?
informer 94.2 88SVM, CRF-informer 93.4 86.2Table 1: Summary of % accuracy for UIUC data.
(1) SNoW accuracy without the related word dictio-nary was not reported.
With the related-word dic-tionary, it achieved 91%.
(2) SNoW with a related-word dictionary achieved 84.2% but the other algo-rithms did not use it.
Our results are summarized inthe last two rows, see text for details.Our contributions: We introduce the notion ofthe answer type informer span of the question (in?2): a short (typically 1?3 word) subsequence ofquestion tokens that are adequate clues for questionclassification; e.g.
: How much does an adult ele-phant weigh?We show (in ?3.2) that a simple linear SVM us-ing features derived from human-annotated informerspans beats all known learning approaches.
Thisconfirms our suspicion that the earlier approachessuffered from a feature localization problem.Of course, informers are useful only if we can findways to automatically identify informer spans.
Sur-prisingly, syntactic pattern-matching and heuristicswidely used in QA systems are not very good at cap-turing informer spans (?3.3).
Therefore, the notionof an informer is non-trivial.Using a parse of the question sentence, we derivea novel set of multi-resolution features suitable fortraining a conditional random field (CRF) (Laffertyet al, 2001; Sha and Pereira, 2003).
Our feature de-sign paradigm may be of independent interest (?4).Our informer tagger is about 85?87% accurate.We use a meta-learning framework (Chan andStolfo, 1993) in which a linear SVM predicts the an-swer type based on features derived from the origi-nal question as well as the output of the CRF.
Thismeta-classifier beats all published numbers on stan-dard question classification benchmarks (?4.4).
Ta-ble 1 (last two rows) summarizes our main results.2 Informer overviewOur key insight is that a human can classify a ques-tion based on very few tokens gleaned from skeletalsyntactic information.
This is certainly true of themost trivial classes (Who wrote Hamlet?
or Howmany dogs pull a sled at Iditarod?)
but is also true ofmore subtle clues (How much does a rhino weigh?
).In fact, informal experiments revealed the surpris-ing property that only one contiguous span of tokensis adequate for a human to classify a question.
E.g.,in the above question, a human does not even needthe how much clue once the word weigh is avail-able.
In fact, ?How much does a rhino cost??
has anidentical syntax but a completely different answertype, not revealed by how much alone.
The onlyexceptions to the single-span hypothesis are multi-function questions like ?What is the name and ageof .
.
.
?, which should be assigned to multiple answertypes.
In this paper we consider questions where onetype suffices.Consider another question with multiple clues:Who is the CEO of IBM?
In isolation, the clue whomerely tells us that the answer might be a person orcountry or organization, while CEO is perfectly pre-cise, rendering who unnecessary.
All of the aboveapplies a forteriori to what and which clues, whichare essentially uninformative on their own, as in?What is the distance between Pisa and Rome?
?Conventional QA systems use mild analysis onthe wh-clues, and need much more sophistication onthe rest of the question (e.g.
inferring author fromwrote, and even verb subcategorization).
We submitthat a single, minimal, suitably-chosen contiguous316span of question token/s, defined as the informerspan of the question, is adequate for question clas-sification.The informer span is very sensitive to the struc-ture of clauses, phrases and possessives in the ques-tion, as is clear from these examples (informers ital-icized): ?What is Bill Clinton?s wife?s profession?,and ?What country?s president was shot at Ford?sTheater?.
The choice of informer spans also de-pends on the target classification system.
Initiallywe wished to handle definition questions separately,and marked no informer tokens in ?What is digi-talis?.
However, what is is an excellent informerfor the UIUC class DESC:def (description, defi-nition).3 The meta-learning approachWe propose a meta-learning approach (?3.1) inwhich the SVM can use features from the originalquestion as well as its informer span.
We show(?3.2) that human-annotated informer spans lead tolarge improvements in accuracy.
However, we show(?3.3) that simple heuristic extraction rules com-monly used in QA systems (e.g.
head of noun phrasefollowing wh-word) cannot provide informers thatare nearly as useful.
This naturally leads us to de-signing an informer tagger in ?4.Figure 1 shows our meta-learning (Chan andStolfo, 1993) framework.
The combiner is a linearmulti-class one-vs-one SVM2, as in the Zhang andLee (2003) baseline.
We did not use ECOC (Ha-cioglu and Ward, 2003) because the reported gain isless than 1%.The word feature extractor selects unigrams andq-grams from the question.
In our experience, q =1 or q = 2 were best; if unspecified, all possibleqgrams were used.
Through tuning, we also foundthat the SVM ?C?
parameter (used to trade betweentraining data fit and model complexity) must be setto 300 to achieve their published baseline numbers.3.1 Adding informer featuresWe propose two very simple ways to derive featuresfrom informers for use with SVMs.
Initially, assumethat perfect informers are known for all questions;2http://www.csie.ntu.edu.tw/?cjlin/libsvm/question CRF Informerspan taggerWord and qgramfeature extractorInformerfeature extractorCombined feature vectorclassSVMMetaLearnerFigure 1: The meta-learning approach.later (?4) we study how to predict informers.Informer q-grams: This comprises of all word q-grams within the informer span, for all possible q.E.g., such features enable effective exploitation ofinformers like length or height to classify to theNUMBER:distance class in the UIUC data.Informer q-gram hypernyms: For each word orcompound within the informer span that is a Word-Net noun, we add all hypernyms of all senses.
Theintuition is that the informer (e.g.
author, crick-eter, CEO) is often narrower than a broad ques-tion class (HUMAN:individual).
Following hy-pernym links up to person via WordNet produces amore reliably correlated feature.Given informers, other question words mightseem useless to the classifier.
However, retainingregular features from other question words is an ex-cellent idea for the following reasons.First, we kept word sense disambiguation (WSD)outside the scope of this work because WSD en-tails computation costs, and is unlikely to be reliableon short single-sentence questions.
Questions likeHow long .
.
.
or Which bank .
.
.
can thus becomeambiguous and corrupt the informer hypernym fea-tures.
Additional question words can often help nailthe correct class despite the feature corruption.Second, while our CRF-based approach to in-former span tagging is better than obvious alterna-tives, it still has a 15% error rate.
For the questionswhere the CRF prediction is wrong, features fromnon-informer words give the SVM an opportunity tostill pick the correct question class.Word features: Based on the above discussion,one boolean SVM feature is created for every wordq-gram over all question tokens.
In experiments, wefound bigrams (q = 2) to be most effective, closelyfollowed by unigrams (q = 1).
As with informers,we can also use hypernyms of regular words as SVM317features (marked ?Question bigrams + hypernyms?in Table 2).3.2 Benefits from ?perfect?
informersWe first wished to test the hypothesis that identi-fying informer spans to an SVM learner can im-prove classification accuracy.
Over and above theclass labels, we had two volunteers tag the 6000UIUC questions with informer spans (which we call?perfect?
?agreement was near-perfect).Features Coarse FineQuestion trigrams 91.2 77.6All question qgrams 87.2 71.8All question unigrams 88.4 78.2Question bigrams 91.6 79.4+informer q-grams 94.0 82.4+informer hypernyms 94.2 88.0Question unigrams + all informer 93.4 88.0Only informer 92.2 85.0Question bigrams + hypernyms 91.6 79.4Table 2: Percent accuracy with linear SVMs, ?per-fect?
informer spans, and various feature encodings.Observe in Table 2 that the unigram baseline isalready quite competitive with the best prior num-bers, and exploiting perfect informer spans beats allknown numbers.
It is clear that both informer q-grams and informer hypernyms are very valuablefeatures for question classification.
The fact that noimprovement was obtained with over Question bi-grams using Question hypernyms highlights the im-portance of choosing a few relevant tokens as in-formers and designing suitable features on them.Table 3 (columns b and e) shows the benefits fromperfect informers broken down into broad questiontypes.
Questions with what as the trigger are thebiggest beneficiaries, and they also form by far themost frequent category.The remaining question, one that we address inthe rest of the paper, is whether we can effectivelyand accurately automate the process of providing in-former spans to the question classifier.3.3 Informers provided by heuristicsIn ?4 we will propose a non-trivial solution to theinformer-tagging problem.
Before that, we must jus-tify that such machinery is indeed required.Some leading QA systems extract words verysimilar in function to informers from the parse treeof the question.
Some (Singhal et al, 2000) pickthe head of the first noun phrase detected by a shal-low parser, while others use the head of the nounphrase adjoining the main verb (Ramakrishnan et al,2004).
Yet others (Harabagiu et al, 2000; Hovyet al, 2001) use hundreds of (unpublished to ourknowledge) hand-built pattern-matching rules on theoutput of a full-scale parser.A natural baseline is to use these extracted words,which we call ?heuristic informers?, with an SVMjust like we used ?perfect?
informers.
All that re-mains is to make the heuristics precise.How: For questions starting with how, we use thebigram starting with how unless the next wordis a verb.Wh: If the wh-word is not how, what or which, usethe wh-word in the question as a separate fea-ture.WhNP: For questions having what and which, usethe WHNP if it encloses a noun.
WHNP is theNoun Phrase corresponding to the Wh-word,given by a sentence parser (see ?4.2).NP1: Otherwise, for what and which questions, thefirst (leftmost) noun phrase is added to yet an-other feature subspace.Table 3 (columns c and f) shows that thesealready-messy heuristic informers do not capture thesame signal quality as ?perfect?
informers.
Our find-ings corroborate Li and Roth (2002), who report lit-tle benefit from adding head chunk features for thefine classification task.Moreover, observe that using heuristic informerfeatures without any word features leads to ratherpoor performance (column c), unlike using perfectinformers (column b) or even CRF-predicted in-former (column d, see ?4).
These clearly establishthat the notion of an informer is nontrivial.4 Using CRFs to label informersGiven informers are useful but nontrivial to recog-nize, the next natural question is, how can we learnto identify them automatically?
From earlier sec-tions, it is clear (and we give evidence later, see Ta-ble 5) that sequence and syntax information will be3186 coarse classesB Only Informers B+ B+ B+Type #Quest.
(Bigrams) Perf.Inf H.Inf CRF.Inf Perf.Inf H.Inf CRF.Infwhat 349 88.8 89.4 69.6 79.3 91.7 87.4 91.4which 11 72.7 100.0 45.4 81.8 100.0 63.6 81.8when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0where 27 100.0 96.3 100.0 96.3 100.0 100.0 100.0who 47 100.0 100.0 100.0 100.0 100.0 100.0 100.0how * 32 100.0 96.9 100.0 100.0 100.0 100.0 100.0rest 6 100.0 100.0 100.0 66.7 100.0 66.7 66.7Total 500 91.6 92.2 77.2 84.6 94.2 90.0 93.450 fine classeswhat 349 73.6 82.2 61.9 78.0 85.1 79.1 83.1which 11 81.8 90.9 45.4 73.1 90.9 54.5 81.8when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0where 27 92.6 85.2 92.6 88.9 88.9 92.5 88.9who 47 97.9 93.6 93.6 93.6 100.0 100.0 97.9how * 32 87.5 84.3 81.2 78.1 87.5 90.6 90.6rest 6 66.7 66.7 66.7 66.7 100.0 66.7 66.7Total 500 79.4 85.0 69.6 78.0 88.0 82.6 86.2a b c d e f gTable 3: Summary of % accuracy broken down by question type (referred from ?3.2, ?3.3 and ?4.4).
a:question bigrams, b: perfect informers only, c: heuristic informers only, d: CRF informers only, e?g:bigrams plus perfect, heuristic and CRF informers.important.We will model informer span identification as asequence tagging problem.
An automaton makesprobabilistic transitions between hidden states y,one of which is an ?informer generating state?, andemits tokens x.
We observe the tokens and have toguess which were produced from the ?informer gen-erating state?.Hidden Markov models are extremely popular forsuch applications, but recent work has shown thatconditional random fields (CRFs) (Lafferty et al,2001; Sha and Pereira, 2003) have a consistent ad-vantage over traditional HMMs in the face of manyredundant features.
We refer the reader to the abovereferences for a detailed treatment of CRFs.
Herewe will regard a CRF as largely a black box3.To train a CRF, we need a set of state nodes, atransition graph on these nodes, and tokenized textwhere each token is assigned a state.
Once the CRFis trained, it can be applied to a token sequence, pro-3We used http://crf.sourceforge.net/ducing a predicted state sequence.4.1 State transition modelsWe started with the common 2-state ?in/out?
modelused in information extraction, shown in the left halfof Figure 2.
State ?1?
is the informer-generatingstate.
Either state can be initial and final (doublecircle) states.0 1 0 1 2What kind of an animal is Winnie the PoohWhat, kind,of, an, is,Winnie, the,PoohanimalWhat, kind,of, anis, Winnie,the, Poohanimalstart startFigure 2: 2- and 3-state transition models.The 2-state model can be myopic.
Consider thequestion pair319A: What country is the largest producer of wheat?B: Name the largest producer of wheatThe i?1 context of producer is identical in A andB.
In B, for want of a better informer, we would wantproducer to be flagged as the informer, although itmight refer to a country, person, animal, company,etc.
But in A, country is far more precise.Any 2-state model that depends on positions i?1to define features will fail to distinguish between Aand B, and might select both country and producerin A.
As we have seen with heuristic informers, pol-luting the informer pool can significantly hurt SVMaccuracy.Therefore we also use the 3-state ?begin/in/out?
(BIO) model.
The initial state cannot be ?2?
in the3-state model; all states can be final.
The 3-statemodel allows at most one informer span.
Once the3-state model chooses country as the informer, it isunlikely to stretch state 1 up to producer.There is no natural significance to using four ormore states.
Besides, longer range syntax dependen-cies are already largely captured by the parser.What is the capital city of JapanWP VBZ DT NN NN IN NNPNP NPPPNPVPSQSBARQWHNP0123456LevelFigure 3: Stanford Parser output example.4.2 Features from a parse of the questionSentences with similar parse trees are likely to havethe informer in similar positions.
This was the in-tuition behind Zhang et al?s tree kernel, and is alsoour starting point.
We used the Stanford LexicalizedParser (Klein and Manning, 2003) to parse the ques-tion.
(We assume familiarity with parse tree notationfor lack of space.)
Figure 3 shows a sample parsetree organized in levels.
Our first step was to trans-i 1 2 3 4 5 6 7yi 0 0 0 1 1 2 2xi What is the capital city of Japan` ?
Features for xis1 WP,1 VBZ,1 DT,1 NN,1 NN,1 IN,1 NNP,12 WHNP,1 VP,1 NP,1 NP,1 NP,1 Null,1 NP,23 Null,1 Null,1 Null,1 Null,1 Null,1 PP,1 PP,14 Null,1 Null,1 NP,1 NP,1 NP,1 NP,1 NP,15 Null,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,16 SBARQ SBARQSBARQSBARQSBARQSBARQSBARQTable 4: A multi-resolution tabular view of the ques-tion parse showing tag and num attributes.
capitalcity is the informer span with y = 1.late the parse tree into an equivalent multi-resolutiontabular format shown in Table 4.Cells and attributes: A labeled question com-prises the token sequence xi; i = 1, .
.
.
and the labelsequence yi, i = 1, .
.
.
Each xi leads to a columnvector of observations.
Therefore we use matrix no-tation to write down x: A table cell is addressed asx[i, `] where i is the token position (column index)and ` is the level or row index, 1?6 in this example.
(Although the parse tree can be arbitrarily deep, wefound that using features from up to level ` = 2 wasadequate.
)Intuitively, much of the information required forspotting an informer can be obtained from the partof speech of the tokens and phrase/clause attachmentinformation.
Conversely, specific word informationis generally sparse and misleading; the same wordmay or may not be an informer depending on its po-sition.
E.g., ?What birds eat snakes??
and ?Whatsnakes eat birds??
have the same words but differentinformers.
Accordingly, we observe two propertiesat each cell:tag: The syntactic class assigned to the cell bythe parser, e.g.
x[4, 2].tag = NP.
It is well-knownthat POS and chunk information are major clues toinformer-tagging, specifically, informers are oftennouns or noun phrases.num: Many heuristics exploit the fact that the firstNP is known to have a higher chance of containinginformers than subsequent NPs.
To capture this po-sitional information, we define num of a cell at [i, `]as one plus the number of distinct contiguous chunksto the left of [i, `] with tags equal to x[4, 2].tag.E.g., at level 2 in the table above, the capital city320forms the first NP, while Japan forms the second NP.Therefore x[7, 2].num = 2.In conditional models, it is notationally conve-nient to express features as functions on (xi, yi).
Toone unfamiliar with CRFs, it may seem strange thatyi is passed as an argument to features.
At trainingtime, yi is indeed known, and at testing time, theCRF algorithm efficiently finds the most probablesequence of yis using a Viterbi search.
True labelsare not revealed to the CRF at testing time.Cell features IsTag and IsNum: E.g., the ob-servation ?y4 = 1 and x[4, 2].tag = NP?
is cap-tured by the statement that ?position 4 fires the fea-ture IsTag1,NP,2?
(which has a boolean value).There is an IsTagy,t,` feature for each (y, t, `)triplet.
Similarly, for every possible state y, ev-ery possible num value n (up to some maximumhorizon), and every level `, we define boolean fea-tures IsNumy,n,`.
E.g., position 7 fires the featureIsNum2,2,2 in the 3-state model, capturing the state-ment ?x[7, 2].num = 2 and y7 = 2?.Adjacent cell features IsPrevTag andIsNextTag: Context can be exploited by aCRF by coupling the state at position i withobservations at positions adjacent to position i(extending to larger windows did not help).
Tocapture this, we use more boolean features: posi-tion 4 fires the feature IsPrevTag1,DT,1 becausex[3, 1].tag = DT and y4 = 1.
Position 4 also firesIsPrevTag1,NP,2 because x[3, 2].tag = NP andy4 = 1.
Similarly we define a IsNextTagy,t,`feature for each possible (y, t, `) triple.State transition features IsEdge: Position ifires feature IsEdgeu,v if yi?1 = u and yi = v.There is one such feature for each state-pair (u, v)allowed by the transition graph.
In addition we havesentinel features IsBeginu and IsEndu markingthe beginning and end of the token sequence.4.3 Informer-tagging accuracyWe study the accuracy of our CRF-based informertagger wrt human informer annotations.
In the nextsection we will see the effect of CRF tagging onquestion classification.There are at least two useful measures ofinformer-tagging accuracy.
Each question has aknown set Ik of informer tokens, and gets a setof tokens Ic flagged as informers by the CRF.
Foreach question, we can grant ourself a reward of 1 ifIc = Ik, and 0 otherwise.
In ?3.1, informers wereregarded as a separate (high-value) bag of words.Therefore, overlap between Ic and Ik would be areasonable predictor of question classification accu-racy.
We use the Jaccard similarity |Ik?Ic|/|Ik?Ic|.Table 5 shows the effect of using diverse feature sets.Fraction JaccardFeatures used Ic = Ik overlapIsTag 0.368 0.396+IsNum 0.474 0.542+IsPrevTag+IsNextTag 0.692 0.751+IsEdge+IsBegin+IsEnd 0.848 0.867Table 5: Effect of feature choices.?
IsTag features are not adequate.?
IsNum features improve accuracy 10?20%.?
IsPrevTag and IsNextTag (?+Prev+Next?)
add over 20% of accuracy.?
IsEdge transition features help exploitMarkovian dependencies and adds another10?15% accuracy, showing that sequentialmodels are indeed required.Type #Quest.
Heuristic 2-state 3-stateInformers CRF CRFwhat 349 57.3 68.2 83.4which 11 77.3 83.3 77.2when 28 75.0 98.8 100.0where 27 84.3 100.0 96.3who 47 55.0 47.2 96.8how * 32 90.6 88.5 93.8rest 6 66.7 66.7 77.8Total 500 62.4 71.2 86.7Table 6: Effect of number of CRF states, and com-parison with the heuristic baseline (Jaccard accuracyexpressed as %).Table 6 shows that the 3-state CRF performsmuch better than the 2-state CRF, especially on diffi-cult questions with what and which.
It also comparesthe Jaccard accuracy of informers found by the CRFvs.
informers found by the heuristics described in?3.3.
Again we see a clear superiority of the CRF321approach.Unlike the heuristic approach, the CRF approachis relatively robust to the parser emitting a somewhatincorrect parse tree, which is not uncommon.
Theheuristic approach picks the ?easy?
informer, who,over the better one, CEO, in ?Who is the CEO ofIBM?.
Its bias toward the NP-head can also be aproblem, as in ?What country?s president .
.
.
?.4.4 Question classification accuracyWe have already seen in ?3.2 that perfect knowledgeof informers can be a big help.
Because the CRFcan make mistakes, the margin may decrease.
In thissection we study this issue.We used questions with human-tagged informers(?3.2) to train a CRF.
The CRF was applied backon the training questions to get informer predictions,which were used to train the 1-vs-1 SVM meta-learner (?3).
Using CRF-tagged and not human-tagged informers may seem odd, but this lets theSVM learn and work around systematic errors inCRF outputs.Results are shown in columns d and g of Table 3.Despite the CRF tagger having about 15% error, weobtained 86.2% SVM accuracy which is rather closeto the the SVM accuracy of 88% with perfect in-formers.The CRF-generated tags, being on the trainingdata, might be more accurate that would be for un-seen test cases, potentially misleading the SVM.This turns out not to be a problem: clearly we arevery close to the upper bound of 88%.
In fact, anec-dotal evidence suggests that using CRF-assignedtags actually helped the SVM.5 ConclusionWe presented a new approach to inferring the typeof the answer sought by a well-formed natural lan-guage question.
We introduced the notion of a spanof informer tokens and extract it using a sequentialgraphical model with a novel feature representationderived from the parse tree of the question.
Our ap-proach beats the accuracy of recent algorithms, evenones that used max-margin methods with sophisti-cated kernels defined on parse trees.An intriguing feature of our approach is thatwhen an informer (actor) is narrower than the ques-tion class (person), we can exploit direct hyper-nymy connections like actor to Tom Hanks, if avail-able.
Existing knowledge bases like WordNet andWikipedia, combined with intense recent work (Et-zioni et al, 2004) on bootstrapping is-a hierarchies,can thus lead to potentially large benefits.Acknowledgments: Thanks to Sunita Sarawagifor help with CRFs, and the reviewers for improv-ing the presentation.ReferencesP.
K Chan and S. J Stolfo.
1993.
Experiments in mul-tistrategy learning by meta-learning.
In CIKM, pages314?323, Washington, DC.S Dumais, M Banko, E Brill, J Lin, and A Ng.
2002.Web question answering: Is more always better?
InSIGIR, pages 291?298.O Etzioni, M Cafarella, et al 2004.
Web-scale informa-tion extraction in KnowItAll.
In WWW Conference,New York.
ACM.K Hacioglu and W Ward.
2003.
Question classifica-tion with support vector machines and error correctingcodes.
In HLT, pages 28?30.S Harabagiu, D Moldovan, M Pasca, R Mihalcea, M Sur-deanu, R Bunescu, R Girju, V Rus, and P Morarescu.2000.
FALCON: Boosting knowledge for answer en-gines.
In TREC 9, pages 479?488.
NIST.E Hovy, L Gerber, U Hermjakob, M Junk, and C.-YLin.
2001.
Question answering in Webclopedia.
InTREC 9.
NIST.R Khardon, D Roth, and L. G Valiant.
1999.
Relationallearning for NLP using linear threshold elements.
InIJCAI.D Klein and C. D Manning.
2003.
Accurate unlexical-ized parsing.
In ACL, volume 41, pages 423?430.C Kwok, O Etzioni, and D. S Weld.
2001.
Scaling ques-tion answering to the Web.
In WWW Conference, vol-ume 10, pages 150?161, Hong Kong.J Lafferty, A McCallum, and F Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML.X Li and D Roth.
2002.
Learning question classifiers.
InCOLING, pages 556?562.G Ramakrishnan, S Chakrabarti, D. A Paranjpe, andP Bhattacharyya.
2004.
Is question answering an ac-quired skill?
In WWW Conference, pages 111?120,New York.F Sha and F Pereira.
2003.
Shallow parsing with condi-tional random fields.
In HLT-NAACL, pages 134?141.A Singhal, S Abney, M Bacchiani, M Collins, D Hindle,and F Pereira.
2000.
AT&T at TREC-8.
In TREC 8,pages 317?330.
NIST.D Zhang and W Lee.
2003.
Question classification usingsupport vector machines.
In SIGIR, pages 26?32.Z Zheng.
2002.
AnswerBus question answering system.In HLT.322
