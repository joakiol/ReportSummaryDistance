Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 182?189,Columbus, June 2008. c?2008 Association for Computational LinguisticsA Framework for Model-based Evaluation of Spoken Dialog SystemsSebastian Mo?llerDeutsche Telekom LaboratoriesTechnische Universita?t Berlin10587 Berlin, Germanysebastian.moeller@telekom.deNigel G. WardComputer Science DepartmentUniversity of Texas at El PasoEl Paso, Texas 79968, USAnigelward@acm.orgAbstractImprovements in the quality, usability and ac-ceptability of spoken dialog systems can befacilitated by better evaluation methods.
Tosupport early and efficient evaluation of dia-log systems and their components, this paperpresents a tripartite framework describing theevaluation problem.
One part models the be-havior of user and system during the interac-tion, the second one the perception and judg-ment processes taking place inside the user,and the third part models what matters to sys-tem designers and service providers.
The pa-per reviews available approaches for some ofthe model parts, and indicates how anticipatedimprovements may serve not only developersand users but also researchers working on ad-vanced dialog functions and features.1 IntroductionDespite the utility of many spoken dialog systemstoday, the user experience is seldom satisfactory.Improving this is a matter of great intellectual in-terest and practical importance.
However improve-ments can be difficult to evaluate effectively, and thismay be limiting the pace of innovation: today, validand reliable evaluations still require subjective ex-periments to be carried out, and these are expensiveand time-consuming.
Thus, the needs of system de-velopers, of service operators, and of the final usersof spoken dialog systems argue for the developmentof additional evaluation methods.In this paper we focus on the prospects for anearly and model-based evaluation of dialog systems.Doing evaluation as early as possible in the de-sign and development process is critical for improv-ing quality, reducing costs and fostering innovation.Early evaluation renders the process more efficientand less dependent on experience, hunches and intu-itions.
With the help of such models predicting theoutcome of user tests, the need for subjective test-ing can be reduced, restricting it to that subset of thepossible systems which have already been vetted inan automatic or semi-automatic way.Several approaches have already been presentedfor semi-automatic evaluation.
For example, thePARADISE framework (Walker et al, 1997) predictsthe effects of system changes, quantified in terms ofinteraction parameters, on an average user judgment.Others (Araki and Doshita, 1997; Lo?pez-Co?zar etal., 2003; Mo?ller et al, 2006) have developed dialogsimulations to aid system optimization.
However thebig picture has been missing: there has been no clearview of how these methods relate to each other, andhow they might be improved and joined to supportefficient early evaluation.The remainder of this paper is organized as fol-lows.
Section 2 gives a brief review of differentevaluation purposes and terminology, and outlines anew tripartite decomposition of the evaluation prob-lem.
One part of our framework models the behav-ior of user and system during the interaction, anddescribes the impact of system changes on the inter-action flow.
The second part models the perceptionand judgment processes taking place inside the user,and tries to predict user ratings on various percep-tual dimensions.
The third part models what mat-ters to system designers and service providers for182a specific application.
Sections 3, 4, and 5 go intospecifics on the three parts of the framework, dis-cussing which components are already available orconceivable.
Finally, Section 6 discusses the poten-tial impact of the approach, and Section 7 lists theissues to be resolved in future work.2 Performance, Quality, Usability andAcceptability EvaluationDevelopers tend to use indices of performance to as-sess their systems.
The performance indicates the?ability of a system to provide the function it hasbeen designed for?
(Mo?ller, 2005).
The functionand an appropriate measure for quantifying the de-gree of fulfillment may easily be determined for cer-tain components ?
e.g.
word accuracy for a speechrecognizer or concept error rate for a speech under-standing module ?
but it is harder to specify forother components, such as a dialog manager or anoutput generation module.
However, definitive mea-sures of component quality are not always neces-sary: what matters for such a module is its contri-bution to the quality of the entire interaction, as it isperceived by the user.We follow the definition of the term quality asintroduced by Jekosch (2000) and now acceptedfor telephone-based spoken dialog services by theInternational Telecommunication Union in ITU-TRec.
P.851 (2003): ?Result of judgment of the per-ceived composition of an entity with respect to itsdesired composition?.
Quality thus involves a per-ception process and a judgment process, duringwhich the perceiving person compares the percep-tual event with a (typically implicit) reference.
It isthe comparison with a reference which associates auser-specific value to the perceptual event.
The per-ception and the comparison processes take place in aparticular context of use.
Thus, both perception andquality should be regarded as ?events?
which hap-pen in a particular personal, spatial, temporal andfunctional context.Usability is one sub-aspect of the quality of thesystem.
Following the definition in ISO 9241 Part11 (1998), usability is considered as the ?extent towhich a product can be used by specified users toachieve specified goals with effectiveness, efficiencyand satisfaction in a specified context of use?.
Us-ability is degraded when interaction problems oc-cur.
Such problems influence the perceptual eventof the user interacting with the system, and conse-quently the quality s/he associates with the systemas a whole.
This may have consequences for theacceptability or the system or service, that is, howreadily a customer will use the system or service.This can be quantified, for example as the ratio ofthe potential user population to the size of the targetgroup.It is the task of any evaluation to quantify as-pects of system performance, quality, usability oracceptability.
The exact target depends on the pur-pose of the evaluation (Paek, 2007).
For example,the system developer might be most interested inquantifying the performance of the system and itscomponents; s/he might further need to know howthe performance affects the quality perceived by theuser.
In contrast, the service operator might insteadbe most interested in the acceptability of the ser-vice.
S/he might further want to know about thesatisfaction of the user, influenced by the usabilityof the system, and also by other (e.g.
hedonic) as-pects like comfort, joy-of-use, fashion, etc.
Differ-ent evaluation approaches may be complementary,in the sense that metrics determined for one purposemay be helpful for other purposes as well.
Thus, it isuseful to describe the components of different eval-uation approaches in a single framework.Figure 1 summarizes our view of the evaluationlandscape.
At the lower left corner is what we canchange (the dialog system), at the right is what theservice operator might be interested in (a metric forthe value of the system).
In between are three com-ponents of a model of the processes taking place inthe evaluation.
The behavior model describes howsystem and user characteristics determine the flowof the interaction and translate this to quantitativedescriptors.
The perception and judgment modeldescribes how the interaction influences the percep-tual and quality events felt by the user, and trans-lates these to observable user judgments.
Finally thevalue model associates a certain value to the qual-ity judgments, depending on the application.
Themodel properties have been grouped in three layers:aspects of the user and his/her behavior, aspects ofthe system in its context-of-use, and the work of anexternal observer (expert) carrying out the evalua-183BehaviorModel Perception and Judgment Model Value ModelUserBehaviorSystemBehaviorInteracionBehaviorUser LayerExpertLayerSystemLayerInteractionPhenomenaInteractionParametersPerceptualEventDimensionDescriptorsQualityJudgmentsReferenceQualityEventQualityAspects ValueDescriptionPerceptualQualityDimensionsDesignerand OperatorRequirementsFigure 1: Tripartite view of a model-based evaluation.
Observable properties are in boxes, inferred or hidden propertiesare in ovals.
The layers organize the properties as mostly user-related, mostly system-related, and mostly expert-related, and mostly system-related.tion.
They have further been classified as to whetherthey are observable (boxes) or hidden from the eval-uator (ovals).The next three sections go through the three partsof the model left-to-right, explaining the needs, cur-rent status, and prospects.3 Behavior ModelThe behavior model translates the characteristics ofthe system and the user into predicted interaction be-havior.
In order to be useful, the representations ofthis behavior must be concise.One way to describe dialog behavior is with in-teraction parameters which quantify the behavior ofthe user and/or the system during the interaction.Such parameters may be measured instrumentally orgiven by expert annotation.
In an attempt to sys-tematize best practice, the ITU-T has proposed acommon set of interaction parameters suitable forthe evaluation of telephone-based spoken dialog sys-tems in ITU-T Suppl.
24 (2005).
These parametershave been developed bottom-up from a collection ofevaluation reports over the last 15 years, and includemetrics related to dialog and communication in gen-eral, meta-communication, cooperativity, task, andspeech-input performance (Mo?ller, 2005).
Unfortu-nately, it as is yet unclear which of these parametersrelate to quality from a user?s point-of-view.
In addi-tion, some metrics are missing which address criticalaspects for the user, e.g.
parameters for the qualityand attractiveness of the speech output.Another manageable way to describe system be-havior is to focus on interaction phenomena.
Sev-eral schemes have been developed for classifyingsuch phenomena, such as system errors, user errors,points of confusion, dead time, and so on (Bernsen etal., 1998; Ward et al, 2005; Oulasvirta et al, 2006).Patterns of interaction phenomena may be reflectedin interaction parameter values, and may be identi-fied on that basis.
Otherwise, they have to be deter-mined by experts and/or users, by means of obser-vation, interviews, thinking-aloud, and other tech-niques from usability engineering.
(Using this ter-minology we can understand the practice of usabilitytesting as being the identification of interaction phe-nomena, also known as ?usability events?
or ?criti-cal incidences?, and using these to estimate specificquality aspects or the overall value of the system.
)Obtaining the interaction parameters and classi-fying the interaction phenomena can be done, ob-viously, from a corpus of user-system interactions.The challenge for early evaluation is to obtain thesewithout actually running user tests.
Thus, we wouldlike to have a system behavior model and a user be-havior model to simulate interaction behavior, andto map from system parameters and user propertiesto interaction parameters or phenomena.
The valueof such models for a developer is clear: they could184enable estimation of how a change in the system(e.g.
a change in the vocabulary) might affect theinteraction properties.
In addition to the desired ef-fects, the side-effects of system changes are also im-portant.
Predicting such side-effects will substan-tially decrease the risk and uncertainty involved indialogue design, thereby decreasing the gap betweenresearch and commercial work on dialog system us-ability (Heisterkamp, 2003; Pieraccini and Huerta,2005).Whereas modeling system behavior in response touser input is clearly possible (since in the last resortit is possible to fully implement the system), user be-havior can probably not be modeled in closed form,because it unavoidably relates to the intricacies ofthe user and reflects the time-flow of the interaction.Thus, it seems necessary to employ a simulationof the interaction, as has been proposed by Arakiand Doshita (1997) and Lo?pez-Co?zar et al (2003),among others.One embodiment of this idea is the MeMo work-bench (Mo?ller et al, 2006), which is based onthe idea of running models of the system and ofthe user in a dedicated usability testing workbench.The system model is a description of the possi-ble tasks (system task model) plus a description ofthe system?s interaction behavior (system interac-tion model).
The user model is a description of thetasks a user would want to carry out with the sys-tem (user task model) plus a description of the stepss/he would take to reach the goal when faced withthe system (user interaction model).
Currently theworkbench uses simple attribute-value descriptionsof tasks the system is able to carry out.
From these,user-desired tasks may be derived, given some back-ground knowledge of the domain and possible tasks.The system interaction model is described by a statediagram which models interactions as paths througha number of dialog states.
The system designer pro-vides one or several ?intended paths?
through the in-teraction, which lead easily and/or effectively to thetask goal.The user?s interaction behavior will strongly de-pend on the system output in the previous turn.Thus, it is reasonable to build the user interactionmodel on top of the system interaction model: Theuser mainly follows the ?intended path?, but at cer-tain points deviations from this path are generated ina probabilistic rule-based manner.
For example, theuser might deviate from the intended path, becauses/he does not understand a long system prompt, orbecause s/he is irritated by a large number of op-tions.
Each deviation from the intended path hasan associated probability; these are calculated fromsystem characteristics (e.g.
prompt length, numberof options) and user characteristics (e.g.
experiencewith dialog systems, command of foreign languages,assumed task and domain knowledge).After the models have been defined, simulationsof user-system interactions can be generated.
Theseinteractions are logged and annotated on differentlevels in order to detect interaction problems.
Us-ability predictions are obtained from the (simulated)interaction problems.
The simulations can also sup-port reinforcement learning or other methods for au-tomatically determining the best dialog strategy.Building user interaction models by hand iscostly.
As an alternative to explicitly defining rulesand probabilities, simulations can be based on datasets of actual interactions, augmented with annota-tions such as indications of the dialog state, currentsubtask, inferred user state, and interaction phenom-ena.
Annotations can be generated by the dialogparticipants themselves, e.g.
by re-listening after thefact (Ward and Tsukahara, 2003), or by top com-municators, decision-makers, trend-setters, expertsin linguistics and communication, and the like.
Ma-chine learning techniques can help by providing pre-dictions of how users tend to react in various situa-tions from lightly annotated data.4 Perception and Judgment ModelOnce the interaction behavior is determined, theevaluator needs to know about the impact it has onthe quality perceived by the user.
As pointed out inSection 2, the perception and judgments processestake place in the human user and are thus hiddenfrom the observer.
The evaluator may, however, askthe user to describe the perceptual event and/or thequality event, either qualitatively in an open form orquantitatively on rating scales.
Provided that the ex-periment is properly planned and carried out, userquality judgments can be considered as direct qual-ity measurements, reflecting the user?s quality per-ception.185Whereas user judgments on quality will reflect theinternal reference and thus depend heavily on thespecific context and application, it may be assumedthat the characteristics of the perceptual event aremore universal.
For example, it is likely that sam-ples of observers and/or users would generally agreeon whether a given system could be characterizedas responsive, smooth, or predictable, etc.
regardlessof what they feel about the importance of each suchquality aspect.
We may take advantage of this bydefining a small set of universal perceptual qualitydimensions, that together are sufficient for predict-ing system value from the user?s point-of-view.In order to quantify the quality event and to iden-tify perceptual quality dimensions, psychometricmeasurement methods are needed, e.g.
interactionexperiments with appropriate measurement scales.Several attempts have been made to come up witha common questionnaire for user perception mea-surement related to spoken dialog systems, for ex-ample the SASSI questionnaire (Hone and Graham,2000) for systems using speech input, and the ITU-standard augmented framework for questionnaires(ITU-T Rec.
P.851, 2003) for systems with bothspeech-input and speech-output capabilities.
Studiesof the validity and the reliability of these question-naires (Mo?ller et al, 2007) show that both SASSIand P.851 can cover a large number of different qual-ity and usability dimensions with a high validity, andmainly with adequate reliability, although the gener-alizability of these results remains to be shown.On the basis of batteries of user judgments ob-tained with these questionnaires, dimension descrip-tors of the perceptual quality dimensions can be ex-tracted by means of factor analysis.
A summary ofsuch multidimensional analyses in Mo?ller (2005b)reveals that users?
perceptions of quality and usabil-ity can be decomposed into around 5 to 8 dimen-sions.
The resulting dimensions include factors suchas overall acceptability, task effectiveness, speed,cognitive effort, and joy-of-use.
It should be notedthat most such efforts have considered task-orientedsystems, where effectiveness, efficiency, and suc-cess are obviously important, however these dimen-sions may be less relevant to systems designed forother purposes, for example tutoring or ?edutain-ment?
(Bernsen et al, 2004), and additional factorsmay be needed for such applications.In order to describe the impact of the interac-tion flow on user-perceived quality, or on some ofits sub-dimensions, we would ideally model the hu-man perception and judgment processes.
Such anapproach has the clear advantage that the resultingmodel would be generic, i.e.
applicable to differ-ent systems and potentially for different user groups,and also analytic, i.e.
able to explain why certain in-teraction characteristics have a positive or negativeimpact on perceived quality.
Unfortunately, the per-ception and judgment processes involved in spoken-dialog interaction are not yet well understood, ascompared, for example, to those involved in listen-ing to transmitted speech samples and judging theirquality.
For the latter, models are available whichestimate quality with the help of peripheral audi-tory perception models and a signal-based compar-ison of representations of the perceptual event andthe assumed reference (Rix et al, 2006).
They areable to estimate user judgments on ?overall quality?with an average correlation of around 0.93, and arewidely used for planning, implementing and moni-toring telephone networks.For interactions with spoken dialog systems, thesituation is more complicated, as the perceptualevents depend on the interaction between user andsystems, and not on one speech signal alone.
A wayout is not to worry about the perception processes,and instead to use simple linear regression modelsfor predicting an average user judgment from vari-ous interaction parameters.
The most widely usedframework designed to support this sort of earlyevaluation is PARADISE (Walker et al, 1997).
Thetarget variable of PARADISE is an average of severaluser judgments (labeled ?user satisfaction?)
of dif-ferent system and interaction aspects, such as systemvoice, perceived system understanding, task ease,interaction pace, or the transparency of the interac-tion.
The interaction parameters are of three types,those relating to efficiency (including elapsed timeand the number of turns), those relating to ?dialogquality?
(including mean recognition score and thenumber of timeouts and rejections), and a measureof effectiveness (task success).
The model can betrained on data, and the results are readily inter-pretable: they can indicate which features of the in-teraction are most critical for improving user satis-faction.186PARADISE-style models can be very helpful toolsfor system developers.
For example, a recent inves-tigation showed that the model can be used to ef-fectively determine the minimum acceptable recog-nition rate for a smart-home system, leading tothe same critical threshold as that obtained fromuser judgments (Engelbrecht and Mo?ller, 2007).However, experience also shows that the PARADISEframework does not reliably give valid predictions ofindividual user judgments, typically covering onlyaround 40-50% of the variance in the data it istrained on.
The generality is also limited: cross-system extrapolation works sometimes but othertimes has low accuracy (Walker et al, 2000; Mo?ller,2005).
These limitations are easy to understand interms of Figure 1: over-ambitious attempts to di-rectly relate interaction parameters to a measure ofoverall system value seem unlikely to succeed ingeneral.
Thus it seems wise to limit the scope of theperception and judgment component to the predic-tion of values on the perceptual quality dimensions.In any case, there are several ways in which suchmodels could be improved.
One issue is that a linearcombination of factors is probably not generally ad-equate.
For example, parameters like the number ofturns required to execute a specific task will have anon-zero optimum value, at least for inexperiencedusers.
An excessively low number of turns will beas sure a sign of interaction problems as an exces-sively large number.
Such non-linear effects can-not be handled by linear models which only supportrelationships like ?the-more-the-better?
or ?the-less-the-better?.
Non-linear algorithms may overcomethese limitations.
A second issue is that of tempo-ral context: instead of using a single input vectorof interaction parameters for each dialog, it may bepossible to apply a sequence of feature vectors, onefor each exchange (user-system utterance pair).
Thefeatures may consist not only of numeric measuresbut also of categories encoding interaction phenom-ena.
Using this input one could then perhaps use aneural network or Hidden-Markov Model to predictvarious user judgments at the end of the interaction.5 Value ModelEven if a model can predict user judgments of ?over-all quality?
with high validity and reliability, this isnot necessarily a good indicator of the acceptabilityof a service.
For example, systems with a sophis-ticated and smooth dialog flow may be unaccept-able for frequent users because what counts for themis effectiveness and efficiency only.
Different usersmay focus on different quality dimensions in differ-ent contexts, and weight them according to the task,context of use, price, etc.A first step towards addressing this problemis to define quality aspects that a system devel-oper or service operator might be concerned about.There can be many such, but in usability engineer-ing they are typically categorized into ?effective-ness?, ?efficiency?
and ?satisfaction?.
A more de-tailed taxonomy of quality aspects can be found inMo?ller (2005).
On the basis of this or other tax-onomizations, value prediction models can be de-veloped.
For example, a system enabling 5-yearold girls to ?talk to Barbie?
might ascribe little im-portance to task completion, speech recognition ac-curacy, or efficiency, but high importance to voicequality, responsiveness, and unpredictability.
Thevalue model will derive a value description whichtakes such a weighting into account.
A model forsystems enabling police officers on patrol to obtaininformation over the telephone would have very dif-ferent weights.Unfortunately, there appear to be no published de-scriptions of value prediction models, perhaps be-cause they are very specific or even proprietary, de-pending on a company?s business logic and cus-tomer base.
Such models probably need not be verycomplex: it likely will suffice to ascribe weights tothe perceptual quality dimensions, or to quality as-pects derived from system developer and/or serviceoperator requirements.
Appropriate weights may beuncovered in stakeholder workshops, where design-ers, vendors, usability experts, marketing strategists,user representatives and so on come together anddiscuss what they desire or expect.6 Broader ImpactsWe have presented a tripartite evaluation frameworkwhich shows the relationship between user and sys-tem characteristics, interaction behavior, perceptualand quality events, their descriptions, and the finalvalue of the system or service.
In doing so, we187have mainly considered the needs of system devel-opers.
However, an evaluation framework that sup-ports judgments of perceived quality could provideadditional benefits for users.
We can imagine user-specific value models, representing what is impor-tant to specified user groups.
These could be so-licited for an entire group, or inferred from eachuser?s own personal history of interactions and deci-sions, e.g, through a personalization database avail-able to the service operator.
The models could alsobe used to support system selection, or to informreal-time system customization or adaptation.Better evaluation will also support the needs ofthe research community.
With the help of model-based evaluation, it will become easier for re-searchers not only to do evaluation more efficiently,but also to to produce more meaningful evaluationresults; saying not just ?this feature was useful?
butalso providing quantitative statements of how muchthe feature affects various interaction parameters,and from that how much it impacts the various qual-ity dimensions, and ultimately the value itself.
Thiswill make evaluation more meaningful and make iteasy for others to determine when an innovation isworth adopting, speeding technology transfer.One might worry that a standardized frameworkmight only be useful for evaluating incremental im-provements, thereby discouraging work on radicallydifferent dialog design concepts.
However well-designed evaluation components should enable thisframework to work for systems of any type, meaningthat it may be easier to explore new regions of thedesign space.
In particular it may enable more ac-curate prediction of the value of design innovationswhich in isolation may not be effective, but which incombination may be.7 Future WorkAlthough examples of some model components areavailable today, notably several interaction simula-tions and the PARADISE framework for predictinguser judgments from interaction parameters, theseare limited.
To realize a complete and generally use-ful evaluation model will require considerable work,for example, on:?
User behavior model: Of the three compo-nents, perhaps the greatest challenges are inthe development of user behavior models.
Weneed to develop methods which produce simu-lated behavior which is realistic (congruent tothe behavior of real users), and/or which pro-duce interaction parameters and/or quality in-dicators comparable to those obtained by sub-jective interaction experiments.
It is yet un-clear whether realistic user behavior can also begenerated for more advanced systems and do-mains, such as computer games, collaborativeproblem solving systems, or educational sys-tems.
We also need to develop models that ac-curately represent the behavior patterns of var-ious user groups.?
Interaction parameters: Several quality aspectsare still not reflected in the current parametersets, e.g.
indices for the quality of speech out-put.
Some approaches are described in Mo?llerand Heimansberg (2006), but the predictivepower is still too limited.
In addition, many pa-rameters still have to be derived by expert an-notation.
It may be possible to automaticallyinfer values for some parameters from proper-ties of the user?s and system?s speech signals,and such analyses may be a source for new pa-rameters, covering new quality aspects.?
Perceptual and quality events and reference:These items are subject of ongoing research inrelated disciplines, such as speech quality as-sessment, sound quality assessment, and prod-uct sound design.
Ideas for better, more realis-tic modeling may be derived from cooperationswith these disciplines.?
Quality judgments and dimension descriptors:In addition to the aspects covered by the SASSIand P.851 questionnaires, psychologists havedefined methods for assessing cognitive load,affect, affinity towards technology, etc.
Inputfrom such questionnaires may provide a betterbasis for developing value models.Although a full model may be out of reach for thenext decade, a more thorough understanding of hu-man behavior, perception and judgment processes isnot only of intrinsic interest but promises benefitsenough to make this a goal worth working towards.188AcknowledgmentsThis work was supported in part by NSF Grant No.0415150.ReferencesM.
Araki, and S. Doshita.
1997.
Automatic EvaluationEnvironment for Spoken Dialogue Systems.
DialogueProcessing in Spoken Language Systems, ECAI?96Workshop Proceedings, Springer Lecture Notes inArtificial Intelligence No.
1236, 183-194, Springer,Berlin.N.
O. Bernsen, H. Dybkj?r, and L. Dybkj?r.
1998.
De-signing Interactive Speech Systems: From First Ideasto User Testing.
Springer, Berlin.N.
O. Bernsen, L. Dybkj?r, L., and S. Kiilerich.
2004.Evaluating Conversation with Hans Christian Ander-sen. Proc.
4th Int.
Conf.
on Language Resources andEvaluation (LREC 2004), 3, pp.
1011-1014, Lisbon.K.-P. Engelbrecht, and S. Mo?ller.
2007.
Using Linear Re-gression Models for the Prediction of Data Distribu-tions.
Proc.
8th SIGdial Workshop on Discourse andDialogue, Antwerp, pp.
291-294.P.
Heisterkamp.
2003.
?Do not attempt to light withmatch!?
: Some Thoughts on Progress and ResearchGoals in Spoken Dialog Systems.
Proc.
8th Europ.Conf.
on Speech Communication and Technology (Eu-rospeech 2003 ?
Switzerland).K.
S. Hone, and R. Graham.
2000.
Towards a Tool for theSubjective Assessment of Speech System Interfaces(SASSI).
Natural Language Engineering, 3(3-4): 287-303.ITU-T Rec.
P.851.
2003.
Subjective Quality Eval-uation of Telephone Services Based on SpokenDialogue Systems.
International TelecommunicationUnion, Geneva.ITU-T Suppl.
24 to P-Series Rec.
2005.
ParametersDescribing the Interaction with Spoken DialogueSystems.
International Telecommunication Union,Geneva.ISO Standard 9241 Part 11.
1998.
Ergonomic Require-ments for Office Work with Visual Display Terminals(VDTs) ?
Part 11: Guidance on Usability.
Interna-tional Organization for Standardization, Geneva.U.
Jekosch.
2000.
Sprache ho?ren und beur-teilen: Ein Ansatz zur Grundlegung derSprachqualita?tsbeurteilung.
Habilitation thesis(unpublished), Universita?t/Gesamthochschule, Essen.R.
Lo?pez-Co?zar, A.
De la Torre, J. Segura, and A. Rubio.2003.
Assessment of Dialog Systems by Means of aNew Simulation Technique.
Speech Communication,40: 387-407.S.
Mo?ller, P. Smeele, H. Boland, and J. Krebber.
2007.Evaluating Spoken Dialogue Systems According toDe-Facto Standards: A Case Study.
Computer Speechand Language, 21: 26-53.S.
Mo?ller, R. Englert, K.-P. Engelbrecht, V. Hafner,A.
Jameson, A. Oulasvirta, A. Raake, and N. Rei-thinger.
2006.
MeMo: Towards Automatic UsabilityEvaluation of Spoken Dialogue Services by User Er-ror Simulations.
Proc.
9th Int.
Conf.
on Spoken Lan-guage Processing (Interspeech 2006 ?
ICSLP), Pitts-burgh PA, pp.
1786-1789.S.
Mo?ller, and J. Heimansberg.
2006.
Estimation ofTTS Quality in Telephone Environments Using aReference-free Quality Prediction Model.
Proc.
2ndISCA/DEGA Tutorial and Research Workshop on Per-ceptual Quality of Systems, Berlin, pp.
56-60.S.
Mo?ller.
2005.
Quality of Telephone-Based Spoken Di-alogue Systems.
Springer, New York NY.S.
Mo?ller.
2005b.
Perceptual Quality Dimensions of Spo-ken Dialogue Systems: A Review and New Exper-imental Results.
Proc.
4th European Congress onAcoustics (Forum Acusticum Budapest 2005), Bu-dapest, pp.
2681-2686.A.
Oulasvirta, S. Mo?ller, K.-P. Engelbrecht, and A. Jame-son.
2006.
The Relationship of User Errors to Per-ceived Usability of a Spoken Dialogue System.
Proc.2nd ISCA/DEGA Tutorial and Research Workshop onPerceptual Quality of Systems, Berlin, pp.
61-67.T.
Paek.
2007.
Toward Evaluation that Leads to BestPractices: Reconciling Dialog Evaluation in Researchand Industry.
Bridging the Gap: Academic and Indus-trial Research in Dialog Technologies Workshop Pro-ceedings, Rochester, pp.
40-47.R.
Pieraccini, J. Huerta.
2005.
Where Do We and Com-mercial Spoken Dialog Systems.
Proc.
6th SIGdialWorkshop on Discourse and Dialogue, Lisbon, pp.
1-10.A.
W. Rix, J. G. Beerends, D.-S. Kim, P. Kroon, andO.
Ghitza.
2006.
Objective Assessment of Speech andAudio Quality ?
Technology and Applications.
IEEETrans.
Audio, Speech, Lang.
Process, 14: 1890-1901.M.
Walker, C. Kamm, and D. Litman.
2000.
TowardsDeveloping General Models of Usability with PAR-ADISE.
Natural Language Engineering, 6: 363-377.M.
A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.1997.
PARADISE: A Framework for Evaluating Spo-ken Dialogue Agents.
Proc.
of the ACL/EACL 35thAnn.
Meeting of the Assoc.
for Computational Linguis-tics, Madrid, Morgan Kaufmann, San Francisco CA,pp.
271-280.N.
Ward, A. G. Rivera, K. Ward, and D. G. Novick.
2005.Root Causes of Lost Time and User Stress in a SimpleDialog System.
Proc.
9th European Conf.
on SpeechCommunication and Technology (Interspeech 2005),Lisboa.N.
Ward, and W. Tsukahara.
2003.
A Study in Respon-siveness in Spoken Dialogue.
International Journal ofHuman-Computer Studies, 59: 603-630.189
