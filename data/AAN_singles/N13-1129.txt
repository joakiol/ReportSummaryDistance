Proceedings of NAACL-HLT 2013, pages 1092?1099,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA method for the approximation of incremental understanding of explicitutterance meaning using predictive models in finite domainsDavid DeVault and David TraumInstitute for Creative Technologies, University of Southern California,12015 Waterfront Dr., Playa Vista, CA 90094 USA{devault,traum}@ict.usc.eduAbstractThis paper explores the relationship between explicitand predictive models of incremental speech under-standing in a dialogue system that supports a finiteset of user utterance meanings.
We present a methodthat enables the approximation of explicit under-standing using information implicit in a predictiveunderstanding model for the same domain.
We showpromising performance for this method in a corpusevaluation, and discuss its practical application andannotation costs in relation to some alternative ap-proaches.1 IntroductionIn recent years, there has been a growing interest amongresearchers in methods for incremental natural languageunderstanding (NLU) for spoken dialogue systems; seee.g.
(Skantze and Schlangen, 2009; Sagae et al 2009;Schlangen et al 2009; Heintze et al 2010; DeVault etal., 2011a; Selfridge et al 2012).
This work has gen-erally been motivated by a desire to make dialogue sys-tems more efficient and more natural, by enabling them toprovide lower latency responses (Skantze and Schlangen,2009), human-like feedback such as backchannels that in-dicate how well the system is understanding user speech(DeVault et al 2011b; Traum et al 2012), and more in-teractive response capabilities such as collaborative com-pletions of user utterances (DeVault et al 2011a), moreadaptive handling of interruptions (Buschmeier et al2012), and others.This paper builds on techniques developed in previouswork that has adopted a predictive approach to incremen-tal NLU (DeVault et al 2011a).
On this approach, atspecific moments while a user?s speech is in progress,an attempt is made to predict what the full meaning ofthe complete user utterance will be.
Predictive modelscan be contrasted with explicit approaches to incremen-tal NLU.
We use the term explicit understanding to referto approaches that attempt to determine the meaning thathas been expressed explicitly in the user?s partial utter-ance so far (without predicting further aspects of mean-ing to come).
Explicit understanding of partial utterancescan be implemented using statistical classification or se-quential tagging models (Heintze et al 2010).Both predictive and explicit incremental NLU capabil-ities can be valuable in a dialogue system.
Predictioncan support specific response capabilities, such as sys-tem completion of user utterances (DeVault et al 2011a)and reduced response latency.1 However, explicit modelssupport additional and complementary capabilities.
Forinstance, depending on the application domain (Heintzeet al 2010) and on the individual utterance (DeVault etal., 2011b), it may be difficult for a system to predict auser?s impending meaning with confidence.
Neverthe-less, it may often be possible for systems to determinethe meaning of what a user has said so far, and to takeaction based on this partial understanding.
As one exam-ple, items in a user interface could be highlighted whenmentioned by a user (Bu?
and Schlangen, 2011).
An-other capability would be to provide grounding feedback,such as verbal back-channels or head nods (in embod-ied systems), to indicate when the system is understand-ing the user?s meaning (Traum et al 2012).
Explicit ut-terance meanings also allow a system to distinguish be-tween meaning that has been expressed and meaning thatis merely implied or inferred, which may be less reliable.In the near future, as incremental processing capabilitiesin dialogue systems grow, it may prove valuable for di-alogue systems to combine both predictive and explicitincremental understanding capabilities.In this paper, we present a technique for approximatinga user?s explicit meaning using an existing predictive un-derstanding framework (DeVault et al 2011a).
The spe-cific new contributions in this paper are (1) to show that1A simple approach to reducing response latency is to begin to plana response to the predicted meaning while the user is still speaking.1092an estimate of a user?s explicit utterance meaning can bederived from this kind of predictive understanding model(Section 2); (2) to quantify the performance of this newmethod in a corpus evaluation (Section 3); (3) to provideconcrete examples and discussion of the annotation costsassociated with implementing this technique, in relationto some alternative approaches to explicit understanding(Section 4).
Our results and discussion show that theproposed method offers promising performance, has rela-tively low annotation costs, and enables explicit and pre-dictive understanding to be easily combined within a di-alogue system.
It may therefore be a useful incrementalunderstanding technique for some dialogue systems.2 Technical Approach and Data SetIn Sections 2.1-2.3, we briefly summarize the data set andapproach to predictive incremental NLU (DeVault et al2011a) that serves as the starting point for the new workin this paper.
Sections 2.4 and 2.5 present our new ap-proach to explicit understanding based on this approach.2.1 Data setFor the experiments reported here, we use a corpus ofuser utterances collected with the SASO-EN spoken dia-logue system (Hartholt et al 2008; Traum et al 2008).Briefly, this system is designed to allow a trainee to prac-tice multi-party negotiation skills by engaging in face toface negotiation with virtual humans.
The scenario in-volves a negotiation about the possible re-location of amedical clinic in an Iraqi village.
A human trainee playsthe role of a US Army captain, and there are two virtualhumans that he negotiates with: Doctor Perez, the headof an NGO clinic, and a local village elder, al-Hassan.The captain?s main objective is to convince the doctor andthe elder to move the clinic out of an unsafe marketplacearea.The corpus used for the experiments in this paper in-cludes 3,826 training and 449 testing utterances drawnfrom user dialogues in this domain.
The corpus and its se-mantic annotation are described in (DeVault et al 2010;DeVault et al 2011a).
All user utterances have been au-dio recorded, transcribed, and manually annotated withthe correct NLU output frame for the entire utterance.
(We discuss the cost of this annotation in Section 4.)
EachNLU output frame contains a set of attributes and valuesthat represent semantic information linked to a domain-specific ontology and task model (Traum, 2003).
Exam-ples of the NLU output frames are included in Figures 2,3, and 5.2.2 Predictive incremental NLUThis approach uses a predictive incremental NLU mod-ule, mxNLU (Sagae et al 2009; DeVault et al 2011a),which is based on maximum entropy classification.
Theapproach treats entire individual frames as output classes,and extracts input features from partial ASR results.
Todefine the incremental understanding problem, the audioof the utterances in the training data were fed throughan ASR module, PocketSphinx (Huggins-Daines et al2006), in 200 millisecond chunks, and each partial ASRresult produced by the ASR was recorded.
Each par-tial ASR result then serves as an incremental input tomxNLU.
NLU is predictive in the sense that, for eachpartial ASR result, the task of mxNLU is to produce asoutput the complete frame that has been associated by ahuman annotator with the user?s complete utterance, evenif that utterance has not yet been fully processed by theASR.The human annotation defines a finite set S ={S1, ..., SN} of possible NLU output frames, where eachframe Si = {e1, ..., en} is a set of key-value pairs orframe elements.
For notation, a user utterance u generallycreates a sequence of m partial ASR results ?r1, ..., rm?,where each ASR result rj is a partial text such as we needto move.
Let Gu denote the correct (or ?gold?)
frame forthe complete utterance u.
For each result rj and for eachcomplete frame Si, the maximum entropy model pro-vides P (Gu = Si|rj).
The NLU output frame SNLUj isthe complete frame for which this probability is highest.2.3 Performance of predictive incremental NLUThe performance of this predictive incremental NLUframework has been evaluated using the training andtest portions of the SASO-EN data set described in Sec-tion 2.1.
Performance is quantified by looking at pre-cision, recall, and F-score of the frame elements thatcompose the predicted (SNLUj ) and correct (Gu) framesfor each partial ASR result.
When evaluated over allthe 5,736 partial ASR results for the 449 test utterances,the precision/recall/F-Score of this predictive NLU, inrelation to the complete frames, are 0.67/0.47/0.56, re-spectively.
When evaluated on only the ASR resultsfor complete test utterances, these scores increase to0.81/0.71/0.76, respectively.2.4 Assigning probability to frame elementsAn interesting question is whether we can use this modelto attach useful probabilities not only to complete pre-dicted frames but also to the individual frame elementsthat make up those frames.
To explore this, for each par-tial ASR result rj in each utterance u, and for each frameelement e in SASO-EN, let us model the probability thate will be part of the correct frame for the complete utter-ance as:P (e ?
Gu|rj) =?Si:e?SiP (Gu = Si|rj) (1)10930.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.000.00.10.20.30.40.50.60.70.80.91.0Probability assigned to frame elementRelativefrequencyofframeelementin correctframe00.10.20.30.40.50.60.7Relativefrequencyofprobabilitybeingassignedtoa frame elementModel calibration (left axis)Perfect calibration (left axis)Relative frequency ofassigned probabilities(right axis)Figure 1: Calibration of frame element probabilities.This method derives the probability of frame elementsfrom the probabilities assigned to the possible frames thatcontain them.
Computing this sum is straightforward in afinite semantic domain such as SASO-EN.We computed this probability for all frame elementse and all partial ASR results rj in our test set, yieldingapproximately 478,000 probability values.
We groupedthese probability values into bins of size 0.05, and cal-culated the frequency with which the frame elements ineach bin were indeed present in the correct frame Gu forthe relevant utterance u.
The results are presented in Fig-ure 1, which shows that the probability values derivedfrom Equation (1) are relatively ?well calibrated?, in thesense that the relative frequency with which a frame el-ement is in the final frame is very close to the numericprobability assigned by Equation (1).
The figure alsoshows how frequently the model assigns various proba-bility ranges to frame elements (blue dotted line, plottedagainst the secondary right axis).
Note that most frameelements are assigned very little probability for most par-tial ASR results.We conclude from these observations that the probabil-ities assigned by (1) could indeed carry useful informa-tion about the likelihood that individual key values willbe present in the complete utterance meaning.2.5 Selecting probable frame elementsIn exploring the model of frame element probabilitiesgiven in Equation (1), we observed that often the reasona frame element has lower probability, at a given pointwithin a user utterance, is that it is a prediction rather thansomething that has been expressed explicitly.
Building onthis observation, our technique for estimating the user?sexplicit meaning uses a probability threshold to selectthose individual frame elements which are most likely tobe in the frame for a complete utterance, according to thepredictive model.
That is, at each partial result rj , weestimate the user?s explicit meaning using a constructedframe:SSUBj = {e|P (e ?
Gu|rj) ?
?}
(2)This approximation could work well if, in practice, themost probable frame elements prove to match fairlyclosely the user?s non-incremental utterance meaning atthe point this frame is constructed.
We evaluate this inthe next section.Note that, in general, the returned subset of frameelements may not be identical to any complete frameSi ?
S; rather it will correspond to parts of these com-plete frames or ?subframes?.3 Performance EvaluationTo evaluate this technique, we constructed subsets offrame elements or ?explicit subframes?
using Equation(2) and various minimum probability thresholds ?
for par-tial ASR results in our test set.
We then compared theresulting subframes both to the final complete frame Gufor each utterance u, and also to manually annotated sub-1094Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframePartial ASR result: hello0.813 <S>.sem.speechact.type greeting <S>.sem.speechact.type greeting<S>.addressee doctor-perez<S>.sem.speechact.type greetingPartial ASR result: hello elder0.945 <S>.sem.speechact.type greeting0.934 <S>.addressee elder-al-hassan<S>.sem.speechact.type greeting<S>.addressee elder-al-hassan<S>.sem.speechact.type greeting<S>.addressee elder-al-hassanFigure 2: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of hello elder.frames that represent human judgments of explicit incre-mental utterance meaning.To collect these judgments, we hand-annotated a word-meaning alignment for 50 random utterances in our testset.2 To perform this annotation, successively larger pre-fixes of each utterance transcript were mapped to succes-sively larger subframes of the full frame for the completeutterance.
The annotated subframes for each utteranceprefix were selected to be explicit; they include only thoseframe elements that are explicitly expressed in the corre-sponding prefix of the user?s utterance.
(We discuss thecost of this annotation in Section 4.
)We provide a simple concrete example in Figure 2.This example shows two partial ASR results duringan utterance of hello elder by a user.
For each par-tial ASR result, three frames are indicated horizon-tally.
At the right, labeled ?Annotated subframe?, weshow the human judgment of explicit incremental ut-terance meaning for this partial utterance.
Our hu-man judge has indicated that the word hello correspondsto the frame element <S>.sem.speechact.typegreeting, and that the words hello elder correspondto an expanded frame that includes the frame element<S>.addressee elder-al-hassan.At the left, labeled ?Explicit subframe?, we showthe subframe selected by Equation (2) for each par-tial ASR result, with threshold ?
= 0.5.
A relevantbackground fact for this example is that in this sce-nario, the user can generally address either of two vir-tual humans who are present, Doctor Perez or ElderAl-Hassan.
After the user has said hello, the frameelement <S>.sem.speechact.type greeting isassigned probability 0.813 by Equation (1), and only thisframe element appears in the explicit subframe.In the middle, labeled ?Predicted complete frame?, thefigure also shows the full predicted frame from mxNLUat each point.
After the user has said hello, the fullpredicted output includes an additional frame element,<S>.addressee doctor-perez, indicating a pre-diction that the addressee of this user utterance will beDoctor Perez rather than Elder al-Hassan.
However, the2Note that no utterances in our training set were annotated.probability assigned to this prediction by Equation (1) isless than 0.5, and so this predicted frame element is ex-cluded from the explicit subframe.
And indeed, this is thecorrect explicit representation of the meaning of hello inthis system.This simple example illustrates how our proposed tech-nique can enable a dialogue system to have access to bothexplicit and predicted utterance meaning as a user?s ut-terance progresses.
An excerpt from a more complexutterance is given in Figure 3.
This example shows in-cremental outputs for two partial ASR results during auser utterance of we will provide transportation at nocost.
In this example, the explicit subframe for wewill includes frame elements that convey that the cap-tain (i.e.
the user) is promising to do something.
Thissubframe does not exactly match the human judgmentof explicit meaning at the right, which does not includeat this point the <S>.sem.agent captain-kirkand <S>.sem.type event frame elements.
How-ever, the explicit subframe more closely matches the hu-man judgment than does the predicted complete framefrom mxNLU (middle column), which includes an in-correct prediction that the captain is promising to de-liver medical supplies (represented by the key values<S>.sem.event deliver and <S>.sem.thememedical-supplies).
For the next partial ASR re-sult shown in the figure, the explicit subframe correctlyadds several additional frame elements which formalizethe meaning of the phrase provide transportation in thisscenario as having the army move the clinic out of themarket area.To understand more quantitatively how well this tech-nique works, we evaluated this technique in the SASO-EN test corpus, using different probability thresholds inthe range [0.5,1.0).
We present the results in Figure 4.
Tounderstand the effect of the threshold ?
, note that, in gen-eral, the effect of selecting a higher threshold should be to?cherry pick?
those frame elements which are most likelyto appear in the complete frame Gu, thereby increasingprecision while decreasing recall of the frame elements inSSUBj in relation to Gu.
In the figure, we can see that thisis indeed the case.
The lines marked ?
(complete frame)?1095Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframePartial ASR result: we will0.856 <S>.mood declarative0.824 <S>.sem.agent captain-kirk0.663 <S>.sem.modal.intention will0.663 <S>.sem.speechact.type promise0.776 <S>.sem.type event<S>.mood declarative<S>.sem.agent captain-kirk<S>.sem.event deliver<S>.sem.modal.intention will<S>.sem.speechact.type promise<S>.sem.theme medical-supplies<S>.sem.type event<S>.mood declarative<S>.sem.modal.intention will<S>.sem.speechact.type promisePartial ASR result: we will provide transportation0.991 <S>.mood declarative0.990 <S>.sem.agent captain-kirk0.927 <S>.sem.event move0.905 <S>.sem.instrument us-army0.964 <S>.sem.modal.intention will0.927 <S>.sem.source market0.964 <S>.sem.speechact.type promise0.928 <S>.sem.theme clinic0.989 <S>.sem.type event<S>.mood declarative<S>.sem.agent captain-kirk<S>.sem.event move<S>.sem.instrument us-army<S>.sem.modal.intention will<S>.sem.source market<S>.sem.speechact.type promise<S>.sem.theme clinic<S>.sem.type event<S>.mood declarative<S>.sem.agent captain-kirk<S>.sem.event move<S>.sem.instrument us-army<S>.sem.modal.intention will<S>.sem.source market<S>.sem.speechact.type promise<S>.sem.theme clinic<S>.sem.type eventFigure 3: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of we will providetransportation at no cost.0.5 0.6 0.7 0.8 0.9 1.00.00.20.40.60.81.0thresholdoooPrecision (complete frame)Precision (annotated subframe)Recall (complete frame)Recall (annotated subframe)F?Score (complete frame)F?Score (annotated subframe)Figure 4: The effect of threshold on precision, recall, and F-Score of explicit subframes.
All scores are measured in relation tocomplete utterance frames and annotated subframes.1096in the figure evaluate the returned subframes in relationto the complete frameGu associated with the user?s com-plete utterance.
We see that this method enables us toselect subsets of frame elements that are most likely toappear in Gu: by increasing the threshold, it is possibleto return subframes which are of increasingly higher pre-cision in relation to the final frame Gu, but that also havelower recall.We also evaluated the returned subframes in relation tothe hand-annotated subframes, to assess its performanceat identifying the user?s explicit meaning.
For an utter-ance u that generates partial ASR results ?r1, ..., rm?,we denote the hand-annotated subframe corresponding topartial ASR result rj by GSUBj .
In the lines marked ?
(an-notated subframe)?, we show the precision, recall, andF-score of the explicit subframe for each ASR result rjin relation to the annotated subframe GSUBj .As a first observation, note that at any threshold level,the explicit subframes do better at recalling the hand-annotated subframe elements than they do at recalling thecomplete frame elements.
This means our new method isbetter at recalling what has been said already by the userthan it is at predicting what will be said, as intended.
Wehave seen two examples of this already, for the partialASR result hello in Figure 2, and for the partial ASR re-sult we will in Figure 3.A second observation in Figure 4 is that precision re-mains better against the complete utterance frame thanagainst the hand-annotated subframe (at all threshold lev-els).
This indicates that the explicit subframes are oftenstill predicting some aspects of the full frame.
An exam-ple of this is given in Figure 5, where the user?s partialutterance we need to is assigned an explicit subframe thatincludes frame elements describing an event of movingthe clinic, which the user has not said explicitly.
Thishappens because, in the SASO-EN domain, in fact thereis nothing else that the interlocutors need to do besidesmove the clinic.
So based on the NLU training data,the data-driven probabilities assigned by Equation (1) de-scribe the additional frame elements as about as probableas the ones capturing the we need to part of the semantics(given at the right).Finally, a third observation is that overall, the preci-sion, recall, and F-score results against the annotated sub-frames using our method are surprisingly strong.
Forexample, when evaluating the explicit subframes overall partial ASR results, an F-score of 0.75 is attained atthresholds in the range 0.5-0.55.
This F-score is sub-stantially better than the F-score of our predictive NLUin relation to the final full frames, which is 0.56 whenevaluated over all partial ASR results.
This means thatour proposed model works better as an explicit incre-mental NLU than mxNLU works as a predictive incre-mental NLU.
Further, we observe that this F-score of0.75 against hand-annotated subframes is approximatelyas good as the F-score of 0.76 that is achieved whenmxNLU is used to interpret complete utterances.
Wetherefore conclude that the proposed model is a promis-ing and viable approach to explicit incremental NLU inSASO-EN.4 Discussion and Related ApproachesIn this section, we discuss some of the practical aspectsof using the technique presented here, in relation to somealternative approaches.An important consideration for NLU techniques is thecost, in both time and knowledge, of the annotation thatis needed.
One attractive aspect of our technique is thatthe only semantic annotation that is required is the asso-ciation of complete user utterances with complete NLUoutput frames.
This task can be performed by anyone fa-miliar with the scenario and the semantic frame format,such as a system developer or scenario designer.
In fact,the annotation of the SASO-EN data set we use in thispaper has been described in (DeVault et al 2010), whichreports that the overall corpus of 4678 token utteranceswas semantically annotated at an average rate of about 10seconds per unique utterance.The model in Equation (2) is what (Heintze et al2010) call a hybrid output approach, in which larger andlarger frames are provided as partial input grows, butin which a detailed alignment between surface text andframes is not provided by the incremental NLU compo-nent.
They contrast hybrid output systems with tech-niques that deliver either whole-frame output (like thepredictive mxNLU) or aligned output that connects indi-vidual words to their meanings.
A data-driven approachto providing aligned outputs would involve preparinga more detailed annotated corpus that aligns individ-ual words and surface expressions to their correspondingframe elements.
Given such a word-aligned corpus, onecould train several kinds of models to produce the alignedoutputs incrementally.
One strategy would be to use a se-quential tagging model such as a CRF to tag partial utter-ances with the frame elements that capture their explicitmeaning, as in (Heintze et al 2010).Using a machine learning approach that models amore detailed alignment between surface text and frameswould be one way to more cleanly separate explicit frompredictive aspects of meaning.
Preparing the training datafor such models, however, would create additional an-notation costs.
As part of creating the annotated sub-frames for the evaluation presented in Section 3, we mea-sured the time requirement for such annotation of word-meaning alignments at about 30 seconds per unique ut-terance.
Performing full word-meaning alignment there-fore takes about three times as much time as the com-plete utterance annotation needed for our technique.
Ad-1097Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframePartial ASR result: we0.753 <S>.mood declarative0.687 <S>.sem.agent captain-kirk0.692 <S>.sem.type event<S>.mood declarative<S>.sem.agent captain-kirk<S>.sem.event deliver<S>.sem.modal.possibility can<S>.sem.speechact.type offer<S>.sem.theme medical-supplies<S>.sem.type eventPartial ASR result: we need to0.945 <S>.mood declarative0.928 <S>.sem.agent captain-kirk0.900 <S>.sem.event move0.816 <S>.sem.modal.deontic must0.900 <S>.sem.source market0.900 <S>.sem.speechact.type statement0.906 <S>.sem.theme clinic0.930 <S>.sem.type event<S>.mood declarative<S>.sem.agent captain-kirk<S>.sem.event move<S>.sem.modal.deontic must<S>.sem.source market<S>.sem.speechact.type statement<S>.sem.theme clinic<S>.sem.type event<S>.mood declarative<S>.sem.modal.deontic must<S>.sem.speechact.type statementFigure 5: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of we need to move theclinic.ditionally, this task requires a greater degree of linguis-tic knowledge and sophistication, as the annotator mustbe able to segment the utterance and align specific sur-face segments with potentially complex aspects of mean-ing such as modality, polarity, speech act types, andothers.
An example of the kinds of complexities thatarise is illustrated in Figure 3, where the relationship be-tween specific words like ?provide?
and ?transportation?to frame elements like <S>.sem.event move and<S>.sem.theme clinic is not transparent, even ifit is straightforward to mark the whole utterance as con-veying that meaning in this domain.
We have generallyfound this alignment task challenging for people withoutadvanced linguistics training.The reason we describe the method in this paper as anapproximation of explicit NLU is that, partly because itis trained without detailed word-meaning alignments, itcan be expected to occasionally include some predictiveaspects of user utterance meaning.
An example of this isthe method?s explicit subframe output for the phrase weneed to in Figure 5.Another way to approximate explicit NLU would beusing the method (Heintze et al 2010) call an ensem-ble of classifiers; it involves training an individual clas-sifier for each frame key.
Like the method presentedhere, an ensemble of classifiers can be easily trained topredict those frame elements that will appear in the fi-nal frame Gu for each utterance.
And like our method,prediction with an ensemble of classifiers does not re-quire detailed annotation of word-meaning alignment inthe training data.
One difference is that, with our method,by selecting an appropriate threshold, it is easy to enforcecertain consistency properties on subframe outputs.
In anensemble of classifiers approach, there is no immediateguarantee that the output frame constructed by the inde-pendent classifiers will be internally consistent from thestandpoint of downstream system modules (Heintze et al2010).
For example, in the SASO-EN domain, an NLUframe should not contain frame elements that mix aspectsof events and states in the SASO-EN ontology; e.g., theframe element <S>.sem.type event should not co-occur in an NLU output frame with the frame element<S>.sem.object-id market (which would be ap-propriate for a state frame but not for an event frame).With the method proposed here, if we select a threshold?
that is greater than 0.5, and if none of the completeNLU frames contain incompatible key values (which isrelatively easy to enforce as part of the annotation task),then it will be mathematically impossible for two incom-patible frame elements to be returned in a subframe.3Ultimately, a classification method that is trained onword-meaning aligned data and that uses additional tech-niques to ensure that only valid, grammatical outputframes are produced could prove to be an attractive ap-proach.
In future work, we will explore such techniques,and compare both their performance as well as their anno-tation and development costs to the approximation tech-nique presented here.5 ConclusionThe analysis in this paper has explored a method of ap-proximating explicit incremental NLU using predictive3Suppose frame element ei is incompatible with ej , and thatP (ei ?
Gu|rj) > 0.5.
By stipulation, no complete frame S ?
Ssuch that ei ?
S will also contain ej .
Since we know that the totalprobability of all the frames containing ei must be greater than 0.5 inorder for ei to be selected, we can infer that the total probability of allframes including ej must be less than 0.5, and thus that ej will not beselected.1098techniques in finite semantic domains.
We have shownthat an estimate of a user?s explicit utterance meaningcan be derived from an existing predictive understand-ing model in an example domain.
We have quantifiedthe performance of this new method in a corpus evalu-ation, showing that the method returns incremental ex-plicit subframes with performance ?
as measured by pre-cision, recall, and F-Score against hand-annotated sub-frames ?
that is competitive with a current statistical,data-driven approach for understanding complete spokenutterances in the same domain.
We have provided ex-amples that illustrate its strengths and weaknesses, anddiscussed the annotation costs associated with imple-menting this technique in relation to some alternative ap-proaches.
The method requires no additional annotationbeyond what is needed for training an NLU module tounderstand complete spoken utterances.
(Hand annota-tion of word-meaning alignment for a small number ofutterances may be performed in order to tune the se-lected threshold and evaluate explicit understanding per-formance.)
The method provides a free parameter thatcan be used to target the most advantageous levels of pre-cision and recall for a particular dialogue system applica-tion.
In future work, we will explore additional machinelearning models that leverage richer training data, and in-vestigate further the combination of explicit and predic-tive techniques.AcknowledgmentsThe project or effort described here has been sponsoredby the U.S. Army Research, Development, and Engi-neering Command (RDECOM).
Statements and opinionsexpressed do not necessarily reflect the position or thepolicy of the United States Government, and no officialendorsement should be inferred.
This material is basedupon work supported by the National Science Founda-tion under Grant No.
IIS-1219253.
Any opinions, find-ings, and conclusions or recommendations expressed inthis material are those of the author(s) and do not neces-sarily reflect the views of the National Science Founda-tion.ReferencesHendrik Buschmeier, Timo Baumann, Benjamin Dosch, Ste-fan Kopp, and David Schlangen.
2012.
Combining incre-mental language generation and incremental speech synthe-sis for adaptive information presentation.
In Proceedings ofthe 13th Annual Meeting of the Special Interest Group onDiscourse and Dialogue, pages 295?303, Seoul, South Ko-rea, July.
Association for Computational Linguistics.Okko Bu?
and David Schlangen.
2011.
Dium - an incrementaldialogue manager that can produce self-corrections.
In Pro-ceedings of the 15th Workshop on the Semantics and Prag-matics of Dialogue (SemDial).David DeVault, Susan Robinson, and David Traum.
2010.IORelator: A graphical user interface to enable rapid seman-tic annotation for data-driven natural language understand-ing.
In Fifth Joint ISO-ACL/SIGSEM Workshop on Interop-erable Semantic Annotation.David DeVault, Kenji Sagae, and David Traum.
2011a.
Incre-mental interpretation and prediction of utterance meaning forinteractive dialogue.
Dialogue & Discourse, 2(1).David DeVault, Kenji Sagae, and David R. Traum.
2011b.
De-tecting the status of a predictive incremental speech under-standing model for real-time decision-making in a spokendialogue system.
In Interspeech, pages 1021?1024.Arno Hartholt, Thomas Russ, David Traum, Eduard Hovy,and Susan Robinson.
2008.
A common ground for vir-tual humans: Using an ontology in a natural language ori-ented virtual human architecture.
In European LanguageResources Association (ELRA), editor, Proc.
LREC, Mar-rakech, Morocco, may.Silvan Heintze, Timo Baumann, and David Schlangen.
2010.Comparing local and sequential models for statistical incre-mental natural language understanding.
In The 11th AnnualMeeting of the Special Interest Group in Discourse and Dia-logue (SIGDIAL 2010).David Huggins-Daines, Mohit Kumar, Arthur Chan, Alan W.Black, Mosur Ravishankar, and Alex I. Rudnicky.
2006.Pocketsphinx: A free, real-time continuous speech recog-nition system for hand-held devices.
In Proceedings ofICASSP.Kenji Sagae, Gwen Christian, David DeVault, and David R.Traum.
2009.
Towards natural language understanding ofpartial speech recognition results in dialogue systems.
InNAACL HLT.David Schlangen, Timo Baumann, and Michaela Atterer.
2009.Incremental reference resolution: The task, metrics for eval-uation, and a bayesian filtering model that is sensitive to dis-fluencies.
In SIGDIAL.Ethan O. Selfridge, Iker Arizmendi, Peter A. Heeman, and Ja-son D. Williams.
2012.
Integrating incremental speechrecognition and pomdp-based dialogue systems.
In Proceed-ings of the 13th Annual Meeting of the Special Interest Groupon Discourse and Dialogue, pages 275?279, Seoul, SouthKorea, July.
Association for Computational Linguistics.Gabriel Skantze and David Schlangen.
2009.
Incremental di-alogue processing in a micro-domain.
In Proceedings ofEACL 2009.David Traum, Stacy Marsella, Jonathan Gratch, Jina Lee, andArno Hartholt.
2008.
Multi-party, multi-issue, multi-strategy negotiation for multi-modal virtual agents.
In Proc.of Intelligent Virtual Agents Conference IVA-2008.David Traum, David DeVault, Jina Lee, Zhiyang Wang, andStacy C. Marsella.
2012.
Incremental dialogue understand-ing and feedback for multi-party, multimodal conversation.In The 12th International Conference on Intelligent VirtualAgents (IVA), Santa Cruz, CA, September.David Traum.
2003.
Semantics and pragmatics of questionsand answers for dialogue agents.
In Proc.
of the Interna-tional Workshop on Computational Semantics, pages 380?394, January.1099
