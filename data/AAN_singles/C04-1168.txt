A Unified Approach in Speech-to-Speech Translation: IntegratingFeatures of Speech Recognition and Machine TranslationRuiqiang Zhang and Genichiro Kikui and Hirofumi YamamotoTaro Watanabe and Frank Soong and Wai Kit LoATR Spoken Language Translation Research Laboratories2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan{ruiqiang.zhang, genichiro.kikui}@atr.jpAbstractBased upon a statistically trained speechtranslation system, in this study, we tryto combine distinctive features derived fromthe two modules: speech recognition andstatistical machine translation, in a log-linear model.
The translation hypothesesare then rescored and translation perfor-mance is improved.
The standard trans-lation evaluation metrics, including BLEU,NIST, multiple reference word error rateand its position independent counterpart,were optimized to solve the weights of thefeatures in the log-linear model.
The exper-imental results have shown significant im-provement over the baseline IBM model 4in all automatic translation evaluation met-rics.
The largest was for BLEU, by 7.9%absolute.1 IntroductionCurrent translation systems are typically of acascaded structure: speech recognition followedby machine translation.
This structure, whileexplicit, lacks some joint optimality in per-formance since the speech recognition moduleand translation module are running rather in-dependently.
Moreover, the translation moduleof a speech translation system, a natural off-spring of text-input based translation system,usually takes a single-best recognition hypoth-esis transcribed in text and performs standardtext-based translation.
Lots of supplementaryinformation available from speech recognition,such as N -best recognition recognition hypothe-ses, likelihoods of acoustic and language models,is not well utilized in the translation process.The information can be effective for improvingtranslation quality if employed properly.The supplementary information can be ex-ploited by a tight coupling of speech recognitionand machine translation (Ney, 1999) or keepingthe cascaded structure unchanged but using anintegration model, log-linear model, to rescorethe translation hypotheses.
In this study thelast approach was used due to its explicitness.In this paper we intended to improve speechtranslation by exploiting these information.Moreover, a number of advanced features fromthe machine translation module were also addedin the models.
All the features from the speechrecognition and machine translation modulewere combined by the log-linear models seam-lessly.In order to test our results broadly, we usedfour automatic translation evaluation metrics:BLEU, NIST, multiple word error rate and po-sition independent word error rate, to measurethe translation improvement.In the following, in section 2 we introduce thespeech translation system.
In section 3, we de-scribe the optimization algorithm used to findthe weight parameters in the log-linear model.In section 4 we demonstrate the effectivenessof our technique in speech translation experi-ments.
In the final two sections we discuss theresults and present our conclusions.2 Feature-based Log-linear Modelsin Speech TranslationThe speech translation experimental systemused in this study illustrated in Fig.
1 is a typi-cal, statistics-based one.
It consists of two ma-jor cascaded components: an automatic speechrecognition (ASR) module and a statistical ma-chine translation (SMT) module.
Additionally,a third module, ?Rescore?, has been added to thesystem and it forms a key component in the sys-tem.
Features derived from ASR and SMT arecombined in this module to rescore translationcandidates.Without loss of its generality, in this paperwe use Japanese-to-English translation to ex-plain the generic speech translation process.
LetX denote acoustic observations of a JapaneseXutterancerecognizedtexttargettranslationEbesttranslationJ N1ASR SMTENxK1RescoreFigure 1: Current framework of speech transla-tionutterance, typically a sequence of short-timespectral vectors received at a frame rate of ev-ery centi-second.
It is first recognized as aJapanese sentence, J .
The recognized sentenceis then translated into a corresponding Englishsentence, E.The conversion from X to J is performed inthe ASR module.
Based on Bayes?
rule, P (J |X)can be written asP (J |X) = Pam(X|J)Plm(J)/P (X)where Pam(X|J) is the acoustic model likeli-hood of the observations given the recognizedsentence J ; Plm(J), the source language modelprobability; and P (X), the probability of allacoustic observations.In the experiment we generated a set of N -best hypotheses, JN1 = {J1, J2, ?
?
?
, JN} 1 andeach Ji is determined byJi = arg maxJ?
?iPam(X|J)Plm(J)where ?i is the set of all possible source sen-tences excluding all higher ranked Jk?s, 1 ?
k ?i ?
1.The conversion from J to E in Fig.
1 isthe machine translation process.
Accordingto the statistical machine translation formal-ism (Brown et al, 1993), the translation processis to search for the best sentence E?
such thatE?
= arg maxEP (E|J) = arg maxEP (J |E)P (E)where P (J |E) is a translation model charac-terizing the correspondence between E and J ;P (E), the English language model probability.In the IBM model 4, the translation modelP (J |E) is further decomposed into four sub-models:?
Lexicon Model ?
t(j|e): probability of aword j in the Japanese language beingtranslated into a word e in the English lan-guage.1Hereafter, J1 is called the single-best hypothesis ofspeech recognition; JN1 , the N -best hypotheses.?
Fertility model ?
n(?|e): probability ofa English language word e generating ?words.?
Distortion model ?
d: probability of distor-tion, which is decomposed into the distor-tion probabilities of head words and non-head words.?
NULL translation model ?
p1: a fixed prob-ability of inserting a NULL word after de-termining each English word.In the above we listed seven features: twofrom ASR (Pam(X|J), Plm(J)) and five fromSMT (P (E), t(j|e), n(?|e), d, p1).The third module in Fig.
1 is to rescore trans-lation hypotheses from SMT by using a feature-based log-linear model.
All translation can-didates output through the speech recognitionand translation modules are re-evaluated by us-ing all relevant features and searching for thebest translation candidate of the highest score.The log-linear model used in our speech trans-lation process, P (E|X), isP?
(E|X) =exp(?Mi=1 ?ifi(X,E))?E?
exp(?Mi=1 ?ifi(X,E?))?
= {?M1 }(1)In the Eq.
1, fi(X,E) is the logarithm valueof the i-th feature; ?i is the weight of the i-th feature.
Integrating different features in theequation results in different models.
In the ex-periments performed in section 4, four differentmodels will be trained by increasing the numberof features successively to investigate the effectof different features for improving speech trans-lation.In addition to the above seven features, thefollowing features are also incorporated.?
Part-of-speech language models: Englishpart-of-speech language models were used.POS dependence of a translated Englishsentence is an effective constraint in prun-ing English sentence candidates.
In our ex-periments 81 part-of-speech tags and a 5-gram POS language model were used.?
Length model P (l|E, J): l is the length(number of words) of a translated Englishsentence.?
Jump weight: Jump width for adjacentcepts in Model 4 (Marcu and Wong, 2002).?
Example matching score: The translatedEnglish sentence is matched with phrasetranslation examples.
A score is derivedbased on the count of matches (Watanabeand Sumita, 2003).?
Dynamic example matching score: Similarto the example matching score but phraseswere extracted dynamically from sentenceexamples (Watanabe and Sumita, 2003).Altogether, we used M(=12) different fea-tures.
In section 3, we review Powell?s algo-rithm (Press et al, 2000) as our tool to opti-mize model parameters, ?M1 , based on differentobjective translation metrics.3 Parameter Optimization Basedon Translation MetricsThe denominator in Eq.
1 can be ignored sincethe normalization is applied equally to every hy-pothesis.
Hence, the choice of the best transla-tion, E?, out of all possible translations, E, isindependent of the denominator,E?
= arg maxEM?i=1?ilogPi(X,E) (2)where we write features, fi(X,E), explicitly inlogarithm, logPi(X,E).The effectiveness of the model in Eq.
2 de-pends upon the parameter optimization of theparameter set ?M1 , with respect to some objec-tively measurable but subjectively relevant met-rics.Suppose we have L speech utterances andfor each utterance, we generate N best speechrecognition hypotheses.
For each recogni-tion hypothesis, K English language transla-tion hypotheses are generated.
For the l-thinput speech utterance, there are then Cl ={El1 , ?
?
?
, ElN?K} translations.
All L speech ut-terances generate L?N?K translations in to-tal.Our goal is to minimize the translation ?dis-tortion?
between the reference translations, R,and the translated sentences, E?
.
?M1 = optimize D(E?
,R) (3)where E?
= {E?1, ?
?
?
, E?L} is a set of translationsof all utterances.
The translation E?l of the l-th utterance is produced by the (Eq.
2), whereE ?
Cl.Let R = {R1, ?
?
?
, RL} be the set of transla-tion references for all utterances.
Human trans-lators paraphrased 16 reference sentences foreach utterance, i.e., Rl contains 16 referencecandidates for the l-th utterance.D(E?
,R) is a translation ?distortion?
or anobjective translation assessment.
The followingfour metrics were used specifically in this study:?
BLEU (Papineni et al, 2002): A weightedgeometric mean of the n-gram matches be-tween test and reference sentences multi-plied by a brevity penalty that penalizesshort translation sentences.?
NIST : An arithmetic mean of the n-grammatches between test and reference sen-tences multiplied by a length factor whichagain penalizes short translation sentences.?
mWER (Niessen et al, 2000): Multiple ref-erence word error rate, which computes theedit distance (minimum number of inser-tions, deletions, and substitutions) betweentest and reference sentences.?
mPER: Multiple reference position inde-pendent word error rate, which computesthe edit distance without considering theword order.The BLEU score and NIST score are calcu-lated by the tool downloadable 2.Because the objective function in the model(Eq.
3) is not smoothed function, we used Pow-ell?s search method to find a solution.
The Pow-ell?s algorithm used in this work is similar as theone from (Press et al, 2000) but we modified theline optimization codes, a subroutine of Powell?salgorithm, with reference to (Och, 2003).Finding a global optimum is usually difficultin a high dimensional vector space.
To makesure that we had found a good local optimum,we restarted the algorithm by using various ini-tializations and used the best local optimum asthe final solution.4 Experiments4.1 Corpus & SystemThe data used in this study was the BasicTravel Expression Corpus (BTEC) (Kikui et al,2003), consisting of commonly used sentenceslisted in travel guidebooks and tour conversa-tions.
The corpus were designed for developingmultiple language speech-to-speech translationsystems.
It contains four different languages:Chinese, Japanese, Korean and English.
OnlyJapanese-English parallel data was used in this2http://www.nist.gov/speech/tests/mt/Table 1: Training, development and test datafrom Basic Travel Expression Corpus(BTEC)Japanese EnglishTrain Sentences 162,318Words 1,288,767 949,377Dev.
Sentences 510Words 4015 2983Test Sentences 508Words 4112 2951study.
The speech data was recorded by multi-ple speakers and was used to train the acousticmodels, while the text database was used fortraining the language and translation models.The standard BTEC training corpus, the firstfile and the second file from BTEC standard testcorpus #01 were used for training, developmentand test respectively.
The statistics of corpus isshown in table 1.The speech recognition engine used in the ex-periments was an HMM-based, large vocabu-lary continuous speech recognizer.
The acousticHMMs were triphone models with 2,100 statesin total, using 25 dimensional, short-time spec-trum features.
In the first and second pass ofdecoding, a multiclass word bigram of a lexiconof 37,000 words plus 10,000 compound wordswas used.
A word trigram was used in rescor-ing the results.The machine translation system is a graph-based decoder (Ueffing et al, 2002).
The firstpass of the decoder generates a word-graph, acompact representation of alternative transla-tion candidates, using a beam search based onthe scores of the lexicon and language mod-els.
In the second pass an A* search traversesthe graph.
The edges of the word-graph, orthe phrase translation candidates, are gener-ated by the list of word translations obtainedfrom the inverted lexicon model.
The phrasetranslations extracted from the Viterbi align-ments of the training corpus also constitute theedges.
Similarly, the edges are also created fromdynamically extracted phrase translations fromthe bilingual sentences (Watanabe and Sumita,2003).
The decoder used the IBM Model 4with a trigram language model and a 5-grampart-of-speech language model.
The training ofIBM model 4 was implemented by the GIZA++package (Och and Ney, 2003).4.2 Model TrainingIn order to quantify translation improvement byfeatures from speech recognition and machinetranslation respectively, we built four log-linearmodels by adding features successively.
Thefour models are:?
Standard translation model(stm): Onlyfeatures from the IBM model 4 (M=5) de-scribed in section 2 were used in the log-linear models.
We did not perform parame-ter optimization on this model.
It is equiv-alent to setting all the ?M1 to 1.
This modelwas the standard model used in most sta-tistical machine translation system.
It isreferred to as the baseline model.?
Optimized standard translation models(ostm): This model consists of the samefeatures as the previous model ?stm?
butthe parameters were optimized by Powell?salgorithm.
We intended to exhibit the ef-fect of parameter optimization by compar-ing this model with the baseline ?stm?.?
Optimized enhanced translation models(oetm): We incorporated additional trans-lation features described in section 2 toenrich the model ?ostm?.
In this modelthe number of the total features, M , is 10.Model parameters were optimized.
We in-tended to show how much the enhancedfeatures can improve translation quality.?
Optimized enhanced speech translationmodels (oestm): Features from speechrecognition, likelihood scores of acousticand language models, were incorporatedadditionally into the model ?oetm?.
Allthe 12 features described in section 2 wereused.
Model parameters were optimized.To optimize ?
parameters of the log-linearmodels, we used the development data of 510speech utterances.
We adopted an N -besthypothesis approach (Och, 2003) to train ?.For each input speech utterance, N?K candi-date translations were generated, where N isthe number of generated recognition hypothe-ses and K is the number of translation hypothe-ses.
A vector of dimension M , corresponding tomultiple features used in the translation model,was generated for each translation candidate.The Powell?s algorithm was used to optimizethese parameters.
We used a large K to ensurethat promising translation candidates were notTable 2: Comparisons of single-best and N -besthypotheses of speech recognition performancein terms of word accuracy, sentence accuracy,insertion, deletion and substitution error ratesword sent ins del subacc(%) acc(%) (%) (%) (%)single-best 93.5 78.7 2.0 0.8 3.6N -best 96.1 87.0 1.2 0.3 2.2pruned out.
In the training, we set N=100 andK=1, 000.By using different objective translation eval-uation metrics described in section 3, for eachmodel we obtained four sets of optimized pa-rameters with respect to BLEU, NIST, mWERand mPER metrics, respectively.4.3 Translation Improvement byAdditional FeaturesAll 508 utterances in the test data were used toevaluate the models.
Similar to processing thedevelopment data, the speech recognizer gen-erated N -best (N=100) recognition hypothe-ses for each test speech utterance.
Table 2shows speech recognition results of the test dataset in single-best and N -best hypotheses.
Weobserved that over 8% sentence accuracy im-provement was obtained from the single-best tothe N -best recognition hypotheses.
The recog-nized sentences were then translated into corre-sponding English sentences.
1,000 such trans-lation candidates were produced for each recog-nition hypothesis.
These candidates were thenrescored by each of the four models with foursets of optimized parameters obtained in thetraining respectively.
The candidates with thebest score were chosen.The best translations generated by a modelwere evaluated by the translation assessmentmetrics used to optimize the model parametersin the development.
The experimental resultsare shown in Table 3.In the experiments we changed the numberof speech recognition hypotheses, N , to see howtranslation performance is changed as N .
Wefound that the best translation was achievedwhen a relatively smaller set of hypotheses,N=5, was used.
Hence, the values in Table 3were obtained when N was set to 5.We test each model by employing the single-best recognition hypothesis translations andthe N -best recognition hypothesis translations.Table 3: Translation improvement from thebaseline model(stm) to the optimized enhancedspeech translation model(oestm): Models areoptimized using the same metric as shown inthe columns.
Numbers are in percentage exceptNIST score.BLEU NIST mWER mPERSingle-best recognition hypothesis translationstm 54.2 7.5 39.8 34.8ostm 59.0 8.9 36.2 34.0oetm 59.2 9.9 34.3 31.5N -best recognition hypothesis translationstm 55.5 7.3 39.8 35.4ostm 61.1 8.8 36.4 33.9oetm 61.1 10.0 34.0 31.1oestm 62.1 10.2 33.7 29.4The single-best translation was from the trans-lation of the single best hypotheses of the speechrecognition and the N -best hypothesis trans-lation was from the translations of all the hy-potheses produced by speech recognition.In Table 3, we observe that a large improve-ment is achieved from the baseline model ?stm?to the final model ?oestm?.
The BLEU, NIST,mWER, mPER scores are improved by 7.9%,2.7, 6.1%, 5.4% respectively.
Note that a highvalue of BLEU and NIST score means a goodtranslation while a worse translation for mWERand mPER.
Consistent performance improve-ment was achieved in the single-best and N -best recognition hypotheses translations.
Weobserved that the improvement were due to thefollowing reasons:?
Optimization.
Models with optimized pa-rameters yielded a better translation thanthe models with unoptimized parameters.It can be seen by comparing the model?stm?
with the model ?ostm?
for both thesingle-best and the N -best results.?
N -best recognition hypotheses.
In major-ity of the cells in Table 3, translation per-formance of the N -best recognition is bet-ter than of the corresponding single-bestrecognition.
N -best BLEU score of ?ostm?improved over the single-best of ?ostm?
by2.1%.
However, NIST score is indifferentto the change.
It appears that NIST scoreis insensitive to detect slight translationchanges.Table 4: Translation improvement of incorrectlyrecognized utterances from single-best(oetm) toN -best(oestm)BLEU NIST mWER mPERsingle-best 29.0 6.1 59.7 51.8N -best 36.3 7.2 54.4 47.9?
Enhanced features.
Translation perfor-mance is improved steadily when more fea-tures are incorporated into the log-linearmodels.
Translation performance of model?oetm?
is better than model ?ostm?
be-cause more effective translation featuresare used.
Model ?oestm?
is better thanmodel ?oetm?
due to its enhanced speechrecognition features.
It confirms that ourapproach to integrate features from speechrecognition and translation features worksvery well.4.4 Recognition Improvement ofIncorrectly Recognized SentencesIn previous experiments we demonstrated thatspeech translation performance was improvedby the proposed enhanced speech translationmodel ?oestm?.
In this section we want to showthat this improvement is because of the signifi-cant improvement of incorrectly recognized sen-tences when N -best recognition hypotheses areused.We carried out the following experiments.Only incorrectly recognized sentences were ex-tracted for translation and re-scored by themodel ?oetm?
for the single-best case and themodel ?oestm?
for the N -best case.
The trans-lation results are shown in Table 4.
Translationof incorrectly recognized sentences are improvedsignificantly as shown in the table.Because we used N -best recognition hypothe-ses, the log-linear model chose the recogni-tion hypothesis among the N hypotheses whichyielded the best translation.
As a result, speechrecognition could be improved if the higher ac-curate recognition hypotheses was chosen fortranslation.
This effect can be observed clearlyif we extracted the chosen recognition hypothe-ses of incorrectly recognized sentences.
Table 5shows the word accuracy and sentence accuracyof the recognition hypotheses selected by thetranslation module.
The sentence accuracy ofincorrectly recognized sentences was improvedby 7.5%.
The word accuracy was also improved.Table 5: Recognition accuracy of incorrectlyrecognized utterance improved by N -best hy-pothesis translation.word acc.
(%) sent.
acc.
(%)single-best 74.6 0N -best BLEU 76.4 7.5mWER 75.9 6.55 DiscussionsAs regards to integrating speech recognitionwith translation, a coupling structure (Ney,1999) was proposed as a speech translation in-frastructure that multiplies acoustic probabili-ties with translation probabilities in a one-stepdecoding procedure.
But no experimental re-sults have been given on whether and how thiscoupling structure improved speech translation.
(Casacuberta et al, 2002) used a finite-statetransducer where scores from acoustic infor-mation sources and lexicon translation modelswere integrated together.
Word pairs of sourceand target languages were tied in the decodinggraph.
However, this method was only testedfor a pair of similar languages, i.e., Spanish toEnglish.
For translating between languages ofdifferent families where the syntactic structurescan be quite different, like Japanese and En-glish, rigid tying of word pair still remains to beshown its effectiveness for translation.Our approach is rather general, easy to imple-ment and flexible to expand.
In the experimentswe incorporated features from acoustic modelsand language models.
But this framework isflexible to include more effective features.
In-deed, the proposed speech translation paradigmof log-linear models have been shown effective inmany applications (Beyerlein, 1998) (Vergyri,2000) (Och, 2003).In order to use speech recognition features,the N -best speech recognition hypotheses wereneeded.
Using N -best could bear computingburden.
However, our experiments have showna smaller N seems to be adequate to achievemost of the translation improvement withoutsignificant increasing of computations.6 ConclusionIn this paper we presented our approach of in-corporating both speech recognition and ma-chine translation features into a log-linearspeech translation model to improve speechtranslation.Under this new approach, translation perfor-mance was significantly improved.
The perfor-mance improvement was confirmed by consis-tent experimental results and measured by us-ing various objective translation metrics.
Inparticular, BLEU score was improved by 7.9%absolute.We show that features derived from speechrecognition: likelihood of acoustic and languagemodels, helped improve speech translation.
TheN -best recognition hypotheses are better thanthe single-best ones when they are used in trans-lation.
We also show that N -best recogni-tion hypothesis translation can improve speechrecognition accuracy of incorrectly recognizedsentences.The success of the experiments owes to theuse of statistical machine translation and log-linear models so that various of effective fea-tures can be jointed and balanced to output theoptimal translation results.AcknowledgmentsWe would like to thank for assistance from Ei-ichiro Sumita, Yoshinori Sagisaka, Seiichi Ya-mamoto and the anonymous reviewers.The research reported here was supported inpart by a contract with the National Instituteof Information and Communications Technol-ogy of Japan entitled ?A study of speech dia-logue translation technology based on a largecorpus?.ReferencesPeter Beyerlein.
1998.
Discriminative modelcombination.
In Proc.of ICASSP?1998, vol-ume 1, pages 481?484.Peter F. Brown, Vincent J. Della Pietra,Stephen A. Della Pietra, and Robert L. Mer-cer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.Francisco Casacuberta, Enrique Vidal, andJuan M. Vilar.
2002.
Architectures forspeech-to-speech translation using finite-statemodels.
In Proc.
of speech-to-speech trans-lation workshop, pages 39?44, Philadelphia,PA, July.Genichiro Kikui, Eiichiro Sumita, ToshiyukiTakezawa, and Seiichi Yamamoto.
2003.
Cre-ating corpora for speech-to-speech transla-tion.
In Proc.of EUROSPEECH?2003, pages381?384, Geneva.Daniel Marcu and William Wong.
2002.
Aphrase-based, joint probability model for sta-tistical machine translation.
In Proc.
ofEMNLP-2002, Philadelphia, PA, July.Hermann Ney.
1999.
Speech translation: Cou-pling of recognition and translation.
In Proc.of ICASSP?1999, volume 1, pages 517?520,Phoenix, AR, March.Sonja Niessen, Franz J. Och, Gregor Leusch,and Hermann Ney.
2000.
An evaluation toolfor machine translation: Fast evaluation formachine translation research.
In Proc.of theLREC (2000), pages 39?45, Athens, Greece,May.Franz Josef Och and Hermann Ney.
2003.
Asystematic comparison of various statisticalalignment models.
Computational Linguis-tics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
InProc.
of ACL?2003, pages 160?167.Kishore A. Papineni, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
Bleu: Amethod for automatic evaluation of machinetranslation.
In Proc.
of ACL?2002, pages311?318, Philadelphia, PA, July.William H. Press, Saul A. Teukolsky,William T. Vetterling, and Brian P. Flan-nery.
2000.
Numerical Recipes in C++.Cambridge University Press, Cambridge,UK.Nicola Ueffing, Franz Josef Och, and HermannNey.
2002.
Generation of word graphs in sta-tistical machine translation.
In Proc.
of theConference on Empirical Methods for Natu-ral Language Processing (EMNLP02), pages156?163, Philadelphia, PA, July.Dimitra Vergyri.
2000.
Use of word level sideinformation to improve speech recognition.
InProc.
of the IEEE International Conferenceon Acoustics, Speech and Signal Processing,2000.Taro Watanabe and Eiichiro Sumita.
2003.Example-based decoding for statistical ma-chine translation.
In Machine TranslationSummit IX, pages 410?417, New Orleans,Louisiana.
