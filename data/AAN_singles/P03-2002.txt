An Ontology-based Semantic Tagger for IE systemNarje`s BoufadenDepartment of Computer ScienceUniversite?
de Montre?alQuebec, H3C 3J7 Canadaboufaden@iro.umontreal.caAbstractIn this paper, we present a method forthe semantic tagging of word chunks ex-tracted from a written transcription of con-versations.
This work is part of an ongo-ing project for an information extractionsystem in the field of maritime Search AndRescue (SAR).
Our purpose is to auto-matically annotate parts of texts with con-cepts from a SAR ontology.
Our approachcombines two knowledge sources a SARontology and the Wordsmyth dictionary-thesaurus, and it uses a similarity measurefor the classification.
Evaluation is carriedout by comparing the output of the systemwith key answers of predefined extractiontemplates.1 IntroductionThis work is a part of a project aiming to imple-ment an information extraction (IE) system in thefield of maritime Search And Rescue (SAR).
It wasoriginally conducted by the Defense Research Es-tablishment Valcartier (DREV) to develop a deci-sion support tool to help in producing SAR plansgiven the information extracted by the SAR IE sys-tem from a collection of transcribed dialogs.
Thegoal of our project is to develop a robust approachto extract relevant words for small-scale corpora andtranscribed speech dialogs.
To achieve this task, wedeveloped a semantic tagger which annotates wordswith domain-specific informations and a selectionprocess to extract or reject a word according to thesemantic tag and the context.
The rationale behindour approach, is that the relevance of a word dependsstrongly on how close it is to the SAR domain andits context of use.
We believe that reasoning on se-mantic tags instead of the word is a way of gettingaround some of the problems of small-scale corpora.In this paper, we focus on semantic taggingbased on a domain-specific ontology, a dictionary-thesaurus and the overlapping coefficient similaritymeasure (Manning and Schutze, 2001) to semanti-cally annotate words.We first describe the corpus (section 2), then theoverall IE system (section 3).
Next we explain thedifferent components of the semantic tagger (section4) and we present the preliminary results of our ex-periments (section 5).
Finally we give some direc-tions for future work (section 6).2 CorpusThe corpus is a collection of 95 manually tran-scribed telephone conversations (about 39,000words).
They are mostly informative dialogs, wheretwo speakers (a caller C and an operator O) dis-cuss the conditions and circumstances related toa SAR mission.
The conversations are either (1)incident reports, such as reporting missing per-sons or overdue boats, (2) SAR mission plans,such as requesting an SAR airplane or coast guardships for a mission, or (3) debriefings, in whichcase the results of the SAR mission are com-municated.
They can also be a combination ofthe three kinds.
Figure 1 is an excerpt of suchconversations.
We can notice many disfluencies1-O:Hi, it?s Mr. Joe Blue?
??
?.PERSON...3-O:We get???
?an overdue boat?
??
?, missing boat?
??
?on the South Coast of Newfoundland?
??
?...STATUS MISSING-VESSEL MISSING-VESSEL LOCATION-TYPE4-O:They did a radar search?
??
?for us in the area?
??
?.DETECTION-MEANS LOCATION5-C:Hum, hum.8-O:And I am wondering?
??
?about the possibility?
??
?of outputting?
??
?an Aurora?
??
?in there for radar search?
??
?.STATUS-REQUEST STATUS-REQUEST TASK SAR-AIRCRAFT-TYPE DETECTION-MEANS...11-O:They got???
?a South East?
??
?to be flowing?
??
?there and it?s just gonna?
??
?be black thicker fog?
??
?the whole, whole South Coast?
??
?.STATUS DIRECTION-TYPE STATUS STATUS WEATHER-TYPE LOCATION-TYPE12-C:OK....56-:Ha, they should go?
??
?to get going?
??
?at first light?
??
?.STATUS STATUS TIMEFigure 1: An Excerpt of a conversation reporting an overdue vessel:the incident, a request for an SARairplane (Aurora) and the use of another SAR airplane (king Air).
The words in bold are candidates for theextraction.
The tag below each bold chunk is a domain-specific information automatically generated by thesemantic tagger.
Chunks like possibility, go, flowing and first light are annotated by using sense taggingoutputs.
Whereas chunk such as Mr. Joe Blue, the South coast of Newfoundland and Aurora are annotatedby the named concept extraction process.
(Shriberg, 1994) such as repetitions (13-O: Ha,do, is there, is there ...) , omissionsand interruptions (3-O: we?ve been, actu-ally had a ...).
And, there is about 3% oftranscription errors such as flowing instead ofblowing (11-O Figure 1).The underlined words are the relevant informa-tions that will be extracted to fill in the IE tem-plates.
They are, for example, the incident, its lo-cation, SAR resources needed for the mission, theresult of the SAR mission and weather conditions.3 Overall systemThe information extraction system is a four stageprocess (Figure 2).
It begins with the extractionof words that could be candidates to the extraction(stage I).
Then, the semantic tagger annotates theextracted words (stage II).
Next, given the contextand the semantic tag a word is extracted or rejected(stage III).
Finally, the extracted words are usedfor the coreference resolution and to fill in IE tem-plates (stage IV).
The knowledge sources used forthe IE task are the SAR ontology and the Wordsmythdictionary-thesaurus1.In this section we describe the extraction of can-didates, the SAR ontology design and the topic seg-mentation which have already been implemented.We leave the description of the topic labeling, theselection of relevant words and the template genera-tion to future work.
The semantic tagger, is detailedin section 4.3.1 Extraction of candidatesCandidates considered in the semantic tagging pro-cess are noun phrases NP, proposition phrases PP,verb phrases VP, adjectives ADJ and adverbs ADV.To gather these candidates we used the Brill trans-formational tagger (Brill, 1992) for the part-of-speech step and the CASS partial parser for the pars-ing step (Abney, 1994).
However, because of thedisfluencies (repairs, substitutions and omissions)encountered in the conversations, many errors oc-curred when parsing large constructions.
So, we re-duced the set of grammatical rules used by CASS tocover only minimal chunks and discard large con-structions such as VP ?
VX NP?
ADV* or noun1URL http://www.wordsmyth.net/.TranscribedConversationStage I .
.
.
.
.
.
.
.
.
.
.
.
.
.
.Extractionof candidatesStage II:Semantic Tagging .
.
.
.
.
.
.
.
.
.
.
.
.
.
.NamedConceptsExtractionSAROntologyooxxpppppppppSense TaggingWordsmythDictionaryThesaurusooStage III:Selecting relevantcandidates.
.
.
.
.
.
.
.
.
.
.
.
.
.
._ _ _ __ _ _ _TopicLabelingwwpppppp_ _ _ __ _ _ _Selectionof relevantwordsTopicSegmentationOO{{wwwwwwwwwStage IV .
.
.
.
.
.
.
.
.
.
.
.
.
.
._ _ _ _ __ _ _ _ _IE TemplatesgenerationFigure 2: Main stages of the full SAR informationextraction system.
Dashed squares represent pro-cesses which are not developed in this paper.phrases NP ?
NP CONJ NP.
The evaluation of thesemantic tagging process shows that about 14.4% ofthe semantic annotation errors are partially due topart-of-speech and parsing errors.3.2 Topic segmentationTopic segmentation takes part to several stages inour IE system (Figure 2).
Dialogue-based IE sys-tems have to deal with scattered information anddisfluencies.
Question-answer pairs, widely used indialogues, are examples where information is con-veyed through consecutive utterances.
By divid-ing the dialog into topical segment, we want to en-sure the extraction of coherent and complete key an-swers.
Besides, topic segmentation is a valuable pre-processing for coreference resolution, which is a dif-ficult task in IE.
Hence, for the extraction of relevantcandidates and the coreference resolution which ispart of the template generation stage (Figure 2), weuse topic segment as context instead of the utteranceor a word window of arbitrary size.The topic segmentation system we developed isbased on a multi-knowledge source modeled by ahidden Markov model.
(N. Boufaden and al., 2001)showed that by using linguistic features modeled bya Hidden Markov Model, it is possible to detectabout 67% of topics boundaries.3.3 The SAR ontologyThe SAR ontology is an important component of ourIE system.
We build it using domain related infor-mations such as airplane names, locations, organi-zations, detection means (radar search, div-ing), status of a SAR mission (completed, con-tinuing, planned), instance of maritime inci-dents (drifting, overdue) and weather condi-tions (wind, rain, fog).
All these informationswere gathered from SAR manuals provided by theNational Search and Rescue Secretariat (SARMan-ual, 2000) and from a sample of conversations (10conversations about 10% of the corpus) to enumer-ate the different status informations.Our ontology was designed for two tasks of thesemantic tagging:1.
Annotate with the corresponding concept allthe extracted words that are instances of the on-tology.
This task is achieved by the named con-cept extraction process (section 4.1).2.
For each word not in the ontology, generatea concept-based representation composed ofsimilarity scores that provide information aboutthe closeness of the word to the SAR domain.This is achieved by the sense tagging process(section 4.2).In addition to SAR manuals and corpus, we usedthe IE templates given by the DREV for the de-sign of the ontology.
We used a combination of thetop-down and bottom-up design approaches (Frid-man and Hafner, 1997).
For the former, we usedthe templates to enumerate the questions to be cov-ered by the ontology and distinguish the major toplevel classes (Figure 4).
For the latter, we collectedthe named entities along with airplane names, ves-sel types, detection means, alert types and incidents.The taxonomy is based on two hierarchical relations:the is-a relation and the part-of relation.
The is-a re-lation is used for the semantic tagging.
Whereas, theENT: wonderSYL: won-derPRO: wuhn dErPOS: intransitive verbINF: wondered, wondering, wondersDEF: 1. to experience a sensation of admiration or amazement (often fol.
by at):EXA: She wondered at his bravery in combat.SYN: marvelSIM: gape, stare, gawkDEF: 2. to be curious or skeptical about something:EXA: I wonder about his truthfulness.SYN: speculate (1)SIM: deliberate, ponder, think, reflect, puzzle, conjecture...Figure 3: A fragment of the Wordsmyth dictionary-thesaurus entry of the verb wonder which is a verbdescribing a STATUS-REQUEST concept (8-O Figure 1).
The ENT, SYL, PRO, POS, INF, DEF, EXA, SYN,SIM acronyms are respectively the entry, the syllable, the pronunciation, the part-of-speech, inflexion form,textual definition, example, synonim words and similar words fields.
To build the SAR ontology we usedthe information given in the fields DEF, SYN and SIM.
Whereas, to compute the similarity scores we usedonly the information of the DEF field.part-of relation will be used in the template genera-tion process.The overall ontology is composed of 31 concepts.In the is-a hierarchy, each concept is represented bya set of instances and their textual definitions.
Foreach instance we added a set of synonyms and simi-lar words and their textual definitions to increase thesize of the SAR vocabulary which was found to beinsufficient to make the sense tagging approach ef-fective.All the synonyms and similar words along withtheir definitions are provided by the Wordsmythdictionary-thesaurus.
Figure 3 is an example ofWordsmyth entries.
Only textual definitions thatfit the SAR context were kept.
This procedure in-creases the ontology size from 480 for a total of 783instances.Location Aircraft Vessel .
.
.
Detectionmeans AAHHHHXXXXXXXPhysicalEntityEvent .
.
.
SearchMission ccConceptualEntity!!!
!````````TFigure 4: Fragment of the is-a hierarchy.
Location,Aircraft .
.
.
are concepts of the ontology4 Semantic taggingThe purpose of the semantic tagging process is to an-notate words with domain-specific informations.
Inour case, domain-specific informations are the con-cepts of the SAR ontology.
We want to determinethe concept Ck which is semantically the most ap-propriate to annotate a word w. Hence, we lookfor C?
which has the highest similarity score for theword w as shown in equation 1.C?
= argmaxCksim(w,Ck) (1)Basically, our approach is a two part process (fig-ure 2).
The named concept extraction is similar tonamed entity extraction based on gazetteer (MUC,1991).
However it is a more general task since italso recognizes entities such as, aircraft names, boatnames and detection means.
It uses a finite stateautomaton and the SAR ontology to recognize thenamed concepts.The sense tagging process generates a based-concept representation for each word which couldn?tbe tagged by the named concept extraction process.The concept-based representation is a vector of sim-ilarity scores that measures how close is a word tothe SAR domain.
As we mentioned before (section1), the concept-based representation using similarityscores is a way to get around the problem of small-scale corpora.
Because we assume that the closer aword is to an SAR concept, the more relevant it is,this process is a key element for the selection of rel-evant words (figure 2).
In the next two sections, wedetail each component of the semantic tagger.4.1 Named concept extractionThis task, like the named entity extraction task, an-notates words that are not instances of the ontol-ogy.
Basically, for every chunk, we look for the firstmatch with an instance concept.
The match is basedon the word and its part-of-speech.
When a matchsucceeds, the semantic tag assigned is the conceptof the instance matched.
The propagation of the se-mantic tag is done by a two level automaton.
Thefirst level propagates the semantic tag of the headto the whole chunk.
The second level deals withcases where the first level automaton fails to recog-nize collocations which are instances of the ontol-ogy.These cases occur when :?
the syntactic parser fails to produce a correctparse.
This mainly happens when the part ofspeech tag isn?t correct because of disfluenciesencountered in the utterance or because of tran-scription errors.?
the grammatical coverage is insufficient toparse large constructions.Whenever one of these reasons occur, the secondlevel automaton tries to match chunk collocations in-stead of individual chunks.
For example, the chunkRescue Coordination Centre which is anorganization, is an example where the parser pro-duces two NP chunks (NP1:Rescue Coordina-tion and NP2:Centre) instead of only one chunk.In this case, the first level automaton fails to recog-nize the organization.
However, in the second levelautomaton, the collocation NP1 NP2 is consideredfor matching with an instance of the concept organi-zation.
Figure 5 shows two output examples of thenamed concept extraction.Finally, if the automaton fails to tag a chunk,it assigns the tag OTHER if it?s an NP, OTHER-PROPERTIES if it?s a ADJ or ADV and OTHER-STATUS if it?s a VP.4.2 Sense taggingSense tagging takes place when a chunk is not aninstance of the ontology.
In this case, the semantictagger looks for the most appropriate concept to an-notate the chunk (equation 1).
However, a first stepbefore annotation is to determine what word senseis intended in conversations.
Many studies (Resnik,1999; Lesk, 1986; Stevenson, 2002) tackle the sensetagging problem with approaches based on similar-ity measures.
Sense tagging is concerned with theselection of the right word sense over all the pos-sible word senses given some context or a particu-lar domain.
Our assumption is that when conversa-tions are domain-specific, relevant words are too.
Itmeans that sense tagging comes back to the prob-lem of selecting the closer word sense with regard tothe SAR ontology.
This assumption is translated inequation 2.w?
= argmaxw(l)1Nl?all concepts ksim(w(l), k)(2)Where Nl is the number of positive similarityscores of the w(l) similarity vector.
w(l) is the wordw given the word sense l. The closer word sense w?is the highest mean computed from element of thew(l) similarity vector.In what follows, we explain how are generated thesimilarity vectors and the result of our experiments.4.3 Similarity vector representationA similarity vector is a vector where each elementis a similarity score between a word(l) (the word wgiven the sense word l) and a concept Ck from theSAR ontology.
The similarity score is based on theoverlap coefficient similarity measure (Manning andSchutze, 2001).
This measure counts the number oflemmatized content words in common between thetextual definition of the word and the concept.
It isdefined as :sim(w(l), Ck) =| Dw(l) | ?
| DCk |min(| Dw(l) |, | DCk |)(3)where Dw(l) and DCk are the sets of lemmatizedcontent words extracted from the textual definitions3-O:an overdue boatVESSEL:[dt,an],[OTHER-PROPERTIES,overdue],[VESSEL,boat]11-O:black thicker fogWEATHER-TYPE:[COLOR-TYPE,black],[OTHER-PROPERTIES,thicker],[WEATHER-TYPE,fog]Figure 5: Output of the named concept extraction process.
For both chunks the head semantic tag is propa-gated to the whole chunkfor each concept Ck of the SAR ontology; Ck ?
{incident,detection-means,status.
.
.
}for each instance Ij of Ck; Ij ?
{broken,missing,overdue.
.
. }
for the concept incidentfor each synonym Si of Ij ; Si ?
{smach,crack.
.
. }
for the instance brokensim(w(l), Si)=|Dw(l)|?|DSi |min(|Dw(l)|,|DSi |)end~vjdef= (sim(w(l), S1), .
.
.
, sim(w(l), SNj ))sim(w(l), Ij)=mediane(~vj)end~vkdef= (sim(w(l), I1), .
.
.
, sim(w(l), IMk))sim(w(l), Ck)=max( ~vk)end~vw(l) def= (sim(w(l), C1), .
.
.
, sim(w(l), CM ))Figure 6: Similarity measure algorithm.
Nj is the number of synonyms for the instance Ij , Mk the numberof the instance for the concept Ck and M the number of concepts in the ontology.of w(l) and Ck.
The textual definitions are providedby the Wordsmyth thesaurus-dictionary.However, since we have represented each conceptby a set of instances and their synonyms in the SARontology (section 3.3), we modified the similaritymeasure to take into account the textual definitionof concept instances and their synonyms.
Basically,we compute the similarity score between w(l) andeach synonym Si of a concept instance Ij .
Then,the similarity score between w(l) and the instanceconcept Ij is the median of the resulting similarityvector representing the similarity scores over all thesynonyms.
Finally, the similarity score between aconcept Ck and w(l) is the highest similarity scoreover all the concept instances.
The algorithm de-scribing these steps is given in Figure 6.5 Preliminary results and discussionThe evaluation of the semantic tagging process wasdone on 521 extracted chunks (about 10 conversa-tions).
Only relevant chunks where considered forChunk Mean sim Nearest conceptsget 0.5 0.5 - statussuitable 0.53 0.53 - statuspossibility 0.14 0.29-status;0.25-personfirst light 0.25 0.25 - timeTable 1: Output samples from the semantic tagger.Mean sim is the mean of the similarity scores.
It isthe selection criteria used to choose the closest wordsense.the evaluation.
The evaluation criteria is an assess-ment about the appropriateness of the selected con-cept to annotate the word.
For example, the concepttime is appropriate for the word first light, whereasthe concept incident is not for the word detachmentwhich is closer to the search unit concept.Table 2 shows the recall and precision scores foreach component and for the overall semantic tagger.The third column shows the input error rates for eachcomponent.
The error rate in the first row comprisesProcess Recall Precis.
Inp.ErrNamed conceptextraction 85.3% 94.8% 7.3%Semantic tagger usingsense tagging output 93.5% 72.6% 11.3%Average performanceof the semantic tagger 89.4% 83.7% 8.3%Table 2: Precision and Recall scores for each com-ponents of the semantic taggererror rates of the part-of-speech tagger, the parsingand the manual transcription.
The error rate in thesecond row are mostly part-of-speech errors.
In spiteof the significant error rate, the approach based onpartial parsing is effective.
The use of a minimalgrammar coverage to produce chunks reduced con-siderably the parsing error rate.As far as we know, no previous published workon domain-specific WSD for speech transcriptionshas been presented, although, word sense disam-biguation is an active research field as demonstratedby SENSEVAL competitions2.
Hence it is diffi-cult to compare our results to similar experiments.However, some comparative studies (Maynard andAnaniadou, 1998; Li Shiuan and Hwee Tou, 1997)on domain-specific well-written texts show resultsranging from 51,25% to 73,90%.
Given the factthat our corpus is composed of speech transcriptionswith the effect of increasing parsing errors, we con-sider our results to be very encouraging.Finally, results reported in Table 2 should be re-garded as a basis for further improvement.
In partic-ular, the selection criteria in the sense tagging pro-cess could be improved by considering other mea-sures than the mean of all similarity scores as shownin equation 2.6 Future workExtraction of relevant words is a hub for several ap-plications such as question-answering and summa-rization.
It is based on semantically tagging wordsand selecting the most relevant ones given the con-text.
In this paper, we developed a semantic tag-ging approach that uses a domain-specific ontology,a dictionary-thesaurus and the overlapping coeffi-2URL:http://www.senseval.org/.cient similarity measure to annotate words.
We haveshown how the use of concepts to represent wordscan alleviate the problem of small-scale corpora forthe selection of relevant words.The next step in our project is the selection of rel-evant words given the concepts annotating them andthe topic segments where they appear.
Selection willbe based on a combination of a probabilistic modeltaking into account the probability of observing aconcept given a word and the probability of observ-ing that concept given a relevant topic.AcknowledgmentsWe are grateful to Robert Parks at Wordsmyth orga-nization for giving us the electronic Wordsmyth ver-sion.
Thanks to the Defense Research EstablishmentValcartier for providing us with the dialog transcrip-tions and to National Search and rescue Secretariatfor the valuable SAR manuals.ReferencesS.
Abney.
1994.
Partial parsing.
Tutorial given at ANLP.N.
Boufaden G. Lapalme and Y. Bengio.
2001.
Topicsegmentation : A first stage to dialog-based informa-tion extraction.
In Natural Language Processing RimSymposium, NLPRS?01, pages 273?280.E.
Brill.
1992.
A simple rule-based part-of-speech tag-ger.
In Proceedings of the Third Conference on Ap-plied Natural Language Processing, Trento, Italy.Manual.
Fisheries and Oceans Canada, Canadian CoastGuard, Search and Rescue, 2000.
SAR SeamanshipReference Manual, Canadian Government Publishing,Public Works and Government Services Canada edi-tion, November.
ISBN 0-660-18352-8.N.
Fridman and C.D.
Hafner.
1997.
State of the art inontology design.
AI Magazine, 18(3):53?74.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: how to tell a pine conefrom an ice cream cone.
In Proceedings of ACM SIG-DOC Conference, pages 24?26, Toronto, Canada.C.
D. Manning and H. Schutze, 2001.
Foundationsof Statistical Natural Language Processing, chapterWord Sense Disambiguation, pages 294?303.
TheMIT Press Cambridge, Massachusetts London Eng-land.MUC,1991.
Proceedings of the Third Message Under-standing Conference.
Morgan Kaufman.D.
Maynard and S. Ananiadou, 1998.
1998.
TermSense Disambiguation using a Domain-Specific The-saurus.
In Proceedings of 1st International Confer-ence on Language Resource and Evaluation (LREC),Granada, Spain.P.
Resnik, 1999.
Natural Language Processing usingVery Large Corpora, chapter Disambiguating NounGroupings with Respect to WordNet senses, pages 77?98.
S. Amstrong, K. Church, P. Isabelle, S. Manzi,E, Tzoukermann and D. Yarowsky, kluwer AcademicPress edition.E.
Shriberg.
Preliminaries to a Theory of Speech Disflu-encies.
The`se de doctorat, University of California atBerkeley.P.
Li Shiuan and N. Hwee Tou 1997.
Domain-SpecificSemantic Class Disambiguation Using Wordnet.
InProceedings of the fifth Workshop on Very Large Cor-pora, pages 56?64, Beijing and Hong Kong.M.
Stevenson.
2002.
Combining Disambiguation Tech-niques to Enrich an Ontology.
In Proceedings ofthe Fifteen European Conference on Artificial Intel-ligence, workshop on Machine Learning and Natu-ral Language Processing for Ontology Engineering,Lyon,France.
