Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 346?349,Prague, June 2007. c?2007 Association for Computational LinguisticsUBC-AS: A Graph Based Unsupervised Systemfor Induction and ClassificationEneko Agirre and Aitor SoroaIXA NLP GroupUBCDonostia, Basque Contry{e.agirre,a.soroa}@si.ehu.esAbstractThis paper describes a graph-based unsu-pervised system for induction and clas-sification.
The system performs a twostage graph based clustering where a co-occurrence graph is first clustered to com-pute similarities against contexts.
The con-text similarity matrix is pruned and the re-sulting associated graph is clustered againby means of a random-walk type algorithm.The system relies on a set of parameters thathave been tuned to fit the corpus data.
Thesystem has participated in tasks 2 and 13of the SemEval-2007 competition, on wordsense induction and Web people search, re-spectively, with mixed results.1 IntroductionThis paper describes a graph-based unsupervisedsystem for induction and classification.
Given a setof data to be classified, the system first induces thepossible clusters and then clusters the data accord-ingly.
The paper is organized as follows.
Section 2gives an description of the general framework of oursystem.
Sections 3 and 4 presents in more detail theimplementation of the framework for the Semeval-2007 WEPS task (Artiles et al, 2007) and Semeval-2007 sense induction task (Agirre and Soroa, 2007),respectively.
Section 5 presents the results obtainedin both tasks, and Section 6 draws some conclusions.2 A graph based system for unsupervisedclassificationThe system performs a two stage graph based clus-tering where a co-occurrence graph is first clusteredto compute similarities against contexts.
The contextsimilarity matrix is pruned and the resulting associ-ated graph is clustered again by means of a random-walk type algorithm.
We will see both steps in turn.First step: calculating hub score vectorsIn a first step, and for each entity to be clustered, agraph consisting on context word co-occurrences isbuilt.
Vertices in the co-occurrence graph are wordsand two vertices share an edge whenever they co-occur in the same context.
Besides, each edge re-ceives a weight, which indicates how strong the in-cident vertices relate each other.As shown in (Ve?ronis, 2004), co-occurrencegraphs exhibit the so called small world structure(Watts and Strogatz, 1998) and, thus, they containhighly dense subgraphs which will represent the dif-ferent clusters the entity may have.
For identifyingthese clusters we have implemented two algorithmsbased on the notion of centrality of the vertices,where some highly dense vertices, called ?hubs?, arechosen as representatives of each cluster.
The algo-rithms are the HyperLex algorithm (Ve?ronis, 2004)and the HITS algorithm (Kleinberg, 1999).Once the hubs are identified, the minimum span-ning tree (MST) of the co-occurrence graph is com-puted.
The root elements of the MST are preciselythe induced hubs and each vertex of the originalgraph ?and, thus, each word of the corpus?
is at-tached to exactly one of these hubs, at a certain dis-tance.
Note that the MST can be considered as asingle link clustering over the co-occurrence graph.The original contexts are then taken one by oneand scored according to the MST in the followingway: each word in the context receives a set of scorevectors, with one score per hub, where all scores are3460 except for the one corresponding to the hub whereit is placed1, which will receive a socre d(hi, v),which is the distance between the hub hi and thenode representing the word v in the MST.
Thus,d(hi, v) assigns a score of 1 to hubs and the scoredecreases as the nodes move away from the hub inthe MST.
As a consequence, each context receives ahub score vector, which is just the sum of the scorevectors of all the words in the context.At this point we can use the hub score vectorsto create clusters of contexts, just assigning to eachcontext the hub with maximum score.
This processis thoroughly explained in (Agirre et al, 2006b).One of the problems of such an approach comesfrom the tendency of the system to produce a highnumber of hubs, somehow favouring small micro-clusters over coarse ones.
Knowing in advance thatthe number of clusters in the tasks we will partici-pate in would not be very high, we decided to per-form a second stage and re-cluster again the resultsobtained in the first step, using a different graph-based technique.
Re-clustering also gives us the op-portunity to feed the system with additional data, aswill be explained below.Second step: clustering via MCLIn this second stage, we compute a square ma-trix with as many rows/columns as contexts, andwhere each element represents the relatedness be-tween two contexts, just computing the cosine dis-tance of its (normalized) hub score vectors obtainedin the first step.
We prune each row in the matrixand keep only the element with maximum values, sothat the percentage of the kept elements?
sum respectthe total is below a given threshold.
The resultingmatrix M represents the adjacency matrix of a di-rected weighted graph, where vertices are contextsand edges represent the similarity between them.
Wecan feed the matrixM with external information justby calculating another dissimilarity matrix betweencontexts and lineally interpolating the matrices witha factor.Finally, we apply the Markov Clustering (MCL)algorithm (van Dongen, 2000) over the graph Mfor calculating the final clusters.
MCL is a graph-clustering algorithm based on simulation of stochas-1Note that each word will be attached to exactly one hub inthe MST.tic flows in graphs, its main idea being that randomwalks within the graph will tend to stay in the samecluster rather than jump between clusters.
MCL hasthe remarkable property that there is no need to a-priori decide how many clusters it must find.
How-ever, it has some parameters which will influence thegranularity of the clusters.In fact, the behavior of the whole process relieson a number of parameters, which can be divided inseveral groups:?
Parameters for calculating the hubs?
Parameters for merging the hubs informationwith external information in the matrix M (?)?
The threshold for pruning the graph (?)?
Parameters of the MCL algorithm (I , inflationparameter)In sections 3 and 4 we describe the parameterswe actually used for the final experiments, as wellas how the tuning of these parameters has been per-formed for the two tasks.3 Web People Search taskIn this section we will explain in more detail howwe implemented the general schema described inthe previous section to the ?Web People Search?task (Artiles et al, 2007).
The task consist on dis-ambiguating person names in a web searching sce-nario.
The input consists on web pages retrievedfrom a web searching engine using person names asa query.
The aim is to determine how many ref-erents (people with the same name) exist for thatperson name, and classify each document with itscorresponding referent.
There is a train set con-sisting on 49 names and 100 documents per name.The test setting consist on 30 unrelated names, with100 document per name.
The evaluation is per-formed following the ?purity?
and ?inverse purity?measures.
Roughly speaking, purity measures howmany classes they are in each cluster (like the pre-cision measure).
If a cluster fits into one class, thepurity equals to 1.
On the other side, inverse puritymeasures how many clusters they are in each class(recall).
The final figure is obtained by combiningpurity and inverse purity by means of the standardF-Measure with ?
= 0.5.The parameters of the system were tuned usingthe train part of the corpus as a development set.
Asusual, the parameters that yielded best results wereused on the test part.347We first apply a home-made wrapper over thehtml files for retrieving the text chunks of the pages,which is usually mixed with html tags, javascriptcode, etc.
The text is split into sentences and parsedusing the FreeLing parser (Atserias et al, 2006).Only the lemmas of nouns are retained.
We filter thenouns and keep only back those words whose fre-quency, according to the British National Corpus, isgreater than 4.
Next, we search for the person nameacross the sentences, and when such a sentence isfound we build a context consisting on its four pre-decessor and four successors, i.e., contexts consistson 9 sentences.
At the end, each document is rep-resented as a set of contexts containing the personname.
Finally, the person names are removed fromthe contexts.For inducing the hubs we apply the HyperLex al-gorithm (Ve?ronis, 2004).
Then, the MST is calcu-lated and every context is assigned with a hub scorevector.
We calculate the hub score vector of thewhole document by averaging the score vectors ofits contexts.
The M matrix of pairwise similaritiesbetween documents is then computed and prunedwith a threshold of 0.2, as described in section 2.We feed the system with additional data aboutthe topology of the pages over the web.
For eachdocument di to be classified we retrieve the set ofdocuments Pi which link to di.
We use the pub-licly available API for Microsoft Search.
Then, foreach pair of documents di and dj we calculate thenumber of overlapping documents linking to them,i.e., lij = #{Pi ?
Pj} with the intuition that, themore pages point to the two documents, the moreprobably is that they both refer to the same per-son.
The resulting matrix, ML is combined withthe original matrix M to give a final matrix M ?, bymeans of a linear interpolation with factor of 0.2, i.e.M ?
= 0.2M + 0.8ML.
Finally, the MCL algorithmis run over M ?
with an inflation parameter of 5.4 Word Sense Induction andDiscrimination taskThe goal of this task is to allow for comparisonacross sense-induction and discrimination systems,and also to compare these systems to other super-vised and knowledge-based systems.
The input con-sist on 100 target words (65 verbs and 35 nouns),each target word having a set of contexts where theword appears.
The goal is to automatically inducethe senses each word has, and cluster the contextsaccordingly.
Two evaluation measures are provided:and unsupervised evaluation (FScore measure) anda supervised evaluation, where the organizers auto-matically map the induced clusters onto senses.
See(Agirre and Soroa, 2007) for more details.In order to improve the overall performance, wehave clustered the 35 nouns and the 65 verbs sepa-rately.
In the case of nouns, we have filtered the orig-inal contexts and kept only noun lemmas, whereasfor verbs lemmas of nouns, verbs and adjectiveswere hold.The algorithm for inducing the hubs is also dif-ferent among nouns and verbs.
Nouns hubs are in-duced with the usual HyperLex algorithm (just likein section 3) but for identifying verb hubs we usedthe HITS algorithm (Kleinberg, 1999), based on pre-liminary experiments.The co-occurrence relatedness is also measureddifferently for verbs: instead of using the originalconditional probabilities, the ?2 measure betweenwords is used.
The reason behind is that condi-tional probabilities, as used in (Ve?ronis, 2004), per-form poorly in presence of words which occur innearly all contexts, giving them an extraordinaryhigh weight in the graph.
Very few nouns hap-pen to occur in many contexts, but they are verbswhich certainly do (be, use, etc).
On the otherhand, ?2 measures to what extent the observed co-occurrences diverge from those expected by chance,so weights of edges incident with very common,non-informant words will be low.Parameter tuning for both nouns and verbs wasperformed over the senseval-3 testbed, and the bestparameter combination were applied over the senseinduction corpus.
However, there is a factor we havetaken into account in tuning directly over the senseinduction corpus, i.e., that the granularity?and thusthe number of classes?
of senses in OntoNotes (theinventory used in the gold standard) is considerablycoarser than in senseval-3.
Therefore, we have man-ually tuned the inflation parameter of the MCL al-gorithm in order to achieve numbers of clusters be-tween 1 and 4.A threshold of 0.6was used when pruning the dis-similarity matrix M for both nouns and verbs.
Wehave tried to feed the system with additional data348System All Nouns VerbsBest 78.7 80.8 76.3Worst 56.1 62.3 45.1Average 65.4 69.0 61.4UBC-AS 78.7 80.8 76.3Table 1: Results of Semeval-2007 Task 2.
Unsuper-vised evaluation (FScore).System All Nouns VerbsBest 81.6 86.8 76.2Worst 77.1 80.5 73.3Average 79.1 82.8 75.0UBC-AS 78.5 80.7 76.0Table 2: Results of Semeval-2007 Task 2.
Super-vised evaluation as recall.
(mostly local and domain features of the contextwords) but, although the system performed slightlybetter, we decided that the little gain (which prob-ably was not statistically significant) was no worththe effort.5 ResultsTable 1 shows the results of the unsupervised evalu-ation in task 2, where our system got the best resultsin this setting.
Table 2 shows the supervised evalua-tion on the same task, where our system got a rank-ing of 4, performing slightly worse than the averageof the systems.In Table 3 we can see the results of Semeval-2007Task 13.
As can be seen, our system didn?t manageto capture the structure of the corpus, and it got theworst result, far below the average of the systems.6 ConclusionsWe have presented graph-based unsupervised sys-tem for induction and classification.
The system per-forms a two stage graph based clustering where a co-occurrence graph is first clustered to compute simi-larities against contexts.
The context similarity ma-trix is pruned and the resulting associated graph isclustered again by means of a random-walk type al-gorithm.
The system has participated in tasks 2 and13 of the SemEval-2007 competition, on word senseinduction and Web people search, respectively, withmixed results.
We did not have time to performan in-depth analysis of the reasons causing such adifferent performance.
One of the reasons for thefailure in the WePS task could be the fact that weSystem F?=0.5Best 78.0Worst 40.0Average 60.0UBC-AS 40.0Table 3: Results of Semeval-2007 Task 13were first-comers, with very little time to developthe system, and we used a very basic and coarse pre-processing of the HTML files.
Another factor couldbe that we intentionally made our clustering algo-rithm return few clusters.
We were mislead by thetraining data provided, as the final test data had moreclasses on average.AcknowledgementsThis work has been partially funded by the Spanisheducation ministry (project KNOW) and by the re-gional government of Gipuzkoa (project DAHAD).ReferencesE.
Agirre and A. Soroa.
2007.
Semeval-2007 task 2:evaluatingword sense induction and discrimination systems.
In Pro-ceedings of Semeval 2007, Association for ComputationalLinguistics.E.
Agirre, D.
Mart?
?nez, O.
Lo?pez de Lacalle, and A. Soroa.2006a.
Evaluating and optimizing the parameters of an un-supervised graph-based wsd algorithm.
In Proceedings ofTextGraphsWorkshop.
NAACL06., pages 89?96.
Associationfor Computational Linguistics, June.E.
Agirre, D.
Mart?
?nez, O.
Lo?pez de Lacalle, and A. Soroa.2006b.
Two graph-based algorithms for state-of-the-art wsd.In Proceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, pages 585?593.
Asso-ciation for Computational Linguistics, July.J.
Artiles, J. Gonzalo, and S. Sekine.
2007.
Establishing abenchmark for the web people search task: The semeval2007 weps track.
In Proceedings of Semeval 2007, Asso-ciation for Computational Linguistics.J.
Atserias, B. Casas, E. Comelles, M. Gonza?lez, L.
Padro?, andM.
Padro?.
2006.
Freeling 1.3: Syntactic and semantic ser-vices in an open-source NLP library.
In Proceedings of the5th International Conference on Language Resources andEvaluation (LREC?06), pages 48?55.Jon M. Kleinberg.
1999.
Authoritative sources in a hyperlinkedenvironment.
Journal of the ACM, 46(5):604?632.Stijn van Dongen.
2000.
A cluster algorithm for graphs.Technical Report INS-R0010, National Research Institutefor Mathematics and Computer Science in the Netherlands,Amsterdam, May.J.
Ve?ronis.
2004.
Hyperlex: lexical cartography for informa-tion retrieval.
Computer Speech & Language, 18(3):223?252.D.
J. Watts and S. H. Strogatz.
1998.
Collective dynamics of?small-world?
networks.
Nature, 393(6684):440?442, June.349
