Proceedings of the 8th Workshop on Asian Language Resources, pages 144?152,Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language ProcessingDevelopment of the Korean Resource Grammar:Towards Grammar CustomizationSanghoun SongDept.
of LinguisticsUniv.
of Washingtonsanghoun@uw.eduJong-Bok KimSchool of EnglishKyung Hee Univ.jongbok@khu.ac.krFrancis BondLinguistics and Multilingual StudiesNanyang Technological Univ.bond@ieee.orgJaehyung YangComputer EngineeringKangnam Univ.jhyang@kangnam.ac.krAbstractThe Korean Resource Grammar (KRG)is a computational open-source grammarof Korean (Kim and Yang, 2003) that hasbeen constructed within the DELPH-INconsortium since 2003.
This paper re-ports the second phase of the KRG devel-opment that moves from a phenomena-based approach to grammar customiza-tion using the LinGO Grammar Matrix.This new phase of development not onlyimproves the parsing efficiency but alsoadds generation capacity, which is nec-essary for many NLP applications.1 IntroductionThe Korean Resource Grammar (KRG) has beenunder development since 2003 (Kim and Yang,2003) with the aim of building an open sourcegrammar of Korean.
The grammatical frame-work for the KRG is Head-driven Phrase Struc-ture Grammar (HPSG: (Pollard and Sag, 1994;Sag et al, 2003)), a non-derivational, constraint-based, and surface-oriented grammatical archi-tecture.
The grammar models human languagesas systems of constraints on typed feature struc-tures.
This enables the extension of grammarin a systematic and efficient way, resulting inlinguistically precise and theoretically motivateddescriptions of languages.The initial stage of the KRG (hereafter,KRG1) has covered a large part of the Koreangrammar with fine-grained analyses of HPSG.However, this version, focusing on linguisticdata with theory-oriented approaches, is unableto yield efficient parsing or generation.
The addi-tional limit of the KRG1 is its unattested parsingefficiency with a large scale of naturally occur-ring data, which is a prerequisite to the practicaluses of the developed grammar in the area of MT.Such weak points have motivated us to movethe development of KRG to a data-driven ap-proach from a theory-based one upon which theKRG1 is couched.
In particular, this secondphase of the KRG (henceforth, KRG2) also startswith two methods: shared grammar libraries (theGrammar Matrix (Bender et al, 2002; Bender etal., 2010)) and data-driven expansion (using theKorean portions of multilingual texts).Next, we introduce the resources we used(?
2).
this is followed by more detailed motiva-tion for our extensions (?
3).
We then detail howwe use the grammar libraries from the GrammarMatrix to enable generation (?
2) and then ex-pand the coverage based on a corpus study (?
5).2 Background2.1 Open Source NLP with HPSGThe Deep Linguistic Processing with HPSGInitiative (DELPH-IN: www.delph-in.net)provides an open-source collection of tools andgrammars for deep linguistic processing of hu-man language within the HPSG and MRS (Min-imal Recursion Semantics (Copestake et al,2005)) framework.
The resources include soft-ware packages, such as the LKB for parsing andgeneration, PET (Callmeier, 2000) for parsing,and a profiling tool [incr tsdb()] (Oepen, 2001).There are also several grammars: e.g.
ERG; the144English Resource Grammar (Flickinger, 2000),Jacy; a Japanese Grammar (Siegel and Bender,2002), the Spanish grammar, and so forth.
Thesealong with some pre-compiled versions of pre-processing or experimental tools are packaged inthe LOGON distribution.1 Most resources are un-der the MIT license, with some parts under otheropen licenses such as the LGPL.2 The KRG hasbeen constructed within this open-source infras-tructure, and is released under the MIT license3.2.2 The Grammar MatrixThe Grammar Matrix (Bender et al, 2002; Ben-der et al, 2010) offers a well-structured envi-ronment for the development of precision-basedgrammars.
This framework plays a role in build-ing a HPSG/MRS-based grammar in a shorttime, and improving it continuously.
The Gram-mar Matrix covers quite a few linguistic phe-nomena constructed from a typological view.There is also a starter-kit, the Grammar Matrixcustomization system which can build the back-bone of a computational grammar from a linguis-tic description.2.3 A Data-driven ApproachNormally speaking, building up a computationalgrammar is painstaking work, because it coststoo much time and effort to develop a grammarby hand only.
An alternative way is a data-drivenapproach which ensures ?cheap, fast, and easy?development.
However, this does not mean thatone is better than the other.
Each of these twoapproaches has its own merits.
To achieve thebest or highest performance of parsing and gen-eration, each needs to complement the other.3 Directions for Improvement3.1 Generation for MTHPSG/MRS-based MT architecture consists ofparsing, transfer, and generation, as assumed inFigure 1 (Bond et al, 2005).
As noted earlier,1wiki.delph-in.net/moin/LogonTop2www.opensource.org/licenses/3It allows people ?.
.
.
without limitation the rights touse, copy, modify, merge, publish, distribute, sublicense,and/or sell copies?
so long as ?The above copyright noticeand this permission notice shall be included .
.
.
?Figure 1: HPSG/MRS-based MT Architecturethe KRG1 with no generation function is limitedonly to the Source Analysis in Figure 1.
In addi-tion, since its first aim was to develop a Koreangrammar that reflects its individual properties indetail, the KRG1 lacks compatible semantic rep-resentations with other grammars such as ERGand Jacy.
The mismatches between the compo-nents of the KRG1 and those of other grammarsmake it difficult to adopt the Korean grammar foran MT system.
To take a representative example,the KRG1 treats tense information as a featuretype of HEAD, while other grammars incorpo-rate it into the semantics; thus, during the trans-fer process in Figure 1, some information will bemissing.
In addition, KRG1 used default inheri-tance, which makes the grammar more compact,but means it could not used with the faster PETparser.
We will discuss this issue in more detailin Section 4.1.Another main issue in the KRG1 is that someof the defined types and rules in the grammar areinefficient in generation.
Because the declaredtypes and rules are defined with theoretical mo-tivations, the run-time for generating any parsingunits within the system takes more than expectedand further causes memory overflow errors tocrop up almost invariably, even though the in-put is quite simple.
This problem is partially dueto the complex morphological inflection systemin the KRG1.
Section 4.2 discusses how KRG2,solves this problem.Third it is better ?to be robust for parsing andstrict for generation?
(Bond et al, 2008).
Thatmeans robust rules will apply in parsing, thoughthe input sentence does not sound perfect, but notin generation.
For example, the sentence (1b),the colloquial form of the formal, standard sen-tence (1a), is used more frequently in colloquialcontext:(1) a. ney-kayou-NOMchamreallyyeppu-ney.pretty-DECL?You are really pretty.?b.
ni-ka cham ippu-neyThe grammar needs to parse both (1a) and145(1b) and needs to yield the same MRS be-cause both sentences convey the same truth-conditional meaning.
However, the KRG1 han-dles only the legitimate sentence (1a), exclud-ing (1b).
The KRG1 is thus not sophisticatedenough to distinguish these two stylistic differ-ent sentences.
Therefore we need to develop thegeneration procedures that can choose a propersentence style.
Section 4.3 proposes the ?STYLE?feature structure as the choice device.3.2 Exploiting CorporaOne of the main motivations for our grammarimprovement is to achieve more balance betweenlinguistic motivation and practical purpose.
Wehave first evaluated the coverage and perfor-mance of the KRG1 using a large size of datato track down the KRG1?s problems that maycause parsing inefficiencies and generating clog.In other words, referring to the experimental re-sults, we patterned the problematic parts in thecurrent version.
According to the error pattern,on the one hand, we expanded lexicon from oc-curring texts in our generalization.
On the otherhand, we fixed the previous rules and sometimesintroduced new rules with reference to the occur-rence in texts.3.3 How to ImproveIn developing the KRG, we have employed twostrategies for improvement; (i) shared grammarlibraries and (ii) exploiting large text corpora.We share grammar libraries with the Gram-mar Matrix in the grammar (Bender et al, 2002)as the foundation of KRG2.
The Grammar Ma-trix provides types and constraints that assist thegrammar in producing well-formed MRS repre-sentations.
The Grammar Matrix customizationsystem provides with a linguistically-motivatedbroad coverage grammar for Korean as well asthe basis for multilingual grammar engineering.In addition, we exploit naturally occurring textsas the generalization corpus.
We chose as ourcorpora Korean texts that have translations avail-able in English or Japanese, because they can bethe baseline of multilingual MT.
Since the data-driven approach is influenced by data type, mul-tilingual texts help us make the grammar moresuitable for MT in the long term.
In developingthe grammar in the next phrase, we assumed thefollowing principles:(2) a.
The Grammar Matrix will apply when a judg-ment about structure (e.g.
semantic represen-tation) is needed.b.
The KRG will apply when a judgment aboutKorean is needed.c.
The resulting grammar has to run on bothPET and LKB without any problems.d.
Parsing needs to be accomplished as robustlyas possible, and generation needs to be doneas strictly as possible.4 GenerationIt is hard to alter the structure of the KRG1from top to bottom in a relatively short time,mainly because the difficulties arise from con-verting each grammar module (optimized onlyfor parsing) into something applicable to gener-ation, and further from making the grammar runseparately for parsing and generation.Therefore, we first rebuilt the basic schema ofthe KRG1 on the Grammar Matrix customiza-tion system, and then imported each grammarmodule from KRG1 to the matrix-based frame(?4.1).
In addition, we reformed the inflectionalhierarchy assumed in the KRG1, so that thegrammar does not impede generation any longer(?
4.2).
Finally, we introduced the STYLE featurestructure for sentence choice in accordance withour principles (2c-d) (?4.3).4.1 Modifying the Modular StructureThe root folder krg contains the basic typedefinition language files (*.tdl.
In theKRG2, we subdivided the types.tdl into:matrix.tdl file which corresponds to gen-eral principles; korean.tdl with languageparticular rules; types-lex.tdl for lex-ical types and types-ph.tdl for phrasaltypes.
In addition, we reorganized the KRG1?slexicons.tdl file into the lex folder con-sisting of several sub-files in accordance with thePOS values (e.g.
; lex-v.tdl for verbs).The next step is to revise grammar modulesin order to use the Grammar Matrix to a full ex-tent.
In this process, when inconsistencies arisebetween KRG1 and KRG2, we followed (2a-b).146We further transplanted each previous moduleinto the KRG2, while checking the attested testitems used in the KRG1.
The test items, con-sisting of 6,180 grammatical sentences, 118 un-grammatical sentences, were divided into sub-groups according to the related phenomena (e.g.light verb constructions).4.2 Simplifying the Inflectional HierarchyKorean has rigid ordering restrictions in the mor-phological paradigm for verbs, as shown in (3).
(3) a. V-base + HON + TNS + MOOD + COMPb.
ka-si-ess-ta-ko ?go-HON-PST-DC-COMP?KRG1 dealt with this ordering of suffixes by us-ing a type hierarchy that represents a chain of in-flectional slots (Figure 2: Kim and Yang (2004)).Figure 2: Korean Verbal HierarchyThis hierarchy has its own merits, but it is notso effective for generating sentences.
This is be-cause the hierarchy requires a large number ofcalculations in the generation process.
Figure 3and Table 1 explains the difference in computa-tional complexity according to each structure.InFigure 3: Calculating ComplexityFigure 3, (a) is similar to Figure 2, while (b) is onthe traditional template approach.
Let us com-pare each complexity to get the target node D.For convenience?
sake, let us assume that eachnode has ten constraints to be satisfied.
In (a),since there are three parents nodes (i.e.
A, B, andC) on top of D, D cannot be generated until A,B, and C are checked previously.
Hence, it costsat least 10,000 (10[A] ?10[B] ?10[C] ?10[D])calculations.
In contrast, in (b), only 100 (10[A]?10[D]) calculations is enough to generate nodeD.
That means, the deeper the hierarchy is, themore the complexity increases.
Table 1 shows(a) requires more than 52 times as much com-plexity as (b), though they have the same numberof nodes.Table 1: Complexity of (a) and (b)(a) (b)B?
10[A]?10[B?]
100 10[A]?10[B?]C?
10[A]?10[B]?10[C?]
1,000 10[A]?10[C?]D?
10[A]?10[B]?10[C]?10[D?]
10,000 10[A]?10[D?
]D 10[A]?10[B]?10[C]?10[D] 10,000 10[A]?10[D]?
21,100 400When generation is processed by LKB, all po-tential inflectional nodes are made before syntac-tic configurations according to the given MRS.Thus, if the hierarchy becomes deeper and con-tains more nodes, complexity of (a)-styled hi-erarchy grows almost by geometric progres-sion.
This makes generation virtually impossi-ble, causing memory overflow errors to the gen-eration within the KRG1.A fully flat structure (b) is not always supe-rior to (a).
First of all, the flat approach ig-nores the fact that Korean is an agglutinativelanguage.
Korean morphological paradigm canyield a wide variety of forms; therefore, to enu-merate all potential forms is not only undesirablebut also even impossible.The KRG2 thus follows a hybrid approach (c)that takes each advantage of (a) and (b).
(c) ismore flattened than (a), which lessens computa-tional complexity.
On the other hand, in (c), thedepth of the inflectional hierarchy is fixed as two,and the skeleton looks like a unary form, thougheach major node (marked as a bigger circle) hasits own subtypes (marked as dotted lines).
Eventhough the depth has been diminished, the hier-archy is not a perfectly flat structure; therefore, itcan partially represent the austere suffix orderingin Korean.
The hierarchy (c), hereby, curtails thecost of generation.In this context, we sought to use the minimumnumber of possible inflectional slots for Korean.We need at least three: root + semantic slot(s)+ syntactic slot(s).
That is, a series of suffixes147Table 2: Complexity of (a-c)Depth Complexity(a) n ?
3 ?
10,000(b) n = 1 100(c) n = 2 10,000that denote semantic information attaches to thesecond slot, and a series of suffixes, likewise,attaches to the third slot.
Since semantic suf-fixes are, almost invariably, followed by syntac-tic ones in Korean, this ordering is convincing,granting that it does not fully represent that thereis also an ordering among semantic forms or syn-tactic ones.
(4) is an example from hierarchy (c).There are three slots; root ka ?go?, semantic suf-fixes si-ess, and syntactic ones ta-ko.
(4) a. V-base + (HON+TNS) + (MOOD+COMP)b. ka-si+ess-ta+ko ?go-HON+PST-DC+COMP?Assuming there are ten constraints on each node,the complexity to generate D in (c) is just 10,000.The measure, of course, is bigger than that of (b),but the number never increases any more.
Thatmeans, all forms at the same depth have equalcomplexity, and it is fully predictable.
Table 2compares the complexity from (a) to (c).
By con-verting (a) to (c), we made it possible to generatewith KRG2.4.3 Choosing a Sentence StyleThe choice between formal or informal (collo-quial) sentence styles depends on context.
A ro-bust parser should cover both styles, but we gen-erally want a consistent style when generating.Figure 4: Type Hierarchy of STYLEIn such a case, the grammar resorts to STYLEto filter out the infelicitous results.
The type hi-erarchy is sketched out in Figure 4. strict is nearto school grammar (e.g.
written is a style ofnewspapers).
On the other hand, some variantforms that stem from the corresponding canoni-cal forms falls under robust in Figure 4.
For in-stance, if the text domain for generation is news-paper, we can select only written as our sentencechoice, which excludes other styled sentencesfrom our result.Let us see (1a-b) again.
ni ?you?
in (1b) is a di-alect form of ney, but it has been used more pro-ductively than its canonical form in daily speech.In that case, we can specify STYLE of ni as di-alect as given below.
In contrast, the neutralform ney has an unspecified STYLE feature:ni := n-pn-2nd-non-pl &[ STEM < ??ni??
>, STYLE dialect ].ney := n-pn-2nd-non-pl &[ STEM < ??ney??
> ].Likewise, since the predicate in (1b) ippu?pretty?
stems from yeppu in (1a), they sharethe predicate name ?
yeppu a 1 rel?
(i.e.
theRMRS standard for predicate names such as?
lemma pos sense rel?
), but differ in eachSTYLE feature.
That means (1a-b) share thesame MRS structure (given below).
KRG herebycan parse (1b) into the same MRS as (1a) andgenerate (1a) from it.?????????????????????????????????
?mrsLTOP h1 hINDEX e2 eRELS????????
?person relLBL h3 hARG0 x4??
?xPNG.PER 2ndPNG.NUM non-pl???????????,??????
?exist q relLBL h5 hARG0 x4RSTR h6 hBODY h7 h???????,????
?cham d 1 relLBL h1ARG0 e9 eARG1 h8 h?????,????
?yeppu a 1 relLBL h10 hARG0 e2ARG1 x4??????HCONS???
?qeqHARG h6LARG h3???,??
?qeqHARG h8LARG h10?????????????????????????????????????
?Figure 5: MRS of (1a-b)These kinds of stylistic differences can takeplace at the level of (i) lexicon, (ii) morpholog-ical combination, and (iii) syntactic configura-tion.
The KRG2 revised each rule with referenceto its style type; therefore, we obtained totally96 robust rules.
As a welcoming result, we couldmanipulate our generation, which was successfulrespect to (2c-d).
Let us call the version recon-structed so far ?base?.1485 Exploiting Corpora5.1 ResourcesThis study uses two multilingual corpora; one isthe Sejong Bilingual Corpora: SBC (Kim andCho, 2001), and the other is the Basic Travel Ex-pression Corpus: BTEC (Kikui et al, 2003).
Weexploited the Korean parts in each corpus, takingthem as our generalization corpus.
Table 3 repre-sents the configuration of two resources (KoEn:Korean-English, KoJa: Korean-Japanese):Table 3: Generalization CorporaSBC BTECType Bilingual MultilingualDomain Balanced Corpus TourismWords KoEn : 243,788 914,199KoJa : 276.152T/T ratio KoEn : 27.63 92.7KoJa : 20.28Avr length KoEn : 16.30 8.46KoJa : 23.30We also make use of nine test suites sortedby three types (Each test suite includes 500 sen-tences).
As the first type, we used three testsets covering overall sentence structures in Ko-rean; Korean Phrase Structure Grammar (kpsg;Kim (2004)), Information-based Korean Gram-mar (ibkg; Chang (1995)), and the SERI test set(seri; Sung and Jang (1997)).Second, we randomly extracted sentencesfrom each corpus, separately from our gener-alization corpus; two suites were taken fromthe Korean-English and Korean-Japanese pair inSBC (sj-ke and sj-kj, respectively).
The othertwo suites are from the BTEC-KTEXT (b-k),and the BTEC-CSTAR (b-c); the former consistsof relatively plain sentences, while the latter iscomposed of spoken ones.Third, we obtained two test suites from samplesentences in two dictionaries; Korean-English(dic-ke), and Korean-Japanese (dic-kj).
Thesesuites assume to have at least two advantageswith respect to our evaluation; (i) the sentencelength is longer than that of BTEC as well asshorter than that of SBC, (ii) the sample sen-tences on dictionaries are normally made up ofuseful expressions for translation.5.2 MethodsWe tried to do experiments and improve theKRG, following the three steps repeatedly: (i)evaluating, (ii) identifying, and (iii) exploiting.In each of the first step, we tried to parse the ninetest suites and generate sentences with the MRSstructures obtained from the parsing results, andmeasured their coverage and performance.
Here,?coverage?
means how many sentences can beparsed or generated, and ?performance?
repre-sents how many seconds it takes on average.
Inthe second step, we identified the most seriousproblems.
In the third step, we sought to exploitour generalization corpora in order to remedy thedrawbacks.
After that, we repeated the proce-dures until we obtain the desired results.5.3 ExperimentsSo far, we have got two versions; KRG1 andbase.
Our further experiments consist of fourphases; lex, MRS, irules, and KRG2.Expanding the lexicon: To begin with, in or-der to broaden our coverage, we expanded ourlexical entries with reference to our generaliza-tion corpus and previous literature.
Verbal itemsare taken from Song (2007) and Song and Choe(2008), which classify argument structures ofKorean verbal lexicon into subtypes within theHPSG framework in a semi-automatic way.
Thereason why we do not use our corpus here isthat verbal lexicon commonly requires subcat-egorization frames, but we cannot induce themso easily only using corpora.
For other wordclasses, we extracted lexical items from the POStagged SBC and BTEC corpora.
Table 4 explainshow many items we extracted from our general-ization corpus.
Let us call this version ?lex?.Table 4: Expansion of Lexical Itemsverbal nouns 4,474verbs and adjectives 1,216common nouns 11,752proper nouns 7,799adverbs 1,757numeral words 1,172MRS: Generation in LKB, as shown in Fig-ure 1, deploys MRS as the input, which meansour generation performance hinges on the well-149formedness of MRS.
In other words, if our MRSis broken somewhere or constructed inefficiently,generation results is directly affected.
For in-stance, if the semantic representation does notscope, we will not generate correctly.
We wereable to identify such sentences by parsing thecorpora, storing the semantic representations andthen using the semantic well formedness check-ers in the LKB.
We identified all rules and lexi-cal items that produced ill-formed MRSs usinga small script and fixed them by hand.
This hadan immediate and positive effect on coverage aswell as performance in generation.
We refer tothese changes as ?MRS?.Different inflectional forms for sentencestyles: Texts in our daily life are actually com-posed of various styles.
For example, spokenforms are normally more or less different fromwritten ones.
The difference between them inKorean is so big that the current version of KRGcan hardly parse spoken forms.
Besides, Ko-rean has lots of compound nouns and derivedwords.
Therefore, we included these forms intoour inflectional rules and expanded lexical en-tries again (3,860 compound nouns, 2,791 de-rived words).
This greatly increased parsing cov-erage.
We call this version ?irules?.Grammaticalized and Lexicalized Forms:There are still remaining problems, because ourtest suites contain some considerable forms.First, Korean has quite a few grammaticalizedforms; for instance, kupwun is composed of adefinite determiner ku and a classifier for humanpwun ?the person?, but it functions like a sin-gle word (i.e.
a third singular personal pronoun).In a similar vein, there are not a few lexical-ized forms as well; for example, a verbal lexemekkamek- is composed of kka- ?peel?
and mek-?eat?, but it conveys a sense of ?forget?, ratherthan ?peel and eat?.
In addition, we also need tocover idiomatic expressions (e.g.
?thanks?)
forrobust parsing.
Exploiting our corpus, we added1,720 grammaticalized or lexicalized forms and352 idioms.
Now, we call this ?KRG2?.Table 5 compares KRG2 with KRG1, and Fig-ure 6 shows how many lexical items we havecovered so far.Table 5: Comparison between KRG1 and KRG2KRG1 KRG2# of default types 121 159# of lexical types 289 593# of phrasal types 58 106# of inflectional rules 86 244# of syntactic rules 36 96# of lexicon 2,297 39,190Figure 6: Size of Lexicon5.4 EvaluationTable 6 shows the evaluation measure of thisstudy.
?p?
and ?g?
stand for ?parsing?
and ?gener-ation?, respectively.
?+?
represents the differencecompared to KRG1.
Since KRG1 does not gen-erate, there is no ?g+?.Table 6: Evaluationcoverage (%) ambiguityp p+ g s p gkpsg 77.0 -5.5 55.2 42.5 174.9 144.4ibkg 61.2 41.8 68.3 41.8 990.5 303.5seri 71.3 -0.8 65.7 46.8 289.1 128.4b-k 43.0 32.6 62.8 27.0 1769.4 90.0b-c 52.2 45.8 59.4 31.0 1175.8 160.6sj-ke 35.4 31.2 58.2 20.6 358.3 170.3sj-kj 23.0 19.6 52.2 12.0 585.9 294.9dic-ke 40.4 31.0 42.6 17.2 1392.7 215.9dic-kj 34.8 25.2 67.8 23.6 789.3 277.9avr 48.7 24.5 59.1 28.8 836.2 198.4On average, the parsing coverage increases24.5%.
The reason why there are negative val-ues in ?p+?
of kpsg and seri is that we discardedsome modules that run counter efficient process-ing (e.g., the grammar module for handling float-ing quantifiers sometimes produces too manyambiguities.).
Since KRG1 has been constructedlargely around the test sets, we expected it toperform well here.
If we measure the parsingcoverage again, after excluding the results ofkpsg and seri, it accounts for 32.5%.4 The gen-eration coverage of KRG2 accounts for almost60% per parsed sentence on average.
Note thatKRG1 could not parse at all.
?s?
(short for ?suc-cess?)
means the portion of both parsed and gen-erated sentences (i.e.
?p???g?
), which accounts4The running times, meanwhile, becomes slower as wewould expect for a grammar with greater coverage.
How-ever, we can make up for it using the PET parser, as shownin Figure 9.1500102030405060708090KRG1 base lex MRS irules KRG2%kpsgibkgserib-kb-csj-kesj-kjdic-kedic-kjFigure 7: Parsing Coverage (%)0102030405060708090base lex MRS irules KRG2%kpsgibkgserib-kb-csj-kesj-kjdic-kedic-kjFigure 8: Generation Coverage (%)for about 29%.
Ambiguity means ?# of parses/#of sentences?
for parsing and ?# of realizations/#of MRSes?
for generation.
The numbers lookrather big, which should be narrowed down inour future study.In addition, we can find out in Table 6 thatthere is a coverage ordering with respect to thetype of test suites; ?test sets > BTEC > dic >SBC?.
It is influenced by three factors; (i) lexi-cal variety, (ii) sentence length, and (iii) text do-main.
This difference implies that it is highlynecessary to use variegated texts in order to im-prove grammar in a comprehensive way.Figure 7 to 10 represent how much each exper-iment in ?5.3 contributes to improvement.
First,let us see Figure 7 and 8.
As we anticipated,lex and irules contribute greatly to the growth ofparsing coverage.
In particular, the line of b-c inFigure 8, which mostly consists of spoken forms,rises rapidly in irules and KRG2.
That impliesKorean parsing largely depends on richness oflexical rules.
On the other hand, as we also ex-pected, MRS makes a great contribution to gen-eration coverage (Figure 8).
In MRS, the growthaccounts for 22% on average.
That implies test-ing with large corpora must take precedence inorder for coverage to grow.Figure 9 and 10 shows performance in pars-ing and generation, respectively.
Comparing toKRG1, our Matrix-based grammars (from base0.05.010.015.020.025.0KRG1 base lex MRS irules KRG2sec.kpsgibkgserib-kb-csj-kesj-kjdic-kedic-kjFigure 9: Parsing Performance (s)0.05.010.015.020.025.0base lex MRS irules KRG2sec.kpsgibkgserib-kb-csj-kesj-kjdic-kedic-kjFigure 10: Generation Performance (s)to KRG2) yields fairly good performance.
It ismainly because we deployed the PET parser thatruns fast, whereas KRG1 runs only on LKB.
Fig-ure 10, on the other hand, shows that the revi-sion of MRS also does much to enhance gen-eration performance, in common with coveragementioned before.
It decreases the running timesby about 3.1 seconds on average.6 ConclusionThe newly developed KRG2 has been success-fully included in the LOGON repository sinceJuly, 2009; thus, it is readily available.
In fu-ture research, we plan to apply the grammar inan MT system (for which we already have aprototype).
In order to achieve this goal, weneed to construct multilingual treebanks; Korean(KRG), English (ERG), and Japanese (Jacy).AcknowledgmentsWe thank Emily M. Bender, Dan Flickinger,Jae-Woong Choe, Kiyotaka Uchimoto, EricNichols, Darren Scott Appling, and StephanOepen for comments and suggestions at vari-ous stages.
Parts of this research was conductedwhile the first and third authors were at the Na-tional Institute for Information and Communi-cations Technologies (NICT), Japan; we thankNICT for their support.
Our thanks also go tothree anonymous reviewers for helpful feedback.151ReferencesBender, Emily M., Dan Flickinger, and StephanOepen.
2002.
The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development ofCross-Linguistically Consistent Broad-CoveragePrecision Grammars.
In Procedings of the Work-shop on Grammar Engineering and Evaluation atthe 19th International Conference on Computa-tional Linguistics.Bender, Emily M., Scott Drellishak, Antske Fokkens,Michael Wayne Goodman, Daniel P. Mills, LauriePoulson, and Safiyyah Saleem.
2010.
GrammarPrototyping and Testing with the LinGO GrammarMatrix Customization System.
In Proceedings ofACL 2010 Software Demonstrations.Bond, Francis, Stephan Oepen, Melanie Siegel, AnnCopestake, and Dan Flickinger.
2005.
OpenSource Machine Translation with DELPH-IN.
InProceedings of Open-Source Machine Transla-tion: Workshop at MT Summit X.Bond, Francis, Eric Nichols, Darren Scott Appling,and Michael Paul.
2008.
Improving StatisticalMachine Translation by Paraphrasing the Train-ing Data.
In Proceedings of the 5th InternationalWorkshop on Spoken Languaeg Translation.Callmeier, Ulrich.
2000.
PET?a Platform for Exper-imentation with Efficient HPSG Processing Tech-niques.
Natural Language Engineering, 6(1):99?107.Chang, Suk-Jin.
1995.
Information-based KoreanGrammar.
Hanshin Publishing, Seoul.Copestake, Ann, Dan Flickinger, Carl Pollard, andIvan A.
Sag.
2005.
Minimal Recursion Seman-tics: An Introduction.
Research on Language andComputation, 3(4):281?332.Flickinger, Dan.
2000.
On Building a More EfficientGrammar by Exploiting Types.
Natural LanguageEngineering, 6 (1) (Special Issue on Efficient Pro-cessing with HPSG):15 ?
28.Kikui, Genichiro, Eiichiro Sumita, ToshiyukiTakezawa, and Seiichi Yamamoto.
2003.
Creatingcorpora for speech-to-speech translation.
In Proc.of the EUROSPEECH03, pages 381?384, Geneve,Switzerland.Kim, Se-jung and Nam-ho Cho.
2001.
The progressand prospect of the 21st century Sejong project.
InICCPOL-2001, pages 9?12, Seoul.Kim, Jong-Bok and Jaehyung Yang.
2003.
Ko-rean Phrase Structure Grammar and Its Implemen-tations into the LKB System.
In Proceedings ofthe 17th Pacific Asia Conference on Language, In-formation and Computation.Kim, Jong-Bok and Jaehyung Yang.
2004.
Projec-tions from Morphology to Syntax in the KoreanResource Grammar: Implementing Typed FeatureStructures.
In Lecture Notes in Computer Science,volume 2945, pages 13?24.
Springer-Verlag.Kim, Jong-Bok.
2004.
Korean Phrase StructureGrammar.
Hankwuk Publishing, Seoul.Oepen, Stephan.
2001.
[incr tsdb()] ?
competenceand performance laboratory.
User manual.
Tech-nical report, Computational Linguistics, SaarlandUniversity.Pollard, Carl and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
The University ofChicago Press, Chicago, IL.Sag, Ivan A., Thomas Wasow, , and Emily M. Bender.2003.
Syntactic Theory: A Formal Introduction.CSLI Publications, Stanford, CA.Siegel, Melanie and Emily M. Bender.
2002.
Effi-cient Deep Processing of Japanese.
In Proceed-ings of the 3rd Workshop on Asian Language Re-sources and International Standardization.Song, Sanghoun and Jae-Woong Choe.
2008.Automatic Construction of Korean Verbal TypeHierarchy using Treebank.
In Proceedings ofHPSG2008.Song, Sanghoun.
2007.
A Constraint-based Analysisof Passive Constructions in Korean.
Master?s the-sis, Korea University, Department of Linguistics.Sung, Won-Kyung and Myung-Gil Jang.
1997.
SERITest Suites ?95.
In Proceedings of the Confer-ence on Hanguel and Korean Language Informa-tion Processing.152
