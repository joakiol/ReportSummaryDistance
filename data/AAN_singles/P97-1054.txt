Co-evolution of Language and of the Language Acquisit ion DeviceTed Br iscoee jb?c l ,  cam.
ac .
ukComputer  LaboratoryUn ivers i ty  of Cambr idgePembroke  StreetCambr idge  CB2 3QG,  UKAbst ractA new account of parameter setting dur-ing grammatical cquisition is presented interms of Generalized Categorial Grammarembedded in a default inheritance hierar-chy, providing a natural partial orderingon the setting of parameters.
Experimentsshow that several experimentally effectivelearners can be defined in this framework.Ew)lutionary simulations suggest that alea.rner with default initial settings for pa-rameters will emerge, provided that learn-ing is memory limited and the environmentof linguistic adaptation contains an appro-priate language.1 Theoret i ca l  BackgroundGrmnnmtical acquisition proceeds on the basis of apartial genotypic specifica.tion of (universal) grmn-mar (UG) complemented with a learning procedureelmbling the child to complete this specification ap-propriately.
The parameter setting frainework ofChomsky (1981) claims that learning involves fix-ing the wdues of a finite set of finite-valued param-eters to select a single fully-specified grammar fromwithin the space defined by the genotypic specifi-cation of UG.
Formal accounts of parameter set-ting have been developed for small fragments buteven in these search spaces contain local maximaand subset-superset relations which may cause alearner to converge to an incorrect grammar (Clark,1992; Gibson and Wexler, 1994; Niyogi and Berwick,1995).
The solution to these problems involves defin-ing d(,fault, umnarked initial values for (some) pa-rameters and/or ordering the setting of parainetersduring learning.Bickerton (1984) argues for the Bioprograin Hy-pothesis a.s an explanation for universal similaritiesbetween historically unrelated creoles, and for therapid increase in gramlnatical complexity accompa-nying the transition from pidgin to creole languages.Prom the perspective of the parameters framework,the Bioprogram Hypothesis claims that children areendowed genetically with a UG which, by default,specifies the stereotypical core creole grammar, withright-branching syntax and subject-verb-object or-der, as in Saramaccan.
Others working within theparameters framework have proposed unmarked, de-fault parameters (e.g.
Lightfoot, 1991), but the Bio-program Hypothesis can be interpreted as towardsone end of a continuum of proposals ranging from allparameters initially unset to all set to default values.2 The Language Acquisition DeviceA model of the Language Acquisition Device (LAD)incorporates a UG with associated parameters, aparser, and an algorithm for updating initial param-eter settings on parse failure during learning.2.1 The Grammar (set)Basic categorial grammar (CG) uses one rule of ap-plication which combines a functor category (con-taining a slash) with an argument category to forma derived category (with one less slashed argumentcategory).
Grammatical constraints of order andagreement are captured by only allowing directedapplication to adjacent matching categories.
Gener-alized Categorial Grammar (GCG) extends CG withfurther rule schemata) The rules of FA, BA, gen-eralized weak permutation (P) and backward andforward colnposition (I?C, BC) are given in Fig-ure 1 (where X, Y and Z are category variables,\[ is a vm'iable over slash and backslash, and .. .denotes zero or more further flmctor arguments).Once pernmtation is included, several semanticallyl\?ood (1993) is a general introduction to CategorialGrammar mid extensions to the basic theory.
The mostclosely related theories to that presented here are thoseof Steedman (e.g.
1988) and Hoffman (1995).418X/Y Y ~ XY X\Y ~ XForward Application:A y \[X(y)\] (y) ::~ X(y)Backward Application:A y \[X(y)\] (y) =~ X(y)X/Y Y/Z ~ X/ZY\Z X\Y ~ X\ZForward Composition:y \[X(y)\] A z \[Y(z)\] =~ A z \[X(Y(z))\]Backward Composition:z \[Y(z)\] A y \[X(y)\] ~ A z \[X(Y(z))\](Generalized Weak) Permutation:(XIY1)... IY, ~ (XIYn)IYI... A Yn..-,Yl \[X(yl ...,y,.
)\] =V A Yl,Y .
.
.
.
\[X(yl ...,Yn)\]Figure 1: GCG Rule SchemataKim loves SandyNP (S\NP)/NP NPkim' A y,x \[love'(x y)\] sandy'P(S/NP)\NPA x,y \[love'(x y)\]-BAS/NPA y \[love'(kim' y)\]FASlove'(kim' sandy')Figure 2: GCG Derivation for Kim loves Sandyequivalent derivations for Kim loves Sandy becomeavailable, Figure 2 shows the non-conventional left-branching one.
Composition also allows alterna-tive non-conventional semantically equivalent (left-branching) derivations.GCG as presented is inadequate as an account ofUG or of any individual grammar.
In particular,the definition of atomic categories needs extendingto deal with featural variation (e.g.
Bouma and vanNoord, 1994), and the rule schemata, especially com-position and weak permutation, must be restrictedin various parametric ways so that overgenerationis prevented for specific languages.
Nevertheless,GCG does represent a plausible kernel of UG; Hoff-man (1995, 1996) explores the descriptive power of avery similar system, in which generalized weak per-mutation is not required because functor argumentsare interpreted as multisets.
She demonstrates thatthis system can handle (long-distance) scramblingelegantly and generates mildly context-sensitive lan-guages (Joshi et al 1991).The relationship between GCG as a theory of UG(GCUG) and as a the specification of a particu-lar grammar is captured by embedding the theoryin a default inheritance hierarchy.
This is repre-sented as a lattice of typed default feature structures(TDFSs) representing subsumption and default in-heritance relationships (Lascarides et al 1996; Las-carides and Copestake, 1996).
The lattice definesintensionally the set of possible categories and ruleschemata via type declarations on nodes.
For ex-ample, an intransitive verb might be treated as asubtype of verb, inheriting subject directionality bydefault from a type gendir  (for general direction).For English, gendir  is default r ight but the node ofthe (intransitive) functor category, where the direc-tionality of subject arguments i specified, overridesthis to left, reflecting the fact that English is pre-dominantly right-branching, though subjects appearto the left of the verb.
A transitive verb would in-herit structure from the type for intransitive verbsand an extra NP argument with default directional-ity specified by gendir,  and so forth.
2For the purposes of the evolutionary simulationdescribed in ?3, GC(U)Gs are represented as a se-quence of p-settings (where p denotes principles orparameters) based on a flat (ternary) sequential en-coding of such default inheritance lattices.
The in-2Bouma and van Noord (1994) and others demon-strate that CGs can be embedded in a constraint-basedrepresentation.
Briscoe (1997a,b) gives further details ofthe encoding of GCG in TDFSs.419NP N S gen-dir  subj -d ir  applicAT  AT  AT  DR DL  DTNP  gendir  applic S N subj -d i rAT  DR DT AT  AT  DL"applic NP  N gen-dir  subj -d i r  SDT  AT  AT  DR DL  ATFigure 3: Sequential encodings of the grammar fragmentheritance hierarchy provides a partial ordering onparameters, which is exploited in the learning pro-cedure.
For example, the atomic categories, N,NP  and S are each represented by a parameter en-coding the presence/absence or lack of specification(T/F/?)
of the category in the (U)G. Since they willbe unordered in the lattice their ordering in the se-quential coding is arbitrary.
However, the orderingof the directional types gendir  and subjd i r  (withvalues L/R) is significant as the latter is a more spe-cific type.
The distinctions between absolute, de-fault or unset specifications also form part of theencoding (A/D/?).
Figure 3 shows several equiva-lent and equally correct sequential encodings of thefragment of the English type system outlined above.A set of grammars based on typological distinc-tions defined by basic constituent order (e.g.
Green-berg, 1966; Hawkins, 1994) was constructed as a(partial) GCUG with independently varying binary-valued parameters.
The eight basic language fami-lies are defined in terms of the unmarked order ofverb (V), subject (S) and objects (0) in clauses.Languages within families further specify the orderof modifiers and specifiers in phrases, the order of ad-positions and further phrasal-level ordering param-eters.
Figure 4 list the language-specific orderingparameters used to define the full set of grammarsin (partial) order of generality, and gives examplesof settings based on familiar languages such as "En-glish", "German" and "Japanese".
3 "English" de-fines an SVO language, with prepositions in whichspecifiers, complementizers and some modifiers pre-cede heads of phrases.
There are other grammars inthe SVO family in which all modifers follow heads,there are postpositions, and so forth.
Not all combi-nations of parameter settings correspond to attestedlanguages and one entire language family (OVS) isunattested.
"Japanese" is an SOV language with3Throughout double quotes around language namesare used as convenient mnemonics for familiar combina-tions of parameters.
Since not all aspects of these actuallanguages are represented in the grammars, conclusionsabout actual anguages must be made with care.postpositions in which specifiers and modifiers followheads.
There are other languages in the SOV familywith less consistent left-branching syntax in whichspecifiers and/or modifiers precede phrasal heads,some of which are attested.
"German" is a morecomplex SOV language in which the parameter verb-second (v2) ensures that the surface order in mainclauses is usually SVO.
4There are 20 p-settings which determine the ruleschemata vailable, the atomic category set, and soforth.
In all, this CGUG defines just under 300grammars.
Not all of the resulting languages are(stringset) distinct and some are proper subsets ofother languages.
"English" without the rule of per-mutation results in a stringset-identical l nguage,but the grammar assigns different derivations tosome strings, though the associated logical forms areidentical.
"English" without composition results ina subset language.
Some combinations of p-settingsresult in 'impossible' grammars (or UGs).
Othersyield equivalent grammars, for example, differentcombinations of default settings (for types and theirsubtypes) can define an identical category set.The grammars defined generate (usually infinite)stringsets of lexical syntactic categories.
Thesestrings are sentence types since each is equivalentto a finite set of grammatical sentences formed byselecting a lexical instance of each lexicai category.Languages are represented as a finite subset of sen-tence types generated by the associated grammar.These represent a sample of degree-1 learning trig-gers for the language (e.g.
Lightfoot, 1991).
Subsetlanguages are represented by 3-9 sentence types and'full' languages by 12 sentence types.
The construc-tions exemplified by each sentence type and theirlength are equivalent across all the languages definedby the grammar set, but the sequences of lexical cat-egories can differ.
For example, two SOV languagerenditions of The man who Bil l  likes gave Fred a4Representation f the vl/v2 parameter(s) in termsof a type constraint determining allowable functor cate-gories is discussed in more detail in Briscoe (1997b).420gen v l  n subj  obj v2 mod spec relcl adpos complEngl R F R L R F R R R R RGer R F R L L T R R R R RJap L F L L L F L L L L ?Figure 4: The Grammar Set - Ordering Parameterspresent, one with premodifying and the other post-modifying relative clauses, both with a relative pro-noun at the right boundary of the relative clause, areshown below with the differing category highlighted.Bill likes who the-man a-present Fred gaveNP8 (S\NP,)\NPo Rc\(S\NPo) NPs \Rc  NPo2NPol ((S\NPs)\NPo2)\NPolThe-man Bill likes who a-present Fred gaveNPs /Rc  NPs (S\NPs)\NPo Rc\(S\NPo) NPo2NPol ((S\NPs)\NPo2)\NPol2.2 The ParserThe parser is a deterministic, bounded-contextstack-based shift-reduce algorithm.
The parser op-erates on two data structures, an input buffer orqueue, and a stack or push down store.
The algo-rithm for the parser working with a GCG which in-cludes application, composition and permutation isgiven in Figure 5.
This algorithm finds the most left-branching derivation for a sentence type because Re-duce is ordered before Shift.
The category sequencesrepresenting the sentence types in the data for theentire language set are designed to be unambiguousrelative to thi s 'greedy', deterministic algorithm, soit will always assign the appropriate logical form toeach sentence type.
However, there are frequently al-ternative less left-branching derivations of the samelogical form.The parser is augmented with an algorithm whichcomputes working memory load during an analy-sis (e.g.
Baddeley, 1992).
Limitations of workingmemory are modelled in the parser by associating acost with each stack cell occupied uring each stepof a derivation, and recency and depth of process-ing effects are modelled by resetting this cost eachtime a reduction occurs: the working memory load(WML) algorithm is given in Figure 6.
Figure 7 givesthe right-branching derivation for Kim loves Sandy,found by the parser utilising a grammar without per-mutation.
The WML at each step is shown for thisderivation.
The overall WML (16) is higher than forthe left-branching derivation (9).The WML algorithm ranks sentence types, and1.
The  Reduce Step: if the top 2 cells of thestack are occupied,then trya) Application, if match, then apply and goto1), else b),b) Combination, if match then apply and goto1), else c),c) Permutation, if match then apply and goto1), else goto 2)2.
The Shift Step: if the first cell of the InputBuffer is occupied,then pop it and move it onto the Stack to-gether with its associated lexical syntactic at-egory and goto 1),else goto 3)3.
The  Halt  Step: if only the top cell of the Stackis occupied by a constituent of category S,then return Success,else return FailThe Match and Apply operation: if a binaryrule schema matches the categories of the top 2 cellsof the Stack, then they are popped from the Stackand the new category formed by applying the ruleschema is pushed onto the Stack.The Permutation operation: each time step lc)is visited during the Reduce step, permutation is ap-plied to one of the categories in the top 2 cells of theStack until all possible permutations of the 2 cate-gories have been tried using the binary rules.
Thenumber of possible permutation operations i  finiteand bounded by the maximum number of argumentsof any functor category in the grammar.Figure 5: The Parsing Algorithm421Stack Input Buffer Operation Step WMLKim loves Sandy 0 0Kim:NP:kim ~ loves Sandy Shift 1 1loves:(S\NP)/NP:A y,x(love' x, y) Sandy Shift 2 3Kim:NP:kim ~Sandy:NP:sandy ~ Shift 3 6loves:(S\NP)/NP:A y,x(love' x, y)Kim:NP:kim ~loves Sandy:S/NP:A x(love' x, sandy') Reduce (A) 4Kim:NP:kim ~Kim loves Sandy:S:(love' kim ~, sandy ~) Reduce (A) 5Figure 7: WML for Kim loves SandyAfter each parse step (Shift, Reduce, Halt (seeFig 5):1.
Assign any new Stack entry in the top cell (in-troduced by Shift or Reduce) a WML value of02.
Increment every Stack cell's WML value by 13.
Push the sum of the WML values of each Stackcell onto the WML-recordWhen the parser halts, return the sum of the WML-record gives the total WML for a derivationFigure 6: The WML Algorithmthus indirectly languages, by parsing each sentencetype from the exemplifying data with the associ-ated grammar and then taking the mean of theWML obtained for these sentence types.
"En-glish" with Permutation has a lower mean WMLthan "English" without Permutation, though theyare stringset-identical, whilst a hypothetical mix-ture of "Japanese" SOV clausal order with "En-glish" phrasal syntax has a mean WML which is 25%worse than that for "English".
The WML algorithmis in accord with existing (psycholinguistically-motivated) theories of parsing complexity (e.g.
Gib-son, 1991; Hawkins, 1994; Rambow and Joshi, 1994).2.3 The Parameter Setting AlgorithmThe parameter setting algorithm is an extension ofGibson and Wexler's (1994) Trigger Learning Al-gorithm (TLA) to take account of the inheritance-based partial ordering and the role of memory inlearning.
The TLA is error-driven - parameter set-tings are altered in constrained ways when a learnercannot parse trigger input.
Trigger input is de-fined as primary linguistic data which, because ofits structure or context of use, is determinately un-parsable with the correct interpretation (e.g.
Light-foot, 1991).
In this model, the issue of ambigu-ity and triggers does not arise because all sentencetypes are treated as triggers represented by p-settingschemata.
The TLA is memoryless in the sense thata history of parameter ( e)settings i not maintained,in principle, allowing the learner to revisit previoushypotheses.
This is what allows Niyogi and Berwick(1995) to formalize parameter setting as a Markovprocess.
However, as Brent (1996) argues, the psy-chological plausibility of this algorithm is doubt-ful - there is no evidence that children (randomly)move between neighbouring rammars along pathsthat revisit previous hypotheses.
Therefore, eachparameter can only be reset once during the learn-ing process.
Each step for a learner can be definedin terms of three functions: P-SETTING, GRAMMARand PARSER, as:PARSERi(GRAMMAR/(P-SETTING/(Sentence j)))A p-setting defines a grammar which in turn definesa parser (where the subscripts indicate theoutput  ofeach function given the previous trigger).
A param-eter is updated on parse failure and, if this resultsin a parse, the new setting is retained.
The algo-rithm is summarized in Figure 8.
Working mem-ory grows through childhood (e.g.
Baddeley, 1992),and this may assist learning by ensuring that triggersentences gradually increase in complexity throughthe acquisition period (e.g.
Elman, 1993) by forcingthe learner to ignore more complex potential triggersthat occur early in the learning process.
The WMLof a sentence type can be used to determine whetherit can function as a trigger at a particular stage inlearning.422Data: {$1, S2, ... Sn}unleSsPARSERi(  GRAMMARi (P -SETT INGi (S j  )) ) : Successthenp-settingj = UPDATE(p-settings)unlessPARSERj (GRAMMARj (P-SETTINGj (Sj))) -- SuccessthenRETURN p-settings/elseRETURN p-settingsyUpdate:Reset the first (most general) default or unset pa-rameter in a left-to-right search of the p-set accord-ing to the following table:Input: D 1 D0 ?
?
\]Output: R 0 R 1 ?
1/0 (random) I (where 1= T/L  and 0 = F/R)Figure 8: The Learning Algorithm3 The  S imulat ion  Mode lThe computational simulation supports the evolu-tion of a population of Language Agents (LAgts),similar to Holland's (1993) Echo agents.
LAgts gen-erate and parse sentences compatible with their cur-rent p-setting.
They participate in linguistic inter-actions which are successful if their p-settings arecompatible.
The relative fitness of a LAgt is a func-tion of the proportion of its linguistic interactionswhich have been successful, the expressivity of thelanguage(s) spoken, and, optionally, of the meanWML for parsing during a cycle of interactions.
Aninteraction cycle consists of a prespecified numberof individual random interactions between LAgts,with generating and parsing agents also selected ran-domly.
LAgts which have a history of mutually suc-cessful interaction and high fitness can 'reproduce'.A LAgt can 'live' for up to ten interaction cycles,but may 'die' earlier if its fitness is relatively low.
Itis possible for a population to become xtinct (forexample, if all the initial LAgts go through ten in-teraction cycles without any successful interactionoccurring), and successful populations tend to growat a modest rate (to ensure a reasonable proportionof adult speakers is always present).
LAgts learnduring a critical period from ages 1-3 and reproducefrom 4-10, parsing and/or generating any languagelearnt throughout their life.During learning a LAgt can reset genuine param-Variables Typical ValuesPopulation Size 32Interaction Cycle 2K InteractionsSimulation Run 50 CyclesCrossover Probability 0.9Mutation Probability 0Learning memory limited yescritical period yesFigure 9: The Simulation Options(Cost/Benefits per sentence (1-6); summed for eachLAgt at end of an interaction cycle and used to cal-culate fitness functions (7-8)):1.
Generate cost: 1 (GC)2.
Parse cost: !
(PC)3.
Generate subset language cost: 1 (GSC)4.
Parse failure cost: 1 (PF)5.
Parse memory cost: WML(st)6.
Interaction success benefit: 1 (SI)7.
Fitness(WML): SI  GC ?
GC+PC X GC+GSC X8.
Fitness(-~WML): sI cc  GC+PC X CC.-\[-GSCFigure 10: Fitness Functionseters which either were unset or had default settings'at birth'.
However, p-settings with an absolutevalue (principles) cannot be altered during the life-time of an LAgt.
Successful LAgts reproduce at theend of interaction cycles by one-point crossover of(and, optionally, single point mutation of) their ini-tial p-settings, ensuring neo-Darwinian rather thanLamarckian inheritance.
The encoding of p-settingsallows the deterministic recovery of the initial set-ting.
Fitness-based reproduction ensures that suc-cessful and somewhat compatible p-settings are pre-served in the population and randomly sampled inthe search for better versions of universal grammar,including better initial settings of genuine parame-ters.
Thus, although the learning algorithm per  seis fixed, a range of alternative learning procedurescan be explored based on the definition of the initalset of parameters and their initial settings.
Figure 9summarizes crucial options in the simulation givingthe values used in the experiments reported in ?4and Figure 10 shows the fitness functions.4234 Exper imenta l  Resu l t s4.1 Effectiveness of Learning ProceduresTwo learning procedures were predefined - a defaultlearner and an unset learner.
These LAgts were ini-tialized with p-settings consistent with a minimal in-herited CGUG consisting of application with NP andS atomic categories.
All the remaining p-settingswere genuine parameters for both learners.
The un-set learner was initialized with all unset, whilst thedefault learner had default settings for the parame-ters gendir  and subjd i r  and argorder  which spec-ify a minimal SVO right-branching grammar, as wellas default (off) settings for comp and perm whichdetermine the availability of Composition and Per-mutation, respectively.
The unset learner epresentsa 'pure' principles-and-parameters learner.
The de-fault learner is modelled on Bickerton's bioprogramlearner.Each learner was tested against an adult LAgtinitialized to generate one of seven full lan-guages in the set which are close to an at-tested language; namely, "English" (SVO, predom-inantly right-branching), "Welsh" (SVOvl, mixedorder), "Malagasy" (VOS, right-branching), "Taga-log" (VSO, right-branching), "Japanese" (SOV,left-branching), "German" (SOVv2, predominantlyright-branching), "Hixkaryana" (OVS, mixed or-der), and an unattested full OSV language with left-branching syntax.
In these tests, a single learner in-teracted with a single adult.
After every ten interac-tions, in which the adult randomly generated a sen-tence type and the learner attempted to parse andlearn from it, the state of the learner's p-settings wasexamined to determine whether the learner had con-verged on the same grammar as the adult.
Table 1shows the number of such interaction cycles (i.e.
thenumber of input sentences to within ten) required byeach type of learner to converge on each of the eightlanguages.
These figures are each calculated from100 trials to a 1% error rate; they suggest hat, ingeneral, the default learner is more effective thanthe unset learner.
However, for the OVS language(OVS languages represent 1.24% of the world's lan-guages, Tomlin, 1986), and for the unattested OSVlanguage, the default (SVO) learner is less effective.So, there are at least two learning procedures in thespace defined by the model which can converge withsome presentation orders on some of the grammarsin this set.
Stronger conclusions require either ex-haustive experimentation r theoretical analysis ofthe model of the type undertaken by Gibson andWexler (1994) and Niyogi and Berwick (1995).Unset Default NoneWML 15 39 26-~WML 34 17 29Table 2: Overall preferences for parameter types4.2 Evolution of Learning ProceduresIn order to test the preference for default versus un-set parameters under different conditions, the fiveparameters which define the difference between thetwo learning procedures were tracked through an--other series of 50 cycle runs initialized with either 16default learning adult speakers and 16 unset learningadult speakers, with or without memory-limitationsduring learning and parsing, speaking one of theeight languages described above.
Each condition wasrun ten times.
In the memory limited runs, defaultparameters came to dominate some but not all pop-ulations.
In a few runs all unset parameters dis-appeared altogether.
In all runs with populationsinitialized to speak "English" (SVO) or "Malagasy"(VOS) the preference for default settings was 100%.In 8 runs with "Tagalog" (VSO) the same preferenceemerged, in one there was a preference for unset pa-rameters and in the other no clear preference.
How-ever, for the remaining five languages there was nostrong preference.The results for the runs without memory limita-tions are different, with an increased preference forunset parameters across all languages but no clear100% preference for any individual language.
Ta-ble 2 shows the pattern of preferences which emergedacross 160 runs and how this was affected by thepresence or absence of memory limitations.To test whether it was memory limitations duringlearning or during parsing which were affecting theresults, another series of runs for "English" was per-formed with either memory limitations during learn-ing but not parsing enabled, or vice versa.
Memorylimitations during learning are creating the bulk ofthe preference for a default learner, though thereappears to be an additive effect.
In seven of theten runs with memory limitations only in learning, aclear preference for default learners emerged.
In fiveof the runs with memory limitations only in parsingthere appeared to be a slight preference for defaultsemerging.
Default learners may have a fitness ad-vantage when the number of interactions required tolearn successfully is greater because they will tend toconverge faster, at least to a subset language.
Thiswill tend to increase their fitness over unset learnerswho do not speak any language until further into the424Learner LanguageSVO SVOvl VOS VSO SOV SOVv2 OVS OSVUnset 60 80 70 80 70 70 70 70Default 60 60 60 60 60 60 80 70Table 1: Effectiveness of Two Learning Procedureslearning period.The precise linguistic environment of adaptationdetermines the initial values of default parameterswhich evolve.
For example, in the runs initializedwith 16 unset learning "Malagasy" VOS adults and16 default (SVO) learning VOS adults, the learn-ing procedure which dominated the population wasa variant VOS default learner in which the valuefor sub jd i r  was reversed to reflect the position ofthe subject in this language.
In some of theseruns, the entire population evolved a default sub-jd i r  'right' setting, though some LAgts always re-tained unset settings for the other two ordering pa-rameters, gendir  and argo, as is illustrated in Fig-ure 11.
This suggests that if the human language fac-ulty has evolved to be a right-branching SVO defaultlearner, then the environment of linguistic adapta-tion must have contained a dominant language fullycompatible with this (minimal) grammar.4.3 Emergence  of  Language and LearnersTo explore the emergence and persistence of struc-tured language, and consequently the emergence ofeffective learners, (pseudo) random initialization wasused.
A series of simulation runs of 500 cycles wereperformed with random initialization of 32 LAgts'p-settings for any combination of p-setting values,with a probability of 0.25 that a setting would be anabsolute principle, and 0.75 a parameter with unbi-ased allocation for default or unset parameters andfor values of all settings.
All LAgts were initializedto be age 1 with a critical period of 3 interactioncycles of 2000 random interactions for learning, amaximum age of 10, and the ability to reproduce bycrossover (0.9 probability) and mutation (0.01 prob-ability) from 4-10.
In around 5% of the runs, lan-guage(s) emerged and persisted to the end of therun.Languages with close to optimal WML scores typi-cally came to dominate the population quite rapidly.However, sometimes sub-optimal languages were ini-tially selected and occasionally these persisted e-spite the later appearance of a more optimal lan-guage, but with few speakers.
Typically, a minimalsubset language dominated - although full and inter-mediate languages did appear briefly, they did notsurvive against less expressive subset languages witha lower mean WML.
Figure 12 is a typical plot ofthe emergence (and extinction) of languages in oneof these runs.
In this run, around 10 of the initialpopulation converged on a minimal OVS languageand 3 others on a VOS language.
The latter is moreoptimal with respect o WML and both are of equalexpressivity so, as expected, the VOS language ac-quired more speakers over the next few cycles.
A fewspeakers also converged on VOS-N, a more expres-sive but higher WML extension of VSO-N-GWP-COMP.
However, neither this nor the OVS languagesurvived beyond cycle 14.
Instead a VSO languageemerged at cycle 10, which has the same minimalexpressivity of the VOS language but a lower WML(by virtue of placing the subject before the object)and this language dominated rapidly and eclipsed allothers by cycle 40.In all these runs, the population settled on sub-set languages of low expressivity, whilst the percent-age of absolute principles and default parameters in-creased relative to that of unset parameters (mean% change from beginning to end of runs: +4.7, +1.5and -6.2, respectively).
So a second identical set often was undertaken, except that the initial popula-tion now contained two SOV-V2 "German" speak-ing unset learner LAgts.
In seven of these runs, thepopulation fixed on a full SOV-V2 language, in twoon the intermediate subset language SOV-V2-N, andin one on the minimal subset language SOV-V2-N-GWP-COMP.
These runs suggest hat if a full lan-guage defines the environment of adaptation thena population of randomly initialized LAgts is morelikely to converge on a (related) full language.
Thus,although the simulation does not model the devel-opment of expressivity well, it does appear that itcan model the emergence of effective learning pro-cedures for (some) full languages.
The pattern oflanguage mergence and extinction followed that ofthe previous series of runs: lower mean WML lan-guages were selected from those that emerged uringthe run.
However, often the initial optimal SVO-V2itself was lost before enough LAgts evolved capableof learning this language.
In these runs, changesin the percentages of absolute, default or unset p-settings in the population show a marked difference:425100/80 -"': /i60 '',,": !
'/40 V2O0 i0 I0I; i i :, , .
/ " '_  .
, - ' , .
.
, , '  " .
.
. '
, , .
.
- ,  .. , , '  ,:' I /"G0g"~ ~di~" ......."G0argo" - ...."G0subjdir .
.
.
.
.
.
.f ,,v, j i / " '~'vi, ,/i\},VIi ~ aI \q9 ,fI I I I20 30 40 50 60 70Interaction CyclesQ.
q)"5Figure 11: Percentage of each default ordering pa-rameter454035302520151050i ; i i IL i"aa-S?"
- -"GB-OVS-N-P-C .
.
.
.
.
.k "ge-y~,o-N .......~ ,., ~GS- ,VOS-N ' .
,  .
..........""GB-VOS-N-~WI~-COMP" k-:::.
""G,8 -VSOrN:GWP-COMP"  - .
.
.
.
'l !ti-/~ i i ; i !
zi! '
i  !
z ' !
/ ~ 1 1  \ '  i zi V - " "  "........ i ~ L / \I ' -V" '~' : ' (  "'''', i I \ i5 10 15 20 25 30Interaction CyclesI / - ' x ,  I I35 40 45 50Figure 12: Emergence of language(s)the mean number of absolute principles declined by6.1% and unset parameters by 17.8%, so the num-ber of default parameters ose by 23.9% on averagebetween the beginning and end of the 10 runs.
Thismay reflect he more complex linguistic environmentin which (incorrect) absolute settings are more likelyto handicap, rather than simply be irrelevant to, theperformance of the LAgt.5 Conc lus ionsPartially ordering the updating of parameters canresult in (experimentally) effective learners with amore complex parameter system than that studiedpreviously.
Experimental comparison of the default(SVO) learner and the unset learner suggests thatthe default learner is more efficient on typologicallymore common constituent orders.
Evolutionary sim-ulation predicts that a learner with default param-eters is likely to emerge, though this is dependentboth on the type of language spoken and the pres-ence of memory limitations during learning and pars-ing.
Moreover, a SVO bioprogram learner is onlylikely to evolve if the environment contains a domi-nant SVO language.The evolution of a bioprogram learner is a man-ifestation of the Baldwin Effect (Baldwin, 1896) -genetic assimilation of aspects of the linguistic envi-ronment during the period of evolutionary adapta-tion of the language learning procedure.
In the caseof grammar learning this is a co-evolutionary processin which languages (and their associated grammars)are also undergoing selection.
The WML account ofparsing complexity predicts that a right-branchingSVO language would be a near optimal selection ata stage in grammatical development when complexrules of reordering such as extraposition, scramblingor mixed order strategies uch as vl  and v2 hadnot evolved.
Briscoe (1997a) reports further exper-iments which demonstrate language selection in themodel.Though, simulation can expose likely evolution-ary pathways under varying conditions, these mighthave been blocked by accidental factors, such as ge-netic drift or bottlenecks, causing premature fixa-tion of alleles in the genotype (roughly correspond-ing to certain p-setting values).
The value of thesimulation is to, firstly, show that a bioprogramlearner could have emerged via adaptation, and sec-ondly, to clarify experimentally the precise condi-tions required for its emergence.
Since in manycases these conditions will include the presence ofconstraints (working memory limitations, expressiv-ity, the learning algorithm etc.)
which will remaincausally manifest, further testing of any conclusionsdrawn must concentrate on demonstrating the ac-426curacy of the assumptions made about such con-straints.
Briscoe (1997b) evaluates the psychologicalplausibility of the account of parsing and workingmemory.Re ferencesBaddeley, A.
(1992) 'Working Memory: the interfacebetween memory and cognition', J. of CognitiveNeuroscience, vol.4.3, 281-288.Baldwin, J.M.
(1896) 'A new factor in evolution',American Naturalist, vol.30, 441-451.Bickerton, D. (1984) 'The language bioprogram hy-pothesis', The Behavioral and Brain Sciences,vol.
7.2, 173-222.Bouma, G. and van Noord, G (1994) 'Constraint-based categorial grammar', Proceedings of the32nd Assoc.
for Computational Linguistics, LasCruces, NM, pp.
147-154.Brent, M. (1996) 'Advances in the computationalstudy of language acquisition', Cognition, vol.
61,1-38.Briscoe, E.J.
(1997a, submitted) 'Language Acquisi-tion: the Bioprogram Hypothesis and the Bald-win Effect', Language,Briscoe, E.J.
(1997b, in prep.)
Working memory andits influence on the development of human lan-guages and the human language faculty, Univer-sity of Cambridge, Computer Laboratory, m.s..Chomsky, N. (1981) Government and Binding, Foris,Dordrecht.Clark, R. (1992) 'The selection of syntactic knowl-edge', Language Acquisition, vol.2.2, 83-149.Elman, J.
(1993) 'Learning and development in neu-ral networks: the importance of starting small',Cognition, vol.48, 71-99.Gibson, E. (1991) A Copmutational Theory of Hu-man Linguistic Processing: Memory Limitationsand Processing Breakdown, Doctoral disserta-tion, Carnegie Mellon University.Gibson, E. and Wexler, K. (1994) 'Triggers', Lin-guistic Inquiry, vol.25.3, 407-454.Greenberg, J.
(1966) 'Some universals of grammarwith particular eference to the order of mean-ingflll elements' in J. Greenberg (ed.
), Univer-sals of Grammar, MIT Press, Cambridge, Ma.,pp.
73-113.Hawkins, J.A.
(1994) A Performance Theory ofOrder and Constituency, Cambridge UniversityPress, Cambridge.Hoffman, B.
(1995) The Computational Analysis ofthe Syntax and Interpretation of 'Free' Word Or-der in Turkish, PhD dissertation, University ofPennsylvania.Hoffman, B.
(1996) 'The formal properties of syn-chronous CCGs', Proceedings o\] the ESSLLI For-mal Grammar Conference, Prague.Holland, J.H.
(1993) Echoing emergence: objectives,rough definitions and speculations for echo-classmodels, Santa Fe Institute, Technical Report 93-04-023.Joshi, A., Vijay-Shanker, K. and Weir, D. (1991)'The convergence of mildly context-sensitivegrammar formalisms' in Sells, P., Shieber, S. andWasow, T.
(ed.
), Foundational Issues in NaturalLanguage Processing, MIT Press, pp.
31-82.Lascarides, A., Briscoe E.J.
, Copestake A.A andAsher, N. (1995) 'Order-independent a d persis-tent default unification', Linguistics and Philos-ophy, vo1.19.1, 1-89.Lascarides, A. and Copestake A.A. (1996, submit-ted) 'Order-independent typed default unifica-tion', Computational Linguistics,Lightfoot, D. (1991) How to Set Parameters: Argu-ments from language Change, MIT Press, Cam-bridge, Ma..Niyogi, P. and Berwick, R.C.
(1995) 'A markovlanguage learning model for finite parameterspaces', Proceedings of the 33rd Annual Meet-ing of the Association for Computational Lin-guistics, MIT, Cambridge, Ma..Rambow, O. and Joshi, A.
(1994) 'A processingmodel of free word order languages' in C. Clifton,L.
Frazier and K. Rayner (ed.
), Perspectives onSentence Processing, Lawrence Erlbaum, Hills-dale, NJ., pp.
267-301.Steedman, M. (1988) 'Combinators and grammars'in R. Oehrle, E. Bach and D. Wheeler (ed.
), Cat-egorial Grammars and Natural Language Struc-tures, Reidel, Dordrecht, pp.
417-442.Tomlin, R. (1986) Basic Word Order: FunctionalPrinciples, Routledge, London.Wood, M.M.
(1993) Categorial-Grammars, Rout-ledge, London.427
