Dialogue Management in the Agreement Negotiation Process:A Model that Involves Natural ReasoningMare KOITInstitute of Computer Science, Tartu UniversityLiivi 250409 Tartu, Estoniakoit@ut.eeHaldur ~)IMDept.
of General Linguistics, Tartu UniversityTiigi 7851014 Tartu, Estoniahoim@psych.ut.eeAbstractIn the paper we describe an approach todialogue management in the agreementnegotiation where one of the central roles isattributed to the model of natural humanreasoning.
The reasoning model consists ofthe model of human motivational sphere,and of reasoning algorithms.
The reasoningmodel is interacting with the model ofcommunication process.
"/'he latter isconsidered as rational activity where centralrole play the concepts of communicativestrategies and tactics.IntroductionSeveral researches have modelled the process ofargument negotiation in cooperative dialoguewhere one participant makes a proposal toanother participant and as the result ofnegotiation this is accepted or rejected.Chu-Carroll and Carberry (1998) present acooperative response-generation model as arecursive cycle Propose-Evaluate-Modify.
Theyconcentrate on dialogues of information sharingand negotiation.
An information sharingdialogue is started, when the agent recognised aturn of his/her partner as a proposal, but does nothave enough information to decide whether toaccept it or not.
A negotiation dialogue isstarted, when the agent concludes that theproposal is in conflict with his/her beliefs andpreferences, i.e.
tends to reject it.Heeman and Hirst (1995) model cooperation bythe cycle Present-Judge-Refashion.
They usetwo levels of modelling - planning andcooperation.
On the first level utterances aregenerated and interpreted, on the second levelthe cooperation of agents is modelled, relating itto agent's mental states and planning processes.The Shared Plans cooperation model deals withplanning processes in which participate multipleagents, see Lochbaurn (1998).
The modelconcentrates on group tasks that can be dividedinto separate, but interacting subtasks, and thecentral problem is coordination of intentions andgoals of partners.Di Eugenio et al (2000) present a modelBalanceProposeDispose: first, the relevantinformation concerning the task is consideredand discussed, then a proposal is made and,lastly, the decision concerning the proposal ismade - it is accepted or rejected.In our model we depart from the same type ofsituation.
One agent, A, addresses another agent,B, with the intention that B will carry out anaction D. After some negotiation, B agrees orrejects the proposal.In this paper we concentrate on the problemsconnected with modelling participants asconversation agents who are able to participatein negotiation i  the form of natural dialogue -dialogue that is carried out in natural anguageand according to the rules of humancommunication.Such a dialogue can be considered as rationalbehaviour which is based on beliefs, wants andintentions of agents, at the same time beingrestricted by their resources, ee Jokinen (1995),Webber (2000).
Conversation agent is a kind ofintelligent agent - a computer program that isable to communicate with humans as anotherhuman being.As it is generally accepted, in a model ofconversation agent it is necessary to represent itscognitive states as well as cognitive processes.102One of the most well-known models of this typeis the BDI model, see Allen (1994).Our main point in this paper is that the generalconcepts of cognitive states and processes usedin BDI-type models hould be extended in orderto include certain factors from humanmotivational sphere and certain social principlesin order to guarantee naturalness of dialogues ofthe type we are concerned with.
This isespecially important in connection with the factthat interest in modelling cooperative dialogueswhere partners are pursuing a common goal hasconsiderably increased in recent years.
On theone hand, this is connected with rapid spreadingof Internet-based services.
On the other hand,the interest in models of full natural dialoguederives from the possibility of building speechinterfaces with different knowledge anddatabases, ee Dybkjaer (2000).
Both of thesedevelopments broaden the concept ofnaturalness of dialogue considerably and presentto it much stronger requirements concerning itsempirical adequacy as it has been generallyaccepted thus far.1 Model of Conversation AgentIn our model a conversation agent, A, is aprogram that consists of 6 (interacting) modules:A = (PL, PS, DM, INT, GEN, LP),where PL - planner, PS - problem solver, DM -dialogue manager, INT - interpreter, GEN -generator, LP - linguistic processor.
PL directsthe work of both DM and PS, where DMcontrols communication process and PS solvesdomain-related tasks.
The task of INT is to makesemantic analysis of partner's utterances andthat of GEN is to generate semanticrepresentations of agent's own contributions.
LPcarries out linguistic analysis and generation.Conversation agent uses in its work goal baseGB and knowledge base KB.
In our model, KBconsists of 4 components:KB = (KBw, KBL, KBD, KBs),where KBw contains world knowledge, KBL -linguistic knowledge, KBD - knowledge aboutdialogue and KBs - knowledge about interactingsubjects.
For instance, KBD contains definitionsof communicative acts, turns and transactions(declarative knowledge), and algorithms that areapplied to reach communicative goals -communicative strategies and tactics (proceduralknowledge); KBs contains knowledge aboutevaluative dispositions of participants towardsthe world (e.g.
what do they consider as pleasantor unpleasant, useful or harmful), and, on theother hand, algorithms that are used to generateplans for acting on the world.A necessary precondition of a communicativeinteraction is existence of shared (mutual)knowledge of interacting agents.
This concernsgoal bases as well as all types of knowledgebases; the intersections of the correspondingbases of interacting agents A and B cannot beempty: GB g n GB a ~:~, KBAw n KBBw ~,KBAL n KBBL ~O, KI3AD n KBBD ~,  KB ABSKBBs :.7~:~, KBBAs ("h KBAs -~:~.In this paper we will consider a specific type ofdialogue where the communicative goal of agentA is to get agent B to agree to carry out anaction D - so-called agreement negotiationdialogue.
We will concentrate here on dialoguemanagement i  such kind of interaction, i.e.
onthe functioning of the module DM.2 Dialogue Management2.1 Reasoning ModelA dialogue participant chooses his/her esponsesto the parter's communicative acts as a result ofcertain reasoning process.
After A has made B aproposal to do D, B can respond with agreementor rejection, depending on the result of his/herreasoning.Because we consider the model of naturalhuman reasoning as one of the importantcomponents in attaining naturalness of dialogueas a whole, we will discuss our model ofreasoning in some detail.
From the point of viewof practical NLP the approach we will presentbelow may seem too abstract.
But without solidtheoretical basis it will appear impossible toguarantee naturalness ofdialogues carried out bycomputers with human users.
We think that themodel we describe here can be taken as a basisfor the corresponding discussion.Our model is not based on any scientific theoryof how human reasoning proceeds; our aim is tomodel a "naive theory of reasoning" whichhumans follow in everyday life when trying tounderstand, predict and influence other persons'decisions and behavior, see Koit and C)im(2000).
The reasoning model consists of twofunctionally linked parts: 1) a model of humanmotivational sphere; 2) reasoning schemes.103In the motivational sphere three basic factorsthat regulate reasoning of a subje, ct concerning Dare differentiated.
First, subject may wish to doD, if pleasant aspects of D for him/heroverweight unpleasant ones; .second, subjectmay fmd reasonable to do D, if D is needed toreach some higher goal, and useful aspects of Doverweight harmful ones; and tlfird, subject canbe in a situation where he/she must (is obliged)to do D - if not doing D will lead to some kindof punishment.
We call these; factors wish-,needed- and must-factors, respectively.For instance, in reasoning about some action D(e.g.
proposed by another agent), an agent as anindividual subject ypically starts with checkinghis/her wish-factor, i.e.
whether D's pleasantaspects overweight unpleasant ones.
If thisholds, then the subject checks his/her esources,and if these exist, proceeds to other positive andnegative aspects of D: its usefulness andharmfulness, and if D is prohibited, then alsopossible punishment(s).
If the positive aspects insum overweight negative ones, the resultingdecision will be to do D, otherwise - not to doD.There can exist other typical situations.
If theagent is an "official" person, or a group ofsubjects formed to fulfil certain tasks and/or topursue certain pre-established goal(s), thentypically the starting point of reasoning isneeded- and/or mast-factor.This means that there exist certain generalprinciples that determine how the reasoningprocess proceeds.
These principles depend, inpart, on the type of the reasoning agent.
Beforestarting to construct a concrete reasoning modelthe types of agents involved should beestablished.
In our implementation the agent issupposed to be a "simple" human being and theactions under consideration are from everydaylife.
In this case as examples of such principlesused in our model we can present the followingones.
For more details, see Oim (1996).P1.
People prefer pleasant (more pleasant)states to unpleasant (less pleasant) ones.P2.
People don "t take an action of which theydon't assume that its consequence will be apleasant (useful) situation, or avoidance of anunpleasant (harmful) situation.The following principles illustrate more concrete(operational) rules.P3.
In assessing an action D the values of(internal - wish- and needed-)factors arechecked before the external (must-) factors.P4.
I f  D is found pleasant enough (i.e.
D'spleasant aspects overweight unpleasant ones),then the needed- and must-factors will first bechecked from the point of view of their negativeaspects ("to what harmful consequences orpunishments D would lead?
"~).The rule P4 explains, for example, why inFigure 1 step 1 is immediately followed by step2.The weights of different aspects of D(pleasantness, unpleasantness, usefulness,harmfulness, punishment for doing a prohibitedaction or not-doing an obligatory action) must besummed up in some way.
Thus, in acomputational model weights must havenumerial values.
In reality people do not operatewith numbers but, rather, with some fuzzy sets.On the other hand, existence of certain scalesalso in human everyday reasoning is apparent.For instance, for the characterisation of pleasantand unpleasant aspects of some action there arespecific words: enticing, delighOCul, enjoyable,attractive, acceptable, unattractive, displeasing,repulsive etc.
Each of these adjectives can beexpressed quantitatively.
This presupposesempirical studies, though.We have represented the model of motivationalsphere by the following vector of weights:w A = (w(resourcesAol), w(pleasAm),w(unpleasAm), w(useAm), w(harmAm),w(obligatoryAm), w(prohibitedAm),w(punishgm), w(punishAnot.Di),...,w(resourcesADn), w(pleasAon), w(unpleasADn),W(useAD~), w(harmAo,), w(obligatoryAD,),w(prohibitedADn), w(punishAm),W(punishAnot.Dn)).Here D~, ..., Dn represent human actions;W(resourcesADi)=I, if A has resources necessaryto do Di (otherwise 0); w(obligatoryAsi)=l, if Diis obligatory for A (otherwise 0);w(prohibitedADi)=l, if Di is prohibited for A(otherwise 0).
The values of other weights arenon-negative natural numbers.The second part of the reasoning model consistsof reasoning schemas, that supposedly regulatehuman action-oriented reasoning.
A reasoningscheme represents teps that the agent goesthrough in his reasoning process; these consist incomputing and comparing the weights of104different aspects of D; and the result is thedecision to do or not to do D.Figure 1 presents the reasoning scheme thatdeparts from the wish of a subject o do D.The scheme also illustrates one of the generalprinciples referred to above.
It explains the orderthe steps are taken by the reasoning agent: if asubject is in a state where he/she wishes to do D,then he/she checks first the harmful/usefulaspects of D, and after this proceeds to aspectsconnected with possible punishments.Presuppos i t ion :w(pleas)  > w(unp leas) .i)for do ing  D?If not  then not  to do D.2) I s  w(p leas)  > w(unpleas)  +w(harm)?If not  then go to step 6.3) Is D proh ib i ted?If not  then to do D.4) Is w(pleas)  > w(unpleas)  +w(harm) + w(punishD)?If yes then to do D.5) Is w(pleas)  + w(use) >w(unpleas)  + w(harm) +w(punish~)?If yes  then to do D elsenot  to do D.6) Is w(p leas)  + w(use) >w(unpleas)  + w(harm)?If yes  then go to step 9.7) Is D ob l igatory?If not  then not  to do D.8) Is w (pleas) + w (use) +w (punishnot.
~) > w (unpleas) +w (harm) ?If yes  then to do D elsenot  to do D.9) Is D proh ib i ted?If not  then to do D.i0) Is w (pleas) + w(use) >w (unpleas) + w (harm) +w (punish~) ?Are there  enough resourcesIf yes  then to do D elsenot  to do D.Figure 1.
The reasoning procedure that departsfrom the wish of  a subject to do D.The prerequisite for triggering this reasoningprocedure is w(pleas) > w(unpleas), which isbased on the following assumption: if a personwishes to do something, then he/she assumesthat the pleasant aspects of D (including itsconsequences) overweigh its unpleasant aspects.The same kinds of reasoning schemes areconstructed for the needed- and must-factors.The reasoning model is connected with thegeneral model of conversation agent in thefollowing way.
First, the planner PL makes useof reasoning schemes and second, the KBscontains the vector w A (A's subjectiveevaluations of all possible actions) as well asvectors w AB (A's beliefs concerning B'sevaluations, where B denotes agents A maycommunicate with).
The vector w As do notrepresent truthful knowledge, it is used as apartner model.When comparing our model with BDI model,then belier are represented byknowledge of theconversation agent with reliability less than 1;desires are generated by the vector of weightsWA; and intentions correspond togoals in GB.
Inaddition to desires, from the weights vector wealso can derive some parameters of themotivational sphere that are not explicitlycovered by the basic BDI model: needs,obligations and prohibitions.
Some wishes orneeds can be stronger than others: if w(pleasADi)- W(unpleasAoi) > w(pleasAoj) - w(unpleasAt~),then the wish to do Di is stronger than the wishto do Dj.
In the same way, some obligations(prohibitions) can be stronger than others,depending on the weight of the correspondingpunishment.
I  should be mentioned that addingobligations to the standard BDI model is notnew.
Traum and Allen (1994) show howdiscourse obligations can be used to account in anatural manner for the connection between aquestion and its answer in dialogue and howobligations can be used along with other parts ofthe discourse context to extend the coverage of adialogue system.2.2 Communicative Strategies andTacticsKnowledge about dialogue KBD, which is usedby the Dialogue Manager, consists of twofunctional parts: knowledge of the regularities ofdialogue, and rules of constructing andcombining speech acts.The top level concept of dialogue rules in ourmodel is communicative strategy.
This conceptis reserved for such basic communication typesas information exchange, directive dialogue,105phatic communication, etc.
On the moreconcrete level, the conversation agent can realisea communicative strategy by means of severalcommunicative tactics; this concept moreclosely corresponds to the: concept ofcommunicative strategy as us~l in some otherapproaches, ee e.g.
Jokinen (1996).
In the caseof directive communication (which is thestrategy we are interested in) the agent A can usetactics of enticing, persuading, threatening.
Inthe case of enticing, A stresses pleasant aspects,in the case of persuading - usel~ aspects of Dfor B; in the case of ordering A addressesobligations of B, in the case of threatening Aexplicitly refers to possible punishment for notdoing D.Which one of these tactics A chooses dependson several factors.
There is one: relevant aspectof human-human communication which isrelatively well studied in pragmatics of humancommunication a d which we have included inour model as the concept of communicativespace.Communicative space is defined by a number ofcoordinates that characterise the relationships ofparticipants in a communicative ncounter.Communication can be collaborative orconfrontational, personal or impersonal; it canbe characterised by the social distance betweenparticipants; by the modality (friendly, ironic,hostile, etc.)
and by intensity (peaceful,vehement, etc.).
Just as in case of motivations ofhuman behaviour, people have an intuitive,"naive theory" of these coordinates.
Thisconstitutes a part of the social conceptualisationof communication, and it also should not beignored in serious attempts to model naturalcommunication in NLP systems.In our model the choice of a communicativetactics depends on the "point" of thecommunicative space in which the participantsplace themselves.
The values of the coordinatesare again given in the form of numerical values.The communicative strategy can be presented asan algofithra (Figure 2).Figure 3 presents a tactic of  enticement.In our model there are three differentcommunicative tactics that A can use within theframes of the directive communicative strategy:those of enticement, persuasion and threatening.Each communicative tactic constitutes aprocedure for compiling aturn in the ongoingdialogue.i) Choose the communicat ivetactic.2) Implement the tactic togenerate an expression (informthe partner of the communicat ivegoal).3) Did the partner agree to doD?
If yes then f inish (thecommunicat ive goal has beenreached).4) Give up?
If yes then f inish(the communicat ive goal has notbeen reached).5) Change the communicat ivetactic?
If yes then choose thenew tactic.6) Implement the tactic togenerate an expression.
Go tostep 3.Figure 2.
Communicative strategy used by theinitiator of communication.i) If wB(resources)=0 thenpresent a counterargument inorder to point at the presenceof possible resources or at theposs ib i l i ty  to gain them.2) If w s(harm) > w as(harm) thenpresent a counterargument inorder to downgrade the value ofharm.3) If wB(obligatory)=l &w B (punish .... o) < w~ (punishno~-~) thenpresent a counterargument inorder to decrease the weight ofthe punishment.4) If wB (prohibited) =l &w ~(punis~) > w ~(punis~)  thenpresent a counterargument inorder to downgrade the weight ofthe punishment.5) If wB(unpleas) > w~(unpleas)thenpresent a counterargument inorder to downgrade the value ofthe unpleasant aspects of D.6) Present a counterargument inorder to stress the p leasantaspects of D.Figure 3.
A's tactics of enticement106The tactic of enticement consists in increasingB's wish to do D; the tactic of persuasionconsists in increasing B's belief of the usefulnessof D for him/her, and the tactic of threateningconsists in increasing B's understanding thathe/she must do D.Communicative tactics are directly related to thereasoning process of the partner.
I rA is applyingthe tactics of enticement he/she should be able toimagine the reasoning process in B that istriggered by the input parameter wish.
If Brefuses to do D, then A should be able to guessat which point the reasoning of B went into the"negative branch", in order to adequatelyconstruct his/her eactive turn.Analogously, the tactic of persuasion is relatedto the reasoning process triggered by the needed-parameter, and the threatening tactic is related tothe reasoning process triggered by the must-parameter.
For more details see, for example,Koit (1996), Koit and 0im (1998), Koit and Oim(1999).Thus, in order to model various communicativetactics, one must know how to model the processof reasoning.2.3 Speech ActsThe minimal communicative unit in our model isspeech act (SA).
In the implementation wemakeuse of a limited number of SAs therepresentational formalism of which is flames.Figure 4 presents the frame of SA Proposal inthe context of co-operative interaction.
OtherSAs are represented in the same form.
Each SAcontains a static (declarative) and a dynamic(procedural) part.
The static part consists ofpreconditions, goal, content (immediate act) andconsequences.
The dynamic part is made upfrom two kinds of procedures: 1) those that theauthor of the SA applies in the generation of acommunicative turn that contains the given SA;2) those that the addressee applies in the processof response generation.As one can see, such a two-part representationcontains also rules for combining SAs in a turn,and on the other hand, guarantees coherence ofturn-takings: when we have tagged in KBDinitiating SAs (such as Question or Proposal),then the following chain of SAs follows fromthe interpretation-generation procedures asapplied by participants.PROPOSAL (author A, rec ip ient  B,A proposes  B to do an act ion D)I. Stat ic  partSETTING(i) A has a goal G(2) A be l ieves  that B in thesame way has the goal G(3) A be l ieves  that in order  toreach G an inst rumenta l  goalG i shou ld  be reached(4) A be l ieves  that B in thesame way be l ieves  that inorder  to reach G aninst rumenta l  goal  G i shou ld  bereached(5) A be l ieves  that to a t ta inthe goal Gi B has to do D(6) A be l ieves  that B hasresources  for do ing  D(7) A be l ieves  that B wi l ldec ide to do DGOAL:  B dec ides  to  do DCONTENT:  A in fo rms B thathe /she  w ishes  B to  do DCONSEQUENCES(i) B knows the SETTING, GOALand CONTENT(2) A knows that B knows theSETTING, GOAL and CONTENTII.
Dynamic  partGenerating procedures (A'sposs ib i l i t i es  to bu i ld  h i s /herturn that conta ins Proposal  asthe dominant  SA).A has Goal G; A be l ieves  that Balso has Goal  G; A be l ieves  thatin order  to reach G, Gi shou ld  bereached; A has dec ided  toformulate this as Proposal  to Bto do D.Procedures (before fo rmulat ingthe turn) cons ist  in check ingwhether  the precond i t ions  ofproposa l  ho ld  and in mak ingdec is ions about in format ion  tobe added in the turn:- in case of (2) : is Gactua l i sed  in B?
If not, thenactua l i se  it by adding SAInform;- in case of (4) : does Bbe l ieve  that in order  toreach B, G~ shou ld  be reachedfirst.
If not, then add SAExp lanat ion  (Argument);- in case of (6) : if A is notsure that B has resources  forD, then add Quest ion;- in case of (7) : if A is notsure that B wi l l  agree to do107D (for this A shou ld  modelB's reasoning)  , then addArgument .Procedures of interpretation-generation(B's poss ib i l i t i es  to react  toproposal )  are s tar ted  after  Bhas recogn ised  SA Proposal :- in case of (2), (4), (5) : ifB does not  have  Goal  G and/orhe /she  does not  haw~ thecor respond ing  be l ie fs  and Ahas not  p rov ided  the neededadd i t iona l  in format ion,  thenadd Quest ion  (ask foradd i t iona l  in format ion)  ;- in case of (6) : if B doesnot  have  Resources  for D,then Re jec t  + Argument ;- in case of (7) : if thedec is ion  of B to do D (as theresu l t  of the app l i ca t ion  ofreason ing  scheme (s)) isnegat ive ,  then Re jec t  +Argument .Figure 4.
Speech act Proposal in the context ofco-operative interaction.Such a representation does not guaranteecoherence of dialogic encounters (transactions)on a more general level.
For instance, it does notcover such phenomena as topic change,inadequate responses caused bymisunderstandings; but, more importantly, alsovarious kinds of initiative overtakings.
Forinstance, after rejecting the Proposal made by A,B can, in addition to explaining the rejection byArgument, initiate various "compensatory"communicative activities.
Such things arenormal in human co-operative interaction andthey are regulated by general pragmaticprinciples that require from participants, inaddition to being co-operative and informative,also being considerate and helpful.
In our casethis means that KBo should also include generallevel dialogue scenarios (in the form of a graph)and formalisations of the mentioned pragmaticprinciples; for an example of the latter, seeJokinen (1996).3 Process of DialogueLet us describe the case where both A and B areintelligent agents; i.e.
computer programs.1.
A constructsa) the frame exemplar of D, putting in it allrelevant information A has about D;b) the model of partner B, putting in it allrelevant information it has about B'sevaluations concerning the contents of theslots in D's frame.2.
A chooses the point in communicative spacefrom which it intents to start he interaction.3.
A starts to apply communicative strategy.
Amodels B's reasoning process, using B'smodel.
First A applies the reasoning schemebased on the wish of B.
If it results in 'to doD', then A actualises the tactic of enticingand generates its first turn which contains aframe exemplar of Proposal.
If the result ofmodelled reasoning results in 'not to do D',then A tries reasoning which starts fromneeded-factor and then the one triggered bymust-factor, and according to the resultactualises tactics of persuading orthreatening, and generates the first utterance.If the application of all reasoning schemesresults in 'not to do D', then A abandons itsgoal.4.
B interprets A's turn and recognisesProposal in it.
B constructs it's the exemplarrepresentation f D (this may not coincidewith that of A).
B starts reasoning, in thecourse of which it may need additionalinformation from A.
On the basis of theframe of Proposal B formulates the result ofreasoning as its response turn: yes/no +(maybe) Argument.5.
A interprets B's answer and determineswhich point in the dialogue scenario thiscorresponds to.
If B's answer was positive(decision to do D), then according tocommunicative strategy the encounter hascome to its successful end.
If B's answer isnegative, then according to the dialoguescenario A must formulate a(counter)Argument.
The communicativestrategy also allows to choose a new point incommunicative space and/or a new merle.To formulate the counter-argument, A usesinformation from the exemplar of D (it maybe updated on the basis of B's negativeanswer) and its model of B (which it had to108change because of B's negative answer).
Amodels anew B's reasoning, i.e.
the processis repeated cyclically.4 Dialogue examples4.1 Example 1The example represents a dialogue where thecomputer plays A's role and is implementing thetactic of enticement.
The user implements thesimple tactic: point out little pleasantness andunpleasantness of the action.Let us suppose that the action D is "travel toVenice and conclude a contract there".
Thecomputer has chosen the tactics of enticementand has generated the following user model:wAa(resources) = 1, wAB(pleas) =10,wAB(unpleas)=4, w~a3(use)=0, wAB(harm)=l 0,wAB(obligatory)= 1, wAB(prohibited)=0,wAB(puniShD)=0 , WAB(puniShnot.D)=6.The reasoning procedure WISH on this modelyields a positive decision.
A following dialoguewas generated (translated from Estonian):C(omputer): Do you want to travel to Venice?Our f irm needs to conclude a contract here.The computer informed the user about itscommunicative goal.U(ser) : It.is not interesting for me.The user pointed out little pleasantness of theaction.
The computer must correct win(pleas) inthe user model.
The new value of wAS(pleas)will be 8.C: You can meet interesting people.The computer chose its reply from the class ofexpressions for increasing pleasantness.
Afterthis turn the value ofwaB(pleas) will be 9.U: What's so pleasant about this trip?The user pointed out little pleasantness oncemore.
The corrected value of wAS(pleas) will be8.C: There are many nice men in Italy.After this turn wAB(pleas)=9.U: This trip is too intense for me.The user indicated unpleasantness of D. Thecorrected value for wm(unpleas) will be 5.C: The company will pay all your expenses.The computer chose its reply from the class ofexpressions for decreasing unpleasantness andsupposes that after its reply the value ofwAa(unpleas) will be 4.U: I don "t like to travel.The user indicated unpleasantness of the actiononce more.
Thus, the new value of wAB(unpleas)will be 5.C: You can take plane.The computer decreased the unpleasantness of Donce more.
The new value of wAB(unpleas) is4.U: You are right - I shall travel.The user agreed to do D, the communicativegoal of the computer is achieved.C: I am glad.4.2 Example 2The example represents a dialogue where thecomputer is implementing the tactic ofthreatening.
The user implements the tactic asbefore: point out little pleasantness andunpleasantness of the action.The action D is "travel to Venice and conclude acontract here" as before.
The computer haschosen the tactic of threatening and hasgenerated the following user model:wAB(resources) = 1, wAB(pleas)= 10,wAB(unpleas)=4, wAB(use)=4, wAB(harm)=6,WAB(obligatory)=l, wAa(prohibited)=0,wAB(puniShD)=0, w~(punishnot.D)=8.The reasoning procedure MUST on this modelyields a positive decision thus threatening ispossible.
A following dialogue was generated:C: You must travel to Venice.
Our firm needs toconclude a contract here.U: It is not interesting for  me.The user pointed out little pleasantness of theaction.
The computer must correct wAB(pleas) inthe user model.
The new value of wAB(pleas)will be 0.C: There are many people interested on gettingyour job.Using the tactic of threatening the computerchose its reply from the class of expressions forincreasing punishment for not doing D. Afterthis turn the value of wAB(punish~ot.D) will be 9.U: What ~ so pleasant about this trip?.The user pointed out little pleasantness oncemore.C: Refusing will be harmful for  your reputation.After this turn WAB(punishnot.D)=10.U: This trip is too intense for  me.The user indicated unpleasantness of D. Thecorrected value for waB(unpleas) will be 14.C: l f  you have money ou can travel by plane.109The computer chose its reply fi:om the class ofexpressions for decreasing unpleasantness.
Thevalue ofwAS(unpleas) will be 13.U: 1don "t like to travelThe user indicated unpleasantness of the actiononce more.The reasoning procedure MUST on the usermodel will give the negatiw~ decision thusthreatening is impossible.
The computer can'tchoose new tactics because reasoningprocedures WISH and NEEDED will give on theuser model the negative decision too.
Thecomputer must give up.C: 1am sorry.ConclusionAt present here exists implemented programwhich can play the role of both A and B in asimple communication situation where the goalof A is that B would decide to do D. At themoment he computer operates with semanticrepresentations of linguistic input/output only,the surface linguistic part of interaction isprovided in the form of a list of possibleutterances.
The work on linguistic processor isin progress.We have deliberately concentrated onmodellingthe processes of reasoning of conversationagents, as these processes form the heart of the"cognitive" part of human communication, andon modelling the use of communicativestrategies and tactics which constitute the"social" part of communication.Although the concepts and models we havereported in the paper may seem too abstractfrom the point of view of practical NLP, we areconvinced that without serious study andmodelling of  cognitive and social aspects ofhuman communication it will appear impossibleto guarantee naturalness of dialogues carried outby a computer system with a human user.As we have so far mostly dealt with agre mentnegotiation dialogues, we have planned as oneof the practical applications of the system as aparticipant in communication training sessions.Here the system can, for instance, establishcertain restrictions on argument types, on theorder in the use of arguments and counter-arguments, etc.Second, we have started to work, using ourexperience in modelling cognitive and socialaspects of dialogue, on modelling informationseeking dialogues in the same lines.
This type ofdialogue clearly will be the area where in thenext few years already systems will be requiredthat would be practically reliable, but at thesame time could follow the rules of naturalhuman communication.AcknowledgementsThis research was supported byScience Foundation (grant No 4467).EstonianReferencesJames Allen (1994) Natural LanguageUnderstanding.
2nd ed.
The Benjamin/CummingsPubl.
Comp., Inc.Jennifer Chu-Carroll and Sandra Carberry (1998)Collaborative Response Generation in PlanningDialogues.
Computational Linguistics, 24/3, pp.355-400.Barbara Di Eugenio, Pamela W. Jordan, RichmondH.
Tlaomason, Johanna D. Moore (2000) TheAcceptance Cycle: An empirical investigation ofhuman-human collaborative dialogues, to appearin International Journal of Human ComputerStudies.Laila Dybkj~er (2000) Preface.
- From SpokenDialogue to Full Natural Interactive Dialogue-Theory, Empirical Analysis and Evaluation.
LREC2000 Workshop Proceedings.
L. Dybkjaer, ed.Athen, pp.
1-2.Peter Heeman and Graeme Hirst (1995)Collaborating on referring expressions.Computational Linguistics, 21/3, pp.
351-382.Kristiina Jokinen (1995) Rational Agency.
In"Rational Agency: Concepts, Theories, Models,and Applications", M. Fehling, ed.. Proe.
of theAAAI Fall Symposium.
MIT, Boston, pp.
89-93.Kristiina Jokinen (1996) Cooperative ResponsePlanning in CDM."
Reasoning aboutCommunicative Strategies.
In "TWLT11.
DialogueManagement in Natural Language Systems", S.LuperFoy, A. Nijholt & G. Veldhuijzen vanZauten, ed.
Enschede: Universiteit Twente, pp.159-168.Mare Koit (1996) lmplementing a dialogue model onthe computer.
In "Estonian in the Changing World.Papers in Theoretical and ComputationalLinguieties", H. Oim, ed.
Tartu, pp.. 99-114.Mare Koit and Haldur Oim (2000) Developing amodel of natural dialogue.
In "From spokendialogue to full natural interactive dialogue-theory,110Empirical analysis and evaluation.
LREC2000Workshop proceedings", L. Dybkj~r, ed.
Athen,pp.
18-21.Mare Koit and Haldur 0im (1999) Communicativestrategies in human-computer interaction: amodelthat involves natural reasoning.
In "23.
DeutscheJahrestag fiir Kfmstliche Intelligenz".
Bonn,http://www.ikp.uni-bonn.de/NDS 99/Finals/1 2.psMare Koit and Haldur Oim (1998) Developing amodel of dialog strategy.
In "Text, Speech,Dialogue - TSD'98 Proceedings".
Brno, pp.
387-390.Karen Lochbaum (1998) A Collaborative PlanningModel of Intentional Structure.
ComputationalLinguistics, 24/4, pp.
525-572.Haldur Oim (1996) Na~'ve theories andcommunicative competence: reasoning incommunication.
In "Estonian in the ChangingWorld.
Papers in Theoretical and ComputationalLinguictics", H. C)im, ed.
Tartu, pp.
211-231.David R. Traum and James F. Allen (1994)Discourse Obligations in Dialogue Processing.
In"Proceedings of the 32rid Annual Meeting of theAssociation for Computational Linguistics (ACL-94)", pp 1-8.Bonnie Webber (2000) Computational Perspectiveson Discourse and Dialogue.
In "The Handbook ofDiscourse Analysis".
D. Schiffrin, D. Tannen, H.Hamilton, ed.
Blackwell Publishers Ltd.111
