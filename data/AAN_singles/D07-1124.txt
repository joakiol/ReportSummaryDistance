Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1139?1143,Prague, June 2007. c?2007 Association for Computational LinguisticsOnline learning for Deterministic Dependency ParsingPrashanth Reddy MannemLanguage Technologies Research CenterIIIT-Hyderabad, Indiaprashanth@research.iiit.ac.inAbstractDeterministic parsing has emerged as an ef-fective alternative for complex parsing algo-rithms which search the entire search spaceto get the best probable parse tree.
In this pa-per, we present an online large margin basedtraining framework for deterministic pars-ing using Nivre?s Shift-Reduce parsing al-gorithm.
Online training facilitates the useof high dimensional features without cre-ating memory bottlenecks unlike the popu-lar SVMs.
We participated in the CoNLLShared Task-2007 and evaluated our systemfor ten languages.
We got an average multi-lingual labeled attachment score of 74.54 %(with 65.50% being the average and 80.32%the highest) and an average multilingual un-labeled attachment score of 80.30% (with71.13% being the average and 86.55% thehighest).1 IntroductionCoNLL-X had a shared task on multilingual depen-dency parsing (Buchholz et al, 2006) by providingtreebanks for 13 languages in the same dependencyformat.
A look at the performance sheet in the con-test shows that two systems with quite different ap-proaches (one using deterministic parsing with SVMand the other using MIRA with nondeterministic anddynamic programming based MST approach ) per-formed with good results (McDonald et al, 2006;Nivre et al, 2006).More recently, deterministic parsing has gener-ated a lot of interest because of their simplicity(Nivre, 2003).
One of the main advantages of de-terministic parsing lies in the ability to use the sub-tree information in the features to decide the nextstep.
Parsing algorithms which search the entirespace (Eisner, 1996; McDonald, 2006) are restrictedin the features they use to score a relation.
They relyonly on the context information and not the historyinformation to score a relation.
Using history infor-mation makes the search intractable.
Whereas, sincedeterministic parsers are at worst O(n2) (Yamadaand Matsumoto, 2003) (Nivre (2003) is only O(2n)in the worst case), they can use the crucial historyinformation to make parsing decisions.
So, in ourwork Nivre?s parsing algorithm has been used to ar-rive at the dependency parse tree.Popular learning algorithms for deterministicparsing like Support Vector Machines (SVM) runinto memory issues for large data since they arebatch learning algorithms.
Though more informa-tion is available in deterministic parsing in terms ofsubtree information, high dimensional features can?tbe used due to the large training times for SVMs.This is where online methods come into the picture.Unlike batch algorithms, online algorithms con-sider only one training instance at a time whenoptimizing parameters.
This restriction to single-instance optimization might be seen as a weakness,since the algorithm uses less information about theobjective function and constraints than batch algo-rithms.
However, McDonald (2006) argues that thispotential weakness is balanced by the simplicity ofonline learning, which allows for more streamlinedtraining methods.
This work focuses purely on on-line learning for deterministic parsing.1139In the remaining part of the paper, we introduceNivre?s parsing algorithm, propose a framework foronline learning for deterministic parsing and presentthe results for all the languages with various featuremodels.2 Parsing AlgorithmWe used Nivre?s top-down/bottom-up linear timeparsing algorithm proposed in Nivre (2003).
Aparser configuration is represented by triples(S, I,E) where S is the stack (represented as a list),I is the list of (remaining) tokens and E is the set ofedges for the dependency graph D. S is a list of par-tially processed tokens, whose subtrees are incom-plete i.e tokens whose parent or children have notyet been established.
top is the top of the stack S,next is the next token in the list I .Nivre?s algorithm consists of four elementary ac-tions Shift, Left, Right and Reduce to buildthe dependency tree from the initial configuration(nil,W, ?
), where W is the input sentence.
Shiftpushes next onto the stack S. Reduce pops thestack.
Right adds an arc from top to next andpushes next onto the stack S. Left adds an arcfrom next to top and pops the stack.
The parser ter-minates when it reaches a configuration (S,nil,E)( for any list S and set of edges E).The labels for each relation are determined aftera new arc is formed (by left and right actions).The parser always constructs a dependency graphthat is acyclic and projective.
For non-projectiveparsing, we followed the pseudo projective pars-ing approach proposed by Nivre and Nilson (2005).In this approach, the training data is projectivizedby a minimal transformation, lifting non-projectivearcs one step at a time, and extending the arc labelof the lifted arcs using the encoding scheme calledHEAD+PATH.
The non-projective arcs can be re-covered by applying an inverse transformation to theoutput of the parser, using a left-to-right, top-down,breadth-first search, guided by the extended arc la-bels.
This method has been used for all the lan-guages.3 Online LearningMcDonald (2005) applied online learning by scoringedges in a connected graph and finding the Maxi-mum Spanning Tree (MST) of the graph.
McDonaldet al (2005) used Edge Based Factorization , wherethe score of a dependency tree is factored as the sumof scores of all edges in the tree.
Let, x = x1 ?
?
?
xnrepresents a generic input sentence , and y representsa generic dependency tree for sentence x.
(i, j) ?
ydenotes the presence of a dependency relation in yfrom word xi (parent) to word xj (child).In Nivre?s parsing algorithm the dependencygraph can be viewed as a graph resulting from aset of parsing decisions (in this case Shift, Reduce, Left & Right) made, starting with the initial con-figuration (nil,W, ?)
.
We define this sequence ofparsing decisions as d = d1 ?
?
?
dm.
So, d is the se-quence of parsing decisions made by the parser toobtain a dependency tree y, from an input sentencex.
Lets also define c = c1 ?
?
?
cm to be the config-uration sequence starting from initial configuration(nil,W, ?)
to the final configuration (S,nil,E).We define the score of a parsing decision for a par-ticular configuration to be the dot product between ahigh dimensional feature vector (based on both thedecision and the configuration) and a weight vector.So,s(di, ci) = w .
f(di, ci)where ci is the configuration at the ith instanceand di is any one of the four actions {Shift, Reduce,Left, Right} .The Margin Infused Relaxed Algorithm (MIRA)proposed by Crammer et al (2003) attempts to keepthe norm of the change to the parameter vector assmall as possible, subject to correctly classifying theinstance under consideration with a margin at leastas large as the loss of the incorrect classifications.McDonald et al (2005) defines the loss of a depen-dency tree inferred by finding the Maximum Span-ning Tree(MST), as the number of words that haveincorrect parent (i.e the no.
of edges that have gonewrong).
This satisfies the global constraint that thecorrect set of edges will have the highest weight.However, in Nivre?s algorithm, as there is no one toone correspondence between parsing decisions andthe graph edges, the number of errors in the edgescan?t be used as a loss function as it won?t reflect theexact loss in the parsing decisions.
In this methodof calculating the loss function based on edges, wefirst get the series of decisions through inference on1140the training data, then concat their feature vectorsand finally run the normal updates with the edgebased loss (since the resulting decisions will producea parse tree).
This method gave very poor results.So we do a factored MIRA for Nivre?s algorithmby factoring the output by decisions to obtain thefollowing constraints:min?w(i+1) ?
w(i)?s.ts(di, ci) ?
s(d?i, ci) ?
1?ci ?
dt(c) and(di, d?i) ?
(Shift, Left,Right,Reduce)where di represents the correct decision and d?irepresents all the other decisions for the same con-figuration ci.
This states that the weight of the cor-rect decision for a particular configuration and theweight of all other decisions must be separated by amargin of 1.
For every sentence in the training data,starting with the initial configuration (nil,W, ?
),weights are adjusted to satisfy the above constraintsbefore proceeding to the next correct configuration.This process is repeated till we reach the final con-figuration (S,nil,E).4 FeaturesThe two central elements in any configuration arethe token on the top of the stack (t) and the next inputtoken (n),the tokens which may be connected by adependency relation in the current configuration.
Wecategorize our features into basic, context, historyand in ?
between feature sets.
The basic featureset contains information about these two tokens tand n. This includes unigram, bigram combinationsof the word forms (FORM), root word (LEMMA),features (FEATS) and the part-of-speech tags (bothCPOS and POS) of these words.
The coarse POS tag(CPOS) is useful and helps solve data sparseness tosome extent.The existence or non-existence of a relation be-tween two words is heavily dependent on the wordssurrounding t and n which is the contextual infor-mation.
The context feature set has the informationabout the surrounding words t?1, t+1, n?1, n+1,n + 2, n + 3.
Unigram and trigram combinations(with t and n) of the lexical items, POS tags, CPOStags of these words are part of this context featureset.
We also included the second topmost element inthe stack (st?
1) word too.The third feature set, which is the history featureset contains the info about the subtree at a partic-ular parser state.
One of the advantages of usingdeterministic parsing algorithm over nondeterminis-tic algorithm is that history can be used as features.History features have information about the Parent(par), Left Sibling (ls) and Right Sibling (rs) of t.Unigram and trigram combinations (with t and n) ofPOS, CPOS, DEPREL tags of these words are in-cluded in the History Features.The features in the in ?
between feature settake the form of POS and CPOS trigrams: thePOS/CPOS of t, that of the word in between andthat of n.All the features in these feature sets are conjoinedwith distance between t & n and the parsing deci-sion.
We experimented with a combination of thesefeature sets in our training.
We define feature mod-els ?1, ?2 and ?3 for our experiments.
?1 is a com-bination on basic and context feature sets.
?2 is amixture of basic, context and in ?
between fea-ture sets whereas ?3 contains basic, context andhistory feature sets.
The feature models ?1?3 arethe same for all the languages.5 Results and DiscussionThe system with online learning and Nivre?s pars-ing algorithm was trained on the data released byCoNLL Shared Task Organizers for all the ten lan-guages (Hajic?
et al, 2004; Aduriz et al, 2003; Mart?
?et al, 2007; Chen et al, 2003; Bo?hmova?
et al, 2003;Marcus et al, 1993; Johansson and Nugues, 2007;Prokopidis et al, 2005; Csendes et al, 2005; Mon-temagni et al, 2003; Oflazer et al, 2003).
We evalu-ated our system using the standard evaluation scriptprovided by the organizers (Nivre et al, 2007).The evaluation metrics are Unlabeled AttachmentScore(UAS) and Labeled Attachment Score(LAS).The results of our system with various featuremodels are listed in Table 11.
The history infor-mation in ?3 contributed to a marginal improve-ment in accuracy of Hungarian, Italian and Turkish.Whereas, Arabic, Catalan, Czech, English, Greek1Results aren?t available for the models with a ?-?
mark.1141Language ?1 ?2 ?3LAS UAS LAS UAS LAS UASArabic 71.55 81.56 72.05 81.93 71.66 81.30Basque 66.35 73.71 65.64 72.86 64.56 71.69Catalan 84.45 89.65 84.47 89.81 - -Chinese 74.06 79.09 73.76 78.84 72.93 77.52Czech 70.49 77.05 70.68 77.20 - -English 81.19 82.41 81.55 82.81 - -Greek 71.52 78.77 71.69 78.89 71.46 78.48Hungarian 70.42 75.01 70.94 75.39 71.05 75.54Italian 78.30 82.54 78.67 82.91 79.18 83.38Turkish 76.42 82.74 76.48 82.85 77.29 83.58Table 1: Results of Online learning with Nivre?s parsing algorithm for feature models ?1, ?2, ?3got their highest accuracies with feature model ?2containing basic, context and in?between featuresets.
The rest of the languages, Basque and Chineseachieved highest accuracies with ?1.
But, a carefullook at the results table shows that there isn?t anysignificant difference in the accuracies of the sys-tem across different feature models.
This is true forall the languages.
The feature models ?2 and ?3did not show any significant difference in accuracieseven though they contain more information.
Featuremodel ?1 with basic and context feature sets hasachieved good accuracies.5.1 K-Best Deterministic ParsingThe deterministic parsing algorithm does not han-dle ambiguity.
By choosing a single parser action ateach opportunity, the input string is parsed determin-istically and a single dependency tree is built duringthe parsing process from beginning to end (no othertrees are even considered).
A simple extension tothis idea is to eliminate determinism by allowing theparser to choose several actions at each opportunity,creating different action sequences that lead to dif-ferent parse trees.
Since a score is assigned to everyparser action, the score of a parse tree can be com-puted simply as the average of the scores of all ac-tions that resulted in that parse tree (the derivationof the tree).
We performed a beam search by carry-ing out a K-best search through the set of possiblesequences of actions as proposed by Johansson andNugues (2006).
However, this did not increase theaccuracy.
Moreover, with larger values of K, therewas a decrease in the parsing accuracy.
The best-first search proposed by Sagae and Lavie (2006) wasalso tried out but there was similar drop in accuracy.6 ConclusionThe evaluation shows that the labeled pseudo projec-tive deterministic parsing with online learning givescompetitive parsing accuracy for most of the lan-guages involved in the shared task.
The level of ac-curacy varies considerably between the languages.Analyzing the results and the effects of various fea-tures with online learning will be an important re-search goal in the future.ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT), pages 201?204.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.S.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.2006.
Conll-x shared task on multilingual dependencyparsing.
In Proceedings of the Conference on Compu-tational Natural Language Learning (CoNLL).K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica treebank: Design criteria,representational issues and implementation.
In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.1142K.
Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.2003.
Online passive aggressive algorithms.
In Pro-ceedings of Neural Information Processing Systems(NIPS).D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings ofthe 16th International Conference on ComputationalLinguistics (COLING), pages 340?345.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
?Snaidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools,pages 110?117.R.
Johansson and P. Nugues.
2006.
Investigatingmultilingual dependency parsing.
In Proceedings ofthe Tenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 206?210.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevelannotated corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics (ACL).R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discrim-inative parser.
In Proceedings of the Conference onComputational Natural Language Learning (CoNLL).R.
McDonald.
2006.
Discriminative learning and span-ning tree algorithms for dependency parsing.
Ph.D.thesis, University of Pennsylvania.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, andR.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(Abeille?, 2003), chap-ter 11, pages 189?210.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 99?106.J.
Nivre, J.
Hall, J. Nilson, G. Eryigit, and S. Marinov.2006.
Labeled pseudo-projective dependency pars-ing with support vector machines.
In Proceedings ofthe Conference on Computational Natural LanguageLearning (CoNLL).J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
of theJoint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of InternationalWorkshop on Parsing Technologies, pages 149?160.K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.2003.
Building a Turkish treebank.
In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andpractical issues in the construction of a Greek depen-dency treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 149?160.K.
Sagae and A. Lavie.
2006.
A best-first probabilis-tic shift-reduce parser.
In Proceedings of the COL-ING/ACL 2006 Main Conference Poster Sessions.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Pro-ceedings of IWPT-2003.1143
