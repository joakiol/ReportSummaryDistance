Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 14?22,Uppsala, July 2010.Towards A Better Understanding ofUncertainties and Speculations in Swedish Clinical Text?
Analysis of an Initial Annotation TrialSumithra VelupillaiDepartment of Computer and Systems Sciences (DSV)Stockholm UniversityForum 100SE-164 40 Kista, Swedensumithra@dsv.su.seAbstractElectronic Health Records (EHRs) containa large amount of free text documentationwhich is potentially very useful for Infor-mation Retrieval and Text Mining appli-cations.
We have, in an initial annotationtrial, annotated 6 739 sentences randomlyextracted from a corpus of Swedish EHRsfor sentence level (un)certainty, and tokenlevel speculative keywords and negations.This set is split into different clinical prac-tices and analyzed by means of descrip-tive statistics and pairwise Inter-AnnotatorAgreement (IAA) measured by F1-score.We identify geriatrics as a clinical prac-tice with a low average amount of uncer-tain sentences and a high average IAA,and neurology with a high average amountof uncertain sentences.
Speculative wordsare often n-grams, and uncertain sentenceslonger than average.
The results of thisanalysis is to be used in the creation of anew annotated corpus where we will refineand further develop the initial annotationguidelines and introduce more levels of di-mensionality.
Once we have finalized ourguidelines and refined the annotations weplan to release the corpus for further re-search, after ensuring that no identifiableinformation is included.1 IntroductionElectronic Health Records (EHRs) contain a largeamount of free text documentation which is po-tentially very useful for Information Retrieval andText Mining applications.
Clinical documentationis specific in many ways; there are many authorsin a document (e.g.
physicians, nurses), there aredifferent situations that are documented (e.g.
ad-mission, current status).
Moreover, they may oftenbe written under time pressure, resulting in frag-mented, brief texts often containing spelling errorsand abbreviations.
With access to EHR data, manypossibilities to exploit documented clinical knowl-edge and experience arise.One of the properties of EHRs is that they con-tain reasoning about the status and diagnoses ofpatients.
Gathering such information for the usein e.g.
medical research in order to find rela-tionships between diagnoses, treatments etc.
hasgreat potential.
However, in many situations, clin-icians might describe uncertain or negated find-ings, which is crucial to distinguish from positiveor asserted findings.
Potential future applicationsinclude search engines where medical researcherscan search for particular diseases where negatedor speculative contexts are separated from assertedcontexts, or text mining systems where e.g.
dis-eases that seem to occur often in speculative con-texts are presented to the user, indicating that moreresearch is needed.
Moreover, laymen may alsobenefit from information retrieval systems that dis-tinguish diseases or symptoms that are more orless certain given current medical expertise andknowledge.We have, in an initial annotation trial, annotated6 739 sentences randomly extracted from a corpusof Swedish EHRs for sentence level (un)certainty,and token level speculative keywords and nega-tions1.
In this paper, a deeper analysis of the re-sulting annotations is performed.
The aims areto analyze the results split into different clinicalpractices by means of descriptive statistics andpairwise Inter-Annotator Agreement (IAA) mea-sured by F1-score, with the goal of identifying a)whether specific clinical practices contain higheror lower amounts of uncertain expressions, b)1This research has been carried out after approvalfrom the Regional Ethical Review Board in Stockholm(Etikpr?ovningsn?amnden i Stockholm), permission number2009/1742-31/514whether specific clinical practices result in higheror lower IAA - indicating a less or more difficultclinical practice for judging uncertainties, and c)identifying the characteristics of the entities anno-tated as speculative words, are they highly lexi-cal or is a deeper syntactic and/or semantic anal-ysis required for modeling?
From this analysis,we plan to conduct a new annotation trial wherewe will refine and further develop the annotationguidelines and use domain experts for annotationsin order to be able to create a useful annotated cor-pus modeling uncertainties, negations and specu-lations in Swedish clinical text, which can be usedto develop tools for the automatic identification ofthese phenomena in, for instance, Text Mining ap-plications.2 Related ResearchIn recent years, the interest for identifying andmodeling speculative language in natural languagetext has grown.
In particular, biomedical scien-tific articles and abstracts have been the object ofseveral experiments.
In Light et al (2004), fourannotators annotated 891 sentences each as eitherhighly speculative, low speculative, or definite,in biomedical scientific abstracts extracted fromMedline.
In total, they found 11 percent specula-tive sentences, resulting in IAA results, measuredwith kappa, between 0.54 and 0.68.
One of theirmain findings was that the majority of the specu-lative sentences appeared towards the end of theabstract.Vincze et al (2008) describe the creation of theBioScope corpus, where more than 20 000 sen-tences from both medical (clinical) free texts (ra-diology reports), biological full papers and biolog-ical scientific abstracts have been annotated withspeculative and negation keywords along withtheir scope.
Over 10 percent of the sentenceswere either speculative or negated.
In the clinicalsub-corpus, 14 percent contained speculative key-words.
Three annotators annotated the corpus, andthe guidelines were modified several times duringthe annotation process, in order to resolve prob-lematic issues and refine definitions.
The IAAresults, measured with F1-score, in the clinicalsub-corpus for negation keywords ranged between0.91 and 0.96, and for speculative keywords be-tween 0.84 and 0.92.
The BioScope corpus hasbeen used to train and evaluate automatic classi-fiers (e.g.
?Ozg?ur and Radev (2009) and Moranteand Daelemans (2009)) with promising results.Five qualitative dimensions for characterizingscientific sentences are defined in Wilbur et al(2006), including levels of certainty.
Here, guide-lines are also developed over a long period of time(more than a year), testing and revising the guide-lines consecutively.
Their final IAA results, mea-sured with F1-score, range between 0.70 and 0.80.Different levels of dimensionality for categorizingcertainty (in newspaper articles) is also presentedin Rubin et al (2006).Expressions for communicating probabilities orlevels of certainty in clinical care may be inher-ently difficult to judge.
Eleven observers wereasked to indicate the level of probability of a dis-ease implied by eighteen expressions in the workpresented by Hobby et al (2000).
They foundthat expressions indicating intermediate probabili-ties were much less consistently rated than thoseindicating very high or low probabilities.
Sim-ilarly, Khorasani et al (2003) performed a sur-vey analyzing agreement between radiologists andnon-radiologists regarding phrases used to conveydegrees of certainty.
In this study, they found lit-tle or no agreement among the survey participantsregarding the diagnostic certainty associated withthese phrases.
Although we do not have access toradiology reports in our corpus, these findings in-dicate that it is not trivial to classify uncertain lan-guage in clinical documentation, even for domainexperts.3 MethodThe annotation trial is based on sentences ran-domly extracted from a corpus of Swedish EHRs(see Dalianis and Velupillai (2010) for an initialdescription and analysis).
These records containboth structured (e.g.
measure values, gender in-formation) and unstructured information (i.e.
freetext).
Each free text entry is written under a spe-cific heading, e.g.
Status, Current medication, So-cial Background.
For this corpus, sentences wereextracted only from the free text entry Assessment(Bed?omning), with the assumption that these en-tries contain a substantial amount of reasoning re-garding a patient?s diagnosis and situation.
A sim-ple sentence tokenizing strategy was employed,based on heuristic regular expressions2.
We haveused Knowtator (Ogren, 2006) for the annotation2The performance of the sentence tokenizer has not beenevaluated in this work.15work.One senior level student (SLS), one undergrad-uate computer scientist (UCS), and one undergrad-uate language consultant (ULC) annotated the sen-tences into the following classes; on a sentencelevel: certain, uncertain or undefined, and on atoken level: speculative words, negations, and un-defined words.The annotators are to be considered naivecoders, as they had no prior knowledge of thetask, nor any clinical background.
The annota-tion guidelines were inspired by those created forthe BioScope corpus (Vincze et al, 2008), withsome modifications (see Dalianis and Velupillai(2010)).
The annotators were allowed to break asentence into subclauses if they found that a sen-tence contained conflicting levels of certainty, andthey were allowed to mark question marks as spec-ulative words.
They did not annotate the linguis-tic scopes of each token level instance.
The anno-tators worked independently, and met for discus-sions in even intervals (in total seven), in order toresolve problematic issues.
No information aboutthe clinic, patient gender, etc.
was shown.
Theannotation trial is considered as a first step in fur-ther work of annotating Swedish clinical text forspeculative language.Clinical practice # sentences # tokenshematology 140 1 494surgery 295 3 269neurology 351 4 098geriatrics 142 1 568orthopaedics 245 2 541rheumatology 384 3 348urology 120 1 393cardiology 128 1 242oncology 550 5 262ENT 224 2 120infection 107 1 228emergency 717 6 755paediatrics 935 8 926total, clinical practice 4 338 43 244total, full corpus 6 739 69 495Table 1: Number of sentences and tokens per clin-ical practice (#sentences > 100), and in total.
ENT= Ear, Nose and Throat.3.1 Annotations and clinical practicesThe resulting corpus consists of 6 739 sentences,extracted from 485 unique clinics.
In order tobe able to analyze possible similarities and dif-ferences across clinical practices, sentences fromclinics belonging to a specific practice type weregrouped together.
In Table 1, the resulting groups,along with the total amount of sentences and to-kens, are presented3.
Only groups with a totalamount of sentences > 100 were used in the anal-ysis, resulting in 13 groups.
A clinic was includedin a clinical practice group based on a priorityheuristics, e.g.
the clinic ?Barnakuten-kir?
(Pae-diatric emergency surgery) was grouped into pae-diatrics.The average length (in tokens) per clinical prac-tice and in total are given in Table 2.
Clinicaldocumentation is often very brief and fragmented,for most clinical practices (except urology andcardiology) the minimum sentence length (in to-kens) was one, e.g.
?basal?, ?terapisvikt?
(ther-apy failure), ?lymf?odem?
(lymphedema), ?viros?
(virosis), ?opanm?ales?
(reported to surgery, com-pound with abbreviation).
We see that the aver-age sentence length is around ten for all practices,where the shortest are found in rheumatology andthe longest in infection.As the annotators were allowed to break up sen-tences into subclauses, but not required to, this ledto a considerable difference in the total amount ofannotations per annotator.
In order to be able toanalyze similarities and differences between theresulting annotations, all sentence level annota-tions were converted into one sentence class only,the primary class (defined as the first sentencelevel annotation class, i.e.
if a sentence was bro-ken into two clauses by an annotator, the first be-ing certain and the second being uncertain, thefinal sentence level annotation class will be cer-tain).
The sentence level annotation class certainwas in clear majority among all three annotators.On both sentence and token level, the class unde-fined (a sentence that could not be classified ascertain or uncertain, or a token which was notclearly speculative) was rarely used.
Therefore,all sentence level annotations marked as undefinedare converted to the majority class, certain, result-ing in two sentence level annotation classes (cer-tain and uncertain) and two token level annotationclasses (speculative words and negations, i.e.
to-3White space tokenization.16kens annotated as undefined are ignored).For the remaining analysis, we focus on thedistributions of the annotation classes uncertainand speculative words, per annotator and annota-tor pair, and per clinical practice.Clinical practice Max Avg Stddevhematology 40 10.67 7.97surgery 57 11.08 8.29neurology 105 11.67 10.30geriatrics 58 11.04 9.29orthopaedics 40 10.37 6.88rheumatology 59 8.72 7.99urology 46 11.61 7.86cardiology 50 9.70 7.46oncology 54 9.57 7.75ENT 54 9.46 7.53infection 37 11.48 7.76emergency 55 9.42 6.88paediatrics 68 9.55 7.24total, full corpus 120 10.31 8.53Table 2: Token statistics per sentence and clinicalpractice.
All clinic groups except urology (min =2) and cardiology (min = 2) have a minimum sen-tence length of one token.Figure 1: Sentence level annotation: uncertain,percentage per annotator and clinical practice.4 ResultsWe have measured the proportions (in percent) perannotator for each clinical practice and in total.This enables an analysis of whether there are sub-stantial individual differences in the distributions,indicating that this annotation task is highly sub-jective and/or difficult.
Moreover, we measureIAA by pairwise F1-score.
From this, we mayFigure 2: Pairwise F1-score, sentence level anno-tation class uncertain.draw conclusions whether specific clinical prac-tices are harder or easier to judge reliably (i.e.
byhigh IAA results).Figure 3: Average length in tokens, per annotatorand sentence class.In Figure 1, we see that the average amount ofuncertain sentences lies between 9 and 12 percentfor each annotator in the full corpus.
In general,UCS has annotated a larger proportion of uncer-tain sentences compared to ULC and SLS.The clinical discipline with the highest averageamount of uncertain sentences is neurology (13.7percent), the lowest average amount is found incardiology (4.7 percent).
Surgery and cardiologyshow the largest individual differences in propor-tions (from 9 percent (ULC) to 15 percent (UCS),and from 2 percent (ULC) to 7 percent (UCS), re-spectively).However, in Figure 2, we see that the pairwiseIAA, measured by F1-score, is relatively low, withan average IAA of 0.58, ranging between 0.54(UCS/SLS) and 0.65 (UCS/ULC), for the entirecorpus.
In general, the annotator pair UCS/ULChave higher IAA results, with the highest for geri-atrics (0.78).
The individual proportions for un-17certain sentences in geriatrics is also lower forall annotators (see Figure 1), indicating a clinicalpractice with a low amount of uncertain sentences,and a slightly higher average IAA (0.64 F1-score).4.1 Sentence lengthsAs the focus lies on analyzing sentences annotatedas uncertain, one interesting property is to look atsentence lengths (measured in tokens).
One hy-pothesis is that uncertain sentences are in generallonger.
In Figure 3 we see that in general, forall three annotators, uncertain sentences are longerthan certain sentences.
This result is, of course,highly influenced by the skewness of the data (i.e.uncertain sentences are in minority), but it is clearthat uncertain sentences, in general, are longer onaverage.
It is interesting to note that the annota-tor SLS has, in most cases, annotated longer sen-tences as uncertain, compared to UCS and ULC.Moreover, geriatrics, with relatively high IAA butrelatively low amounts of uncertain sentences, haswell above average sentence lengths in the uncer-tain class.4.2 Token level annotationsWhen it comes to the token level annotations,speculative words and negations, we observedvery high IAA for negations (0.95 F1-score (exactmatch) on average in the full corpus, the lowest forneurology, 0.94).
These annotations were highlylexical (13 unique tokens) and unambiguous, andspread evenly across the two sentence level anno-tation classes (ranging between 1 and 3 percent ofthe total amount of tokens per class).
Moreover,all negations were unigrams.On the other hand, we observed large variationsin IAA results for speculative words.
In Figure4, we see that there are considerable differencesbetween exact and partial matches4between allannotator pairs, indicating individual differencesin the interpretations of what constitutes a spec-ulative word and how many tokens they cover,and the lexicality is not as evident as for nega-tions.
The highest level of agreement we find be-tween UCS/ULC in orthopaedics (0.65 F1-score,partial match) and neurology (0.64 F1-score, par-tial match), and the lowest in infection (UCS/SLS,0.31 F1-score).4Partial matches are measured on a character level.Figure 4: F1-score, speculative words, exact andpartial match.4.2.1 Speculative words ?
most commonThe low IAA results for speculative words invitesa deeper analysis for this class.
How is this inter-preted by the individual annotators?
First, we lookat the most common tokens annotated as specu-lative words, shared by the three annotators: ???,?sannolikt?
(likely), ?ev?
(possibly, abbreviated),?om?
(if).
The most common speculative wordsare all unigrams, for all three annotators.
Thesetokens are similar to the most common specu-lative words in the clinical BioScope subcorpus,where if, may and likely are among the top fivemost common.
Those tokens that are most com-mon per annotator and not shared by the other two(among the five most frequent) include ?bed?oms?
(judged), ?kan?
(could), ?helt?
(completely) and?st?allningstagande?
(standpoint).Looking at neurology and urology, with a higheroverall average amount of uncertain sentences, wefind that the most common words for neurologyare similar to those most common in total, whilefor urology we find more n-grams.
In Table 3, thefive most common speculative words per annotatorfor neurology and urology are presented.When it comes to the unigrams, many of theseare also not annotated as speculative words.
Forinstance, ?om?
(if), is annotated as speculative inonly 9 percent on average of its occurrence in theneurological data (the same distribution holds, onaverage, in the total set).
In Morante and Daele-mans (2009), if is also one of the words that aresubject to the majority of false positives in theirautomatic classifier.
On the other hand, ?sanno-likt?
(likely) is almost always annotated as a spec-ulative word (over 90 percent of the time).18UCS ULC SLSneurology ?
?
?sannolikt (likely) kan (could) sannolikt (likely)kan (could) sannolikt (likely) ev (possibly, abbr)om (if) om (if) om (if)pr?ova (try) verkar (seems) st?allningstagande (standpoint)ter (seem) ev (possibly, abbr) m?ojligen (possibly)urology kan vara (could be) mycket (very) tyder p?a(indicates)tyder p?a(indicates) inga tecken (no signs) i f?orsta hand (primarily)ev (possibly, abbr) kan vara (could be) misst?ankt (suspected)misst?ankt (suspected) kan (could) kanske (perhaps)kanske (perhaps) tyder (indicates) skall vi f?ors?oka (should we try)planeras tydligen (apparently planned) misst?ankt (suspected) kan vara (could be)Table 3: Most common speculative words per annotator for neurology and urology.4.2.2 Speculative words ?
n-gramsSpeculative words are, in Swedish clinical text,clearly not simple lexical unigrams.
In Figure 5we see that the average length of tokens anno-tated as speculative words is, on average, 1.34,with the longest in orthopaedics (1.49) and urol-ogy (1.46).
We also see that SLS has, on aver-age, annotated longer sequences of tokens as spec-ulative words compared to UCS and ULC.
Thelongest n-grams range between three and six to-kens, e.g.
?kan inte se n?agra tydliga?
(can?t seeany clear), ?kan r?ora sig om?
(could be about),?inte helt har kunnat uteslutas?
(has not been ableto completely exclude), ?i f?orsta hand?
(primarily).In many of these cases, the strongest indicator isactually a unigram (?kan?
(could)), within a verbphrase.
Moreover, negations inside a speculativeword annotation, such as ?inga tecken?
(no signs)are annotated differently among the individual an-notators.Figure 5: Average length, speculative words.4.3 ExamplesWe have observed low average pairwise IAA forsentence level annotations in the uncertain class,with more or less large differences between the an-notator pairs.
Moreover, at the token level and forthe class speculative words, we also see low av-erage agreement, and indications that speculativewords often are n-grams.
We focus on the clinicalpractices neurology, because of its average largeproportion of uncertain sentences, geriatrics forits high IAA results for UCS/ULC and low aver-age proportion of uncertain sentences, and finallysurgery, for its large discrepancy in proportionsand low average IAA results.In Example 1 we see a sentence where two an-notators (ULC, SLS) have marked the sentenceas uncertain, also marking a unigram (?ospecifik?
(unspecific) as a speculative word.
This exampleis interesting since the utterance is ambiguous, itcan be judged as certain as in the dizziness is con-firmed to be of an unspecific type or uncertain asin the type of dizziness is unclear, a type of ut-terance which should be clearly addressed in theguidelines.<C> Yrsel av ospecifik typ.
</C><U> Yrsel av <S> ospecifik </S> typ.</U><U> Yrsel av <S> ospecifik </S> typ.</U>Dizziness of unspecific typeExample 1: Annotation example, neurology.
Am-biguous sentence, unspecific as a possible specu-lation cue.
C = Certain, U = Uncertain, S = Spec-ulative words.An example of different interpretations of theminimum span a speculative word covers is givenin Example 2.
Here, we see that ?inga egentligam?arkbara?
(no real apparent) has been annotatedin three different ways.
It is also interesting to19note the role of the negation as part of ampli-fying speculation.
Several such instances weremarked by the annotators (for further examples,see Dalianis and Velupillai (2010)), which con-forms well with the findings reported in Kilicogluand Bergler (2008), where it is showed that ex-plicit certainty markers together with negation areindicators of speculative language.
In the Bio-Scope corpus (Vincze et al, 2008), such instancesare marked as speculation cues.
This example, aswell as Example 1, is also interesting as they bothclearly are part of a longer passage of reasoning ofa patient, with no particular diagnosis mentionedin the current sentence.
Instead of randomly ex-tracting sentences from the free text entry Assess-ment, one possibility would be to let the annotatorsjudge all sentences in an entry (or a full EHR).
Do-ing this, differences in where speculative languageoften occur in an EHR (entry) might become ev-ident, as for scientific writings, where it has beenshowed that speculative sentences occur towardsthe end of abstracts (Light et al, 2004).<U> <S><N> Inga </N> egentliga </S><S> m?arkbara</S> minnessv?arigheter undersamtal.
</U>.<U> <N> Inga </N> <S> egentliga </S>m?arkbara minnessv?arigheter under samtal.
</U>.<U> <S><N> Inga </N> egentliga m?arkbara</S> minnessv?arigheter under samtal.
</U>.No real apparent memory difficulties duringconversationExample 2: Annotation example, neurology.
Dif-ferent annotation coverage over negation and spec-ulation.
C = Certain, U = Uncertain, S = Specula-tive words, N = NegationIn geriatrics, we have observed a lower thanaverage amount of uncertain sentences, and highIAA between UCS and ULC.
In Example 3 we seea sentence where UCS and ULC have matchingannotations, whereas SLS has judged this sentenceas certain.
This example shows the difficulty ofinterpreting expressions indicating possible spec-ulation ?
is ?ganska?
(relatively) used here as amarker of certainty (as certain as one gets whendiagnosing this type of illness)?The word ?sannolikt?
(likely) is one of the mostcommon words annotated as a speculative wordin the total corpus.
In Example 4, we see a sen-<U> B?ade anamnestiskt och testm?assigt <S>ganska </S> stabil vad det g?aller Alzheimersjukdom.
</U>.<U> B?ade anamnestiskt och testm?assigt <S>ganska </S> stabil vad det g?ller Alzheimersjukdom.
</U>.<C> B?ade anamnestiskt och testm?assigt ganskastabil vad det g?aller Alzheimer sjukdom.
</C>.Both anamnesis and tests relatively stabilewhen it comes to Alzheimer?s disease.Example 3: Annotation example, geriatrics.
Dif-ferent judgements for the word ?ganska?
(rela-tively).
C = Certain, U = Uncertain, S = Specu-lative words.tence where the annotators UCS and SLS havejudged it to be uncertain, while UCS and ULChave marked the word ?sannolikt?
(likely) as aspeculative word.
This is an interesting exam-ple, through informal discussions with clinicianswe were informed that this word might as well beused as a marker of high certainty.
Such instancesshow the need for using domain experts in futureannotations of similar corpora.<C>En 66-?arig kvinna med <S>sannolikt</S>2 synkrona tum?orer v?anster colon/sigmoideum ochd?ar till levermetastaser.</C>.<U>En 66-?arig kvinna med <S>sannolikt</S>2 synkrona tum?orer v?anster colon/sigmoideum ochd?ar till levermetastaser.</U>.<C>En 66-?arig kvinna med sannolikt 2 synkronatum?orer v?anster colon/sigmoideum och d?ar tilllevermetastaser.</C>.A 66 year old woman likely with 2 synchronoustumours left colon/sigmoideum in addition to livermetastasis.Example 4: Annotation example, surgery.
Differ-ent judgements for the word ?sannolikt?
(likely).
C= Certain, U = Uncertain, S = Speculative words.5 DiscussionWe have presented an analysis of an initial anno-tation trial for the identification of uncertain sen-tences as well as for token level cues (specula-tive words) across different clinical practices.
Ourmain findings are that IAA results for both sen-tence level annotations of uncertainty and tokenlevel annotations for speculative words are, on av-20erage, fairly low, with higher average agreementin geriatrics and rheumatology (see Figures 1 and2).
Moreover, by analyzing the individual distri-butions for the classes uncertain and speculativewords, we find that neurology has the highest aver-age amount of uncertain sentences, and cardiologythe lowest.
On average, the amount of uncertainsentences ranges between 9 and 12 percent, whichis in line with previous work on sentence level an-notations of uncertainty (see Section 2).We have also showed that the most commonspeculative words are unigrams, but that a substan-tial amount are n-grams.
The n-grams are, how-ever, often part of verb phrases, where the head isoften the speculation cue.
However, it is evidentthat speculative words are not always simple lex-ical units, i.e.
syntactic information is potentiallyvery useful.
Question marks are the most commonentities annotated as speculative words.
Althoughthese are not interesting indicators in themselves,it is interesting to note that they are very commonin clinical documentation.From the relatively low IAA results we draw theconclusion that this task is difficult and requiresmore clearly defined guidelines.
Moreover, usingnaive coders on clinical documentation is possiblynot very useful if the resulting annotations are tobe used in, e.g.
a Text Mining application for med-ical researchers.
Clinical documentation is highlydomain-specific and contains a large amount ofinternal jargon, which requires judgements fromclinicians.
However, we find it interesting to notethat we have identified differences between dif-ferent clinical practices.
A consensus corpus hasbeen created from the resulting annotations, whichhas been used in an experiment for automatic clas-sification, see Dalianis and Skeppstedt (2010) forinitial results and evaluation.During discussions among the annotators, somespecific problems were noted.
For instance, theextracted sentences were not always about the pa-tient or the current status or diagnosis, and in manycases an expression could describe (un)certainty ofsomeone other than the author (e.g.
another physi-cian or a family member), introducing aspects ofperspective.
The sentences annotated as certain,are difficult to interpret, as they are simply not un-certain.
We believe that it is important to intro-duce further dimensions, e.g.
explicit certainty,and focus (what is (un)certain?
), as well as time(e.g.
current or past).6 ConclusionsTo our knowledge, there is no previous research onannotating Swedish clinical text for sentence andtoken level uncertainty together with an analysisof the differences between different clinical prac-tices.
Although the initial IAA results are in gen-eral relatively low for all clinical practice groups,we have identified indications that neurology is apractice which has an above average amount ofuncertain elements, and that geriatrics has a be-low average amount, as well as higher IAA.
Boththese disciplines would be interesting to continuethe work on identifying speculative language.It is evident that clinical language contains a rel-atively high amount of uncertain elements, but itis also clear that naive coders are not optimal touse for interpreting the contents of EHRs.
More-over, more care needs to be taken in the extrac-tion of sentences to be annotated, in order to en-sure that the sentences actually describe reason-ing about the patient status and diagnosis.
For in-stance, instead of randomly extracting sentencesfrom within a free text entry, it might be better tolet the annotators judge all sentences within an en-try.
This would also enable an analysis of whetherspeculative language is more or less frequent inspecific parts of EHRs.From our findings, we plan to further developthe guidelines and particularly focus on specify-ing the minimal entities that should be annotatedas speculative words (e.g.
?kan?
(could)).
Wealso plan to introduce further levels of dimension-ality in the annotation task, e.g.
cues that indi-cate a high level of certainty, and to use domainexperts as annotators.
Although there are prob-lematic issues regarding the use of naive codersfor this task, we believe that our analysis has re-vealed some properties of speculative language inclinical text which enables us to develop a usefulresource for further research in the area of specula-tive language.
Judging an instance as being certainor uncertain is, perhaps, a task which can neverexclude subjective interpretations.
One interestingway of exploiting this fact would be to exploit in-dividual annotations similar to the work presentedin Reidsma and op den Akker (2008).
Once wehave finalized the annotated set, and ensured thatno identifiable information is included, we plan tomake this resource available for further research.21ReferencesHercules Dalianis and Maria Skeppstedt.
2010.
Cre-ating and Evaluating a Consensus for Negated andSpeculative Words in a Swedish Clinical Corpus.
Tobe published in the proceedings of the Negation andSpeculation in Natural Language Processing Work-shop, July 10, Uppsala, Sweden.Hercules Dalianis and Sumithra Velupillai.
2010.How Certain are Clinical Assessments?
Annotat-ing Swedish Clinical Text for (Un)certainties, Spec-ulations and Negations.
In Proceedings of the ofthe Seventh International Conference on LanguageResources and Evaluation, LREC 2010, Valletta,Malta, May 19-21.J.
L. Hobby, B. D. M. Tom, C. Todd, P. W. P. Bearcroft,and A. K. Dixon.
2000.
Communication of doubtand certainty in radiological reports.
The BritishJournal of Radiology, 73:999?1001, September.R.
Khorasani, D. W. Bates, S. Teeger, J. M. Rotschild,D.
F. Adams, and S. E. Seltzer.
2003.
Is terminol-ogy used effectively to convey diagnostic certaintyin radiology reports?
Academic Radiology, 10:685?688.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: a linguistically motivated perspective.
BMCBioinformatics, 9(S-11).Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: Facts, spec-ulations, and statements in between.
In LynetteHirschman and James Pustejovsky, editors, HLT-NAACL 2004 Workshop: BioLINK 2004, LinkingBiological Literature, Ontologies and Databases,pages 17?24, Boston, Massachusetts, USA, May 6.Association for Computational Linguistics.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.In BioNLP ?09: Proceedings of the Workshop onBioNLP, pages 28?36, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Philip V. Ogren.
2006.
Knowtator: a prot?eg?e plug-infor annotated corpus construction.
In Proceedings ofthe 2006 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 273?275, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Arzucan?Ozg?ur and Dragomir R. Radev.
2009.
De-tecting speculations and their scopes in scientifictext.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing,pages 1398?1407, Singapore, August.
Associationfor Computational Linguistics.Dennis Reidsma and Rieks op den Akker.
2008.
Ex-ploiting ?subjective?
annotations.
In HumanJudge?08: Proceedings of the Workshop on Human Judge-ments in Computational Linguistics, pages 8?16,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Victoria L. Rubin, Elizabeth D. Liddy, and NorikoKando.
2006.
Certainty identification in texts: Cat-egorization model and manual tagging results.
InComputing Affect and Attitutde in Text: Theory andApplications.
Springer.Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas,Gy?orgy M?ora, and J?anos Csirik.
2008.
The bio-scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(S-11).J.
W. Wilbur, A. Rzhetsky, and H. Shatkay.
2006.
Newdirections in biomedical text annotation: definitions,guidelines and corpus construction.
BMC Bioinfor-matics, 7:356+, July.22
