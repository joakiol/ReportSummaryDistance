Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1928?1939, Dublin, Ireland, August 23-29 2014.Latent Domain Translation Models in Mix-of-Domains HaystackHoang Cuong and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamScience Park 107, 1098 XG Amsterdam, The NetherlandsAbstractThis paper addresses the problem of selecting adequate training sentence pairs from a mix-of-domains parallel corpus for a translation task represented by a small in-domain parallel corpus.We propose a novel latent domain translation model which includes domain priors, domain-dependent translation models and language models.
The goal of learning is to estimate theprobability of a sentence pair in mix-domain corpus to be in- or out-domain using in-domaincorpus statistics as prior.
We derive an EM training algorithm and provide solutions for esti-mating out-domain models (given only in- and mix-domain data).
We report on experiments indata selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consistingof a mix of a rather diverse set of domains.
Our results show that our latent domain invitationapproach outperforms the existing baselines significantly.
We also provide analysis of the meritsof our approach relative to existing approaches.Large parallel corpora are important for training statistical MT systems.
Besides size, the relevanceof a parallel training corpus to the translation task at hand can be decisive for system performance, cf.
(Axelrod et al., 2011; Koehn and Haddow, 2012).
In this paper we look at data selection where wehave access to a large parallel data repository Cmix, representing a rather varied mix of domains, andwe are given a sample of in-domain parallel data Cin, exemplifying a target translation task.
Simplyconcatenating Cinwith Cmixdoes not always deliver best performance, because including irrelevantsentences might be more harmful than beneficial, cf.
(Axelrod et al., 2011).
To make the best ofavailable data, we must select sentences from Cmixfor their relevance to translating sentences from Cin.Axelrod et al.
(2011) and follow-up work, e.g., (Haddow and Koehn, 2012; Koehn and Haddow,2012), select sentence pairs in Cmixusing the cross-entropy difference between in- and mix-domain lan-guage models, both source and target sides, a modification of the Moore and Lewis method (Moore andLewis, 2010).
In the translation context, however, often a source phrase has different senses/translationsin different domains, which cannot be distinguished with monolingual language models.
The depen-dence of translation choice on domain suggests that the word alignments themselves can better be con-ditioned on domain information.
However, in the data selection setting, corpus Cmixoften does notcontain useful domain markers, and Cincontains only a small sample of in-domain sentence pairs.In this paper we present a latent domain translation model which weights every sentence pair ?f , e?
?Cmixwith a probability P (D | f , e) for being in-domain (D1) or out-domain (D0).
Our model definesP (e, f) =?D?
{D1,D0}P (D)P (e, f | D), using a latent domain variable D ?
{D0, D1}.
Using bi-directional translation models, this leads to a domain prior P (D), domain-dependent translation modelsPt(?
|?, D) and language models Plm(?
| D) as in Equation 1:P (e, f | D) =12?
{Plm(e | D)Pt(f | e, D) + Plm(f | D)Pt(e | f , D)} (1)This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1928For efficiency we assume IBM Model I alignments a and translation tables t(?
), e.g., Pt(e |f , D) ?
?a?it(ei|fai, D).
Language models (LMs) Plmare trained separately, albeit one problem not ad-dressed by earlier work is how to train out-domain LMs given only in- and mix-domain data?In our model, initially both the translation and LM probabilities estimated from Cinserve as priors forweighting sentence pairs in Cmixas being more relevant for in-domain translation than not.
This initialweighting reveals pseudo out-domain data in Cmix, which we use to train out-domain language modelsas well as initialize out-domain word alignment tables.1With these sharpened translation and languagemodels, training commences using a version of EM (Dempster et al., 1977).
Because the potentiallyrelevant data in Cmixmight be a superset of any in-domain data, the estimates from Cinserve merelyas initial model estimates.
Metaphorically, iterative EM training resembles party invitationson social networks (hence, the Invitation model): if initially in/out-domain sentence pairs (the hosts)invite some sentence pairs from Cmix, in the next iteration the new pseudo in/out-domain sentenceshelp invite more sentence pairs.
In EM, sentence pairs receive weighted, rather than absolute, invitationsfrom in- and out-domain models.We present extensive experiments on a rather difficult selection task exploiting a large mix-domaincorpus of 4.61M sentence pairs.
Initially we conduct intrinsic evaluation on the mix-domain corpuswhere we also hide in-domain data and seek to retrieve it.
Subsequently we conduct full MT experimentsover the task.
The results show that our Invitation model gives far better selections as well as translationperformance than the baseline trained on the large data Cmix.1 Invitation models of weighting and selectionBy now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod etal., 2011; Gasc?o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013).Data selection has a different (but complementary) goal than domain adaptation, which aims at adaptingan existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007;Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language modeladaptation (Eidelman et al., 2012).
Our setting is in line with data selection approaches (Moore andLewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting(Matsoukas et al., 2009; Foster et al., 2010).
In this paper we explicitly draw attention to the specialcase of a mix-domain parallel corpus consisting of a large and rather diverse set of domains.Our model assigns to every sentence pair ?f , e?
?
Cmixa probability as in Equation 2:P (D | f , e) =P (f , e, D)?D?
{D1,D0}P (f , e, D)(2)P (f , e, D) =12?
P (D)?
{Plm(e | D)Pt(f | e, D) + Plm(f | D)Pt(e | f , D)}Viewed as learning two latent corpora C1and C0, the task is to assign every ?f , e?
?
Cmixan expectedcount P (Dx| f , e) that it is in Cx?
{C0, C1}.
Next we discuss the model components each in turn.The domain-dependent translation models Pt(?
|D) can be viewed as modeling the probability that etranslates as f in domain D ?
{D0, D1}.
Given f = f1, f2, .
.
.
, fmand e = e1, e2, .
.
.
, el, we assume(hidden) alignments a = a1, a2, .
.
.
, amakin to IBM Model I (Brown et al., 1993):Pt(f ,a | e, D) =(l + 1)m?mj=1t(fj|eaj, D) (3)Pt(f | e, D) =?aPt(f ,a|e, D) =(l + 1)m?mj=1?li=0t(fj|ei, D).
(4)1Earlier work on data selection exploits the contrast between in-domain and mix-domain instead of (pseudo) out-domainlanguage models.
However, the mix-domain language models trained on a mix of rather diverse set of domains could beconsidered kind of wide-coverage, which makes for a rather weak contrast with the in-domain language models.1929where t(fj|eaj, D) is the domain-dependent lexical probability of fjgiven eajwith respect to D. Onecrucial aspect about model inspired by IBM-Model-I is that Pt(f | e, D) can be estimated efficiently, asin Equation 4.
This makes the training particularly efficient as detailed in Section 2The in-/out-domain source and target language models are not the same as in previous work, e.g.,(Axelrod et al., 2011), which employ in-/mix-domain language models.
This makes explicit the difficultyin finding data to train out-domain language models, and we present a solution in Section 2.The domain priors P (D1) and P (D0) represent the percentage of the pairs that are in-/ and outdomain respectively in Cmixlearned by our model.
Their estimate during training might be a reasonableselection cut-off threshold.
However, we found that it is not entirely clear whether these cut-off criteriamight exclude other relevant/irrelevant pairs that are not exactly in-domain.
We leave this extension forfuture work.2Finally, it should be noted that the domain-dependent word alignment model, t(f |e,D) is a gener-alization of the standard (domain-independent) word alignment model, t(f |e), in which, t(f |e,D) =t(f |e)t(D|f,e)?ft(f |e)t(D|f,e).
Here, t(D|f, e) can be thought of as the latent word-relevance models, i.e., the proba-bility that a word pair is relevant for in- (D1) or out-domain (D0).
Empirical results (beyond the scopeof this work) show that training the latent in-domain alignment model, t(f |e,D1) often gives bettertranslation systems than training the standard (domain-independent) alignment model, t(f |e).2 TrainingWith all language models trained separately, our selection model can be viewed to have two sets ofdomain-dependent parameters ?
= {?D0,?D1}.
The parameters ?Dconsist of the domain-dependentlexical parameters (e.g., t?D(f |e,D), t?D(e|f,D)) and the domain prior parameter (e.g., P?D(D)).Our training procedure seeks the parameters ?
that maximize the log-likelihood of Cmix:L =?f ,elogP?
(f , e) =?f ,elog?D?aP?D(a, D, f , e) (5)Because of the latent variables a and D, there is no closed form solution and the model is fit using theEM algorithm (Dempster et al., 1977).
EM can be seen to maximize L via block-coordinate ascent on alower bound F(q,?)
using an auxiliary distribution over the latent variables q(a, D|f , e)L ?
F(q,?)
=?f ,e?D?aq(a, D | f , e) logP?D(a, D, f , e)q(a, D | f , e)(6)where the inequality results from log being concave and Jensen?s inequality.
We rewrite the Free energyF(q,?)
(Neal and Hinton, 1999) as follows:F(q,?)
=?f ,e?D?aq(a, D | f , e) logP?D(a, D, f , e)q(a, D | f , e)=?f ,e?D,aq(a, D | f , e) logP?D(a, D | f , e)q(a, D | f , e)+?f ,e?D,aq(a, D | f , e) logP?
(f , e)=?f ,elogP?
(f , e)?KL[q(a, D | f , e) || P?D(a, D|f , e)] (7)where KL[?||?]
is the KL-divergence.
To find q?
(a, D|f , e) that maximizes F(q,?):q?
(a, D|f , e) = argmaxq(a,D|f ,e)F(q,?)
= argminq(a,D|f ,e)KL[q(a, D|f , e)||P?D(a, D|f , e)]= P?D(a, D|f , e) = P?D(D|f , e)P?D(a|f , e, D).
(8)2We especially thank an anonymous reviewer who gave valuable comments related to this point.1930HereP?D(a|f , e, D) =P?D(f ,a|e,D)P?D(f |e,D)=?mj=1t(fj|eaj, D)?mj=1?li=0t(fj|ei, D)(9)The distribution q?
(a, D|f , e) together with q?
(D|f , e) =?aq?
(a, D|f , e) = P?D(D|f , e) can beused to softly fill in the values of a and D respectively to estimate model parameters.We now state our derived EM update formulas.
We use the notation P(c)and t(c)for current iterationestimates, and P(+)and t(+)for the re-estimates.
We denote the expected counts that e aligns to f inthe translation (f |e) with respect to a domain D with c(f |e; f , e, D).
Similarly, we denote the expectedcount of (f |e) with respect to a domain D by c(D; f , e).E-step ?D ?
{D0, D1} doc(D; f , e) = P(c)(D | f , e)c(f |e; f , e, D) = P(c)(D | f , e)t(c)(f | e,D)?li=0t(c)(f | ei, D)?mj=1?
(f, fj)?li=0?
(e, ei)M-step ?D ?
{D0, D1} dot(+)(f |e,D) =?f ,ec(f |e; f , e, D)?f?f ,ec(f |e; f , e, D)P(+)(D) =?f ,ec(D; f , e)?D?f ,ec(D; f , e)To re-estimate P (D | f , e) we substitute the M-step estimates into Equations 3, 4 and 2.
We initial-ize translation tables t(f |e,D1) and t(e|f,D1) with non-zero estimates obtained from applying IBMmodel I to in-domain corpus Cin.3Before EM training starts we must train the LMs.
The in-domainLMs Plm(e|D1) and Plm(f |D1) are trained on the source and target sides of Cinrespectively.
For theout-domain LMs Plm(e|D0) and Plm(f |D0) we need an out-domain data set to train them.
It would alsobe reasonable to use the set to train the out-domain tables, t(?
| ?, D0).
This raises an hitherto unattendedquestion regarding how to construct such an out-domain data set.Inspired by burn-in in sampling, initially we isolate all LMs from our model to train the translationmodels for a single EM iteration; we initialize the model with a translation table constructed on Cinand uniform otherwise.
Using the re-estimates, we score sentence pairs in Cmixwith P (D1|f , e) andselect a burn-in subset of smallest scoring pairs as pseudo out-domain data which can be used to trainPlm(e|D0) and Plm(f |D0).
Choosing the optimal size of this subset is difficult, but in practice, weusually choose a subset that has similar size (number of words) to the given in-domain corpus.
Therationale behind this choice is to avoid the risk that pseudo out-domain models would dominate the in-domain models during further training.
We observe that choosing the same size for a pseudo out-domaincorpus is not guaranteed to always give optimal performance, and this point deserves further study.Finally, once the domain-dependent LMs have been trained, the domain-dependent LM probabilitiesstay fixed during EM.
Crucially, it is important to scale the probabilities of the four LMs to make themcomparable: we normalize the probability that a LM assigns to a sentence by the total probability thisLM assigns to all sentences in Cmix.3 Experimental settingWe carry out experiments in data selection (intrinsic) as well as in machine translation (extrinsic).
Webuild an English-Spanish mix-domain corpus consisting of a large and rather varied set of domains (a3Note that in practice, we usually use only one iteration to train IBM Model I.
To simplify the implementation, we ignorefactor(l+1)min the model (Equation 3), which serves a minor role.
It should be also noted that we set a (small) threshold,e.g., t(?|?, ?)
= 0.0001 for all word pairs that do not occur in the in-domain corpus to avoid over-fitting.1931haystack) in a way that allows us to directly measure selection quality.
Starting out from a general-domain corpus Cgconsisting of 4.51M sentence pairs, collected from multiple resources includingEuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, News Commentary, TAUS Software,TAUS Hardware, and TAUS Pharmacy, and a 177K in-domain (TAUS Legal) sentence pairs.We create Cmixby selecting an arbitrary 100K pairs of in-domain set and adding them to Cg; theremaining 77K in-domain pairs constitute Cin.
We think of this as hiding in-domain data in Cmixsowe can evaluate our ability to retrieve it; in this setting we can evaluate selection directly using pseudo-precision/recall defined as the percentage of selected in-domain pairs to the total selected or to the hidden100K pairs respectively.Table 1 summarizes the data and the translation task.
It should be noted that a mix-domain corpus,that contains a large and rather varied set of domains, frequently contains subsets with a vocabulary thatis close to the in-domain adaptation task; in this case, e.g., Europarl and TAUS Legal share big portionsof their source vocabulary, whereas their translations could differ.
This makes the selection task far moredifficult than assumed by previous approaches as we will show next.Task Corpora English SpanishMix-Domain Corpus (4.51M sents) 125, 339, 057 139, 655, 311TAUS LegalIn-Domain Corpus (77K sents) 1, 555, 342 1, 733, 370Dev (2K sents) 27, 983 30, 501Test (2K sents) 45, 736 48, 999Table 1: The data preparation - training, dev and testing corpora (size in words).
Note that the dev setcontains sentences of 10-25 words, while the test set contains sentences that vary substantially in length,from 5-10 words up to 45-50 words.Our Invitation model takes 3 EM-iterations to train.4We then weigh sentence pairs under our modelwith P (D1| e, f).
We test various baseline models, including the bilingual cross-entropy differencemodel, and the two cross-entropy difference models (on the source language and on the target lan-guage).5We report pseudo-precision/recall at the sentence-level using a range of cut-off criteria forselecting the top scoring instances in the mix-domain corpus.We use Moses (Koehn et al., 2007) with GIZA++ (Och and Ney, 2003) and k-best batch MIRA(Cherry and Foster, 2012).
Final MT systems use the same non-adapted language models trained on2.2M English Europarl sentences plus 248.8K sentences from News Commentary Corpus (WMT 2013).We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER(Snover et al., 2006).
Statistical significance uses 95% confidence intervals using paired bootstrapre-sampling (Press et al., 1992; Koehn, 2004).
The k-best batch MIRA optimizer (Cherry and Foster,2012) was run at least three times to optimize any SMT system to avoid instability (Clark et al., 2011).64 ResultsTable 2 presents the results showing substantial improvement in selection performance compared to allthe baselines.
Subsequently we build SMT systems over the selected subsets.
We report the transla-tion yielded by these systems over the task in Table 2 as well.
It can be easily seen that the baselineapproaches that simply train on in- and mix-domain data do not work that well for a difficult selectiontask from a mix-domain corpus consisting of a large and rather diverse set of domains.
The SMT sys-4To train the LM probs, we construct interpolated 4-gram Kneser-Ney language models using BerkeleyLM (Pauls andKlein, 2011).
This setting for training language models is used for all experiments in this work.5The script we use to train these models is developed by Luke Orland and available at: https://github.com/lukeorland/moore\_and\_lewis\_data\_selection.6Note that metric scores for the systems are averages over multiple runs.1932Cut-off ModelIn-domainPairspseudo-Precisionpseudo-RecallBLEU METEOR TER50KCE Difference (source side) 370 0.74 0.37 20.5 28.0 62.3CE Difference (target side) 375 0.75 0.38 19.3 26.8 63.3Bilingual CE Difference 413 0.83 0.41 18.7 26.3 64.3Invitation 19156 38.31 19.16 36.5 36.4 47.1100KCE Difference (source side) 592 0.59 0.59 24.8 30.8 57.8CE Difference (target side) 572 0.57 0.57 22.1 29.7 60.1Bilingual CE Difference 649 0.65 0.65 23.1 30.0 58.9Invitation 30474 30.47 30.47 37.1 36.9 47.0150KCE Difference (source side) 753 0.50 0.75 26.4 32.0 56.2CE Difference (target side) 742 0.49 0.74 23.9 31.2 58.8Bilingual CE Difference 793 0.53 0.79 24.4 30.9 58.1Invitation 38424 25.62 38.42 37.1 37.0 46.7200KCE Difference (source side) 874 0.44 0.87 26.6 32.4 56.0CE Difference (target side) 888 0.44 0.88 25.8 32.1 57.2Bilingual CE Difference 932 0.93 0.65 25.7 32.0 57.0Invitation 44392 22.17 44.39 37.5 37.4 46.2250KCE Difference (source side) 994 0.40 0.99 27.3 32.8 55.4CE Difference (target side) 997 0.40 0.10 26.3 32.4 56.3Bilingual CE Difference 1062 0.42 1.06 26.6 32.7 55.6Invitation 49419 19.77 49.42 37.3 37.3 46.1300KCE Difference (source side) 1122 0.37 1.12 28.2 33.4 54.5CE Difference (target side) 1093 0.36 1.09 26.4 32.7 56.0Bilingual CE Difference 1169 0.39 1.17 27.8 33.3 54.9Invitation 53892 17.96 53.89 37.7 37.5 46.0Table 2: Systematic comparison between selection models.tems trained on the selection of our model perform significantly and consistently better (with p-value= 0.0001 for all cases) than the others trained on the selection of the baselines.SentencesBilingual CE Difference1by assisting in the placement and financing of used and end-of-lease aircraft , atr asset management has helped broadenatr ?s customer base , notably in emerging markets , by providing quality reconditioned aircraft at attractive prices andhas helped maintain residual values of used aircraft .al participar en la colocaci?on y en la financiaci?on de los aviones usados al final del per?
?odo de arrendamiento , atrgesti?on de activos ha podido ampliar la base de su clientela , en particular en los pa?
?ses de econom?
?as emergentes , alproporcionar aparatos entregados en buen estado a precios interesantes y ha contribuido a mantener el valor residualde los aviones usados .2in contrast , recent improvements in western europe are not expected to be reversed significantly .en cambio no se espera que las recientes mejoras en europa occidental se inviertan significativamente .3creating xml file ...creando el archivo xml ...Invitation Model1as she has said , the harmonisation of the requirements for information to appear on the invoice will mean that tradersoperating within the single market will be subject to a single legislation , while until now they have had to know , complywith and apply fifteen different legislations .como ella ha dicho , la armonizaci?on de los requisitos de informaci?on que deben constar en la factura permitir?a a loscomerciantes que operen en el mercado interior sujetarse a una sola legislaci?on , mientras que hasta ahora ten?
?an queconocer , sujetarse y aplicar quince legislaciones diferentes .2the solicitation documents shall specify the estimated period of time following dispatch of the notice of acceptance thatwill be required to obtain the approval .en el pliego de condiciones se indicar?a el plazo de tiempo previsto , a partir de la expedici?on del aviso de aceptaci?on ,que ser?a requerido para obtener la aprobaci?on .3there is no doubt that disadvantages will result for the consumer and for the manufacturer of branded goods , for examplewith regard to consumer health protection .ello generar?a , sin duda alguna , desventajas para el consumidor y el productor de art?
?culos de marca , entre otrosaspectos tambi?en en lo que se refiere a la protecci?on de la salud del consumidor .Table 3: Top pairs from mix-domain corpus with highest scores according to models.Table 3 presents some random top ranked sentence pairs from the bilingual cross-entropy difference1933Cut-off: 50K Cut-off: 100K Cut-off: 200KModel English Spanish English Spanish English SpanishCE Difference (source side) 8.65 8.70 11.92 12.21 15.50 16.22CE Difference (target side) 8.14 10.09 11.61 14.13 15.45 18.50Bilingual CE Difference 7.03 8.16 10.38 11.96 14.34 16.43Invitation 40.16 44.70 37.30 41.59 34.32 38.32Table 4: Average words in selected sentences.model against our Invitation model for the task.
This shows clearly more relevant pairs for our selectionmodel than for the baselines.
It should be noted that the baseline models tend to prefer shorter sentences,while our model suffers less from this kind of bias.
Table 4 presents the average length (in words) ofselected sentences selected by different models over various cut-offs.Cut-off ModelIn-domainPairspseudo-Precisionpseudo-RecallBLEU METEOR TER300KWithout Translation Model 34156 11.39 34.16 35.8 36.6 47.3Without Language Model 51991 17.33 51.99 37.4 37.4 46.6Full model 53892 17.96 53.89 37.7 37.5 46.0Table 5: Experiments exploring the roles of individual components in our model.Which component type (language or translation models) contributes more to performance?
We neu-tralize each component in turn and build a selection system with the remaining model parameters.
Ta-ble 5 shows translation models are crucial for performance, while domain-dependent LMs make a small,yet noteworthy contribution.
It should also be noted that using the LMs derived separately from in- andout-domain data yields far better performance than the LMs derived from in- and mix-domain data forthis task.System Phrases BLEU METEOR TERLarge data Cmix236.74M 36.8 37.2 47.1Subset of 300K 22.47M 37.7 37.5 46.0Table 6: Translation accuracy comparison.Finally, we compare a system trained on a selection of the top scored 300K sentences to a baselinelarge-scale SMT system trained on Cmix(4.61M sentences).
The baseline trained on Cmixworks with236.74M phrase pairs, whereas the Invitation trained system employs a small table of 22.47M phrases.Tabel 6 shows the results.
It is interesting that the small MT system trained by Invitation performssignificantly better (with p-value = 0.0001 for all metrics) than the large-scale system baseline trainedon all of Cmix.Inputcada estado miembro supervisar?a la categor?
?a cient?
?fica de la evaluaci?on y las actividades de los miembrosde los comit?es y de los expertos que haya designado, pero se abstendr?a de darles instrucciones incompatiblescon las funciones que les competen.Referenceeach member state shall monitor the scientific level of the evaluation carried out and supervise the activitiesof members of the committees and the experts it nominates, but shall refrain from giving them any instructionwhich is incompatible with the tasks incumbent upon them.Large Cmixeach member state will oversee the category scientific assessment and the activities of members of the com-mittees and experts which designated, but abstain of instruct incompatible with their regulatory functions.Subset 300Keach member state will monitor the scientific category of the evaluation and the activities of the membersof the committees and of experts who has designated, but refrain from giving them instructions incompatiblewith the required functions assumed.Table 7: Translation example yielded by systems.To give a sense of the improvement in translation, we present an example in Table 7.
The exampleis indeed illuminating because it shows the difference in choice between the mix-domain system and1934our selection-trained system.
The example shows different translation pairs: ?supervisar?a-monitor?
vs.?supervisar?a-oversee?, ?evaluaci?on-evaluation?
vs.
?evaluaci?on-assessment?, and ?abstendr?a de-refrainfrom?
vs. ?abstendr?a de-abstain?.
Table 8 presents phrase table entries, i.e., p(e | f) and p(f | e), for thepairs of words in each system.supervisar?a evaluaci?on abstendr?a deSystem Entry monitor oversee evaluation assessment refrain from abstainLarge data Cmix?
(e|f) 0.002 0.020 0.579 0.429 0.002 0.013?
(f |e) 0.119 0.081 0.391 0.403 0.014 0.060Subset of 300K?
(e|f) 0.012 0.024 0.487 0.357 0.015 ??
(f |e) 0.203 0.072 0.338 0.417 0.143 ?Table 8: Phrase entry examples.
Note that the system trained on the subset of top 300K pairs of sentencesdoes not contain the phrase pair ?refrain from-abstain?.5 Final Machine Translation experiments: Putting all data togetherFor final adaptation evaluations we follow (Koehn and Schroeder, 2007; Nakov, 2008) and (Axelrod etal., 2011; Sennrich, 2012), by passing multiple phrase tables directly to the Moses decoder and tuninga system using these different tables together.
Table 9 presents the result, showing the consistent im-provement of adaptation with Invitation model compared to the baselines (with p-value = 0.0001 for allcases) over the mixture data Cmix.Data System BLEU METEOR TERIn-domain 36.66 37.19 44.7650K+ CE Difference (source side) 37.1 36.7 48.1+ CE Difference (target side) 37.1 36.6 48.2+ Bilingual CE Difference 37.1 36.6 48.2+ Invitation 38.0 37.2 47.3100K+ CE Difference (source side) 37.3 36.8 47.9+ CE Difference (target side) 37.2 36.8 48.0+ Bilingual CE Difference 37.2 36.8 48.0+ Invitation 38.4 37.4 46.9150K+ CE Difference (source side) 37.1 36.9 48.2+ CE Difference (target side) 37.3 36.9 47.9+ Bilingual CE Difference 37.0 36.8 48.1+ Invitation 38.6 37.5 46.6200K+ CE Difference (source side) 37.3 36.9 47.7+ CE Difference (target side) 37.3 36.9 47.9+ Bilingual CE Difference 37.3 36.9 47.8+ Invitation 38.4 37.6 46.7250K+ CE Difference (source side) 37.4 36.9 47.7+ CE Difference (target side) 37.3 37.0 47.7+ Bilingual CE Difference 37.3 37.0 47.8+ Invitation 38.6 37.7 46.5300K+ CE Difference (source side) 37.3 37.0 47.8+ CE Difference (target side) 37.1 37.0 48.0+ Bilingual CE Difference 37.3 36.9 47.8+Invitation 38.9 37.9 46.3Table 9: Translation results from our domain-adapted SMT systems.Finally, we also test the adaptation evaluations between the system trained on the small selection oftop 300K sentences against the large-scale SMT system trained on Cmixwhen combined with the in-domain trained system.
Table 10 presents the results, revealing comparable translation performance,although they are trained on data sets that are significantly different in size.1935System BLEU METEOR TERIn-domain + Large data Cmix39.0 38.0 46.3In-domain + Subset of 300K 38.9 37.9 46.3Table 10: Translation results from our domain-adapted SMT system and the large-scale SMT system.Note that the baseline is slightly better than our domain-adapted SMT system under BLEU and ME-TEOR, however, not statistically significant.6 Final notes on mix-domain data selectionThe specific data selection scenario studied in this paper brings up different aspects that did not receive(sufficient) attention in earlier work on data selection and domain adaptation:?
The mix-domain parallel corpus Cmixcontains a large variety of domains that overlap and but alsodiffer in lexical choice and translation.
This is radically different from the in-/out-domain settingusually assumed in adaptation and constitutes a major challenge for existing selection approaches.?
The way the small in-domain corpus relates to the large mix-domain corpus is also challengingbecause translation performance often depends on selecting relevant sentence pairs, aside fromthose that are clearly in-domain.?
The lack of out-domain data in a realistic mix-domain scenario, suggests that efforts are neededat finding data that contrasts enough with the in-domain data.
In this work we propose an initialtraining period (burn-in) for isolating pseudo out-domain data.
But it might be that relevance-related approaches could also turn out more effective for this.In our current model we implement the P (e | D) and P (f | D) as language models, inspired by theapproaches based on the contrast between the cross-entropies of in- and mix-domain language models(Moore and Lewis, 2010; Axelrod et al., 2011).
However, P (e | D) and P (f | D) should work withrelevance models, i.e., assessing the relevance of sentences to domain D. Relevance is a different con-cept than fluency as embodied by language models, and this aspects demands special attention in futurework.7In ongoing large-scale experiments, we now explore the behavior of our Invitation model on a varietyof different data settings and compare that to a range of alternative existing approaches.
We are alsoexploring new variations of our Invitation model to find out what the optimal settings might be fordifferent mixes of domains.
So far we find that the burn-in and size of pseudo out-domain selectionafter burn-in can be important in certain situations.
We also observe that estimating the suitable sizeof the selection set is also a topic that demands more attention because the estimate of P (D1) withthe interpretation percentage of relevant data in Cmixlike likely to demand suitable relevance modelsinstead of language models.We observe that the present Invitation model could be approached from a discriminative perspective,which could be effective for specific data settings.
Finally, it is theoretically not clear whether a singleapproach will be most effective for all practical data scenarios.7 ConclusionsThis work looks at modeling the relevance of sentence pairs from the mix-domain corpus to a task repre-sented by an in-domain sample.
In contrast with previous work we cast this as a translation problem witha latent domain variable.
Our Invitation model based on iterative weighted Invitations using EM, offersa new view on data selection for MT.
Our model also offers principled cut-off points for selecting in-domain and other relevant subsets.
Experiments on the in-domain task shows our approach outperformsthe existing data selection for such a very complex mixture training data.7We thank Amir Kamran for bringing this difference to our attention through ongoing joint experimental work.1936The high accuracy in our experiments in this kind of data compared to the baseline suggests that ourmodel might also offer good estimates that can be used for data weighting.
In future work we aim to testthe Invitation model for instance weighting and explore avenues for using it for selecting and weightingsub-sentential translation pairs (e.g., phrase pairs) that can be used directly for building SMT systems.A further issue is to improve the quality of word alignments induced for mix-domain corpora.
We alsoaim at exploring a discriminative learning approach in conjunction with our model.AcknowledgementsThe first author is supported by the EXPERT (EXPloiting Empirical appRoaches to Translation) InitialTraining Network (ITN) of the European Union?s Seventh Framework Programme.
We thank Transla-tion Automation Society (TAUS.com) for providing us with suitable data for the mix-domain scenario.We also thank Amir Kamran and Bart Mellebeek for help and collaboration on experiments related todata selection and domain adaptation.
We thank Milo?s Stanojevi?c and three anonymous reviewers fortheir valuable comments on earlier versions.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.
Domain adaptation via pseudo in-domain data selection.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages355?362, Stroudsburg, PA, USA.
Association for Computational Linguistics.Pratyush Banerjee, Sudip Kumar Naskar, Johann Roturier, Andy Way, and Josef van Genabith.
2012.
Translationquality-based supplementary data selection by incremental update of translation models.
In Martin Kay andChristian Boitet, editors, COLING 2012, 24th International Conference on Computational Linguistics, Pro-ceedings of the Conference: Technical Papers, 8-15 December 2012, Mumbai, India, pages 149?166.
IndianInstitute of Technology Bombay.Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: parameter estimation.
Comput.
Linguist., 19:263?311, June.Boxing Chen, George Foster, and Roland Kuhn.
2013.
Adaptation of reordering models for statistical machinetranslation.
In Proceedings of the 2013 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, pages 938?946, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Colin Cherry and George Foster.
2012.
Batch tuning strategies for statistical machine translation.
In Proceedingsof the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, NAACL HLT ?12, pages 427?436, Stroudsburg, PA, USA.
Association forComputational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer instability.
In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT?11, pages 176?181, Stroudsburg, PA, USA.
Association for Computational Linguistics.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Maximum likelihood from incomplete data via the emalgorithm.
JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1?38.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3: Automatic metric for reliable optimization and evaluationof machine translation systems.
In Proceedings of the Sixth Workshop on Statistical Machine Translation,WMT ?11, pages 85?91, Stroudsburg, PA, USA.
Association for Computational Linguistics.Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada.
2013.
Adaptation data selection usingneural language models: Experiments in machine translation.
In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume 2: Short Papers), pages 678?683, Sofia, Bulgaria, August.Association for Computational Linguistics.1937Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik.
2012.
Topic models for dynamic translation modeladaptation.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:Short Papers - Volume 2, ACL ?12, pages 115?119, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA.
Association forComputational Linguistics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.
Discriminative instance weighting for domain adaptationin statistical machine translation.
In Proceedings of the 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 451?459, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Guillem Gasc?o, Martha-Alicia Rocha, Germ?an Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco Casacuberta.2012.
Does more data always yield better translations?
In Proceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Linguistics, EACL ?12, pages 152?161, Stroudsburg, PA, USA.Association for Computational Linguistics.Barry Haddow and Philipp Koehn.
2012.
Analysing the effect of out-of-domain data on smt systems.
In Proceed-ings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 422?432, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ann Irvine, John Morgan, Marine Carpuat, Hal Daume III, and Dragos Munteanu.
2013.
Measuring machinetranslation errors in new domains.
Transactions of the Association for Computational Linguistics (TACL).Philipp Koehn and Barry Haddow.
2012.
Towards effective use of training data in statistical machine transla-tion.
In Proceedings of the Seventh Workshop on Statistical Machine Translation, Montreal, Canada, June.Association for Computational Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experiments in domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227,Stroudsburg, PA, USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra Constantin, andEvan Herbst.
2007.
Moses: Open source toolkit for statistical machine translation.
In Proceedings of the45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180,Stroudsburg, PA, USA.
Association for Computational Linguistics.Philipp Koehn.
2004.
Statistical significance tests for machine translation evaluation.
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpus for Statistical Machine Translation.
In Conference Proceed-ings: the tenth Machine Translation Summit, pages 79?86, Phuket, Thailand.
AAMT, AAMT.Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang.
2009.
Discriminative corpus weight estimation formachine translation.
In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Pro-cessing, pages 708?717, Singapore, August.
Association for Computational Linguistics.Robert C. Moore and William Lewis.
2010.
Intelligent selection of language model training data.
In Proceedingsof the ACL 2010 Conference Short Papers, ACLShort ?10, pages 220?224, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Preslav Nakov.
2008.
Improving english-spanish statistical machine translation: Experiments in domain adapta-tion, sentence paraphrasing, tokenization, and recasing.
In Proceedings of the Third Workshop on StatisticalMachine Translation, StatMT ?08, pages 147?150, Stroudsburg, PA, USA.
Association for Computational Lin-guistics.Radford M. Neal and Geoffrey E. Hinton.
1999.
A view of the em algorithm that justifies incremental, sparse,and other variants.
In Michael I. Jordan, editor, Learning in Graphical Models, pages 355?368.
MIT Press,Cambridge, MA, USA.1938Franz Josef Och and Hermann Ney.
2003.
A systematic comparison of various statistical alignment models.Comput.
Linguist., 29(1):19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalu-ation of machine translation.
In Proceedings of the 40th Annual Meeting on Association for ComputationalLinguistics, ACL ?02, pages 311?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.Adam Pauls and Dan Klein.
2011.
Faster and smaller n-gram language models.
In Proceedings of the 49thAnnual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1,HLT ?11, pages 258?267, Stroudsburg, PA, USA.
Association for Computational Linguistics.William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery.
1992.
Numerical Recipes in C(2Nd Ed.
): The Art of Scientific Computing.
Cambridge University Press, New York, NY, USA.Rico Sennrich.
2012.
Perplexity minimization for translation model domain adaptation in statistical machinetranslation.
In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA.
Association for Computational Linguis-tics.Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla, and J. Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings of Association for Machine Translation in the Americas, pages223?231.1939
