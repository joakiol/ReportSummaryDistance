Clustering Co-occurrence Graph based on TransitivityK,,mlko TANAKA-IshiiElectrotechnical L boratory1-1-4 Umezono, Tsukuba~ Ibaragi 305 JAPAN.klmiko~etl, go.
jpHideya IWASAKITokyo Univ.
of Agriculture and Technology2-24-16 Naka-cho, Koganei, Tol~'o 184 JAPA.N.iwasaki@ipl, el.
runt.
ac.
jpAbst ractWord co-occurrences form a graph, regardingwords as nodes and co-occurrence r lations asbranches.
Thus, a co-occurrence graph can beconstructed by co-occurrence r lations in a cor-pus.
This paper discusses a clustering methodof the co-occurrence graph, the decompositionof the graph, from a graph-theoretical view-point.
Since one of the applications for theclustering results is the ambiguity resolution,each output cluster is expected to have no am-biguity and be specialized in a single topic.
Weobserved that a graph has no ambiguity if itsbranches representing co-occurrence r lationsare transitive.
An algorithm to extract suchgraphs are proposed and its uniqueness of theoutput is discussed.
The effectiveness of ourmethod is examined by an experiment usingco-occurrence graph obtained from a 30M bytescorpus.1 In t roduct ionClustering is the  operation to group words bysome criterion.
Thesauri and synonym dictio-naries are some of its manual examples.
Auto-matic outputs can be used not only to revisethem, but also to aid ambiguity resolution, anessential problem in natural anguage process-ing.
For instance, the me~ing  of an ambiguousword can be decided by e.xamln'i~g the duster itbelongs to.
Furthermore, clusters grouped ac-cording to topics have many application areassuch as automatic document classification.
Theinput in this paper is the word co-occurrencegraph obta~ued from corpus.
The output is itssubgraphs with the condition that each sub-graph is specialized in a topic.Many automatic clustering methods havebeen already proposed.
Most of them are basedon the statistical similarity between two words.Our approach is different; it is graph theoreti-cal.
We tried to find out the special structurein linguistic graph.Having a huge co-occurrence graph obtainedfrom a corpus, we first tried to decomposeit to analyze its graph structure using graphtheoretical tools, such as maximum stronglyconnected components, or biconnected compo-nents.
Although both tools decompose a graphinto tightly connected subgraphs, these trialsresulted in vain.
The question arose; what mustbe taken into account o decompose the co-occurrence graph.
7 The answer is the ambigu-ity.
Furthermore, we reached to the conclusionthat the ambiguity can be explained in termsof intransitivity.
This feature is developed intoan algorithm for clustering.This paper is organized as follows.
The fol-lowing chapter describes the relationship be-tween the transitivity in the graph and theambiguity resolution.
Chapter 3 shows the re-lationships between clustering and transitivity.Chapter 4 proposes and discusses an algorithmfor clustering.
Related work is resumed inChapter 5.
Our method is examined in Chap-ter 6 by some experiments.2 Word  Ambigu i ty  and  Trans i t iv i tyTwo words are said to co-occur when they fre-quently appear close to each other within texts.Regarding words as nodes and co-occurring re-91latious as branches, a graph can be constructedfrom a given corpus.
We define such a graphas co -occur rence  graph.When a portion of a corpus specializes in atopic, we can sti\]l extract a co-occurrence graphfrom the portion.
A general corpus, such asnewspaper corpus, contains many corpus por-tious, each specializing in one topic.
Therefore,the whole co-occurrence graph obtained froma general corpus cont~.in.q subgraphs, each spe-cializing in one topic.
Our question is to extractsuch subgraphs of topics from a co-occurrencegraph.We denote V as the set of nodes (words), Eas the set of branches (co-occurrence r lations),G=< V, E ~ as an input graph and I1/"1 as thenumber of nodes.
English words referred asexamples will be w~itten in this font.2.1 TrAnsit iv i ty in Co-occur rence  Rela-t ionThe most basic mathematical laws discussedabout relations between elements in a set arereflective, symmetric and transitive laws.
Hav-ing a, b, c E V and R as a relation, they can bedescribed as follows:Ref lect ive aRa.Symmetr ic aRb ~ bRa.Trans i t ive aRb, bRc ~ arc.Let V be word set and R be co-occurrence r -lation.
When each property holds for .EL, wordsa, b and c can be explained as follows from thelinguistic viewpoint:Reflective Word a co-occurs with itself.Symmetr ic  Co-occurrence relation does notdepend on the occurrence order.Transitive Word b does not have two-sidedmeanings (ambiguity).
For instance, doc-tor, which has both medical and academicmeanings, co-occurs with nurse within amedical topic, and co-occurs with professorwithin an academic topic.
However, nurseand professor do not co-occur, so the tran-sitivity between nurse: doctor and professordoes not hold.
!IG @ a Cba c a c' db % - i |(5) d (6) e dc c cb bb(7) a d (8) dc @ cO. = ~.
.=mO~clmr branch duplicate brm-tchFigure 1: Graph DecompositionOur request is to extract subgraphs each ofwhich focuses on one topic with no ambiguity.Therefore, we perform clustering by extractingsubgraphs whose branches form transitive co-occurrence relations.3 TrAnsitivity and C luster ing3.1 Decompos i t ion  and Dupl i ca t ionThe simplest case is a graph of three nodes.Figure 1-(1) is a graph in which the transitivitydoes not hold.
For example, when b is doctor,a is nurse and  c is professor, nurse and professordo not co-occur due to the node b's two-sidedmeanings.
Therefore, as in Figure 1-(2), we"duplicate" bso that each duplicated node cor-responds only to a single me~in,~.
Then theambiguity within b is resolved and the entiregraph is divided into two subgraphs, the aca-demic one and the medical one.
To sum up,when the transitivity does not hold, a graphcan be decomposed by duplicating the ambigu-ous node.On the other hand, when the transitivityholds among three nodes (Figure 1-(3)), the92Igraph cannot be decomposed by duplication ofI b (Figure I-(4)).
This can be explained that 2"~::'" Tthe graph does not have the ambiguity.
(1) .= .~.
.q .g~ (2)m~.umm m * m~tt t :~m ~We extend the above into the case of four ' ' "  ~ - "i nodes(Figure 1-(5)).
Here, transitivity does not ?hold in a-b-d because there is no branch be- b btween a-d.
When b--c is duplicated, the graph ~I can be decomposed into two subgraphs (Figure (3) a d (4)a di-(6)) in which the transitivity holds On thecontrary, Figure 1-(7) c~not be decomposed c cby duplicating b-c due to the branch a-d (Fig-ure 1-(8)); this shows that b-c is not ambigu-ous.
Note that Figure 1-(7) is a complete graphof 4 nodes.
We deYme dupl icate branch  as abranch to be duplicated for graph decomposi-tion (such as b-c) and anchor  branch  as abranch which i~hlbit graph decomposition byduplication (such as a-d).In general, when a graph could not be sep-arated by duplicating its subgraph, then thesubgraph is regarded not to have ambiguity.Therefore, ideal clustering is to decomposegraphs into subgraphs which cannot be decom-posed further by duplication.
Unfortunately,this constraint is too strict because such agraph is restricted to a complete graph.
Inaddition, extracting complete graphs withln agiven graph is NP-complete.
Therefore we dis-cuss in the following how to loosen the con-straint.3.2 Transit ive GraphThere are two methods to loosen the con-straint.The first is to decrease the nllmber of anchorbranches.
In the complete graph of more than5 nodes, several anchor branches exist for eachduplicate branch (Figure 2-(1)).
However, onlyone anchor branch is sufficient o inhibit thedecomposition.
The less the number of anchorbranch is, the looser the constraint is.This intuitively corresponds to loosen thesharpness of the focus of the topic in the re-sulting cluster.
For instance, two words pneu-monia and cancer do not always co-occur, butthey do co-occur with words as doctor, nurseand hospital forming the core of medical topics.anchor distance 2 anchor distance nFigure 2: Loose Constraint for Graph Decom-positionPneumonia will be included into a cluster if itis connected with these three words even if itis not connected with cancer.
If cancer is alsoconnected with these three words, both cancerand pneumonia, the different subtopical wordswithin a medical topic are included in a cluster.The second is to loosen the transitivity itself.It was defined in Section 2.1 within three nodes.We may prepare a loose transitivity as follows:VlJ~:~21 - ' .
, ~n_l~:~Vn ---> ~ l_~nWe define anchor  distance as the maximumdistance of the minimum distances of a--b-d anda-c--d. For example, when minimum distanceof a-b-d is 4 and that of a-c-d is 6 then theanchor distance is 6.
The tightest constraintis when anchor distance is 2 as in Figure 2-(3).
This also blurs the topic focus of a cluster.In the example of pneumonia, the word will beincluded if it is connected directly with at leastone of the words among doctor, hospital, nurse,and cancer, and connected indirectly with theothers.For m,n  < IVI- 1, G is called (m,n)-t ransi t ive graph  whenfor all e E E, there ave m anchorbranches et E E of anchor distance_<n.
(m, n)-transitive graphs can be extracted asthe subgraphs of the input graph.
Figure 3shows a map of (m,n)-transitive graphs.
Theaxis of ordinates describes the number of an-chor branches (m).
The axis of abscissas de-93.
?tlght constraint = > Ioeseloose2 -fight 3 -4-?
IV I .
I  -2 4 I -1!
o-~GS1 ....... G52$i---~SO.....?om pier e graphnumber of anchor branches mFigure 3: (m, n)-Transitive Graphscribes the anchor distance (n).
The constraintis loose when m is small and n is large.
For thesame input, (ml,n)-transitive graphs are in-cluded in (m2, n)-transitive graphs when ml <m2, and (m, nl)-transitive graphs are includedin (m, n2)-transitive graphs when n2 < hi.GS2 in Figure 3 is the clusters obtained un-der the loosest constraint: m is the maximumand n is the ~n i~um.
In GS2, all ambiguityof a branch and nodes at its ends are resolved.G$O are the transitive graphs of the tightestconstraint.
All transitive graphs on the hori-zontal line including G$O are complete.4 Ext ract ion  o f  T rans i t ive  GraphsSo far, we did not explain how to detect heduplicate and anchor branches, given a graph.An algorithm for clustering can be top-downor bottom-up.
The former gives clusters by de-composing the input graph by detecting dupli-cate branches.Although we have explained our clusteringmethod top-down up to now, we propose ourclustering method as bottom-up.
We obtainclusters by accumulating adjacent nodes sothat every branch has anchor branches and theresulting clusters include no duplicate branch.Thus, in the bottom-up method, we need no~detect duplicate branches.
This is convenient,because the condition for anchor and dupli-cate branches i  denoted by local relationshipsamong nodes.The branches in the input graph are assumedto be all symmetric.
In this section, we useterms clusters as our output and subgraphsas their candidates.4.1 Definit ion of  ClustersWe extract GS1, the (1, 2)-transitive graph.A subgraph A including a branch e in theinput graph can be extracted as follows:Step 1.
Put a triangle graph including e intoA.Step 2.
Take a branch e ~ in A and a node vwhich makes a triangle with e t (Figure 4).If the following condition is satisfied, putv into A.There exists a node v t E G (in-put graph) whose distance frome ~ is 1, and it is connected to vwith a branch.
Here, the branchJ-~v is the anchor branch so thate t is hindered to be the duplicatebranch in the resulting cluster.Additionally, put every branch betweenv" E G and v into A.Step 3.
Repeat Step 2 until A c~nnot be ex-tended.Performing the above procedure starting fromevery branch in the input graph, we obtainmany subgraphs.
Considering the inclusionrelation between subgraphs, they constitute apartial order (Figure 5).
We define clusters asmaximal subgraphs in this partial order chain.They are subgraphs not included in any othersubgraphs.
The uniqueness of the clusters foran input is self-evident.4.2 A lgor i thm for Cluster ingIn the previous ection, the procedure to obtainsubgraphs hould begin from every branch inthe input.
However, it is su~cient to calculateas follows.Step  O. i --  0IIiliIIlII!IIIaII94YFigure 4: Extraction of (1,2)-Transitive Graph~ D cl/~~ste~Figure 5: Subgrapl~ and their Partial OrderStep 1.
Choose a branch e E G not includedin Go , " ' ,  Gi-1.
If no e is found, go toStep 5.
Gi --- < 0,0 >.
Put a trianglegraph including e into Gi.Step 2 and 3.
Extend Gi using Step 2 and 3of the previous ectionStep 4.
Set i - i q- 1 and goto Step 1.Step 5.
Examine every pair of subgraphs (A,B), andi fA includes B, then drop B. Theremaining subgraphs are defined as clus-ters.A maximal subgraph c~ot  be missed.
Itsstarting branch is encountered without fail inthe above algorithm.
If it is encountered asthestarting branch in Step 1, the maximal sub-graph is obtained.
If it is captured into a sub-graph and becomes e~ in Step 2, the subgraphextends to the size of maximal subgraph; ifit gets larger, the subgraph contradicts beingmaximal as the result of the last section.The algorithm halts since the input graph isfinite, and the output is unique for an input.5 Related Work\[Li and Abe, 1996\] compared clustering meth-ods proposed so far \[Hindle, 1990\]\[Brown et ?1., 1992\ ]  \[Pereira et al, 1993\]\[Tokunaga etel., 1995\]\[Li and Abe, 199@Most of them are so-called hard ch~tering:each word is included only in one cluster.
Wedo not follow the trend, from the sense that ourobjective is the extraction of clusters of topics.It is natural that an ambiguous word should beincluded in different clusters.\[Pereira et ?l., 1993\] adopts sof~ clustering.They measured co-occurrence between ounsand verbs, and clustered nouns of the same dis-tribution of verbs.\[Fukumoto and Tsujii, 1994\]'swork has common motivation with us: the am-biguity should be resolved for clustering.
Theyclustered verbs using the gravity of multivari-ate analysis.\[Sugihara, 1995\]'s approach as a commonpoint in that it focuses on graph structure forclustering and tries to structurize the inputgraph, a bipartite graph of words and concepts(such as food, fruit etc.).
His clustering methodis so called Dlllrnage-Mendelsohn decomposi-tion in graph theory.
The output naturallygives a partial order of clusters which can becompared with conventional thesauri.Our input is not bipartite.
In the begin-ning, we tried to decompose input graph intomaximum strongly connected components toobtain graphs of topics from the observationthat nodes in a cycles are strongly related 1.However, subgraphs about different opics ismerged into the same cluster by two ambiguouswords which bridge these two subgraphs(Figure6).
Next, we observed that articulation odesare ambiguous, so we performed ecomposi-tion into biconnected components.
In this casewhen several biconnected components are con-nected in a ring, articulation odes could notbe detected (Figure 7).
The observation thatthere are no co-occurrence r lationship betweenX\[Tokunagaaud Tanaka, 1990\] discusses on extrac-tion of cycles formed by trauslation relations fzom bilin-gual dictionary.95Figure 6: Problematic ~Structure in MSCCClustering  mblguous wordclusters of dffferem toph:sFigure 7: Problematic Structure in Bicon-nected Component Clusteringtwo biconnected graph across the articulationnode was the start point of this paper.6 Exper iments6.1 P rocedure  of  C luster ingFirst, we make the input graph from a 30Mbytes of Wall Street Journal.
Co-occurrencesof no~s  and verbs are extracted by a morpho-logical analyzer.
We defined that a word co-occurs with 5 words ahead of the word within asentence.
Co-occurrence degree is measured bymutual information\[Church and Hanks, 1990\].We set a certain threshold to the values to ex-tract the input graph.The number of resulting clusters depends onthe input graph as follows:1 One huge connected subgraph.2 Several medium sized subgraphs.3 Many small sized subgraphs.When the threshold value is too high, the out-put is 3.
On the contrary, when it is too low,then the output becomes 1.
Both 1 and 3 areIINumber of dusters of size more than 740 i30 I20 I101 r t - , I ,  .Thresh?Id ~11 2s4ss~Figure 8: Threshold and the Outputnot interesting, because 1 is a graph includingall topics, and 3 generates graphs of too smalltopics to check the global trend of topics in theinput.
Therefore, we varied threshold from 1.0up to 7.0 by 0.5 steps to make the input graph,applied our algorithm to each input in order todetect he best threshold.The result is shown in Figure 8.
The num-ber of clusters whose sizes are more than 7 isplotted against he threshold value.
When thethreshold is 3.5, such dusters were most nu-merous.
In 39 dusters, there were 727 differentwords out of 15347 in the input graph.6.2 Evaluation of ClustersIn Appendix, 39 clusters are shown their con-tents sorted by size.
Words judged inappro-priate in each cluster are attached ~t'- Wordstmdecidable being suitable in their clusters areput "~".All 39 clusters are attached four items as fol-lows:?
Subjectively judged topic?
Cluster size (CS): number of words in acluster?
Error rate (ER): rate of inappropriatewords (attached " t ' )?
Uncertainty rate (UR): rate of uncertainwords (attached "~" and "~")The average of the above items were CS=20,96ER=10.3%, UP,.= 14.7%; hence, the ambiguitywas removed from clusters up to 85% on aver-age.
The number of the cluster whose topic wasinestimable is only 1.
The estimation of topicbecomes clltTicult with two factors, CS and UR.When CS is too small, even when UR is 0.0, thecluster itself lacks in information.
When UR ishigh, it is natural that the topic becomes ines-timable.6.3 Eva luat ion  of  Words  Conta ined inMore  thun Two ClustersThe number of words belonging to more thantwo clusters amounts to 57.
They are classifiedas follows (numbers in parenthesis are clusternumber in Appendix):i.
A word with differentwords).men-lugs (10l cell (1, 15)I ice (3,8) Ipanel (1, 7) treat (1, 3)2.
A word with the same me~nlng but usedin different contexts (32 words).star (9,12,14) brand (3,22)3.
A word with the same meaning in the samecontext (7 words).4.
Others (One of the words is uncertain, orits cluster's context is not estimated.
6words)Words of class 1 is the ambiguous words.
Cellin Cluster I means cells of tissue, whereas thatin Cluster 15 means battery.
Ice in Cluster 3means ice for cooling beverage, whereas that inCluster 8 means ice to skate on.According to our objectives to obtain sub-graphs of topics, words in class 2 is quite im-portant to be duplicated.
For instance, star inCluster 9 is a sport player star, that in Clus-ter 12 is a singer star and that in Cluster 14is a movie star.
If star were not duplicated,the three different topics would be merged intoa single subgraph.
The same situation is ob-served for children: it would merge topics ofchildbirth and education into a graph if it wasnot duplicated.
We are apt to pay attentiononly to the words of class 1, but that of class 2plays an important role in clustering.Words in class 3 is not ambiguous: theyshould connect two subgraphs into one (seeSection7).6.4 C luster  H ierarchyAn output subgraph of higher threshold is in-cluded as that of lower threshold.
With thisinclusion relation, the clusters form a hierar-chy(Figure 9).
A part of the hierarchy is shownbelow:Thresho ld  3.75A admission college scholarship\]3 admission applicant collegeC campus children classroom college edu-cation enroll faculty grade math parentscholarship school student sugar teachteacher tuition tutor university voucherD birth child children couple marriagemarry parent wedlock womanE birth infant weightF birth boy marryThresho ld  3.5 Cluster 6,24 in Appendix.Thresho ld  3.25G admission applicant baby birth boycampus century child children class-room college couple daughter educa-tion endowment enroll enrollment es-tablishment faculty father gift girl godgrade homework husband infant ivy kidlive love man marriage marry mathmother oxford parent professor psychol-ogist scholar scholarship school son stu-dent study sugar taught teach teacherteaching toy tuition tutor universityvoucher wed wedlock womanAt threshold 3.75, the origins of educa-t ion (Cluster 6) and ch i ldb i r th(C luster  24)clusters are already formed.
Among educa-tion, there are subtopics on scholarship schooland school entrance.
They are merged intoCluster 6 when the threshold is lowered to 3.5.Cluster 24 is also formed from Clusters D,E,Fof threshold 3.75.
Then Cluster 6 and 24 aremerged into Cluster G when the threshold is97Threshold 3.253.53.75cAA B CFigure 9: Cluster Hierarchylowered again to 3.25.
The clusters' hierarchi-cal relationships are shown in Figure 9.We may see that the topic is more specializedwhen the threshold is high.
Clusters which aremerged between threshold 3.75 and 3.5 werethose within a topic (A,B,C or D,E,F), buttopics of different clusters are merged at 3.25(Clusters 6and 24).
Thus, the lower the thresh-old is, the more the cluster contains ambiguity.The reason is that the two words in differenttopics do not co-occur.7 D iscuss ionThe best threshold iffers in topics.
Some ex-amples are:Economic topic: Although Wall Street Jour-nal articles have economic tendency, clus-ters of economic topic cannot be found inthe dusters with more than ?
words ofthreshold 3.5.
They appear in clusters atthreshold 3.0 as follows:?
accountant audit bracket deductionfiler income offset tax taxpayer?
convert conversion debenture debtholder out.stand prefer redeem redemp-tion repay tidewaterThe threshold should be lower for thistopic.Medica l  topic:  Cluster 1 have too manywords.
Despite of a medical cluster, potatoappears in the cluster.
At the thresh-old 3.0, Cluster 3 is completely mergedwith Cluster 1.
The appearance of potatoshows the sign that the merge of two clif-ferent opic has already begun.
Therefore,a higher threshold is preferred.98Trial topic: Several clusters exist on trial inAppendix.
They should form a clusterwith relatively lower threshold.Consequently, one of the most important futurework is to integrate two stages, the first stage ofmalc;ng input graph with the static threshold,and the second stage of clustering, into a singlestage with dynamic threshold.8 Conc lus ionWe have discussed a method to duster a co-occurrence graph obtained from a corpus, froma graph-theoretical viewpoint.
A graph has noambiguity if its branches, co-occurrence r la-tions are transitive.
This graph theoretical p-proach using graph is characteristic and it dif-fers from the conventional clustering method.We proposed an algorithm to extract subgraphswhose branches are transitive co-occurrence re-lations and discussed its features.
The effec-tiveness of our method was examined using the30M corpus.References\[Li and Abe, 1996\] H. Li and N. Abe.
Clus-tering Words with the MDL Principle.In Proceedings of the 15th InternationalConfervnce on Computational Linguistics,roll.
pp.4-10, 1996.\[Sugihara, 1995\] K. Sugihara.
A Graph-The-oretical Method For Monitoring ConceptFormation.
Pattern Recognition, Vol.28,No.ll, pp.
1635--1643, 1995.\[Tokunaga etal., 1995\]T. Tokunaga, M. Iwayama nd H. Tanaka.Automatic Thesaurus Construction basedon Grammatical Relations.
In Proceedingsof the International Joint Conference onArtificial Intelligence '95, 1995.\[Fukmnoto and Tsujii, 1994\] F. Fu.kumoto andJ.
Tsujii.
Automatic Recognition of VerbalPolysemy.
In Proceedings of the igth In-ternational Conference on ComputationalLinguistics , vol.2, pp.762-768, 1994.\[Pereira et al, 1993\] F. Pereira, N. Tishby andL.
Lee.
Distributional Clustering of En-glish Words.
In Proceedings of t~e MstACt, pp.
183-190, 1993.\[Brown et aL, 1992\] P. Brown, V. Pietra, etal.
Class-based n-gram Models of NaturalLanguage.
Computational Lingui.~ics, 18(4), pp.
283-298, 1992.\[TokLmaga and Tanaka~ 1990\] Tokunaga, T.and Tauaka, H. The Automatic Extrac-tion of Conceptual Items from BilingualDictionaries.
PRICAI, 1990.~Rindle, 1990\] D. Hindle.
Noun Classificationfrom Predicate - -  Argument Structures.In Procceed~gs of fAe ~8~h ACL, pp.168-175, 1990.\[Church and Hanks, 1990\] K. W. Church andP.
Hanks.
Word Association Norms, Mu-tual Information, and Lexicography.
Com-putatior~al L~guistics, vol.
16 (1), pp.
22-29, 1990.AppendixThe triples are (CS, Ei~ UR) (See Section 6.2).Cluster 1: medicine (105, 4.8%, 5.7~)afflict cancer disease gene researcher cell hepatitis lungpatient bacterium protein repair scientist herapy micetissue virus fibrositis implant mouse vaccine antigennicotine treat infect receptor switch~ blood enzyme in-ject insulin molecule pill aid heart infection transplantprostate stroke symptom transmit reatment cure de-pression diagnose kidney trial t bone chemotherapydose marrow ulcer cause stomach doctor breast disor-der prescribe blockers clot eradicate medication sam-ple brain nerve potato~ donate plasma patch arthri-tis drug lithium placebo version t test laboratory pro-tease prevention syndrome panel schizophrenia sub-stance tumor psychotheraw sclerosis suffer die painphysician lab urine rat radiation animal cholesterolpharmacist pharmacy collagen internist hospital pre-scription medicine care referral~nonprescriptionCluster 2: transportation (101, 12.6%,14.6%)air surveillance mlssilet plane fighte~ radar aircraftpatriot~ransport airline jet tank boat pilot serb~trainer airliner crash engine flight fly aviation passen-ger bus carrier bump airport hub saudi airplane de-livery surfaces machinist attendant wing airway shut-tle nonstop route delta fare hanoi~ walkout mechanictransportation haul denver railroad courier rail freightmile truck shipment diesel pickup minivan gasolinefuel sportt buiitt midsize~ assembly heat inventorydetroit vehicle model oil petroleum car auto styling'~emission:\[: brake cylinder sedan equip antilock omegaairbag luxury bag driver jaguar dodger accident pumpmotor rover wheel neont ford cherokee~ rear frontdealershiptexplorer plant build shreveportCluster 3: meal (60, 8.3%, iL7% )alcohol beer brew label liquor miller beverage tastebottle brewer brewery ale ice brand vodka coca colawine can drink milk coke diet fruit juice dinner draftdairy tea nestle supermarket grocer toronto'~ bottlercrownf flavor \[ithuania~ atlantat pepper cranberryinfection~, soup cheese cow hormone meat refrigeratecalorie category spray t eat ocean:\[: meal chicken varietytreat food sauce beef restaurantCluster 4: agriculture (32, 3.1%, 15.6%)acre flood corn farmer grain harvest agriculture hog,soybean wheat bushel crop depress'~ exporter feed live-stock cotton rain africa~ cattle farm meat brazil~ julyrice shipment forecast~, bale frost season flake~ sugarCluster 5: Near East Asia (32, 0.0%, 0.0%)arab qatar saud} egypt kuwait israel king peace arabiabahrain gaza emirate oil jew jordan lebanon occupyfighter jerusalem syria palestine barrel settler massacreterritory kill strip cabinet liberation jerichoCluster 6: education (31, 3.2%, 12.9%)admission scholarship school college applicant stu-dent university education professor taught teach tu-ition classroom faculty girl t teacher enroll enrollmentgrade homework math parent campus psychologist.~scholar ivy voucher children tutor sugar t endowmentC\]uster 7: politics (25, 24.0, 40.0%)bidden t senate senator democrat nomination biparti-san vermont.~ chafe t dole \]nman~ maine committeesubcommittee confirmation rhode~ bob t columnistsjudiciary.~ ranking chairmen panel chairman hearingoversight t oregon~Cluster 8: winter sports (24, 4.2%, 12.5%)alpine race ski cup norway winter medal skate skierboa~ cross kilometer sport silver snow tommy# men tathlete skater boot ice meter speed trackCluster 9: ball games (24, 16.7%, 16.7%o)baseball sports basketball fan football league liberty"l"team pro soccer televise coach stadium star foxycowboyj" bowl conferenceS" franchise game hockeyplayer tournament collegeCluster 10: computer network (18, 5.6%, 5.6%)bulletin user prodigy~ computer message internet mailpager laptop facsimile machine desk'top modem note-book dell subscription communicate subscriberCluster 11: Asia (17, 23.5%, 23.5%)asia singapore thailand indonesia malaysia philippinesvietnam taiwan korea burma china interbanktneighborvisito~ pakistan status t human~ bellingCluster 12: song (17, 0.0%, 0.0%)album pop sing song music radio singer jackson artistrecording disk band channel jazz star copy concert99Cluster 13: Soviet Union (17, 0.0%, 5.9%)arsenal russia soviet ukraine weapon dismantlemoscow baltic ruble bloc warhead kiev parliament re-former cabinet uranium quit:~Cluster ld:  movie (17, 5.9%, 5.9%)academy hollywood film picture studio movie poly-gram actor library video turner lansin~screen scriptactress star theaterC\]uster 15: satellite broadcast (17, 5.9%,zz.s%)affiliate station broadcast fox~ network radio televisionclutteQ antenna satellite channel beam signal trans-mit format dish cell'Cluster 16: software (16,0.0%, 0.0%)apple version window mac macintosh user applicationsoftware unix lotus spreadsheet developer copyrightexcel feature wordCluster 17: Eoxope (15, 0.0%, 0.0%)austria belgium france italy netherlands sweden fin-land spain britain denmark holland switzerland norwaykingdom membershipCluster 18: petro lum (14, 14.3%, 28.6%)alberta,: energy calgary~ gas oil pipeline feet barrelbasin~ refine chevron 1" exploration petroleum conden-sateCluster 19: art (13, 15.4%, 15.4%)art works exhibit exhibition paint collection gallery mu-seum photograph landscape artist avenue1" floodC\]oster 20: Korea (13, 15.4%, 23.1%)artillery tank weapon missile korea pyongyang regimeseoul iraqt patriot inspector scuds stance tCluster 21: Balks- Peninsula (12, 0.0%, 0.0%)artillery serb strike weapon muslim bomb troop peacepeacekeeper croat negotiator withdrawC.luster 22: ciga.tette (11, 0.0%, 0.0%)addict smoker cigarette nicotine smoke camel morristobacco ban antismoke brandCluster 23: multimedia.
(11, 0.0%, 0.0%)audio text video disk cassette library multimedia tapemusic videocassette entertainmentCluster 24: childbirth (11, 0.0%, 0.0%)birth wedlock child marriage marry mother parentwoman couple children infantCluster 25: calamity (11, 9.1%, 9.1%)crop relief disaster flood earthquake riot~ hurricanequake rebuilding victim franciscoCluster 26: goods (10, 10.0%, 10.0%)apparel retailer store furnishing mart merchandisewarehouse outlet mass'~ chainCluster 27: trial (10, 20.0%, 20.0%)appeal reinstate ruling court: upheld circuit~ arbitrationjudge overturn t arbitratorCluster 28: trash (10, 50.0%, 50.0%)cleanup sitet superfundt dump pollute insurert wastetrash ferrist gluQCluster 29: ?
(10, 100.0%, 100.0%)arkansasf rock~ thriftt guaranty'~ madison'~ supervise'~failure'~ limitationt surroundin~ investigation'~Cluster 30: earthquake (9, 22.2%, 22.2%)aftershock quake damage earthquake repair epicenterfreewayt homeownert inspectChzster 31: real estate (9, 0.0%, 11.1%)apartment bedroom rent avenue manhattan tower ten-ant subsidizer landlordCluster 32: trial (9, 0.0%, 0.0%)convict prison sentence jail parole conviction 'fine of-fender pleadCluster 33: trial (9, 0.0%, 0.0%)award damage jury plaintiff verdict defendant case ju-ror convictCluster 34: winter resort (9, 0.0%, 0.0%)crest resort ski skier mountain snow valley slope skyCluster 35: trial (8, 0.0%, 0.0%)bureau probe subpoena investigation prosecutor coun-sel inquiry suicideCluster 36: telephone network (8, 0.0%, 0.0%)cable wire fiber phone transmission voice fax mi-crowaveCluster 37: bond (8, 0.0%, 75.0%)bondholder reorganizations chapters creditorreorganizer pet'rl:ion~ proceedings 'filingtCluster 38: book (8, 12.5%, 12.5%)audio'~ publisher title book bestseller bookstore refer-ence copyCluster 39 guercilla (8, 12.5%, 12.5%) guer-rilla syria negotiate peace rebel uprising negotiatormexicot100
