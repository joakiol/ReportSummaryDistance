Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683?691,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPAutomatic sense prediction for implicit discourse relations in textEmily Pitler, Annie Louis, Ani NenkovaComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USAepitler,lannie,nenkova@seas.upenn.eduAbstractWe present a series of experiments on au-tomatically identifying the sense of im-plicit discourse relations, i.e.
relationsthat are not marked with a discourse con-nective such as ?but?
or ?because?.
Wework with a corpus of implicit relationspresent in newspaper text and report re-sults on a test set that is representativeof the naturally occurring distribution ofsenses.
We use several linguistically in-formed features, including polarity tags,Levin verb classes, length of verb phrases,modality, context, and lexical features.
Inaddition, we revisit past approaches usinglexical pairs from unannotated text as fea-tures, explain some of their shortcomingsand propose modifications.
Our best com-bination of features outperforms the base-line from data intensive approaches by 4%for comparison and 16% for contingency.1 IntroductionImplicit discourse relations abound in text andreaders easily recover the sense of such relationsduring semantic interpretation.
But automaticsense prediction for implicit relations is an out-standing challenge in discourse processing.Discourse relations, such as causal and contrastrelations, are often marked by explicit discourseconnectives (also called cue words) such as ?be-cause?
or ?but?.
It is not uncommon, though, for adiscourse relation to hold between two text spanswithout an explicit discourse connective, as the ex-ample below demonstrates:(1) The 101-year-old magazine has never had to woo ad-vertisers with quite so much fervor before.
[because] It largely rested on its hard-to-fault demo-graphics.In this paper we address the problem of au-tomatic sense prediction for discourse relationsin newspaper text.
For our experiments, we usethe Penn Discourse Treebank, the largest exist-ing corpus of discourse annotations for both im-plicit and explicit relations.
Our work is alsoinformed by the long tradition of data intensivemethods that rely on huge amounts of unanno-tated text rather than on manually tagged corpora(Marcu and Echihabi, 2001; Blair-Goldensohn etal., 2007).In our analysis, we focus only on implicit dis-course relations and clearly separate these fromexplicits.
Explicit relations are easy to iden-tify.
The most general senses (comparison, con-tingency, temporal and expansion) can be disam-biguated in explicit relations with 93% accuracybased solely on the discourse connective used tosignal the relation (Pitler et al, 2008).
So report-ing results on explicit and implicit relations sepa-rately will allow for clearer tracking of progress.In this paper we investigate the effectiveness ofvarious features designed to capture lexical andsemantic regularities for identifying the sense ofimplicit relations.
Given two text spans, previouswork has used the cross-product of the words inthe spans as features.
We examine the most infor-mative word pair features and find that they are notthe semantically-related pairs that researchers hadhoped.
We then introduce several other methodscapturing the semantics of the spans (polarity fea-tures, semantic classes, tense, etc.)
and evaluatetheir effectiveness.
This is the first study whichreports results on classifying naturally occurringimplicit relations in text and uses the natural dis-tribution of the various senses.2 Related WorkExperiments on implicit and explicit relationsPrevious work has dealt with the prediction of dis-course relation sense, but often for explicits and atthe sentence level.Soricut and Marcu (2003) address the task of683parsing discourse structures within the same sen-tence.
They use the RST corpus (Carlson et al,2001), which contains 385 Wall Street Journal ar-ticles annotated following the Rhetorical StructureTheory (Mann and Thompson, 1988).
Many ofthe useful features, syntax in particular, exploitthe fact that both arguments of the connective arefound in the same sentence.
Such features wouldnot be applicable to the analysis of implicit rela-tions that occur intersententially.Wellner et al (2006) used the GraphBank (Wolfand Gibson, 2005), which contains 105 AssociatedPress and 30 Wall Street Journal articles annotatedwith discourse relations.
They achieve 81% accu-racy in sense disambiguation on this corpus.
How-ever, GraphBank annotations do not differentiatebetween implicits and explicits, so it is difficult toverify success for implicit relations.Experiments on artificial implicits Marcu andEchihabi (2001) proposed a method for cheap ac-quisition of training data for discourse relationsense prediction.
Their idea is to use unambiguouspatterns such as [Arg1, but Arg2.]
to create syn-thetic examples of implicit relations.
They deletethe connective and use [Arg1, Arg2] as an exampleof an implicit relation.The approach is tested using binary classifica-tion between relations on balanced data, a settingvery different from that of any realistic applica-tion.
For example, a question-answering appli-cation that needs to identify causal relations (i.e.as in Girju (2003)), must not only differentiatecausal relations from comparison relations, butalso from expansions, temporal relations, and pos-sibly no relation at all.
In addition, using equalnumbers of examples of each type can be mislead-ing because the distribution of relations is knownto be skewed, with expansions occurring most fre-quently.
Causal and comparison relations, whichare most useful for applications, are less frequent.Because of this, the recall of the classificationshould be the primary metric of success, whilethe Marcu and Echihabi (2001) experiments reportonly accuracy.Later work (Blair-Goldensohn et al, 2007;Sporleder and Lascarides, 2008) has discoveredthat the models learned do not perform as well onimplicit relations as one might expect from the testaccuracies on synthetic data.3 Penn Discourse TreebankFor our experiments, we use the Penn DiscourseTreebank (PDTB; Prasad et al, 2008), the largestavailable annotated corpora of discourse relations.The PDTB contains discourse annotations over thesame 2,312 Wall Street Journal (WSJ) articles asthe Penn Treebank.For each explicit discourse connective (such as?but?
or ?so?
), annotators identified the two textspans between which the relation holds and thesense of the relation.The PDTB also provides information about lo-cal implicit relations.
For each pair of adjacentsentences within the same paragraph, annotatorsselected the explicit discourse connective whichbest expressed the relation between the sentencesand then assigned a sense to the relation.
In Exam-ple (1) above, the annotators identified ?because?as the most appropriate connective between thesentences, and then labeled the implicit discourserelation Contingency.In the PDTB, explicit and implicit relations areclearly distinguished, allowing us to concentratesolely on the implicit relations.As mentioned above, each implicit and explicitrelation is annotated with a sense.
The sensesare arranged in a hierarchy, allowing for annota-tions as specific as Contingency.Cause.reason.
Inour experiments, we use only the top level of thesense annotations: Comparison, Contingency, Ex-pansion, and Temporal.
Using just these four rela-tions allows us to be theory-neutral; while differ-ent frameworks (Hobbs, 1979; McKeown, 1985;Mann and Thompson, 1988; Knott and Sanders,1998; Asher and Lascarides, 2003) include differ-ent relations of varying specificities, all of theminclude these four core relations, sometimes underdifferent names.Each relation in the PDTB takes two arguments.Example (1) can be seen as the predicate Con-tingency which takes the two sentences as argu-ments.
For implicits, the span in the first sentenceis called Arg1 and the span in the following sen-tence is called Arg2.4 Word pair features in prior workCross product of words Discourse connectivesare the most reliable predictors of the semanticsense of the relation (Marcu, 2000; Pitler et al,2008).
However, in the absence of explicit mark-ers, the most easily accessible features are the684words in the two text spans of the relation.
In-tuitively, one would expect that there is some rela-tionship that holds between the words in the twoarguments.
Consider for example the followingsentences:The recent explosion of country funds mirrors the ?closed-end fund mania?
of the 1920s, Mr.
Foot says, when narrowlyfocused funds grew wildly popular.
They fell into oblivionafter the 1929 crash.The words ?popular?
and ?oblivion?
are almostantonyms, and one might hypothesize that theiroccurrence in the two text spans is what triggersthe contrast relation between the sentences.
Sim-ilarly, a pair of words such as (rain, rot) might beindicative of a causal relation.
If this hypothesis iscorrect, pairs of words (w1, w2) such that w1 ap-pears in the first sentence and w2 appears in thesecond sentence would be good features for iden-tifying contrast relations.Indeed, word pairs form the basic featureof most previous work on classifying implicitrelations (Marcu and Echihabi, 2001; Blair-Goldensohn et al, 2007; Sporleder and Las-carides, 2008) or the simpler task of predictingwhich connective should be used to express a rela-tion (Lapata and Lascarides, 2004).Semantic relations vs. function word pairs Ifthe hypothesis for word pair triggers of discourserelations were true, the analysis of unambiguousrelations can be used to discover pairs of wordswith causal or contrastive relations holding be-tween them.
Yet, feature analysis has not been per-formed in prior studies to establish or refute thispossibility.At the same time, feature selection is alwaysnecessary for word pairs, which are numerous andlead to data sparsity problems.
Here, we present ameta analysis of the feature selection work in threeprior studies.One approach for reducing the number of fea-tures follows the hypothesis of semantic rela-tions between words.
Marcu and Echihabi (2001)considered only nouns, verbs and and other cuephrases in word pairs.
They found that evenwith millions of training examples, prediction re-sults using all words were superior to those basedon only pairs of non-function words.
However,since the learning curve is steeper when functionwords were removed, they hypothesize that usingonly non-function words will outperform using allwords once enough training data is available.In a similar vein, Lapata and Lascarides (2004)used pairings of only verbs, nouns and adjectivesfor predicting which temporal connective is mostsuitable to express the relation between two giventext spans.
Verb pairs turned out to be one of thebest features, but no useful information was ob-tained using nouns and adjectives.Blair-Goldensohn et al (2007) proposed sev-eral refinements of the word pair model.
Theyshow that (i) stemming, (ii) using a small fixedvocabulary size consisting of only the most fre-quent stems (which would tend to be dominatedby function words) and (iii) a cutoff on the mini-mum frequency of a feature, all result in improvedperformance.
They also report that filtering stop-words has a negative impact on the results.Given these findings, we expect that pairs offunction words are informative features helpful inpredicting discourse relation sense.
In our workthat we describe next, we use feature selection toinvestigate the word pairs in detail.5 Analysis of word pair featuresFor the analysis of word pair features, we usea large collection of automatically extracted ex-plicit examples from the experiments in Blair-Goldensohn et al (2007).
The data, from now onreferred to as TextRels, has explicit contrast andcausal relations which were extracted from the En-glish Gigaword Corpus (Graff, 2003) which con-tains over four million newswire articles.The explicit cue phrase is removed from eachexample and the spans are treated as belonging toan implicit relation.
Besides cause and contrast,the TextRels data include a no-relation categorywhich consists of sentences from the same text thatare separated by at least three other sentences.To identify features useful for classifying com-parison vs other relations, we chose a random sam-ple of 5000 examples for Contrast and 5000 Otherrelations (2500 each of Cause and No-relation).For the complete set of 10,000 examples, wordpair features were computed.
After removingword pairs that appear less than 5 times, the re-maining features were ranked by information gainusing the MALLET toolkit1.Table 1 lists the word pairs with highest infor-mation gain for the Contrast vs. Other and Causevs.
Other classification tasks.
All contain very fre-quent stop words, and interestingly for the Con-1mallet.cs.umass.edu685trast vs. Other task, most of the word pairs containdiscourse connectives.This is certainly unexpected, given that wordpairs were formed by deleting the discourse con-nectives from the sentences expressing Contrast.Word pairs containing ?but?
as one of their ele-ments in fact signal the presence of a relation thatis not Contrast.Consider the example shown below:The government says it has reached most isolated townshipsby now, but because roads are blocked, getting anything butbasic food supplies to people remains difficult.Following Marcu and Echihabi (2001), the pair[The government says it has reached most isolatedtownships by now, but] and [roads are blocked,getting anything but basic food supplies to peo-ple remains difficult.]
is created as an example ofthe Cause relation.
Because of examples like this,?but-but?
is a very useful word pair feature indi-cating Cause, as the but would have been removedfor the artifical Contrast examples.
In fact, the top17 features for classifying Contrast versus Otherall contain the word ?but?, and are indications thatthe relation is Other.These findings indicate an unexpected anoma-lous effect in the use of synthetic data.
Since re-lations are created by removing connectives, if anunambiguous connective remains, its presence is areliable indicator that the example should be clas-sified as Other.
Such features might work well andlead to high accuracy results in identifying syn-thetic implicit relations, but are unlikely to be use-ful in a realistic setting of actual implicits.Comparison vs. Other Contingency vs. Otherthe-but s-but the-in the-and in-the the-ofof-but for-but but-but said-said to-of the-ain-but was-but it-but a-and a-the of-theto-but that-but the-it* to-and to-to the-inand-but but-the to-it* and-and the-the in-ina-but he-but said-in to-the of-and a-ofsaid-but they-but of-in in-and in-of s-andTable 1: Word pairs with highest information gain.Also note that the only two features predic-tive of the comparison class (indicated by * inTable 1): the-it and to-it, contain only func-tion words rather than semantically related non-function words.
This ranking explains the obser-vations reported in Blair-Goldensohn et al (2007)where removing stopwords degraded classifierperformance and why using only nouns, verbs oradjectives (Marcu and Echihabi, 2001; Lapata andLascarides, 2004) is not the best option2.6 Features for sense prediction ofimplicit discourse relationsThe contrast between the ?popular?/?oblivion?
ex-ample we started with above can be analyzed interms of lexical relations (near antonyms), but alsocould be explained by different polarities of thetwo words: ?popular?
is generally a positive word,while ?oblivion?
has negative connotations.While we agree that the actual words in the ar-guments are quite useful, we also define severalhigher-level features corresponding to various se-mantic properties of the words.
The words in thetwo text spans of a relation are taken from thegold-standard annotations in the PDTB.Polarity Tags: We define features that representthe sentiment of the words in the two spans.
Eachword?s polarity was assigned according to its en-try in the Multi-perspective Question AnsweringOpinion Corpus (Wilson et al, 2005).
In this re-source, each sentiment word is annotated as posi-tive, negative, both, or neutral.
We use the numberof negated and non-negated positive, negative, andneutral sentiment words in the two text spans asfeatures.
If a writer refers to something as ?nice?in Arg1, that counts towards the positive sentimentcount (Arg1Positive); ?not nice?
would count to-wards Arg1NegatePositive.
A sentiment word isnegated if a word with a General Inquirer (Stoneet al, 1966) Negate tag precedes it.
We also havefeatures for the cross products of these polaritiesbetween Arg1 and Arg2.We expected that these features could helpComparison examples especially.
Consider thefollowing example:Executives at Time Inc. Magazine Co., a subsidiary ofTime Warner, have said the joint venture with Mr. Langwasn?t a good one.
The venture, formed in 1986, was sup-posed to be Time?s low-cost, safe entry into women?s maga-zines.The word good is annotated with positive po-larity, however it is negated.
Safe is tagged ashaving positive polarity, so this opposition couldindicate the Comparison relation between the twosentences.Inquirer Tags: To get at the meanings of thespans, we look up what semantic categories each2In addition, an informal inspection of 100 word pairswith high information gain for Contrast vs. Other (the longestword pairs were chosen, as those are more likely to be contentwords) found only six semantically opposed pairs.686word falls into according to the General Inquirerlexicon (Stone et al, 1966).
The General In-quirer has classes for positive and negative polar-ity, as well as more fine-grained categories such aswords related to virtue or vice.
The Inquirer evencontains a category called ?Comp?
that includeswords that tend to indicate Comparison, such as?optimal?, ?other?, ?supreme?, or ?ultimate?.Several of the categories are complementary:Understatement versus Overstatement, Rise ver-sus Fall, or Pleasure versus Pain.
Pairs where oneargument contains words that indicate Rise and theother argument indicates Fall might be good evi-dence for a Comparison relation.The benefit of using these tags instead of justthe word pairs is that we see more observations foreach semantic class than for any particular word,reducing the data sparsity problem.
For example,the pair rose:fell often indicates a Comparison re-lation when speaking about stocks.
However, oc-casionally authors refer to stock prices as ?jump-ing?
rather than ?rising?.
Since both jump and riseare members of the Rise class, new jump examplescan be classified using past rise examples.Development testing showed that including fea-tures for all words?
tags was not useful, so we in-clude the Inquirer tags of only the verbs in the twoarguments and their cross-product.
Just as for thepolarity features, we include features for both eachtag and its negation.Money/Percent/Num: If two adjacent sen-tences both contain numbers, dollar amounts, orpercentages, it is likely that a comparison rela-tion might hold between the sentences.
We in-cluded a feature for the count of numbers, percent-ages, and dollar amounts in Arg1 and Arg2.
Wealso included the number of times each combina-tion of number/percent/dollar occurs in Arg1 andArg2.
For example, if Arg1 mentions a percent-age and Arg2 has two dollar amounts, the featureArg1Percent-Arg2Money would have a count of 2.This feature is probably genre-dependent.
Num-bers and percentages often appear in financial textsbut would be less frequent in other genres.WSJ-LM: This feature represents the extent towhich the words in the text spans are typical ofeach relation.
For each sense, we created uni-gram and bigram language models over the im-plicit examples in the training set.
We computeeach example?s probability according to each ofthese language models.
The features are the ranksof the spans?
likelihoods according to the vari-ous language models.
For example, if of the un-igram models, the most likely relation to generatethis example was Contingency, then the examplewould include the feature ContingencyUnigram1.If the third most likely relation according to thebigram models was Expansion, then it would in-clude the feature ExpansionBigram3.Expl-LM: This feature ranks the text spans ac-cording to language models derived from the ex-plicit examples in the TextRels corpus.
However,the corpus contains only Cause, Contrast and No-relation, hence we expect the WSJ language mod-els to be more helpful.Verbs: These features include the number ofpairs of verbs in Arg1 and Arg2 from the sameverb class.
Two verbs are from the same verb classif each of their highest Levin verb class (Levin,1993) levels (in the LCS Database (Dorr, 2001))are the same.
The intuition behind this feature isthat the more related the verbs, the more likely therelation is an Expansion.The verb features also include the averagelength of verb phrases in each argument, as wellas the cross product of this feature for the two ar-guments.
We hypothesized that verb chunks thatcontain more words, such as ?They [are allowed toproceed]?
often contain rationales afterwards (sig-nifying Contingency relations), while short verbphrases like ?They proceed?
might occur more of-ten in Expansion or Temporal relations.Our final verb features were the part of speechtags (gold-standard from the Penn Treebank) ofthe main verb.
One would expect that Expansionwould link sentences with the same tense, whereasContingency and Temporal relations would con-tain verbs with different tenses.First-Last, First3: The first and last words ofa relation?s arguments have been found to be par-ticularly useful for predicting its sense (Wellner etal., 2006).
Wellner et al (2006) suggest that thesewords are such predictive features because theyare often explicit discourse connectives.
In ourexperiments on implicits, the first and last wordsare not connectives.
However, some implicits havebeen found to be related by connective-like ex-pressions which often appear in the beginning ofthe second argument.
In the PDTB, these are an-notated as alternatively lexicalized relations (Al-tLexes).
To capture such effects, we included thefirst and last words of Arg1 as features, the first687and last words of Arg2, the pair of the first wordsof Arg1 and Arg2, and the pair of the last words.We also add two additional features which indicatethe first three words of each argument.Modality: Modal words, such as ?can?,?should?, and ?may?, are often used to expressconditional statements (i.e.
?If I were a wealthyman, I wouldn?t have to work hard.?)
thus signal-ing a Contingency relation.
We include a featurefor the presence or absence of modals in Arg1 andArg2, features for specific modal words, and theircross-products.Context: Some implicit relations appear imme-diately before or immediately after certain explicitrelations far more often than one would expect dueto chance (Pitler et al, 2008).
We define a featureindicating if the immediately preceding (or follow-ing) relation was an explicit.
If it was, we includethe connective trigger of the relation and its senseas features.
We use oracle annotations of the con-nective sense, however, most of the connectivesare unambiguous.One might expect a different distribution of re-lation types in the beginning versus further in themiddle of a paragraph.
We capture paragraph-position information using a feature which indi-cates if Arg1 begins a paragraph.Word pairs Four variants of word pair mod-els were used in our experiments.
All the modelswere eventually tested on implicit examples fromthe PDTB, but the training set-up was varied.Wordpairs-TextRels In this setting, we traineda model on word pairs derived from unannotatedtext (TextRels corpus).Wordpairs-PDTBImpl Word pairs for trainingwere formed from the cross product of words inthe textual spans (Arg1 x Arg2) of the PDTB im-plicit relations.Wordpairs-selected Here, only word pairs fromWordpairs-PDTBImpl with non-zero informationgain on the TextRels corpus were retained.Wordpairs-PDTBExpl In this case, the modelwas formed by using the word pairs from the ex-plicit relations in the sections of the PDTB usedfor training.7 Classification ResultsFor all experiments, we used sections 2-20 of thePDTB for training and sections 21-22 for testing.Sections 0-1 were used as a development set forfeature design.We ran four binary classification tasks to iden-tify each of the main relations from the rest.
Aseach of the relations besides Expansion are infre-quent, we train using equal numbers of positiveand negative examples of the target relation.
Thenegative examples were chosen at random.
Weused all of sections 21 and 22 for testing, so thetest set is representative of the natural distribution.The training sets contained: Comparison (1927positive, 1927 negative), Contingency (3500each), Expansion3 (6356 each), and Temporal(730 each).The test set contained: 151 examples of Com-parison, 291 examples of Contingency, 986 exam-ples of Expansion, 82 examples of Temporal, and13 examples of No-relation.We used Naive Bayes, Maximum Entropy(MaxEnt), and AdaBoost (Freund and Schapire,1996) classifiers implemented in MALLET.7.1 Non-Wordpair FeaturesThe performance using only our semantically in-formed features is shown in Table 7.
Only theNaive Bayes classification results are given, asspace is limited and MaxEnt and AdaBoost gaveslightly lower accuracies overall.The table lists the f-score for each of the targetrelations, with overall accuracy shown in brack-ets.
Given that the experiments are run on naturaldistribution of the data, which are skewed towardsExpansion relations, the f-score is the more impor-tant measure to track.Our random baseline is the f-score one wouldachieve by randomly assigning classes in propor-tion to its true distribution in the test set.
The bestresults for all four tasks are considerably higherthan random prediction, but still low overall.
Ourfeatures provide 6% to 18% absolute improve-ments in f-score over the baseline for each of thefour tasks.
The largest gain was in the Contin-gency versus Other prediction task.
The least im-provement was for distinguishing Expansion ver-sus Other.
However, since Expansion forms thelargest class of relations, its f-score is still thehighest overall.
We discuss the results per relationclass next.Comparison We expected that polarity featureswould be especially helpful for identifying Com-3The PDTB also contains annotations of entity relations,which most frameworks consider a subset of Expansion.Thus, we include relations annotated as EntRel as positiveexamples of Expansion.688Features Comp.
vs. Not Cont.
vs. Other Exp.
vs. Other Temp.
vs. Other Four-wayMoney/Percent/Num 19.04 (43.60) 18.78 (56.27) 22.01 (41.37) 10.40 (23.05) (63.38)Polarity Tags 16.63 (55.22) 19.82 (76.63) 71.29 (59.23) 11.12 (18.12) (65.19)WSJ-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)Expl-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)Verbs 18.55 (26.19) 36.59 (62.44) 59.36 (52.53) 12.61 (41.63) (65.33)First-Last, First3 21.01 (52.59) 36.75 (59.09) 63.22 (56.99) 15.93 (61.20) (65.40)Inquirer tags 17.37 (43.8) 15.76 (77.54) 70.21 (58.04) 11.56 (37.69) (62.21)Modality 17.70 (17.6) 21.83 (76.95) 15.38 (37.89) 11.17 (27.91) (65.33)Context 19.32 (56.66) 29.55 (67.42) 67.77 (57.85) 12.34 (55.22) (64.01)Random 9.91 19.11 64.74 5.38Table 2: f-score (accuracy) using different features; Naive Bayes.parison relations.
Surprisingly, polarity was actu-ally one of the worst classes of features for Com-parison, achieving an f-score of 16.33 (in contrastto using the first, last and first three words of thesentences as features, which leads to an f-score of21.01).
We examined the prevalence of positive-negative or negative-positive polarity pairs in ourtraining set.
30% of the Comparison examplescontain one of these opposite polarity pairs, while31% of the Other examples contain an oppositepolarity pair.
To our knowledge, this is the firststudy to examine the prevalence of polarity wordsin the arguments of discourse relations in theirnatural distributions.
Contrary to popular belief,Comparisons do not tend to have more oppositepolarity pairs.The two most useful classes of features for rec-ognizing Comparison relations were the first, lastand first three words in the sentence and the con-text features that indicate the presence of a para-graph boundary or of an explicit relation just be-fore or just after the location of the hypothesizedimplicit relation (19.32 f-score).Contingency The two best features for the Con-tingency vs. Other distinction were verb informa-tion (36.59 f-score) and first, last and first threewords in the sentence (36.75 f-score).
Contextagain was one of the features that led to improve-ment.
This makes sense, as Pitler et al (2008)found that implicit contingencies are often foundimmediately following explicit comparisons.We were surprised that the polarity featureswere helpful for Contingency but not Comparison.Again we looked at the prevalence of opposite po-larity pairs.
While for Comparison versus Otherthere was not a significant difference, for Contin-gency there are quite a few more opposite polaritypairs (52%) than for not Contingency (41%).The language model features were completelyuseless for distinguishing contingencies fromother relations.Expansion As Expansion is the majority classin the natural distribution, recall is less of a prob-lem than precision.
The features that help achievethe best f-score are all features that were found tobe useful in identifying other relations.Polarity tags, Inquirer tags and context werethe best features for identifying expansions withf-scores around 70%.Temporal Implicit temporal relations are rela-tively rare, making up only about 5% of our testset.
Most temporal relations are explicitly markedwith a connective like ?when?
or ?after?.Yet again, the first and last words of the sen-tence turned out to be useful indicators for tem-poral relations (15.93 f-score).
The importance ofthe first and last words for this distinction is clear.It derives from the fact that temporal implicits of-ten contain words like ?yesterday?
or ?Monday?
atthe end of the sentence.
Context is the next mosthelpful feature for temporal relations.7.2 Which word pairs help?For Comparison and Contingency, we analyze thebehavior of word pair features under several differ-ent settings.
Specifically we want to address twoimportant related questions raised in recent workby others: (i) is unannotated data from explicitsuseful for training models that disambiguate im-plicit discourse relations and (ii) are explicit andimplicit relations intrinsically different from eachother.Wordpairs-TextRels is the worst approach.
Thebest use of word pair features is Wordpairs-selected.
This model gives 4% better absolute f-score for Comparison and 14% for Contingencyover Wordpairs-TextRels.
In this setting the Tex-tRels data was used to choose the word pair fea-tures, but the probabilities for each feature wereestimated using the training portion of the PDTB689Comp.
vs. OtherWordpairs-TextRels 17.13 (46.62)Wordpairs-PDTBExpl 19.39 (51.41)Wordpairs-PDTBImpl 20.96 (42.55)First-last, first3 (best-non-wp) 21.01 (52.59)Best-non-wp + Wordpairs-selected 21.88 (56.40)Wordpairs-selected 21.96 (56.59)Cont.
vs. OtherWordpairs-TextRels 31.10 (41.83)Wordpairs-PDTBExpl 37.77 (56.73)Wordpairs-PDTBImpl 43.79 (61.92)Polarity, verbs, first-last, first3,modality, context (best-non-wp)42.14 (66.64)Wordpairs-selected 45.60 (67.10)Best-non-wp + Wordpairs-selected 47.13 (67.30)Expn.
vs. OtherBest-non-wp + wordpairs 62.39 (59.55)Wordpairs-PDTBImpl 63.84 (60.28)Polarity, inquirer tags, context (best-non-wp)76.42 (63.62)Temp.
vs. OtherFirst-last, first3 (best-non-wp) 15.93 (61.20)Wordpairs-PDTBImpl 16.21 (61.98)Best-non-wp + Wordpairs-PDTBImpl 16.76 (63.49)Table 3: f-score (accuracy) of various feature sets;Naive Bayes.implicit examples.We also confirm that even within the PDTB,information from annotated explicit relations(Wordpairs-PDTBExpl) is not as helpful asinformation from annotated implicit relations(Wordpairs-PDTBImpl).
The absolute differencein f-score between the two models is close to 2%for Comparison, and 6% for Contingency.7.3 Best resultsAdding other features to word pairs leads to im-proved performance for Contingency, Expansionand Temporal relations, but not for Comparison.For contingency detection, the best combina-tion of our features included polarity, verb in-formation, first and last words, modality, contextwith Wordpairs-selected.
This combination ledto a definite improvement, reaching an f-score of47.13 (16% absolute improvement in f-score overWordpairs-TextRels).For detecting expansions, the best combinationof our features (polarity+Inquirer tags+context)outperformed Wordpairs-PDTBImpl by a widemargin, close to 13% absolute improvement (f-scores of 76.42 and 63.84 respectively).7.4 Sequence Model of Discourse RelationsOur results from the previous section show thatclassification of implicits benefits from informa-tion about nearby relations, and so we expectedimprovements using a sequence model, rather thanclassifying each relation independently.We trained a CRF classifier (Lafferty et al,2001) over the sequence of implicit examples fromall documents in sections 02 to 20.
The test setis the same as used for the 2-way classifiers.
Wecompare against a 6-way4 Naive Bayes classifier.Only word pairs were used as features for both.Overall 6-way prediction accuracy is 43.27% forthe Naive Bayes model and 44.58% for the CRFmodel.8 ConclusionWe have presented the first study that predicts im-plicit discourse relations in a realistic setting (dis-tinguishing a relation of interest from all others,where the relations occur in their natural distri-butions).
Also unlike prior work, we separate thetask from the easier task of explicit discourse pre-diction.
Our experiments demonstrate that fea-tures developed to capture word polarity, verbclasses and orientation, as well as some lexicalfeatures are strong indicators of the type of dis-course relation.We analyze word pair features used in priorwork that were intended to capture such semanticoppositions.
We show that the features in fact donot capture semantic relation but rather give infor-mation about function word co-occurrences.
How-ever, they are still a useful source of informationfor discourse relation prediction.
The most bene-ficial application of such features is when they areselected from a large unannotated corpus of ex-plicit relations, but then trained on manually an-notated implicit relations.Context, in terms of paragraph boundaries andnearby explicit relations, also proves to be usefulfor the prediction of implicit discourse relations.It is helpful when added as a feature in a standard,instance-by-instance learning model.
A sequencemodel also leads to over 1% absolute improvementfor the task.9 AcknowledgmentsThis work was partially supported by NSF grantsIIS-0803159, IIS-0705671 and IGERT 0504487.We would like to thank Sasha Blair-Goldensohnfor providing us with the TextRels data and forthe insightful discussion in the early stages of ourwork.4the four main relations, EntRel, NoRel690ReferencesN.
Asher and A. Lascarides.
2003.
Logics of conver-sation.
Cambridge University Press.S.
Blair-Goldensohn, K.R.
McKeown, and O.C.
Ram-bow.
2007.
Building and Refining Rhetorical-Semantic Relation Models.
In Proceedings ofNAACL HLT, pages 428?435.L.
Carlson, D. Marcu, and M.E.
Okurowski.
2001.Building a discourse-tagged corpus in the frame-work of rhetorical structure theory.
In Proceedingsof the Second SIGdial Workshop on Discourse andDialogue, pages 1?10.B.J.
Dorr.
2001.
LCS Verb Database.
Technical Re-port Online Software Database, University of Mary-land, College Park, MD.Y.
Freund and R.E.
Schapire.
1996.
Experiments witha New Boosting Algorithm.
In Machine Learning:Proceedings of the Thirteenth International Confer-ence, pages 148?156.R.
Girju.
2003.
Automatic detection of causal relationsfor Question Answering.
In Proceedings of the ACL2003 workshop on Multilingual summarization andquestion answering-Volume 12, pages 76?83.D.
Graff.
2003.
English gigaword corpus.
Corpusnumber LDC2003T05, Linguistic Data Consortium,Philadelphia.J.
Hobbs.
1979.
Coherence and coreference.
Cogni-tive Science, 3:67?90.A.
Knott and T. Sanders.
1998.
The classification ofcoherence relations and their linguistic markers: Anexploration of two languages.
Journal of Pragmat-ics, 30(2):135?175.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Interna-tional Conference on Machine Learning 2001, pages282?289.M.
Lapata and A. Lascarides.
2004.
Inferringsentence-internal temporal relations.
In HLT-NAACL 2004: Main Proceedings.B.
Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
Chicago, IL.W.C.
Mann and S.A. Thompson.
1988.
Rhetoricalstructure theory: Towards a functional theory of textorganization.
Text, 8.D.
Marcu and A. Echihabi.
2001.
An unsupervisedapproach to recognizing discourse relations.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 368?375.D.
Marcu.
2000.
The Theory and Practice of Dis-course and Summarization.
The MIT Press.K.
McKeown.
1985.
Text Generation: Using Dis-course strategies and Focus Constraints to Gener-ate Natural Language Text.
Cambridge UniversityPress, Cambridge, England.E.
Pitler, M. Raghupathy, H. Mehta, A. Nenkova,A.
Lee, and A. Joshi.
2008.
Easily identifiable dis-course relations.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING08), short paper.R.
Soricut and D. Marcu.
2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
In HLT-NAACL.C.
Sporleder and A. Lascarides.
2008.
Using automat-ically labelled examples to classify rhetorical rela-tions: An assessment.
Natural Language Engineer-ing, 14:369?416.P.J.
Stone, J. Kirsh, and Cambridge Computer Asso-ciates.
1966.
The General Inquirer: A ComputerApproach to Content Analysis.
MIT Press.B.
Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,and R. Sauri.
2006.
Classification of discourse co-herence relations: An exploratory study using mul-tiple knowledge sources.
In Proceedings of the 7thSIGdial Workshop on Discourse and Dialogue.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing contextual polarity in phrase-level sentimentanalysis.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 347?354.F.
Wolf and E. Gibson.
2005.
Representing discoursecoherence: A corpus-based study.
ComputationalLinguistics, 31(2):249?288.691
