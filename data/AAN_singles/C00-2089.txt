Tagging and Chunking with BigramsFerran P la ,  Anton io  Mo l ina  and Nat iv idad  Pr ie toUnivers i ta t  Po l i tbcn ica  de Val5nciaDepar tament  de S is temes In form\ t i cs  i Computac i6Camf  de Vera s /n46(120 ValSncia{ fp la ,amol ina ,npr ie to}@ds ic .upv .esAbst ractIn this paper we present an integrated system fortagging and chunking texts from a certain language.The approach is based on stochastic finite-statemodels that are learnt automatically.
This includesbigrmn models or tinite-state automata learnt usinggrammatical inference techniques.
As the models in-volved in our system are learnt automatically, thisis a very flexible and portable system.Itl order to show the viability of our approach wet)resent results for tagging mid chunking using bi-grain models on the Wall Street Journal corpus.
Wehave achieved an accuracy rate for tagging of 96.8%,and a precision rate tbr NP chunks of 94.6% with arecall rate of 93.6%.1 In t roduct ionPart of Speech Tagging and Shallow Parsing are twowell-known problems in Natural Language Process-ing.
A Tagger can be considered as 2 translator thatreads sentences from a certain language and outputsthe corresponding sequences of part of speech (POS)tags, taking into account he context in which eachword of the sentence appears.
A Shallow Parser in-volves dividing sentences into non-overlapping seg-ments on the basis of very superticial analysis.
It;includes discovering the main constituents of thesentences (NPs, VPs, PPs, ...) and their heads.Shallow Parsing usually identifies non-recnrsive con-stituents, also called chunks (Abney, 1991) (such asnon-recursive Noun Phrases or base NP, base VP,and so on).
It can include deterlnining syntacticalrelationships such as subject-verb, verb-object, etc.,Shallow parsing wlfich always follows tlm taggingprocess, is used as a fast and reliable pre-l)rocessingphase for full or partial parsing.
It can be used forhffbrmation Retrieval Systems, Information Extrac-tion, Text Summarization and Bilingual Alignment.In addition, it is also used to solve colnputationallinguistics tasks such as disambiguation t)roblems.1.1 POS Tagging ApproachesThe different aI)proaches for solving this problemcan be classified into two main classes dependingoi1 tile tendencies followed for establishing tile Lan-guage Model (LM): tile linguistic apI)roach, basedoil hand-coded linguistic rules and the learning ap-I)roach derived fi'om a corpora (labelled or non-labelled).
Other at)proximations that use hybridmethods have also been proposed (Voutilaiuen andPadr6, 1997).In tim linguistic apl)roach, an exI)ert linguist isneeded to formalise the restrictions of the language.This implies a very lfigh cost and it is very depen-dent on each particular language.
We can lind animportant contribution (Voutilainen, :1995) that usesConstraint Grammar tbrmalism.
Supervised learn-ing methods were proposed in (Brill, 1995) to learna set, of transforlnation rules that repair tim errorcommitted by a probabilistic tagger.
The main a(t-vantage of the linguistic approach is that the modelis constructed from a linguistic I)oint of view andcontains many and complex kinds of knowledge_iI1 tim lem'ning approach, tile most extendedtbrmalism is based on n-grains or IIMM.
In tiffscase, the language inodel can be estimated froma labelled corpus (supervised methods) (Church,1988)(Weisehedel t al., 1.993) or from a non-labelled corpus (unsupervised methods) (Cutting et21., 1992).
In the first; case, the model is trained fromthe relative observed Dequencies.
In the second one,the model is learned using the Baunl-\?elch algo-rithm from an initial model which is estimated usinglabelled corpora (Merialdo, 1994).
The advantagesof the unsupervised approach are the facility to tmildlanguage models, the flexibility of choice of cate-gories and the ease of apt)lication to other languages.We can find some other machine-learning approachesthat use more sophisticated LMs, such as DecisionTrees (Mhrquez and Rodrfguez, 1998)(Magerman,1996), memory-based approaclms to learn special de-cision trees (Daelemans et al, 1996), maximmn en-tropy approaches that combine statistical informa-tion from different sources (Ratnaparkhi, 1996), fi-nite state autonmt2 inferred using Grammatical In-ference (Pla and Prieto, 1998), etc.The comparison among different al)t)roaches is d i fficult due to the nmltiple factors that can be eonsid-614ered: tile languagK, tile mmfl)er and tyt)e of the tags,the size of tilt vocabulary, thK ambiguity, the diiti-culty of the test ski, Kte.
The best rKsults rel)ortedon the Wall Street ,lore'hal (WSJ) %'e('.l)ank (\]~'\[al'CllSel al., 1993), using statistical language models, havean ae(:uracy rack) between 95% and 97% (del)Kndingon the different factors mKntiono.d al)ove).
For thelinguistic al)proach tim results ark l)etter.
For exmn-p\]e, in (Voutilaineu, 1995) an accuracy of 99.7% isrel)orted , but cKrtain ambiguities ill thK ou|;tnl(; re-main unsolved.
Some works have recently l)een pul)-lished (Brill and Wu, 1998) in which a sel; of taggersare combined in order to lint)rove the.Jr l/erfornmn(:e.In some cases, these methods achieve an accuracy of97.9% (llalterKn (31; al., 1998).1,2 Shal low Pars ing  A1)t)roachesSince the early 90's~ sKveral l;Kchni(tues for carry-ing out shalh)w parsing have been d(3velol)ed.
Tlms(~techniques can also bK classified into two maingroups: basKd on hand-codKd linguistic rules andbased on iKarning algorithms.
ThKsK approadmsll~we a conunon chara(:tcristi(:: thKy take, l;he se-(lUKnCK of 1Kxi(:al tags 1)rot)oscd t)y a POS tagger asinput, for both the h;arning and the (:bunking pro-C(~sses.1.2.1 Techniques  based  on hand-codedlinguistiK rulesThese methods use a hand-written set of rules thatark defined l lsing POS as tKrnfinals of tim gI'gtlll-mar.
Most of these works use tinit(!
slate \]nel;llo(lsfor (tel;Kcl;ing (:hunks or f()r a(:(:olni)lishing el;her lin-guisti(: l;asks (EjKrhed, 1988), (:\lm(~y, 1996), (At oMokhtar and Chanod, :19!)7).
()ther works use (tit'--ferellI; ~ralt l l l lgd;ical \]'orlllalislllS~ S/l(;h as (:OllSl;r;/illl;grmnmars (Voutilainen, 1993), or (:oral)inK th('.
gram-mar rules with a set of heuristi(:s (Bourigault, :1992).ThesK works usually use.
a small test SKi that is lllall-ually evaluated, so the achieved results are not sig-ni\[icant.
The regular KXln:cssions defined in (Ejer-lied, 1988) identified both non-recursive clauses andnon-recursive NPs in English text.
The cxperimKn-tation on l;he Brown (:ortms achiKvKd a prK(:ision ratKof 87% (for clauses) and 97.8 % (for NPs).
Ab-hey introduced the concept of chunk (Almey, 1991)m)d l/resentKd an incremental l)artial parser (Abney,1996).
This parsKr identities chunks l)ase on theparts of Sl)eKch, and it then chooses how to con>bine them tbr higher level analysis using lexical in-tbrmation.
ThK average 1)rKcision and recall rates forchunks were 87.9% and 87.1%, rest)ectivKly , on a tKstset of 1000 sKntKneKS.
An iimrenmntal architKctureof finite--state transducers for French is pres(mted in(At-Mokhtar and Chanod, 1.997).
Each transducer1)ert'orms a linguisti(; task su(:h as id(3ntif~ying sKg-ments or syntactic strueturKs and dKtecting subjectsand ol)jects.
The system was (3wfluated on variouscorpora for subject and object detKction.
The pre-cision rate varied between 9(,).2% and 92.6%.
Therecall rate varied between 97.8% and 82.6%.The NP2bol llarsKr described in (Voutilainen,1993) identified nmximal-length noun phrases.NPtool gave a precision ral, e of 95-98% and a re-call ratK of 98.5-100%.
These results were criticisedin (Raulshaw and Marcus, 1.995) due to some in-consistencies and aplmrenl; mistakKs which appearedon thK sample given in (Voutilainen, 1993).
Bouri-gault dKvelopKd the LECTER parser fin" French us-ing grmnmatical rules and soum hem'istics (Bouri-gault, 1992).
lit achieved a recall rate of 95% iden-tit~ying maxilnal ength ternfinological noun phrases,but tie (lid not givK a prKcision ratK, so it is difficult;to Kvaluate the actual pKribrmance of tile parsKr.1..2.2 LKarning Techn iquesThese al)lnoachcs automa.tica.lly (:onstruel; a lan-guage model from a labello.d alld brackKted corpus.The lirst probabilistic approach was proposed in(Church, 1988).
This method learn(; a bigram modelfor detecting simph3 noun phrasKs on the Brown cor-pus.
Civ('n a sequen('e of parts of st)(3eeh as inl)ug ,the Church program inserts the most prol)able open-ings and Kndings of NPs, using a Viterbiqiko.
dy-namic programming algorithm.
Church did not giVKprecision and recall rates.
He showKd that 5 out of24:3 NP were omitted, but in a very small test witha POS tagging ac(:uraey of 99.5%.Transfornlation-based 1Karning (TBI,) was USKd in(\]~;unshaw an(l Mar(:us, 1995) to (lc, t(',('t baSK NP.In this work ('hunldng was considKre(1 as a taggingtechnique, so that each P()S could be tagged withI (inside lmseNP), O (outside baseNl )) or B (insidea baseNP, but 1;11(3 pre(:eding word was ill mlotherbasKNP).
This at)preach rKsulted in a precision rateof 91.8% and a rKcall rate of 92.3%.
This iesultwas automatically Kwlhlat;ed el l  ,q.
(;est set; (200,000words) extracl;Kd from the WS.\] Treebank.
The maindrawlmek to this approach are the high requiremKntstbr tilne and space which ark needed to train ~he sys-l;elll; it needs to train 100 tKmplates of combinationsof words.There are s(;v(;ral works that use a m('mory-basedh,arning algorithm.
ThKse at)proaehKs construct aclassifier tbr a task by storing a sKI; of exmnples ininemory.
Each (;xamI)le is definKd l)y a set of fhaturesthat havK to 1)c. learnt from a 1)racketed corpus.
TheMemory-Based Learning (MBL) algorithm (l)aele,-roans (3t al., 1999) takes into account lexical and POSinformation.
It stores the following features: thKword form mid POS tag of thK two words to the left,the tbeus word and onK word to the right.
This sys-tKm achiKved a precision rate of 93.7'7o and a recallrate of 94.0% on t\]lK WSJ Treebank.
HowevKr, whenonly POS information was used the l)erformance de-creased a.chiKving a precision rate of 90.3% mid a615recall rate of 90.1%.
Tile Memory-Based SequenceLearning (MBSL) algorithm (Argamon et al, 1998)learns substrings or sequences of POS and brackets.Precision and recall rates were 92.4% on the samedata used in (Ramshaw and Marcus, 1995).A simple approach is presented in (Cardie andPierce, 1998) called Treebank Apl)roach (TA).
Thistechtfique matches POS sequences from an initialnoun phrase grammar which was extracted fl'om anannotated corpus.
The precision achieved for eachrule is used to rank and prune the rules, discardingthose rules whose score is lower than a predefinedthreshold.
It uses a longest match heuristic to de-termine base NP.
Precision and recall on the WSJTreebank was 89.4% and 90.0%, respectively.It is difficult to compare the different al)proachesdue fbr various reasons.
Each one uses a differentdefinition of base NP.
Each one is evaluated on adifferent corpus or on different parts of the samecortms.
Some systems have even been evaluated byhand on a very small test set.
Table 1 summarizestile precision and recall rates for learning approachesthat use data extracted from the WSJ Treebank.Method NP-Pl'ecision NP-RecallTBL 91.8 92.3MBSL 92.4 92.4TA 89.4 90.9MBL 93.7 94.0MBL (only POS) 90.3 90.1Tat)le 1: Precision and recall rates tbr diflhrent NPparsers.2 General Descript ion of ourIntegrated approach to Taggingand ChunkingWe propose an integrated system (Figure 1) thatcombines different knowledge sources (lexical prob-abilities, LM for chunks and Contextual LM tbrthe sentences) in order to obtain the correspond-ing sequence of POS tags and the shallow parsing(\[su WllC~W.~/c~ su\] W.~lC~ ... \[su W, lC,, su\])from a certain input string (1'I:1,I?.2, ...,I/l:n).
Oursystem is a transducer composed by two levels: theupper one represents the Contextual LM for tilesentences, and the lower one modelize the chunksconsidered.
The formalism that we have used in alllevels are finite-state automata.
To be exact, wehave used models of bigrmns which are smoothedusing the backoff technique (Katz, 1987) in order toachieve flfll coverage of the language.
The bigramsLMs (bigram probabilities) was obtained by meansof the SLM TOOLKIT  (Clarksond and Ronsenfeld,LEAIINING ~-\[-C,m,zxtuall.~ I2"l"?
'~.Chunks \] l l'e?icalPmbabilities JCIUNKIN(; ~ ~Figure 1: Overview of the System.1997) from tile sequences of categories in thetraining set.
Then, they have been rei)resented likefinite-state automata.2.1 The learning phase.The models have been estimated from labelled andbracketed corpora.
The training set is composed bysentences like:\[su w,/c,w.,/c., su\] w~/c~ ... \[su ~,~:,~/c,~ su\] ./.where Wi are the words, Ci are part-of-speech tagsand SU are tile chunks considered.Tile models learnt are:?
Contextual LM: it is a smoothed bigram modellearnt from tile sequences of part -o f  speech tags(Ci) and chunk descrit)tors (XU) present in thetraining corpus (see Figure 2a).?
Models for the chunks: they are smoothed bi-gram models learnt fl'om the sequences of part-of-speech tags eorrest)onding to each chunk ofthe training corpus (see Figure 2b).?
Lexical Probabilities: they are estilnated fromthe word frequencies, tile tag frequencies andthe word per tag frequencies.
A tag dictio-nary is used which is built from the full cor-pus which gives us the possible lexical categories(POS tags) for each word; this is equivalent ohaving an ideal morphological analyzer.
Theprobabilities for each possible tag are assignedfrom this information taking into account theobtained statistics.
Due to the fact that theword cannot have been seen at training, or ithas only been seen in some of the possible cat-egories, it is compulsory to apply a smoothingmechanism.
In our case, if the word has notpreviously been seen~ the same probability isassigned to all the categories given by the die-tionary; if it has been seen, but not in all the616(b)  LM fo r  Chunks.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i, ', z f+@, ,  - -_ .
.
.Jiitii .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(c) Integrated LMi I(<SU>\[( ) x *Figure 2: Integrated Language Model fin" Tagging and Chunking.categories, the smoothing called "add one" isapplied.
Afterwards, a renormalization processis carried out.Once the LMs have been learnt, a regular substi-tution of the lower model(s) into the upper one ismade.
In this way, we get a single Illtegrated LMwhich shows the possible concatenations of lexicaltags and syntactical uu i ts ,  with their own transitionprobabilities which also include the lexical probabil-ities ms well (see Figure 2c).
Not(', that the modelsin Figure 2 are not smoothed).2.2 The Decod ing  Process: Wagging andPars ingThe tagging and shallow parsing process consists offinding out the sequence of states of maximum 1)rob-ability on the Integrated LM tor an input sentence.Therefore, this sequence must be compatible withthe contextual, syntactical and lexical constraints.This process can be carried out by Dynamic Pro-gt'ammiitg using the Viterbi algorithm, which is con-veniently modified to allow for (;ransitions betweencertain states of the autotnata without consmningany symbols (epsilon l;ransitious).
A portion of theDynamic Progranmfing trellis for a generic sentenceus ing  the Integrated LM shown in Figure 2c can beseen in Figure 3.
The states of the automata thatcan be reached and that are compatible with thelexical constraints are marked with a black circle(i.e., fl'om the state Ck it is possible to reach thestate Ci if the transition is in the automata nd thelexical probability P(Wi\[Ci) is not null).
Also, thetransitions to initial and final states of the modelsfor chunks (i.e., fl'om Ci to < SU >) are allowed;these states are marked in Figure 3 with a white cir-cle and in this case no symbol is consumed.
Ill allthese cases, the transitions to initial and final pro-duce transitions to their successors (the dotted linesin Figure 3) where now symbols must be consumed.Once the Dynamic Programing trellis is built, wecan obtain the maximum probability path for theinput sentence, and thus the best sequence of lexicaltags and the best segmentation i chunks.<s>Cicj<Is> .
.
.
.
.
.
.
\]\ ~ "~ \]',  `% l{inalx\ ". '
State<~u> .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
it>, .
.
.
.
~>; ,  .
.
.
.
.Ci / "" / " ',': ~ t I L(:k ' , s',{.
J i l l  , / t?
/ /c,, ............ ........ 7~-o.. .
....... //~ .
.
.
.
.</S U> "%3~- - - - t ~ - -hlput: .
.
.
Wll-2 Wll- I  Wn </S>Output: .
.
.
Wn~2/Ci I SU Wnq/Cn SUI Wn/Ck </s>Figure 3: Partial %'ellis for Programming Decodingbased oil tile Integrated LM.3 Exper imenta l  WorkIn this section we will describe a set of experimentsthat we carried out in order to demonstrate the ca-pabilities of the proposed approach for tagging andshallow parsing.
The experiments were carried out617on the WSJ corpus, using the POS tag set; definedin (Marcus etlal.
, 1993), considering only the NPchunt{s (lefine~l by (Church, 1988) and using tilemodels that we have presented above.
Nevertheless,the use of this apt)roach on other corpora (chang-ing the reference language), other lexical tag sets orother kinds of chunks can be done in a direct way.3.1 Corpus Description.We used a t)ortion of the WSJ corpus (900,000words), which was tagged according to the PennTreebank tag set and bracketed with NP markers,to train and test the system.The tag set contained 45 different tags.
About36.5% of the words in the cortms were mnbiguous,with an ambiguity ratio of 2.44 tag/word over theambiguous words, 1.52 overall.3.2 Exper imental  Results.In order to train the models and to test the system,we randomly divided the corpora into two parts: ap-proximately 800,000 words for training aud 100,000words tbr testing.Both the bigram models for representing contex-tual information mid syntactic description of the NPchunk and the lexical probabilities were estimatedfrom training sets of different sizes.
Due to the factthat we did not use a morphological nalyser for En-glish, we constructed a tag dictionary with the lex-icon of the training set and the test set used.
Thisdictionary gave us tile possible lexical tags for eachword fl'om the corpus.
In no case, was the test usedto estimate the lexical probabilities.1009998079695949392BIGBIG-BIG\[\[100 200\[i (~ {\] {1LIi i i i300 400 500 60O#Words x 1000Figure 4: Accuracy Rate of Tagging on WSJ forincrementM training sets.In Figure 4, we show the results of tagging on thetest set in terms of the training set size using threeat)proaches: the simplest (LEX) is a tagging processwhich does not take contextual information into ac-count, so the lexical tag associated to a word will1009990970695049392Prec is ion  ?Recall~, +<,,+, i , _ _  i i100 200 300 4o0 500 600 7(30 800#Words x 1000Figure 5: NP-chunldng results on WSJ for incremen-tal training sets.TaggerTaggingAccuracyBIG-BIG 96.8Lex 94.3BIG 96.9IDEAL 100 (assumed)NP-ClmnkingPrecision I Recall94.6 193.690.8 91.394.9 94.195.5 94.7Table 2: Tagging and NP-Chunking results t'or dif-ferents taggers (training set of 800,000 words).be that which has aI)peared more often in the train-ing set.
Tile second method corresponds to a taggerbased on a bigram model (BIG).
The third one usesthe Integrated LM described in this pai)er (BIG-BIG).
The tagging accuracy for BIG and BIG-BIGwas close, 96.9% and 96.8% respectively, whereaswithout the use of the language model (LEX), tiletagging accuracy was 2.5 points lower.
The trend inall the cases was that an increment in the size of thetraining set resulted in an increase in the taggingaccuracy.
After 300,000 training words, the resultbecame stabilized.In Figure 5, we show the precision (#correctproposed NP/#proposed  NP) and recall (#correctproposed NP/#NP in the reference) rates for NPchunking.
The results obtained using the IntegratedLM were very satisfactory achieving a precision rateof 94.6% and a recall rate of 93.6%.
The perfor-mance of the NP chunker improves as the train-ing set size increases.
This is obviously due to thefact that tile model is better learnt when the sizeof the training set increases, and the tagging errordecreases as we have seen above.The usual sequential 1)rocess for chunking a sen-tence can also be used.
That is, first we tag the sen-tence and then we use the Integrated LM to carryout the chunking.
In this case, only tim contextualt)robabilities are taken into account in the decoding6181)recess.
In Table 2, we show the most relevant re-suits that we obtained for tagging and tbr NP chunk-ing.
The first row shows the result when the taggingand the chunking are done in a integrated way.
Thefollowing rows show the performmme of the sequen-tial process using different aggers:?
LEX: it takes into account only lexical proba-t)ilities.
In this case, the tagging accuracy was94.3%.?
BIG: it is based on a bigram model thatachieved an accuracy of 96.9%.?
IDEAL: it siinulates a tagger with an accuracyrate of 100%.
To do this, we used the taggedsentences of the WSJ corlms directly.These results confirm that precision and recallrates increase when the accuracy of the tagger isbeN;er.
The pert'ormmme of 1;he, se(tuential process(u:dng the BIG tagger) is slightly 1letter than thepet'formance of the integrated process (BIG-BIG).We think that this is 1)robably b(;cause of the waywe combined the I)robabilities of t;he ditthrent mod-els.4 Conclusions and Future  WorkIn this 1)aper, we have t)rcscntcd a system tot" Tag-ging and Chunldng based on an Integrated Lan-guage Model that uses a homogeneous tbrmalism(finite-state machine) to combine different knowl-edge sources: lexical, syntacti(:al and contextualinodels.
It is feasible l)oth in terms of 1)erfl)rmanc(;and also in terms of computational (:tliciency.All the models involv(:d are learnt automaticallyfi'om data, so the system is very tlexibte and 1)ortableand changes in the reference language., lexical tagsor other kinds of chunks can be made in a direct way.The tagging accuracy (96.9% using BIG and96.8% using BIG-BIG) is higher tlmn other similaralIl)roaches.
This is because we have used the tagdi('tionary (including the test set in it) to restrictthe possible tags for unknown words, this assmnp-lion obviously in(:rease the rates of tagging (we havenot done a quantitative study of this factor).As we have mentioned above, the comparison withother approaches i  ditficult due mnong other reasonsto tim following ones: the definitions of base NP arenot always the stone, the sizes of the train and thetest sets are difl'erent and the knowledge sources usedin the learning process are also different.
The pre-cision for NP-chunking is similm' to other statisticalat)preaches t)resented in section 1, tbr 1)oth the in-tegrated process (94.6%) and l;tm sequential processusing a tagger based on 1)igrams (94.9%).
The recallrate is slightly lower than for some apl)roaches usingthe integrated system (93.6%) and is similar for thesequential process (94.1%).
When we used the se-quential system taking an error ti'ee input (IDEAL),the performance of the system obviously increased(95.5% precision and 94.7% recall).
These resultsshow the influence of tagging errors on the process.Nevertheless, we are studying why the results lie-tween the integrated process and the sequential pro-cess are diflbrent.
We are testing how the introduc-tion of soIne adjustnmnt factors among the modelstk)r we, ighting the difl'erent 1)robability distributioncan lint)rove the results.The models that we have used in this work, are ill-grams, but trigrams or any stochastic regular modelcan be used.
In this respect, we have worked on amore coml)lex LMs, formalized as a. finite-state au-tomata which is learnt using Grammatical Inferencetectufiques.
Also, our ai)l)roach would benefit fl'omthe inclusion of lexical-contextual in%rmation intothe LM.5 AcknowledgmentsThis work has been partially supl)orted 1)y theStmnish I{esem'ch Projct:t CICYT (TIC97-0671-C02-O11O2).ReferencesS.
Abney.
1991.
Parsing by Chunks.
R. Berwick, S.Almey and C. Tcnny (eds.)
Principle -based Pars-ing.
Kluwer Acadenfic Publishers, Dordrecht.S.
Almey.
1996.
Partial Parsing via Finit('.-Sta~eCascades.
In Proceedings of the ES,S'LLI'96 Ro-bust Parsinfl Workshop, l?rague, Czech l{elmblie.S.
Argamon, I. Dagan, and Y. Krymolowski.
1.998.A Memory based Approach to Learning ShallowNatural Language, Patterns.
In l~roceedi'ngs oft,h,e joint 17th, International Conference on Com-putational Linguistics and 36th Annual Meetingof the Association for Computational Linguistics,COLING-ACL, pages 67 73, Montrdal, Canada.S.
At-Mokhtar and ,l.P. Chanod.
1997.
Incremen-tal Finite-State Parsing.
In Proceedings of the 5th,Conference on Applied Natural Language Process-ing, \Vashington D.C., USA.D.
Bourigault.
1992.
Surface Grmnmatical Anal-,),sis for tim Extraction of ~l~.~rminological NomlPhrases.
In Proceedings of the 15th InternationalConference on Computational Linguistics, pages977-981.Eric Brill and Jun Wu.
1998.
Classifier Combi-nation for hnproved Lexical Disambiguation.
InProcccdings of the joint 17th, International Con-fcrcncc on Computational Linguistics and 36thAnnual Meeting of thc Association for Computa-tional Linguistics, COLING-ACL, pages 191-195,Montrdal, Canada.E.
Brill.
1995.
Transibnnation-based Error-drivenLearning and Natural Language Processing: A619Case Study in Part-of-sI)eech Tagging.
Compu-tational Linguistics, 21 (4) :543-565.C.
Car(lie and D. Pierce.
1998.
Error-Driven Prun-ning of Treebank Grammars for Base Noun PhraseIdentification.
In Proceedings of the joint 17thInternational Conference on Computational Lin-guistics and 36th Annual Meeting of the Asso-ciation for Computational Linguistics, COLING-ACL, pages 218 224, Montrdal, Canada, August.K.
W. Church.
1988.
A Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text.In Proceedings of the 1st Conference on AppliedNatural Language Processing, ANLP, pages 136-143.
ACL.P.
Clarksond and R. Ronsenfeld.
1997.
StatisticalLanguage Modeling using the CMU-CambridgeToolkit.
In Procccdinfls of Eurospccch, Rhodes,C,-reece.D.
Cutting, J. Kut)iec , J. Pederson, and P. Nil)un.1992.
A Practical Part-of-speech Tagger.
In Pfv-cccdings of the 3rd Confcrcnce oft Applied Natu-ral Language Processing, ANLP, pages 133 140.ACL.W.
Daelelnans, J. Zavrel, P. Berck, and S. Gillis.1996.
MBT: A MeInory-Based Part of speechTagger Generator.
In Proceedings of the /tthWorkshop on Very Large Cmpora, pages 14-27,Copenhagen, Denmark.W.
Daelemans, S. Buchholz, and J. Veenstra.
1999.Memory-Based Shallow Parsing.
In Proceedingsof EMNLP/VLC-99, pages 239 246, University ofMaryla.nd, USA, June.E.
Ejerhed.
1988.
Finding Clauses in UnrestrictedText by Finitary and Stochastic Methods.
In Pro-cccdings of Second Confcrcncc on Applied NaturalLanguage Processing, pages 219-227.
ACL.H.
van Halteren, J. Zavrel, and W. Daelemans.
1998.Improving Data Driven Wordclass Tagging bySystem Combination.
In Proceedings of the joint17th International Confcr'cncc oft ComputationalLinguistics and 36th Annual Mccting of the Asso-ciation for Computational Linguistics, COLING-ACL, pages 491-497, Montrdal, Canada, August.S.
M. Katz.
1987.
Estimation of Probabilities fromSparse Data for tile Language Model Componentof a Speech Recognizer.
IEEE T~nnsactions onAcoustics, Speech and Signal Processing, 35.D.
M. Magerman.
1996.
Learning GrammaticalStructure Using Statistical Decision-Trees.
InProceedings of the 3rd International Colloquiumon GTnmmatical Inference, ICGI, pages 1-21.Springer-Verlag Lecture Notes Series in ArtificialIntelligence 1147.M.
P. Marcus, M. A. Marcinkiewicz, and B. San-torini.
1993.
Building a Large Annotated Cortmsof English: Tile Penn Treebank.
ComputationalLinguistics, 19(2).Llu/s Mhrquez and Horacio RodHguez.
1998.
Part-of Speech T~gging Using Decision Trees.
In C.Nddellee and C. Rouveirol, editor, LNAI 1398:Proceedings of thc lOth European Conferenceon Machine Learning, ECML'98, pages 25-36,Chemnitz, GermNly.
Springer.B.
Merialdo.
1994.
Tagging English Text with aProbabilistic Model.
Computational Linguistics,20(2):155-171.F.
Pla and N. Prieto.
1998.
Using GrammaticalInference Methods tbr Automatic Part of speechTagging.
In Proceedings of 1st International Con-ference on Language Resources and Evaluation,LREC, Granada, Spain.L.
Ramshaw and M. Marcus.
1995.
Text ChunkingUsing ~lYansfbrmation-Based Learning.
In Pro-cccdings of third Workshop on Very Large Colpora, pages 82 94, June.A.
Ratnapm'khi.
1996.
A Maximum Entrol)y Partof-speech Tagger.
In Proceedings of the 1st Con-fcrcncc on Empirical Methods in Natural Lan-guagc Processing, EMNLP.Atro Voutilainen and Llufs Padrd.
1997.
Develol)-inn a Hybrid NP Parser.
In Proceedings ofthe 5thConference on Applied Natural Language Prvecss-ing, ANLP, pages 80 87, Washington DC.
ACL.Atro Voutilainen.
1993.
NPTool, a Detector of En-glish Noun Phrases.
In Proceedings of the Work-shop on Very Lafflc Corpora.
ACL, June.Atro Voutilainen.
1995.
A Syntax-Based Part o fspeech Analyzer.
In Prvcccdings of the 7th Con-ference of the European Ch, aptcr of the Associationfor Computational Linguistics, EACL, Dut)lin,h'eland.R.
Weischedel, R. Schwartz, J. Pahnueci, M. Meteer,and L. Ramshaw.
1993.
Coping with Ambiguityand Unknown \~or(ls through Probabilistic Mod-els.
Computational Linguistics, 19(2):260-269.620
