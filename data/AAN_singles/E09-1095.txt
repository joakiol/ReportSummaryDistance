Proceedings of the 12th Conference of the European Chapter of the ACL, pages 835?842,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsGrowing Finely-Discriminating Taxonomies from Seedsof Varying Quality and SizeTony VealeSchool of Computer ScienceUniversity College DublinIrelandtony.veale@ucd.ieGuofu LiSchool of Computer ScienceUniversity College DublinIrelandguofu.li@ucd.ieYanfen HaoSchool of Computer ScienceUniversity College DublinIrelandyanfen.hao@ucd.ieAbstractConcept taxonomies offer a powerful meansfor organizing knowledge, but this organiza-tion  must  allow  for  many  overlapping  andfine-grained perspectives if a general-purposetaxonomy is  to  reflect  concepts  as  they areactually employed and reasoned about in ev-eryday  usage.
We present  here  a  means  ofbootstrapping  finely-discriminating  tax-onomies from a variety of different  startingpoints, or seeds, that are acquired from threedifferent sources: WordNet, ConceptNet andthe web at large.1 IntroductionTaxonomies  provide  a  natural  and  intuitivemeans of organizing information, from the bio-logical taxonomies of the Linnaean system to thelayout of supermarkets and bookstores to the or-ganizational structure of companies.
Taxonomiesalso provide the structural backbone for ontolo-gies  in  computer  science,  from common-senseontologies like Cyc (Lenat and Guha, 1990) andSUMO (Niles and Pease, 2001) to lexical ontolo-gies like WordNet (Miller  et al, 1990).
Each ofthese uses is based on the same root-branch-leafmetaphor:  the  broadest  terms  with  the  widestscope occupy the highest positions of a taxono-my, near the root, while specific terms with themost local concerns are located lower in the hier-archy,  nearest  the  leaves.
The  more  interiornodes that  a taxonomy possesses,  the finer  theconceptual distinctions and the more gradated thesimilarity judgments it can make (e.g., Budanit-sky and Hirst, 2006).General-purpose  computational  taxonomiesare called upon to perform both coarse-grainedand  fine-grained  judgments.
In  NLP,  for  in-stance,  the  semantics  of  ?eat?
requires  justenough  knowledge  to  discriminate  foods  liketofu and cheese from non-foods like  wool  andsteel, while specific applications in the domain ofcooking  and  recipes  (e.g.,  Hammond?s  (1986)CHEF)  require  enough  discrimination  to  knowthat tofu can be replaced with clotted cheese inmany recipes because each is a soft, white andbland food.So while much depends on the domain of us-age, it remains an open question as to how manynodes a good taxonomy should possess.
Prince-ton WordNet,  for  instance,  strives for  as manynodes as there are word senses in English, yet italso contains a substantial number of compositenodes  that  are  lexicalized not  as  single  words,but as complex phrases.
Print dictionaries intend-ed for human consumption aim for some econo-my of structure, and typically do not include themeaning  of  phrases  that  can  be  understood  asstraightforward compositions of the meaning oftheir  parts  (Hanks,  2004).
But  WordNet  alsoserves another purpose, as a lexical knowledge-base  for  computers,  not  humans,  a  context  inwhich concerns about space seem quaint.
Whenspace is not a issue, there seems no good reasonto exclude nodes from a concept taxonomy mere-ly for being composites of other ideas; the realtest of entry is whether a given node adds valueto a taxonomy, by increasing its level of internalorganization through the systematic dissection ofoverly broad categories into finer, more intuitiveand manageable clusters.In this paper we describe a means by whichfinely-discriminating  taxonomies  can  be  grownfrom  a  variety  of  different  knowledge  seeds.These taxonomies comprise composite categoriesthat  can  be  lexicalized  as  phrases  of  the  form?ADJ NOUN?, such as Sharp-Instrument, whichrepresents the set of all instruments that are typi-cally considered sharp, such as knives, scissors,chisels and can-openers.
While WordNet aleadycontains  an  equivalent  category,  named  Edge-835Tool, which it defines with the gloss ?any cuttingtool  with a sharp cutting edge?,  it  provides  nostructural basis for inferring that any member ofthis  category can be considered  sharp.
For  themost part, if two ideas (word senses) belong tothe same semantic category X in WordNet, themost we can infer is that both possess the trivialproperty  X-ness.
Our  goal  here  is  to  constructtaxonomies whose form makes explicit the actualproperties that accrue from membership in a cat-egory.Past work on related approaches to taxonomycreation are discussed in section 2, while section3  describes  the  different  knowledge  seeds  thatserve as the starting point for our bootstrappingprocess.
In section 4 we describe the bootstrap-ping process in more detail;  such processes areprone to noise, so we also discuss how the ac-quired categorizations are validated and filteredafter each bootstrapping cycle.
An evaluation ofthe key ideas is then presented in section 5, todetermine which seed yields the highest qualitytaxonomy once bootstrapping is completed.
Thepaper then concludes with some final remarks insection 6.2 Related WorkSimple pattern-matching techniques can be sur-prisingly effective for the extraction of lexico-se-mantic relations from text when those relationsare expressed using relatively stable and unam-biguous  syntagmatic  patterns  (Ahlswede  andEvens, 1988).
For instance, the work of Hearst(1992) typifies this surgical approach to relationextraction,  in  which a system fishes in  a largetext for particular word sequences that stronglysuggest  a  semantic  relationship  such  as  hyper-nymy  or,  in  the  case  of  Charniak and Berland(1999), the part-whole relation.
Such efforts offerhigh precision but can exhibit low recall on mod-erate-sized corpora, and extract just a tiny (butvery useful) subset of the semantic content of atext.
The  KnowItAll system  of  Etzioni  et  al.
(2004)  employs  the  same  generic  patterns  asHearst (e.g., ?NPs such as NP1, NP2, ??
),  andmore besides, to extract a whole range of factsthat can be exploited for web-based question-an-swering.
Cimiano  and  Wenderoth  (2007)  alsouse a range of Hearst-like patterns to find text se-quences in web-text that are indicative of the lex-ico-semantic  properties  of  words;  in  particular,these  authors  use  phrases  like  ?to  *  a  newNOUN?
and ?the purpose of NOUN is to *?
toidentify the formal (isa), agentive (made by) andtelic (used for) roles of nouns.Snow, Jurafsky and Ng (2004) use supervisedlearning techniques to acquire those syntagmaticpatterns that prove most useful for extracting hy-pernym relations from text.
They train their sys-tem using pairs of WordNet terms that exemplifythe hypernym relation; these are used to identifyspecific sentences in corpora that are most likelyto express the relation in lexical terms.
A binaryclassifier is then trained on lexico-syntactic fea-tures that are extracted from a dependency-struc-ture  parse  of  these  sentences.
Kashyap  et  al.,(2005) experiment with a bootstrapping approachto  growing concept  taxonomies  in  the  medicaldomain.
A  gold  standard  taxonomy  providesterms that are used to retrieve documents whichare  then  hierarchically  clustered;  cohesivenessmeasures are used to yield a taxonomy of termsthat can then further drive the retrieval and clus-tering cycle.
Kozareva  et al (2008) use a boot-strapping approach that extends the fixed-patternapproach  of  Hearst  (1992)  in  two  intriguingways.
First, they use a doubly-anchored retrievalpattern of the form ?NOUNcat such as NOUNexam-ple and  *?
to  ground the  retrieval  relative  to  aknown example of hypernymy,  so that any val-ues extracted for the wildcard * are likely to becoordinate terms of  NOUNexample and even morelikely to be good examples of NOUNcat.
Second-ly, they construct a graph of terms that co-occurwithin this pattern to determine which terms aresupported by others,  and by how much.
Theseauthors also use two kinds of bootstrapping: thefirst  variation,  dubbed  reckless,  uses the candi-dates extracted from the double-anchored pattern(via *) as exemplars (NOUNexample) for successiveretrieval cycles; the second variation first checkswhether a candidate is sufficiently supported tobe used as an exemplar in future retrieval cycles.The approach we describe here is most similarto that of Kozareva  et al (2008).
We too use adouble-anchored pattern, but place the anchors indifferent  places  to  obtain  the  query  patterns?ADJcat NOUNcat such as *?
and ?ADJcat * suchas NOUNexample?.
As a result, we obtain a finely-discriminating  taxonomy  based  on  categoriesthat are explicitly annotated with the properties(ADJcat)  that  they  bequeath  to  their  members.These categories have an obvious descriptive andorganizational  utility,  but  of  a kind that  one isunlikely  to  find  in  conventional  resources  likeWordNet and Wikipedia.
Kozareva et al (2008)test their approach on relatively simple and ob-jective  categories  like  states,  countries (both836closed sets), singers and fish (both open, the for-mer more so than the latter), but not on complexcategories in which members are tied both to ageneral category, like food, and to a stereotypicalproperty, like  sweet (Veale and Hao, 2007).
Byvalidating  membership  in  these  complex  cate-gories  using WordNet-based heuristics,  we  canhang these categories and members  on specificWordNet senses, and thus enrich WordNet withthis additional taxonomic structure.3 Seeds for Taxonomic GrowthA fine-grained taxonomy can be viewed as a setof triples Tijk = <Ci, Dj, Pk>, where Ci denotes a child of the parent term Pk that possesses the dis-criminating  property  Dj;  in  effect,  each  suchtriple expresses that Ci is a specialization of thecomplex  taxonym  Dj-Pk.
Thus,  the  belief  thatcola  is  a  carbonated-drink  is  expressed  by thetriple <cola, carbonated, drink>.
From this triplewe  can  identify  other  categorizations  of  cola(such as treat and refreshment) via the web query?carbonated * such as cola?, or we can identifyother similarly fizzy drinks via the query ?car-bonated  drinks  such  as  *?.
So  this  web-basedbootstrapping  of  fine-grained  category  hierar-chies requires that we already possess a collec-tion  of  fine-grained  distinctions  of  a  relativelyhigh-quality.
We  now  consider  three  differentstarting points for this bootstrapping process, asextracted from three different resources:  Word-Net, ConceptNet and the web at large.3.1 WordNetThe noun-sense taxonomy of WordNet makes anumber  of  fine-grained  distinctions  that  proveuseful in clustering entities into smaller and morenatural groupings.
For instance, WordNet differ-entiates  {feline,  felid} into  the  sub-categories{true_cat,  cat} and  {big_cat,  cat},  the  formerserving  to  group  domesticated  cats  with  othercats of a similar size, the latter serving to clustercats  that  are  larger,  wilder  and  more  exotic.However, such fine-grained distinctions are theexception rather than the norm in WordNet, andnot  one of  the  60+ words  of  the  form  Xess inWordNet that denote a person (such as huntress,waitress, Jewess, etc.)
express the defining prop-erty  female in  explicit  taxonomic  terms.Nonetheless, the free-text glosses associated withWordNet sense-entries often do state the kind ofdistinctions we would wish to find expressed asexplicit  taxonyms.
A  shallow  parse  of  theseglosses  thus  yields  a  sizable  number  of  fine-grained  distinctions,  such  as  <lioness,  female,lion>,   <espresso,  strong,  coffee>  and  both<messiah, awaited, king> and <messiah, expect-ed, deliverer>.3.2 ConceptNetDespite  its  taxonomic  organization,  WordNetowes much to the centralized and authority-pre-serving  craft  of  traditional  lexicography.
Con-ceptNet (Liu and Singh, 2004), in contrast, is afar less authoritative knowledge-source, one thatowes more to the workings of the WWW than toconventional print dictionaries.
Comprising fac-toids culled from the template-structured contri-butions of thousands of web users,  ConceptNetexpresses many relationships that accurately re-flect  a  public,  common-sense  view on a  giventopic (from vampires to dentists) and many morethat are simply bizarre or ill-formed.
Looking tothe relation that interests us here, the IsA rela-tion,  ConceptNet  tells  us  that  an  espresso is  astrong coffee (correctly, like WordNet) but that abagel is a Jewish word (confusing use with men-tion).
Likewise, we find that expressionism is anartistic style (correct, though WordNet deems itan  artistic movement) but that an  explosion is asuicide attack (confusing formal and telic roles).Since we cannot trust the content of ConceptNetdirectly, lest we bootstrap from a highly unreli-able starting point, we use WordNet as a simplefilter.
While  the  concise  form  of  ConceptNetcontains over 30,000 IsA propositions, we con-sider as our seed collection only those that definea noun concept (such as ?espresso?)
in terms of abinary  compound  (e.g.,  ?strong coffee?)
wherethe head of the latter (e.g.,  ?coffee?)
denotes aWordNet hypernym of some sense of the former.This  yields  triples  such  as  <Wyoming,  great,state>,  <wreck,  serious,  accident>  and  <wolf,wild, animal>.3.3 Web-derived StereotypesVeale and Hao (2007) also use the observationsof web-users to acquire common perceptions ofoft-mentioned ideas, but do so by harvesting sim-ile expressions of the form ?as ADJ as a NOUN?directly from the web.
Their approach hinges onthe fact that similes exploit stereotypes to drawout the salient properties of a target, thereby al-lowing rich  descriptions of those stereotypes tobe easily acquired, e.g., that snowflakes are pureand unique, acrobats are agile and nimble, knifesare  sharp and dangerous,  viruses  are  maliciousand infectious, and so on.
However, because theyfind that almost 15% of their web-harvested sim-837iles are ironic (e.g., ?as subtle as a rock?, ?as bul-letproof as a sponge-cake?, etc.
), they filter ironyfrom these associations by hand, to yield a siz-able  database  of  stereotypical  attributions  thatdescribes over 6000 noun concepts in terms ofover  2000  adjectival  properties.
However,  be-cause Veale and Hao?s data directly maps stereo-typical properties to simile vehicles, it does notprovide  a  parent  category  for  these  vehicles.Thus, the seed triples derived from this data areonly partially instantiated;  for  instance,  we ob-tain <surgeon, skilful, ?>, <virus, malicious, ?>and <dog, loyal, ?>.
This does not prove to be aserious  impediment,  however,  as  the  missingfield  of  each triple  is  quickly identified duringthe first cycle of bootstrapping.3.4 Overview of Seed ResourcesNeither of these three seeds is an entirely usefulknowledge-base in its own right.
The WordNet-based seed is clearly a representation of conve-nience,  since  it  contains  only  those  propertiesthat can be acquired from the glosses that happento be amenable  to a simple  shallow-parse.
TheConceptNet seed is likewise a small collection oflow-hanging fruit, made smaller still by the useof WordNet as a coarse but very necessary noise-filter.
And while the simile-derived distinctionsobtained from Veale and Hao paint a richly de-tailed  picture  of  the  most  frequent  objects  ofcomparison, this seed offers no coverage for themajority of concepts that are insufficiently note-worthy to be found in web similes.
A quantita-tive comparison of all three seeds is provided inTable 1 below.WordNet ConceptNet Simile# termsin total 12,227 1,133 6512# triplesin total 51,314 1808 16,688# triplesper term 4.12 1.6 2.56# fea-tures 2305 550 1172Table 1:  The size of seed collections yielded fromdifferent sources.We can see that WordNet-derived seed is clearlythe largest and apparently the most comprehen-sive knowledge-source of the  three:  it  containsthe most terms (concepts), the most features (dis-criminating properties of those concepts), and themost triples (which situate those concepts in par-ent  categories  that  are  further  specialized  bythese  discriminating  features).
But  size  is  onlyweakly suggestive of quality, and as we shall seein  the  next  section,  even such  dramatic  differ-ences in scale can disappear after several cyclesof bootstrapping.
In section 5 we will then con-sider  which  of  these  seeds  yields  the  highestquality taxonomies after bootstrapping has beenapplied.4 Bootstrapping from SeedsThe seeds of the previous section each representa different starting collection of triples.
It is thegoal of the bootstrapping process to grow thesecollections  of  triples,  to  capture  more  of  theterms ?
and more of the distinctions ?
that a tax-onomy is expected to know about.
The expansionset  of  a  triple  Tijk =  <Ci,  Dj,  Pk> is  the  set  oftriples that can be acquired from the web usingthe  following  query  expansions  (*  is  a  searchwildcard):1.
?Dj * such as Ci?2.
?Dj Pk such as *?In the first query, a noun is sought to yield anoth-er categorization of Ci, while in the second, othermembers of the fine-grained category Dj-Pk aresought to accompany Ci.
In parsing the text snip-pets  returned by these  queries,  we also exploittext sequences that match the following patterns:3.
?
* and Dj Pk such as *?4.
?
* and Dj * such as Ci?These last two patterns allow us to learn new dis-criminating  features  by  noting  how  these  dis-criminators are combined to reinforce each otherin  some  ad-hoc  category  formulations.
For  in-stance, the phrase ?cold and refreshing beveragessuch  as  lemonade?
allows  us  to  acquire  thetriples <lemonade, cold, beverage> and <lemon-ade, refreshing, beverage>.
This pattern is neces-sary if the bootstrapping process is to expand be-yond  the  limited  vocabulary  of  discriminatingfeatures  (Dj)  found in  the  original  seed collec-tions of triples.We denote the mapping from a triple T to theset of additional triples that can be acquired fromthe web using the above queries/patterns as  ex-pand(T').
We currently implement this functionusing  the  Google  search  API.
Our  experienceswith each query suggest  that  200 snippets is  agood search range for the first query, while 50 isusually more than adequate for the second.838We can now denote the knowledge that is ac-quired when starting from a given seed collectionS after t cycles of bootstrapping as KtS.
Thus,K 0S=SK 1S=K 0S ?
{T ?
T '?S ?
T?expand ?T ' ?
}K t?1S =K tS ?
{T ?
T '?K tS ?
T?expand ?T ' ?
}Web queries, and the small snippets of text thatthey return, offer just a keyhole view of languageas it is used in real documents.
Unsurprisingly,the  new triples  acquired from the  web via  ex-pand(T') are likely to be very noisy indeed.
Fol-lowing Kozareva et al (2008), we can either in-dulge  in  reckless  bootstrapping,  which  ignoresthe  question  of  noise  until  all  bootstrapping  isfinished, or we can apply a noise filter after eachincremental step.
The latter approach has the ad-ditional advantage of keeping the search-space assmall as possible, which is a major considerationwhen bootstrapping from sizable seeds.
We use asimple WordNet-based filter called near-miss:  anew triple <Ci,  Dj,  Pk> is accepted if WordNetcontains  a  sense  of  Ci that  is  a  descendant  ofsome sense of Pk (a hit), or a sense of Ci that is adescendant of the direct hypernym of some senseof Pk (a near-miss).
This allows the bootstrappingprocess to acquire structures that are not simply adecorated version of the basic WordNet taxono-my,  but  to acquire hierarchical  relations whoseundifferentiated forms are not in WordNet (yetare largely compatible with WordNet).
This non-reckless bootstrapping process can be expressedas follows:K t?1S =K tS ?
{T ?
T '?K tS ?T?
filter near?miss?expand ?T ' ??
}Figure 1 and figure 2 below illustrate the rate ofgrowth  of  triple-sets  from  each  of  our  threeseeds.Referring again to table 1, we note that whilethe ConceptNet collection is by far the smallestof  the three seeds ?
more  that  7 times smallerthan the simile-derived seed, and almost 40 timessmaller than the WordNet seed ?
this differenceis  size  shrinks  considerably over  the  course  offive  bootstrapping  cycles.
The  WordNet  near-miss filter ensures that the large body of triplesgrown from each  seed  are  broadly  sound,  andthat  we  are  not  simply  generating  comparablequantities of nonsense in each case.Figure 1: Growth in the number of acquired triples,over 5 cycles of bootstrapping from different seeds.Figure 2: Growth in the number of terms described bythe acquired triples, over 5 cycles of bootstrappingfrom different seeds.4.1 An  ExampleConsider cola, for which the simile seed has onetriple: <cola, refreshing, beverage>.
After a sin-gle cycle of bootstrapping, we find that cola cannow be described as an effervescent beverage, asweet  beverage,  a  nonalcoholic  beverage andmore.
After a second cycle, we find it describedas a sugary food, a fizzy drink and a dark mixer.After a third cycle, it is found to be a  sensitivebeverage, an  everyday beverage and a  commondrink.
After a fourth cycle, it is also found to bean  irritating food and an  unhealthy drink.
Afterthe  fifth  cycle,  it  is  found to  be  a  stimulatingdrink, a toxic food and a corrosive substance.
Inall, the single cola triple in the simile seed yields14 triples after 1 cycle, 43 triples after 2 cycles,72 after 3 cycles, 93 after 4 cycles, and 102 after5 cycles.
During these bootstrapping cycles, thedescription  refreshing beverage additionally be-comes  associated  with  the  terms  champagne,lemonade and beer.0 1 2 3 4 5020000040000060000080000010000001200000140000016000001800000 WordNetSimileConceptNetBootstrapping Cycle#Triples0 1 2 3 4 5050000100000150000200000250000300000350000WordNetSimileConceptNetBootstrapping Cycle#Terms8395 Empirical EvaluationThe WordNet  near-miss filter thus ensures thatthe parent field (Pk) of every triple contains a val-ue  that  is  sensible  for  the  given  child  concept(Ci), but does not ensure that the discriminatingproperty  (Dj)  in  each  triple  is  equally  sensibleand apropos.
To see  whether the bootstrappingprocess  is  simply  padding  the  seed  taxonomywith large quantities of noise,  or whether the ac-quired Dj values do indeed mark out the implicitessence of the Ci terms they describe, we need anevaluation framework that can quantify the onto-logical usefulness of these Dj values.
For this, weuse  the  experimental  setup  of  Almuhareb  andPoesio  (2005),  who  use  information  extractionfrom the web to acquire attribute values for dif-ferent terms/concepts, and who then compare thetaxonomy that can be induced by clustering thesevalues  with the  taxonomic  backbone  of  Word-Net.Almuhareb and Poesio first created a balancedset  of  402  nouns  from  21  different  semanticclasses in WordNet.
They then acquired attestedattribute values for these nouns (such as  hot forcoffee,  red for car, etc.)
using the query "(a|an|the) * Ci  (is|was)" to find corresponding Dj val-ues for each Ci.
Unlike our work, these authorsdid  not seek to acquire hypernyms  for each Ciduring this search, and did not try to link the ac-quired attribute values to a particular branchingpoint  (Pk) in the taxonomy (they did,  however,seek matching attributes for these values, such asTemperature for  hot, but that aspect is not rele-vant here).
They acquired 94,989 attribute valuesin all for the 402 test nouns.
These values werethen used as features of the corresponding nounsin  a  clustering  experiment,  using  the  CLUTOsystem of Karypis (2002).
By using attribute val-ues  as  a  basis  for  partitioning  the  set  of  402nouns  into  21  different  categories,  Almuhareband Poesio attempted to reconstruct the original21  WordNet  categories  from which  the  nounswere drawn.
The more accurate the match to theoriginal WordNet clustering, the more these at-tribute values can be seen (and used) as a repre-sentation of conceptual structure.
In their first at-tempt, they achieved just a 56.7% clustering ac-curacy against the original human-assigned cate-gories of WordNet.
But after using a noise-filterto remove almost  half of the web-harvested at-tribute values, they achieve a higher cluster accu-racy of 62.7%.
More specifically, Poesio and Al-muhareb achieve a cluster purity of 0.627 and acluster entropy of 0.338 using 51,345 features todescribe and cluster the 402 nouns.1We?replicate?the?above?experiments?using?the?same?402?nouns,?and?assess?the?clustering?accur?acy ?
(again ?using ?WordNet ?
as ?
a ?
gold?standard)?after?each?bootstrapping?cycle.
?Recall?that?we?use?only?the?Dj?fields?of?each?triple?as?features?for?the?clustering ?process, ?
so ?
the ?comparison ?with ?
the?WordNet?gold?standard?is?still?a?fair?one.
?Once?again,?the?goal?is?to?determine?how?much?like?the?human?crafted ?
WordNet ?
taxonomy ?
is ?
the ?
tax?onomy?
that ?
is ?clustered?automatically ?from?the?discriminating?words?Dj?only.
?The?clustering?ac?curacy?for?all?three?seeds?are?shown?in?Tables?2,?3?and?4.Cycle  E  P # Features Coverage1st .327 .629 907 66%2nd .253 .712 1,482 77%3rd .272 .717 2,114 82%4th .312 .640 2,473 83%5th .289 .684 2,752 83%Table 2: Clustering accuracy using the WordNet seedcollection (E denotes Entropy and P stands for Purity)Cycle E P # Features Coverage1st .115 .842 363 41%2nd .255 .724 787 59%3rd .286 .694 1,362 74%4th .279 .694 1,853 79%5th .299 .673 2,274 82%Table 3: Clustering accuracy using the ConceptNetseed collectionCycle E P # Features Coverage1st .254 .716 837 59%2nd .280 .712 1,338 73%3rd .289 .693 1,944 79%4th .313 .660 2,312 82%5th .157 .843 2,614 82%Table 4: Clustering accuracy using the Simile seedcollectionThe test-set of 402 nouns contains some low-fre-quency words, such as casuarina,  cinchona,  do-decahedron, and  concavity, and Almuhareb and1 We use cluster purity as a reflection of clustering accu-racy.
We express accuracy as a percentage; hence a pu-rity of 0.627 is seen as an accuracy of 62.7%.840Poesio note that one third of their data-set has alow-frequency of between 5-100 occurrences inthe British National Corpus.
Looking to the cov-erage  column  of  each  table,  we  thus  see  thatthere  are  words  in  the  Poesio  and  Almuharebdata set for which no triples can be acquired in 5cycles  of  bootstrapping.
Interestingly,  thougheach seed is quite different in origin and size (seeagain Table 1), all reach similar levels of cover-age (~82%) after  5 bootstrapping cycles.
Testnouns for which all three seeds fail to reach a de-scription  include  yesteryear,  nonce (very rare),salient (more typically an adjective), jag, droop,fluting,  fete,  throb,  poundage,  stinging,  rouble,rupee,  riel,  drachma,  escudo,  dinar,  dirham,lira, dispensation,  hoard,  airstream (not typical-ly a solid compound), riverside and curling.
Fig-ures 3 and 4 summarize the key findings in theabove tables: while bootstrapping from all threeseeds converges to the same level of coverage,the simile seed clearly produces the highest qual-ity taxonomy.Figure 3: Growth in the coverage from differentseed sources.Figure 4: Divergence in the clustering Purityachieved using different seed sources.
The results ofPoesio and Almuhareb are shown as the straight line:y = 0.627.Both  the  WordNet  and  ConceptNet  seedsachieve comparable accuracies of 68% and 67%respectively  after  5  cycles  of  bootstrapping,which compares well with the accuracy of 62.7%achieved  by  Poesio  and  Almuhareb.
However,the simile seed clearly yields the best accuracy of84.3%,  which  also  exceeds  the  accuracy  of66.4% achieved by Poesio and Almuhareb whenusing both values  and attributes (such as  Tem-perature, Color, etc.)
for clustering, or the accu-racy of 70.9% they achieve when using attributesalone.
Furthermore, bootstrapping from the simi-le seed yields higher cluster accuracy on the 402-noun data-set than Veale and Hao (2008) them-selves achieve with their simile data on the sametest-set (69.85%).But most striking of all is the concision of therepresentations that are acquired using bootstrap-ping.
The simile seed yields a high cluster accu-racy using a pool of just 2,614 fine discrimina-tors,  while  Poesio  and  Almuhareb  use  51,345features even after their feature-set has been fil-tered  for  noise.
Though starting  from differentinitial scales, each seed converges toward a fea-ture-set that is roughly twenty times smaller thanthat used by Poesio and Almuhareb.6 ConclusionsThese experiments reveal that seed knowledge ofdifferent authoritativeness, quality and size willtend to converge toward roughly the same num-ber  of  finely  discriminating  properties  and  to-ward much the same coverage after 5 or so cy-cles of bootstrapping.
Nonetheless, quality winsout,  and  the  simile-derived  seed  knowledgeshows itself to be a clearly superior basis for rea-soning  about  the  structure  and  organization  ofconceptual  categories.
Bootstrapping  from  thesimile  seed yields  a slightly smaller  set of  dis-criminating features than bootstrapping from theWordNet  seed,  one that  is  many times  smallerthan the Poesio and Almuhareb feature set.
Whatmatters is that they are the right features to dis-criminate with.There appears to be a number of reasons forthis  significant  difference  in  quality.
For  one,Veale and Hao (2007) show that similes expresshighly  stereotypical  beliefs  that  strongly  influ-ence the affective disposition of a term/concept;negatively  perceived  concepts  are  commonlyused to exemplify negative properties in similes,while  positively perceived  concepts  are  widelyused to exemplify positive properties.
Veale andHao (2008) go on to argue that similes offer avery concise snapshot of those widely-held be-liefs that are the cornerstone of everyday reason-1 2 3 4 50.400.450.500.550.600.650.700.750.800.850.90WordNetSimileConceptNetBootstrapping CycleCoverage1 2 3 4 50.400.500.600.700.800.901.00WordNetSimileConceptNetPoesio & Alm.Bootstrapping CyclePurity841ing, and which should thus be the corner-stone ofany general-purpose taxonomy.
In addition, be-liefs expressed via the ?as Dj as Ci?
form of simi-les  appear  to  lend  themselves  to  re-expressionvia the ?Dj Pk such as  Ci?
form; in each case, aconcept Ci is held up as an exemplar of a salientproperty  Dj.
Since  the  ?such  as?
bootstrappingpattern seeks out  expressions of  prototypicalityon the web, a simile-derived seed set is likely thebest starting point for this search.All three seeds appear to suffer the same cov-erage limitations,  topping out  at  about  82% ofthe words in the Poesio and Almuhareb data-set.Indeed,  after  5  bootstrapping  cycles,  all  threeseeds give rise to taxonomies that overlap on 328words from the 402-noun test-set, accounting for81.59% of the test-set.
In effect then, bootstrap-ping stumbles over the same core of hard wordsin each case, no matter the seed that is used.
Assuch, the problem of coverage lies not in the seedcollection, but in the queries used to perform thebootstrapping.
The  same  coverage  limitationswill thus apply to other bootstrapping approachesto  knowledge acquisition,  such as  Kozareva  etal.
(2008), which rely on much the same stockpatterns.
So  while  bootstrapping may not  be  ageneral  solution  for  acquiring  all  aspects  of  ageneral-purpose taxonomy, it is clearly useful inacquiring large swathes  of  such a  taxonomy ifgiven  a  sufficiently  high-quality  seed  to  startfrom.ReferencesAhlswede, T. and Evans, M. (1988).
Parsing vs. TextProcessing in the analysis of dictionary definitions.In Proc.
of the 26th Annual Meeting of the ACL, pp217-224.Almuhareb,  A.  and  Poesio,  M.  (2005).
ConceptLearning  and  Categorization  from  the  Web.
InProc.
of the annual meeting of the Cognitive  Sci-ence Society, Italy, July.Budanitsky,  A.  and  Hirst,  G. (2006).
EvaluatingWordNet-based Measures of Lexical Semantic Re-latedness.
Computational Linguistics, 32(1):13-47.Cimiano, P. and Wenderoth, J.
(2007).
Automatic Ac-quisition  of  Ranked  Qualia  Structures  from  theWeb.
In Proc.
of  the 45th Annual Meeting of  theACL, pp 888-895.Charniak, E. and Berland, M. (1999).
Finding parts invery  large  corpora.
In  Proc.
of  the  37th AnnualMeeting of the ACL, pp 57?64.Etzioni,  O.,  Kok,  S.,  Soderland,  S.,  Cafarella,  M.,Popescu, A-M., Weld, D., Downey, D., Shaked, T.and Yates,  A.
(2004).
Web-scale information ex-traction  in  KnowItAll  (preliminary  results).
InProc.
of the 13th WWW Conference, pp 100?109.Hammond, K. J.
(1986).
CHEF : A Model of Case--based Planning.
In Proc.
of the 5th National Con-ference  on  Artificial  Intelligence,  pp  267--271,Philadelphia, Pennsylvania.
American Associationfor Artificial Intelligence.Hanks, P. (2004).
WordNet: What is to be done?
InProc.
of GWC?2004, the 2nd Global WordNet con-ference, Masaryk University, Brno.Hearst,  M. (1992).
Automatic  acquisition  of  hy-ponyms  from large  text  corpora.
In  Proc.
of  the14th Int.
Conf.
on  Computational  Linguistics,  pp539?545.Kashyap,  V.  Ramakrishnan,  C.  and  Sheth,  T.
A.(2005).
TaxaMiner: an experimentation frameworkfor automated taxonomy bootstrapping.
Int.
Jour-nal of Web and Grid Services 1(2), pp 240-266.Karypis,  G. (2002).
CLUTO:  A  clustering  toolkit.Technical Report 02-017, University of Minnesota.http://www-users.cs.umn.edu/~karypis/cluto/.Kozareva, Z., Riloff, E. and Hovy, E. (2008).
Seman-tic  Class  Learning from the Web with HyponymPattern Linkage Graphs.
In Proc.
of the 46th Annu-al Meeting of the ACL.Lenat, D. B. and Guha, R. V. (1990).
Building largeknowledge-based  systems:  representation  and  in-ference in the Cyc project.
NY: Addison-Wesley.Liu, H. and Singh, P. (2004), ConceptNet: A PracticalCommonsense Reasoning Toolkit.
BT TechnologyJournal, 22(4):211-226.Miller, G., Beckwith,R., Fellbaum, C., Gross, D. andMiller,  K.J.
(1990).
Introduction  to  WordNet:  anon-line lexical database.
Int.
Journal of Lexicogra-phy, 3(4):235 ?
244.Niles, I. and Pease, A.
(2001).
Toward a standard up-per ontology.
In Proc.
of the 2nd International Con-ference  on Formal  Ontology  in  Information  Sys-tems (FOIS-2001).Snow, R., Jurafsky, D. and Ng, A. Y.
(2004).
Learn-ing syntactic patterns for automatic hypernym dis-covery.
Advances  in Neural Information Process-ing Systems 17.Veale,  T.  and Hao,  Y.
(2007).
Making Lexical  On-tologies Functional and Context-Sensitive.
In Proc.of the 45th Annual Meeting of the ACL, pp 57?64.Veale, T. and  Hao, Y.
(2008).
A Fluid KnowledgeRepresentation for  Understanding and GeneratingCreative Metaphors.
In Proc.
of Coling 2008, The22nd International  Conference  on  ComputationalLinguistics, Manchester.842
