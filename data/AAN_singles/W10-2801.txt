Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1?6,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsCapturing Nonlinear Structure in Word Spaces through DimensionalityReductionDavid JurgensUniversity of California, Los Angeles,4732 Boelter HallLos Angeles, CA 90095jurgens@cs.ucla.eduKeith StevensUniversity of California, Los Angeles,4732 Boelter HallLos Angeles, CA 90095kstevens@cs.ucla.eduAbstractDimensionality reduction has been shownto improve processing and information ex-traction from high dimensional data.
Wordspace algorithms typically employ lin-ear reduction techniques that assume thespace is Euclidean.
We investigate the ef-fects of extracting nonlinear structure inthe word space using Locality Preserv-ing Projections, a reduction algorithm thatperforms manifold learning.
We applythis reduction to two common word spacemodels and show improved performanceover the original models on benchmarks.1 IntroductionVector space models of semantics frequently em-ploy some form of dimensionality reduction forimprovement in representations or computationaloverhead.
Many of the dimensionality reduc-tion algorithms assume that the unreduced wordspace is linear.
However, word similarities havebeen shown to exhibit many non-metric proper-ties: asymmetry, e.g North Korea is more sim-ilar to Red China than Red China is to NorthKorea, and non-transitivity, e.g.
Cuba is similarthe former USSR, Jamaica is similar to Cuba,but Jamaica is not similar to the USSR (Tversky,1977).
We hypothesize that a non-linear wordspace model might more accurately preserve thesenon-metric relationships.To test our hypothesis, we capture the non-linear structure with dimensionality reduction byusing Locality Preserving Projection (LPP) (Heand Niyogi, 2003), an efficient, linear approxi-mation of Eigenmaps (Belkin and Niyogi, 2002).With this reduction, the word space vectors are as-sumed to exist on a nonlinear manifold that LPPlearns in order to project the vectors into a Eu-clidean space.
We measure the effects of us-ing LPP on two basic word space models: theVector Space Model and a Word Co-occurrencemodel.
We begin with a brief overview of theseword spaces and common dimensionality reduc-tion techniques.
We then formally introduce LPP.Following, we use two experiments to demonstrateLPP?s capacity to accurately dimensionally reduceword spaces.2 Word Spaces and ReductionsWe consider two common word space modelsthat have been used with dimensionality reduc-tion.
The first is the Vector Space Model (VSM)(Salton et al, 1975).
Words are represented asvectors where each dimension corresponds to adocument in the corpus and the dimension?s valueis the number of times the word occurred in thedocument.
We label the second model the WordCo-occurrence (WC) model: each dimension cor-respond to a unique word, with the dimension?svalue indicating the number of times that dimen-sion?s word co-occurred.Dimensionality reduction has been applied toboth models for three kinds of benefits: to im-prove computational efficiency, to capture higherorder relationships between words, and to reducenoise by smoothing or eliminating noisy features.We consider three of the most popular reductiontechniques and the general word space models towhich they have been applied: linear projections,feature elimination and random approximations.The most frequently applied linear projectiontechnique is the Singular Value Decomposition(SVD).
The SVD factors a matrix A, which rep-resents a word space, into three matrices U?V ?such that ?
is a diagonal matrix containing thesingular values of A, ordered descending based ontheir effect on the variance in the values of A. Theoriginal matrix can be approximated by using onlythe top k singular values, setting all others to 0.The approximation matrix, A?
= Uk?kV ?k , is theleast squares best-fit rank-k approximation of A.1The SVD has been used with great success onboth models.
Latent Semantic Analysis (LSA)(Landauer et al, 1998) extends the (VSM) by de-composing the space using the SVD and mak-ing the word space the left singular vectors, Uk.WC models have also utilized the SVD to improveperformance (Schu?tze, 1992; Bullinaria and Levy,2007; Baroni and Lenci, 2008).Feature elimination reduces the dimensional-ity by removing those with low information con-tent.
This approach has been successfully appliedto WC models such as HAL (Lund and Burgess,1996) by dropping those with low entropy.
Thistechnique effectively removes the feature dimen-sions of high frequency words, which provide lit-tle discriminatory content.Randomized projections have also been suc-cessfully applied to VSM models, e.g.
(Kanervaet al, 2000) and WC models, e.g.
(Sahlgren et al,2008).
This reduction statistically approximatesthe original space in a much lower dimensionalspace.
The projection does not take into accountthe structure of data, which provides only a com-putational benefit from fewer dimensions, unlikethe previous two reductions.3 Locality Preserving ProjectionFor a set of vectors, x1, x2, .
.
.
, xn ?
Rm, LPPpreserves the distance in the k-dimensional space,where k ?
m, by solving the following minimiza-tion problem,minw?ij(w?xi ?w?xj)2Sij (1)where w is a transformation vector that projects xinto the lower dimensional space, and S is a ma-trix that represents the local structure of the origi-nal space.
Minimizing this equation is equivalentto finding the transformation vector that best pre-serves the local distances in the original space ac-cording to S. LPP assumes that the data points xiexist on a manifold.
This is in contrast to the SVD,which assumes that the space is Euclidean and per-forms a global, rather than local, minimization.
Intreating the space as a manifold, LPP is able to dis-cover some of the nonlinear structure of the datafrom its local structure.To solve the minimization problem in Equation1, LPP uses a linear approximation of the Lapla-cian Eigenmaps procedure (Belkin and Niyogi,2002) as follows:1.
Let X be a matrix where xi is the ith row vec-tor.
Construct an adjacency matrix, S, whichrepresents the local structure of the originalvector space, by making an edge betweenpoints xi and xj if xj is locally proximate toxi.
Two variations are available for determin-ing proximity: either the k-nearest neighbors,or all the data points with similarity > ?.2.
Weight the edges in S proportional to thecloseness of the data points.
Four main op-tions are available: a Gaussian kernel, a poly-nomial kernel, cosine similarity, or binary.3.
Construct the diagonal matrix D where entryDii =?j Sij .
Let L = D ?
S. Then solvethe generalized eigenvector problem:XLX?w = ?XDX?w.
(2)He and Niyogi (2003) show that solving thisproblem is equivalent to solving Equation 1.4.
Let Wk = [w1, .
.
.
,wk] denote the matrix oftransformation vectors, sorted in descendingorder according to their eigenvalues ?.
Theoriginal space is projected into k dimensionsby W?k X ?
Xk.For many applications of LPP, such as doc-ument clustering (He et al, 2004), the originaldata matrix X is transformed by first perform-ing Principle Component Analysis and discardingthe smallest principle components, which requirescomputing the full SVD.
However, for large datasets such as those frequently used in word spacealgorithms, performing the full SVD is computa-tionally infeasible.To overcome this limitation, Cai et al (2007a)show how Spectral Regression may be used asan alternative for solving the same minimizationequation through an iterative process.
The princi-ple idea is that Equation 2 may be recast asSy = ?Dy (3)where y = X?w, which ensures y will be aneigenvector with the same eigenvalue for the prob-lem in Equation 2.
Finding the transformationmatrix Wk, used in step 4, is done in two steps.First, Equation 3 is solved to produce eigenvectors[y0, .
.
.
,yk], sorted in decreasing order accordingto their eigenvalues ?.
Second, the set of trans-formation vectors composing Wk, [w1, .
.
.
,wk],is found by a least-squares regression:wj = argminwn?i=1(w?xi ?
yji )2 + ?||w||2 (4)2where yji denotes the value of the jth dimensionof yi.
The ?
parameter penalizes solutions pro-portionally to their magnitude, which Cai et al(2007b) note ensures the stability of w as an ap-proximate eigenproblem solution.4 ExperimentsTwo experiments measures the effects of nonlin-ear dimensionality reduction for word spaces.
Forboth, we apply LPP to two basic word space mod-els, the VSM and WC.
In the first experiment,we measure the word spaces?
abilities to modelsemantic relations, as determined by priming ex-periments.
In the second experiment, we evaluatethe representation capabilities of the LPP-reducedmodels on standard word space benchmarks.4.1 SetupFor the VSM-based word space, we consider threedifferent weighting schemes: no weighting, TF-IDF and the log-entropy (LE) used in (Landaueret al, 1998).
For the WC-based word space, weuse a 5 word sliding window.
Due to the large pa-rameter space for LPP models, we performed onlya limited configuration search.
An initial analysisusing the 20 nearest neighbors and cosine simi-larity did not show significant performance differ-ences when the number of dimensions was variedbetween 50 and 1000.
We therefore selected 300dimensions for all tests.
Further work is needed toidentify the impact of different parameters.
Stopwords were removed only for the WC+LPP model.We compare the LPP-based spaces to three mod-els: VSM, HAL, and LSA.Two corpora are used to train the models in bothexperiments.
The first corpus, TASA, is a collec-tion of 44,486 essays that are representative of thereading a student might see upon entering college,introduced by (Landauer et al, 1998).
The cor-pus consists of 98,420 unique words; no filteringis done when processing this corpus.
The secondcorpus, WIKI, is a 387,082 article subset of a De-cember 2009 Wikipedia snapshot consisting of allthe articles with more than 1,000 tokens.
The cor-pus is filtered to retain the top 100,000 most fre-quent tokens in addition to all the tokens used ineach experiment?s data set.4.2 Experiment 1Semantic priming measures word associationbased on human responses to a provided cue.Priming studies have been used to evaluate wordspaces by equating vector similarity with an in-creased priming response.
We use data from twotypes of priming experiments to measure whetherLPP models better correlate with human perfor-mance than non-LPP word spaces.Normed Priming Nelson et al (1998) collectedfree association responses to 5,019 prime words.An average of 149 participants responded to eachprime with the first word that came to mind.Based on this dataset, we introduce a newbenchmark that correlates word space similaritywith the associative strength of semantic primingpairs.
We use three measures for modeling prime-target strength, which were inspired by Steyverset al (2004).
Let Wab be the percentage of partici-pants who responded to prime a with target b. Thethree measures of associative strength areS1ab = WabS2ab = Wab +WbaS3ab = S2ab +?c S2acS2cbThese measure three different levels of semanticrelatedness between words a and b. S1ab measuresthe relationship from a to b, which is frequentlyasymmetric due to ordering, e.g.
?orange?
pro-duces ?juice?
more frequently than ?juice?
pro-duces ?orange.?
S2ab measures the symmetric asso-ciation between a and b; Steyvers et al (2004) notethat this may better model the associative strengthby including weaker associates that may have beena suitable second response.
S3ab further increasesthe association by including the indirect associa-tions between a and b from all cued primes.For each measure, we rank a prime?s targetsaccording to their strength and then compute theSpearman rank correlation with the prime-targetsimilarities in the word space.
The rank compari-son measures how well word space similarity cor-responds to the priming association.
We report theaverage rank correlation of associational strengthsover all primes.Priming Effect The priming study by Hodgson(1991), which evaluated how different semanticrelationships affected the strength of priming, pro-vides the data for our second priming test.
Six re-lationships were examined in the study: antonymy,synonymy, conceptual association (sleep and bed),categorical coordinates (mist and rain), phrasal as-sociates (pony and express), and super- and sub-ordinates.
Each relationship contained an average3Antonymy Conceptual CoordinatesAlgorithm Rb U E R U E R U EVSM+LPP+LE 0.103 0.018 0.085 0.197 0.050 0.147 0.071 0.027 0.044VSM+LPP+TF-IDF 0.348 0.321 0.027 0.408 0.414 -0.005 0.323 0.294 0.029VSM+LPP 0.247 0.122 0.124 0.312 0.120 0.193 0.230 0.111 0.119VSM+LPPa 0.298 0.070 0.228 0.284 0.033 0.252 0.321 0.037 0.284WC+LPP 0.255 0.071 0.185 0.413 0.110 0.303 0.431 0.134 0.298HAL 0.813 0.716 0.096 0.845 0.814 0.031 0.861 0.809 0.052HALa 0.915 0.879 0.037 0.867 0.846 0.021 0.913 0.861 0.052LSA 0.235 0.023 0.213 0.392 0.028 0.364 0.199 0.014 0.185LSAa 0.287 0.061 0.226 0.362 0.041 0.321 0.316 0.037 0.278VSM 0.051 0.011 0.040 0.111 0.012 0.099 0.032 0.008 0.024Phrasal Ordinates SynonymyAlgorithm R U E R U E R U EVSM+LPP+LE 0.147 0.039 0.108 0.225 0.032 0.193 0.081 0.027 0.053VSM+LPP+TF-IDF 0.438 0.425 0.013 0.277 0.290 -0.013 0.344 0.328 0.017VSM+LPP 0.234 0.107 0.127 0.273 0.115 0.158 0.237 0.157 0.080VSM+LPPa 0.202 0.031 0.171 0.270 0.032 0.238 0.299 0.069 0.230WC+LPP 0.274 0.087 0.186 0.324 0.076 0.248 0.345 0.111 0.233HAL 0.805 0.776 0.029 0.825 0.789 0.036 0.757 0.681 0.076HALa 0.866 0.856 0.010 0.881 0.857 0.024 0.898 0.879 0.019LSA 0.280 0.021 0.258 0.258 0.018 0.240 0.197 0.019 0.178LSAa 0.269 0.030 0.238 0.326 0.032 0.294 0.327 0.052 0.275VSM 0.104 0.013 0.091 0.061 0.008 0.053 0.052 0.009 0.043a Processed using the WIKI corpusb R are related primes, U are unrelated primes, E is the priming effectTable 1: Experiment 1 priming results for the six relation categories from Hodgson (1991)Word Choice Word AssociationAlgorithm Corpus TOEFL ESL RDWP F. et al R.&G.
DeeseVSM+LPP+le TASA 24.000 50.000 45.313 0.296 0.092 0.034VSM+LPP+tf-idf TASA 22.667 25.000 37.209 0.023 0.086 0.001VSM+LPP TASA 41.333 54.167 39.063 0.219 0.136 0.045VSM+LPP Wiki 33.898 48.780 43.434 0.530 0.503 0.108WC+LPP TASA 46.032 40.000 45.783 0.423 0.414 0.126HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042LSA TASA 56.000 50.000 55.814 0.516 0.651 0.349LSA Wiki 60.759 54.167 59.200 0.614 0.681 0.206VSM TASA 61.333 52.083 84.884 0.396 0.496 0.200Table 2: Results from Experiment 2 on six word space benchmarksof 23 word pairs.
Hodgson?s results showed thatpriming effects were exhibited by the prime-targetpairs in all six categories.We use the same methodology as Pado?
and La-pata (2007) for this data set; the prime-target (Re-lated Primes) cosine similarity is compared withthe average cosine similarity between the primeand all other targets (Unrelated Primes) within thesemantic category.
The priming effect is the dif-ference between the two similarity values.4.3 Experiment 2We use six standard word space benchmarks totest our hypothesis that LPP can accurately capturegeneral semantic knowledge and association basedrelations.
The benchmarks come in two forms:word association and word choice tests.Word choice tests provide a target word and alist of options, one of which has the desired rela-tion to the target.
To answer these questions, weselect the option with the highest cosine similar-ity with the target.
Three word choice synonymybenchmarks are used: the Test of English as a For-eign Language (TOEFL) test set from (Landaueret al, 1998), the English as a Second Language(ESL) test set from (Turney, 2001), and the Cana-dian Reader?s Digest Word Power (RDWP) from(Jarmasz and Szpakowicz, 2003).4Algorithm Corpus S1 S2 S3VSM+LPP+LE TASA 0.457 0.413 0.255VSM+LPP+TF-IDF TASA 0.464 0.390 0.207VSM+LPP TASA 0.457 0.427 0.275VSM+LPP Wiki 0.472 0.440 0.333WC+LPP TASA 0.469 0.437 0.315HAL TASA 0.485 0.434 0.310HAL Wiki 0.462 0.406 0.266LSA TASA 0.494 0.481 0.414LSA Wiki 0.489 0.472 0.398VSM TASA 0.484 0.460 0.407Table 3: Experiment 1 results for normed priming.Word association tests measure the semantic re-latedness of two words by comparing their simi-larity in the word space with human judgements.These tests are more precise than word choice testsbecause they take into account the specific valueof the word similarity.
Three word associationbenchmarks are used: the word similarity data setof Rubenstein and Goodenough (1965), the word-relatedness data set of Finkelstein et al (2002),and the antonymy data set of Deese (1964), whichmeasures the degree to which high similarity cap-tures the antonymy relationship.
The Finkelsteinet al test is notable in that the human judges werefree to score based on any word relationship.5 Results and DiscussionThe LPP-based models show mixed performancein comparison to existing models on normed prim-ing tasks, shown in Table 3.
Adding LPP tothe VSM decreased performance; however, whenWIKI was used instead of TASA, the VSM+LPPmodel increased .15 on all correlations, whereasLSA?s performance decreased.
This suggests thatLPP needs more data than LSA to properly modelthe word space manifold.
WC+LPP performscomparably to HAL, which indicates that LPPis effective in retaining the original WC space?sstructure in significantly fewer dimensions.For the categorical priming tests shown in Ta-ble 1, LPP-based models show competitive results.VSM+LPP with the WIKI corpus performs muchbetter than other VSM+LPP configurations.
Un-like in the previous priming experiment, addingLPP to the base models resulted in a significantperformance improvement.
We also note that bothHAL models and the VSM+LPP+TF-IDF modelhave high similarity ratings for unrelated primes.We posit that these models?
feature weighting re-sults in poor differentiation between words in thesame semantic category, which causes their de-creased performance.For experiment 2, LPP-based spaces showedmixed results on word choice benchmarks, whileshowing notable improvement on the more pre-cise word association benchmarks.
Table 2 liststhe results.
Notably, LPP-based spaces performedwell on the ESL synonym benchmark but poorlyon the TOEFL synonym benchmark, even whenthe larger WIKI corpus was used.
This suggeststhat LPP was not effective in retaining the re-lationship between certain classes of synonyms.Given that performance did not improve with theWIKI corpus, further analysis is needed to iden-tify whether a different representation of the localstructure would improve results or if the poor per-formance is due to another factor.
While LSA andVSM model performed best on all benchmarks,LPP-based spaces performed competitively on theword association tests.
In all but two tests, theWC+LPP model outperformed HAL.The results from both experiments indicate thatLPP is capable of accurately representing distri-butional information in a much lower dimensionalspace.
However, in many cases, applications usingthe SVD-reduced representations performed bet-ter.
In addition, application of standard weight-ing schemes worsened LPP-models?
performance,which suggests that the local neighborhood is ad-versely distorted.
Nevertheless, we view these re-sults as a promising starting point for further eval-uation of nonlinear dimensionality reduction.6 Conclusions and Future WorkWe have shown that LPP is an effective dimen-sionality reduction technique for word space algo-rithms.
In several benchmarks, LPP provided asignificant benefit to the base models and in a fewcases outperformed the SVD.
However, it does notperform consistently better than existing models.Future work will focus on four themes: identifyingoptimal LPP parameter configurations; improvingLPP with weighting; measuring LPP?s capacity tocapture higher order co-occurrence relationships,as was shown for the SVD (Lemaire et al, 2006);and investigating whether more computationallyexpensive nonlinear reduction algorithms such asISOMAP (Tenenbaum et al, 2000) are better forword space algorithms.
We plan to release imple-mentations of the LPP-based models as a part ofthe S-Space Package (Jurgens and Stevens, 2010).5ReferencesMarco Baroni and Alessandro Lenci.
2008.
Con-cepts and properties in word spaces.
From context tomeaning: Distributional models of the lexicon in lin-guistics and cognitive science (Special issue of theItalian Journal of Linguistics), 1(20):55?88.Mikhail Belkin and Partha Niyogi.
2002.
LaplacianEigenmaps and Spectral Techniques for Embeddingand Clustering.
In Advances in Neural InformationProcessing Systems, number 14.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: a computational study.
Behav-ior Research Methods, 39:510?526.Deng Cai, Xiaofei He, and Jiawei Han.
2007a.
Spec-tral regression for efficient regularized subspacelearning.
In IEEE International Conference onComputer Vision (ICCV?07).Deng Cai, Xiaofei He, Wei Vivian Zhang, , and JiaweiHan.
2007b.
Regularized Locality Preserving In-dexing via Spectral Regression.
In Proceedings ofthe 2007 ACM International Conference on Infor-mation and Knowledge Management (CIKM?07).James Deese.
1964.
The associative structure ofsome common english adjectives.
Journal of VerbalLearning and Verbal Behavior, 3(5):347?357.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Woflman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions of InformationSystems, 20(1):116?131.Xiaofei He and Partha Niyogi.
2003.
Locality preserv-ing projections.
In Advances in Neural InformationProcessing Systems 16 (NIPS).Xiaofei He, Deng Cai, Haifeng Liu, and Wei-YingMa.
2004.
Locality preserving indexing for doc-ument representation.
In SIGIR ?04: Proceedingsof the 27th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 96?103.James M. Hodgson.
1991.
Informational constraintson pre-lexical priming.
Language and CognitiveProcesses, 6:169?205.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sthesaurus and semantic similarity.
In Conference onRecent Advances in Natural Language Processing,pages 212?219.David Jurgens and Keith Stevens.
2010.
The S-SpacePackage: An Open Source Package for Word SpaceModels.
In Proceedings of the ACL 2010 SystemDemonstrations.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for latentsemantic analysis.
In L. R. Gleitman and A. K. Josh,editors, Proceedings of the 22nd Annual Conferenceof the Cognitive Science Society, page 1036.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
Introduction to Latent Semantic Analy-sis.
Discourse Processes, (25):259?284.Beno?
?t Lemaire, , and Guy Henhie?re.
2006.
Effectsof High-Order Co-occurrences on Word SemanticSimilarities.
Current Psychology Letters, 1(18).Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occrrence.
Behavoir Research Methods, Instru-ments & Computers, 28(2):203?208.Douglas L. Nelson, Cathy L. McEvoy, andThomas A. Schreiber.
1998.
The Uni-versity of South Florida word association,rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-Based Construction of Seman-tic Space Models.
Computational Linguistics,33(2):161?199.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8:627?633.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a means to encode or-der in word space.
In Proceedings of the 30thAnnual Meeting of the Cognitive Science Society(CogSci?08).Gerard Salton, A. Wong, and C. S. Yang.
1975.
Avector space model for automatic indexing.
Com-munications of the ACM, 18(11):613?620.Hinrich Schu?tze.
1992.
Dimensions of meaning.In Proceedings of Supercomputing ?92, pages 787?796.Mark Steyvers, Richard M. Shiffrin, and Douglas L.Nelson, 2004.
Word association spaces for predict-ing semantic similarity effects in episodic memory.American Psychological Assocation.Joshua B. Tenenbaum, Vin de Silva, and John C.Langford.
2000.
A global geometric frameworkfor nonlinear dimensionality reduction.
Science,290(5500):2319?2323.Peter D. Turney.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedingsof the Twelfth European Conference on MachineLearning (ECML-2001), pages 491?502.Amos Tversky.
1977.
Features of similarity.
Psycho-logical Review, 84:327?352.6
