DOCUMENT DETECTIONSUMMARY OF RESULTSDonna HarmanNational Institute of  Standards and TechnologyGaithersburg, MD.
208991.
INTRODUCTIONThis section presents a summary of the TIPSTER results,including some comparative system performance andsome conclusions about the success of the detection halfof the TIPSTER phase I project.
For more details on theindividual experiments, please see the system overviews.Four contractors were involved in the document detectionhalf of TIPSTER.
Two of the contractors worked in En-glish only (Syracuse University and HNC Inc.), one con-tractor worked in Japanese only (TRW Systems Develop-ment Division), and one contractor worked in both lan-guages (University of Massachusetts at Amherst).
Thefour contractors had extremely varied approaches to thedetection task.
TRW transformed an operational Englishretrieval system (based on pattern matching using a fasthardware approach), into a Japanese version of the sameoperation, with a special interface designed to facilitatework in Japanese.
The University of Massachusetts ap-proach involved taking a relatively small experimentalsystem using a probabilistic inference net methodology,scaling it up to handle the very large amounts of text andlong topics in TIPSTER, and modifying the algorithms tohandle Japanese.
Both Syracuse University and HNC Inc.built completely new systems to handle the English col-lection.
In the case of Syracuse University, their system isbased heavily on a natural anguage approach to retrieval,with many of the techniques traditionally used in docu-ment understanding applied to the retrieval task.
HNCInc.
took a totally different approach, applying statisticaltechniques based on robust mathematical models (includ-ing the use of neural networks).There were three evaluations of the contractors' work; oneat 12 months, one at 18 months, and the final one at 24months.
In each case, the contractors working in Englishhave made multiple experimental runs using the test col-lection, and turned in the top list of documents found.These results were first used to create the sample pool forassessment, and then were scored against he correct an-swers based on results from all runs (including TREC-Iruns for the 18-month evaluation and TREC-2 runs for the24-month evaluation).
Standard tables using re-call/precision and recall/fallout measures were distributedand compared.
The evaluation of the Japanese work tookplace only at the 24-month period.2.
12 -MONTH EVALUATIONThe work done for the 12-month evaluation was mainly ascaling effort.
Not all data was available so only partialresults were completed.
In particular, the University ofMassachusetts urned in 7 runs using the adhoc topics,with experiments rying different parts of the topic to au-tomatically create the query, and also adding phrases.Additionally they tried some manually edited queries.HNC Inc. turned in 4 runs using the adhoc topics, withexperiments also using different parts of the topic to auto-matically generate queries.
Additionally they tried vari-ous types of "bootstrapping" methodologies to generatecontext vectors.
Syracuse University turned in no runs,but had completed the extensive design work as proposedin their timeline.
The University of Massachusetts alsodid 4 runs on the routing topics, but the lack of good train-ing data made this very difficult.
In general the results forthe systems was good, with the University of Mas-sachusetts outperforming HNC Inc. on the adhoc runs, butit was felt by all that this evaluation represented a very"baseline" effort.
For these reasons, no graphs of these re-suits will be presented.3.
18 -MONTH EVALUATIONBy the 18-month mark, the systems had finished muchmore extensive sets of experiments.
The University ofMassachusetts continued to investigate the effects of usingdifferent parts of the topic for the adhoc runs, but this timetrying different combinations using the inference netmethodology.
Figure 1 shows three INQRY runs for theadhoc topics done for the 18-month evaluation.
The plotfor INQRYV represents results from queries created auto-matically using most of the fields of the topics.
The IN-QRYJ results are from the same queries, but includingphrases and concept operators.
The INQRYQ results33show the effects of manually editing the INQRYJ queries,with those modifications restricted to eliminating wordsand phrases, adding additional words and phrases fromthe narrative field, and inserting paragraph-level operatorsaround words and phrases.
As can be seen, the use ofphrases in addition to single terms helped somewhat, butthe results from manual modification of the queries werethe best adhoc runs.Figure 1 also shows the two HNC adhoc results for the18-month evaluation period.
These runs represent finalresults from many sets of bootstrapping runs in whichHNC evolved techniques for completely automaticallycreating context vectors for the documents.
The plotmarked HNC2aV represents he results using these con-text vectors for retrieval and using automatically builtqueries from the concepts field of the topic.
TheHNC2aM results adds a rough Boolean filter to the pro-cess, requiring that three terms in the concepts field matchterms in the documents before the context vectors areused for ranking.
This Boolean filter provided consider-able improvements.Figure 2 shows the routing results for the 18-month evalu-ation.
The three plots for the INQRY system represent abaseline method (INQRYA, same as INQRYJ adhoc ap-proach), and two modifications to this method.
The IN-QRYP results how the effect of manually modifying thequery, similar to the method used in producing the IN-QRYQ adhoc results.
The plot for INQRYR shows thefirst results from probabilistic query expansion andreweighting performed automatically using relevancefeedback techniques on the training data.
Both methodsimprove results, with the automatic feedback method re-suits approaching the manual-modification method, espe-cially at the high recall area of the curve.The HNC routing results hown on figure 2 represent theuse of two different ypes of neural networks.
The plotmarked HNCrtl is the baseline result, created by using theadhoc methods imilar to those used in run HNC2aV.
TheHNCrt2 results represent using neural network techniquesto learn improved stem weights for the context vectorsbased on the training data.
The HNCrt3 results comefrom using the training data to determine what type ofrouting query to use, i.e.
an automated adhoc query (simi-lar to HNC2aM), a manual query, or a query using theneural network techniques (HNCrt2).
Clearly the neuralnetwork learning techniques significantly improve perfor-mance, with the per topic "customization" performance(HNCrt3) working best.In terms of system comparison, the University of Mas-sachusetts runs were consistently better than the HNCruns for the adhoc topics, whereas for the routing topicsboth groups were similar.
The results were a major im-provement over their baseline (12-month) results for bothgroups.At the 18-month evaluation period, Syracuse Universityhad the first stage of their system in operation and turnedin results for the first time.
The results for adhoc androuting are shown in figures 3 and 4.
Since the results arefor only a subset of the data used by the other contractors,they cannot be directly compared.
Additionally since theresults are only for the ffi/st stage of retrieval, which em-phasizes high recall, they should not be viewed as the fi-nal results from the system.Figure 3 shows four Syracuse runs on the adhoc topics.The documents used are the subset of the collection hav-ing the Wall Street Journal only.
The first three plots,DRsfcl, DRpnal, and DRtsal, represent three operationsin the DR-LINK system.
The first operation does a roughfiltering operation on the data, only retrieving documentswith suitable subject field codes.
The next two operationslocate proper nouns and look at document structure.There is a considerable improvement in performance be-tween the first two operations.
The fourth run (DRfull)used a manual version of the second stage to produce finalresults.
These results are for only half the topics, so can-not be strictly compared to the first three runs, but they doindicate the potential improvements toprecision that canbe expected from the second stage.Figure 4 shows the same operations, and generally thesame improvements, for the routing topics.
In this casethe subset of documents used was the AP newswire docu-ments.
The same three operations discussed above areplotted here.
There was no second stage trial for the rout-ing topics.
These two graphs represent the baseline of theSyracuse system.4.
24 -MONTH EVALUATIONFor the 24-month evaluation, all groups turned in manyruns.
The runs were much more elaborate, with many dif-ferent ypes of parameters being tried.
The University ofMassachusetts tried 7 experiments with the adhoc topics,using complex methods of combining the topic fields,proximity restrictions, noun phrases, and paragraph opera-tors.
Additionally an automatically-built thesaurus wastried.
They also did 15 runs with the routing topics, try-ing various experiments combining relevance feedback,query combinations, proximity operators and specialphrase additions.
HNC Inc. did 4 adhoc runs using vari-ous types of learned context vectors.
Additionally theytried a simulated feedback query construction run.
Forrouting they did 5 runs, trying multiple experiments indifferent combinations of adhoc and neural net approach-es.
Syracuse University turned in 10 runs for their341.000.800 .60~0.400 .200 .00  v v0.00  0 .20 0.40 0 .60 0 .80Reca l l+ INQRYJ  + INQRYV ~ INQRYQHNC2aM --o-- HNC2aV1.001.000 .80T IPSTER 18 Month  Rout ing0.60oo ~~" 0 .400 .200 .000 .00  0 .20 0.40 0 .60 0 .80  1.00Reca l lINQRYA_~_  INQRYP ~ INQRYR+ HNCrt  1 _~_ HNCrt2  + HNCrt3T IPSTER 18 Month  AdhocFigures 1 and 2: Adhoc and routing performance atthe 18-month evaluation period (using full collection)351 .ooT IPSTER 18 Month  Syracuse  Adhoc0.400.800.600.200.00DRsfa l- -  m i i0.00  0 .20  0 .40  0 .60  0 .80  1 .00Reca l lDRpna l  ~ DRtsa l  = DRfu l l1.00T IPSTER 18 Month  Syracuse  Rout ing"30.400.800.600.200.00 m ~ n u n0.00  0 .20  0 .40  0 .60  0 .80  1 .00Reca l lDRs fc  1 + DR+pnl  ~ DR+ts  1Figures 3 and 4:ly)Adhoc and routing performance at the 18-month evaluation period (using WJS or AP on-36"upstream processing module" (3 adhoc and 7 routing),trying various types of ranking formulas.
Additionallythey did 13 runs using the full retrieval system (4 adhocand 9 routing).
Full descriptions of these runs are givenin the system overviews.Figures 5 through 12 show the results from the 24-monthevaluation.
Figures 5 and 6 show some of the adhoc re-suits for the full collection, and figures 7 and 8 show someof the routing results.
The results from Syracuse Universi-ty on a smaller subset of the document collection areshown in figures 9 through 12.Figure 5 shows the recall/precision curves for the adhoctopics.
The three INQRY runs include their baselinemethod (INQ009), which is same as the baseline methodINQRYJ developed at the 18-month evaluation period.The first modification (INQ012) uses the inference net to"combine" weights from the documents and weights fromthe best-matching paragraphs in the document.
The sec-ond modification (INQ015) shows the new term expan-sion method using an automatically-built thesaurus.
Bothmodifications show some improvements over the baselinemethod.The three HNC runs shown on figure 5 include a baseline(HNCadl) that is similar to their best 18-month adhoc ap-proach (HNC2aM), but that uses a required match of 4terms rather than 3.
The HNCad3 results how the effectsof using a larger context vector of 512 terms rather thanonly 280 terms for the baseline results.
This causes aslight improvement.
The HNCad2 results are using somemanual relevance feedback.The University of Massachusetts results are better than theHNC results, but there were improvements in both sys-tems over the 18-month evaluation.
Figure 6 shows therecall/fallout curves for the best runs of these two sys-tems.
Both plots show the same differences in perfor-mance, but it can be seen on the recall/fallout curve thatboth systems are retrieving at a very high accuracy.
At arecall of about 60 percent (i.e.
about 60 percent of the rel-evant documents have been retrieved) the precision of theINQRY results is about 30 percent.
The fallout, however,is about 0.0004, meaning that most non-relevant docu-ments are being properly screened out.
This correspondsto a probability of false alarm rate of 0.0004 at this point,in ROC terminology.Figure 7 shows the routing results for both groups.
Therun marked INQ026 is the baseline run of the INQRY sys-tem and uses the same methodology asthe adhoc INQ009run.
The other two runs add some type of relevance feed-back using the training documents.
The plot markedINQ023 uses both relevance f edback and proximity oper-ators to add 30 terms and 30 pairs of terms from the rele-vant documents to the query.
The most complex~ run,INQ030, constructed the queries imilarly to run INQ023,but additionally weighted the documents using a combina-tion method similar to adhoc run INQ012.
These runsrepresented the best results from many different experi-ments, and the relevance feedback gives significant im-provement over the baseline runs.The HNC routing results also represent the best of manyexperiments.
The results for HNCrt5 show the neural net-work learning using stem weighting, similar to HNCrt2 atthe 18-month evaluation.
The second two sets of resultsrepresent data fusion techiques, with HNCrtl being fusionof four types of retrievals, using the same combinationsfor all topics, and HNCrt2 using different combinationsfor different opics.
The data fusion combinations bothwork well, but the per topic combination works the best,just as the less sophisticated version of this run workedbest at the I8-month evaluation.Again the University of Massachusetts results were betterthan the HNC results, but with major improvements inboth systems over the 18-month evaluation.
Figure 8shows the recall/fallout curves for the best runs of bothgroups.The Syracuse runs were on a subset of the full collectionso are not directly comparable.
However they alsoshowed significant improvements over their 18-monthbaseline.
Figure 9 shows three first-stage Syracuse runs,the results of trying different complex methods of com-bining the information (subject field code, text structure,and other information) that is detected in the first-stagemodules.
The results of this combination are passed tothe second stage (figure 10).
Note that due to processingerrors there were documents lost between stages, andthese official results are therefore inaccurate.
Additional-ly only 19 topics (out of 50) are shown in figure I0.
Theimprovements that could have been expected o not showbecause of these problems.Figures 11 and 12 show the Syracuse routing runs.
Thefirst stage runs show not only the combinations from theadhoc, but also additional ways of integrating the data.Again there were processing errors with the second stageresults, and therefore no improvement is shown using thesecond stage.37T IPSTER 24 Month  Adhoc1 .
0 00.800.60~0.400:200.00 - -0.00 0.20 0.40 0.60 0.80Reca l lINQ009 _~_ INQ012 ~ INQ015+ HNCad l  _~_ HNCad2 _~_ HNCad31.00Best  Adhoc  Fa l lout -Reca l l  Curves1.000.800.600.400.20I I ~ 1 ; !
I 0.000.0000 0.0400 0.0800 0.1200 0.1600 0.2000Fa l lout  x 100INQ012 _~_ HNCad3Figures 5 and 6: Adhoc performance at the 24-month evaluation period (using full collection)380.80T IPSTER 24 Month  Rout ing0.60~" 0 .400 .200 .000 .00  0 .20  0 .40  0 .60  0 .80  1 .00Recal l--m- INQ026 + INQ023 -A- INQ030HNCrt5  e. HNCrt l -a -HNCrt21.00Best  Rout ing Fa l lout -Reca l l  Curves0.800 .600 .400 .201 .000 .00  I I z I I I0.0000 0 .0400 0 .0800 0 .1200 0 .1600 0 .2000Fa l lout  x 100+ INQ024 + HNCrt2Figures 7 and 8: Routing performance atthe 24-month evaluation period (using full collection)39T IPSTER 2,4 Month  Syracuse  Adhoc  (F i r s t  S tage)0.00 0.20 0.40 0.60 0.80 1.00Reca l l__,,_ DRwuml ._o_  DRwur l  _._ DRwus lT IPSTER 24  Month  Syracuse  Adhoc  (Second Stage)1.00?
~0.801.000.800.60"~ 0.400.200.000.600.400.200.00 , , - = =0.00 0.20 0.40 0.60 0.80 1.00Reca l l__,,__ DR lbw2 + DR2bw2 __  DR3bw2 + DR4bw2Figures 9 and 10: Adhoc performance atthe 24-month evaluation period (using WJS only)400.800 .800 .60.
~0.400 .200 .000 .00  0.20 0.40 0 .60 0.80 1.00Reca l lDRsar l  + DRsas l  ~ DRsdr l-w-DRsds l  o DRsur l  +DRsus lT IPSTER 24  Month  Syracuse  Rout ing  (2nd  Stage)1.000.600.
~~0.400 .200 .00  ?
m0.00 0.20 0.40 0.60 0.80 1.00Reca l l_~_ DR lba2  + DRl r i2  A DR3ba2+ DR3r i2  + DR4r i2  ~ DRcom2T IPSTER 24  Month  Syracuse  Rout ing  (F i r s t  S tage)1.00Figures 11 and 12: Routing performance atthe 24-month evaluation period (using SJMN only)415.
COMPARISON WITH TREC RESULTSHow do the TIPSTER results compare with the TREC-2results?
Two of the TIPSTER contractors submitted re-suits for TREC-2 and these can be seen in Figures 13 and14.
These figures show the best TREC-2 adhoc and rout-ing results for the full collection.
More information aboutthe various TREC-2 runs can be found in the TREC-2proceedings \[1\].
The results marked "INQ001" are theTIPSTER INQUERY system, using methods imilar totheir baseline TIPSTER INQ009 run.
The "dortQ2","Brkly3" and "crnlL2" are all based on the use of the Cot-nell SMART system, but with important variations.
The"cmlL2" run is the basic SMART system, but using lessthan optimal term weightings (by mistake).
The "dortQ2"results come from using the training data to find parame-ter weights for various query factors, whereas the "Brk-ly3" results come from performing statistical regressionanalysis to learn term weighting.
The "CLARTA" systemadds noun phrases found in an automatically-constructedthesaurus to improve the query terms taken from the top-ic.
The plot marked "HNCadl" is the baseline adhoc runfor the TIPSTER 24-month evaluation.
The TIPSTERINQUERY system is one of the best performing systemsfor the TREC-2 adhoc topics.The routing results from TREC-2 (shown in figure 14) ex-hibit more differences between the systems.
Again threesystems are based on the Cornell SMART system.
Theplot marked "crnlCl" is the actual SMART system, usingthe basic Rocchio relevance feedback algorithms, andadding many terms (up to 500) from the relevant trainingdocuments o the terms in the topic.
The "dortPl" resultscome from using a probabilistically-based r levance feed-back instead of the vector-space algorithm.
These twosystems have the best routing results.
The "Brkly5" sys-tem uses statistical regression on the relevant training doc-uments to learn new term weights.
The "cityr2" resultsare based on a traditional probabilistic reweighting fromthe relevant documents, adding only a small number ofnew terms (10-25) to the topic.
The "INQ003" results ai-m use probabilistic reweighting and add 30 new terms tothe topics.
The "hnc2c" results are similar to the HNCrtlfusion results for the 24-month TIPSTER evaluation.These plots mask important information as they are aver-ages over the 50 adhoc or routing topics.
Whereas oftenthe averages show little difference between systems, thesesystems are performing quite differently when viewed ona topic by topic basis.
Table 1 shows the "top 8" TREC-2systems for each adhoc topic.
The various ystem tags il-lustrate that a wide variety of systems do well on thesetopics, and that often a system that does not do well onaverage may perform best for a given topic.
This is an in-herent performance characteristic of information retrievalsystems, and emphasizes the importance of getting be-yond the averages in doing evaluation.
Clearly systemsthat perform well on average reflect better overall method-ologies, but often much can be learned by analyzing whya given system performs well or poorly on a given topic.This is where more work is needed with respect to analyz-ing the TIPSTER and TREC results.Tables 2 and 3 show some prefiminary analysis of two ofthe topics with respect to the TIPSTER contractors.
Table2 gives the ranks of the relevant documents retrieved ei-ther by the HNCrtl run or the INQ023 run.
Clearly theHNC run is better for this topic, providing much higherranks for most of the relevant documents.
Note that fiveof the relevant documents were not retrieved by eithersystem.Table 3 shows a sfighfly different view of the same phe-nomena, but for topic 121.
There were a total of 55 rele-vant documents for this topic, with only 13 of them foundby the TIPSTER systems.
Table 3 fists those 13 docu-ments, the rank at which they were regieved, and the"tag" of the system retrieving them.
Note that for this top-ic the INQUERY system is performing better than theHNC system.
These tables illustrate the varying perfor-mance of different methods across the topics.
A majorchallenge facing each group is to determine which strate-gies are successful for most topics, and which strategiesare successful only for some topics (including how toidentify in advance this topic subse0.6.
JAPANESE RESULTSBy the 24-month evaluation, only 7 topics were ready fortesting.
Both TRW and the University of Massachusettsran these topics successfully, and the results are discussedin their system evaluations.
No comparison of the resultsis possible between the two systems because of the pre-liminary nature of having only 7 topics.
However, theUniversity of Massachusetts (who did both English andJapanese) reported that minimal effort was necessary forporting their English techniques to Japanese, especiallygiven the availability of the JUMAN Japanese word seg-mentor.
Additionally the new TRW Japanese interfacewas judged a major success by the beta site tests.7.
CONCLUSIONSWhat are some of the conclusions that can be drawn fromthe many experiments performed in the TIPSTER andTREC evaluations, and equally important, what is the last-ing value of this two-year project?First, the statistical techniques (using non-Boolean meth-ods without any formal query language) that were used onthe smaller test collections DO scale up.
The simplest ex-ample of this is the consistently high performance of theComell SMART system in TREC.
This very basic system42relies on the vector-space model and on carefully craftedterm weighting to produce their high results.
A morecomplex example of the successful se of statistical tech-niques is the University of Massachusetts INQUERY sys-tem, which uses the more sophisticated inference networkapproach to achieve their high performance.
This systemhas been very successful throughout the TIPSTER pro-ject, and has achieved this success using variations ontheir original system rather than having to completely re-vise their techniques.Second, the results obtained by the best systems in TIP-STER and TREC are at a level of performance that is gen-erally accepted to be superior to the best current Booleanretrieval system.
More importantly, this performance isachieved from simple natural language input, allowingconsistently superior retrieval performance without ex-haustive training or experience.
These systems are clearlyready to be tested in fully operational environments.Third, the use of a large test collection has shown someunexpected results.
Techniques that should have broughtimprovements have not done so.
The use of phrases in-stead of single terms has not resulted in significant im-provements; the use of proximity or paragraph-level re-trieval has not shown especially good results; and the useof more complex NLP techniques have not worked wellyet.
Conversely, techniques that have not been successfulbefore such as using types of automatic thesaurii for topicexpansion have had unexpected success.
These unexpect-ed results using a large test collection are reopening re-search on old discarded ideas and starting research in newareas.
It is much too early to draw firm conclusions onany of these techniques.
Often poor performance that isattributed to one problem may be the result of lack of bal-ance in parameter adjustment, e.g., the lack of im-provement from phrases may be caused by the difficultyin balancing the weights of these phrases and the weightsof single terms.What is the lasting value of the document detection halfof the TIPSTER phase I project?
The first contribution inmy opinion has been the development of a large test col-lection and the wide acceptance of its use via the TRECconferences.
The lack of a large test collection has been amajor barrier in the field of information retrieval and itsremoval allows an expansion of research by many groupsworld-wide.The second lasting value is the demonstration f the feasi-bility of using the non-Boolean, statistically-based re-trieval systems both in the ARPA community and in thebroader commercial sector.
Not only have well-established small-scale research groups braved the scalingeffort, but at least four new commercial products haveused the TIPSTER/TREC program as launching pads.The TIPSTER program has caused the establishment oftwo major new retrieval research groups; both SyracuseUniversity and HNC Inc. have built systems during theTIPSTER project hat are approaching the power of thebest of the TIPSTER/TREC systems.
Additionally manyof the TREC systems are either new groups in the infor-marion retrieval research arena or are older groups ex-panding their small programs to tackle this major etrievalexperiment.The final lasting value of the TIPSTER project has beenthe joining of the NLP community and the information re-trieval (IR) community in the project.
This has led to thehigh expecrions for combining these disjoint echnologiesin phase II and has helped cement the important coUabo-ration of two diverse groups of researchers.These three lasting contributions are not only of value in-dividnally, but will lead to a resurgence of research in theinformation retrieval area.
The combination of the largetest collection, the growing demand for improved retrievalproducts, and the increased collaboration between theNLP and IR communities will result in new techniquesthat will finally achieve the breakthrough in performancethat is TIPSTER's goal.8.
REFERENCES\[1\] Hat'man D.
(Ed.
).The Second Text REtrieval Confer-ence (TREC-2).
National Institute of Standards and Tech-nology Special Publication 500-215, in press.430.80Best  Automat ic  Adhoc0.600.400 .200 .00  I I i0.00  0 .20  0 .40  0 .60  0 .80  1 .00Reca l lINQO01 + dor tQ2 _._ Brk ly3o CLARTA o cmlL2  6 HNCad l1.001 .00Best  Automat ic  Rout ing0.800 .600 .400 .200 .000 .00  0 .20  0 .40  0 .60  0 .80  1.00Reca l lcmlC1 + dor tP1  .
c i ty r2+ INQO03 + Brk ly5  ~ hnc2cFigures 13 and 14: Best TREC-2 adhoc and routing performance using full collection44, Topic Top 8 Systems101 rutcombl VTcms2 crnlV2 INQ002 dortQ2 pircs3 Brkly3 " CLARTM102 emiL2 crnlV2 VTcms2 siems3 dortL2 INQ002 siems2 CLARTM103 siems3 siems2 schaul citril crnlV2 lsiasm HNCad2 HNCadl104 dortQ2 CLARTM CLARTA pircs4 pircs3 dortL2 HNCad2 lsiasm105 citri2 lsiasm citril siems2 siems3 ernlV2 schaul crnlL2106 VTcms2 INQ002 INQ001 TOPIC2 pircs4 pircs3 CLARTM dortL2107 CnQstl CnQst2 rutcombl TOPIC2 VTcms2 INQ002 ruffmed CLARTM108 eitril dortQ2 siems3 VTcms2 siems2 HNCad2 schaul dortL2109 dortL2 crnlL2 dortQ2 CLARTA CLARTM pircs3 cmlV2 _ pircs4110 INQ002 INQ001 Brkly3 dortQ2 nyuir3 nyuir2 cityau siems2111 CLARTA CLARTM INQ001 dortQ2 Brkly3 siems2 siems3 pircs4112 INQ002 INQ001 VTcms2 nyuir2 nyuir3 HNCadl HNCad2 CnQst2113 VTcms2 emiL2 dortL2 crniV2 nyuirl siems2 CLARTM INQ002114 INQ002 cityau VTcms2 INQ001 siems3 siems2 lsial TOPIC2115 nyuir2 nyuir3 nyuirl siems2 dortL2 crnlV2 siems3 cmlL2116 VTcms2 CLARTA HNCad2 HNCadl siems3 siems2 CLARTM Brkly3117 citri2 citril dortQ2 INQ001 TMC8 lsiasm gecrd2 schaul118 nyuir2 nyuir3 nyuirl TOPIC2 citymf dortQ2 CLARTA INQ001119 nyuirl nyuir2 nyuir3 INQ002 INQ001 dortQ2 citymf VTcms2120 citymf nyuir2 nyuir3 nyuirl CnQst2 CnQstl VTcms2 erima2121 TOPIC2 CLARTM VTcms2 Brkly3 nyuirl prceol INQ002 rutfmed122 siems2 siems3 INQ002 INQ001 dortQ2 Brkly3 CLARTM crnlV2123 nyuirl nyuir2 nyuir3 CLARTA INQ001 INQ002 CLARTM pircs4124 nyuir2 nyuir3 nyuirl dortL2 dortQ2 INQ001 Brkly3 TMC9125 crnlV2 Brkly3 emiL2 CLARTM siems3 CLARTA pircs4 pircs3126 siems3 emiL2 siems2 Brkly3 cmlV2 INQ002 CLARTM INQ001127 cityau Brkly3 CLARTA HNCad2 INQ001 INQ002 siems2 siems3128 VTcms2 CLARTA siems3 siems2 CLARTM TOPIC2 citril lsiasm129 INQ001 INQ002 cityau CLARTM siems2 Brkly3 crnlL2 CLARTA130 INQ002 INQ001 dortQ2 crnlL2 pircs4 CLARTM dortL2 pircs3131 TOPIC2 VTcms2 HNCadl HNCad2 siems3 Brkly3 siems2 INQ002132 dortL2 INQ001 INQ002 citri 1 citri2 dortQ2 HNCad2 crnlL2133 CnQst2 CnQstl rutcombl pircs4 INQ002 pircs3 cityau INQ001134 emiL2 dortL2 nyuirl nyuir2 nyuir3 INQ002 INQ001 dortQ2135 nyuir2 nyuir3 nyuirl Brkly3 INQ001 INQ002 siems3 siems2136 VTcms2 CnQstl CnQst2 CLARTM pircs4 CLARTA dortQ2 TOPIC2137 CLARTA nyuir2 nyuir3 Brkly3 siems2 siems3 CLARTM nyuirl138 nyuir2 nyuir3 rutfmed rutcombl nyuirl schaul gecrd2 citril139 nyuir2 nyuir3 nyuirl VTcms2 dortL2 HNCad2 dortQ2 HNCadl140 nyuir2 nyuir3 nyuirl dortQ2 dortL2 INQ002 siems3 siems2141 VTcms2 INQ002 CnQst2 INQ001 Brkly3 dortL2 dortQ2 CnQstl142 dortQ2 siems2 crnlL2 VTcms2 siems3 CLARTM cmlV2 Brkly3143 INQ002 INQ001 siems2 siems3 crnlL2 crnlV2 nyuir2 nyuir3144 VTcms2 Brkly3 citymf crnlV2 siems3 lsiasm siems2 HNCad2145 crnlL2 crnlV2 dortL2 CLARTM nyuirl siems3 siems2 dortQ2146 Brkly3 siems3 siems2 lsiasm cmlV2 schaul CLARTM citril147 HNCad2 HNCadl VTcms2 citril INQ002 INQ001 citymf CLARTA148 lsiasm cmlL2 crnlV2 siems2 siems3 Brkly3 dortL2 dortQ2149 nyuirl CnQst2 TOPIC2 CnQstl CLARTA rutfmed Brkly3 rutcombl150 crnlL2 dortQ2 CLARTM siems3 INQ002 INQ001 crnlV2 siems2Table 1: The TREC-2 system rankings (using average precision) on individual topics45RelevantDocuments HNCrtl INQ023AP900115-0033AP900410-0012AP900905-0174SJMN91-06059027SJMN91-06072107AP900910-0080AP900914-0252AP901018-0234AP900822-0032SJMN91-06034021SJMN91-06063161AP900818-0028AP900924-0260AP900906-0203AP900903-0137AP900816-0111AP900829-023913481054208?55984990436019228849923140549245322555668Table 2: Ranks of retrieved relevant documents for topic 89RelevantRank Run Tag DocumentsAP880214-0002 80AP880223-0008 26AP880622-0070 59AP880815-0056 40AP890522-0036 5AP891004-0223 39AP891130-0147 49AP891206-0043 23WSJ870325-0156 51WSJ900801-0135 124ZF08-270-494 105ZF08-305-768 3ZF08-386-296 57INQOIOINQ013~QOlOINQO~O~QOIOINQ013~QOlOINQ013INQ013INQ011,INQ012HNCad2HNCadl (all)HNCad2Table 3: Relevant Documents for topic 121 (55 total relevant, 13 found TIPSTER 24-month)46
