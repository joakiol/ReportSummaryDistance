BioNLP 2007: Biological, translational, and clinical language processing, pages 199?200,Prague, June 2007. c?2007 Association for Computational LinguisticsEvaluating and combining biomedical named entity recognition systemsAndreas VlachosWilliam Gates BuildingComputer LaboratoryUniversity of Cambridgeav308@cl.cam.ac.ukAbstractThis paper is concerned with the evaluationof biomedical named entity recognition sys-tems.
We compare two such systems, onebased on a Hidden Markov Model and onebased on Conditional Random Fields andsyntactic parsing.
In our experiments weused automatically generated data as wellas manually annotated material, includinga new dataset which consists of biomedi-cal full papers.
Through our evaluation, weassess the strengths and weaknesses of thesystems tested, as well as the datasets them-selves in terms of the challenges they presentto the systems.1 IntroductionThe domain of biomedical text mining has becomeof importance for the natural language processing(NLP) community.
While there is a lot of textual in-formation available in the domain, either in the formof publications or in model organism databases,there is paucity in material annotated explicitly forthe purpose of developing NLP systems.
Most ofthe existing systems have been developed using datafrom the newswire domain.
Therefore, the biomedi-cal domain is an appropriate platform to evaluate ex-isting systems in terms of their portability and adapt-ability.
Also, it motivates the development of newsystems, as well as methods for developing systemswith these aspects in focus in addition to the perfor-mance.The biomedical named entity recognition (NER)task in particular has attracted a lot of attentionfrom the community recently.
There have beenthree shared tasks (BioNLP/NLPBA 2004 (Kim etal., 2004), BioCreative (Blaschke et al, 2004) andBioCreative2 (Krallinger and Hirschman, 2007))which involved some flavour of NER using manu-ally annotated training material and fully supervisedmachine learning methods.
In parallel, there havebeen successful efforts in bootstrapping NER sys-tems using automatically generated training materialusing domain resources (Morgan et al, 2004; Vla-chos et al, 2006).
These approaches have a signif-icant appeal, since they don?t require manual anno-tation of training material which is an expensive andlengthy process.Named entity recognition is an important task be-cause it is a prerequisite to other more complex ones.Examples include anaphora resolution (Gasperin,2006) and gene normalization (Hirschman et al,2005).
An important point is that until now NERsystems have been evaluated on abstracts, or on sen-tences selected from abstracts.
However, NER sys-tems will be applied to full papers, either on theirown or in order to support more complex tasks.Full papers though are expected to present additionalchallenges to the systems than the abstracts, so it isimportant to evaluate on the former as well in or-der to obtain a clearer picture of the systems and thetask (Ananiadou and McNaught, 2006).In this paper, we compare two NER systems ina variety of settings.
Most notably, we use auto-matically generated training data and we evaluate onabstracts as well as a new dataset consisting of fullpapers.
To our knowledge, this is the first evalua-tion of biomedical NER on full paper text instead of199abstracts.
We assess the performance and the porta-bility of the systems and using this evaluation wecombine them in order to take advantage of theirstrengths.2 Named entity recognition systemsThis section presents the two biomedical named en-tity recognition systems used in the experiments ofSection 4.
Both systems have been used success-fully for this task and are domain-independent, i.e.they don?t use features or resources that are tailoredto the biomedical domain.2.1 Hidden Markov ModelThe first system used in our experiments was theHMM-based (Rabiner, 1990) named entity recogni-tion module of the open-source NLP toolkit Ling-Pipe1.
It is a hybrid first/second order HMMmodel using Witten-Bell smoothing (Witten andBell, 1991).
It estimates the following joint proba-bility of the current token xt and label yt conditionedon the previous label yt?1 and previous two tokensxt?1 and xt?2:P (xt, yt|yt?1, xt?1, xt?2) (1)Tokens unseen in the training data are passed toa morphological rule-based classifier which assignsthem to predefined classes according to their capital-ization and whether they contain digits or punctua-tion.
In order to use these classes along with the or-dinary tokens, during training a second pass over thetraining data is performed in which tokens that ap-pear fewer times than a given threshold are replacedby their respective classes.
In our experiments, thisthreshold was set experimentally to 8.
Vlachos etal.
(2006) employed this system and achieved goodresults on bootstrapping biomedical named entityrecognition.
They also note though that due to its re-liance on seen tokens and the restricted way in whichunseen tokens are handled its performance is not asgood on unseen data.1http://www.alias-i.com/lingpipe.
The version used in theexperiments was 2.1.2.2 Conditional Random Fields with SyntacticParsingThe second NER system we used in our experimentswas the system of Vlachos (2007) that participatedin the BioCreative2 Gene Mention task (Krallingerand Hirschman, 2007).
Its main components are theConditional Random Fields toolkit MALLET2 (Mc-Callum, 2002) and the RASP syntactic parsingtoolkit3 (Briscoe et al, 2006), which are both pub-licly available.Conditional Random Fields (CRFs) (Lafferty etal., 2001) are undirected graphical models trained tomaximize the conditional probability of the outputsequence given the inputs, or, in the case of token-based natural language processing tasks, the condi-tional probability of the sequence of labels y givena sequence of tokens x.
Like HMMs, the number ofprevious labels taken into account defines the orderof the CRF model.
More formally:P (y|x) = 1Z(x)exp{T?t=1K?k=1?kfk(y, xt)} (2)In the equation above, Z(x) is a normalizationfactor computed over all possible label sequences,fk is a feature function and ?k its respective weight.y represents the labels taken into account as contextand it is defined by the order of the CRF.
For a n-thorder model, y becomes yt, yt?1..., yt?n.
It is alsoworth noting that xt is the feature representation ofthe token in position t, which can include featuresextracted by taking the whole input sequence intoaccount, not just the token in question.
The mainadvantage is that as a conditionally-trained modelCRFs do not need to take into account dependen-cies in input, which as a consequence, allows the useof features dependent on each other.
Compared toHMMs, their main disadvantage is that during train-ing, the computation time required is significantlylonger.
The interested reader is referred to the de-tailed tutorial of Sutton & McCallum (2006).Vlachos (2007) used a second order CRF modelcombined with a variety of features.
These canbe divided into simple orthographic features and in2http://mallet.cs.umass.edu/index.php/Main Page3http://www.informatics.susx.ac.uk/research/nlp/rasp/200those extracted from the output of the syntactic pars-ing toolkit.
The former are extracted for every tokenand they are rather common in the NER literature.They include the token itself, whether it containsdigits, letters or punctuation, information about cap-italization, prefixes and suffixes.The second type of features are extracted fromthe output of RASP for each sentence.
The part-of-speech (POS) tagger was parameterized to generatemultiple POS tags for each token in order to amelio-rate unseen token errors.
The syntactic parser usesthese sequences of POS tags to generate parses foreach sentence.
The output is in the form of grammat-ical relations (GRs), which specify the links betweenthe tokens in the sentence accoring to the syntacticparser and they are encoded using the SciXML for-mat (Copestake et al, 2006).
From this output, foreach token the following features are extracted (ifpossible):?
the lemma and the POS tag(s) associated withthe token?
the lemmas for the previous two and the fol-lowing two tokens?
the lemmas of the verbs to which this token issubject?
the lemmas of the verbs to which this token isobject?
the lemmas of the nouns to which this tokenacts as modifier?
the lemmas of the modifiers of this tokenAdding the features from the output of the syntac-tic parser allows the incorporation of features froma wider context than the two tokens before and aftercaptured by the lemmas, since GRs can link tokenswithin a sentence independently of their proximity.Also, they result in more specific features, since therelation between two tokens is determined.
The CRFmodels in the experiments of Section 4 were traineduntil convergence.It must be mentioned that syntactic parsing is acomplicated task and therefore feature extraction onits output is likely to introduce some noise.
TheRASP syntactic parser is domain independent butit has been developed using data from general En-glish corpora mainly, so it is likely not to performas well in the biomedical domain.
Nevertheless,the results of the system in the BioCreative2 GeneMention task suggest that the use of syntactic pars-ing features improve performance.
Also, despite thelack of domain-specific features, the system is com-petitive with other systems, having performance inthe second quartile of the task.
Finally, the BIOEWscheme (Siefkes, 2006) was used to tag the tok-enized corpora, under which the first token of a mul-titoken mention is tagged as B, the last token as E,the inner ones as I, single token mentions as W andtokens outside an entity as O.3 CorporaIn our experiments we used two corpora consistingof abstracts and one consisting of full papers.
Oneof the abstracts corpora was automatically generatedwhile the other two were manually annotated.
Allthree were created using resources from FlyBase4and they are publicly available5 .The automatically generated corpus was createdin order to bootstrap a gene name recognizer in Vla-chos & Gasperin (2006).
The approach used wasintroduced by Morgan et al(2004).
In brief, the ab-stracts of 16,609 articles curated by FlyBase wereretrieved and tokenized by RASP (Briscoe et al,2006).
For each article, the gene names and theirsynonyms that were recorded by the curators wereannotated automatically in its abstract using longest-extent pattern matching.
The pattern matching isflexible in order to accommodate capitalization andpunctuation variations.
This process resulted in alarge but noisy dataset, consisting of 2,923,199 to-kens and containing 117,279 gene names, 16,944 ofwhich are unique.
The noise is due to two reasonsmainly.
First, the lists constructed by the curatorsfor each paper are incomplete in two ways.
Theydon?t necessarily contain all the genes mentioned inan abstract because not all genes are always curatedand also not all synonyms are recorded, thus result-ing in false negatives.
The other cause is the overlapbetween gene names and common English words orbiomedical terms, which results in false positives for4http://www.flybase.net/5http://www.cl.cam.ac.uk/ nk304/Project Index/#resources201abstracts with such gene names.The manually annotated corpus of abstracts wasdescribed in Vlachos & Gasperin (2006).
It con-sists of 82 FlyBase abstracts that were annotatedby a computational linguist and a FlyBase curator.The full paper corpus was described in Gasperin etal.
(2007).
It consists of 5 publicly available full pa-pers which were annotated by a computational lin-guist and a FlyBase curator with named entities aswell as anaphoric relations in XML.
To use it forthe gene name recognition experiments presented inthis paper, we converted it from XML to IOB formatkeeping only the annotated gene names.noisy golden fullabstracts abstracts papersabstracts / 16,609 82 5paperssentences 111,820 600 1,220tokens 2,923,199 15,703 34,383gene names 117,279 629 2,057unique 16,944 326 336gene namesunique non- 60,943 3,018 4,113gene tokensTable 1: Statistics of the datasetsThe gene names in both manually created cor-pora were annotated using the guidelines presentedin Vlachos & Gasperin (2006).
The main idea ofthese guidelines is that gene names are annotatedanywhere they are encountered in the text, evenwhen they are used to refer to biomedical entitiesother than the gene itself.
The distinction betweenthe possible types of entities the gene name can re-fer to is performed at the level of the shortest nounphrase surrounding the gene name.
This resulted inimproved inter-annotator agreement (Vlachos et al,2006).Statistics on all three corpora are presented in Ta-ble 1.
From the comparisons in this table, an in-teresting observation is that the gene names in fullpapers tend to be repeated more frequently than thegene names in the manually annotated abstracts (6.1compared to 1.9 times respectively).
Also, the lat-ter contain approximately 2 unique gene names ev-ery 100 tokens while the full papers contain just 1.This evidence suggests that annotating abstracts ismore likely to provide us with a greater variety ofgene names.
Interestingly, the automatically anno-tated abstracts contain only 0.6 unique gene namesevery 100 tokens which hints at inclusion of falsenegatives during the annotation.Another observation is that, while the manuallyannotated abstracts and full papers contain roughlythe same number of unique genes, the full paperscontain 36% more unique tokens that are not partof a gene name (?unique non-gene tokens?
in Ta-ble 1).
This suggests that the full papers contain agreater variety of contexts, as well as negative ex-amples, therefore presenting greater difficultiy to agene name recognizer.4 ExperimentsWe ran experiments using the two NER systems andthe three datasets described in Sections 2 and 3.In order to evaluate the performance of the sys-tems, apart from the standard recall, precision andF-score metrics, we measured the performance onseen and unseen gene names independently, as sug-gested by Vlachos & Gasperin (2006).
In brief, thegene names that are in the test set and the outputgenerated by the system are separated according towhether they have been encountered in the trainingdata as gene names.
Then, the standard recall, pre-cision and F-score metrics are calculated for each ofthese lists independently.HMM CRF+RASPRecall 75.68 63.43overall Precision 89.14 90.89F-score 81.86 74.72Recall 94.48 76.32seen Precision 93.62 95.4genes F-score 94.05 84.80Recall 33.51 34.54unseen Precision 68.42 73.63genes F-score 44.98 47.02seen genes 435unseen genes 194Table 2: Results on training on noisy abstracts andtesting on manually annotated abstracts202HMM CRF+RASPRecall 58.63 61.40overall Precision 80.56 89.19F-score 67.87 72.73Recall 89.82 72.51seen Precision 87.83 94.82genes F-score 88.81 82.18Recall 35.12 53.03unseen Precision 69.48 84.05genes F-score 46.66 65.03seen genes 884unseen genes 1173Table 3: Results on training on noisy abstracts andtesting on full papersTables 2 and 3 report in detail the performance ofthe two systems when trained on the noisy abstractsand evaluated on the manually annotated abstractsand full papers respectively.
As it can be seen, theperformance of the HMM-based NER system is bet-ter than that of CRF+RASP when evaluating on ab-stracts and worse when evaluating on full papers(81.86 vs 74.72 and 67.87 vs 72.73 respectively).Further analysis of the performance of the twosystems on seen and unseen genes reveals that thisresult is more likely to be due to the differences be-tween the two evaluation datasets and in particularthe balance between seen and unseen genes with re-spect to the training data used.
In both evaluations,the performance of the HMM-based NER system issuperior on seen genes while the CRF+RASP sys-tem performs better on unseen genes.
On the ab-stracts corpus the performance on seen genes be-comes more important since there are more seenthan unseen genes in the evaluation, while the op-posite is the case for the full paper corpus.The difference in the performance of the two sys-tems is justified.
The CRF+RASP system uses acomplex but more general representation of the con-text based on the features extracted from the outputof syntactic parser, namely the lemmas, the part-of-speech tags and the grammatical relationships, whilethe HMM-based system uses a simple morphologi-cal rule-based classifier.
Also, the CRF+RASP sys-tem takes the two previous labels into account, whilethe HMM-based only the previous one.
Therefore,it is expected that the former has superior perfor-mance on unseen genes.
This difference between theCRF+RASP and the HMM-based system is substan-tially larger when evaluating on full papers (65.03versus 46.66 respectively) than on abstracts (47.02versus 44.98 respectively).
This can be attributedto the fact that the training data used is generatedfrom abstracts and when evaluating on full papersthe domain shift can be handled more efficiently bythe CRF+RASP system due to its more complex fea-ture set.However, the increased complexity of theCRF+RASP system renders it more vulnerable tonoise.
This is particularly important in these experi-ments because we are aware that our training datasetcontains noise since it was automatically generated.This noise is in addition to that from inaccurate syn-tactic parsing employed, as explained in Section 2.2.On the other hand, the simpler HMM-based sys-tem is likely to perform better on seen genes, whoserecognition doesn?t require complex features.We also ran experiments using the manually an-notated corpus of abstracts as training data and eval-uated on the full papers.
The results in Table 4confirmed the previous assessment, that the perfor-mance of the CRF+RASP system is better on the un-seen genes and that the HMM-based one is better onseen genes.
In this particular evaluation, the smallnumber of unique genes in the manually annotatedcorpus of abstracts results in the majority of genenames being unseen in the training data, which fa-vors the CRF+RASP system.It is important to note though that the perfor-mances for both systems were substantially lowerthan the ones achieved using the large and noisyautomatically generated corpus of abstracts.
Thiscan be attributed to the fact that both systems havebetter performance in recognizing seen gene namesrather than unseen ones.
Given that the automati-cally generated corpus required no manual annota-tion and very little effort compared to the manuallyannotated one, it is a strong argument for bootstrap-ping techniques.A known way of reducing the effect of noise insequential models such as CRFs is to reduce theirorder.
However, this limits the context taken into ac-count, potentially harming the performance on un-seen gene names.
Keeping the same feature set, we203HMM CRF+RASPRecall 52.65 49.88overall Precision 46.56 72.77F-score 49.42 59.19Recall 96.49 47.37seen Precision 58.51 55.1genes F-score 72.85 50.94Recall 51.4 49.95unseen Precision 46.04 73.4genes F-score 48.57 59.45seen genes 57unseen genes 2000Table 4: Results on training on manually annotatedabstracts and testing on full paperstrained a first order CRF model on the noisy ab-stracts corpus and we evaluated on the manually an-notated abstracts and full papers.
As expected, theperformance on the seen gene names improved butdeteriorated on the unseen ones.
In particular, whenevaluating on abstracts the F-scores achieved were93.22 and 38.1 respectively (compared to 84.8 and47.02) and on full papers 86.64 and 59.86 (comparedto 82.18 and 65.03).
The overall performance im-proved substantially for the abstract where the seengenes are the majority (74.72 to 80.69), but onlymarginally for the more balanced full papers (72.73to 72.89).Ideally, we wouldn?t want to sacrifice the perfor-mance on unseen genes of the CRF+RASP systemin order to deal with noise.
While the large noisytraining dataset provides good coverage of the pos-sible gene names, it is unlikely to contain every genename we would encounter, as well as all the possiblecommon English words which can become precisionerrors.
Therefore we attempted to combine the twoNER systems based on the evaluation presented ear-lier.
Since the HMM-based system is performingvery well on seen gene names, for each sentence wecheck whether it has recognized any gene names un-seen in the training data (potential unseen precisionerrors) or if it considered as ordinary English wordsany tokens not seen as such in the training data (po-tential unseen recall errors).
If either of these is true,then we pass the sentence to the CRF+RASP sys-tem, which has better performance on unseen genenames.Such a strategy is expected to trade some of theperformance of the seen gene names of the HMM-based system for improved performance on the un-seen gene names by using the predictions of theCRF+RASP system.
This occurs because in thesame sentence seen and unseen gene names may co-exist and choosing the predictions of the latter sys-tem could result in more errors on the seen genenames.
This strategy is likely to improve the per-formance on datasets where there are more unseengene names and the difference in the performanceof the CRF+RASP on them is substantially betterthan the HMM-based.
Indeed, using this strategy weachieved 73.95 overall F-score on the full paper cor-pus which contains slightly more unseen gene names(57% of the total gene names).
For the corpus ofmanually annotated abstracts the performance wasreduced to 80.21, which is expected since the major-ity of gene names (69%) are seen in the training data.and the performance of the CRF+RASP system onthe unseen data is better only by a small margin thanthe HMM-based one (47.02 vs 44.98 in F-score re-spectively).5 Discussion - Related workThe experiments of the previous section are to ourknowledge the first to evaluate biomedical namedentity recognition on full papers.
Furthermore, weconsider that using abstracts as the training mate-rial for such evaluation is a very realistic scenario,since abstracts are generally publicly available andtherefore easy to share and distribute with a trainablesystem, while full papers on which they are usuallyapplied are not always available.Differences between abstracts and full papers canbe important when deciding what kind of material toannotate for a certain purpose.
For example, if theannotated material is going to be used as trainingdata and given that higher coverage of gene namesin the training data is beneficial, then it might bepreferable to annotate abstracts because they con-tain greater variety of gene names which would re-sult in higher coverage in the dataset.
On the otherhand, full papers contain a greater variety of con-texts which can be useful for training a system andas mentioned earlier, they can be more appropriate204for evaluation.It would be of interest to train NER systems ontraining material generated from full papers.
Con-sidering the effort required in manual annotationthough, it would be difficult to obtain quantities ofsuch material large enough that would provide ade-quate coverage of a variety of gene names.
An alter-native would be to generate it automatically.
How-ever, the approach employed to generate the noisyabstracts corpus used in this paper is unlikely to pro-vide us with material of adequate quality to train agene name recognizer.
This is because more noiseis going to be introduced, since full papers are likelyto contain more gene names not recorded by the cu-rators, as well as more common English words thathappen to overlap with the genes mentioned in thepaper.The aim of this paper is not about deciding onwhich of the two models is better but about howthe datasets used affect the evaluation and how tocombine the strengths of the models based on theanalysis performed.
In this spirit, we didn?t attemptany of the improvements discussed by Vlachos &Gasperin (2006) because they were based on obser-vations on the behavior of the HMM-based system.From the analysis presented earlier, the CRF+RASPsystem behaves differently and therefore it?s not cer-tain that those strategies would be equally beneficialto it.As mentioned in the introduction, there has beena lot of work on biomedical NER, either throughshared tasks or independent efforts.
Of particularinterest is the work of Morgan et al(2004) whobootstrapped an HMM-based gene name recognizerusing FlyBase resources and evaluate on abstracts.Also of interest is the system presented by Set-tles (2004) which used CRFs with rich feature setsand suggested that one could use features from syn-tactic parsing with this model given their flexibility.Direct comparisons with these works are not possi-ble since different datasets were used.Finaly, combining models has been a successfulway of achieving good results, such as those of Flo-rian et al (2003) who had the top performance inthe named entity recognition shared task of CoNLL2003 (Tjong Kim Sang and De Meulder, 2003).6 Conclusions- Future workIn this paper we compared two different named en-tity recognition systems on abstracts and full pa-per corpora using automatically generated trainingdata.
We demonstrated how the datasets affect theevaluation and how the two systems can be com-bined.
Also, our experiments showed that bootstrap-ping using automatically annotated abstracts can beefficient even when evaluating on full papers.As future work, it would be of interest to de-velop an efficient way to generate data automati-cally from full papers which could improve the re-sults further.
An interesting approach would be tocombine dictionary-based matching with an exist-ing NER system in order to reduce the noise.
Also,different ways of combining the two systems couldbe explored.
With constrained conditional randomfields (Kristjansson et al, 2004) the predictions ofthe HMM on seen gene names could be added asconstraints to the inference performed by the CRF.The good performance of bootstrapping genename recognizers using automatically created train-ing data suggests that it is a realistic alternative tofully supervised systems.
The latter have benefitedfrom a series of shared tasks that, by providing atestbed for evaluation, helped assessing and improv-ing their performance.
Given the variety of meth-ods that are available for generating training dataefficiently automatically using extant domain re-sources (Morgan et al, 2004) or semi-automatically(active learning approaches like Shen et al (2004)or systems using seed rules such as Mikheev etal.
(1999)), it would be of interest to have a sharedtask in which the participants would have access toevaluation data only and they would be invited to usesuch methods to develop their systems.ReferencesSophia Ananiadou and John McNaught, editors.
2006.Text Mining in Biology and Biomedicine.
ArtechHouse, Inc.Christian Blaschke, Lynette Hirschman, and AlexanderYeh, editors.
2004.
Proceedings of the BioCreativeWorkshop, Granada, March.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-205ings of the COLING/ACL 2006 Interactive Presenta-tion Sessions.Ann Copestake, Peter Corbett, Peter Murray-Rust,CJ Rupp, Advaith Siddharthan, Simone Teufel, andBen Waldron.
2006.
An architecture for language pro-cessing for scientific texts.
In Proceedings of the UKe-Science All Hands Meeting 2006.Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Walter Daelemans and MilesOsborne, editors, Proceedings of CoNLL-2003, pages168?171.
Edmonton, Canada.C.
Gasperin, N. Karamanis, and R. Seal.
2007.
Annota-tion of anaphoric relations in biomedical full-text arti-cles using a domain-relevant scheme.
In Proceedingsof DAARC.Caroline Gasperin.
2006.
Semi-supervised anaphora res-olution in biomedical texts.
In Proceedings of BioNLPin HLT-NAACL, pages 96?103.Lynette Hirschman, Marc Colosimo, Alexander Morgan,and Alexander Yeh.
2005.
Overview of biocreativetask 1b: normalized gene lists.
BMC Bioinformatics.J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,editors.
2004.
Proceedings of JNLPBA, Geneva.Martin Krallinger and Lynette Hirschman, editors.
2007.Proceedings of the Second BioCreative ChallengeEvaluation Workshop, Madrid, April.Trausti Kristjansson, Aron Culotta, Paul Viola, and An-drew McCallum.
2004.
Interactive information ex-traction with constrained conditional random fields.J.
D. Lafferty, A. McCallum, and F. C. N. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proceed-ings of ICML 2001, pages 282?289.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.A.
Mikheev, M. Moens, and C. Grover.
1999.
Namedentity recognition without gazetteers.A.
A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh,and J.
B. Colombe.
2004.
Gene name identificationand normalization using a model organism database.J.
of Biomedical Informatics, 37(6):396?410.L.
R. Rabiner.
1990.
A tutorial on hidden markov mod-els and selected apllications in speech recognition.
InA.
Waibel and K.-F. Lee, editors, Readings in SpeechRecognition, pages 267?296.
Kaufmann, San Mateo,CA.Burr Settles.
2004.
Biomedical Named Entity Recog-nition Using Conditional Random Fields and NovelFeature Sets.
Proceedings of the Joint Workshop onNatural Language Processing in Biomedicine and itsApplications.D.
Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan.
2004.Multi-criteria-based active learning for named entityrecongition.
In Proceedings of ACL 2004, Barcelona.Christian Siefkes.
2006.
A comparison of tagging strate-gies for statistical information extraction.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Companion Volume: Short Papers,pages 149?152, New York City, USA, June.
Associ-ation for Computational Linguistics.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.
MITPress.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In WalterDaelemans and Miles Osborne, editors, Proceedingsof CoNLL-2003, pages 142?147.
Edmonton, Canada.A.
Vlachos and C. Gasperin.
2006.
Bootstrapping andevaluating named entity recognition in the biomedicaldomain.
In Proceedings of BioNLP in HLT-NAACL,pages 138?145.A.
Vlachos, C. Gasperin, I. Lewin, and T. Briscoe.
2006.Bootstrapping the recognition and anaphoric linking ofnamed entities in drosophila articles.
In Proceedingsof PSB 2006.Andreas Vlachos.
2007.
Tackling the BioCreative2Gene Mention task with Conditional Random Fieldsand Syntactic Parsing.
In Proceedings of the SecondBioCreative Challenge Evaluation Workshop.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
IEEETransactions on Information Theory, 37(4):1085?1094.206
