Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1459?1465,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsReserating the awesometastic:An automatic extension of the WordNet taxonomy for novel termsDavid JurgensSchool of Computer ScienceMcGill Universityjurgens@cs.mcgill.caMohammad Taher PilehvarDepartment of Computer ScienceSapienza University of Romepilehvar@di.uniroma1.itAbstractThis paper presents CROWN, an automatically con-structed extension of WordNet that augments itstaxonomy with novel lemmas from Wiktionary.CROWN fills the important gap in WordNet?s lexi-con for slang, technical, and rare lemmas, and morethan doubles its current size.
In two evaluations, wedemonstrate that the construction procedure is accu-rate and has a significant impact on a WordNet-basedalgorithm encountering novel lemmas.1 IntroductionSemantic knowledge bases are an essential, enablingcomponent of many NLP applications.
A notable exam-ple is WordNet (Fellbaum, 1998), which encodes a tax-onomy of concepts and semantic relations between them.As a result, WordNet has enabled a wide variety of NLPtechniques such as Word Sense Disambiguation (Agirreet al, 2014), information retrieval (Varelas et al, 2005),semantic similarity (Pedersen et al, 2004; B?ar et al,2013), and sentiment analysis (Baccianella et al, 2010).However, semantic knowledge bases such as WordNet areexpensive to produce; as a result, their scope and domainare often constrained by the resources available and mayomit highly-specific concepts or lemmas, as well as newterminology that emerges after their construction.
For ex-ample, WordNet does not contain the nouns ?stepmom,??broadband,?
and ?prequel.
?Because of the coverage limitations of WordNet, sev-eral approaches have attempted to enrich WordNet withnew relations and concepts.
One group of approaches hasenriched WordNet by aligning its structure with that ofother resources such as Wikipedia or Wiktionary (Ruiz-Casado et al, 2005; Navigli and Ponzetto, 2012; Millerand Gurevych, 2014; Pilehvar and Navigli, 2014).
How-ever, because these approaches identify correspondinglemmas with identical lexicalizations, they are often un-able to directly add novel lemmas to the existing taxo-nomic structure.
The second group of approaches per-forms taxonomy induction to learn hypernymy relation-ships between words (Moro and Navigli, 2012; Meyerand Gurevych, 2012).
However, these approaches oftenproduce separate taxonomies from WordNet, which arealso generally not readily accessible as resources.We introduce a new resource CROWN (Community-enRiched Open WordNet) that extends the existingWordNet taxonomy, more than doubling the existingnumber of synsets, and attaches these novel synsetsto their appropriate hypernyms in WordNet.
Novelsense data is extracted from Wiktionary, a large-scalecollaboratively-constructed dictionary, and attached us-ing multiple heuristics.
CROWN fills an important gap inWordNet?s limited coverage of both domain-specific lem-mas and slang terminology and idioms.1In two experi-ments, we demonstrate that (1) our construction processaccurately associates a novel sense with its correct hy-pernym and (2) the resulting resource has an immediatebenefit for existing WordNet-based applications.
Impor-tantly, CROWN v1.0 is publicly available and released inWordNet format, making it seamlessly integratable withall existing WordNet libraries and tools.2 WiktionaryWiktionary is a multilingual online dictionary that, as ofMay 2014, contains more than 470K English gloss defi-nitions.
Thanks to its collaboratively-constructed nature,Wiktionary provides a high coverage of novel domain-specific, idiomatic and slang terms or meanings, acrossall parts of speech, while featuring a wide variety oflinguistic information such as morphology, etymology,pronunciation and alternative lexicalizations of a lemma.Given these characteristics, Wiktionary is an ideal re-source for improving the coverage of hand-crafted lexi-cons, such as WordNet.In addition to definitions, Wiktionary contains twosources of semantic relations.
First, the Wiktionary entry1For example, ?reserate?
is correctly included inCROWN as a hypernym of unlock%2:35:00:: (toopen the lock of) and ?awesometastic?
as a synonym offantastic%3:00:00:extraordinary:00 (extraordinar-ily good or great).1459for a lemma may contain a note stating its relationshipwith another lemma.
Second, Wiktionary includes a sep-arate thesaurus, Wikisaurus, which specifies (1) a lemmaand its gloss and (2) all other lemmas sharing a relationwith that sense.
However, these Wiktionary relations can-not directly be used to enrich WordNet for two reasons.First, Wiktionary entries are defined in terms of lemmas,rather than senses.
As a result, directly ontologizing theresource or integrating its semantic relations requires dis-ambiguating each relation?s lemmas, which is not alwayspossible due to the limited context.
Second, semantic re-lations in Wiktionary are infrequent, with 19.8% of allwords having any specified relation and only 0.3% hav-ing a hypernym relation.
As a result of this sparsity, struc-ture alignment-based approaches for extending WordNetcannot be directly applied.3 Extending WordNetCROWN is created by identifying lemmas that are out ofvocabulary (OOV) in WordNet but have one or more as-sociated glosses in Wiktionary.
A new synset is createdfor that lemma and a hypernym relation is added to theappropriate WordNet synset.
The CROWN attachmentprocess rates hypernym candidates using two methods.First, where possible, we exploit structural or morpholog-ical information to identify highly-probable candidates.Second, following previous work on resource alignmentshowing that lexical overlap accurately measures glosssemantic similarity (Meyer and Gurevych, 2011; Navigliand Ponzetto, 2012), candidates are found by measuringthe similarity of the Wiktionary gloss with the glossesof synsets found by a constrained search of the WordNetgraph.
We note that attaching OOV lemmas by first align-ing WordNet and Wiktionary is not possible due to rela-tion sparsity within Wiktionary, where most OOV wordswould not be connected to the aligned network.
Follow-ing, we first describe the Wiktionary preprocessing stepsand then detail both OOV attachment methods.3.1 PreprocessingWiktionary was parsed using JWKTL (Zesch et al, 2008)to extract the text associated with each Wiktionary defini-tion and remove Wiktionary markup.
The extracted textswere then partitioned into two sets: (1) those expressinga lexicalization, e.g., ?1337?
is an alternative spelling of?elite?
and (2) those indicating a definition.
Novel lexi-calizations that are not already handled by the WordNetmorphological analyzer (Morphy) were added to the lex-icalization exception lists in CROWN.Definitions are processed using two methods to iden-tify a set of candidate lemmas whose senses might beidentical or near to the appropriate hypernym synset.First, candidates are obtained by parsing the gloss withStanford CoreNLP (Manning et al, 2014) and extract-ing the head word and all other words joined to it by aconjunction.
Second, additional candidates are collectedfrom the first hyperlinked term or phrase in the gloss,which is similar to the approach of Navigli and Velardi(2010) for hypernym extraction in Wikipedia.
Candidatesare then filtered to ensure that (1) they have the same partof speech as the definition?s term and (2) they are definedin WordNet, which is necessary for the attachment.3.2 Structural and Lexical AttachmentThree types of structural or lexical heuristics were used toattach OOV lemmas when the appropriate data was avail-able.
First, Wikisaurus or Wiktionary synonym relationscreate sets of mutually-synonymous lemmas, which maycontain OOV lemmas.
The common hypernym of theselemmas is estimated by computing the most frequent hy-pernym synset for all the senses of the set?s lemmas thatare in WordNet.
Any OOV lemma also in the set is thenattached to this estimated hypernym.Second, some Wiktionary glosses follow regular pat-terns that identify a particular meaning.
Two patternheuristics were used: (1) a group of Person patterns and(2) a Genus pattern.
The Person patterns match glossesthat start with phrases such as ?somebody who.?
Senseswith such glosses have their set of candidate attachmentsrestricted to descendants of the human sense of the nounperson; the sense is then attached to a descendant usingthe gloss ranking procedure for lexical attachment (de-scribed below).
The Genus pattern matches glosses thatstart with ?Any member of the?
and later contain a propernoun matching a scientific genus in WordNet; in suchcases the OOV lemma is attached to the same hypernymas the synsets with a holonymy relation to the genus?ssynset.Third, an Antonymy heuristic is used to identify OOVlemmas with an antonym relation to lemmas already inWordNet.
OOV lemmas are tested for having a prefix in-dicating it could be an antonym, e.g., ?anti.?
If the lemmaformed from the remainder after prefix is in WordNet,then the OOV lemma is treated as its antonym and at-tached to the antonym?s hypernym.
Furthermore, the twosynsets are marked as antonyms in CROWN.3.3 Gloss-based AttachmentEach OOV lemma is associated with one or more Wik-tionary senses, s1...n, where each sense siis associatedwith a set of lemmas li, one of whose senses may beits hypernym.
The gloss-based attachment method ana-lyzes each sense separately, first generating a set of can-didate hypernym synsets and then ranking each synsetaccording to its gloss similarity, both defined next.
Ulti-mately the OOV lemma is attached to the highest-scoringsynset across all of its Wiktionary senses.
This procedureis intended to maximize precision by attaching only the1460ukWaC microsoft, e-learning, helpline, mp3, unsubscribeTwitter selfie, retweet, hella, bday, homieWikipedia admin, verifiability, bio, sockpuppetry, same-sexTable 1: Examples of high-frequency lemmas in CROWNbut not in WordNet, from three corpora.lemma?s dominant sense, though we note that most OOVlemmas are monosemous.The initial set C of candidate hypernym synsets forWiktionary sense siis generated from the union of thesynsets of the lemmas in li.
Then, C is expanded by in-cluding all WordNet synsets reachable from each synsetci?
C by a path of hypernym or hyponym edges, wherea path (1) has at most three edges and (2) contains at mostone hypernym edge.
The second constraint is designed toavoid including overly-general concepts.The glosses of the synsets in C are then compared withthe Wiktionary sense?s gloss.
Directly comparing glosseswith string similarity measures omits the important de-tail that certain lemmas can be highly-specific and moststrongly indicate that two glosses refer to the same con-cept.
Therefore, prior to comparison, the lemmas occur-ring in all glosses are assigned a weight?log1f(w), wheref(w) denotes the number of glosses in which lemma wappeared.
Glosses?
similarity is measured by (1) lemma-tizing their texts and computing the lemmas in common,and then (2) summing the weights of the in-common lem-mas.
This similarity function assigns higher scores toglosses sharing more specific concepts.3.4 Resource CreationThe resulting attachments are converted into WordNetlexicography files and then integrated with the existingWordNet taxonomy using the GRIND program.
Table2 shows the resulting statistics for CROWN in compari-son to WordNet.
The attachment process more than dou-bles the number of synsets and adds a significant num-ber of new lexicalizations which are essential for cap-turing common spelling variants that are not reflectedin WordNet.
Additionally, 4739 new antonym relationswere added.
Of the OOV lemmas, 87.8% were attachedusing the lexical attachment procedure.
Of the remain-ing, the Person and Antonymy heuristics were the mostfrequently used, accounting for 4.2% and 2.7% of casesrespectively.
The infrequent use of the structural and lex-ical heuristics underscores the sparsity of the availabledata in Wiktionary for straight-forward attachments.As an initial test of additional content present inCROWN but not in WordNet, all lemmas unique toCROWN were extracted and their occurrences countedin three corpora: (1) all of the English Wikipedia, (2)the web-gathered ukWaC corpus (Ferraresi et al, 2008),and (3) a sample of 50M microtext message from Twit-PoSWordNet new CROWN new CROWNsynsets synsets lex.
variantsNoun 82115 124967 29563Verb 13767 16199 43318Adj.
18156 25534 6902Adv.
3621 2031 481Table 2: The number of synsets in WordNet and newsynsets and lexicalizations added by CROWN.ter.
Table 1 shows five example high-frequency lemmasfrom each corpus that are only present in CROWN , high-lighting the types of commonly-recognized terms not inWordNet due to their technical, informal, or recently-created nature.
Indeed, ?selfie?
was only recently in-cluded in the Merriam Webster dictionary as of 2014,2demonstrating the potential for quickly integrating newterminology into CROWN from the frequently-updatedentries of Wiktionary.4 EvaluationTwo evaluations were performed.
The first estimates at-tachment accuracy by simulating OOV attachment withlemmas that are already in WordNet.
The second calcu-lates the benefit of using CROWN in an example applica-tion using a WordNet-based algorithm to measure simi-larity.4.1 WordNet ReplicationNo standard dataset exists for where OOV lemmas shouldbe attached to WordNet; therefore in the first evaluation,we assess construction accuracy by simulating the inclu-sion of OOV lemmas using those already in WordNet,which allows testing on tens of thousands of lemmas.Specifically, the CROWN attachment approach is used toreattach all monosemous lemmas in WordNet.
We optedfor monosemous terms as they can have only one validlocation in the taxonomy.4.1.1 MethodologyGlosses were extracted for 36,605 of the 101,863nouns that were monosemous in WordNet and alsopresent in Wiktionary, and for 4668 of the 6277 verbsmatching the same condition.
These glosses were thenprovided as input to the CROWN attachment process.
Wenote that these lemmas are not necessarily monosemousin Wiktionary, with nouns and verbs having on average1.40 and 1.76 senses, respectively; however, the construc-tion process will attach only the highest-scoring of thesesenses.
Once a lemma is attached, accuracy is measuredas the number of hyponym or hypernym edges away thatCROWN placed the lemma from its original position.2http://www.merriam-webster.com/new-words/2014-update.htm1461Att.Cor.Att.Cor.Att.
Cor.
Att.Cor.Att.Cor.
(a) 13,067 (b) 1722 (c) 993 (d) 831 (e) 724Figure 1: The five most-frequent error patterns and theirfrequencies seen in the results of monosemous lemmaevaluation.
Graphs show the attachment point (Att.)
andcorrect hypernym synset (Cor.
), with downward edges in-dicating hypernym relations and upward indicating hy-ponym.
The overall error trend reveals that the vast ma-jority of error was due to attaching a new sense to a more-specific concept than its actual hypernym.4.1.2 ResultsThe CROWN construction process was able to attach34,911 of the 36,605 monosemous noun lemmas (95.4%)and 4209 of the 4668 verb lemmas (90.2%).
The medianerror for attaching monosemous nouns was three edgesand for verbs was only one edge, indicating the attach-ment process is highly accurate for both.
The most com-mon form of error was attaching the OOV lemma to ahyponym of the correct hypernym, occurring in 13,067of the erroneous attachments.Figure 1 shows the five most common displacementpatterns when incorrectly attaching a monosemous noun,revealing that the majority of incorrect placements wereto a more-specific concept than what was actually the hy-pernym.
Furthermore, examining the 50 furthest-awaynoun and verb placements, we find that 28% of nounsand 20% of verbs were attached using a novel sense ofthe lemma not in WordNet (but in Wiktionary) and theplacement is in fact reasonable.
As a result, the medianerror is likely an overestimate of the expected error forthe CROWN construction process.4.2 Application-based evaluationSemantic similarity is one of the core features of manyNLP applications.
The second evaluation measures theperformance improvement of using CROWN instead ofWordNet for measuring semantic similarity when facedwith slang or OOV lemmas.
Notably, prior semanticsimilarity benchmarks such as SimLex-999 (Hill et al,2014) and the ESL test questions (Turney, 2001) havelargely omitted these types of words.
However, the recentdataset of SemEval-2014 Task 3 (Jurgens et al, 2014)includes similarity judgments between a WordNet senseand a word not defined in WordNet?s vocabulary or witha slang interpretation not present in WordNet.All Regular OOV SlangWordNet 0.195 0.463 0.0 -0.170CROWN 0.248 0.452 0.448 0.138GST Baseline 0.148 0.283 0.148 0.018Best System 0.389 0.529 0.501 0.146Table 3: The Pearson correlation performance of ADWwhen using the WordNet and CROWN semantic networkson the word-to-sense test dataset of SemEval-2014 Task3.
We also show results for the string-based baseline sys-tem (GST) and for the best participating system in theword-to-sense comparison type of Task 3.4.2.1 MethodologySemantic similarity was measured using the similarityalgorithm of Pilehvar et al (2013), ADW,3which firstrepresents a given linguistic item (such as a word or aconcept) using random walks over the WordNet seman-tic network, where random walks are initialized fromthe synsets associated with that item.
The similaritybetween two linguistic items is accordingly computedin terms of the similarity of their corresponding repre-sentations.
ADW is an ideal candidate for measuringthe impact of CROWN for two reasons.
First, the algo-rithm obtains state-of-the-art performance on both word-based and sense-based benchmarks using only WordNetas a knowledge source.
Second, the method is both un-supervised and requires no parameter tuning, removingpotential performance differences between WordNet andCROWN being due to these factors.To perform the second experiment, the ADW algo-rithm was used to generate similarity judgments for thedata of Task 3, changing only the underlying semanticnetwork to be either (1) the WordNet 3.0 network, withadditional edges from disambiguated glosses,4or (2) thesame network with novel synsets from CROWN.
As theADW algorithm is unchanged between settings, any per-formance change is due only to the differences betweenthe two networks.
Performance is measured using Pear-son correlation with the gold standard judgments.4.2.2 ResultsOf the 60 OOV lemmas and 38 OOV slang terms inthe test data, 51 and 26 were contained in CROWN, re-spectively.
Table 3 shows the Pearson correlation perfor-mance of ADW in the two settings for all lemmas in thedataset, and for three subsets of the dataset: OOV, slang,and regular lemmas, the latter of which are in Word-Net; the bottom rows show the performance of the Task?sbest participating system for the word-to-sense compar-ison type (Kashyap et al, 2014) and the most competi-3https://github.com/pilehvar/ADW4http://wordnet.princeton.edu/glosstag.shtml1462tive baseline, based on Greedy String Tiling (GST) (Wise,1996).ADW sees large performance improvements in theOOV and slang words when using CROWN insteadof WordNet, which are both statistically significant atp<0.01.
The overall improvement of ADW would placeit as the fifth best system in this comparison type of Task3.
The performance on regular in-WordNet and OOVlemmas is approximately equal, indicating the high ac-curacy of OOV hypernym attachment in CROWN.
No-tably, on OOV and Slang, the unsupervised ADW, whencoupled with the additional information in CROWN , pro-duces competitive results with the best performing sys-tem, which is a multi-feature supervised system utilizingextensive external dictionaries and distributional meth-ods.5 Related WorkMost related is the work of Poprat et al (2008), who at-tempted to automatically build an extension of WordNetwith biomedical terminology; however, they were unsuc-cessful in constructing the resource.
Other work has at-tempted to leverage distributional similarity techniques(Snow et al, 2006) or exploit the structured informationin Wikipedia (Ruiz-Casado et al, 2005; Toral et al, 2008;Ponzetto and Navigli, 2009; Yamada et al, 2011) in orderto extend WordNet with new synsets.
However, structure-based approaches are limited only to the concepts appear-ing in Wikipedia article titles, which almost always corre-spond to noun concepts.
Distributional and probabilisticapproaches are also limited to OOV terms for which it ispossible to gather enough statistics.
As Wiktionary con-tains all parts of speech and our method is independent ofword frequency, neither limitation applies to this work.Other related work has attempted to tap resourcessuch as Wikipedia for automatically constructing new on-tologies (Suchanek et al, 2007; Dandala et al, 2012;Moro and Navigli, 2012; Meyer and Gurevych, 2012),extending existing ones through either alignment-basedmethods (Matuschek and Gurevych, 2013; Pilehvar andNavigli, 2014) or inferring the positions of new sensesby their shared attributes which are extracted from text(Reisinger and Pas?ca, 2009).
Extension and alignmentapproaches based on Wikipedia are limited mainly tonoun concepts in Wikipedia; furthermore, these tech-niques cannot be directly applied to Wiktionary becauseits lack of taxonomic structure would prevent addingmost OOV data to the existing WordNet taxonomy.6 ConclusionThis work has introduced CROWN version 1.0, a new ex-tension of WordNet that merges sense definitions fromWiktionary to add new hypernym and antonym relations.The resulting taxonomy has more than doubled the num-ber of synsets in WordNet and includes many technicaland slang terms, as well as non-standard lexicalizations.CROWN is released in the same format as WordNet5andtherefore is fully compatible with all existing WordNet-based tools and libraries.
Furthermore, the software forbuilding CROWN has been opened-sourced and will beupdated with future versions.
In two experiments wedemonstrated that the CROWN construction process is ac-curate and that the resulting resource has a real benefit toWordNet-based applications.Immediate future work will add support for includingnew lemmas as synonyms in existing synsets and linkingnewly-created synsets with all appropriate types of Word-Net semantic relationship.
Longer-term future work willpursue more sophisticated methods for taxonomy enrich-ment to improve the quality of integrated content and willaim to integrate additional dictionaries, with a special em-phasis on adding domain-specific terminology.ReferencesEneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014.Random walks for knowledge-based Word Sense Disam-biguation.
Computational Linguistics, 40(1):57?84.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani.2010.
SentiWordNet 3.0: An enhanced lexical resource forsentiment analysis and opinion mining.
In Proceedings of theSeventh International Conference on Language Resourcesand Evaluation (LREC), volume 10, pages 2200?2204, Val-letta, Malta.Daniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.
DKProSimilarity: An open source framework for text similarity.
InProceedings of the 51st Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 121?126, Sofia,Bulgaria.Bharath Dandala, Rada Mihalcea, and Razvan Bunescu.
2012.Towards building a multilingual semantic network: Identify-ing interlingual links in wikipedia.
In Proceedings of theFirst Joint Conference on Lexical and Computational Se-mantics (*SEM), pages 30?37, Montreal, Canada.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and SilviaBernardini.
2008.
Introducing and evaluating ukWaC, a verylarge web-derived corpus of English.
In Proceedings of the4th Web as Corpus Workshop (WAC-4), Morocco.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.
Simlex-999: Evaluating semantic models with (genuine) similarityestimation.
arXiv preprint arXiv:1408.3456.David Jurgens, Mohammad Taher Pilehvar, and Roberto Nav-igli.
2014.
Semeval-2014 task 3: Cross-level semantic sim-ilarity.
In Proceedings of the 8th International Workshop onSemantic Evaluation (SemEval-2014), pages 17?26, Dublin,Ireland.5Both the software for creating CROWN and the data itself are avail-able at https://github.com/davidjurgens/crown.1463Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman,Taneeya Satyapanich, Sunil Gandhi, and Tim Finin.
2014.Meerkat mafia: Multilingual and cross-level semantic textualsimilarity systems.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation, pages 416?423, Dublin,Ireland.Christopher D. Manning, Mihai Surdeanu, John Bauer, JennyFinkel, Steven J. Bethard, and David McClosky.
2014.
TheStanford CoreNLP natural language processing toolkit.
InProceedings of 52nd Annual Meeting of the Association forComputational Linguistics: System Demonstrations, pages55?60, Baltimore, Maryland.Michael Matuschek and Iryna Gurevych.
2013.
Dijkstra-WSA: A graph-based approach to word sense alignment.Transactions of the Association for Computational Linguis-tics (TACL), 1:151?164.Christian M. Meyer and Iryna Gurevych.
2011.
What psy-cholinguists know about Chemistry: Aligning Wiktionaryand WordNet for increased domain coverage.
In Proceedingsof the 5th International Joint Conference on Natural Lan-guage Processing, pages 883?892, Chiang Mai, Thailand.Christian M. Meyer and Iryna Gurevych.
2012.
OntoWik-tionary constructing an ontology from the collaborative on-line dictionary Wiktionary.
In Semi-Automatic Ontology De-velopment: Processes and Resources, chapter 6, pages 131?161.
IGI Global.Tristan Miller and Iryna Gurevych.
2014.
WordNet?Wikipedia?Wiktionary: Construction of a three-way align-ment.
In Proceedings of the 9th International Conference onLanguage Resources and Evaluation (LREC), pages 2094?2100, Reykjavik, Iceland.Andrea Moro and Roberto Navigli.
2012.
WiSeNet: Buildinga Wikipedia-based semantic network with ontologized rela-tions.
In Proceedings of the 21st ACM Conference on Infor-mation and Knowledge Management (CIKM), pages 1672?1676, Maui, HI, USA.Roberto Navigli and Simone Paolo Ponzetto.
2012.
BabelNet:The automatic construction, evaluation and application of awide-coverage multilingual semantic network.
Artificial In-telligence, 193:217?250.Roberto Navigli and Paola Velardi.
2010.
Learning Word-ClassLattices for definition and hypernym extraction.
In Proceed-ings of the 48th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 1318?1327.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.2004.
WordNet:: Similarity: measuring the relatednessof concepts.
In Proceedings of Fifth Annual Meeting ofthe North American Chapter of the Association for Com-putational Linguistics (NAACL), pages 38?41, Boston, Mas-sachusetts.Mohammad Taher Pilehvar and Roberto Navigli.
2014.
A ro-bust approach to aligning heterogeneous lexical resources.
InProceedings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (ACL 2014), pages 468?478,Baltimore, Maryland.Mohammad Taher Pilehvar, David Jurgens, and Roberto Nav-igli.
2013.
Align, Disambiguate and Walk: a Unified Ap-proach for Measuring Semantic Similarity.
In Proceedingsof the 51st Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 1341?1351, Sofia, Bulgaria.Simone Paolo Ponzetto and Roberto Navigli.
2009.
Large-scale taxonomy mapping for restructuring and integratingWikipedia.
In Proceedings of the 21st International JointConference on Artificial Intelligence (IJCAI), pages 2083?2088, Pasadena, California, USA.Michael Poprat, Elena Beisswanger, and Udo Hahn.
2008.Building a BioWordNet by using WordNet?s data formatsand WordNet?s software infrastructure: a failure story.
InProceedings of the Workshop on Software Engineering, Test-ing, and Quality Assurance for Natural Language Process-ing, pages 31?39, Columbus, Ohio.Joseph Reisinger and Marius Pas?ca.
2009.
Latent variablemodels of concept-attribute attachment.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume 2, pages620?628, Suntec, Singapore.Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells.2005.
Automatic assignment of Wikipedia encyclopedic en-tries to WordNet synsets.
In Proceedings of the Third Inter-national Conference on Advances in Web Intelligence, pages380?386, Lodz, Poland.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.
Seman-tic Taxonomy Induction from Heterogenous Evidence.
InProceedings of the 21st International Conference on Com-putational Linguistics and the 44th Annual Meeting of theAssociation for Computational Linguistics (COLING-ACL),pages 801?808, Sydney, Australia.Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.2007.
YAGO: A core of semantic knowledge.
unifyingWordNet and Wikipedia.
In Proceedings of the 16th WorldWide Web Conference (WWW), pages 697?706, Banff, Al-berta, Canada.Antonio Toral, Rafael Muoz, and Monica Monachini.
2008.Named Entity WordNet.
In Proceedings of the Sixth Inter-national Conference on Language Resources and Evaluation(LREC), pages 741?747.Peter Turney.
2001.
Mining the web for synonyms: PMI-IRversus LSA on toefl.
In Proceedings of the Twelfth EuropeanConference on Machine Learning (ECML-2001), pages 491?502, London, UK, UK.Giannis Varelas, Epimenidis Voutsakis, Paraskevi Raftopoulou,Euripides GM Petrakis, and Evangelos E Milios.
2005.
Se-mantic similarity methods in WordNet and their applicationto information retrieval on the Web.
In Proceedings of the7th annual ACM international workshop on Web informationand data management, pages 10?16.Michael J.
Wise.
1996.
YAP3: improved detection of similar-ities in computer program and other texts.
In Proceedingsof the twenty-seventh SIGCSE technical symposium on Com-puter science education, pages 130?134, Philadelphia, Penn-sylvania, USA.Ichiro Yamada, Jong-Hoon Oh, Chikara Hashimoto, KentaroTorisawa, Jun?ichi Kazama, Stijn De Saeger, and TakuyaKawada.
2011.
Extending WordNet with hypernyms andsiblings acquired from Wikipedia.
In Proceedings of 5th In-ternational Joint Conference on Natural Language Process-ing, pages 874?882, Chiang Mai, Thailand.Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008.Extracting lexical semantic knowledge from Wikipedia and1464Wiktionary.
In Proceedings of the 6th International Confer-ence on Language Resources and Evaluation (LREC), pages1646?1652, Morocco.1465
