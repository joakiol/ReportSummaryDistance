Proceedings of the SIGDIAL 2014 Conference, pages 318?326,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsThe SJTU System for Dialog State Tracking Challenge 2Kai Sun, Lu Chen, Su Zhu and Kai YuDepartment of Computer Science and Engineering, Shanghai Jiao Tong UniversityShanghai, China{accreator, chenlusz, paul2204, kai.yu}@sjtu.edu.cnAbstractDialog state tracking challenge providesa common testbed for state tracking al-gorithms.
This paper describes the SJTUsystem submitted to the second DialogueState Tracking Challenge in detail.
Inthe system, a statistical semantic parser isused to generate refined semantic hypothe-ses.
A large number of features are thenderived based on the semantic hypothe-ses and the dialogue log information.
Thefinal tracker is a combination of a rule-based model, a maximum entropy and adeep neural network model.
The SJTUsystem significantly outperformed all thebaselines and showed competitive perfor-mance in DSTC 2.1 IntroductionDialog state tracking is important because spo-ken dialog systems rely on it to choose properactions as spoken dialog systems interact withusers.
However, due to automatic speech recog-nition (ASR) and spoken language understanding(SLU) errors, it is not easy for the dialog man-ager to maintain the true state of the dialog.
Inrecent years, much research has been devoted todialog state tracking.
Many approaches have beenapplied to dialog state tracking, from rule-basedto statistical models, from generative models todiscriminative models (Wang and Lemon, 2013;Zilka et al., 2013; Henderson et al., 2013; Leeand Eskenazi, 2013).
Recently, shared researchtasks like the first Dialog State Tracking Challenge(DSTC 1) (Williams et al., 2013) have provideda common testbed and evaluation suite for dialogstate tracking (Henderson et al., 2013).Compared with DSTC 1 which is in the bustimetables domain, DSTC 2 introduces more com-plicated and dynamic dialog states, which maychange through the dialog, in a new domain, i.e.restaurants domain (Henderson et al., 2014).
Foreach turn, a tracker is supposed to output a setof distributions for each of the three componentsof the dialog state: goals, method, and requestedslots.
At a given turn, the goals consists of theuser?s true required value having been revealedfor each slot in the dialog up until that turn; themethod is the way the user is trying to interact withthe system which may be by name, by constraints,by alternatives or finished; and the requested slotsconsist of the slots which have been requested bythe user and not replied by the system.
For evalua-tion in DSTC 2, 1-best quality measured by accu-racy, probability calibration measured by L2, anddiscrimination measured by ROC are selected asfeatured metrics.
Further details can be found inthe DSTC 2 handbook (Henderson et al., 2013).Previous research has demonstrated the effec-tiveness of rule-based (Zilka et al., 2013), maxi-mum entropy (MaxEnt) (Lee and Eskenazi, 2013)and deep neural network (DNN) (Henderson et al.,2013) models separately.
Motivated by this, theSJTU system employs a combination of a rule-based model, a MaxEnt and a DNN model.
Thethree models were first trained (if necessary) onthe training set and tested for each of the threecomponents of the dialog state, i.e goals, method,and requested slots on the development set.
Then,models with the best performance for each of thethree components were selected to form a com-bined model.
Finally, the combined model wasretrained using both training set and developmentset.
Additionally, as the live SLU was found notgood enough with some information lost com-pared with the live ASR, a new semantic parserwas implemented which took the live ASR as in-put and the SJTU system used the result from thenew semantic parser instead of the live SLU.The remainder of the paper is organized as fol-lows.
Section 2 describes the design of the new318semantic parser.
Section 3 presents the rule-basedmodel.
Section 4 describes the statistical mod-els including the maximum entropy model and thedeep neural network model.
Section 5 shows anddiscusses the performance of the SJTU system.
Fi-nally, section 6 concludes the paper.2 Semantic ParserIt was found that the live SLU provided by the or-ganisers has poor quality.
Hence, a new statisticalsemantic parser is trained to parse the live ASRhypotheses.2.1 Semantic Tuple ClassifierThe semantics of an utterance is represented infunctor form called dialogue act consisting of adialogue act type and a list of slot-value pairs, forexample:request(name,food=chinese)where ?request?
is the dialogue act type,?name?is a slot requested and ?food=chinese?
is aslot-value pair which provides some informa-tion to the system.
In DSTC 2, there aremany different dialogue act types (e.g.
?request?,?inform?, ?deny?, etc) and different slot-valuepairs (e.g.
?food=chinese?, ?pricerange=cheap?,?area=center?, etc), which are all called semanticitems.A semantic tuple (e.g.
act type, type-slot pair,slot-value pair) classifier (STC) approach devel-oped by Mairesse et al.
( 2009) is used in the SJTUsystem.
It requires a set of SVMs to be trained onn-gram features from a given utterance: a multi-class SVM is used to predict the dialogue act type,and a binary SVM is used to predict the exis-tence of each slot-value pair.
Henderson et al.
(2012) improved this method with converting theSVM outputs to probabilities, and approximatingthe probability of a dialogue-act d of type d-typejwith a set of slot-value pairs S by:P (d|u) = P (d-typej|u)?sv?SP (sv|u)?sv/?S(1?
P (sv|u)) (1)where u denotes an utterance and sv runs over allpossible slot-value pairs.2.2 Dialogue Context FeaturesIn addition to the n-gram feature used in the orig-inal STC parser, the dialogue context can be ex-ploited to constrain the semantic parser.
In DSTC2, the dialogue context available contains the his-tory information of user?s ASR hypotheses, thesystem act and the other output of system (e.g.whether there is a barge-in from the user or not,the turn-index) and so on.
In the SJTU system,the context features from the last system act (in-dicators for all act types and slot-value pairs onwhether they appear), an indicator for ?barge-in?and the reciprocal of turn-index are combined withthe original n-gram feature to be the final featurevector.2.3 Generating Confidence ScoresFor testing and predicting the dialogue act, the se-mantic parser is applied to each of the top N ASRhypotheses hi, and the set of results Diwith midistinct dialogue act hypotheses would be mergedin following way:P (d|o) =N?i=1{p(hi|o)p(d|hi) if d ?
Di0 otherwisewhere o is the acoustic observation, d runs overeach different dialogue act in Di, i = 1, ..., N ,p(hi|o) denotes the ASR posterior probability ofthe i-th hypothesis, p(d|hi) denotes the semanticposterior probability given the i-th ASR hypoth-esis as in equation (1).
Finally, a normalizationshould be done to guarantee the sum of P (d|o) tobe one.2.4 ImplementationThe STCs-based semantic parser is implementedwith linear kernel SVMs trained using the Lib-SVM package (Chang and Lin, 2011).
The SVMmisclassification cost parameters are optimised in-dividually for each SVM classifier by performingcross-validations on the training data.3 Rule-based ModelIn this section, the rule-based model which isslightly different from the focus tracker (Hender-son et al., 2013) and HWU tracker (Wang, 2013) isdescribed.
The idea of the rule-based model is tomaintain beliefs based on basic probability opera-tions and a few heuristic rules that can be observedon the training set.
In the following the rule-basedmodel for joint goals, method and requested slotsare described in detail.3193.1 Joint GoalsFor slot s, the i-th turn and value v, p+s,i,v(p?s,i,v) isused to denote the sum of all the confidence scoresassigned by the SLU to the user informing or af-firming (denying or negating) the value of slot s isv.
The belief of ?the value of slot s being v in thei-th turn?
denoted by bs,i,vis defined as follows.?
If v 6= ?None?,bs,i,v= (bs,i?1,v+ p+s,i,v(1?
bs,i?1,v))(1?
p?s,i,v??v?6=vp+s,i,v?)?
Otherwise,bs,i,v= 1?
?v?6=?None?bs,i,v?In particular, when i = 0, bs,0,v= 1 if v =?None?, otherwise 0.
The motivation here comesfrom HWU tracker (Wang, 2013) that only p+s,?,vpositively contributes to the belief of slot s beingv, and both p+s,?,v?
(v?6= v) and p?s,?,vcontribute tothe belief negatively.3.2 MethodFor the i-th turn, pi,mis used to denote the sum ofall the confidence scores assigned by the SLU tomethod m. Then the belief of ?the method beingm in the i-th turn?
denoted by bi,mis defined asfollows.?
If m 6= ?none?,bi,m= bi?1,m(1??m?6=?none?pi,m?)
+ pi,m?
Otherwise,bi,m= 1?
?m?6=?none?bi,m?In particular, b0,m= 0 when i = 0 and m 6=?none?.
An explanation of the above formulais given by Zilka et al.
(2013).
The idea is alsoadopted by the focus tracker (Henderson et al.,2013).3.3 Requested SlotsFor the i-th turn and slot r, pi,ris used to denotethe sum of all the confidence scores assigned bythe SLU to r is one of the requested slots.
Thenthe belief of ?r being one of the requested slots inthe i-th turn?
denoted by bi,ris defined as follows.?
If i = 1, or system has at least one ofthe following actions: ?canthelp?, ?offer?,?reqmore?, ?confirm-domain?, ?expl-conf?,?bye?, ?request?,bi,r= pi,r?
Otherwise,bi,r= bi?1,r(1?
pi,r) + pi,rThis rule is a combination of the idea of HWUtracker (Wang, 2013) and an observation from thelabelled data that once system has some certain ac-tions, the statistics of requested slots from the pastturn should be reset.4 Statistical ModelIn this section, two statistical models, one is theMaxEnt model, the other is the DNN model, aredescribed.4.1 FeaturesThe performance of statistical models is highly de-pendent on the feature functions.Joint GoalsFor slot s, the i-th turn and value v, the featurefunctions designed for joint goals are listed below.?
f1, inform(s, i, v) = the sum of all thescores assigned by the SLU to the user in-forming the value of slot s is v.?
f2, affirm(s, i, v) = the sum of all thescores assigned by the SLU to the user af-firming the value of slot s is v.?
f3, pos(s, i, v) = inform(s, i, v) +affirm(s, i, v).?
f4, deny(s, i, v) = the sum of all the scoresassigned by the SLU to the user denying thevalue of slot s is v.?
f5, negate(s, i, v) = the sum of all thescores assigned by the SLU to the user negat-ing the value of slot s is v.320?
f6, neg(s, i, v) = deny(s, i, v) +negate(s, i, v).?
f7, acc(s, i, v) = pos(s, i, v)?neg(s, i, v).?
f8, rule(s, i, v) = the confidence scoregiven by the rule-based model.?
f9, rank inform(s, i, v) = the sum of allthe reciprocal rank of the scores assigned bythe SLU to the user informing the value ofslot s is v, or 0 if informing v cannot be foundin the SLU n-best list.?
f10, rank affirm(s, i, v) = the sum of allthe reciprocal rank of the scores assigned bythe SLU to the user affirming the value of slots is v, or 0 if affirming v cannot be found inthe SLU n-best list.?
f11, rank+(s, i, v) =rank inform(s, i, v) +rank affirm(s, i, v).?
f12, rank deny(s, i, v) = the sum of all thereciprocal rank of the scores assigned by theSLU to the user denying the value of slot sis v, or 0 if denying v cannot be found in theSLU n-best list.?
f13, rank negate(s, i, v) = the sum of allthe reciprocal rank of the scores assigned bythe SLU to the user negating the value of slots is v, or 0 if negating v cannot be found inthe SLU n-best list.?
f14, rank?
(s, i, v) = rank deny(s, i, v)+rank negate(s, i, v).?
f15, rank(s, i, v) = rank+(s, i, v) ?rank?
(s, i, v).?
f16, max(s, i, v) = the largest score givenby SLU to the user informing, affirming,denying, or negating the value of slot s is vfrom the 1-st turn.?
f17, rest(s, i, v) = 1 if v = ?None?, oth-erwise 0.?
f18, pos(s, i, v) =?k=1?ipos(s,k,v)i, whichis the arithmetic mean of pos(s, ?, v) from the1-st turn to the i-th turn.
Similarly, f19,neg(s, i, v), f20, rank+(s, i, v) and f21,rank?
(s, i, v) are defined.?
f22, (f22,1, f22,2, ?
?
?
, f22,10), wheref22,j, bin pos(s, i, v, j) =totpos(s,i,v,j)Z,where totpos(s, i, v, j) = the total numberof slot-value pairs from the 1-st turn to thei-th turn with slot s and value v whichwill fall in the j-th bin if the range ofconfidence scores is divided into 10 bins,and Z =?k?i,1?j?
?10,v?totpos(s, k, v?, j?
),which is the normalization factor.
Simi-larly, f23, (f23,1, f23,2, ?
?
?
, f23,10) wheref23,j, bin neg(s, i, v, j) is defined.?
f24, (f24,1, f24,2, ?
?
?
, f24,10).
Wheref24,j, bin rule(s, i, v, j) =totrule(s,i,v,j)Z,where totrule(s, i, v, j) = the total numberof rule(s, ?, v) from the 1-st turn to the i-th turn which will fall in the j-th bin if therange of rule(?, ?, ?)
is divided into 10 bins,and Z =?k?i,1?j?
?10,v?totrule(s, k, v?, j?
),which is the normalization factor.
Simi-larly, f25, (f25,1, f25,2, ?
?
?
, f25,10) wheref25,j, bin rank(s, i, v, j), and f26,(f26,1, f26,2, ?
?
?
, f26,10) where f26,j,bin acc(s, i, v, j) are defined.?
f27, (f27,1, f27,2, ?
?
?
, f27,10).
Wheref27,j, bin max(s, i, v, j) = 1 ifmax(s, i, v) will fall in the j-th bin ifthe range of confidence scores is divided into10 bins, otherwise 0.?
f28, (f28,1, f28,2, ?
?
?
, f28,17).
Wheref28,j, user acttype(s, i, v, uj) = the sumof all the scores assigned by the SLU to theuser act type being uj(1 ?
j ?
17).
Thereare a total of 17 different user act types de-scribed in the handbook of DSTC 2 (Hender-son et al., 2013).?
f29, (f29,1, f29,2, ?
?
?
, f29,17).
Wheref29,j, machine acttype(s, i, v,mj) = thenumber of occurrences of act type mj(1 ?j ?
17) in machine act.
There are a total of17 different machine act types described inthe handbook of DSTC 2 (Henderson et al.,2013).?
f30, canthelp(s, i, v) = 1 if the system can-not offer a venue with the constrain s = v,otherwise 0.?
f31, slot confirmed(s, i, v) = 1 if the sys-tem has confirmed s = v, otherwise 0.321?
f32, slot requested(s, i, v) = 1 if the sys-tem has requested the slot s, otherwise 0.?
f33, slot informed(s, i, v) = 1 if the sys-tem has informed s = v, otherwise 0.?
f34, bias(s, i, v) = 1.In particular, all above feature function are 0when i ?
0.MethodFor the i-th turn and method m, the feature func-tions designed for method are listed below.?
f1, slu(i,m) = the sum of all the scoresassigned by the SLU to the method being m.?
f2, rank(i,m) = the sum of all the recip-rocal rank of the scores assigned by the SLUto the method being m.?
f3, rule(i,m) = the confidence score givenby the rule-based model.?
f4, slu(i,m) =?ik=1slu(k,m)i, which isthe arithmetic mean of slu(?,m) from the1-st turn to the i-th turn.
Similarly, f5,rank(i,m) and f6, rule(i,m) are defined.?
f7, score name(i) = the sum of all thescores assigned by the SLU to the user in-forming the value of slot name is somevalue.?
f8, venue offered(i) = 1 if at least onevenue has been offered to the user by the sys-tem from the 1-st turn to the i-th turn, other-wise 0.?
f9, (f9,1, f9,2, ?
?
?
, f9,17).
Where f9,j,user acttype(i, uj) = the sum of all thescores assigned by the SLU to the user acttype being uj(1 ?
j ?
17).?
f10, bias(i) = 1.In particular, all above feature function are 0when i ?
0.Requested SlotsFor the i-th turn and slot r, the feature functionsdesigned for requested slots are listed below.?
f1, slu(i, r) = the sum of all the scoresassigned by the SLU to r being one of therequested slots.?
f2, rank(i, r) = the sum of all the recipro-cal rank of the scores assigned by the SLU tor being one of the requested slots.?
f3, rule(i, r) = the confidence score givenby the rule-based model.?
f4, bias(i, r) = 1In particular, all above feature function are 0when i ?
0.4.2 Maximum Entropy ModelTotal 6 MaxEnt models (Bohus and Rudnicky,2006) are employed, four models for the jointgoals, one for the method and one for the re-quested slots.
The Maximum Entropy model is anefficient means that models the posterior of classy given the observations x:P (y|x) =1Z(x)exp (?Tf(y,x))Where Z(x) is the normalization constant.
?
isthe parameter vector and f(y,x) is the featurevector.The models for the joint goals are implementedfor four informable slots (i.e.
area, food, nameand pricerange) separately.
In the k-th turn, forevery informable slot s and its value v, i.e.
slot-value pair in SLU, the MaxEnt model for the cor-responding slot is used to determine whether thevalue v for the slot s in the user goals is right ornot.
The input consists of 160 features1whichare selected from the feature functions describedin section 4.1 Joint Goals:{f34}i=k?
?k?2?i?k{f1, ?
?
?
, f15,f28, ?
?
?
, f33}Where i is the turn index .
The output of the modelis the confidence score that the value v for the slots is right.In the k-th turn, the model for the method isused to determine which way the user is trying tointeract with the system.
The input consists of 97features which are selected from the feature func-1For the feature function whose range is not 1 dimen-sion, the number of features defined by the feature functionis counted as the number of dimensions rather than 1.
Forexample, the number of features defined by f28is 17.322tions described in section 4.1 Method:{f10}i=k?
?k?3?i?k{f7, f8,f9}?
?mk ?
3 ?
i ?
k{f3}and the output consists of five confidence scoresthat the method belongs to every one of the fiveways (i.e.
by name, by constraints, by alternatives,finished and none).The model for the requested slots is used to de-termine whether the requestable slot r in the SLU?request(slot)?
is truly requested by the user or notin the k-th turn.
The input consists of 10 featureswhich are selected from the feature functions de-scribed in section 4.1 Requested Slots:{f4}i=k?
?k?2?i?k{f1, f2, f3}and the output is the confidence score that r is trulyrequested by the user in this turn.The parameters of the 6 MaxEnt models are op-timised separately through maximizing the likeli-hood of the training data.
The training process isstopped when the likelihood change is less than10?4.4.3 Deep Neural Network Model4 DNNs for joint goals (one for each slot), 1 formethod and 1 for requested slots are employed.All of them have similar structure with Sigmoidfor hidden layer activation and Softmax for out-put layer activation.
As shown in figure 1, eachDNN has 3 hidden layers and each layer has 64nodes.
DNNs take the feature set (which will bedescribed in detail later) of a certain value of goal,method, or requested slots as the input, then out-put two values (donated by X and Y ), through thehidden layer processing, and finally the confidenceof the value can be got byeXeX+eY.For slot s, the k-th turn and value v, the featureset of goal consisting of 108 features is defined as:?k?5?i?k{f3, f6, f7, f8, f11, f14, f15}?
{f18, ?
?
?
, f21}i=k?6?
{f16, f17,f22, ?
?
?
,f27}i=kFor the k-th turn and method m, the feature setof method consisting of 15 features is defined as:?k?3?i?k{f1, f2, f3} ?
{f4, f5, f6}i=k?4An input layerwith |feature_set| nodes3 hidden layersEach has 64 nodesAn output layerwith 2 nodes???
?Figure 1: Structure of the DNN ModelFor the k-th turn and slot r, the feature set ofrequested slots consisting of 12 features is definedas:?k?3?i?k{f1, f2, f3}Bernoulli-Bernoulli RBM was applied to pre-train DNNs and Stochastic Gradient Descent withcross-entropy criterion to fine-tune DNNs.
For thefine-tuning process, 3/4 of the data was used fortraining and 1/4 for validation.5 ExperimentsDSTC 2 provides a training dataset of 1612 dia-logues (11677 utterances) and a development setof 506 dialogues (3934 utterances).
The trainingdata was first used to train the semantic parserand the MaxEnt and the DNN models for internalsystem development as shown in section 5.1 and5.2.
These systems were tested on the develop-ment data.
Once the system setup and parameterswere determined, the training and development setwere combined together to train the final submit-ted system.
The final system was then tested onthe final evaluation data as shown in section 5.3.5.1 Effect of the STC Semantic ParserIn DSTC 2, as the live semantic information wasfound to be poor, two new semantic parsers werethen trained as described in section 2.
One usedthe top ASR hypothesis n-gram features and theother one employed additional system feedbackfeatures (the last system act, ?barge-in?
and turn-index).Table 1 shows the performance of two new se-mantic parser in terms of the precision, recall,323System Precision Recall F-score ICEbaseline 0.6659 0.8827 0.7591 2.18501-best 0.7265 0.8894 0.7997 1.4529+ sys fb 0.7327 0.8969 0.8065 1.3449Table 1: Performance of semantic parsers with dif-ferent features on the development set.F-score of top dialogue act hypothesis and theItem Cross Entropy (ICE) (Thomson et al., 2008)which measures the overall quality of the confi-dences distribution of semantic items (the less thebetter).
The baseline is the original live seman-tic hypotheses, ?1-best?
(row 3) represents the se-mantic parser trained on the top ASR hypothesiswith n-gram feature, and ?sys fb?
(row 4) rep-resents the semantic parser added with the sys-tem feedback features.
The STC semantic parserssignificantly improve the quality of semantic hy-potheses compared with baseline in the score ofprecision, recall, F-score and ICE.
And the parserusing context features (row 4) scored better thanthe other one (row 3).The improved semantic parsers are expected toalso yield better performance in dialogue statetracking.
Hence, the parsers were used in focusbaseline provided by the organiser.
As shown inJoint Goals Method Requestedbaseline 0.6121 0.8303 0.89361-best 0.6613 0.8764 0.8987+ sys fb 0.6765 0.8764 0.9297Table 2: Results for focus baseline tracker withdifferent parserstable 2, the new parsers achieved consistent im-provement on the accuracy of joint goals, methodand requested slots.
So the semantic hypothesesof parser using the system feedback features wasused for later development.5.2 Internal System DevelopmentTable 3 shows the the results of rule-based model,the MaxEnt model and the DNN model on the de-velopment set.
From the table we can see thatthe DNN model has the best performance for jointgoals, the MaxEnt model has the best performancefor method and the rule-based model has the bestperformance for requested slots.
So the combinedmodel is a combination of those three models, onefor one of the three components where it has thebest performance, that is, the rule-based modelfor requested slots, the MaxEnt model for method,and the DNN model for joint goals.Joint Goals Method RequestedRule-based 0.6890 0.8955 0.9668MaxEnt 0.6741 0.9079 0.9665DNN 0.6906 0.8991 0.9661Table 3: Performance of three tracking models5.3 Evaluation PerformanceThe official results of the challenge are publiclyavailable and the SJTU team is team 7.
Entry0, 1, 2, 3 of team 7 is the combined model, therule-based model, the DNN model and the Max-Ent model respectively.
They all used the new se-mantic parser based on live ASR hypotheses.
En-try 4 of team 7 is also a combined model but itdoes not use the new semantic parser and takes thelive SLU as input.Table 4 shows the results on the final evalua-tion test set.
As expected, the semantic parser doeswork, and the combined model has the best perfor-mance for joint goals and method, however, thatis not true for requested slots.
Notice that on thedevelopment set, the difference of the accuracy ofrequested slots among the 3 models is significantlysmaller than that of joint goals and method.
Onereasonable explanation is that one cannot claimthat the rule-based model has better performancefor requested slots than the MaxEnt model and theDNN model only with an accuracy advantage lessthan 0.1%.Joint Goals Method RequestedBaseline 0.6191 0.8788 0.8842Focus 0.7193 0.8670 0.8786HWU 0.7108 0.8971 0.8844HWU+ 0.6662 0.8846 0.8830Rule-based 0.7387 0.9207 0.9701MaxEnt 0.7252 0.9357 0.9717DNN 0.7503 0.9287 0.9710Combined+ 0.7503 0.9357 0.9701Combined- 0.7346 0.9102 0.9458Table 4: Accuracy of the combined model (Com-bined+) compared with the rule-based model,the MaxEnt model, the DNN model, the com-bined model without the new semantic parser(Combined-) and four baselines on the test set.Four baselines are the baseline tracker (Base-line), the focus tracker (Focus), the HWU tracker(HWU) and the HWU tracker with ?original?
flagset to (HWU+) respectively.Figure 2 summaries the performance of the ap-proach relative to all 31 entries in the DSTC 2.32400.10.20.30.40.50.60.70.80.9MaxMinMedianQuartileSJ T U0.480.520.560.60.640.680.720.760.80.840.8 80.920.961MaxMinMedianQuartileSJTUJoint  Goals        Method      Requested Slots  Joint  Goals        Method      Requested Slots(a) Accuracy  (b) L2Figure 2: Performance of the combined model among 31 trackers.
SJTU is the combined model (entry 0of team 7).As ROC metric is only comparable between sys-tems of similar accuracy, only accuracy and L2are compared.
The results of the combined modelis competitive for all the three components, espe-cially for joint goals.5.4 Post Evaluation AnalysisTwo strategies and two kinds of features wereadded to the MaxEnt model for the requested slotsafter DSTC 2 based on some observations on thetraining set and development set.
The first strategyis that the output for the requested slots of the firstturn is set to empty by force.
The second strategyis that the output of the confidence is additionallymultiplied by (1?Cf), where Cfdenotes the con-fidence given by the MaxEnt model to the methodof current turn being finished.
As for the two kindsof features, one is the slot indicator and the otheris the acttype-slot tuple.
They are defined as2:?
f5, (f5,1, f5,2, ?
?
?
, f5,8), where f5,j,slot indicator(i, r, sj) = 1 if the index of theslot r is j, i.e.
sj= r, otherwise 0.?
f6, (f6,1, f6,2, ?
?
?
, f6,33), where f6,j,user act slot(i, r, tj) = the sum of all thescores assigned by the SLU to the j-th useracttype-slot tuple tj.
The acttype-slot tuple isthe combination of dialog act type and possi-ble slot, e.g.
inform-food, confirm-area.There are 33 user acttype-slot tuples.2The feature number is consistent with that in section 4.1.?
f7, (f7,1, f7,2, ?
?
?
, f7,46), where f7,j,sys act slot(i, r, tj) = the number of occur-rences of the j-th machine acttype-slot tupletjin the dialog acts.
There are 46 machineacttype-slot tuples.With those strategies and features, the Max-Ent model achieved an accuracy of 0.9769 for therequested slots, which is significantly improvedcompared with the submitted system.6 ConclusionThis paper describes the SJTU submission forDSTC 2 in detail.
It is a combined system con-sisting of a rule-based model, a maximum entropymodel and a deep neural network model with aSTC semantic parser.
The results show that theSJTU system is competitive and outperforms mostof the other systems in DSTC 2 on test datasets.Post evaluation analysis reveal that there is stillroom for improvement by refining the features.AcknowledgmentsThis work was supported by the Program for Pro-fessor of Special Appointment (Eastern Scholar)at Shanghai Institutions of Higher Learning andthe China NSFC project No.
61222208.ReferencesBlaise Thomson, Kai Yu, Milica Gasic, Simon Keizer,Francois Mairesse, Jost Schatzmann and Steve325Young.
2008.
Evaluating semantic-level confi-dence scores with multiple hypotheses.
In INTER-SPEECH, pp.
1153-1156.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology(TIST), 2(3), 27.Dan Bohus and Alex Rudnicky.
2006.
A K-hypotheses+ Other Belief Updating Model.
In Proc.
of AAAIWorkshop on Statistical and Empirical Approachesfor Spoken Dialogue Systems.Franc?ois Mairesse, Milica Gasic, Filip Jurc?
?cek, SimonKeizer, Blaise Thomson, Kai Yu and Steve Young.2009.
Spoken language understanding from un-aligned data using discriminative classification mod-els.
Acoustics, Speech and Signal Processing, 2009.ICASSP 2009.
IEEE International Conference on,pp.
4749-4752.
IEEE.Jason Williams, Antoine Raux, Deepak Ramachandranand Alan Black.
2013.
The Dialog State TrackingChallenge.
In SIGDIAL.Lukas Zilka, David Marek, Matej Korvas and Filip Ju-rcicek.
2013.
Comparison of Bayesian Discrim-inative and Generative Models for Dialogue StateTracking.
In SIGDIAL.Matthew Henderson, Milica Gasic, Blaise Thomson,Pirros Tsiakoulis, Kai Yu and Steve Young.
2012.Discriminative spoken language understanding us-ing word confusion networks.
Spoken LanguageTechnology Workshop (SLT), 2012 IEEE, pp.
176-181.
IEEE.Matthew Henderson, Blaise Thomson and SteveYoung.
2013.
Deep Neural Network Approach forthe Dialog State Tracking Challenge.
In SIGDIAL.Matthew Henderson, Blaise Thomson and JasonWilliams.
2013.
Dialog State Tracking Challenge2 & 3.
Technical report, University of Cambridge.Matthew Henderson, Blaise Thomson and JasonWilliams.
2014.
The Second Dialog State Track-ing Challenge.
In SIGDIAL.Sungjin Lee and Maxine Eskenazi.
2013.
Recipe ForBuilding Robust Spoken Dialog State Trackers: Di-alog State Tracking Challenge System Description.In SIGDIAL.Zhuoran Wang and Oliver Lemon.
2013.
A Simpleand Generic Belief Tracking Mechanism for the Di-alog State Tracking Challenge: On the believabilityof observed information.
In SIGDIAL.Zhuoran Wang.
2013.
HWU Baseline Belief Trackerfor DSTC 2 & 3.
Technical report, Heriot-Watt Uni-versity.326
