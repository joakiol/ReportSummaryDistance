Coling 2010: Poster Volume, pages 267?275,Beijing, August 2010Exploring the Data-Driven Prediction of Prepositions in EnglishAnas Elghafari Detmar Meurers Holger WunschSeminar fu?r SprachwissenschaftUniversita?t Tu?bingen{aelgafar,dm,wunsch}@sfs.uni-tuebingen.deAbstractPrepositions in English are a well-knownchallenge for language learners, and thecomputational analysis of preposition us-age has attracted significant attention.Such research generally starts out by de-veloping models of preposition usage fornative English based on a range of fea-tures, from shallow surface evidence todeep linguistically-informed properties.While we agree that ultimately a com-bination of shallow and deep features isneeded to balance the preciseness of ex-emplars with the usefulness of generaliza-tions to avoid data sparsity, in this paperwe explore the limits of a purely surface-based prediction of prepositions.Using a web-as-corpus approach, we in-vestigate the classification based solely onthe relative number of occurrences for tar-get n-grams varying in preposition usage.We show that such a surface-based ap-proach is competitive with the publishedstate-of-the-art results relying on complexfeature sets.Where enough data is available, in a sur-prising number of cases it thus is possibleto obtain sufficient information from therelatively narrow window of context pro-vided by n-grams which are small enoughto frequently occur but large enoughto contain enough predictive informationabout preposition usage.1 IntroductionThe correct use of prepositions is a well-knowndifficulty for learners of English, and correspond-ingly the computational analysis of prepositionusage has attracted significant attention in re-cent years (De Felice and Pulman, 2007; De Fe-lice, 2008; Lee and Knutsson, 2008; Gamon etal., 2008; Chodorow et al, 2007; Tetreault andChodorow, 2008a, 2008b).As a point of reference for the detection ofpreposition errors in learner language, most ofthe research starts out by developing a model ofpreposition usage for native English.
For thispurpose, virtually all previous approaches em-ploy a machine learning setup combining a rangeof features, from surface-based evidence to deeplinguistically-informed properties.
The overalltask is approached as a classification problemwhere the classes are the prepositions and the in-stances to be classified are the contexts, i.e., thesentences with the prepositions omitted.A focus of the previous literature is on the ques-tion which linguistic and lexical features are thebest predictors for preposition usage.
Linguisticfeatures used include the POS tags of the sur-rounding words, PP attachment sites, WordNetclasses of PP object and modified item.
Lexicalfeatures used include the object of the PP and thelexical item modified by the PP.
Those syntactic,semantic and lexical features are then extractedfrom the training instances and used by the ma-chine learning tool to predict the missing preposi-tion in a test instance.While we agree that ultimately a combinationof shallow and linguistically informed features isneeded to balance the preciseness of exemplars267with the usefulness of generalizations to avoiddata sparsity problems, in this paper we want toexplore the limits of a purely surface-based pre-diction of prepositions.
Essentially, our ques-tion is how much predictive information can befound in the immediate distributional context ofthe preposition.
Is it possible to obtain n-gramcontexts for prepositions which are small enoughto occur frequently enough in the available train-ing data but large enough to contain enough pre-dictive information about preposition usage?This perspective is related to that underlyingthe variation-n-gram approach for detecting errorsin the linguistic annotation of corpora (Dickin-son and Meurers, 2003; Dickinson and Meurers,2005; Boyd et al, 2008).
Under that approach, er-rors in the annotation of linguistic properties (lexi-cal, constituency, or dependency information) aredetected by identifying units which recur in thecorpus with sufficient identical context so as tomake variation in their annotation unlikely to becorrect.
In a sense, the recurring n-gram contextsare used as exemplar references for the local do-mains in which the complex linguistic propertiesare established.
The question now is to what ex-tent basic1 n-gram contexts can also be success-fully used to capture the linguistic properties andrelations determining preposition usage, explor-ing the trade-off expressed in the question endingthe previous paragraph.To address this question, in this paper we makeuse of a web-as-corpus approach in the spirit ofLapata and Keller (2005).
We employ the Yahoosearch engine to investigate a preposition classifi-cation setup based on the relative number of webcounts obtained for target n-grams varying in thepreposition used.
We start the discussion with abrief review of key previous approaches and theresults they obtain for the preposition classifica-tion task in native English text.
In section 2,we then describe the experimental setup we used1While Dickinson and Meurers (2005) also employ dis-continuous n-grams, we here focus only on contiguous n-gram contexts.
Using discontinuous n-gram contexts forpreposition prediction could be interesting to explore in thefuture, once, as a prerequisite for the effective generationof discontinuous n-grams, heuristics have been identified forwhen which kind of discontinuities should be allowed to arisefor preposition classification contexts.for our exploration and discuss our results in sec-tion 3.1.1 Previous work and resultsThe previous work on the preposition predictiontask varied in i) the features selected, ii) the num-ber of prepositions tackled, and iii) the trainingand testing corpora used.De Felice (2008) presents a system that (amongother things) is used to predict the correct prepo-sition for a given context.
The system tackles thenine most frequent prepositions in English: of, to,in, for, on, with, at, by, from.
The approach uses awide variety of syntactic and semantic features:the lexical item modified by the PP, the lexicalitem that occurs as the object of the preposition,the POS tags of three words to the left and threewords to the right of the preposition, the grammat-ical relation that the preposition is in with its ob-ject, the grammatical relation the preposition is inwith the word modified by the PP, and the Word-Net classes of the preposition?s object and the lex-ical item modified by the PP.
De Felice (2008) alsoused a named entity recognizer to extract general-izations about which classes of named entities canoccur with which prepositions.
Further, the verbs?subcategorization frames were taken as features.For features that used lexical sources (WordNetclasses, verbs subcategorization frames), only par-tial coverage of the training and testing instancesis available.The overall accuracy reported by De Felice(2008) for this approach is 70.06%, testing on sec-tion J of the British National Corpus (BNC) aftertraining on the other sections.
As the most exten-sive discussion of the issue, using an explicit setof prepositions and a precisely specified and pub-licly accessible test corpus, De Felice (2008) iswell-suited as a reference approach.
Correspond-ingly, our study in this paper is based on the sameset of prepositions and the same test corpus.Gamon et al (2008) introduce a system for thedetection of a variety of learner errors in non-native English text, including preposition errors.For the preposition task, the authors combine theoutputs of a classifier and a language model.
Thelanguage model is a 5-gram model trained on theEnglish Gigaword corpus.
The classifier is trained268on Encarta encyclopedia and Reuters news text.It operates in two stages: The presence/absenceclassifier predicts first whether a preposition needsto be inserted at a given location.
Then, the choiceclassifier determines which preposition is to be in-serted.
The features that are extracted for eachpossible insertion site come from a six-token win-dow around the possible insertion site.
Those fea-tures are the relative positions, POS tags, and sur-face forms of the tokens in that window.
Thechoice classifier predicts one of 13 prepositions:in, for, of, on, to, with, at, by, as, from, since,about, than, and other.
The accuracy of the choiceclassifier, the part of the system to which the workat hand is most similar, is 62.32% when tested ontext from Encarta and Reuters news.Tetreault and Chodorow (2008a) present a sys-tem for detecting preposition errors in learner text.Their approach extracts a total of 25 features fromthe local contexts: the adjacent words, the headsof the nearby phrases, and the POS tags of allthose.
They combine word-based features withPOS tag features to better handle cases where aword from the test instance has not been seenin training.
For each test instance, the systempredicts one of 34 prepositions.
In training andtesting performed on the Encarta encyclopedia,Reuters news text and additional training materialan accuracy figure of 79% is achieved.Bergsma et al (2009) extract contextual fea-tures from the Google 5-gram corpus to train anSVM-based classifier for predicting prepositions.They evaluate on 10 000 sentences taken from theNew York Times section of the Gigaword corpus,and achieve an accuracy of 75.4%.Following De Felice (2008, p. 66), we summa-rize the main results of the mentioned approachesto preposition prediction for native text in Fig-ure 1.2 Since the test sets and the prepositions tar-geted differ between the approaches, such a com-parison must be interpreted with caution.
In termsof the big picture, it is useful to situate the resultswith respect to the majority baseline reported byDe Felice (2008).
It is obtained by always choos-ing of as the most common preposition in sectionJ of the BNC.
De Felice also reports another inter-2The Gamon et al (2008) result differs from the one re-ported in De Felice (2008); we rely on the original paper.esting figure included in Figure 1, namely the ac-curacy of the human agreement with the originaltext, averaged over two English native-speakers.Approach AccuracyGamon et al (2008) 62.32%Tetreault and Chodorow (2008a) 79.00%Bergsma et al (2009) 75.50%De Felice (2008) system 70.06%Majority baseline (of) 26.94%Human agreement 88.60%Figure 1: Preposition prediction results2 Experiments2.1 DataAs our test corpus, we use section J of the BNC,the same corpus used by De Felice (2008).
Basedon the tokenization as given in the corpus, wejoin the tokens with a single space, which alsomeans that punctuation characters end up as sep-arate, white-space separated tokens.
We select allsentences that contain one or more prepositions,using the POS annotation in the corpus to iden-tify the prepositions.
The BNC is POS-annotatedwith the CLAWS-5 tagset, which distinguishes thetwo tags PRF for of and PRP for all other preposi-tions.3 We mark every occurrence of these prepo-sition tags in the corpus, yielding one predictiontask for each marked preposition.
For example,the sentence (1) yields four prediction tasks, onefor each of the prepositions for, of, from, and in inthe sentence.
(1) But for the young, it is rather a questionof the scales falling from their eyes, andhaving nothing to believe in any more.In each task, one preposition is masked usingthe special marker -*-MASKED-*-.
Figure 2shows the four marked-up prediction tasks result-ing for example (1).Following De Felice (2008), we focus our ex-periments on the top nine prepositions in theBNC: of, to, in, for, on, with, at, by, from.
For3http://www.natcorp.ox.ac.uk/docs/URG/posguide.html#guidelines269But -*-MASKED-*-for the young , it israther a question of the scales fallingfrom their eyes , and having nothing tobelieve in any more .But for the young , it is rather aquestion -*-MASKED-*-of the scalesfalling from their eyes , and havingnothing to believe in any more .But for the young , it is rathera question of the scales falling-*-MASKED-*-from their eyes , and havingnothing to believe in any more .But for the young , it is rather aquestion of the scales falling fromtheir eyes , and having nothing tobelieve -*-MASKED-*-in any more .Figure 2: Four prediction tasks for example (1)each occurrence of these nine prepositions in sec-tion J of the BNC, we extract one prediction task,yielding a test set of 522 313 instances.Evaluating on this full test set would involve aprohibitively large number of queries to the Ya-hoo search engine.
We therefore extract a ran-domly drawn subset of 10 000 prediction tasks.From this subset, we remove all prediction taskswhich are longer than 4000 characters in length,as Yahoo only supports queries up to that length.Finally, in a web-as-corpus setup, the indexing ofthe web pages performed by the search engine es-sentially corresponds to the training step in a typi-cal machine learning setup.
In order to avoid test-ing on the training data, we thus need to ensurethat the test cases are based on text not indexed bythe search engine.
To exclude any such cases, wequery the search engine with each complete sen-tence that a prediction task is based on and removeany prediction task for which the search engine re-turns hits for the complete sentence.
The final testset consists of 8060 prediction tasks.42.2 Experimental SetupRecall that the general issue we are interested inis whether one can obtain sufficient informationfrom the relatively narrow distributional windowof context provided by n-grams which are smallenough to occur frequently enough in the trainingdata but large enough to contain enough predic-4For a copy of the test set, just send us an email.tive information about preposition usage for theinstances to be classified.
By using a web-as-corpus approach we essentially try to maximizethe training data size.
For the n-gram size, we ex-plore the use of a maximum order of 7, containingthe preposition in the middle and three words ofcontext on either side.For each prediction task, we successively insertone of the nine most frequent prepositions intothe marked preposition slot of the 8060 n-gramsobtained from the test set.
Thus, for each pre-diction task, we get a cohort consisting of ninedifferent individual queries, one query for eachpotential preposition.
For example, the secondprediction task of Figure 2 yields the cohort ofnine queries in Figure 3 below, where the candi-date prepositions replace the location marked by-*-MASKED-*-of.
The correct preposition ofis stripped off and kept for later use in the evalua-tion step.1.
rather a question of the scalesfalling2.
rather a question to the scalesfalling3.
rather a question in the scalesfalling...9. rather a question from the scalesfallingFigure 3: Cohort of nine queries resulting for thesecond prediction task of Figure 2In cases where a preposition is closer than fourwords to the beginning or the end of the corre-sponding sentence, a lower-order n-gram results.For example, in the first prediction task in Fig-ure 2, the preposition occurs already as the sec-ond word in the sentence, thus not leaving enoughcontext to the left of the preposition for a sym-metric 7-gram.
Here, the truncated asymmetric 5-gram ?But <prep> the young ,?
includ-ing only one word of context on the left wouldget used.We issue each query in a cohort to the Ya-hoo search engine, and determine the numberof hits returned for that query.
To that end,we use Yahoo?s BOSS service, which offers a270JSON interface supporting straightforward auto-mated queries.
As part of its response to a query,the BOSS service includes the deephits field,which gives an ?approximate count that reflectsduplicate documents and all documents from ahost?.5 In other words, this number is an approx-imate measure of how many web pages there arethat contain the search pattern.With the counts for all nine queries in a cohortretrieved from Yahoo, we select the preposition ofthe query with the highest count.
For the casesin which none of the counts in a 7-gram cohort isgreater than zero, we use one of two strategies:In the baseline condition, for all n-gram cohortswith zero counts (5160 out of the 8060 cases) wepredict the most frequent preposition of, i.e., themajority baseline.
This results in an overall accu-racy of 50%.In the full back-off condition, we explore thetrade-off between the predictive power of the n-gram as context and the likelihood of having seenthis n-gram in the training material, i.e., findingit on the web.
In this paper we never abstract orgeneralize away from the surface string (e.g., bymapping all proper names to an abstract name tag;but see the outlook discussion at the end of the pa-per), so the only option for increasing the numberof occurrences of an n-gram is to approximate itwith multiple shorter n-grams.Concretely, if no hits could be found for any ofthe queries in a cohort, we back off to the sumof the hits for the two overlapping 6-grams con-structed in the way illustrated in Figure 4.
[rather a question of the scales falling]?
[rather a question of the scales][a question of the scales falling]Figure 4: Two overlapping 6-grams approximatea 7-gram for back-off.If still no hits can be obtained after backing offto 6-grams for any of the queries in a cohort, thesystem backs off further to overlapping 5-grams,and so on, down to trigrams.65Cited from http://developer.yahoo.com/search/boss/boss_guide/ch02s02.html6When backing off, the left-most and the right-most tri-3 ResultsFigure 5 shows the results of the full back-offapproach.
Compared to the baseline condition,accuracy goes up significantly to 76.5%.
Thus,the back-off strategy is effective in increasing theamount of available data using lower-order n-grams.
This increase of data is also reflected inthe number of cases with zero counts for a cohort,which goes down to none.Full back-offCorrect 6166Incorrect 1894Total 8060Accuracy 76.5%Figure 5: Overall results of our experiments.Figure 6 provides a detailed analysis of theback-off experiment.
It lists back-off sequencesseparately for each maximum n-gram order.
Theprediction tasks for which a full 7-gram can beextracted are displayed in the third column, withback-off orders of 6 down to 3.
Prediction tasksfor which only asymmetric 6-grams can be ex-tracted follow in column 4, and so on until 4-grams.
There are no predictions tasks that areshorter than four words.
Therefore, n-grams witha length of less than 4 do not occur.The ?sum?
column shows the combined resultsof the full 7-gram prediction tasks and the pre-diction tasks involving truncated, asymmetric n-grams of lower orders.There are 6999 prediction tasks for which full7-grams can be extracted.
The remaining 1061of the 8060 prediction tasks are the cases wherethe system extracts only asymmetric lower-ordern-grams, for the reasons explained in section 2.2.For 2195 of the 6999 7-gram prediction tasks,we find full 7-gram contexts on the web, of which1931 lead to a correct prediction, and 264 to anincorrect one, leaving 4804 prediction tasks stillto be solved through the back-off approach.
Thus,full 7-gram contexts lead to high-quality predic-tions at 88% precision, but they are rare and witha recall of 28,7% cover only a fraction of all cases.gram do not include the target preposition of the original 7-gram.
However, this only affects 13 cases, cf.
Figure 6.271sum 7-grams 6-grams 5-grams 4-grams(3 + prep + 3) (truncated 7-gram) (truncated 7-gram) (truncated 7-gram)Total 8060 6999 656 182 223Predictions 2900 2195 379 119 207correct 2495 1931 326 91 147incorrect 405 264 53 28 60Requiring back-off 5160 4804 277 63 16Precision 86% 88% 86% 76.5% 71%Recall 32.6% 28.7% 79.6% 59.1% 90.2%Back-off order 6Predictions 2028 2028correct 1620 1620incorrect 408 408Still requiring back-off 2776 2776Predict.
orders 7+6 4223 4223correct 3551 3551incorrect 672 672Precision 84.1% 84.1%Recall 56.1% 56.1%Back-off order 5Predictions 2180 2020 160correct 1542 1411 131incorrect 638 609 29Still requiring back-off 873 756 117Predict.
orders 7 ?
5 6782 6243 539correct 5419 4962 457incorrect 1363 1281 82Precision 79.9% 79.5% 84.8%Recall 86.1% 86.8% 79.6%Back-off order 4Predictions 905 743 106 56correct 488 382 68 38incorrect 417 361 38 18Still requiring back-off 31 13 11 7Predict.
orders 7 ?
4 7806 6986 645 175correct 5998 5344 525 129incorrect 1808 1642 120 46Precision 76.8% 76.5% 81.4% 73.7%Recall 99.5% 99.8% 97.9% 94.9%Back-off order 3Predictions 47 13 11 7 16correct 21 5 7 3 6incorrect 26 8 4 4 10Still requiring back-off 0 0 0 0 0Predict.
orders 7 ?
3 8060 6999 656 182 223correct 6166 5349 532 132 153incorrect 1894 1650 124 50 70Precision 76.5% 76.4% 81.1% 72.5% 68.6%Recall 100% 100% 100% 100% 100%Figure 6: The results of our experiments272Figure 7: Development of precision and recall inrelation to back-off orderApproximating 7-grams with two overlapping6-grams as the first back-off step provides theevidence needed to correctly predict 1620 addi-tional prepositions, with 408 additional false pre-dictions.
The number of correctly solved predic-tion tasks thus rises to 3551, and the number ofincorrect predictions rises to 672.
This back-offstep almost doubles recall (56.1%).
At the sametime, precision drops to 84.1%.
For 2776 pre-diction tasks, a further back-off step is necessarysince still no evidence can be found for them.
Thispattern repeats with the back-off steps that fol-low.
To summarize, by adding more data usingless restricted contexts, more prediction tasks canbe solved.
The better coverage however comes atthe price of reduced precision: Less specific con-texts are worse predictors of the correct preposi-tion than more specific contexts.Figure 7 visualizes the development of preci-sion and recall with full and truncated 7-gramscounted together as in the ?sum?
column in Fig-ure 6.
With each back-off step, more predictiontasks can be solved (as shown by the rising recallcurve).
At the same time, the overall quality ofthe predictions drops due to the less specific con-texts (as shown by the slightly dropping precisioncurve).
While the curve for recall rises steeply,the curve for precision remains relatively flat.
Theback-off approach thus succeeds in adding datawhile preserving prediction quality.As discussed above, we use the same set ofprepositions and test corpus as De Felice (2008),but only make use of 8060 test cases.
Figure 8shows that the accuracy stabilizes quickly afterabout 1000 predictions, so that the difference inthe size of the test set should have no impact onthe reported results.Figure 8: The accuracy of the n-gram predictionstabilizes quickly.4 Conclusions and OutlookIn this paper, we explored the potential and thelimits of a purely surface-based strategy of pre-dicting prepositions in English.
The use ofsurface-based n-grams ensures that fully specificexemplars of a particular size are stored in train-ing, but avoiding abstractions in this way leads tothe well-known data sparsity issues.
We showedthat using a web-as-corpus approach maximizingthe size of the ?training data?, one can work withn-grams which are large enough to predict the oc-currence of prepositions with significant precisionwhile at the same time ensuring that these specificn-grams have actually been encountered during?training?, i.e., evidence for them can be foundon the web.For the random sample of the BNC section Jwe tested on, the surface-based approach resultsin an accuracy of 77% for the 7-gram model withback-off to overlapping shorter n-grams.
It thusoutperforms De Felice?s (2008) machine learning273approach which uses the same set of prepositionsand the full BNC section J as test set.
In broaderterms, the result of our surface-based approachis competitive with the state-of-the art results forpreposition prediction in English using machinelearning to combine sophisticated sets of lexicaland linguistically motivated features.In this paper, we focused exclusively on theimpact of n-gram size on preposition prediction.Limiting ourselves to pure surface-based informa-tion made it possible to maximize the ?trainingdata?
by using a web-as-corpus approach.
Return-ing from this very specific experiment to the gen-eral issue, there are two well-known approachesto remedy the data sparseness problem arisingfrom storing large, specific surface forms in train-ing.
On the one hand, one can use smaller ex-emplars, which is the method we used as back-off in our experiments in this paper.
This onlyworks if the exemplars contain enough context forthe linguistic property or relation that we need tocapture the predictive power.
On the other hand,one can abstract parts of the surface-based train-ing instances to more general classes.
The cru-cial question this raises is which generalizationspreserve the predictive power of the exemplarsand can reliably be identified.
The linguistically-informed features used in the previous approachesin the literature naturally provide interesting in-stances of answers to this question.
In the fu-ture, we intend to compare the results we ob-tained using the web-as-corpus approach with onebased on the Google-5-gram corpus to study us-ing controlled, incremental shallow-to-deep fea-ture development which abstractions or linguisticgeneralizations best preserve the predictive con-text while lowering the demands on the size of thetraining data.Turning to a linguistic issue, it could be use-ful to distinguish between lexical and functionalprepositions when reporting test results.
This isan important distinction because the informationneeded to predict functional prepositions typicallyis in the local context, whereas the informationneeded to predict lexical prepositions is not nec-essarily present locally.
To illustrate, a competenthuman speaker presented with the sentence Johnis dependent his brother and asked to fill inthe missing preposition, would correctly pick on.This is a case of a functional preposition wherethe relevant information is locally present: the ad-jective dependent selects on.
On the other hand,the sentence John put his bag the table ismore problematic, even for a human, since bothon and under are reasonable choices; the infor-mation needed to predict the omitted prepositionin this case is not locally present.
In line withthe previous research, in the work in this paperwe made predictions for all prepositions alike.
Inthe future, it could be useful to annotate the testset so that one can distinguish functional and lex-ical uses and report separate figures for these twoclasses in order to empirically confirm their dif-ferences with respect to locality.ReferencesBergsma, Shane, Dekang Lin, and Randy Goebel.2009.
Web-scale n-gram models for lexical disam-biguation.
In IJCAI?09: Proceedings of the 21st in-ternational jont conference on Artifical intelligence,pages 1507?1512, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Boyd, Adriane, Markus Dickinson, and Detmar Meur-ers.
2008.
On detecting errors in dependency tree-banks.
Research on Language and Computation,6(2):113?137.Chodorow, Martin, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involv-ing prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30,Prague, Czech Republic, June.De Felice, Rachele and Stephen Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the 4th ACL-SIGSEM Workshop onPrepositions, pages 45?50, Prague, Czech Republic,June.
Association for Computational Linguistics.De Felice, Rachele.
2008.
Automatic Error Detectionin Non-native English.
Ph.D. thesis, St Catherine?sCollege, University of Oxford.Dickinson, Markus and W. Detmar Meurers.
2003.Detecting errors in part-of-speech annotation.
InProceedings of the 10th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL-03), pages 107?114, Budapest,Hungary.Dickinson, Markus and W. Detmar Meurers.
2005.Detecting errors in discontinuous structural anno-274tation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 322?329.Gamon, Michael, Jianfeng Gao, Chris Brockett,Alexander Klementiev, William Dolan, Dmitriy Be-lenko, and Lucy Vanderwende.
2008.
Using con-textual speller techniques and language modelingfor esl error correction.
In Proceedings of IJCNLP,Hyderabad, India.Lapata, Mirella and Frank Keller.
2005.
Web-based models for natural language processing.
ACMTransactions on Speech and Language Processing,2(1):1?30, February.Lee, John and Ola Knutsson.
2008.
The role of ppattachment in preposition generation.
In Gelbukh,A., editor, Proceedings of CICLing 2008.Tetreault, Joel and Martin Chodorow.
2008a.
Na-tive judgments of non-native usage: Experimentsin preposition error detection.
In Proceedings ofCOLING-08, Manchester.Tetreault, Joel and Martin Chodorow.
2008b.
The upsand downs of preposition error detection in esl writ-ing.
In Proceedings of COLING-08, Manchester.275
