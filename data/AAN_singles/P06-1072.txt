Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 569?576,Sydney, July 2006. c?2006 Association for Computational LinguisticsAnnealing Structural Bias in Multilingual Weighted Grammar Induction?Noah A. Smith and Jason EisnerDepartment of Computer Science / Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218 USA{nasmith,jason}@cs.jhu.eduAbstractWe first show how a structural locality bias can improve theaccuracy of state-of-the-art dependency grammar inductionmodels trained by EM from unannotated examples (Kleinand Manning, 2004).
Next, by annealing the free parame-ter that controls this bias, we achieve further improvements.We then describe an alternative kind of structural bias, to-ward ?broken?
hypotheses consisting of partial structuresover segmented sentences, and show a similar pattern of im-provement.
We relate this approach to contrastive estimation(Smith and Eisner, 2005a), apply the latter to grammar in-duction in six languages, and show that our new approachimproves accuracy by 1?17% (absolute) over CE (and 8?30%over EM), achieving to our knowledge the best results on thistask to date.
Our method, structural annealing, is a gen-eral technique with broad applicability to hidden-structurediscovery problems.1 IntroductionInducing a weighted context-free grammar fromflat text is a hard problem.
A common start-ing point for weighted grammar induction isthe Expectation-Maximization (EM) algorithm(Dempster et al, 1977; Baker, 1979).
EM?smediocre performance (Table 1) reflects two prob-lems.
First, it seeks to maximize likelihood, but agrammar that makes the training data likely doesnot necessarily assign a linguistically defensiblesyntactic structure.
Second, the likelihood surfaceis not globally concave, and learners such as theEM algorithm can get trapped on local maxima(Charniak, 1993).We seek here to capitalize on the intuition that,at least early in learning, the learner should searchprimarily for string-local structure, because moststructure is local.1 By penalizing dependencies be-tween two words that are farther apart in the string,we obtain consistent improvements in accuracy ofthe learned model (?3).We then explore how gradually changing ?
overtime affects learning (?4): we start out with a?This work was supported by a Fannie and John HertzFoundation fellowship to the first author and NSF ITR grantIIS-0313193 to the second author.
The views expressed arenot necessarily endorsed by the sponsors.
We thank threeanonymous COLING-ACL reviewers for comments.1To be concrete, in the corpora tested here, 95% of de-pendency links cover ?
4 words (English, Bulgarian, Por-tuguese), ?
5 words (German, Turkish), ?
6 words (Man-darin).model selection among values of ?
and ?
(0)worst unsup.
sup.
oracleGerman 19.8 19.8 54.4 54.4English 21.8 41.6 41.6 42.0Bulgarian 24.7 44.6 45.6 45.6Mandarin 31.8 37.2 50.0 50.0Turkish 32.1 41.2 48.0 51.4Portuguese 35.4 37.4 42.3 43.0Table 1: Baseline performance of EM-trained dependencyparsing models: F1 on non-$ attachments in test data, withvarious model selection conditions (3 initializers?
6 smooth-ing values).
The languages are listed in decreasing order bythe training set size.
Experimental details can be found in theappendix.strong preference for short dependencies, then re-lax the preference.
The new approach, structuralannealing, often gives superior performance.An alternative structural bias is explored in ?5.This approach views a sentence as a sequenceof one or more yields of separate, independenttrees.
The points of segmentation are a hiddenvariable, and during learning all possible segmen-tations are entertained probabilistically.
This al-lows the learner to accept hypotheses that explainthe sentences as independent pieces.In ?6 we briefly review contrastive estimation(Smith and Eisner, 2005a), relating it to the newmethod, and show its performance alone and whenaugmented with structural bias.2 Task and ModelIn this paper we use a simple unlexicalized depen-dency model due to Klein and Manning (2004).The model is a probabilistic head automaton gram-mar (Alshawi, 1996) with a ?split?
form that ren-ders it parseable in cubic time (Eisner, 1997).Let x = ?x1, x2, ..., xn?
be the sentence.
x0 is aspecial ?wall?
symbol, $, on the left of every sen-tence.
A tree y is defined by a pair of functionsyleft and yright (both {0, 1, 2, ..., n} ?
2{1,2,...,n})that map each word to its sets of left and right de-pendents, respectively.
The graph is constrainedto be a projective tree rooted at $: each word ex-cept $ has a single parent, and there are no cycles569or crossing dependencies.2 yleft(0) is taken to beempty, and yright(0) contains the sentence?s singlehead.
Let yi denote the subtree rooted at positioni.
The probability P (yi | xi) of generating thissubtree, given its head word xi, is defined recur-sively:?D?
{left ,right}pstop(stop | xi,D , [yD(i) = ?])
(1)?
?j?yD (i)pstop(?stop | xi,D ,firsty(j))?pchild(xj | xi,D)?
P (yj | xj)where firsty(j) is a predicate defined to be true iffxj is the closest child (on either side) to its parentxi.
The probability of the entire tree is given byp?
(x,y) = P (y0 | $).
The parameters ?
are theconditional distributions pstop and pchild.Experimental baseline: EM.
Following com-mon practice, we always replace words by part-of-speech (POS) tags before training or testing.
Weused the EM algorithm to train this model on POSsequences in six languages.
Complete experimen-tal details are given in the appendix.
Performancewith unsupervised and supervised model selec-tion across different ?
values in add-?
smoothingand three initializers ?
(0) is reported in Table 1.The supervised-selected model is in the 40?55%F1-accuracy range on directed dependency attach-ments.
(Here F1 ?
precision ?
recall; see ap-pendix.)
Supervised model selection, which usesa small annotated development set, performs al-most as well as the oracle, but unsupervised modelselection, which selects the model that maximizeslikelihood on an unannotated development set, isoften much worse.3 Locality Bias among TreesHidden-variable estimation algorithms?including EM?typically work by iterativelymanipulating the model parameters ?
to improvean objective function F (?).
EM explicitlyalternates between the computation of a posteriordistribution over hypotheses, p?
(y | x) (wherey is any tree with yield x), and computing a newparameter estimate ?.32A projective parser could achieve perfect accuracy on ourEnglish and Mandarin datasets, > 99% on Bulgarian, Turk-ish, and Portuguese, and > 98% on German.3For weighted grammar-based models, the posterior doesnot need to be explicitly represented; instead expectations un-der p?
are used to compute updates to ?.00.10.20.30.40.50.60.7-1 -0.8 -0.6 -0.4 -0.2  0  0.2?F (EM baseline)GermanEnglishBulgarianMandarinTurkishPortugueseFigure 1: Test-set F1 performance of models trained by EMwith a locality bias at varying ?.
Each curve correspondsto a different language and shows performance of supervisedmodel selection within a given ?, across ?
and ?
(0) values.
(See Table 3 for performance of models selected across ?s.
)We decode with ?
= 0, though we found that keeping thetraining-time value of ?
would have had almost no effect.
TheEM baseline corresponds to ?
= 0.One way to bias a learner toward local expla-nations is to penalize longer attachments.
Thiswas done for supervised parsing in different waysby Collins (1997), Klein and Manning (2003),and McDonald et al (2005), all of whom con-sidered intervening material or coarse distanceclasses when predicting children in a tree.
Eis-ner and Smith (2005) achieved speed and accuracyimprovements by modeling distance directly in aML-estimated (deficient) generative model.Here we use string distance to measure thelength of a dependency link and consider the inclu-sion of a sum-of-lengths feature in the probabilis-tic model, for learning only.
Keeping our originalmodel, we will simply multiply into the probabil-ity of each tree another factor that penalizes longdependencies, giving:p??
(x,y) ?
p?(x,y)?e?????n?i=1?j?y(i)|i?
j|????
(2)where y(i) = yleft(i) ?
yright(i).
Note that if?
= 0, we have the original model.
As ?
?
?
?,the new model p??
will favor parses with shorterdependencies.
The dynamic programming algo-rithms remain the same as before, with the appro-priate e?|i?j| factor multiplied in at each attach-ment between xi and xj .
Note that when ?
= 0,p??
?
p?.Experiment.
We applied a locality bias to thesame dependency model by setting ?
to different57000.10.20.30.40.50.60.7-1 -0.5  0  0.5  1  1.5?FGermanBulgarianTurkishFigure 2: Test-set F1 performance of models trained by EMwith structural annealing on the distance weight ?.
Herewe show performance with add-10 smoothing, the all-zeroinitializer, for three languages with three different initial val-ues ?0.
Time progresses from left to right.
Note that it isgenerally best to start at ?0  0; note also the importance ofpicking the right point on the curve to stop.
See Table 3 forperformance of models selected across smoothing, initializa-tion, starting, and stopping choices, in all six languages.values in [?1, 0.2] (see Eq.
2).
The same initial-izers ?
(0) and smoothing conditions were tested.Performance of supervised model selection amongmodels trained at different ?
values is plotted inFig.
1.
When a model is selected across all condi-tions (3 initializers ?
6 smoothing values ?
7 ?s)using annotated development data, performance isnotably better than the EM baseline using the sameselection procedure (see Table 3, second column).4 Structural AnnealingThe central idea of this paper is to graduallychange (anneal) the bias ?.
Early in learning, localdependencies are emphasized by setting ?
0.Then ?
is iteratively increased and training re-peated, using the last learned model to initialize.This idea bears a strong similarity to determin-istic annealing (DA), a technique used in clus-tering and classification to smooth out objectivefunctions that are piecewise constant (hence dis-continuous) or bumpy (non-concave) (Rose, 1998;Ueda and Nakano, 1998).
In unsupervised learn-ing, DA iteratively re-estimates parameters likeEM, but begins by requiring that the entropy ofthe posterior p?
(y | x) be maximal, then gradu-ally relaxes this entropy constraint.
Since entropyis concave in ?, the initial task is easy (maximizea concave, continuous function).
At each step theoptimization task becomes more difficult, but theinitializer is given by the previous step and, inpractice, tends to be close to a good local max-imum of the more difficult objective.
By the lastiteration the objective is the same as in EM, but theannealed search process has acted like a good ini-tializer.
This method was applied with some suc-cess to grammar induction models by Smith andEisner (2004).In this work, instead of imposing constraints onthe entropy of the model, we manipulate bias to-ward local hypotheses.
As ?
increases, we penal-ize long dependencies less.
We call this structuralannealing, since we are varying the strength of asoft constraint (bias) on structural hypotheses.
Instructural annealing, the final objective would bethe same as EM if our final ?, ?f = 0, but wefound that annealing farther (?f > 0) works muchbetter.4Experiment: Annealing ?.
We experimentedwith annealing schedules for ?.
We initialized at?0 ?
{?1,?0.4,?0.2}, and increased ?
by 0.1 (inthe first case) or 0.05 (in the others) up to ?f = 3.Models were trained to convergence at each ?-epoch.
Model selection was applied over the sameinitialization and regularization conditions as be-fore, ?0, and also over the choice of ?f , with stop-ping allowed at any stage along the ?
trajectory.Trajectories for three languages with three dif-ferent ?0 values are plotted in Fig.
2.
Generallyspeaking, ?0  0 performs better.
There is con-sistently an early increase in performance as ?
in-creases, but the stopping ?f matters tremendously.Selected annealed-?
models surpass EM in all sixlanguages; see the third column of Table 3.
Notethat structural annealing does not always outper-form fixed-?
training (English and Portuguese).This is because we only tested a few values of ?0,since annealing requires longer runtime.5 Structural Bias via SegmentationA related way to focus on local structure earlyin learning is to broaden the set of hypothe-ses to include partial parse structures.
If x =?x1, x2, ..., xn?, the standard approach assumesthat x corresponds to the vertices of a single de-pendency tree.
Instead, we entertain every hypoth-esis in which x is a sequence of yields from sepa-rate, independently-generated trees.
For example,?x1, x2, x3?
is the yield of one tree, ?x4, x5?
is the4The reader may note that ?f > 0 actually corresponds toa bias toward longer attachments.
A more apt description inthe context of annealing is to say that during early stages thelearner starts liking local attachments too much, and we needto exaggerate ?
to ?coax?
it to new hypotheses.
See Fig.
2.57100.10.20.30.40.50.60.7-1.5-1-0.5 0 0.5?FGermanBulgarianTurkishFigure 3: Test-set F1 performance of models trained by EMwith structural annealing on the breakage weight ?.
Herewe show performance with add-10 smoothing, the all-zeroinitializer, for three languages with three different initial val-ues ?0.
Time progresses from left (large ?)
to right.
See Ta-ble 3 for performance of models selected across smoothing,initialization, and stopping choices, in all six languages.yield of a second, and ?x6, ..., xn?
is the yield of athird.
One extreme hypothesis is that x is n single-node trees.
At the other end of the spectrum is theoriginal set of hypotheses?full trees on x. Eachhas a nonzero probability.Segmented analyses are intermediate represen-tations that may be helpful for a learner to useto formulate notions of probable local structure,without committing to full trees.5 We only allowunobserved breaks, never positing a hard segmen-tation of the training sentences.
Over time, we in-crease the bias against broken structures, forcingthe learner to commit most of its probability massto full trees.5.1 Vine ParsingAt first glance broadening the hypothesis spaceto entertain all 2n?1 possible segmentations mayseem expensive.
In fact the dynamic program-ming computation is almost the same as sum-ming or maximizing over connected dependencytrees.
For the latter, we use an inside-outside al-gorithm that computes a score for every parse treeby computing the scores of items, or partial struc-tures, through a bottom-up process.
Smaller itemsare built first, then assembled using a set of rulesdefining how larger items can be built.6Now note that any sequence of partial treesover x can be constructed by combining the sameitems into trees.
The only difference is that we5See also work on partial parsing as a task in its own right:Hindle (1990) inter alia.6See Eisner and Satta (1999) for the relevant algorithmused in the experiments.are willing to consider unassembled sequences ofthese partial trees as hypotheses, in addition tothe fully connected trees.
One way to accom-plish this in terms of yright(0) is to say that theroot, $, is allowed to have multiple children, in-stead of just one.
Here, these children are inde-pendent of each other (e.g., generated by a uni-gram Markov model).
In supervised dependencyparsing, Eisner and Smith (2005) showed that im-posing a hard constraint on the whole structure?specifically that each non-$ dependency arc crossfewer than k words?can give guaranteed O(nk2)runtime with little to no loss in accuracy (for sim-ple models).
This constraint could lead to highlycontrived parse trees, or none at all, for somesentences?both are avoided by the allowance ofsegmentation into a sequence of trees (each at-tached to $).
The construction of the ?vine?
(se-quence of $?s children) takes only O(n) time oncethe chart has been assembled.Our broadened hypothesis model is a proba-bilistic vine grammar with a unigram model over$?s children.
We allow (but do not require) seg-mentation of sentences, where each independentchild of $ is the root of one of the segments.
We donot impose any constraints on dependency length.5.2 Modeling SegmentationNow the total probability of an n-length sentencex, marginalizing over its hidden structures, sumsup not only over trees, but over segmentations ofx.
For completeness, we must include a proba-bility model over the number of trees generated,which could be anywhere from 1 to n. The modelover the number T of trees given a sentence oflength n will take the following log-linear form:P (T = t | n) = et?/n?i=1ei?where ?
?
R is the sole parameter.
When ?
= 0,every value of T is equally likely.
For ?
0, themodel prefers larger structures with few breaks.At the limit (?
?
??
), we achieve the standardlearning setting, where the model must explain xusing a single tree.
We start however at ?
0,where the model prefers smaller trees with morebreaks, in the limit preferring each word in x to beits own tree.
We could describe ?brokenness?
as afeature in the model whose weight, ?, is chosenextrinsically (and time-dependently), rather thanempirically?just as was done with ?.572model selection among values of ?2 and ?
(0)worst unsup.
sup.
oracleDORT1 32.5 59.3 63.4 63.4Ger.LENGTH 30.5 56.4 57.3 57.8DORT1 20.9 56.6 57.4 57.4Eng.LENGTH 29.1 37.2 46.2 46.2DORT1 19.4 26.0 40.5 43.1Bul.LENGTH 25.1 35.3 38.3 38.3DORT1 9.4 24.2 41.1 41.1Man.LENGTH 13.7 17.9 26.2 26.2DORT1 7.3 38.6 58.2 58.2Tur.LENGTH 21.5 34.1 55.5 55.5DORT1 35.0 59.8 71.8 71.8Por.LENGTH 30.8 33.6 33.6 33.6Table 2: Performance of CE on test data, for different neigh-borhoods and with different levels of regularization.
Bold-face marks scores better than EM-trained models selected thesame way (Table 1).
The score is the F1 measure on non-$attachments.Annealing ?
resembles the popular bootstrap-ping technique (Yarowsky, 1995), which starts outaiming for high precision, and gradually improvescoverage over time.
With strong bias (?
0), weseek a model that maintains high dependency pre-cision on (non-$) attachments by attaching mosttags to $.
Over time, as this is iteratively weak-ened (?
?
??
), we hope to improve coverage(dependency recall).
Bootstrapping was appliedto syntax learning by Steedman et al (2003).
Ourapproach differs in being able to remain partly ag-nostic about each tag?s true parent (e.g., by giving50% probability to attaching to $), whereas Steed-man et al make a hard decision to retrain on awhole sentence fully or leave it out fully.
In ear-lier work, Brill and Marcus (1992) adopted a ?lo-cal first?
iterative merge strategy for discoveringphrase structure.Experiment: Annealing ?.
We experimentedwith different annealing schedules for ?.
The ini-tial value of ?, ?0, was one of {?12 , 0,12}.
AfterEM training, ?
was diminished by 110 ; this was re-peated down to a value of ?f = ?3.
Performanceafter training at each ?
value is shown in Fig.
3.7We see that, typically, there is a sharp increasein performance somewhere during training, whichtypically lessens as ?
?
??.
Starting ?
too highcan also damage performance.
This method, then,7Performance measures are given using a full parser thatfinds the single best parse of the sentence with the learnedparsing parameters.
Had we decoded with a vine parser, wewould see a precision?, recall?
curve as ?
decreased.is not robust to the choice of ?, ?0, or ?f , nor doesit always do as well as annealing ?, although con-siderable gains are possible; see the fifth columnof Table 3.By testing models trained with a fixed value of ?
(for values in [?1, 1]), we ascertained that the per-formance improvement is due largely to annealing,not just the injection of segmentation bias (fourthvs.
fifth column of Table 3).86 Comparison and Combination withContrastive EstimationContrastive estimation (CE) was recently intro-duced (Smith and Eisner, 2005a) as a class of alter-natives to the likelihood objective function locallymaximized by EM.
CE was found to outperformEM on the task of focus in this paper, when ap-plied to English data (Smith and Eisner, 2005b).Here we review the method briefly, show how itperforms across languages, and demonstrate thatit can be combined effectively with structural bias.Contrastive training defines for each example xia class of presumably poor, but similar, instancescalled the ?neighborhood,?
N(xi), and seeks tomaximizeCN(?)
=?ilog p?
(xi | N(xi))=?ilog?y p?(xi,y)?x?
?N(xi)?y p?
(x?,y)At this point we switch to a log-linear (ratherthan stochastic) parameterization of the sameweighted grammar, for ease of numerical opti-mization.
All this means is that ?
(specifically,pstop and pchild in Eq.
1) is now a set of nonnega-tive weights rather than probabilities.Neighborhoods that can be expressed as finite-state lattices built from xi were shown to give sig-nificant improvements in dependency parser qual-ity over EM.
Performance of CE using two ofthose neighborhoods on the current model anddatasets is shown in Table 2.9 0-mean diagonalGaussian smoothing was applied, with differentvariances, and model selection was applied oversmoothing conditions and the same initializers as8In principle, segmentation can be combined with the lo-cality bias in ?3 (?).
In practice, we found that this usuallyunder-performed the EM baseline.9We experimented with DELETE1, TRANSPOSE1, DELE-TEORTRANSPOSE1, and LENGTH.
To conserve space weshow only the latter two, which tend to perform best.573EM fixed ?
annealed ?
fixed ?
annealed ?
CE fixed ?
+ CE?
?0 ?
?f ?
?0 ?
?f N N, ?German 54.4 61.3 0.2 70.0 -0.4 ?
0.4 66.2 0.4 68.9 0.5 ?
-2.4 63.4 DORT1 63.8 DORT1, -0.2English 41.6 61.8 -0.6 53.8 -0.4 ?
0.3 55.6 0.2 58.4 0.5 ?
0.0 57.4 DORT1 63.5 DORT1, -0.4Bulgarian 45.6 49.2 -0.2 58.3 -0.4 ?
0.2 47.3 -0.2 56.5 0 ?
-1.7 40.5 DORT1 ?Mandarin 50.0 51.1 -0.4 58.0 -1.0 ?
0.2 38.0 0.2 57.2 0.5 ?
-1.4 43.4 DEL1 ?Turkish 48.0 62.3 -0.2 62.4 -0.2 ?
-0.15 53.6 -0.2 59.4 0.5 ?
-0.7 58.2 DORT1 61.8 DORT1, -0.6Portuguese 42.3 50.4 -0.4 50.2 -0.4 ?
-0.1 51.5 0.2 62.7 0.5 ?
-0.5 71.8 DORT1 72.6 DORT1, -0.2Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters.
Supervised modelselection was applied in all cases, including EM (see the appendix).
Boldface marks the best performance overall and trialsthat this performance did not significantly surpass under a sign test (i.e., p 6< 0.05).
The score is the F1 measure on non-$attachments.
The fixed ?
+ CE condition was tested only for languages where CE improved over EM.before.
Four of the languages have at least one ef-fective CE condition, supporting our previous En-glish results (Smith and Eisner, 2005b), but CEwas harmful for Bulgarian and Mandarin.
Perhapsbetter neighborhoods exist for these languages, orthere is some ideal neighborhood that would per-form well for all languages.Our approach of allowing broken trees (?5) isa natural extension of the CE framework.
Con-trastive estimation views learning as a process ofmoving posterior probability mass from (implicit)negative examples to (explicit) positive examples.The positive evidence, as in MLE, is taken to bethe observed data.
As originally proposed, CE al-lowed a redefinition of the implicit negative ev-idence from ?all other sentences?
(as in MLE)to ?sentences like xi, but perturbed.?
Allowingsegmentation of the training sentences redefinesthe positive and negative evidence.
Rather thanmoving probability mass only to full analyses ofthe training example xi, we also allow probabilitymass to go to partial analyses of xi.By injecting a bias (?
6= 0 or ?
> ??)
amongtree hypotheses, however, we have gone beyondthe CE framework.
We have added features tothe tree model (dependency length-sum, numberof breaks), whose weights we extrinsically manip-ulate over time to impose locality bias CN and im-prove search on CN.
Another idea, not exploredhere, is to change the contents of the neighborhoodN over time.Experiment: Locality Bias within CE.
Wecombined CE with a fixed-?
locality bias forneighborhoods that were successful in the earlierCE experiment, namely DELETEORTRANSPOSE1for German, English, Turkish, and Portuguese.Our results, shown in the seventh column of Ta-ble 3, show that, in all cases except Turkish, thecombination improves over either technique on itsown.
We leave exploration of structural annealingwith CE to future work.Experiment: Segmentation Bias within CE.For (language, N) pairs where CE was effec-tive, we trained models using CE with a fixed-?
segmentation model.
Across conditions (?
?
[?1, 1]), these models performed very badly, hy-pothesizing extremely local parse trees: typicallyover 90% of dependencies were length 1 andpointed in the same direction, compared with the60?70% length-1 rate seen in gold standards.
Tounderstand why, consider that the CE goal is tomaximize the score of a sentence and all its seg-mentations while minimizing the scores of neigh-borhood sentences and their segmentations.
An n-gram model can accomplish this, since the samen-grams are present in all segmentations of x,and (some) different n-grams appear in N(x)(for LENGTH and DELETEORTRANSPOSE1).
Abigram-like model that favors monotone branch-ing, then, is not a bad choice for a CE learner thatmust account for segmentations of x and N(x).Why doesn?t CE without segmentation resort ton-gram-like models?
Inspection of models trainedusing the standard CE method (no segmentation)with transposition-based neighborhoods TRANS-POSE1 and DELETEORTRANSPOSE1 did havehigh rates of length-1 dependencies, while thepoorly-performing DELETE1 models found lowlength-1 rates.
This suggests that a bias towardlocality (?n-gram-ness?)
is built into the formerneighborhoods, and may partly explain why CEworks when it does.
We achieved a similar localitybias in the likelihood framework when we broad-ened the hypothesis space, but doing so under CEover-focuses the model on local structures.5747 Error AnalysisWe compared errors made by the selected EM con-dition with the best overall condition, for each lan-guage.
We found that the number of corrected at-tachments always outnumbered the number of newerrors by a factor of two or more.Further, the new models are not getting betterby merely reversing the direction of links madeby EM; undirected accuracy also improved signif-icantly under a sign test (p < 10?6), across all sixlanguages.
While the most common correctionswere to nouns, these account for only 25?41% ofcorrections, indicating that corrections are not ?allof the same kind.
?Finally, since more than half of corrections inevery language involved reattachment to a nounor a verb (content word), we believe the improvedmodels to be getting closer than EM to the deepersemantic relations between words that, ideally,syntactic models should uncover.8 Future WorkOne weakness of all recent weighted grammarinduction work?including Klein and Manning(2004), Smith and Eisner (2005b), and the presentpaper?is a sensitivity to hyperparameters, includ-ing smoothing values, choice of N (for CE), andannealing schedules?not to mention initializa-tion.
This is quite observable in the results we havepresented.
An obstacle for unsupervised learn-ing in general is the need for automatic, efficientmethods for model selection.
For annealing, in-spiration may be drawn from continuation meth-ods; see, e.g., Elidan and Friedman (2005).
Ideallyone would like to select values simultaneously formany hyperparameters, perhaps using a small an-notated corpus (as done here), extrinsic figures ofmerit on successful learning trajectories, or plau-sibility criteria (Eisner and Karakos, 2005).Grammar induction serves as a tidy examplefor structural annealing.
In future work, we envi-sion that other kinds of structural bias and anneal-ing will be useful in other difficult learning prob-lems where hidden structure is required, includingmachine translation, where the structure can con-sist of word correspondences or phrasal or recur-sive syntax with correspondences.
The techniquebears some similarity to the estimation methodsdescribed by Brown et al (1993), which startedby estimating simple models, using each model toseed the next.9 ConclusionWe have presented a new unsupervised parameterestimation method, structural annealing, for learn-ing hidden structure that biases toward simplic-ity and gradually weakens (anneals) the bias overtime.
We applied the technique to weighted de-pendency grammar induction and achieved a sig-nificant gain in accuracy over EM and CE, raisingthe state-of-the-art across six languages from 42?54% to 58?73% accuracy.ReferencesS.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.
Florestasinta?
(c)tica: a treebank for Portuguese.
In Proc.
of LREC.H.
Alshawi.
1996.
Head automata and bilingual tiling:Translation with minimal representations.
In Proc.
ofACL.N.
B. Atalay, K. Oflazer, and B.
Say.
2003.
The annotationprocess in the Turkish treebank.
In Proc.
of LINC.J.
K. Baker.
1979.
Trainable grammars for speech recogni-tion.
In Proc.
of the Acoustical Society of America.S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER Treebank.
In Proc.
of Workshop onTreebanks and Linguistic Theories.E.
Brill and M. Marcus.
1992.
Automatically acquiringphrase structure using distributional analysis.
In Proc.
ofDARPA Workshop on Speech and Natural Language.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared task onmultilingual dependency parsing.
In Proc.
of CoNLL.E.
Charniak.
1993.
Statistical Language Learning.
MITPress.M.
Collins.
1997.
Three generative, lexicalised models forstatistical parsing.
In Proc.
of ACL.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximum like-lihood estimation from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society B, 39:1?38.J.
Eisner and D. Karakos.
2005.
Bootstrapping without theboot.
In Proc.
of HLT-EMNLP.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexicalcontext-free grammars and head automaton grammars.
InProc.
of ACL.J.
Eisner and N. A. Smith.
2005.
Parsing with soft and hardconstraints on dependency length.
In Proc.
of IWPT.J.
Eisner.
1997.
Bilexical grammars and a cubic-time proba-bilistic parser.
In Proc.
of IWPT.G.
Elidan and N. Friedman.
2005.
Learning hidden variablenetworks: the information bottleneck approach.
Journalof Machine Learning Research, 6:81?127.D.
Hindle.
1990.
Noun classification from predicate-argument structure.
In Proc.
of ACL.D.
Klein and C. D. Manning.
2002.
A generative constituent-context model for improved grammar induction.
In Proc.of ACL.D.
Klein and C. D. Manning.
2003.
Fast exact inference witha factored model for natural language parsing.
In NIPS 15.D.
Klein and C. D. Manning.
2004.
Corpus-based inductionof syntactic structure: Models of dependency and con-stituency.
In Proc.
of ACL.575M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19:313?330.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.
ofACL.K.
Oflazer, B.
Say, D. Z. Hakkani-Tu?r, and G. Tu?r.
2003.Building a Turkish treebank.
In A. Abeille, editor,Building and Exploiting Syntactically-Annotated Cor-pora.
Kluwer.K.
Rose.
1998.
Deterministic annealing for clustering, com-pression, classification, regression, and related optimiza-tion problems.
Proc.
of the IEEE, 86(11):2210?2239.K.
Simov and P. Osenova.
2003.
Practical annotation schemefor an HPSG treebank of Bulgarian.
In Proc.
of LINC.K.
Simov, G. Popova, and P. Osenova.
2002.
HPSG-based syntactic treebank of Bulgarian (BulTreeBank).
InA.
Wilson, P. Rayson, and T. McEnery, editors, A Rain-bow of Corpora: Corpus Linguistics and the Languagesof the World, pages 135?42.
Lincom-Europa.K.
Simov, P. Osenova, A. Simov, and M. Kouylekov.
2004.Design and implementation of the Bulgarian HPSG-basedTreebank.
Journal of Research on Language and Compu-tation, 2(4):495?522.N.
A. Smith and J. Eisner.
2004.
Annealing techniquesfor unsupervised statistical language learning.
In Proc.of ACL.N.
A. Smith and J. Eisner.
2005a.
Contrastive estimation:Training log-linear models on unlabeled data.
In Proc.
ofACL.N.
A. Smith and J. Eisner.
2005b.
Guiding unsupervisedgrammar induction using contrastive estimation.
In Proc.of IJCAI Workshop on Grammatical Inference Applica-tions.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003.Bootstrapping statistical parsers from small datasets.
InProc.
of EACL.N.
Ueda and R. Nakano.
1998.
Deterministic annealing EMalgorithm.
Neural Networks, 11(2):271?282.N.
Xue, F. Xia, F.-D. Chiou, and M. Palmer.
2004.
The PennChinese Treebank: Phrase structure annotation of a largecorpus.
Natural Language Engineering, 10(4):1?30.D.
Yarowsky.
1995.
Unsupervised word sense disambigua-tion rivaling supervised methods.
In Proc.
of ACL.A Experimental SetupFollowing the usual conventions (Klein and Man-ning, 2002), our experiments use treebank POSsequences of length ?
10, stripped of words andpunctuation.
For smoothing, we apply add-?, withsix values of ?
(in CE trials, we use a 0-mean di-agonal Gaussian prior with five different values of?2).
Our training datasets are:?
8,227 German sentences from the TIGER Tree-bank (Brants et al, 2002),?
5,301 English sentences from the WSJ PennTreebank (Marcus et al, 1993),?
4,929 Bulgarian sentences from the BulTree-Bank (Simov et al, 2002; Simov and Osenova,2003; Simov et al, 2004),?
2,775 Mandarin sentences from the Penn Chi-nese Treebank (Xue et al, 2004),?
2,576 Turkish sentences from the METU-Sabanci Treebank (Atalay et al, 2003; Oflazer etal., 2003), and?
1,676 Portuguese sentences from the Bosqueportion of the Floresta Sinta?
(c)tica Treebank(Afonso et al, 2002).The Bulgarian, Turkish, and Portuguese datasetscome from the CoNLL-X shared task (Buchholzand Marsi, 2006); we thank the organizers.When comparing a hypothesized tree y to agold standard y?, precision and recall measuresare available.
If every tree in the gold standard andevery hypothesis tree is such that |yright(0)| = 1,then precision = recall = F1, since |y| = |y?|.|yright(0)| = 1 for all hypothesized trees in thispaper, but not all treebank trees; hence we reportthe F1 measure.
The test set consists of around500 sentences (in each language).Iterative training proceeds until either 100 it-erations have passed, or the objective convergeswithin a relative tolerance of  = 10?5, whicheveroccurs first.Models trained at different hyperparameter set-tings and with different initializers are selectedusing a 500-sentence development set.
Unsuper-vised model selection means the model with thehighest training objective value on the develop-ment set was chosen.
Supervised model selectionchooses the model that performs best on the anno-tated development set.
(Oracle and worst modelselection are chosen based on performance on thetest data.
)We use three initialization methods.
We run asingle special E step (to get expected counts ofmodel events) then a single M step that renormal-izes to get a probabilistic model ?(0).
In initializer1, the E step scores each tree as follows (only con-nected trees are scored):u(x,yleft ,yright) =n?i=1?j?y(i)(1 +1|i?
j|)(Proper) expectations under these scores are com-puted using an inside-outside algorithm.
Initial-izer 2 computes expected counts directly, withoutdynamic programming.
For an n-length sentence,p(yright(0) = {i}) = 1n and p(j ?
y(i)) ?1|i?j| .These are scaled by an appropriate constant foreach sentence, then summed across sentences tocompute expected event counts.
Initializer 3 as-sumes a uniform distribution over hidden struc-tures in the special E step by setting all log proba-bilities to zero.576
