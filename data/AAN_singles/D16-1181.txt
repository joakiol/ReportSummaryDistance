Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1754?1764,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLSTM Shift-Reduce CCG ParsingWenduan XuComputer LaboratoryUniversity of Cambridgewx217@cam.ac.ukAbstractWe describe a neural shift-reduce parsingmodel for CCG, factored into four unidirec-tional LSTMs and one bidirectional LSTM.This factorization allows the linearization ofthe complete parsing history, and results ina highly accurate greedy parser that outper-forms all previous beam-search shift-reduceparsers for CCG.
By further deriving a glob-ally optimized model using a task-based loss,we improve over the state of the art by up to2.67% labeled F1.1 IntroductionCombinatory Categorial Grammar (CCG; Steedman,2000) parsing is challenging due to its so-called?spurious?
ambiguity that permits a large num-ber of non-standard derivations (Vijay-Shanker andWeir, 1993; Kuhlmann and Satta, 2014).
To ad-dress this, the de facto models resort to chart-basedCKY (Hockenmaier, 2003; Clark and Curran, 2007),despite CCG being naturally compatible with shift-reduce parsing (Ades and Steedman, 1982).
Morerecently, Zhang and Clark (2011) introduced thefirst shift-reduce model for CCG, which also showedsubstantial improvements over the long-establishedstate of the art (Clark and Curran, 2007).The success of the shift-reduce model (Zhang andClark, 2011) can be tied to two main contributingfactors.
First, without any feature locality restric-tions, it is able to use a much richer feature set;while intensive feature engineering is inevitable, ithas nevertheless delivered an effective and concep-tually simpler alternative for both parameter estima-tion and inference.
Second, it couples beam searchwith global optimization (Collins, 2002; Collins andRoark, 2004; Zhang and Clark, 2008), which makesit less prone to search errors than fully greedy mod-els (Huang et al, 2012).In this paper, we present a neural architecture forshift-reduce CCG parsing based on long short-termmemories (LSTMs; Hochreiter and Schmidhuber,1997).
Our model is inspired by Dyer et al (2015),in which we explicitly linearize the complete historyof parser states in an incremental fashion by requir-ing no feature engineering (Zhang and Clark, 2011;Xu et al, 2014), and no atomic feature sets (Chenand Manning, 2014).
However, a key difference isthat we achieve this linearization without relying onany additional control operations or compositionaltree structures (Socher et al, 2010; Socher et al,2011; Socher et al, 2013), both of which are vitalin the architecture of Dyer et al (2015).
Crucially,unlike the sequence-to-sequence transduction modelof Vinyals et al (2015), which primarily conditionson the input words, our model is sensitive to all as-pects of the parsing history, including arbitrary po-sitions in the input.As another contribution, we present a globalLSTM parsing model by adapting an expected F-measure loss (Xu et al, 2016).
As well as natu-rally incorporating beam search during training, thisloss optimizes the model towards the final evaluationmetric (Goodman, 1996; Smith and Eisner, 2006;Auli and Lopez, 2011b), allowing it to learn shift-reduce action sequences that lead to parses with highexpected F-scores.
We further show the globally op-timized model can be leveraged with greedy infer-ence, resulting in a deterministic parser as accurate1754Dexter likes experimentsNP (S\NP)/NP NP>TS/(S\NP)>BS/NP>SFigure 1: A CCG derivation, in which each point corresponds tothe result of a shift-reduce action.
In this example, composition(B) and application (>) are re actions, and type-raising (T) is aun action.as its beam-search counterpart.On standard CCGBank tests, we clearly outper-form all previous shift-reduce CCG parsers; and bycombining the parser with an attention-based LSTMsupertagger (?4), we obtain further significant im-provements (?5).2 Shift-Reduce CCG ParsingCCG is strongly lexicalized by definition.
A CCGgrammar extracted from CCGBank (Hockenmaierand Steedman, 2007) contains over 400 lexical typesand over 1,500 non-terminals (Clark and Curran,2007), which is an order of magnitude more thanthose of a typical CFG parser.
This lexicalized na-ture raises another unique challenge for parsing?any parsing model for CCG needs to perform lexicaldisambiguation.
This is true even in the approachof Fowler and Penn (2010), in which a context-free cover grammar extracted from CCGBank isused to parse CCG.
Indeed, as noted by Auli andLopez (2011a), the search problem for CCG pars-ing is equivalent to finding an optimal derivationin the weighted intersection of a regular language(generated by the supertagger) and a mildly context-sensitive language (generated by the parser), whichcan quickly become expensive.The shift-reduce paradigm (Aho and Ullman,1972; Yamada and Matsumoto, 2003; Nivre andScholz, 2004) applied to CCG (Zhang and Clark,2011) presents a more elegant solution to this prob-lem by allowing the parser to conduct lexical as-signment ?incrementally?
as a complete parse is be-ing built by the decoder.
This is not possible witha chart-based parser, in which complete derivationsmust be built first.
Therefore, a shift-reduce parseris able to consider a much larger set of categoriesper word for a given input, achieving higher lexi-cal assignment accuracy than the C&C parser (Clarkand Curran, 2007), even with the same supertaggingmodel (Zhang and Clark, 2011; Xu et al, 2014).In our parser, we follow this strategy and adoptthe Zhang and Clark (2011) style shift-reduce tran-sition system, which assumes a set of lexical cate-gories has been assigned to each word using a su-pertagger (Bangalore and Joshi, 1999; Clark andCurran, 2004).
Parsing then proceeds by applyinga sequence of actions to transform the input main-tained on a queue, into partially constructed deriva-tions, kept on a stack, until the queue and availableactions on the stack are both exhausted.
At each timestep, the parser can choose to shift (sh) one of thelexical categories of the front word onto the stack,and remove that word from the queue; reduce (re)the top two subtrees on the stack using a CCG rule,replacing them with the resulting category; or takea unary (un) action to apply a CCG type-raising ortype-changing rule to the stack-top element.
For ex-ample, the deterministic sequence of shift-reduce ac-tions that builds the derivation in Fig.1 is: sh?
NP ,un ?
S/(S\NP), sh ?
(S\NP)/NP , re ?S/NP , sh ?
NP and re ?
S , where we use?
toindicate the CCG category produced by an action.13 LSTM Shift-Reduce Parsing3.1 LSTMRecurrent neural networks (RNNs; e.g., see Elman,1990) are factored into an input layer xt and a hid-den state (layer) ht with recurrent connections, andthey can be represented by the following recurrence:ht = ??
(xt,ht?1), (1)where xt is the current input, ht?1 is the previoushidden state and ?
is a set of affine transformationsparametrized by ?.
Here, we use a variant of RNNreferred to as LSTMs, which augment Eq.
1 with acell state, ct, s.t.ht, ct = ??
(xt,ht?1, ct?1).
(2)Compared with conventional RNNs, this extra fa-cility gives LSTMs more persistent memories over1Our parser models normal-form derivations (Eisner, 1996)in CCGBank.
However, unlike Zhang and Clark (2011), deriva-tions are not restricted to be normal-form during inference.1755longer time delays and makes them less suscepti-ble to the vanishing gradient problem (Bengio et al,1994).
Hence, they are better at modeling temporalevents that are arbitrarily far in a sequence.Several extensions to the vanilla LSTM have beenproposed over time, each with a modified instan-tiation of ??
that exerts refined control over e.g.,whether the cell state could be reset (Gers et al,2000) or whether extra connections are added to thecell state (Gers and Schmidhuber, 2000).
Our in-stantiation is as follows for all LSTMs:it = ?
(Wixxt + Wihht?1 + Wicct?1 + bi)ft = ?
(Wfxxt + Wfhht?1 + Wfcct?1 + bf )ct = ft  ct?1+it  tanh(Wcxxt + Wchht?1 + bc)ot = ?
(Woxxt + Wohht?1 + Wocct + bo)ht = ot  tanh(ct),where ?
is the sigmoid activation and  is theelement-wise product.In addition to unidirectional LSTMs thatmodel an input sequence x0,x1, .
.
.
,xn?1 ina strict left-to-right order, we also use bidirec-tional LSTMs (Graves and Schmidhuber, 2005)(BLSTMs), which read the input from both direc-tions with two independent LSTMs.
At each step,the forward hidden state ht is computed using Eq.
2for t = (0, 1, .
.
.
, n ?
1); and the backward hiddenstate h?t is computed similarly but from the reversedirection for t = (n ?
1, n ?
2, .
.
.
, 0).
Together,the two hidden states at each step t capture both pastand future contexts, and the representation for eachxt is obtained as the concatenation [ht; h?t].3.2 EmbeddingsThe neural network model employed by Chen andManning (2014), and followed by a number of otherparsers (Weiss et al, 2015; Zhou et al, 2015; Am-bati et al, 2016; Andor et al, 2016; Xu et al, 2016)allows higher-order feature conjunctions to be au-tomatically discovered from a set of dense featureembeddings.
However, a set of atomic feature tem-plates, which are only sensitive to contexts from thetop few elements on the stack and queue are stillneeded to dictate the choice of these embeddings.Instead, we dispense with such templates and seekinput: w0 .
.
.
wn?1axiom: 0 : (0, , ?, ?
)goal: 2n?
1 + ?
: (n, ?, ,?
)t : (j, ?, xwj |?,?
)t+ 1 : (j + 1, ?|xwj , ?,?
)(sh; 0 ?
j < n)t : (j, ?|s1|s0, ?,?
)t+ 1 : (j, ?|x, ?,?
?
?x?))
(re; s1s0 ?
x)t : (j, ?|s0, ?,?
)t+ 1 : (j, ?|x, ?,?)
(un; s0 ?
x)Figure 2: The shift-reduce deduction system.
For the sh de-duction, xwj denotes an available lexical category for wj ; forre, ?x?
denotes the set of dependencies on x.to design a model that is sensitive to both local andnon-local contexts, on both the stack and queue.Consequently, embeddings represent atomic inputunits that are added to our parser and are preservedthroughout parsing.
In total we use four types ofembeddings, namely, word, CCG category, POS andaction, where each has an associated look-up tablethat maps a string of that type to its embedding.
Thelook-up table for words is Lw ?
Rk?|w|, where k isthe embedding dimension and |w| is the size of thevocabulary.
Similarly, we have look-up tables forCCG categories, Lc ?
Rl?|c|, for the three types ofactions, La ?
Rm?3, and for POS tags, Lp ?
Rn?|p|.3.3 ModelParser.
Fig.
2 shows the deduction system of ourparser.2 We denote each parse item as (j, ?, ?,?
),where j is the positional index of the word at thefront of the queue, ?
is the stack (with its top ele-ment s0 to the right), and ?
is the queue (with itstop element wj to the left) and ?
is the set of CCGdependencies realized for the input consumed so far.Each item is also associated with a step indicator t,signifying the number of actions applied to it andthe goal is reached in 2n ?
1 + ?
steps, where ?
isthe total number of un actions.
We also define eachaction in our parser as a 4-tuple (?t, ct, wct , pwct ),where ?t ?
{sh, re, un} for t ?
1, ct is the resultingcategory of ?t, and wct is the head word attached to2We assume greedy inference unless otherwise stated.1756Figure 3: An example representation for a parse item at timestep t, with the 4 unidirectional LSTMs (left) and the bidirec-tional LSTM (right).
The shaded cells on the left represent?t = [hUt ;hVt ;hXt ;hYt ] (Eq.
3); and the shaded cells on the rightrepresent wj = [hWj ; h?Wj ].ct with pwct being its POS tag.3LSTM model.
LSTMs are designed to handletime-series data, in a purely sequential fashion; andwe try to exploit this fact by completely linearizingall aspects of the parsing history.
Concretely, we fac-tor the model as five LSTMs, comprising four uni-directional ones, denoted as U, V, X and Y, and anadditional BLSTM, denoted as W (Fig.
3).
Beforeparsing each sentence, we feed W with the completeinput (padded with a special embedding?
as the endof sentence token); and we use wj = [hWj ; h?Wj ] torepresent wj in subsequent steps.4 We also add ?
tothe other 4 unidirectional LSTMs as initialization.Given this factorization, the stack representationfor a parse item (j, ?, ?,?)
at step t, for t ?
1, isobtained as?t = [hUt ;hVt ;hXt ;hYt ], (3)and together with wj , [?t;wj ] gives a representationfor the parse item.
For the axiom item, we repre-sent it as [?0;w0], where ?0 = [hU?;hV?;hX?;hY?]
isa representation for its stack.Each time the parser applies an action(?t, ct, wct , pwct ), we update the model by addingthe embedding of ?t, denoted as La(?t), onto U,and adding the other three embeddings of the action4-tuple, namely Lc(ct), Lw(wct) and Lp(pwct ),onto V, X and Y respectively.To predict the next action, we first derive an actionhidden layer bt, by passing the parse item represen-tation [?t;wj ] through an affine transformation, s.t.bt = f(B[?t;wj ] + r), (4)3In case of multiple heads, we always choose the first one.4Word and POS embeddings are concatenated at each inputposition j, for 0 ?
j < n; and wn = [hW?
; h?W?
].where B is a parameter matrix of the model, r isa bias vector and f is a ReLU non-linearity (Nairand Hinton, 2010).
Then we apply another affinetransformation (with A as the weights and s as thebias) to bt:at = f(Abt + s),and obtain the probability of the ith action in at asp(?
it |bt) =exp{ait}?
?kt ?T (?t,?t) exp{akt },where T (?t, ?t) is the set of feasible actions for thecurrent parse item, and ?
it ?
T (?t, ?t).3.4 Derivations and Dependency StructuresOur model naturally linearizes CCG derivations ?in-crementally?
following their post-order traversals.As such, the four unidirectional LSTMs always havethe same number of steps; and at each step, the con-catenation of their hidden states (Eq.
3) represents apoint in a CCG derivation (i.e., an action 4-tuple).Due to the large amount of flexibility in how de-pendencies are realized in CCG (Hockenmaier, 2003;Clark and Curran, 2007), and in line with most ex-isting CCG parsing models, including dependencymodels, we have chosen to model CCG derivations,rather than dependency structures.5 We also hypoth-esize that tree structures are not necessary for thecurrent model, since they are already implicit in thelinearized derivations; and similarly, we have foundthe action embeddings to be nonessential (?5.2).3.5 TrainingAs a baseline, we first train a greedy model, inwhich we maximize the log-likelihood of each tar-get action in the training data.
More specifically, let(?
g1 , .
.
.
, ?gTn) be the gold-standard action sequencefor a training sentence n, a cross-entropy criterion isused to obtain the error gradients, and for each sen-tence, training involves minimizingL(?)
= ?
logTn?t=1p(?
gt |bt) = ?Tn?t=1log p(?
gt |bt),where ?
is the set of all parameters in the model.5Most CCG dependency models (e.g., see Clark and Curran(2007) and Xu et al (2014)) model CCG derivations with de-pendency features.1757As other greedy models (e.g., see Chen and Man-ning (2014) and Dyer et al (2015)), our greedymodel is locally optimized, and suffer from the labelbias problem (Andor et al, 2016).
A partial solutionto this is to use beam search at test time, thereby re-covering higher scoring action sequences that wouldotherwise be unreachable with fully greedy infer-ence.
In practice, this has limited effect (Table 2),and a number of more principled solutions have beenrecently proposed to derive globally optimized mod-els during training (Watanabe and Sumita, 2015;Weiss et al, 2015; Zhou et al, 2015; Andor et al,2016).
Here, we extend our greedy model into aglobal one by adapting the expected F-measure lossof Xu et al (2016).
To our best knowledge, this isthe first attempt to train a globally optimized LSTMshift-reduce parser.Let ?
= {U,V,X,Y,W,B,A} be the weightsof the baseline greedy model,6 we initialize theweights of the global model, which has the same ar-chitecture as the baseline, to ?, and we reoptimize ?in multiple training epochs as follows:1.
Pick a sentence xn from the training set, decodeit with beam search, and generate a k-best listof output parses with the current ?, denoted as?(xn).72.
For each parse yi in ?
(xn), compute itssentence-level F1 using the set of dependenciesin the ?
field of its parse item.
In addition, let|yi| be the total number of actions that derivedyi and s?
(yji ) be the softmax action score of thejth action, given by the LSTM model.
Com-pute the log-linear score of its action sequenceas ?
(yi) = ?|yi|j=1 log s?
(yji ).3.
Compute the negative expected F1 objective(defined below) for xn and minimize it usingstochastic gradient descent (maximizing ex-pected F1).
Repeat these three steps for the re-maining sentences.6We use boldface letters to designate the weights of the cor-responding LSTMs, and omit bias terms for brevity.7As in Xu et al (2016), we did not preset k, and found k =11.06 on average with a beam size of 8 that we used for thistraining.More formally, the loss J(?
), is defined asJ(?)
= ?XF1(?
)= ??yi??(xn)p(yi|?
)F1(?yi ,?Gxn),where F1(?yi ,?Gxn) is the sentence level F1 of theparse derived by yi, with respect to the gold-standarddependency structure ?Gxn of xn; p(yi|?)
is the nor-malized probability score of the action sequence yi,computed asp(yi|?)
=exp{??(yi)}?y??
(xn) exp{??
(y)},where ?
is a parameter that sharpens or flattens thedistribution (Tromble et al, 2008).8 Different fromthe maximum-likelihood objective, XF1 optimizesthe model on a sequence level and towards the finalevaluation metric, by taking into account all actionsequences in ?
(xn).4 Attention-Based LSTM SupertaggingIn addition to the size of the label space, supertag-ging is difficult because CCG categories can encodelong-range dependencies and tagging decisions fre-quently depend on non-local contexts.
For example,in He went to the zoo with a cat, a possible cate-gory for with, (S\NP)\(S\NP)/NP , depends onthe word went further back in the sentence.Recently a number of RNN models have beenproposed for CCG supertagging (Xu et al, 2015;Lewis et al, 2016; Vaswani et al, 2016; Xu et al,2016), and such models show dramatic improve-ments over non-recurrent models (Lewis and Steed-man, 2014b).
Although the underlying models differin their exact architectures, all of them make eachtagging decision using only the hidden states at thecurrent input position, and this imposes a potentialbottleneck in the model.
To mitigate this, we gen-eralize the attention mechanisms of Bahdanau et al(2015) and Luong et al (2015), and adapt them tosupertagging, by allowing the model to explicitlyuse hidden states from more than one input posi-tions for tagging each word.
Similar to Bahdanau etal.
(2015) and Luong et al (2015), a key feature in8We found ?
= 1 to be a good choice during development.1758our model is a soft alignment vector that weights therelative importance of the considered hidden states.For an input sentence w0, w1, .
.
.
, wn?1, we con-sider wt = [ht; h?t] (?3.1) to be the representa-tion of the tth word (0 ?
t < n, wt ?
R2d?1),given by a BLSTM with a hidden state size d forboth its forward and backward layers.9 Let k bea context window size hyperparameter, we defineHt ?
R2d?
(k?1) asHt = [wt?bk/2c, .
.
.
,wt?1,wt+1, .
.
.
,wt+bk/2c],which contains representations for all words in thesize k window except wt.
At each position t, theattention model derives a context vector ct ?
R2d?1(defined below) from Ht, which is used in conjunc-tion with wt to produce an attentional hidden layer:xt = f(M[ct;wt] + m), (5)where f is a ReLU non-linearity, M ?
Rg?4d is alearned weight matrix, m is a bias term, and g is thesize of xt.
Then xt is used to produce another hiddenlayer (with N as the weights and n as the bias):zt = Nxt + n,and the predictive distribution over categories is ob-tained by feeding zt through a softmax activation.In order to derive the context vector ct, we firstcompute bt ?
R(k?1)?1 from Ht and wt using ?
?R1?4d, s.t.
the ith entry in bt isbit = ?
[wT [i];wt],for i ?
[0, k?1), T = [t?bk/2c, .
.
.
, t?1, t+1, .
.
.
, t+bk/2c]; and ct is derived as follows:at = softmax(bt),ct = Htat,where at is the alignment vector.
We also exper-iment with two types of attention reminiscent ofthe global and local models in Luong et al (2015),where the first attends over all input words (k = n)and the second over a local window.It is worth noting that two other works have con-currently tackled supertagging with BLSTM mod-els.
In Vaswani et al (2016), a language model9Unlike in the parsing model, POS tags are excluded.layer is added on top of a BLSTM, which allowsembeddings of previously predicted tags to propa-gate through and influence the pending tagging de-cision.
However, the language model layer is onlyeffective when both scheduled sampling for train-ing (Bengio et al, 2015) and beam search for infer-ence are used.
We show our attention-based mod-els can match their performance, with only standardtraining and greedy decoding.
Additionally, Lewiset al (2016) presented a BLSTM model with twolayers of stacking in each direction; and as an inter-nal baseline, we show a non-stacking BLSTM with-out attention can achieve the same accuracy.5 ExperimentsDataset and baselines.
We conducted all experi-ments on CCGBank (Hockenmaier and Steedman,2007) with the standard splits.10 We assigned POStags with the C&C POS tagger, and used 10-foldjackknifing for both POS tagging and supertagging.All parsers were evaluated using F1 over labeledCCG dependencies.For supertagging, the baseline models are theRNN model of Xu et al (2015), the bidirectionalRNN (BRNN) model of Xu et al (2016), andthe BLSTM supertagging models in Vaswani et al(2016) and Lewis et al (2016).
For parsing exper-iments, we compared with the global beam-searchshift-reduce parsers of Zhang and Clark (2011)and Xu et al (2014).
One neural shift-reduce CCGparser baseline is Ambati et al (2016), which is abeam-search shift-reduce parser based on Chen andManning (2014) and Weiss et al (2015); and the oth-ers are the RNN shift-reduce models in Xu et al(2016).
Additionally, the chart-based C&C parserwas included by default.Model and training parameters.11 All ourLSTM models are non-stacking with a singlelayer.12 For the supertagging models, the LSTM10Training: Sections 02-21 (39,604 sentences).
Develop-ment: Section 00 (1,913 sentences).
Test: Section 23 (2,407sentences).11We implemented all models using the CNN toolkit:https://github.com/clab/cnn.12The BLSTMs have a single layer in each direction.
Weexperimented with 2 layers in all models during developmentand found negligible improvements.1759Model Dev TestC&C 91.50 92.02Xu et al (2015) 93.07 93.00Xu et al (2016) 93.49 93.52Lewis et al (2016) 94.1 94.3Vaswani et al (2016) 94.08 -Vaswani et al (2016) +LM +beam 94.24 94.50BLSTM 94.11 94.29BLSTM-local 94.31 94.46BLSTM-global 94.22 94.42Table 1: 1-best supertagging results on both the dev and testsets.
BLSTM is the baseline model without attention; BLSTM-local and -global are the two attention-based models.hidden state size is 256, and the size of the atten-tional hidden layer (xt, Eq.
5) is 200.
All parsingmodel LSTMs have a hidden state size of 128, andthe size of the action hidden layer (bt, Eq.
4) is 80.Pretrained word embeddings for all models are100-dimensional (Turian et al, 2010), and all otherembeddings are 50-dimensional.
We also pretrainedCCG lexical category and POS embeddings on theconcatenation of the training data and a Wikipediadump parsed with C&C.13 All other parameters wereuniformly initialized in ?
?6/(r + c), where r andc are the number of rows and columns of a ma-trix (Glorot and Bengio, 2010).For training, we used plain non-minibatchedstochastic gradient descent with an initial learningrate ?0 = 0.1 and we kept iterating in epochs untilaccuracy no longer increases on the dev set.
For allmodels, a learning rate schedule ?e = ?0/(1 + ?e)with ?
= 0.08 was used for e ?
11.
Gradients wereclipped whenever their norm exceeds 5.
Dropouttraining as suggested by Zaremba et al (2014), witha dropout rate of 0.3, and an `2 penalty of 1?
10?5,were applied to all models.5.1 Supertagging ResultsTable 1 summarizes 1-best supertagging results.
Ourbaseline BLSTM model without attention achievesthe same level of accuracy as Lewis et al (2016)and the baseline BLSTM model of Vaswani et al(2016).
Compared with the latter, our hidden statesize is 50% smaller (256 vs. 512).For training and testing the local attention model(BLSTM-local), we used an attention window size13We used the gensim word2vec toolkit: https://radimrehurek.com/gensim/.Supertagger ?Beam 0.09 0.07 0.06 0.01 0.0011 86.49 86.52 86.56 86.26 85.802 86.55 86.58 86.63 86.39 86.018 86.61 86.64 86.67 86.40 86.07Table 2: Tuning beam size and supertagger ?
on the dev set.Model LP LR LF CATLSTM-w 90.13 76.99 83.05 94.24LSTM-w+c 89.37 83.25 86.20 94.34LSTM-w+c+a 89.31 83.39 86.25 94.38LSTM-w+c+a+p 89.43 83.86 86.56 94.47Table 3: F1 on dev for all the greedy models.of 5 (tuned on the dev set), and it gives an improve-ment of 0.94% over the BRNN supertagger (Xuet al, 2016), achieving an accuracy on par withthe beam-search (size 12) model of Vaswani et al(2016) that is enhanced with a language model.
De-spite being able to consider wider contexts than thelocal model, the global attention model (BLSTM-global) did not show further gains, hence we usedBLSTM-local for all parsing experiments below.5.2 Parsing ResultsAll parsers we consider use a supertagger probabil-ity cutoff ?
to prune categories less likely than ?times the probability of the best category in a distri-bution: for the C&C parser, it uses an adaptive strat-egy to backoff to more relaxed ?
values if no span-ning analysis is found given an initial ?
setting; forall the shift-reduce parsers, fixed ?
values are usedwithout backing off.
Since ?
determines the deriva-tion space of a parser, it has a large impact on thefinal parsing accuracy.For the maximum-likelihood greedy model, wefound using a small ?
value (bigger ambiguity) fortraining significantly improved accuracy, and wechose ?
= 1?
10?5 (5.22 categories per word withjackknifing) via development experiments.
This re-inforces the findings in a number of other CCGparsers (Clark and Curran, 2007; Auli and Lopez,2011a; Zhang and Clark, 2011; Lewis and Steed-man, 2014a; Xu et al, 2014): even though a morerelaxed ?
increases ambiguity, it leads to more ac-curate models at test time.
On the other hand, wefound using smaller ?
values at test time led to sig-nificantly better results (Table 2).
And this observa-tion differs from the beam-search models which usethe same ?
value for both training and testing.1760787980818283848586870  5  10  15  20  25  30F1(labeled) ondevsetTraining epochsLSTM-wLSTM-wcLSTM-wcaLSTM-wcap(a) Dev F1 of the greedy models86.686.786.886.98787.187.287.387.487.50  5  10  15  20F1(labeled) ondevsetTraining epochsLSTM-XF1 (beam = 8)(b) Dev F1 of the XF1 modelFigure 4: Learning curves with dev F-scores for all models.Section 00 Section 23Model Beam LP LR LF CAT LP LR LF CATC&C (normal-form) - 85.18 82.53 83.83 92.39 85.45 83.97 84.70 92.83C&C (dependency hybrid) - 86.07 82.77 84.39 92.57 86.24 84.17 85.19 93.00Zhang and Clark (2011) 16 87.15 82.95 85.00 92.77 87.43 83.61 85.48 93.12Xu et al (2014) 128 86.29 84.09 85.18 92.75 87.03 85.08 86.04 93.10Ambati et al (2016) 16 - - 85.69 93.02 - - 85.57 92.86Xu et al (2016)-greedy 1 88.12 81.38 84.61 93.42 88.53 81.65 84.95 93.57Xu et al (2016)-XF1 8 88.20 83.40 85.73 93.56 88.74 84.22 86.42 93.87LSTM-greedy 1 89.43 83.86 86.56 94.47 89.75 84.10 86.83 94.63LSTM-XF1 1 89.68 85.29 87.43 94.41 89.85 85.51 87.62 94.53LSTM-XF1 8 89.54 85.46 87.45 94.39 89.81 85.81 87.76 94.57Table 4: Parsing results on the dev (Section 00) and test (Section 23) sets with 100% coverage, with all LSTM models using theBLSTM-local supertagging model.
All experiments using auto POS.
CAT (lexical category assignment accuracy).
LSTM-greedyis the full greedy parser.The greedy model.
Table 3 shows the dev set re-sults for all greedy models, where the four typesof embeddings, that is, word (w), CCG category(c), action (a) and POS (p), are gradually intro-duced.
The full model LSTM-w+c+a+p surpassesall previous shift-reduce models (Table 4), achiev-ing a dev set accuracy of 86.56%.
Category em-beddings (LSTM-w+c) yielded a large gain over us-ing word embeddings alone (LSTM-w); action em-beddings (LSTM-w+c+a) provided little improve-ment, but further adding POS embeddings (LSTM-w+c+a+p) gave noticeable recall (+0.61%) and F1improvements (+0.36%) over LSTM-w+c.
Fig.
4ashows the learning curves, where all models con-verged in under 30 epochs.The XF1model.
Table 4 also shows the results forthe XF1 models (LSTM-XF1), which use all fourtypes of embeddings.
We used a beam size of 8, andModel Dev TestLSTM-BRNN 85.86 86.37LSTM-BLSTM 86.26 86.64LSTM-greedy 86.56 86.83Table 5: Effect of different supertaggers on the full greedyparser.
LSTM-greedy is the same parser as in Table 4, whichuses the BLSTM-local supertagger.a ?
value of 0.06 for both training and testing (tunedon the dev set); and training took 12 epochs to con-verge (Fig.
4b), with an F1 of 87.45% on the devset.
Decoding the XF1 model with greedy inferenceonly slightly decreased recall and F1, and this re-sulted in a highly accurate deterministic parser.
Onthe test set, our XF1 greedy model gives +2.67%F1 improvement over the greedy model in Xu et al(2016); and the beam-search XF1 model achieves anF1 improvement of +1.34% compared with the XF1model of Xu et al (2016).1761Model LP LR LFXu et al (2015) 87.68 86.41 87.04Lewis et al (2016) 87.7 86.7 87.2Lewis et al (2016)?
88.6 87.5 88.1Vaswani et al (2016)?
- - 88.32Lee et al (2016) - - 88.7LSTM-XF1 (beam = 1) 89.85 85.51 87.62LSTM-XF1 (beam = 8) 89.81 85.81 87.76Table 6: Comparison of our XF1 models with chart-basedparsers on the test set.
?
denotes a tri-trained model and ?
indi-cates a different POS tagger.Effect of the supertagger.
To isolate the parsingmodel from the supertagging model, we first ex-perimented with the BRNN supertagging model asin Xu et al (2016) for both training and testingthe full greedy LSTM parser.
Using this supertag-ger, we still achieved the highest F1 (85.86%) onthe dev set (LSTM-BRNN, Table 5) in compari-son with all previous shift-reduce models; and animprovement of 1.42% F1 over the greedy modelof Xu et al (2016) was obtained on the test set(Table 4).
We then experimented with using thebaseline BLSTM supertagging model for parsing(LSTM-BLSTM), and observed the attention-basedsetup (LSTM-greedy) outperformed it, despite theattention-based supertagger (BLSTM-local) did notgive better multi-tagging accuracy.
We owe this tothe fact that very tight ?
cutoff values?resultingin almost deterministic supertagging decisions onaverage?are required by the parser during infer-ence; for instance, BLSTM-local has an average am-biguity of 1.09 on the dev set with ?
= 0.06.14Comparison with chart-based models.
For com-pleteness and to put our results in perspective, wecompare our XF1 models with other CCG parsersin the literature (Table 6): Xu et al (2015) is thelog-linear C&C dependency hybrid model with anRNN supertagger front-end; Lewis et al (2016)is an LSTM supertagger-factored parser using theA?
CCG parsing algorithm of Lewis and Steed-man (2014a); Vaswani et al (2016) combine aBLSTM supertagger with a new version of the C&Cparser (Clark et al, 2015) that uses a max-violationpercetrpon, which significantly improves over the14All ?
cutoffs were tuned on the dev set; for BRNN, wefound the same ?
settings as in Xu et al (2016) to be optimal;for BLSTM, ?
= 4 ?
10?5 for training (with an ambiguity of5.27) and ?
= 0.02 for testing (with an ambiguity of 1.17).original C&C models; and finally, a global recursiveneural network model with A?
decoding (Lee et al,2016).
We note that all these alternative models?with the exception of Xu et al (2015) and Lewis etal.
(2016)?use structured training that accounts forviolations of the gold-standard, and we conjecturefurther improvements for our model are possible byincorporating such mechanisms.156 ConclusionWe have presented an LSTM parsing model for CCG,with a factorization allowing the linearization of thecomplete parsing history.
We have shown that thissimple model is highly effective, with results out-performing all previous shift-reduce CCG parsers.We have also shown global optimization benefits anLSTM shift-reduce model; and contrary to previousfindings with the averaged percetpron (Zhang andClark, 2008), we empirically demonstrated beam-search inference is not necessary for our globally op-timized model.
For future work, a natural directionis to explore integrated supertagging and parsing ina single neural model (Zhang and Weiss, 2016).AcknowledgmentI acknowledge the support from the Carnegie Trustfor the Universities of Scotland through a CarnegieScholarship.ReferencesAnthony Ades and Mark Steedman.
1982.
On the orderof words.
In Linguistics and philosophy.
Springer.Alfred Aho and Jeffrey Ullman.
1972.
The theory ofparsing, translation, and compiling.
Prentice-Hall.Bharat Ram Ambati, Tejaswini Deoskar, and Mark Steed-man.
2016.
Shift-reduce CCG parsing using neuralnetwork models.
In Proc.
of NAACL (Volume 2).Daniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally nor-malized transition-based neural networks.
In Proc.
ofACL.Michael Auli and Adam Lopez.
2011a.
A comparison ofloopy belief propagation and dual decomposition for15Our XF1 training considers shift-reduce action sequences,but not violations of the gold-standard (e.g., see Huang etal.
(2012), Watanabe and Sumita (2015), Zhou et al (2015)and Andor et al (2016)).1762integrated CCG supertagging and parsing.
In Proc.
ofACL.Michael Auli and Adam Lopez.
2011b.
Training a log-linear parser with loss functions via softmax-margin.In Proc.
of EMNLP.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proc.
of ICLR.Srinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
In Computa-tional linguistics.
MIT Press.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gradientdescent is difficult.
In Neural Networks, IEEE Trans-actions on.
IEEE.Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and NoamShazeer.
2015.
Scheduled sampling for sequence pre-diction with recurrent neural networks.
In Proc.
ofNIPS.Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.
InProc.
of EMNLP.Stephen Clark and James Curran.
2004.
The importanceof supertagging for wide-coverage CCG parsing.
InProc.
of COLING.Stephen Clark and James Curran.
2007.
Wide-coverageefficient statistical parsing with CCG and log-linearmodels.
In Computational Linguistics.
MIT Press.Stephen Clark, Darren Foong, Luana Bulat, and Wend-uan Xu.
2015.
The Java version of the C&C parser.Technical report, University of Cambridge ComputerLaboratory.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proc.
ofACL.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In Proc.
of ACL.Jason Eisner.
1996.
Efficient normal-form parsing forCombinatory Categorial Grammar.
In Proc.
of ACL.Jeffrey Elman.
1990.
Finding structure in time.
In Cog-nitive science.
Elsevier.Timothy Fowler and Gerald Penn.
2010.
Accu-rate context-free parsing with Combinatory CategorialGrammar.
In Proc.
of ACL.Felix Gers and Ju?rgen Schmidhuber.
2000.
Recurrentnets that time and count.
In Neural Networks.
IEEE.Felix Gers, Ju?rgen Schmidhuber, and Fred Cummins.2000.
Learning to forget: Continual prediction withLSTM.
In Neural computation.
MIT Press.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In Proc.
of AISTATS.Joshua Goodman.
1996.
Parsing algorithms and metrics.In Proc.
of ACL.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional LSTMand other neural network architectures.
In Neural Net-works.
Elsevier.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
In Neural computation.
MITPress.Julia Hockenmaier and Mark Steedman.
2007.
CCG-Bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
In Com-putational Linguistics.
MIT Press.Julia Hockenmaier.
2003.
Data and Models for Statis-tical Parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.of NAACL.Marco Kuhlmann and Giorgio Satta.
2014.
A new pars-ing algorithm for Combinatory Categorial Grammar.In Transactions of the Association for ComputationalLinguistics.
ACL.Kenton Lee, Mike Lewis, and Luke Zettlemoyer.
2016.Global neural CCG parsing with optimality guaran-tees.
In Proc.
of EMNLP.Mike Lewis and Mark Steedman.
2014a.
A* CCGparsing with a supertag-factored model.
In Proc.
ofEMNLP.Mike Lewis and Mark Steedman.
2014b.
Improved CCGparsing with semi-supervised supertagging.
In Trans-actions of the Association for Computational Linguis-tics.
ACL.Mike Lewis, Kenton Lee, and Luke Zettlemoyer.
2016.LSTM CCG parsing.
In Proc.
of NAACL.Minh-Thang Luong, Hieu Pham, and Christopher Man-ning.
2015.
Effective approaches to attention-basedneural machine translation.
In Proc.
of EMNLP.Vinod Nair and Geoffrey Hinton.
2010.
Rectified linearunits improve restricted boltzmann machines.
In Proc.of ICML.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proc.
of COL-ING.David Smith and Jason Eisner.
2006.
Minimum-riskannealing for training log-linear models.
In Proc.
ofCOLING-ACL.Richard Socher, Christopher Manning, and Andrew Ng.2010.
Learning continuous phrase representations andsyntactic parsing with recursive neural networks.
In1763Proc.
of the NIPS Deep Learning and UnsupervisedFeature Learning Workshop.Richard Socher, Cliff Lin, Christopher Manning, and An-drew Ng.
2011.
Parsing natural scenes and naturallanguage with recursive neural networks.
In Proc.
ofICML.Richard Socher, John Bauer, Christopher Manning, andAndrew Ng.
2013.
Parsing with compositional vectorgrammars.
In Proc.
of ACL.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Roy Tromble, Shankar Kumar, Franz Och, and WolfgangMacherey.
2008.
Lattice minimum bayes-risk de-coding for statistical machine translation.
In Proc.
ofEMNLP.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proc.
of ACL.Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and RyanMusa.
2016.
Supertagging with LSTMs.
In Proc.
ofNAACL (Volume 2).Krishnamurti Vijay-Shanker and David Weir.
1993.Parsing some constrained grammar formalisms.
InComputational Linguistics.
MIT Press.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Proc.
of NIPS.Taro Watanabe and Eiichiro Sumita.
2015.
Transition-based neural constituent parsing.
In Proc.
of ACL.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proc.
of ACL.Wenduan Xu, Stephen Clark, and Yue Zhang.
2014.Shift-reduce CCG parsing with a dependency model.In Proc.
of ACL.Wenduan Xu, Michael Auli, and Stephen Clark.
2015.CCG supertagging with a recurrent neural network.
InProc.
of ACL (Volume 2).Wenduan Xu, Michael Auli, and Stephen Clark.
2016.Expected F-measure training for shift-reduce parsingwith recurrent neural networks.
In Proc.
of NAACL.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis using support vector machines.In Proc.
of IWPT.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.
InProc.
of ICLR.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proc.
of EMNLP.Yue Zhang and Stephen Clark.
2011.
Shift-reduce CCGparsing.
In Proc.
of ACL.Yuan Zhang and David Weiss.
2016.
Stack-propagation:Improved representation learning for syntax.
In Proc.of ACL.Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen.2015.
A neural probabilistic structured-predictionmodel for transition-based dependency parsing.
InProc.
of ACL.1764
