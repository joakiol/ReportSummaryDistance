Detecting and Correcting Speech RepairsPeter Heeman and James AllenDepartment ofComputer ScienceUniversity of RochesterRochester, New York, 14627{heeman, j ames}@cs, rochester, eduAbstractInteractive spoken dialog provides many new challenges forspoken language systems.
One of the most critical is theprevalence of speech repairs.
This paper presents an al-gorithm that detects and corrects peech repairs based onfinding the repair pattern.
The repair pattern is built by find-ing word matches and word replacements, and identifyingfragments and editing terms.
Rather than using a set of pre-built templates, we build the pattern on the fly.
In a fair test,our method, when combined with a statistical model to filterpossible repairs, was successful at detecting and correcting80% of the repairs, without using prosodic information or aparser.IntroductionInteractive spoken dialog provides many new challenges forspoken language systems.
One of the most critical is theprevalence ofspeech repairs.
Speech repairs are dysfluencieswhere some of the words that the speaker utters need tobe removed in order to correctly understand the speaker'smeaning.
These repairs can be divided into three types:fresh starts, modifications, and abridged.
A fresh start iswhere the speaker abandons what she was saying and startsagain.the current plan is we take - okay let's say we start with thebananas (d91-2.2 uttl05)A modification repair is where the speech-repair modifieswhat was said before.after the orange juice is at - the oranges are at the OJ factory(d93-19.3 utt59)An abridged repair is where the repair consists olely of afragment and/or editing terms.we need to -um manage to get the bananas to Dansville morequickly (d93-14.3 utt50)These examples also illustrate how speech repairs can bedivided into three intervals: the removed text, the editingterms, and the resumed text (cf.
Levelt, (1983), Nakataniand Hirschberg, (1993)).
The removed text, which mightend in a word fragment, is the text that the speaker intends toreplace.
The end of the removed text is called the interruptionpoint, which is marked in the above examples as "-".
Thisis then followed by editing terms, which can either be filledpauses, such as "urn", "uh", and "er", or cue phrases, suchas "I mean", "I guess", and "well".
The last interval is theresumed text, the text that is intended to replace the removedtext.
(All three intervals need notbe present in a given speechrepair.)
In order to correct a speech repair, the removed textand the editing terms need to be deleted in order to determinewhat the speaker intends to say.
1In our corpus of problem solving dialogs, 25% of turnscontain at least one repair, 67% of repairs occur with at leastone other repair in the turn, and repairs in the same turnoccur on average within 6 words of each other.
As a result,no spoken language system will perform well without aneffective way to detect and correct speech repairs.We propose that most speech repairs can be detected andcorrected using only local clues--it should not be neces-sary to test the syntactic or semantic well-formedness of theentire utterance.
People do not seem to have problems com-prehending speech repairs as they occur, and seem to haveno problem even when multiple repairs occur in the sameutterance.
So, it should be possible to construct an algorithmthat runs on-line, processing the input a word at a time, andcommitting to whether a string of words is a repair by theend of the string.
Such an algorithm could precede aparser,or even operate in lockstep with it.An ulterior motive for not using higher level syntactic orsemantic knowledge is that the coverage of parsers and se-mantic interpreters i  not sufficient for unrestricted dialogs.Recently, Dowding et al (1993) reported syntactic and se-mantic overage of 86% for the DARPA Airline reservationcorpus (Dowding et al, 1993).
Unrestricted ialogs willpresent even more difficulties; not only will the speech be lessgrammatical, but there is also the problem of segmenting thedialog into utterance units (cf.
Wang and Hirschberg, 1992)?If speech repairs can be detected and corrected before pars-ing and semantic interpretation, this should simplify thosemodules as well as make them more robust.In this paper, we present an algorithm that detects andcorrects modification and abridged speech repairs withoutdoing syntactic and semantic processing.
The algorithm de-termines the text hat needs to be removed by building a repairpattern, based on identification of word fragments, editing~The removed text and editing terms might still contain prag-matic information, as the following example displays, "Peter was?..
well.., he was fired.295terms, and word correspondences between the removed andthe resumed text (cf.
Bear, Dowding and Shriberg, 1992).The resulting potential repairs are then passed to a statis-tical model that judges the proposal as either fluent speechor an actual repair.Previous WorkSeveral different strategies have been discussed in the liter-ature for detecting and correcting speech repairs.
A way tocompare the effectiveness of these approaches i  to look attheir recall and precision rates.
For detecting repairs, therecall rate is the number of correctly detected repairs com-pared to the number of repairs, and the precision rate is thenumber of detected repairs compared to the number of de-tections (including false positives).
But the true measuresof success are the correction rates.
Correction recall is thenumber of repairs that were properly corrected compared tothe number of repairs.
Correction precision is the numberof repairs that were properly corrected compared to the totalnumber of corrections.Levelt (1983) hypothesized that listeners can use the fol-lowing rules for determining the extent of the removed text(he did not address how a repair could be detected).
I f the lastword before the interruption is of the same category as theword before, then delete the last word before the interruption.Otherwise, find the closest word prior to the interruption thatis the same as the first word after the interruption.
That wordis the start of the removed text.
Levelt found that this strategywould work for 50% of all repairs (including fresh starts), get2% wrong, and have no comment for the remaining 48%.
2In addition, Levelt showed that different editing terms makedifferent predictions about whether a repair is a fresh startor not.
For instance, "uh" strongly signals an abridged ormodification repair, whereas a word like "sorry" signals afresh start.Hindle (1983) addressed the problem of correcting self-repairs by adding rules to a deterministic parser that wouldremove the necessary text.
Hindle assumed the presence ofan edit signal that would mark the interruption point, andwas able to achieve arecall rate of 97% in finding the correctrepair.
For modification repairs, Hindle used three rulesfor "expuncting" text.
The first rule "is essentially a non-syntactic rule" that matches repetitions (of any length); thesecond matches repeated constituents, both complete; andthe third, matches repeated constituents, in which the first isnot complete, but the second is.However, Hindle's results are difficult to translate intoactual performance.
First, his parsing strategy depends uponthe "successful disambiguation f the syntactic ategories.
"Although syntactic ategories can be determined quite wellby their local context (as is needed by a deterministic parser),Hindle admits that "\[self-repair\], by its nature, disrupts thelocal context."
Second, Hindle's algorithm depends on thepresence of an edit signal; so far, however, the abrupt cut-off2Levelt claims (pg.
92) that the hearer can apply his strategysafely for 52% of all repairs, but this figure includes the 2% that hehearer would get wrong.that some have suggested signals the repair (cf.
Labov, 1966)has been difficult to find, and it is unlikely to be representedas a binary feature (cf.
Nakatani and Hirschberg, 1993).The SRI group (Bear et al, 1992) employed simple patternmatching techniques for detecting and correcting modifica-tion repairs.
3For detection, they were able to achieve arecallrate of 76%, and a precision of 62%, and they were able tofind the correct repair 57% of the time, leading to an over-all correction recall of 43% and correction precision of 50%.They also tried combining syntactic and semantic knowledgein a "parser-first" approach--first try to parse the input andif that fails, invoke repair strategies based on word patternsin the input.
In a test set containing 26 repairs (Dowdinget al, 1993), they obtained adetection recall rate of 42% anda precision of 84.6%; for correction, they obtained a recallrate of 30% and a recall rate of 62%.Nakatani and Hirschberg (1993) investigated using acous-tic information to detect he interruption point of speech re-pairs.
In their corpus, 74% of all repairs are marked bya word fragment.
Using hand-transcribed prosodic annota-tions, they trained a classifier on a 172 utterance trainingset to identify the interruption point (each utterance con-tained at least one repair).
On a test set of 186 utteranceseach containing at least one repair, they obtained a recallrate of 83.4% and a precision of 93.9% in detecting speechrepairs.
The clues that they found relevant were durationof pause between words, presence of fragments, and lexicalmatching within a window of three words.
However, theydo not address the problem of determining the correction ordistinguishing modification repairs from abridged repairs.Young and Matessa (Young and Matessa, 1991) have alsodone work in this area.
In their approach, speech repairs arecorrected after a opportunistic case-frame parser analyzes theutterance.
Their system looks for parts of the input utterancethat were not used by the parser, and then uses semantic andpragmatic knowledge (of the limited domain) to correct heinterpretation.The  CorpusAs part of the TRAINS project (Allen and Schubert, 199 I),which is a long term research project o build a conversation-ally proficient planning assistant, we are collecting a corpusof problem solving dialogs.
The dialogs involve two partici-pants, one who is playing the role of a user and has a certaintask to accomplish, and another, who is playing the role of thesystem by acting as a planning assistant.
4 The entire corpusconsists of 112 dialogs totaling almost eight hours in lengthand containing about 62,000 words, 6300 speaker turns, and40 different speakers.
These dialogs have been segmentedinto utterance files (cf.
Heeman and Allen, 1994b); words3They referred to modification repairs as nontrivial repairs, andto abridged repairs as trivial repairs; however, these terms are mis-leading.
Consider the utterance "send it back to Elmira uh to makeOJ".
Determining that he corrected text should be "send it back toElmira to make OJ" rather than "send it back to make OJ" is nontrivial.4Gross, Allen and Traum (1992) discuss the manner in whichthe first set of dialogues were collected, and provide transcriptions.296have been transcribed and the speech repairs have been an-notated.
For a training set, we use 40 of the dialogs, consist-ing of 24,000 words, 725 modification and abridged repairs,and 13 speakers; and for testing, 7 of the dialogs, consistingof 5800 words, 142 modification and abridged repairs, andseven speakers, none of which were included in the trainingset.The speech repairs in the dialog corpus have been hand-annotated.
There is typically a correspondence b tweenthe removed text and the resumed text, and followingBear, Dowding and Shriberg (1992), we annotate this usingthe labels m for word matching and r for word replacements(words of the same syntactic ategory).
Each pair is givena unique index.
Other words in the removed text and re-sumed text are annotated with an x.
Also, editing terms(filled pauses and clue words) are labeled with et, and themoment of interruption with int, which will occur beforeany editing terms associated with the repair, and after thefragment, if present.
(Further details of this scheme can befound in (Heeman and Allen, 1994a).)
Below is a sampleannotation, with removed text "go to oran-", editing term"um", and resumed text "go to" (d93-14.2 utt60).gol tol oran-I uml gol tol Corningml I m2 I x I int \[  et I ml I m2 IA speech repair can also be characterized by its repair pat-tern, which is a string that consists of the repair labels (wordfragments are labeled as -, the interruption point by a period,and editing terms by e).
The repair pattern for the exampleis mm-.emm.Repair IndicatorsIn order to correct speech repairs, we first need todetect them.
If we were using prosodic informa-tion, we could focus on the actual interruption point(cf.
Nakatani and Hirschberg, 1993); however, we are re-stricting ourselves to lexical clues, and so need to be morelenient.Table 1 gives a breakdown of the modification speechrepairs and the abridged repairs, based on the hand-annotations} Modification repairs are broken down intofour groups, single word repetitions, multiple word repeti-tions, one word replacing another, and others.
Also, thepercentage of each type of repair that include fragments andediting terms is given.This table shows that strictly looking for the presence offragments and editing terms will miss at least 41% of speechrepairs.
So, we need to look at word correspondences in or-der to get better coverage of our repairs.
In order to keep thefalse positive rate down, we restrict ourselves to the follow-ing types of word correspondences: (1)word matching withat most three intervening words, denoted by m-m; (2) twoadjacent words matching two others with at most 6 wordsintervening, denoted by mm-mm;  and (3) adjacent replace-ment, denoted by rr.
Table 2 the number of repairs in the5Eight repairs were excluded from this analysis.
These repairscould not be automatically separated from other repairs that over-lapped with them.with with EditTotal Frag.
TermModification Repair 450 14.7% 19.3%Word Repetition 179 16.2% 16.2%Larger Repetition 58 17.2% 19.0%Word Replacement 72 4.2% 13.9%Other 141 17.0% 26.2%Abridged Repair 267 46.4% 54.3%Total 717 26.5% 32.4%Table 1: Occurrence of Types of Repairstraining corpus that can be deleted by each clue, based onthe hand-annotations.
For each clue, we give the number ofrepairs that it will detect in the first column.
In the next threecolumns, we give a breakdown of these numbers in terms ofhow many clues apply.
As the table shows, most repairs aresignal by only one of the 3 clues.Total I 1 clue I 2 clues I 3 clues IFragment 190Editing Termsm-mmm-mmITothersTotal23233194 412599717 I127 58 5164 63 5296 111 5n.a.
n.a.
n.a.587 I 116 I 5Table 2: Repair IndicatorsAlthough the m-m clue and mm-mm clue do not pre-cisely locate the interruption point, we can, by using simplelexical clues, detect 97.7% (708/725) of all the repairs.
But,we still will have a problem with false positives, and detect-ing the extent of the repair.Determining the CorrectionBased on the work done at SRI (Bear, Dowding and Shriberg,1992), we next looked at the speech repair patterns in ourannotated training corpus.
If we can automatically determinethe pattern, then the deletion of the removed text along withthe editing terms gives the correction.
Since the size of thepattern can be quite large, especially when editing termsand word fragments are added in, the number of possibletemplates becomes very large.
In our training corpus of450 modification repairs, we found 72 different patterns (notincluding variations due to editing terms and fragments).
Allpatterns with at least 2 occurrences are listed in table 3.Adding to the PatternRather than doing template matching, we build the repairpattern on the fly.
When a possible repair is detected, thedetection itself puts constraints on the repair pattern.
Forinstance, if we detect a word fragment, the location of thefragment limits the extent of the editing terms.
It also limits297m.m 79r.r t2mm.mm llmr.mr L7mx.m L5mmm.mmm L4rm.rm 12m.xm 6mmr.rnmr 5m.xxm 5x.xx 4x.
4mmx.mmmrm.mrmmmmr.mmmrmm.mxmr .x rmxxx .mmx,mxmmrm.mmrmmmmx.mmmmmmm.mmmmm.mx43332222222Table 3: Repair Patterns and Occurrencesthe extent of the resumed text and removed text, and so onrestricts word correspondences that can be part of the repair.In this section, we present he rules we use for buildingrepair patterns.
These rules not only limit the search space,but more importantly, are intended to keep the number offalse positives as low as possible, by capturing a notion of'well-formness' for speech repairs.The four rules listed below follow from the model of re-pairs that we presented in the introduction.
They capturehow a repair is made up of three intervals--the removedtext, which can end in a word fragment, possible editingterms, and the resumed text--and how the interruption pointis follows the removed text and precedes the editing terms.1.
Editing terms must be adjacent.2.
Editing terms must immediately follow the interrup-tion point.3.
A fragment, if present, must immediately precede theinterruption point.4.
Word correspondences must straddle the interruptionpoint and can not be marked on a word labeled as anediting term or fragment.The above rules alone do not restrict he possible wordcorrespondences enough.
Based on an analysis of the hand-coded repairs in the training corpus, we propose the followingadditional rules.Rule (5) captures the regularity that word correspondencesof a modification repair are rarely, if ever, embedded in eachother.
Consider the following exception.how would that - how long that would takeIn this example, the word correspondence involving "that"is embedded inside of the correspondence on "would".
Thespeaker actually made a uncorrected speech error (and so nota speech repair) in the resumed text, for he should have said"how long would that take."
Without his ungrammaticality,the two correspondences would not have been embedded,and so would not be in conflict with the following rule.5.
Word correspondences must be cross-serial; a wordcorrespondence annot be embedded inside of an-other correspondence.The next rule is used to limit the application of wordcorrespondences when no correspondences are yet in therepair pattern.
In this case, the repair would have beendetected by the presence of a fragment or editing terms.
Thisrule is intended to prevent spurious word correspondencesfrom being added to the repair.
For instance in the followingexample, the correspondence b tween the two instances of'T '  is spurious, since the second 'T '  in fact replaces "we".I think we need to uh I needSo, when no correspondences are yet included in the repair,the number of intervening words needs to be limited.
Fromour test corpus, we have found that 3 intervening words,excluding fragments and editing terms is sufficient.6.
I f  there are no other word correspondences, there canonly be 3 intervening words, excluding fragments andediting terms, between the first part and the secondpart of the correspondence.The next two rules restrict he distance between two wordcorrespondences.
Figure 1 shows the distance between twoword correspondences, indexed by i and j.
The intervalsx and y are sequences of the words that occur between themarked words in the removed text and in the resumed text,respectively.
The word correspondences of interest are thosethat are adjacent, in order words, the ones that have no labeledwords in the x and y intervals.mi ,2 ,~ mj .
-  .
in t .
.
.
m i ,?
,~ m jz yFigure 1: Distance between correspondencesFor two adjacent word correspondences, Rule (7) ensuresthat there is at most 4 intervening words in the removed text,and Rule (8) ensures that there are at most 4 interveningwords in the resumed text.7.
In the removed text, two adjacent matches can haveat most 4 intervening words (Izl < 4).8.
In the resumed text, two adjacent matches can haveat most 4 intervening words (lyl -< 4).The next rule, Rule (9), is used to capture the regularitythat words are rarely dropped from the removed text, insteadthey tend to be replaced.9.
For two adjacent matches, the number of interveningwords in the removed text can be at most one morethan the number of intervening words in the resumedtext (Izl ___ lyl + 1).The last rule, Rule (10), is used to restrict word replace-ments.
From an analysis of our corpus, we found that wordreplacement correspondences are rarely isolated from otherword correspondences.10.
A word replacement (except hose added by the de-tection clues) must either only have fragments andediting terms between the two words that it marks, orthere must be a word correspondence in which thereare no intervening words in either the removed textor the resumed text (x = y = 0).298An ExampleTo illustrate the above set of well-formedness constraints onrepair patterns, consider the example given above "I thinkwe need to - uh I need."
The detection clues will mark theword "uh" as being a possible diting term, giving the partialpattern given below.I th ink we need to uh\[ I needet INow let's consider the two instances of "I".
Adding thiscorrespondence to the repair pattern will violate Rule (6),since there are four intervening words, excluding the editingterms.
The correspondence b tween the two instances of'need' is acceptable though, since it straddles the editingterm, and there are only two intervening words between thecorresponding words, excluding editing terms.Even with the correspondence b tween the two instancesof 'need' ,  the matching between the ' I 's still cannot be added.There are 2 intervening words between 'T '  and "need" in theremoved text, but none in the resumed side, so this corre-spondence violates Rule (9).
The word replacement of "we"by the second instance of 'T' ,  does not violate any of  therules, including Rule (10), so it is added, resulting in thefollowing labeling.I think we I need l to uh I I I need lr I m I et I r\] m IAlgorithmOur algorithm for labeling potential repair patterns encodesthe assumption that speech repairs can be processed one at atime.
The algorithm runs in lockstep with a part-of-speechtagger (Church, 1988), which is used for deciding possibleword replacements.
Words are fed in one at a time.
Thedetection clues are checked first.
If one of them succeeds,and there is not a repair being processed, then a new repairpattern is started.
Otherwise, if the clue is consistent with thecurrent repair pattern, then the pattern is updated; otherwise,the current one is sent off to be judged, and a new repairpattern is started.When a new repair is started, asearch is made to see if anyof the text can contribute word correspondences to the repair.Likewise, if there is currently a repair being built, a searchis made to see if there is a suitable word correspondencefor the current word.
Anytime a correspondence is found,a search is made for any additional correspondences that itmight sanction.Since there might be a conflict between two possible cor-respondences that can be added to a labeling, the one thatinvolves the most recent pair of  words is preferred.
For in-stance, in the example above, the correspondence b tweenthe second instance of 'T '  and "we" is prefered over thecorrespondence b tween the second instance of 'T '  and thefirst.The last issue to account for is the judging of a potentialrepair.
If the labeling consists of just cue phrases, then itis judged as not being a repair.
6 Otherwise, if the point of6This prevents phrases uch as "I guess" from being marked asinterruption of the potential repair is uniquely determined,then it is taken as a repair.
This will be the case if there isat least one editing term, a word fragment, or there are nounaccounted for words between the last removed text part ofthe last correspondence and the resumed text part of the firstcorrespondence.Results of Pattern BuildingThe input to the algorithm is the word transcriptions, aug-mented with turn-taking markers.
Since we are not tryingto account for fresh starts, break points are put in to denotethe cancel, and its editing terms are deleted (this is done toprevent he algorithm from trying to annotate the fresh startas a repair).
The speech is not marked with any intonationalinformation, nor is any form of punctuation inserted.
Theresults are given in Table 4.TrainingSetDetection Recall 94.9%Detection Precision 55.8%Correction Recall 89.2%Correction Precision 52.4%TestSet91.5%45.3%85.9%42.5%Table 4: Results of Pattern MatchingThe pattern builder gives many false positives in detectingspeech repairs due to word correspondences in fluent speechbeing mis-interpreted is evidence of a modification repair.Also, in correcting the repairs, word correspondences acrossan abridged repair cause the abridged repair to be interpretedas a modification repair, thus lowering the correction re-call rate.
7 For example, the following abridged repair hastwo spurious word correspondences, between "need to" and"manage to".we need to -um manage to get the bananas to Dansville morequicklyThis spurious word correspondence will cause the patternbuilder to hypothesize that this is a modification repair, andso propose the wrong correction.Adding A Statistical FilterWe make use of a part-of-speech tagger to not only determinepart-of-speech ategories (used for deciding possible wordreplacements), but also to judge modification repairs that areproposed by the pattern builder.
For modification repairs,the category transition probabilities from the last word ofthe removed text to the first word of the resumed text havea different distribution than category transitions for fluentspeech.
So, by giving these distributions to the part-of-speech tagger (obtained from our test corpus), the tagger candecide if a transition signals a modification repair or not.editing terms when they have a sentential meanings, as in "I guesswe should load the oranges.
"7About half of the difference between the detection recall rateand the correction recall rate is due to abridged repairs being mis-classified as modification repairs.299Part-of-speech tagging is the process of assigning to aword the category that is most probable given the sententialcontext (Church, 1988).
The sentential context is typicallyapproximated by only a set number of previous categories,usually one or two.
Good part-of-speech results can be ob-tained using only the preceding category (Weischedel t al.,1993), which is what we will be using.
In this case, thenumber of states of the Markov model will be N, whereN is the number of tags.
By using the Viterbi algorithm,the part-of-speech tags that lead to the maximum probabilitypath can be found in linear time.Figure 2 gives a simplified view of a Markov model forpart-of-speech tagging, where Ci is a possible category forthe ith word, wi, and Gi+l is a possible category for wordwi+l.
The category transition probability is simply the prob-ability of category Ci+l following category Gi, which iswritten as P(Ci+l \]Ci).
The probability of word wi+l givencategory Ci+l is P(wi+l ICi+l).
The category assignmentthat maximizes the product of these probabilities i taken tobe the best category assignment.p(w~lCd p(w~+\]lC~+~)Figure 2: Markov Model of Part-of-Speech TaggingTo incorporate knowledge about modification repairs, welet Ri be a variable that indicates whether the transitionfrom word w~ to wi+1 contains the interruption point of amodification repair.
Rather than tag each word, wi, withjust a category, C~, we will tag it with Ri_lCi, the cat-egory and the presence of a modification repair.
So, wewill need the following probabilities, P(RiCi+1\[Ri_IC 0and P(wiIRi_lCi).
To keep the model simple, and easeproblems with sparse data, we make several independenceassumptions.
By assuming that Ri-1 and RiCi+l are inde-pendent, given Ci, we can simplify the first probability toP(RiICi) * P(C~+I C~Rd; and by assuming that R~_\] andwi are independent, given Ci, we can simplify the secondone to P(wilCO.
The model that results from this is givenin Figure 3.
As can be seen, these manipulations allow us toview the problem as tagging null tokens between words as ei-ther the interruption point of a modification repair, R~ = T~,or as fluent speech, R~ = ?i.Modification repairs can be signaled by other indicatorsthan just syntactic anomalies.
For instance, word matches,editing terms, and word fragments also indicate their pres-ence.
This information can be added in by viewing thepresence of such clues as the 'word' that is tagged by therepair indicator Ri.
By assuming that these clues are in-dependent, given the presence of a modification repair, wecan simply use the product of the individual probabilities.So, the repair state would have an output probability ofP(FiIR~) * P(EiIRi) * P(MiIR~), where Fi, Ei, and Miare random variables ranging over fragments, editing terms,types of word matches, respectively.
So for instance, the?Figure 3: Statistical Model of Speech Repairsmodel can account for how "uh" is more likely to signal amodification repair than "um".
Further details are given inHeeman and Allen (1994c).Overall ResultsThe pattern builder on its own gives many false positivesdue to word correspondences in fluent speech being mis-interpreted evidence of a modification repair, and due toword correspondences across an abridged repair causing theabridged repair to be interpreted as a modification repair.This results in an overall correction recall rate of 86% and aprecision rate of 43%.
However, the real result comes fromcoupling the pattern builder with the decision routine, whichwill eliminate most of the false positives.Potential repairs are divided into two groups.
The firstincludes abridged repairs and modification repairs involvingonly word repetitions.
These are classified as repairs out-fight.
The Rest of the modification repairs are judged bythe statistical model.
Any potential repair that it rejects, butwhich contains aword fragment or filled pause is accepted asan abridged repair.
Table 5 gives the results of the combinedapproach on the training and test sets.TrainingCorpusDetectionRecall 91%Precision 96%CorrectionRecall 88%Precision 93%TestCorpus83%89%80%86%Table 5: Overall ResultsComparing our results to others that have been reported inthe literature must be done with caution.
Such a comparisonis limited due to differences in both the type of repairs thatare being studied and in the datasets used for drawing results.Bear, Dowding, and Shriberg (1992) use the ATIS corpus,which is a collection of queries made to an automated airlinereservation system.
As stated earlier, they removed all ut-terances that contained abridged repairs.
For detection theyobtained a recall rate of 76% and a precision of 62%, and forcorrection, a recall rate of 43% and a precision of 50%.
Itis not clear whether their results would be better or worse if300abridged repairs were included.
Dowding et al (1993) useda similar setup for their data.
As part of a complete system,they obtained a detection recall rate of 42% and a precision of85%; and for correction, a recall rate of 30% and a precisionof 62%.
Lastly, Nakatani and Hirschberg (1993) also usedthe ATIS corpus, but in this case, focused only on detection,but detection of all three types of repairs.
However, theirtest corpus consisted entirely of utterances that contained atleast one repair.
This makes it hard to evaluate their re-sults, reporting a detection recall rate of 83% and precisionof 94%.
Testing on an entire corpus would clearly decreasetheir precision.
As for our own data, we used a corpus ofnatural dialogues that were segmented only by speaker turns,not by individual utterances, and we focused on modificationrepairs and abridged repairs, with fresh starts being markedin the input so as not to cause interference in detecting theother two types.The performance of our algorithm for correction is sig-nificantly better than other previously reported work, witha recall rate of 80.2% and a precision rate of 86.4% on afair test.
While Nakatani and Hirschberg report comparabledetection rates, and Hindle reports better correction rates,neither of these researchers attack the complete problem ofboth detection and correction.
Both of them also dependon externally supplied annotations not automatically derivedfrom the input.
As for the SRI work, their parser-first rategyand simple repair patterns cause their rates to be much lowerthan ours.
A lot of speech repairs do not look ill-formed,such as "and a boxcar o f -  and a tanker of OJ", and "andbring - and then bring that orange juice," and are mainlysignaled by either lexical or acoustic lues.Over lapp ing  Repa i rsOur algorithm is also novel in that it handles overlappingrepairs.
Two repairs overlap if part of the text is used in bothrepairs.
Such repairs occur fairly frequently in our corpus,and for the most part, our method of processing repairs, evenoverlapping ones, in a sequential fashion appears uccess-ful.
Out of the 725 modification and abridged repairs in thetraining corpus, 164 of them are overlapping repairs, andour algorithm is able to detect and correct 86.6% of them,which is just slightly less than the correction recall rate forall modification and abridged repairs in the entire trainingcorpus.Consider the following example (d93-14.2 utt26), whichcontains four speech repairs, with the last one overlappingthe first three.and pick up um the en- I guess the entire um p- pick up theload of oranges at ComingThe algorithm is fed one word at a time.
When it encoun-ters the first "um", the detection rule for editing terms getsactivated, and so a repair pattern is started, with "um" beinglabeled as an editing term.
The algorithm then processesthe word "the", for which it can find no suitable correspon-dences.
Next is the fragment"en-".
This causes the detectionrule for fragments to fire.
Since this fragment comes afterthe editing term in the repair being built, adding it to therepair would violate Rule (2) and Rule (3).
So, the algorithmmust finish with the current repair, the one involving "um".Since this consists of just a filled pause, it is judged as beingan actual repair.Now that he alogrithm is finished with the repair involving"um", it can move on to the next one, the one signaled bythe fragment "en-".
The next words that are encountered are"I guess", which get labeled as an editing phrase.
The nexttoken is the word "the", for which the algorithm finds a wordcorrespondence with the previous instance of "the".
At thispoint, it realizes that the repair is complete (since there is aword correspondence and all words between the first markedword and the last are accounted for) and so sends it off to bejudged by the statistical model.
The model tags it as a repair.Deleting the removed text and the editing terms indicatedby the labeling results in the following, with the algorithmcurrently processing "the".and pick up the entire um p- pick up the load of oranges atComingContinuing on, the next potential repair is triggered by thepresence of "um", which is labeled as an editing term.
Thenext token encountered, a fragment, also indicates apotentialrepair, but adding it to the labeling will violate Rule (2) andRule (3).
So, the pattern builder is forced to finish up withthe potential repair involving "um".
Since this consists ofjust a filled pause, it is accepted.
This leaves us with thefollowing text, with the algorithm currently processing "p-",which it has marked as a fragment.and pick up the entire p- pick up the load of oranges at ComingThe next word it encounters i "pick".
This word is toofar from the preceding "pick" to allow this correspondenceto be added.
However, the detection clue ram-ram doesfire, due to the matching of the pair of adjacent words "pickup".
This clue is consistent with "p-" being marked as theword fragment of the repair, and so these correspondencesare added.
The next token encountered is "the", and thecorrespondence for it is found.
Then "load" is processed,but no correspondence is found for it, nor for the remainingwords.
So, the repair pattern that is built contains an un-labeled token, namely "entire".
But due to the presence ofthe word fragment, he interruption point can be determined.The repair pattern is set off to be judged, which tags it asa repair.
This leaves the following text not labeled as theremoved text nor as the editing terms of a repair.and pick up the load of oranges at CorningDue to the sequential processing of the algorithm and its abil-ity to commit o a repair without seeing the entire utterance,overlapping repairs do not pose a major problem.Some overlapping repairs can cause problems however.Problems can occur when word correspondences are at-tributed to the wrong repair.
Consider the following example(d93-15.2 utt46).you have w- one you have two boxcarThis utterance contains two speech repairs, the first is the re-placement of"w-" by "one", and the second the replacementof "you have one" by "you have two".
Since no analysisof fragments i done, the correspondence b tween "w-" and301"one" is not detected.
So, our greedy algorithm decidesthat the repair after "w-" also contains the word matches for"you" and "have", and that the occurrence of "one" after the"w-" is an inserted word.
Due to the presence of the partialand the word matching, the statistical model accepts this pro-posal, which leads to the erroneous correction of "one youhave two boxcars," which blocks the subsequent repair frombeing found.ConclusionThis paper described a method of locally detecting and cor-rection modification and abridged speech repairs.
Our workshows that a large percentage of speech repairs can be re-solved prior to parsing.
Our algorithm assumes that thespeech recognizer produces a sequence of words and identi-fies the presence of word fragments.
With the exception ofidentifying fresh starts, all other processing is automatic anddoes not require additional hand-tailored transcription.
Wewill be incorporating this method of detecting and correctingspeech repairs into the next version of the TRAINS system,which will use spoken input.There is an interesting question as to how good the per-formance can get before a parser is required in the process.Clearly, some examples require a parser.
For instance, wecan not account for the replacement of a noun phrase witha pronoun, as in "the engine can take as many um-  it cantake up to three loaded boxcars" without using syntacticknowledge.
On the other hand, we can expect to improve onour performance significantly before requiring aparser.
Thescores on the training set, as indicated in table 5, suggest thatwe do not have enough training data yet.
In addition, wedo not yet use any prosodic ues.
We are currently investi-gating methods of automatically extracting simple prosodicmeasures that can be incorporated into the algorithm.
GivenNakatani and Hirschberg's results, there is reason to believethat this would significantly improve our performance.Although we did not address fresh starts, we feel that ourapproach of combining local information from editing terms,word fragments, and syntactic anomalies will be successfulin detecting them.
However, the problem lies in determin-ing the extent of the removed text.
In our corpus of spokendialogues, the speaker might make several contributions ina turn, and without incorporating other knowledge, it is dif-ficult to determine the extent of the text that needs to beremoved.
We are currently investigating approaches to au-tomatically segment a turn into separate utterance units byusing prosodic information.AcknowledgmentsWe wish to thank Bin Li, Greg Mitchell, and Mia Stern fortheir help in both transcribing and giving us useful commentson the annotation scheme.
We also wish to thank HannahBlau, John Dowding, Elizabeth Shriberg, and David Traumfor helpful comments.
Funding gratefully received fromthe Natural Sciences and Engineering Research Council ofCanada, from NSF under Grant IRI-90-13160, and fromONR/DARPA under Grant N00014-92-J- 1512.ReferencesAllen, J. F. and Schubert, L. K. (1991).
The TRAINS project.Technical Report 382, Department of Computer Sci-ence, University of Rochester.Bear, J., Dowding, J., and Shriberg, E. (1992).
Integratingmultiple knowledge sources for detection and correc-tion of repairs in human-computer dialog.
In Proceed-ings of the 30 th Annual Meeting of the Association forComputational Linguistics, pages 56--63.Church, K. (1988).
A stochastic parts program and nounphrase parser for unrestricted text.
In Preceedings ofthe 2nd Conference on Applied Natural Language Pro-cessing, pages 136-143.Dowding, J., Gawron, J. M., Appelt, D., Bear, J., Cherny, L.,Moore, R., and Moran, D. (1993).
Gemini: A naturallanguage system for spoken-language understanding.In Proceedings ofthe 31 th Annual Meeting of the As-sociation for Computational Linguistics, pages 54-61.Gross, D., Allen, J., and Traum, D. (1992).
The TRAINS 91dialogues.
Trains Technical Note 92-1, Department ofComputer Science, University of Rochester.Heeman, P. A. and Allen, J.
(1994a).
Annotating speechrepairs, unpublished manuscript.Heeman, P. A. and Allen, J.
(1994b).
Dialogue transcriptiontools, unpublished manuscript.Heeman, P. A. and Allen, J.
(1994c).
Tagging speech repairs.In ARPA Workshop on Human Language Technology,Princeton.Hindle, D. (1983).
Deterministic parsing of syntactic non-fluencies.
In Proceedings ofthe 21 st Annual Meetingof the Association for Computational Linguistics, pages123-128.Labov, W. (1966).
On the grammaticality of everydayspeech.
Paper presented at the Linguistic Society ofAmerica Annual Meeting.Levelt, W. J. M. (1983).
Monitoring and self-repair inspeech.
Cognition, 14:41-104.Nakatani, C. and Hirschberg, J.
(1993).
A speech-first modelfor repair detection and correction.
In Proceedings ofth the 31 Annual Meeting of the Association for Compu-tational Linguistics, pages 46--53.Wang, M. Q. and Hirschberg, J.
(1992).
Automatic lassi-fication of intonational phrase boundaries.
ComputerSpeech and Language, 6:175-196.Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L.,and Palmucci, J.
(1993).
Coping with ambiguity andunknown words through probabilistic models.
Compu-tational Linguistics, 19(2):359-382.Young, S. R. and Matessa, M. (1991).
Using pragmatic andsemantic knowledge to correct parsing of spoken lan-guage utterances.
In Proceedings ofthe 2nd EuropeanConference on Speech Communication a d Technology(Eurospeech 91), Genova, Italy.302
