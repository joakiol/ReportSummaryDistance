IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIilIIAn Attempt to Use Weighted Cusums to Identify SublanguagesHarold SomersCentre for Computational Linguistics, UMISTPO Box 88, Manchester M60 1QDEnglandharol d@ccl, umist, ac.
ukAbst rac tThis paper explores the use of weighted cusums, atechnique found in authorship attribution studies, forthe purpose of identifying sublanguages.
The tech-nique, and its relation to standard cusums (cumulativesum charts) is first described, and the formulae forcalculations given in detail.
The technique comparestexts by testing for the incidence of linguistic 'features'of a superficial nature, e.g.
proportion of 2- and3---letter words, words beginning with a vowel, and.
soon, and measures whether two texts differ significantlyin respect of these features.
The paper describes anexperiment inwhich 14 groups of three texts each repre-senting different sublanguages are compared with eachother using the technique.
The texts are first comparedwithin each group to establish that the technique canidentify the groups as being homogeneous.
The textsare then compared with each other, and the resultsanalysed.
Taking the average of seven different ests,the technique is able to distinguish the sublanguages inonly 43% of the case.
But if the best score is taken,79% of pairings can be distinguished.
This is a betterresult, and the test seems able to quantify the differencebetween sublanguages.Keywords: sublanguage, genre, register, weightedcusum.1 IntroductionThis paper concerns a technique which we use tomeasure whether two texts are representative of thesame text genre or sublanguage.
It is very much in thespirit of the well-known work in this field by DouglasBiber (1988, 1990, 1995), but differs crucially in thatwe avoid the explicit selection of linguistic featuresthought a priori likely to be important in distinguishingsublangnages, and instead use a set of low-level featuresbased on trivial aspects of the words such as lengthand initial letter.
Our technique is borrowed fromthe neighbouring field of authorship attribution (foran overview of this field see Ule 1982; Smith 1982;Potter 1991; Burrows 1992; Holmes 1994).
It is astraightforward calculation, simple to implement, andvery general in application.
It can be used withfairly small texts.
This paper describes an experimentto see whether the technique can be used for thesublanguage identification task, even though it wasoriginally designed for a somewhat different problem.In Somers (forthcoming), we used a techniquecalled 'weighted cusums' to investigate how well aparody of Lewis Carroll had imitated his style.
Look-hag also at other writings by Carroll, including his'serious' mathematical works (under his real name,Charles Dodgson), letters to adults and children, hisdiaries, formal and whimsical articles in newspapers,we found that the technique, although unable to identifyCarroll/Dodgson as the unique author of all the texts, asthe authorship attribution literature would demand andexpect, seemed to be able to group together his writingsaccording to genre and/or topic.
This was an interestingfinding, because the technique, as has already beenhinted, measures the most banal of linguistic features.This finding suggested to us the idea of the experimentreported ha this paper: could the technique be used toidentify sublanguages?2 Background2.1 SublanguageWe will assume that readers of this paper are fairly fa-miliar with the literature on sublanguage ( .g.
Kittredge& Lehrberger 1982; Grishman & Kittredge 1986),including definitions of the notion, history of the basicidea, and, above all, why it is a useful concept.
Somereaders will prefer terms like 'register' (which Biberuses); an affinity with work on genre detection will alsobe apparent.
Because there is sometimes some disputeabout the use of the term 'sublanguage', let us clarifyfrom the start that for our purposes a sublanguage isan identifiable genre or text-type in a given subjectfield, with a relatively or even absolutely closed set ofsyntactic structures and vocabulary.
In recent years,the availability of large corpora and 'new' methodsto process them have led to renewed interest in thequestion of sublanguage identification (e.g.
Sekine1997), while Karlgren & Cutting (1994) and Kessleret al (1997) have focussed on the narrower but clearlyrelated questio.n of genre.Our purpose in this paper is to explore a techniquefor identifying whether a set of texts 'belong to' theSomers 131 Use Weighted Cusums to Identify SublanguagesHarold Somers (1998) An Attempt to Use Weighted Cusums to Identify Sublanguages.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL 98 : New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 131-139.same sublanguage, and of quantifying the differencebetween texts: our technique compares texts palrwiseand delivers a 'score' which can be used to group textsjudged similar by the technique.
As we shall see later,what is of interest here is that the score is derivedfrom a simple count of linguistic features uch as wordlength and whether words begin with a vowel; yetthis apparently unpromising approach seems to deliverusable results.In his well-known study, Biber (1988) took anumber of potentially distinct ext genres and measuredthe incidence of 67 different linguistic features inthe texts to see what correlation there was betweengenre and linguistic feature.
He also performed factoranalysis on the features to see how they could begrouped, and thereby see if sublanguages could bedefined in terms of these factors.The linguistic features that Biber used I are amixture of lexical and syntactic ones, and almost allrequire a quite sophisticated level of analysis of thetext data - dictionary look-up, tagging, a parser.
Theyare presumably also, it should be said, hand-picked asfeatures whose use might differ significantly from onegenre to another.
Although Biber gives details of thealgorithms used to extract he features, it is not a trivialmatter to replicate his experiments.Kessler et al (1997) make the same criticism ofBiber and of Karlgren & Cutting (1994), and restricttheir experimentation genre recognition to "surfacecues".
In their paper they do not give any detailabout the cues they use, except to say that they are"mainly punctuation cues and other separators anddelimiters used to mark text categories like phrases,clauses, and sentences" (p. 34); however, HinrichSchfitze (personal eornmunication) has elaborated that"The cues are punctuation, non-content words (pro-nouns, prepositions, auxiliaries), counts of words, \[of\]unique words, \[of\] sentences, and \[of\] characters; anddeviation features (standard eviation of word lengthand sentence length)".
As we shall see below, the useof superficial linguistic aspects of the text is a featureof the approach described here.2.2 Authorship attribution and weighted eusumsAuthorship attribution has for a long time been asignificant part of literary stylistics, familiar even tolay people in questions uch as "Did Shakespeare allywrite all of his plays?
", "Who wrote the Bible?
", andI The features can be grouped into "sixteen major categories: (A)tense and aspect markers, (B) place and time adverbials, (C) pronounsand pro-verbs, (D) questions, (E) nominal forms, (F) passives, ((3)stative forms, (H) subordination features, (I) adjectives and adverbs,(J) lexical specificity, (K) specialized lexical classes, (L) medals,(M) specialized verb classes, (hi) reduced or dispreferred forms, (O)coordination, and (P) negation."
(Biber 1988:223)so on.
With the advent of computers, this once rathersubjective field of study has become more rigorous,attracting also the attention of statisticians, o that nowthe field of 'stylometrics' - the objective measurementof (aspects) of literary style - has become a preciseand technical science.One technique that has been used in authorshipattribution studies, though not without controversy, isthe cumulative sum chart ('cusum') technique, a variantof which: we shall be using for our own investigation.Since we" are not actually using standard cusums here,our explanation can be relatively brief.
Cusums area fairly well-known statistical device used in processcontrol.
The technique was adapted for author identifi-cation by Morton (1978) - see also Farringdon (1996)- and achieved some notoriety for its use in court cases(e.g.
to identify faked or coerced confessions) as well asin literary studies.
The technique is easy to implement,and requires only small mounts of text.A cusum is a graphic plot based on a sequenceof measures.
For example, suppose we have a setof measures (11,7,4, 10,2 .
.
.
.  )
with a mean value of6.
The corresponding divergences from the meanare (5 ,1 , -2 ,4 , -4 , .
.
. )
.
The cusum chart plotsnot these divergences, but their aggregate sum, i.e.
(5, 6, 4, 8, 4, .
.
. )
,  the sequence inevitably ending in 0.The plot reflects the variability of the measure: thestraighter the line, the more stable the measure.
Inauthorship attribution studies, the eusum chart is usedto plot the homogeneity of a text with respect to alinguistic 'feature' such as use of two- and three-letterwords on a sentence-by-sentence basis.
Two graphsare plotted, one for the sentence lengths, the other forthe incidence of the feature, and superimposed afterscaling so that they cover roughly the same range.
Theauthorship identification technique involves taking thetexts in question, concatenating them, and then plottingthe eusum chart.
I f  the authors differ in their use ofthe linguistic feature chosen, this will manifest itselfas a marked divergence in the two plots at or near thepoint(s) where the texts have been joined.There are a number of drawbacks with thismethod, the main one being the manner in whichthe result of the test is arrived at, namely the need toscrutinize the plot and use one's skill and experience(i.e.
subjective judgment) to determine whether thereis a "significant discrepancy" at or near the join pointin the plot.A solution to this and several other problems withthe standard cusum technique is offered by Hilton &Holmes (1993) and Bissell (1995a,b) in the form ofweighted cusums (henceforth WQsums).
Since this isthe technique we shall use for our experiments, we needto describe it in full detail.Somers 132 Use Weighted Cusums to Identify SublanguagesIIIIIIIIIIIIIIIIIIIIIIIIIIII?IIII//IlI!/l//3 Weighted cusums3.1 The calculationsAs in the standard cusum, the WQsum is a measureof the variation and homogeneity of use of a linguisticfeature on a sentence-by-sentence basis throughout atext.
It captures not only the relative amount of use ofthe feature, but also whether its use is spread evenlythroughout the texts in question.In a WQsum, instead of summing the divergencefrom the mean wi - ~ for the sentence lengths w andsimilarly xi-?"
for the linguistic feature x, we sum x i -~'wi, where /r, the 'weight', is the overall proportionof feature words in the whole text, as given by (I).As Hilton & Holmes (1993) explain, this weightingmeans that we are calculating "the cumulative sum ofthe difference between the observed number of featureoccurrences and the 'expected' number of occurrences"(p. 75).~_  E~i  (1)As we shall see shortly, the variation in a WQsumcan be measured systematically, and its statistical sig-nificance quantified with something like a t-test.
Thismeans that visual inspection of the WQsum plot is notnecessary.
There is no need, either, to concatenate orsandwich the texts to be compared.
For the t-test, thetwo texts, .4 and B, are treated as separate samples.The formula for t is (2).t = I~rA -- ~'S\[ (2)The t-value is, in the words of Hilton & Holmes, "a ?measure of the evidence against the null hypothesisthat the frequency of usage of the habit \[i.e.
linguisticfeature\] under consideration is the same in Text Aand Text B.
The higher the t-value, the more evidenceagainst he hypothesis" (.p.
76).
The formula chosenfor the calculation of variance e in (2) is given in (3),where n is the number of sentences in the text.The resulting value is looked up in a standard t-table, which will tell us how confidently we can assertthat the difference is significant.
For this we need toknow the degrees of freedom v, which depends on thenumber of sentences in the respective t xts, and is givenby (4).
Tradition suggests that p < .05 is the minimumacceptable confidence level, i.e.
the probability is lessthan 5% that the differences between the texts are dueto chance.v = nA + nB - 2 (4)3.2 The linguistic featuresA point of interest for us is that both the cusums andWQsums have been used in the stylometrics field tomeasure the incidence of linguistically banal features,easily measured and counted.
The linguistic featuresproposed by Farringdon (1996:25), and used in thisexperiment, involve the number of words of a givenlength, and/or beginning with a vowel, as listed in Table1.Table 1 Linguistic featuresidentified by Farringdon (1996:25).Habit AbbreviationTwo- and three-letter words Iw23Two-, three- and four-letter words 1w234Three- and four-letter words lw34Initial-vowel words vowelTwo- and three-letter words or lw23vinitial-vowel wordsTwo-, three- and four-letter words or lw234vinitial-vowel wordsThree- and four-letter words or lw34vinitial-vowel wordsOther experimemers have suggested counting thenumber of nouns and other parts of speech, but it isnot clear if there are any limitations on the linguisticfeatures that could be used for this test, except the obvi-ous one that the feature should in principle be roughlycorrelated with sentence length.
In any case, part ofthe attraction for our experiment is that the features areso fundamentally different from the linguistic featuresused by Biber in his experiments, and so will offer apoint of comparison.
Furthermore, they are easy tocompute and involve no overheads (lexicons, parsersetc.)
whatsoever.It is also interesting to note that the WQsum is ameasure of  variation, a type of metric which, accordingto Kessler et al (1997) has not previously used in thistype of study.In authorship identification, it is necessary firstto determine which of these features is "distinctive"for a given author, and then to test the documents inquestion for that feature.
This is not appropriate for oursublanguage experiment, so for each text comparisonwe run all seven tests.
Each test gives us a t-score fromwhich a confidence l vel can be determined.
Obviously,the result over the seven tests may vary somewhat.For our experiment we simply take the average of theseven t-scores as the result of text comparison.
ItSomers 133 Use Weighted Cusums to Idenn'fy Sublanguagesis not obvious that it makes sense any more to treatthis as a t-score, and in the experiments describedbelow we tend to treat it as a raw score, a lowerscore indicating cohesion, a higher score suggestingdifference.
Nevertheless it is useful to bear in mindthat, given the degrees of freedom involved in all eases(the texts are all roughly the same length), the thresholdfor siguifieance is around 1.65.4 The methodOur experiment is to use the WQsum test on a corpus ofsmall texts which we believe can be grouped accordingto genre or sublanguage.
We gathered 15 sets ofdifferent ext-types: each set of three texts is assumedto represent a different sublanguage, and each text waswritten, as far as we know, by a different author.
The15 groups of texts were as follows:blurbs publishers' announcements of scientific text-booksBMJ abstracts of articles appearing in the BritishMedical Journalchildrcns extracts from children's torieschurch articles from local Catholic church newsletterseconomy economic reports from a Swiss banke-mails discussing arrangement of a meetingfootie reports of soccer matches from the same news-paper, same datelawreps extracts from The Weekly Law Reportsobits obituaries of Jacques Cousteau, from differentnewspapersrecipes recipes from the Interact Chef web siteTVscripts Autocue scripts from Central TV Newsprogrammestourism extracts from the "Shopping" seetiorL ofBerlitz guidesunivs descriptions of  Computer Science coursesweather state-wide general weather forecasts from USNational Weather Servicexwords sets of  clues to cryptic crosswordsOur first task is to see that the WQsum test canconfirm the homogeneity of the text triplets.
For eachgroup of three texts, we ran our test and averaged thet-scores for each group.
Table 2 shows an example ofthis for the "church' group of texts.
Table 3 lists the 14groups together with some information about the texts,including their 'homogeneity score', an indication oftheir length (average number of sentences, and averagewords per sentence), and their source.The first thing to note is that all the groups oftexts are well within the 1.65 threshold of significantdifference.
In other words, the pairwise WQsum testfor each group firmly indicates homogeneity within thegroups.Table 2 WQsum test results for 'church' text set.
Scoresmarked '*' suggest a difference significant at p < .05.A-B A-C B--C overallIw23 0.576 0.388 0.0551w234 0.131 0.781 0.8341w34 0.906 0.102 0.843vowel 1.860" 1.729" 0.489lw23v 0.256 0.402 0.502lw234v 0.569 1.211 0.963lw34v 0.301 0.845 0.683av'ge 0.657 0.780 0.624 0.687Table 3 The 15 gems, in order of 'homogeneity'.The texts marked WWW were taken from the web, BNCthe British National Corpus, and ECI the ACL/ECICD-rom.
Other texts are from my personal rchive.Group Source Score Av'ge lengthsent wordsobits WWW 0.440 25.67 19.10lawreps BNC 0.543 17.00 22.53emails 0.633 11.00 16.15univs WWW 0.659 21.33 24.87church BNC 0.687 18.00 19.39xwords 0.696 29.67 7.11TVscripts BNC 0.755 18.00 14.88BMJ BNC 0.802 19.00 17.19economy ECI 0.889 19.33 20.50weather WWW 0.890 24.33 9.69recipes WWW 0.976 26.00 7.68tourism 0.987 27.33 18.22blurbs WWW 1.083 11.67 23.00ehildrens BNC 1.174 26.00 11.99footie WWW 1.175 19.00 35.59We now proceed to compare all the texts witheach other, pairwise.
It is fortunate that the WQsumprocedure is so simple, since this pairwise comparisoninvolves a huge number of iterations: each text com-parison involves even applications of the WQsum test,each group comparison i volves nine text comparisons,and there are 105 pairwise group comparisons, makinga total of 6615 tests.
In the following section we willattempt to summarize the findings to be had from thislarge body of data.5 ResultsThe full results of the comparison are given in Table 4.This table shows the pairwise average t-scores, repli-cated for ease of consultation.
The groups are orderedas in Table 3, so that results in the top left-hand comerSomers 134 Use Weighted Cusums to Identify Sublanguages11111IIII11111111IIIIIIIIIIIIIIIIIIIIof the table are between the most homogeneous groups,results in the bottom right the least homogeneous.
Thescores given on the diagonal are repeated from Table 3and show the average score for the internal comparisonof the texts in that group.This time we are looking for high scores tosupport he hypothesis that the WQsum test can identifythe texts as belonging to different sublanguages.
At firstglance the results look disappointing.
If we again takea score of  1.65 as the notional cut-off point, then only43% (45 out of 105) of the results qualify.
On theother hand, if we compare the scores with those for thegroup-internal comparisons (Table 3), we may view theresults more positively.
The average internal score was0.885 (s.d.
= 0.232), the worst score 1.175; 67% of ourscores are better than that.One problem stems from averaging the scoresfor all the tests.
When the WQsum test is used inauthorship attribution, it is necessary first to determinewhich linguistic feature is significant for the authorunder investigation.
Looking at the raw scores for ourexperiment, we see that very often consistently highscores with one test are undermined by low scores onothers.
Table 5 shows an example of this, where anaverage score of 2.197 on the '1w34' test is mitigated byinsignificant scores on the other test, giving an overallaverage of 1.074.Table 5 Raw scores for 'childrens'-'emails' comparison.lw23 1w234 lw34 vowel lw23v lw234viw34vA-X 2.322 2.596 3.397 2.660 0.174 0.737 0.299A-Y 0.732 1.896 3.725 0.796 0.261 1.460 1.205A-Z 0.932 1.253 1.684 1.359 0.237 0.383 0.190B-X 0.085 1.062 2.822 2.271 0.900 0.129 0.041B-Y 1.732 0.714 3.090 0.941 1.215 0.553 0.622B-Z 0.765 0.328 1.633 1.349 0.927 0.070 0.138C-X 0.648 0.604 1.460 3.553 0.342 0.133 0.522C-Y 1.420 0.169 1.795 1.449 0.692 0.419 0.455C-Z 0.314 0.310 0.171 2.058 0.343 0.381 0.714av'ge 0.994 0.992 2.197 1.826 0.565 0.473 0.465So an alternative that suggests itself is to takein each case the highest of the average scores for eachlinguistic feature, on a pairwise basis.
These alternativeresults are presented in Table 6, which also shows ineach ease which linguistic feature gave the best result.Since we are now taking the highest rather than theaverage score for the pairwise comparisons, we shouldalso take the highest score for within-group comparison,which is again shown on the diagonal.
As in Table 4,the groups are ordered from 'best' to 'worst' within-group score.The 'improvement' in the results is considerable:this time 82 of the 105 results (78%) are above the1.650 threshold.
However, taking the highest ratherthan the average score for the within-groups comparisonleaves four of the groups -  'TVseripts', 'recipes','tourism' and 'childrens' m with scores above the 1.65threshold, and a fifth group, 'weather', has a score veryclose to this.
The scores for these groups are otten highfor comparisons with other texts, but they are also highfor the within-group comparison: this suggests that thetexts in these groups are not homogeneous, so we haveto take this into account when we consider the resultsin the discussion that follows.6 Discussion6.1 Does the WQsum test identifydifferent sublanguages?Let us consider first the results as shown in Table 6.Our main concern of course is to see whether theWQsurn test can identify and distinguish the groups.Taking 1.65 as our threshold, we can rank the groupsaccording to the number of other groups with whicheach gets a pairwise score average above this threshold.In addition, since the 'ideal' situation as far as ourhypothesis goes would be for a low within-group aver-age score suggesting homogeneity, and a high averagescore for comparisons with other groups, suggestingdistinctiveness, as a further, informal measure of theextent to which the groups meet this condition, wecan divide the average comparison score by the within-group score.
Table 7 shows a ranking of the groupsalong these lines.The groups eem to divide into roughly four types.The first type, groups which support our hypothesisthe best, have a low within-group average, a highpairwise average, and can easily be distinguished frommost of  the other groups.
In this group are 'xwords','univs', 'blurbs', 'BMJ', and 'economy'.
At the otherend of the scale, at the bottom of Table 7, are thosegroups which can have a low 'Sig.'
score.
This groupis not necessarily marked by a low pairwise averageor a high within-group score: the 'obits' group forexample has the second lowest within-group average,and scores quite highly on our informal ratio score.
Yetthe WQsum test cannot distinguish it from six of theother groups.A third type is where the 'Sig.'
score is highdespite a high within-group average which would sug-gest lack of homogeneity.
The 'recipes' group, forexample, stands out as a distinct sublanguage, withhighly significant scores compared to all other groups.Despite the fact that the within-group score is above theSomers 135 Use Weighted Cusums to Idenn'fy Sublanguages~Eoou.~ ?.~_ ~0?J000 ?.
)t~t~ >\[-0a<,.I=?-?=0~ ~'~ ~'~ ~D r~ O~ ~"  0 r ~ O~ ~-  "4~- -  o - -  ?4 o - -  - -  ~ - -  ~ ~ - -  t4  - -1?q??
)0tt%p~r~r~r~00 ~4Dre)t~?
?~ ?.4o 0 ?
'~0 ?
'4 "~" r,~0"~" 0 t'~ o~ t ~ ~ oo?
?~ ?~ t "~ oo ??)
?~",D r ~ ~ ~D ?
?~oo  t ~ oo "~~'~ ~ ~ O~ t "~?,4 ,'~~.
~ ~ ,?..
~ .
,,.. ~ ~ ~.
r,, r,- q o .
~ .Somers  136 Use Weighted  Cusums to  Ident i fy  Sub languagesIIIll|IIlIlIl|II///l///////II/= > e',i ~ e',i e4 "n: ~ ,ff eq ,,6 ~ ?
'-i ~ e,': e',i e,i_ ~ = ~  ~ ~ e , -~ -~ '~_- " ~ "~ - -  ~ .~, ~ ~ " ,,o?
_ - .
~  ~ N  = = =.o_ ~ ~.
~ ~.
_.
~, ,.~ '~  ~"- ~ .
~ ,~ ' ..~ ~ ~ ~.
~ .
~.
~ ~ ~ ~ ,'~ ~?
.
?
.~ --~ ,~ ~?~~.=?~..=-~? '
r l  i ~ i It~',-?
~ ~ ~ t', l  ~ ~ ~ ~--~ tt%?
-.
: ,--; ea ~ ~ eq ,--; ?
',i e,:, ~ t',,i?
~ ~ ,~- ~ ~ .
~ ~,  eq e -?
.
~ .
~ e~ t",,I ?,q ~o i _ ,  .
?
.~ ~ ~ ~ ~ ~ ~ o ~  ~ ~ ~ ~ ~g -- x .~ ~ .~ ~ ~ ~ ~ ~ t.-.ICt 'qt",, Iw ' lt~,D" ,4Somers 137 Use Weighted Cusums to Identify SublanguagesTable 7 The 15 groups ranked first according to total ofpairwise averages above the 1.65 threshold ('Sig.
'),and secondly according to the informal scoredescribed in the text.
An asterisk indicates a group wherethe within-group average isabove the 1.65 threshold.Group Pairwise Within Score Sig.average grouprecipes * 5.558 1.870 2.972 14xwords 2.380 0.930 2.559 14TVscripts * 3.527 2.254 1.565 14univs 3.634 1.507 2.411 13childrens * 3.631 2.045 1.776 13blurbs 3.399 1.363 2.494 12BMJ 3.340 1.555 2.148 12economy 2.495 1.276 1.955 12lawreps 2.316 0.866 2.674 10emails 2.399 1.025 2.340 10tourism * 2.685 1.815 1.479 I0footie 2.585 1.386 1.865 9obits 2.369 0.948 2.499 8church 2.179 1.359 1.603 8weather * 1.907 1.638 1.164 71.65 threshold, suggesting lack of homogeneity amongthe recipes, the average of the scores for pairwisecomparisons with other groups is sufficiently high tocompensate this: as Table 6 shows, the average scoresfor recipes are consistently high, and ot~en the highestin any row.
This can be contrasted with the case ofthe 'church' group, where the within-group average isbelow the 1.65 threshold, but so are nearly half thescores for pairwise comparisons.
But the situation canalso be contrasted with the 'TVseripts' and 'childrens'groups: pairwise scores with all the other grofapsindicate significant differences, but so does the within-group average.
This means that each TV script orchildren's story seems significantly different from allthe other samples, including the other TV scripts orchildren's stories.
For the 'tourism' group, too, thescores for pairwise comparison are about the same asthe within-group score.
It so happens that these scoresare a bit nearer the threshold, so we get a 10-4 'Sig.
'score rather than 14--0, but the conclusion is the same:the WQsum earmot distinguish these sublanguages.Finally we have the case of the 'lawreps' and'emails', which are internally homogenous, and can bedistinguished from some, but not all of the other groups.Let us now summarize these observations, andcategorize the four types:A Good result.
Homogeneous and distinctive sub-language: 'xwords', 'univs', 'blurbs', 'BMJ', and'economy'.Somers 138B1 Distinctive sublanguage though internally less ho-mogeneous: 'recipes'B2 Coherent sublanguage though not always distinc-tive: 'lawreps', 'emails'C Distinctive but not coherent: 'TVscripts', 'chil-drens'D Not distinguishable from other groups: "obits','tourism', 'footie', 'church', 'weather'Not surprisingly, if we look at an ordered list ofthe individual pairwise scores, we find that the worstscores (Table 8) are mostly between the groups of typeBz and D.Table 8 Ten worst-scoring pairwise comparisons.lawreps (B2) obits (D) 0.905church (D) obits (D) 0.915church (D) lawreps (B2) 1.020lawreps (B2) weather (D) 1.086church (D) emails (B2) 1.139emails (B2) lawreps (B2) 1.141blurbs (A) BMJ (A) 1.204church (D) footie (D) 1.216footie (D) weather (D) 1.290footie (D) lawreps (B2) 1.299This suggests that the WQsum test is able toquantify the similarity of individual groups, as well asto distinguish sublanguages.
In this experiment we havetaken groups of texts and compared them, but in factthe WQsum algorithm is designed to work on the basisof  individual texts.
In principle, we could simply takea pair of texts and use the algorithm to determine towhat extent they are the same sublanguage.
It mustbe said however that it seems to make more sense touse the test in the comparative manner illustrated here,for example comparing three texts to see which pair ismost similar.
It also seems important to have a baselinescore for an established group of texts belonging to thesame sublanguage.6.2 Reservations and future directionsA very short time before the final version of this paperwas due to be delivered, a further possibility cameto our notice.
Tweedie & Donnelly (1996) describean alternative, multivariate st using weighted eusurnsto compare more than two texts.
Although we havenot had a chance to study this proposal, it claims togive more accurate results than the pairwise applicationof the WQsum formula that has been reported in thispaper.
An obvious next step is to try their proposal.Also, a further step that we might ake would be toanswer the criticism that the scale of our investigationis too small.
The fact that we have taken only three 25-sentence samples of each sublanguage obviously meansUse Weighted Cusums to ldentify SublanguagesIIIIII!1II!1tlIII!IIIIIIIIIIIIIIIIilIIIIIIIIIIthat our conclusions must be somewhat limited.
Ananonymous reviewer commented that "the texts were sodifferent, that it shouldn't be hard at all to discriminatebetween them".
The results in Tables 4 and 6 showthat this is not the ease at all: the groups that the testfailed to distinguish are not necessarily those which tothe human eye are most similar (see Table 8), nor arethe successfully identified groups necessarily the mostdissimilar.
Perhaps this finding is not so surprisingwhen we consider that the linguistic features that areused in the test are so superficial: there is no reasonto expect that the incidence of words beginning witha vowel, for example, would correlate highly withsublanguage type.
And therein lies the real interestof this technique: because the linguistic features aresuperficial, it seems that there is no intuition that wecan appeal to here.Finally, throughout this paper we have referredto 'sublanguage', and the possibility that bur WQsumalgorithm can identify different sublanguages.
It seemsthat the algorithm can distinguish texts, but it is byno means clear what aspect of their difference it iscapturing.
It could for example be merely genre, orsome other aspect of sublanguage, that it is capturingthough again intuitions are difficult to appeal to becauseof the superficiality of the linguistic features used.
Weneed to look more closely at the differences betweenthe text pairs it fails to distinguish and those where itsucceeds, in order to try to get a feel for what, exactly,the test is capturing.
Nevertheless, we feel that it is aninteresting avenue to explore, the more so as it seems tobe quite unlike the other methods described in this field.ReferencesBiber, Douglas.
1988.
Variation across Speech andWriting.
Cambridge University Press.Biber, Douglas.
1990.
Methodological issues regardingcorpus-based analyses of linguistic variation.Literary and Linguistic Computing 5:257-269.Biber, Douglas.
1995.
Dimensions of RegisterVariation: A Cross-Linguistic Comparison.Cambridge University Press.Bissell, A. F. 1995a.
Weighted cumulative sums fortext analysis using word eotmts.
Journal of theRoyal Statistical Society A, 158:525-545.Bissell, Derek.
1995b.
Statistical Methods for TextAnalysis by Word-Counts.
Swanseaf EuropeanBusiness Management School, University of Wales.Burrows, J.F.
1992.
Computers and the study ofliterature.
In Christopher S. Butler (ed.)
Computersand Written Texts, Oxford: Blackwell, 167-204.Farringdon, Jill M. 1996.
Analysing for Authorship: AGuide to the Cusum Technique.
Cardiff: Universityof Wales Press.Grishman, Ralph & Richard Kittredge.
1986.Analyzing Language in Res~'cted Domains.Hillsdale N J: Lawrence Erlbaum Associates.Hilton, Michael L. and David I. Holmes.
1993.
Anassessment of cumulative sum charts for authorshipattribution.
Literary and Linguistic Computing8:73-80.Holmes, David, I.
1994.
Authorship attribution.Computers and the Humanities 28:87-106.Karlgren, Jussi & Douglas Cutting.
1994.
Recognizingtext genres with simple metrics using diseriminantanalysis.
COLING 94: The 15th InternationalConference on Computational Linguistics, Kyoto,Japan, 1071-1075.Kessler, Brett, Geoffrey Nunberg & Hinrich Schiatze.1997.
Automatic detection of text genre.35th Annual Meeting of the Association forComputational Linguistics and "Sth Conferenceof the European Chapter of the Association forComputational Linguistics, Madrid, Spain, 39--47.
"Kittredge, Richard & John Lehrberger (eds) 1982.Sublanguage: Studies of Language in RestrictedSemantic Domains.
Berlin: de Gruyter.Morton, Andrew Queen.
1978.
Literary Detection:How to Prove Authorship and Fraud in Literatureand Documents.
London: Bowker.Potter, Rosarme G. 1991.
Statistical analysis ofliterature: a retrospective on Computers andthe Humanities, 1966-1990.
Computers and theHumanities 25:401--429.Sekine, Satoshi.
1997.
A new direction forsublanguage NLP.
In D. B. Jones & H. L. Somers(eds) New Methods in Language Processing,London: UCL Press, 165--177.Smith, M.W.A.
1982.
Recent experience and newdevelopments of methods for the determination ofauthorship.
ALLC Bulletin 11:73--82.Somers, Harold.
Forthcoming.
Using weighted cusumsto evaluate a Lewis Carroll pastiche.
To appear inComputers and the Humanities.Tweedie, Fiona J.
& Christi A. Donnelly.
1996.
Amultivariate test for the attribution of authorshipof N texts.
In G. Perissinotto (ed.)
Researchin Humanities Computing 5, Oxford: OxfordUniversity Press, 243---248.Ule, L. 1982.
Recent progress in computer methods ofauthorship determination.
ALLCBulletin 10:73---89.Somers 139 Use Weighted Cusums to Identify Sublanguagesmmmmmmmmmmmm
