Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 91?96,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLightly-Supervised Training for Hierarchical Phrase-Based MachineTranslationMatthias Huck1 and David Vilar1,2 and Daniel Stein1 and Hermann Ney11 Human Language Technology and Pattern 2 DFKI GmbHRecognition Group, RWTH Aachen University Berlin, Germany<surname>@cs.rwth-aachen.de david.vilar@dfki.deAbstractIn this paper we apply lightly-supervisedtraining to a hierarchical phrase-based statis-tical machine translation system.
We employbitexts that have been built by automaticallytranslating large amounts of monolingual dataas additional parallel training corpora.
We ex-plore different ways of using this additionaldata to improve our system.Our results show that integrating a secondtranslation model with only non-hierarchicalphrases extracted from the automatically gen-erated bitexts is a reasonable approach.
Thetranslation performance matches the result weachieve with a joint extraction on all train-ing bitexts while the system is kept smallerdue to a considerably lower overall number ofphrases.1 IntroductionWe investigate the impact of an employment of largeamounts of unsupervised parallel data as trainingdata for a statistical machine translation (SMT) sys-tem.
The unsupervised parallel data is created by au-tomatically translating monolingual source languagecorpora.
This approach is called lightly-supervisedtraining in the literature and has been introduced bySchwenk (2008).
In contrast to Schwenk, we do notapply lightly-supervised training to a conventionalphrase-based system (Och et al, 1999; Koehn et al,2003) but to a hierarchical phrase-based translation(HPBT) system.In hierarchical phrase-based translation (Chiang,2005) a weighted synchronous context-free gram-mar is induced from parallel text, the search isbased on CYK+ parsing (Chappelier and Rajman,1998) and typically carried out using the cube prun-ing algorithm (Huang and Chiang, 2007).
In addi-tion to the contiguous lexical phrases as in standardphrase-based translation, the hierarchical phrase-based paradigm also allows for phrases with gapswhich are called hierarchical phrases.
A genericnon-terminal symbol serves as a placeholder thatmarks the gaps.In this paper we study several different waysof incorporating unsupervised training data intoa hierarchical system.
The basic techniques weemploy are the use of multiple translation mod-els and a distinction of the hierarchical and thenon-hierarchical (i.e.
lexical) part of the transla-tion model.
We report experimental results onthe large-scale NIST Arabic-English translation taskand show that lightly-supervised training yields sig-nificant gains over the baseline.2 Related WorkLarge-scale lightly-supervised training for SMT aswe define it in this paper has been first carried outby Schwenk (2008).
Schwenk translates a largeamount of monolingual French data with an initialMoses (Koehn et al, 2007) baseline system into En-glish.
In Schwenk?s original work, an additionalbilingual dictionary is added to the baseline.
Withlightly-supervised training, Schwenk achieves im-provements of around one BLEU point over thebaseline.
In a later work (Schwenk and Senellart,2009) he applies the same method for translationmodel adaptation on an Arabic-French task with91gains of up to 3.5 points BLEU.
1Hierarchical phrase-based translation has been pi-oneered by David Chiang (Chiang, 2005; Chiang,2007) with his Hiero system.
The hierarchicalparadigm has been implemented and extended byseveral groups since, some have published their soft-ware as open source (Li et al, 2009; Hoang et al,2009; Vilar et al, 2010).Combining multiple translation models has beeninvestigated for domain adaptation by Foster andKuhn (2007) and Koehn and Schroeder (2007) be-fore.
Heger et al (2010) exploit the distinction be-tween hierarchical and lexical phrases in a similarway as we do.
They train phrase translation proba-bilities with forced alignment using a conventionalphrase-based system (Wuebker et al, 2010) and em-ploy them for the lexical phrases while the hierarchi-cal phrases stay untouched.3 Using the Unsupervised DataThe most straightforward way of trying to improvethe baseline with lightly-supervised training wouldbe to concatenate the human-generated parallel dataand the unsupervised data and to jointly extractphrases from the unified parallel data (after havingtrained word alignments for the unsupervised bitextsas well).
This method is simple and expected tobe effective usually.
There may however be twodrawbacks: First, the reliability and the amount ofparallel sentences may differ between the human-generated and the unsupervised part of the trainingdata.
It might be desirable to run separate extrac-tions on the two corpora in order to be able to dis-tinguish and weight phrases (or rather their scores)according to their origin during decoding.
Second, ifwe incorporate large amounts of additional unsuper-vised data, the amount of phrases that are extractedmay become much larger.
We would want to avoidblowing up our phrase table sizes without an appro-1Schwenk names the method lightly-supervised training be-cause the topics that are covered in the monolingual source lan-guage data that is being translated may potentially also be cov-ered by parts of the language model training data of the systemwhich is used to translate them.
This can be considered as aform of light supervision.
We loosely apply the term lightly-supervised training if we mean the process of utilizing a ma-chine translation system to produce additional bitexts that areused as training data, but still refer to the automatically pro-duced bilingual corpora as unsupervised data.Arabic EnglishSentences 2 514 413Running words 54 324 372 55 348 390Vocabulary 264 528 207 780Singletons 115 171 91 390Table 1: Data statistics for the preprocessed Arabic-English parallel training corpus.
In the corpus, numer-ical quantities have been replaced by a special categorysymbol.dev (MT06) test (MT08)Sentences 1 797 1 360Running words 49 677 45 095Vocabulary 9 274 9 387OOV [%] 0.5 0.4Table 2: Data statistics for the preprocessed Arabic partof the dev and test corpora.
In the corpus, numericalquantities have been replaced by a special category sym-bol.priate effect on translation quality.
This holds in par-ticular in the case of hierarchical phrases.
Phrase-based machine translation systems are usually ableto correctly handle local context dependencies, butoften have problems in producing a fluent sentencestructure across long distances.
It is thus an intuitivesupposition that using hierarchical phrases extractedfrom unsupervised data in addition to the hierar-chical phrases extracted from the presumably morereliable human-generated bitexts does not increasetranslation quality.
We will compare a joint extrac-tion to the usage of two separate translation mod-els (either without separate weighting, with a binaryfeature, or as a log-linear mixture).
We will furthercheck if including hierarchical phrases from the un-supervised data is beneficial or not.4 ExperimentsWe use the open source Jane toolkit (Vilar et al,2010) for our experiments, a hierarchical phrase-based translation software written in C++.4.1 Baseline SystemThe baseline system has been trained using ahuman-generated parallel corpus of 2.5M Arabic-English sentence pairs.
Word alignments in both92directions were produced with GIZA++ and sym-metrized according to the refined method that wassuggested by Och and Ney (2003).The models integrated into our baseline systemare: phrase translation probabilities and lexicaltranslation probabilities for both translation direc-tions, length penalties on word and phrase level,three binary features marking hierarchical phrases,glue rule, and rules with non-terminals at the bound-aries, four simple additional count- and length-based binary features, and a large 4-gram languagemodel with modified Kneser-Ney smoothing thatwas trained with the SRILM toolkit (Stolcke, 2002).We ran the cube pruning algorithm, the depth ofthe hierarchical recursion was restricted to one byusing shallow rules as proposed by Iglesias et al(2009).The scaling factors of the log-linear model com-bination have been optimized towards BLEU withMERT (Och, 2003) on the MT06 NIST test corpus.MT08 was employed as held-out test data.
Detailedstatistics for the parallel training data are given inTable 1, for the development and the test corpus inTable 2.4.2 Unsupervised DataThe unsupervised data that we integrate has beencreated by automatic translations of parts of theArabic LDC Gigaword corpus (mostly from theHYT collection) with a standard phrase-based sys-tem (Koehn et al, 2003).
We thus in fact conduct across-system and cross-paradigm variant of lightly-supervised training.
Translating the monolingualArabic data has been performed by LIUM, Le Mans,France.
We thank Holger Schwenk for kindly pro-viding the translations.The score computed by the decoder for eachtranslation has been normalized with respect to thesentence length and used to select the most reliablesentence pairs.
Word alignments for the unsuper-vised data have been produced in the same way asfor the baseline bilingual training data.
We reportthe statistics of the unsupervised data in Table 3.4.3 Translation ModelsWe extracted three different phrase tables, one fromthe baseline human-generated parallel data only,one from the unsupervised data only, and one jointArabic EnglishSentences 4 743 763Running words 121 478 207 134 227 697Vocabulary 306 152 237 645Singletons 130 981 102 251Table 3: Data statistics for the Arabic-English unsuper-vised training corpus after selection of the most reliablesentence pairs.
In the corpus, numerical quantities havebeen replaced by a special category symbol.phrase table from the concatenation of the baselinedata and the unsupervised data.
We will denote thedifferent extractions as baseline, unsupervised, andjoint, respectively.The conventional restrictions have been appliedfor phrase extraction in all conditions, i.e.
a maxi-mum length of ten words on source and target sidefor lexical phrases, a length limit of five (includingnon-terminal symbols) on source side and ten on tar-get side for hierarchical phrases, and at most twonon-terminals per rule which are not allowed to beadjacent on the source side.
To limit the number ofhierarchical phrases, a minimum count cutoff of oneand an extraction pruning threshold of 0.1 have beenapplied to them.
Note that we did not prune lexicalphrases.Statistics on the phrase table sizes are presentedin Table 4.2 In total the joint extraction results inalmost three times as many phrases as the baselineextraction.
The extraction from the unsuperviseddata exclusively results in more than two times asmany hierarchical phrases as from the baseline data.The sum of the number of hierarchical phrases frombaseline and unsupervised extraction is very closeto the number of hierarchical phrases from the jointextraction.
If we discard the hierarchical phrases ex-tracted from the unsupervised data and use the lex-ical part of the unsupervised phrase table (27.3Mphrases) as a second translation model in addition tothe baseline phrase table (67.0M phrases), the over-all number of phrases is increased by only 41% com-pared to the baseline system.2The phrase tables have been filtered towards the phrasesneeded for the translation of a given collection of test corpora.93number of phraseslexical hierarchical totalextraction from baseline data 19.8M 47.2M 67.0Mextraction from unsupervised data 27.3M 115.6M 142.9Mphrases present in both tables 15.0M 40.1M 55.1Mjoint extraction baseline + unsupervised 32.1M 166.5M 198.6MTable 4: Phrase table sizes.
The phrase tables have been filtered towards a larger set of test corpora containing a totalof 2.3 million running words.dev (MT06) test (MT08)BLEU TER BLEU TER[%] [%] [%] [%]HPBT baseline 44.1 49.9 44.4?0.9 49.4?0.8HPBT unsupervised only 45.3 48.8 45.2 49.1joint extraction baseline + unsupervised 45.6 48.7 45.4?0.9 49.1?0.8baseline hierarchical phrases + unsupervised lexical phrases 45.1 49.1 45.2 49.2baseline hierarchical phrases + joint extraction lexical phrases 45.3 48.7 45.3 49.1baseline + unsupervised lexical phrases 45.3 48.9 45.3 49.0baseline + unsupervised lexical phrases (with binary feature) 45.3 48.8 45.4 49.0baseline + unsupervised lexical phrases (separate scaling factors) 45.3 48.9 45.0 49.3baseline + unsupervised full table 45.6 48.6 45.1 48.9baseline + unsupervised full table (with binary feature) 45.5 48.6 45.2 48.8baseline + unsupervised full table (separate scaling factors) 45.5 48.7 45.3 49.0Table 5: Results for the NIST Arabic-English translation task (truecase).
The 90% confidence interval is given for thebaseline system as well as for the system with joint phrase extraction.
Results in bold are significantly better than thebaseline.4.4 Experimental ResultsThe empirical evaluation of all our systems on thetwo standard metrics BLEU (Papineni et al, 2002)and TER (Snover et al, 2006) is presented in Ta-ble 5.
We have also checked the results for statisticalsignificance over the baseline.
The confidence in-tervals have been computed using bootstrapping forBLEU and Cochran?s approximate ratio variance forTER (Leusch and Ney, 2009).When we combine the full baseline phrase ta-ble with the unsupervised phrase table or the lexi-cal part of it, we either use common scaling factorsfor their source-to-target and target-to-source trans-lation costs, or we use common scaling factors butmark entries from the unsupervised table with a bi-nary feature, or we optimize the four translation fea-tures separately for each of the two tables as part ofthe log-linear model combination.Including the unsupervised data leads to a sub-stantial gain on the unseen test set of up to +1.0%BLEU absolute.
The different ways of combiningthe manually produced data with the unsupervisedhave little impact on translation quality.
This holdsspecifically for the combination with only the lexicalphrases, which, when marked with a binary feature,is able to obtain the same results as the full (jointextraction) system but with much less phrases.
Wecompared the decoding speed of these two setupsand observed that the system with less phrases isclearly faster (5.5 vs. 2.6 words per second, mea-sured on MT08).
The memory requirements of thesystems do not differ greatly as we are using a bi-narized representation of the phrase table with on-demand loading.
All setups consume slightly lessthan 16 gigabytes of RAM.945 ConclusionWe presented several approaches of applyinglightly-supervised training to hierarchical phrase-based machine translation.
Using the additional au-tomatically produced bitexts we have been able toobtain considerable gains compared to the baselineon the large-scale NIST Arabic-to-English transla-tion task.
We showed that a joint phrase extractionfrom human-generated and automatically generatedparallel training data is not required to achieve sig-nificant improvements.
The same translation qual-ity can be achieved by adding a second translationmodel with only lexical phrases extracted from theautomatically created bitexts.
The overall amount ofphrases can thus be kept much smaller.AcknowledgmentsThe authors would like to thank Holger Schwenkfrom LIUM, Le Mans, France, for making the au-tomatic translations of the Arabic LDC Gigawordcorpus available.
This work was partly supportedby the Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-08-C-0110.Any opinions, findings and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the DARPA.ReferencesJean-Ce?dric Chappelier and Martin Rajman.
1998.
AGeneralized CYK Algorithm for Parsing StochasticCFG.
In Proc.
of the First Workshop on Tabulationin Parsing and Deduction, pages 133?137, April.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.
ofthe 43rd Annual Meeting of the Assoc.
for Computa-tional Linguistics (ACL), pages 263?270, Ann Arbor,MI, June.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, 33(2):201?228,June.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proc.
of the Second Work-shop on Statistical Machine Translation, pages 128?135, Prague, Czech Republic, June.Carmen Heger, Joern Wuebker, David Vilar, and Her-mann Ney.
2010.
A Combination of Hierarchical Sys-tems with Forced Alignments from Phrase-Based Sys-tems.
In Proc.
of the Int.
Workshop on Spoken Lan-guage Translation (IWSLT), Paris, France, December.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A Unified Framework for Phrase-Based, Hierarchical,and Syntax-Based Statistical Machine Translation.
InProc.
of the Int.
Workshop on Spoken Language Trans-lation (IWSLT), pages 152?159, Tokyo, Japan.Liang Huang and David Chiang.
2007.
Forest Rescoring:Faster Decoding with Integrated Language Models.
InProc.
of the Annual Meeting of the Assoc.
for Com-putational Linguistics (ACL), pages 144?151, Prague,Czech Republic, June.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule Filtering by Patternfor Efficient Hierarchical Translation.
In Proc.
of the12th Conf.
of the Europ.
Chapter of the Assoc.
forComputational Linguistics (EACL), pages 380?388,Athens, Greece, March.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proc.
of the Second Workshop on StatisticalMachine Translation, pages 224?227, Prague, CzechRepublic, June.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.
ofthe Human Language Technology Conf.
/ North Amer-ican Chapter of the Assoc.
for Computational Lin-guistics (HLT-NAACL), pages 127?133, Edmonton,Canada, May/June.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Proc.of the Annual Meeting of the Assoc.
for ComputationalLinguistics (ACL), pages 177?180, Prague, Czech Re-public, June.Gregor Leusch and Hermann Ney.
2009.
Edit distanceswith block movements and error rate confidence esti-mates.
Machine Translation, December.Zhifei Li, Chris Callison-Burch, Chris Dyer, SanjeevKhudanpur, Lane Schwartz, Wren Thornton, JonathanWeese, and Omar Zaidan.
2009.
Joshua: An OpenSource Toolkit for Parsing-Based Machine Transla-tion.
In Proc.
of the Workshop on Statistical MachineTranslation, pages 135?139, Athens, Greece, March.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved Alignment Models for Statistical Ma-chine Translation.
In Proc.
of the Joint SIGDAT Conf.on Empirical Methods in Natural Language Process-ing and Very Large Corpora (EMNLP99), pages 20?28, University of Maryland, College Park, MD, June.95Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proc.
of the An-nual Meeting of the Assoc.
for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of the 40thAnnual Meeting of the Assoc.
for Computational Lin-guistics (ACL), pages 311?318, Philadelphia, PA, July.Holger Schwenk and Jean Senellart.
2009.
TranslationModel Adaptation for an Arabic/French News Trans-lation System by Lightly-Supervised Training.
In MTSummit XII, Ottawa, Ontario, Canada, August.Holger Schwenk.
2008.
Investigations on Large-ScaleLightly-Supervised Training for Statistical MachineTranslation.
In Proc.
of the Int.
Workshop on Spo-ken Language Translation (IWSLT), pages 182?189,Waikiki, Hawaii, October.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Conf.
of the Assoc.
for Machine Translationin the Americas (AMTA), pages 223?231, Cambridge,MA, August.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Spoken Language Processing (ICSLP), volume 3,Denver, CO, September.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010.
Jane: Open Source Hierarchical Transla-tion, Extended with Reordering and Lexicon Models.In ACL 2010 Joint Fifth Workshop on Statistical Ma-chine Translation and Metrics MATR, pages 262?270,Uppsala, Sweden, July.Joern Wuebker, Arne Mauser, and Hermann Ney.
2010.Training Phrase Translation Models with Leaving-One-Out.
In Proc.
of the Annual Meeting of the As-soc.
for Computational Linguistics (ACL), pages 475?484, Uppsala, Sweden, July.96
