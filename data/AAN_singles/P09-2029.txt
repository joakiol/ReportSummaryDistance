Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 113?116,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPAn Integrated Multi-document Summarization Approach based onWord Hierarchical RepresentationYou Ouyang, Wenji Li, Qin LuDepartment of ComputingThe Hong Kong Polytechnic University{csyouyang,cswjli,csluqin}@comp.polyu.edu.hkAbstractThis paper introduces a novel hierarchicalsummarization approach for automatic multi-document summarization.
By creating ahierarchical representation of the words in theinput document set, the proposed approach isable to incorporate various objectives of multi-document summarization through anintegrated framework.
The evaluation isconducted on the DUC 2007 data set.1 Introduction and BackgroundMulti-document summarization requires creatinga short summary from a set of documents whichconcentrate on the same topic.
Sometimes anadditional query is also given to specify theinformation need of the summary.
Generally, aneffective summary should be relevant, conciseand fluent.
It means that the summary shouldcover the most important concepts in the originaldocument set, contain less redundant informationand should be well-organized.Currently, most successful multi-documentsummarization systems follow the extractivesummarization framework.
These systems firstrank all the sentences in the original documentset and then select the most salient sentences tocompose summaries for a good coverage of theconcepts.
For the purpose of creating moreconcise and fluent summaries, some intensivepost-processing approaches are also appended onthe extracted sentences.
For example,redundancy removal (Carbonell and Goldstein,1998) and sentence compression (Knight andMarcu, 2000) approaches are used to make thesummary more concise.
Sentence re-orderingapproaches (Barzilay et al, 2002) are used tomake the summary more fluent.
In most systems,these approaches are treated as independent steps.A sequential process is usually adopted in theirimplementation, applying the various approachesone after another.In this paper, we suggest a new summarizationframework aiming at integrating multipleobjectives of multi-document summarization.The main idea of the approach is to employ ahierarchical summarization process which ismotivated by the behavior of a humansummarizer.
While the document set may bevery large in multi-document summarization, thelength of the summary to be generated is usuallylimited.
So there are always some concepts thatcan not be included in the summary.
A naturalthought is that more general concepts should beconsidered first.
So, when a human summarizerfaces a set of many documents, he may follow ageneral-specific principle to write the summary.The human summarizer may start with findingthe core topic in a document set and write somesentences to describe this core topic.
Next hemay go to find the important sub-topics andcover the subtopics one by one in the summary,then the sub-sub-topics, sub-sub-sub-topics andso on.
By this process, the written summary canconvey the most salient concepts.
Also, thegeneral-specific relation can be used to serveother objectives, i.e.
diversity, coherence and etc.Motivated by this experience, we propose ahierarchical summarization approach whichattempts to mimic the behavior of a humansummarizer.
The approach includes two phases.In the first phase, a hierarchical tree isconstructed to organize the important concepts ina document set following the general-to-specificorder.
In the second phase, an iterative algorithmis proposed to select the sentences based on theconstructed hierarchical tree with considerationof the various objectives of multi-documentsummarization.2 Word Hierarchical  Representation2.1 Candidate Word IdentificationAs a matter of fact, the concepts in the originaldocument set are not all necessary to be includedin the summary.
Therefore, before constructingthe hierarchical representation, we first conduct a113filtering process to remove the unnecessaryconcepts in the document set in order to improvethe accuracy of the hierarchical representation.
Inthis study, concepts are represented in terms ofwords.
Two types of unnecessary words areconsidered.
One is irrelevant words that are notrelated to the given query.
The other is generalwords that are not significant for the specifieddocument set.
The two types of words arefiltered through two features, i.e.
query-relevance and topic-specificity.The query-relevance of a word is defined asthe proportion of the number of sentences thatcontains both the word and at least one queryword to the number of sentences that contains theword.
If a feature value is large, it means that theco-occurrence rate of the word and the query ishigh, thus it is more related to the query.
Thetopic-specificity of a word is defined as theentropy of its frequencies in different documentsets.
If the feature value is large, it means that theword appears uniformly in document sets, so itssignificance to a specified document set is low.Thus, the words with very low query-relevanceor with very high topic-specificity are filteredout1.2.2 Word Relation Identification andHierarchical RepresentationTo construct a hierarchical representation for thewords in a given document set, we follow theidea introduced by Lawrie et al (2001) who usethe subsuming relation to express the general-to-specific structure of a document set.
Asubsumption is defined as an association of twowords if one word can be regarded as a sub-concept of the other one.
In our approach, thepointwise mutual information (PMI) is used toidentify the subsumption between words.Generally, two words with a high PMI isregarded as related.
Using the identified relations,the word hierarchical tree is constructed in a top-bottom manner.
Two constraints are used in thetree construction process:(1) For two words related by a subsumptionrelation, the one which appears more frequentlyin the document set serves as the parent node inthe tree and the other one serves as the childnode.
(2) For a word, its parent node in the hierarchicaltree is defined as the most related word, which isidentified by PMI.1 Experimental thresholds are used on the evaluated data.2 http://duc.nist.gov/The construction algorithm is detailed below.Algorithm 1: Hierarchical Tree Construction1: Sort the identified key words by theirfrequency in the document set in descendingorder, denoted as T = {t1, t2 ,?, tn}2: For each ti, i from 1 to n, find the mostrelevant word tj from all the words before ti in T,as Ti = {t1, t2 ,?, ti-1}.
Here the relevance of twowords is calculated by their PMI, i.e.
)()(*),(log),(jijiji tfreqtfreqNttfreqttPMIIf the coverage rate of word ti by word tj2.0)(),()|( tijiji tfreqttfreqttP , ti is regarded asbeing subsumed by tj.
Here freq(ti) is thefrequency of ti in the document set and  freq(ti,ti) is the co-occurrence of ti and tj in the samesentences of the document set.
N is the totalnumber of tokens in the document set.4: After all the subsumption relations are found,the tree is constructed by connecting the relatedwords from the first word t1.An example of a tree fragment is demonstratedbelow.
The tree is constructed on the documentset D0701A from DUC 20072, the query of thisdocument set is ?Describe the activities ofMorris Dees and the Southern Poverty LawCenter?.3 Summarization based on WordHierarchical Representation3.1 Word Significance EstimationIn order to include the most significant conceptsinto the summary, before using the hierarchicaltree to create an extract, we need to estimate thesignificance of the words on the tree first.Initially, a rough estimation of the significance ofa word is given by its frequency in the documentset.
However, this simple frequency-basedmeasure is obviously not accurate.
One thing weobserve from the constructed hierarchical tree isthat a word which subsumes many other words isusually very important, though it may not appearCenterDee Law groupMorris hatePoverty Southernlawyer organizationcivil Klan114frequently in the document set.
The reason is thatthe word covers many key concepts so it isdominant in the document set.
Motivated by this,we develop a bottom-up algorithm whichpropagates the significance of the child nodes inthe hierarchical tree backward to their parentnodes to boost the significance of nodes withmany descendants.Algorithm 2: Word Scoring Theme1: Set the initial score of each word in T as itslog-frequency, i.e.
score(ti) =log freq(ti).2: For ti from n to 1, propagate an importancescore to its parent node par(ti) (if exists)according to their relevance, i.e.
score(par(ti)) =score(par(ti)) + log freq(ti, par(ti)).3.2 Sentence SelectionBased on the word hierarchical tree and theestimated word significance, we propose aniterative algorithm to select sentences which isable to integrate the multiple objectives forcomposing a relevant, concise and fluentsummary.
The algorithm follows a general-to-specific order to select sentences into thesummary.
In the implementation, the idea iscarried out by following a top-down order tocover the words in the hierarchical tree.
In thebeginning, we consider several ?seed?
wordswhich are in the top-level of the tree (thesewords are regarded as the core concepts in thedocument set).
Once some sentences have beenextracted according to these ?seed?
words, thealgorithm moves to down-level words throughthe subsumption relations between the words.Then new sentences are added according to thedown-level words and the algorithm continuesmoving to lower levels of the tree until the wholesummary is generated.
For the purpose ofreducing redundancy, the words already coveredby the extracted sentences will be ignored whileselecting new sentences.
To improve the fluencyof the generated summary, after a sentence isselected, it is inserted to the position according tothe subsumption relation between the words ofthis sentence and the sentences which are alreadyin the summary.
The detailed process of thesentence selection algorithm is described below.Algorithm 3: Summary Generation1: For the words in the hierarchical tree, set theinitial states of the top n words3 as ?activated?and the states of other words as ?inactivated?.2: For all the sentences in the document set,3 n is set to 3 experimentally on the evaluation data set.select the sentence with the largest scoreaccording to the ?activated?
word set.
Thescore of a sentence s is defined as?
)(|| 1)( itscoressscore  where ti is a wordbelongs to s and the state of ti should be?activated?.
| s | is the number of words in s.3: For the selected sentence sk, the subsumptionrelations between it and the existing sentencesin the current summary are calculated and themost related sentence sl is selected.
sk is theninserted to the position right behind sl.4: For each word ti belongs to the selectedsentence sk, set its state to ?inactivated?
; foreach word tj which is subsumed by ti, set itsstate to ?activated?.5: Repeat step 2-4 until the length limit of thesummary is exceeded.4 ExperimentExperiments are conducted on the DUC 2007data set which contains 45 document sets.
Eachdocument set consists of 25 documents and atopic description as the query.
In the taskdefinition, the length of the summary is limitedto 250 words.
In our summarization system, pre-processing includes stop-word removal and wordstemming (conducted by GATE4).One of the DUC evaluation methods, ROUGE(Lin and Hovy, 2003), is used to evaluate thecontent of the generated summaries.
ROUGE is astate-of-the-art automatic evaluation methodbased on N-gram matching between systemsummaries and human summaries.
In theexperiment, our system is compared to the topsystems in DUC 2007.
Moreover, a baselinesystem which considers only the frequencies ofwords but ignores the relations between words isincluded for comparison.
Table 1 below showsthe average recalls of ROUGE-1, ROUGE-2 andROUGE-SU4 over the 45 DUC 2007documentsets.
In the experiment, the proposedsummarization system outperforms the baselinesystem, which proves the benefit of consideringthe relations between words.
Also, the systemranks the 6th among the 32 submitted systems inDUC 2007.
This shows that the proposedapproach is competitive.ROUGE-1 ROUGE-2 ROUGE-SU4S15 0.4451 0.1245 0.1771S29 0.4325 0.1203 0.1707S4 0.4342 0.1189 0.1699S24 0.4526 0.1179 0.17594 http://gate.ac.uk/115S13 0.4218 0.1117 0.1644Ours 0.4257 0.1110 0.1608Baseline 0.4088 0.1040 0.1542Table 1.
ROUGE Evaluation ResultsTo demonstrate the advantage of the proposedapproach, i.e.
its ability to incorporate multiplesummarization objectives, the fragments of thegenerated summaries on the data set D0701A arealso provided below as a case study.The summary produced by our systemThe Southern Poverty Law Center tracks hategroups, and Intelligence Report covers right-wingextremists.Morris Dees, co-founder of the Southern PovertyLaw Center in Montgomery, Ala.Dees, founder of the Southern Poverty LawCenter, has won a series of civil right suits againstthe Ku Klux Klan and other racist organizations ina campaign to drive them out of business.In 1987, Dees won a $7 million verdict against aKu Klux Klan organization over the slaying of a19-year-old black man in Mobile, Ala.The summary produced by the baseline systemMorris Dees, co-founder of the Southern PovertyLaw Center in Montgomery, Ala.The Southern Poverty Law Center tracks hategroups, and Intelligence Report covers right-wingextremists.The Southern Poverty Law Center previouslyrecorded a 20-percent increase in hate groupsfrom 1996 to 1997.The verdict was obtained by lawyers for theSouthern Poverty Law Center, a nonprofitorganization in Birmingham, Ala.Comparing the generated summaries of thetwo systems, we can see that the summarygenerated by the proposed approach is better incoherence and fluency since these factors areconsidered in the integrated summarizationframework.
Various summarization approaches,i.e.
sentence ranking, redundancy removal andsentence re-ordering, are all implemented in thesentence selection algorithm based on the wordhierarchical tree.
However, we also observe thatthe proposed approach fails to generate bettersummaries on some document sets.
The mainproblem is that the quality of the constructedhierarchical tree is not always satisfied.
In theproposed summarization approach, we mainlyrely on the PMI between the words to constructthe hierarchical tree.
However, a single PMI-based measure is not enough to characterize theword relation.
Consequently the constructed treecan not always well represent the concepts forsome document sets.
Another problem is that thetwo constraints used in the tree constructionalgorithm are not always right in real data.
So weregard developing better tree constructionapproaches as of primary importance.
Also, thereare other places which can be improved in thefuture, such as the word significance estimationand sentence inserting algorithms.
Nevertheless,we believe that the idea of incorporating themultiple summarization objectives into oneintegrated framework is meaningful and worthfurther study.5 ConclusionWe introduced a summarization frameworkwhich aims at integrating various summarizationobjectives.
By constructing a hierarchical treerepresentation for the words in the originaldocument set, we proposed a summarizationapproach for the purpose of generating a relevant,concise and fluent summary.
Experiments onDUC 2007 showed the advantages of theintegrated framework.AcknowledgmentsThe work described in this paper was partiallysupported by Hong Kong RGC Projects (No.PolyU 5217/07E) and partially supported by TheHong Kong Polytechnic University internalgrants (A-PA6L and G-YG80).ReferencesR.
Barzilay, N. Elhadad, and K. R. McKeown.
2002.Inferring strategies for sentence ordering inmultidocument news summarization.
Journal ofArtificial Intelligence Research, 17:35-55, 2002.J.
Carbonell and J. Goldstein.
1998.
The Use of MMR,Diversity-based Reranking for ReorderingDocuments and Producing Summaries.
InProceedings of ACM SIGIR 1998, pp 335-336.K.
Knight and D. Marcu.
2000.
Statistics-basedsummarization --- step one: Sentence compression.In Proceeding of The American Association forArtificial Intelligence Conference (AAAI-2000),pp 703-710.D.
Lawrie, W. B. Croft and A. Rosenberg.
2001.Finding topic words for hierarchicalsummarization.
In Proceedings of ACM SIGIR2001, pp 349-357.C.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurance statistics.In Proc.
of HLT-NAACL 2003, pp 71-78.116
