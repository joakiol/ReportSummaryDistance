Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647?657,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDeep Learning for Chinese Word Segmentation and POS TaggingXiaoqing ZhengFudan University220 Handan RoadShanghai, 200433, Chinazhengxq@fudan.edu.cnHanyang ChenFudan University220 Handan RoadShanghai, 200433, Chinachenhy12345@gmail.comTianyu XuFudan University220 Handan RoadShanghai, 200433, Chinaxty213@gmail.comAbstractThis study explores the feasibility of perform-ing Chinese word segmentation (CWS) andPOS tagging by deep learning.
We try to avoidtask-specific feature engineering, and use deeplayers of neural networks to discover relevantfeatures to the tasks.
We leverage large-scaleunlabeled data to improve internal representa-tion of Chinese characters, and use these im-proved representations to enhance supervisedword segmentation and POS tagging models.Our networks achieved close to state-of-the-art performance with minimal computationalcost.
We also describe a perceptron-style al-gorithm for training the neural networks, asan alternative to maximum-likelihood method,to speed up the training process and make thelearning algorithm easier to be implemented.1 IntroductionWord segmentation has been a long-standing chal-lenge for the Chinese NLP community.
It has re-ceived steady attention over the past two decades.Previous studies show that joint solutions usuallylead to the improvement in accuracy over pipelinedsystems by exploiting POS information to help wordsegmentation and avoiding error propagation.
How-ever, traditional joint approaches usually involve agreat number of features, which arises four limita-tions.
First, the size of the result models is too largefor practical use due to the storage and computingconstraints of certain real-world applications.
Sec-ond, the number of parameters is so large that thetrained model is apt to overfit on training corpus.Third, a longer training time is required.
Last butnot the least, the decoding by dynamic programmingtechnique might be intractable since a large searchspace is faced by the decoder.The choice of features, therefore, is a critical suc-cess factor for these systems.
Most of the state-of-the-art systems address their tasks by applying linearstatistical models to the features carefully optimizedfor the tasks.
This approach is effective because re-searchers can incorporate a large body of linguisticknowledge into the models.
However, the approachdoes not scale well when it is used to perform morecomplex joint tasks, for example, the task of jointword segmentation, POS tagging, parsing, and se-mantic role labeling.
A challenge for such a jointmodel is the large combined search space, whichmakes engineering effective task-specific featuresand structured learning of parameters very hard.
In-stead, we use multilayer neural networks to discoverthe useful features from the input sentences.There are two main contributions in this paper.
(1)We describe a perceptron-style algorithm for train-ing the neural networks, which not only speeds upthe training of the networks with negligible loss inperformance, but also can be implemented more eas-ily; (2) We show that the tasks of Chinese word seg-mentation and POS tagging can be effectively per-formed by the deep learning.
Our networks achievedclose to state-of-the-art performance by transferringthe unsupervised internal representations of Chinesecharacters into the supervised models.Section 2 presents the general architecture ofneural networks, and our perceptron-style trainingalgorithm for tagging.
Section 3 describes how to647leverage large unlabeled data to obtain more usefulcharacter embeddings, and reports the experimentalresults of our systems.
Section 4 presents a briefoverview of related work.
The conclusions are givenin section 5.2 The Neural Network ArchitectureChinese word segmentation and part-of-speech tag-ging tasks can be formulated as assigning labels tocharacters of an input sentence.
The performanceof the traditional tagging approaches is heavily de-pendent on the choice of features, for example, con-ditional random fields (CRFs), often with a set offeature templates.
For that reason, much of the ef-fort in designing such systems goes into the featureengineering, which is important but labor-intensive,mainly based on human ingenuity and linguistic in-tuition.In order to make learning algorithms less depen-dent on the feature engineering, we chose to use avariant of the neural network architecture first pro-posed by (Bengio et al 2003) for probabilistic lan-guage model, and reintroduced later by (Collobertet al 2011) for multiple NLP tasks.
The networktakes the input sentence and discovers multiple lev-els of feature extraction from the inputs, with higherlevels representing more abstract aspects of the in-puts.
The architecture is shown in Figure 1.
The firstlayer extracts features for each Chinese character.The next layer extracts features from a window ofcharacters.
The following layers are classical neuralnetwork layers.
The output of the network is a graphover which tag inference is achieved with a Viterbialgorithm.2.1 Mapping Characters into Feature VectorsThe characters are fed into the network as indicesthat are used by a lookup operation to transformcharacters into their feature vectors.
We considera fixed-sized character dictionary D1.
The vectorrepresentations are stored in a character embeddingmatrixM ?
Rd?|D|, where d is the dimensionalityof the vector space (a hyper-parameter to be chosen)and |D| is the size of the dictionary.1Unless otherwise specified, the character dictionary is ex-tracted from the training set.
Unknown characters are mappedto a special symbol that is not used elsewhere....?
??
?1"A dog sits in the corner"2345d ?
1d......6?............TextInput WindowFeaturesLookup TableConcatenate......Number of Hidden UnitsLinearb2?
+SigmoidW 3 b3?
+g( )Number of tagsW 2LinearTag InferenceBESIf(t|1) f(t|2) f(t|n)f(t|n?1)f(t|i)AijNumber of Hidden UnitsFigure 1: The neural network architecture.Formally, assume we are given a Chinese sen-tence c[1:n] that is a sequence of n characters ci, 1 ?i ?
n. For each character ci ?
D that has an associ-ated index ki into the column of the embedding ma-trix, an d-dimensional feature vector representationis retrieved by the lookup table layer ZD(?)
?
Rd:ZD(ci) =Meki (1)where we use a binary vector eki ?
R|D|?1 which iszero in all positions except at the ki-th index.
Thelookup operation can be seen as a simple projectionlayer.
The feature vector of each character, startingfrom a random initialization, can be automaticallytrained by back propagation to be relevant to the taskof interest.In practice, it is common that one might want toprovide other additional features that is thought tobe helpful for the task.
For example, for the nameentity recognition task, one could provide a featurewhich says if a character is in a list of the commonChinese surnames or not.
Another common practiceis to introduce some statistics-based measures, such648as boundary entropy (Jin and Tanaka-Ishii, 2006)and accessor variety (Feng et al 2004), which arecommonly used in unsupervised CWS models.
Weassociate a lookup table to each additional feature,and the character feature vector becomes the con-catenation of the outputs of all these lookup tables.2.2 Tag scoringA neural network can be considered as a functionf?(?)
with parameters ?.
Any feed-forward neuralnetwork with L layers can be seen as a compositionof functions f l?(?)
defined for each layer l:f?(?)
= fL?
(fL?1?
(.
.
.
f1?
(?)
.
.
.))
(2)For each character in a sentence, a score is pro-duced for every tag by applying several layers of theneural network over the feature vectors produced bythe lookup table layer.
We use a window approachto handle the sequences of variable sentence length.The window approach assumes that the tag of a char-acter depends mainly on its neighboring characters.More precisely, given an input sentence c[1:n], weconsider all successive windows of size w (a hyper-parameter), siding over the sentence, from characterc1 to cn.
At position ci, the character feature win-dow produced by the first lookup table layer can bewritten as:f1?
(ci) =????????ZD(ci?w/2)...ZD(ci)...ZD(ci+w/2)????????
(3)The characters with indices exceeding the sentenceboundaries are mapped to one of two special sym-bols, namely ?start?
and ?stop?
symbols.The fixed-sized vector f1?
is fed to two stan-dard Linear Layers that successively perform affinetransformations over f1?
, interleaved with some non-linearity function g(?
), to extract highly non-linearfeatures.
Given a set of tags T for the task of inter-est, the network outputs a vector of size |T | for eachcharacter at position i, interpreted as a score for eachtag in T and each character ci in the sentence:f?
(ci) = f3?
(g(f2?
(f1?
(ci))))= W 3g(W 2f1?
(ci) + b2) + b3(4)where the matrices W 2 ?
RH?
(wd), b2 ?
RH ,W 3 ?
R|T |?H and b3 ?
R|T | are the parameters tobe trained.
The hyper-parameter H is usually calledthe number of hidden units.
As non-linear function,we chose a sigmoidal function2:g(x) = 1/(1 + e?x) (5)2.3 Tag InferenceThere are strong dependencies between charactertags in a sentence for the tasks like word segmen-tation and POS tagging.
The tags are organized inchunks, and it is impossible for some tags to fol-low other tags.
We introduce a transition score Aijfor jumping from i ?
T to j ?
T tags in succes-sive characters, and an initial scores A0i for startingfrom the i-th tag for taking into account the sentencestructure.
We want the valid paths of tags to be en-couraged, while discouraging all other paths.Given an input sentence c[1:n], the network out-puts the matrix of scores f?(c[1:n]).
We use a nota-tion f?
(t|i) to indicate the score output by the net-work with parameters ?, for the sentence c[1:n] andfor the t-th tag, at the i-th character.
The score of asentence c[1:n] along a path of tags t[1:n] is then givenby the sum of transition and network scores:s(c[1:n], t[1:n], ?)
=n?i=1(Ati?1ti + f?
(ti|i)) (6)Given a sentence c[1:n], we can find the best tag patht?
[1:n] by maximizing the sentence score:t?
[1:n] = argmax?t?
[1:n]s(c[1:n], t?
[1:n], ?)
(7)The Viterbi algorithm can be used for this inference.Now we are prepared to show how to train the para-meters of the network in an end-to-end fashion.2.4 TrainingThe training problem is to determine all the para-meters of the network ?
= (M,W 2, b2,W 3, b3, A)from training data.
The network generally is trained2In our experiments, the sigmoidal function performsslightly better than the ?hard?
version of the hyperbolic tangentused by (Collobert, 2011).649by maximizing a likelihood over all the sentences inthe training setR with respect to ?:?
7???
(c,t)?Rlog p(t|c, ?)
(8)where c represents a sentence and its associated fea-tures, and t denotes the corresponding tag sequence.We drop the subscript [1 : n] from now for nota-tion simplification.
The probability p(?)
is calcu-lated from the outputs of the neural network.
Wewill present in the following section how to interpretneural network outputs as probabilities.Maximizing the log-likelihood (8) with the gradi-ent ascent algorithm3 is achieved by iteratively se-lecting a example (c, t) and applying the followinggradient update rule:?
?
?
+ ??
log p(t|c, ?)??
(9)where ?
is the learning rate (a hyper-parameter).The gradient in (9) can be computed by a classicalback propagation: the differentiation chain rule isapplied through the network, until the character em-bedding layer.2.4.1 Sentence-Level Log-LikelihoodThe score of a sentence (6) is interpreted as a con-ditional tag path probability by taking it to the expo-nential (making the score positive) and normalizingit over all possible tag paths (summing to 1 over allpaths).
Taking the log, the conditional probability ofthe true path t is given by4:log p(t|c, ?)
= s(c, t, ?)?
log?
?t?exp{s(c, t?, ?
)}(10)3We did not use the stochastic gradient ascent algorithm(Bottou, 1991) to train the network as (Collobert et al2011).
The gradient ascent algorithm was used instead for fairlycomparing our algorithm with the sentence-level maximum-likelihood method (see Section 2.4.1).
The gradient ascent al-gorithm requires a loop over all the examples to compute thegradient of the cost function, which will not cause a problemsince all the training sets used in this article are finite.4The cost functions are differentiable everywhere thanksto the differentiability of sigmoidal function chosen as non-linearity instead of a ?hard?
version of the hyperbolic tangent.For details about gradient computations, see Appendix A of(Collobert et al 2011).The number of terms in (10) grows exponentiallywith the length of the input sentence.
Although onecan compute it in linear time with the Viterbi algo-rithm, it is quite computationally expensive to com-pute the conditional probability of the true path, andits derivatives with respect to f?
(t|i) and Aij .
Thegradients with respect to the trainable parametersother than f?
(t|i) and Aij can all be computed usingthe derivatives with respect to f?
(t|i) by applyingthe differentiation chain rule.
We will see in the nextsection our training algorithm that has the advantageof being much cheaper to compute the gradients.2.5 A New Training MethodThe log-likelihood (10) can be seen as the differencebetween the forward score constrained over the validpath and the sum of the scores of all possible paths.While this training criterion is used, the neural net-works are trained by maximizing the likelihood oftraining data.
In fact, a CRF maximizes the samelog-likelihood (Lafferty et al 2001) by using a lin-ear model in stead of a nonlinear neural network.As an alternative to maximum-likelihood method,we propose the following training algorithm inspiredby the work of (Collins, 2002).
Given a trainingexample (c, t), the network outputs the matrix ofscores f?
(c) under the current parameter settings.The highest scoring sequence of tags for the inputsentence c then can be found using the Viterbi al-gorithm: this tagged sequence is denoted by t?.
Forevery character ci where ti 6= t?i, we simply set?L?
(t, t?|c)?f?(ti|i)++,?L?
(t, t?|c)?f?(t?i|i)??
(11)and for every transition where ti?1 6= t?i?1 or ti 6= t?i,we set?L?
(t, t?|c)?Ati?1ti++,?L?
(t, t?|c)?At?i?1t?i??
(12)where ?++?
(which increases a value by one) and????
(which decreases a value by one) are twounary operators, and L?
(t, t?|c) is a new functionwhich we nowwant to maximize over all the trainingpairs (c, t).
The functionL?
(t, t?|c) can be viewed asthe difference between the score of the correct pathand that of the incorrect one (which is the highestscoring sequence produced by the network under thecurrent parameters ?
).650As an example, say the correct tag sequence of thesentence?3p ?A dog sits in the corner?
is/S ?/S 3/S p/B /Eand under the current parameter settings the highestscoring tag sequence is/S ?/B 3/E p/B /EThen the derivatives with respect to f?(S|?
), andf?
(S|3) will be set to 1, that with respect to ASB ,ABE , AEB , f?(B|?
), and f?
(E|3) to?1, and thatwith respect to ASS to 2 respectively5.
Intuitivelythese assignments have the effect of updating the pa-rameter values in a way that increases the score ofthe correct tag sequence and decreases the score ofthe incorrect one output by the network with the cur-rent parameter settings.
If the tag sequence producedby the network is correct, no changes are made to thevalues of parameters.Inputs:R: a training set.N : a specified maximum number of iterations.E: a desired tagging precision.Initialization: set the initial parameters of the network withsmall random values.Output: the trained parameters ?
?Algorithm:dofor each example (c, t) ?
Rget the matrix f?
(c) by the neural network under thecurrent parameters ?find the highest scoring sequence of tags t?
for c withf?
(c) and Aij by using the Viterbi algorithmif (t 6= t?
)compute the gradients with respect to f?
(c) andAij as (11) and (12)compute the gradients with respect to the weightsfrom output layer to character embedding layerupdate the parameters of the network by (9)until the desired precision E achieved or maximum num-ber of iterations N reachedreturn ?
?Figure 2: The training algorithm for tagging.We propose the training algorithm in Figure 2.Note that the perceptron algorithm of (Collins,2002) was designed for discriminatively training an5The derivatives with respect to ASB will be set to 0, be-cause it is increased first and decreased afterwards.HMM-style tagger, while our algorithm is used tocalculate the ?direction?
in which the parameters areupdated (i.e.
the gradient of the function we want tomaximize).
Due to space limitations, we do not giveconvergence theorems justifying the training algo-rithm in this paper.
Intuitively it can be achieved bycombining the theorems of convergence for the per-ceptron applied to tagging problem from (Collins,2002) with the convergence results of backpropaga-tion algorithm from (Rumelhart et al 1986).3 ExperimentsWe conducted three sets of experiments.
The goalof the first one is to test several variants for eachtraining algorithm on the development set, to gainsome understanding of how the choice of hyper-parameters impacts upon the performance.
We ap-plied the network both with the sentence-level log-likelihood (SLL) and our perceptron-style trainingalgorithm (PSA) to the two Chinese NLP problems:word segmentation, and joint CWS and POS tag-ging.
We ran this set of experiments on the part ofChinese Treebank 4 (CTB-4)6.
Ninety percent of thesentences (1529) were randomly chosen for trainingand the rest (168) were used as development set.The second set of experiments was run on theChinese Treebank (CTB) data sets from Bakeoff-3(Levow, 2006), which contains a training and a testcorpus for supervised word segmentation and POStagging tasks.
The results were obtained without us-ing any extra knowledge (i.e.
the closed test), andare comparable with other models in the literature.In the third experiment, we study to see how welllarge unlabeled texts can be used to enhance thesupervised learning.
Following (Collobert et al2011), we first use large unlabeled data set to ob-tain character embeddings carrying more syntacticand semantic information, and then use these im-proved embeddings to initialize the character lookuptables of the networks instead of previous randomvalues.
Our corpus is the Sina news7 that containsabout 325MB data.6The data set was sections 1?43, 144?169, and 900?931 ofthe treebank, containing 78,023 characters, 45,135 words and1,697 sentences.
These files are double-annotated and can beregarded as golden standard files.7Available at http://www.sina.com.cn/651We implemented two versions of the network:one for the sentence-level log-likelihood and onefor our perceptron-style training algorithm.
Bothare written in Java language.
All experiments wererun on a computer equipped with an Intel Core i3processor working at 2.13GHz, with 2GB RAM,running Linux and Java Development Kit 1.6.
Thestandard F-score was used to evaluate the perfor-mance of both word segmentation and joint wordsegmentation and POS tagging tasks.
F-score is theharmonic mean of precision p and recall r, which isdefined as 2pr/(p + r).9 08 07 0601 3 5 7 9Window SizeF-score10 0Fword:  S E G  (P S A)Fword :  S E G  (S L L )Foov:  S E G  (P S A)Foov:  S E G  (S S L )Fword :  J W P  (P S A)Fword :  J W P  (S L L )Foov:  J W P  (P S A)Foov:  J W P  (S S L )Fpos :  J W P  (P S A)Fpos:  J W P  (S S L )Figure 3: Average F-score versus window size.3.1 Tagging SchemesThe network will output the scores for all the possi-ble tags for the task of interest.
For word segmen-tation, each character will be assigned one of fourpossible boundary tags: ?B?
for a character locatedat the beginning of a word, ?I?
for that inside of aword, ?E?
for that at the end of a word, and ?S?
fora character that is a word by itself.Following Ng and Lou (2004) we perform jointword segmentation and POS tagging task in a la-beling fashion by expanding boundary labels to in-clude POS tags.
For instance, we describe verbphases using four different tags.
Tag ?S VP?
is usedto mark a verb phase containing a single character.Other tags ?B VP?, ?I VP?, and ?E VP?
are used to9 08 07 06010 0 30 0 50 0 7 0 0 9 0 010 0F-scoreN u m b er of  H idden U nit sFword:  S E G  (P S A)Fword :  S E G  (S L L )Foov:  S E G  (P S A)Foov:  S E G  (S S L )Fword:  J W P  (P S A)Fword:  J W P  (S L L )Foov:  J W P  (P S A)Foov:  J W P  (S S L )Fpos:  J W P  (P S A)Fpos:  J W P  (S S L )Figure 4: Average F-score versus number of hidden units.mark the first, in-between and last characters of theverb phrase.
In fact, we used the ?IOBES?
taggingscheme, and tag ?O?
is not applicable to Chineseword segmentation and POS tagging tasks.3.2 The Choice of Hyper-parametersWe tuned the hyper-parameters by trying only afew different networks.
We report in Figure 3the F-scores on the development set versus win-dow size for word segmentation (SEG) and jointword segmentation and POS tagging (JWP) taskswith the sentence-level log-likelihood (SLL) and ourperceptron-style training algorithm (PSA), and re-port in Figure 4 the F-scores on the same data setversus number of hidden units.
The average F-scores were obtained over 5 runs with different ran-dom initialization for each setting of the network.The F-scores of the word segmentation, out-of-vocabulary, and POS tagging are denoted by Fword,Foov and Fpos respectively.Generally, the number of hidden units has a lim-ited impact on the performance if it is large enough,which is consistent with the findings of (Collobertet al 2011) for English.
It can be seen from Fig-ure 3 that the performance drops smoothly when thewindow size is larger than 3.
In particularly, the F-score of out-of-vocabulary identification decreasesrelatively fast beyond window size 5, which shows652that the size of window (and the number of parame-ters) is too large that the trained network has over-fitted on training data.
An explanation for this resultis that most Chinese words are less than 3 charac-ters, and the neighboring characters outside of thewindow (size 5) become ?noise?
when we performword segmentation.The hyper-parameters of the network used in allthe following experiments are shown in Table 1.
Al-though the top performance was obtained by the net-work with window size 3, we chose the architecturewith window size 5 because a larger training corpuswill be used in the following experiments, and thesparseness problem would be alleviated.
Further-more, in order to obtain character embeddings byusing large unlabeled data, we prefer to ?observe?a character within a slightly larger window to betterdiscover its syntactic and semantic information.Hyper-parameter ValueWindow size 5Number of hidden units 300Character feature dimension 50Learning rate 0.02Table 1: Hyper-parameters of the network.We report in Table 2 the F-score of the first fiveiterations on the development set for word segmen-tation with SLL and PSA.
The data in the fourthand fifth rows of the table shows the convergenceof PSA.
The difference of the F-scores between thenetworks with SLL and PSA can be negligible afterthe number of iteration is greater than 5.
In our im-plementation, for each iteration the training time isreduced at least 10% by using PSA, compared withSLL.
The training time can be reduced further formore complex tasks like POS tagging and semanticrole labeling in which a larger tag set is used.Iteration 1 2 3 4 5SSLFword 49.89 69.56 88.91 90.19 91.24Foov 15.92 27.54 54.37 55.89 59.74Time (s) 209 398 586 737 886PSAFword 49.04 68.54 87.79 89.07 91.19Foov 13.61 25.79 52.30 55.15 60.49Time (s) 184 343 497 610 754Table 2: Word segmentation results with SLL and PSAfor the first five iterations.Many exponential sums (?i exp(xi)) are re-quired for training the networks with SLL, and inmost cases the values of exponential sums will ex-ceed the range of double-precision floating-pointarithmetic defined in popular programming lan-guages.
These sums need to be estimated by an-alytic number theory.
In comparison, a lot of thecomputation-intensive exponential sums are avoidedin our training algorithm, which not only speed upthe training of the networks but also make it easierto be implemented.3.3 Closed Test on the SIGHAN BakeoffWe trained the networks with PSA on the ChineseTreebank (CTB) data set from Bakeoff-3 for bothSEG and JWP tasks.
The results are reported in Ta-ble 3.
The hyper-parameters of our networks are re-ported in Table 1.
Although results show that ournetworks with PSA are behind the state-of-the-artsystems, the networks perform comparatively well,considering we did not use any extra information.Many other systems used some extra heuristics or re-sources to improve their performance.
For example,a key parameter in the system of (Wang et al 2006)was optimized in advance by using an external seg-mented corpus, and a manually prepared list of char-acters as well as their types was used in (Zhao et al2006; Zhu et al 2006; Kruengkrai et al 2009).It is worth noting that the comparison for jointword segmentation and POS tagging task is indirectbecause the different versions of CTB were used.We reported the results on CTB-3 from SIGHANBakeoff-3, while both (Jiang et al 2008) and (Kru-engkrai et al 2009) used CTB-5.
Both (Ng andLou, 2004) and (Zhang and Clark, 2008) evenly par-titioned the sentences in CTB3 into ten groups, andused nine groups for training and the rest for testing.Following (Bengio et al 2003; Collobert et al2011), we want semantically and syntactically sim-ilar characters to be close in the embedding space.If we knew that ?dog?
andc ?cat?
were similarsemantically, and similarly for?
?sit?
and2 ?lie?,we could generalize from ?3p ?A dog sitsin the corner?
toc?3p ?A cat sits in the cor-ner?, and toc23p ?A cat lies in the corner?
inthe same way.
We describe the way to obtain thesecharacter embeddings by using large unlabeled datain the next section.653Approach Fword Roov FposSEG(Zhao et al 2006) 93.30 70.70 ?
(Wang et al 2006) 93.00 68.30 ?
(Zhu et al 2006) 92.70 63.40 ?
(Zhang et al 2006) 92.60 61.70 ?
(Feng et al 2006) 91.70 68.00 ?PSA 92.59 64.24 ?PSA + LM 94.57 70.12 ?JWP(Ng and Lou, 2004) 95.20 ?
?
(Zhang and Clark, 2008) 95.90 ?
91.34(Jiang et al 2008) 97.30 ?
92.50(Kruengkrai et al 2009) 96.11 ?
90.85PSA 93.83 68.21 90.79PSA + LM 95.23 72.38 91.82Table 3: Comparison of the F-scores on the Penn ChineseTreebank3.4 Combined ApproachWe used the corpus of Sina news to obtain charac-ter embeddings carrying more semantic and syntac-tic information by training a language model thatevaluates the acceptability of a piece of text.
Thislanguage model is again the neural network, and wealso use PSA to train the language model.
Following(Collobert et al 2011), we minimize the followingcriterion with respect to the parameters ?:?
7???h?H??c?
?Dmax{0, 1?
f?
(c|h) + f?
(c?|h)}(13)where the score f?
(c|h) is the output of the networkwith parameters ?
for a character c at the center ofa window h, D is the dictionary of characters, H isthe set of all possible text windows (i.e.
character se-quences) from the training data, and c?|h denotes thewindow obtained by replacing the central characterof the window h by the character c?.We used a dictionary consisting of the charac-ters extracted from all the data sets in Bakeoff-3, which contains about eight thousand characters.The total unsupervised training time was about twoweeks.
Our combined approach works as initializingthe lookup tables of the supervised networks withthe character embeddings obtained by unsupervisedlearning, and then performing supervised training onCTB-3.
The lookup tables will not be modified at thesupervised training stage.We reported the results in Table 3, in which ourcombined approach is indicated by ?PSA + LM?.It can be seen from Table 3 that this approach re-sults in a performance boost for both SEG and JWPtasks.
The POS tagging F-score of our approachwas comparable to but still less than the model of(Jiang et al 2008).
They achieved the best score byfirst separately training multiple word-, character-,and POS n-gram based models, and then integrat-ing them by cascading method.
In comparison, ournetworks achieve the performance by automaticallydiscovering useful features by itself and avoiding thetask-specific engineering.Table 4 compares the decoding speeds on the testdata from CTB-3 for our system and for two CRFs-based word segmentation systems.
Regardless ofthe differences in implementation, the neural net-works clearly run considerably faster than the sys-tems based on the CRFs model.
They also requiremuch more memory than our neural networks.System Number of parameters Time (s)(Tsai et al 2006) 3.1?
106 1669(Zhao et al 2006) 3.8?
106 2382Neural network 4.7?
105 138Table 4: Comparison of computational cost.4 Related WorkWord segmentation has been pursued with consid-erable efforts in the Chinese NLP community, andstatistical approaches are clearly dominant in thelast decade.
A popular statistical approach is thecharacter-based tagging solution that treats wordsegmentation as a sequence tagging problem, as-signing labels to the characters indicating whether acharacter locates at the beginning of, inside, or at theend of a word.
The character-based tagging solutionwas first proposed in (Xue, 2003).
This work causedquite a number of character position tagging basedCWS studies because known and unknown wordscan be treated in the same way.
Peng, Feng and Mc-Callum (2004) first introduced a linear-chain CRFsmodel to the character tagging based word segmen-tation.
Zhang and Clark (2007) proposed a word-based CWS approach using a discriminative percep-tron learning algorithm, which allows word-level in-formation to be added as features.Recent years have seen a rise of joint word seg-mentation and POS tagging approach that improves654the accuracies of both tasks and does not suffer fromthe error propagation.
Ng and Lou (2004) performsuch joint task in a labeling fashion by expandingboundary labels to include POS tags.
Zhang andClark (2008) proposed a linear model for the samejoint task, which overcomed the disadvantage of(Ng and Lou, 2004), in which it was unable to incor-porate ?whole word + POS tag?
features.
Sun (2011)described a sub-word model using stacked learn-ing technique for the joint task, which explored thecomplementary strength of different character- andword-based segmenters with different views.The majority of the state-of-the-art systems ad-dress their tasks by applying linear statistical mod-els to ad-hoc features.
The researchers first chosetask-specific features which are then fed to a classi-fication algorithm.
The selected features may varygreatly because they are usually chosen in a empir-ical process, mainly based first on linguistic intu-ition, and then trial and error.
It seems reasonableto assume that the number and effectiveness of fea-tures constitutes a major factor in the performanceof the various systems, and might even more impor-tant than the particular statistical models they used.In comparison, we try to avoid task-specific featureengineering, and use the neural network to learn sev-eral layers of feature extraction from the inputs.
Tothe best of our knowledge, this study is among thefirst ones to perform Chinese word segmentation andPOS tagging by deep learning.It was reported that supervised and unsupervisedapproaches can be integrated to improve on the over-all performance of word segmentation by combin-ing the strengths of both.
Zhao and Kit (2011) ex-plored the feasibility of enhancing supervised seg-mentation by informing the supervised learner ofgoodness scores obtained from large unlabeled cor-pus.
Sun and Xu (2011) investigated how to improveon the accuracy of supervised word segmentation byleveraging the statistics-based features derived fromlarge unlabeled in-domain corpus and the documentto be segmented.
The basic idea of these integrationsolutions is to incorporate a set of statistics-basedmeasures into a CRFs model after these measuresare derived from unlabeled data and discretized intofeature values.
In comparison, we use large unla-beled data to obtain the character embeddings withmore syntactic and semantic information.Several works have investigated how to use deeplearning for NLP applications (Bengio et al 2003;Collobert et al 2011; Collobert, 2011; Socher etal., 2011).
In most cases, words are fed to the neuralnetworks as inputs, and the lookup tables map eachword to a vector representation.
Our network is dif-ferent in that the inputs to the network are charac-ters, more raw units than words.
In many Asianlanguages, such as Chinese and Japanese, they arewritten without using whitespace to delimit words.For these languages, the character becomes a morenatural form of input.
Furthermore, a perceptron-style algorithm for tagging is proposed for trainingthe networks.5 ConclusionWe have described a perceptron-style algorithm fortraining the neural networks, which is much easier tobe implemented, and has speed advantage over themaximum-likelihood scheme, while the loss in per-formance is negligible.
The neural networks trainedwith PSA have been applied to Chinese word seg-mentation and POS tagging tasks, and the networksachieved close to state-of-the-art performance by us-ing the character representations learned from largeunlabeled corpus.Although we focus on the question of how far wecan go for Chinese word segmentation and POS tag-ging without using the extra task-specific features inthis study, there are at least three ways to further im-prove the performance of the networks, which areworthy to be explored in the future: (1) introducespecific linguistic features (e.g.
gazetteer features)that are helpful for the tasks; (2) incorporate somecommon techniques, such as cascading, voting, andensemble; and (3) use the special network architec-ture tailored for the tasks of interest.AcknowledgmentsThe authors would like to thank the anonymous re-viewers for their valuable comments.
The workwas supported by a grant from the National Nat-ural Science Foundation of China (No.
60903078),a grant from Shanghai Leading Academic Disci-pline Project (No.
B114), a grant from Shang-hai Municipal Natural Science Foundation (No.13ZR1403800), and a grant from FDUROP.655ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic langu-gage model.
Journal of Machine Learning Research,3: 1137?1155.Le?on Bottou.
1991.
Stochastic gradient learning inneural networks.
In Proceedings of the Neuro-N?
?mes.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of the In-ternational Conference on Empirical Methods in Nat-ural Language Processing (EMNLP?02).Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In Proceedings of the 14th In-ternational Conference on Artificial Intelligence andStatistics (AISTATS?11).Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12: 2493?2537.Haodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004.
Accessor variety criteria for Chineseword extraction.
Computational Linguistics, 30(1):75?93.Yuanyong Feng, Sun Le, and Yuanhua Lv.
2006.
Chi-nese word segmentation and name entity recognitionbased on conditional random fields models.
In Pro-ceedings of the Fifth SIGHAN Workshop on ChineseLanguage Processing (SIGHAN?06).Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu.
2008.A cascaded linear model for joint Chinese word seg-mentation and part-of-speech tagging.
In Proceed-ings of the 46th Annual Meeting of the Association forComputational Linguistics (ACL?08).Zhihui Jin, and Kumiko Tanaka-Ishii.
2006.
Unsuper-vised segmentation of Chinese text by use of branch-ing entropy.
In Proceedings of the International Con-ference on Computational Linguistics and the AnnualMeeting of the Association for Computational Linguis-tics (COLING/ACL?06).Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hy-brid model for joint Chinese word segmentation andpos tagging.
In Proceedings of the 47th Annual Meet-ing of the Association for Computational Linguistics(ACL?09).John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on Machinelearning (ICML?01).Gina A. Levow.
2006.
The third international Chi-nese language processing bakeoff: Word segmentationand named entity recognition.
In Proceedings of theSIGHAN Workshop on Chinese Language Processing(SIGHAN?06).Hwee T. Ng and Jin K. Lou.
2004.
Chinese part-of-speech tagging: one-at-atime or all-at-once?
word-based or character-based?
In Proceedings of the Inter-national Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?04).Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proceedings of the20th International Conference on Computational Lin-guistics (COLING?04).David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning internal representations bybackpropagating errors.
Parallel Distributed Process-ing: Explorations in the Microstructure of Cognition,1: 318?362.
MIT Press.Richard Socher, Cliff C-Y.
Lin, Andrew Y. Ng, andChristopher D. Manning 2011.
Parsing NaturalScenes and Natural Language with Recursive NeuralNetworks.
In Proceedings of the International Con-ference on Machine learning (ICML?11).Weiwei Sun.
2011.
A stacked sub-word model for jointChinese word segmentation and part-of-speech tag-ging.
In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics (ACL?11).Weiwei Sun and Jia Xu.
2011.
Enhancing Chinese wordsegmentation using unlabeled data.
In Proceedings ofthe International Conference on Empirical Methods inNatural Language Processing (EMNLP?11).Richard T.-H. Tsai, Hsieh C. Hung, Chenglung Sung,Hongjie Dai, and Wenlian Hsu.
2006.
On closedtask of Chinese word segmentation: An improved CRFmodel coupled with character clustering and automat-ically generated template matching.
In Proceedingsof the Fifth SIGHAN Workshop on Chinese LanguageProcessing (SIGHAN?06).Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, andXihong Wu.
2006.
Chinese word segmentation withmaximum entropy and n-gram language model.
InProceedings of the Fifth SIGHAN Workshop on Chi-nese Language Processing (SIGHAN?06).Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1): 29?48.Min Zhang, GuoDong Zhou, LingPeng Yang, andDongHong Ji.
2006.
Chinese word segmentationand named entity recognition based on a context-dependent mutual information independence Model.In Proceedings of the Fifth SIGHAN Workshop on Chi-nese Language Processing (SIGHAN?06).656Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-base perceptron algorithm.
In Pro-ceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL?07).Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging using a single perceptron.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL?08).Hai Zhao, Chang N. Huang, and Mu Li.
2006.
An im-proved Chinese word segmentation system with con-ditional random field.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Processing(SIGHAN?06).Hai Zhao and Chunyu Kit.
2011.
Integrating unsu-pervised and supervised word segmentation: The roleof goodness measures.
Information Sciences, 181(1):163?183.Muhua Zhu, Yilin Wang, Zhenxing Wang, HuizhenWang, and Jingbo Zhu.
2006.
Designing special post-processing rules for SVM-based Chinese word seg-mentation.
In Proceedings of the Fifth SIGHAN Work-shop on Chinese Language Processing (SIGHAN?06).657
