Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478?486,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsTreebank Grammar Techniques for Non-Projective Dependency ParsingMarco KuhlmannUppsala UniversityUppsala, Swedenmarco.kuhlmann@lingfil.uu.seGiorgio SattaUniversity of PaduaPadova, Italysatta@dei.unipd.itAbstractAn open problem in dependency parsingis the accurate and efficient treatment ofnon-projective structures.
We propose toattack this problem using chart-parsingalgorithms developed for mildly context-sensitive grammar formalisms.
In this pa-per, we provide two key tools for this ap-proach.
First, we show how to reduce non-projective dependency parsing to parsingwith Linear Context-Free Rewriting Sys-tems (LCFRS), by presenting a techniquefor extracting LCFRS from dependencytreebanks.
For efficient parsing, the ex-tracted grammars need to be transformedin order to minimize the number of nonter-minal symbols per production.
Our secondcontribution is an algorithm that computesthis transformation for a large, empiricallyrelevant class of grammars.1 IntroductionDependency parsing is the task of predicting themost probable dependency structure for a givensentence.
One of the key choices in dependencyparsing is about the class of candidate structuresfor this prediction.
Many parsers are confined toprojective structures, in which the yield of a syn-tactic head is required to be continuous.
A majorbenefit of this choice is computational efficiency:an exhaustive search over all projective structurescan be done in cubic, greedy parsing in linear time(Eisner, 1996; Nivre, 2003).
A major drawback ofthe restriction to projective dependency structuresis a potential loss in accuracy.
For example, around23% of the analyses in the Prague DependencyTreebank of Czech (Hajic?
et al, 2001) are non-projective, and for German and Dutch treebanks,the proportion of non-projective structures is evenhigher (Havelka, 2007).The problem of non-projective dependency pars-ing under the joint requirement of accuracy andefficiency has only recently been addressed in theliterature.
Some authors propose to solve it by tech-niques for recovering non-projectivity from the out-put of a projective parser in a post-processing step(Hall and Nov?k, 2005; Nivre and Nilsson, 2005),others extend projective parsers by heuristics thatallow at least certain non-projective constructionsto be parsed (Attardi, 2006; Nivre, 2007).
McDon-ald et al (2005) formulate dependency parsing asthe search for the most probable spanning tree overthe full set of all possible dependencies.
However,this approach is limited to probability models withstrong independence assumptions.
Exhaustive non-projective dependency parsing with more powerfulmodels is intractable (McDonald and Satta, 2007),and one has to resort to approximation algorithms(McDonald and Pereira, 2006).In this paper, we propose to attack non-project-ive dependency parsing in a principled way, us-ing polynomial chart-parsing algorithms developedfor mildly context-sensitive grammar formalisms.This proposal is motivated by the observation thatmost dependency structures required for the ana-lysis of natural language are very nearly projective,differing only minimally from the best projectiveapproximation (Kuhlmann and Nivre, 2006), andby the close link between such ?mildly non-project-ive?
dependency structures on the one hand, andgrammar formalisms with mildly context-sensitivegenerative capacity on the other (Kuhlmann andM?hl, 2007).
Furthermore, as pointed out by Mc-Donald and Satta (2007), chart-parsing algorithmsare amenable to augmentation by non-local inform-ation such as arity constraints and Markovization,and therefore should allow for more predictive stat-istical models than those used by current systemsfor non-projective dependency parsing.
Hence,mildly non-projective dependency parsing prom-ises to be both efficient and accurate.478Contributions In this paper, we contribute twokey tools for making the mildly context-sensitiveapproach to accurate and efficient non-projectivedependency parsing work.First, we extend the standard technique for ex-tracting context-free grammars from phrase-struc-ture treebanks (Charniak, 1996) to mildly con-text-sensitive grammars and dependency treebanks.More specifically, we show how to extract, froma given dependency treebank, a lexicalized LinearContext-Free Rewriting System (LCFRS) whosederivations capture the dependency analyses in thetreebank in the same way as the derivations ofa context-free treebank grammar capture phrase-structure analyses.
Our technique works for arbit-rary, even non-projective dependency treebanks,and essentially reduces non-projective dependencyto parsing with LCFRS.
This problem can be solvedusing standard chart-parsing techniques.Our extraction technique yields a grammarwhose parsing complexity is polynomial in thelength of the sentence, but exponential in both ameasure of the non-projectivity of the treebank andthe maximal number of dependents per word, re-flected as the rank of the extracted LCFRS.
Whilethe number of highly non-projective dependencystructures is negligible for practical applications(Kuhlmann and Nivre, 2006), the rank cannot eas-ily be bounded.
Therefore, we present an algorithmthat transforms the extracted grammar into a nor-mal form that has rank 2, and thus can be parsedmore efficiently.
This contribution is importanteven independently of the extraction procedure:While it is known that a rank-2 normal form ofLCFRS does not exist in the general case (Rambowand Satta, 1999), our algorithm succeeds for a largeand empirically relevant class of grammars.2 PreliminariesWe start by introducing dependency trees andLinear Context-Free Rewriting Systems (LCFRS).Throughout the paper, for positive integers i and j ,we write ?i; j ?
for the interval f k j i  k  j g,and use ?n?
as a shorthand for ?1; n?.2.1 Dependency TreesDependency parsing is the task to assign depend-ency structures to a given sentence w. For thepurposes of this paper, dependency structures areedge-labelled trees.
More formally, let w be a sen-tence, understood as a sequence of tokens oversome given alphabet T , and let L be an alphabetof edge labels.
A dependency tree for w is a con-structD D .w;E; /, where E forms a rooted tree(in the standard graph-theoretic sense) on the set?jwj?, and  is a total function that assigns everyedge in E a label in L. Each node of D representsa (position of a) token in w.Example 1 Figure 2 shows a dependency tree forthe sentence A hearing is scheduled on the issuetoday, which consists of 8 tokens and the edgesf .2; 1/; .2; 5/; .3; 2/; .3; 4/; .4; 8/; .5; 7/; .7; 6/ g.The edges are labelled with syntactic functionssuch as sbj for ?subject?.
The root node is markedby a dotted line.
Let u be a node of a dependency treeD.
A node u0is a descendant of u, if there is a (possibly empty)path from u to u0.
A block of u is a maximalinterval of descendants of u.
The number of blocksof u is called the block-degree of u.
The block-degree of a dependency tree is the maximum amongthe block-degrees of its nodes.
A dependency treeis projective, if its block-degree is 1.Example 2 The tree shown in Figure 2 is notprojective: both node 2 (hearing) and node 4(scheduled) have block-degree 2.
Their blocks aref 2 g; f 5; 6; 7 g and f 4 g; f 8 g, respectively.2.2 LCFRSLinear Context-Free Rewriting Systems (LCFRS)have been introduced as a generalization of sev-eral mildly context-sensitive grammar formalisms.Here we use the standard definition of LCFRS(Vijay-Shanker et al, 1987) and only fix our nota-tion; for a more thorough discussion of this formal-ism, we refer to the literature.Let G be an LCFRS.
Recall that each nonter-minal symbol A of G comes with a positive integercalled the fan-out of A, and that a production pof G has the formA!
g.A1; : : : ; Ar/ I g.Ex1; : : : ; Exr/ D E?
;whereA;A1; : : : ; Ar are nonterminals with fan-outf; f1; : : : ; fr , respectively, g is a function symbol,and the equation to the right of the semicolon spe-cifies the semantics of g. For each i 2 ?r?, Exi isan fi -tuple of variables, and E?
D h?1; : : : ; f?
i is atuple of strings over the variables on the left-handside of the equation and the alphabet of terminalsymbols in which each variable appears exactlyonce.
The production p is said to have rank r ,fan-out f , and length j?1jC   C j f?
jC .f  1/.4793 Grammar ExtractionWe now explain how to extract an LCFRS from adependency treebank, in very much the same wayas a context-free grammar can be extracted from aphrase-structure treebank (Charniak, 1996).3.1 Dependency Treebank GrammarsA simple way to induce a context-free grammarfrom a phrase-structure treebank is to read off theproductions of the grammar from the trees.
We willspecify a procedure for extracting, from a givendependency treebank, a lexicalized LCFRS G thatis adequate in the sense that for every analysis Dof a sentencew in the treebank, there is a derivationtree of G that is isomorphic to D, meaning thatit becomes equal to D after a suitable renamingand relabelling of nodes, and has w as its derivedstring.
Here, a derivation tree of an LCFRS G isan ordered tree such that each node u is labelledwith a production p of G, the number of childrenof u equals the rank r of p, and for each i 2 ?r?,the i th child of u is labelled with a production thathas as its left-hand side the i th nonterminal on theright-hand side of p.The basic idea behind our extraction procedureis that, in order to represent the compositional struc-ture of a possibly non-projective dependency tree,one needs to represent the decomposition and relat-ive order not of subtrees, but of blocks of subtrees(Kuhlmann and M?hl, 2007).
We introduce someterminology.
A component of a node u in a de-pendency tree is either a block B of some child u0of u, or the singleton interval that contains u; thisinterval will represent the position in the string thatis occupied by the lexical item corresponding to u.We say that u0 contributes B , and that u contrib-utes ?u; u?
to u.
Notice that the number of com-ponents that u0 contributes to its parent u equalsthe block-degree of u0.
Our goal is to constructfor u a production of an LCFRS that specifies howeach block of u decomposes into components, andhow these components are ordered relative to oneanother.
These productions will make an adequateLCFRS, in the sense defined above.3.2 Annotating the ComponentsThe core of our extraction procedure is an efficientalgorithm that annotates each node u of a given de-pendency tree with the list of its components, sor-ted by their left endpoints.
It is helpful to think ofthis algorithm as of two independent parts, one that1: Function Annotate-L.D/2: for each u of D, from left to right do3: if u is the first node of D then4: b WD the root node of D5: else6: b WD the lca of u and its predecessor7: for each u0 on the path b   u do8: left?u0?
WD left?u0?
 uFigure 1: Annotation with componentsannotates each node u with the list of the left end-points of its components (Annotate-L) and onethat annotates the corresponding right endpoints(Annotate-R).
The list of components can thenbe obtained by zipping the two lists of endpointstogether in linear time.Figure 1 shows pseudocode for Annotate-L;the pseudocode for Annotate-R is symmetric.
Wedo a single left-to-right sweep over the nodes of theinput treeD.
In each step, we annotate all nodes u0that have the current node u as the left endpoint ofone of their components.
Since the sweep is fromleft to right, this will get us the left endpoints of u0in the desired order.
The nodes that we annotate arethe nodes u0 on the path between u and the leastcommon ancestor (lca) b of u and its predecessor,or the path from the root node to u, in case that uis the leftmost node of D.Example 3 For the dependency tree in Figure 2,Annotate-L constructs the following lists left?u?of left endpoints, for u D 1; : : : ; 8:1; 1  2  5; 1  3  4  5  8; 4  8; 5  6; 6; 6  7; 8The following Lemma establishes the correctnessof the algorithm:Lemma 1 Let D be a dependency tree, and let uand u0 be nodes of D. Let b be the least commonancestor of u and its predecessor, or the root nodein case that u is the leftmost node of D. Then u isthe left endpoint of a component of u0 if and onlyif u0 lies on the path from b to u.
Proof It is clear that u0 must be an ancestor of u.If u is the leftmost node of D, then u is the leftendpoint of the leftmost component of all of itsancestors.
Now suppose that u is not the leftmostnode of D, and let Ou be the predecessor of u. Dis-tinguish three cases: If u0 is not an ancestor of Ou,then Ou does not belong to any component of u0;therefore, u is the left endpoint of a component480of u0.
If u0 is an ancestor of Ou but u0 ?
b, then Ouand u belong to the same component of u0; there-fore, u is not the left endpoint of this component.Finally, if u0 D b, then Ou and u belong to differentcomponents of u0; therefore, u is the left endpointof the component it belongs to.
We now turn to an analysis of the runtime of thealgorithm.
Let n be the number of componentsof D. It is not hard to imagine an algorithm thatperforms the annotation task in time O.n logn/:such an algorithm could construct the componentsfor a given node u by essentially merging the list ofcomponents of the children of u into a new sortedlist.
In contrast, our algorithm takes time O.n/.The crucial part of the analysis is the assignmentin line 6, which computes the least common an-cestor of u and its predecessor.
Using markers forthe path from the root node to u, it is straightfor-ward to implement this assignment in time O.jj/,where  is the path b   u.
Now notice that, by ourcorrectness argument, line 8 of the algorithm is ex-ecuted exactly n times.
Therefore, the sum over thelengths of all the paths  , and hence the amortizedtime of computing all the least common ancest-ors in line 6, is O.n/.
This runtime complexity isoptimal for the task we are solving.3.3 Extraction ProcedureWe now describe how to extend the annotation al-gorithm into a procedure that extracts an LCFRSfrom a given dependency tree D. The basic idea isto transform the list of components of each node uof D into a production p. This transformation willonly rename and relabel nodes, and therefore yieldan adequate derivation tree.
For the constructionof the production, we actually need an extendedversion of the annotation algorithm, in which eachcomponent is annotated with the node that contrib-uted it.
This extension is straightforward, and doesnot affect the linear runtime complexity.Let D be a dependency tree for a sentence w.Consider a single node u of D, and assume that uhas r children, and that the block-degree of u is f .We construct for u a production p with rank rand fan-out f .
For convenience, let us order thechildren of u, say by their leftmost descendants,and let us write ui for the i th child of u accordingto this order, and fi for the block-degree of ui ,i 2 ?r?.
The production p has the formL!
g.L1; : : : ; Lr/ I g.Ex1; : : : ; Exr/ D E?
;where L is the label of the incoming edge of u(or the special label root in case that u is the rootnode of D) and for each i 2 ?r?
: Li is the label ofthe incoming edge of ui ; Exi is a fi -tuple of vari-ables of the form xi;j , where j 2 ?fi ?
; and E?
isan f -tuple that is constructed in a single left-to-right sweep over the list of components computedfor u as follows.
Let k 2 ?fi ?
be a pointer to a cur-rent segment of E?
; initially, k D 1.
If the currentcomponent is not adjacent (as an interval) to theprevious component, we increase k by one.
If thecurrent component is contributed by the child ui ,i 2 ?r?, we add the variable xi;j to ?k , where jis the number of times we have seen a componentcontributed by ui during the sweep.
Notice thatj 2 ?fi ?.
If the current component is the (unique)component contributed by u, we add the token cor-responding to u to ?k .
In this way, we obtain acomplete specification of how the blocks of u (rep-resented by the segments of the tuple E?)
decomposeinto the components of u, and of the relative orderof the components.
As an example, Figure 2 showsthe productions extracted from the tree above.3.4 Parsing the Extracted GrammarOnce we have extracted the grammar for a depend-ency treebank, we can apply any parsing algorithmfor LCFRS to non-projective dependency parsing.The generic chart-parsing algorithm for LCFRSruns in timeO.jP j  jwjf .rC1//, where P is the setof productions of the input grammar G, w is the in-put string, r is the maximal rank, and f is the max-imal fan-out of a production inG (Seki et al, 1991).For a grammar G extracted by our technique, thenumber f equals the maximal block-degree pernode.
Hence, without any further modification, weobtain a parsing algorithm that is polynomial in thelength of the sentence, but exponential in both theblock-degree and the rank.
This is clearly unaccept-able in practical systems.
The relative frequencyof analyses with a block-degree  2 is almost neg-ligible (Havelka, 2007); the bigger obstacle in ap-plying the treebank grammar is the rank of the res-ulting LCFRS.
Therefore, in the remainder of thepaper, we present an algorithm that can transformthe productions of the input grammar G into anequivalent set of productions with rank at most 2,while preserving the fan-out.
This transformation,if it succeeds, yields a parsing algorithm that runsin time O.jP j  r  jwj3f /.4811A 2hearing 3is 4scheduled 5on 6the 7issue 8todaynmod sbjroot nodevcppnmodnptmpnmod!
g1 g1 D hAisbj!
g2.nmod; pp/ g2.hx1;1i; hx2;1i/ D hx1;1 hearing; x2;1iroot!
g3.sbj; vc/ g3.hx1;1; x1;2i; hx2;1; x2;2i/ D hx1;1 is x2;1 x1;2 x2;2ivc!
g4.tmp/ g4.hx1;1i/ D hscheduled; x1;1ipp!
g5.np/ g5.hx1;1i/ D hon x1;1inmod!
g6 g6 D htheinp!
g7.nmod/ g7.hx1;1i/ D hx1;1 issueitmp!
g8 g8 D htodayiFigure 2: A dependency tree, and the LCFRS extracted for it4 AdjacencyIn this section we discuss a method for factorizingan LCFRS into productions of rank 2.
Before start-ing, we get rid of the ?easy?
cases.
A production pis connected if any two strings ?i , j?
in p?s defini-tion share at least one variable referring to the samenonterminal.
It is not difficult to see that, when p isnot connected, we can always split it into new pro-ductions of lower rank.
Therefore, throughout thissection we assume that LCFRS only have connec-ted productions.
We can split p into its connectedcomponents using standard methods for finding thestrongly connected components of an undirectedgraph.
This can be implemented in time O.r  f /,where r and f are the rank and the fan-out of p,respectively.4.1 Adjacency GraphsLet p be a production with length n and fan-out f ,associated with function a g. The set of positionsof p is the set ?n?.
Informally, each position rep-resents a variable or a lexical element in one of thecomponents of the definition of g, or else a ?gap?between two of these components.
(Recall that nalso accounts for the f   1 gaps in the body of g.)Example 4 The set of positions of the productionfor hearing in Figure 2 is ?4?
: 1 for variable x1, 2for hearing, 3 for the gap, and 4 for y1.
Let i1; j1; i2; j2 2 ?n?.
An interval ?i1; j1?
is ad-jacent to an interval ?i2; j2?
if either j1 D i2   1(left-adjacent) or i1 D j2 C 1 (right-adjacent).
Amulti-interval, or m-interval for short, is a set v ofpairwise disjoint intervals such that no interval in vis adjacent to any other interval in v. The fan-outof v, written f .v/, is defined as jvj.We use m-intervals to represent the nonterminalsand the lexical element heading p. The i th nonter-minal on the right-hand side of p is represented bythe m-interval obtained by collecting all the pos-itions of p that represent a variable from the i thargument of g. The head of p is represented by them-interval containing the associated position.
Notethat all these m-intervals are pairwise disjoint.Example 5 Consider the production for is inFigure 2.
The set of positions is ?5?.
Thefirst nonterminal is represented by the m-inter-val f ?1; 1?
; ?4; 4?
g, the second nonterminal byf ?3; 3?
; ?5; 5?
g, and the lexical head by f ?2; 2?
g. For disjoint m-intervals v1; v2, we say that v1 isadjacent to v2, denoted by v1 !
v2, if for everyinterval I1 2 v1, there is an interval I2 2 v2 suchthat I1 is adjacent to I2.
Adjacency is not symmet-ric: if v1 D f ?1; 1?
; ?4; 4?
g and v2 D f ?2; 2?
g, thenv2 !
v1, but not vice versa.Let V be some collection of pairwise disjointm-intervals representing p as above.
The ad-jacency graph associated with p is the graphG D .V;!G/ whose vertices are the m-intervalsin V , and whose edges!G are defined by restrict-ing the adjacency relation!
to the set V .For m-intervals v1; v2 2 V , the merger of v1and v2, denoted by v1 ?
v2, is the (uniquelydetermined) m-interval whose span is the unionof the spans of v1 and v2.
As an example, ifv1 D f ?1; 1?
; ?3; 3?
g and v2 D f ?2; 2?
g, thenv1 ?
v2 D f ?1; 3?
g. Notice that the way in whichwe defined m-intervals ensures that a merging oper-ation collapses all adjacent intervals.
The proof ofthe following lemma is straightforward and omittedfor space reasons:4821: Function Factorize.G D .V;!G//2: R WD ;;3: while!G ?
; do4: choose .v1; v2/ 2 !G ;5: R WD R [ f .v1; v2/ g;6: V WD V   f v1; v2 g [ f v1 ?
v2 g;7: !G WD f .v; v0/ j v; v0 2 V; v !
v0 g;8: if jV j D 1 then9: output R and accept;10: else11: reject;Figure 3: Factorization algorithmLemma 2 If v1 !
v2, then f .v1 ?
v2/  f .v2/.4.2 The Adjacency AlgorithmLet G D .V;!G/ be some adjacency graph, andlet v1!G v2.
We can derive a new adjacencygraph from G by merging v1 and v2.
The resultinggraph G0 has vertices V 0 D V  f v1; v2 g[ f v1?v2 g and set of edges!G0 obtained by restrictingthe adjacency relation !
to V 0.
We denote thederive relation as G ).v1;v2/ G0.Informally, ifG represents some LCFRS produc-tion p and v1; v2 represent nonterminals A1; A2,thenG0 represents a production p0 obtained from pby replacing A1; A2 with a fresh nonterminal A.
Anew production p00 can also be constructed, expand-ing A into A1; A2, so that p0; p00 together will beequivalent to p. Furthermore, p0 has a rank smallerthan the rank of p and, from Lemma 2, A does notincrease the overall fan-out of the grammar.In order to simplify the notation, we adopt thefollowing convention.
Let G ).v1;v2/ G0 andlet v!G v1, v ?
v2.
If v!G0 v1 ?
v2, thenedges .v; v1/ and .v; v1 ?
v2/ will be identified,and we say that G0 inherits .v; v1 ?
v2/ from G.If v 6!G0 v1?v2, then we say that .v; v1/ does notsurvive the derive step.
This convention is used forall edges incident upon v1 or v2.Our factorization algorithm is reported in Fig-ure 3.
We start from an adjacency graph repres-enting some LCFRS production that needs to befactorized.
We arbitrarily choose an edge e of thegraph, and push it into a set R, in order to keepa record of the candidate factorization.
We thenmerge the two m-intervals incident to e, and werecompute the adjacency relation for the new setof vertices.
We iterate until the resulting graph hasan empty edge set.
If the final graph has one onevertex, then we have managed to factorize our pro-duction into a set of productions with rank at mosttwo that can be computed from R.Example 6 Let V D f v1; v2; v3 g with v1 Df ?4; 4?
g, v2 D f ?1; 1?
; ?3; 3?
g, and v3 Df ?2; 2?
; ?5; 5?
g. Then !G D f .v1; v2/ g. Aftermerging v1; v2 we have a new graph G with V Df v1 ?
v2; v3 g and!G D f .v1 ?
v2; v3/ g. Wefinally merge v1 ?
v2; v3 resulting in a new graphG with V D f v1 ?
v2 ?
v3 g and!G D ;.
Wethen accept and stop.
4.3 Mathematical PropertiesWe have already argued that, if the algorithm ac-cepts, then a binary factorization that does notincrease the fan-out of the grammar can be builtfrom R. We still need to prove that the algorithmanswers consistently on a given input, despite ofpossibly different choices of edges at line 4.
We dothis through several intermediate results.A derivation for an adjacency graph G is a se-quence of edges d D he1; : : : ; eni, n  1, suchthat G D G0 and Gi 1 )ei Gi for every i with1  i  n. For short, we write G0 )d Gn.Two derivations for G are competing if one is apermutation of the other.Lemma 3 If G )d1 G1 and G )d2 G2 with d1and d2 competing derivations, then G1 D G2.Proof We claim that the statement of the lemmaholds for jd1j D 2.
To see this, let G )e1G01 )e2 G1 and G )e2 G02 )e1 G2 be validderivations.
We observe that G1 and G2 have thesame set of vertices.
Since the edges of G1 and G2are defined by restricting the adjacency relation totheir set of vertices, our claim immediately follows.The statement of the lemma then follows fromthe above claim and from the fact that we can al-ways obtain the sequence d2 starting from d1 byrepeatedly switching consecutive edges.
We now consider derivations for the same adja-cency graph that are not competing, and show thatthey always lead to isomorphic adjacency graphs.Two graphs are isomorphic if they become equalafter some suitable renaming of the vertices.Lemma 4 The out-degree of G is bounded by 2.Proof Assume v!G v1 and v!G v2, with v1 ?v2, and let I 2 v. I must be adjacent to some in-terval I1 2 v1.
Without loss of generality, assumethat I is left-adjacent to I1.
I must also be adja-cent to some interval I2 2 v2.
Since v1 and v2483are disjoint, I must be right-adjacent to I2.
Thisimplies that I cannot be adjacent to an interval inany other m-interval v0 of G. A vertex v of G such that v!G v1 and v!G v2is called a bifurcation.Example 7 Assume v D f ?2; 2?
g, v1 Df ?3; 3?
; ?5; 5?
g, v2 D f ?1; 1?
g with v!G v1 andv!G v2.
The m-interval v?
v1 D f ?2; 3?
; ?5; 5?
gis no longer adjacent to v2.
The example above shows that, when choosing oneof the two outgoing edges in a bifurcation for mer-ging, the other edge might not survive.
Thus, sucha choice might lead to distinguishable derivationsthat are not competing (one derivation has an edgethat is not present in the other).
As we will see (inthe proof of Theorem 1), bifurcations are the onlycases in which edges might not survive a merging.Lemma 5 Let v be a bifurcation of G with outgo-ing edges e1; e2, and let G )e1 G1, G )e2 G2.Then G1 and G2 are isomorphic.Proof (Sketch) Assume e1 has the formv!G v1 and e2 has the form v!G v2.
Letalso VS be the set of vertices shared by G1 andG2.
We show that the statement holds under theisomorphism mapping v ?
v1 and v2 in G1 to v1and v ?
v2 in G2, respectively.When restricted to VS , the graphs G1 and G2are equal.
Let us then consider edges from G1 andG2 involving exactly one vertex in VS .
We showthat, for v0 2 VS , v0!G1 v ?
v1 if and only ifv0!G2 v1.
Consider an arbitrary interval I0 2 v0.If v0!G1 v?v1, then I0 must be adjacent to someinterval I1 2 v ?
v1.
If I1 2 v1 we are done.Otherwise, I1 must be the concatenation of twointervals I1v and I1v1 with I1v 2 v and I1v1 2v1.
Since v!G2 v2, I1v is also adjacent to someinterval in v2.
However, v0 and v2 are disjoint.Thus I 0 must be adjacent to I1v1 2 v1.
Conversely,if v0!G2 v1, then I0 must be adjacent to someinterval I1 2 v1.
Because v0 and v are disjoint, I 0must also be adjacent to some interval in v ?
v1.Using very similar arguments, we can concludethat G1 and G2 are isomorphic when restricted toedges with at most one vertex in VS .Finally, we need to consider edges from G1 andG2 that are not incident upon vertices in VS .
Weshow that v ?
v1!G1 v2 only if v1!G2 v ?
v2;a similar argument can be used to prove the con-verse.
Consider an arbitrary interval I1 2 v?v1.
Ifv ?
v1!G1 v2, then I1 must be adjacent to someinterval I2 2 v2.
If I1 2 v1 we are done.
Other-wise, I1 must be the concatenation of two adjacentintervals I1v and I1v1 with I1v 2 v and I1v1 2 v1.Since I1v is also adjacent to some interval I 02 2 v2(here I 02 might as well be I2), we conclude thatI1v1 2 v1 is adjacent to the concatenation of I1vand I 02, which is indeed an interval in v?
v2.
Notethat our case distinction is exhaustive.
We thusconclude that v1!G2 v ?
v2.A symmetrical argument can be used to showthat v2!G1 v ?
v1 if and only if v ?
v2!G2 v1,which concludes our proof.
Theorem 1 Let d1 and d2 be derivations for G,describing two different computations c1 and c2 ofthe algorithm of Figure 3 on input G. Computationc1 is accepting if and only if c2 is accepting.Proof First, we prove the claim that if e is not anedge outgoing from a bifurcation vertex, then in thederive relation G )e G0 all of the edges of G bute and its reverse are inherited by G0.
Let us writee in the form v1!G v2.
Obviously, any edge ofG not incident upon v1 or v2 will be inherited byG0.
If v!G v2 for some m-interval v ?
v1, thenevery interval I 2 v is adjacent to some intervalin v2.
Since v and v1 are disjoint, I will also beadjacent to some interval in v1?v2.
Thus we havev!G0 v1 ?
v2.
A similar argument shows thatv!G v1 implies v!G0 v1 ?
v2.If v2!G v for some v ?
v1, then every in-terval I 2 v2 is adjacent to some interval in v.From v1!G v2 we also have that each intervalI12 2 v1 ?
v2 is either an interval in v2 or elsethe concatenation of exactly two intervals I1 2 v1and I2 2 v2.
(The interval I2 cannot be adjacentto more than an interval in v1, because v2!G v).In both cases I12 is adjacent to some interval inv, and hence v1 ?
v2!G0 v. This concludes theproof of our claim.Let d1, d2 be as in the statement of the the-orem, with G )d1 G1 and G )d2 G2.
If d1and d2 are competing, then the theorem followsfrom Lemma 3.
Otherwise, assume that d1 and d2are not competing.
From our claim above, somebifurcation vertices must appear in these deriva-tions.
Let us reorder the edges in d1 in such a waythat edges outgoing from a bifurcation vertex areprocessed last and in some canonical order.
Theresulting derivation has the form dd 01, where d01involves the processing of all bifurcation vertices.We can also reorder edges in d2 to obtain dd 02,where d 02 involves the processing of all bifurcation484not context-free 102 687 100.00%not binarizable 24 0.02%not well-nested 622 0.61%Table 1: Properties of productions extracted fromthe CoNLL 2006 data (3 794 605 productions)vertices in exactly the same order as in d 01, but withpossibly different choices for the outgoing edges.Let G )d Gd )d 01 G01 and G )d Gd )d 02G02.
Derivations dd01 and d1 are competing.
Thus,by Lemma 3, we haveG01 D G1.
Similarly, we canconclude that G02 D G2.
Since bifurcation verticesin d 01 and in d02 are processed in the same canonicalorder, from repeated applications of Lemma 5 wehave that G01 and G02 are isomorphic.
We then con-clude that G1 and G2 are isomorphic as well.
Thestatement of the theorem follows immediately.
We now turn to a computational analysis of thealgorithm of Figure 3.
Let G be the representationof an LCFRS production p with rank r .
G hasr vertices and, following Lemma 4, O.r/ edges.Let v be an m-interval of G with fan-out fv.
Theincoming and outgoing edges for v can be detectedin time O.fv/ by inspecting the 2  fv endpoints ofv.
Thus we can compute G in time O.jpj/.The number of iterations of the while cycle in thealgorithm is bounded by r , since at each iterationone vertex of G is removed.
Consider now aniteration in which m-intervals v1 and v2 have beenchosen for merging, with v1!G v2.
(These m-intervals might be associated with nonterminalsin the right-hand side of p, or else might havebeen obtained as the result of previous mergingoperations.)
Again, we can compute the incomingand outgoing edges of v1?v2 in time proportionalto the number of endpoints of such an m-interval.By Lemma 2, this number is bounded by O.f /, fthe fan-out of the grammar.
We thus conclude thata run of the algorithm on G takes time O.r  f /.5 DiscussionWe have shown how to extract mildly context-sensitive grammars from dependency treebanks,and presented an efficient algorithm that attemptsto convert these grammars into an efficiently par-seable binary form.
Due to previous results (Ram-bow and Satta, 1999), we know that this is notalways possible.
However, our algorithm may faileven in cases where a binarization exists?our no-tion of adjacency is not strong enough to captureall binarizable cases.
This raises the question aboutthe practical relevance of our technique.In order to get at least a preliminary answer tothis question, we extracted LCFRS productionsfrom the data used in the 2006 CoNLL shared taskon data-driven dependency parsing (Buchholz andMarsi, 2006), and evaluated how large a portionof these productions could be binarized using ouralgorithm.
The results are given in Table 1.
Since itis easy to see that our algorithm always succeeds oncontext-free productions (productions where eachnonterminal has fan-out 1), we evaluated our al-gorithm on the 102 687 productions with a higherfan-out.
Out of these, only 24 (0.02%) could not bebinarized using our technique.
We take this numberas an indicator for the usefulness of our result.It is interesting to compare our approachwith techniques for well-nested dependency trees(Kuhlmann and Nivre, 2006).
Well-nestedness isa property that implies the binarizability of theextracted grammar; however, the classes of well-nested trees and those whose corresponding pro-ductions can be binarized using our algorithm areincomparable?in particular, there are well-nestedproductions that cannot be binarized in our frame-work.
Nevertheless, the coverage of our techniqueis actually higher than that of an approach thatrelies on well-nestedness, at least on the CoNLL2006 data (see again Table 1).We see our results as promising first steps in athorough exploration of the connections betweennon-projective and mildly context-sensitive pars-ing.
The obvious next step is the evaluation of ourtechnique in the context of an actual parser.As a final remark, we would like to point outthat an alternative technique for efficient non-pro-jective dependency parsing, developed by G?mezRodr?guez et al independently of this work, ispresented elsewhere in this volume.Acknowledgements We would like to thankRyan McDonald, Joakim Nivre, and the anonym-ous reviewers for useful comments on drafts of thispaper, and Carlos G?mez Rodr?guez and David J.Weir for making a preliminary version of their pa-per available to us.
The work of the first authorwas funded by the Swedish Research Council.
Thesecond author was partially supported by MIURunder project PRIN No.
2007TJNZRE_002.485ReferencesGiuseppe Attardi.
2006.
Experiments with a mul-tilanguage non-projective dependency parser.
InTenth Conference on Computational Natural Lan-guage Learning (CoNLL), pages 166?170, NewYork, USA.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency pars-ing.
In Tenth Conference on Computational NaturalLanguage Learning (CoNLL), pages 149?164, NewYork, USA.Eugene Charniak.
1996.
Tree-bank grammars.
In 13thNational Conference on Artificial Intelligence, pages1031?1036, Portland, Oregon, USA.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In 16th In-ternational Conference on Computational Linguist-ics (COLING), pages 340?345, Copenhagen, Den-mark.Carlos G?mez-Rodr?guez, David J. Weir, and JohnCarroll.
2009.
Parsing mildly non-projective de-pendency structures.
In Twelfth Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL), Athens, Greece.Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevov?,Eva Hajic?ov?, Petr Sgall, and Petr Pajas.
2001.Prague Dependency Treebank 1.0.
Linguistic DataConsortium, 2001T10.Keith Hall and V?clav Nov?k.
2005.
Corrective mod-elling for non-projective dependency grammar.
InNinth International Workshop on Parsing Technolo-gies (IWPT), pages 42?52, Vancouver, Canada.Jir??
Havelka.
2007.
Beyond projectivity: Multilin-gual evaluation of constraints and measures on non-projective structures.
In 45th Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 608?615, Prague, Czech Republic.Marco Kuhlmann and Mathias M?hl.
2007.
Mildlycontext-sensitive dependency languages.
In 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 160?167, Prague, CzechRepublic.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In 21st In-ternational Conference on Computational Linguist-ics and 44th Annual Meeting of the Association forComputational Linguistics (COLING-ACL), MainConference Poster Sessions, pages 507?514, Sydney,Australia.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Eleventh Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL), pages 81?88, Trento, Italy.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Tenth International Conference on Pars-ing Technologies (IWPT), pages 121?132, Prague,Czech Republic.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Human Lan-guage Technology Conference (HLT) and Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 523?530, Vancouver,Canada.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 99?106, Ann Arbor, USA.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Eighth InternationalWorkshop on Parsing Technologies (IWPT), pages149?160, Nancy, France.Joakim Nivre.
2007.
Incremental non-projectivedependency parsing.
In Human Language Tech-nologies: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL), pages 396?403, Rochester,NY, USA.Owen Rambow and Giorgio Satta.
1999.
Independentparallelism in finite copying parallel rewriting sys-tems.
Theoretical Computer Science, 223(1?2):87?120.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,and Tadao Kasami.
1991.
On Multiple Context-Free Grammars.
Theoretical Computer Science,88(2):191?229.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions pro-duced by various grammatical formalisms.
In 25thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), pages 104?111, Stanford,CA, USA.486
