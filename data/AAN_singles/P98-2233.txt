Feasibility Study for Ellipsis Resolution in Dialoguesby Machine-Learning TechniqueYAMAMOTO Kazuh ide  and SUMITA E i i ch i roATR Interpreting Telecommunications Research LaboratoriesE-mail: yamamot o?it I. atr.
co. jpAbst ractA method for resolving the ellipses that appearin Japanese dialogues i  proposed.
This methodresolves not only the subject ellipsis, but alsothose in object and other grammatical cases.
Inthis approach, a machine-learning algorithm isused to select he attributes necessary for a res-olution.
A decision tree is built, and used asthe actual ellipsis resolver.
The results of blindtests have shown that the proposed method wasable to provide a resolution accuracy of 91.7%for indirect objects, and 78.7% for subjects witha verb predicate.
By investigating the decisiontree we found that topic-dependent a tributesare necessary to obtain high performance res-olution, and that indispensable attributes varyaccording to the grammatical case.
The prob-lem of data size relative to decision-tree trainingis also discussed.1 In t roduct ionIn machine translation systems, it is necessaryto resolve ellipses when the source languagedoesn't express the subject or other grammat-ical cases and the target must express it.
Theproblem of ellipsis resolution is also troublesomein information extraction and other natural lan-guage processing fields.Several approaches have been proposed toresolve ellipses, which consist of endophoric(intrasentential or anaphoric) ellipses and ex-ophoric (or extrasentential) e lipses.
One of themajor approaches for endophoric ellipsis in the-oretical basis utilizes the centering theory.
How-ever, its application to complex sentences hasnot been established because most studies haveonly investigated its effectiveness with succes-sive simple sentences.Several studies of this problem have beenmade using the empirical approach.
Amongthem, Murata and Nagao (1997) proposed ascoring approach where each constraint is man-ually scored with an estimation of possibility,and the resolution is conducted by totaling thepoints each candidate receives.
On the otherhand, Nakaiwa and Shirai (1996) proposed aresolving algorithm for Japanese exophoric el-lipses of written texts, utilizing semantic andpragmatic onstraints.
They claimed that 100%of the ellipses with exophoric referents could beresolved, but the experiment was a closed testwith only a few samples.
These approaches al-ways require some effort to decide the scoringor the preference of provided constraints.Aone and Bennett (1995) applied a machine-learning technique to anaphora resolution inwritten texts.
They attempted endophoric ellip-sis resolution as a part of anaphora resolution,with approximately 40% recall and 74~ preci-sion at best from 200 test samples.
However,they were not concerned with exophoric ellipsis.In contrast, we applied a machine-learning ap-proach to ellipsis resolution (Yamamoto et al,1997).
In this previous work we resolved theagent case ellipses in dialogue, with a limitedtopic, and performed with approximately 90%accuracy.
This does not sufficiently determinethe effectiveness of the decision tree, and thefeasibility of this technique in resolving ellipsesby each surface case is also unclear.We propose a method to resolve the ellipsesthat appear in Japanese dialogues.
This methodresolves not only the subject ellipsis, but alsothe object and other grammatical cases.
In thisapproach, a machine-learning algorithm is usedto build a decision tree by selecting the neces-sary attributes, and the decision tree is used asthe actual ellipsis resoh'er.Another purpose of this paper is to discusshow effective the machine-learning approach is1428in the problem of ellipsis resolution.
In the fol-lowing sections, we discuss topic-dependency indecision trees and compare the resolution effec-tiveness of each grammatical case.
The problemof data size relative to the decision-tree trainingis also discussed.In this paper, we assume that the detectionof ellipses is performed by another module, suchas a parser.
We only considered ellipses that arecommonly and dearly identified.2 When to  Reso lve  E l l ips is  in MT ?As described above, our major application forellipsis resolution is in machine translation.
Inan MT process, there can be several approachesabout the timing of ellipsis resolution: whenanalyzing the source language, when generat-ing the target language, or at the same time astranslating process.
Among these candidates,most of the previous works with Japanese chosethe source-language approach.
For instance,Nakaiwa and Shirai (1996) attempted to re-solve Japanese ellipsis in the source languageanalysis of J-to-E MT, despite utilizing target-dependent resolution candidates.We originally thought hat ellipsis resolutionin the MT was a generation problem, namelya target-driven problem which utilizes somehelp, if necessary, of source-language informa-tion.
This is because the problem is output-dependent and it relies on demands from atarget language.
In the J-to-Korean or J-to-Chinese MT, all or most of the ellipses thatmust be resolved in J-to-E are not necessary toresolve.However, we adopted source-language policyin this paper, with the necessity that we con-sider a multi-lingual MT system TDMT (Furuseet al; 1995), that deals with both J-to-E and J-to-German MT.
English and German grammarare not generally believed to be similar.3 E l l ips is  Reso lu t ion  by  Mach ineLearn ingSince a huge text corpus has become widelyavailable, the machine-learning approach hasbeen utilized for some problems in natural an-guage processing.
The most popular touchstonein this field is the verbal case frame or the trans-lation rules (Tanaka, 1994).
Machine-learningalgorithm has also been attempted to solve someTable 1: Tagged Ellipsis TypesTag Meaning<lsg><lpl>(2sg>(2pl)(g)(a)first person, singularfirst person, pluralsecond person, singularsecond person, pluralperson(s) ~n generalanaphoricdiscourse processing problems, for example, indiscourse segment boundaries or discourse cuewords (Walker and Moore, 1997).
This sec-tion describes a method to apply a decision-treelearning approach, which is one of the machine-learning approaches, to ellipsis resolution.3.1 Ell ipsis Tagg ingIn order to train and evaluate our ellipsis re-solver, we tagged some ellipsis types to a di-alogue corpus.
The ellipsis types used to tagthe corpus are shown in Table 1.
Each ellipsismarker is tagged at the predicate.
We made adistinction between first or second person andperson(s) in general.
Note that 'person(s) ingeneral' refers to either an unidentified or anunspecified person or persons.
In Far-Easternlanguages uch as Japanese, Korean, and Chi-nese, there is no grammatically obligatory casesuch as the subject in English.
It is thus neces-sary to distinguish such ellipses.We also made a tag '(a/' which means thementioned ellipsis is anaphoric; in case we needto refer back to the antecedent in the dialogue.In this paper we are not concerned with resolv-ing the antecedent that such ellipses refer to,because it is necessary to have another moduleto deal with the context for resolving such en-dophoric ellipses, and the main target of thispaper is the exophoric ellipses.3.2 Learn ing  MethodWe used the C~.5 algorithm by Quinlan (1993),which is a well-known automatic lassifier thatproduces a binary decision tree.
Although itmay be necessary to prune decision trees, nopruning is performed throughout his experi-ment, since we want to concentrate the dis-cussion on the feasibility of machine learning.As shown in the experiment by Aone and Ben-1429Table 2: Number of training attributesAttributes Num.Content words (predicate) 100Content words (case frame) 100Func.
words (case particle) 9Func.
words (conj.
particle) 21Func.
words (auxiliary verb) 132Func.
words (other) 4Exophoric information 1Total 367nett (1995), which attempted to discuss prun-ing effects on the decision tree, no more con-clusions are expected other than a trade-off be-tween recall and precision.
We leave the detailsof decision-tree l arning research to itself.3.3 Tra in ing  At t r ibutesThe training attributes that we prepared forJapanese ellipsis resolution are listed in Table2.
The training attributes in the table are clas-sified into the following three groups:?
Exophoric information:Speaker's ocial role.?
Topic-dependent i formation:Predicates and their semantic ategories.?
Topic-independent i formation:Functional words which express tense,modality, etc.There is one approach that only uses topic-independent information to resolve ellipsesthat appear in dialogues.
However, we tookthe position that both topic-dependent and -independent information should have differentknowledge.
Thus, approaches utilizing onlytopic-independent knowledge must have a per-formance limit for developing an ellipsis resolu-tion system.
It is practical to seek an automat-ically trainable system that utilizes both typesof knowledge.The effective use of exophoric information,i.e., from the actual world, may perform wellfor resolving an ellipsis.
Exophoric informationconsists of a lot of elements, such as the time,the place, the speaker, and the listener of the ut-terance.
However, it is difficult to become awareof some of them, and some are rather difficultto prescribe.
Thus we utilize one element, thespeaker's social role, i.e., whether the speaker isthe customer or the clerk.
The reason for thisis that it must be an influential attribute, andit is easy to detect in the actual world.
Many ofus would accept a real system such as a spoken-language translation system that detects peechwith independent microphones.It is generally agreed that attributes to re-solve ellipses should be different in each case.Thus although we have to prepare them on acase by case basis, we trained a resolver withthe same attributes.Because we must deal with the noisy inputthat appears in real applications, the trainingattributes, other than the speaker's ocial role,are questioned on a morphological basis.
Wegive each attribute its positional information,i.e., search space of morphemes from the targetpredicate.
Positional information can be one offive kinds: before, at the latest, here, next, andafterward.
For example, a case particle is giventhe position of 'before', the search position of aprefix 'o-' or 'go-' is the 'latest', and an auxil-iary verb is 'after' the predicate.
The attributesof predicates, and their semantic ategories areplaced in 'here'.For predicate semantics, we utilized the toptwo layers of Kadokawa Ruigo Shin-Jiten, athree-layered hierarchical Japanese thesaurus.4 D iscuss ionIn this section we discuss the feasibility of the el-lipsis resolver via a decision tree in detail fromthree points of view: the amount of trainingdata, the topic dependency, and the case differ-ence.
The first two are discussed against 'ga(v.)'case (see subsection 4.3).We used F-measures metrics to evaluate theperformance of ellipsis resolution.
The F-measure is calculated by using recall and pre-cision:2xPxRF-  P+R (1)where P is precision and R is recall.
In thispaper, F-measure is described with a percentage(%).1430Table 3: Training size and performanceDial.
Samp.25 46350 863100 1710200 3448400 690671.0 55.6 66.2 59.076.4 69.7 71.5 67.282.1 76.4 77.0 73.285.1 79.8 79.7 76.784.7 81.1 82.0 78.74.1 Amount  of  Tra in ing DataWe trained decision trees with a varied num-ber of training dialogues, namely 25, 50, 100,200 and 400 dialogues, each of which includeda smaller set of training dialogues.
The exper-iment was done with 100 test dialogues (1685subject ellipses), and none were included in thetraining dialogues.Table 3 indicates the training size and perfor-mance calculated by F-measure.
This illustratesthat the performance improves as the trainingsize increases in all types of ellipses.
Althoughit is not shown in the table, we note that theresults in both recall and precision improve con-tinuously as well as those in F-measure.The performance difference of all ellipsistypes by training size is also plotted in Fig-ure 1 on a semi-logarithmic scale.
It is in-teresting to see from the figures that the rateof improvement gradually decelerates and thatsome of the ellipsis types seem to have practi-cally stopped improving at around 400 trainingdialogues (6806 samples).
Aone and Bennett(1995) claimed that the overall anaphora res-olution performance seems to have reached aplateau at around 250 training examples.
Thisresult, however, indicates that 104 ,,~ 10 s train-ing samples would be enough to train the treesin this task.The chart gives us more information that per-formance limitation with our approach wouldbe 80% ,,~ 85% because ach ellipsis type seemsto approach the similar value, in particular forthose in large training samples (lsg) and (2sg).Greater performance improvement is expectedby conducting more training in (2pl) and (g).4.2 Topic Dependenc iesIt is completely satisfactory to build resolutionknowledge only with topic-independent i for-mation.
However, is it practical?
We will dis-cuss this question by conducting a few experi-A mE0E0n10080604020.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~o.~.
- - - * "  o .
.
.
?
.?
?~-"  ?
.
.
.
.
.
?
.
?m.?
.
.
,  .
.
.
.
.
.
, .
.~?
....... .
~ .
.
.
.
o" "?
j .
. '
? "
~ - -m .
.
.
........................ " ...
....-?"
<2sg> .................  ......... Total ,-"":i ................................. ' "  <Ip , ........ ,,.
... <g> ........<2pl> ........t 1 , ,  i i i ,25 50 100 200 400Training size (dialogues)Figure 1: Training size and performancements.We utilized the ATI~ travel arrangement cor-pus (Furuse et al, 1994).
The corpus containsdialogues exchanged between two people.
Var-ious topics of travel arrangements such as im-migration, sightseeing, shopping, and ticket or-dering are included in the corpus.
A dialogueconsists of 10 to 30 exchanges.
We classified i-alogues of the corpus into four topic categories:H1 Hotel room reservation, modification andcancellationH2 Hotel service inquiry and troubleshootingHR Other hotel arrangements, such as hotel se-lection and an explanation of hotel facilitiesR Other travel arrangementsFifty dialogues were chosen randomly from thecorpus in the topic category H1, H2, R, and theoverall topic T(= H1 + H2 + HR + R) as train-ing dialogues.
We used 100 unseen dialogues astest samples again, which were the same as thesamples used in the training-size xperiment.Table 4 shows the topic-dependency of eachtopic category that we provide with the F-measure.
For instance, the first figure in the'T/' row (73.4) denotes that the accuracy withthe F-measure is 73.4% against topic H1 testsamples when training is conducted on T, i.e.,all topics.
Note that the second row of the tableindicates the ingredient of each topic in the testsamples (and thus, the corpus).1431Table 4: Topic dependencyTrain/Test(%)H1/H~IR~T~/H1 /g2 /ttn /R20.1 27.7 11.2 40.978.1 55.9 65.3 61.671.3 67.0 62.6 62.675.1 61.7 61.1 75.473.4 62.5 62.6 66.2Total100.063.765.669.966.2T-  Hn/  73.7 61.9 59.5 63.9 64.8The results illustrate that very high accu-racy is obtained when a training topic and atest topic coincide.
This implies the impor-tance not to train dialogues of unnecessary top-ics if the resolution topic is imaginable or re-stricted, in order to obtain higher performance.Among four topic subcategories, topic R showsthe highest accuracy (69.9%) in total perfor-mance.
The reason is not that topic R hassomething important o train, but that topicR contains the most test dialogues chosen atrandom.The table also illustrates that a resolvertrained in various kinds of topics ('T/') demon-strates higher resolving accuracy against thetesting data set.
It performs with better thanaverage accuracy in every topic compared to onewhich is trained in a biased topic.
By lookingat some examples it may be possible to build anall-around ellipsis resolver, but topic-dependentfeatures are necessary for better performance.The 'T - Hn/ '  resolver shows the lowest per-formance (59.5%) against ' /Hn' test set.
Thisresult is more evidence supporting the impor-tance of topic-dependent features.4.3 Dif ference in Surface CaseWe applied a machine-learned resolver to agentcase ellipses (Yamamoto et at., 1997).
In thispaper, we discuss whether this technique is ap-plicable to surface cases.We examined the feasibility of a machine-learned ellipsis resolver for three principal sur-face cases in Japanese, 'ga', 'wo', and 'hi q.Roughly speaking, they express the subject, thedirect object, and the indirect object of a sen-tence respectively.
We classified the 'ga' caseinto two samples: a predicate of a sentence witha 'ga' case ellipsis that is a verb or an adjective.1We cannot, investigate other optional cases due to alack of samples.Table 5: Performance of major types in caseCa?ega(adj.
)woni(lsg) (2sg) C a) Total58.3 68.1 85.9 79.766.7 - -  97.7 95.695.2 95.7 81.9 91.7ga(v.) 84.7 81.1 82.0 78.7In other words, this distinction corresponds towhether a sentence in English is a be-verb or ageneral-verb sentence.
Henceforth, we call them'ga(v.)' and 'ga(adj.)'
respectively.The training attributes provided are the samein all surface cases.
They are listed in Table 2.In the experiment, 300 training dialogues and100 unseen test dialogues were used.
The fol-lowing results are shown in Table 52 .
The tableillustrates that the ga(adj.)
resolver has a simi-lar performance to the ga(v.) resolver, whereasthe former has a distinctive tendency toward thelatter in each ellipsis type.
The ga(adj.)
caseresolver produces unsatisfactory esults in Clsg/and (2sg/ellipses, ince insufficient samples ap-peared in the training set.In the 'wo' case, more than 90% of the sam-ples are tagged with Ca), thus they are easily rec-ognized as anaphoric.
Although it may be diffi-cult to decide the antecedents in the anaphoricellipses by using information in Table 2, the re-sults show that it is possible to simply recog-nize them.
After recognizing that the ellipsis isanaphoric, it is possible to resolve them in othercontextual processing modules, such as center-ing.It is important to note that a satisfactory per-formance is presented for the 'ni' case (mostlyindirect object).
One reason for this could bethat many indirect objects refer to exophoricpersons, and thus an approach utilizing a deci-sion tree that makes a selection from fixed de-cision candidates i suitable for 'ni' resolution.5 Ins ide  a Dec is ion  TreeA decision tree is a convenient resolver for somekinds of problems, but we should not regard itas a black-box tool.
It tells us what attributesare important, whether or not the attributes are2The result of the ga(v.) case is the same as '400 '  inTable 3.143203 (D"100z5000200010005002001005O3O0ga(v.) ,.oga(a. )
.
*I ' l l  =WO xxi I i i500 1000 2000 5000Training samplesFigure 2: Training samples vs. nodes10000Table 6: Depth and maximumwidth of decisiontreega/25 /100 /400 ga(adj.)
wo niDepth 27 34 49 28 10 18Width 26 58 146 52 10 28sufficient, and sometimes more.
In this section,we investigate decision trees and discuss themin detail.5.1 Tree ShapeThe relation between the number of trainingsamples and the number of nodes in a decisiontree is shown logarithmically in Figure 2.
Itis clear from the chart that the two factors of'ga(v.)' case are logarithmically linear.
This isbecause no pruning is conducted in building adecision tree.
We also see that a more compacttree is built in the order of 'wo', 'nz', 'ga(adj.
)'and :ga(v.)'.
This implies that the 'wo' case isthe easiest of the four cases for characterizingthe individuality among the ellipsis types.Table 6 shows node depth and the maximumwidth in the decision trees we have built.
Bystudying Table 5 and Table 6, we can see thatthe shallower the decision tree is, the better theresolver performs.
One explanation for this maybe that a deeper (and maybe bigger) decisiontree fails to characterize each ellipsis type well,and thus it performs worse.5.2 At t r ibute  CoverageWe define a factor 'coverage' for each attribute.Attribute coverage is the rate of the samplesused to reach a decision about the samples usedto build a decision tree.
If an attribute is usedat the top node of a decision tree, the attributecoverage is 100% in the definition, because allsamples use it (first) to reach their decision.From this, we can learn the participation of eachattribute, i.e., each attribute's importance.Some typical attribute-coverages are ex-pressed in Table 7.
Note that 'ga(25)' denotesthe results of 'ga(v.)' with 25-dialogue training.A glance at the table will reveal that the cover-age is not constant with an increasing numberof training dialogues.
Here we build a hypothe-sis from the table that more genera\] attributesare preferred with a increase in training size.The table illustrates that the topic-independent attributes increase with a risein training size, such as '-tekudasaru' or 'teitadaku' (both auxiliary verbs which expressthe hearer's action toward the speaker with thespeaker's respect).
The table shows in contrastthat the topic-dependent a tributes decrease,such as ':before 72' (a category in which wordsconcerned with intention are included beforethe predicate mentioned) or ':before 94'.
Thereare also some topic-independent words such as'-ka' (a particle that expresses that the sentenceis interrogative) or ':before ~1/~3 '  which arestill important regardless of the training size.This indicates the advantages of a machine-learning approach, because difficulties alwaysarise in differentiating these words in manualapproaches.Table 8 also contrasts typical coverage in sur-face cases.
It illustrates that there is a distinctdifference between 'ga(v.)' and 'ga(adj.)'.
Theresolver of the 'ga(adj.)'
case is interested inanother cases, such as '-de' or contents of an-other case ':before 16/34', whereas 'ga(v.)' caseresolver checks some predicates and influentialfunctional words.
Coverage of each attribute inthe 'hi' case has similar tendencies to those inthe 'ga(v.)' case, except for a few attributes.6 Conc lus ion  and  Future  WorkThis paper proposed a method for resolving theellipsis that appear in Japanese dialogues.
Amachine-learning algorithm is used as the ac-3\Ve practically regard them as topic-independentwords, because expressing the speaker's inten-tion/thought is topic-independent.1433Table 7: Training Size vs. CoverageAttribute:here 43(intention):here 41(thought)'-ka'(question)'- tekudasa ru'(poli te)honorific verbs'-teitadaku'(poli te )'-suru' (to do)ga/25 ga/lO0 ga/400100.0 100.0 100.072.8 84.8 86.553.1 83.2 66.39.1 49.1 49.8- -  39.9 36.8- -  33.2 33.94.1 22.0 26.1:before 72(facilities) 55.1 0.5 3.8:before 94(building) 28.5 9.8 7.7:before 83(language) 25.1 1.1 1.3Speaker's role 11.7 9.1 20.5Table 8: Case vs. CoverageAttribute ga/400 ga(adj.)
ni'-gozaimasu'(poliie) - -  100.0 - -:before 16(situation) 5.1 68.5 0.5:before 34(statement) 5.3 59.0 11.2'-de'(case particle) 5.2 23.9 1.9'-o/-go' 46.4 7.0 100.0:here 43(intention) 100.0 - -  49.8:here 41(thought) 86.5 - -  43.5Speaker's role 20.5 33.1 28.0tual ellipsis resolver with this approach.
Theresults of blind tests have proven that the pro-posed method is able to provide a satisfactoryresolution accuracy of 91.7% in indirect objects,and 78.7~ in subjects with verb predicates.We also discussed training size, topic depen-dency and difference in grammatical case in adecision tree.
By investigating decision trees,we conclude that topic-dependent attributes arealso necessary for obtaining higher performance,and that indispensable attributes depend on thegrammatical case to resolve.Although this paper limits its scope, the pro-posed approach may also be applicable to otherproblems, such as referential property and thenumber of nouns, and in other languages uchas Korean.
In addition, we will explore contex-tua\] ellipses in the future, since it was foundthat most of the ellipses that appeared in spo-ken dialogues are found to be anaphoric in the: WO' case .AcknowledgmentThe authors would like to thank Dr. NaoyaArakawa, who provided data regarding case el-lipsis.
We are also thankful to Mr. HitoshiNishimura for conducting some experiments.ReferencesC.
Aone and S. W. Bennett.
1995.
Evaluat-ing Automated and Manual Acquisition ofAnaphora Resolution Strategies.
In Proc.
of33rd Annual Meeting of the A CL, pages 122-129.O.
Furuse, Y. Sobashima, T. Takezawa, andN.
Uratani.
1994.
Bilingual Corpus forSpeech Translation.
In Proc.
of AAAI'94Workshop on the Integration of Natural Lan-guage and Speech Processing, pages 84-91.O.
Furuse, J. Kawai, H. \[ida, S. Akamine,and D.-B.
Kim.
1995.
Multi-lingual Spoken-Language Translation Utilizing TranslationExamples.
In Proc.
of Natural Language Pro-cessing Pacific-Rim Symposium (NLPRS'95),pages 544-549.M.
Murata and M. Nagao.
1997.
An Estimateof Referents of Pronouns in Japanese Sen-tences using Examples and Surface Expres-sions.
Journal of Natural Language Process-ing, 4(1):87-110. written in Japanese.H.
Nakaiwa and S. Shirai.
1996.
Anaphora Res-olution of Japanese Zero Pronouns with Deic-tic Reference.
In Proc.
of COLING-96, pages812-817.J.
R. Quinlan.
1993.
C~.5: Programs for Ma-chine Learning.
Morgan Kaufmann.H.
Tanaka.
1994.
Verbal Case Frame Ac-quisition from a Biliungual Corpus: Grad-ual Knowledge Acquisition.
In Proc.
ofCOLING-94, pages 727-731.M.
Walker and J. D. Moore.
1997.
EmpiricalStudies in Discourse.
Computational Linguis-tics, 23(1):1-12, March.K.
Yamamoto, E. Sumita, O. Furuse, andH.
\[ida.
1997.
Ellipsis Resolution in Dia-logues via Decision-Tree Learning.
In Proc.of Natural Language Processing Pacific-RimSymposium (NLPRS'97), pages 423-428.1434d~.
~% g~m ~-~ATR ~ - ~ : ~E-mail: yamamoto@i~l.atr.co.jp( .~ '~r~?
)~:~)  ~i~:~,'~  ~ ~,~ zoo:~-v ,  II, ~ '~.~?~:~:~ (decision~ree) l,:$ ZO~.
~'~'~\]~i~o)~E~,-\]~ =-,,~l l~ .#:~# (exophoric ellipsis) ?
)~:  ~ 3C8~$~#(endophoric ellipsis) o)~,~ ~ ~,, -5 Po~'ab zoo ~$ ZOo0)~~0)3 :~I I  80% ,,., 85% ~:-~.
'2,~~o~ ~'~ ~:-~-~-~o-i~'~--h~JL~ ~ zoo ~I~I1435
