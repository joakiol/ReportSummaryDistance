Cross-lingual Projected Expectation Regularization forWeakly Supervised LearningMengqiu Wang and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305 USA{mengqiu,manning}@cs.stanford.eduAbstractWe consider a multilingual weakly supervisedlearning scenario where knowledge from an-notated corpora in a resource-rich languageis transferred via bitext to guide the learningin other languages.
Past approaches projectlabels across bitext and use them as featuresor gold labels for training.
We propose anew method that projects model expectationsrather than labels, which facilities transferof model uncertainty across language bound-aries.
We encode expectations as constraintsand train a discriminative CRF model usingGeneralized Expectation Criteria (Mann andMcCallum, 2010).
Evaluated on standardChinese-English and German-English NERdatasets, our method demonstrates F1 scoresof 64% and 60% when no labeled data isused.
Attaining the same accuracy with su-pervised CRFs requires 12k and 1.5k labeledsentences.
Furthermore, when combined withlabeled examples, our method yields signifi-cant improvements over state-of-the-art super-vised methods, achieving best reported num-bers to date on Chinese OntoNotes and Ger-man CoNLL-03 datasets.1 IntroductionSupervised statistical learning methods have en-joyed great popularity in Natural Language Process-ing (NLP) over the past decade.
The success of su-pervised methods depends heavily upon the avail-ability of large amounts of annotated training data.Manual curation of annotated corpora is a costly andtime consuming process.
To date, most annotated re-sources resides within the English language, whichhinders the adoption of supervised learning methodsin many multilingual environments.To minimize the need for annotation, significantprogress has been made in developing unsupervisedand semi-supervised approaches to NLP (Collinsand Singer 1999; Klein 2005; Liang 2005; Smith2006; Goldberg 2010; inter alia) .
More recentparadigms for semi-supervised learning allow mod-elers to directly encode knowledge about the taskand the domain as constraints to guide learning(Chang et al., 2007; Mann and McCallum, 2010;Ganchev et al., 2010).
However, in a multilingualsetting, coming up with effective constraints requireextensive knowledge of the foreign1 language.Bilingual parallel text (bitext) lends itself as amedium to transfer knowledge from a resource-richlanguage to a foreign languages.
Yarowsky and Ngai(2001) project labels produced by an English tag-ger to the foreign side of bitext, then use the pro-jected labels to learn a HMM model.
More recentwork applied the projection-based approach to morelanguage-pairs, and further improved performancethrough the use of type-level constraints from tagdictionary and feature-rich generative or discrimina-tive models (Das and Petrov, 2011; Ta?ckstro?m et al.,2013).In our work, we propose a new projection-basedmethod that differs in two important ways.
First,we never explicitly project the labels.
Instead, weproject expectations over the labels.
This projection1For experimental purposes, we designate English as theresource-rich language, and other languages of interest as ?for-eign?.
In our experiments, we simulate the resource-poor sce-nario using Chinese and German, even though in reality thesetwo languages are quite rich in resources.55Transactions of the Association for Computational Linguistics, 2 (2014) 55?66.
Action Editor: Lillian Lee.Submitted 9/2013; Revised 12/2013; Published 2/2014.
c?2014 Association for Computational Linguistics.acts as a soft constraint over the labels, which al-lows us to transfer more information and uncertaintyacross language boundaries.
Secondly, we encodethe expectations as constraints and train a model byminimizing divergence between model expectationsand projected expectations in a Generalized Expec-tation (GE) Criteria (Mann and McCallum, 2010)framework.We evaluate our approach on Named EntityRecognition (NER) tasks for English-Chinese andEnglish-German language pairs on standard publicdatasets.
We report results in two settings: a weaklysupervised setting where no labeled data or a smallamount of labeled data is available, and a semi-supervised settings where labeled data is available,but we can gain predictive power by learning fromunlabeled bitext.2 Related WorkMost semi-supervised learning approaches embodythe principle of learning from constraints.
There aretwo broad categories of constraints: multi-view con-straints, and external knowledge constraints.Examples of methods that explore multi-viewconstraints include self-training (Yarowsky, 1995;McClosky et al., 2006),2 co-training (Blum andMitchell, 1998; Sindhwani et al., 2005), multi-view learning (Ando and Zhang, 2005; Carlson etal., 2010), and discriminative and generative modelcombination (Suzuki and Isozaki, 2008; Druck andMcCallum, 2010).An early example of using knowledge as con-straints in weakly-supervised learning is the workby Collins and Singer (1999).
They showed that theaddition of a small set of ?seed?
rules greatly im-prove a co-training style unsupervised tagger.
Changet al.
(2007) proposed a constraint-driven learning(CODL) framework where constraints are used toguide the selection of best self-labeled examples tobe included as additional training data in an iterativeEM-style procedure.
The kind of constraints usedin applications such as NER are the ones like ?thewords CA, Australia, NY are LOCATION?
(Changet al., 2007).
Notice the similarity of this partic-2A multi-view interpretation of self-training is that the self-tagged additional data offers new views to learners trained onexisting labeled data.ular constraint to the kinds of features one wouldexpect to see in a discriminative MaxEnt model.The difference is that instead of learning the valid-ity (or weight) of this feature from labeled exam-ples ?
since we do not have them ?
we can con-strain the model using our knowledge of the domain.Druck et al.
(2009) also demonstrated that in an ac-tive learning setting where annotation budget is lim-ited, it is more efficient to label features than ex-amples.
Other sources of knowledge include lexi-cons and gazetteers (Druck et al., 2007; Chang etal., 2007).While it is straight-forward to see how resourcessuch as a list of city names can give a lot of mileagein recognizing locations, we are also exposed to thedanger of over-committing to hard constraints.
Forexample, it becomes problematic with city namesthat are ambiguous, such as Augusta, Georgia.3To soften these constraints, Mann and McCallum(2010) proposed the Generalized Expectation (GE)Criteria framework, which encodes constraints as aregularization term over some score function thatmeasures the divergence between the model?s ex-pectation and the target expectation.
The connectionbetween GE and CODL is analogous to the relation-ship between hard (Viterbi) EM and soft EM, as il-lustrated by Samdani et al.
(2012).Another closely related work is the PosteriorRegularization (PR) framework by Ganchev et al.(2010).
In fact, as Bellare et al.
(2009) have shown,in a discriminative model these two methods opti-mize exactly the same objective.4 The two differin optimization details: PR uses a EM algorithmto approximate the gradients which avoids the ex-pensive computation of a covariance matrix betweenfeatures and constraints, whereas GE directly cal-culates the gradient.
However, later results (Druck,2011) have shown that using the Expectation Semir-ing techniques of Li and Eisner (2009), one cancompute the exact gradients of GE in a ConditionalRandom Fields (CRF) (Lafferty et al., 2001) at costs3This is a city in the state of Georgia in USA, famous for itsgolf courses.
It is ambiguous since both Augusta and Georgiacan also be used as person names.4The different terminology employed by GE and PR maybe confusing to discerning readers, but the ?expectation?
in thecontext of GE means the same thing as ?marginal posterior?
asin PR.56no greater than computing the gradients of ordinaryCRF.
And empirically, GE tends to perform more ac-curately than PR (Bellare et al., 2009; Druck, 2011).Obtaining appropriate knowledge resources forconstructing constraints remain as a bottleneck inapplying GE and PR to new languages.
However,a number of past work recognizes parallel bitext as arich source of linguistic constraints, naturally cap-tured in the translations.
As a result, bitext hasbeen effectively utilized for unsupervised multilin-gual grammar induction (Alshawi et al., 2000; Sny-der et al., 2009), parsing (Burkett and Klein, 2008),and sequence labeling (Naseem et al., 2009).A number of recent work also explored bilin-gual constraints in the context of simultaneous bilin-gual tagging, and showed that enforcing agreementsbetween language pairs give superior results thanmonolingual tagging (Burkett et al., 2010; Che etal., 2013; Wang et al., 2013a).
Burkett et al.
(2010)also demonstrated a uptraining (Petrov et al., 2010)setting where tag-induced bitext can be used as ad-ditional monolingual training data to improve mono-lingual taggers.
A major drawback of this approachis that it requires a readily-trained tagging models ineach languages, which makes a weakly supervisedsetting infeasible.
Another intricacy of this approachis that it only works when the two models have com-parable strength, since mutual agreements are en-forced between them.Projection-based methods can be very effectivein weakly-supervised scenarios, as demonstrated byYarowsky and Ngai (2001), and Xi and Hwa (2005).One problem with projected labels is that they areoften too noisy to be directly used as training sig-nals.
To mitigate this problem, Das and Petrov(2011) designed a label propagation method to au-tomatically induce a tag lexicon for the foreign lan-guage to smooth the projected labels.
Fossum andAbney (2005) filter out projection noise by com-bining projections from from multiple source lan-guages.
However, this approach is not always viablesince it relies on having parallel bitext from multi-ple source languages.
Li et al.
(2012) proposed theuse of crowd-sourced Wiktionary as additional re-sources for inducing tag lexicons.
More recently,Ta?ckstro?m et al.
(2013) combined token-level andtype-level constraints to constrain legitimate labelsequences and and recalibrate the probability distri-bution in a CRF.
The tag dictionary used for POStagging are analogous to the gazetteers and namelexicons used for NER by Chang et al.
(2007).Our work is also closely related to Ganchev etal.
(2009).
They used a two-step projection methodsimilar to Das and Petrov (2011) for dependencyparsing.
Instead of using the projected linguis-tic structures as ground truth (Yarowsky and Ngai,2001), or as features in a generative model (Dasand Petrov, 2011), they used them as constraintsin a PR framework.
Our work differs by project-ing expectations rather than Viterbi one-best labels.We also choose the GE framework over PR.
Experi-ments in Bellare et al.
(2009) and Druck (2011) sug-gest that in a discriminative model (like ours), GEis more accurate than PR.
More recently, Ganchevand Das (2013) further extended this line of workto directly train discriminative sequence models us-ing cross lingual projection with PR.
The types ofconstraints applied in this new work are similar tothe ones in the monolingual PR setting proposed byGanchev et al.
(2010), where the total counts of la-bels of a particular kind are expected to match somefraction of the projected total counts.
Our work dif-fer in that we enforce expectation constraints at to-ken level, which gives tighter guidance to learningthe model.3 ApproachGiven bitext between English and a foreign lan-guage, our goal is to learn a CRF model in theforeign language from little or no labeled data.Our method performs Cross-Lingual ProjectedExpectation Regularization (CLiPER).For every aligned sentence pair in the bitext, wefirst compute the posterior marginal at each word po-sition on the English side using a pre-trained EnglishCRF tagger; then for each aligned English word, weproject its posterior marginal as expectations to thealigned word position on the foreign side.
Figure 1shows a snippet of a sentence from real corpus.
No-tice that if we were to directly project the Viterbibest assignment from English to Chinese, all threeChinese words that are named entities would havegotten the wrong tags.
But projecting the EnglishCRF model expectations preserves some uncertain-ties, informing the Chinese model that there is a 40%57a reception in Luobu Linka .
.
.
.
.
.
met with representatives of Zhongguo RibaoO:0.0032 O:0.0037 GPE:0.0000 GPE:0.0000PER:0.0000 PER:0.0000 PER:0.0000GPE:0.0042 GPE:0.0042 LOC:0.0003 LOC:0.0003GPE:0.0000 GPE:0.0000 GPE:0.0000ORG:0.0308 ORG:0.0307 O:0.0012 O:0.0011ORG:0.0000 ORG:0.0000 ORG:0.0000LOC:0.3250 LOC:0.3256 ORG:0.4060 ORG:0.4061LOC:0.0000 LOC:0.0000 LOC:0.0000PER:0.6369 PER:0.6377 PER:0.5925 PER:0.5925O:1.0000 O:1.0000 O:1.0000?
????
??
?
???
.
.
.
.
.
.
??
?
??
??
?
?PER:0.6373 PER:0.5925 PER:0.5925O:1.0000 O:1.0000 O:1.0000LOC:0.3253 ORG:0.4060 ORG:0.4061LOC:0.0000 LOC:0.0000 LOC:0.0000ORG:0.0307 O:0.0012 O:0.0011ORG:0.0000 ORG:0.0000 ORG:0.0000GPE:0.0042 LOC:0.0003 LOC:0.0003GPE:0.0000 GPE:0.0000 GPE:0.0000O:0.0035 GPE:0.0000 GPE:0.0000PER:0.0000 PER:0.0000 PER:0.0000Figure 1: Diagram illustrating the projection of model expectation from English to Chinese.
The posteriorprobabilities assigned by the English CRF model is shown above each English word; automatically inducedword alignments are shown in red; the correct projected labels for Chinese words are shown in green, andincorrect labels are shown in red.chance that ??????
(China Daily) is an organi-zation in this context.We would like to learn a CRF model in the for-eign language that has similar expectations as theprojected expectations from English.
To this end,we adopt the Generalized Expectation (GE) Crite-ria framework introduced by Mann and McCallum(2010).
In the remainder of this section, we followthe notation used in (Druck, 2011) to explain our ap-proach.3.1 CLiPERThe general idea of GE is that we can express ourpreferences over models through constraint func-tions.
A desired model should satisfy the imposedconstraints by matching the expectations on theseconstraint functions with some target expectations(attained by external knowledge like lexicons or inour case transferred knowledge from English).
Wedefine a constraint function ?i,lj for each word po-sition i and output label assignment lj .
?i,lj = 0 isa constraint in that position i cannot take label lj .The set {l1, ?
?
?
, lm} denotes all possible label as-signment for each yi, and m is number of label val-ues.
Ai is the set of English words aligned to Chi-nese word i.
?i,lj are defined for all position i suchthat Ai 6= ?.
In other words, the constraint functionapplies only to Chinese word positions that have atleast one aligned English word.
Each ?i,lj (y) canbe treated as a Bernoulli random variable, and weconcatenate the set of all ?i,lj into a random vector?
(y), where ?k = ?i,lj if k = i ?m + j.
We dropthe (y) in ?
for simplicity.The target expectation over ?i,lj , denoted as ?
?i,lj ,is the expectation of assigning label lj to Englishword Ai under the English conditional probabilitymodel.
When multiple English words are aligned tothe same foreign word, we average the expectations.The expectation over ?
under a conditional prob-ability model P (y|x;?)
is denoted as EP (y|x;?)[?
],and simplified as E?[?]
whenever it is unambigu-ous.The conditional probability model P (y|x;?)
inour case is defined as a standard linear-chain CRF:5P (y|x;?)
= 1Z(x;?
)exp( n?i?f(x, yi, yi?1))where f is a set of feature functions; ?
are the match-ing parameters to learn; n = |x|.The objective function to maximize in a standardCRF is the log probability over a collection of la-beled documents:LCRF (?)
=a?
?a=1logP (y?a|xa;?)
(1)a?
is the number of labeled sentences.
y?
is an ob-served label sequence.The objective function to maximize in GE is de-fined as the sum over all unlabeled examples on the5We simplify notation by dropping the L2 regularizer in theCRF definition, but apply it in our experiments.58foreign side of bitext, denoted as xb, over some costfunction S between the model expectation over ?(E?[?])
and the target expectation (??
).We choose S to be the negative L22 squared errorsum6 defined as:LGE(?)
=n?
?b=1S(EP (yb|xb;?)[?
(yb)], ??b)=n??b=1???
?b ?
E?[?
(yb)]?22 (2)n?
is the total number of unlabeled bitext sentencepairs.When both labeled and bitext training data areavailable, the joint objective is the sum of Eqn.
1and 2.
Each is computed over the labeled trainingdata and foreign half in the bitext, respectively.We can optimize this joint objective by comput-ing the gradients and use a gradient-based optimiza-tion method such as L-BFGS.
Gradients of LCRFdecomposes down to the gradients over each la-beled training example (x,y?).
Computing the gra-dient of LGE decomposes down to the gradients ofS(EP (y|xb;?[?])
for each unlabeled foreign sentencex and the constraints over this example ?
.
The gra-dients can be calculated as:???S(E?[?])
= ????(???
E?[?
])T (???
E?[?
])= 2(???
E?[?
])T ( ???E?[?
])We redefine the penalty vector u = 2(???
E?[?
])to be u.
???E?[?]
is a matrix where each columncontains the gradients for a particular model feature?
with respect to all constraint functions ?.
It can be6In general, other loss functions such as KL-divergence canalso be used for S. We found L22 to work well in practice.computed as:???E?[?]
=?y?
(y) ??
?P (y|x;?)=?y?
(y) ???
( 1Z(x;?
)exp(?T f(x,y)))=?y?(y)(1Z(x;?
)( ???
exp(?T f(x,y)))+ exp(?T f(x,y))( ???1Z(x;?)))=?y?
(y)(P (y|x;?)f(x,y)T?
P (y|x;?
)?y?P (y?|x;?)f(x,y?
)T)=?yP (y|x;?)?y?(y)f(x,y)T?
(?yP (y|x;?)?
(y))(?yP (y|x;?
)f(x,y)T)= COVP (y|x;?)
(?
(y), f(x,y)) (3)= E?
[?fT ]?
E?[?]E?
[fT ] (4)Eqn.
3 gives the intuition of how optimization worksin GE.
In each iteration of L-BFGS, the model pa-rameters are updated according to their covariancewith the constraint features, scaled by the differ-ence between current expectation and target expec-tation.
The term E?
[?fT ] in Eqn.
4 can be com-puted using a dynamic programming (DP) algo-rithm, but solving it directly requires us to store amatrix of the same dimension as fT in each stepof the DP.
We can reduce the complexity by usingthe same trick as in (Li and Eisner, 2009) for com-puting Expectation Semiring.
The resulting algo-rithm has complexity O(nm2), which is the same asthe standard forward-backward inference algorithmfor CRF.
(Druck, 2011, 93) gives full details of thisderivation.3.2 Hard vs. soft ProjectionProjecting expectations instead of one-best label as-signments from English to foreign language canbe thought of as a soft version of the method de-scribed in (Das and Petrov, 2011) and (Ganchev et59al., 2009).
Soft projection has its advantage: whenthe English model is not certain about its predic-tions, we do not have to commit to the current bestprediction.
The foreign model has more freedomto form its own belief since any marginal distribu-tion it produces would deviates from a flat distri-bution by just about the same amount.
In general,preserving uncertainties till later is a strategy thathas benefited many NLP tasks (Finkel et al., 2006).Hard projection can also be treated as a special casein our framework.
We can simply recalibrate pos-terior marginal of English by assigning probabilitymass 1 to the most likely outcome, and zero ev-erything else out, effectively taking the argmax ofthe marginal at each word position.
We refer tothis version of expectation as the ?hard?
expecta-tion.
In the hard projection setting, GE training re-sembles a ?project-then-train?
style semi-supervisedCRF training scheme (Yarowsky and Ngai, 2001;Ta?ckstro?m et al., 2013).
In such a training scheme,we project the one-best predictions of English CRFto the foreign side through word alignments, then in-clude the newly ?tagged?
foreign data as additionaltraining data to a standard CRF in the foreign lan-guage.
Rather than projecting labels on a per-wordbasis, Yarowsky and Ngai (2001) also explored analternative method for noun-phrase (NP) bracketingtask that amounts to projecting the spans of NPsbased on the observation that individual NPs tend toretain their sequential spans across translations.
Weexperimented with the same method for NER, butfound that this method of projecting the NE spansdoes not help in reducing noise and actually lowersmodel performance.Besides the difference in projecting expecta-tions rather than hard labels, our method and the?project-then-train?
scheme also differ by optimiz-ing different objectives: CRF optimizes maximumconditional likelihood of the observed label se-quence, whereas GE minimizes squared error be-tween model?s expectation and ?hard?
expectationbased on the observed label sequence.
In the casewhere squared error loss is replaced with a KL-divergence loss, GE has the same effect as marginal-izing out all positions with unknown projected la-bels, allowing more robust learning of uncertaintiesin the model.
As we will show in the experimen-O PER LOC ORG GPEO 291339 391 141 1281 221PER 1263 6721 5 56 73LOC 409 23 546 123 133ORG 2423 143 52 8387 196GPE 566 239 69 668 6604O PER LOC ORG MISCO 81209 24 38 155 103PER 77 5725 41 69 10LOC 49 40 3743 127 60ORG 178 102 142 4075 91MISC 175 41 30 114 1826Table 1: Raw counts in the error confusion matrix ofEnglish CRF models.
Top table contains the countson OntoNotes test data, and bottom table containsCoNLL-03 test data counts.
Rows are the true la-bels and columns are the observed labels.
For exam-ple, item at row 2, column 3 of the top table reads:we observed 5 times where the true label should bePERSON, but English CRF model output label LO-CATION.tal results in Section 4.2, soft projection in combi-nation of the GE objective significantly outperformsthe project-then-train style CRF training scheme.3.3 Source-side noiseAn additional source of noise comes from errorsgenerated by the source-side English CRF mod-els.
We know that the English CRF models givesF1 score of 81.68% on the OntoNotes dataset forEnglish-Chinese experiment, and 90.45% on theCoNLL-03 dataset for English-German experiment.We present a simple way of modeling English-sidenoise by picturing the following process: the la-bels assigned by the English CRF model (denotedas y) are some noised version of the true labels (de-noted as y?).
We can recover the probability of thetrue labels by marginalizing over the observed la-bels: P (y?|x) =?y P (y?|y) ?
P (y|x).
P (y|x) isthe posterior probabilities given by the CRF model,and we can approximate P (y?|y) by the column-normalized error confusion matrix shown in Table 1.This source-side noise model is likely to be overlysimplistic.
Generally speaking, we could build muchmore sophisticated noising model for the source-side, possibly conditioning on context, or capturinghigher-order label sequences.604 ExperimentsWe conduct experiments on Chinese and GermanNER.
We evaluate CLiPER in two learning set-tings: weakly supervised and semi-supervised.
Inthe weakly supervised setting, we simulate the con-dition of having no labeled training data, and evalu-ate the model learned from bitext alone.
We thenvary the amount of labeled data available to themodel, and examine the model?s learning curve.
Inthe semi-supervised setting, we assume our modelhas access to the full labeled data; our goal is toimprove performance of the supervised method bylearning from additional bitext.4.1 Dataset and setupWe used the latest version of Stanford NER Toolkit7as our base CRF model in all experiments.
Fea-tures for English, Chinese and German CRFs aredocumented extensively in (Che et al., 2013) and(Faruqui and Pado?, 2010) and omitted here forbrevity.
It it worth noting that the current Stan-ford NER models include recent improvements fromsemi-supervise learning approaches that induces dis-tributional similarity features from large word clus-ters.
These models represent the current state-of-the-art in supervised methods, and serve as a verystrong baseline.For Chinese NER experiments, we follow thesame setup as Che et al.
(2013) to evaluate on thelatest OntoNotes (v4.0) corpus (Hovy et al., 2006).8A total of 8,249 sentences from the parallel Chineseand English Penn Treebank portion 9 are reservedfor evaluation.
Odd-numbered documents are usedas development set, and even-numbered documentsare held out as blind test set.
The rest of OntoNotesannotated with NER tags are used to train the En-glish and Chinese CRF base taggers.
There areabout 16k and 39k labeled sentences for Chinese andEnglish training, respectively.
The English CRF tag-ger trained on this training corpus gives F1 scoreof 81.68% on the OntoNotes test set.
Four enti-ties types10 are used for both Chinese and Englishwith a IO tagging scheme.11 The English-Chinese7http://www-nlp.stanford.edu/ner8LDC catalogue No.
: LDC2011T039File numbers: chtb 0001-0325, ectb 1001-107810PERSON, LOCATION, ORGANIZATION and GPE.11We did not adopt the commonly seen BIO tagging schemebitext comes from the Foreign Broadcast Informa-tion Service corpus (FBIS).12 We randomly sampled80k parallel sentence pairs to use as bitext in ourexperiments.
It is first sentence aligned using theChampollion Tool Kit,13 then word aligned with theBerkeleyAligner.14For German NER experiments, we evaluate us-ing the standard CoNLL-03 NER corpus (Sang andMeulder, 2003).
The labeled training set has 12k and15k sentences, containing four entity types.15 AnEnglish CRF model is also trained on the CoNLL-03 English data with the same entity types.
For bi-text, we used a randomly sampled set of 40k parallelsentences from the de-en portion of the News Com-mentary dataset.16 The English CRF tagger trainedon CoNLL-03 English training corpus gives F1 scoreof 90.4% on the CoNLL-03 test set.We report typed entity precision (P), recall (R)and F1 score.
Statistical significance tests are doneusing a paired bootstrap resampling method with1000 iterations, averaged over 5 runs.
We com-pare against three recently approaches that were in-troduced in Section 2.
They are: semi-supervisedlearning method using factored bilingual modelswith Gibbs sampling (Wang et al., 2013a); bilin-gual NER using Integer Linear Programming (ILP)with bilingual constraints, by (Che et al., 2013);and constraint-driven bilingual-reranking approach(Burkett et al., 2010).
The code from (Che et al.,2013) and (Wang et al., 2013a) are publicly avail-able.17 Code from (Burkett et al., 2010) is obtainedthrough personal communications.Since the objective function in Eqn.
2 is non-convex, we adopted the early stopping trainingscheme from (Turian et al., 2010) as the following:after each iteration in L-BFGS training, the model(Ramshaw and Marcus, 1999), because when projected acrossswapping word alignments, the ?B-?
and ?I-?
tag distinctionmay not be well-preserved and may introduce additional noise.12The FBIS corpus is a collection of radio news casts andcontains translations of openly available news and informationfrom media sources outside the United States.
The LDC cata-logue No.
is LDC2003E14.13champollion.sourceforge.net14code.google.com/p/berkeleyaligner15PERSON, LOCATION, ORGANIZATION and MISCELLA-NEOUS.16http://www.statmt.org/wmt13/training-parallel-nc-v8.tgz17https://github.com/stanfordnlp/CoreNLP61is evaluated against the development set; the train-ing procedure is terminated if no improvements havebeen made in 20 iterations.4.2 Weakly supervised resultsFigure 2a and 2b show results of weakly supervisedlearning experiments.
Quite remarkably, on Chinesetest set, our proposed method (CLiPER) achieves aF1 score of 64.4% with 80k bitext, when no labeledtraining data is used.
In contrast, the supervisedCRF baseline would require as much as 12k labeledsentences to attain the same accuracy.
Results on theGerman test set is less striking.
With no labeled dataand 40k of bitext, CLiPER performs at F1 of 60.0%,the equivalent of using 1.5k labeled examples in thesupervised setting.
When combined with 1k labeledexamples, performance of CLiPER reaches 69%, again of over 5% absolute over supervised CRF.
Wealso notice that supervised CRF model learns muchfaster in German than Chinese.
This result is not toosurprising, since it is well recognized that ChineseNER is more challenging than German or English.The best supervised results for Chinese is 10-20%(F1 score) behind best German and English super-vised results.
Chinese NER relies more on lexical-ized features, and therefore needs more labeled datato achieve good coverage.
The results suggest thatCLiPER seems to be very effective at transferringlexical knowledge from English to Chinese.Figure 2c and 2d compares soft GE projectionwith hard GE projection and the ?project-then-train?style CRF training scheme (cf.
Section 3.2).
Weobserve that both soft and hard GE projection sig-nificantly outperform the ?project-then-train?
styletraining scheme.
The difference is especially pro-nounced on the Chinese results when fewer labeledexamples are available.
Soft projection gives betteraccuracy than hard projection when no labeled datais available, and also has a faster learning rate.Incorporating source-side noise using the methoddescribed in Section 3.3 gives a small improvementon Chinese with supervised data, increasing F1 scorefrom 64.40% to 65.50%.
This improvement is statis-tically significant at 92% confidence interval.
How-ever, on the German data, we observe a tiny de-crease with no statistical significance in F1 score,dropping from 59.88% to 59.66%.
A likely ex-planation of the difference is that the English CRFmodel in the English-Chinese experiment, which istrained on OntoNotes data, has a much higher errorrate (18.32%) than the English CRF model in theEnglish-German experiment trained on CoNLL-03(9.55%).
Therefore, modeling noise in the English-Chinese case is likely to have a greater effect thanthe English-German case.4.3 Semi-supervised resultsIn the semi-supervised experiments, we let the CRFmodel use the full set of labeled examples in addi-tion to the unlabeled bitext.
Results on the test setare shown in Table 2.
All semi-supervised baselinesare tested with the same number of unlabeled bitextas CLiPER in each language.
The ?project-then-train?
semi-supervised training scheme severelyhurts performance on Chinese, but gives a small im-provement on German.
Moreover, on Chinese itlearns to achieve high precision but at a significantloss in recall.
On German its behavior is the oppo-site.
Such drastic and erratic imbalance suggest thatthis method is not robust or reliable.
The other threesemi-supervised baselines (row 3-5) all show im-provements over the CRF baseline, consistent withtheir reported results.
CLIPERs gives the best re-sults on both Chinese and German, yielding statis-tically significant improvements over all baselinesexcept for CWD13 on German.
The hard projectionversion of CLiPER also gives sizable gain over CRF.However, in comparison, CLIPERs is superior.The improvements of CLIPERs over CRF onChinese test set is over 2.8% in absolute F1.
Theimprovement over CRF on German is almost a per-cent.
To our knowledge, these are the best reportednumbers on the OntoNotes Chinese and CoNLL-03German datasets.4.4 EfficiencyAnother advantage of our proposed approach is ef-ficiency.
Because we eliminated the previous multi-stage ?uptraining?
paradigm, but instead integratingthe semi-supervised and supervised objective intoone joint objective, we are able to attain signifi-cant speed improvements over all methods exceptCRFptt.
Table 3 shows the required training time.620 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1501020304050607080# of labeled training sentences [k]F1score[%]supervised CRFCLiPPER soft(a) Chinese Test0 1 2 3 4 5 6 7 8 9 10 11 1201020304050607080# of labeled training sentences [k]F1score[%]supervised CRFCLiPPER soft(b) German Test0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 154446485052545658606264666870727476# of labeled training sentences [k]F1score[%]CRF projectionCLiPPER hardCLiPPER soft(c) Soft vs. Hard on Chinese Test0 1 2 3 4 5 6 7 8 9 10 11 125456586062646668707274767880# of labeled training sentences [k]F1score[%]CRF projectionCLiPPER hardCLiPPER soft(d) Soft vs. Hard on German Test[??]
???
?
[??]
?
?A monument commemorating [Vice President Gao GangPER ] was completed in [HengshanLOC ](e) Word proceeding ?monument?
is PERSON[??]
[???]
??
[??]
???
?
?Introduction of [QikouLOC ] [Chairman MaoPER ] [Yellow RiverLOC ] crossing monument(f) Word proceeding ?monument?
is LOCATIONFigure 2: Top four figures show performance curves of CLiPER with varying amounts of available labeledtraining data in a weakly supervised setting.
Vertical axes show the F1 score on the test set.
Performancecurves of supervised CRF and ?project-then-train?
CRF are plotted for comparison.
Bottom two figures areexamples of aligned sentence pairs in Chinese and English.63Chinese GermanP R F1 P R F1CRF 79.09 63.59 70.50 86.69 71.30 78.25CRFptt 84.01 45.29 58.85 81.50 75.56 78.41BPBK10 79.25 65.67 71.83 84.00 72.17 77.64CWD13 81.31 65.50 72.55 85.99 72.98 78.95WCD13a 80.31 65.78 72.33 85.98 72.37 78.59WCD13b 78.55 66.54 72.05 85.19 72.98 78.62CLiPERh 83.67 64.80 73.04??
86.52 72.02 78.61?CLiPERs 82.57 65.99 73.35????
87.11 72.56 79.17???
?Table 2: Test set Chinese, German NER results.Best number of each column is highlighted inbold.
CRF is the supervised baseline.
CRFptt isthe ?project-then-train?
semi-supervised scheme forCRF.
BPBK10 is (Burkett et al., 2010), WCD13 is(Wang et al., 2013a), CWD13A is (Che et al., 2013),and WCD13B is (Wang et al., 2013b) .
CLIPERsand CLIPERh are the soft and hard projections.
?indicates F1 scores that are statistically significantlybetter than CRF baseline at 99.5% confidence level;?
marks significance over CRFptt with 99.5% con-fidence; ?
and ?
marks significance over WCD13with 99.9% and 94% confidence; and  marks sig-nificance over CWD13 with 99.7% confidence; ?marks significance over BPBK10 with 99.9% con-fidence.5 DiscussionsFigure 2e and 2f give two examples of cross-lingualprojection methods in action.
Both examples havea named entity that immediately proceeds the word?????
(monument) in the Chinese sentence.
InFigure 2e, the word ????
has literal meaning of ahillock located at a high position, which also hap-pens to be the name of a former vice president ofChina.
Without having previously observed thisword as a person name in the labeled training data,the CRF model does not have enough evidence tobelieve that this is a PERSON, instead of LOCATION.But the aligned words in English (?Gao Gang?)
areclearly part of a person name as they were pre-ceded by a title (?Vice President?).
The Englishmodel has high expectation that the aligned Chi-nese word of ?Gao Gang?
is also a PERSON.
There-fore, projecting the English expectations to Chineseprovides a strong clue to help disambiguating thisword.
Figure 2f gives another example: the word????
(Huang He, the Yellow River of China) canChinese GermanCRF 19m30s 7m15sCRFptt 34m2s 12m45sWCD13 3h17m 1h1mCWD13a 16h42m 4h49mCWD13b 16h42m 4h49mBPBK10 6h16m 2h42mCLiPERh 1h28m 16m30sCLiPERs 1h40m 18m51sTable 3: Timing stats during model training.be confused with a person name since ???
(Huangor Hwang) is also a common Chinese last name.18.Again, knowing the translation in English, whichhas the indicative word ?River?
in it, helps disam-biguation.The CRFptt and CLIPERh methods successfullylabeled these two examples correctly, but failed toproduce the correct label for the example in Fig-ure 1.
On the other hand, a model trained with theCLIPERs method does correctly label both entitiesin Figure 1, demonstrating the merits of the soft pro-jection method.6 ConclusionWe introduced a domain and language independentsemi-supervised method for training discriminativemodels by projecting expectations across bitext.
Ex-periments on Chinese and German NER show thatour method, learned over bitext alone, can rival per-formance of supervised models trained with thou-sands of labeled examples.
Furthermore, applyingour method in a setting where all labeled examplesare available also shows improvements over state-of-the-art supervised methods.
Our experiments alsoshowed that soft expectation projection is more fa-vorable to hard projection.
This technique can begeneralized to all sequence labeling tasks, and canbe extended to include more complex constraints.For future work, we plan to apply this method tomore language pairs and also explore data selectionstrategies and modeling alignment uncertainties.18In fact, a people search of the name??
on the most pop-ular Chinese social network (renren.com) returns over 13,000matches.64AcknowledgmentsThe authors would like to thank Jennifer Gillenwa-ter for a discussion that inspired this work, BehrangMohit and Nathan Schneider for their help with theArabic NER data, and David Burkett for providingthe source code of their work for comparison.
Wewould also like to thank editor Lillian Lee and thethree anonymous reviewers for their valuable com-ments and suggestions.
We gratefully acknowledgethe support of the U.S. Defense Advanced ResearchProjects Agency (DARPA) Broad Operational Lan-guage Translation (BOLT) program through IBM.Any opinions, findings, and conclusion or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the view ofDARPA, or the US government.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Head-transducer models for speech translationand their automatic acquisition from bilingual data.Machine Translation, 15.Rie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for textchunking.
In Proceedings of ACL.Kedar Bellare, Gregory Druck, and Andrew McCallum.2009.
Alternating projections for learning with expec-tation constraints.
In Proceedings of UAI.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In Proceedingsof EMNLP.David Burkett, Slav Petrov, John Blitzer, and Dan Klein.2010.
Learning better monolingual models with unan-notated bilingual text.
In Proceedings of CoNLL.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of WSDM.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In Proceedings of ACL.Wanxiang Che, Mengqiu Wang, and Christopher D. Man-ning.
2013.
Named entity recognition with bilingualconstraints.
In Proceedings of NAACL.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL.Gregory Druck and Andrew McCallum.
2010.
High-performance semi-supervised learning using discrim-inatively constrained generative models.
In Proceed-ings of ICML.Gregory Druck, Gideon Mann, and Andrew McCallum.2007.
Leveraging existing resources using generalizedexpectation criteria.
In Proceedings of NIPSWorkshopon Learning Problem Design.Gregory Druck, Burr Settles, and Andrew McCallum.2009.
Active learning by labeling features.
In Pro-ceedings of EMNLP.Gregory Druck.
2011.
Generalized Expectation Criteriafor Lightly Supervised Learning.
Ph.D. thesis, Univer-sity of Massachusetts Amherst.Manaal Faruqui and Sebastian Pado?.
2010.
Training andevaluating a German named entity recognizer with se-mantic generalization.
In Proceedings of KONVENS.Jenny Rose Finkel, Christopher D. Manning, and An-drew Y. Ng.
2006.
Solving the problem of cascadingerrors: Approximate bayesian inference for linguisticannotation pipelines.
In Proceedings of EMNLP.Victoria Fossum and Steven Abney.
2005.
Automaticallyinducing a part-of-speech tagger by projecting frommultiple source languages across aligned corpora.
InProceedings of IJCNLP.Kuzman Ganchev and Dipanjan Das.
2013.
Cross-lingual discriminative learning of sequence modelswith posterior regularization.
In Proceedings ofEMNLP.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of ACL.Kuzman Ganchev, Jo ao Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
JMLR, 10:2001?2049.Andrew B. Goldberg.
2010.
New Directions in Semi-supervised Learning.
Ph.D. thesis, University ofWisconsin-Madison.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% solution.
In Proceedings of NAACL-HLT.Dan Klein.
2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis, Stanford Univer-sity.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of ICML.65Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofEMNLP.Shen Li, Jo ao Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedings ofEMNLP-CoNLL.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.Gideon Mann and Andrew McCallum.
2010.
General-ized expectation criteria for semi-supervised learningwith weakly labeled data.
JMLR, 11:955?984.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of NAACL-HLT.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein,and Regina Barzilay.
2009.
Multilingual part-of-speech tagging: Two unsupervised approaches.
JAIR,36:1076?9757.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate deter-ministic question parsing.
In Proceedings of EMNLP.Lance A. Ramshaw and Mitchell P. Marcus.
1999.
Textchunking using transformation-based learning.
Natu-ral Language Processing Using Very Large Corpora,11:157?176.Rajhans Samdani, Ming-Wei Chang, and Dan Roth.2012.
Unified expectation maximization.
In Proceed-ings of NAACL.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: language-independent named entity recognition.
In Proceedingsof CoNLL.Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.2005.
A co-regularization approach to semi-supervised learning with multiple views.
In Proceed-ings of ICML Workshop on Learning with MultipleViews, International Conference on Machine Learn-ing.Noah A. Smith.
2006.
Novel Estimation Methods forUnsupervised Discovery of Latent Structure in Natu-ral Language Text.
Ph.D. thesis, Johns Hopkins Uni-versity.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In Proceedings of ACL.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-wordscale unlabeled data.
In Proceedings of ACL.Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-Donald, and Joakim Nivre.
2013.
Token and typeconstraints for cross-lingual part-of-speech tagging.
InProceedings of ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of ACL.Mengqiu Wang, Wanxiang Che, and Christopher D. Man-ning.
2013a.
Effective bilingual constraints for semi-supervised learning of named entity recognizers.
InProceedings of AAAI.Mengqiu Wang, Wanxiang Che, and Christopher D. Man-ning.
2013b.
Joint word alignment and bilingualnamed entity recognition using dual decomposition.In Proceedings of ACL.Chenhai Xi and Rebecca Hwa.
2005.
A backoff modelfor bootstrapping resources for non-english languages.In Proceedings of HLT-EMNLP.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofNAACL.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of ACL.66
