Extending the BLEU MT Evaluation Method with Frequency WeightingsBogdan BabychCentre for Translation StudiesUniversity of LeedsLeeds, LS2 9JT, UKbogdan@comp.leeds.ac.ukAnthony HartleyCentre for Translation StudiesUniversity of LeedsLeeds, LS2 9JT, UKa.hartley@leeds.ac.ukAbstractWe present the results of an experimenton extending the automatic method ofMachine Translation evaluation BLUEwith statistical weights for lexical items,such as tf.idf scores.
We show that thisextension gives additional informationabout evaluated texts; in particular it al-lows us to measure translation Adequacy,which, for statistical MT systems, is oftenoverestimated by the baseline BLEUmethod.
The proposed model uses a sin-gle human reference translation, whichincreases the usability of the proposedmethod for practical purposes.
The modelsuggests a linguistic interpretation whichrelates frequency weights and human in-tuition about translation Adequacy andFluency.1.
IntroductionAutomatic methods for evaluating different as-pects of MT quality ?
such as Adequacy, Fluencyand Informativeness ?
provide an alternative toan expensive and time-consuming process ofhuman MT evaluation.
They are intended to yieldscores that correlate with human judgments oftranslation quality and enable systems (machineor human) to be ranked on this basis.
Severalsuch automatic methods have been proposed inrecent years.
Some of them use human referencetranslations, e.g., the BLEU method (Papineni etal., 2002), which is based on comparison ofN-gram models in MT output and in a set of hu-man reference translations.However, a serious problem for the BLEUmethod is the lack of a model for relative impor-tance of matched and mismatched items.
Wordsin text usually carry an unequal informationalload, and as a result are of differing importancefor translation.
It is reasonable to expect that thechoices of right translation equivalents for certainkey items, such as expressions denoting principalevents, event participants and relations in a textare more important in the eyes of human evalua-tors then choices of function words and a syntac-tic perspective for sentences.
Accurate renderingof these key items by an MT system boosts thequality of translation.
Therefore, at least forevaluation of translation Adequacy (Fidelity), theproper choice of translation equivalents for im-portant pieces of information should count morethan the choice of words which are used forstructural purposes and without a clear translationequivalent in the source text.
(The latter may bemore important for Fluency evaluation).The problem of different significance of N-gram matches is related to the issue of legitimatevariation in human translations, when certainwords are less stable than others across inde-pendently produced human translations.
BLEUaccounts for legitimate translation variation byusing a set of several human reference transla-tions, which are believed to be representative ofseveral equally acceptable ways of translatingany source segment.
This is motivated by theneed not to penalise deviations from the set of N-grams in a single reference, although the re-quirement of multiple human references makesautomatic evaluation more expensive.However, the ?significance?
problem is not di-rectly addressed by the BLEU method.
On theone hand, the matched items that are present inseveral human references receive the sameweights as items found in just one of the refer-ences.
On the other hand the model of legitimatetranslation variation cannot fully accommodatethe issue of varying degrees of ?salience?
formatched lexical items, since alternative syn-onymic translation equivalents may also behighly significant for an adequate translationfrom the human perspective (Babych and Hart-ley, 2004).
Therefore it is reasonable to suggestthat introduction of a model which approximatesintuitions about the significance of the matchedN-grams will improve the correlation betweenautomatically computed MT evaluation scoresand human evaluation scores for translation Ade-quacy.In this paper we present the result of an ex-periment on augmenting BLEU N-gram compari-son with statistical weight coefficients whichcapture a word?s salience within a given docu-ment: the standard tf.idf measure used in the vec-tor-space model for Information Retrieval (Saltonand Leck, 1968) and the S-score proposed forevaluating MT output corpora for the purposes ofInformation Extraction (Babych et al, 2003).Both scores are computed for each term in eachof the 100 human reference translations fromFrench into English available in DARPA-94 MTevaluation corpus (White et al, 1994).The proposed weighted N-gram model for MTevaluation is tested on a set of translations byfour different MT systems available in theDARPA corpus, and is compared with the resultsof the baseline BLEU method with respect totheir correlation with human evaluation scores.The scores produced by the N-gram modelwith tf.idf and S-Score weights are shown to beconsistent with baseline BLEU evaluation resultsfor Fluency and outperform the BLEU scores forAdequacy (where the correlation for the S-scoreweighting is higher).
We also show that theweighted model may still be reliably used if thereis only one human reference translation for anevaluated text.Besides saving cost, the ability to dependablywork with a single human translation has an addi-tional advantage: it is now possible to create Re-call-based evaluation measures for MT, whichhas been problematic for evaluation with multiplereference translations, since only one of thechoices from the reference set is used in transla-tion (Papineni et al 2002:314).
Notably, Recallof weighted N-grams is found to be a good esti-mation of human judgements about translationAdequacy.
Using weighted N-grams is essentialfor predicting Adequacy, since correlation of Re-call for non-weighted N-grams is much lower.It is possible that other automatic methodswhich use human translations as a reference mayalso benefit from an introduction of an explicitmodel for term significance, since so far thesemethods also implicitly assume that all words areequally important in human translation, and useall of them, e.g., for measuring edit distances(Akiba et al 2001; 2003).The weighted N-gram model has been imple-mented as an MT evaluation toolkit (which in-cludes a Perl script, example files anddocumentation).
It computes evaluation scoreswith tf.idf and S-score weights for translationAdequacy and Fluency.
The toolkit is available athttp://www.comp.leeds.ac.uk/bogdan/evalMT.html2.
Set-up of the experimentThe experiment used French?English transla-tions available in the DARPA-94 MT evaluationcorpus.
The corpus contains 100 French newstexts (each text is about 350 words long) trans-lated into English by 5 different MT systems:?Systran?, ?Reverso?, ?Globalink?, ?Metal?,?Candide?
and scored by human evaluators; thereare no human scores for ?Reverso?, which wasadded to the corpus on a later stage.
The corpusalso contains 2 independent human translationsof each text.
Human evaluation scores are avail-able for each of the 400 texts translated by the 4MT systems for 3 parameters of translation qual-ity: ?Adequacy?, ?Fluency?
and ?Informative-ness?.
The Adequacy (Fidelity) scores are givenon a 5-point scale by comparing MT with a hu-man reference translation.
The Adequacy pa-rameter captures how much of the originalcontent of a text is conveyed, regardless of howgrammatically imperfect the output might be.The Fluency scores (also given on a 5-pointscale) determine intelligibility of MT withoutreference to the source text, i.e., how grammati-cal and stylistically natural the translation ap-pears to be.
The Informativeness scores (whichwe didn?t use for our experiment) determinewhether there is enough information in MT out-put to enable evaluators to answer multiple-choice questions on its content (White, 2003:237)In the first stage of the experiment, each of thetwo sets of human translations was used to com-pute tf.idf and S-scores for each word in each ofthe 100 texts.
The tf.idf score was calculated as:tf.idf(i,j) = (1 + log (tfi,j)) log (N / dfi),if tfi,j ?
1; where:?
tfi,j is the number of occurrences of theword wi in the document dj;?
dfi is the number of documents in the cor-pus where the word wi occurs;?
N is the total number of documents in thecorpus.The S-score was calculated as: ( ))()()(),( /)(log),(icorpiidoccorpjidocPNdfNPPjiS??
?= ?where:?
Pdoc(i,j) is the relative frequency of theword in the text; (?Relative frequency?
isthe number of tokens of this word-typedivided by the total number of tokens).?
Pcorp-doc(i) is the relative frequency of thesame word in the rest of the corpus, with-out this text;?
(N ?
df(i)) / N is the proportion of texts inthe corpus, where this word does not oc-cur (number of texts, where it is notfound,  divided by number of texts in thecorpus);?
Pcorp(i) is the relative frequency of theword in the whole corpus, including thisparticular text.In the second stage we carried out N-gram basedMT evaluation, measuring Precision and Recallof N-grams in MT output using a single humanreference translation.
N-gram counts were ad-justed with the tf.idf weights and S-scores forevery matched word.
The following procedurewas used to integrate the S-scores / tf.idf scoresfor a lexical item into N-gram counts.
For everyword in a given text which received an S-scoreand tf.idf score on the basis of the human refer-ence corpus, all counts for the N-grams contain-ing this word are increased by the value of therespective score (not just by 1, as in the baselineBLEU approach).The original matches used for BLEU and theweighted matches are both calculated.
The fol-lowing changes have been made to the Perl scriptof the BLEU tool: apart from the operator whichincreases counts for every matched N-gram $ngrby 1, i.e.
:$ngr .= $words[$i+$j] . "
";$$hashNgr{$ngr}++;the following code was introduced:[?
]$WORD = $words[$i+$j];$WEIGHT = 0;if(exists$WordWeight{$TxtN}{$WORD}){$WEIGHT=$WordWeight{$TxtN}{$WORD};}$ngr .= $words[$i+$j] . "
";$$hashNgr{$ngr}++;$$hashNgrWEIGHTED{$ngr}+= $WEIGHT;[?]?
where the hash data structure:$WordWeight{$TxtN}{$WORD}=$WEIGHTrepresents the table of tf.idf scores or S-scores forwords in every text in the corpus.The weighted N-gram evaluation scores ofPrecision, Recall and F-measure may be pro-duced for a segment, for a text or for a corpus oftranslations generated by an MT system.In the third stage of the experiment theweighted Precision and Recall scores were testedfor correlation with human scores for the sametexts and compared to the results of similar testsfor standard BLEU evaluation.Finally we addressed the question whether theproposed MT evaluation method allows us to usea single human reference translation reliably.
Inorder to assess the stability of the weightedevaluation scores with a single reference, tworuns of the experiment were carried out.
The firstrun used the ?Reference?
human translation,while the second run used the ?Expert?
humantranslation (each time a single reference transla-tion was used).
The scores for both runs werecompared using a standard deviation measure.3.
The results of the MT evaluation withfrequency weightsWith respect to evaluating MT systems, the cor-relation for the weighted N-gram model wasfound to be stronger, for both Adequacy and Flu-ency, the improvement being highest for Ade-quacy.
These results are due to the fact that theweighted N-gram model gives much more accu-rate predictions about the statistical MT system?Candide?, whereas the standard BLEU approachtends to over-estimate its performance for trans-lation Adequacy.Table 1 present the baseline results for non-weighted Precision, Recall and F-score.
It showsthe following figures:?
Human evaluation scores for Adequacy andFluency (the mean scores for all texts producedby each MT system);?
BLEU scores produced using 2 human refer-ence translations and the default script settings(N-gram size = 4);?
Precision, Recall and F-score for the weightedN-gram model produced with 1 human refer-ence translation and N-gram size = 4.?
Pearson?s correlation coefficient r for Preci-sion, Recall and F-score correlated with humanscores for Adequacy and Fluency r(2) (with 2degrees of freedom) for the sets which includescores for the 4 MT systems.The scores at the top of each cell show the resultsfor the first run of the experiment, which used the?Reference?
human translation; the scores at thebottom of the cells represent the results for thesecond run with the ?Expert?
human translation.System[ade] / [flu]BLEU[1&2]Prec.1/2Recall1/2Fscore1/2CANDIDE0.677 / 0.4550.3561 0.40680.40120.38060.37900.39330.3898GLOBALINK0.710 / 0.3810.3199 0.34290.34140.34650.34840.34470.3449MS0.718 / 0.3820.3003 0.32890.32860.36500.36820.34600.3473REVERSONA / NA0.3823 0.39480.39230.40120.40250.39800.3973SYSTRAN0.789 / 0.5080.4002 0.40290.39810.41290.41180.40780.4049Corr r(2) with[ade] ?
MT0.59180.18090.18710.66910.69880.40630.4270Corr r(2) with[flu] ?
MT0.98070.90960.91240.95400.93530.98360.9869Table 1.
Baseline non-weighted scores.Table 2 summarises the evaluation scores forBLEU as compared to tf.idf weighted scores, andTable 3 summarises the same scores as comparedto S-score weighed evaluation.System[ade] / [flu]BLEU[1&2]Prec.
(w) 1/2Recall(w) 1/2Fscore(w) 1/2CANDIDE0.677 / 0.4550.3561 0.52420.51760.30940.30510.38920.3839GLOBALINK0.710 / 0.3810.3199 0.49050.48900.29190.29110.36600.3650MS0.718 / 0.3820.3003 0.49190.49020.30830.31000.37910.3798REVERSONA / NA0.3823 0.53360.53420.34000.34130.41540.4165SYSTRAN0.789 / 0.5080.4002 0.54420.53750.35210.34910.42760.4233Corr r(2) with[ade] ?
MT0.59180.52480.55610.83540.86670.76910.8119Corr r(2) with[flu] ?
MT0.98070.99870.99980.88490.83500.94080.9070Table 2.
BLEU vs tf.idf weighted scores.System[ade] / [flu]BLEU[1&2]Prec.
(w) 1/2Recall(w) 1/2Fscore(w) 1/2CANDIDE0.677 / 0.4550.3561 0.50340.49820.25530.25540.33880.3377GLOBALINK0.710 / 0.3810.3199 0.46770.46720.24640.24930.32280.3252MS0.718 / 0.3820.3003 0.47660.47930.26350.26790.33940.3437REVERSONA / NA0.3823 0.52040.52140.29300.29670.37490.3782SYSTRAN0.789 / 0.5080.4002 0.53140.52180.30340.30220.38630.3828Corr r(2) with[ade] ?
MT0.59180.60550.61370.90690.92150.85740.8792Corr r(2) with[flu] ?
MT0.98070.99120.97690.80220.74990.87150.8247Table 3.
BLEU vs S-score weights.It can be seen from the table that there is astrong positive correlation between the baselineBLEU scores and human scores for Fluency:r(2)=0.9807, p <0.05.
However, the correlationwith Adequacy is much weaker and is not statis-tically significant: r(2)= 0.5918, p >0.05.
Themost serious problem for BLEU is predictingscores for the statistical MT system Candide,which was judged to produce relatively fluent,but largely inadequate translation.
For other MTsystems (developed with the knowledge-basedMT architecture) the scores for Adequacy andFluency are consistent with each other: more flu-ent translations are also more adequate.
BLEUscores go in line with Candide?s Fluency scores,but do not account for its Adequacy scores.When Candide is excluded from the evaluationset, r correlation goes up, but it is still lower thanthe correlation for Fluency and remains statisti-cally insignificant: r(1)=0.9608, p > 0.05.
There-fore, the baseline BLEU approach fails toconsistently predict scores for Adequacy.Correlation figures between non-weighted N-gram counts and human scores are similar to theresults for BLEU: the highest and statisticallysignificant correlation is between the F-score andFluency: r(2)=0.9836, p<0.05, r(2)=0.9869,p<0.01, and there is somewhat smaller and statis-tically significant correlation with Precision.
Thisconfirms the need to use modified Precision inthe BLEU method that also in certain respect in-tegrates Recall.The proposed weighted N-gram model outper-forms BLEU and non-weighted N-gram evalua-tion in its ability to predict Adequacy scores:weighted Recall scores have much stronger cor-relation with Adequacy (which for MT-onlyevaluation is still statistically insignificant at thelevel p<0.05, but come very close to that point:t=3.729 and t=4.108; the required value forp<0.05 is t=4.303).Correlation figures for S-score-based weightsare higher than for tf.idf weights (S-score: r(2)=0.9069, p > 0.05; r(2)= 0.9215, p > 0.05, tf.idfscore: r(2)= 0.8354, p >0.05; r(2)= 0.8667, p>0.05).The improvement in the accuracy of evalua-tion for the weighted N-gram model can be illus-trated by the following example of translating theFrench sentence:ORI-French: Les trente-huit chefs d'entre-prise mis en examen dans le dossier ont d?j?fait l'objet d'auditions, mais trois d'entre euxont ?t?
confront?s, mercredi, dans la foul?e dela confrontation "politique".English translations of this sentence by theknowledge-based system Systran and statisticalMT system Candide have an equal number ofmatched unigrams (highlighted in italic), there-fore conventional unigram Precision and Recallscores are the same for both systems.
However,for each translation two of the matched unigramsare different (underlined) and receive differentfrequency weights (shown in brackets):MT ?Systran?
:The thirty-eight heads (tf.idf=4.605; S=4.614) ofundertaking put in examination in the file alreadywere the subject of hearings, but three of themwere confronted, Wednesday, in the tread of "po-litical" confrontation (tf.idf=5.937; S=3.890).Human translation ?Expert?
:The thirty-eight heads of companies ques-tioned in the case had already been heard, butthree of them were brought together Wednes-day following the "political" confrontation.MT ?Candide?
:The thirty-eight counts of company put into con-sideration in the case (tf.idf=3.719; S=2.199) al-ready had (tf.idf=0.562; S=0.000) the object ofhearings, but three of them were checked,Wednesday, in the path of confrontal "political.
"(In the human translation the unigrams matchedby the Systran output sentence are in italic, thosematched by the Candide sentence are in bold).It can be seen from this example that the uni-grams matched by Systran have higher term fre-quency weights (both tf.idf and S-scores):heads (tf.idf=4.605;S=4.614)confrontation (tf.idf=5.937;S=3.890)The output sentence of Candide insteadmatched less salient unigrams:case (tf.idf=3.719;S=2.199)had (tf.idf=0.562;S=0.000)Therefore for the given sentence weighted uni-gram Recall (i.e., the ability to avoid under-generation of salient unigrams) is higher forSystran than for Candide (Table 4):Systran CandideR 0.6538 0.6538R * tf.idf 0.5332 0.4211R * S-score 0.5517 0.3697P 0.5484 0.5484P * tf.idf 0.7402 0.9277P * S-score 0.7166 0.9573Table 4.
Recall, Precision, and weighted scoresWeighted Recall scores capture the intuition thatthe translation generated by Systran is more ade-quate than the one generated by Candide, since itpreserves more important pieces of information.On the other hand, weighted Precision scoresare higher for Candide.
This is due to the fact thatSystran over-generates (doesn?t match in the hu-man translation) much more ?exotic?, unordinarywords, which on average have higher cumulativesalience scores, e.g., undertaking, exami-nation, confronted, tread ?
vs. thecorresponding words ?over-generated?
by Can-dide: company, consideration,checked, path.
In some respect higherweighted precision can be interpreted as higherFluency of the Candide?s output sentence, whichintuitively is perceived as sounding more natu-rally (although not making much sense).On the level of corpus statistics the weightedRecall scores go in line with Adequacy, andweighted Precision scores (as well as the Preci-sion-based BLEU scores) ?
with Fluency, whichconfirms such interpretation of weighted Preci-sion and Recall scores in the example above.
Onthe other hand, Precision-based scores and non-weighted Recall scores fail to capture Adequacy.The improvement in correlation for weightedRecall scores with Adequacy is achieved by re-ducing overestimation for the Candide system,moving its scores closer to human judgementsabout its quality in this respect.
However, this isnot completely achieved: although in terms ofRecall weighted by the S-scores Candide is cor-rectly ranked below MS (and not ahead of it, aswith the BLEU scores), it is still slightly ahead ofGlobalink, contrary to human evaluation results.For both methods ?
BLEU and the WeightedN-gram evaluation ?
Adequacy is found to beharder to predict than Fluency.
This is due to thefact that there is no good linguistic model oftranslation adequacy which can be easily formal-ised.
The introduction of S-score weights may bea useful step towards developing such a model,since correlation scores with Adequacy are muchbetter for the Weighted N-gram approach thanfor BLEU.Also from the linguistic point of view, S-scoreweights and N-grams may only be reasonablygood approximations of Adequacy, which in-volves a wide range of factors, like syntactic andsemantic issues that cannot be captured by N-gram matches and require a thesaurus and otherknowledge-based extensions.
Accurate formalmodels of translation variation may also be use-ful for improving automatic evaluation of Ade-quacy.The proposed evaluation method also pre-serves the ability of BLEU to consistently predictscores for Fluency: Precision weighted by tf.idfscores has the strongest positive correlation withthis aspect of MT quality, which is slightly betterthan the values for BLEU; (S-score: r(2)=0.9912, p<0.01; r(2)= 0.9769, p<0.05; tf.idfscore: r(2)= 0.9987, p<0.001; r(2)= 0.9998,p<0.001).The results suggest that weighted Precisiongives a good approximation of Fluency.
Similarresults with non-weighted approach are onlyachieved if some aspect of Recall is integratedinto the evaluation metric (either as modified pre-cision, as in BLEU, or as an aspect of the F-score).
Weighted Recall (especially with S-scores) gives a reasonably good approximation ofAdequacy.On the one hand using 1 human reference withuniform results is essential for our methodology,since it means that there is no more ?trouble withRecall?
(Papineni et al, 2002:314) ?
a system?sability to avoid under-generation of N-grams cannow be reliably measured.
On the other hand,using a single human reference translation in-stead of multiple translations will certainly in-crease the usability of N-gram based MTevaluation tools.The fact that non-weighted F-scores also havehigh correlation with Fluency suggests a newlinguistic interpretation of the nature of these twoquality criteria: it is intuitively plausible that Flu-ency subsumes, i.e.
presupposes Adequacy (simi-larly to the way the F-score subsumes Recall,which among all other scores gives the best cor-relation with Adequacy).
The non-weighted F-score correlates more strongly with Fluency thaneither of its components: Precision and Recall;similarly Adequacy might make a contribution toFluency together with some other factors.
It isconceivable that people need adequate transla-tions (or at least translations that make sense) inorder to be able to make judgments about natu-ralness, or Fluency.Being able to make some sense out of a textcould be the major ground for judging Adequacy:sensible mistranslations in MT are relatively rareevents.
This may be the consequence of a princi-ple similar to the ?second law of thermodynam-ics?
applied to text structure, ?
in practice it ismuch rarer to some alternative sense to be cre-ated (even if the number of possible error typescould be significant), than to destroy the existingsense in translation, so the majority of inadequatetranslations are just nonsense.
However, in con-trast to human translation, fluent mistranslationsin MT are even rarer than disfluent ones, accord-ing to the same principle.
A real difference inscores is made by segments which make senseand may or may not be fluent, and things whichdo not make any sense and about which it is hardto tell whether they are fluent.This suggestion may be empirically tested: ifAdequacy is a necessary precondition for Flu-ency, there should be a greater inter-annotatordisagreement in Fluency scores on texts or seg-ments which have lower Adequacy scores.
Thiswill be a topic of future research.We note that for the DARPA corpus the corre-lation scores presented are highest if the evalua-tion unit is an entire corpus of translationsproduced by an MT system, and for text-levelevaluation, correlation is much lower.
A similarobservation was made in (Papineni et al, 2002:313).
This may be due to the fact that humanjudges are less consistent, especially for puzzlingsegments that do not fit the scoring guidelines,like nonsense segments for which it is hard todecide whether they are fluent or even adequate.However, this randomness is leveled out if theevaluation unit increases in size ?
from the textlevel to the corpus level.Automatic evaluation methods such as BLEU(Papineni et al, 2002), RED (Akiba et al, 2001),or the weighted N-gram model proposed heremay be more consistent in judging quality ascompared to human evaluators, but human judg-ments remain the only criteria for meta-evaluating the automatic methods.4.
Stability of weighted evaluation scoresIn this section we investigate how reliable is theuse of a single human reference translation.
Thestability of the scores is central to the issue ofcomputing Recall and reducing the cost of auto-matic evaluation.
We also would like to comparethe stability of our results with the stability of thebaseline non-weighted N-gram model using asingle reference.In this stage of the experiment we measuredthe changes that occur for the scores of MT sys-tems if an alternative reference translation is used?
both for the baseline N-gram counts and for theweighted N-gram model.
Standard deviation wascomputed for each pair of evaluation scores pro-duced by the two runs of the system with alterna-tive human references.
An average of thesestandard deviations is the measure of stability fora given score.
The results of these calculationsare presented in Table 5.systems StDev-baslnStDev-tf.idfStDev-S-scoreP candide 0.004 0.0047 0.0037globalink 0.0011 0.0011 0.0004ms 0.0002 0.0012 0.0019reverso 0.0018 0.0004 0.0007systran 0.0034 0.0047 0.0068AVE SDEV 0.0021 0.0024 0.0027R candide 0.0011 0.003 0.0001globalink 0.0013 0.0006 0.0021ms 0.0023 0.0012 0.0031reverso 0.0009 0.0009 0.0026systran 0.0008 0.0021 0.0008AVE SDEV 0.0013 0.0016 0.0017F candide 0.0025 0.0037 0.0008globalink 0.0001 0.0007 0.0017ms 0.0009 0.0005 0.003reverso 0.0005 0.0008 0.0023systran 0.0021 0.003 0.0025AVE SDEV 0.0012 0.0018 0.0021Table 5.
Stability of scoresStandard deviation for weighted scores is gener-ally slightly higher, but both the baseline and theweighted N-gram approaches give relatively sta-ble results: the average standard deviation wasnot greater than 0.0027, which means that bothwill produce reliable figures with just a singlehuman reference translation (although interpreta-tion of the score with a single reference should bedifferent than with multiple references).Somewhat higher standard deviation figuresfor the weighted N-gram model confirm the sug-gestion that a word?s importance for translationcannot be straightforwardly derived from themodel of the legitimate translation variation im-plemented in BLEU and needs the salienceweights, such as tf.idf or S-scores.5.
Conclusion and future workThe results for weighted N-gram models have asignificantly higher correlation with human intui-tive judgements about translation Adequacy andFluency than the baseline N-gram evaluationmeasures which are used in the BLEU MTevaluation toolkit.
This shows that they are apromising direction of research.
Future work willapply our approach to evaluating MT into lan-guages other than English, extending the experi-ment to a larger number of MT systems built ondifferent architectures and to larger corpora.However, the results of the experiment mayalso have implications for MT development: sig-nificance weights may be used to rank the rela-tive ?importance?
of translation equivalents.
Atpresent all MT architectures (knowledge-based,example-based, and statistical) treat all transla-tion equivalents equally, so MT systems cannotdynamically prioritise rule applications, andtranslations of the central concepts in texts areoften lost among excessively literal translationsof less important concepts and function words.For example, for statistical MT significanceweights of lexical items may indicate whichwords have to be introduced into the target textusing the translation model for source and targetlanguages, and which need to be brought there bythe language model for the target corpora.
Simi-lar ideas may be useful for the Example-basedand Rule-based MT architectures.
The generalidea is that different pieces of information ex-pressed in the source text are not equally impor-tant for translation: MT systems that have nomeans for prioritising this information often in-troduce excessive information noise into the tar-get text by literally translating structuralinformation, etymology of proper names, collo-cations that are unacceptable in the target lan-guage, etc.
This information noise often obscuresimportant translation equivalents and preventsthe users from focusing on the relevant bits.
MTquality may benefit from filtering out this exces-sive information as much as from frequently rec-ommended extension of knowledge sources forMT systems.
The significance weights mayschedule the priority for retrieving translationequivalents and motivate application of compen-sation strategies in translation, e.g., adding ordeleting implicitly inferable information in thetarget text, using non-literal strategies, such astransposition or modulation (Vinay and Darbel-net, 1995).
Such weights may allow MT systemsto make an approximate distinction between sali-ent words which require proper translationequivalents and structural material both in thesource and in the target texts.
Exploring applica-bility of this idea to various MT architectures isanother direction for future research.AcknowledgmentsWe are very grateful for the insightful commentsof the three anonymous reviewers.ReferencesAkiba, Y., K. Imamura and E. Sumita.
2001.
Using mul-tiple edit distances to automatically rank machinetranslation output.
In Proc.
MT Summit VIII.
p. 15?20.Akiba, Y., E. Sumita, H. Nakaiwa, S. Yamamoto andH.G.
Okuno.
2003.
Experimental Comparison of MTEvaluation Methods: RED vs. BLEU.
In Proc.
MTSummit IX, URL: http://www.amtaweb.org/summit/MTSummit/ FinalPapers/55-Akiba-final.pdf.Babych, B., A. Hartley and E. Atwell.
2003.
StatisticalModelling of MT output corpora for Information Ex-traction.
In: Proceedings of the Corpus Linguistics2003 conference, Lancaster University (UK), 28 - 31March 2003, pp.
62-70.Babych, B. and A. Hartley.
2004.
Modelling legitimatetranslation variation for automatic evaluation of MTquality.
In: Proceedings of LREC 2004 (forthcoming).Papineni, K., S. Roukos, T. Ward, W.-J.
Zhu.
2002BLEU: a method for automatic evaluation of machinetranslation.
Proceedings of the 40th Annual Meeting ofthe Association for the Computational Linguistics(ACL), Philadelphia, July 2002, pp.
311-318.Salton, G. and M.E.
Lesk.
1968.
Computer evaluation ofindexing and text processing.
Journal of the ACM,15(1) , 8-36.Vinay, J.P. and J.Darbelnet.
1995.
Comparative stylisticsof French and English : a methodology for translation/ translated and edited by Juan C. Sager, M.-J.
Hamel.J.
Benjamins Pub., Amsterdam, Philadelphia.White, J., T. O?Connell and F. O?Mara.
1994.
TheARPA MT evaluation methodologies: evolution, les-sons and future approaches.
Proceedings of the 1stConference of the Association for Machine Transla-tion in the Americas.
Columbia, MD, October 1994.pp.
193-205.White, J.
2003.
How to evaluate machine translation.
In:H. Somers.
(Ed.)
Computers and Translation: a trans-lator?s guide.
Ed.
J. Benjamins B.V., Amsterdam,Philadelphia, pp.
211-244.
