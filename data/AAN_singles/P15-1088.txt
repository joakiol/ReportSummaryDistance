Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 909?919,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMultiple Many-to-Many Sequence Alignment for CombiningString-Valued Variables: A G2P ExperimentSteffen EgerText Technology LabGoethe University Frankfurt am MainFrankfurt am Main, Germanysteeger@em.uni-frankfurt.deAbstractWe investigate multiple many-to-manyalignments as a primary step in integrat-ing supplemental information strings instring transduction.
Besides outlining DPbased solutions to the multiple alignmentproblem, we detail an approximation ofthe problem in terms of multiple sequencesegmentations satisfying a coupling con-straint.
We apply our approach to boostingbaseline G2P systems using homogeneousas well as heterogeneous sources of sup-plemental information.1 IntroductionString-to-string translation (string transduction) isthe problem of converting one string x over analphabet ?
into another string y over a possi-bly different alphabet ?.
The most prominentapplications of string-to-string translation in nat-ural language processing (NLP) are grapheme-to-phoneme conversion, in which x is a letter-string and y is a string of phonemes, translit-eration (Sherif and Kondrak, 2007), lemmatiza-tion (Dreyer et al, 2008), and spelling error cor-rection (Brill and Moore, 2000).
The classi-cal learning paradigm in each of these settingsis to train a model on pairs of strings {(x,y)}and then to evaluate model performance on testdata.
Thereby, all state-of-the-art modelings weare aware of (e.g., (Jiampojamarn et al, 2007;Bisani and Ney, 2008; Jiampojamarn et al, 2008;Jiampojamarn et al, 2010; Novak et al, 2012))proceed by first aligning the string pairs (x,y)in the training data.
Also, these modelings ac-knowledge that alignments may typically be of arather complex nature in which several x sequenceph oe n i xf i n I ksTable 1: Sample monotone many-to-many align-ment between x = phoenix and y = finIks.characters may be matched up with several y se-quence characters; Table 1 illustrates.
Once thetraining data is aligned, since x and y sequencesare then segmented into equal number of seg-ments, string-to-string translation may be seen asa sequence labeling (tagging) problem in which x(sub-)sequence characters are observed variablesand y (sub-)sequence characters are hidden states(Jiampojamarn et al, 2007; Jiampojamarn et al,2010).In this work, we extend the problem of classi-cal string-to-string translation by assuming that, attraining time, we have available (M + 2)-tuplesof strings {(x,?y(1), .
.
.
,?y(M),y)}, where x is theinput string,?y(m), for 1 ?
m ?
M , are sup-plemental information strings, and y is the de-sired output string; at test time, we wish to pre-dict y from (x,?y(1), .
.
.
,?y(M)).
Generally, wemay think of?y(1), .
.
.
,?y(M)as arbitrary stringsover arbitrary alphabets ?
(m), for 1 ?
m ?
M .For example, x might be a letter-string and?y(m)might be a transliteration of x in language Lm(cf.Bhargava and Kondrak (2012)).
Alternatively, andthis is our model scenario in the current work, xmight be a letter input string and?y(m)might bethe predicted string of phonemes, given x, pro-duced by an (offline) system Tm.
This situationis outlined in Table 3.
In the table, we also illus-trate a multiple (monotone) many-to-many align-ment of (x,?y(1), .
.
.
,?y(M),y).
By this, we meanan alignment where (1) subsequences of all M +2strings may be matched up with each other (many-909to-many alignments), and where (2) the match-ing up of subsequences obeys monotonicity.
Notethat such a multiple alignment generalizes classi-cal monotone many-to-many alignments betweenpairs of strings, as shown in Table 1.
Furthermore,such an alignment may apparently be quite useful.For instance, while none of the strings?y(m)in thetable equals the true phonetic transcription y of x,taking a position-wise majority vote of the multi-ple alignment of (?y(1), .
.
.
,?y(M)) yields y. More-over, analogously as in the case of pairs of alignedstrings, we may perceive the so extended string-to-string translation problem as a sequence label-ing task once (x,?y(1), .
.
.
,?y(M),y) are multiplyaligned, but now, with additional observed vari-ables (or features), namely, (sub-)sequence char-acters of each string?y(m).To further motivate our approach, consider thesituation of training a new G2P system on the ba-sis of, e.g., Combilex (Richmond et al, 2009).For each letter form in its database, Combilexprovides a corresponding phonetic transcription.Now, suppose that, in addition, we can poll anexternal knowledge source such as Wiktionary for(its) phonetic transcriptions of the respective Com-bilex letter words as outlined in Table 2.
The cen-Input form Wiktionary Combilexneutrino nju:t?i:noU nutrinFwooded wUdId wUd@dwrench ?En?S rEn<Table 2: Input letter words, Wiktionary and Com-bilex transcriptions.tral question we want to answer is: can we traina system using this additional information whichperforms better than the ?baseline?
system that ig-nores the extra information?
Clearly, a systemwith more information should not perform worsethan a system with less information (unless the ad-ditional information is highly noisy), but it is apriori not clear at all how the extra informationcan be included, as Bhargava and Kondrak (2012)note: output predictions may be in distinct alpha-bets and/or follow different conventions, and sim-ple rule-based conversions may even deterioratea baseline system?s performance.
Their solutionto the problem is to let the baseline system out-put its n-best phonetic transcriptions, and then tore-rank these n-best predictions via an SVM re-ranker trained on the supplemental representationsx = schizo s ch i z o?y(1)= skaIz@U s k aI z @U?y(2)= saIz@U s - aI z @U?y(3)= skIts@ s k I ts @?y(4)= Sits@U S - i ts @U?y(5)= skIts@ s k I ts @y = skIts@U s k I ts @UTable 3: Left: Input string x, predictions of 5systems, and output string y.
Right: A multiplemany-to-many alignment of (x,?y(1), .
.
.
,?y(5),y).Skips are marked by a dash (?-?).
(see their figure 2).
Our approach is much differ-ent from this: we character (or substring) alignthe supplemental information strings with the in-put letter strings and then sequentially transduceinput character substrings as in the standard G2Papproach, but where the sequential transducer isaware of the corresponding subsequences of thesupplemental information strings.Our goals in the current work are first, in Sec-tion 2, to formally introduce the multiple many-to-many alignment problem, which, to our knowl-edge, has not yet been formally considered, andto indicate how it can be solved (by standard ex-tensions of well-known DP recursions).
Secondly,we outline an ?approximation algorithm?, also inSection 2, with much better runtime complexity,to solving the multiple many-to-many alignmentproblem.
This proceeds by optimally segmentingindividual strings to align under the global con-straint that the number of segments must agreeacross strings.
Thirdly, we demonstrate exper-imentally, in Section 5, that multiple many-to-many alignments may be an extremely useful firststep in boosting the performance of a G2P model.In particular, we show that by conjoining a basesystem with additional systems very high perfor-mance increases can be achieved.
We also inves-tigate the effects of using our introduced approxi-mation algorithm instead of ?exactly?
determiningalignments.
We discuss related work in Section3, present data and systems in Section 4 and con-clude in Section 6.2 Mult.
Many-to-Many Alignm.
ModelsWe now formally define the problem of multiplyaligning several strings in a monotone and many-to-many alignment manner.
For notational conve-nience, in this section, let the N strings to align be910denoted by w1, .
.
.
,wN(rather than x,?y(m),y,etc.).
Let each wn, for 1 ?
n ?
N , be an arbitrarystring over some alphabet ?(n).
Let `n= |wn| de-note the length of wn.
Moreover, assume that a setS ?
?Nn=1{0, .
.
.
, `n}\{0N} of allowable stepsis specified, where 0N= (0, .
.
.
, 0?
??
?N times).1We interpretthe elements of S as follows: if (s1, s2, .
.
.
, sN) ?S, then subsequences of w1of length s1, subse-quences of w2of length s2, .
.
., subsequences ofwNof length sNmay be matched up with eachother.
In other words, S defines the types of valid?many-to-many match-up operations?.2While wecould drop S from consideration and simply al-low every possible matching up of character sub-sequences, it is convenient to introduce S becausealgorithmic complexity may then be specified interms of S, and by choosing particular S, one mayretrieve special cases otherwise considered in theliterature (see next section).As indicated, for us, a multiple alignment of(w1, .
.
.
,wN) is any schemew1,1w1,2?
?
?
w1,kw2,1w2,2?
?
?
w2,k............wN,1wN,2?
?
?
wN,ksuch that (|w1,i| , .
.
.
, |wN,i|) ?
S, for all i =1, .
.
.
, k, and such that wn= wn,1?
?
?wn,k, forall 1 ?
n ?
N .
Let AS= AS(w1, .
.
.
,wN)denote the set of all multiple alignments of(w1, .
.
.
,wN).
For an alignment a ?
AS, de-note by score(a) = f(a) the score of align-ment a under alignment model f , where f :AS(w1, .
.
.
,wN)?
R. We now investigate solu-tions to the problem of finding the alignment withmaximal score under different choices of align-ment models f , i.e., we search to efficiently solvemaxa?AS(w1,...,wN)f(a).
(1)Unigram alignment model For our first align-ment model f , we assume that f(a), for a ?
AS,is the scoref(a) =k?i=1sim1(w1,i, .
.
.
,wN,i) (2)1Here,?denotes the Cartesian product of sets.2In the case of two strings, this is sometimes denoted inthe manner M -N (e.g., 3-2, 1-0), indicating that M charac-ters of one string may be matched up with N characters of theother string.
Analogously, we could write here s1-s2-s3-?
?
?
.for a real-valued similarity function sim1:?Nn=1(?(n))??
R. We call the model f in(2) a unigram model because f(a) is the sumof the similarity scores of the matched-up subse-quences (w1,i, .
.
.
,wN,i), ignoring context.
Dueto this independence assumption, solving max-imization problem in Eq.
(1) under specifica-tion (2) is straightforward via a dynamic pro-gramming (DP) recursion.
To do so, define byMS,sim1(i1, i2, .
.
.
, iN) the score of the best align-ment, under alignment model f =?sim1andset of steps S, of (w1(1 : i1), .
.
.
,wN(1 : iN)).3Then, MS,sim1(i1, .
.
.
, iN) is equal tomax(j1,...,jN)?SMS,sim1(i1?
j1, .
.
.
, iN?
jN)+ sim1(w(i1?
j1+ 1 : i1), .
.
.
,w(iN?
jN+ 1 : jN)).
(3)This recurrence directly leads to a DP algorithm,shown in Algorithm 1, for computing the scoreof the best alignment of (w1, .
.
.
,wN); the ac-tual alignment can be found by storing pointers tothe maximizing steps taken.
If similarity evalua-tions sim1(w1,i, .
.
.
,wN,i) are thought of as tak-ing constant time, this algorithm?s run time isO(?Nn=1`n?
|S|).
When ` = `1= ?
?
?
= `nand|S| = `N?
1 (?worst case?
size of S), then the al-gorithm?s runtime is thus O(`2N), which quicklybecomes untractable as N , the number of stringsto align, increases.Of course, the unigram alignment model couldbe generalized to an m-gram alignment model.An m-gram alignment model would exhibit worst-case runtime complexity of O(`(m+1)N) underanalogous DP recursions as for the unigrammodel.Algorithm 11: procedure UNIGRAM-ALIGN(w1, .
.
.
,wN;S, sim1)2: M(i1, .
.
.
, iN) ?
??
for all(i1, .
.
.
, iN) ?
ZN3: M(0N)?
04: for i1= 0 .
.
.
`1do5: for ?
?
?
do6: for iN= 0 .
.
.
`Ndo7: if (i1, .
.
.
, iN) 6= 0Nthen8: M(i1, .
.
.
, iN)?
Eq.
(3)9: return M(`1, .
.
.
, `N)Separable alignment models For our sec-ond model class, assume that, for any a ?3We denote by x(a : b) the substring xaxa+1?
?
?xbofthe string x1x2?
?
?xt.911AS(w1, .
.
.
,wN), f(a) decomposes intof(a) = ?(fw1(w1,1?
?
?w1,k), .
.
.
, fwN(wN,1?
?
?wN,k))(4)for some models fw1, .
.
.
, fwNand where ?
:RN?
R is non-decreasing in its arguments (e.g.,?
(fw1, .
.
.
, fwN) =?Nn=1fwn).
If f(a) decom-poses in such a manner, then f(a) is called sep-arable.4The advantage with separable models isthat we can solve the ?subproblems?
fw1, .
.
.
, fwNindependently.
Thus, in order to find optimalmultiple alignments of (w1, .
.
.
,wN) under sucha specification, we would only have to find thebest segmentations of sequences wnunder mod-els fwn, for 1 ?
n ?
N , subject to the constraintthat the segmentations must agree in their numberof segments (the coupling variable).
Let Swn?
{0, 1, .
.
.
, `n} denote the constraints on segmentlengths, similar to the interpretation of steps inS.
If fwnis a unigram segmentation model thenthe problem of finding the best segmentation ofwnwith exactly j segments can be solved in timeO(`n|Swn| j).
Thus, if each fwnis a unigramsegmentation model, worst-case time complexityfor each subproblem would be O(`3n) (if stringwncan be segmented into at most `nsegments)and then the overall problem (1) under specifica-tion (4) is solvable in worst-case time N ?
O(`3).More generally, if each fwnis an m-gram seg-mentation model, then worst-case time complexityamounts to N ?
O(`m+2).
Importantly, this scaleslinearly with the number N of strings to align,rather than exponentially as the O(`(m+1)N) un-der the (non-separable) m-gram alignment modeldiscussed above.Unsupervised alignments The algorithms pre-sented may be applied iteratively in order to in-duce multiple alignments in an unsupervised (EM-like) fashion in which sim1is gradually learnt(e.g., starting from a uniform initialization ofsim1).
We skip details of this, as we do not makeus of it in our current experiments.
Rather, in ourexperiments below, we directly specify sim1as asum of pairwise similarity scores which we ex-tract from alignments produced by an off-the-shelfpairwise aligner.4Note the difference between Eqs.
(2) and (4).
While eachfwnin (4) operates on a ?row?
of an alignment scheme, sim1in (2) acts on the ?columns?.
In other words, the unigramalignment model correlates the multiply matched-up subse-quences, while the separable alignment model assumes inde-pendence here.3 Related workMonotone alignments have a long tradition, bothin NLP and bioinformatics.
The classicalNeedleman-Wunsch algorithm (Needleman andWunsch, 1970) computes the optimal alignmentbetween two sequences when only single charac-ter matches, mismatches, and skips are allowed.It is a special case of the unigram model (2)in optimization problem (1) for which N = 2,S = {(1, 0), (0, 1), (1, 1)} and sim1takes on val-ues from {0,?1}, depending on whether com-pared input subsequences match or not.
As iswell-known, this alignment specification is equiv-alent to the edit distance problem (Levenshtein,1966) in which the minimal number of inser-tions, deletions and substitutions is sought thattransforms one string into another.
Substring-to-substring edit operations ?
or equivalently,(monotone) many-to-many alignments ?
have ap-peared in the NLP context, e.g., in (Deligne etal., 1995), (Brill and Moore, 2000), (Jiampoja-marn et al, 2007), (Bisani and Ney, 2008), (Ji-ampojamarn et al, 2010), or, significantly earlier,in (Ukkonen, 1985), (V?eronis, 1988).
Learningedit distance/monotone alignments in an unsuper-vised manner has been the topic of, e.g., (Ris-tad and Yianilos, 1998), (Cotterell et al, 2014),besides the works already mentioned.
All ofthese approaches are special cases of our uni-gram model outlined in Section 2 ?
i.e., theyconsider particular S (most prominently, S ={(1, 0), (0, 1), (1, 1)}) and/or restrict attention toonly N = 2 strings.5Alignments between multiple sequences, i.e.,multiple sequence alignment, has also been an is-sue both in NLP (e.g., Covington (1998), Bhar-gava and Kondrak (2009)) and bioinformatics(e.g., Durbin et al (1998)).
An interesting applica-tion of alignments of multiple sequences is to de-termine what has been called median string (Ko-honen, 1985) or Steiner consensus string (Gus-field, 1997), defined as the string?s that minimizesthe sum of distances, for a given distance functiond(x,y), to a list of strings s1, .
.
.
, sN(Jiang et al,2012); typically, d is the standard edit distance.As Gusfield (1997) shows, the Steiner consen-sus string may be retrieved from a multiple align-5In Cotterell et al (2014), context influences alignments,so that the approach goes beyond the unigram model sketchedin (2), but there, too, the focus is on the situation N = 2 andS = {(1, 0), (0, 1), (1, 1)}.912ment of s1, .
.
.
, sNby concatenating the column-wise majority characters in the alignment, ignor-ing skips.
Since median string computation (andhence also the multiple many-to-many alignmentproblem, as we consider) is an NP-hard problem(Sim and Park, 2003), designing approximations isan active field of research.
For example, Marti andBunke (2001) ignore part of the search space bydeclaring matches-up of distant characters as un-likely, and Jiang et al (2012) apply an approxima-tion based on string embeddings in vector spaces.Paul and Eisner (2012) apply dual decompositionto compute Steiner consensus strings.
Via the ap-proach taken in this paper, median strings may becomputed in case d is a (distance) function tak-ing substring-to-substring edit operations into ac-count, a seemingly straightforward, yet extremelyuseful generalization in several NLP applications,as indicated in the introduction.Our approach may also be seen in the context ofclassifier combination for string-valued variables.While ensemble methods for structured predictionhave been considered in several works (see, e.g.,Nguyen and Guo (2007), Cortes et al (2014), andreferences therein), a typical assumption in thissituation is that the sequences to be combined haveequal length, which clearly cannot be expectedto hold when, e.g., the outputs of several G2P,transliteration, etc., systems must be combined.
Infact, the multiple many-to-many alignment modelsinvestigated in this work could act as a preprocess-ing step in this setup, since the alignment preciselyserves the functionality of segmenting the stringsinto equal number of segments/substructures.
Ofcourse, combining outputs with varying numberof elements is also an issue in machine transla-tion (e.g., Macherey and Och (2007), Heafield etal.
(2009)), but, there, the problem is harder due tothe potential non-monotonicities in the ordering ofelements, which typically necessitates (additional)heuristics.
One approach for constructing multi-ple alignments is here progressive multiple align-ment (Feng and Doolittle, 1987) in which a multi-ple (typically one-to-one) alignment is iterativelyconstructed from successive pairwise alignments(Bangalore et al, 2001).
Matusov et al (2006)apply word reordering and subsequent pairwisemonotone one-to-one alignments for MT systemcombination.4 Data and systems4.1 DataWe conduct experiments on the General Ameri-can (GA) variant of the Combilex data set (Rich-mond et al, 2009).
This contains about 144,000grapheme-phoneme pairs as exemplarily illus-trated in Table 2.
In our experiments, we splitthe data into two disjoint parts, one for test-ing (about 28,000 word pairs) and one for train-ing/development (the remainder).4.2 SystemsBASELINE Our baseline system is a linear-chainconditional random field model (CRF)6(Laffertyet al, 2001) which we apply in the manner in-dicated in the introduction: after many-to-manyaligning the training data as in Table 1, at trainingtime, we use the CRF as a tagging model that istrained to label each input character subsequencewith an output character subsequence.
As fea-tures for the CRF, we use all n-grams of subse-quences of x that fit inside a window of size 5centered around the current subsequence (contextfeatures).
We also include linear-chain featureswhich allow previously generated output charactersubsequences to influence current output charac-ter subsequences.
In essence, our baseline modelis a standard discriminative approach to G2P.
It is,all in all, the same approach as described in Ji-ampojamarn et al (2010), except that we do notinclude joint n-gram features.
At test time, we firstsegment a new input string x and then apply theCRF.
Thereby, we train the segmentation moduleon the segmented x sequences, as available fromthe aligned training data.7BASELINE+X As competitors for the base-line system, we introduce systems that rely onthe predictions of one or several additional (blackbox/offline) systems.
At training time, we firstmultiply many-to-many align the input string x,the predictions?y(1), .
.
.
,?y(M)and the true tran-scription y as illustrated in Table 3 (see Section4.3 for details).
Then, as for the baseline sys-tem, we train a CRF to label each input character6We made use of the CRF++ package available athttps://code.google.com/p/crfpp/.7To be more precise on the training of the segmentationmodule, in an alignment as in Table 1, we consider the seg-mented x string ?
ph-oe-n-i-x ?
and then encode this seg-mentation in a binary string where 1?s indicate splits.
Thus,segmentation becomes, again, a sequence labling task; see,e.g., Bartlett et al (2008) or Eger (2013) for details.913subsequence with the corresponding output char-acter subsequence.
However, this time, the CRFhas access to the subsequence suggestions (as thealignments indicate) produced by the offline sys-tems.
As features for the extended models, we ad-ditionally include context features for all predictedstrings?y(m)(all n-grams in a window of size 3centered around the current subsequence predic-tion).
We also include a joint feature firing onthe tuple of the current subsequence value of x,?y(1), .
.
.
,?y(M).
To illustrate, when BASELINE+Xtags position 2 in the (split up) input string in Ta-ble 3, it sees that its value is ch, that the previousinput position contains s, that the next containsi, that the next two contain (i,z), that the predic-tion of the first system at position 2 is k, that thefirst system?s next prediction is ai, and so forth.At test time, we first multiply many-to-many alignx,?y(1), .
.
.
,?y(M), and then apply the enhancedCRF.4.3 AlignmentsTo induce multiple monotone many-to-manyalignments of input strings, offline system predic-tions and output strings, we proceed in one of twomanners.Exact alignments Firstly, we specify sim1inEq.
(2), as sim1(xi,?y(1)i, .
.
.
,?y(M)i,yi) =(M?m=1psim(xi,?y(m)i))+ psim(xi,yi),where psim is a pair-similarity function.
The ad-vantage with this specification is that the similarityof a tuple of subsequences is defined as the sum ofpairwise similarity scores, which we can directlyestimate from pairwise alignments of (x,?y(m))that an off-the-shelf pairwise aligner can produce(we use the Phonetisaurus aligner for this).
We setpsim(u,v) as log-probability of observing the tu-ple (u,v) in the training data of pairwise alignedsequences.
To illustrate, we define the similar-ity of (o,@U,@U,@,@U,@,@U) in the example in Table3 as the pairwise similarity of (o,@U) (as inferredfrom pairwise alignments of x strings and sys-tem 1 transcriptions) plus the pairwise similarityof (o,@U) (as inferred from pairwise alignments ofx strings and system 2 transcriptions), etc.
At testtime, we use the same procedure but drop the termpsim(xi,yi) when inducing alignments.
For ourcurrent purposes, we label the outlined modus asexact (alignment) modus.Approx.
alignments Secondly, we derive theoptimal multiple many-to-many alignment of thestrings in question by choosing an alignment thatsatisfies the condition that (1) each individualstring x,?y(1), .
.
.
,?y(M),y is optimally segmented(e.g., ph-oe-n-i-x rather than pho-eni-x, f-i-n-I-ksrather than f-inIk-s) subject to the global constraintthat (2) the number of segments must agree acrossthe strings to align.
This constitutes a separa-ble alignment model as discussed in Section 2,and thus has much lower runtime complexity asthe first model.
Segmentation models can be di-rectly learned from the pairwise alignments thatPhonetisaurus produces by focusing on either thesegmented x or y/?y(m)sequences; we choose toimplement bigram individual segmentation mod-els.
This second model type may be considered anapproximation of the first, since in a good align-ment, we would not only expect individually goodsegmentations and agreement of segment numbersbut also that subsegments are likely correlationsof each other, precisely as our first model typecaptures.
Therefore, we shall call this alignmentmodus approximate (alignment) modus, for ourpresent purposes.5 ExperimentsWe now describe two sets of experiments, a con-trolled experiment on the Combilex data setwhere we can design our offline/black box sys-tems ourselves and where the black box systemsare trained on a similar distribution as the base-line and the extended baseline systems.
In partic-ular, the black box systems operate on the sameoutput alphabet as the extended baseline systems,which constitutes an ?ideal?
situation.
Thereafter,we investigate how our extended baseline systemperforms in a ?real-world?
scenario: we train asystem on Combilex that has as supplemental in-formation corresponding Wiktionary (and PTE, asexplained below) transcriptions.Throughout, we use as accuracy measures forall our systems word accuray (WACC).
Word ac-curacy is defined as the number of correctly tran-scribed strings among all transcribed strings in atest sample.
WACC is a strict measure that penal-izes even tiny deviations from the gold-standardtranscriptions, but has nowadays become standardin G2P.9145.1 A controlled experimentIn our first set of experiments, we let our of-fline/black box systems be the Sequitur G2P mod-eling toolkit (Bisani and Ney, 2008) (S) andthe Phonetisaurus modeling toolkit (Novak etal., 2012) (P).
We train them on disjoint setsof 20,000 grapheme-to-phoneme Combilex stringpairs each.
The performance of these two sys-tems, on the test set of size 28,000, is indicatedin Table 4.
Next, we train BASELINE on dis-Phonetisaurus SequiturWACC 72.12 71.70Table 4: Word-accuracy (in %) on the test data, forthe two systems indicated.joint sets (disjoint from both the training sets ofP and S) of size 2,000, 5,000, 10,000 and 20,000.Making BASELINE?s training sets disjoint fromthe training sets of the offline systems is both re-alistic (since a black box system would typicallyfollow a partially distinct distribution from one?sown training set distribution) and also preventsthe extended baseline systems from fully adaptingto the predictions of either P or S, whose train-ing set accuracy is an upward biased representa-tion of their true accuracy.
As baseline extensions,we consider the systems BASELINE+P (+P), andBASELINE+P+S (+P+S).8Results are shown in Figures 1 and 2.
Wesee that conjoining the base system with thepredictions of the offline Phonetisaurus and Se-quitur models substantially increases the base-line WACC, especially in the case of little train-ing data.
In fact, WACC increases here by al-most 100% when the baseline system is comple-mented by?y(P)and?y(S).
As training set sizeincreases, differences become less and less pro-nounced.
Eventually, we would expect them todrop to zero, since beyond some training set size,the additional features may provide no new infor-mation.9We also note that conjoining the two sys-tems is more valuable than conjoining only onesystem, and, in Figure 2, that the models which arebased on exact multiple alignments outperform themodels based on approximate alignments, but not8We omit BASELINE+S since it yielded similar results asBASELINE+P.9In fact, in follow-up work, we find that the additionalinformation may also confuse the base system when trainingset sizes are large enough.by a wide margin.0.30.350.40.450.50.550.60.650.70.750.80 5T 10T 20T 30TAccuracyTraining set sizeBASELINE+P+P+SFigure 1: WACC as a function of training set sizefor the system indicated.
Exact align.
modus.0.670.680.690.70.710.720.730.740.750.760 5T 10T 20T 30TAccuracyTraining set size+P+P+S+PAPRX+P+SAPRXFigure 2: Comparison of models based on exactand approximate alignments; WACC as a functionof training set size.
APRX denotes the approxima-tion alignment model.Concerning differences in alignments betweenthe two alignment types, exact vs. approximate, anillustrative example where the approximate modelfails and the exact model does not is (?false?
align-ment based on the approximate model indicated):r ee n t e r e dr i E n t @?
r dr i E n t @?
r dwhich nicely captures the inability of the approx-imate model to account for correlations betweenthe matched-up subsequences.
That is, while thesegmentations of the three shown sequences ap-pear acceptable, a matching of graphemic t with915phonemic n, etc., seems quite unlikely.
Still, itis very promising to see that these differences inalignment quality translate into very small differ-ences in overall string-to-string translation modelperformance, as Figure 2 outlines.
Namely, dif-ferences in WACC are typically on the level of1% or less (always in favor of the exact alignmentmodel).
This is a very important finding, as it in-dicates that string-to-string translation need not be(severely) negatively impacted by switching to theapproximate alignment model, a tractable alterna-tive to the exact models, which quickly becomepractically infeasible as the number of strings toalign increases.5.2 Real-world experimentsTo test whether our approach may also succeed ina ?real-world setting?, we use as offline/black boxsystems GA Wiktionary transcriptions of our in-put forms as well as PhotoTransEdit (PTE) tran-scriptions,10a lexicon-based G2P system whichoffers both GA and RP (received pronunciation)transcription of English strings.
We train and teston input strings for which both Combilex and PTEtranscriptions are available, and for which bothCombilex and Wiktionary transcriptions are avail-able.11Test set sizes are about 1,500 in the case ofPTE and 3,500 in the case of Wiktionary.
We onlytest here the performance of the exact alignmentmethod, noting that, as before, approximate align-ments produced slightly weaker results.Clearly, Wiktionary and PTE differ from theCombilex data.
First, both Wiktionary and PTEuse different numbers of phonemic symbols thanCombilex, as Table 5 illustrates.
Some differencesDataset |?|Combilex 54WiktionaryGA107WiktionaryRP116PTEGA44PTERP57Table 5: Sizes of phonetic inventaries of differentdata sets.arise from the fact that, e.g., lengthening of vowelsis indicated by two output letters in some data sets10Downloadable from http://www.photransedit.com/.11This yields a clear method of comparison.
An alternativewould be to provide predictions for missing transcriptions.
Inany case, by our task definition, all systems must provide ahypothesis for an input string.and only one in others.
Also, phonemic transcrip-tion conventions differ, as becomes most strikinglyevident in the case of RP vs. GA transcriptions ?Table 6 illustrates.
Finally, Wiktionary has manymore phonetic symbols than the other datasets, afinding that we attribute to its crowd-sourced na-ture and lacking of normalization.
Despite thesedifferences in phonemic annotation standards be-tween Combilex, Wiktionary and PTE, we observethat conjoining input strings with predicted Wik-tionary or PTE transcriptions via multiple align-ments leads to very good improvements in WACCover only using the input string as informationsource.
Indeed, as shown in Table 7, for PTE,WACC increases by as much as 80% in case ofsmall training sample (1,099 string pairs) and asmuch as 37% in case of medium-sized trainingsample (2,687 string pairs).
Thus, comparing withthe previous situation of homogenous systems, wealso observe that the gain from including hetero-geneous system is relatively weaker, as we wouldexpect due to distinct underlying assumptions, butstill impressive.
Performance increases when in-cluding Wiktionary are slightly lower, most likelybecause it constitutes a very heterogenous sourceof phonetic transcriptions with user-idiosyncraticannotations (however, training set sizes are alsodifferent).12BASEL.
BASEL.+PTEGABASEL.+PTERP1,099 31.34 56.47 50.222,687 45.75 60.80 62.80BASEL.
BASEL.+WikGABASEL.+WikRP2,000 38.44 60.71 62.185,000 51.69 65.81 65.9610,000 58.97 67.30 68.66Table 7: Top: WACC in % for baseline CRFmodel and the models that integrate PTE in theGA versions and RP versions, respectively.
Bot-tom: BASELINE and BASELINE+Wiktionary.6 ConclusionWe have generalized the task description of stringtransduction to include supplemental informationstrings.
Moreover, we have suggested multiple12To provide, for the interested reader, a comparison withPhonetisaurus and Sequitur: for the Wiktionary GA data,performance of Phonetisaurus is 41.80% (training set size2,000), 55.70% (5,000) and 62.47% (10,000).
Respectivenumbers for Sequitur are 40.58%, 54.84%, and 61.58%.
OnPTE, results are, similarly, slightly higher than our baseline,but substantially lower than the extended baseline.916b o t ch i ngb o t S I Nb A - tS I Nb a rr edb a - db A r da s th m a t i c s?
s - m ?
t I k sa z 0 m a t I k sTable 6: Multiple alignments of input string, predicted PTE transcription and true (Combilex) transcrip-tion.
Differences may be due to alternative phonemic conventions (e.g., Combilex has a single phonemiccharacter representing the sound tS) and/or due to differences in pronunciation in GA and RP, resp.many-to-many alignments ?
and a subsequentstandardly extended discriminative approach ?for solving string transduction (here, G2P) in thisgeneralized setup.
We have shown that, in a real-world setting, our approach may significantly beata standard discriminative baseline, e.g., when weadd Wiktionary transcriptions or predictions ofa rule-based system as additional information tothe input strings.
The appeal of this approachlies in the fact that almost any sort of externalknowledge source may be integrated to improvethe performance of a baseline system.
For exam-ple, supplemental information strings may appearin the form of transliterations of an input stringin other languages; they may be predictions ofother G2P systems, whether carefully manuallycrafted or learnt from data; they might even ap-pear in the form of phonetic transcriptions of theinput string in other dialects or languages.
Whatdistinguishes our solution to integrating supple-mental information strings in string transductionsettings from other research (e.g., (Bhargava andKondrak, 2011; Bhargava and Kondrak, 2012)) isthat rather than integrating systems on the globallevel of strings, we integrate them on the lo-cal level of smaller units, namely, substrings ap-propriated to the domain of application (e.g., inour context, phonemes/grapheme substructures).Both approaches may be considered complemen-tary.
Finally, another important contribution of ourwork is to outline an ?approximation algorithm?to inducing multiple many-to-many alignments ofstrings, which is otherwise an NP-hard problemfor which (most likely) no efficient exact solu-tions exist, and to investigate its suitability for theproblem task.
In particular, we have seen that ex-act alignments lead to better overall model perfor-mance, but that the margin over the approximationis not wide.The scope for future research of our modeling ishuge: multiple many-to-many alignments may beuseful in aligning cognates in linguistic research;they may be the first necessary step for many otherensemble techniques in string transduction as wehave considered (Cortes et al, 2014), and theymay allow, on a large scale, to boost G2P (translit-eration, lemmatization, etc.)
systems by inte-grating them with many traditional (or modern)knowledge resources such as rule- and dictionary-based lemmatizers, crowd-sourced phonetic tran-scriptions (e.g., based on Wiktionary), etc., withthe outlook of significantly outperforming currentstate-of-the-art models which are based solely oninput string information.Finally, we note that we have thus far shownthat supplemental information strings may be ben-eficial in case of overall little training data and thatimprovements decrease with data size.
Further in-vestigating this relationship will be of importance.Morevoer, it will be insightful to compare theexact and approximate alignment algorithms pre-sented here with other (heuristic) alignment meth-ods, such as iterative pairwise alignments as em-ployed in machine translation, and to investigatehow alignment quality of multiple strings impactsoverall G2P performance in the setup of additionalinformation strings.ReferencesS.
Bangalore, G. Bodel, and G. Riccardi.
2001.
Com-puting consensus translation from multiple machinetranslation systems.
In In Proceedings of IEEEAutomatic Speech Recognition and UnderstandingWorkshop (ASRU-2001, pages 351?354.Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.2008.
Automatic syllabification with structuredsvms for letter-to-phoneme conversion.
In Kath-leen McKeown, Johanna D. Moore, Simone Teufel,James Allan, and Sadaoki Furui, editors, ACL, pages568?576.
The Association for Computer Linguis-tics.Aditya Bhargava and Grzegorz Kondrak.
2009.
Mul-tiple word alignment with Profile Hidden MarkovModels.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Companion Volume: Student Re-search Workshop and Doctoral Consortium, pages91743?48, Boulder, Colorado, June.
Association forComputational Linguistics.Aditya Bhargava and Grzegorz Kondrak.
2011.
Howdo you pronounce your name?
: Improving g2p withtransliterations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11, pages 399?408, Stroudsburg, PA, USA.Association for Computational Linguistics.Aditya Bhargava and Grzegorz Kondrak.
2012.
Lever-aging supplemental representations for sequentialtransduction.
In HLT-NAACL, pages 396?406.
TheAssociation for Computational Linguistics.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.In Proceedings of the 38th Annual Meeting on As-sociation for Computational Linguistics, ACL ?00,pages 286?293, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Corinna Cortes, Vitaly Kuznetsov, and Mehryar Mohri.2014.
Ensemble methods for structured prediction.In Proceedings of the 31th International Conferenceon Machine Learning, ICML 2014, Beijing, China,21-26 June 2014, pages 1134?1142.Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2014.Stochastic contextual edit distance and probabilis-tic FSTs.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL), Baltimore, June.
6 pages.Michael A. Covington.
1998.
Alignment of multiplelanguages for historical comparison.
In Proceedingsof the 36th Annual Meeting of the Association forComputational Linguistics and 17th InternationalConference on Computational Linguistics, Volume1, pages 275?279, Montreal, Quebec, Canada, Au-gust.
Association for Computational Linguistics.Sabine Deligne, Franois Yvon, and Fr?ed?eric Bimbot.1995.
Variable-length sequence matching for pho-netic transcription using joint multigrams.
In EU-ROSPEECH.
ISCA.Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductionswith finite-state methods.
In EMNLP, pages 1080?1089.
ACL.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological SequenceAnalysis: Probabilistic Models of Proteins and Nu-cleic Acids.
Cambridge University Press.Steffen Eger.
2013.
Sequence segmentation by enu-meration: An exploration.
Prague Bull.
Math.
Lin-guistics, 100:113?132.D.
F. Feng and R. F. Doolittle.
1987.
Progressive se-quence alignment as a prerequisite to correct phy-logenetic trees.
Journal of molecular evolution,25(4):351?360.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences - Computer Science and ComputationalBiology.
Cambridge University Press.Kenneth Heafield, Greg Hanneman, and Alon Lavie.2009.
Machine translation system combinationwith flexible word ordering.
In Proceedings of theEACL 2009 Fourth Workshop on Statistical MachineTranslation, pages 56?60, Athens, Greece, March.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phonemeconversion.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 372?379, Rochester, New York, April.
Association forComputational Linguistics.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In Pro-ceedings of ACL-08: HLT, pages 905?913, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2010.
Integrating joint n-gram featuresinto a discriminative training framework.
In HLT-NAACL, pages 697?700.
The Association for Com-putational Linguistics.Xiaoyi Jiang, Jran Wentker, and Miquel Ferrer.
2012.Generalized median string computation by means ofstring embedding in vector spaces.
Pattern Recog-nition Letters, 33(7):842?852.T.
Kohonen.
1985.
Median strings.
Pattern Recogni-tion Letters, 3:309?313.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc.
18th International Conf.
onMachine Learning, pages 282?289.
Morgan Kauf-mann, San Francisco, CA.VI Levenshtein.
1966.
Binary Codes Capable of Cor-recting Deletions, Insertions and Reversals.
SovietPhysics Doklady, 10:707.Wolfgang Macherey and Franz Josef Och.
2007.
Anempirical study on computing consensus transla-tions from multiple machine translation systems.
InEMNLP-CoNLL, pages 986?995.
ACL.Urs-Viktor Marti and Horst Bunke.
2001.
Use of posi-tional information in sequence alignment for multi-ple classifier combination.
In Josef Kittler and FabioRoli, editors, Multiple Classifier Systems, volume9182096 of Lecture Notes in Computer Science, pages388?398.
Springer.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multi-ple machine translation systems using enhanced hy-potheses alignment.
In Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 33?40, Trento, Italy, April.Saul B. Needleman and Christian D. Wunsch.
1970.A general method applicable to the search for sim-ilarities in the amino acid sequence of two pro-teins.
Journal of Molecular Biology, 48(3):443?453, March.Nam Nguyen and Yunsong Guo.
2007.
Comparisonsof sequence labeling algorithms and extensions.
InZoubin Ghahramani, editor, ICML, volume 227 ofACM International Conference Proceeding Series,pages 681?688.
ACM.Josef R. Novak, Nobuaki Minematsu, and Keikichi Hi-rose.
2012.
WFST-based grapheme-to-phonemeconversion: Open source tools for alignment,model-building and decoding.
In Proceedings of the10th International Workshop on Finite State Meth-ods and Natural Language Processing, pages 45?49,Donostia?San Sebastin, July.
Association for Com-putational Linguistics.Michael J. Paul and Jason Eisner.
2012.
Implicitly in-tersecting weighted automata using dual decompo-sition.
In HLT-NAACL, pages 232?242.
The Associ-ation for Computational Linguistics.Korin Richmond, Robert A. J. Clark, and Susan Fitt.2009.
Robust LTS rules with the Combilex speechtechnology lexicon.
In INTERSPEECH, pages1295?1298.
ISCA.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string-edit distance.
IEEE Trans.
Pattern Anal.Mach.
Intell., 20(5):522?532.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In John A. Carroll, Antalvan den Bosch, and Annie Zaenen, editors, ACL.The Association for Computational Linguistics.Jeong Seop Sim and Kunsoo Park.
2003.
The consen-sus string problem for a metric is np-complete.
J. ofDiscrete Algorithms, 1(1):111?117, February.Esko Ukkonen.
1985.
Algorithms for approximatestring matching.
Information and Control, 64:100?118.Jean V?eronis.
1988.
Computerized correction ofphonographic errors.
Computers and the Humani-ties, 22(1):43?56.919
