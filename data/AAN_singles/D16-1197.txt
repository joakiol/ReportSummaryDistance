Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1918?1923,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNeural Generation of Regular Expressions from Natural Languagewith Minimal Domain KnowledgeNicholas LocascioCSAIL, MITnjl@mit.eduKarthik NarasimhanCSAIL, MITkarthikn@mit.eduEduardo DeLeonCSAIL, MITedeleon4@mit.eduNate KushmanMicrosoftnate@kushman.orgRegina BarzilayCSAIL, MITregina@csail.mit.eduAbstractThis paper explores the task of translating nat-ural language queries into regular expressionswhich embody their meaning.
In contrast toprior work, the proposed neural model doesnot utilize domain-specific crafting, learningto translate directly from a parallel corpus.To fully explore the potential of neural mod-els, we propose a methodology for collectinga large corpus1 of regular expression, naturallanguage pairs.
Our resulting model achievesa performance gain of 19.6% over previousstate-of-the-art models.1 IntroductionThis paper explores the task of translating natu-ral language text queries into regular expressionswhich embody their meaning.
Regular expressionsare built into many application interfaces, yet mostusers of these applications have difficulty writingthem (Friedl, 2002).
Thus a system for automat-ically generating regular expressions from naturallanguage would be useful in many contexts.
Fur-thermore, such technologies can ultimately scale totranslate into other formal representations, such asprogram scripts (Raza et al, 2015).Prior work has demonstrated the feasibility of thistask.
Kushman and Barzilay (2013) proposed amodel that learns to perform the task from a parallelcorpus of regular expressions and the text descrip-tions.
To account for the given representational dis-parity between formal regular expressions and natu-ral language, their model utilizes a domain specific1The corpus and code used in this paper is available at https://github.com/nicholaslocascio/deep-regexcomponent which computes the semantic equiva-lence between two regular expressions.
Since theirmodel relies heavily on this component, it cannotbe readily applied to other formal representationswhere such semantic equivalence calculations arenot possible.In this paper, we reexamine the need for such spe-cialized domain knowledge for this task.
Given thesame parallel corpus used in Kushman and Barzi-lay (2013), we use an LSTM-based sequence to se-quence neural network to perform the mapping.
Ourmodel does not utilize semantic equivalence in anyform, or make any other special assumptions aboutthe formalism.
Despite this and the relatively smallsize of the original dataset (824 examples), our neu-ral model exhibits a small 0.1% boost in accuracy.To further explore the power of neural networks,we created a much larger public dataset, NL-RX.Since creation of regular expressions requires spe-cialized knowledge, standard crowd-sourcing meth-ods are not applicable here.
Instead, we employa two-step generate-and-paraphrase procedure thatcircumvents this problem.
During the generatestep, we use a small but expansive manually-craftedgrammar that translates regular expression into nat-ural language.
In the paraphrase step, we rely oncrowd-sourcing to paraphrase these rigid descrip-tions into more natural and fluid descriptions.
Usingthis methodology, we have constructed a corpus of10,000 regular expressions, with corresponding ver-balizations.Our results demonstrate that our sequence to se-quence model significantly outperforms the domainspecific technique on the larger dataset, reaching a1918gain of 19.6% over of the state-of-the-art technique.2 Related WorkRegular Expressions from Natural LanguageThere have been several attempts at generating reg-ular expressions from textual descriptions.
Early re-search into this task used rule-based techniques tocreate a natural language interface to regular expres-sion writing (Ranta, 1998).
Our work, however,is closest to Kushman and Barzilay (2013).
Theylearned a semantic parsing translation model from aparallel dataset of natural language and regular ex-pressions.
Their model used a regular expression-specific semantic unification technique to disam-biguate the meaning of the natural language descrip-tions.
Our method is similar in that we require onlydescription and regex pairs to learn.r However, wetreat the problem as a direct translation task withoutapplying any domain-specific knowledge.Neural Machine Translation Recent advances inneural machine translation (NMT) (Bahdanau et al,2014; Devlin et al, 2014) using the framework of se-quence to sequence learning (Sutskever et al, 2014)have demonstrated the effectiveness of deep learn-ing models at capturing and translating language se-mantics.
In particular, recurrent neural networksaugmented with attention mechanisms (Luong etal., 2015) have proved to be successful at handlingvery long sequences.
In light of these successes, wechose to model regular expression generation as aneural translation problem.3 Regex Generation as TranslationWe use a Recurrent Neural Network (RNN) with at-tention (Mnih et al, 2014) for both encoding anddecoding (Figure 1).Let W = w1, w2...wm be the input text descrip-tion where each wi is a word in the vocabulary.
Wewish to generate the regex R = r1, r2, ...rn whereeach ri is a character in the regex.We use Long Short-Term Memory(LSTM) (Hochreiter and Schmidhuber, 1997)cells in our model, the transition equations forwhich can be summarized as:it = ?
(U (i)xt + V (i)ht?1 + b(i)),ft = ?
(U (f)xt + V (f)ht?1 + b(f)),ot = ?
(U (o)xt + V (o)ht?1 + b(o))zt = tanh(U (z)xt + V (z)ht?1 + b(z))ct = it  zt + ft  ct?1ht = ot  tanh(ct)(1)where ?
represents the sigmoid function and is el-ementwise multiplication.
it refers to the input gate,ft is the forget gate, and ot is the output gate at eachtime step.
The U and V variables are weight matri-ces for each gate while the b variables are the biasparameters.
The input xt is a word (wt) for the en-coder and the previously generated character rt?1for the decoder.The attention mechanism is essentially a ?soft?weighting over the encoder?s hidden states duringdecoding:?t(e) =exp(score(ht, he))?e?
exp(score(ht, he?
))where he is a hidden state in the encoder and scoreis the scoring function.
We use the general attentionmatrix weight (as described in (Luong et al, 2015))for our scoring function.
The outputs of the decoderrt are generated using a final softmax layer.Our model is six layers deep, with one word em-bedding layer, two encoder layers, two decoder lay-ers, and one dense output layer.
Our encoder and de-coder layers use a stacked LSTM architecture witha width of 512 nodes.
We use a global attentionmechanism (Bahdanau et al, 2014), which consid-ers all hidden states of the encoder when comput-ing the model?s context vector.
We perform standarddropout during training (Srivastava et al, 2014) afterevery LSTM layer with dropout probability equal to0.25.
We train for 20 epochs, utilizing a minibatchsize of 32, and a learning-rate of 1.0.
The learningrate is decayed by a factor 0.5 if evaluation perplex-ity does not increase.4 Creating a Large Corpus of NaturalLanguage / Regular Expression PairsPrevious work in regular expression generation hasused fairly small datasets for training and evaluation.1919Figure 1: Deep-Regex Encoder-Decoder setup.
Bluecells represent the encoder and the green ones representthe decoder.Non-Terminalsx & y?
x and y x | y?
x or y ?(x)?
not x.*x.*y?
x followed by y .*x.*?
contains x x{N,} ?
x, N or more timesx& y& z?
x and y and z x | y | z?
x or y or z x{1,N} ?
x, at most N timesx.*?
starts with x .*x?
ends with x \b x\b?
words with x(x)+?
x, at least once (x)*?
x, zero or more times x?
only xTerminals[AEIOUaeiou]?
a vowel [0-9]?
a number word?
the string ?word?[A-Z]?
an uppercase letter [a-z]?
a lowercase letter .
?
a characterTable 1: Regex?
Synthetic Grammar for Data Genera-tionIn order to fully utilize the power of neural transla-tion models, we create a new large corpus of regularexpression, natural language pairs titled NL-RX.The challenge in collecting such corpora is thattypical crowdsourcing workers do not possess thespecialized knowledge to write regular expressions.To solve this, we employ a two-step generate-and-paraphrase procedure to gather our data.
This tech-nique is similar to the methods used by Wang et al(2015) to create a semantic parsing corpus.In the generate step, we generate regular expres-sion representations from a small manually-craftedgrammar (Table 1).
Our grammar includes 15 non-terminal derivations and 6 terminals and of bothbasic and high-level operations.
We identify thesevia frequency analysis of smaller datasets from pre-vious work (Kushman and Barzilay, 2013).
Ev-ery grammar rule has associated verbalizations forboth regular expressions and language descriptions.We use this grammar to stochastically generate reg-ular expressions and their corresponding syntheticlanguage descriptions.
This generation process isshown in Figure 2.While the automatically generated descriptionsare semantically correct, they do not exhibit rich-ness and variability of human-generated descrip-tions.
To obtain natural language (non-synthetic)descriptions, we perform the paraphrase step.
Inthis step, Mechanical Turk (Amazon, 2003) humanworkers paraphrase the generated synthetic descrip-Figure 2: Process for generating Synthetic Descriptionsfrom Regular Expressions.
Grammar rules from Table 1are applied to a node?s children and the resulting string ispassed to the node?s parent.Synthetic: lines not words with starting with a capital letterParaphrased: lines that do not contain words that beginwith a capital letterRegex: ?
(\b([A-Z])(.
*)\b)Table 2: NL-RX Text Descriptions and Regular Expres-siontions into the fluent verbalizations.NL-RX Using the procedure described above, wecreate a new public dataset (NL-RX) comprising of10,000 regular expressions and their correspondingnatural language descriptions.
Table 2 shows an ex-ample from our dataset.Our data collection procedure enables us to createa substantially larger and more varied dataset thanpreviously possible.
Employing standard crowd-source workers to paraphrase is more cost-efficientand scalable than employing professional regex pro-grammers, enabling us to create a much largerdataset.
Furthermore, our stochastic generation ofregular expressions from a grammar results in amore varied dataset because it is not subject to thebias of human workers who, in previous work, wrotemany duplicate examples (see Results).Corpora Statistics Our seed regular expressiongrammar (Table 1), covers approximately 85% ofthe original KB13 regular expressions.
Addition-ally, NL-RX contains exact matches with 30.1% ofthe original KB13 dataset regular expressions.
Thismeans that 248 of the 824 regular expressions in the1920Verbalization Frequency?the word x?
12.6%?x before y?
9.1%?x or y?
7.7%?x, at least once?
6.2%?a vowel?
5.3%Table 3: Top Frequent Verbalizations from NL-RXKB13 dataset were also in our dataset.
The aver-age length of regular expressions in NL-RX is 25.9characters, the average in the KB13 dataset is 19.7characters.
We also computed the grammar break-down of our NL-RX.
The top 5 occurring terminalsin our generated regular expressions are those cor-responding with the verbalizations shown in Table3.Crowdsourcing details We utilize MechanicalTurk for our crowdsource workers.
A total of 243workers completed the 10,000 tasks, with an averagetask completion time of 101 seconds.
The workersproved capable of handling complex and awkwardphrasings, such as the example in Table 2, which isone of the most difficult in the set.We applied several quality assurance measures onthe crowdsourced data.
Firstly, we ensured that ourworkers performing the task were of high quality, re-quiring a record of 97% accuracy over at least 1000other previous tasks completed on Mechanical Turk.In addition, we ran automatic scripts that filtered outbad submissions (e.g.
submissions shorter than 5characters).
In all, we rejected 1.1% of submissions,which were resubmitted for another worker to com-plete.
The combination of these measures ensured ahigh quality dataset, and we confirmed this by per-forming a manual check of 100 random examples.This manual check determined that approximately89% of submissions were a correct interpretation,and 97% were written in fluent English.5 ExperimentsDatasets We split the 10,000 regexp and descrip-tion pairs in NL-RX into 65% train, 10% dev, and25% test sets.In addition, we also evaluate our model onthe dataset used by Kushman and Barzilay (2013)(KB13), although it contains far fewer data points(824).
We use the 75/25 train/test split used in theirwork in order directly compare our performance totheirs.Training We perform a hyper-parameter grid-search (on the dev set), to determine our modelhyper-parameters: learning-rate = 1.0, encoder-depth = 2, decoder-depth = 2, batch size = 32,dropout = 0.25.
We use a Torch (Collobert et al,2002) implementation of attention sequence to se-quence networks from (Kim, 2016).
We train ourmodels on the train set for 20 epochs, and choosethe model with the best average loss on the dev set.Evaluation Metric To accurately evaluate ourmodel, we perform a functional equality checkcalled DFA-Equal.
We employ functional equalitybecause there are many ways to write equivalent reg-ular expressions.
For example, (a|b) is functionallyequivalent to (b|a), despite their string representa-tions differing.
We report DFA-Equal accuracy asour model?s evaluation metric, using Kushman andBarzilay (2013)?s implementation to directly com-pare our results.Baselines We compare our model against twobaselines:BoW-NN: BoW-NN is a simple baseline that isa Nearest Neighbor classifier using Bag Of Wordsrepresentation for each natural language description.For a given test example, it finds the closest cosine-similar neighbor from the training set and uses theregexp from that example for its prediction.Semantic-Unify: Our second baseline, Semantic-Unify, is the previous state-of-the-art model from(Kushman and Barzilay, 2013), explained above.
26 ResultsOur model significantly outperforms the baselineson the NL-RX dataset and achieves comparable per-formance to Semantic Unify on the KB13 dataset(Table 4).
Despite the small size of KB13, ourmodel achieves state-of-the-art results on this veryresource-constrained dataset (824 examples).
UsingNL-RX, we investigate the impact of training datasize on our model?s accuracy.
Figure 3 shows how2We trained and evaluated Semantic-Unify in consultationwith the original authors.1921Models NL-RX-Synth NL-RX-Turk KB13Dev Test Dev Test TestBoW NN 31.7% 30.6% 18.2% 16.4% 48.5%Semantic-Unify 41.2% 46.3% 39.5% 38.6% 65.5%Deep-RegEx 85.75% 88.7% 61.2% 58.2% 65.6%Table 4: DFA-Equal accuracy on different datasets.KB13: Dataset from Kushman and Barzilay(2013), NL-RX-Synth: NL Dataset with original synthetic descrip-tions, NL-RX-Turk: NL Dataset with Mechanical Turkparaphrased descriptions.
Best scores are in bold.Figure 3: Our model?s performance, like many deeplearning models, increases significantly with largerdatasets.
String-Equal:Accuracy on direct string match,DFA-Equal:Accuracy using the DFA-Equal evaluation.our model?s performance improves as the number oftraining examples grows.Differences in Datasets Keeping the previoussection in mind, a seemingly unusual finding isthat the model?s accuracy is higher for the smallerdataset, KB13, than for the larger dataset, NL-RX-Turk.
On further analysis, we learned that the KB13dataset is a much less varied and complex datasetthan NL-RX-Turk.
KB13 contains many dupli-cates, with only 45% of its regular expressions be-ing unique.
This makes the translation task easierbecause over half of the correct test predictions willbe exact repetitions from the training set.
In con-trast, NL-RX-Turk does not suffer from this vari-ance problem and contains 97% unique regular ex-pressions.
The relative easiness of the KB13 datasetis further illustrated by the high performance of theNearest-Neighbor baselines on the KB13 dataset.7 ConclusionsIn this paper we demonstrate that generic neu-ral architectures for generating regular expressionsoutperform customized, heavily engineered mod-els.
The results suggest that this technique canbe employed to tackle more challenging problemsin broader families of formal languages, such asmapping between language description and programscripts.
We also have created a large parallel corpusof regular expressions and natural language queriesusing typical crowd-sourcing workers, which wemake available publicly.AcknowledgmentsWe thank the anonymous reviewers for their helpfulfeedback and suggestions.References[Amazon2003] Amazon.
2003.
Mechanical turk.
https://mturk.com.
[Bahdanau et al2014] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2014.
Neural machinetranslation by jointly learning to align and translate.CoRR, abs/1409.0473.
[Collobert et al2002] Ronan Collobert, Samy Bengio,and Johnny Marithoz.
2002.
Torch: A modular ma-chine learning software library.
https://torch.ch.
[Devlin et al2014] Jacob Devlin, Rabih Zbib,Zhongqiang Huang, Thomas Lamar, Richard MSchwartz, and John Makhoul.
2014.
Fast and robustneural network joint models for statistical machinetranslation.
In ACL (1), pages 1370?1380.
Citeseer.
[Friedl2002] Jeffrey EF Friedl.
2002.
Mastering regularexpressions.
?
O?Reilly Media, Inc.?.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJu?rgen Schmidhuber.
1997.
Long short-term memory.Neural computation, 9(8):1735?1780.
[Kim2016] Yoon Kim.
2016.
Seq2seq-attn.
https://github.com/harvardnlp/seq2seq-attn.
[Kushman and Barzilay2013] Nate Kushman and ReginaBarzilay.
2013.
Using semantic unification to gener-ate regular expressions from natural language.
NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL).
[Luong et al2015] Thang Luong, Hieu Pham, andChristopher D. Manning.
2015.
Effective approachesto attention-based neural machine translation.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages19221412?1421, Lisbon, Portugal, September.
Associationfor Computational Linguistics.
[Mnih et al2014] Volodymyr Mnih, Nicolas Heess, AlexGraves, et al 2014.
Recurrent models of visual atten-tion.
In Advances in Neural Information ProcessingSystems, pages 2204?2212.
[Ranta1998] Aarne Ranta.
1998.
A multilingual natural-language interface to regular expressions.
In Pro-ceedings of the International Workshop on Finite StateMethods in Natural Language Processing, pages 79?90.
Association for Computational Linguistics.
[Raza et al2015] Mohammad Raza, Sumit Gulwani, andNatasa Milic-Frayling.
2015.
Compositional programsynthesis from natural language and examples.
In-ternational Joint Conference on Artificial Intelligence(IJCAI).
[Srivastava et al2014] Nitish Srivastava, Geoffrey Hin-ton, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov.
2014.
Dropout: A simple way to pre-vent neural networks from overfitting.
The Journal ofMachine Learning Research, 15(1):1929?1958.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals, andQuoc V Le.
2014.
Sequence to sequence learning withneural networks.
In Advances in neural informationprocessing systems, pages 3104?3112.
[Wang et al2015] Yushi Wang, Jonathan Berant, andPercy Liang.
2015.
Building a semantic parserovernight.
Association for Computational Linguistics(ACL).1923
