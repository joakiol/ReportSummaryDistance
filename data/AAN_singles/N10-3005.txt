Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 23?28,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTowards a Matrix-based Distributional Model of MeaningEugenie GiesbrechtFZI Forschungszentrum Informatikat the University of KarlsruheHaid-und-Neu-Str.
10-14, Karlsruhe, Germanygiesbrecht@fzi.deAbstractVector-based distributional modelsof semantics have proven usefuland adequate in a variety of naturallanguage processing tasks.
How-ever, most of them lack at leastone key requirement in order toserve as an adequate representa-tion of natural language, namelysensitivity to structural informationsuch as word order.
We propose anovel approach that offers a poten-tial of integrating order-dependentword contexts in a completely un-supervised manner by assigning towords characteristic distributionalmatrices.
The proposed model isapplied to the task of free associa-tions.
In the end, the first results aswell as directions for future workare discussed.1 IntroductionIn natural language processing as well as in informa-tion retrieval, Vector Space Model (VSM) (Salton etal., 1975) and Word Space Model (WSM) (Schu?tze,1993; Lund and Burgess, 1996) have become themainstream for text representation.
VSMs embodythe distributional hypothesis of meaning, the mainassumption of which is that a word is known ?bythe company it keeps?
(Firth, 1957).
VSMs provedto perform well in a number of cognitive tasks suchas synonymy identification (Landauer and Dumais,1997), automatic thesaurus construction (Grefen-stette, 1994) and many others.
However, it has beenlong recognized that these models are too weak torepresent natural language to a satisfactory extent.With VSMs, the assumption is made that word co-occurrence is essentially independent of word order.All the co-occurrence information is thus fed intoone vector per word.Suppose our ?background knowledge?
corpusconsists of one sentence: Peter kicked the ball.
Itfollows that the distributional meanings of both PE-TER and BALL would be in a similar way defined bythe co-occurring KICK which is insufficient, as BALLcan be only kicked by somebody but not kick itself;in case of PETER, both ways of interpretation shouldbe possible.
To overcome the aforementioned prob-lems with vector-based models, we suggest a noveldistributional paradigm for representing text in thatwe introduce a further dimension into a ?standard?two-dimensional word space model.
That allows usto count correlations for three words at a time.
Inshort, given a vocabulary V , context width w = mand tokens t1, t2, t3, ..., ti ?
V , for token ti a matrixof size V ?
V is generated that has nonzero valuesin cells where ti appears between ti?m and ti+m.Note that this 3-dimensional representation al-lows us to integrate word order information into themodel in a completely unsupervised manner as wellas to achieve a richer word representation as a matrixinstead of a vector.The remainder of the paper is organized as fol-lows.
After a recap of basic mathematical no-tions and operations used in the model in Section 2,we introduce the proposed three-dimensional tensor-based model of text representation in Section 3.
Firstevaluation experiments are reported in Section 4.23After a brief overview of related work in Section 5,we provide some concluding remarks and sugges-tions for future work in Section 6.2 PreliminariesIn this section, we provide a brief introduction totensors and the basics of mathematical operationsthat are employed in the suggested model.First, given d natural numbers n1, .
.
.
, nd, a (real)n1?
.
.
.
?nd tensor can be defined as a functionT : {1, .
.
.
, n1}?
.
.
.?
{1, .
.
.
, nd} ?
R, map-ping d-tuples of natural numbers to real numbers.Intuitively, a tensor can best be thought of as a d-dimensional table (or array) carrying real numbersas entries.
Thereby n1, .
.
.
, nd determine the exten-sion of the array in the different directions.
Obvi-ously, matrices can be conceived as n1?n2-tensorsand vectors as n1-tensors.In our setting, we will work with tensors whered = 3 and for the sake of better understandabilitywe will introduce the necessary notions for this caseonly.Our work employs higher-order singular valuedecomposition (HOSVD), which generalizes themethod of singular value decomposition (SVD)from matrices to arbitrary tensors.Given an n1?n2?n3 tensor T , its Tucker decom-position (Tucker, 1966) for given natural numbersm1, m2, m3 consists of an m1?m2?m3 tensor Gand three matrices A,B, and C of formats n1?m1,n2?m2, and n3?m3, respectively, such thatT (i, j, k) =m1?r=1m2?s=1m3?t=1G(r, s, t)?A(i, r)?B(j, s)?C(k, t).The idea here is to represent the large-size ten-sor T by the smaller ?core?
tensor G. The matricesA, B, and C can be seen as linear transformations?compressing?
input vectors from dimension ni intodimension mi.
Note that a precise representation ofT is not always possible.
Rather one may attemptto approximate T as well as possible, i.e.
find thetensor T ?
for which a Tucker decomposition existsand which has the least distance to T .
Thereby, thenotion of distance is captured by ?T ?
T ?
?, whereT ?
T ?
is the tensor obtained by entry-wise subtrac-tion and ?
?
?
is the Frobenius norm defined by?M?
=???
?n1?r=1n2?s=1n3?t=1(M(r, s, t))2.In fact, the described way of approximating a ten-sor is called dimensionality reduction and is oftenused for reducing noise in multi-dimensional data.3 Proposed ModelOur motivation is to integrate structure into the ge-ometrical representation of text meaning while ad-hering to the ideas of distributional semantics.
Forthis, we introduce a third dimension that allows usto separate the left and right contexts of the words.As we process text, we accumulate the left and rightword co-occurrences to represent the meaning of thecurrent word.
Formally, given a corpus K, a list Lof tokens, and a context width w, we define its ten-sor representation TK by letting TK(i, j, k) be thenumber of occurrences of L(j) s L(i) s?
L(k) insentences in K where s, s?
are (possibly empty) se-quences of at most w ?
1 tokens.
For example, sup-pose our corpus consists of three sentences: ?Paulkicked the ball slowly.
Peter kicked the ball slowly.Paul kicked Peter.?
We let w = 1, presuming priorstop words removal.
We obtain a 5 ?
5 ?
5 tensor.Table 1 displays two i-slices of the resulting tensorT showing left vs. right context dependencies.KICK PETER PAUL KICK BALL SLOWLYPETER 0 0 0 1 0PAUL 1 0 0 1 0KICK 0 0 0 0 0BALL 0 0 0 0 0SLOWLY 0 0 0 0 0BALL PETER PAUL KICK BALL SLOWLYPETER 0 0 0 0 0PAUL 0 0 0 0 0KICK 0 0 0 0 2BALL 0 0 0 0 0SLOWLY 0 0 0 0 0Table 1: Slices of T for the terms KICK (i = 3) and BALL(i = 4).Similarly to traditional vector-based distributionalmodels, dimensionality reduction needs to be per-formed in three dimensions either, as the resultingtensor is very sparse (see the examples of KICK and24BALL).
To this end, we employ Tucker decompo-sition for 3 dimensions as introduced in Section 2.For this, Matlab Tensor Toolbox1 (Bader and Kolda,2006) is used.A detailed overview of computational complexityof Tucker decomposition algorithms in Tensor Tool-box is provided in Turney (2007).
The drawback ofthose is that their complexity is cubic in the numberof factorization dimensions and unfeasible for largedatasets.
However, new memory efficient tensor de-composition algorithms have been proposed in themeantime.
Thus, Memory Efficient Tucker (MET)is available in Matlab Tensor Toolbox since Version2.3.
Rendle and Schmidt-Thieme (2010) present anew factorization method with linear complexity.4 Evaluation Issues4.1 TaskVector-based distributional similarity methods haveproven to be a valuable tool for a number of taskson automatic discovery of semantic relatedness be-tween words, like synonymy tests (Rapp, 2003) ordetection of analogical similarity (Turney, 2006).A somewhat related task is the task of finding outto what extent (statistical) similarity measures cor-relate with free word associations2.
Furthermore,this task was suggested as a shared task for the eval-uation of word space models at Lexical SemanticsWorkshop at ESSLLI 2008.
Free associations arethe words that come to the mind of a native speakerwhen he or she is presented with a so-called stimu-lus word.
The percent of test subjects that producecertain response to a given stimulus determines thedegree of a free association between a stimulus anda response.Despite the widespread usage of vector-basedmodels to retrieve semantically similar words, it isstill rather unclear what type of linguistic phenom-ena they model (cf.
Heylen et al (2008), Wand-macher et al (2008)).
The same is true for free as-sociations.
There are a number of relations accord-ing to which a word may be associated with another1Version 2.32One of the reasons to choose this evaluation setting was thatthe dataset for free word associations task is freely available athttp://wordspace.collocations.de/doku.php/data:esslli2008:start(in contrast to, e.g., the synonymy test set).word.
For example, Aitchison (2003) distinguishesfour types of associations: co-ordination, colloca-tion, superordination and synonymy.
This affordsan opportunity to use the task of free associations asa ?baseline?
for distributional similarity.For this task, workshop organizers have proposedthree subtasks, one of which - discrimination - weadapt in this paper.
Test sets have been providedby the workshop organizers.
The former are basedon the Edinburgh Associative Thesaurus3 (EAT),a freely available database of English associationnorms.Discrimination task includes a test set of over-all 300 word pairs that were classified according tothree classes of association strengths:?
FIRST strongly associated word pairs as indi-cated by more than 50% of test subjects as firstresponses;?
HAPAX word associations that were producedby a single test subject;?
RANDOM random combinations of words fromEAT that were never produced as a stimulus -response pair.4.2 ProcedureTo collect the three-way co-occurrence information,we experiment with the UKWAC corpus (A. Fer-raresi and Bernardini, 2008), as suggested by theworkshop organizers, in order to get comparable re-sults.
As UKWAC is a huge Web-derived corpusconsisting of about 2 billion tokens, it was impos-sible at the current stage to process the whole cor-pus.
As the subsections of UKWAC contain ran-domly chosen documents, one can train the modelon any of the subsections.We limited out test set to the word pairs for whichthe constituent words occur more than 50 times inthe test corpus.
Thereby, we ended up with a test setconsisting of 222 word pairs.We proceed in the following way.
For each pairof words:1.
Gather N sentences, i.e.
contexts, for each ofthe two words4, here N = 50;3http://www.eat.rl.ac.uk/4This corpus ?preprocessing?
step was mainly due to lim-252.
Build a 3-dimensional tensor from the subcor-pus obtained in (1), given a context width w=5,i.e.
5 words to the left and 5 words to the rightof the target word), taking sentence boundariesinto consideration;3.
Reduce 5 times the dimensionality of the tensorobtained in (2) by means of Tucker decomposi-tion;4.
Extract two matrices of both constituents of theword pair and compare those by means of co-sine similarity5.Here, we follow the tradition of vector-basedmodels where cosine is usually used to measure se-mantic relatedness.
One of the future direction inmatrix-based meaning representation is to investi-gate further matrix comparison metrics.4.3 Results and DiscussionTables 2 and 3 show the resulting accuracies6 fortraining and test sets.
th denotes cosine thresholdvalues that were used for grouping the results.
Here,th is taken to be the function of the size s of the dataset.
Thus, given a training set of size s = 60 and3 classes, we define an ?equally distributed?
thresh-old th1 = 60/3 = 20 (s. Table 2) and a ?linearlygrowing?
threshold th2 = 14 ,13 , rest (s. Table 3).It is not quite apparent, how the threshold fordifferentiating between the groups should be deter-mined under given conditions.
Usually, such mea-sures are defined on the basis of training data (e.g.Wandmacher et al (2008)).
It was not applicablein our case as, due to the current implementation ofthe model as well as insufficient computational re-sources for the time being, we could not build onebig model for all experiment iterations.Also, the intuition we have gained with this kindof thresholds is that as soon as you change the un-derlying corpus or the model parameters, you mayneed to define new thresholds (cf.
Tables 2 and 3).ited processing power we had at our disposal at the moment theexperiments were conducted.
With this step, we considerablyreduced the size of the corpus and guaranteed a certain numberof contexts per relevant word.5Cosine similarity is determined as a normalized inner prod-uct6Accuracy is defined in the following way: Accuracy =right/(right + wrong)Thresholds in geometric models of meaning can notbe just fixed, just as the measure of similarity cannotbe easily quantified by humans.It would be straightforward to compare the perfor-mance of the proposed model with its 2-dimensionalanalogue.
Wandmacher et al (2008) obtain in aver-age better results with their LSA-based model forthis task.
Specifically, they observe very good re-sults for RANDOM associations (78.2% accuracy)but the lowest results for the FIRST, i.e.
strongest,associations (50%).
In constrast, the outcome forRANDOM in our model is the worst.
However, thebigger the threshold, the more accurate is gettingthe model for the FIRST associations.
For exam-ple, with a threshold of th = 0.2 for the test set- 4 out of 5 highest ranked pairs were highly asso-ciated (FIRST) and the fifth pair was from the HA-PAX group.
For HAPAX word associations, no simi-lar regularities could be observed.The resulting accuracies may seem to be poor atthis stage.
However, it is worth mentioning thatthis is a highly difficult and corpus-dependent taskfor automatic processing.
The reported results havebeen obtained based on very small corpora, contain-ing ca.
100 sentences per iteration (cf.
Wandmacheret al (2008) use a corpus of 108 million words totrain their LSA-Model).
Consequently, it is not pos-sible to compare both results directly, as they havebeen produced under very different conditions.5 Related Work5.1 Matrix ApproachesThere have been a number of efforts to integrate syn-tax into vector-based models with alternating suc-cess.
Some used (dependency) parsing to feed themodels (Grefenstette, 1994; Lin, 1998; Pado?
and La-pata, 2007); the others utilized only part of speechinformation, e.g., Widdows (2003).In many cases, these syntactically enhanced mod-els improved the performance (Grefenstette, 1994;Lin, 1998; Pado?
and Lapata, 2007).
Sometimes,however, rather controversial results were observed.Thus, Widdows (2003) reported both positive andnegative effects for the task of developing tax-onomies.
On the one side, POS information in-creased the performance for common nouns; on theother side, it degraded the outcome for proper nouns26TRAIN TESTFIRST 12/20 (60%) (th = 0.022) 25/74 (33%) (th = 0.078))HAPAX 7/20 (35%) (th = 0.008) 35/74 (47%) th = 0.042)RANDOM 8/20 (40%) 23/74 (31%)TOTAL (F/H/R) 27/60 (45%) 83/222 (37.4%)FIRST/HORR7 44/60 (73.33%) 125/222 (56.3%)Table 2: Accuracies for the ?equally distributed?
threshold for training and test setsTRAIN TESTFIRST 9/15 (60%) (th = 0.0309) 20/55 (36.4%) (th = 0.09)HAPAX 8/20 (40%) (th = 0.0101) 39/74 (52.7%) (th = 0.047)RANDOM 10/25 (40%) 24/93 (25.8%)TOTAL (F/H/R) 27/60 (45%) 108/222 (48.6%)FIRST/HORR8 43/60 (71.60%) 113/222 (50.9%)Table 3: Accuracies for a ?linearly growing?
threshold for training and test setsand verbs.Sahlgren et al (2008) incorporate word order in-formation into context vectors in an unsupervisedmanner by means of permutation.Recently, Erk and Pado?
(2008) proposed a struc-tured vector space model where a word is repre-sented by several vectors reflecting the words lexicalmeaning as well as its selectional preferences.
Themotivation behind their work is very close to ours,namely, that single vectors are too weak to representword meaning.
However, we argue that a matrix-based representation allows us to integrate contex-tual information in a more general manner.5.2 Tensor ApproachesAmong the early attempts to apply higher-order ten-sors instead of vectors to text data is the work of Liuet al (2005) who show that Tensor Space Model isconsistently better than VSM for text classification.Cai et al (2006) suggest a 3-dimensional represen-tation for documents and evaluate the model on thetask of document clustering.The above as well as a couple of other projects inthis area in information retrieval community leaveopen the question of how to convey text into a three-dimensional tensor.
They still use vector-based rep-resentation as the basis and then just mathematicallyconvert vectors into tensors, without linguistic justi-fication of such transformations.Further, there are few works that extend the term-document matrix with metadata as a third dimension(Chew et al, 2007; Sun et al, 2006).Turney (2007) is one of the few to study the ap-plication of tensors to word space models.
However,the emphasis in that paper is more on the evaluationof different tensor decomposition models for suchspaces than on the formal model of text representa-tion in three dimensions.
Van de Cruys (2009) sug-gests a three-way model of co-occurrence similar toours.
In contrast to Van de Cruys (2009), we arenot using any explicit syntactic preprocessing.
Fur-thermore, our focus is more on the model itself as ageneral model of meaning.6 Summary and Future WorkIn this paper, we propose a novel approach to textrepresentation inspired by the ideas of distributionalsemantics.
In particular, our model suggests a solu-tion to the problem of integrating word order infor-mation in vector spaces in an unsupervised manner.First experiments on the task of free associations arereported.
However, we are not in the position yet tocommit ourselves to any representative statements.A thorough evaluation of the model still needs to bedone.
Next steps include, amongst others, evaluat-ing the suggested model with a bigger data corpus aswell as using stemming and more sophisticated fill-ing of word matrices, e.g., by introducing advancedweighting schemes into the matrices instead of sim-ple counts.Furthermore, we started with evaluation on thetask which has been proposed for the evaluation of27word space models at the level of word meaning.
Weneed, however, to evaluate the model for the taskswhere word order information matters more, e.g.
onselectional preferences or paraphrasing.Last but not least, we plan to address the issue ofmodeling compositional meaning with matrix-baseddistributional model of meaning.AcknowledgmentsThis work is supported by German ?Federal Min-istry of Economics?
(BMWi) under the project The-seus (number 01MQ07019).
Many thanks to theanonymous reviewers for their insightful comments.ReferencesM.
Baroni A. Ferraresi, E. Zanchetta and S. Bernardini.2008.
Introducing and evaluating ukWaC, a very largeWeb-derived corpus of English.
In Proceedings of theWAC4 Workshop at LREC?08.Jean Aitchison.
2003.
Words in the Mind: An Introduc-tion to the Mental Lexicon.
Wiley-Blackwell.Brett W. Bader and Tamara G. Kolda.
2006.
Algorithm862: Matlab tensor classes for fast algorithm prototyp-ing.
ACM Trans.
Math.
Softw., 32(4):635?653.Deng Cai, Xiaofei He, and Jiawei Han.
2006.
Tensorspace model for document analysis.
In SIGIR, pages625?626.
ACM.Peter Chew, Brett Bader, Tamara Kolda, and Ahmed Ab-delali.
2007.
Cross-language information retrieval us-ing PARAFAC2.
In Proc.
KDD?07, pages 143?152.ACM.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InEMNLP, pages 897?906.
ACL.J.R.
Firth.
1957.
A synopsis of linguistic theory 1930-55.
Studies in linguistic analysis, pages 1?32.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Springer.Kris Heylen, Yves Peirsman, Dirk Geeraerts, and DirkSpeelman.
2008.
Modelling word similarity: an eval-uation of automatic synonymy extraction algorithms.In Proceedings of LREC?08, pages 3243?3249.T.
K. Landauer and S. T Dumais.
1997.
Solution toPlato?s Problem: The Latent Semantic Analysis The-ory of Acquisition, Induction and Representation ofKnowledge.
Psychological Review, 104(2):211?240.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of ACL?98, pages 768?774.
ACL.Ning Liu, Benyu Zhang, Jun Yan, Zheng Chen, WenyinLiu, Fengshan Bai, and Leefeng Chien.
2005.
Textrepresentation: from vector to tensor.
In Proc.ICDM05.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instrumen-tation, and Computers, pages 203?20.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Reinhard Rapp.
2003.
Word sense discovery based onsense descriptor dissimilarity.
In Proceedings of theNinth Machine Translation Summit, pages 315?322.Steffen Rendle and Lars Schmidt-Thieme.
2010.
Pair-wise interaction tensor factorization for personalizedtag recommendation.
In WSDM ?10: Proceedings ofthe third ACM international conference on Web searchand data mining, pages 81?90, New York, NY, USA.ACM.M.
Sahlgren, A. Holst, and P. Kanerva.
2008.
Permu-tations as a means to encode order in word space.
InProc.
CogSci08, pages 1300?1305.G.
Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Commun.
ACM,18(11):613?620.Hinrich Schu?tze.
1993.
Word space.
In Advances inNIPS 5, pages 895?902.J.
Sun, D. Tao, and C. Faloutsos.
2006.
Beyondstreams and graphs: Dynamic tensor analysis.
In Proc.KDD?06, pages 374?383.L.R.
Tucker.
1966.
Some mathematical notes on three-mode factor analysis.
Psychometrika, 31(3).Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.P.
Turney.
2007.
Empirical evaluation of four tensor de-composition algorithms.
Technical report.
TechnicalReport ERB-1152.Tim Van de Cruys.
2009.
A non-negative tensor factor-ization model for selectional preference induction.
InGEMS ?09: Proceedings of the Workshop on Geomet-rical Models of Natural Language Semantics, pages83?90, Morristown, NJ, USA.
ACL.Tonio Wandmacher, Ekaterina Ovchinnikova, andTheodore Alexandrov.
2008.
Does Latent SemanticAnalysis reflect human associations.
In Proceedingsof the Lexical Semantics workshop at ESSLLI, Ham-burg, Germany.Dominic Widdows.
2003.
Unsupervised methods for de-veloping taxonomies by combining syntactic and sta-tistical information.
In Proceedings of NAACL?03,pages 197?204.
ACL.28
