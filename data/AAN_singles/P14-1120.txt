Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1274?1283,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsPattern Dictionary of English PrepositionsKen LitkowskiCL Research9208 Gue RoadDamascus, MD 20872 USAken@clres.comAbstractWe present a new lexical resource for thestudy of preposition behavior, the PatternDictionary of English Prepositions (PDEP).This dictionary, which follows principles laidout in Hanks?
theory of norms and exploita-tions, is linked to 81,509 sentences for 304prepositions, which have been made availableunder The Preposition Project (TPP).
Nota-bly, 47,285 sentences, initially untagged,provide a representative sample of preposi-tion use, unlike the tagged sentences used inprevious studies.
Each sentence has beenparsed with a dependency parser and our sys-tem has near-instantaneous access to featuresdeveloped with this parser to explore and an-notate properties of individual senses.
Thefeatures make extensive use of WordNet.
Wehave extended feature exploration to includelookup of FrameNet lexical units andVerbNet classes for use in characterizingpreposition behavior.
We have designed oursystem to allow public access to any of thedata available in the system.1 IntroductionRecent studies (Zapirain et al (2013); Srikumarand Roth (2011)) have shown the value of prepo-sitional phrases in joint modeling with verbs forsemantic role labeling.
Although recent studieshave shown improved preposition disambigua-tion, they have received little systematic treat-ment from a lexicographic perspective.
Recently,a new corpus has been made available that prom-ises to be much more representative of preposi-tion behavior.
Our initial examination of thiscorpus has suggested clear indications of sensespreviously overlooked and reduced prominencefor senses thought to constitute a large role inpreposition use.In section 2, we describe the interface to thePattern Dictionary of English Prepositions(PDEP), identifying how we are building upondata developed in The Preposition Project (TPP)and investigating its sense inventory with corpo-ra also made available under TPP.
Section 3 de-scribes the procedures for tagging a representa-tive corpus drawn from the British National Cor-pus, including some findings that have emergedin assessing previous studies of preposition dis-ambiguation.
Section 4 describes how we areable to investigate the relationship of WordNet,FrameNet, and VerbNet to this effort and howthis examination of preposition behavior can beused in working with these resources.
Section 5describes how we can use PDEP for the analysisof semantic role and semantic relation invento-ries.
Section 6 describes how we envision furtherdevelopments of PDEP and how the data areavailable for further analysis.
In section 7, wepresent our conclusions for PDEP.2 The Pattern Dictionary of EnglishPrepositionsLitkowski and Hargraves (2005) and Litkowskiand Hargraves (2006) describe The PrepositionProject (TPP) as an attempt to describe preposi-tion behavior using a sense inventory madeavailable for public use from the Oxford Dic-tionary of English (Stevenson and Soanes, 2003)by tagging sentences drawn from FrameNet.
InTPP, each sense was characterized with its com-plement and attachment (or governor) properties,its class and semantic relation, substitutableprepositions, its syntactic positions, and anyFrameNet frame and frame element usages(where available).
The FrameNet sentences weresense-tagged using the sense inventory and were1274later used as the basis for a preposition disam-biguation task in SemEval 2007 (Litkowski andHargraves, 2007).Initial results in SemEval achieved a best ac-curacy of 69.3 percent (Ye and Baldwin, 2007).The data from SemEval has subsequently beenused in several further investigations of preposi-tion disambiguation.
Most notably, Tratz (2011)achieved a result of 88.4 percent accuracy andSrikumar and Roth (2013) achieved a similarresult.
However, Litkowski (2013b) showed thatthese results did not extend to other corpora,concluding that the FrameNet-based corpus maynot have been representative, with a reduction ofaccuracy to 39.4 percent using a corpus devel-oped by Oxford.Litkowski (2013a) announced the creation ofthe TPP corpora in order to develop a more rep-resentative account of preposition behavior.
TheTPP corpora includes three subcorpora: (1) thefull SemEval 2007 corpus (drawn fromFrameNet data, henceforth FN), (2) sentencestaken from the Oxford English Corpus to exem-plify preposition senses in the Oxford Dictionaryof English (henceforth, OEC), and (3) a sampleof sentences drawn from the written portion ofthe British National Corpus (BNC), using theWord Sketch Engine as implemented in the sys-tem for the Corpus Pattern Analysis of verbs(henceforth, CPA or TPP).We have used the TPP data and the TPP cor-pora to implement an editorial interface, the Pat-tern Dictionary of English Prepositions (PDEP).1This dictionary is intended to identify the proto-typical syntagmatic patterns with which preposi-tions in use are associated, identifying linguisticunits used sequentially to make well-formedstructures and characterizing the relationship be-tween these units.
In the case of prepositions, theunits are the complement (object) of the preposi-tion and the governor (point of attachment) of theprepositional phrase.
The editorial interface isused to make changes in the underlying data-bases, as described in the following subsections.Editorial access to make changes is limited, butthe system can be explored publicly and the un-derlying data can be accessed publicly, either inits entirety or through publicly available scriptsused in accessing the data during editorial opera-tions.Standard dictionaries include definitions ofprepositions, but only loosely characterize thesyntagmatic patterns associated with each sense.1 http://www.clres.com/db/TPPEditor.htmlPDEP takes this a step further, looking for proto-typical sentence contexts to characterize the pat-terns.
PDEP is modeled on the principles of Cor-pus Pattern Analysis (CPA), developed to char-acterize syntagmatic patterns for verbs.
2  Theseprinciples are described more fully in Hanks(2013).
Currently, CPA is being used in the pro-ject Disambiguation of Verbs by Collocation todevelop a Pattern Dictionary of English Verbs(PDEV).
3  PDEP is closely related to PDEV,since most syntagmatic patterns for prepositionsare related to the main verb in a clause.
PDEP isviewed as subordinate to PDEV, sufficiently sothat PDEP employs significant portions of codebeing used in PDEV, with appropriate modifica-tions as necessary to capture the syntagmatic pat-terns for prepositions.42.1 The Preposition InventoryAfter a start page for entry into PDEP, a table ofall prepositions in the sense inventory is dis-played.
Figure 1 contains a truncated snapshot ofthis table.
The table has a row for each of 304prepositions as identified in TPP.
The secondcolumn indicates the number of patterns (senses)for each preposition.
The next two columns showthe number of TPP (CPA) instances that havebeen tagged and the total number of TPP in-stances that have been obtained as the samplefrom the total number of instances in the BNC.Figure 1.
Preposition InventoryAdditional columns not shown in Figure 1show (1) the status of the analysis for the prepo-sition, (2) the number of instances fromFrameNet (i.e., FN Insts, as developed forSemEval 2007), and (3) the number of instancesfrom the Oxford English Corpus (i.e., OECInsts).
The number of prepositions with2 See http://nlp.fi.muni.cz/projects/cpa/.3 See http://clg.wlv.ac.uk/projects/DVC4 PDEP is implemented as a combination of HTMLand Javascript.
Within the Javascript code, calls aremade to PHP scripts to retrieve data from MySQLdatabase tables and from additional files (describedbelow).1275FrameNet instances is 57 (larger than the 34prepositions used in SemEval).
There are noOEC instances for 57 prepositions.
There are noTPP instances for 41 prepositions.
Notwithstand-ing the lack of instances, there are TPP charac-terizations for all 304 prepositions.The BNC frequency shown in Figure 1 pro-vides a basis for extrapolating results from PDEPto the totality of prepositions.
In total, the num-ber of instances in the BNC is 5,391,042, whichcan be used as the denominator when examiningthe relative frequency of any preposition (e.g.,between has a frequency of 0.0109,58,865/5,391,042).5In general, the target sample size was 250CPA instances.
If the number available was lessthan 250, all instances were used.
The TPP CPAcorpus contains 250 instances for 170 preposi-tions.
Where the number of senses for a preposi-tion was large (about 15 or more), larger samplesof 750 (of, to, on, and with) or 500 (in, for, by,from, at, into, over, like, and through) weredrawn.2.2 Preposition PatternsWhen a row in Figure 1 is clicked, the preposi-tion is selected and a new page is opened to showthe patterns for that preposition.
Figure 2 showsthe four patterns for below.
Each pattern is pre-sented as an instance of the template [[Gover-nor]] prep [[Complement]], followed by itsprimary implicature, where the current definitionis substituted for the preposition.Figure 2.
Preposition Pattern ListThe display in Figure 2 provides an overviewfor each preposition, with the top line showingthe number of tagged instances available from5 The total number of instances for of and in in thisestimate is 1,000,000.
As a result, the relative fre-quency calculation should not be construed as com-pletely accurate.each corpus.
For the TPP instances, this identi-fies the number of instances that have beentagged and the number that remain to be tagged.In the body of the table, the first column showsthe TPP sense number.
The next three columnsshow the number of instances that have beentagged with this sense.
Note that the top line ofthe pattern list includes a menu option for addinga pattern, for the case when we find that a newsense is required by the corpus evidence.Clicking on any row in the pattern list opensthe details for that pattern, with a pattern boxentitled with the preposition and the patternnumber, as shown in Figure 3.
The pattern boxcontains data developed in TPP and several newfields intended to capture our enhancements.TPP data include the fields for the Comple-ment, the Governor, the TPP Class, the TPPRelation, the Substitutable Prepositions, theSyntactic Position, the Quirk Reference, theSense Relation, and the Comment.
We haveadded the checkboxes for complement type(common nouns, proper nouns, WH-phrases, and-ing phrases), as well as a field to identify a par-ticular lexical item (lexset) if the sense is an idi-omatic usage.
We have added the Selector fieldsfor the complement and the governor.
For thecomplement, we have a field Category to holdits ontological category (using the shallow ontol-ogy being developed for verbs in the DVC pro-ject mentioned above).6 We also provided a fieldfor the Semantic Class of the governor; this fieldhas not yet been implemented.We have added two Cluster/Relation fields.The Cluster field is based on data available fromTratz (2011), where senses in the SemEval 2007data have been put into 34 clusters.
The Relationfield is based on data available from Srikumarand Roth (2013), where senses in the SemEval2007 data have been put into 32 classes.
A keyelement of Srikumar and Roth was the use ofthese classes to model semantic relations acrossprepositions (e.g., grouping all the Temporalsenses of the SemEval prepositions).
In the pat-tern box, each of these two fields has a drop-down list of the clusters and relations, enablingus to categorize the senses of other prepositionswith these classes.
Below, we describe how weare able to use the TPP classes and relationsalong with the Tratz clusters and Srikumar rela-tions in an analysis of these classes across the6 This ontology is an evolution of the Brandeis Se-mantic Ontology (Pustejovsky et al, 2006).1276full set of prepositions, instead of just those usedin SemEval.Any number of pattern boxes may be openedat one time.
The data in any of the fields may bealtered (with the menu bar changing color to red)and then saved to the underlying databases.
Anindividual pattern box may then be closed.The drop-down box labeled Corpus Instancesin the menu bar is used to open the set of corpusinstances for the given sense.
As shown in Figure2, this sense has 6 FN instances, 20 OEC in-stances, and 15 TPP instances.
The drop-downbox has an option for each of these sets, alongwith an option for all TPP instances that have notyet been tagged.
When one of these options isselected, the corresponding set of instances isopened in a new tab, discussed in the next sec-tion.2.3 Preposition Corpus InstancesAs indicated, selecting an instance set from thepattern box opens this set in a separate tab, asshown in Figure 4.
This tab, labeled Annotation:below (3(1b)), identifies the preposition and thesense, if any, associated with the instance set (thesense will be identified as unk if the set has notyet been tagged.
The instance set is displayed,identifying the corpus, the instance identifier, theTPP sense (if identified, or ?unk?
if not), the lo-cation in the sentence of the target preposition,and the sentence, with the preposition in bold.This tab is where the annotation takes place.Any set of sentences may be selected; each se-lected sentence is highlighted in yellow (asshown in Figure 6).
The sense value may bechanged using the drop-down box labeled TagInstances in the menu bar.
This drop-down boxcontains all the current senses for the preposition,along with possible tags x (to indicate that theinstance is invalid for the preposition) and unk(to indicate that a tagging decision has not yetbeen made).
The sense tags in Figure 4 wereoriginally untagged in the CPA (TPP) corpus andwere tagged in this manner.In general, sense-tagging follows standard lex-icographic principles, where an attempt is madeto group instances that appear to represent dis-tinct senses.
PDEP provides an enhanced envi-ronment for this process.
Firstly, we can makeuse of the current TPP sense inventory to tagsentences.
Since the pattern sets (definitions) arebased on the Oxford Dictionary of English, thelikelihood that the coverage and accuracy of thesense distinctions is quite high.
However, sinceprepositions have not generally received theclose attention of words in other parts of speech,Figure 3.
Preposition Pattern DetailsFigure 4.
Preposition Corpus Instance Annotation1277PDEP is intended to ensure the coverage and ac-curacy.
During the tagging of the SemEval in-stances, the lexicographer found it necessary toincrease the number of senses by about 10 per-cent.
Since the lack of coverage of FrameNet iswell-recognized, the representative sample de-veloped for the TPP corpus should provide thebasis for ensuring the coverage and accuracy.In addition to adhering to standard lexico-graphic principles, the availability of the taggedFN and OEC instances can be used as the basisfor tagging decisions.
Where available, thesetagged instances can be opened in separate tabsand used as examples for tagging the unknownTPP instances.3 Tagging the TPP Corpus3.1 Examining Corpus InstancesThe main contribution of the present work is theability to interactively examine characteristics ofthe context surrounding the target preposition inthe corpus instances.
In the menu bar shown inFigure 4, there is an Examine item.
Next to it aretwo drop-down boxes, one labeled WFRs (word-finding rules) and one labeled FERs (feature ex-traction rules).
These rules are taken from thesystem described in Tratz and Hovy (2011) andTratz (2011).
7  The TPP corpora described inLitkowski (2013a) includes full dependencyparses and feature files for all sentences.
Eachsentence may have as many as 1500 features de-scribing the context of the target preposition.
Wehave made the feature files for these sentences(1309 MB) available for exploration in PDEP.In our system, we make available seven word-finding rules and nine feature extraction rules.The word-finding rules fall into two groups:words pertaining to the governor and words per-taining to the complement.
The five governorword-finding rules are (1) verb or head to the left(l), (2) head to the left (hl), (3) verb to the left(vl), (4) word to the left (wl), and (5) governor(h).
The two complement word-finding rules are(1) syntactic preposition complement (c) and (2)heuristic preposition complement (hr).
The fea-ture extraction rules are (1) word class (wc), (2)part of speech (pos), (3) lemma (l), (4) word (w),(5) WordNet lexical name (ln), (6) WordNetsynonyms (s), (7) WordNet hypernyms (h), (8)whether the word is capitalized (c), and (9) affix-es (af).
Thus, we are able to examine any of 637 An updated version of this system is available athttp://sourceforge.net/projects/miacp/.WFR FER combinations for whatever corpus sethappens to be open.In addition to these features, we are able to de-termine the extent to which prepositions associ-ated with FrameNet lexical units and VerbNetclasses occur in a given corpus set.
In Figure 4,there is a checkbox labeled FN next to the FERsdrop-down list to examine FrameNet lexicalunits.
There is a similar checkbox labeled VN toexamine members of VerbNet classes.
Theseboxes appear only when either of these resourceshas identified the given preposition as part of itsframe (75 for FrameNet and 31 for VerbNet).When a particular WFR-FER combination isselected and the Examine menu item is clicked,a new tab is opened showing the values for thosefeatures for the given corpus set, as shown inFigure 5.
The tab shows the WFR and FER thatwere used, the number of features for which thevalue was found in the feature data, the values,and the count for each feature.
The descriptioncolumn is used when displaying results for thepart of speech, the affix type, FrameNet frameelements, and VerbNet classes, since the valuecolumn for these hits are not self-explanatory.The example in Figure 5 is showing the lemma,which requires no further explanation.Figure 5.
Feature Examination ResultsFor most features (e.g., lemma or part ofspeech), the number of possible values is rela-tively small, limited by the number of instancesin the corpus set.
For features such as theWordNet lexical name, synonyms andhypernyms, the number of values may be muchlarger.
For FrameNet and VerbNet, the featureexamination is limited to the combination of theWFR for the governor (h) and the FER lemma(l), both of which will generally identify verbs inthe value column.The general objective of examining features isto identify those that are diagnostic of specificsenses.
When applied to the full untagged TPPcorpus set, this process is akin to developing1278word sketches for prepositions (Kilgarriff et al,2004).
However, since we have tagged corpussets for most preposition senses, we can beginour efforts looking at these sets.
The hypothesisis that the tagged corpora will show patternswhich can then be used for tagging instances inthe TPP corpus.8The first step in examining features generallyis to look at the word classes and parts of speechfor the complement and the governor.9 These areuseful for filling in their checkboxes in Figure 3.Another useful feature is word to the left (wl),which can be used to verify the syntactic positioncheckboxes, particularly the adverbial positions(adjunct, subjunct, disjunct, and conjunct).
Thesefirst steps provide a general overview of asense?s behavior.The next step of feature examination delvesmore into the semantic characteristics of thecomplement and the governor.
Tratz (2011) re-ported that the use of heuristics provided a moreaccurate identification of the preposition com-plement; this is the WFR hr in our system.
Aftergetting some idea of the word class and the partof speech, we next examine the WordNet lexicalname of the complement to determine its broadsemantic grouping.
As mentioned, this featuremay return a number of values larger than thesize of the corpus set, since WordNet senses for agiven lexeme may be polysemous.
Notwithstand-ing, this feature examination generally shows thedominant categories and can be used to charac-8 Currently, 21.5 percent of the TPP instances (10347of 47,285) have been tagged.9 Accurate identification of the complement and gov-ernor is likely improved with the reliance on the Tratzdependency parser.
Moreover, this is likely to im-prove the word sketches in PDEP.
Ambati et al(2012) report that dependency parses provide im-proved word sketches over purpose-built finite-stategrammars.
Their findings provide additional supportfor the methods presented here.terize and act as a selector for the complement inthe pattern details.
Similar procedures are usedfor characterizing the governor selection criteria.In the example in Figure 3, for below, sense3(1b), our preliminary analysis shows hr:pos:cd(i.e., a cardinal number) and hr:l:average,standard (i.e., the lemmas average and stand-ard) are particularly useful for identifying thissense.3.2 Selecting Corpus InstancesIn addition to enabling feature examination,PDEP also facilitates selection of corpus instanc-es.
We can use the specifications for any WFR -FER combination, along with one of the values(as shown in Figure 5), to select the corpus in-stances having that feature.
Figure 6 shows, inpart, the result of the WFR hr and FER l with thevalue average, against the instances in the opencorpus set.As shown in the menu bar in Figure 6, we canselect all instances and unselect all selections.Based on any selections, we can then tag suchinstances with one of the options that appear inthe Tag Instances drop-down box.
In the specif-ic example, we could change all the selected in-stances to some other sense, if we have decidedthat the current assignment is not the best.The selection mechanism is not used absolute-ly.
For example, in examining the untagged in-stances for over, we used the specificationhr:ln:noun.time (looking for instances with theheuristic complement having the WordNet lexi-cal name noun.time).
Out of 500 instances, wefound 122 with this property.
We then scrolledthrough the selected items, deselecting instancesthat did not provide a time period, and thentagged 99 instances with the sense 14(5), withthe meaning expressing duration.
Once we havemade such a tagging, we can look at just thoseinstances the next time we examine this sense.
Inthis case, we might decide, pace the TPP lexicog-rapher?s comment, that the instances should beFigure 6.
Selected Corpus Instances1279broken down into those which express a timeperiod and those which describe ?accompanyingcircumstances?
(e.g., over coffee).3.3 Accuracy of FeaturesPDEP uses the output from Tratz?
system (2011),which is of high quality, but which is not alwayscorrect.
In addition, the TPP corpus also hassome shortcomings, which are revealed in exam-ining the instances.
The TPP corpus has not beencleaned in the same manner as the FN and theOEC corpora.
As a result, we see many caseswhich are more difficult to parse and hence, fromwhich to generate feature sets.
We believe thisprovides a truer real-world picture of the com-plexities of preposition behavior.
As a result, inthe Tag Instances drop-down box, we have in-cluded an option to tag a sentence as x, to indi-cate that it is not a valid instance.A small percentage of the TPP instances areill-formed, i.e., incomplete sentences; these aremarked as x.
For some prepositions, e.g., down, asubstantial number of instances are not preposi-tions, but rather adverbs or particles.
For somephrasal prepositions, such as on the strength of,the phrase is literal, rather than the prepositionidiom; in this case, 20 of 124 instances weremarked as x.
The occurrence of these invalid in-stances provides an opportunity for improvingtaggers, parsers, and semantic role labelers.4 Assessment of Lexical ResourcesSince the PDEP system enables exploration offeatures from WordNet, FrameNet, and VerbNet,we are able to make some assessment of theseresources.WordNet played a statistically significant rolein the systems developed by Tratz (2011) andSrikumar and Roth (2013).
This includes theWordNet lexicographer?s file name (e.g.,noun.time), synsets, and hypernyms.
We makeextensive use of the file name, but less so fromthe synsets and hypernyms.
However, in general,we find that the file names are too coarse-grainedand the synsets and hypernyms too fine-grainedfor generalizations on the selectors for the com-plements and the governors.
The issue of granu-larity also affects the use of the DVC ontology.We discuss this issue further in section 6, on in-vestigations of suitable categorization schemesfor PDEP.In using FrameNet, our results illustrate theunbalanced corpus used in SemEval 2007 (assuggested in Litkowski (2013b)).
For the senseof of, ?used to indicate the contents of a contain-er?, we first examined the FrameNet corpus setfor that sense, which contains 278 instances (outof 4482, or 6.2 percent).
Using PDEP, we foundthat FrameNet feature values for the governoraccounted for 264 of these instances (95 per-cent), all of which were related to the frame ele-ments Contents or Stuff.
However, in the TPPcorpus, only 3 out of 750 instances were identi-fied for this sense (0.4 percent).
Thus, whileFrameNet culled a large number of instanceswhich had these frame element realizations, the-se instances do not appear to be representative oftheir occurrence in a random sample of of uses.We have seen similar patterns for the otherSemEval prepositions.A similar situation exists for Cause senses ofmajor prepositions: for (385 in FrameNet, 5/500in TPP), from (71 in FrameNet, 16/500 in TPP),of (68 in FrameNet, 0/750 in TPP), and with (127in FrameNet, 8/750 in TPP).
Each of these casesfurther emphasizes how the SemEval 2007 in-stances are not representative and thus degradethe ability to apply existing preposition disam-biguation results beyond these instances.
)Wediscuss Cause senses further in the wider contextof all PDEP prepositions in the next section onclass analyses.
)As indicated earlier, VerbNet identifies fewerprepositions in its frames than FrameNet.
Webelieve this is the case since VerbNet preposi-tions are generally arguments, rather than ad-juncts.
Many of the FrameNet prepositions areevoking peripheral and extra-thematic frame el-ements, so the number of prepositions is corre-spondingly higher.
Also, VerbNet contains fewermembers in its verb classes.
As a result, thenumber of hits when using VerbNet is somewhatsmaller, although some use of VerbNet classes ispossible with the governor selectors.PDEP provides a vehicle for expanding theitems in all these resources.
While prepositionsare not central to these resources, their support-ing role provides additional information thatmight be useful in developing and using theseother resources.5 Class AnalysesIn SemEval 2007, Yuret (2007) investigated thepossibility of using the substitutable prepositionsas the basis for disambiguation (as part of moregeneral lexical sample substitution).
Althoughhis methodology yielded significant gains overthe baseline, his best results were only 54.7 per-1280cent accuracy, concluding that preposition use ishighly idiosyncratic.
Srikumar and Roth (2013)broadened this perspective by considering aclass-based approach by collapsing semantically-related senses across prepositions, thereby deriv-ing a semantic relation inventory.
While theiremphasis was on modeling semantic relations,they achieved an accuracy of 83.53 percent forpreposition disambiguation.As mentioned above, PDEP has a field for theSrikumar semantic relation, initially populatedfor the SemEval prepositions, and being extend-ed to cover all other prepositions.
For example,Srikumar and Roth identified 21 temporal sensesacross 14 SemEval prepositions, while we havethus far identified 62 senses across 50 preposi-tions.
Similar increases in the sizes of other clas-ses occur as well.
For causal senses, Srikumarand Roth identified 11 senses over 7 preposi-tions, while PDEP has 27 senses under 25 prepo-sitions.PDEP enables an in-depth analysis of TPPclasses, Tratz clusters, and Srikumar semanticrealations.
First, we query the database underly-ing Figure 3 to identify all senses with a particu-lar class.
We then examine each sense on eachlist in detail.We follow the procedures laid out above forexamining the features to add information aboutselectors, complement types, and categories.
Weuse this information to tag the TPP instances,conservatively assuring the tagging, e.g., leavinguntagged questionable instances.
Finally, wecarefully place each sense into a prepositionclass or subclass, grouping senses together andmaking annotations that attempt to capture anynuance of meaning that distinguishes the sensefrom other members of the class.To build a description of the class and its sub-classes, we make use of the Quirk reference inFigure 3 (i.e., the relevant discussions in Quirk etal.
(1985)).
We build the description of a class asa separate web page and make this available as amenu item in Figure 3 (not shown for the Scalarclass when that screenshot was made).
The de-scription provides an overview of the class, mak-ing use of the TPP data and the Quirk discussion,and indicating the number of senses and thenumber of prepositions.
Next, the descriptionprovides a list of the categories within the class,characterizing the complements of the categoryand then listing each sense in the category, withany nuance of meaning as necessary.
Finally, weattempt to summarize the selection criteria thathave been used across all the senses in the class.The process of building a class description re-veals inconsistencies in each of the class fields.When we place a preposition sense into the class,we may find it necessary to make changes in theunderlying data.At the top level, these class analyses in effectconstitute a coarse-grained sense inventory.
Asthe subclasses are developed, a finer-grainedanalysis of a particular area is available.
We be-lieve these analyses may provide a comprehen-sive characterization of particular semantic rolesthat can be used for various NLP applications.6 Availability of PDEP Data and Poten-tial for Further EnhancementsAs indicated above, each of the tables shown inthe figures is generated in Javascript through asystem call to a PHP script.
Each of these scriptsis described in detail at the PDEP web site.
Eachscript returns data in Javascript Object Notation(JSON), enabling users to obtain whatever data isof interest to them and perhaps using this datadynamically.While PDEP provides access to a largeamount of data, the architecture is very flexibleand easy to extend.
For this, we are grateful forthe Tratz parser and the DVC code.In building PDEP, we found it necessary toreprocess the SemEval 2007 data of the full28,052 sentences that were available throughTPP, rather than just those that were used in theSemEval task itself.
Tagging, parsing, and creat-ing feature files for these sentences took less than10 minutes, with an equal time to upload the fea-ture files.
We would be able to add or substitutenew corpora to the PDEP databases with rela-tively little effort.Similarly, we can add new elements or modifyexisting elements that describe preposition pat-terns.
This would require easily-made modifica-tions to the underlying MySQL database tables.The PHP scripts that access these tables are alsoeasily developed or modified.
Most of thesescripts use less than 100 lines of code.In developing PDEP, we have added variousresources incrementally.
This applies to suchresources as the DVC ontology, FrameNet, andVerbNet.
Each of these resources required rela-tively little effort to integrate into PDEP.
We willcontinue to investigate the utility of other re-sources that will assist in characterizing preposi-tion behavior.
We have begun to look at the nounclusters used in Srikumar and Roth (2013) forbetter characterizing complements.
We are also1281examining an Oxford noun hierarchy as anotheralternative for complement analysis.
We are ex-amining the WordNet detour to FrameNet, asdescribed in Burchardt et al (2005), particularlyfor use in further characterizing the governors.We recognize that an important element ofPDEP will be in its utility for preposition disam-biguation.
While we have not yet begun the nec-essary experimentation and evaluation, we be-lieve the representativeness and sample sizes ofthe TPP corpus (mostly with 250 or more sen-tences per preposition) should provide a basis forconstructing the needed studies.
We expect thatthis will follow techniques used by Cinkova et al(2012), in examining the Pattern Dictionary ofEnglish Verbs developed as the precursor toDVC.We expect that interaction with the NLPcommunity will help PDEP evolve into a usefulresource, not only for characterizing prepositionbehavior, but also for assisting in the develop-ment of other lexical resources.7 Conclusion and Future PlansWe have described the Pattern Dictionary ofEnglish Prepositions (PDEP) as a new lexicalresource for examining and recording prepositionbehavior.
PDEP does not introduce any ideas thathave not already been explored in the investiga-tion of other parts of speech.
However, by bring-ing together work from these disparate sources,we have shown that it is possible to analyzepreposition behavior in a manner equivalent tothe major parts of speech.
Since dictionary pub-lishers have not previously devoted much effortin analyzing preposition behavior, we believePDEP may serve an important role, particularlyfor various NLP applications in which semanticrole labeling is important.On the other hand, PDEP as described in thispaper is only in its initial stages.
In following theprinciples laid out for verbs in PDEV, a maingoal is to provide a sufficient characterization ofhow frequently different preposition patterns(senses) occur, with some idea of a statisticalcharacterization of the probability of the con-junction of a preposition, its complement, and itsgovernor.
Better development of a desired syn-tagmatic characterization of preposition behav-ior, consistent with the principles of TNE, is stillneeded.
Since preposition behavior is stronglylinked to verb behavior, further effort is neededto link PDEP to PDEV.The resource will benefit from futher experi-mentation and evaluation stages.
We expect thatdesired improvements will come from usage invarious NLP tasks, particularly word-sense dis-ambiguation and semantic role labeling.
In par-ticular, we anticipate that interaction with theNLP community will identify further enhance-ments, developments, and hints from usage.AcknowledgmentsStephen Tratz (and Dirk Hovy) provided consid-erable assistance in using the Tratz parser.
VivekSrikumar graciously provided his data on prepo-sition classes.
Vitek Baisa similarly helped withthe adaptation of the PDEV Javascript modules.Orin Hargraves, Patrick Hanks, and EduardHovy continued to provide valuable insights.Reviewer comments helped sharpen the draftversion of the paper.ReferencesBharat Ram Ambati, Siva  Reddy, and AdamKilgarriff.
2012.
Word Sketches for Turkish.
InProceedings of the Eighth International Confer-ence on Language Resources and Evaluation(LREC).
Istanbul, 2945-2950.Aljoscha Burchardt, Katrin Erk, and Anette Frank.2005.
A WordNet Detour to FrameNet.
Proceed-ings of GLDV workshop GermaNet II.
Bonn.Silvie Cinkova, Martin Holub, Adam Rambousek, andLenka Smejkalova.
2012.
A database of semanticclusters of verb usages.
Lexical Resources andEvaluation Conference.
Istanbul, 3176-83.Patrick Hanks.
2004.
Corpus Pattern Analysis.
InEURALEX Proceedings.
Vol.
I, pp.
87-98.
Lorient,France: Universit?
de Bretagne-Sud.Patrick Hanks.
2013.
Lexical Analysis: Norms andExploitations.
MIT Press.Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and Da-vid Tugwell.
2004.
The Sketch Engine.
Proceed-ings of EURALEX.
Lorient, France, pp.
105-16.Ken Litkowski.
2013a.
The Preposition Project Cor-pora.
Technical Report 13-01.
Damascus, MD: CLResearch.Ken Litkowski.
2013b.
Preposition Disambiguation:Still a Problem.
Technical Report 13-02.
Damas-cus, MD: CL Research.Ken Litkowski and Orin Hargraves.
2005.
The prepo-sition project.
ACL-SIGSEM Workshop on ?TheLinguistic Dimensions of Prepositions and TheirUse in Computational Linguistic Formalisms andApplications?, pages 171?179.Ken Litkowski and Orin Hargraves.
2006.
Coverageand Inheritance in The Preposition Project.
In:Proceedings of the Third ACL-SIGSEM Workshopon Prepositions.
Trento, Italy.ACL.
89-94.1282Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word-Sense Disambiguation ofPrepositions.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations(SemEval-2007), Prague, Czech Republic.James Pustejovsky, Catherine Havasi, JessicaLittman, Anna Rumshisky, and Marc Verhagen.2006.
Towards a Generative Lexical Resource: TheBrandeis Semantic Ontology.
5th Edition of the In-ternational Conference on Lexical Resources andEvaluation., 1702-5.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
New York: Long-man Inc.Vivek Srikumar and Dan Roth.
2011.
A Joint Modelfor Extended Semantic Role Labeling.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing.
ACL, 129-139.Vivek Srikumar and Dan Roth.
2013.
Modeling Se-mantic Relations Expressed by Prepositions.Transactions of the Association for ComputationalLinguistics, 1.Angus Stevenson and Catherine Soanes (Eds.).
2003.The Oxford Dictionary of English.
Oxford: Claren-don Press.Stephen Tratz.
2011.
Semantically-Enriched Parsingfor Natural Language Understanding.
PhD Thesis,University of Southern California.Stephen Tratz and Eduard Hovy.
2011.
A Fast, Accu-rate, Non-Projective, Semantically-Enriched Par-ser.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Pro-cessing.
Edinburgh, Scotland, UK.Deniz Yuret.
2007.
KU: Word Sense Disambiguationby Substitution.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations(SemEval-2007), Prague, Czech Republic.Zapirain, B., E. Agirre, L. Marquez, and M. Surdeanu.2013.
Selectional Preferences for Semantic RoleClassification.
Computational Linguistics, 39:3.1283
