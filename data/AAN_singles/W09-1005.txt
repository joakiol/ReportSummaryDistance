Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 24?32,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsGREAT: a finite-state machine translation toolkit implementing aGrammatical Inference Approach for Transducer Inference (GIATI)Jorge Gonz?lez and Francisco CasacubertaDepartamento de Sistemas Inform?ticos y Computaci?nInstituto Tecnol?gico de Inform?ticaUniversidad Polit?cnica de Valencia{jgonzalez,fcn}@dsic.upv.esAbstractGREAT is a finite-state toolkit which isdevoted to Machine Translation and thatlearns structured models from bilingualdata.
The training procedure is based ongrammatical inference techniques to ob-tain stochastic transducers that model boththe structure of the languages and the re-lationship between them.
The inferenceof grammars from natural language causesthe models to become larger when a lessrestrictive task is involved; even more ifa bilingual modelling is being considered.GREAT has been successful to implementthe GIATI learning methodology, usingdifferent scalability issues to be able todeal with corpora of high volume of data.This is reported with experiments on theEuroParl corpus, which is a state-of-the-art task in Statistical Machine Translation.1 IntroductionOver the last years, grammatical inference tech-niques have not been widely employed in the ma-chine translation area.
Nevertheless, it is not un-known that researchers are trying to include somestructured information into their models in order tocapture the grammatical regularities that there arein languages together with their own relationship.GIATI (Casacuberta, 2000; Casacuberta et al,2005) is a grammatical inference methodology toinfer stochastic transducers in a bilingual mod-elling approach for statistical machine translation.From a statistical point of view, the translationproblem can be stated as follows: given a sourcesentence s = s1 .
.
.
sJ , the goal is to find a targetsentence t?
= t1 .
.
.
tI?
, among all possible targetstrings t, that maximises the posterior probability:t?
= argmaxtPr(t|s) (1)The conditional probability Pr(t|s) can be re-placed by a joint probability distribution Pr(s, t)which is modelled by a stochastic transducer beinginferred through the GIATI methodology (Casacu-berta et al, 2004; Casacuberta and Vidal, 2004):t?
= argmaxtPr(s, t) (2)This paper describes GREAT, a software pack-age for bilingual modelling from parallel corpus.GREAT is a finite-state toolkit which was bornto overcome the computational problems that pre-vious implementations of GIATI (Pic?, 2005) hadin practice when huge amounts of data were used.Even more, GREAT is the result of a very metic-ulous study of GIATI models, which improvesthe treatment of smoothing transitions in decod-ing time, and that also reduces the required time totranslate an input sentence by means of an analysisthat will depend on the granularity of the symbols.Experiments for a state-of-the-art, voluminoustranslation task, such as the EuroParl, are re-ported.
In (Gonz?lez and Casacuberta, 2007),the so called phrase-based finite-state transducerswere concluded to be a better modelling option forthis task than the ones that derive from a word-based approach.
That is why the experiments hereare exclusively related to this particular kind ofGIATI-based transducers.The structure of this work is as follows: first,section 2 is devoted to describe the training proce-dure, which is in turn divided into several lines, forinstance, the finite-state GIATI-based models aredefined and their corresponding grammatical in-ference methods are described, including the tech-niques to deal with tasks of high volume of data;then, section 3 is related to the decodification pro-cess, which includes an improved smoothing be-haviour and an analysis algorithm that performsaccording to the granularity of the bilingual sym-bols in the models; to continue, section 4 deals24with an exhaustive report on experiments; and fi-nally, the conclusions are stated in the last section.2 Finite state modelsA stochastic finite-state automaton A is a tuple(?, Q, i, f, P ), where ?
is an alphabet of symbols,Q is a finite set of states, functions i : Q ?
[0, 1]and f : Q ?
[0, 1] refer to the probability of eachstate to be, respectively, initial and final, and par-cial function P : Q ?
{?
?
?}
?
Q ?
[0, 1] de-fines a set of transitions between pairs of states insuch a way that each transition is labelled with asymbol from ?
(or the empty string ?
), and is as-signed a probability.
Moreover, functions i, f, andP have to respect the consistency property in or-der to define a distribution of probabilities on thefree monoid.
Consistent probability distributionscan be obtained by requiring a series of local con-straints which are similar to the ones for stochasticregular grammars (Vidal et al, 2005):?
?q?Qi(q) = 1?
?q ?
Q :???{???},q?
?QP (q, ?, q?
)+f(q) = 1A stochastic finite-state transducer is definedsimilarly to a stochastic finite-state automaton,with the difference that transitions between statesare labelled with pairs of symbols that belong totwo different (input and output) alphabets, thatis, (?
?
?)
?
(?
?
?).
Then, given some in-put and output strings, s and t, a stochastic finite-state transducer is able to associate them a jointprobability Pr(s, t).
An example of a stochasticfinite-state transducer can be observed in Figure 1.10.20.20.20.3a : CAb : ?
0.3c : C 0.2c : C 0.2c : BC 0.5c : ABC 0.8?
: ?
0.3 ?
: ?
0.3?
: ?
0.7?
: ?
0.8Q0Q1Q2Q3Q4Figure 1: A stochastic finite-state transducer2.1 Inference of stochastic transducersThe GIATI methodology (Casacuberta et al,2005) has been revealed as an interesting approachto infer stochastic finite-state transducers throughthe modelling of languages.
Rather than learn-ing translations, GIATI first converts every pairof parallel sentences in the training corpus into acorresponding extended-symbol string in order to,straight afterwards, infer a language model from.More concretely, given a parallel corpus con-sisting of a finite sample C of string pairs: first,each training pair (x?, y?)
?
?????
is transformedinto a string z?
?
??
from an extended alphabet,yielding a string corpus S; then, a stochastic finite-state automaton A is inferred from S; finally, tran-sition labels in A are turned back into pairs ofstrings of source/target symbols in ??
?
?
?, thusconverting the automaton A into a transducer T .The first transformation is modelled by some la-belling function L : ??
???
?
?
?, while the lasttransformation is defined by an inverse labellingfunction ?(?
), such that ?
(L(C)) = C. Build-ing a corpus of extended symbols from the originalbilingual corpus allows for the use of many usefulalgorithms for learning stochastic finite-state au-tomata (or equivalent models) that have been pro-posed in the literature on grammatical inference.2.2 Phrase-based n-gram transducersPhrase-based n-gram transducers represent an in-teresting application of the GIATI methodology,where the extended symbols are actually bilingualphrase pairs, and n-gram models are employed aslanguage models (Gonz?lez et al, 2008).
Figure 2shows a general scheme for the representation ofn-grams through stochastic finite state automata.NULLTRANSITIONSBACKOFFTRANSITIONSBACKOFFTRANSITIONSBIGRAMTRANSITIONSBACKOFFTRANSITIONSBACKOFFTRANSITIONSUNIGRAMHISTORY LEVEL 0<s> </s> <unk>TRANSITIONSTRIGRAMTRANSITIONS(N?1)?GRAMHISTORY LEVEL N?1TRANSITIONSN?GRAM.
.
.. .
.. .
....HISTORY LEVEL 2HISTORYLEVEL 1u1 u2 u3 uK1b1 b2 b3 bK2n1 n2 n3 nKNFigure 2: A finite-state n-gram model25The states in the model refer to all the n-gramhistories that have been seen in the string corpus Sin training time.
Consuming transitions jump fromstates in a determined layer to the one immediatelyabove, increasing the history level.
Once the toplevel has been reached, n-gram transitions allowfor movements inside this layer, from state to state,updating the history to the last n?
1 seen events.Given that an n-gram event?n?1?n?2 .
.
.
?2?1?0 is statistically statedas Pr(?0|?n?1?n?2 .
.
.
?2?1), then it is appro-priately represented as a finite state transitionbetween their corresponding up-to-date histories,which are associated to some states (see Figure 3).
?n?1?n?2 .
.
.
?2?1?0Figure 3: Finite-state representation of n-grams?n?1 .
.
.
?3?2?1 ?n?2 .
.
.
?2?1?0?0Therefore, transitions are labelled with a sym-bol from ?
and every extended symbol in ?
is atranslation pair coming from a phrase-based dic-tionary which is inferred from the parallel corpus.Nevertheless, backoff transitions to lower his-tory levels are taken for smoothing purposes.
Ifthe lowest level is reached and no transition hasbeen found for next word sj , then a transition tothe <unk> state is fired, thus considering sj as anon-starting word for any bilingual phrase in themodel.
There is only 1 initial state, which is deno-ted as <s>, and it is placed at the 1st history level.The inverse labelling function is applied overthe automaton transitions as in Figure 4, obtaininga single transducer (Casacuberta and Vidal, 2004).Q?QQ Q?On demande une activit?Action is requiredPr = pPr = pOn /?
demande /?
une /?activit?
/Action is requiredPr = 1Pr = 1 Pr = 1Figure 4: Phrase-based inverse labelling functionIntermediate states are artificially created sincethey do not belong to the original automatonmodel.
As a result, they are non-final states, withonly one incoming and one outcoming edge each.2.3 Transducer pruning via n-gram eventsGREAT implements this pruning technique, whichis inspired by some other statistical machine trans-lation decoders that usually filter their phrase-based translation dictionaries by means of thewords in the test set sentences (Koehn et al, 2007).As already seen in Figure 3, any n-gram eventis represented as a transition between their cor-responding historical states.
In order to be ableto navigate through this transition, the analy-sis must have reached the ?n?1 .
.
.
?3?2?1 stateand the remaining input must fit the source ele-ments of ?0.
In other words, the full source se-quence from the n-gram event ?n?1 .
.
.
?3?2?1?0has to be present in the test set.
Otherwise,its corresponding transition will not be able tobe employed during the analysis of the test setsentences.
As a result, n-gram events that arenot in the test set can skip their transition gener-ation, since they will not be affected during de-coding time, thus reducing the size of the model.If there is also a backoff probability that is asso-ciated to the same n-gram event, its correspond-ing transition generation can be skipped too, sinceits source state will never be reached, as it is thestate which represents the n-gram event.
Nev-ertheless, since trained extended-symbol n-gramevents would typically include more than n sourcewords, the verification of their presence or theirabsence inside the test set would imply hashing allthe test-set word sequences, which is rather im-practical.
Instead, a window size is used to hashthe words in the test set, then the trained n-gramevents are scanned on their source sequence usingthis window size to check if they might be skippedor not.
It should be clear that the bigger the win-dow size is, the more n-gram rejections there willbe, therefore the transducer will be smaller.
How-ever, the translation results will not be affected asthese disappearing transitions are unreachable us-ing that test set.
As the window size increases, theresulting filtered transducer is closer to the mini-mum transducer that reflects the test set sentences.3 Finite state decodingEquation 2 expresses the MT problem in terms ofa finite state model that is able to compute the ex-26pression Pr(s, t).
Given that only the input sen-tence is known, the model has to be parsed, takinginto account all possible t that are compatible withs.
The best output hypothesis t?
would be that onewhich corresponds to a path through the transduc-tion model that, with the highest probability, ac-cepts the input sequence as part of the input lan-guage of the transducer.Although the navigation through the model isconstrained by the input sentence, the search spacecan be extremely large.
As a consequence, onlythe most scored partial hypotheses are being con-sidered as possible candidates to become the solu-tion.
This search process is very efficiently carriedout by a beam-search approach of the well knownViterbi algorithm (Jelinek, 1998), whose temporalasymptotic cost is ?
(J ?
|Q| ?M), where M is theaverage number of incoming transitions per state.3.1 Parsing strategies: from words to phrasesThe trellis structure that is commonly employedfor the analysis of an input sentence through astochastic finite state transducer has a variable sizethat depends on the beam factor in a dynamicbeam-search strategy.
That way, only those nodesscoring at a predefined threshold from the best onein every stage will be considered for the next stage.A word-based parsing strategy would start withthe initial state <s>, looking for the best transi-tions that are compatible with the first word s1.The corresponding target states are then placedinto the output structure, which will be used for theanalysis of the second word s2.
Iteratively, everystate in the structure is scanned in order to get theinput labels that match the current analysis wordsi, and then to build an output structure with thebest scored partial paths.
Finally, the states thatresult from the last analysis step are then rescoredby their corresponding final probabilities.This is the standard algorithm for parsinga source sentence through an non-deterministicstochastic finite state transducer.
Nevertheless, itmay not be the most appropriate one when dealingwith this type of phrase-based n-gram transducers.As it must be observed in Figure 4, a set ofconsecutive transitions represent only one phrasetranslation probability after a given history.
Infact, the path from Q to Q?
should only be fol-lowed if the remaining input sentence, which hasnot been analysed yet, begins with the full inputsequence On demande une activit?.
Otherwise, itshould not be taken into account.
However, as faras the words in the test sentence are compatiblewith the corresponding transitions, and accordingto the phrase score, this (word) synchronous pars-ing algorithm may store these intermediate statesinto the trellis structure, even if the full path willnot be accomplished in the end.
As a consequence,these entries will be using a valuable position in-side the trellis structure to an idle result.
This willbe not only a waste of time, but also a distortionon the best score per stage, reducing the effectivepower of the beam parameter during the decoding.Some other better analysis options may be rejectedbecause of their a-priori lower score.
Therefore,this decoding algorithm can lead the system to aworse translation result.
Alternatively, the beamfactor can be increased in order to be large enoughto store the successful paths, thus more time willbe required for the decoding of any input sentence.On the other hand, a phrase-based analysis stra-tegy would never include intermediate states in-side a trellis structure.
Instead, these artificialstates are tried to be parsed through until an ori-ginal state is being reached, i.e.
Q?
in Figure 4.Word-based and phrase-based analysis are con-ceptually compared in Figure 5, by means of theirrespective edge generation on the trellis structure.WORD?BASED EDGESPHRASE?BASED EDGESQ Q?On demande une activit?Figure 5: Word-based and phrase-based analysisHowever, in order to be able to use a scrolledtwo-stage implementation of a Viterbi phrase-based analysis, the target states, which may bepositioned at several stages of distance from thecurrent one, are directly advanced to the next one.Therefore, the nodes in the trellis must be storedtogether with their corresponding last input posi-tion that was parsed.
In the same manner, statesin the structure are only scanned if their posi-tion indicator is lower than the current analysisword.
Otherwise, they have already taken it intoaccount so they are directly transfered to the nextstage.
The algorithm remains synchronous withthe words in the input sentence, however, on this27particular occasion, states in the i-th step of anal-ysis are guaranteed to have parsed at least untilthe i-th word, but maybe they have gone further.Figure 6 is a graphical diagram about this concept.QiQ?j Q?j Q?jQ?ji jOn demande une activit?Figure 6: A phrase-based analysis implementationMoreover, all the states that are being stored inthe successive stages, that is, the ones from the ori-ginal topology of the finite-state representation ofthe n-gram model, are also guaranteed to lead toa final state in the model, because if they are notfinal states themselves, then there will always be asuccessful path towards a final state.GREAT incorporates an analysis strategy thatdepends on the granularity of the bilingual sym-bols themselves so that a phrase-based decodingis applied when a phrase-based transducer is used.3.2 Backoff smoothingTwo smoothing criteria have been explored in or-der to parse the input through the GIATI model.First, a standard backoff behaviour, where back-off transitions are taken as failure transitions, wasimplemented.
There, backoff transitions are onlyfollowed if there is not any other successful paththat has been compatible with the remaining input.However, GREAT also includes another morerefined smoothing behaviour, to be applied overthe same bilingual n-gram transducers, wheresmoothing edges are interpreted in a different way.GREAT suggests to apply the backoff crite-rion according to its definition in the grammati-cal inference method which incorporated it intothe language model being learnt and that will berepresented as a stochastic finite-state automaton.In other words, from the n-gram point of view,backoff weights (or finite-state transitions) shouldonly be employed if no transitions are found in then-gram automaton for a current bilingual symbol.Nevertheless, input words in translation applica-tions do not belong to those bilingual languages.Instead, input sequences have to be analysed insuch a way as if they could be internally repre-senting any possible bilingual symbol from the ex-tended vocabulary that matches their source sides.That way, bilingual symbols are considered to be asort of input, so the backoff smoothing criterion isthen applied to each compatible, bilingual symbol.For phrase-based transducers, it means that for asuccessful transition (x?, y?
), there is no need to gobackoff and find other paths consuming that bilin-gual symbol, but we must try backoff transitionsto look for any other successful transition (x?
?, y??
),which is also compatible with the current position.Conceptually, this procedure would be as if theinput sentence, rather than a source string, was ac-tually composed of a left-to-right bilingual graph,being built from the expansion of every input wordinto their compatible, bilingual symbols as in acategory-based approach.
Phrase-based bilingualsymbols would be graphically represented as a sortof skip transitions inside this bilingual input graph.This new interpretation about the backoffsmoothing weights on bilingual n-gram models,which is not a priori a trivial feature to be included,is easily implemented for stochastic transducersby considering backoff transitions as ?/?
transi-tions and keeping track of a dynamic list of forbid-den states every time a backoff transition is taken.An outline about the management of state ac-tiveness, which is integrated into the parsing algo-rithm, is shown below:ALGORITHMfor Q in {states to explore}for Q-Q?
in {transitions} (a)if Q?
is active[...]set Q?
to inactiveif Q is not NULLif Q not in the top levelfor Q?
in {inactive states}set Q?
to activeQ??
:= backoff(Q?
)set Q??
to inactiveQ := backoff(Q)GoTo (a)else[...]for Q?
in {inactive states}set Q?
to active[...]END ALGORITHM28The algorithm will try to translate several con-secutive input words as a whole phrase, always al-lowing a backoff transition in order to cover allthe compatible phrases in the model, not only theones which have been seen after a given history,but after all its suffixes as well.
A dynamic listof forbidden states will take care to accomplish anexploration constraint that has to be included intothe parsing algorithm: a path between two statesQ and Q?
has necessarily to be traced through theminimum number of backoff transitions; any otherQ-Q?
or Q-Q?
paths, where Q?
is the destinationof a Q?-Q?
backoff path, should be ignored.
Thisconstraint will cause that only one transition perbilingual symbol will be followed, and that it willbe the highest in the hierarchy of history levels.Figure 7 shows a parsing example over a finite-state representation of a smoothed bigram model.Q<backoff>p1p1p1p2p2p2p3p3?Figure 7: Compatible edges for a bigram model.Given a reaching state Q, let us assume that thetransitions that correspond to certain bilingualphrase pairs p1 , p2 and p3 are all compatible withthe remaining input.
However, the bigram (Q,p3) did not occur throughout the training corpus,therefore there is no a direct transition from Q top3 .
A backoff transition enables the access to p3because the bigram (Q, p3) turns into a unigramevent that is actually inside the model.
Unigramtransitions to p1 and p2 must be ignored becausetheir corresponding bigram events were success-fully found one level above.4 ExperimentsGREAT has been successfully employed to workwith the French-English EuroParl corpus, that is,the benchmark corpus of the NAACL 2006 sharedtask of the Workshop on Machine Translationof the Association for Computational Linguistics.The corpus characteristics can be seen in Table 1.Table 1: Characteristics of the Fr-En EuroParl.French EnglishSentences 688031Training Run.
words 15.6 M 13.8 MVocabulary 80348 61626Sentences 2000Dev-Test Run.
words 66200 57951The EuroParl corpus is built on the proceedingsof the European Parliament, which are publishedon its web and are freely available.
Because ofits nature, this corpus has a large variability andcomplexity, since the translations into the differ-ent official languages are performed by groups ofhuman translators.
The fact that not all transla-tors agree in their translation criteria implies that agiven source sentence can be translated in variousdifferent ways throughout the corpus.Since the proceedings are not available in everylanguage as a whole, a different subset of the cor-pus is extracted for every different language pair,thus evolving into somewhat a different corpus foreach pair of languages.4.1 System evaluationWe evaluated the performance of our methods byusing the following evaluation measures:BLEU (Bilingual Evaluation Understudy) score:This indicator computes the precision of uni-grams, bigrams, trigrams, and tetragramswith respect to a set of reference translations,with a penalty for too short sentences (Pap-ineni et al, 2001).
BLEU measures accuracy,not error rate.WER (Word Error Rate): The WER criterion calcu-lates the minimum number of editions (subs-titutions, insertions or deletions) that areneeded to convert the system hypothesis intothe sentence considered ground truth.
Be-cause of its nature, this measure is very pes-simistic.Time.
It refers to the average time (in milliseconds)to translate one word from the test corpus,without considering loading times.294.2 ResultsA set of experimental results were obtained in or-der to assess the impact of the proposed techniquesin the work with phrase-based n-gram transducers.By assuming an unconstrained parsing, that is,the successive trellis structure is large enough tostore all the states that are compatible within theanalysis of a source sentence, the results are notvery sensitive to the n-gram degree, just showingthat bigrams are powerful enough for this corpus.However, apart from this, Table 2 is also show-ing a significant better performance for the second,more refined behaviour for the backoff transitions.Table 2: Results for the two smoothing criteria.nBackoff 1 2 3 4 5baselineBLEU 26.8 26.3 25.8 25.7 25.7WER 62.3 63.9 64.5 64.5 64.5GREATBLEU 26.8 28.0 27.9 27.9 27.9WER 62.3 61.9 62.0 62.0 62.0From now on, the algorithms will be tested onthe phrase-based bigram transducer, being builtaccording to the GIATI method, where backoff isemployed as ?/?
transitions with forbidden states.In these conditions, the results, following aword-based and a phrase-based decoding strategy,which are in function of the dynamic beam factor,can be analysed in Tables 3 and 4.Table 3: Results for a word-based analysis.beam Time (ms) BLEU WER1.00 0.1 0.4 94.61.02 0.3 12.8 81.91.05 5.2 20.0 74.01.10 30.0 24.9 68.21.25 99.0 27.1 64.61.50 147.0 27.5 62.92.00 173.6 27.8 62.13.50 252.3 28.0 61.9From the comparison of Tables 3 and 4, it canbe deduced that a word-based analysis is itera-tively taking into account a quite high percentageof useless states, thus needing to increase the beamparameter to include the successful paths into theanalysis.
The price for considering such a long listTable 4: Results for a phrase-based analysis.beam Time (ms) BLEU WER1.00 0.2 19.8 71.81.02 0.4 22.1 68.61.05 0.7 24.3 66.01.10 2.4 26.1 64.21.25 7.0 27.1 62.81.50 9.7 27.5 62.32.00 11.4 27.8 62.03.50 12.3 28.0 61.9of states in every iteration of the algorithm is interms of temporal requirements.However, a phrase-based approach only storesthose states that have been successfully reached bya full phrase compatibility with the input sentence.Therefore, it takes more time to process an indi-vidual state, but since the list of states is shorter,the search method performs at a better speed rate.Another important element to point out betweenTables 3 and 4, is about the differences on qualityresults for a same beam parameter in both tables.Word-based decoding strategies suffer the effec-tive reduction on the beam factor that was men-tioned on section 3.1 because their best scores onevery analysis stage, which determine the explo-ration boundaries, may refer to a no way out path.Logically, these differences are progressively re-duced as the beam parameter increases, since thesearch space is explored in a more exhaustive way.Table 5: Number of trained and survived n-grams.n-gramsWindow size unigrams bigramsNo filter 1,593,677 4,477,3822 299,002 512,9433 153,153 141,8834 130,666 90,2655 126,056 78,8246 125,516 77,341On the other hand, a phrase-based extended-symbol bigram model, being learnt by means ofthe full training data, computes an overall set ofapproximately 6 million events.
The applicationof the n-gram pruning technique, using a grow-ing window parameter, can effectively reduce thatnumber to only 200,000.
These n-grams, whenrepresented as transducer transitions, suppose a re-duction from 20 million transitions to only those30500,000 that are affected by the test set sentences.As a result, the size of the model to be parseddecreases, therefore, the decoding time also de-creases.
Tables 5 and 6 show the effect of thispruning method on the size of the transducers, thenon the decoding time with a phrase-based analysis,which is the best strategy for phrase-based models.Table 6: Decoding time for several windows sizes.Window size Edges Time (ms)No filter 19,333,520 362.42 2,752,882 41.33 911,054 17.34 612,006 12.85 541,059 11.96 531,333 11.8Needless to say that BLEU and WER keep ontheir best numbers for all the transducer sizes asthe test set is not present in the pruned transitions.5 ConclusionsGIATI is a grammatical inference technique tolearn stochastic transducers from bilingual datafor their usage in statistical machine translation.Finite-state transducers are able to model both thestructure of both languages and their relationship.GREAT is a finite-state toolkit which was born toovercome the computational problems that previ-ous implementations of GIATI present in practicewhen huge amounts of parallel data are employed.Moreover, GREAT is the result of a very metic-ulous study of GIATI models, which improvesthe treatment of smoothing transitions in decod-ing time, and that also reduces the required time totranslate an input sentence by means of an analysisthat will depend on the granularity of the symbols.A pruning technique has been designed for n-gram approaches, which reduces the transducersize to integrate only those transitions that are re-ally required for the translation of the test set.
Thathas allowed us to perform some experiments con-cerning a state-of-the-art, voluminous translationtask, such as the EuroParl, whose results have beenreported in depth.
A better performance has beenfound when a phrase-based decoding strategy isselected in order to deal with those GIATI phrase-based transducers.
Besides, this permits us to ap-ply a more refined interpretation of backoff transi-tions for a better smoothing translation behaviour.AcknowledgmentsThis work was supported by the ?Vicerrectoradode Innovaci?n y Desarrollo de la UniversidadPolit?cnica de Valencia?, under grant 20080033.ReferencesFrancisco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Comput.
Linguistics, 30(2):205?225.Francisco Casacuberta, Hermann Ney, Franz JosefOch, Enrique Vidal, Juan Miguel Vilar, Sergio Bar-rachina, Ismael Garc?a-Varea, David Llorens, C?sarMart?nez, and Sirko Molau.
2004.
Some ap-proaches to statistical and finite-state speech-to-speech translation.
Computer Speech & Language,18(1):25?47.Francisco Casacuberta, Enrique Vidal, and David Pic?.2005.
Inference of finite-state transducers from reg-ular languages.
Pattern Recognition, 38(9):1431?1443.F.
Casacuberta.
2000.
Inference of finite-state trans-ducers by using regular grammars and morphisms.In A.L.
Oliveira, editor, Grammatical Inference: Al-gorithms and Applications, volume 1891 of LectureNotes in Computer Science, pages 1?14.
Springer-Verlag.
5th International Colloquium GrammaticalInference -ICGI2000-.
Lisboa.
Portugal.J.
Gonz?lez and F. Casacuberta.
2007.
Phrase-basedfinite state models.
In Proceedings of the 6th In-ternational Workshop on Finite State Methods andNatural Language Processing (FSMNLP), Potsdam(Germany), September 14-16.J.
Gonz?lez, G. Sanchis, and F. Casacuberta.
2008.Learning finite state transducers using bilingualphrases.
In 9th International Conference on Intel-ligent Text Processing and Computational Linguis-tics.
Lecture Notes in Computer Science, Haifa, Is-rael, February 17 to 23.Frederick Jelinek.
1998.
Statistical Methods forSpeech Recognition.
The MIT Press, January.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL.
The Association for Computer Linguistics.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.31David Pic?.
2005.
Combining Statistical and Finite-State Methods for Machine Translation.
Ph.D. the-sis, Departamento de Sistemas Inform?ticos y Com-putaci?n.
Universidad Polit?cnica de Valencia, Va-lencia (Spain), September.
Advisor: Dr. F. Casacu-berta.E.
Vidal, F. Thollard, F. Casacuberta C. de la Higuera,and R. Carrasco.
2005.
Probabilistic finite-state ma-chines - part I. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 27(7):1013?1025.32
