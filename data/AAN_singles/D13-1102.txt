Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000?1010,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsIdentifying Web Search Query Reformulation using Concept basedMatchingAhmed HassanMicrosoft ResearchOne Microsoft WayRedmond, WA 98053, USAhassanam@microsoft.comAbstractWeb search users frequently modify theirqueries in hope of receiving better results.This process is referred to as ?Query Refor-mulation?.
Previous research has mainly fo-cused on proposing query reformulations inthe form of suggested queries for users.
Someresearch has studied the problem of predictingwhether the current query is a reformulationof the previous query or not.
However, thiswork has been limited to bag-of-words modelswhere the main signals being used are wordoverlap, character level edit distance and wordlevel edit distance.
In this work, we showthat relying solely on surface level text sim-ilarity results in many false positives wherequeries with different intents yet similar top-ics are mistakenly predicted as query reformu-lations.
We propose a new representation forWeb search queries based on identifying theconcepts in queries and show that we can sig-nificantly improve query reformulation perfor-mance using features of query concepts.1 IntroductionWeb search is a process of querying, learning andreformulating queries to satisfy certain informationneeds.
When a user submits a search query, thesearch engine attempts to return the best results tothat query.
Oftentimes, users modify their searchqueries in hope of getting better results.
Typicalsearch users have low tolerance to viewing lowlyranked search results and they prefer to reformu-late the query rather than wade through result list-ings (Jansen and Spink, 2006).
Previous stud-ies have also shown that 37% of search queriesare reformulations to previous queries (Jansen etal., 2007) and that 52% of users reformulate theirqueries (Jansen et al 2005).Understanding query reformulation behavior andbeing able to accurately identify reformulationqueries have several benefits.
One of these benefitsis learning from user behavior to better suggest au-tomatic query refinements or query alterations.
An-other benefit is using query reformulation predic-tion to identify boundaries between search tasks andhence segmenting user activities into topically co-herent units.
Also, if we are able to accurately iden-tify query reformulations, then we will be in a bet-ter position to evaluate the satisfaction of users withquery results.
For example, search satisfaction istypically evaluated using clickthrough informationby assuming that if a user clicks on a result, andpossibly dwells for a certain amount of time, thenthe user is satisfied.
Identifying query reformulationcan be very useful for finding cases where the usersare not satisfied even after a click on a result thatmay have seemed relevant given its title and sum-mary but then turned out to be not relevant to theuser?s information need.Previous work on query reformulation has eitherfocused on automatic query refinement by the searchsystem, e.g.
(Jones et al 2006; Boldi et al2008) or on defining taxonomies for query refor-mulation strategies, e.g.
(Lau and Horvitz, 1999;Anick, 2003).
Other work has proposed solutionsfor the query reformulation prediction problem orfor the similar problem of task boundary identifi-cation (Radlinski and Joachims, 2005; Jones andKlinkner, 2008).
These solutions have adopted the1000bag-of-words approach for representing queries andmostly used features of word overlap or characterand word level edit distances.
Take the queries ?ho-tels in New York City?
and ?weather in New YorkCity?
as an example.
The two queries are very likelyto have been issued by a user who is planning totravel to New York City.
The two queries have 5words each, 4 of them are shared by the two queries.Hence, most of the solutions proposed in previouswork for this problem will incorrectly assume thatthe second query is a reformulation of the first dueto the high word overlap ratio and the small edit dis-tance.
In this work, we propose a method that goesbeyond the bag-of-words method by identifying theconcepts underlying these queries.
In the previousexample, we would like our method to realize thatin the first query, the user is searching for ?hotels?while for in the second query, she is searching for the?weather?
in New York City.
Hence, despite similarin terms of shared terms, the two queries have differ-ent intents and are not reformulations of one another.To this end, we conducted a study where we col-lected thousands of consecutive queries and trainedjudges to label them as either reformulations ornot.
We then built a classifier to identify queryreformulation pairs and showed that the proposedclassifier outperforms the state-of-the-art methodson identifying query reformulations.
The proposedmethod significantly reduces false positives (non-reformulation pairs incorrectly classified as refor-mulation) while achieving high recall and precision.2 Related WorkThere are three areas of work related to the researchpresented in this paper: (i) query reformulation tax-onomies, (ii) automatic query refinement, and (iii)search tasks boundary identification.
We cover eachof these areas in turn.2.1 Query Reformulation TaxonomiesExisting research has studied how web search en-gines can propose reformulations, but has given lessattention to how people perform query reformula-tions.
Most of the research on manual query re-formulation has focused on building taxonomies ofquery reformulation.
These taxonomies are gener-ally constructed by examining a small set of querylogs.
Anick (2003) classified a random sample of100 reformulations by hand into eleven categories.Jensen et al(2007) identified 6 different kindsof reformulation states (New, Assistance, ContentChange, Generalization, Reformulation, and Spe-cialization) and provided heuristics for identifyingthem.
They also used them to predict when a useris most receptive to automatic query suggestions.The same categories were used in several other stud-ies (Guo et al 2008; Lau and Horvitz, 1999).Huang and Efthimis (2010) proposed another re-formulation taxonomy.
Their taxonomy was lexi-cal in nature (e.g., word reorder, adding words, re-moving words, etc.).
They also proposed the use ofregular expressions to identify them.
While study-ing re-finding behavior, Teevan et al(2006) con-structed a taxonomy of query re-finding by manu-ally examining query logs, and implemented algo-rithms to identify repeat queries, equal click queriesand overlapping click queries.
None of this workhas built an automatic classifier distinguishing refor-mulation queries from other queries.
Heuristics andregular expressions have been used in (Huang et al2010) and (Jansen et al 2007) to identify differenttypes of reformulations.
This line of work is relevantto our work because it studies query reformulationstrategies.
Our work is different because we build amachine-learned predictive model to identify queryreformulation while this line of work mainly focuseson defining taxonomies for reformulation strategies.2.2 Automatic Query RefinementA close problem that has received most of the re-search attention in this area is the problem of auto-matically generating query refinements.
These re-finements are typically offered as query suggestionsto the users or used to alter the user query beforesubmitting it to the search engine.Boldi et al(2008) introduced the concept ofthe query-flow graph where every query is repre-sented by a node and edges connect queries if it islikely for users to move from one query to another.Mei et al(2008) used random walks over a bipar-tite graph of queries and URLs to find query refine-ments.
Query logs were used to suggest query re-finements in (Baeza-Yates et al 2005).
Hierarchi-cal agglomerative clustering was used to group sim-ilar queries that can be used as suggestions for one1001another.
Other research has adopted methods basedon query expansion (Mitra et al 1998) or querysubstitution (Jones et al 2006).
This line of workis different from our work because it focuses on au-tomatically generating query refinements while thiswork focuses on identifying cases of manual queryreformulations.2.3 Search Task Boundary IdentificationThe problem of classifying the boundaries of theuser search tasks within sessions in web search logshas been widely addressed before.
This problem isclosely related to the problem of identifying queryreformulation.
A search task has been defined in(Jones and Klinkner, 2008) as a single informationneed that may result in one or more queries.
Sim-ilarly , Jansen et al(2007) defined a session asa series of interactions by the user toward address-ing a single information need.
On the other hand, aquery reformulation is intended to modify a previ-ous query in hope of getting better results to satisfythe same information need.
From these definitions,it is clear how query reformulation and task bound-ary detection are two sides of the same problem.Boldi et al(2008) presented the concept of thequery-flow graph.
A query-flow graph representschains of related queries in query logs.
They usethis model for finding logical session boundaries andquery recommendation.
Ozmutlu (2006) proposeda method for identifying new topics in search logs.He demonstrated that time interval, search patternand position of a query in a user session, are ef-fective for shifting to a new topic.
Radlinski andJoachims (2005) study sequences of related queries(query chains).
They used that to generate new typesof preference judgments from search engine logs tolearn better ranked retrieval functions.Arlitt (2000) found session boundaries using acalculated timeout threshold.
Murray et al(2006)extended this work by using hierarchical clusteringto find better timeout values to detect session bound-aries.
Jones and Klinkner (2008) also addressedthe problem of classifying the boundaries of thegoals and missions in search logs.
They showedthat using features like edit distance and commonwords achieves considerably better results comparedto timeouts.
Lucchese et al(Lucchese et al 20011)uses a similar set of features as (Jones and Klinkner,2008), but uses clustering to group queries in thesame task together as opposed to identifying taskboundary as in (Jones and Klinkner, 2008).
Thisline of work is perhaps the closest to our work.
Ourwork is different because it goes beyond the bag ofwords approach and tries to assess query similaritybased on the concepts represented in each query.
Wecompare our work to the state-of-the-art work in thisarea later in this paper.3 Problem DefinitionWe start by defining some terms that will be usedthroughout the paper:Definition: Query Reformulation is the act ofsubmitting a query Q2 to modify a previous searchquery Q1 in hope of retrieving better results to sat-isfy the same information need.Definition: A Search Session is group of queriesand clicks demarcated with a 30-minute inactiv-ity timeout, such as that used in previous work(Downey et al 2007; Radlinski and Joachims,2005).Search engines receive streams of queries fromusers.
In response to each query, the engine returns aset of search results.
Depending on these results, theuser may decide to click on one or more results, sub-mit another query, or end the search session.
In thiswork, we focus on cases where the user submits an-other query.
Our objective is to solve the followingproblem: Given a queryQ1, and the following queryQ2, predict whether Q2 is reformulation of Q1.4 ApproachIn this section, we propose methods for predictingwhether the current query has been issued by theuser to reformulate the previous query.4.1 Query NormalizationWe perform standard normalization where we re-place all letters with their corresponding lower caserepresentation.
We also replace all runs of whites-pace characters with a single space and remove anyleading or trailing spaces.
In addition to the stan-dard normalization, we also break queries that do notrespect word boundaries into words.
Word break-ing is a well-studied topic that has proved to be1002Table 1 : Examples of queries, the corresponding segmentation, and the concept representation.Phrases are separated by ?|?
and different tokens in a keyword are separated by ?
?Query Phrases and Keywords Concept Representationhotels in new york city hotels in new york city Concept1 {head=?hotels?,modifiers = ?new york city?
}hyundai roadside assistance hyundai roadside assistance Concept1 {head = ?roadsidephone number | phone number assistance?, modifiers = ?hyundai?
},Concept2{?phone number?
}kodak easyshare recharger chord kodak easyshare recharger cord Concept1{head =?recharger cord?,modifiers ??
?kodak easyshare?
}user reviews for apple iphone user reviews for apple iphone Concept1{head=?user reviews?, modifiers = ?apple iphone?
}user reviews for apple ipad user reviews for apple ipad Concept1{head =?user reviews?,modifiers = ?apple ipad?
}tommy bhama rug tommy bhama rug Concept1{head =?rug?,modifiers ??
?tommy bhama?
}tommy bhama perfume tommy bhama perfume Concept1{head =?perfume?,modifiers ??
?tommy bhama?
}useful for many natural language processing appli-cations.
This becomes a frequent problems withqueries when users do not observe the correct wordboundaries (for example: ?southjerseycraigslist?
for?south jersey craiglist?)
or when users are searchingfor a part of a URL (for example ?quincycollege?for ?quincy college?).
We used a freely availableword breaker Web service that has been described at(Wang et al 2011).4.2 Queries to ConceptsLexical similarity between queries has been oftenused to identify related queries (Jansen et al 2007).The problem with lexical similarity is that it intro-duces many false negatives (e.g.
synonyms) , butthis can be handled by other features as we will de-scribe later.
More seriously, it introduces many falsepositives.
Take the following query pair as an exam-ple Q1: weather in new york city and Q2: ?hotels innew york city?.
Out of 5 words, 4 words are sharedbetween Q1 and Q2.
Hence, any lexical similarityfeature would predict that the user submitted Q2 asa reformulation of Q1.
What we would like to dois to have a query representation that recognizes thedifference between Q1 and Q2.If we look closely at the two queries, we will no-tice that in the first query, the user is looking forthe ?weather?, while in the second query the useris looking for ?hotels?.
We would like to recog-nize ?weather?, and ?hotels?
as the head keywordsof Q1 and Q2 respectively, while ?new york city?
isa modifier of the head keyword in both cases.
Tobuild such a representation, we start by segmentingeach query into phrases.
Query segmentation is theprocess of taking a users search query and divid-ing the tokens into individual phrases or semanticunits (Bergsma and Wang, 2007).
Many approachesto query segmentation have been presented in recentresearch.
Some of them pose the problem as a super-vised learning problem (Bergsma and Wang, 2007;Yu and Shi, 2009).
Many of the supervised methodsthough use expensive features that are difficult to re-implement.On the other hand, many unsupervised methodsfor query segmentation have also been proposed(Hagen et al 2011; Hagen et al 2010).
Mostof these methods use only raw web n-gram fre-quencies and are very easy to re-implement.
Ad-ditionally, Hagen et al(2010) have shown thatthese methods can achieve segmentation accuracycomparable to current state-of-the-art techniques us-ing supervised learning.
We opt for the unsuper-vised techniques to perform query segmentation.More specifically, we adopt the mutual information1003method (MI) used throughout the literature.
A seg-mentation for a query is obtained by computing thepointwise mutual information score for each pairof consecutive words.
More formally, for a queryx = {x1, x2, ..., xn}PMI(xi, xi+1) = logp(xi, xi+1)p(xi)p(xi+1)(1)where p(xi, xi+1) is the joint probability of occur-rence of the bigram (xi, xi+1) and p(xi) and p(xi+1)are the individual occurrence probabilities of the twotokens xi and xi+1 .A segment break is introduced whenever the pointwise mutual information between two consecutivewords drops below a certain threshold ?
.
The thresh-old we used, ?
= 0.895 , was selected to max-imize the break accuracy (Jones et al 2006) onthe Bergsma-Wang-Corpus (Bergsma and Wang,2007).
Furthermore, we do not allow a break to hap-pen between a noun and a proposition (e.g.
no breakcan be introduced between ?hotels?
and ?in?
or ?in?and ?new York?
in the query ?hotels in new yorkcity?).
We will shorty explain how we obtained thepart-of-speech tags.In addition to breaking the query into phrases, wewere also interested in grouping multi-word key-words together (e.g.
?new york?, ?Michael Jack-son?, etc.).
The intuition behind that is that aquery containing the keyword ?new york?
and an-other containing the keyword ?new mexico?
shouldnot be awarded because they share the word ?new?.We do that by adopting a hierarchical segmentationtechnique where the same segmentation method de-scribed above is reapplied to every resulting phrasewith a new threshold ?s < ?
.
We selected thenew threshold, ?
= 1.91 , to maximize the breakaccuracy over a set of a random sample of 10,000Wikipedia title of persons, cities, countries and or-ganizations and a random sample of bigrams and tri-grams from Wikipedia text.In our implementation, the probabilities for allwords and n-grams have been computed using thefreely available Microsoft Web N-Gram Service(Huang et al 2010).Now that we have the phrases and keywords ineach query, we assume that every phrase corre-sponds to a semantic unit.
Every semantic unithas a head and a zero or more modifiers.
Depen-dency parsing could be used to identify the headand modifiers from every phrase.
However, becausequeries are typical short and not always well-formedsentences, this may pose a challenge to the depen-dency parser.
But as we are mainly interested inshort noun phrases, we can apply a simple set ofrules to identify the head keyword of each phraseusing the part of speech tags of the words in thephrase.
A part-of-speech (POS) tagger assigns partsof speech to each word in an input text, such as noun,verb, adjective, etc.
We used the Stanford POS tag-ger, using Stanford CoreNLP, to assign POS tags toqueries (Toutanova et al 2003).
To identify the headand attributes of every noun phrase, we use the fol-lowing rules:?
For phrases with of the form: ?NNX+?
(i.e.
onemore nouns, where NNX could be NN: noun,singular, NNS: noun, plural, NNP: propernoun, singular or NNPS: proper noun, plural),the head is the last noun keyword and all otherkeywords are treated as attributes/modifiers.?
For the phrases of the form ?NNX+ IN NNX+?,where IN denotes a preposition or a subordinat-ing conjunction (e.g.
?in?, ?of?, etc.
), the headis the last noun keyword before the preposition.Table 1 shows different examples of queries,the corresponding phrases, keywords, and con-cepts.
For example the query ?kodak easysharerecharger chord?
consists of a single semanticunit (phrase) and two keywords ?Kodak easyshare?and ?recharger cord?.
The head of this semanticunit is the keyword ?recharger cord?
and ?kodakeasyshare?
is regarded as an attribute/modifier.
An-other example is the two queries ?tommy bhamarug?
and ?tommy bhama perfume?.
The head of theformer is ?rug?, while the head of the latter is ?per-fume?.
Both share the attribute ?tommy bhama?.This shows that the user had two different intentseven though most of the words in the two queriesare shared.4.3 Matching ConceptsPhrases in two concepts may have full term over-lap, partial term overlap, or no direct overlap yet aresemantically similar.
To capture concept similarity,1004we define four different ways of matching conceptsranked from the most to the least strict:?
Exact Match: The head and the attributes of thetwo concepts match exactly.?
Approximate Match: To capture spelling vari-ants and misspelling, we allow two keywords tomatch if the Levenshtein edit distance betweenthem is less than 2.?
Lemma Match: Lemmatization is the processof reducing an inflected spelling to its lexicalroot or lemma form.
We match two concepts ifthe lemmas of their keywords can be matched.?
Semantic Match: We compute the concept sim-ilarity by measuring the semantic similarity be-tween the two phrases from which the conceptswhere extracted.
Let Q = {q1, ..., qI} be onephrase and S = {s1, ..., sJ} be another, thesemantic similarity between these two phrasescan be measured by estimating the probabil-ity of one of them being a translation of an-other.
The translation probabilities can be es-timated using the IBM Model 1 (Brown et al1993; Berger and Lafferty, 1999).
The modelwas originally proposed to model the probabil-ity of translating from one sequence of wordsin one language to another.
It has been alsoused in different IR applications to estimate theprobability of translating from one sequenceof words to another sequence in the same lan-guage (e.g.
(Gao et al 2012), (Gao et al2010) and (White et al 2013)).
More for-mally, the similarity between two sequences ofwords, Q = {q1, ..., qI} and S = {s1, ..., sJ},can be defined as:P (S|Q) =I?i=1J?j=1P (si|qj)P (qj |Q) (2)where P (q|Q) is the unigram probability ofword q in query Q.
The word translation prob-abilities P (s|q) are estimated using the query-title pairs derived from the clickthrough searchlogs, assuming that the title terms are likely tobe the desired alternation of the paired query.The word translation probabilities P (s|q) (i.e.the model parameters ?)
are optimized by max-imizing the probability of generating documenttitles from queries over the entire training cor-pus:??
= argmax?M?i=1P (S|Q, ?)
(3)where P (S|Q, ?)
is defined as:P (S|Q, ?)
=(J+!
)II?i=1J?j=1P (si|qj) (4)where  is a constant, I is the token length of S,and J is the token length of Q.
The query-title pairsused for model training are sampled from one yearworth of search logs from a commercial search en-gine.
The search logs do not intersect with the searchlogs where the data described in Section5.1 has beensampled from.
S and Q are considered a match ifP (S|Q, ?)
> 0.5.4.4 Features4.4.1 Textual FeaturesJones and Klinkner (2008) showed that word andcharacter edit features are very useful for identifyingsame task queries.
The intuition behind this is thatconsecutive queries which have many words and/orcharacters in common tend to be related.
The fea-tures they used are:?
normalized Levenshtein edit distance?
1 if lev > 2, 0 otherwise?
Number of characters in common starting fromthe left?
Number of characters in common starting fromthe right?
Number of words in common starting from theleft?
Number of words in common starting from theright?
Number of words in common?
Jaccard distance between sets of words10054.4.2 Concept FeaturesAs we explained earlier the word and characteredit features capture similarity between many pairsof queries.
However, they also tend to mis-classifymany other pairs especially when the two queriesshare many words yet have different intents.
Weused the conceptual representation of queries de-scribed in the previous subsection to compute thefollowing set of features, notice that every featurehas two variants one at the concept level and theother at the keyword (head or attribute) level:?
Number of ?exact match?
concepts in common?
Number of ?approximate match?
concepts incommon?
Number of ?lemma match?
concepts in com-mon?
Number of ?semantic match?
concepts in com-mon?
Number of concepts in Q1?
Number of concepts in Q2?
Number of concepts in Q1 but not in Q2?
Number of concepts in Q1 but not in Q2?
1 if Q1 contains all Q2s concepts?
1 if Q2 contains all Q1s concepts?
all above features recomputed for keywords in-stead of concepts4.4.3 Other FeaturesOther features, that have been also used in (Jonesand Klinkner, 2008), include temporal features:?
time between queries in seconds?
time between queries as a binary feature (5mins, 10 mins, 20 mins, 30 mins, 60 mins, 120mins)and search results feature:?
cosine distance between vectors derived fromthe first 10 search results for the query terms.4.5 Predicting Reformulation TypeThere are different strategies users use to reformu-late a query which results in different types of queryreformulations:?
Generalization: A generalization reformula-tion occurs when the second query is intendedto seek more general information compared tothe first query?
Specification: A specification reformulationoccurs when the second query is intended toseek more specific information compared to thefirst query?
Spelling: A spelling reformulation occurswhen the second query is intended to correctone or more misspelled words in the first query?
Same Intent: A same intent reformulation oc-curs when the second query is intended to ex-press the same intent as the first query.
Thiscan be the result of word substitution or wordreorder.We used the following features to predict thequery reformulation type:?
Length (num.
characters and num.
words) ofQ1, Q2 and difference between them?
Number of out-of-vocabulary words in Q1, Q2and the difference between them?
num.
of ?exact match?
concepts in common?
num.
of ?approximate match?
concepts in com-mon?
num.
of ?lemma match?
concepts in common?
num.
of ?semantic match?
concepts in common?
num.
of concepts in Q1, Q2 and the differencebetween them?
num.
of concepts in Q1 but not in Q2?
num.
of concepts in Q1 but not in Q2?
1 if Q1 contains all Q2s concepts?
1 if Q2 contains all Q1s concepts?
all concept features above recomputed for key-words instead of concepts10065 Experiments and Results5.1 DataOur data consists of query pairs randomly sampledfrom the queries submitted to a commercial searchengine during a week in mid-2012.
Every recordin our data consisted of a consecutive query pair(Qi,Qi+1) submitted to the search engine by thesame user and in the same session (i.e.
within lessthan 30 minutes of idle time, the 30 minutes thresh-old has been frequently used in previous work, e.g.
(White and Drucker, 2007)).
Identical queries wereexcluded from the data because they are always la-beled as reformulation and their label is very easy topredict.
Hence, when included, they result in unre-alistically high estimates of the performance of theproposed methods.
All data in the session to whichthe sampled query pair belongs were recorded.
Inaddition to queries, the data contained a timestampfor each page view, all elements shown in responseto that query (e.g.
Web results, answers, etc.
), andvisited Web page or clicked answers.
Intranet andsecure URL visits were excluded.
Any personallyidentifiable information was removed from the dataprior to analysis.Annotators were instructed to exhaustively exam-ine each session and ?re-enact?
the user?s experi-ence.
The annotators inspected the entire searchresults page for each of Qi and Qi+1, includingURLs, page titles, relevant snippets, and other fea-tures.
They were also shown clicks to aid them intheir judgments.
Additionally, they were also shownqueries and clicks before and after the query pair ofinterest.
They were asked to then use their assess-ment of the user?s objectives to determine whetherQi+1 is a reformulation of Qi.
Each query pair waslabeled by three judges and the majority vote amongjudges was used.
Because the number of positiveinstances is much smaller than the number of neg-ative instances, we used all positive instances andan equal number of randomly selected negative in-stances leaving us with approximately 6000 querypairs.Judges were also asked to classify reformulationsinto one of four different categories: Generalization(second query is intended to seek more general in-formation), Specification (second query is intendedto seek more specific information), Spelling (second00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallHeurisitcTextualConceptsAllFigure 1 Precision-Recall Curves for the Reformu-lation Prediction Methods00.050.10.150.20.250.30.350.40.450.5Same intent Spelling Generalization SpecificationFigure 2 Distribution of Query ReformulationTypesquery is intended to correct spelling mistakes), andSame Intent (second query is intended to express thesame intent in a different way).5.2 Predicting Query ReformulationIn this section we describe the experiments we con-ducted to evaluate the reformulation prediction clas-sifier.
We perform experiments using the data de-scribed in the previous section.
We compare the per-formance of four different systems:?
The first one, Heuristic, simply computes thesimilarity between two queries as the percent-age of common words to the length of thelonger query in terms of the number of words.1007Table 2 : Heuristics vs. Textual vs. Concept Features for Reformulation PredictionAccuracy Reform.
F1 No-Reform.
F1Heuristics 77.10% 75.60% 69.07%Textual 82.90% 71.75% 87.75%Concepts 87.60% 81.20% 90.78%All 89.02% 83.63% 91.75%00.10.20.30.40.50.60.70.80.91Same intent Spelling Generalization SpecificationPrecision RecallFigure 3 Precision and Recall for Query Reformu-lation Type PredictionWhen finding common words, it allows twowords to be matched if their Levenshtein editdistance is less than or equals 2.
The secondquery is predicted to be a reformulation of thefirst if similarity?
?sim and the time difference?
?time minutes.
The two thresholds were setto 0.35 and 5 minutes respectively using gridsearch to maximize accuracy over the trainingdata.?
The second system, Textual, uses the textualfeatures from previous work that have been de-scribed in Section 4.4.1 and the temporal andresults features described in Section 4.4.3.?
The third system, Concepts, uses the conceptfeatures that we presented in Section 4.4.2 andthe temporal and results features described inSection 4.4.3.?
Finally, the last system, All, uses both the tex-tual features, the conceptual features and thetemporal and results features.For all methods, we used gradient boosted regres-sion trees as a classifier with 10-fold cross valida-tion.
We also tried other classifiers like SVM andlogistic regression but we got the best performanceusing the gradient boosted regression trees.
All re-ported differences are statistically significant at the0.05 level according to a two-tailed student t-test.The accuracy, positive (reformulation) F1, andnegative (non-reformulation) F1 for the four meth-ods are shown in Table 2.
The precision recallcurves for all methods are shown in Figure 1; theheuristic method uses fixed thresholds resulting ina single operating point.
The results show thatthe concept features outperform the textual features.Combining them together results in a small gainover using the concept features only.
The conceptfeatures were able to achieve higher precision rateswhile not sacrificing recall because they were moreeffective in eliminating false reformulation cases.We examined the cases where the classifier failedto predict the correct label to understand when theclassifier fails to work properly.
We identified sev-eral cases where this happens.
For example, theclassifier failed to match some terms that have thesame semantic meaning.
Many of these cases wereacronyms (e.g.
?AR?
and ?Accelerated Reader?,?GE?
and ?General Electric?).
These cases canbe handled by using a semantic matching methodthat yields higher coverage especially in cases ofacronyms.The classifier also failed in cases where the key-word extractor and/or the POS tagger failed to cor-rectly parse the queries (e.g.
?last to know?wasnot recognized as a song name).
These cases canbe handled by identifying named entities as a pre-processing step and treating them accordingly whenidentifying keywords or assigning POS tags to key-words.1008Another dominant class of cases where the classi-fier failed were cases where the dependency rulesfailed to correctly identify the head keyword in aquery.
In many such cases, the query was a non well-formed sequence of words (e.g.
?dresses Christmastoddler?).
This is the hardest class to handle.
Sinceit is hard to correctly parse short text and it is evenharder when the text it is not well-formed.5.3 Predicting Reformulation TypeWe conducted another experiment to evaluate theperformance of the reformulation type classifier.
Weperformed experiments using the data described ear-lier where judges were asked to select the type ofreformulation for every reformulation query.
Thedistribution of reformulations across types is shownin Figure 2.
The figure shows that most popular re-formulations types are those where users move toa more specific intent or express the same intent ina different way.
Reformulations with spelling sug-gestions and query generalizations are less popular.We conducted a one-vs-all experiment using gradi-ent boosted regression trees with 10-fold cross val-idation.
The precision and recall of every type areshown in Figure 3.
The micro-averaged and macro-averaged accuracy was 78.13% and 72.52% respec-tively.6 ConclusionsIdentifying query reformulations is an interestingand useful application in Information Retrieval.
Re-formulation identification is useful for automaticquery refinements, task boundary identification andsatisfaction prediction.
Previous work on this prob-lem has adopted a bag-of-words approach wherelexical similarity and word overlap are the key fea-tures for identifying query reformulation.
We pro-posed a method for identifying concepts in searchqueries and using them to identify query reformula-tions.
The proposed method outperforms previouswork because it can better represent the informationintent underlying the query and hence can better as-sess query similarity.
We showed that the proposedmethod significantly outperforms the other methods.We also showed that we can reliably predict the typeof the reformulation with high accuracy.ReferencesPeter Anick.
2003.
Learning noun phrase query segmen-tation.
In Proceedings of the 26th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pages 88?95.M.
Arlitt.
2000.
Characterizing web user sessions.
ACMSIGMETRICS Performance Eval Review, 28(2):50?63.Ricardo Baeza-Yates, Carlos Hurtado, Marcelo Men-doza, and Georges Dupret.
2005.
Modeling usersearch behavior.
In LA-WEB ?05: Proceedings of theThird Latin American Web Congress, Washington, DC,USA.
IEEE Computer Society.A.
L. Berger and J. Lafferty.
1999.
Information retrievalas statistical translation.
In Proceedings of the 22thannual international ACM SIGIR conference on Re-search and development in information retrieval (SI-GIR 1999), pages 222?229.S.
Bergsma and I. Q. Wang.
2007.
Learning noun phrasequery segmentation.
In Proceedings of the Conferenceon Empirical Methods on Natural Language Process-ing, pages 816?826.Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-ora Donato, Aristides Gionis, and Sebastiano Vigna.2008.
The query-flow graph: model and applications.In Proceeding of the 17th ACM conference on In-formation and knowledge management (CIKM 2008),pages 609?618.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Computa-tional Linguistics, 19(2):263?311.Doug Downey, Susan Dumais, and Eric Horvitz.
2007.Models of searching and browsing: Languages, stud-ies, and applications.
Journal of the American Soci-ety for Information Science and Technology (JASIST),58(6):862?871.J.
Gao, X.
He, and J. Nie.
2010.
Clickthrough-basedtranslation models for web search: from word mod-els to phrase models.
In Proceeding of the ACM con-ference on Information and knowledge management(CIKM 2010), pages 1139?1148.J.
Gao, S. Xie, X.
He, and A. Ali.
2012.
Learning lexiconmodels from search logs for query expansion.
In Pro-ceeding of the Conference on Emprical Methods forNatural Language Processing (EMNLP 2012).J.
Guo, G. Xu, H. Li, and X. Cheng.
2008.
A unified anddiscriminative model for query refinement.
In Pro-ceedings of the annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 379?386.M.
Hagen, M. Potthast, B. Stein, and C. Brautigam.2010.
The power of naiv query segmentation.
In Pro-ceeding of the ACM Conference of the Special Interest1009Group on Information Retrieval (SIGIR 2010), pages797?798,.M.
Hagen, M. Potthast, B. Stein, and C. Brautigam.2011.
Query segmentation revisited.
In Proceeding ofthe ACM World Wide Web Conference (WWW 2011),pages 97?106.J.
Huang, J. Gao, J. Miao, X. Li, K. Wang, and F. Behr.2010.
Exploring web scale language models for searchquery processing.
In Proceeding of the ACM WorldWide Web Conference (WWW 2010), pages 451?460.Bernard J. Jansen and Amanda Spink.
2006.
How are wesearching the world wide web?
: a comparison of ninesearch engine transaction logs.
Inf.
Process.
Manage.,42:248?263, January.B.
J. Jansen, A. Spink, and J. Pedersen.
2005.
A tem-poral comparison of altavista web searching.
Journalof the American Society for Information Science andTechnology, 56:559?570.B.
J. Jansen, M. Zhang, and A. Spink.
2007.
Patterns andtransitions of query reformulation during web search-ing.
International Journal of Web Information Sys-tems.Rosie Jones and Kristina Klinkner.
2008.
Beyond thesession timeout: Automatic hierarchical segmentationof search topics in query logs.
In Proceedings of ACM17th Conference on Information and Knowledge Man-agement (CIKM 2008).Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substititions.
In Pro-ceedings of the Fifteenth International Conference onthe World-Wide Web (WWW06), pages 387?396.Tessa Lau and Eric Horvitz.
1999.
Patterns of search:Analyzing and modeling web query refinement.
InACM Press, editor, Proceedings of the Seventh Inter-national Conference on User Modeling.C.
Lucchese, S. Orlando, R. Perego, F. Silvestri, andG.
Tolomei.
20011.
Identifying task-based sessionsin search engine query logs.
In Proceedings of ACMConference on Web Search and Data Mining(WSDM2011).Q.
Mei, D. Zhou, and K. Church.
2008.
Query sug-gestion using hitting time.
In Proceeding of the 17thACM conference on Information and knowledge man-agement (CIKM 2008), pages 469?478.M.
Mitra, A. Singhal, and C. Buckley.
1998.
Improv-ing automatic query expansion.
In Proceedings ofthe 21th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 206?214.G.
V. Murray, J. Lin, and A. Chowdhury.
2006.
Identifi-cation of user sessions with hierarchical agglomerativeclustering.
ASIST, 43(1):934?950.Seda Ozmutlu.
2006.
Automatic new topic identificationusing multiple linear regression.
Information Process-ing and Management, 42(4):934?950.Filip Radlinski and Thorsten Joachims.
2005.
Querychains: learning to rank from implicit feedback.
InRobert Grossman, Roberto Bayardo, and Kristin P.Bennett, editors, KDD, pages 239?248.
ACM.Jamie Teevan, Eytan Adar, Rosie Jones, and MichaelPotts.
2006.
History repeats itself: Repeat queriesin yahoo?s logs.
In Proceedings of the 29th annual in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 703?704.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proceeding of the Hu-man Language Technologies Conference and the An-nual Meeting of the North American Association ofComputational Linguists (HLT-NAACL 2003), pages252?259.K.
Wang, C. Thrasher, and B. Hsu.
2011.
Web scale nlp:A case study on url word breaking.
In Proceeding ofthe ACM World Wide Web Conference (WWW 2011),pages 357?366.Ryen W. White and Steven M. Drucker.
2007.
Inves-tigating behavioral variability in web search.
In Pro-ceedings of the 16th international conference on WorldWide Web.Ryen W. White, Wei Chu, Ahmed Hassan, Xiaodong He,Yang Song, and Hongning Wang.
2013.
Enhancingpersonalized search by mining and modeling task be-havior.
In Proceedings of the 22nd international con-ference on World Wide Web, WWW ?13, pages 1411?1420.X.
Yu and H. Shi.
2009.
Query segmentation using con-ditional random fields.
In Proceedings of the Work-shop on Keyword Search on Structured Data (KEYS),pages 21?26.1010
