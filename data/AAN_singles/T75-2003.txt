COMPUTATIONAL UNDERSTANDINGChristopher K. RiesbeckI.
METHODOLOGICAL POSITIONThe problem of computat ionalunderstanding has often been broken into twosub-problems: how to syntact ical ly  analyze anatural  language sentence and how tosemant ica l ly  interpret the results of thesyntact ic analysis.
There are many reasonsfor this subdivis ion of the task, involvinghistor ical  inf luences from Americanstructural  l inguist ics and the early"knowledge-free" approaches to Art i f ic ia lIntel l igence.
The sub-divis ion has remainedbasic to much work in the area becausesyntact ic analysis seems to be much moreamenable to computat ional  methods thansemantic interpretat ion does, and thus moreworkers have been attracted developingsyntact ic analyzers first.It is my bel ief that this subdivis ionhas hindered rather than helped workers inthis area.
It has led to much wasted efforton syntact ic parsers as ends in themselves.It raises false issues, such as how muchsemantics should be done by the syntact icanalyzer and how much syntactics should bedone by the semantic interpreter.
It leadsresearchers into a l l -or -none choices onlanguage processing when they are trying todevelop complete systems.
E i ther  theresearcher tries to build a syntact icanalyzer first, and usually gets no farther,or he ignores language processingaltogether.The point to real ize is that theseproblems arise from an overemphasis on thesyntax/semant ics  dist inction.
Certa in lyboth syntact ic knowledge and semanticknowledge are used in the process ofcomprehension.
The false problems arisewhen the comprehension process i tself  issect ioned off into weakly communicat ingsub-processes, one of which does syntact icanalysis  and the other of which doessemantic.
Why should considerat ion of themeaning of a sentence have to depend uponthe successful  syntactic analysis of thatsentence?
This is certainly not arestr ict ion that appl ies to people.
Whyshould computer programs be more l imited?A better model of comprehensiontherefore is one that uses a coherent set ofprocesses operat ing upon information ofdi f ferent varieties.
When this is done itbecomes clearer that the real problems ofcomputat ional  understanding involvesquest ions like: what information isnecessary for understanding a part iculartext, how does the text cue in thisinformation, how is general informat ion"tuned" to the current context, how isinformat ion removed from play, and so on.These quest ions must be asked for all thedif ferent kinds of information that areused.Notice that these quest ions are thesame ones that must be asked about ANY modeliiof memory processes.
The reason for this isobvious: COMPREHENSION IS A MEMORY PROCESS.This simple statement has several impor tantimpl icat ions about what a comprehensionmodel should look like.
Comprehension as amemory process implies a set of concernsvery different from those that arose whennatural  language processing was looked at byl inguistics.
It implies that the answersinvolve the generat ion of simple mechanismsand large data bases.
It implies that thesemechanisms should either be or at least looklike the mechanisms used for common-sensereasoning.
It implies that the informationin the data bases should be organized forusefulness -- i.e., so that textual cueslead to the RAPID retr ieval  of ALL theRELEVANT information -- rather than foruni formity -- e.g., syntax in one place,semantics in another.The next section of this paper isconcerned with a system of analysismechanisms that I have been developing.While the discussion is l imited pr imari ly tothe problem of computat ional  understanding,I hope it wil l  be clear that both themechanisms and the organizat ion of the database given are part of a more general modelof human memory.II.
ANALYSIS MECHANISMSIt has been recognized for some timenow that understanding even apparent lysimple texts can involve the appl icat ion ofquite general world knowledge, that is, ofknowledge that would not normal ly beconsidered part of one's knowledge of thelanguage in which the text is written.
Theset of informat ion that might be needed forunderstanding a text is thereforetremendous.
Clearly an understanding systemcannot be applying all it knows 'toeverything it reads all the time.
It musthave mechanisms for guessing whatinformation is l ikely to be needed in thenear future.
As long as its guesses aregood, and the understander  updates them inthe light of new input, understanding canproceed at a reasonable rate.In other words, the understander mustbe good at PREDICTING what it is l ikely tosee.
Further the data base must beorganized so that coherent clusters ofrelevant informat ion can be accessed quicklywith these predict ions.
But since no finitestatic data base can have exact ly the  rightinformat ion for every input, theunderstander  must be able to prune andmodify the information that the data basecontains so that it appl ies more preciselyto the s i tuat ion at hand.The analyzer  which I developed in mythesis \[Riesbeck, 1974\] was based on theconcept of "expectat ion".
The analyzerprogram consisted of a fairly simple monitorprogram and a lexicon.
The lexicon was adata base whose contents were organizedunder words and their roots.
Theinformat ion in the data base was in the formof pairs of predicates and programs, whichwere cal led EXPECTATIONS.The analysis  p rocesscons is ted  of themonitor reading sentences, one word at atime, from left to right.
As each word wasread, the monitor  did two things.
It lookedup the word (or word root if no entry wasfound for the word) in the lexicon, andadded the associated expectat ions (if any)to a master  list of expectat ions.
Then eachelement of this master list was checked.Those expectat ions with predicates thatevaluated to true were "tr iggered" -- i.e.,their  programs were executed and theexpectat ions were removed from the masterlist.
Those expectat ions that were nott r iggered were left on the master list.When the end of the sentence was reached,the meaning of the sentence was thatstructure (if any) which the tr igger ings ofthe various expectat ions had built.A general  idea of the way the analyzerworked can be obtained by fo l lowing the flowof analys is  of the simple sentence "Johngave Mary a beating."
The chart on the nextpage gives an outl ine of the basic sequenceof events that takes place in the analyzeras the sentence is read, one word at a time,from left to right.
The column headed "WORDREAD" indicates where the analyzer is in thesentence when something occurs.
The columnheaded "EXPECTATIONS WAITING" gives the12predicate portion for all the act ivated butnot yet tr iggered expectations.
The columnheaded "EXPECTATIONS TRIGGERED" indicates,when a number is placed in that column,which expectat ion has just been tr iggered atthat point in the analysis.
The columnheaded "ACTIONS TAKEN" indicates whateffects the tr iggered expectat ions had.INPUT refers to whatever has just been reador constructed from the input stream.Step 0 is the init ial  state of theanalyzer  before the sentence is begun.
Theanalyzer  sets up one expectat ion whichassumes that the first NP it sees is thesubject of a verb that wil l  come later.In Step I, the firstword -- "John" -- is read.
Because "John"is a proper name, it is treated as a nounphrase and thus Expectat ion I is tr iggered.The program for Expectat ion I chooses "John"to be the subject of whatever verb wil lfo l low.
Expectat ion I is then removed fromthe set of active expectations.
There wereno expectat ions l isted in the lexical  entryfor "John".In Step 2, "gave" is read.
The lexicalentry for the root form "give" has threeexpectat ions l isted an~ these are added tothe set of active expectat ions.
None ofthem are tr iggered.In Step 3, "Mary" is read.
"Mary" is anoun phrase referr ing to a human and soExpectat ion 2 is tr iggered.
The program forExpectat ion 2 chooses "Mary" to be therecipient of the verb "give".
ThenExpectat ion 2 is removed.
There were noexpectatons in the lexical  entry for "Mary".In Step 4, "a" is read.
There is oneexpectat ion in the lexicon for "a".
This isExpectat ion 5 which has a predicate that isalways true.
That means that Expectat ion 5is t r iggered immediately.
The program forExpectat ion 4 is a complex one.
It setsaside in a temporary storage area thecurrent list of act ive expectat ions.
In itsplace it puts Expectat ion 6, which wil l  betr iggered when something in the input streamindicates that the noun phrase begun by "a"is complete.In Step 5, "beating" is read.
Thereare no lexical  entr ies and "beating" is nota word that f inishes a noun phrase, sonothing happens.In Step 6, the end of the sentence isseen.
This does f inish a noun phrase and soExpectat ion 6 is tr iggered.
The program forExpectat ion 5 builds a noun phrase from thewords that have been read since the "a" wasseen.
It places this back in the inputstream and brings back the set ofexpectat ions that Expectat ion 5 had setaside.In Step 7, the input "a beating,,tr iggers Expectat ion 4.
The program forExpectat ion 4 builds a conceptual  structurerepresent ing the idea of someone hi t t ingsomeone else repeatedly.
It uses thesubject "John" as the actor and theIII!IIIiIIIII1I1IIIIrecipient "Mary" as the Object being hit.The final result therefore is arepresentat ion that says that John hit Maryrepeatedly.The program portions of theexpectat ions therefore produced the meaningof a sentence.
These programs were notl imited in power.
Not only could theybuild, modify and delete syntactic andconceptual structures, but they could add,modify and delete the list of expectat ionsas well.
This is why the analysis monitorwas so simple.
All the real work was doneby the program portions of the expectations.The predicates were predict ions aboutlikely situations that would be encounteredin the processing of the sentence.
Some ofthese predict ions were about what words orl word types would be seen.
For example, oneof the expectat ion pairs in the lexicalentry for "a" contained a predicate that anoun would be seen soon.
Elsewhere in thel lexicon, there were expectat ions whosepredicates were about the structures thatother expectat ions had built or would build.There were also expectat ions with predicatesthat were true in all situations.
In thisl case the programs were supposed to beexecuted whenever the word referencing themin the lexicon was read.The predict ive power of the predicatesarose from the fact that the predicate didnot look at all the things that an inputmight mean.
Rather it asked if the inputCOULD mean some part icular thing.
If so theexpectat ion was triggered.
The predicateportions of expectat ions were thedisambiguat ing component of the analyzerbecause they chose only those word meaningsthat the sentential  context had use for.To general ize this discr ipt ion of theanalyzer a bit more, the basic memorymechanism used was the expectation, whichl consisted of a predict ion about a possiblefuture situation and instruct ions on what todo if that s ituation occurred.
The basicorganizat ion of memory was to have clustersi of these expectat ions attached to words andword roots.
The access to this memory wasthrough the words seen in a sentence beingunderstood.I The thrust of the work of the analyzerhad been on the development of theexpectat ion mechanism as a viable analysistool.
This meant defining what kinds ofI expectat ions were needed and how they couldbe easi ly retrieved.
One of the majorweaknesses of the analyzer was the lack ofany sat isfactory control over the set of,.
current ly active expectations.
There was noI real tuning of the set of expectat ions foundin the lexicon to fit the situation at hand.The only interact ion between expectat ionsoccurred when expectat ions were tr iggeredl and produced concrete structures.
The onlymechanism for removing untr iggeredexpectat ions was the wholesale clearing ofactive memory at the end of a sentence.I The extension of the concept ofexpectat ions to make them more control lable13without destroying their general i ty has beenthe core of the work that I have been doingsince the thesis.
Programming is going onright now to incorporate the extensions intoa second version of the analyzer.The first basic extension to thepredicate-program format of the expectat ionswas the addit ion of explicit informationabout the purposes of various expectations.That is, an expectat ion was made and -- moreimportant ly -- kept around because there wassome need that the tr iggering of thisexpectat ion would fulfill.
For example, theverb "give"had listed in its lexical entryseveral expectat ions which could fill therecipient slot for that verb if triggered.There was one which looked for the next nounphrase referr ing to a human.
Thisexpectation, act ivated as soon as "give" wasseen, would fill the recipient slot insentences like "John gave Mary a book."
Aseparate expectation, act ivated at the sametime, looked for the preposit ion "to"fol lowed by  a noun phrase referr ing tosomething that was at least a physicalobject .
This expectat ion if tr iggered wouldfill the recipient of "give" with the objectof the "to", as in sentences like "John gavethe book to Mary.
"Both of these expectat ions have thesame purpose: to fill the recipient case ofthe verb "give".
As long as no recipient isfound there is a reason for keeping bothexpectat ions active.
And this implies thatwhen the recipient case is f inally filled,either by one of the expectat ions set up by"give" or by some expectat ion set up by somelater word, then there is no longer anyreason for keeping any of these expectat ionsand they should all be removed.If the monitor ing program is to becapable of both loading and removing thevarious expectations, it must know what thepurposes of the expectat ions are.Unfortunately,  there are no constraints onwhat sorts of functions can appear aspredicates and programs in an expectation,which makes such a capabi l i ty impossible.However it is not necessary for the monitorto recognize purposes for ALL expectations.It is suff ic ient for it to know about justthose expectat ions that fill emptyconceptual  or syntactic slots when they aretr iggered.
The two expectat ion examplesgiven above for f i l l ing the recipient caseof the verb "give" are of this type.
We canspecify the purposes of such expectat ions bys imply specify ing what slot they fil l iftr iggered.
The monitor can tell with theseexpectat ions when they should be kept andwhen they should be removed.
The monitorleaves alone actions -- such as those thatmanipulate other expectat ions -- which arenot l inkable to simple purposes.While this was the first importantextension to the expectat ion format it wasnot the last.
Almost immediately it wasreal ized that many expectat ions aredependent upon others in the sense that theycannot possibly be tr iggered unti l  the otherones are.
For example, suppose we have anexpectat ion whose predicate looks at thesyntactic object slot of the verb "give" andwhose program builds some conceptualstructure using this information.
Furthersuppose we have another expectat ion activeat the same time whose predicate looks for anoun phrase in the input stream and whoseprogram will fill in the syntactic objectslot for "give" with that noun phrase.
Thenclearly the former expectat ion must wait forthe latter to be tr iggered first before ithas a chance of being tr iggered itself.This kind of dependency relat ionshipbetween expectat ions is not just aninterest ing observation.
Remember that thepredicate portion of an expectat ion was aPREDICTION about what might be seen.
Thismeans that  the first expectat ion -- the onewhose predicate looks at the syntacticobject of "give" when it is f inallyfi l led -- is not only wait ing for the secondexpectat ion to be tr iggered but in fact ismaking a predict ion about what the secondexpectat ion will produce.
This has twoimpl icat ion s ?First, if the second expectat ion cannotproduce a structure that will satisfy thepredicate of the first expectation, butthere is an expectat ion that can, then thesecond expectat ion is less preferable tothis third one, which means that the thirdone would be checked first when new inputarrives.
A dynamic ordering has beeninduced on the set of active expectations.Second, structure bui lding expectat ionsoften build from pieces of structures thatother expectat ions build.
If we have apredict ion about what an expectat ion shouldproduce, we can then make predict ions aboutthe sub-structures that the expectat ionbuilds with.
These new predict ions can theninf luence the expectat ions producing thosesub-structures,  and so on.For example, consider the twoexpectat ions for "give" that were givenabove.
Suppose the predicate of firstexpectat ion looks for a syntactic objectreferr ing to an action -- such as "a sock"in one interpretat ion of the sentence "Johngave Mary a sock."
Since the secondexpectat ion is the one that fil ls in thesyntact ic object slot of "give", there isnow a predict ion that the second expectat ionwil l  produce a noun phrase referr ing to anaction.
Since the second expectat ion fi l lsthe syntact ic object of "give" with a nounphrase that it finds in the input stream,the monitor  can predict that a noun phrasereferr ing to an act ion will appear in theinput stream.
The effect of this predict ionis that when words are seen in the input,the first thing that is looked for is to seeif they can refer to an action.
If so, thenthat sense of the word is taken immediately.Thus a word like "sock" is d isambiguatedimmediately as a result of an expectat ionor ig inal ly  made about the syntactic objectof "give".To pass the information from oneexpectat ion to the next about what anexpectat ion would like to see, we need toknow where the expectat ion is looking.
That14is we need to know what the predicate of theexpectat ion is applied to.
This informationcan be specif ied in the same way that thepurpose of the expectat ion was: by giving aconceptual  or syntactic slot.
In this case,instead of giving the slot that theexpectat ion fil ls if tr iggered, we specifythe slot that the predicate of theexpectat ion is applied to.
Then by knowingwhat slot an expectat ion looks at, we knowwhat expectaions this expectat ion dependson.
It depends on those expectat ions thatfill this slot -- i.e., that have a "purposeslot" equal to the "lock at slot" of theexpectation.Let me summarize this discussion bygiving the current format for speci fy ingexpectat ions:(NEED FOCUS TEST ACTION SIDE-EFFECTS)whereNEED is the slot the expectat ion fills iftr iggered,FOCUS is the slot the expectat ion looks at,TEST is the predicate portion of theexpectation,ACTION is the structure bui lding portion ofthe expectation,S IDE-EFFECTS are those programs that actupon other expectat ions and are not -- atthe moment -- incorporated into thenetwork of dependencies and predictions.The analysis monitor  is fair lycontent- independent.
Its job is to takeinput, use it to access clusters ofexpectations, keep active those expectat ionsthat might fill slots that are stil l  emptyin part ia l ly-bui l t  structures, and keeptrack of the predict ions/preferences thatare induced by the dependency re lat ionshipsbetween expectations.
The actual knowledgeabout language and the world is stil lcontained in the expectations, as was truein the original  analyzer.This encoding of knowledge into smallpieces of programs that have both proceduraland declarat ive aspects is of both practicaland theoret ical  importance.
In terms ofimplement ing an AI model, I have found itmuch easier to specify procedural  knowledgein small units of "in s i tuat ion X do Y".Further it is much easier, as a programmer,to extend and modify procedures written inthis form.
It is also easier for a programto manipulate knowledge in this way.Theoretical ly,  the expectat ion formatseems to me to be a viable memoryrepresentat ion for highly proceduralknowledge.
With it we can design expl ic i t lya theory of computat ional  understanding thatdoes not have the forced divis ion betweensyntact ic and semantic analysis.
Indiv idualexpectat ions are usual ly concerned withsyntact ic  or conceptual  structures, but allof the expectat ions are maintained in onelarge set.
This al lows for those importantexpectat ions that convert information aboutsyntact ic structures in semantic informationand vice-versa.
Thus information thator ig inal ly  started as an abstract conceptualIIIIIIIiiIIIIIIiIIIIpredict ion can be quickly disseminatedthroughout a dependency network ofexpectat ions and lead eventual ly topredict ions about things like word senses.For example, my thesis describes howthe interpretat ion of the text "John was madat Mary.
He gave her a sock," uses aconceptual predict ion that "John wantssomething bad to happen to Mary," whichfollows from the first sentence, to choosethe appropr iate sense of the word "sock" inthe second sentence the first time the'wordis seen.
This can be done because thegeneral conceptual predict ion in interact ionwith the expectat ions in the lexical entryfor "give" led to predict ions about thenature of the syntactic object of "give",which in turn led to predict ions about thewords that would be seen in the inputstream.In other words, the analysissystem -- both the original  one and the newversion -- as an approach to thecomputat ional  understanding problem,exempli f ies the general points made in themethodological  portion of this paper.
Itdemonstrates the feasibi l i ty of doingunderstanding using very simple mechanismsfor manipulat ing small but f lexible units ofknowledge, without forcing the developmentof independent syntactic analyzers orsemantic interpreters.
These simplemechansisms al low for a direct attack onsuch problems as what information isabsolutely necessary for understanding, howit is cal led for, and how a workably sizedset of active information can be maintained.REFERENCERiesbeck, C. "Computat ional  Understanding:Analysis of Sentences and Context,"Ph.D. Thesis, Computer Science Dept.,Stanford University, Stanford, CA.1974.ISSTEP WORD READ EXPECTAT IONS EXPECTAT IONS ACT ION TAKENACT IVE  TR IGGERED0 none  I - is INPUT a none  noneNP?I J ohn  I - is INPUT a I choose  " John  to beNP?
the  sub jec t  of  theverb  to come2 gave  2 - does  INPUT re fer  none  noneto a human?3 - does  INPUT re ferto a phys ica lob jec t?4 - does  INPUT re ferto an ac t ion?3 Mary  2 - does  INPUT re fer  2 choose  "Mary"  toto a human?
be the  rec ip ient3 - does  INPUT re fer  o f  "g ive"to a phys ica lob jec t?4 - does  INPUT re ferto an ac t ion?4 a 3 - does  INPUT re fer  5 save  the  cur rentto a phys ica l  set  ofob jec t?
expectat ions  and4 - does  INPUT re fer  rep lace  it w i th :to an ac t ion?
6 - does  INPUT end5 - t rue  a NP?5 beat ing  6 - does  INPUT end  none  nonea NP?6 per iod  6 - does  INPUT end 6 set  INPUT to thea NP?
NP "a beat ing"  andreset  theexpectat ion  set7 none  4 set  the  mainac t ion  of  thein terpretat ionto the  ac t ionnamed by INPUT;set  the  ac tor  tothe  sub jec t  ( John)and  set  the  ob jec tto the  rec ip ient(Mary)3 - does  INPUT re ferto a phys ica lob jec t?4 - does  INPUT re ferto an ac t ion?16IIIIll.Ili1D.DIII1!III
