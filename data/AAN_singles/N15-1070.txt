Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 683?693,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsOntologically Grounded Multi-sense Representation Learning for SemanticVector Space ModelsSujay Kumar Jauhar Chris Dyer Eduard HovyLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{sjauhar, cdyer, hovy}@cs.cmu.eduAbstractWords are polysemous.
However, most ap-proaches to representation learning for lexicalsemantics assign a single vector to every sur-face word type.
Meanwhile, lexical ontologiessuch as WordNet provide a source of com-plementary knowledge to distributional infor-mation, including a word sense inventory.
Inthis paper we propose two novel and generalapproaches for generating sense-specific wordembeddings that are grounded in an ontology.The first applies graph smoothing as a post-processing step to tease the vectors of differ-ent senses apart, and is applicable to any vec-tor space model.
The second adapts predictivemaximum likelihood models that learn wordembeddings with latent variables representingsenses grounded in an specified ontology.
Em-pirical results on lexical semantic tasks showthat our approaches effectively captures infor-mation from both the ontology and distribu-tional statistics.
Moreover, in most cases oursense-specific models outperform other mod-els we compare against.1 IntroductionVector space models (VSMs) of word meaning playa central role in computational semantics.
Theserepresent meanings of words as contextual featurevectors in a high-dimensional space (Deerwester etal., 1990) or some embedding thereof (Collobertand Weston, 2008) and are learned from unanno-tated corpora.
Word vectors in these continuousspace representations can be used for meaningful se-mantic operations such as computing word similar-ity (Turney, 2006), performing analogical reasoning(Turney, 2013) and discovering lexical relationships(Mikolov et al, 2013b).
They have also proved use-ful in downstream NLP applications such as infor-mation retrieval (Manning et al, 2008) and questionanswering (Tellex et al, 2003), among others.However, VSMs remain flawed because they as-sign a single vector to every word, thus ignoringthe possibility that words may have more than onemeaning.
For example, the word ?bank?
can ei-ther denote a financial institution or the shore ofa river.
The ability to model multiple meanings isan important component of any NLP system, givenhow common polysemy is in language.
The lackof sense annotated corpora large enough to robustlytrain VSMs, and the absence of fast, high qualityword sense disambiguation (WSD) systems makeshandling polysemy difficult.Meanwhile, lexical ontologies, such as WordNet(Miller, 1995) specifically catalog sense invento-ries and provide typologies that link these sensesto one another.
These hand-curated ontologies pro-vide a complementary source of information to dis-tributional statistics.
Recent research tries to lever-age this information to train better VSMs (Yu andDredze, 2014; Faruqui et al, 2014), but does nottackle the problem of polysemy.
Parallely, work onpolysemy for VSMs revolves primarily around tech-niques that cluster contexts to distinguish betweendifferent word senses (Reisinger and Mooney, 2010;Huang et al, 2012), but does not integrate ontologiesin any way.In this paper we present two novel approaches tointegrating ontological and distributional sources ofinformation.
Our focus is on allowing already exist-ing, proven techniques to be adapted to produce on-tologically grounded word sense embeddings.
Ourfirst technique is applicable to any sense-agnostic683VSM as a post-processing step that performs graphpropagation on the structure of the ontology.
Thesecond is applicable to the wide range of currenttechniques that learn word embeddings from predic-tive models that maximize the likelihood of a corpus(Collobert and Weston, 2008; Mnih and Teh, 2012;Mikolov et al, 2013a).
Our technique adds a latentvariable representing the word sense to each tokenin the corpus, and uses EM to find parameters.
Us-ing a structured regularizer based on the ontologicalgraph, we learn grounded sense-specific vectors.There are several reasons to prefer ontologies asdistant sources of supervision for learning sense-aware VSMs over previously proposed unsuper-vised context clustering techniques.
Clustering ap-proaches must often parametrize the number ofclusters (senses), which is neither known a priorinor constant across words (Kilgarriff, 1997).
Alsothe resulting vectors remain abstract and uninter-pretable.
With ontologies, interpretable sense vec-tors can be used in downstream applications such asWSD, or for better human error analysis.
Moreover,clustering techniques operate on distributional simi-larity only whereas ontologies support other kinds ofrelationships between senses.
Finally, the existenceof cross-lingual ontologies would permit learningmulti-lingual vectors, without compounded errorsfrom word alignment and context clustering.We evaluate our methods on 3 lexical semantictasks across 7 datasets and show that our sense-specific VSMs effectively integrate knowledge fromthe ontology with distributional statistics.
Empir-ically, this results in consistently and significantlybetter performance over baselines in most cases.
Inthe more marginal cases, analysis reveals that ourperformance is a result of the deficient structureof the ontology.
We discuss and compare the twodifferent approaches from the perspectives of per-formance, generalizability, flexibility and computa-tional efficiency.
Finally, we qualitatively analyzethe vectors and show that they indeed capture sense-specific semantics.2 Unified Symbolic and DistributionalSemanticsIn this section, we present our two techniques for in-ferring sense-specific vectors grounded in an ontol-ogy.
We begin with notation.
LetW = {w1, ..., wn}be a set of word types, and Ws= {sij| ?wi?W , 1 ?
j ?
ki} a set of senses, with kithe num-ber of senses of wi.
Moreover, let ?
= (T?, E?
)be an ontology represented by an undirected graph.The vertices T?= {tij| ?
sij?
Ws} correspondto the word senses in the set Ws, while the edgesE?= {erij?i?j?}
connect some subset of word sensepairs (sij, si?j?)
by semantic relation r1.2.1 Retrofitting Vectors to an OntologyOur first technique assumes that we already havea vector space embedding of a vocabulary?U ={u?i| ?
wi?
W}.
We wish to infer vectors V ={vij| ?
sij?
Ws} for word senses that are maxi-mally consistent with both?U and ?, by some notionof consistency.
We formalize this notion as MAPinference in a Markov network (MN).The MN we propose contains variables for everyvector in?U and V .
These variables are connected toone another by dependencies as follows.
Variablesfor vectors vijand vi?j?are connected iff there existsan edge erij?i?j??
E?connecting their respectiveword senses in the ontology.
Furthermore, vectorsu?ifor the word types wiare each connected to allthe vectors vijof the different senses sijof wi.
If wiis not contained in the ontology, we assume it hasa single unconnected sense and set it?s only sensevector vi1to it?s empirical estimate u?i.The structure of this MN is illustrated in Figure1, where the neighborhood of the ambiguous word?bank?
is presented as a factor graph.We set each pairwise clique potential to be of theform exp(a?u ?
v?2) between neighboring nodes.Here u and v are the vectors corresponding to thesenodes, and a is a weight controlling the strength ofthe relation between them.
We use the Euclideannorm instead of a distance based on cosine similaritybecause it is more convenient from an optimizationperspective.Our inference problem is to find the MAP esti-mate of the vectors V , given?U , which may be stated1For example there might be a ?synonym?
edge between theword senses ?cat(1)?
and ?feline(1)?.684Figure 1: A factor graph depicting the retrofitting modelin the neighborhood of the word ?bank?.
Observed vari-ables corresponding to word types are shaded in grey,while latent variables for word senses are in white.as follows:C(V ) = arg minV?i?ij??u?i?
vij?2+?ij?i?j??r?vij?
vi?j?
?2(1)Here ?
is the sense-agnostic weight and ?rarerelation-specific weights for different semantic re-lations.
This objective encourages vectors of neigh-boring nodes in the MN to pull closer together, lever-aging the tension between sense-agnostic neighbors(the first summation term) and ontological neighbors(the second summation term).
This allows the dif-ferent neighborhoods of each sense-specific vectorto tease it apart from its sense-agnostic vector.Taking the partial derivative of the objective inequation 1 with respect to vector vijand setting tozero gives the following solution:vij=?u?i+?i?j??Nij?rvi?j??+?i?j?
?Nij?r(2)where Nijdenotes the set of neighbors of ij.
Thus,the MAP sense-specific vector is an ?-weightedcombination of its sense-agnostic vector and the ?r-weighted sense-specific vectors in its ontologicalneighborhood.We use coordinate descent to iteratively updatethe variables V using equation 2.
The optimiza-tion problem in equation 1 is convex, and we nor-mally converge to a numerically satisfactory station-ary point within 10 to 15 iterations.
This procedureAlgorithm 1 Outputs a sense-specific VSM, given asense-agnostic VSM and ontology1: function RETROFIT(?U,?
)2: V(0)?
{v(0)ij= u?i| ?sij?Ws}3: while ?v(t)ij?
v(t?1)ij?
?
 ?i, j do4: for tij?
T?do5: v(t+1)ij?
update using equation 26: end for7: end while8: return V(t)9: end functionis summarized in Algorithm 1.
The generality of thisalgorithm allows it to be applicable to any VSM asa computationally attractive post-processing step.An implementation of this technique is avail-able at https://github.com/sjauhar/SenseRetrofit.2.2 Adapting Predictive Models with LatentVariables and Structured RegularizersMany successful techniques for semantic represen-tation learning are formulated as models where thedesired embeddings are parameters that are learnt tomaximize the likelihood of a corpus (Collobert andWeston, 2008; Mnih and Teh, 2012; Mikolov et al,2013a).
In our second approach we extend an exist-ing probability model by adding latent variables rep-resenting the senses, and we use a structured priorbased on the topology of the ontology to ground thesense embeddings.
Formally, we assume a corpusD = {(w1, c1), .
.
.
, (wN, cN)} of pairs of target andcontext words, and the ontology ?, and we wish toinfer sense-specific vectors V = {vij| ?
sij?Ws}.Consider a model with parameters ?
(V ?
?
)that factorizes the probability over the corpus as?
(wi,ci)?Dp(wi, ci; ?).
We propose to extend sucha model to learn ontologically grounded sense vec-tors by presenting a general class of objectives of thefollowing form:C(?)
= arg max??
(wi,ci)?Dlog(?sijp(wi, ci, sij; ?))
+ log p?(?
)(3)This objective introduces latent variables sijforsenses and adds a structured regularizer p?(?
)685Chase loaned moneybank2bankp(s | w)p(c | s)Context word(observed)Sense(latent)Word(observed)Figure 2: The generative process associated with theskip-gram model, modified to account for latent senses.Here, the context of the ambiguous word ?bank?
is gen-erated from the selection of a specific latent sense.that grounds the vectors V in an ontology.
Thisform permits flexibility in the definition of bothp(wi, ci, sij; ?)
and p?(?)
allowing for a general yetpowerful framework for adapting MLE models.In what follows we show that the popular skip-gram model (Mikolov et al, 2013a) can be adaptedto generate ontologically grounded sense vectors.The classic skip-gram model uses a set of parame-ters ?
= (U, V ), with U = {ui| ?ci?
W} andV = {vi| ?wi?
W} being sets of vectors forcontext and target words respectively.
The genera-tive story of the skip-gram model involves generat-ing the context word ciconditioned on an observedword wi.
The conditional probability is defined tobe p(ci| wi; ?)
=exp(ui?
vi)?ci??Wexp(ui??
vi).We modify the generative story of the skip-grammodel to account for latent sense variables by firstselecting a latent word sense sijconditional on theobserved word wi, then generating the context wordcifrom the sense distinguished word sij.
This pro-cess is illustrated in Figure 2.
The factorizationp(ci| wi; ?)
=?sijp(ci| sij; ?)
?
p(sij| wi; ?
)follows from the chain rule since senses are word-specific.
To parameterize this distribution, we de-fine a new set of model parameters ?
= (U, V,?
),where U remains identical to the original skip-gram,V = {vij| ?sij?Ws} are a set of vectors for wordsenses, and ?
are the context-independent sense pro-portions piij= p(sij| wi).
We use a Dirichlet priorover the multinomial distributions piifor every wi,with a shared concentration parameter ?.We define the ontological prior on vectors asp?(?)
?
exp(???ij?i?j??r?vij?
vi?j?
?2), where ?controls the strength of the prior.
We note the sim-ilarity to the retrofitting objective in equation 1, ex-cept with ?
= 0.
This leads to the following realiza-tion of the objective in equation 3:C(?)
= arg max??
(wi,ci)?Dlog(?sijp(ci| sij; ?
)?p(sij| wi; ?))?
??ij?i?j??r?vij?
vi?j?
?2(4)This objective can be optimized using EM, for thelatent variables, and with lazy updates (Carpenter,2008) every k words to account for the prior regu-larizer.
However, since we are primarily interestedin learning good vector representations, and we wantto learn efficiently from large datasets, we make thefollowing simplifications.
First, we perform ?hard?EM, selecting the most likely sense at each positionrather than using the full posterior over senses.
Also,given that the structured regularizer p?(?)
is essen-tially the retrofitting objective in equation 1, we runretrofitting periodically every k words (with ?
= 0in equation 2) instead of lazy updates.2The following decision rule is used in the ?hard?E-step:sij= arg maxsijp(ci| sij; ?
(t))pi(t)ij(5)In the M-step we use Variational Bayes to update ?with:pi(t+1)ij?exp(?(c?
(wi, sij) + ?pi(0)ij))exp (?
(c?
(wi) + ?
))(6)where c?(?)
is the online expected count and ?(?)
isthe digamma function.
This approach is motivatedby Johnson (2007) who found that naive EM leadsto poor results, while Variational Bayes is consis-tently better and promotes faster convergence of thelikelihood function.
To update the parameters U andV , we use negative sampling (Mikolov et al, 2013a)which is an efficient approximation to the originalskip-gram objective.
Negative sampling attempts todistinguish between true word pairs in the data, rel-ative to noise.
Stochastic gradient descent on thefollowing equation is used to update the model pa-2We find this gives slightly better performance.686rameters U and V :L = log ?(ui?
vij) +?j?j?6=jlog ?(?ui?
vij?)+?mEci?
?Pn(c)[log ?(?ui??
vij)](7)Here ?(?)
is the sigmoid function, Pn(c) is a noisedistribution computed over unigrams and m is thenegative sampling parameter.
This is almost exactlythe same as negative sampling proposed for the orig-inal skip-gram model.
The only change is that weadditionally take a negative gradient step with re-spect to all the senses that were not selected in thehard E-step.
We summarize the training procedurefor the adapted skip-gram model in Algorithm 2.Algorithm 2 Outputs a sense-specific VSM, given acorpus and an ontology1: function SENSEEM(D,?
)2: ?(0)?
initialize3: for (wi, ci) ?
D do4: if period > k then5: RETROFIT(?(t),?
)6: end if7: (Hard) E-step:8: sij?
find argmax using equation 59: M-step:10: ?(t+1)?
update using equation 611: U(t+1), V(t+1)?
update using equation 712: end for13: return ?
(t)14: end function3 EvaluationIn this section we detail experimental results on 3lexical semantics tasks across 8 different datasets.We begin by detailing the training and setup for ourexperiments.3.1 Resources, Data and TrainingWe use WordNet (Miller, 1995) as the sense repos-itory and ontology in all our experiments.
WordNetis a large, hand-annotated ontology of English com-posed of 117,000 clusters of senses, or ?synsets?
thatare related to one another through semantic relationssuch as hypernymy and hyponymy.
Each synset ad-ditionally comprises a list of sense specific lemmaswhich we use to form the nodes in our graph.
Thereare 206,949 such sense specific lemmas, which weconnect with synonym, hypernym and hyponym3re-lations for a total of 488,432 edges.To show the applicability of our techniques to dif-ferent VSMs we experiment with two different kindsof base vectors.Global Context Vectors (GC) (Huang et al,2012): These word vectors were trained using a neu-ral network which not only uses local context butalso defines global features at the document level tofurther enhance the VSM.
We distinguish three vari-ants: the original single-sense vectors (SINGLE), amulti-prototype variant (MULTI), ?
both are avail-able as pre-trained vectors for download4?
and asense-based version obtained by running retrofittingon the original vectors (RETRO).Skip-gram Vectors (SG) (Mikolov et al,2013a): We use the word vector tool Word2Vec5to train skip-gram vectors.
We define 6 variants:a single-sense version (SINGLE), two multi-sensevariants that were trained by first sense disambiguat-ing the entire corpus using WSD tools, ?
one unsu-pervised (Pedersen and Kolhatkar, 2009) (WSD) andthe other supervised (Zhong and Ng, 2010) (IMS)?
a retrofitted version obtained from the single-sense vectors (RETRO), an EM implementation ofthe skip-gram model with the structured regularizeras described in section 2.2 (EM+RETRO), and thesame EM technique but ignoring the ontology (EM).All models were trained on publicly available WMT-20116English monolingual data.
This corpus of 355million words, although adequate in size, is smallerthan typically used billion word corpora.
We use thiscorpus because the WSD baseline involves prepro-cessing the corpus with sense disambiguation, whichis slow enough that running it on corpora orders ofmagnitude larger was infeasible.Retrofitted variants of vectors (RETRO) aretrained using the procedure described in algorithm1.
We set the convergence criteria to  = 0.01 witha maximum number of iterations of 10.
The weights3We treat edges as undirected, so hypernymy and hyponymyare collapsed and unified in our representation schema.4http://nlp.stanford.edu/~socherr/ACL2012_wordVectorsTextFile.zip5https://code.google.com/p/word2vec/6http://www.statmt.org/wmt11/687in the update equation 2 are set heuristically: thesense agnostic weight ?
is 1.0, and relations-specificweights ?rare 1.0 for synonyms and 0.5 for hyper-nyms and hyponyms.
EM+RETRO vectors are theexception where we use a weight of ?
= 0.0 instead,as required by the derivation in section 2.2.For skip-gram vectors (SG) we use the followingstandard settings, and do not tune any of the values.We filter all words with frequency < 5, and pre-normalize the corpus to replace all numeric tokenswith a placeholder.
We set the dimensionality of thevectors to 80, and the window size to 10 (5 con-text words to either side of a target).
The learningrate is set to an initial value of 0.025 and diminishedlinearly throughout training.
The negative samplingparameter is set to 5.
Additionally for the EM vari-ants (section 2.2) we set the Dirichlet concentrationparameter ?
to 1000.
We use 5 abstract senses forthe EM vectors, and initialize the priors uniformly.For EM+RETRO, WordNet dictates the number ofsenses; also when available WordNet lemma countsare used to initialize the priors.
Finally, we set theretrofitting period k to 50 million words.3.2 Experimental ResultsWe evaluate our models on 3 kinds of lexical seman-tic tasks: similarity scoring, synonym selection, andsimilarity scoring in context.Similarity Scoring: This task involves using asemantic model to assign a score to pairs of words.We use the following 4 standard datasets in thisevaluation: WS-353 (Finkelstein et al, 2002), RG-65 (Rubenstein and Goodenough, 1965), MC-30(Miller and Charles, 1991) and MEN-3k (Bruni etal., 2014).
Each dataset consists of pairs of wordsalong with an averaged similarity score obtainedfrom several human annotators.
For example an itemin the WS-353 dataset is ?book, paper?
7.46?.
Weuse standard cosine similarity to assign a score toword pairs in single-sense VSMs, and the followingaverage similarity score to multi-sense variants, asproposed by Reisinger and Mooney (2010):avgSim(wi, wi?)
=1kikj?j,j?cos(vij, vi?j?)
(8)The output of systems is evaluated against the goldstandard using Spearman?s rank correlation coeffi-cient.Synonym Selection: In this task, VSMs are usedto select the semantically closest word to a tar-get from a list of candidates.
We use the follow-ing 3 standard datasets in this evaluation: ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakow-icz, 2004) and TOEFL-80 (Landauer and Dumais,1997).
These datasets consist of a list of target wordsthat appear with several candidate lexical items.
Anexample from the TOEFL dataset is ?rug ?
sofa,ottoman, carpet, hallway?, with ?carpet?
being themost synonym-like candidate to the target.
We be-gin by scoring all pairs composed of the target andone of the candidates.
We use cosine similarity forsingle-sense VSMs, and max similarity for multi-sense models7:maxSim(wi, wi?)
= maxj,j?cos(vij, vi?j?)
(9)These scores are then sorted in descending order,with the top-ranking score yielding the semanticallyclosest candidate to the target.
Systems are evalu-ated on the basis of their accuracy at discriminatingthe top-ranked candidate.The results for similarity scoring and synonymselection are presented in table 1.
On both tasksand on all datasets, with the partial exception ofWS-353 and MEN-3k, our vectors (RETRO &EM+RETRO) consistently yield better results thanother VSMs.
Notably, both our techniques performbetter than preprocessing a corpus with WSD infor-mation in unsupervised or supervised fashion (SG-WSD & SG-IMS).
Simple EM without an ontolog-ical prior to ground the vectors (SG-EM) also per-forms poorly.We investigated the observed drop in performanceon WS-353 and found that this dataset consists oftwo parts: a set of similar word pairs (e.g.
?tiger?and ?cat?)
and another set of related word pairs (e.g.?weather?
and ?forecast?).
The synonym, hypernymand hyponym relations we use tend to encouragesimilarity to the detriment of relatedness.We ran an auxiliary experiment to show this.
SG-EM+RETRO training also learns vectors for contextwords ?
which can be thought of as a proxy for re-latedness.
Using this VSM we scored a word pairby the average similarity of all the sense vectors of7Here we are specifically looking for synonyms, so the maxmakes more sense than taking an average.688Word Similarity (?)
Synonym Selection (%)WS-353 RG-65 MC-30 MEN-3k ESL-50 RD-300 TOEFL-80GCSINGLE 0.623 0.629 0.657 0.314 47.73 45.07 60.87MULTI 0.535 0.510 0.309 0.359 27.27 47.89 52.17RETRO 0.543 0.661 0.714 0.528 63.64 66.20 71.01SGSINGLE 0.639 0.546 0.627 0.646 52.08 55.66 66.67EM 0.194 0.278 0.167 0.228 27.08 33.96 40.00WSD 0.481 0.298 0.396 0.175 16.67 49.06 42.67IMS 0.549 0.579 0.606 0.591 41.67 53.77 66.67RETRO 0.552 0.673 0.705 0.560 56.25 65.09 73.33EM+RETRO 0.321 0.734 0.758 0.428 62.22 66.67 68.63Table 1: Similarity scoring and synonym selection in English across several datasets involving different VSMs.
Higherscores are better; best scores within each category are in bold.
In most cases our models consistently and significantlyoutperform the other VSMs.one word to the context vector of the other word,averaged over both words.
With this scoring func-tion the correlation ?
jumped from 0.321 to 0.493.While still not as good as some of the other VSMs, itshould be noted that this scoring function negativelyinfluences the similar word pairs in the dataset.The MEN-3k dataset is crowd-sourced and con-tains much diversity, with word pairs evidencingsimilarity as well as relatedness.
However, we aren?tsure why the performance for GC-RETRO improvesgreatly over GC-SINGLE for this dataset, while thatof SG-RETRO and SG-RETRO+EM drops in rela-tion to SG-SINGLE.Similarity Scoring in Context: As outlined byReisinger and Mooney (2010), multi-sense VSMscan be used to consider context when computingsimilarity between words.
We use the SCWS dataset(Huang et al, 2012) in these experiments.
Thisdataset is similar to the similarity scoring datasets,except that they additionally are presented in con-text.
For example an item involving the words?bank?
and ?money?, gives the words in their re-spective contexts, ?along the east bank of the DesMoines River?
and ?the basis of all money laun-dering?
with a low averaged similarity score of 2.5(on a scale of 1.0 to 10.0).
Following Reisinger andMooney (2010) we use the following function to as-sign a score to word pairs in their respective con-texts, given a multi-sense VSM:avgSimC(wi, ci, wi?, ci?)
=?j,j?p(sij|ci, wi)p(si?j?|ci?, wi?
)cos(vij, vi?j?
)(10)Vectors SCWS (?
)SG-WSD 0.343SG-IMS 0.528SG-RETRO 0.417GC-RETRO 0.420SG-EM 0.613SG-EM+RETRO 0.587GC-MULTI 0.657Table 2: Contextual word similarity in English.
Higherscores are better.As with similarity scoring, the output of systemsis evaluated against gold standard using Spearman?srank correlation coefficient.The results are presented in table 2.
Pre-processing a corpus with WSD information in an un-supervised fashion (SG-WSD) yields poor results.In comparison, the retrofitted vectors (SG-RETRO& GC-RETRO) already perform better, even thoughthey do not have access to context vectors, and thusdo not take contextual information into account.
Su-pervised sense vectors (SG-IMS) are also compe-tent, scoring better than both retrofitting techniques.Our EM vectors (SG-EM & SG-EM+RETRO) yieldeven better results and are able to capitalize oncontextual information, however they still fall shortof the pretrained GC-MULTI vectors.
We weresurprised that SG-EM+RETRO actually performedworse than SG-EM, given how poorly SG-EM per-formed in the other evaluations.
However, an anal-ysis again revealed that this was due to the kindof similarity encouraged by WordNet rather thanan inability of the model to learn useful vectors.The SCWS dataset, in addition to containing related689Method CPU TimeRETRO ~20 secsEM+RETRO ~4 hoursIMS ~3 daysWSD ~1 yearTable 3: Training time associated with different methodsof generating sense-specific VSMs.words ?
which we showed, hurt our performanceon WS-353 ?
also contains word pairs with differ-ent POS tags.
WordNet synonymy, hypernymy andhyponymy relations are exclusively defined betweenlemmas of the same POS tag, which adversely af-fects performance further.3.3 DiscussionWhile both our approaches are capable of integrat-ing ontological information into VSMs, an impor-tant question is which one should be preferred?From an empirical point of view, the EM+RETROframework yields better performance than RETROacross most of our semantic evaluations.
Addi-tionally EM+RETRO is more powerful, allowing toadapt more expressive models that can jointly learnother useful parameters ?
such as context vectors inthe case of skip-gram.
However, RETRO is far moregeneralizable, allowing it to be used for any VSM,not just predictive MLE models, and is also empiri-cally competitive.
Another consideration is compu-tational efficiency, which is summarized in table 3.Not only is RETRO much faster, but it scaleslinearly with respect to the vocabulary size, un-like EM+RETRO, WSD, and IMS which are de-pendent on the input training corpus.
Nevertheless,both our techniques are empirically superior as wellas computationally more efficient than both unsu-pervised and supervised word-sense disambiguationparadigms.Both our approaches are sensitive to the structureof the ontology.
Therefore, an important considera-tion is the relations we use and the weights we as-sociate with them.
In our experiments we selectedthe simplest set of relations and assigned weightsheuristically, showing that our methods can effec-tively integrate ontological information into VSMs.A more exhaustive selection procedure with weighttuning on held-out data would almost certainly leadto better performance on our evaluation suite.3.4 Qualitative AnalysisWe qualitatively attempt to address the question ofwhether the vectors are truly sense specific.
In ta-ble 4 we present the three most similar words ofan ambiguous lexical item in a standard VSM (SG-SINGLE) in comparison with the three most similarwords of different lemma senses of the same lexicalitem in grounded sense VSMs (SG-RETRO & SG-EM+RETRO).Word or Sense Top 3 Most Similarhanging hung dangled hangshanging (suspending) shoring support suspensionhanging (decoration) tapestry braid smockclimber climbers skier Loretanclimber (sportsman) lifter swinger sharpshooterclimber (vine) woodbine brier kiwiTable 4: The top 3 most similar words for two polyse-mous types.
Single sense VSMs capture the most fre-quent sense.
Our techniques effectively separates out thedifferent senses of words, and are grounded in WordNet.The sense-agnostic VSMs tend to capture only themost frequent sense of a lexical item.
On the otherhand, the disambiguated vectors capture sense speci-ficity of even less frequent senses successfully.
Thisis probably due to the nature of WordNet where thenearest neighbors of the words in question are in factthese rare words.
A careful tuning of weights willlikely optimize the trade-off between ontologicallyrare neighbors and distributionally common words.In our analyses, we noticed that lemma senses thathad many neighbors (i.e.
synonyms, hypernyms andhyponyms), tended to have more clearly sense spe-cific vectors.
This is expected, since it is these neigh-borhoods that disambiguate and help to distinguishthe vectors from their single sense embeddings.4 Related WorkSince Reisinger and Mooney (2010) first proposeda simple context clustering technique to generatemulti-prototype VSMs, a number of related effortshave worked on adaptations and improvements rely-ing on the same clustering principle.
Huang et al(2012) train their vectors with a neural network andadditionally take global context into account.
Nee-lakantan et al (2014) extend the popular skip-grammodel (Mikolov et al, 2013a) in a non-parametric690fashion to allow for different number of senses forwords.
Guo et al (2014) exploit bilingual align-ments to perform better context clustering duringtraining.
Tian et al (2014) propose a probabilisticextension to skip-gram that treats the different pro-totypes as latent variables.
This is similar to our sec-ond EM training framework, and turns out to be aspecial case of our general model.
In all these pa-pers, however, the multiple senses remain abstractand are not grounded in an ontology.Conceptually, our work is also similar to Yu andDredze (2014) and Faruqui et al (2014), who treatlexicons such as the paraphrase database (PPDB)(Ganitkevitch et al, 2013) or WordNet (Miller,1995) as an auxiliary thesaurus to improve VSMs.However, they do not model senses in any way.
Pile-hvar et al (2013) do model senses from an ontologyby performing random-walks on the Wordnet graph,however their approach does not take distributionalinformation from VSMs into account.Thus, to the best of our knowledge, ourwork presents the first attempt at producing sensegrounded VSMs that are symbolically tied to lexi-cal ontologies.
From a modelling point of view, itis also the first to outline a unified, principled andextensible framework that effectively combines thesymbolic and distributional paradigms of semantics.Both our models leverage the graph structureof ontologies to effectively ground the senses ofa VSM.
This ties into previous research (Das andSmith, 2011; Das and Petrov, 2011) that propa-gates information through a factor graph to per-form tasks such as frame-semantic parsing and POS-tagging across languages.
More generally, this ap-proach can be viewed from the perspective of semi-supervised learning, with an optimization over agraph loss function defined on smoothness proper-ties (Corduneanu and Jaakkola, 2002; Zhu et al,2003; Subramanya and Bilmes, 2009).Related to the problem of polysemy is the issue ofdifferent shades of meaning a word assumes basedon context.
The space of research on this topiccan be divided into three broad categories: modelsfor computing contextual lexical semantics based oncomposition (Mitchell and Lapata, 2008; Erk andPad?, 2008; Thater et al, 2011), models that usefuzzy exemplar-based contexts without composingthem (Erk and Pad?, 2010; Reddy et al, 2011), andmodels that propose latent variable techniques (Dinuand Lapata, 2010; S?aghdha and Korhonen, 2011;Van de Cruys et al, 2011).
Our work, which tacklesthe stronger form of lexical ambiguity in polysemyfalls into the latter two of three categories.5 Conclusion and Future WorkWe have presented two general and flexibleapproaches to producing sense-specific VSMsgrounded in an ontology.
The first technique is ap-plicable to any VSM as an efficient post-processingstep while the second provides a framework to in-tegrate ontological information with existing MLE-based predictive models.
We presented an evalua-tion of 3 semantic tasks on 7 datasets.
Our resultsshow that our proposed methods are effectively ableto capture the different senses in an ontology.
Inmost cases this results in significant improvementsover baselines.
We have also discussed the trade-offs between the two techniques from several differ-ent perspectives.
Finally, we have presented a qual-itative analysis investigating the nature of the sense-specific vectors, and shown that they capture the se-mantics of different senses.Our findings suggest several avenues for futureresearch.
We propose to use sense-specific vectorsas features in downstream applications such a WordSense Disambiguation.
Our current approach as-sumes a fixed ontology, but we hope to explore amore bi-directional relationship between ontologyand VSM in future work.
In particularly we envisagesimultaneously incrementing ontologies with struc-ture learning in addition to improving VSMs.
Wealso hope to extend our research to the multi-lingualdomain.
We are particularly excited by the idea ofusing multi-lingual WordNets to learn sense specificsemantic vectors that generalize across languages.AcknowledgmentsThe authors would like to thank Manaal Faruqui,Jesse Dodge and Noah Smith for their insight andfeedback.
Thanks also go to the anonymous re-viewers for their valuable comments and sugges-tions to improve the quality of the paper.
Thiswork was supported in part by the following grants:NSF grant IIS-1143703, NSF award IIS-1147810,DARPA grant FA87501220342.691ReferencesElia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Arti-ficial Intelligence Research (JAIR), 49:1?47.Bob Carpenter.
2008.
Lazy sparse stochastic gradient de-scent for regularized multinomial logistic regression.technical report, alias-i.
available at http://lingpipe-blog.com/ lingpipe-white-papers.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference on Machine learn-ing, ICML ?08, pages 160?167, New York, NY, USA.ACM.Adrian Corduneanu and Tommi Jaakkola.
2002.
On in-formation regularization.
In Proceedings of the Nine-teenth conference on Uncertainty in Artificial Intelli-gence, pages 151?158.
Morgan Kaufmann PublishersInc.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proc.
of ACL.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised frame-semantic parsing for unknown pred-icates.
In Proc.
of ACL.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society forInformation Science.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172.
Associationfor Computational Linguistics.Katrin Erk and Sebastian Pad?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 897?906, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Katrin Erk and Sebastian Pad?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof the ACL 2010 Conference Short Papers, pages 92?97.
Association for Computational Linguistics.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, ChrisDyer, Eduard Hovy, and Noah A Smith.
2014.Retrofitting word vectors to semantic lexicons.
arXivpreprint arXiv:1411.4166.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: The con-cept revisited.
In ACM Transactions on InformationSystems, volume 20, pages 116?131, January.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages 758?764, Atlanta, Georgia, June.
Association for Compu-tational Linguistics.Jiang Guo, Wanxiang Che, Haifeng Wang, and TingLiu.
2014.
Learning sense-specific word embeddingsby exploiting bilingual resources.
In Proceedings ofCOLING, pages 497?507.Eric H Huang, Richard Socher, Christopher D Manning,and Andrew Y Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of the 50th ACL: Long Papers-Volume1, pages 873?882.Mario Jarmasz and Stan Szpakowicz.
2004.
Roget?sthesaurus and semantic similarity.
Recent Advancesin Natural Language Processing III: Selected Papersfrom RANLP, 2003:111.Mark Johnson.
2007.
Why doesn?t em find good hmmpos-taggers?
In EMNLP-CoNLL, pages 296?305.Citeseer.Adam Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31(2):91?113.Thomas K Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review, pages 211?240.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Sch?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NY,USA.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the NAACL: Human Language Technolo-gies, pages 746?751, Atlanta, Georgia, June.George Miller and Walter Charles.
1991.
Contextual cor-relates of semantic similarity.
In Language and Cog-nitive Processes, pages 1?28.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244.Andriy Mnih and Yee Whye Teh.
2012.
A fast and sim-ple algorithm for training neural probabilistic languagemodels.
In Proceedings of the 29th International Con-ference on Machine Learning, pages 1751?1758.692Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of EMNLP.Ted Pedersen and Varada Kolhatkar.
2009.
Wordnet::Senserelate:: Allwords: a broad coverage word sensetagger that maximizes semantic relatedness.
In Pro-ceedings of human language technologies: The 2009annual conference of the north american chapter of theassociation for computational linguistics, companionvolume: Demonstration session, pages 17?20.
Associ-ation for Computational Linguistics.Mohammad Taher Pilehvar, David Jurgens, and RobertoNavigli.
2013.
Align, disambiguate and walk: A uni-fied approach for measuring semantic similarity.
InACL (1), pages 1341?1351.Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, andSuresh Manandhar.
2011.
Dynamic and static pro-totype vectors for semantic composition.
In IJCNLP,pages 705?713.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InProceedings of the 11th Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-2010), pages 109?117.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.Diarmuid ?
S?aghdha and Anna Korhonen.
2011.
Prob-abilistic models of similarity in syntactic context.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP ?11, pages1047?1057, Stroudsburg, PA, USA.
Association forComputational Linguistics.Amarnag Subramanya and Jeff A Bilmes.
2009.
En-tropic graph regularization in non-parametric semi-supervised classification.
In NIPS, pages 1803?1811.Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fernan-des, and Gregory Marton.
2003.
Quantitative evalu-ation of passage retrieval algorithms for question an-swering.
In SIGIR, pages 41?47.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effec-tive vector model.
In IJCNLP, pages 1134?1143.Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,Enhong Chen, and Tie-Yan Liu.
2014.
A probabilisticmodel for learning multi-prototype word embeddings.In Proceedings of COLING, pages 151?160.Peter D. Turney.
2001.
Mining the web for syn-onyms: Pmi-ir versus lsa on toefl.
In Proceedingsof the 12th European Conference on Machine Learn-ing, EMCL ?01, pages 491?502, London, UK, UK.Springer-Verlag.Peter D. Turney.
2006.
Similarity of semantic relations.Comput.
Linguist., 32(3):379?416, September.Peter D Turney.
2013.
Distributional semantics be-yond words: Supervised learning of analogy and para-phrase.
Transactions of the Association for Computa-tional Linguistics, 1:353?366.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2011.
Latent vector weighting for word mean-ing in context.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 1012?1022.
Association for Computational Lin-guistics.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proceedings ofthe 52nd Annual Meeting of the Association for Com-putational Linguistics.Zhi Zhong and Hwee Tou Ng.
2010.
It makes sense:A wide-coverage word sense disambiguation systemfor free text.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 78?83.
Association for Com-putational Linguistics.Xiaojin Zhu, Zoubin Ghahramani, John Lafferty, et al2003.
Semi-supervised learning using gaussian fieldsand harmonic functions.
In ICML, volume 3, pages912?919.693
