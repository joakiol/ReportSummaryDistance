Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 3?11,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSample Selection for Statistical Parsers: Cognitively DrivenAlgorithms and Evaluation MeasuresRoi ReichartICNCHebrew University of Jerusalemroiri@cs.huji.ac.ilAri RappoportInstitute of computer scienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractCreating large amounts of manually annotatedtraining data for statistical parsers imposesheavy cognitive load on the human annota-tor and is thus costly and error prone.
Itis hence of high importance to decrease thehuman efforts involved in creating trainingdata without harming parser performance.
Forconstituency parsers, these efforts are tradi-tionally evaluated using the total number ofconstituents (TC) measure, assuming uniformcost for each annotated item.
In this paper, weintroduce novel measures that quantify aspectsof the cognitive efforts of the human annota-tor that are not reflected by the TC measure,and show that they are well established in thepsycholinguistic literature.
We present a novelparameter based sample selection approachfor creating good samples in terms of thesemeasures.
We describe methods for global op-timisation of lexical parameters of the sam-ple based on a novel optimisation problem, theconstrained multiset multicover problem, andfor cluster-based sampling according to syn-tactic parameters.
Our methods outperformpreviously suggested methods in terms of thenew measures, while maintaining similar TCperformance.1 IntroductionState of the art statistical parsers require largeamounts of manually annotated data to achieve goodperformance.
Creating such data imposes heavycognitive load on the human annotator and is thuscostly and error prone.
Statistical parsers are ma-jor components in NLP applications such as QA(Kwok et al, 2001), MT (Marcu et al, 2006) andSRL (Toutanova et al, 2005).
These often oper-ate over the highly variable Web, which consists oftexts written in many languages and genres.
Sincethe performance of parsers markedly degrades whentraining and test data come from different domains(Lease and Charniak, 2005), large amounts of train-ing data from each domain are required for usingthem effectively.
Thus, decreasing the human effortsinvolved in creating training data for parsers withoutharming their performance is of high importance.In this paper we address this problem throughsample selection: given a parsing algorithm and alarge pool of unannotated sentences S, select a sub-set S1 ?
S for human annotation such that the hu-man efforts in annotating S1 are minimized whilethe parser performance when trained with this sam-ple is maximized.Previous works addressing training sample sizevs.
parser performance for constituency parsers(Section 2) evaluated training sample size using thetotal number of constituents (TC).
Sentences differin length and therefore in annotation efforts, and ithas been argued (see, e.g, (Hwa, 2004)) that TC re-flects the number of decisions the human annotatormakes when syntactically annotating the sample, as-suming uniform cost for each decision.In this paper we posit that important aspects ofthe efforts involved in annotating a sample are notreflected by the TC measure.
Since annotators ana-lyze sentences rather than a bag of constituents, sen-tence structure has a major impact on their cognitiveefforts.
Sizeable psycholinguistic literature pointsto the connection between nested structures in thesyntactic structure of a sentence and its annotationefforts.
This has motivated us to introduce (Sec-tion 3) three sample size measures, the total and av-3erage number of nested structures of degree k in thesample, and the average number of constituents persentence in the sample.Active learning algorithms for sample selectionfocus on sentences that are difficult for the parsingalgorithm when trained with the available trainingdata (Section 2).
In Section 5 we show that activelearning samples contain a high number of complexstructures, much higher than their number in a ran-domly selected sample that achieves the same parserperformance level.
To avoid that, we introduce (Sec-tion 4) a novel parameter based sample selection(PBS) approach which aims to select a sample thatenables good estimation of the model parameters,without focusing on difficult sentences.
In Section 5we show that the methods derived from our approachselect substantially fewer complex structures thanactive learning methods and the random baseline.We propose two different methods.
In clusterbased sampling (CBS), we aim to select a samplein which the distribution of the model parameters issimilar to their distribution in the whole unlabelledpool.
To do that we build a vector representation foreach sentence in the unlabelled pool reflecting thedistribution of the model parameters in this sentence,and use a clustering algorithm to divide these vectorsinto clusters.
In the second method we use the factthat a sample containing many examples of a certainparameter yields better estimation of this parameter.If this parameter is crucial for model performanceand the selection process does not harm the distri-bution of other parameters, then the selected sam-ple is of high quality.
To select such a sample weintroduce a reduction between this selection prob-lem and a variant of the NP-hard multiset-multicoverproblem (Hochbaum, 1997).
We call this problemthe constrained multiset multicover (CMM) problem,and present an algorithm to approximate it.We experiment (Section 5) with the WSJ Pen-nTreebank (Marcus et al, 1994) and Collins?
gen-erative parser (Collins, 1999), as in previous work.We show that PBS algorithms achieve good resultsin terms of both the traditional TC measure (signifi-cantly better than the random selection baseline andsimilar to the results of the state of the art tree en-tropy (TE) method of (Hwa, 2004)) and our novelcognitively driven measures (where PBS algorithmssignificantly outperform both TE and the randombaseline).
We thus argue that PBS provides a way toselect a sample that imposes reduced cognitive loadon the human annotator.2 Related WorkPrevious work on sample selection for statisticalparsers applied active learning (AL) (Cohn and Lad-ner, 1994) to corpora of various languages and syn-tactic annotation schemes and to parsers of differentperformance levels.
In order to be able to compareour results to previous work targeting high parserperformance, we selected the corpus and parserused by the method reporting the best results (Hwa,2004), WSJ and Collins?
parser.Hwa (2004) used uncertainty sampling with thetree entropy (TE) selection function1 to select train-ing samples for the Collins parser.
In each it-eration, each of the unlabelled pool sentences isparsed by the parsing model, which outputs a listof trees ranked by their probabilities.
The scoredlist is treated as a random variable and the sentenceswhose variable has the highest entropy are selectedfor human annotation.
Sample size was measuredin TC and ranged from 100K to 700K WSJ con-stituents.
The initial size of the unlabelled pool was800K constituents (the 40K sentences of sections 2-21 of WSJ).
A detailed comparison between the re-sults of TE and our methods is given in Section 5.The following works addressed the task of sam-ple selection for statistical parsers, but in signifi-cantly different experimental setups.
Becker andOsborne (2005) addressed lower performance lev-els of the Collins parser.
Their uncertainty sam-pling protocol combined bagging with the TE func-tion, achieving a 32% TC reduction for reaching aparser f-score level of 85.5%.
The target sample sizeset contained a much smaller number of sentences(?5K) than ours.
Baldridge and Osborne (2004) ad-dressed HPSG parse selection using a feature basedlog-linear parser, the Redwoods corpus and commit-tee based active learning, obtaining 80% reductionin annotation cost.
Their annotation cost measurewas related to the number of possible parses of thesentence.
Tang et al (2002) addressed a shallowparser trained on a semantically annotated corpus.1Hwa explored several functions in the experimental setupused in the present work, and TE gave the best results.4They used an uncertainty sampling protocol, wherein each iteration the sentences of the unlabelled poolare clustered using a distance measure defined onparse trees to a predefined number of clusters.
Themost uncertain sentences are selected from the clus-ters, the training taking into account the densities ofthe clusters.
They reduced the number of trainingsentences required for their parser to achieve its bestperformance from 1300 to 400.The importance of cognitively driven measures ofsentences?
syntactic complexity has been recognizedby Roark et al (2007) who demonstrated their utilityfor mild cognitive impairment diagnosis.
Zhu et al(2008) used a clustering algorithm for sampling theinitial labeled set in an AL algorithm for word sensedisambiguation and text classification.
In contrast toour CBS method, they proceeded with iterative un-certainty AL selection.
Melville et al (2005) usedparameter-based sample selection for a classifier ina classic active learning setting, for a task very dif-ferent from ours.Sample selection has been applied to many NLPapplications.
Examples include base noun phrasechunking (Ngai, 2000), named entity recognition(Tomanek et al, 2007) and multi?task annotation(Reichart et al, 2008).3 Cognitively Driven Evaluation MeasuresWhile the resources, capabilities and constraints ofthe human parser have been the subject of extensiveresearch, different theories predict different aspectsof its observed performance.
We focus on struc-tures that are widely agreed to impose a high cog-nitive load on the human annotator and on theoriesconsidering the cognitive resources required in pars-ing a complete sentence.
Based on these, we derivemeasures for the cognitive load on the human parserwhen syntactically annotating a set of sentences.Nested structures.
A nested structure is a parsetree node representing a constituent created whileanother constituent is still being processed (?open?
).The degree K of a nested structure is the number ofsuch open constituents.
In this paper, we enumer-ate the constituents in a top-down left-right order,and thus when a constituent is created, only its an-cestors are processed2.
A constituent is processed2A good review on node enumeration of the human parserin given in (Abney and Johnson, 1991).SNP1JJLastNNweekNP2NNPIBMVPVBDboughtNP3NNPLotusFigure 1: An example parse tree.until the processing of its children is completed.
Forexample, in Figure 1, when the constituent NP3 iscreated, it starts a nested structure of degree 2, sincetwo levels of its ancestors (VP, S) are still processed.Its parent (VP) starts a nested structure of degree 1.The difficulty of deeply nested structures for thehuman parser is well established in the psycholin-guistics literature.
We review here some of the vari-ous explanations of this phenomenon; for a compre-hensive review see (Gibson, 1998).According to the classical stack overflow theory(Chomsky and Miller, 1963) and its extension, theincomplete syntactic/thematic dependencies theory(Gibson, 1991), the human parser should track theopen structures in its short term memory.
When thenumber of these structures is too large or when thestructures are nested too deeply, the short term mem-ory fails to hold them and the sentence becomes un-interpretable.According to the perspective shifts theory(MacWhinney, 1982), processing deeply nestedstructures requires multiple shifts of the annotatorperspective and is thus more difficult than process-ing shallow structures.
The difficulty of deeplynested structured has been demonstrated for manylanguages (Gibson, 1998).We thus propose the total number of nested struc-tures of degree K in a sample (TNSK) as a measureof the cognitive efforts that its annotation requires.The higher K is, the more demanding the structure.Sentence level resources.
In the psycholinguis-tic literature of sentence processing there are manytheories describing the cognitive resources requiredduring a complete sentence processing.
These re-sources might be allocated during the processing ofa certain word and are needed long after its con-stituent is closed.
We briefly discuss two lines oftheory, focusing on their predictions that sentencesconsisting of a large number of structures (e.g., con-5stituents or nested structures) require more cognitiveresources for longer periods.Levelt (2001) suggested a layered model of themental lexicon organization, arguing that when onehears or reads a sentence s/he activates word forms(lexemes) that in turn activate lemma information.The lemma information contains information aboutsyntactic properties of the word (e.g., whether it isa noun or a verb) and about the possible sentencestructures that can be generated given that word.
Theprocess of reading words and retrieving their lemmainformation is incremental and the lemma informa-tion for a given word is used until its syntactic struc-ture is completed.
The information about a word in-clude all syntactic predictions, obligatory (e.g., theprediction of a noun following a determiner) and op-tional (e.g., optional arguments of a verb, modifierrelationships).
This information might be relevantlong after the constituents containing the word areclosed, sometimes till the end of the sentence.Another line of research focuses on workingmemory, emphasizing the activation decay princi-ple.
It stresses that words and structures perceivedduring sentence processing are forgotten over time.As the distance between two related structures in asentence grows, it is more demanding to reactivateone when seeing the other.
Indeed, supported bya variety of observations, many of the theories ofthe human parser (see (Lewis et al, 2006) for a sur-vey) predict that processing items towards the end oflonger sentences should be harder, since they mostoften have to be integrated with items further back.Thus, sentences with a large number of structuresimpose a special cognitive load on the annotator.We thus propose to use the number of structures(constituents or nested structures) in a sentence as ameasure of its difficulty for human annotation.
Themeasures we use for a sample (a sentence set) are theaverage number of constituents (AC) and the aver-age number of nested structures of degree k (ANSK)per sentence in the set.
Higher AC or ANSK valuesof a set imply higher annotation requirements3.Pschycolinguistics research makes finer observa-3The correlation between the number of constituents andsentence length is very strong (e.g., correlation coefficient of0.93 in WSJ section 0).
We could use the number of words, butwe prefer the number of structures since the latter better reflectsthe arguments made in the literature.tions about the human parser than those describedhere.
A complete survey of that literature is beyondthe scope of this paper.
We consider the proposedmeasures a good approximation of some of the hu-man parser characteristics.4 Parameter Based Sampling (PBS)Our approach is to sample the unannotated pool withrespect to the distribution of the model parametersin its sentences.
In this paper, in order to compare toprevious works, we apply our methods to the Collinsgenerative parser (Collins, 1999).
For any sentences and parse tree t it assigns a probability p(s, t),and finds the tree for which this probability is maxi-mized.
To do that, it writes p(t, s) as a product of theprobabilities of the constituents in t and decomposesthe latter using the chain rule.
In simplified notation,it uses:p(t, s) =?P (S1 ?
S2 .
.
.
Sn) =?P (S1)?.
.
.
?P (Sn|?
(S1 .
.
.
Sn))(1)We refer to the conditional probabilities as the modelparameters.Cluster Based Sampling (CBS).
We describehere a method for sampling subsets that leads to aparameter estimation that is similar to the parame-ter estimation we would get if annotating the wholeunannotated set.To do that, we randomly select M sentences fromthe unlabelled pool N , manually annotate them,train the parser with these sentences and parse therest of the unlabelled pool (G = N ?
M ).
Usingthis annotation we build a syntactic vector repre-sentation for each sentence in G. We then clusterthese sentences and sample the clusters with respectto their weights to preserve the distribution of thesyntactic features.
The selected sentences are man-ually annotated and combined with the group of Msentences to train the final parser.
The size of thiscombined sample is measured when the annotationefforts are evaluated.Denote the left hand side nonterminal of a con-stituent by P and the unlexicalized head of the con-stituent by H .
The domain of P is the set of non-terminals (excluding POS tags) and the domain of His the set of nonterminals and POS tags of WSJ.
Inall the parameters of the Collins parser P and H areconditioned upon.
We thus use (P,H) pairs as the6features in the vector representation of each sentencein G. The i-th coordinate is given by the equation:?c?t(s)?iFi(Q(c) == i) ?
L(c) (2)Where c are the constituents of the sentence parset(s), Q is a function that returns the (P,H) pairof the constituent c, Fi is a predicate that returns 1iff it is given pair number i as an argument and 0otherwise, and L is the number of modifying non-terminals in the constituent plus 1 (for the head),counting the number of parameters that conditionon (P,H).
Following equation (2), the ith coordi-nate of the vector representation of a sentence in Gcontains the number of parameters that will be cal-culated conditioned on the ith (P,H) pair.We use the k-means clustering algorithm, with theL2 norm as a distance metric (MacKay, 2002), to di-vide vectors into clusters.
Clusters created by thisalgorithm contain adjacent vectors in a Euclideanspace.
Clusters represent sentences with similar fea-tures values.
To initialize k-means, we sample theinitial centers values from a uniform distributionover the data points.We do not decide on the number of clusters in ad-vance but try to find inherent structure in the data.Several methods for estimating the ?correct?
num-ber of clusters are known (Milligan and Cooper,1985).
We used a statistical heuristic called theelbow test.
We define the ?within cluster disper-sion?
Wk as follows.
Suppose that the data is di-vided into k clusters C1 .
.
.
Ck with |Cj | points inthe jth cluster.
Let Dt = ?i,j?Ct di,j wheredi,j is the squared Euclidean distance, then Wk :=?kt=112|Ct|Dt.
Wk tends to decrease monotonicallyas k increases.
In many cases, from some k this de-crease flattens markedly.
The heuristic is that thelocation of such an ?elbow?
indicates the appropriatenumber of clusters.
In our experiments, an obviouselbow occurred for 15 clusters.ki sentences are randomly sampled from eachcluster, ki = D |Ci|?j |Cj |, where D is the numberof sentences to be sampled from G. That way weensure that in the final sample each cluster is repre-sented according to its size.CMM Sampling.
All of the parameters in theCollins parser are conditioned on the constituent?shead word.
Since word statistics are sparse, sam-pling from clusters created according to a lexicalvector representation of the sentences does not seempromising4.Another way to create a sample from which theparser can extract robust head word statistics is toselect a sample containing many examples of eachword.
More formally, we denote the words that oc-cur in the unlabelled pool at least t times by t-words,where t is a parameter of the algorithm.
We want toselect a sample containing at least t examples of asmany t-words as possible.To select such a sample we introduce a novel op-timisation problem.
Our problem is a variant of themultiset multicover (MM) problem, which we callthe constrained multiset multicover (CMM) prob-lem.
The setting of the MM problem is as fol-lows (Hochbaum, 1997): Given a set I of m ele-ments to be covered each bi times, a collection ofmultisets Sj ?
I , j ?
J = {1, .
.
.
, n} (a multiset isa set in which members?
multiplicity may be greaterthan 1), and weights wj , find a subcollection C ofmultisets that covers each i ?
I at least bi times, andsuch that?j?C wj is minimized.CMM differs from MM in that in CMM the sumof the weights (representing the desired number ofsentences to annotate) is bounded, while the num-ber of covered elements (representing the t-words)should be maximized.
In our case, I is the set ofwords that occur at least t times in the unlabelledpool, bi = t,?i ?
I , the multisets are the sentencesin that pool and wj = 1,?j ?
J .Multiset multicover is NP-hard.
However, there isa good greedy approximation algorithm for it.
De-fine a(sj , i) = min(R(sj , i), di), where di is thedifference between bi and the number of instancesof item i that are present in our current sample, andR(sj , i) is the multiplicity of the i-th element in themultiset sj .
Define A(sj) to be the multiset contain-ing exactly a(sj , i) copies of any element i if sj isnot already in the set cover and the empty set if itis.
The greedy algorithm repeatedly adds a set mini-mizing wj|A(sj)| .
This algorithm provenly achieves anapproximation ratio between ln(m) and ln(m) + 1.In our case all weights are 1, so the algorithm would4We explored CBS with several lexical features schemes andgot only marginal improvement over random selection.7simply add the sentence that maximizes A(sj) to theset cover.The problem in directly applying the algorithm toour case is that it does not take into account the de-sired sample size.
We devised a variant of the algo-rithm where we use a binary tree to ?push?
upwardsthe number of t-words in the whole batch of unan-notated sentences that occurs at least t times in theselected one.
Below is a detailed description.
D de-notes the desired number of items to sample.The algorithm has two steps.
First, we iter-atively sample (without replacement) D multisets(sentences) from a uniform distribution over themultisets.
In each iteration we calculate for the se-lected multiset its ?contribution?
?
the number ofitems that cross the threshold of t occurrences withthis multiset minus the number of items that crossthe t threshold without this multiset (i.e.
the contri-bution of the first multiset is the number of t-wordsoccurring more than t times in it).
For each multisetwe build a node with a key that holds its contribu-tion, and insert these nodes in a binary tree.
Inser-tion is done such that all downward paths are sortedin decreasing order of key values.Second, we iteratively sample (from a uniformdistribution, without replacement) the rest of themultisets pool.
For each multiset we perform twosteps.
First, we prepare a node with a key as de-scribed above.
We then randomly choose Z leaves5in the binary tree (if the number of leaves is smallerthan Z all of the leaves are chosen).
For each leaf wefind the place of the new node in the path from theroot to the leaf (paths are sorted in decreasing orderof key values).
We insert the new node to the high-est such place found (if the new key is not smallerthan the existing paths), add its multiset to the set ofselected multisets, and remove the multiset that cor-responds to the leaf of this path from the batch andthe leaf itself from the binary tree.
We finally choosethe multisets that correspond to the highest D nodesin the tree.An empirical demonstration of the quality of ap-proximation that the algorithm provides is given inFigure 2.
We ran our algorithm with the thresholdparameter set to t ?
[2, 14] and counted the num-5We tried Z values from 10 to 100 in steps of 10 and ob-served very similar results.
We report results for Z = 100.0 100 200 300 400 500 600 700 800010002000300040005000600070008000900010000number of training constituents (thousands)numberoft?wordst=2t=8t=11t=14t=5randomFigure 2: Number of t-words for t = 5 in samples selectedby CMM runs with different values of the threshold pa-rameter t and in a randomly selected sample.
CMM witht = 5 is significantly higher.
All the lines except for theline for t = 5 are unified.
For clarity, we do not show all tvalues: their curves are also similar to the t 6= 5 lines.Method 86% 86.5% 87% 87.5% 88%TE 16.9% 27.1% 26.9% 14.8% 15.8%(152K) (183K) (258K) (414K) (563 K)CBS 19.6% 16.8% 19% 21.1% 9%(147K) (210K) (286K) (382K) (610K)CMM 9% 10.4% 8.9% 10.3% 14%(167K) (226K) (312K) (433K) (574K)Table 1: Reduction in annotation cost in TC terms com-pared to the random baseline for tree entropy (TE), syn-tactic clustering (CBS) and CMM.
The compared samplesare the smallest samples selected by each of the methodsthat achieve certain f-score levels.
Reduction is calcu-lated by: 100 ?
100 ?
(TCmethod/TCrandom).ber of words occurring at least 5 times in the se-lected sample.
We followed the same experimen-tal protocol as in Section 5.
The graph shows thatthe number of words occurring at least 5 times in asample selected by our algorithm when t = 5 is sig-nificantly higher (by about a 1000) than the numberof such words in a randomly selected sample and insamples selected by our algorithm with other t pa-rameter values.
We got the same pattern of resultswhen counting words occurring at least t times forthe other values of the t parameter ?
only the run ofthe algorithm with the corresponding t value createda sample with significantly higher number of wordsnot below threshold.
The other runs and random se-lection resulted in samples containing significantlylower number of words not below threshold.In Section 5 we show that the parser performancewhen it is trained with a sample selected by CMMis significantly better than when it is trained with arandomly selected sample.
Improvement is similaracross the t parameter values.886% 87% 88%Method TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK(1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22)TE 34.9% 3.6% - 8.9% - 61.3% 42.2% 14.4% - 9.9% - 62.7% 25% 8.1% - 6.3% - 30%CBS 21.3% 18.6% - 0.5% - 3.5% 19.6% 24.2% - 0.3% - 1.8% 8.9% 8.6 % 0% - 0.3%CMM 10.18% 8.87% -0.82% -3.39% 11% 16.22% -0.34% -1.8% 14.65% 14.11% -0.02% - 0.08%Table 2: Annotation cost reduction in TNSK and ANSK compared to the random baseline for tree entropy (TE), syntacticclustering (CBS) and CMM.
The compared samples are the smallest selected by each of the methods that achieve certainf-score levels.
Each column represents the reduction in total or average number of structures of degree 1?6 or 7?22.Reduction for each measure is calculated by: 100?
100?
(measuremethod/measurerandom).
Negative reductionis an addition.
Samples with a higher reduction in a certain measure are better in terms of that measure.0 5 10 15 20 25?10123x 104KTNSK(K)CMM(t=8) ?
TECBS ?
TE0 line0 5 10 15 20 2511.11.21.31.41.51.61.7KANSKmethod/ANSKrandomTECMM,t=8CBS86  86.5 87  87.5 88  182022242628F scoreAveragenumberofconstituentsTECMM,  t = 8CBS0 1 2 3 3.5x 10411.251.5Number  of  sentencesACmethod/ACrandomTECMM, t=8CBSFigure 3: Left to right: First: The difference between the number of nested structures of degree K of CMM and TE andof CBS and TE.
The curves are unified.
The 0 curve is given for reference.
Samples selected by CMM and CBS havemore nested structures of degrees 1?6 and less nested structures of degrees 7?22.
Results are presented for the smallestsamples required for achieving f-score of 88.
Similar patterns are observed for other f-score values.
Second: Averagenumber of nested structures of degree K as a function of K for the smallest sample required for achieving f-score of88.
Results for each of the methods are normalized by the average number of nested structures of degree K in thesmallest randomly selected sample required for achieving f-score of 88.
The sentences in CMM and CBS samples arenot more complex than sentences in a randomly selected sample.
In TE samples sentences are more complex.
Third:Average number of constituents (AC) for the smallest sample of each of the methods that is required for achieving agiven f-score.
CMM and CBS samples contain sentences with a smaller number of constituents.
Fourth: AC values forthe samples created by the methods (normalized by AC values of a randomly selected sample).
The sentences in TEsamples, but not in CMM and CBS samples, are more complex than sentences in a randomly selected sample.5 ResultsExperimental setup.
We used Bikel?s reimplemen-tation of Collins?
parsing model 2 (Bikel, 2004).Sections 02-21 and 23 of the WSJ were strippedfrom their annotation.
Sections 2-21 (39832 sen-tences, about 800K constituents) were used for train-ing, Section 23 (2416 sentences) for testing.
Nodevelopment set was used.
We used the gold stan-dard POS tags in two cases: in the test section (23)in all experiments, and in Sections 02-21 in theCBS method when these sections are to be parsedin the process of vector creation.
In active learn-ing methods the unlabelled pool is parsed in eachiteration and thus should be tagged with POS tags.Hwa (2004) (to whom we compare our results) usedthe gold standard POS tags for the same sectionsin her work6.
We implemented a random baseline6Personal communication with Hwa.
Collins?
parser uses anwhere sentences are uniformly selected from the un-labelled pool for annotation.
For reliability we re-peated each experiment with the algorithms and therandom baseline 10 times, each time with differentrandom selections (M sentences for creating syntac-tic tagging and k-means initialization for CBS, sen-tence order in CMM), and averaged the results.Each experiment contained 38 runs.
In each runa different desired sample size was selected, from1700 onwards, in steps of 1000.
Parsing perfor-mance is measured in terms of f-scoreResults.
We compare the performance of ourCBS and CMM algorithms to the TE method (Hwa,2004)7, which is the only sample selection work ad-input POS tag only if it cannot tag its word using the statisticslearned from the training set.7Hwa has kindly sent us the samples selected by her TE.
Weevaluated these samples with TC and the new measures.
The TCof the minimal sample she sent us needed for achieving f-score9dressing our experimental setup.
Unless otherwisestated, we report the reduction in annotation cost:100?
100?
(measuremethod/measurerandom).CMM results are very similar for t ?
{2, 3, .
.
.
, 14},and presented for t = 8.Table 1 presents reduction in annotation cost inTC terms.
CBS achieves greater reduction for f =86, 87.5, TE for f = 86.5, 87, 88.
For f = 88, TEand CMM performance are almost similar.
Examin-ing the f-score vs. TC sample size over the wholeconstituents range (not shown due to space con-straints) reveals that CBS, CMM and TE outperformrandom selection over the whole range.
CBS andTE performance are quite similar with TE being bet-ter in the ranges of 170?300K and 520?650K con-stituents (42% of the 620K constituents compared)and CBS being better in the ranges of 130?170K and300?520K constituents (44% of the range).
CMMperformance is worse than CBS and TE until 540Kconstituents.
From 650K constituents on, wherethe parser achieves its best performance, the perfor-mance of CMM and TE methods are similar, outper-forming CBS.Table 2 shows the annotation cost reduction inANSK and TNSK terms.
TE achieves remarkablereduction in the total number of relatively shallowstructures (TNSK K = 1?6).
Our methods, in con-trast, achieve remarkable reduction in the number ofdeep structures (TNSK K = 7?22)8.
This is true forall f-score values.
Moreover, the average number ofnested structures per sentence, for every degree K(ANSK for every K) in TE sentences is much higherthan in sentences of a randomly selected sample.
Insamples selected by our methods, the ANSK valuesare very close to the ANSK values of randomly se-lected samples.
Thus, sentences in TE samples aremuch more complex than in CBS and CMM samples.The two leftmost graphs in Figure 3 demonstrate(for the minimal samples required for f-score of 88)that these reductions hold for each K value (ANSK)and for each K ?
[7, 22] (TNSK) not just on the av-of 88 is different from the number reported in (Hwa, 2004).
Wecompare our TC results with the TC result in the sample sent usby Hwa.8We present results where the border between shallow anddeep structures is set to be Kborder = 6.
For every Kborder ?
{7, 8, .
.
.
, 22} TNSK reductions with CBS and CMM are muchmore impressive than with TE for structures whose degree isK ?
[Kborder, 22].erage over these K values.
We observed similar re-sults for other f-score values.The two rightmost graphs of Figure 3 demon-strates AC results.
The left of them shows that forevery f-score value, the AC measure of the minimalTE sample required to achieve that f-score is higherthan the AC value of PBS samples (which are verysimilar to the AC values of randomly selected sam-ples).
The right graph demonstrates that for everysample size, the AC value of TE samples is higherthan that of PBS samples.All AL based previous work (including TE) is it-erative.
In each iteration thousands of sentencesare parsed, while PBS algorithms perform a singleiteration.
Consequently, PBS computational com-plexity is dramatically lower.
Empirically, using aPentium 4 2.4GHz machine, CMM requires about anhour and CBS about 16.5 hours, while the TE parsingsteps alone take 662 hours (27.58 days).6 Discussion and Future WorkWe introduced novel evaluation measures: AC,TNSK and ANSK for the task of sample selectionfor statistical parsers.
Based on the psycholinguis-tic literature we argue that these measures reflect as-pects of the cognitive efforts of the human annota-tor that are not reflected by the traditional TC mea-sure.
We introduced the parameter based sample se-lection (PBS) approach and its CMM and CBS algo-rithms that do not deliberately select difficult sen-tences.
Therefore, our intuition was that they shouldselect a sample that leads to an accurate parameterestimation but does not contain a high number ofcomplex structures.
We demonstrated that CMM andCBS achieve results that are similar to the state of theart TE method in TC terms and outperform it whenthe cognitively driven measures are considered.The measures we suggest do not provide a fulland accurate description of human annotator efforts.In future work we intend to extend and refine ourmeasures and to revise our algorithms accordingly.We also intend to design stopping criteria for thePBS methods.
These are criteria that decide whenthe selected sample suffices for the parser best per-formance and further annotation is not needed.10ReferencesSteven Abney and Mark Johnson, 1991.
Memory re-quirements and local ambiguities of parsing strategies.Psycholinguistic Research, 20(3):233?250.Daniel M. Biken, 2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Jason Baldridge and Miles Osborne, 2004.
Active learn-ing and the total cost of annotation.
EMNLP ?04.Markus Becker and Miles Osborne, 2005.
A two-stagemethod for active learning of statistical grammars.
IJ-CAI 05.Markus Becker, 2008.
Active learning ?
an explicit treat-ment of unreliable parameters.
Ph.D. thesis, The Uni-versity of Edinburgh.Noam Chomsky and George A. Miller, 1963.
Fini-tary models of language users.
In R. Duncan Luce,Robert R. Bush, and Eugene Galanter, editors, Hand-book of Mathematical Psychology, volume II.
JohnWiley, New York, 419?491.David Cohn, Les Atlas and Richard E. Ladner, 1994.
Im-proving generalization with active learning.
MachineLearning, 15(2):201?221.Michael Collins, 1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Edward Gibson, 1991.
A computational theory of hu-man linguistic processing: memory limitations andprocessing breakdowns.
Ph.D. thesis, Carnegie Mel-lon University, Pittsburg, PA.Edward Gibson, 1998.
Linguistic complexity: localityof syntactic dependencies.
Cognition, 68:1?76.Dorit Hochbaum (ed), 1997.
Approximation algorithmsfor NP-hard problems.
PWS Publishing, Boston.Rebecca Hwa, 2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Cody Kwok, Oren Etzioni, Daniel S. Weld, 2001.
Scal-ing question answering to the Web.
WWW ?01.Matthew Lease and Eugene Charniak, 2005.
Parsingbiomedical literature.
IJCNLP ?05.Willem J.M.
Levelt, 2001.
Spoken word production: Atheory of lexical access.
PNAS, 98(23):13464?13471.Richard L. Lewis, Shravan Vasishth and Julie Van Dyke,2006.
Computational principles of working memoryin sentence comprehension.
Trends in Cognitive Sci-ence, October:447-454.David MacKay, 2002.
Information theory, inference andlearning algorithms.
Cambridge University Press.Brian MacWhinney, 1982.
The competition model.
InB.
MacWhinney, editor, Mechanisms of language ac-quisition.
Hillsdale, NJ: Lawrence Erlbaum, 249?308.Daniel Marcu, Wei Wang, Abdessamabad Echihabi, andKevin Knight, 2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.EMNLP ?06.P.
Melville, M. Saar-Tsechansky, F. Provost and R.J.Mooney, 2005.
An expected utility approach to ac-tive feature-value acquisition.
5th IEEE Intl.
Conf.
onData Mining ?05.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz, 1994.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics, 19(2):313?330.G.W.
Milligan and M.C Cooper, 1985.
An examinationof procedures for determining the number of clustersin a data set.
Psychometrika, 58(2):159?157.Grace Ngai and David Yarowski, 2000.
Rule writingor annotation: cost?efficient resource usage for basenoun phrase chunking.
ACL ?00.Roi Reichart, Katrin Tomanek, Udo Hahn and Ari Rap-poport, 2008.
Multi-task active learning for linguisticannotations.
ACL ?08.Brian Roark, Margaret Mitchell and Kristy Hollingshead,2007.
Syntactic complexity measures for detectingmild cognitive impairment.
BioNLP workshop, ACL?07.Min Tang, Xiaoqiang Luo, and Salim Roukos, 2002.
Ac-tive learning for statistical natural language parsing.ACL ?02.Katrin Tomanek, Joachim Wermtre, and Udo Hahn,2007.
An approach to text corpus construction whichcuts annotation costs and maintains reusability of an-notated data.
EMNLP ?07.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning, 2005.
Joint learning improves semantic rolelabeling.
ACL ?05.Jingbo Zhu, Huizhen Wang, Tianshun Yao, and BenjaminK.
Tsou, 2008.
Active learning with sampling by un-certainty and density for word sense disambiguationand text classification.
COLING ?08.11
