Proceedings of the 12th Conference of the European Chapter of the ACL, pages 139?147,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsPredicting the fluency of text with shallow structural features: case studiesof machine translation and human-written textJieun ChaeUniversity of Pennsylvaniachaeji@seas.upenn.eduAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractSentence fluency is an important compo-nent of overall text readability but fewstudies in natural language processinghave sought to understand the factors thatdefine it.
We report the results of an ini-tial study into the predictive power of sur-face syntactic statistics for the task; we usefluency assessments done for the purposeof evaluating machine translation.
Wefind that these features are weakly but sig-nificantly correlated with fluency.
Ma-chine and human translations can be dis-tinguished with accuracy over 80%.
Theperformance of pairwise comparison offluency is also very high?over 90% for amulti-layer perceptron classifier.
We alsotest the hypothesis that the learned modelscapture general fluency properties applica-ble to human-written text.
The results donot support this hypothesis: prediction ac-curacy on the new data is only 57%.
Thisfinding suggests that developing a dedi-cated, task-independent corpus of fluencyjudgments will be beneficial for further in-vestigations of the problem.1 IntroductionNumerous natural language applications involvethe task of producing fluent text.
This is a coreproblem for surface realization in natural languagegeneration (Langkilde and Knight, 1998; Banga-lore and Rambow, 2000), as well as an impor-tant step in machine translation.
Considerationsof sentence fluency are also key in sentence sim-plification (Siddharthan, 2003), sentence compres-sion (Jing, 2000; Knight and Marcu, 2002; Clarkeand Lapata, 2006; McDonald, 2006; Turner andCharniak, 2005; Galley and McKeown, 2007), textre-generation for summarization (Daume?
III andMarcu, 2004; Barzilay and McKeown, 2005; Wanet al, 2005) and headline generation (Banko et al,2000; Zajic et al, 2007; Soricut and Marcu, 2007).Despite its importance for these popular appli-cations, the factors contributing to sentence levelfluency have not been researched indepth.
Muchmore attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of co-herence and good text flow (Lapata, 2003; Barzi-lay and Lapata, 2008; Karamanis et al, to appear).In many applications fluency is assessed incombination with other qualities.
For example, inmachine translation evaluation, approaches suchas BLEU (Papineni et al, 2002) use n-gram over-lap comparisons with a model to judge overall?goodness?, with higher n-grams meant to capturefluency considerations.
More sophisticated waysto compare a system production and a model in-volve the use of syntax, but even in these cases flu-ency is only indirectly assessed and the main ad-vantage of the use of syntax is better estimation ofthe semantic overlap between a model and an out-put.
Similarly, the metrics proposed for text gener-ation by (Bangalore et al, 2000) (simple accuracy,generation accuracy) are based on string-edit dis-tance from an ideal output.In contrast, the work of (Wan et al, 2005)and (Mutton et al, 2007) directly sets as a goalthe assessment of sentence-level fluency, regard-less of content.
In (Wan et al, 2005) the mainpremise is that syntactic information from a parsercan more robustly capture fluency than languagemodels, giving more direct indications of the de-gree of ungrammaticality.
The idea is extended in(Mutton et al, 2007), where four parsers are used139and artificially generated sentences with varyinglevel of fluency are evaluated with impressive suc-cess.
The fluency models hold promise for ac-tual improvements in machine translation outputquality (Zwarts and Dras, 2008).
In that work,only simple parser features are used for the pre-diction of fluency, but no actual syntactic prop-erties of the sentences.
But certainly, problemswith sentence fluency are expected to be mani-fested in syntax.
We would expect for examplethat syntactic tree features that capture commonparse configurations and that are used in discrim-inative parsing (Collins and Koo, 2005; Charniakand Johnson, 2005; Huang, 2008) should be use-ful for predicting sentence fluency as well.
In-deed, early work has demonstrated that syntac-tic features, and branching properties in particular,are helpful features for automatically distinguish-ing human translations from machine translations(Corston-Oliver et al, 2001).
The exploration ofbranching properties of human and machine trans-lations was motivated by the observations duringfailure analysis that MT system output tends tofavor right-branching structures over noun com-pounding.
Branching preference mismatch man-ifest themselves in the English output when trans-lating from languages whose branching propertiesare radically different from English.
Accuracyclose to 80% was achieved for distinguishing hu-man translations from machine translations.In our work we continue the investigation ofsentence level fluency based on features that cap-ture surface statistics of the syntactic structure ina sentence.
We revisit the task of distinguishingmachine translations from human translations, butalso further our understanding of fluency by pro-viding comprehensive analysis of the associationbetween fluency assessments of translations andsurface syntactic features.
We also demonstratethat based on the same class of features, it is possi-ble to distinguish fluent machine translations fromdisfluent machine translations.
Finally, we test themodels on human written text in order to verifyif the classifiers trained on data coming from ma-chine translation evaluations can be used for gen-eral predictions of fluency and readability.For our experiments we use the evaluationsof Chinese to English translations distributed byLDC (catalog number LDC2003T17), for whichboth machine and human translations are avail-able.
Machine translations have been assessedby evaluators for fluency on a five point scale (5:flawless English; 4: good English; 3: non-nativeEnglish; 2: disfluent English; 1: incomprehen-sible).
Assessments by different annotators wereaveraged to assign overall fluency assessment foreach machine-translated sentence.
For each seg-ment (sentence), there are four human and threemachine translations.In this setting we address four tasks with in-creasing difficulty:?
Distinguish human and machine translations.?
Distinguish fluent machine translations frompoor machine translations.?
Distinguish the better (in terms of fluency)translation among two translations of thesame input segment.?
Use the models trained on data from MTevaluations to predict potential fluency prob-lems of human-written texts (from the WallStreet Journal).Even for the last most challenging task resultsare promising, with prediction accuracy almost10% better than a random baseline.
For the othertasks accuracies are high, exceeding 80%.It is important to note that the purpose of ourstudy is not evaluation of machine translation perse.
Our goal is more general and the interest is infinding predictors of sentence fluency.
No generalcorpora exist with fluency assessments, so it seemsadvantageous to use the assessments done in thecontext of machine translation for preliminary in-vestigations of fluency.
Nevertheless, our findingsare also potentially beneficial for sentence-levelevaluation of machine translation.2 FeaturesPerceived sentence fluency is influenced by manyfactors.
The way the sentence fits in the con-text of surrounding sentences is one obvious factor(Barzilay and Lapata, 2008).
Another well-knownfactor is vocabulary use: the presence of uncom-mon difficult words are known to pose problemsto readers and to render text less readable (Collins-Thompson and Callan, 2004; Schwarm and Osten-dorf, 2005).
But these discourse- and vocabulary-level features measure properties at granularitiesdifferent from the sentence level.Syntactic sentence level features have not beeninvestigated as a stand-alone class, as has been140done for the other types of features.
This is whywe constrain our study to syntactic features alone,and do not discuss discourse and language modelfeatures that have been extensively studied in priorwork on coherence and readability.In our work, instead of looking at the syntac-tic structures present in the sentences, e.g.
thesyntactic rules used, we use surface statistics ofphrase length and types of modification.
The sen-tences were parsed with Charniak?s parser (Char-niak, 2000) in order to calculate these features.Sentence length is the number of words in a sen-tence.
Evaluation metrics such as BLEU (Papineniet al, 2002) have a built-in preference for shortertranslations.
In general one would expect thatshorter sentences are easier to read and thus areperceived as more fluent.
We added this featurein order to test directly the hypothesis for brevitypreference.Parse tree depth is considered to be a measureof sentence complexity.
Generally, longer sen-tences are syntactically more complex but whensentences are approximately the same length thelarger parse tree depth can be indicative of in-creased complexity that can slow processing andlead to lower perceived fluency of the sentence.Number of fragment tags in the sentence parseOut of the 2634 total sentences, only 165 con-tained a fragment tag in their parse, indicatingthe presence of ungrammaticality in the sentence.Fragments occur in headlines (e.g.
?Cheney will-ing to hold bilateral talks if Arafat observes U.S.cease-fire arrangement?)
but in machine transla-tion the presence of fragments can signal a moreserious problem.Phrase type proportion was computed forprepositional phrases (PP), noun phrases (NP)and verb phrases (VP).
The length in number ofwords of each phrase type was counted, then di-vided by the sentence length.
Embedded phraseswere also included in the calculation: for ex-ample a noun phrase (NP1 ... (NP2)) wouldcontribute length(NP1) + length(NP2) to thephrase length count.Average phrase length is the number of wordscomprising a given type of phrase, divided by thenumber of phrases of this type.
It was computedfor PP, NP, VP, ADJP, ADVP.
Two versions ofthe features were computed?one with embeddedphrases included in the calculation and one just forthe largest phrases of a given type.
Normalized av-erage phrase length is computed for PP, NP andVP and is equal to the average phrase length ofgiven type divided by the sentence length.
Thesewere computed only for the largest phrases.Phrase type rate was also computed for PPs,VPs and NPs and is equal to the number of phrasesof the given type that appeared in the sentence, di-vided by the sentence length.
For example, thesentence ?The boy caught a huge fish this morn-ing?
will have NP phrase number equal to 3/8 andVP phrase number equal to 1/8.Phrase length The number of words in a PP,NP, VP, without any normalization; it is computedonly for the largest phrases.
Normalized phraselength is the average phrase length (for VPs, NPs,PPs) divided by the sentence length.
This wascomputed both for longest phrase (where embed-ded phrases of the same type were counted onlyonce) and for each phrase regardless of embed-ding.Length of NPs/PPs contained in a VP The aver-age number of words that constitute an NP or PPwithin a verb phrase, divided by the length of theverb phrase.
Similarly, the length of PP in NP wascomputed.Head noun modifiers Noun phrases can be verycomplex, and the head noun can be modified in va-riety of ways?pre-modifiers, prepositional phrasemodifiers, apposition.
The length in words ofthese modifiers was calculated.
Each feature alsohad a variant in which the modifier length was di-vided by the sentence length.
Finally, two morefeatures on total modification were computed: onewas the sum of all modifier lengths, the other thesum of normalized modifier length.3 Feature analysisIn this section, we analyze the association of thefeatures that we described above and fluency.
Notethat the purpose of the analysis is not featureselection?all features will be used in the later ex-periments.
Rather, the analysis is performed in or-der to better understand which factors are predic-tive of good fluency.The distribution of fluency scores in the datasetis rather skewed, with the majority of the sen-tences rated as being of average fluency 3 as canbe seen in Table 1.Pearson?s correlation between the fluency rat-ings and features are shown in Table 2.
First of all,fluency and adequacy as given by MT evaluators141Fluency score The number of sentences1 ?
fluency < 2 71 ?
fluency < 2 2952 ?
fluency < 3 17893 ?
fluency < 4 5214 ?
fluency < 5 22Table 1: Distribution of fluency scores.are highly correlated (0.7).
This is surprisinglyhigh, given that separate fluency and adequacy as-sessments were elicited with the idea that theseare qualities of the translations that are indepen-dent of each other.
Fluency was judged directly bythe assessors, while adequacy was meant to assessthe content of the sentence compared to a humangold-standard.
Yet, the assessments of the twoaspects were often the same?readability/fluencyof the sentence is important for understanding thesentence.
Only after the assessor has understoodthe sentence can (s)he judge how it compares tothe human model.
One can conclude then that amodel of fluency/readability that will allow sys-tems to produce fluent text is key for developing asuccessful machine translation system.The next feature most strongly associated withfluency is sentence length.
Shorter sentences areeasier and perceived as more fluent than longerones, which is not surprising.
Note though that thecorrelation is actually rather weak.
It is only oneof various fluency factors and has to be accommo-dated alongside the possibly conflicting require-ments shown by the other features.
Still, lengthconsiderations reappear at sub-sentential (phrasal)levels as well.Noun phrase length for example has almost thesame correlation with fluency as sentence lengthdoes.
The longer the noun phrases, the less fluentthe sentence is.
Long noun phrases take longer tointerpret and reduce sentence fluency/readability.Consider the following example:?
[The dog] jumped over the fence and fetched the ball.?
[The big dog in the corner] fetched the ball.The long noun phrase is more difficult to read,especially in subject position.
Similarly the lengthof the verb phrases signal potential fluency prob-lems:?
Most of the US allies in Europe publicly [object to in-vading Iraq]V P .?
But this [is dealing against some recent remarks ofJapanese financial minister, Masajuro Shiokawa]V P .VP distance (the average number of words sep-arating two verb phrases) is also negatively corre-lated with sentence fluency.
In machine transla-tions there is the obvious problem that they mightnot include a verb for long stretches of text.
Buteven in human written text, the presence of moreverbs can make a difference in fluency (Bailin andGrafstein, 2001).
Consider the following two sen-tences:?
In his state of the Union address, Putin also talkedabout the national development plan for this fiscal yearand the domestic and foreign policies.?
Inside the courtyard of the television station, a recep-tion team of 25 people was formed to attend to thosewho came to make donations in person.The next strongest correlation is with unnormal-ized verb phrase length.
In fact in terms of correla-tions, in turned out that it was best not to normal-ize the phrase length features at all.
The normal-ized versions were also correlated with fluency,but the association was lower than for the directcount without normalization.Parse tree depth is the final feature correlatedwith fluency with correlation above 0.1.4 Experiments with machine translationdata4.1 Distinguishing human from machinetranslationsIn this section we use all the features discussed inSection 2 for several classification tasks.
Note thatwhile we discussed the high correlation betweenfluency and adequacy, we do not use adequacy inthe experiments that we report from here on.For all experiments we used four of the classi-fiers in Weka?decision tree (J48), logistic regres-sion, support vector machines (SMO), and multi-layer perceptron.
All results are for 10-fold crossvalidation.We extracted the 300 sentences with highest flu-ency scores, 300 sentences with lowest fluencyscores among machine translations and 300 ran-domly chosen human translations.
We then triedthe classification task of distinguishing human andmachine translations with different fluency quality(highest fluency scores vs. lowest fluency score).We expect that low fluency MT will be more easily142adequacy sentence length unnormalized NP length VP distance0.701(0.00) -0.132(0.00) -0.124(0.00) -0.116(0.00)unnormalized VP length Max Tree depth phrase length avr.
NP length (embedded)-0.109(0.00) -0.106(0.00) -0.105(0.00) -0.097(0.00)avr.
VP length (embedded) SBAR length avr.
largest NP length Unnormalized PP-0.094(0.00) -0.086(0.00) -0.084(0.00) -0.082(0.00)avr PP length (embedded) SBAR count PP length in VP Normalized PP1-0.070(0.00) -0.069(0.001) -0.066(0.001) 0.065(0.001)NP length in VP PP length normalized VP length PP length in NP-0.058(0.003) -0.054(0.006) 0.054(0.005) 0.053(0.006)Fragment avr.
ADJP length (embedded) avr.
largest VP length-0.049(0.011) -0.046(0.019) -0.038(0.052)Table 2: Pearson?s correlation coefficient between fluency and syntactic phrasing features.
P-values aregiven in parenthesis.worst 300 MT best 300 MT total MT (5920)SMO 86.00% 78.33% 82.68%Logistic reg.
77.16% 79.33% 82.68%MLP 78.00% 82% 86.99%Decision Tree(J48) 71.67 % 81.33% 86.11%Table 3: Accuracy for the task of distinguishing machine and human translations.distinguished from human translation in compari-son with machine translations rated as having highfluency.Results are shown in Table 3.
Overall thebest classifier is the multi-layer perceptron.
Onthe task using all available data of machine andhuman translations, the classification accuracy is86.99%.
We expected that distinguishing the ma-chine translations from the human ones will beharder when the best translations are used, com-pared to the worse translations, but this expecta-tion is fulfilled only for the support vector machineclassifier.The results in Table 3 give convincing evi-dence that the surface structural statistics can dis-tinguish very well between fluent and non-fluentsentences when the examples come from humanand machine-produced text respectively.
If this isthe case, will it be possible to distinguish betweengood and bad machine translations as well?
In or-der to answer this question, we ran one more bi-nary classification task.
The two classes were the300 machine translations with highest and lowestfluency respectively.
The results are not as good asthose for distinguishing machine and human trans-lation, but still significantly outperform a randombaseline.
All classifiers performed similarly on thetask, and achieved accuracy close to 61%.4.2 Pairwise fluency comparisonsWe also considered the possibility of pairwisecomparisons for fluency: given two sentences,can we distinguish which is the one scored morehighly for fluency.
For every two sentences, thefeature for the pair is the difference of features ofthe individual sentences.There are two ways this task can be set up.
First,we can use all assessed translations and make pair-ings for every two sentences with different fluencyassessment.
In this setting, the question being ad-dressed is Can sentences with differing fluency bedistinguished?, without regard to the sources ofthe sentence.
The harder question is Can a morefluent translation be distinguished from a less flu-ent translation of the same sentence?The results from these experiments can be seenin Table 4.
When any two sentences with differ-ent fluency assessments are paired, the predictionaccuracy is very high: 91.34% for the multi-layerperceptron classifier.
In fact all classifiers have ac-curacy higher than 80% for this task.
The surfacestatistics of syntactic form are powerful enough todistinguishing sentences of varying fluency.The task of pairwise comparison for translationsof the same input is more difficult: doing well onthis task would be equivalent to having a reliablemeasure for ranking different possible translationvariants.In fact, the problem is much more difficult as143Task J48 Logistic Regression SMO MLPAny pair 89.73% 82.35% 82.38% 91.34%Same Sentence 67.11% 70.91% 71.23% 69.18%Table 4: Accuracy for pairwise fluency comparison.
?Same sentence?
are comparisons constrainedbetween different translations of the same sentences, ?any pair?
contains comparisons of sentences withdifferent fluency over the entire data set.can be seen in the second row of Table 4.
Lo-gistic regression, support vector machines andmulti-layer perceptron perform similarly, withsupport vector machine giving the best accuracyof 71.23%.
This number is impressively high, andsignificantly higher than baseline performance.The results are about 20% lower than for predic-tion of a more fluent sentence when the task is notconstrained to translation of the same sentence.4.3 Feature analysis: differences among tasksIn the previous sections we presented three varia-tions involving fluency predictions based on syn-tactic phrasing features: distinguishing humanfrom machine translations, distinguishing goodmachine translations from bad machine transla-tions, and pairwise ranking of sentences with dif-ferent fluency.
The results differ considerably andit is interesting to know whether the same kindof features are useful in making the three distinc-tions.In Table 5 we show the five features with largestweight in the support vector machine model foreach task.
In many cases, certain features appearto be important only for particular tasks.
For ex-ample the number of prepositional phrases is animportant feature only for ranking different ver-sions of the same sentence but is not important forother distinctions.
The number of appositions ishelpful in distinguishing human translations frommachine translations, but is not that useful in theother tasks.
So the predictive power of the featuresis very directly related to the variant of fluency dis-tinctions one is interested in making.5 Applications to human written text5.1 Identifying hard-to-read sentences inWall Street Journal textsThe goal we set out in the beginning of this pa-per was to derive a predictive model of sentencefluency from data coming from MT evaluations.In the previous sections, we demonstrated thatindeed structural features can enable us to per-form this task very accurately in the context ofmachine translation.
But will the models conve-niently trained on data from MT evaluation be atall capable to identify sentences in human-writtentext that are not fluent and are difficult to under-stand?To answer this question, we performed an ad-ditional experiment on 30 Wall Street Journal ar-ticles from the Penn Treebank that were previ-ously used in experiments for assessing overalltext quality (Pitler and Nenkova, 2008).
The arti-cles were chosen at random and comprised a to-tal of 290 sentences.
One human assessor wasasked to read each sentence and mark the ones thatseemed disfluent because they were hard to com-prehend.
These were sentences that needed to beread more than once in order to fully understandthe information conveyed in them.
There were 52such sentences.
The assessments served as a gold-standard against which the predictions of the flu-ency models were compared.Two models trained on machine translation datawere used to predict the status of each sentence inthe WSJ articles.
One of the models was that fordistinguishing human translations from machinetranslations (human vs machine MT), the otherwas the model for distinguishing the 300 best fromthe 300 worst machine translations (good vs badMT).
The classifiers used were decision trees forhuman vs machine distinction and support vectormachines for good vs bad MT.
For the first modelsentences predicted to belong to the ?human trans-lation?
class are considered fluent; for the secondmodel fluent sentences are the ones predicted to bein the ?best MT?
class.The results are shown in Table 6.
The twomodels vastly differ in performance.
The modelfor distinguishing machine translations from hu-man translations is the better one, with accuracyof 57%.
For both, prediction accuracy is muchlower than when tested on data from MT evalu-ations.
These findings indicate that building a new144MT vs HT good MT vs Bad MT Ranking Same sentence Rankingunnormalized PP SBAR count avr.
NP lengt normalized NP lengthPP length in VP Unnormalized VP length normalized PP length PP countavr.
NP length post attribute length NP count normalized NP length# apposition VP count normalized NP length max tree depthSBAR length sentence length normalized VP length avr.
phrase lengthTable 5: The five features with highest weights in the support vector machine model for the differenttasks.Model Acc P Rhuman vs machine trans.
57% 0.79 0.58good MT vs bad MT 44% 0.57 0.44Table 6: Accuracy, precision and recall (for fluentclass) for each model when test on WSJ sentences.The gold-standard is assessment by a single readerof the text.corpus for the finer fluency distinctions present inhuman-written text is likely to be more beneficialthan trying to leverage data from existing MT eval-uations.Below, we show several example sentences onwhich the assessor and the model for distinguish-ing human and machine translations (dis)agreed.Model and assessor agree that sentence is prob-lematic:(1.1) The Soviet legislature approved a 1990 budget yes-terday that halves its huge deficit with cuts in defense spend-ing and capital outlays while striving to improve supplies tofrustrated consumers.
(1.2) Officials proposed a cut in the defense budget thisyear to 70.9 billion rubles (US$114.3 billion) from 77.3 bil-lion rubles (US$125 billion) as well as large cuts in outlaysfor new factories and equipment.
(1.3) Rather, the two closely linked exchanges have beendrifting apart for some years, with a nearly five-year-oldmoratorium on new dual listings, separate and different list-ing requirements, differing trading and settlement guidelinesand diverging national-policy aims.The model predicts the sentence is good, but theassessor finds it problematic:(2.1) Moody?s Investors Service Inc. said it lowered theratings of some $145 million of Pinnacle debt because of?accelerating deficiency in liquidity,?
which it said was ev-idenced by Pinnacle?s elimination of dividend payments.
(2.2) Sales were higher in all of the company?s businesscategories, with the biggest growth coming in sales of food-stuffs such as margarine, coffee and frozen food, which rose6.3%.
(2.3) Ajinomoto predicted sales in the current fiscal yearending next March 31 of 480 billion yen, compared with460.05 billion yen in fiscal 1989.The model predicts the sentences are bad, butthe assessor considered them fluent:(3.1) The sense grows that modern public bureaucraciessimply don?t perform their assigned functions well.
(3.2) Amstrad PLC, a British maker of computer hardwareand communications equipment, posted a 52% plunge in pre-tax profit for the latest year.
(3.3) At current allocations, that means EPA will be spend-ing $300 billion on itself.5.2 Correlation with overall text qualityIn our final experiment we focus on the relation-ship between sentence fluency and overall textquality.
We would expect that the presence of dis-fluent sentences in text will make it appear lesswell written.
Five annotators had previously as-sess the overall text quality of each article on ascale from 1 to 5 (Pitler and Nenkova, 2008).
Theaverage of the assessments was taken as a singlenumber describing the article.
The correlation be-tween this number and the percentage of fluentsentences in the article according to the differentmodels is shown in Table 7.The correlation between the percentage of flu-ent sentences in the article as given by the humanassessor and the overall text quality is rather low,0.127.
The positive correlation would suggest thatthe more hard to read sentence appear in a text,the higher the text would be rated overall, whichis surprising.
The predictions from the model fordistinguishing good and bad machine translationsvery close to zero, but negative which correspondsbetter to the intuitive relationship between the two.Note that none of the correlations are actuallysignificant for the small dataset of 30 points.6 ConclusionWe presented a study of sentence fluency based ondata from machine translation evaluations.
Thesedata allow for two types of comparisons: human(fluent) text and (not so good) machine-generated145Fluency given by Correlationhuman 0.127human vs machine trans.
model -0.055good MT vs bad MT model 0.076Table 7: Correlations between text quality assess-ment of the articles and the percentage of fluentsentences according to different models.text, and levels of fluency in the automatically pro-duced text.
The distinctions were possible evenwhen based solely on features describing syntac-tic phrasing in the sentences.Correlation analysis reveals that the structuralfeatures are significant but weakly correlated withfluency.
Interestingly, the features correlated withfluency levels in machine-produced text are not thesame as those that distinguish between human andmachine translations.
Such results raise the needfor caution when using assessments for machineproduced text to build a general model of fluency.The captured phenomena in this case might bedifferent than these from comparing human textswith differing fluency.
For future research it willbe beneficial to build a dedicated corpus in whichhuman-produced sentences are assessed for flu-ency.Our experiments show that basic fluency dis-tinctions can be made with high accuracy.
Ma-chine translations can be distinguished from hu-man translations with accuracy of 87%; machinetranslations with low fluency can be distinguishedfrom machine translations with high fluency withaccuracy of 61%.
In pairwise comparison of sen-tences with different fluency, accuracy of predict-ing which of the two is better is 90%.
Results arenot as high but still promising for comparisons influency of translations of the same text.
The pre-diction becomes better when the texts being com-pared exhibit larger difference in fluency quality.Admittedly, our pilot experiments with humanassessment of text quality and sentence level flu-ency are small, so no big generalizations can bemade.
Still, they allow some useful observationsthat can guide future work.
They do show that forfurther research in automatic recognition of flu-ency, new annotated corpora developed speciallyfor the task will be necessary.
They also givesome evidence that sentence-level fluency is onlyweakly correlated with overall text quality.
Dis-course apects and language model features thathave been extensively studied in prior work are in-deed much more indicative of overall text quality(Pitler and Nenkova, 2008).
We leave direct com-parison for future work.ReferencesA.
Bailin and A. Grafstein.
2001.
The linguistic as-sumptions underlying readability formulae: a cri-tique.
Language and Communication, 21:285?301.S.
Bangalore and O. Rambow.
2000.
Exploiting aprobabilistic hierarchical model for generation.
InCOLING, pages 42?48.S.
Bangalore, O. Rambow, and S. Whittaker.
2000.Evaluation metrics for generation.
In INLG?00:Proceedings of the first international conference onNatural language generation, pages 1?8.M.
Banko, V. Mittal, and M. Witbrock.
2000.
Head-line generation based on statistical translation.
InProceedings of the 38th Annual Meeting of the As-sociation for Co mputational Linguistics.R.
Barzilay and M. Lapata.
2008.
Modeling local co-herence: An entity-based approach.
ComputationalLinguistics, 34(1):1?34.R.
Barzilay and K. McKeown.
2005.
Sentence fusionfor multidocument news summarization.
Computa-tional Linguistics, 31(3).E.
Charniak and M. Johnson.
2005.
Coarse-to-finen-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meetingof the Association for Computational Linguistics(ACL?05), pages 173?180.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In NAACL-2000.J.
Clarke and M. Lapata.
2006.
Models for sen-tence compression: A comparison across domains,training requirements and evaluation measures.
InACL:COLING?06, pages 377?384.M.
Collins and T. Koo.
2005.
Discriminative rerank-ing for natural language parsing.
Comput.
Linguist.,31(1):25?70.K.
Collins-Thompson and J. Callan.
2004.
A languagemodeling approach to predicting reading difficulty.In Proceedings of HLT/NAACL?04.S.
Corston-Oliver, M. Gamon, and C. Brockett.
2001.A machine learning approach to the automatic eval-uation of machine translation.
In Proceedings of39th Annual Meeting of the Association for Compu-tational Linguistics, pages 148?155.H.
Daume?
III and D. Marcu.
2004.
Generic sentencefusion is an ill-defined summarization task.
In Pro-ceedings of the Text Summarization Branches OutWorkshop at ACL.146M.
Galley and K. McKeown.
2007.
Lexicalizedmarkov grammars for sentence compression.
InProceedings of Human Language Technologies: TheAnnual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL-HLT).Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594.H.
Jing.
2000.
Sentence simplification in automatictext summarization.
In Proceedings of the 6th Ap-plied NLP Conference, ANLP?2000.N.
Karamanis, M. Poesio, C. Mellish, and J. Oberlan-der.
(to appear).
Evaluating centering for infor-mation ordering using corpora.
Computational Lin-guistics.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction: A probabilistic approachto sentence compression.
Artificial Intelligence,139(1).I.
Langkilde and K. Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InCOLING-ACL, pages 704?710.Mirella Lapata.
2003.
Probabilistic text structuring:Experiments with sentence ordering.
In Proceed-ings of ACL?03.R.
McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In EACL?06.A.
Mutton, M. Dras, S. Wan, and R. Dale.
2007.
Gleu:Automatic evaluation of sentence-level fluency.
InACL?07, pages 344?351.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: A method for automatic evaluation of ma-chine translation.
In Proceedings of ACL.E.
Pitler and A. Nenkova.
2008.
Revisiting readabil-ity: A unified framework for predicting text quality.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages186?195.S.
Schwarm and M. Ostendorf.
2005.
Reading levelassessment using support vector machines and sta-tistical language models.
In Proceedings of ACL?05,pages 523?530.A.
Siddharthan.
2003.
Syntactic simplification andText Cohesion.
Ph.D. thesis, University of Cam-bridge, UK.R.
Soricut and D. Marcu.
2007.
Abstractive head-line generation using widl-expressions.
Inf.
Process.Manage., 43(6):1536?1548.J.
Turner and E. Charniak.
2005.
Supervised and un-supervised learning for sentence compression.
InACL?05.S.
Wan, R. Dale, and M. Dras.
2005.
Searchingfor grammaticality: Propagating dependencies in theviterbi algorithm.
In Proceedings of the Tenth Eu-ropean Workshop on Natural Language Generation(ENLG-05).D.
Zajic, B. Dorr, J. Lin, and R. Schwartz.
2007.Multi-candidate reduction: Sentence compression asa tool for document summarization tasks.
Inf.
Pro-cess.
Manage., 43(6):1549?1570.S.
Zwarts and M. Dras.
2008.
Choosing the righttranslation: A syntactically informed classificationapproach.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 1153?1160.147
