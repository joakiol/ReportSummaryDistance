Proceedings of the 6th Workshop on Statistical Machine Translation, pages 393?398,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsExperiments with word alignment, normalization and clause reordering forSMT between English and GermanMaria Holmqvist, Sara Stymne and Lars AhrenbergDepartment of Computer and Information ScienceLinko?ping University, Swedenfirstname.lastname@liu.seAbstractThis paper presents the LIU system for theWMT 2011 shared task for translation be-tween German and English.
For English?German we attempted to improve the trans-lation tables with a combination of standardstatistical word alignments and phrase-basedword alignments.
For German?English trans-lation we tried to make the German text moresimilar to the English text by normalizing Ger-man morphology and performing rule-basedclause reordering of the German text.
This re-sulted in small improvements for both transla-tion directions.1 IntroductionIn this paper we present the LIU system for theWMT11 shared task, for translation between En-glish and German in both directions.
We added anumber of features that address problems for trans-lation between German and English such as word or-der differences, incorrect alignment of certain wordssuch as verbs, and the morphological complexityof German compared to English, as well as dealingwith previously unseen words.In both translation directions our systems in-clude compound processing, morphological se-quence models, and a hierarchical reordering model.For German?English translation we also added mor-phological normalization, source side reordering,and processing of out-of-vocabulary words (OOVs).For English?German translation, we extracted wordalignments with a supervised method and combinedthese alignments with Giza++ alignments in variousways to improve the phrase table.
We experimentedwith different ways of combining the two alignmentssuch as using heuristic symmetrization and interpo-lating phrase tables.Results are reported on three metrics, BLEU (Pa-pineni et al, 2002), NIST (Doddington, 2002) andMeteor ranking scores (Agarwal and Lavie, 2008)based on truecased output.2 Baseline SystemThis years improvements were added to the LIUbaseline system (Stymne et al, 2010).
Our base-line is a factored phrase based SMT system that usesthe Moses toolkit (Koehn et al, 2007) for transla-tion model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stol-cke, 2002) an KenLM (Heafield, 2011) for languagemodelling and minimum error rate training (Och,2003) to tune model feature weights.
In addition,the LIU baseline contains:?
Compound processing, including compoundsplitting and for translation into German alsocompound merging?
Part-of-speech and morphological sequencemodelsAll models were trained on truecased data.
Trans-lation and reordering models were trained using thebilingual Europarl and News Commentary corporathat were concatenated before training.
We createdtwo language models.
The first model is a 5-grammodel that we created by interpolating two language393models from bilingual News Commentary and Eu-roparl with more weight on the News Commentarymodel.
The second model is a 4-gram model trainedon monolingual News only.
All models were cre-ated using entropy-based pruning with 10?8 as thethreshold.Due to time constraints, all tuning and evaluationwere performed on half of the provided shared taskdata.
Systems were tuned on 1262 sentences fromnewstest2009 and all results reported in Tables 1 and2 are based on a devtest set of 1244 sentences fromnewstest2010.2.1 Sequence models with part-of-speech andmorphologyTo improve target word order and agreement in thetranslation output, we added an extra output factor inour translation models consisting of tags with POSand morphological features.
For English we usedtags that were obtained by enriching POS tags fromTreeTagger (Schmid, 1994) with additional morpho-logical features such as number for determiners.
ForGerman, the POS and morphological tags were ob-tained from RFTagger (Schmid and Laws, 2008)which provides morphological information such ascase, number and gender for nouns and tense forverbs.
We trained two sequence models for eachsystem over this output factor and added them asfeatures in our baseline system.
The first sequencemodel is a 7-gram model interpolated from modelsof bilingual Europarl and News Commentary.
Thesecond model is a 6-gram model trained on mono-lingual News only.2.2 Compound processingIn both translation directions we split compounds,using a modified version of the corpus-based split-ting method of Koehn and Knight (2003).
We splitnouns, verb, and adjective compounds into knownparts that were content words or cardinal numbers,based on the arithmetic mean of the frequency ofthe parts in the training corpus.
We allowed 10 com-mon letter changes (Langer, 1998) and hyphens atsplit points.
Compound parts were kept in their sur-face form and compound modifiers received a part-of-speech tag based on that of the tag of the full com-pound.For translation into German, compounds weremerged using the POS-merging strategy of Stymne(2009).
A compound part in the translation output,identified by the special part-of-speech tags, wasmerged with the next word if that word had a match-ing part-of-speech tag.
If the compound part wasfollowed by the conjunction und (and), we added ahyphen to the part, to account for coordinated com-pounds.2.3 Hierarchical reorderingIn our baseline system we experimented with twolexicalized reordering models.
The standard modelin Moses (Koehn et al, 2005), and the hierarchi-cal model of Galley and Manning (2008).
In bothmodels the placement of a phrase is compared tothat of the previous and/or next phrase.
In the stan-dard model up to three reorderings are distinguished,monotone, swap, and discontinuous.
In the hier-archical model the discontinuous class can be fur-ther subdivided into two classes, left and right dis-continuous.
The hierarchical model further differsfrom the standard model in that it compares the or-der of the phrase with the next or previous block ofphrases, not only with the next or previous singlephrase.We investigated one configuration of eachmodel.
For the standard model we used the msd-bidirectional-fe setting, which uses three orienta-tions, is conditioned on both the source and targetlanguage, and considers both the previous and nextphrase.
For the hierarchical model we used all fourorientations, and again it is conditioned on both thesource and target language, and considers both theprevious and next phrase.The result of replacing the standard reorderingmodel with an hierarchical model is shown in Table1 and 2.
For translation into German adding the hi-erarchical model led to small improvements as mea-sured by NIST and Meteor.
For translation in theother direction, the differences on automatic metricswere very small.
Still, we decided to use the hierar-chical model in all our systems.3 German?EnglishFor translation from German into English we fo-cused on making the German source text more sim-ilar to English by removing redundant morphology394and changing word order before training translationmodels.3.1 NormalizationWe performed normalization of German words to re-move distinctions that do not exist in English, suchas case distinctions on nouns.
This strategy is sim-ilar to that of El-Kahlout and Yvon (2010), but weused a slightly different set of transformations, thatwe thought better mirrored the English structure.For morphological tags we used RFTagger and forlemmas we used TreeTagger.
The morphologicaltransformations we performed were the following:?
Nouns:?
Replace with lemma+s if plural number?
Replace with lemma otherwise?
Verbs:?
Replace with lemma if present tense, notthird person singular?
Replace with lemma+p if past tense?
Adjectives:?
Replace with lemma+c if comparative?
Replace with lemma+sup if superlative?
Replace with lemma otherwise?
Articles:?
Definite articles:?
Replace with des if genitive?
Replace with der otherwise?
Indefinite articles:?
Replace with eines if genitive?
Replace with ein otherwise?
Pronouns:?
Replace with RELPRO if relative?
Replace with lemma if indefinite, interrog-ative, or possessive pronouns?
Add +g to all pronouns which are geni-tive, unless they are possessiveFor all word types that are not mentioned in thelist, surface forms were kept.BLEU NIST MeteorBaseline 21.01 6.2742 41.32+hier reo 20.94 6.2800 41.24+normalization 20.85 6.2370 41.04+source reordering 21.06 6.3082 41.40+ OOV proc.
21.22 6.3692 41.51Table 1: German?English translation results.
Results arecumulative.We also performed those tokenization andspelling normalizations suggested by El-Kahloutand Yvon (2010), that we judged could safely bedone for translation from German without collect-ing corpus statistics.
We split words with numbersand letters, such as 40-ja?hrigen or 40ja?hrigen (40year-old), unless the suffix indicates that it is a ordi-nal, such as 70sten (70th).
We also did some spellingnormalization by exchanging ?
with ss and replacingtripled consonants with doubled consonants.
Thesechanges would have been harmful for translationinto German, since they change the language into anormalized variant, but for translation from Germanwe considered them safe.3.2 Source side reorderingTo make the word order of German input sen-tences more English-like a version of the rules of(Collins et al, 2005) were partially implemented us-ing tagged output from the RFTagger.
Basically,beginnings of subordinate clauses, their subjects (ifpresent) and final verb clusters were identified basedon tag sequences, and the clusters were moved tothe beginning of the clause, and reordered so thatthe finite verb ended up in the second clause posi-tion.
Also, some common adverbs were moved withthe verb cluster and placed between finite and non-finite verbs.
After testing, we decided to apply theserules only to subordinate clauses at the end of sen-tences, since these were the only ones that could beidentified with good precision.
Still, some 750,000clauses were reordered.3.3 OOV ProcessingWe also added limited processing of OOVs.
In a pre-processing step we replaced unknown words withknown cased variants if available, removed markupfrom normalized words if that resulted in an un-395known token, and split hyphened words.
We alsosplit suspected names in cases where we had a pat-tern with a single upper-case letter in the middle of aword, such as ConocoPhillips into Conoco Phillips.In a post-processing step we changed the numberformatting of unknown numbers by changing dec-imal points and thousand separators, to agree withEnglish orthography.
This processing only affectsa small number of words, and cannot be expectedto make a large impact on the final results.
Outof 884 OOVs in the devtest, 39 had known casedoptions, 126 hyphened words were split, 147 caseshad markup from the normalization removed, and 13suspected names were split.3.4 ResultsThe results of these experiments can be seen in Table1 where each new addition is added to the previoussystem.
When we compare the new additions withthe baseline with hierarchical reordering, we see thatwhile the normalization did not seem to have a posi-tive effect on any metric, both source reordering andOOV processing led to small increases on all scores.4 English?GermanFor translation from English into German we at-tempted to improve the quality of the phrase table byadding new word alignments to the standard Giza++alignments.4.1 Phrase-based word alignmentWe experimented with different ways of com-bining word alignments from Giza++ with align-ments created using phrase-based word alignment(PAL) which previously has been shown to improvealignment quality for English?Swedish (Holmqvist,2010).
The idea of phrase-based word alignment isto use word and part-of-speech sequence patternsfrom manual word alignments to align new texts.First, parallel phrases containing a source segment,a target segment and links between source and targetwords are extracted from word aligned texts (Figure1).
In the second step, these phrases are matchedagainst new parallel text and if a matching phraseis found, word links from the phrase are added tothe corresponding words in the new text.
In orderto increase the number of matching phrases and im-prove word alignment recall, words in the parallelEn: a typical exampleDe: ein typisches BeispielLinks: 0-0 1-1 2-2En: a JJ exampleDe: ein ADJA BeispielLinks: 0-0 1-1 2-2En: DT JJ NNDe: ART ADJA NLinks: 0-0 1-1 2-2Figure 1: Examples of parallel phrases used in wordalignment.BLEU NIST MeteorBaseline 16.16 6.2742 50.89+hier reo 16.06 6.2800 51.25+pal-gdfa 16.14 5.6527 51.10+pal-dual 15.71 5.5735 50.43+pal-inter 15.92 5.6230 50.73Table 2: English?German translation results, resultsare cumulative except for the three alternative PAL-configurations.segments were replaced by POS/morphological tagsfrom RFTagger.Alignment patterns were extracted from 1000 sen-tences in the manually word aligned sample ofEnglish?German Europarl texts from Pado and Lap-ata (2006).
All parallel phrases were extracted fromthe word aligned texts, as when extracting a trans-lation model.
Parallel phrases that contain at least3 words were generalized with POS tags to formword/POS patterns for alignment.
A subset of thesepatterns, with high alignment precision (> 0.80) onthe 1000 sentences, were used to align the entiretraining corpus.We combined the new word alignments withthe Giza++ alignments in two ways.
In the firstmethod, we used a symmetrization heuristic similarto grow-diag-final-and to combine three word align-ments into one, the phrase-based alignment and twoGiza++ alignments in different directions.
In thesecond method we extracted a separate phrase ta-ble from the sparser phrase-based alignment usinga constrained method of phrase extraction that lim-ited the number of unaligned words in each phrasepair.
The reason for constraining the phrase table396extraction was that the standard extraction methoddoes not work well for the sparse word alignmentsthat PAL produces, but we think it could still beuseful for extracting highly reliable phrases.
Aftersome experimentation we decided to allow an unlim-ited number of internal unaligned words, that is un-aligned words that are surrounded by aligned words,but limit the number of external unaligned words,i.e., unaligned words at the beginning or end of thephrase, to either one each in the source and targetphrase, or to zero.We used two ways to include the sparse phrase-table into the translation process:?
Have two separate phrase-tables, the sparse ta-ble, and the standard GIZA++ based phrase-table, and use Moses?
dual decoding paths.?
Interpolate the sparse phrase-table with thestandard phrase-table, using the mixture modelformulation of Ueffing et al (2007), with equalweights, in order to boost the probabilities ofhighly reliable phrases.4.2 ResultsWe evaluated our systems on devtest data and foundthat the added phrase-based alignments did not pro-duce large differences in translation quality com-pared to the baseline system with hierarchical re-ordering as shown in Table 2.
The system createdwith a heuristic combination of PAL and Giza++(pal-gdfa) had a small increase in BLEU, but no im-provement on the other metrics.
Systems using aphrase table extracted from the sparse alignmentsdid not produce better results than baseline.
The sys-tem using dual decoding paths (pal-dual) producedworse results than the system using an interpolatedphrase table (pal-inter).5 Submitted systemsThe LIU system participated in German?Englishand English?German translation in the WMT 2011shared task.
The new additions were a combina-tion of unsupervised and supervised word align-ments, spelling normalization, clause reordering andOOV processing.
Our submitted systems containall additions described in this paper.
For English-German we used the best performing method ofBLEUSystem Devtest Testen-debaseline +hier 16.1 14.5submitted 16.1 14.8de-enbaseline +hier 20.9 19.3submitted 21.2 19.9Table 3: Summary of devtest results and shared task testresults for submitted systems and LIU baseline with hier-archical reordering.word alignment combination which was the methodthat uses heuristic combination similar to grow-diag-final-and.The results of our submitted systems are shownin Table 3 where we compare them to the LIU base-line system with hierarchical reordering models.
Wereport modest improvements on the devtest set forboth translation directions.
We also found small im-provements of our submitted systems in the officialshared task evaluation on the test set newstest2011.ReferencesAbhaya Agarwal and Alon Lavie.
2008.
Meteor,M-BLEU and M-TER: Evaluation metrics for high-correlation with human rankings of machine transla-tion output.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 115?118,Columbus, Ohio.Michael Collins, Philipp Koehn, and Ivona Kucerova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the ACL, pages 531?540, Ann Arbor, Michigan.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurencestatistics.
In Proceedings of the Second InternationalConference on Human Language Technology, pages228?231, San Diego, California.I?lknur Durgar El-Kahlout and Franc?ois Yvon.
2010.
Thepay-offs of preprocessing for German-English statisti-cal machine translation.
In Proceedings of the Inter-national Workshop on Spoken Language Translation,pages 251?258, Paris, France.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages848?856, Honolulu, Hawaii.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of the Sixth397Workshop on Statistical Machine Translation, Edin-burgh, UK.Maria Holmqvist.
2010.
Heuristic word alignmentwith parallel phrases.
In Proceedings of the SeventhConference on International Language Resources andEvaluation, pages 744-748, Valletta, Malta.Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In Proceedings of theTenth Conference of EACL, pages 187?193, Budapest,Hungary.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InProceedings of the International Workshop on SpokenLanguage Translation, Pittsburgh, Pennsylvania.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL, Demon-stration Session, 177?180, Prague, Czech Republic.Stefan Langer.
1998.
Zur Morphologie und Semantikvon Nominalkomposita.
In Tagungsband der 4.
Kon-ferenz zur Verarbeitung natu?rlicher Sprache, pages83?97, Bonn, Germany.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the ACL, pages 160?167, Sap-poro, Japan.Sebastian Pado and Mirella Lapata.
2006.
Optimal con-stituent alignment with edge covers for semantic pro-jection.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the ACL, pages 1161?1168, Sydney, Aus-tralia.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of ACL, pages 311?318,Philadelphia, Pennsylvania.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees and anapplication to fine-grained POS tagging.
In Proceed-ings of the 20th International Conference on Compu-tational Linguistics, pages 777?784, Manchester, UK.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In International Conferenceon New Methods in Language Processing, pages 44?49, Manchester, UK.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the Seventh Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, Colorado.Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.2010.
Vs and OOVs: Two problems for translationbetween German and English.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 189?194, Uppsala,Sweden.Sara Stymne.
2009.
A comparison of merging strategiesfor translation of German compounds.
In Proceedingsof the EACL Student Research Workshop, pages 61?69, Athens, Greece.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Semi-supervised model adaptation for statisticalmachine translation.
Machine Translation, 21(2):77?94.398
