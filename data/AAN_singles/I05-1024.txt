Automatic Term Extraction Based on Perplexityof Compound WordsMinoru Yoshida1,2 and Hiroshi Nakagawa1,21 Information Technology Center, University of Tokyo,7-3-1 Hongo, Bunkyo-ku, Tokyo 113-00332 JST CREST, Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012mino@r.dl.itc.u-tokyo.ac.jp, nakagawa@dl.itc.u-tokyo.ac.jpAbstract.
Many methods of term extraction have been discussed interms of their accuracy on huge corpora.
However, when we try to applyvarious methods that derive from frequency to a small corpus, we maynot be able to achieve sufficient accuracy because of the shortage ofstatistical information on frequency.
This paper reports a new way ofextracting terms that is tuned for a very small corpus.
It focuses on thestructure of compound terms and calculates perplexity on the term unit?sleft-side and right-side.
The results of our experiments revealed that theaccuracy with the proposed method was not that advantageous.
However,experimentation with the method combining perplexity and frequencyinformation obtained the highest average-precision in comparison withother methods.1 IntroductionTerm extraction, which is the task of extracting terminology (or technical terms)from a set of documents, is one of major topics in natural language processing.
Ithas a wide variety of applications including book indexing, dictionary generation,and keyword extraction for information retrieval systems.Most automatic term extraction systems make a sorted list of candidate termsextracted from a given corpus according to the ?importance?
scores of the terms,so they require scores of ?importance?
for the terms.
Existing scores includeTF-IDF, C-Value [1], and FLR [9].
In this paper, we propose a new methodthat involves revising the definition of the FLR method in a more sophisticatedway.
One of the advantages of the FLR method is its size-robustness, i.e, it canbe applied to small corpus with less significant drop in performance than otherstandard methods like TF and IDF, because it is defined using more fine-grainedfeatures called term units.
Our new method, called FPP, inherit this propertywhile exhibiting better performance than FLR.At the same time, we also propose a new scheme for evaluating term ex-traction systems.
Our idea is to use summaries1 of articles as a gold standard.This strategy is based on the assumption that summaries of documents can1 In more detail, an article revised for display on mobile phones.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
269?279, 2005.c?
Springer-Verlag Berlin Heidelberg 2005270 M. Yoshida and H. Nakagawaserve as collections of important terms because, in writing summaries, peo-ple may make an original document shorter by dropping unnecessary partsof original documents, while retaining essential fragments.
Thus, we regard aterm in an original document to be important if it also appears in thesummary.2 Term ExtractionTerm extraction is the task of extracting important terms from a given corpus.Typically, term extraction systems first extract term candidates, which are usu-ally the noun phrases detected by handcrafted POS sequence patterns, from thecorpus.
After that, term candidates are sorted according to some importancescore.
Important terms, (i.e., terms that appear in the summary, in our problemsetting,) are desired to be ranked higher than others.
In this paper we focuson the second step, i.e., term candidate sorting by importance scores.
We pro-pose a new score of term importance by modifying an existing one in a moresophisticated manner.In the remainder of this paper, a term candidate is represented by W = w1w2?
?
?
wn where wi represents a term unit contained in W , and n is the number of termunits contained in W .
Here, a term unit is the basic element comprising term can-didates that is not further decomporsable without destruction of meaning.
Termunits are used to calculate of the LR score that is explained in the next section.3 Related WorkMany methods of term scoring have been proposed in the literature [7] [3] [4].Methods that use corpus statistics have especially emerged over the past decadedue to the increasing number of machine-readable documents such as news arti-cles and WWW documents.
These methods can be mainly categorized into thefollowing three types according to what types of features are used to calculatethe scores.?
Measurement by frequencies?
Measurement by internal structures of term candidates?
Combination of the above3.1 Score by Frequency: TFFrequency is one of the most basic features of term extraction.
Usually, a termthat appears frequently is assumed to be important.
We introduce a score of thistype: tf(W ).tf(W ) represents the TF(Term Frequency) of W .
It is defined as the numberof occurrences of W in all documents.
Note that tf(W ) is the result of thebrute force counting of W occurrences.
This method, for example, counts theAutomatic Term Extraction Based on Perplexity of Compound Words 271term natural even if it is merely part of another phrase such as natural languageprocessing.23.2 Score by Internal Structures in Term Candidates: LRAn LR method [9] is based on the intuition that some words are used as termunits more frequently than others, and a phrase that contains such ?good?
termunits is likely to be important.
The left score l(wi) of each term unit wi of a targetterm is defined as the number (or the number of types) of term units connectedto the left of wi (i.e., appearing just in the left of wi in term candidates), and theright score r(wi) is defined in the same manner.3 An LR score lr(wi) is definedas the geometric mean of left and right scores:lr(wi) =?l(wi)r(wi)The total LR score of W is defined as a geometric mean of the scores of termunits as:LR(W ) = (lr(w1)lr(w2) ?
?
?
lr(wn))1n .An example of LR score calculation is given in the next section.3.3 Mixed MeasuresC-Value.
C-Value[1] is defined by the following two expressions:t(W ): frequency of terms that contain W ,c(W ): number of types of terms that contain W .Note that t(W ) does not count W itself.
Intuitively, t(W ) is the degree of beingpart of another term, and c(W ) is the degree of being part of various types ofterms.C-Value is defined by using these two expressions in the following way.c-val(W ) = (n ?
1) ?
(tf(W ) ?
t(W )c(W ))Note that the value is zero where n = 1.
MC-Value [9] is a modified versionof C-Value adapted for use in term collections that include the term of length 1(i.e., n = 1).MC-val(W ) = n ?
(tf(W ) ?
t(W )c(W ))We used MC-Value in the experiments because our task was to extract termsregardless of whether each term is one-word term or not.2 We can also use another frequency score F(Frequency), or f(W ), that is defined asthe number of independent occurrences of W in all documents.
(Independent meansthat W is not included in any larger term candidate.)
However, we observed thatf(W ) (or the combination of f(W ) and another score) had no advantage over tf(W )(or the combination of tf(W ) and another score) in the experiments,so in this paperwe omit scores that are the combination of f(W ) and other scores.3 In addition, we apply the adding-one smoothing to both of them to avoid the scorebeing zero when wi has no connected terms.272 M. Yoshida and H. NakagawaFLR.
The LR method reflects the number of appearances of term units, but doesnot reflect that of a whole term itself.
For example, even if ?natural language?
ismore frequent than ?language natural?
and the former should be given a higherscore than the latter, LR cannot be used to do this.An FLR method [9] was proposed to overcome this shortcoming of LR.
Itreflects both the frequencies and inner structures of terms.
FLR(W ) is definedas the product of LR(W ) and tf(W ) as:FLR(W ) = tf(W )LR(W ).4 Our Method: Combining Types and Frequencies viaEntropy4.1 Preliminaries: Token-LR and Type-LRFigure 1 outlines example statistics for term unit connections.
For example, theterm disaster information appeared three times in the corpus.Disaster3 timesInformationEthics 2 timesSystem 1 timesSecurity 3 timesFig.
1.
An example of statistics for term unit connectionsLR scores have two versions: Token-LR and Type-LR.
Token-LR (and Type-LR) are calculated by simply counting the frequency (and the types) of termsconnected to each term unit, respectively.
In this case, a Type-LR score for theterm unit ?information?
isl(information) = 1 + 14, r(information) = 3 + 1, LR(information) =?8,and a Token-LR score isl(information) = 3 + 1, r(information) = 6 + 1, LR(information) =?28.4 Note that the adding-one smoothing is applied.Automatic Term Extraction Based on Perplexity of Compound Words 273Type-LR cannot reflect frequencies which suggest whether there are spe-cially important connecting terms or not.
However, Token-LR cannot reflect thenumber of types that suggest the variety of connections.
To solve these short-comings with LR measures, we propose a new kind that combines these twothrough perplexity.4.2 Term Extraction by PerplexityOur method is based on the idea of perplexity [8].
The score of a term is definedby the left perplexity and right perplexity of its term units.
In this subsection wefirst give a standard definition of the perplexity of language, from which our leftand right perplexity measures are derived.
After that, we describe how to scoreterms by using these perplexities.Perplexity of language.
Assume that language L is information source thatproduces word lists of length n and each word list is produced independentlywith probability P (wn1 ).
Then, the entropy of language L is calculated as:H0(L) = ?
?wn1P (wn1 ) log P (wn1 ).The entropy per word is then calculated as:H(L) = ?
1n?wn1P (wn1 ) log P (wn1 ).This value indicates the number of bits needed to express each word generatedfrom L. Perplexity of language L is defined using H(L) as:Perplexity = 2H(L).Perplexity can be seen as the average number of types of words that follow eachpreceding word.
The larger the perplexity of L, the less predictable the wordconnection in L.Left and right perplexity.
Assume that k types of unit words can connect tothe right of wi (see Figure 2).Also assume that Ri is a random variable assigned to the i-th term unit whichrepresents its right connections and takes its value from the set {r1, r2, ?
?
?
, rk}.Then, entropy H(Ri) is calculated as:H(Ri) = ?k?j=1P (rj) log2 P (rj)Note that we define 0 log 0 = 0, according to the fact that x log x ?
0 wherex ?
0.274 M. Yoshida and H. Nakagawaffiflfiwiffiflfir1ffiflfir2...ffiflfirkFig.
2.
Example of term unit and term units connected to its rightThis entropy value can be thought of as a variety of terms that connect tothe right of wi, or, more precisely, the number of bits needed to describe wordsthat connect to the right of wi.Then right perplexity ppr(wi) of term unit wi is defined asppr(wi) = 2H(Ri).This value can be seen as the number of branches, in the sense of informationtheory, of right-connection from wi.
It naturally reflects both the frequency andnumber of types of each connection between term units.Random variable Li for the left connections is defined in the same manner.The perplexity for left connections is thus defined as:ppl(wi) = 2H(Li).Term Score by Perplexity.
We define our measure by substituting l and rin the definition of LR with ppl and ppr.
First, a combination of left and rightperplexities is defined as the geometric mean of both:pp(wi) = (ppl(wi) ?
ppr(wi))12 .After that, perplexity score PP (W ) for W is defined as the geometric mean ofall pp(wi)s:PP (W ) =[n?i=1pp(wi)]1n.Automatic Term Extraction Based on Perplexity of Compound Words 275We used log PP (W ) instead of PP (W ) to make implementation easier.
Noticethat log x is a monotonic (increasing) function of x.PP (W ) =[n?i=1{ppl(wi) ?
ppr(wi)}12]1n?
log2 PP (W ) =1nlog2(n?i=1{ppl(wi) ?
ppr(wi)}12)?
log2 PP (W ) =12nn?i=1(log2 ppl(wi) + log2 ppr(wi))Using ppr(wi) = 2H(Ri) and ppl(wi) = 2H(li), we obtainlog2 PP (W ) =12nn?i=1(H(Ri) + H(Li)).The right side means the sum of the left and right entropies of all term units.4.3 Term Extraction by Perplexity and TFPerplexity itself serves as a good score for terms, but combining it with TF,which is a measure from another point of view, can provide a still better scorethat reflects both the inner structures of term candidates and their frequencieswhich are regarded as global information about the whole corpus.Our new score, FPP (W ), which is a combination of PP and TF, is definedas their product:FPP (W ) = tf(W )PP (W )?
log2 FPP (W ) = log2 tf(W ) + log2 PP (W )?
log2 FPP (W ) = log2 tf(W ) +12nn?i=1(H(Ri) + H(Li))We avoided the problem of log2 tf(W ) being undefined with tf(W ) = 05by applying the adding-one smoothing to tf(W ).
Therefore, the above defi-nition of log FPP (W ) changed as follows:log2 FPP?
(W ) = log2(tf(W ) + 1) +12nn?i=1(H(Ri) + H(Li)).We used this log2 FPP?
(W ) measure for evaluation.5 This situation occurs when we want to score a new term candidate from outside ofcorpus.276 M. Yoshida and H. Nakagawa5 Experiments5.1 Test CollectionWe collected news articles and their summaries from the Mainichi Web Newsfrom April, 2001 to March, 2002.
The articles were categorized into four genres:Economy, Society, World, and Politics.
A shorter version of each article wasprovided for browsing on mobile phones.
Articles for mobile phones were writtenmanually from the original ones, which were shorter versions of the originalarticles adapted to small displays.
We regard them as summaries of the originalarticles and used them to evaluate whether the extracted terms were corrector not.
If a term in the original article was also in the summary, the term wascorrect, and incorrect if otherwise.
Each article had a size of about 300 lettersand each summary had a size of about 50.Table 1 lists the number of articles in each category.Table 1.
Number of articles in test collectionEconomy Society World Politics# of articles 4,177 5,952 6,153 4,4285.2 Experimental SetupWe used test data on the various numbers of articles to investigate how theperformance of each measure changed according to corpus size.
A corpus of eachsize was generated by singly adding an article randomly selected from the corpusof each genre.
We generated test data consisting of 50 different sizes (from 1 to50) for each genre.
The average number of letters in the size 50 corpus was about19,000, and the average number of term candidates was about 1,300.
We usedfive different seed numbers to randomly select articles.
The performance of eachmethod was evaluated in terms of recall and precision, which were averaged overthe five trials.5.3 Preprocessing: Term Candidate ExtractionEach article was preprocessed with a morphological analyzer, the Chasen 2.3.3.
[2]The output of Chasen was further modified according to heuristic rules as follows.?
Nouns and undefined words were extracted for further processes and otherwords were discarded.?
Suffixes and prefixes were concatenated to their following and precedingwords, respectively.The result was a set of term candidates to be evaluated with the term importancescores described in the previous sections.We applied the following methods to the term candidates: F, TF, DF(Document Frequency) [8], LR, MC-Value, FLR, TF-IDF [8], PP, and FPP?.Automatic Term Extraction Based on Perplexity of Compound Words 2775.4 Evaluation MethodWe used average precision [8] for the evaluation.
Let D be a set of all the termcandidates and Dq ?
D be a set of the correct ones among them.
The extractedterm was correct if it appeared in the summary.
Then, the average precision canbe calculated in the following manner.Average-Precision =1|Dq|?1?k?|D|??
?rk ???1k?1?i?kri????
?where ri = 1 if the i-th term is correct, and ri = 0 if otherwise.Note that the total number of correct answers was |Dq|.
The next sectionpresents the experimental results obtained by average precision.Table 2.
Average precision on corpus of 1, 10, and 50 articles.
Each cell containsresults for the Economy/World/Society/Politics genres.Measure SIZE=1 SIZE=10 SIZE=50F 0.275/0.274/0.246/0.406 0.337/0.350/0.325/0.378 0.401/0.415/0.393/0.425TF 0.305/0.388/0.281/0.430 0.386/0.406/0.376/0.435 0.454/0.462/0.436/0.477DF 0.150/0.173/0.076/0.256 0.237/0.253/0.234/0.294 0.337/0.357/0.332/0.378LR 0.192/0.370/0.194/0.378 0.255/0.280/0.254/0.317 0.303/0.302/0.273/0.320MC-Val 0.218/0.296/0.240/0.388 0.317/0.334/0.307/0.365 0.399/0.400/0.369/0.420FLR 0.305/0.410/0.298/0.469 0.361/0.397/0.364/0.429 0.423/0.435/0.404/0.455TF-IDF 0.150/0.173/0.076/0.256 0.388/0.407/0.376/0.437 0.457/0.465/0.438/0.479PP 0.223/0.327/0.285/0.514 0.285/0.299/0.282/0.331 0.329/0.317/0.279/0.331FPP?
0.320/0.457/0.380/0.561 0.407/0.444/0.409/0.471 0.487/0.480/0.448/0.4936 Results and DiscussionTable 2 shows the results on the corpus of 1, 10, and 50 articles in all the gen-res.
Figure 3 plots the average precision for each corpus size (from 1 to 50) inthe economy category.6 In some cases, results on one article were better thanthose on 10 and 50 articles.
This was mainly caused by the fact that the av-erage precision is tend to be high on articles of short length, and the averagelength for one article was much shorter than that of ten articles in some genres.PP outperformed LR in most cases.
We think the reason was that PP couldprovide more precious information about connections among term units.
We ob-served that PP depended less on the size of the corpus than frequency-basedmethods like TF and MC-Val.
FPP?
had the best performance of all methods inall genres.6 We only show a graph in the economy genre, but the results in other genres weresimilar to this.278 M. Yoshida and H. Nakagawa0.10.20.30.40.51 11 21 31 41Corpus size (# of articles)AverageprecisionF TF DFLR MC-Val FLRTF-IDF PP FPP'Fig.
3.
Results in economy genre0.30.40.50.650 150 250 350 450 550 650 750 850 950Corpus size (# of articles)AverageprecisionF TF DFLR MC-Val FLRTF-IDF PP FPP'Fig.
4.
Results on 50 ?
1000 articlesAutomatic Term Extraction Based on Perplexity of Compound Words 279Figure 4 plots the results in the economy genre when the corpus size wasincreased to 1,000 in increments of 50 articles.
We observed that the perfor-mance of PP and LR got close with the increase in corpus size, especially with200 articles and more.
FPP?
once again outperformed all the other methods inthis experiment.
The FPP?
method exhibited the best performance regardless ofcorpus size.7 Conclusion and Future WorkWe proposed a new method for extracting terms.
It involved the combination oftwo LR methods: Token-LR and Type-LR.
We showed that these two could becombined by using the idea of perplexity, and gave a definition for the combinedmethod.
This new method was then combined with TF and experimental resultson the test corpus consisting of news articles and their summaries revealed thatthe new method (FPP?)
outperformed existing methods including TF, TF-IDF,MC-Value, and FLR.In future work, we would like to improve the performance of the method by,for example, adding preprocessing rules, such as the appropriate treatment ofnumerical characters, and developing more sophisticated methods for combin-ing TF and PP.
We also plan to extend our experiments to include other testcollections like TMREC [6].References1.
Ananiadou, S.: A methodology for automatic term recognition.
In Proceedings ofthe 15th InternationalConference on Computational Linguistcs (COLING) (1994),pp.
1034?1038.2.
Asahara, M., Matsumoto, Y.: Extended Models and Tools for High-performancePart-of-Speech Tagger.
Proceedings of COLING 2000.
(2000).3.
COMPUTERM?98 First Workshop on Computational Terminology.
(1998).4.
COMPUTERM?02 Second Workshop on Computational Terminology.
(2002).5.
Frantzi, K. and Ananiadou, S.: The C-value/NC-value method for ATR.
Journal ofNLP, Vol.
6, No.
3, (1999).
pp.145?179.6.
Kageura, K.: TMREC Task: Overview and Evaluation.
Proc.
of the First NTCIRWorkshop on Research in Japanese Text Retrieval and Term Recognition, (1999).pp.
411?440.7.
Kageura, K and Umino, B.: Methods of automatic term recognition: A review.Terminology, Vol.
3, No.
2, (1996).
pp.
259?289.8.
Manning, C.D., and Schutze, H..: Foundations of Statistical Natural Language Pro-cessing.
(1999).
The MIT Press.9.
Nakagawa, H. and Mori, T.: Automatic Term Recognition based on Statistics ofCompound Nouns and their Components.
Terminology, Vol.
9, No.
2, (2003).
pp.201?219.
