Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 64?73,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsDetermining Compositionality of Word Expressions UsingVarious Word Space Models and MeasuresLubom?
?r Krc?ma?r?1,21University of West Bohemia,Faculty of Applied Sciences,NTIS ?
New Technologiesfor the Information Society,Pilsen, Czech Republiclkrcmar@kiv.zcu.czKarel Jez?ek22University of West Bohemia,Faculty of Applied Sciences,Department of ComputerScience and Engineering,Pilsen, Czech Republicjezek ka@kiv.zcu.czPavel Pecina33Charles University in Prague,Faculty of Mathematics andPhysics, Institute of Formaland Applied Linguistics,Prague, Czech Republicpecina@ufal.mff.cuni.czAbstractThis paper presents a comparative studyof 5 different types of Word Space Mod-els (WSMs) combined with 4 differentcompositionality measures applied to thetask of automatically determining seman-tic compositionality of word expressions.Many combinations of WSMs and mea-sures have never been applied to the taskbefore.The study follows Biemann and Gies-brecht (2011) who attempted to find a listof expressions for which the composition-ality assumption ?
the meaning of an ex-pression is determined by the meaning ofits constituents and their combination ?does not hold.
Our results are very promis-ing and can be appreciated by those inter-ested in WSMs, compositionality, and/orrelevant evaluation methods.1 IntroductionOur understanding of WSM is in agreement withSahlgren (2006): ?The word space model is acomputational model of word meaning that uti-lizes the distributional patterns of words collectedover large text data to represent semantic similar-ity between words in terms of spatial proximity?.There are many types of WSMs built by differentalgorithms.
WSMs are based on the Harris distri-butional hypothesis (Harris, 1954), which assumesthat words are similar to the extent to which theyshare similar linguistic contexts.
WSM can beviewed as a set of words associated with vectorsrepresenting contexts in which the words occur.Then, similar vectors imply (semantic) similarityof the words and vice versa.
Consequently, WSMsprovide a means to find words semantically simi-lar to a given word.
This capability of WSMs isexploited by many Natural Language Processing(NLP) applications as listed e.g.
by Turney andPantel (2010).This study follows Biemann and Giesbrecht(2011), who attempted to find a list of non-compositional expressions whose meaning is notfully determined by the meaning of its con-stituents and their combination.
The task turnedout to be frustratingly hard (Johannsen et al2011).
Biemann?s idea and motivation is that non-compositional expressions could be treated as sin-gle units in many NLP applications such as In-formation Retrieval (Acosta et al 2011) or Ma-chine Translation (Carpuat and Diab, 2010).
Weextend this motivation by stating that WSMs couldalso benefit from a set of non-compositional ex-pressions.
Specifically, WSMs could treat se-mantically non-compositional expressions as sin-gle units.
As an example, consider ?kick thebucket?, ?hot dog?, or ?zebra crossing?.
Treat-ing such expressions as single units might improvethe quality of WSMs since the neighboring wordsof these expressions should not be related to theirconstituents (?kick?, ?bucket?, ?dog?
or ?zebra?
),but instead to the whole expressions.Recent works, including that of Lin (1999),Baldwin et al(2003), Biemann and Giesbrecht(2011), Johannsen et al(2011), Reddy et al(2011a), Krc?ma?r?
et al(2012), and Krc?ma?r?
et al(2013), show the applicability of WSMs in deter-mining the compositionality of word expressions.The proposed methods exploit various types ofWSMs combined with various measures for de-termining the compositionality applied to variousdatasets.
First, this leads to non-directly compa-rable results and second, many combinations of64WSMs and measures have never before been ap-plied to the task.
The main contribution and nov-elty of our study lies in systematic research ofseveral basic and also advanced WSMs combinedwith all the so far, to the best of our knowledge,proposed WSM-based measures for determiningthe semantic compositionality.The explored WSMs, described in more detailin Section 2, include the Vector Space Model,Latent Semantic Analysis, Hyperspace Analogueto Language, Correlated Occurrence Analogue toLexical Semantics, and Random Indexing.
Themeasures, including substitutability, endocentric-ity, compositionality, and neighbors-in-common-based, are described in detail in Section 3.
Sec-tion 4 describes our experiments performed onthe manually annotated datasets ?
DistributionalSemantics and Compositionality dataset (DISCO)and the dataset built by Reddy et al(2011a).
Sec-tion 5 summarizes the results and Section 6 con-cludes the paper.2 Word Space ModelsThe simplest and oldest types of WSMs1 are theVector Space Model (VSM) and Hyperspace Ana-logue to Language (HAL).
More recent and ad-vanced models include Latent Semantic Analy-sis (LSA), which is based on VSM, and Corre-lated Occurrence Analogue to Lexical Semantics(COALS), which originates from HAL.
RandomIndexing (RI) is WSM joining the principles ofLSA and HAL.
Many other WSMs have been pro-posed too.
Their description is outside the scopeof this paper and can be found e.g.
in Turney andPantel (2010) or Jurgens and Stevens (2010).VSM is based on the assumption that similar (re-lated) words tend to occur in the same documents.2VSM stores occurrence counts of all word typesin documents a given corpus in a co-occurrencematrix C. The row vectors of the matrix corre-spond to the word types and the columns to thedocuments in the corpus.
The numbers of occur-rences cij in C are usually weighted by the prod-uct of the local and global weighting functions(Nakov et al 2001).
The local function weightscij by the same mathematical function; typicallynone (further denoted as no), log(cij + 1) (de-1WSMs are also referred to as distributional models ofsemantics, vector space models, or semantic spaces.2VSM was originally developed for the SMART informa-tion retrieval system (Salton, 1971).noted as log) or?cij (denoted as sqrt).
Thepurpose of local weighting is to lower the im-portance of highly occurring words in the docu-ment.
The global function weights every valuein row i of C by the same value calculated forrow i.
Typically: none (denoted as No), In-verse Document Frequency (denoted as Idf ) ora function referred to as Entropy (Ent).
Idfis calculated as 1 + log(ndocs/df(i)) and Entas 1 + {?j p(i, j) log p(i, j)}/ log ndocs, wherendocs is the number of documents in the corpora,df(i) is the number of documents containing wordtype i, and p(i, j) is the probability of occurrenceof word type i in document j.LSA builds on VSM and was introduced byLandauer and Dumais (1997).
The LSA algo-rithm works with the same co-occurrence matrixC which can be weighted in the same manner asin VSM.
The matrix is than transformed by Sin-gular Value Decomposition (SVD) (Deerwester etal., 1990) into C. The purpose of SVD is toproject the row vectors and column vectors of Cinto a lower-dimensional space and thus bring thevectors of word types and vectors of documents,respectively, with similar meanings near to eachother.3 The output number of dimensions is a pa-rameter of SVD and typically ranges from 200 to1000 (Landauer and Dumais, 1997; Rohde et al2005).HAL was first explored by Lund and Burgess(1996).
It differs from VSM and LSA in that itonly exploits neighboring words as contexts forword types.
HAL processes the corpus by movinga sliding double-sided window with a size rang-ing from 1 to 5 around the word type in focusand accumulating the weighted co-occurrences ofthe preceding and following words into a matrix.Typically, the linear weighting function is usedto ensure that the occurrences of words whichare closer to the word type in focus are moresignificant.
The dimensions of the resulting co-occurrence matrix are of size |V | and 2|V |, whereV denotes the vocabulary consisting of all theword types occurring in the processed corpora.
Fi-nally, the HAL co-occurrence matrix can be re-duced by retaining the most informative columnsonly.
The columns with the highest values of en-tropy (?
?j pj log pj , where pj denotes the prob-3In this way, LSA is able to capture higher-order co-occurrences.65ability of a word in the investigated column j) canbe considered as the most informative.
The alter-natives and their description can be found e.g.
inSong et al(2004).COALS was introduced by Rohde et al(2005).Compared to HAL, COALS also processes a cor-pus by using a sliding window and linear weight-ing, but differs in several aspects: the window sizeof COALS is 4 and this value is fixed; COALSdoes not distinguish between the preceding andfollowing words and treats them equally; applyingCOALS supposes that all but the most frequent mcolumns reflecting the most common open-classwords are discarded; COALS transforms weightedcounts in the co-occurrence matrix in a specialway (all the word pair correlations are calculated,negative values are set to 0, and non-negative onesare square rooted ?
corr); and optionally, Singu-lar Value Decomposition (Deerwester et al 1990)can be applied to the COALS co-occurrence ma-trix.RI is described in Sahlgren (2005) and can beviewed as a mixture of HAL and LSA.
First, RIassigns random vectors to each word type in thecorpus.
The random vectors, referred to as indexvectors, are very sparse, typically with a lengthof thousands, and contain only several (e.g.
7)non-zero values from the {-1,1} set.
Second, RIprocesses the corpus by exploiting a sliding win-dow like HAL and COALS.
However, RI does notaccumulate the weighted co-occurrence counts ofneighboring words to the vector of the word typein focus.
Instead, RI accumulates the index vec-tors of the co-occurring words.
For accounting theword order, the permutation variant of RI was alsodeveloped (Sahlgren et al 2008).
This variantpermutes the index vectors of neighboring wordsof the word type in focus according to the wordorder.3 Compositionality MeasuresWe experimented with four basically differentcompositionality measures (further referred to asMeasures) (Krc?ma?r?
et al 2013).
Each Measureemploys a function to measure similarity of WSMvectors.
We experimented with the followingones: cosine (cos), Euclidian (inverse to Euclid-ian distance) (euc), and Pearson correlation (cor).The mathematical formulas are presented below.cos(a,b) =?ni=1 aibi?
?ni=1(ai)2?ni=1(bi)2euc(a,b) =11 +?
?ni=1 (ai ?
bi)2cor(a,b) =?ni=1 (ai ?
a?
)(bi ?
b?)?
?ni=1(ai ?
a?
)2?ni=1(bi ?
b?
)2where a?
=?ni=1 ain, b?
=?ni=1 binSU The substitutability-based Measure is basedon the fact that the replacement of non-compositional expressions?
constituents by thewords similar to them leads to anti-collocations(Pearce, 2002).
The compositionality of expres-sions is calculated as the ratio between the num-ber of occurrences of the expression in a corporaand the sum of occurrences of its alternatives ?possibly anti-collocations.
In a similar way, wecan compare pointwise mutual information scores(Lin, 1999).
As an example, consider the possibleoccurrences of ?hot dog?
and ?warm dog?
in thecorpora.Formally, adopted from Krc?ma?r?
et al(2012),we calculate the compositionality score csu for anexamined expression as follows:csu =?Hi=1W ?ahi ,m?
?
?Mj=1W ?h, amj ?W ?h,m?,where ?h,m?
denotes the number of corpora oc-currences of the examined expression consistingof a head and a modifying word, ahi and amj denotei-th and j-th most similar word4 in a certain WSMto the head and modifying word of the expression,respectively.
W stands for a weighting function;following Krc?ma?r?
et al(2012), we experimentedwith no (no) and logarithm (log) weighting.
The?
symbol stands for one of the two operators: ad-dition (plus) and multiplication (mult).EN The endocentricity-based Measure, also re-ferred to as component or constituent-based, com-pares the WSM vectors of the examined expres-sions and their constituents.
The vectors expectedto be different from each other are e.g.
the vectorrepresenting the expression ?hot dog?
and the vec-tor representing the word ?dog?.
Formally, the4When exploiting POS tags, we constrained the similarwords to be of the same POS category in our experiments.66compositionality score cen can be calculated asfollows:cen = f(xh, xm) ,where xh and xm denote the similarity (sim) orinverse rank distance (?dist) between the exam-ined expression and its head and modifying con-stituent, respectively, with regards to a certainWSM.
Function f stands for a combination of itsparameters: 0.5xh + 0.5xm (avg), 0xh + 1xm(mOnly), 1xh + 0xm (hOnly), min(xh, xm) (min),and max(xh, xm) (max).CO The compositionality-based Measure com-pares the true co-occurrence vector of the exam-ined expression and the vector obtained from thevectors corresponding to the constituents of theexpression using some compositionality function(Reddy et al 2011a).
Commonly used compo-sitionality functions are vector addition (?)
andpointwise vector multiplication (?)
(Mitchell andLapata, 2008).
The vectors expected to be dif-ferent from each other are e.g.
?hot dog?
and?hot???dog?.
Formally,cco = s(ve, vh ?
vm) ,where ve, vh, and vm stand for vectors of an ex-amined expression, its head and modifying con-stituents, respectively.
?
stands for a vector opera-tion.NE The neighbors-in-common-based Measureis based on overlap of the most similar words tothe examined expression and to its constituents(McCarthy et al 2003).
As an example, considerthat ?hot dog?
is similar to ?food?
or ?chips?
and?dog?
is similar to ?cat?
or ?bark?.
On the otherhand, the list of neighbors of a semantically com-positional expression such as ?black dog?
is sup-posed to overlap with at least one of the lists ofneighbors of both the expression constituents.
For-mally,cne = ohN + omN ,where ohN and omN stand for the number of samewords occurring in the list of the most similarwords to the examined expression and to its headand modifying constituent, respectively.4 ExperimentsWe evaluated the ability of various combinationsof WSMs and Measures to rank expressions as thehuman annotators had done ahead of time.Datasets We experimented with the DISCO(Biemann and Giesbrecht, 2011) and Reddy(Reddy et al 2011a) human annotated datasets,built for the task of automatic determining of se-mantic compositionality.
The DISCO and Reddydatasets consist of manually scored expressionsof adjective-noun (AN), verb-object (VO), andsubject-verb (SV) types and the noun-noun (NN)type, respectively.
The DISCO dataset consistsof 349 expressions divided into training, valida-tion, and test data (TestD); the Reddy dataset con-sists of one set containing 90 expressions.
Sincethe DISCO validation data are of low size (35),we concatenated them with the training data (Tr-ValD).
To TrValD and TestD we added the Reddydataset, which we had divided stratifically aheadof time.
Numbers of expressions of all the differ-ent types are summarized in Table 1.dataset AN-VO-SV AN VO SV NNTrValD 175 68 68 39 45TestD 174 77 62 35 45Table 1: Numbers of expressions of all the differ-ent types from the DISCO and Reddy datasets.WSM construction Since the DISCO andReddy data were extracted from the ukWaC cor-pus (Baroni et al 2009), we also build our WSMsfrom the same corpus.
We use our own modifica-tion of the S-Space package (Jurgens and Stevens,2010).
The modification lies in treating multiwordexpressions and handling stopwords.
Specifically,we extended the package with the capability ofbuilding WSM vectors for the examined expres-sions in such a way that the WSM vectors previ-ously built for words are preserved.
This differen-tiates our approach e.g.
from Baldwin et al(2003),who label the expressions in the corpus ahead oftime and treat them as single words.5 As for treat-ing stopwords, we map trigrams containing deter-miners as the middle word into bigrams withoutthe determiners.
The intuition is to extract betterco-occurrence statistics for VO expressions oftencontaining an intervening determiner.
As an ex-ample, compare the occurrences of ?reinvent (de-5Since many single word occurrences disappear, theWSM vectors for words change.
The more expressions aretreated as single words, the more WSM changes.
Conse-quently, we believe that this approach cannot be used forbuilding a list of all expressions occurring in an examinedcorpus ordered by their compositionality score.67terminer) wheel?
and ?reinvent wheel?
in ukWaCbeing 623 and 27, respectively.We experimented with lemmas (noT) or withlemmas concatenated with their part of speech(POS) tags (yesT).
We labeled the followingstrings in ukWaC as stopwords: low-frequencywords (lemmas with frequency< 50), strings con-taining two adjacent non-letter characters (thusomitting sequences of various symbols), andclosed-class words.For our experiments, we built WSMs using var-ious parameters examined in previous works (seeSection 2) and parameters which are implied fromour own experience with WSMs.
Figure 1 sum-marizes all the parameters we used for buildingWSMs.Measure settings We examined various Mea-sure settings (see Section 3), summarized in Ta-ble 2.
For all the vector comparisons, we used thecos similarity.
Only for HAL we also examinedeuc and for COALS cor, since these are the rec-ommended similarity functions for these particu-lar WSMs (Lund and Burgess, 1996; Rohde et al2005).Met.
par.
possible valuesall sim.
cos, euc if HAL, cor if COALSSU H 0,1,...,20,30,...,100SU M 0,1,...,20,30,...,100SU W no, logSU ?
plus, multEN x sim, ?distEN f avg, mOnly, hOnly, min, maxCO ?
?, ?NE N 10,20,...,50,100,200,...,500,1000Table 2: All the parameters of Measures for de-termining semantic compositionality described inSection 3 used in our experiments.Experimental setup Following Biemann andGiesbrecht (2011), Reddy et al(2011a), Krc?ma?r?et al(2012), and Krc?ma?r?
et al(2013), we usethe Spearman correlation (?)
for the evaluation ofall the combinations of WSMs and Measures (Se-tups).
Since the distribution of scores assigned toReddy?s NN dataset might not have correspondedto the distribution of DISCO scores, we decidednot to map them to the same scale.
Thus, we do notcreate a single list consisting of all the examinedexpressions.
Instead, we order our Setups accord-ing to the weighted average of Spearman corre-lations calculated across all the expression types.The weights are directly proportional to the fre-quencies of the particular expression types.
Thus,the Setup score (wAvg) is calculated as follows:wAvg =|AN |?AN + |V O|?V O + |SV |?SV + |NN |?NN|AN | + |V O| + |SV | + |NN |.Having the evaluation testbed, we tried to findthe optimal parameter settings for all WSMs com-bined with all Measures with the help of TrValD.Then, we applied the found Setups to TestD.Notes Because several expressions or their con-stituents concatenated with their POS tags did notoccur sufficiently often (for expressions: ?
0,for constituents: ?
50) in ukWaC, we removedthem from the experiments; we removed ?numbercrunching?, ?pecking order?, and ?sacred cow?from TrValD and ?leading edge?, ?broken link?,?spinning jenny?, and ?sitting duck?
from TestD.5 ResultsThe Setups achieving the highest wAvg when ap-plied to TrValD are depicted in Table 3.
The sameSetups and their results when applied to TestD aredepicted in Table 4.
The values of Spearman cor-relations in TestD confirm many of the observa-tions from TrValD6:Almost all the combinations of WSMs andMeasures achieve correlation values which are sta-tistically significant.
This is best illustrated by the?
(AN ?V O?SV ) column in Table 4, where alot of correlation values are statistically (p<0.05)or highly statistically (p<0.001) significant, withregards to the number of expressions (172).The results suggest that for every expressiontype, the task of determining compositionality isof varying difficulty.
While determining the com-positionality of the NN expression type seems tobe the simplest (the highest correlations observed),determining the compositionality of the SV ex-pression type seems to be hard since the majorityof values in the ?SV column are not statisticallysignificant; taking into account the number of SVexpressions in TestD ?
35, the statistically signifi-cant value of ?
at the p<0.05 level is 0.34.The correlation values differ with regards to theexpression type.
Certain WSMs combined with6A test of statistical difference between two values of theSpearman correlation is adopted from Papoulis (1990).68Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments.
Semicolondenotes OR.
All the examined combinations of parameters are implied from reading the diagram fromleft to right.certain Measures, although achieving high corre-lations upon certain expression types, fail to cor-relate with the rest of the expression types.
Com-pare e.g.
the correlation values of VSM and LSAcombined with the SU Measure upon the AN andSV types with the correlation values upon the VOand NN types.The results, as expected, illustrate that employ-ing more advanced alternatives of basic WSMs ismore appropriate.
Specifically, LSA outperformsVSM and COALS outperforms HAL in 21 and 23correlation values out of 24, respectively.
Con-cerning RI, the values of correlations seem to beclose to the values of VSM and HAL.An interesting observation showing the appro-priateness of using wAvg(of?)
as a good evalu-ation score is supported by a comparison of thewAvg(of?)
and ?
(AN?V O?SV ) columns.
Thecolumns suggest that some Setups might only beable to order the expressions of the same type andmight not be able to order the expressions of dif-ferent types among each other.
As an example,compare the value of ?
= 0.42 in wAvg(of?
)with ?
= 0.28 in ?
(AN?V O?SV ) in the row cor-responding to COALS combined with SU.
Con-sider also that all the values of correlations arehigher or equal to the value in ?
(AN?V O?SV ).As for the parameters learned from applyingall the combinations of differently set WSM algo-rithms and Measures to TrValD, their diversity iswell illustrated in Tables 5 and 6.
Due to this diver-sity, we cannot recommend any particular settingsexcept for one.
All our SU Measures benefit fromweighting numbers of expression occurrences bylogarithm.The correlation values in TestD are slightlylower ?
probably due to overfitting ?
than theones observed in TrValD.
HAL combined with theMeasures using euc similarity was not as success-ful as when combined with cos.7For comparison, the results of Reddy et al(2011b) and Chakraborty et al(2011) as theresults of the best performing Setups based onWSMs and association measures, respectively, ap-plied to the DISCO data, are presented (Biemannand Giesbrecht, 2011).
The correlation values ofour Setups based on LSA and COALS, respec-tively, are mostly higher.
However, the improve-ments are not statistically significant.
Also, the re-cent results achieved by Krc?ma?r?
et al(2012) em-ploying COALS and Krc?ma?r?
et al(2013) employ-7However, using HAL combined with euc, we observedsignificant negative correlations which deserve further explo-ration.69ing LSA are depicted.Discussion As described above, we observeddifferent values of correlations for different ex-pression types.
This motivates us to think aboutother classes of expressions different from types;Measures could be e.g.
varyingly successful withregards to different occurrence frequency classesof expressions (Evert, 2005).
However, with suchsmall datasets, as shown e.g.
by the fact that themajority of our results are statistically indistin-guishable, we cannot carry out any deeper in-vestigations.
A large dataset would provide amore reliable comparison.
Ideally, this wouldconsist of all the candidate expressions occurringin some smaller corpus.
Also, we would pre-fer the annotated dataset not to be biased towardsnon-compositional expressions and to be providedwith an inner-annotator agreement (Pecina, 2008);which is unfortunately not the case of the DISCOdataset.6 ConclusionOur study suggests that different WSMs combinedwith different Measures perform reasonably wellin the task of determining the semantic composi-tionality of word expressions of different types.Especially, LSA and COALS perform well inour experiments since their results are better thanthose of their basic variants (VSM and HAL, re-spectively) and, although not statistically signifi-cantly, they outperform the best results of the pre-viously proposed approaches (Table 4).Importantly, our results demonstrate (Section 5)that the datasets used for the experiments are smallfor: first, a statistical learning of optimal parame-ters of both WSM algorithms and Measures; sec-ond, a thorough (different types) and reliable (sta-tistically significant) comparison of our and thepreviously proposed approaches.Therefore, we plan to build a larger manually-annotated dataset.
Finally, we plan to extracta list of semantically non-compositional expres-sions from a given corpus and experiment with us-ing it in NLP applications.AcknowledgmentsWe thank to V?
?t Suchomel for providing theukWaC corpus and the anonymous reviewersfor their helpful comments and suggestions.This work was supported by the EuropeanRegional Development Fund (ERDF), projectNTIS ?
New Technologies for the Informa-tion Society, European Centre of Excellence,CZ.1.05/1.1.00/02.0090; by Advanced Comput-ing and Information Systems (grant no.
SGS-2013-029); and by the Czech Science Foun-dation (grant no.
P103/12/G084).
Also, theaccess to the CERIT-SC computing facilitiesprovided under the programme Center CERITScientific Cloud, part of the Operational Pro-gram Research and Development for Innovations,reg.
no.
CZ.1.05/3.2.00/08.0144 is highly appreci-ated.ReferencesOtavio Costa Acosta, Aline Villavicencio, and Vi-viane P. Moreira.
2011.
Identification and treatmentof multiword expressions applied to information re-trieval.
In Proceedings of the Workshop on Multi-word Expressions: from Parsing and Generation tothe Real World, MWE ?11, pages 101?109, Strouds-burg, PA, USA.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
Proceed-ings of the ACL 2003 workshop on Multiword ex-pressions analysis acquisition and treatment, pages89?96.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wide web:a collection of very large linguistically processedweb-crawled corpora.
Journal of Language Re-sources And Evaluation, 43(3):209?226.Chris Biemann and Eugenie Giesbrecht.
2011.
Dis-tributional semantics and compositionality 2011:shared task description and results.
In Proceedingsof the Workshop on Distributional Semantics andCompositionality, DiSCo ?11, pages 21?28.Marine Carpuat and Mona Diab.
2010.
Task-basedevaluation of multiword expressions: a pilot studyin statistical machine translation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 242?245, Stroudsburg, PA, USA.Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal,Tanik Saikh, and Sivaju Bandyopadhyay.
2011.Shared task system description: Measuring the com-positionality of bigrams using statistical methodolo-gies.
In Proceedings of the Workshop on Distribu-tional Semantics and Compositionality, pages 38?42, Portland, Oregon, USA.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.70WSM Measure wAvg(of ?)
?AN-VO-SV ?AN ?VO ?SV ?NNVSM1 SU1 0.31 0.11 -0.03 0.36 0.31 0.75VSM2 EN1 0.36 0.32 0.41 0.30 0.10 0.61VSM3 CO1 0.40 0.34 0.40 0.26 0.39 0.64VSM1 NE1 0.34 0.26 0.20 0.48 0.07 0.60LSA1 SU2 0.34 0.19 -0.05 0.46 0.42 0.71LSA2 EN2 0.56 0.53 0.54 0.51 0.59 0.65LSA3 CO1 0.55 0.53 0.49 0.56 0.63 0.58LSA2 NE2 0.50 0.45 0.46 0.37 0.64 0.62HAL1 SU3 0.45 0.36 0.28 0.50 0.40 0.67HAL2 EN3 0.36 0.35 0.47 0.28 0.27 0.38HAL3 CO1 0.23 0.15 0.28 0.12 -0.01 0.54HAL4 NE3 0.27 0.25 0.31 0.21 0.17 0.39COALS1 SU4 0.48 0.41 0.28 0.56 0.49 0.68COALS2 EN2 0.58 0.54 0.6 0.63 0.37 0.68COALS2 CO1 0.59 0.54 0.6 0.64 0.37 0.70COALS2 NE4 0.58 0.56 0.61 0.58 0.46 0.67RI1 SU5 0.52 0.44 0.45 0.51 0.52 0.68RI2 EN3 0.45 0.44 0.41 0.57 0.33 0.45RI3 CO1 0.21 0.13 0.13 0.16 0.11 0.54RI2 NE5 0.43 0.43 0.43 0.53 0.21 0.49Table 3: The Spearman correlations ?
of the best performing (wAvg) combinations of particular WSMsand Measures from all the tested Setups applied to TrValD.
The highest correlation values in the particularcolumns and the correlation values which are not statistically different from them (p < 0.05) are in bold(yet we do not know how to calculate the stat.
significance for the wAvg(of ?)
column).
The parametersof WSMs and Measures corresponding to the indexes are depicted in Tables 5 and 6, respectively.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Stefan Evert.
2005.
The statistics of word cooccur-rences : word pairs and collocations.
Ph.D. the-sis, Universita?t Stuttgart, Holzgartenstr.
16, 70174Stuttgart.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Anders Johannsen, Hector Martinez Alonso, ChristianRish?j, and Anders S?gaard.
2011.
Shared task sys-tem description: frustratingly hard compositionalityprediction.
In Proceedings of the Workshop on Dis-tributional Semantics and Compositionality, DiSCo?11, pages 29?32, Stroudsburg, PA, USA.David Jurgens and Keith Stevens.
2010.
The s-space package: an open source package for wordspace models.
In Proceedings of the ACL 2010 Sys-tem Demonstrations, ACLDemos ?10, pages 30?35,Stroudsburg, PA, USA.Lubom?
?r Krc?ma?r?, Karel Jez?ek, and Massimo Poesio.2012.
Detection of semantic compositionality usingsemantic spaces.
Lecture Notes in Computer Sci-ence, 7499 LNAI:353?361.Lubom?
?r Krc?ma?r?, Karel Jez?ek, and Pavel Pecina.
2013.Determining compositionality of word expressionsusing word space models.
In Proceedings of the 9thWorkshop on Multiword Expressions, pages 42?50,Atlanta, Georgia, USA.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Dekang Lin.
1999.
Automatic identification ofnon-compositional phrases.
In Proceedings of the37th annual meeting of the Association for Compu-tational Linguistics on Computational Linguistics,ACL ?99, pages 317?324, Stroudsburg, PA, USA.Kevin Lund and Curt Burgess.
1996.
Produc-ing high-dimensional semantic spaces from lexi-cal co-occurrence.
Behavior Research Methods,28(2):203?208.Diana McCarthy, Bill Keller, and John Carroll.2003.
Detecting a continuum of compositionalityin phrasal verbs.
In Proceedings of the ACL 2003workshop on Multiword expressions analysis acqui-sition and treatment, volume 18 of MWE ?03, pages73?80.71WSM Measure wAvg(of ?)
?AN-VO-SV ?AN ?VO ?SV ?NNVSM1 SU1 0.28 0.03 0.01 0.51 0.04 0.62VSM2 EN1 0.26 0.19 0.08 0.29 0.04 0.69VSM3 CO1 0.32 0.26 0.24 0.23 0.25 0.65VSM1 NE1 0.32 0.19 0.36 0.25 -0.13 0.73LSA1 SU2 0.31 0.06 0.05 0.50 0.20 0.59LSA2 EN2 0.50 0.40 0.39 0.55 0.32 0.78LSA3 CO1 0.48 0.36 0.29 0.60 0.42 0.69LSA2 NE2 0.44 0.33 0.34 0.40 0.44 0.67HAL1 SU3 0.29 0.16 0.09 0.32 0.34 0.56HAL2 EN3 0.36 0.28 0.33 0.35 0.26 0.53HAL3 CO1 0.24 0.22 0.25 0.16 0.15 0.42HAL4 NE3 0.21 0.14 0.02 0.33 0.06 0.47COALS1 SU4 0.42 0.28 0.28 0.54 0.30 0.59COALS2 EN2 0.49 0.44 0.52 0.51 0.07 0.72COALS2 CO1 0.47 0.40 0.47 0.51 0.07 0.74COALS2 NE4 0.52 0.48 0.55 0.50 0.21 0.74RI1 SU5 0.30 0.14 0.14 0.29 0.12 0.72RI2 EN3 0.44 0.34 0.37 0.54 0.20 0.63RI3 CO1 0.23 0.23 0.29 0.17 0.17 0.26RI2 NE5 0.31 0.26 0.26 0.42 0.04 0.44Reddy-WSM - 0.35 - - - -StatMix - 0.33 - - - -Krcmar-COALS - 0.42 0.42 0.69 0.24 -Krcmar-LSA - 0.50 0.50 0.56 0.41 -Table 4: The Spearman correlations ?
of the best performing (wAvg) combinations of particular WSMsand Measures trained in TranValD applied to TestD.
The highest correlation values in the particularcolumns and the correlation values which are not statistically different from them (p < 0.05) are in bold(yet we do not know how to calculate the stat.
significance for the wAvg(of ?)
column).
Reddy-WSM andStatMix stand for the best performing system based on WSMs and association measures, respectively,applied to the DISCO task (Biemann and Giesbrecht, 2011).
Krcmar-COALS and Krcmar-LSA stand forthe best published results achieved upon the dataset presented in Krc?ma?r?
et al(2012) and Krc?ma?r?
et al(2013), respectively.
The parameters of WSMs and Measures corresponding to the indexes are depictedin Tables 5 and 6, respectively.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, Ohio.Preslav Nakov, Antonia Popova, and Plamen Ma-teev.
2001.
Weight functions impact on lsa per-formance.
In Proceedings of the EuroConferenceRecent Advances in Natural Language Processing(RANLP?01), pages 187?193.Athanasios Papoulis.
1990.
Probability & statistics.Prentice Hall.Darren Pearce.
2002.
A Comparative Evaluation ofCollocation Extraction Techniques.
In Proceedingsof the Third International Conference on LanguageResources and Evaluation, LREC.Pavel Pecina.
2008.
Reference data for Czech collo-cation extraction.
In Proceedings of the LREC 2008Workshop Towards a Shared Task for Multiword Ex-pressions, pages 11?14, Marrakech, Morocco.
Euro-pean Language Resources Association.Siva Reddy, Diana McCarthy, and Suresh Manand-har.
2011a.
An empirical study on composition-ality in compound nouns.
In Proceedings of 5th In-ternational Joint Conference on Natural LanguageProcessing, pages 210?218, Chiang Mai, Thailand,November.
Asian Federation of Natural LanguageProcessing.Siva Reddy, Diana McCarthy, Suresh Manandhar, andSpandana Gella.
2011b.
Exemplar-based word-space model for compositionality detection: Sharedtask system description.
In Proceedings of the Work-shop on Distributional Semantics and Composition-ality, pages 54?60, Portland, Oregon, USA.72WSM parametersVSM tags trans.VSM1 noT noNoVSM2 yesT noNoVSM3 yesT noIdfLSA tags trans.
dim.LSA1 noT logEnt 900LSA2 yesT noNo 300LSA3 noT noIdf 300HAL tags win s. ret.
c.HAL1 noT 5 20000HAL2 yesT 5 20000HAL3 noT 2 10000HAL4 yesT 5 allCOALS tags ret.
c.COALS1 noT 7000COALS2 yesT 7000RI tags win.
s. vec.
s. perm.RI1 noT 2 4000 noRI2 noT 4 4000 noRI3 noT 2 4000 yesTable 5: Parameters of WSMs (Section 2) which,combined with particular Measures, achieved thehighest average correlation in TrValD.Douglas L. Rohde, Laura M. Gonnerman, and David C.Plaut.
2005.
An improved model of semantic sim-ilarity based on lexical co-occurrence.
Unpublishedmanuscript.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a means to encode order inword space.
In V. Sloutsky, B.
Love, and K. Mcrae,editors, Proceedings of the 30th Annual Conferenceof the Cognitive Science Society, pages 1300?1305.Cognitive Science Society, Austin, TX.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Methods and Applications of SemanticIndexing Workshop at the 7th International Confer-ence on Terminology and Knowledge Engineering,Leipzig, Germany.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.Gerard Salton.
1971.
The SMART Retrieval Sys-tem; Experiments in Automatic Document Process-ing.
Prentice-Hall, Inc., Upper Saddle River, NJ,USA.Dawei Song, Peter Bruza, and Richard Cole.
2004.Concept learning and information inferencing on aMeasure parametersSU sim.
?
W H MSU1 cos plus log 30 3SU2 cos plus log 100 5SU3 cos mult log 12 2SU4 cos mult log 80 4SU5 cos mult log 4 3EN sim.
func.
xEN1 cos min simEN2 cos avg simEN3 cos min ?distCO sim.
?CO1 cos ?NE sim.
ONE1 cos 1000NE2 cos 500NE3 cos 50NE4 cor 500NE5 cos 20Table 6: Parameters of Measures (Section 3)which, combined with particular WSMs, achievedthe highest average correlation in TrValD.highdimensional semantic space.
In ACM SIGIR2004 Workshop on Mathematical/Formal Methodsin Information Retrieval.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: vector space models of seman-tics.
J. Artif.
Int.
Res., 37(1):141?188.73
