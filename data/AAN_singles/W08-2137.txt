CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253?257Manchester, August 2008Dependency Tree-based SRL with Proper Pruning and ExtensiveFeature EngineeringHongling Wang    Honglin Wang   Guodong Zhou   Qiaoming ZhuJiangSu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and Technology,Soochow University, Suzhou, China 215006{redleaf, 064227065055,gdzhou, qmzhu}@suda.edu.cnAbstractThis paper proposes a dependency tree-based SRL system with proper pruning andextensive feature engineering.
Officialevaluation on the CoNLL 2008 shared taskshows that our system achieves 76.19 in la-beled macro F1 for the overall task, 84.56in labeled attachment score for syntacticdependencies, and 67.12 in labeled F1 forsemantic dependencies on combined testset, using the standalone MaltParser.
Be-sides, this paper also presents our unofficialsystem by 1) applying a new effectivepruning algorithm; 2) including additionalfeatures; and 3) adopting a better depend-ency parser, MSTParser.
Unofficial evalua-tion on the shared task shows that our sys-tem achieves 82.53 in labeled macro F1,86.39 in labeled attachment score, and78.64 in labeled F1, using MSTParser oncombined test set.
This suggests that properpruning and extensive feature engineeringcontributes much in dependency tree-basedSRL.1 IntroductionAlthough CoNLL 2008 shared task mainlyevaluates joint learning of syntactic and semanticparsing, we focus on dependency tree-based se-mantic role labeling (SRL).
SRL refers to labelthe semantic roles of predicates (either verbs ornouns) in a sentence.
Most of previous SRL sys-tems (Gildea and Jurafsky, 2002; Gildea andPalmer, 2002; Punyakanok et al, 2005; Pradhan?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.et al, 2004, 2005) work on constituent structuretrees and has shown to achieve remarkable re-sults.
For example, Punyakanok et al (2005)achieved the best performance in the CoNLL2005 shared task with 79.44 in F-measure on theWSJ test set and 77.92 on the combined test set(WSJ +Brown).With rapid development of dependency pars-ing in the last few years, more and more re-searchers turn to dependency tree-based SRLwith hope to advance SRL from viewpoint ofdependency parsing.
Hacioglu (2004) pioneeredthis work by formulating SRL as a classificationproblem of mapping various dependency rela-tions into semantic roles.
Compared with previ-ous researches on constituent structure tree-basedSRL which adopts constituents as labeling units,dependency tree-based SRL adopts dependencyrelations as labeling units.
Due to the differencebetween constituent structure trees and depend-ency trees, their feature spaces are expected to besomewhat different.In the CoNLL 2008 shared task, we extend theframework by Hacioglu (2004) with maximumentropy as our classifier.
For evaluation, we willmainly report our official SRL performance us-ing MaltParser (Nivre and Nilsson, 2005).
Be-sides, we will also present our unofficial systemby 1) applying a new effective pruning algorithm;2) including additional features; and 3) adoptinga better dependency parser, MSTParser (McDon-ald, 2005).In the remainder of this paper, we will brieflydescribe our system architecture, present variousfeatures used by our models and report the per-formance on CoNLL 2008 shared task (both offi-cial and unofficial).2532 System DescriptionIn CoNLL 2008 shared task, we adopt a standardthree-stage process for SRL: pruning, argumentidentification and argument classification.
Tomodel the difference between verb and nounpredicates, we carry out separate training andtesting for verb and noun predicates respectively.In addition, we adopt OpenNLP maximum en-tropy package 1  in argument identification andclassification.2.1 Predicate identificationMost of Previous SRL systems only considergiven predicates.
However, predicates are notgiven in CoNLL 2008 shared task and requiredto be determined automatically by the system.Therefore, the first step of the shared task is toidentify the verb and noun predicates in a sen-tence.
Due to time limitation, a simple algorithmis developed to identify noun and verb predicates:1) For the WSJ corpus, we simply adopt theannotations provided by PropBank andNomBank.
That is, we only consider the verband noun predicates annotated in PropBankand NomBank respectively.2) For the Brown corpus, verb predicates areidentified simply according to its POS tagand noun predicates are determined using asimple method that only those nouns whichcan also be used as verbs are identified.
Toachieve this goal, an English lexicon of about56K word is applied to identify noun predi-cates.Evaluation on the test set of CoNLL 2008shared task shows that our simple predicate iden-tification algorithm achieves the accuracies of98.6% and 92.7 in the WSJ corpus for verb andnoun predicates respectively, with overall accu-racy of 95.5%, while it achieves the accuracies of73.5% and 43.1% in the Brown corpus for verband noun predicates respectively with overallaccuracy of 61.8%.
This means that the perform-ance of predicate identification in the Browncorpus is much lower than the one in the WSJcorpus.
This further suggests that much work isrequired to achieve reasonable predicate identifi-cation performance in future work.2.2 PreprocessingUsing the dependency relations returned by adependency parser (either MaltParser or1https://sourceforge.net/project/showfiles.php?group_id=5961MSTParser in this paper), we can construct cor-responding dependency tree for a given sentence.For example, Figure 1 shows the dependencytree of the sentence ?Meanwhile, overall evi-dence on the economy remains fairly clouded.
?.Here, W is composed of two parts: word and itsPOS tag with ?/?
as a separator while R means adependency relation and ARG represents a se-mantic role.In Hacioglu (2004), a simple pruning algo-rithm is applied to filter out unlikely dependencyrelation nodes in a dependency tree by onlykeeping the parent/children/grand-children of thepredicate, the siblings of the predicates, and thechildren/grandchildren of the siblings.
This paperextends the algorithm a little bit by including thenodes two more layers upward and downwardwith regard to the predicate?s parent, such as thepredicate?s grandparent, the grandparent?s chil-dren and the grandchildren?s children.
For theexample as shown in Figure 1, all the nodes inthe entire tree are kept.
Evaluation on the trainingset shows that our pruning algorithm signifi-cantly reduces the training instances by 76.9%.This is at expanse of wrongly pruning 1.0% se-mantic arguments for verb predicates.
However,this figure increases to 43.5% for noun predicatesdue to our later observation that about half ofsemantic arguments of noun predicates distrib-utes over ancestor nodes out of our consideration.This suggests that a specific pruning algorithm isnecessary for noun predicates to include moreancestor nodes.2.3 FeaturesSome of the features are borrowed from Ha-cioglu (2004) with some additional features mo-tivated by constituent structure tree-based SRL(Pradhan et al2005; Xue and Palmer, 2004).
Inthe following, we explain these features and giveexamples with regard to the dependency tree asshown in Figure 1.
We take the word evidence inFigure 1 as the predicate and the node ?on?
asthe node on focus.The following eight basic features are moti-vated from constituent structure tree-based SRL:1)  Predicate: predicate lemma.
(evidence)2) Predicate POS: POS of current predicate.
(NN)3)  Predicate Voice: Whether the predicate (verb)is realized as an active or passive construc-tion.
If the predicate is a noun, the value isnull and presented as ?_?.
( _ )254Figure 1.
Example of a dependency tree augmented with semantic rolesfor the given predicate evidence.4)  Relation type: the dependency relation typeof the current node.
(NMOD)5) Path: the chain of relations from current rela-tion node to the predicate.
(NMOD->SBJ)6) Sub-categorization: The relation type ofpredicate and the left-to-right chain of the re-lation label sequence of the predicate?s chil-dren.
(SBJ->NMOD-NMOD)7)  Head word: the head word in the relation,that is, the headword of the parent of the cur-rent node.
(evidence)8)  Position: the position of the headword of thecurrent node with respect to the predicate po-sition in the sentence, which can be before,after or equal.
(equal)Besides, we also include following additionalfeatures borrowed from Hacioglu (2004):1) Family membership: the relationship be-tween current node and the predicate node inthe family tree, such as parent, child, sibling.
(child)2)  Dependent word: the modifying word in therelation, that is, the word of current node.
(on)3) POS of headword: the POS tag of the head-word of current word.
(NN)4)  POS of dependent word: the POS tag of cur-rent word.
(IN)5)  POS pattern of predicate's children: theleft-to-right chain of the POS tag sequence ofthe predicate?s children.
(JJ-IN)6)  Relation pattern of predicate?s children:the left-to-right chain of the relation label se-quence of the predicate?s children.
(NMOD-NMOD)7)  POS pattern of predicate?s siblings: theleft-to-right chain of the POS tag sequence ofthe predicate?s siblings.
(RB-.-VBN-.
)8)  Relation pattern of predicate?s siblings: theleft-to-right chain of the relation label se-quence of the predicate?s siblings.
(TMP-P-PRD-P)3 System PerformanceAll  the training data are included in our system,which costs 70 minutes in training and 5 secondson testing on a PC platform with a Pentium D3.0G CPU and 2G Memory.
In particular, theargument identification stage filters out thosenodes whose probabilities of not being semanticarguments are more than 0.98 for verb and nounpredicates.LabeledMacro F1LabeledF1LASTest WSJ 78.39 70.41 85.50Test Brown 59.89 42.67 77.06Test WSJ+Brown 76.19 67.12 84.56Table 1: Official performance using MaltParser(with the SRL model trained and tested on theautomatic output of MaltParser)All the performance is returned on the test setusing the CoNLL 2008 evaluation scripteval08.pl provided by the organizers.
Table 1shows the official performance using MaltParser(with the SRL model trained and tested on theautomatic output of MaltParser provided by thetask organizers) as the dependency parser.
Itshows that our system performs well on the WSJcorpus and badly on the Brown corpus largelydue to bad performance on predicate identifica-tion.4 Post-evaluation SystemTo gain more insights into dependency tree-based SRL, we improve the system with a new255pruning algorithm and additional features, aftersubmitting our official results.4.1 Effective pruningOur new pruning algorithm is motivated by theone proposed by Xue and Palmer (2004), whichonly keeps those siblings to a node on the pathfrom current predicate to the root are included,for constituent structure tree-based SRL.
Ourpruning algorithm further cuts off the nodeswhich are not related with the predicate.
Besides,it filters out those nodes which are punctuationsor with ?symbol?
dependency relations.
Evalua-tion on the Brown corpus shows that our pruningalgorithm significantly reduces the training databy 75.5% at the expense of wrongly filtering out0.7% and 0.5% semantic arguments for verb andnoun predicates respectively.
This suggests thatour new pruning algorithm significantly performsbetter than the old one in our official system, es-pecially for the identification of noun predicates.Furthermore, the argument identification stagefilters out those nodes whose probabilities of notbeing semantic arguments are more than 0.90and 0.85 for verb and noun predicates respec-tively, since we that our original threshold of0.98 in the official system is too reserved.Finally, those rarely-occurred semantic roleswhich occur less than 200 in the training set arefiltered out and thus not considered in our system,such as A5, AA, C-A0, C-AM-ADV, R-A2 and SU.4.2 Extensive Feature EngineeringMotivated by constituent structure tree-basedSRL, two more combined features are consideredin our post-evaluation system:1) Predicate + Headword: (evidence + remain)2) Headword + Relation: (remain + Root)In order to better evaluate the contribution ofvarious additional feature, we build a baselinesystem using hand-corrected dependency rela-tions and the eight basic features, motivated byconstituent structure tree-based SRL, as de-scribed in Section 2.3.
Table 2 shows the effectof various additional features by adding one in-dividually to the baseline system.
It shows thatthe feature of dependent word is most useful,which improves the labeled F1 score from81.38% to 84.84%.
It also shows that the twofeatures about predicate?s sibling deteriorate theperformance.
Therefore, we delete these two fea-tures from remaining experiments.
Although thecombined feature of ?predicate+head word?
isuseful in constituent structure tree-based SRL, itslightly decrease the performance in dependencytree-based SRL.
For convenience, we include itin our system.P R F1Baseline 84.31 78.64 81.38+ Family membership 84.70 78.87 81.68+ Dependent word  86.74 83.01 84.84+ POS of headword 84.44 78.55 81.38+ POS of dependentword84.42 78.33 81.47+ POS pattern ofpredicate's children84.35 78.73 81.47+ Relation pattern ofpredicate?s children84.75 78.97 81.76+ Relation pattern ofpredicate?s siblings84.29 78.52 81.30+ POS pattern ofpredicate?s siblings83.75 78.32 80.95+ Predicate  +  Head-word83.30 78.94 81.30+Headword + Relation 84.66 79.37 81.93Table 2: Effects of various additional features4.3 Best performanceTable 3 shows our system performance after ap-plying above effective pruning strategy and addi-tional features using the default MaltParser.
Ta-ble 3 also reports our performance using thestate-of-the-art MSTParser.
To show the impactof predicate identification in dependency tree-based SRL, Table 4 report the performance ongold predicate identification, i.e.
only using an-notated predicates in the corpora.Comparison of Table 1 and Table 3 using theMaltParser shows that our new extension witheffective pruning and extensive engineering sig-nificantly improves the performance.
It alsoshows that MSTParser-based SRL performsslightly better than MaltParser-based one, muchless than the performance difference on depend-ency parsing between them.
This suggests thatsuch difference between these two state-of-the-art dependency parsers does not much affect cor-responding SRL systems.
This is also confirmedby the results in Table 4.Comparison of Table 3 and Table 4 in labeledF1 on the Brown test data shows that the systemwith gold predicate identification significantlyoutperforms the one with automatic predicateidentification using our simple algorithm byabout 22 in labeled F1.
This suggests that theperformance of predicate identification is criticalto SRL.256MSTParser MaltParserLabeled MacroF1Labeled F1 LAS Labeled MacroF1Labeled F1 LASTest WSJ 84.50 81.95 87.01 83.69 81.82 85.50Test Brown 67.61 53.69 81.46 65.09 53.03 77.06TestWSJ+Brown 82.53 78.64 86.39 81.52 78.45 84.56Table 3: Unofficial performance using MSTParser and MaltParserwith predicates automatically identifiedMSTParser MaltParserLabeled MacroF1Labeled F1 LAS Labeled MacroF1Labeled F1 LASTest WSJ 84.75 82.45 87.01 84.04 82.52 85.50Test Brown 78.31 75.07 81.46 75.72 74.28 77.06TestWSJ+Brown 84.05 81.66 86.39 83.13 81.64 84.56Table 4: Unofficial performance using MSTParser and MaltParser with gold predicate identification5 ConclusionsThis paper presents a dependency tree-basedSRL system by proper pruning and extensivefeature engineering.
Evaluation on the CoNLL2008 shared task shows that proper pruning andextensive feature engineering contributes much.It also shows that SRL heavily depends on theperformance of predicate identification.In future work, we will explore better ways inpredicate identification.
In addition, we will ex-plore more on dependency parsing and furtherjoint learning on syntactic and semantic parsing.AcknowledgmentThis research is supported by Project 60673041under the National Natural Science Foundationof China and Project 2006AA01Z147 under the?863?
National High-Tech Research and Devel-opment of China.ReferencesGildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28:3, pages 245-288.Gildea, Daniel and Martha Palmer.
2002.
The Neces-sity of Syntactic Parsing for Predicate ArgumentRecognition.
In Proceedings of the 40th  Associa-tion for Computational Linguistics,  2002.Hacioglu, Kadri.
2004.
Semantic Role Labeling UsingDependency Trees.
In Proceedings of the Interna-tional Conference on Computational Linguistics(COLING).
2004.McDonald, Ryan, Fernando Pereira, Kiril Ribarov,Jan Haji?.
2005.
Non-Projective Dependency Pars-ing using Spanning Tree Algorithms.
In the pro-ceedings of Human Language Technology Confer-ence and Conference on Empirical Methods inNatural Language Processing, 2005Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In Proceedingsof the 12th Conference on Computational NaturalLanguage Learning (CoNLL-2008).Nivre, Joakim and Jens Nilsson.
2005.
Pseudo-Projective Dependency Parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics, pp.
99-106, 2005Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,James H. Martin, Dan Jurafsky.
2004.
ShallowSemantic Parsing Using Support Vector Machines.In Proceedings of (HLT-NAACL-2004), 2004.Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,James H. Martin, Dan Jurafsky.
2005.
Semanticrole labeling using different syntactic views.
InProceedings of the 43rd  Association for Computa-tional Linguistics (ACL-2005), 2005.Punyakanok, Vasin, Peter Koomen, Dan Roth, andWen-tau Yih.
2005.
Generalized inference withmultiple semantic role labeling systems.
In Pro-ceedings of 9th Conference on ComputationalNatural Language Learning (CoNLL-2005).2005Xue, Nianwen and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), 2004.257
