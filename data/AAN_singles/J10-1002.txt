A Graph-Theoretic Framework forSemantic DistanceVivian Tsang?University of TorontoSuzanne Stevenson?
?University of TorontoMany NLP applications entail that texts are classified based on their semantic distance (howsimilar or different the texts are).
For example, comparing the text of a new document to that ofdocuments of known topics can help identify the topic of the new text.
Typically, a distributionaldistance is used to capture the implicit semantic distance between two pieces of text.
However,such approaches do not take into account the semantic relations between words.
In this article, weintroduce an alternative method of measuring the semantic distance between texts that integratesdistributional information and ontological knowledge within a network flow formalism.
We firstrepresent each text as a collection of frequency-weighted concepts within an ontology.
We thenmake use of a network flow method which provides an efficient way of explicitly measuring thefrequency-weighted ontological distance between the concepts across two texts.
We evaluate ourmethod in a variety of NLP tasks, and find that it performs well on two of three tasks.
We developa new measure of semantic coherence that enables us to account for the performance differenceacross the three data sets, shedding light on the properties of a data set that lends itself well toour method.1.
IntroductionMany natural language tasks can be cast as a problem of comparing texts in terms oftheir semantic distance.
For example, given a suitable text distance measure, documentclassification can be performed by comparing the text of a new document to the text ofvarious documents whose topics are known.
The new document is then labelled withthe topic of the document whose text is most similar to it.
In general, the texts to becompared may be full documents, as in this example, or may be portions of documents,or even collections of documents.
Using text comparison to perform semantic classifi-cation has been adopted in a variety of natural language processing (NLP) tasks, fromdocument classification (Scott and Matwin 1998; Rennie 2001; Al-Mubaid and Umair2006), to prepositional phrase attachment (Pantel and Lin 2000), to spelling correction(Budanitsky and Hirst 2001).?
Department of Computer Science, University of Toronto, 6 King?s College Road, Toronto, Ontario M5S3G4, Canada.
E-mail: vyctsang@cs.toronto.edu.??
Department of Computer Science, University of Toronto, 6 King?s College Road, Toronto, Ontario M5S3G4, Canada.
E-mail: suzanne@cs.toronto.edu.Submission received: 16 December 2007; revised submission received: 18 June 2008; accepted for publication:20 August 2008.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 1Distributional methods for semantic distance are widely used and highly successfulin comparing texts that are represented as bags of words with associated frequenciesof occurrence (Lee 2001; Weeds, Weir, and McCarthy 2004; Pedersen, Banerjee, andPatwardhan 2005).
In document classification, for example, the text of a document maybe represented as a word frequency vector, which is compared using a distributionaldistance measure to each of the word frequency vectors of the texts of the documentsof known topics.
In this way, distributional distance between word vectors captures thesemantic distance between two texts that is implicitly encoded in the set of words usedin each.Semantic distance can also be measured more explicitly, by using the relations inan ontology as the direct encoding of semantic association.
However, such approacheshave generally been limited to calculating the distance between two individual con-cepts, rather than capturing the distance between two sets of concepts correspondingto two texts.
Numerous measures have been proposed, for example, for capturingthe distance between two concepts in WordNet, typically relying on the synonymy(synset) and hyponymy (is-a) relations (Wu and Palmer 1994; Resnik 1995; Jiang andConrath 1997, among others).
Using such an ontological measure to compare two texts(collections of words instead of single words) might involve mapping each word of atext to its appropriate concept(s) in the ontology, and then calculating the aggregatedistance between the two resulting sets of concepts across the ontological relations.For example, one might calculate the semantic distance between the two texts as theaverage, minimum, maximum, or summed ontological distance between the individualelements of the two sets of concepts (Corley and Mihalcea 2005).Observe that each of these approaches to text comparison?distributional andontological?encodes information not contained in the other.
Distributional distancecaptures important information about frequency of occurrence of the words that consti-tute the target text, whereas ontological distance captures essential semantic knowledgethat has been encoded in the relations of an ontology.
In response, previous work hasattempted to combine distributional and ontological information in computing seman-tic distance.
For example, researchers have developed measures of semantic distancebetween texts that apply distributional distances to concept vectors of frequencies ratherthan to word vectors (McCarthy 2000; Mohammad and Hirst 2006).
However, these ap-proaches onlymake pairwise comparisions between the elements of the concept vectors,and do not take into account the important ontological relations among the concepts.
Inorder to capture such relations, other methods have instead integrated distributionalinformation into an ontological method.
However, such approaches have heretoforebeen limited to measuring distance between two individual concepts.
For example,some ontological measures use corpus frequencies of words to yield concept weightsthat are taken into account in measuring the distance between two concepts (Resnik1995; Jiang and Conrath 1997).
What has been missing is an approach to semanticdistance between two texts?two sets of words?that can truly integrate distributionaland ontological (relational) information, drawing more fully on their complementaryadvantages for text comparison.In this article, we describe a new graph-based distance measure that achieves thedesired integration of distributional and ontological factors in measuring semanticdistance between two sets of concepts (mapped from two texts).
An ontology is treatedas a graph in the usual manner, in which the concepts are nodes and the relations areedges.
A text is represented as a subgraph of the ontology, by mapping the words inthe text into their corresponding concepts, which are weighted according to the wordfrequencies.
We call the resulting set of frequency-weighted concepts a semantic profile.32Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceBy exploiting the relational structure of the ontology, we can explicitly measure theontological distance over the paths between two profiles.
Using the frequencies on theconcept nodes, we weight these paths according to the frequency distribution of wordsin the two texts.
The resulting calculation yields a frequency-weighted ontologicaldistance between the two sets of concepts.
Thus, we view a text not as a set of items tobe compared individually to those in another set (with those individual distances thensomehow combined, e.g., as in Corley and Mihalcea [2005]), but rather as a distributionof ?mass?
within a graph that encodes the semantic relations across the two sets, anduse a weighted graph-based approach that captures the aggregate distance between thetwo frequency masses.To our knowledge, this is the first method to integrate ontological and distributionalinformation in the graphical calculation of text distance.
This article describes the useof the new measure in several different types of NLP text comparison tasks, in order toexplore the situations in which such an approach can be effective.
Given the noveltyof the approach, the task-based evaluation is not intended as the last word on theusefulness of the method, but rather as a first suite of experiments across different typesof text comparison tasks to illuminate some of the strengths and weaknesses of suchan approach to text distance.
We thus analyze the results in detail to identify futuredirections for further illuminating when and to what extent the methodmight be useful.The analysis reveals that our method is not consistently successful across oursample tasks.
We hypothesize that, because ontological relations play an integral rolein our semantic distance measure, the measure is less effective when the semanticprofile for a text (the set of corresponding concepts) lacks semantic coherence.
Otherwork has explored ways to measure the semantic coherence of a set of concepts interms of their connectedness within an ontology (Gurevych et al 2003).
Because asemantic profile in our work includes both ontological (relational) and distributional(frequency) knowledge, we require a measure of semantic coherence that takes bothinto account.
We develop a novel measure of semantic coherence called profile densitythat captures both the ontological and distributional coherence of a set of frequency-weighted concepts, and apply it to the data sets used in the different tasks to betterunderstand the performance of our semantic distance measure.Our distance measure is cast as a graphical text comparison task within a networkflow framework as described in Section 2.
In Section 3, we give an overview of ourexploration of the method on three types of text comparison problems.
The followingthree sections present experimental results and analysis of applying our method to thevarious tasks: verb alternation detection (Section 4), name disambiguation (Section 5),and document classification (Section 6).
In Section 7, we describe our profile densitymeasure and use it to analyze the properties of the data sets that lead to the performancedifferential across the tasks.
We conclude the paper with a description of related workin text comparison and graph-theoretic NLP approaches (Section 8) and a discussion ofsome future directions for our research (Section 9).2.
The Network Flow MethodAs noted previously, we treat an ontology as a graph and represent a text as a semanticprofile?a collection of nodes in the graph (concepts in the ontology), each having aweight (its frequency).
For example, in Figure 1, a small text consisting of the words{cheese, wheat}, with frequencies of 4 and 10, respectively, is represented as a smallweighted subgraph in an ontology by uniformly distributing the word frequenciesamong the associated concepts.
In this way, a text is a weighted subgraph within a33Computational Linguistics Volume 36, Number 1Figure 1A small text represented as a collection of weighted nodes in a fragment of WordNet.larger graph (with the thickness of the boxes in the figure indicating weight), and twosuch weighted subgraphs are connected via a set of paths in the graph.Our goal is to measure the distance between two subgraphs (representing twotexts to be compared), taking into account both the ontological distance between thecomponent concepts and their frequency distributions.
To achieve this, we measure theamount of ?effort?
required to transform one profile to match the other graphically:The more similar they are, the less effort it takes to transform one into the other.
(Thisview is similar to that motivating the use of ?earth mover?s distance?
in computervision [Levina and Bickel 2001].)
In Section 2.1, we first give the intuitive motivationfor the approach in terms of the properties of semantic distance that we want to cap-ture by considering transport effort.
We then present the mathematical formulation ofour graph-based method as a minimum cost flow (MCF) problem in Section 2.2, anddescribe the formulation of our task within this network flow framework in Section 2.3.In Section 2.4, we return to the properties we identify in Section 2.1 to explain how theyare reflected in the MCF formulation.2.1 An Intuitive OverviewIn Figure 2(a), we show a diagrammatic representation of an ontology (the large opentriangle) with two profiles, one indicated with filled squares and the other with filledtriangles.
The location of a filled shape indicates the location of a profile concept in theontology, and its size indicates its frequency within the profile.
We omit edges betweenthe nodes to simplify the diagram, but note that we assume we have a hierarchical, con-nected ontology; hyponymy links are sufficient.
Our goal is to calculate the similaritybetween the two profiles by determining howmuch effort is required to transport, alongthe ontological links, the frequency mass from all of the squares to ?fill?
the availablespace in the triangles.
The amount of mass to move and the amount of space availableare indicated by the sizes of the squares and triangles, respectively.
The degree of effortrequired to transport one to the other indicates the degree of semantic distance.34Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFigure 2Two subgraphs (one represented by squares, the other, triangles) with varying degrees ofoverlap and, therefore, similarity within an ontology.
Figure (b) differs from Figure (a) in termsof the ontological distance between the square and the triangle clusters.
Figure (c) differs fromFigure (a) in terms of the size of the individual squares.The transport effort is determined by both the amount of mass to move and thegraphical distance over which it must travel.
First consider graphical (ontological)distance between the profiles.
Assume the calculated distance between the two profilesin Figure 2(a) is d. In Figure 2(b), the triangle profile is exactly the same.
By contrast,although the square profile has the same internal properties (same frequency distribu-tion and graphical structure), its location is further from the triangles.
Because the twoprofiles occupymore distant portions of the ontological space, they are less semanticallysimilar than in Figure 2(a).
As desired, the extra ontological distance over which thesquare frequency mass must be transported to the triangles will cause the calculateddistance in Figure 2(b) to be larger than d.Next consider the effect of varying the frequency distribution over the profile nodes.Again, in Figure 2(c), the triangle profile is exactly the same as in Figure 2(a).
However,whereas the nodes of the square profile in Figure 2(c) are in the same locations as inFigure 2(a), their distributional properties are different.
The bulk of the frequency distri-bution is now shifted closer to the nodes of the triangle profile.
Because the two profileshave more distributional weight located closer within the ontology, this indicates thatthe semantic space they occupy is more similar than in Figure 2(a).
Correspondingly,because much of the mass of the square profile needs to travel less far to fill the space ofthe triangle nodes, the calculated distance in Figure 2(c) will be less than d.35Computational Linguistics Volume 36, Number 1It is worth noting explicitly that this notion of semantic distance as transport effortof concept frequency over the relations (edges) of an ontology differs significantly froman approach to semantic distance that utilizes concept vectors of frequency.
By cruciallyutilizing the relations between concepts in calculating semantic distance, our approachcan determine the distance between texts that use related but non-equivalent concepts.For example, our measure will find greater similarity between a text that discussesmilk and one that discusses cheese than between one that discusses milk and one thatdiscusses bread.
A vector distance would find each of these equally dissimilar, becausethere are no concepts in common, and there is no way to relate milk to cheese.1The intuitive examples in Figure 2 show that calculating semantic distance astransport effort captures in a well-motivated way both the ontological distance betweenthe profiles and their weighting by the distributional amounts of the concept nodes.
Inthe next subsection, we describe a mathematical formulation that captures the relevantproperties of our problem in a network flow framework.
Network flow methods areoften used in computer science for modelling such transport effort, for example, incommunication or transportation networks.2.2 Minimum Cost FlowOur intuitive transport effort examples above can be viewed as a supply?demandproblem, in which we find the minimum cost flow (MCF) from the supply profile to thedemand profile to meet the requirements of the latter.
Mathematically, let G = (N,E) bea connected graph representing an ontology, where N is the set of nodes representingthe individual concepts, and E is the set of edges representing the relations betweenthe concepts.
(Most ontologies are connected; in the case of a forest, adding an arbi-trary root node yields a connected graph.)
Each edge has a cost c : E ?
R, which isthe ontological distance of the edge.
Each node i ?
N is associated with a value b(i)such that b : N ?
R indicates its available supply (b(i) > 0), its demand (b(i) < 0), orneither (b(i) = 0).
The goal is to find a flow from supply nodes to demand nodes thatsatisfies the supply/demand constraints of each node and minimizes the overall ?trans-port cost.
?First, we have to define a function to describe the flow entering i via an incomingedge (h, i) and exiting i via an outgoing edge (i, j).
Let INi be the set of edges (h, i)with a flow entering node i; similarly, let OUTi be the set of edges (i, j) with a flowexiting node i.
Then, the flow entering and exiting node i is captured by x : E ?
R suchthat we can observe the combined incoming flow,?
(h,i)?INi x(h, i), from the enteringedges INi, as well as the combined outgoing flow,?
(i,j)?OUTi x(i, j), via the exiting edgesOUTi (see Figure 3).
A valid flow, x, must be found such that the net flow at eachnode?the difference between its exiting flow and its entering flow?equals its specifiedsupply or demand constraints.
For example, in Figure 2 where the squares representthe supply and the triangles represent the demand, a solution for x would allow us totransport all the weight at the squares to fill the triangles, via a set of routes connectingthem.1 Techniques such as SVD or LSA could be applied to the concept vectors, as with word vectors, yieldingpotential relations through unnamed concepts (e.g., Landauer and Dumias 1997).
Note, however, thatsuch methods are dependent on the usages of the concepts implicitly encoding such connections,whereas an ontology-based method draws on a knowledge base that explicitly encodes the relationsregardless of the particular usages of the concepts.36Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFigure 3An illustration of flow entering and exiting node i.Formally, the MCF problem can be stated as follows (from Chva?tal 1983):Minimize z(x ) =?
(i,j)?Ec(i, j) ?
x(i, j) (1)subject to?
(i,j)?OUTix(i, j)??
(h,i)?INix(h, i) = b(i),?i ?
N (2)and x(i, j) ?
0,?
(i, j) ?
E (3)The constraint specified by Equation (2) ensures that the difference between the flowentering and exiting each node i matches its supply or demand b(i) exactly.
The nextconstraint, Equation (3), ensures that the flow is transported from the supply to thedemand but not in the opposite direction.
The calculation of z in Equation (1) (which issubject to these constraints) multiplies the amount of flow travelling along each edge,x(i, j), by the transportation cost of using that edge, c(i, j).
Taking the summation over alledges of the product c(i, j) ?
x(i, j) yields the desired transport effort of using the supplyto fill the demand.22.3 Semantic Distance as MCFTo cast our text comparison task into this framework, we first represent each text as asemantic profile in an ontology.
The profile of one text is chosen as the supply (S) and theother as the demand (D); our distance measure is symmetric, so this choice is arbitrary.In our examples in Section 2.1, the square profile was seen as the supply and the triangle2 We cast our text comparison problem as an uncapacitated minimum-cost flow problem, i.e., there is noupperbound constraint placed on the amount of flow along each edge (see Equation (3)).
Unlike acapacitated version of MCF, which is NP-complete (Garey and Johnson 1979), our problem is tractableand can be solved in polynomial time.37Computational Linguistics Volume 36, Number 1profile as the demand.
The concept frequencies of the profiles are normalized, so thatthe total supply equals the total demand.The cost of the routes between nodes is determined by a semantic distance measuredefined over the nodes in the ontology?that is, a measure of individual concept-to-concept distance.
A relation (such as hyponymy) between two concepts i and j isrepresented by an edge (i, j), and the cost c on the edge (i, j) can be defined as theconcept-to-concept distance between i and j.
For simplicity in this article, we use edgedistance as our concept-to-concept distance measure c; that is, each edge (i, j) has a costof 1, and the distance between any two concepts is the number of edges separatingthem.3Next, we must determine the value of b(i) at each concept node i.
In the simplecase, i occurs in only one profile or the other.
If i ?
S, b(i) is set to the normalized sup-ply frequency, fS(i).
If i ?
D, b(i) is set to the negative of the normalized demandfrequency, ?fD(i), since demand is indicated by a value less than zero.
However, i maybe part of both the supply and demand profiles, and then b(i) must be set to the netsupply/demand at node i.
Thus we have:b(i) = fS(i)?
fD(i) (4)For example, if the supply profile contains a node car with frequency of 0.25, and thesame node in the demand profile has a frequency of 0.7, then b(car) is ?0.45.
In otherwords, the node car has a net demand of 0.45.Recall that our goal is to transport all the supply to meet the demand; the keystep is to determine the optimal routes between S and D such that the constraints inEquation (2) and Equation (3) are satisfied.
The total distance of the routes, or theMCF?z(x ) in Equation (1)?is the distance between the two semantic profiles.2.4 Ontological and Distributional Factors in MCFTo see how the factors of ontological distance and frequency distribution play out inthe MCF formulation, let?s return to our square and triangle profile example.
Considera hypothetical zoomed-in area of the earlier diagram in Figure 2(a), shown in Figure 4.Here we assume that the square nodes have a net supply (b(i) > 0) and the trianglenodes have a net demand (b(i) < 0).4 The size of the square and triangle nodes inthe figure indicates |b(i)|?i.e., the relative supply/demand, respectively.
The circlesindicate nodes with neither supply nor demand constraints?i.e., b(i) = 0.
Each arrowfrom node i to node j indicates the source and destination for transported flow from asquare node to a triangle.
The length of an arrow represents the ontological distance,c(i, j), and the width indicates the amount of flow, x(i, j).
Note that the mass at therightmost square in the figure has to be distributed over the two triangles, and themass at the leftmost square is transported over a path with one edge (as indicated by3 Some semantic distances, such as those of Lin (1998) and Resnik (1995), do not directly use theunderlying graph structure of the ontology in calculating the distance between two concepts.
Using thistype of distance in our MCF framework requires an extra graph transformation step; see Tsang andStevenson (2006) for more details.4 Earlier we made the simplifying assumption that square nodes were the supply profile and trianglenodes the demand profile.
We have now seen that a node can belong to both profiles, and itscharacterization more accurately is stated in terms of net supply/demand.
Thus, for example, a squarenode may belong to just the supply profile or to both the supply and demand profile; the defining factoris that it has a net supply.38Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFigure 4An example of transporting the weights at the square nodes (supply nodes) to the triangle nodes(demand nodes).
The circle nodes have zero supply/demand requirement.the arrow nearby) instead of a path with three edges (with two circle nodes on the path).The aggregated length and width of the three arrows corresponds to the minimum costflow, i.e., the semantic distance between the profiles represented by the squares andtriangles.Both the ontological distance between nodes and the node weights are importantin determining the minimum cost flow.
The role of ontological information in the MCFformulation is clear.
If the squares were further away from the triangles in the ontologyin Figure 4?that is, if more edges separated the squares and the triangles?the sets ofconcepts they represent would be less semantically similar.
The length of the arrows(representing c(i, j)) would be greater, and the resulting MCF would be larger, reflectingthe greater semantic distance between the profiles.
Distributional information in thismethod is equally critical to the distance calculation, because it determines the amountof supply/demand at each node.
If the squares in Figure 4 were more uniformly sized,the two profiles would be more semantically similar because the weight would bedistributed more similarly across the ontological space.
In this case, less flow wouldhave to travel from the rightmost square to the leftmost triangle (i.e., the correspondingarrow would be thinner, representing x(i, j)), and the resulting MCF would therefore besmaller.
In short, our MCF method captures the desired property that both ontologicaldistance between profile nodes and their frequency distributions determine the overallsemantic distance between two profiles.3.
Evaluation: Experimental Tasks and MethodologyWe select three different NLP tasks that can be formulated as text classification problemsbased on semantic distance between the texts.
In each case, the texts to be comparedare treated as bags of words with associated frequencies.
The tasks are chosen to reflectdifferent types of relations used to extract the relevant words, to see if a varying amountof constraint on the words comprising a text influences the performance of our method.In verb alternation detection (Section 4), we identify which verbs, out of a set oftarget and filler verbs, allow a certain variation in the syntactic expression of their39Computational Linguistics Volume 36, Number 1underlying argument structure.
The task is achieved by comparing the set of headwords that occur with the verb in each of two different syntactic positions (e.g., subjectof intransitive and object of transitive).
In this task, the words that make up the textsto be compared have a particular syntactic relation to the verb under consideration.
Inproper name disambiguation (Section 5), we classify the sense of an ambiguous nameaccording to its local context.
This task is similar to word sense disambiguation (WSD),in picking the intended sense of a term, but also has similarities to topic identification,since the proper name delineates a particular domain of discourse.
In this task, wecompare the text constituting the ambiguous instance to texts representing each of theknown referents of the name.
Here, the words of a text are extracted from a smallwindow of occurrence around the target name token (25 words on each side), regardlessof the syntactic relations among the words.
For the known referents, the words fromthese windows are aggregated across a small set of labelled instances.
In documentclassification (Section 6), a text is classified into one of a restricted number of topiccategories.
The text to be classified consists of all the words in a document; for eachtopic, it is compared to a set of words corresponding to a small set of known documentsfor that topic.
The extracted words are not constrained by syntactic relation (as in verbalternation) or even by distance to a target element (as in name disambiguation).In each case, the resulting bag of words for a text must be mapped into a semanticprofile?a frequency-weighted set of concepts in an ontology.
Because all three of ourtasks involve general domain text, we use WordNet as our ontology (Fellbaum 1998).5(A domain-restricted task may motivate the use of a domain-specific ontology, suchas UMLS for comparing medical texts as in Bodenreider [2004].)
Because the nounhierarchy of the WordNet ontology is most developed, we restrict our semantic profilesto use only the nouns from the bag of words corresponding to a text: Any word inthe text that appears in the noun hierarchy of WordNet is included in the bag ofnouns.The bag of nouns with their associated frequencies must be mapped to the appro-priate concepts in WordNet.
Given the current state of unsupervised WSD, there isgenerally no attempt to disambiguate the words of a text when performing this kindof mapping?that is, there is no selection of the most appropriate concept or set ofconcepts to map the words to, given the context of their use.
The simplest methodis to distribute the frequency of each word uniformly to its corresponding concepts.For example, Ribas (1995) maps the word frequency to the most specific concept(s) forthe word, including all of the possible synsets for the word, but not their hypernyms.Resnik (1993) also distributes the word frequency uniformly, but does so across the mostspecific concept(s) and all of their hypernyms.
Other approaches, although still avoidingthe difficulties of WSD, do try to capture the overall semantic ?tendencies?
of the set ofwords.
Such methods estimate the appropriate probability distribution over a set ofconcepts to represent a given bag of nouns as a whole (Li and Abe 1998; Clark and Weir2002).
However, such techniques still start with a mapping of each word to all of itsimmediate concepts.5 There is disagreement over the suitability of treating WordNet as an ontology, rather than as a lexicalnetwork (Gangemi, Guarino, and Oltramari 2001; Hirst 2009).
However, the intention of the creators ofWordNet is apparently that its synsets correspond to concepts, and the relations between them includeboth ?conceptual-semantic and lexical relations?
(http://wordnet.princeton.edu/), qualifying it, undersome views, as a general domain ontology.
Although recognizing the limitations and difficulties of usinga primarily lexical resource as an ontology, we note that WordNet is standardly used as such incomputational linguistics, and so we adopt this use here.40Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFor all three of our tasks, we take the simple approach of mapping each nounindividually to its most specific concepts (not their hypernyms), uniformly dividingthe word frequency among them.
In verb alternation, we also experiment with thepossibility of finding the best set of frequency-weighted concepts for the full bag ofnouns (using the techniques of Li and Abe [1998] and Clark and Weir [2002]), to see ifthis affects the performance of our method.The precise classification experiment performed using these semantic profiles isdescribed in detail subsequently in the section for each task.
In each case, we comparethe performance of our MCF method on the semantic profiles to one or more purelydistributional methods using the original word frequency vectors.4.
Task 1: Verb Alternation DetectionVerb alternation refers to variations in the syntactic expression of verbal arguments.
If averb participates in an alternation, the same underlying semantic argument may appearin varying positions (slots) of the verb?s subcategorization frames.
For example, thefollowing sentences show that the argument undergoing the melting action can appearas the subject of an intransitive use of melt (1a) or as the object of a transitive use (1b).1a.
The chocolate melted.1b.
The cook melted the chocolate.This type of intranstive/transitive pairing is known as the causative alternation becauseof the explicit expression of the causer (the cook) in the transitive alternant.It has long been hypothesized that the semantics of a verb and its relations to itsarguments at least partially determine the syntactic expression of those arguments (seePinker [1989], among others).
Influential work by Levin (1993) showed that this rela-tionship could be exploited ?in reverse?
by using alternation behavior as an indicatorof the underlying semantics of a verb?specifically, that verbs undergoing the same setsof alternations form classes with similar semantics.
Computational linguists have builton this work by demonstrating that statistical cues to alternation behavior can be usedto automatically place verbs into semantic classes (Merlo and Stevenson 2001; Schulteim Walde 2006).Detection of verb alternation behavior can be cast as a text comparison problem(McCarthy 2000; Merlo and Stevenson 2001).
Consider an alternation such as thecausative illustrated in Example (1).
The set of nouns appearing as the subject of theintransitive (such as chocolate) have the same relation to the verb as the set of nounsappearing as the object of the transitive.
Because the verb places constraints on whatkinds of entities can be in that relation (here, things that are meltable), the two sets ofnouns should be similar.
Hence, to identify a particular alternation for a verb, the setof nouns in a certain slot of one of its subcategorization frames is compared to the setof nouns in the alternating slot for that semantic argument in another subcategorizationframe.For example, Merlo and Stevenson (2001) devise a simple lemma overlap scorethat counts the number of tokens appearing in both of the relevant syntactic slots.McCarthy (2000) instead compares two semantic profiles in WordNet that contain theconcepts corresponding to the nouns from the two argument positions.
In McCarthy?smethod, the profiles are first generalized to a set of higher level nodes in the hierarchy(starting with the method of Li and Abe [1998]); next, skew divergence is used to find41Computational Linguistics Volume 36, Number 1the distance between the resulting vectors of concepts.
Here we use our network flowmethod to directly compare the semantic profiles corresponding to the noun sets.
Ourmethod allows us to compare sets of weighted concepts as in McCarthy?s, but usinga distance method that applies within the ontology graph, rather than simply using adistributional distance measure over concept vectors.4.1 Experimental Set-upWe adopt the data set from an investigation of a semantic distance measure that wasa precursor to our network flow method (Tsang and Stevenson 2004).
The selectionof these verbs and extraction of their arguments are discussed in the following twosections; we then describe our evaluation methodology.4.1.1 Experimental Verbs.We evaluate our method on the causative alternation.
As notedpreviously, in this alternation the target syntactic slots for comparison are the subjectof the intransitive (Subj-Intrans) and the object of the transitive (Obj-Trans).
(These arethe positions of the chocolate in Examples (1a) and (1b), respectively.)
To identify verbsundergoing this alternation, we randomly selected verbs from among Levin classes thatare indicated to allow the causative alternation.
This allows us to test the ability of adistance measure to detect alternation behavior among verbs from a range of semanticclasses which may differ in other respects.We refer to the verbs that are expected to undergo the causative alternation ascausative verbs.
For comparison, we randomly selected an equal number of filler verbs,subject to the constraint that their Levin classes do not allow a causative alternation.
(Specifically, none of the classes containing a filler verb allows an alternation in whichthe same underlying argument appears in the Subj-Intrans slot as well as the Obj-Transslot.)
The full set of potential causative and filler verbs were filtered according to corpuscounts, as described next.4.1.2 Corpus Data and Argument Extraction.
We used a randomly selected 35M-wordportion of the British National Corpus (BNC; Burnard 2000).
The text was parsed usingthe RASP parser of Briscoe and Carroll (2002), and subcategorization frames wereextracted using the system of Briscoe and Carroll (1997).
Each subcategorization frameentry for a verb includes a list of the observed argument heads per slot along with theirfrequencies.
For each verb/slot pair, we thus extracted the set of nouns used in that slotalong with their frequency of occurrence.Verbs were filtered from the potential list of experimental items if they occurredless than 10 times in our corpus in either the transitive or intransitive frame.
The verbswere then divided into multiple frequency bands: high (at least 450 instances), medium(between 150 and 400 instances), and low (between 10 and 100 instances).
An equalnumber of verbs of each type (causative and filler) were randomly selected within eachband, yielding a total of 120 experimental verbs in balanced data sets of 60 items fordevelopment and 60 items for testing.
The development data was used in our earlierwork to select a profile-generation method for the test data (Tsang and Stevenson 2004).In our current work, we did not make any adjustments to our method based on resultson the development set (i.e., it was not used to set any parameters or select a particularimplementation approach).
Hence, we report the evaluation of our method on the fullset of 60 verbs in each of the data sets, as well as individually on the three frequencybands of 20 verbs each.
We refer here to the original ?development?
and ?test?
data setsas ?dataset1?
and ?dataset2?.42Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance4.1.3 Evaluation Methodology.
For each verb, we create a semantic profile for each ofthe Subj-Intrans and Obj-Trans slots.
We first take the argument heads with their fre-quencies from the appropriate slots in the extracted subcategorization frame for theverb.
We then map these words with their frequencies to the corresponding nodes inWordNet, as described in Section 3.
(We also consider here different profile generationmethods, discussed later in Section 4.2.2.)
We then calculate the network flow distancebetween the two semantic profiles for each verb, yielding a distance calculation forthat verb.
Recall that we expect verbs that participate in the alternation to have moresimilar semantic profiles corresponding to the Subj-Intrans and Obj-Trans nouns.
Forexample, a causative verb like melt, as in Examples (1a) and (1b), may have wordslike chocolate, sherbet, and glacier in the Subj-Intrans slot, and words like chocolate, butter,and bronze in the Obj-Trans slot.
In contrast, a non-causative verb like fry will typicallyhave more dissimilar sets of words that contribute to the two profiles (e.g., cook, wife,and chef in the Subj-Intrans slot, and egg, noodle, and onion in the Obj-Trans slot).
Wethus rank all the verbs by the distance calculation, and (as in McCarthy 2000) set athreshold to divide the verbs into causative (smaller distance values) and non-causative(larger distance values).
FollowingMcCarthy, we experimented with both the mean andmedian values as the threshold, but found little difference.
We report the results usingthemedian distance as the threshold, because this providedmore consistent results withour method.Because we label all verbs in our experiments as causative or non-causative, we useaccuracy as the performance measure.
Since we have balanced data sets, the randombaseline is 50%.
We compare our results as well to a number of distributional methods(as enumerated in the next section).
Given the small size of our data sets, a simplestatistical test on the resulting accuracies is not powerful enough to reveal differenceswhen the accuracies are close.
However, because the difference in methods is due tovariation in how they rank the experimental items, we perform a Wilcoxon signedrank test (Wilcoxon 1945) to determine when the rankings between two methods aresignificantly different, using a p value of .05.4.2 Results and AnalysisAs noted herein, we present results on two sets of data, and also examine the effect of us-ing alternative profile generation methods.
We compare our network flow distance (NF)to a number of other distance measures including probability distributional distancesgiven by Jensen-Shannon divergence (JS) and skew divergence (skew div) (Lee 2001),as well as the general vector distances of cosine, Manhattan distance, and Euclideandistance.4.2.1 Experimental Results.
On dataset1, our network flow distance performs better thanor as well as all other measures on the individual frequency bands, as shown in Table 1.On all verbs combined (the ?All?
column) the performance of our method is not thebest, although the Wilcoxon test shows no significant difference between the rankingsof NF and the best measure (Manhattan).
(The difference in rankings between NF andall other measures is significant.
)Interestingly, we find that the ?All Verbs?
performance of NF (and that of severalother methods) is indeed worse than the performance on the individual frequencybands.
We examined the distance values across the frequency bands to determine thecause for this pattern.
We found that low frequency verbs tend to have smaller distances43Computational Linguistics Volume 36, Number 1Table 1Accuracies on dataset1 by the network flow method (NF), cosine, Manhattan distance, Euclideandistance, skew divergence (skew div), and Jensen-Shannon divergence (JS).
Best accuracies ineach condition are shown in boldface.All Frequency Bands Avg ofVerbs High Medium Low BandsNF 0.60 0.70 0.70 0.70 0.70cosine 0.57 0.60 0.60 0.60 0.60Manhattan 0.63 0.70 0.70 0.70 0.70Euclidean 0.47 0.40 0.50 0.40 0.43skew div 0.57 0.60 0.60 0.50 0.57JS 0.60 0.70 0.60 0.70 0.67Table 2Accuracies on dataset2 by the network flow method (NF), cosine, Manhattan distance, Euclideandistance, skew divergence (skew div), and Jensen-Shannon divergence (JS).
Best accuracies ineach condition are shown in boldface.All Frequency Bands Avg ofVerbs High Medium Low BandsNF 0.67 0.60 0.80 0.60 0.67cosine 0.50 0.60 0.50 0.50 0.53Manhattan 0.63 0.60 0.80 0.60 0.67Euclidean 0.60 0.50 0.70 0.50 0.57skew div 0.63 0.60 0.80 0.60 0.67JS 0.70 0.60 0.80 0.60 0.67between the two slots and high frequency verbs tend to have larger distances.
This isdue to the fact that higher frequency verbs typically occur with a wider range of nouns,leading to a more dispersed semantic profile (i.e., a larger number of concepts).
As aresult, the best threshold for separating the alternating and non-alternating verbs differsacross the frequency bands, and the threshold for all verbs together lies in betweenthe thresholds for the high and low frequency bands.
When classifying all verbs, thefrequency effect may result in more false positives for low frequency verbs (which havegenerally smaller distance values), and more false negatives for high frequency verbs(which have generally larger distance values).
The column labelled ?Avg?
in Table 1shows the performance when averaging the results across the individual frequencybands.
For most methods, including ours, the ?Avg?
results are much better than whenconsidering all verbs together (the ?All?
column).Table 2 reports the performance on dataset2, which is similar to that on dataset1.Again, we find that our method is tied for the best performance in every conditionexcept for all verbs combined.
(Here we find that all four methods over .60 accuracyin the ?All?
condition have statistically indistinguishable rankings of the experimentalitems.)
On this data set, taking the average of the frequency bands does not helpperformance of our method compared to ?All,?
but neither does it hurt (and for mostmethods ?Avg?
does better or the same as ?All?).
We conclude that separating items byfrequency may be required to achieve robust results in this type of task.Although our method is tied for best in every condition except ?All,?
neitheris our method distinguished from several of the other distance measures.
Given the44Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceTable 3Average accuracies by the network flow method (NF), Manhattan distance (Man), skewdivergence (skew div), and Jensen-Shannon divergence (JS) on different profiles: original(?raw?
), Li and Abe, and Clark and Weir profiles.
Best accuracies in each condition are shown inboldface.raw Li and Abe Clark and WeirDataset1 Dataset2 Dataset1 Dataset2 Dataset1 Dataset2NF 0.70 0.67 0.50 0.67 0.73 0.70Manhattan 0.70 0.67 0.57 0.67 0.60 0.57skew div 0.57 0.67 0.53 0.67 0.68 0.60JS 0.67 0.67 0.63 0.67 0.63 0.53relatively small amounts of data per verb (with profiles averaging about 900 nodesin size), it is possible that the raw profiles suffer from a sparse data problem and arenot sufficiently capturing the conceptual similarities among alternating slots.
McCarthy(2000) addressed this issue by using a technique for generalizing concept nodes prior tocomparing profiles.
We explore this issue next.4.2.2 Comparing Different Profile Generation Methods.
Our experiments use semantic pro-files created directly from the word frequencies, as described earlier.
However, researchhas explored the possibility of generalizing this kind of ?raw?
data to a semantic profilethat more appropriately reflects the coherent concepts expressed in the original set ofweighted concept nodes.
This can be especially useful when creating semantic profilesfrom small amounts of data, given the noise introduced in the mapping of words toconcepts.6 To explore the effect of different profile generation methods on this task, weconsider here two approaches, that of Li and Abe (1998) and Clark and Weir (2002).Both these methods start with a semantic profile generated as described in Section 3and attempt to find the set of nodes in the ontology that appropriately generalize theconcepts in the ?raw?
profile.Table 3 compares the performance of the network flow distance with that of severalother measures on the original (?raw?)
profiles, the Li and Abe profiles, and the Clarkand Weir profiles.
Results are reported for the average of the individual frequencybands, since that produced the best results overall in our earlier experiments.
The resultsfor cosine and Euclidean distance are omitted, because they perform worse overall thanthe other measures.7The best results across both data sets are achieved by our network flow method onthe Clark andWeir profiles.
Considering the results across all profile types, the networkflow approach is most consistent, achieving the best (or tied for best) performancein but one condition (dataset1 with Li and Abe profiles).
The distributional methods6 Because we divide the frequency of a word uniformly among all the word?s concepts, with no attempt atdisambiguation or informed weighting, much noise is introduced.
Given the small amounts of data, thenoise may be sufficient to mislead our network flow method.7 Because these results use the approach of averaging results across the frequency bands, we cannot applythe Wilcoxon signed rank test to the rankings.
(The individual frequency bands have too few items forthe test to detect differences.)
On All Verbs combined (results not reported in this table), the rankings ofNF are different from all other methods on each combination of data set and profile generation approach,except in the single case of Manhattan and JS on dataset2 using Li and Abe to create the profiles.45Computational Linguistics Volume 36, Number 1(Manhattan, skew div, JS) in almost all cases perform worse on the generalized profilesthan on the ?raw?
profiles.
(The one exception is that skew divergence does better ondataset1 on the Clark and Weir profiles.
)Overall, then, it seems that raw data is likely best for a purely distributional method,but the Clark andWeir profiles enable the network flowmethod to outperform them byexploiting the graph structure of the ontology.
Indeed, when comparing our methodto the others on the Clark and Weir profiles for the individual frequency bands (notshown in the table), we find that much of our performance advantage comes on thelow frequency verbs.
This indicates that the combination of our method with a suitablegeneralization technique is especially important when dealing with sparse data.We examine the data further to discover why the Li and Abe profiles yield poorerperformance inmost cases on dataset1.We find that Li andAbe?s (1998) method tends togenerate profiles with more general concepts.
For example, when given an original setof concepts such as Edam, Brie, Sockeye, and Chinook, the method may produce a singlegeneral concept such as food instead of the two concepts cheese and salmon that capturethe two kinds of food that are indicated.
The loss of semantic information from usingoverly general concepts may produce the decrease in performance.For comparison, we also apply McCarthy?s (2000) method to our dataset2, andfind that it achieves only 0.60 on all verbs and 0.53 averaged over the three frequencybands.
Her method is especially poor on low frequency verbs (below chance at 0.40).We hypothesize that her method is less robust to low frequency counts because itmay overgeneralize the data by first applying Li and Abe?s (1998) method, and thengeneralizing the nodes even further.We see that although some amount of generalization of the semantic profiles isuseful in this task, overgeneralization may be harmful.
We leave it to future workto explore the interaction of our network flow method with different types of profilegeneration across various tasks.
Because the next two tasks we consider use largeramounts of data, we only experiment with raw profiles in these cases.5.
Task 2: Name DisambiguationInterest in the NLP problem of name disambiguation has increased as the growth ofthe World Wide Web has led to large numbers of ambiguous name references in on-line text.
For example, Web sites or documents containing the name John Edwards mayrefer to the U.S. presidential candidate for 2008, an NBA basketball player, or a Britishmedical geneticist.
An ambiguous name may be resolved by comparing its local textualcontext?the set of words it co-occurs with?with the local textual contexts of the namewhen its reference is known.
For example, the text surrounding the name John Edwardsin its various uses are very likely to include distinguishing words such as politician vs.game vs. research.
Many approaches have been proposed for resolving name ambiguityby using distributional methods over contextual information (Xu, Liu, and Gong 2003;Han, Zha, and Giles 2005; Pedersen, Purandare, and Kulkarni 2005).In this section, we present the application of our network flow distance measure to aname disambiguation task, and demonstrate the benefits of combining ontological anddistributional knowledge in this task.
The particular task we examine is one of ?pseudoname disambiguation,?
in which the texts containing matched pairs of different namesare extracted, and then the two different names are replaced by a single symbol, leadingto an ambiguous ?name?
across the two sets of texts.
The goal is to recover the correcttarget name in each instance.
For example, the names of two soccer players (Ronaldoand David Beckham) form one disambiguation task, and the names of an ethnic group46Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distanceand a diplomat (Tajik and Rolf Ekeus) form another.
This task was established byPedersen, Purandare, and Kulkarni (2005) to provide ?annotated?
experimental data(with each text indicating the correct name) without the need for expensive manualannotation.In Pedersen, Purandare, and Kulkarni (2005), an unsupervised method of namediscrimination through text clustering was used to address this task.
This is infeasiblefor a method like ours, in which each distance calculation requires access to an ontology.
(The worst-case complexity of clustering with our method is quadratic in the size of theontology used; a detailed discussion can be found in Tsang and Stevenson [2006].)
In-stead, we use a supervisedmethodology, but experiment with varying small amounts ofdata in a minimally supervised approach.
Although our method requires extra manualeffort in the form of data annotation for training, we find that the amount of annotateddata required is modest.5.1 Experimental Methodology5.1.1 Corpus Data.
We use Pedersen, Purandare, and Kulkarni?s (2005) data set, whichwas taken from the Agence France-Press English Service portion of the GigaWordEnglish corpus distributed by the Linguistic Data Consortium.
They extracted the localcontext of six pairs of names of varying confusability, including: the names of twosoccer players (Ronaldo and David Beckham); an ethnic group and a diplomat (Tajikand Rolf Ekeus); two companies (Microsoft and IBM); two politicians (Shimon Peresand Slobodan Milos?evic?
); a nation and a nationality (Jordan and Egyptian); and twocountries (France and Japan).
For each name instance, the extracted text consists of50 words (25 words to the left and to the right of the target name), with the targetname obfuscated.
For example, for the task of distinguishingDavid Beckham andRonaldo,the target name in each instance becomes David BeckhamRonaldo.
The original name ineach instance is retained only for evaluating the results (and for training, in the caseof our method, as described subsequently).
(Note that this approach to data creationavoids the use of manually annotated data for this experimental task, but in an actualapplication, manual annotation of truly ambiguous names would be necessary.)
Eachpair of names thus serves as one of six name disambiguation tasks.
Table 4 shows thenumber of instances per task (name pair).
The ?Majority?
column also indicates therelative frequency of the majority name in each pair, which we adopt as the baselineaccuracy.5.1.2 Classification Using the Network Flow Method.
As mentioned previously, we take asupervised approach, in which name instances are classified with the use of trainingTable 4The pairs to be identified, the raw frequencies, and the relative frequency of the majority name.Name 1 Count Name 2 Count Total MajorityRonaldo 1,700 David Beckham 752 2,452 0.69Tajik 3,002 Rolf Ekeus 1,071 4,073 0.74Microsoft 3,401 IBM 2,406 5,807 0.59Shimon Peres 7,686 Slobodan Milos?evic?
6,048 13,734 0.56Jordan 25,039 Egyptian 21,392 46,431 0.54Japan 116,379 France 110,435 226,814 0.5147Computational Linguistics Volume 36, Number 1data annotated by the original name in the instance.
To generate our training data, werandomly select a portion of the instances for each of the 12 names.
All the traininginstances for a name are used to form a single aggregate semantic profile, which servesas the gold-standard for that name.
The remaining instances serve as test data; foreach of these, we build an individual semantic profile.
All profiles are generated asdescribed in Section 3, namely, each frequency count for a word is distributed uniformlyamong the corresponding concepts in WordNet.
A gold-standard profile is constructedin exactly the same way except that its word frequency vector is created by aggregatingthe word counts from all the relevant training instances.
Note that there is nothingspecial about such a profile or how it is formed; it simply aggregates counts frommultiple contexts.8To classify a name instance, we measure the network-flow distance between theindividual profile of the ambiguous instance and each of the two gold-standard profilesfor that task.
The name whose gold-standard profile has the shortest distance to theinstance profile is the name assigned to the ambiguous instance.
For example, assumewe have a ?David BeckhamRonaldo?
instance to be classified.
We compare its profile toeach of the gold standard profiles for ?David Beckham?
and ?Ronaldo?
by measuringthe distance between each of the two pairs of profiles.
If the instance profile has ashorter distance to the profile for ?David Beckham?
than to that of ?Ronaldo,?
thenit is classified as ?David Beckham,?
otherwise as ?Ronaldo.
?5.1.3 Evaluation Methodology.
We use the accuracy of labelling all instances as our eval-uation measure.
To compare to prior results using F-measure, we report that in sometables.
Because we label all instances, accuracy and F-measure are equivalent in ourmethod, using 2rp/(r+ p) as the definition of F-measure.The random baseline for our task is the accuracy of labelling all instances with thepredominant name, as shown in the ?Majority?
column of Table 4.
Because we use thedata set of Pedersen, Purandare, and Kulkarni (2005), we compare our performance totheir distributional method (reporting their best results both with and without singularvalue decomposition).
Because their method is an unsupervised one, we also train andtest a supervised learner using distributional data (LIBSVM by Chang and Lin [2001]).For each set of training data, we remove stopwords and use the remaining words (withtheir frequencies) as input features for the SVM.
We then obtain the optimal parameters(i.e., optimal values for cost and gamma in LIBSVM) by using 10-fold cross-validationover the training data.
Finally, we perform classification on the test data using thoseparameters.
This enables us to compare our results to a purely distributional methodwith access to the same training data.Because our method is supervised, it is important to minimize the amount ofannotated data required to build the gold-standard profiles.
(Lengthy training time canalso be an issue for a supervised method, but here ?training?
is the straightforwardtask of building an aggregate semantic profile.)
Because it is unclear a priori whatamount of training data is sufficient, we experiment with several quantities.
We initiallyselect 200 random instances per pair of names, respecting the relative proportions ofthe two names overall.
(Two hundred instances constitute about 0.1?10% of the dataper pair of names.)
Subsequently, we decrease the quantity further, to one-half and one-quarter the original amount (100 and 50 instances, respectively) to observe how the8 In our later experiment in document classification, on a subset of our data, we tried a nearest neighborapproach to all training instances rather than aggregating them, but this did not perform as well.48Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceTable 5Network flow results using 200 training instances on the random samples and their averageperformance.Name Pair Random Samples Average of1 2 3 4 5 SamplesRonaldo/Beckham 0.78 0.83 0.76 0.79 0.84 0.80Tajik/Ekeus 0.98 0.98 0.97 0.96 0.98 0.97Microsoft/IBM 0.73 0.72 0.73 0.74 0.73 0.73Peres/Milos?evic?
0.96 0.96 0.97 0.96 0.97 0.96Jordan/Egyptian 0.79 0.78 0.78 0.77 0.76 0.77Japan/France 0.79 0.73 0.77 0.70 0.73 0.75Table 6Average results for the network flow (NF) results using 200 instances per gold-standard profile,SVM using 200 training vectors, and Ped05 and Ped05SVD (the best results without and withSVD, respectively).
All results are F-measure (the same as accuracy for our method and SVM).The weighted average is calculated based on the number of instances in each pair of names.
Thebest result for each name pair is indicated in boldface.Name Pair Majority Ped05 Ped05SVD SVM200 NF200Ronaldo/Beckham 0.69 0.73 0.65 0.85 0.80Tajik/Ekeus 0.74 0.96 0.89 0.90 0.97Microsoft/IBM 0.59 0.51 0.59 0.62 0.73Peres/Milos?evic?
0.56 0.97 0.94 0.90 0.96Jordan/Egyptian 0.54 0.59 0.62 0.72 0.77Japan/France 0.51 0.51 0.50 0.48 0.75Unweighted Average 0.61 0.71 0.70 0.75 0.84Weighted Average 0.53 0.55 0.55 0.55 0.77performance is influenced by the amount of data used to construct the gold standardprofiles.9 To reduce the impact of possible skewed sampling of training data, we repeatthe random sampling five times, with no overlap between the random samples.
Wereport the performance of each sample set as well as the average over the five samples.5.2 Results and Analysis5.2.1 Initial Experiments.
Table 5 shows the performance of our method over five randomsamples of 200 training instances per task.
Observe that the performance over the fiverounds varies very little (a maximum difference of 0.08, and most are much closer).This shows the robustness of our method to different make-ups of training data.
Table 6shows the average performance of our method, in comparison to the chance (majority)baseline, as well as the results produced by the unsupervised method of Pedersen,Purandare, and Kulkarni (2005) (with singular value decomposition [SVD] reported as9 We also experiment with 400 training instances to see whether increasing the amount of training datahelps.
The performance benefit is minimal: two tasks have the same average performance, three improveby 1%, and one by 2%, with an improvement in the average over all the tasks of 1.25%.
A paired t-testbetween the results on 400 and 200 training instances yields a high p value (p = 0.73), indicating that thedifferences between the two are statistically insignificant.49Computational Linguistics Volume 36, Number 1Ped05SVD, and without SVD as Ped05), and the supervised SVM on the same trainingdata as our method.
Observe that our method not only significantly outperforms therandom baseline, it is moreover the best performer among all the methods (pairedt-test, p < 0.05).There are cases for which Pedersen, Purandare, and Kulkarni?s (2005) methodshave at best chance performance (Microsoft/IBM and Japan/France).
The authors suggestthat these pairs of names arise in the context of news text in which there are ?no consis-tently strong discriminating features?
useful in the clustering algorithm.
(Interestingly,this is the case even with SVD, where words are grouped into a small number of un-named concepts.)
Even the SVM has difficulty with these pairs, also performing at justaround chance.
Yet our method performs well above chance for these pairs.
In general,SVM produces results that are little better on average than the unsupervised results inPedersen, Purandare, and Kulkarni (2005) (with some tasks performing better, and someworse).
This shows that the performance improvement from the network flow methoddoes not depend solely on access to training data.
Instead, it seems that the use ofontological relations in calculating distance can significantly enhance the discriminatorypower over simply using words.Note that there is one difference between the data used in the SVM and the networkflow experiments: The SVM is trained using all words as features, while only WordNetnoun concepts are used in the network flow experiments.
It is possible that using justnouns or a mapping of nouns to WordNet concepts could bring the performance of theSVM into line with our network flow measure.
We thus perform two replications of theSVM experiments, one using only nouns as features and one using noun concepts asfeatures (with the relevant frequencies as the feature values in both cases).
However,both of these approaches produce little to no improvement over the all-words resultsreported in Table 6.
We conclude that our network flow method is superior to, andmore consistent than, the purely distributional methods, and that this difference isattributable to the integration of distributional and ontological (relational) informationin our measure.5.2.2 Reducing the Amount of Training Data.
Because, in contrast to Pedersen, Purandare,and Kulkarni (2005), we use a supervised approach, we want to determine whether wecan reduce our dependence on training data.
Here, we report experiments using one-half (100 instances) and one-quarter (50 instances) of the training data used earlier.
Asbefore, we repeat the random sampling of the training instances five times in each case,and report the average performance here.Table 7 shows the network flow performance for 200, 100, and 50 training instances.Numerically, the results do not differ by much when the training data is reduced from200 to 100 instances, and a paired t-test finds the difference to be non-significant.
Theperformance drop is more pronounced in the 50-instance experiment, where every pairof names shows some drop in performance compared to 100 instances.
Here, a pairedt-test shows that the performance drop in the 50-instance experiment is statisticallysignificant (p = 0.04).
Despite this, we still outperform the other methods: Our resultsusing 50 training instances are much better than those of Pedersen, Purandare, andKulkarni (2005) in all but one task, and even better overall than the SVM in 200 traininginstances (compare the SVM column of Table 6).For comparison, we also train the SVM in 100 training instances, and find a decreaseof 3% on average from using 200 training instances.
We conclude that our method ismore robust to minimal training conditions.
To explore the least amount of training dataneeded for our measure, we further reduce the amount for producing gold-standard50Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceTable 7Average classification results of the network flow method using 200, 100, and 50 training dataper classification task.
The weighted average is calculated based on the number of test instancesper task.Name Pair Number of Training Instances200 100 50Ronaldo/Beckham 0.80 0.79 0.76Tajik/Ekeus 0.97 0.98 0.96Microsoft/IBM 0.73 0.73 0.72Peres/Milos?evic?
0.96 0.97 0.94Jordan/Egyptian 0.77 0.74 0.70Japan/France 0.75 0.75 0.70Unweighted Average 0.83 0.83 0.80Weighted Average 0.77 0.76 0.72profiles to 20 and 5 instances per task, and observe a continual drop in performance.
Theperformance of one task (Ronaldo/David Beckham) drops below chance with 20 traininginstances and another (Microsoft/IBM) drops below chance with 5.
For this set of data,we conclude that 50 instances per task are required to provide enough discriminatorypower for our method.Although unsupervised methods have the advantage of requiring no training data,in our case, 50 to 100 training instances constitute only a very small portion of thedata, as well as a small amount of annotation effort in absolute terms.
We concludethat the (small) labelling effort is justified by the performance gain achieved using ourminimally supervised approach.6.
Task 3: Document ClassificationDocument classification is an NLP task in which a previously unseen document is givena topic label (or a set of such labels) based on its subject matter.
For example, a financialdocument discussing the fluctuation of crude oil prices may be labelled ?commerce?
or?crude oil?
in the Reuters Corpus (Lewis et al 2004).
In our version of the task, eachdocument has a single topic label.
Document classification is typically performed bycomparing the text of an unlabelled document to the text of documents whose topics(labels) are known, and assigning the label of the closest such document (Joachims 2002;Iwayama et al 2003; Esuli, Fagni, and Sebastiani 2006; Nigam, McCallum, and Mitchell2006).
This task is thus similar to the name disambiguation task in the previous section,and our approach is similar as well: Here again, we form gold-standard profiles from asmall collection of texts of known classes, and then compare each test instance to eachof the gold-standard profiles.
As in name disambiguation, we experiment with differentamounts of training data for creating the gold-standard profiles.There are two differences of note in comparison to name disambiguation.
First, indocument classification we use the entire set of words constituting the document tocreate a semantic profile, rather than a smaller window around a target word.
Second,whereas each ambiguous name instance in the earlier task had exactly two potentiallabels (and thus there were two gold-standard profiles for comparison), the number oflabels in the document classification task is much larger, leading to more ambiguity inthe task.51Computational Linguistics Volume 36, Number 16.1 Experimental Set-up6.1.1 Corpus Data.
Our data is a corpus of articles from 20 different Usenet newsgroupsreleased by Mitchell (1999).
Because each newsgroup corresponds to a topic, the articlescan be classified using the (single) newsgroup label.
We use the collectionmaintained byRennie (2001), in which all the duplicates (cross-posts) are removed, resulting in 18,828articles.
The articles are approximately evenly distributed among the 20 newsgroups.Stopwords and article headers are removed before processing each text.Work that relies on word frequency vectors to represent the texts in documentclassification has revealed the importance of preprocessing the word frequency data toemphasize those terms that are likely to be most meaningful.
For example, word fre-quencies have typically been weighted by inverse document frequencies (tf ?
idf ) tolessen the impact of very common but less distinguishing words.
According toRennie (2001), their best system on the same corpus uses thelog tf+1log idf weighting scheme.In order to compare our system to theirs, we use this same word weighting schemein the creation of the word vectors that are used to produce our semantic profiles.
(We have experimented with using raw word frequencies as well as tf ?
idf to pro-duce profiles.
Both methods yield approximately the same results as thelog tf+1log idffrequency weighting scheme.
)6.1.2 Training and Evaluation.
As mentioned before, we treat the classification task simi-larly to name disambiguation, taking a minimally supervised approach.
We randomlyselect a small number of documents as training data for creating the gold-standardsemantic profiles.
We use 10 or 30 documents per newsgroup, or approximately 1?3%of the documents.
The remaining documents are used as testing data.
Again, we usea random sample of documents for each gold-standard profile, repeated five times tominimize the impact of a possible skewed sampling.
We report the average accuracyover the five samples.Because there are 20 possible topic labels, the random baseline is very low, at 5%.
(Using the predominant label raises this only slightly.)
A more informative evaluationof our method is to compare to a state-of-the-art approach that is purely distributional.A comparison to Rennie (2001) is natural, since we use the same data set.
However,they trained an SVM on 30 documents per class and tested on 10% of the documents,repeated 10 times.
Because our training approach differs somewhat (training on 10 or30 documents per class, testing on all remaining documents, repeated 5 times), wealso replicate their SVM experiment using our training and test sets.
As in the namedisambiguation task, we use the LIBSVM software package (Chang and Lin 2001) andtune the classifier in the training phase for the best SVM parameters prior to the testing.Also as in our name disambiguation task, we additionally train and test the SVM onjust the nouns in a document (rather than all words), and also on the nouns mappedto concepts (with the relevant frequencies as the feature values in both cases).
Thus wereport results of the SVM on three different types of input frequency vectors: all words,nouns, and concepts.6.2 Results and Analysis6.2.1 Initial Results.
Table 8 presents the classification results using 10 and 30 trainingdocuments per class for our network flow and SVMmethods.
Our network flowmethodperforms well above the random baseline, but is far from achieving state-of-the-artresults.
The SVM experiments using all words in the document perform much betterthan our network flow method, and are consistent with the accuracy of 68.7% achieved52Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceTable 8Average classification results using 10 and 30 training documents per newsgroup.Training Size / Class SVM SVM SVM NFAll words Nouns (Words) Noun Concepts10 47.1 47.8 42.7 31.230 66.2 66.4 61.4 32.0by Rennie (2001) using an SVM.
One possible reason is that the SVM is trained onall words (minus stopwords and article headers), whereas our network flow methodapplies to noun concepts only.
The SVM performance on noun-only data is similar tothat of all words.
Although there is a marked decrease in performance on the conceptfrequency vectors, SVM still outperforms our method.The poorer SVM performance on concept frequencies suggests that concept fre-quency vectors are less easily distinguishable than word frequency vectors.
Recall,however, that we found no difference with these various training approaches for SVMin name disambiguation.
It is possible that the mapping from words to concepts is aproblem here because the full text is used, rather than a relatively small window arounda target word.
Because each word can map to multiple (potentially unrelated) concepts,the use of a larger, unconstrained bag of words may lead to a high degree of ambiguity,introducing more noise in the semantic profile than our method can handle.
This mayalso explain why the network flow method does not improve with additional trainingdata, showing virtually no improvement between 10 and 30 training instances (0.8%difference).
We speculate that the amount of noise in a semantic profile based on thelarger amount of text may increase along with the increase in the training size, offsettingany potential gain from having additional data.If this hypothesis is correct, it is natural to ask why the SVM result using conceptsshows a substantial increase in accuracy from 10 to 30 training documents.
If larger textsyield nosier semantic profiles, why does this not negatively affect the SVM as well?
Thishighlights a fundamental distinction of our approach: our method is novel because itfinds the distance between concepts as embedded in a graph (the ontology), not just betweenconcept vectors.
Generally, our thesis is that this is an advantage of our model: It entailsthat all concepts generated from a text play a role in determining the distance of thattext from another.
As we noted earlier, this allows us to find similarity between textsthat use related but not equivalent concepts.
However, the performance of our methodin this document classification task reveals a potential drawback of this property of ourmethod.
Because it takes all concepts into account in determining distance, it is moresusceptible to noise.
Figure 5 illustrates the problem.We see that the square and triangleprofiles are noisy?that is, they each have a number of nodes that are not part of theircoherent semantic content.
These noisy aspects of the two profiles are less separatedin ontological space, making the two profiles more similar according to our measurethan their ?true?
semantic content would indicate.
Because a vector representation ofconcepts does not form connections between differing concepts, it is not led astray inthe way our method is.6.2.2 Removing Noise from the Profiles.
Our conjecture is that the poor performance of ournetwork flowmethod is due to noise caused by ambiguity in the mapping of each wordto all of its concepts (i.e., not just the relevant ones to the topic).
This effect could also be53Computational Linguistics Volume 36, Number 1Figure 5Two noisy profiles, one represented by squares, the other by triangles.exacerbated by the fact that, in using the full document, we may have a higher numberof less relevant words than when a profile is formed from a more constrained set ofwords (as in verb alternation detection and name disambiguation).
If this hypothesisis true, then the noisy (irrelevant) concepts should be distributed within each profileaccording to some prior probability distribution.
If we knew that distribution, then wecould ?subtract out?
the noise and form more semantically coherent profiles.
Referringto Figure 5, the idea is that we would like to remove the small, dispersed squares andtriangles, leaving only the larger ones that form a semantically more coherent set.We test this idea, experimenting with two possible noise distributions.
The first issimply the uniform distribution, and the second is a distribution determined empiri-cally using frequency counts from a domain-general corpus.
For the latter, we determinea distribution over concepts based on the nouns in the BNC.
Because the BNC is abalanced corpus, the distribution of its nouns can be considered a prior that is treatableas noise compared to the distribution in a newsgroup posting that is specific to aparticular topic.
In each case, we create a semantic profile representing the expectednoise, and then ?subtract?
the resulting noise profile from each of our gold-standardsemantic profiles in the document classification task.
The ?subtraction?
is actually aprocess of setting to zero all of the semantic profile frequencies that are less than thenoise value for that concept.
Any node with a value higher than the noise value forthat node is expected to be a potentially relevant concept.
We leave such nodes at theiroriginal value so that they are more distinguished from the remaining values (now setto zero).
Figure 6 illustrates the result of applying this kind of noise reduction to theprofiles in Figure 5.
We can see that low-frequency concept nodes are zeroed out, withhigher frequency nodes maintaining their concept weight.Table 9 presents the network flow results on the noise-subtracted data, showing a3?5% increase in the performance using 30 training documents per class.
The perfor-mance decreases with noise-subtraction when we have only 10 training documents perclass, suggesting that there may not be enough data in this case to use this simplisticsubtractive method.Interestingly, subtracting the uniform noise distribution from the profiles has amorefavorable effect than subtracting the BNC noise distribution.
The BNC distribution isperhaps inappropriate for our data.
Newsgroup data includes a variety of subjects54Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFigure 6The same two profiles in Figure 5.
The profile masses that are ?subtracted?
are shaded in gray.which may make it more similar to a balanced corpus than we have originally antic-ipated, thus what we are treating as a ?noise?
distribution in this case may not actuallyrepresent noise.
That said, there is a small but notable increase even using the BNCnoise distribution when we have sufficient training data.
The idea of subtracting outnoise seems promising, but we leave the appropriate representation of noise, and themechanism for removing it effectively, as an area of future research.7.
Profile Density: A Measure of Coherence of Semantic ProfilesWe have seen a performance difference across the three tasks we used in evaluation:the network flow method outperforms purely distributional measures on verb alterna-tion detection and name disambiguation, but does poorly on document classificationcompared to a distributional approach.
(See Table 10 for a summary of the results.)
Weuse the same ontology (WordNet) and the same concept distance (number of edges) inour network flow measure across all three tasks, hence there must be some differencein the three data sets themselves that impacts the ability of our method to distinguishthe semantic profiles corresponding to one class of data (one usage of an ambiguousname, for example) from the profiles of a different class of data (the other usage of thename).
In this section, we develop a measure that can capture this property and explainthe performance differential we have observed for our method.Table 9Average classification results using 30 and 10 training documents per newsgroup, using theoriginal profiles (NF), and using profiles after the ?noise subtraction?
process described in thetext (?NF ?
Uniform?
and ?NF ?
BNC?
are results subtracting the uniform distribution andthe BNC noun frequency distribution, respectively).Training Size / Class NF NF ?
Uniform NF ?
BNC10 31.2 28.2 27.430 32.0 37.2 35.655Computational Linguistics Volume 36, Number 1Table 10Summary of task-based results.
The numbers in parentheses indicate the number of traininginstances used.
The best result for each task is shown in bold.Verb Alternation Detection random Manhattan skew div JS NFDataset1 Avg 0.50 0.70 0.57 0.67 0.70Dataset2 Avg 0.50 0.67 0.67 0.67 0.67Name Disambiguation random SVM (100) SVM (200) NF (100) NF (200)Unweighted Avg 0.61 0.72 0.75 0.83 0.83Weighted Avg 0.53 0.52 0.55 0.76 0.77Document Classification random SVM (10) SVM (30) NF (10) NF (30)20 newsgroups 0.05 0.43 0.61 0.31 0.327.1 Profile CoherenceOur goal is to find a property of individual semantic profiles that, when averaged acrossthe profiles in a data set, indicates whether our method will be able to distinguishprofiles of different classes in that data set.
That is, we aim to learn about the overallseparability of the classes in a data set by investigating the properties of individualprofiles that constitute the data set.
Our hypothesis is that the important factor for ourmethod is what we refer to as profile coherence: the degree to which profile mass isconcentrated within a constrained space (or set of constrained spaces) of the ontology.The more spatially coherent the sets of weighted concepts are for the profiles in a dataset, the more likely it is that our method will be able to distinguish contrasting profiles.Conversely, less coherent profiles, whose frequency mass is more distributed across awider area of the ontology, will be more difficult to separate into classes.
(Note thatprofile coherence is not a sufficient condition for data separability, but we hypothesizethat it can be a useful indicator.
)For example, consider the square and triangle profiles in Figure 7.
Coherent profileshave their profile mass (the concept weights) focused within small, distinct regions ofthe ontology, as in Figure 7(a).
These types of profiles tend to be highly distinguishablefrom each other.
Less coherent profiles, whose mass is more dispersed through the on-tology, such as those in Figure 7(b), are likely to be less distinguishable.
Note, however,that it is not simply occupying greater or fewer nodes in the hierarchy that determinesprofile coherence (and distinguishability).
The profiles in Figure 7(c) are ?spread out?as in (b), but are more coherent (and distinguishable) due to having areas of high mass.The considerations illustrated in Figure 7 suggest that both distributional and onto-logical factors contribute to the coherence of a semantic profile, and that we must deter-mine a suitable measure of coherence that captures both factors.
A simpler, alternativehypothesis is that either purely distributional or purely ontological factors may suffi-ciently capture the coherence of a semantic profile.
To explore these ideas, we examinedifferent ways to assess the coherence of the semantic profiles in our example data sets.We develop various measures of coherence, and then evaluate whether the degree ofcoherence as determined by each measure indeed corresponds to the performance of56Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFigure 7Examples of two profiles (indicated by squares and triangles) of varying coherence.
The profilesin (a) are more distinguishable than those in (b) and (c); the profile in (c) is in turn moredistinguishable than that in (b).
The degree of distinguishability of these profiles is reflected intheir degree of coherence.our network flowmethod on the data sets in our three tasks.
We expect a useful measureof profile coherence to have a high average value across the data sets on which weperform well (verb alternation and name disambiguation), and a low average valueacross the data set on which we perform poorly (document classification).In Section 7.2, we briefly review several measures intended to separately capturethe distributional or ontological coherence of a semantic profile.
We show that suchmeasures are insufficient for accounting for the performance differences of our methodacross the data sets.
In Section 7.3, we develop a novel measure to capture the coherenceof our profiles in terms of both distributional and ontological information.
Thismeasure,called profile density, expresses the degree to which a semantic profile forms a coherentclustering of weighted concepts in an ontology.
We demonstrate that our profile densitymeasure can account for the performance differential across our data sets.7.2 Separate Distributional and Ontological ApproachesWe explored several (unsuccessful) means for capturing profile coherence with a purelydistributional or purely ontological measure.
Although we could not exhaustivelyinvestigate all possible measures of this kind, the underlying reasons for the lack ofsuccess of these measures in explaining the differing performance of our method across57Computational Linguistics Volume 36, Number 1the data sets convinced us of the need for a measure that integrates distributional andontological factors (which we present in the following section).
We mention the single-factor measures here for completeness.7.2.1 Potential Distributional Coherence.
Recall that Section 6.2.2 shows that removingthe ?noise?
distribution from each profile improves the document classification per-formance of our method.
In other words, subtracting the noise distribution from aprofile can make it distributionally more distinct from other profiles.
Based on thisobservation, we hypothesize that the less a profile resembles a noise distribution overthe ontology, the more coherent it is?that is, the more likely the frequency mass issituated in meaningful clusters of concepts.
To test this hypothesis, we calculate theaverage distance (using KL-divergence [Kullback and Leibler 1951]) of the profiles ina data set from a profile created from a noise distribution (the uniform distributionof words, or their distribution in the BNC, as in Section 6.2.2).
Higher values of thismeasure indicate further distance from the noise distribution.7.2.2 Potential Ontological Coherence.
Here we consider two observations.
First, wehypothesize that profiles with fewer concepts are more coherent, because a smallernumber of concepts is more likely to be less dispersed in the ontology.
We simplyuse average profile size to capture this property (here, smaller values of profile sizeindicate greater coherence).
Second, we hypothesize that profiles whose concepts havegreater specificity are more coherent, because use of less specific concepts is indicativeof vagueness and potential lack of coherence.
Because specificity corresponds well todepth in WordNet, we use a simple measure of average profile depth to indicate thespecificity of the set of concepts in a profile (here, greater values of depth should indicategreater coherence).7.2.3 Analysis of the Single-Factor Measures.
For each task, we calculate the average ofeach of the hypothesized distributional and ontological coherence measures over theprofiles in the data set, and find that there is no consistent correspondence with theperformance of our network flow method across the tasks.
Despite the intuitions andobservations presented herein, these results are not surprising.
For example, the profilesof a data set may all be distributionally very similar overall to the noise profile, sup-posedly indicating low coherence, but they may be quite coherent in the actual ontolog-ical space they occupy.
Similarly, the profiles in a data set may all have a small averagedepth in the ontology or large size (again supposedly indicating low coherence), buttheir distributional properties (the weights on the concepts that are occupied) may yieldcoherent clusters of mass in the profile.
This analysis then confirms our hypothesis that,because distributional and ontological information are intertwined in the representationof a semantic profile, a useful measure of profile coherence must take into account anintegration of these two information sources.7.3 Integrating Distributional and Ontological Factors in a Coherence MeasureAs noted earlier, and tentatively confirmed by the results herein, we assume that theinteraction of distributional and ontological factors determines the coherence of pro-files (i.e., a coherent profile has its frequency mass concentrated within a reasonablyconstrained space [or set of constrained spaces] of the ontology).
We observe that thisis similar to the geographical notion of population density, which is determined bythe population mass divided by the area occupied.
Here we extend the geographical58Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceFigure 8Two examples of profile density within an ontology.
The hollow triangles are the commonancestors of the filled triangles, which are concept nodes in the profile.
The profile in (a) is fairlydispersed, requiring a single but distant ancestor node.
The profile in (b) is more clustered; twoancestor nodes are required but each is close to its descendants.definition of density within our network framework by relating population mass todistributional weights on concepts, and occupied area to the spread of the weightedconcepts in the ontology.
We call the resulting measure of profile coherence profiledensity.7.3.1 The Profile Density Measure.
To adapt the definition of geographical density to ourproblem, we first need to determine the analogs of population mass and occupied areain a semantic profile.
The profile mass at each concept node is directly analogous tothe population mass.
Defining the occupied area within an ontology is not as straight-forward, as there is no simple definition of area within a graph.
For example, Agirre andRigau (1996) use the number of nodes within a subgraph as its area, but this fails to takeinto account how dispersed the nodes are throughout the ontology.
We instead developa definition of area that captures the actual spatial spread of the profile mass throughthe ontology.To begin, we note that any subgraph of the WordNet hypernym hierarchy is hi-erarchical itself.
Thus, any region of the ontology that contains some profile mass isa hierarchy rooted at some common ancestor of those profile nodes.10 As shown inFigure 8, themore dispersed (less closely clustered together) a set of nodes is, the furtheraway their common ancestor is.
That is, a highly related (and spatially constrained) setof concept nodes can be generalized to a more specific ancestor concept (i.e., near thedescendants, as in Figure 8(b)), whereas a semantically distant set of concepts will begeneralized to a semantically general ancestor concept (i.e., far from the descendants,as in Figure 8(a)).
The ontological distance between a set of nodes and their commonancestor thus indicates how closely clustered the descendant nodes are.Next note that any semantic profile can be represented by a set of ancestor nodes,and these ancestor nodes capture the spatial clusterings of the profilemass.
For example,10 Although WordNet contains instances of multiple inheritance, the rate is low.
As a result, the likelihoodof a set of profile nodes sharing multiple ancestors is low as well.59Computational Linguistics Volume 36, Number 1Figure 9These two profiles have equal density value given our original profile density formula inEquation (5), but are suitable distinguished (with the profile in (b) having higher density thanthat in (a)) by the norm density formula in Equation (6).
See the text for discussion.the profile in Figure 8(a) is represented by one ancestor node, and that in Figure 8(b) bytwo such nodes.
Combining these observations, we see that given a suitable manner foridentifying ancestor nodes to represent a profile, we can use the combined ontologicaldistance between each of those nodes and their descendants as an indication of howclosely clustered the concepts of the profile are.
We can now complete our definitionof profile density by using the total distance between each identified ancestor and itsdescendants as an indication of the occupied area of the ontology.Formally, let P be a profile and A be a set of ancestor concept nodes such thateach profile node d ?
P is guaranteed to have an ancestor a ?
A.
(We will explain inSection 7.3.2 how to find the set A.)
The profile density of P is then defined as follows:profile density(P) =?a?A?d?P,d?descendant(a)mass(d)distance(d, a)(5)where mass(d) is the profile mass (concept frequency) at node d, and distance(d, a) is thedistance in the ontology between node d and node a, as given by a suitable concept-to-concept distancemeasure (such as the edge distance that we have used in our task-basedevaluations).There is one more subtle detail we must address.
Consider the two examples inFigure 9, where the distance between each ancestor and all its descendants is the same(here, say, a distance of 1), but the distribution of the profile mass differs.
The firstdiagram has ten equally weighted profile nodes, and the second has two.
Our currentformulation in Equation (5) yields a density of 1 for both diagrams (i.e., (0.1/1)?
10 =1 = (0.5/1)?
2).
However, the profile mass in diagram (a) is distributed among morenodes than that in diagram (b).
Intuitively, the second profile is more densely clusteredand should have a higher density value.Looking more closely at our density formula in Equation (5), observe that thenumber of profile nodes has an impact on the calculation?that is, density increasesas the number of profile nodes increases due to the inner summation in the formula.
To60Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distanceachieve an appropriate density measure, then, we normalize the original density valueby the number of profile nodes, resulting in a normalized density for a profile:norm density(P) =density(P)sizeof (P)= 1sizeof (P)?a?A?d?P,d?descendant(a)mass(d)distance(d, a)(6)Returning to our example in Figure 9, Equation (6) assigns the first profile a normalizeddensity of 0.1, and the second profile a normalized density of 0.5.
The modified measurenow appropriately distinguishes the two profile densities, indicating that the profile inFigure 9(a) is less tightly clustered than the profile in Figure 9(b).7.3.2 Finding the Ancestor Set for Profile Density.
As noted earlier, our definition of profiledensity depends on identifying a suitable set of ancestor nodes of the concept nodesin the profile: the aggregate distance of the ancestors to the profile nodes indirectlyindicates the degree to which the profile nodes are spatially clustered close together.Thus, given a profile P, we need to find A, the set of nodes that are ancestors of theprofile nodes d ?
P. (The nodes a ?
A correspond to the hollow triangles indicated inFigure 8 and Figure 9.)
Recall that these ancestor nodes are intended to be a set ofconcepts that serve as an appropriate generalization of the nodes in the profile?eachancestor in a sense represents a coherent cluster of profile nodes.
However, we do notknow a priori what the appropriate level of generalization is?we simply want a levelthat gives a useful assessment of how clustered together the profile nodes are.For this purpose, we make use of Clark and Weir?s (2002) method for generalizinga set of weighted concept nodes in an ontology.
As we noted in Section 4, given afrequency distribution over all concept nodes, Clark andWeir use a statistical method tosearch for the set of nodes (i.e., our node setA) that best generalize the original weightedconcepts.
This method is particularly appropriate for our purposes because it includes aparameter,?
?
(0, 1), that controls the level of generalization.We vary?
over five values(0.05, 0.25, 0.5, 0.75, and 0.95) to obtain five different (more to less generalized) sets ofancestors.
In our analysis, we calculate the density using each ancestor set in order toevaluate the impact of the precise choice of ancestor nodes on our measure.7.3.3 Results and Analysis.
For each of the three tasks in our earlier task-based evaluation,we calculate the profile density of the corresponding data set.
We define the profiledensity of a data set to be the average of the normalized density values over its profiles.For the verb alternation detection task, we perform the analysis on all 240 profiles usedin the task (120 verbs, with 2 profiles per verb, one for the subject slot, one for the objectslot).
In the remaining two tasks, because each instance profile is compared to a gold-standard profile, we believe that the performance depends primarily on the coherence ofthe gold-standard profiles.
We thus perform our analysis on the gold-standard profilesonly.
For name disambiguation, we have 60 profiles (5 samplings with 12 gold-standardprofiles each); for document classification, we have 100 profiles (5 samplings with 20gold-standard profiles each).
For each profile, we calculate the normalized density usingeach of five ancestor sets (based on the ?
value, as noted above).
For the concept-to-concept distance measure, distance(d, a) in Equation (6), we use edge distance, the samemeasure used in the tasks in earlier sections of the paper.61Computational Linguistics Volume 36, Number 1Table 11The profile density scores for each data set at five different values of ?, as well as the averagescores across the ?
values.?
value 0.05 0.25 0.5 0.75 0.95 AvgVerb Alternation 5.59e-4 5.90e-4 6.32e-4 7.14e-4 8.87e-4 6.76e-4Name Disambiguation (200) 8.93e-5 9.89e-5 1.08e-4 1.18e-4 1.35e-4 1.10e-4Name Disambiguation (100) 1.11e-4 1.26e-4 1.38e-4 1.52e-4 1.78e-4 1.41e-4Doc Classification (30) 5.25e-5 5.94e-5 6.59e-5 7.43e-5 8.78e-5 6.80e-5Doc Classification (10) 8.03e-5 8.85e-5 9.87e-5 1.11e-5 1.33e-5 5.84e-5We expect that, if our profile density measure does indeed reflect the coherenceof a data set, then we will see a correspondence between the density values and theperformance of our network flow method.
Higher density values indicate a profilewhose weighted concepts form more coherent clusters in the ontology.
Specifically,then, we expect higher density values for the data sets from our verb alternation de-tection and name disambiguation tasks (on which our method had better performancethan distributional methods), and lower density values for the document classificationdata set (on which our method had worse performance than a purely distributionalmethod).11Table 11 shows the profile densities of each data set.
First note that the densityvalues are relatively stable across all values of ?, indicating that the precise level ofgeneralization is not critical to the usefulness of our density measure.
Next, observethat, as predicted, the document classification data set is shown to have the lowestdensity for both training set sizes.
This observation is in accord with our hypothesisthat the profile density measure indicates the coherence of the profiles in a data set andis therefore informative about the network flow performance on that data set.Interestingly, we also observe that, across all values of ?
and training set sizes, theverb alternation data set has the largest densities, followed by the name disambiguationdata set, then the document classification data.
(The differences between all three datasets are statistically significant, p 0.05.)
This result might stem from the fact thatthere are varying degrees of constraint placed upon the data in the three tasks.
Inverb alternation, the nouns used to generate a profile appear either all in the subjector all in the object position of the target verb.
In name disambiguation, we loosen therestriction to include all nouns in a small window surrounding the target word.
Lastly,in document classification, the only restriction on the nouns used to generate a profileis that they appear in the same document.
This suggests that the syntactic and semanticconstraints placed upon a set of nouns can have an impact on the coherence of the profilecreated from them.This latter observation suggests that our profile density measure may be usefulnot only in indicating the ability of our network flow method to distinguish relevantprofiles.
More generally, it may also reflect the varying degrees of syntactic and semantic11 Note that because our method in each task is compared to different kinds of alternative distributionalmethods, we do not expect to find a mathematical correlation between the performance improvementand the density values; rather, good performance should be reflected in higher density values and poorperformance in lower density values.62Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distanceconstraints placed upon the set of words that generate a profile.
Our profile densitymeasure may indeed be generally useful as a measure of the semantic coherence of a setof concepts in an ontology (Gurevych et al 2003), a matter we plan to explore in futurework.In summary, our analysis in this section has shown that both distributional andontological properties contribute to the coherence of a profile, but neither alone isindicative of the network flow performance in a particular task.
Our new measure ofprofile density serves as a tool for analyzing profiles that integrates their distributionaland ontological coherence, and provides a post hoc means for explaining the perfor-mance differential of our method across the different tasks we performed here.
Theresults also point to the possibility of devising a diagnostic tool for the suitability of thenetwork flow method on novel data.
An analysis of the data and results across a largerset of tasks will allow us to investigate the possibility of determining a density thresholdthat would be indicative of expected positive results with our method.8.
Related WorkTo the best of our knowledge, our method is the only work that measures text distanceby combining ontological knowledge and distributional information together via agraph-based algorithm.
Although there are existing methods that use either or bothtypes of information in measuring the semantic distance of texts (Corley and Mihalcea2005; Mohammad and Hirst 2006), our work is unique in that it integrates the ontolog-ical distance between individual words across two texts as well as the distributionaldifferences between the texts.
Here we review existing work on both text comparisonand graph-based approaches in CL, given the relevance of these two areas to ourresearch.8.1 Text ComparisonOur work stems from the studies on measuring the semantic distance between twowords or concepts using an ontological resource (which is extensively covered inPedersen, Banerjee, and Patwardhan [2005] and Budanitsky andHirst [2006]).
To extendthese methods for the comparison of two texts, we incorporate ontological distancebetween concepts and distributional information in a systematic and efficient manner.Other research that attempts to include the two takes a more modular approach.
Forexample, Corley and Mihalcea (2005) consider the ontological distance between theconcepts representing the texts but ignore their distributional information.
On the otherhand, Scott andMatwin (1998), McCarthy (2000), andMohammad and Hirst (2006) takethe distributional distance between concept vectors representing the texts but do notconsider the ontological relations among the concepts.Most recent work on text comparison tends to be word-based and distributional(Lee 2001; Weeds, Weir, and McCarthy 2004; Pedersen, Purandare, and Kulkarni 2005;Al-Mubaid and Umair 2006).
In the case of high dimensionality and data sparseness,words are grouped into a smaller number of (unnamed) concepts using some matrixfactorization technique (e.g., SVD) or some clustering method (Pereira, Tishby, andLee 1993).
In other words, words are grouped together based on their distributionalproperties instead of their explicit semantic/ontological properties.
Furthermore, unlikein our method, once the words are collapsed into unnamed concepts, the individualelements (i.e., the unnamed concepts) across data points cannot be compared.
As shown63Computational Linguistics Volume 36, Number 1in our experiments, taking into account this extra piece of information is beneficial forsome applications.8.2 Graph ApproachesIn recent years, we have seen an increasing use of graph-based methods in NLP (Pangand Lee 2004; Mihalcea 2005; Navigli and Velardi 2005).
The graph-theoretic approachis popular due to the elegance of representing appropriate NLP problems and theavailability of a number of efficient algorithms.
One of the most straightforward NLPexamples is the use of WordNet.
Besides our work here, much prior research has takenadvantage of the graphical structure of WordNet.
For example, Agirre and Rigau?s(1996) conceptual density uses WordNet as a graph and calculates the density withina subgraph (the number of relevant concepts within a subgraph), which was found tobe useful for WSD.Graphs in general are the obvious mathematical formalism to encode the relation-ships (represented as edges) between either words or longer units of text (representedas nodes).
(The reverse is possible, using nodes to represent relations and edges forsemantic entities.
The choice of representation clearly depends on the NLP task itself.
)Once we formulate a problem into a standard graph problem, there are existing efficientgraph-based algorithms that we can use to find an optimal or near-optimal solution.For example, both Pang and Lee (2004) and Barzilay and Lapata (2005) use a minimum-cut algorithm for two vastly different applications, document polarity classification andcontent selection, respectively.
In these approaches, the sentences are represented asnodes in a graph, and the edge connecting each pair of nodes is weighted with anassociation score between the sentences, reflecting, for example, the distance (numberof sentences) between a pair of sentences.
The minimum-cut method allows them toclassify the nodes, and thus the sentences, into different categories.Another popular graphmethod is the randomwalk algorithm, which is successfullyemployed by the PageRank approach for ranking Web pages (Brin and Page 1998).Similar to the minimum-cut algorithm, here, nodes represent semantic entities (e.g.,words), and edges represent associations between the nodes (e.g., word co-occurrence).The random walk algorithm allows for the classification of each node based on therelevance of its neighbors.
For example, Mihalcea (2006) uses random walk for WSDby constructing a graph in the following way.
Each node represents an ambiguous (test)word, or a (training) word labelled with one of its senses.
Each edge indicates that thecorresponding two words co-occur in some context.
The sense of an ambiguous wordis determined by the sense of its most relevant neighbor(s), by randomly traversingthe graph until an equilibrium state has been reached.
Hughes and Ramage (2007)also use a random walk method, with the goal of determining semantic relatednessbetween individual words (not sets of words, as in our work).
In their work, the randomwalk method computes a probability distribution over WordNet concepts.
Note that theprobability distributions resulting from random walks centered at different conceptsin WordNet are distinct.
One can then measure the semantic relatedness between twoconcepts by calculating the divergence between their probability distributions overWordNet concepts as a result of the two random walks centered at them.In comparison to other graph approaches to NLP, we choose to use a minimum-cost flow algorithm based on our graph formulation.
Because a profile is a collectionof frequency-weighted concepts, some concept nodes are weighted more heavily thanothers, therefore the routes between such nodes across the two profiles are alsoweighted more heavily.
An algorithm solving a minimum-cost flow problem provides64Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distancean efficient mechanism to find these weighted routes as our solution, making MCF,rather than the shortest paths or maximum-flow-minimum-cut, the best choice forformalizing the constraints we define in the text comparison problem.9.
ConclusionWe have presented a graph-theoretic approach to calculating semantic distance betweentwo texts, which encompasses both ontological knowledge and distributional informa-tion.
We have developed a network flow method that takes advantage of the graphicalstructure of an ontology.
Given a suitable ontology, a word frequency vector for a textcan be transformed into a frequency distribution over concept nodes.
Hence, we treattexts as weighted subgraphs within a larger graph (the ontology).
By incorporating thesemantic distance between individual concepts, the graphical structure representingthe ontology becomes a metric space in which we can measure the distance betweensubgraphs, weighted by their frequencies.In this article, we use edge distance exclusively for the individual distance betweenconcepts.
Given that the distance between concepts is an integral part of our for-mulation, and that other sophisticated concept-to-concept distances have been shownto outperform edge distance for comparing concepts (Jarmasz and Szpakowicz 2003;Weeds 2003), we also investigate the use of such distances.
However, incorporatingthem can lead to a quadratic growth in complexity.
To remedy this, a pre-processingstep is required to reduce the complexity to reasonable computation time.
In Tsang andStevenson (2006), we introduce one such method by performing a graph transformationon the original network prior to the network flow calculation.
The transformed networkis more efficient to process without compromising the performance accuracy.
We referthe reader to that paper for further information.In the task-based evaluation presented here, our method has been shown to providesuperior performance on verb alternation detection and name disambiguation, in com-parison to alternative distributional approaches?even in cases where the alternativemethods have attempted to incorporate additional semantic knowledge (McCarthy2000; Pedersen, Purandare, and Kulkarni 2005).
Unlike existing distributional distancesand clustering techniques, the use of our text representation as well as the integrationof ontological distance allows a systematic way of capturing appropriate semanticdistinctions between the texts in these tasks.In contrast, our method does not perform as well on document classification as astate-of-the-art machine learning algorithm using a purely distributional approach.
Inorder to examine the performance discrepancy across tasks, we explore measures of thecoherence of the profiles in a data set, as potential indicators of how easily semanticprofiles of different classes can be distinguished.
The purely distributional and purelyontological indicators we consider are not useful in explaining the relatively poor per-formance of our method on document classification.
In response, we develop a measureof profile coherence, called profile density, that integrates these factors by determiningthe degree to which a profile forms distributionally and ontologically coherent clustersof concepts.
As a result, we are able to explain the performance of our method on thedata sets in terms of their density values.Recall also that we saw a performance difference in the verb alternation task de-pending on the different method used to generate the semantic profiles from the bag ofwords of the text (i.e., using ?raw?
data, versus a method to generalize to the best set ofconcepts for the bag of words).
Given also that we found that the profiles in documentclassification have a low density (i.e., their concepts are overly dispersed), one focus for65Computational Linguistics Volume 36, Number 1future work will be to explore further means for generating profiles that best capture theintended senses of the words within the text.
One option may be to use Mohammad?s(2008) unsupervised method for building concept vectors from word frequency data,which focuses the frequencies onto the most likely senses of the words according tocoarse ontological categories.Another strand of future work relates to our profile density measure.
We suggestthat not only is our profile density useful in predicting the performance of our networkflowmethod on unseen data, it may also be useful formeasuring the semantic coherenceof a text.
Note that a text that is semantically coherent tends to form profiles withhighly frequent and highly related concepts within an ontology.
Coincidentally, ourprofile density formulation measures the overall relatedness, and thus coherence, of acollection of concepts by taking into account the distance between the concepts as wellas the frequency distribution.
For example, if we relax the notion of a text to includeverbal arguments, semantic coherence of a text can be thought of as the selectionalpreference strength a verb imposes on its arguments.
As future work, we intend toinvestigate profile density as an indicator of selectional preference strength.
Generally,we believe profile density may offer a quantitative measure for semantic coherence andother related NLP applications.AcknowledgmentsWe would like to thank our colleagues inToronto, in particular Afsaneh Fazly andthe CL research group at the University ofToronto, for helpful discussions.
We wouldalso thank the anonymous reviewers fortheir detailed comments.
We gratefullyacknowledge the financial support providedby the Natural Sciences and EngineeringResearch Council of Canada (NSERC),Ontario Graduate Scholarship (OGS),and the University of Toronto.ReferencesAgirre, Eneko and German Rigau.
1996.Word sense disambiguation usingconceptual density.
In Proceedings of the12th International Conference ofComputational Linguistics (COLING-1996),pages 16?22, Copenhagen.Al-Mubaid, Hisham and Syed A. Umair.2006.
A new text categorization techniqueusing distributional clustering andlearning logic.
IEEE Transaction onKnowledge and Data Engineering,18(9):1156?1165.Barzilay, Regina and Mirella Lapata.
2005.Collective content selection forconcept-to-text generation.
In Proceedingsof the Joint Conference on Human LanguageTechnology / Empirial Methods in NaturalLanguage Processing (HLT/EMNLP),pages 331?338, Vancouver, Canada.Bodenreider, Olivier.
2004.
The unifiedmedical language system (UMLS):Integrating biomedical terminology.Nucleic Acids Research, 32:D267?D270.Brin, Sergey and Lawrence Page.
1998.The anatomy of a large-scalehypertextual Web search engine.Computer Networks and ISDN Systems,30(1?7):107?117.Briscoe, Ted and John Carroll.
1997.Automatic extraction of subcategorizationfrom corpora.
In Proceedings of the 5thApplied Natural Language ProcessingConference (ANLP), pages 356?363,Washington, DC, USA.Briscoe, Ted and John Carroll.
2002.
Robustaccurate statistical annotation of generaltext.
In Proceedings of the Third InternationalConference on Language Resources andEvaluation (LREC 2002), pages 1499?1504,Las Palmas, Spain.Budanitsky, Alex and Graeme Hirst.
2001.Semantic distance in Wordnet: Anexperimental, application-orientedevaluation of five measures.
In Proceedingsof the Workshop on WordNet and OtherLexical Resources, in the North AmericanChapter of the Association for ComputationalLinguistics (NAACL-2001), pages 29?34,Pittsburgh, PA, USA.Budanitsky, Alex and Graeme Hirst.
2006.Evaluating WordNet-based measures ofsemantic distance.
ComputationalLinguistics, 32(1):13?47.Burnard, Lou.
2000.
The British NationalCorpus Users Reference Guide.
OxfordUniversity Computing Services,Oxford, UK.66Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceChang, Chih-Chung and Chih-Jen Lin, 2001.LIBSVM: a library for support vectormachines.
Software available atwww.csie.ntu.edu.tw/?cjlin/libsvm.Chva?tal, Vas?ek.
1983.
Linear Programming.W.H.
Freeman and Company, New York.Clark, Stephen and David Weir.
2002.Class-based probability estimation using asemantic hierarchy.
ComputationalLinguistics, 28(2):187?206.Corley, Courtney and Rada Mihalcea.
2005.Measuring the semantic similarity of texts.In Proceedings of the ACL Workshop onEmpirical Modeling of Semantic Equivalenceand Entailment, pages 13?18, Ann Arbor,Michigan, USA.Esuli, Andrea, Tiziano Fagni, and FabrizioSebastiani.
2006.
TreeBoost.MH: Aboosting algorithm for multi-labelhierarchical text categorization.
InProceedings of the 13th InternationalSymposium on String Processing andInformation Retrieval (SPIRE?06),pages 13?24, Glasgow.Fellbaum, Christiane, editor.
1998.WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Gangemi, Aldo, Nicola Guarino, andAlessandro Oltramari.
2001.
Conceptualanalysis of lexical taxonomies: The case ofWordNet top-level.
In Chris Welty andBarry Smith, editors, Formal Ontology inInformation Systems: Collected papers fromthe Second International Conference.
ACMPress, pages 285?296, New York, USA.Garey, Michael R. and David S. Johnson.1979.
Computers and Intractability: A Guideto the Theory of NP-Completeness.
W.H.Freeman and Co., New York.Gurevych, Iryna, Rainer Malaka, RobertPorzel, and Hans-Peter Zorn.
2003.Semantic coherence scoring using anontology.
In Proceedings of the Joint HumanLanguage Technology and Northern Chapter ofthe Association for Computational LinguisticsConference (HLT-NAACL), pages 88?95,Edmonton.Han, Hui, Hongyuan Zha, and C. Lee Giles.2005.
Name disambiguation in authorcitations using a K-way spectral clusteringmethod.
In Joint Conference on DigitalLibraries (JCDL?05), pages 334?343, Denver,CO, USA.Hirst, Graeme.
2009.
Ontology and thelexicon.
In Steffen Staab and Rudi Studer,editors, Handbook on Ontologies.International Handbooks on InformationSystems.
Springer, New York,pages 269?292.Hughes, Thad and Daniel Ramage.
2007.Lexical semantic relatedness withrandom graph walks.
In Proceedings ofthe Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 581?589, Prague.Iwayama, Makoto, Atsushi Fujii, NorikoKando, and Yuzo Marukawa.
2003.
Anempirical study on retrieval models fordifferent document genres: Patents andnewspaper articles.
In Proceedings of the26th ACM SIGIR International Conference onResearch and Development in InformationRetrieval, pages 251?258, Toronto, Canada.Jarmasz, Mario and Stan Szpakowicz.
2003.Roget?s thesaurus and semantic similarity.In Proceedings of Conference on RecentAdvances in Natural Language Processing(RANLP 2003), pages 212?219, Borovets.Jiang, Jay and David Conrath.
1997.
Semanticsimilarity based on corpus statistics andlexical taxonomy.
In Proceedings on theInternational Conference on Research inComputational Linguistics, pages 19?33,Taipei, Taiwan.Joachims, T. 2002.
Learning to Classify TextUsing Support Vector Machines?Methods,Theory, and Algorithms.
Kluwer/Springer,New York.Kullback, S. and R. A. Leibler.
1951.
Oninformation and sufficiency.
Annals ofMathematical Statistics, 22:79?86.Landauer, Thomas K. and Susan T. Dumais.1997.
A solution to Plato?s problem: TheLatent Semantic Analysis theory of theacquisition, induction, and representationof knowledge.
Psychological Review,(104):211?240.Lee, Lillian.
2001.
On the effectiveness of theskew divergence for statistical languageanalysis.
In Artificial Intelligence andStatistics, pages 65?72.Levin, Beth.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.Levina, Elizaveta and Peter Bickel.
2001.The earth mover?s distance is themallows distance: Some insights fromstatistics.
In Proceedings of the EighthIEEE International Conference on ComputerVision, volume 2, pages 251?256,Vancouver, Canada.Lewis, David D., Yiming Yang, Tony G. Rose,and Fan Li.
2004.
RCV1: A new benchmarkcollection for text categorization research.Journal of Machine Learning Research,(5):361?397.Li, Hang and Naoki Abe.
1998.
Wordclustering and disambiguation based on67Computational Linguistics Volume 36, Number 1co-occurrence data.
In Proceedings ofCOLING-ACL 1998, pages 749?755,Montreal, Canada.Lin, Dekang.
1998.
An information-theoreticdefinition of similarity.
In Proceedings ofInternational Conference on MachineLearning, pages 296?304, Madison,Wisconsin, USA.McCarthy, Diana.
2000.
Using semanticpreferences to identify verbalparticipation in role switching alternations.In Proceedings of Applied Natural LanguageProcessing and North American Chapter of theAssociation for Computational Linguistics(ANLP-NAACL 2000), pages 256?263,Seattle, Washington, USA.Merlo, Paola and Suzanne Stevenson.
2001.Automatic verb classification based onstatistical distributions of argumentstructure.
Computational Linguistics,27(3):393?408.Mihalcea, Rada.
2005.
Unsupervisedlarge-vocabulary word sensedisambiguation with graph-basedalgorithms for sequence data labeling.In Proceedings of the Joint Conference onHuman Language Technology / EmpirialMethods in Natural Language Processing(HLT/EMNLP), pages 411?418, Vancouver,Canada.Mihalcea, Rada.
2006.
Random walks on textstructures.
In Proceedings of ComputationalLinguistics and Intelligent Text Processing(CICLing) 2006, pages 249?262, MexicoCity, Mexico.Mitchell, Tom.
1999.
20 newsgroups usenetarticles.
http://kdd.ics.uci.edu//databases/20newsgroups/20newsgroups.data.html.Mohammad, Saif.
2008.Measuring SemanticDistance using Distributional Profiles ofConcepts.
Ph.D. thesis, University ofToronto, Canada.Mohammad, Saif and Graeme Hirst.
2006.Distributional measures ofconcept-distance: A task-orientedevaluation.
In Proceedings of the 2006Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2006),pages 35?43, Sydney.Navigli, Roberto and Paola Velardi.
2005.Structural semantic interconnections: Aknowledge-based approach to word sensedisambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence,27(7), pages 1075?1086.Nigam, Kamal, Andrew McCallum, and TomMitchell.
2006.
Semi-Supervised textclassification using EM.
In OlivierChapelle, Bernhard Scho?lkopf, andAlexander Zien, editors, Semi-SupervisedLearning.
MIT Press, Cambridge, MA,pages 33?56.Pang, Bo and Lillian Lee.
2004.
Asentimental education: Sentimentanalysis using subjectivity summarizationbased on minimum cuts.
In Proceedings ofthe 42nd ACL, pages 271?278, Barcelona,Spain.Pantel, Patrick and Dekang Lin.
2000.
Anunsupervised approach to prepositionalphrase attachment using contextuallysimilar words.
In Proceedings of Associationfor Computational Linguistics (ACL-00),pages 101?108, Hong Kong.Pedersen, Ted, Satanjeev Banerjee, andSiddharth Patwardhan.
2005.
Maximizingsemantic relatedness to perform wordsense disambiguation.
Technical ReportUMSI 2005/25, University of Minnesota,Duluth.Pedersen, Ted, Amruta Purandare, andAnagha Kulkarni.
2005.
Namediscrimination by clustering similarcontext.
In Proceedings of the SixthInternational Conference on Intelligent TextProcessing and Computational Linguistics,pages 226?237, Mexico City, Mexico.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributionalclustering of English words.
InProceedings of the 31st Annual Meetingof the Association for ComputationalLinguistics, pages 183?190, Columbus,Ohio, USA.Pinker, Steven.
1989.
Learnability andCognition: The Acquisition of ArgumentStructure.
MIT Press, Cambridge, MA.Rennie, Jason.
2001.
Improving Multi-classText Classification with Naive Bayes.Master?s thesis, Massachusetts Institute ofTechnology, Cambridge, MA.Resnik, Philip.
1993.
Selection and Information:A Class-Based Approach to LexicalRelationships.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Resnik, Philip.
1995.
Using informationcontent to evaluate semantic similarity in ataxonomy.
In Proceedings of the 14thInternational Joint Conference on ArtificialIntelligence, pages 448?453, Montreal,Canada.Ribas, Francesc.
1995.
On learning moreappropriate selectional restrictions.
InProceedings of the Seventh Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 112?118,Dublin.68Tsang and Stevenson A Graph-Theoretic Framework for Semantic DistanceSchulte im Walde, Sabine.
2006.
Experimentson the automatic induction of Germansemantic verb classes.
ComputationalLinguistics, 32(2):159?194.Scott, Sam and Stan Matwin.
1998.
Textclassification using WordNet hypernyms.In Proceedings of the COLING-ACLWorkshop on Usage of WordNet in NaturalLanguage Processing Systems, pages 45?51,Montreal, Canada.Tsang, Vivian and Suzanne Stevenson.
2004.Calculating semantic distance betweenword sense probability distributions.
InProceedings of the Eighth Conference onComputational Natural Language Learning(CoNLL-2004), pages 81?88, Boston,MA, USA.Tsang, Vivian and Suzanne Stevenson.2006.
Context comparison as a minimumcost flow problem.
In Proceedings ofHLT-NAACL 2006 Workshop onTextgraphs: Graph-based Algorithms forNatural Language Processing, pages 97?104,New York, NY.Weeds, Julie.
2003.Measures and Applicationsof Lexical Distributional Similarity.
Ph.D.thesis, University of Sussex, Sussex, UK.Weeds, Julie, David Weir, and DianaMcCarthy.
2004.
Characterising measuresof lexical distributional similarity.
InProceedings of the 20th InternationalConference of Computational Linguistics(COLING-2004), pages 1015?1021, Geneva,Switzerland.Wilcoxon, Frank.
1945.
Individualcomparisons by ranking methods.Biometrics, 1:80?83.Wu, Zhibiao and Martha Palmer.
1994.
Verbsemantics and lexical selection.
InProceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics,pages 133?138, Las Cruces, New Mexico,USA.Xu, Wei, Xin Liu, and Yihong Gong.
2003.Document clustering based onnon-negative matrix factorization.
InProceedings of the 26th ACM SIGIRInternational Conference on Research andDevelopment in Information Retrieval,pages 267?273, Toronto, Canada.69
