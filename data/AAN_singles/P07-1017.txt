Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128?135,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGenerating Complex Morphology for Machine TranslationEinat Minkov?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, USAeinatm@cs.cmu.eduKristina ToutanovaMicrosoft ResearchRedmond, WA, USAkristout@microsoft.comHisami SuzukiMicrosoft ResearchRedmond, WA, USAhisamis@microsoft.comAbstractWe present a novel method for predicting in-flected word forms for generating morpho-logically rich languages in machine trans-lation.
We utilize a rich set of syntacticand morphological knowledge sources fromboth source and target sentences in a prob-abilistic model, and evaluate their contribu-tion in generating Russian and Arabic sen-tences.
Our results show that the proposedmodel substantially outperforms the com-monly used baseline of a trigram target lan-guage model; in particular, the use of mor-phological and syntactic features leads tolarge gains in prediction accuracy.
We alsoshow that the proposed method is effectivewith a relatively small amount of data.1 IntroductionMachine Translation (MT) quality has improvedsubstantially in recent years due to applying dataintensive statistical techniques.
However, state-of-the-art approaches are essentially lexical, consider-ing every surface word or phrase in both the sourcesentence and the corresponding translation as an in-dependent entity.
A shortcoming of this word-basedapproach is that it is sensitive to data sparsity.
This isan issue of importance as aligned corpora are an ex-pensive resource, which is not abundantly availablefor many language pairs.
This is particularly prob-lematic for morphologically rich languages, whereword stems are realized in many different surfaceforms, which exacerbates the sparsity problem.?
This research was conducted during the author?s intern-ship at Microsoft Research.In this paper, we explore an approach in whichwords are represented as a collection of morpholog-ical entities, and use this information to aid in MTfor morphologically rich languages.
Our goal is two-fold: first, to allow generalization over morphologyto alleviate the data sparsity problem in morphologygeneration.
Second, to model syntactic coherence inthe form of morphological agreement in the targetlanguage to improve the generation of morphologi-cally rich languages.
So far, this problem has beenaddressed in a very limited manner in MT, most typ-ically by using a target language model.In the framework suggested in this paper, we traina model that predicts the inflected forms of a se-quence of word stems in a target sentence, giventhe corresponding source sentence.
We use wordand word alignment information, as well as lexi-cal resources that provide morphological informa-tion about the words on both the source and targetsides.
Given a sentence pair, we also obtain syntacticanalysis information for both the source and trans-lated sentences.
We generate the inflected forms ofwords in the target sentence using all of the availableinformation, using a log-linear model that learns therelevant mapping functions.As a case study, we focus on the English-Russianand English-Arabic language pairs.
Unlike English,Russian and Arabic have very rich systems of mor-phology, each with distinct characteristics.
Trans-lating from a morphology-poor to a morphology-rich language is especially challenging since de-tailed morphological information needs to be de-coded from a language that does not encode this in-formation or does so only implicitly (Koehn, 2005).We believe that these language pairs are represen-128tative in this respect and therefore demonstrate thegenerality of our approach.There are several contributions of this work.
First,we propose a general approach that shows promisein addressing the challenges of MT into morpholog-ically rich languages.
We show that the use of bothsyntactic and morphological information improvestranslation quality.
We also show the utility ofsource language information in predicting the wordforms of the target language.
Finally, we achievethese results with limited morphological resourcesand training data, suggesting that the approach isgenerally useful for resource-scarce language pairs.2 Russian and Arabic MorphologyTable 1 describes the morphological features rele-vant to Russian and Arabic, along with their possiblevalues.
The rightmost column in the table refers tothe morphological features that are shared by Rus-sian and Arabic, including person, number, genderand tense.
While these features are fairly generic(they are also present in English), note that Rus-sian includes an additional gender (neuter) and Ara-bic has a distinct number notion for two (dual).
Acentral dimension of Russian morphology is casemarking, realized as suffixation on nouns and nom-inal modifiers1.
The Russian case feature includessix possible values, representing the notions of sub-ject, direct object, location, etc.
In Arabic, like otherSemitic languages, word surface forms may includeproclitics and enclitics (or prefixes and suffixes aswe refer to them in this paper), concatenated to in-flected stems.
For nouns, prefixes include conjunc-tions (wa: ?and?, fa: ?and, so?
), prepositions (bi:?by, with?, ka: ?like, such as?, li: ?for, to?)
and a de-terminer, and suffixes include possessive pronouns.Verbal prefixes include conjunction and negation,and suffixes include object pronouns.
Both objectand possessive pronouns are captured by an indica-tor function for its presence or absence, as well asby the features that indicate their person, numberand gender.
As can be observed from the table, alarge number of surface inflected forms can be gen-erated by the combination of these features, making1Case marking also exists in Arabic.
However, in many in-stances, it is realized by diacritics which are ignored in standardorthography.
In our experiments, we include case marking inArabic only when it is reflected in the orthography.the morphological generation of these languages anon-trivial task.Morphologically complex languages also tend todisplay a rich system of agreements.
In Russian, forexample, adjectives agree with head nouns in num-ber, gender and case, and verbs agree with the sub-ject noun in person and number (past tense verbsagree in gender and number).
Arabic has a similarlyrich system of agreement, with unique characteris-tics.
For example, in addition to agreement involv-ing person, number and gender, it also requires a de-terminer for each word in a definite noun phrase withadjectival modifiers; in a noun compound, a deter-miner is attached to the last noun in the chain.
Also,non-human subject plural nouns require the verb tobe inflected in a singular feminine form.
Generatingthese morphologically complex languages is there-fore more difficult than generating English in termsof capturing the agreement phenomena.3 Related WorkThe use of morphological features in language mod-elling has been explored in the past for morphology-rich languages.
For example, (Duh and Kirchhoff,2004) showed that factored language models, whichconsider morphological features and use an opti-mized backoff policy, yield lower perplexity.In the area of MT, there has been a large bodyof work attempting to modify the input to a transla-tion system in order to improve the generated align-ments for particular language pairs.
For example,it has been shown (Lee, 2004) that determiner seg-mentation and deletion in Arabic sentences in anArabic-to-English translation system improves sen-tence alignment, thus leading to improved over-all translation quality.
Another work (Koehn andKnight, 2003) showed improvements by splittingcompounds in German.
(Nie?en and Ney, 2004)demonstrated that a similar level of alignment qual-ity can be achieved with smaller corpora applyingmorpho-syntactic source restructuring, using hierar-chical lexicon models, in translating from Germaninto English.
(Popovic?
and Ney, 2004) experimentedsuccessfully with translating from inflectional lan-guages into English making use of POS tags, wordstems and suffixes in the source language.
More re-cently, (Goldwater and McClosky, 2005) achievedimprovements in Czech-English MT, optimizing a129Features Russian Arabic BothPOS (11 categories) (18 categories)Person 1,2,3Number dual sing(ular), pl(ural)Gender neut(er) masc(uline), fem(inine)Tense gerund present, past, future, imperativeMood subjunctive, jussiveCase dat(ive), prep(ositional), nom(inative), acc(usative), gen(itive)instr(umental)Negation yes, noDeterminer yes, noConjunction wa, fa, nonePreposition bi, ka, li, noneObjectPronoun yes, noPers/Numb/Gend of pronoun, nonePossessivePronoun Same as ObjectPronounTable 1: Morphological features used for Russian and Arabicset of possible source transformations, incorporat-ing morphology.
In general, this line of work fo-cused on translating from morphologically rich lan-guages into English; there has been limited researchin MT in the opposite direction.
Koehn (2005) in-cludes a survey of statistical MT systems in both di-rections for the Europarl corpus, and points out thechallenges of this task.
A recent work (El-Kahloutand Oflazer, 2006) experimented with English-to-Turkish translation with limited success, suggestingthat inflection generation given morphological fea-tures may give positive results.In the current work, we suggest a probabilisticframework for morphology generation performed aspost-processing.
It can therefore be considered ascomplementary to the techniques described above.Our approach is general in that it is not specific toa particular language pair, and is novel in that it al-lows modelling of agreement on the target side.
Theframework suggested here is most closely related to(Suzuki and Toutanova, 2006), which uses a proba-bilistic model to generate Japanese case markers forEnglish-to-Japanese MT.
This work can be viewedas a generalization of (Suzuki and Toutanova, 2006)in that our model generates inflected forms of words,and is not limited to generating a small, closed set ofcase markers.
In addition, the morphology genera-tion problem is more challenging in that it requireshandling of complex agreement phenomena alongmultiple morphological dimensions.4 Inflection Prediction FrameworkIn this section, we define the task of of morphologi-cal generation as inflection prediction, as well as thelexical operations relevant for the task.4.1 Morphology Analysis and GenerationMorphological analysis can be performed by ap-plying language specific rules.
These may includea full-scale morphological analysis with contextualdisambiguation, or, when such resources are notavailable, simple heuristic rules, such as regardingthe last few characters of a word as its morphogicalsuffix.
In this work, we assume that lexicons LS andLT are available for the source and translation lan-guages, respectively.
Such lexicons can be createdmanually, or automatically from data.
Given a lexi-con L and a surface word w, we define the followingoperations:?
Stemming - let Sw = {s1, ..., sl} be the set ofpossible morphological stems (lemmas) of waccording to L.2?
Inflection - let Iw = {i1, ..., im} be the set ofsurface form words that have the same stem asw.
That is, i ?
Iw iff Si?Sw 6= ?.?
Morphological analysis - let Aw = {a1, ..., av}be the set of possible morphological analysesfor w. A morphological analysis a is a vector ofcategorical values, where the dimensions andpossible values for each dimension in the vectorrepresentation space are defined by L.4.2 The TaskWe assume that we are given aligned sentence pairs,where a sentence pair includes a source and a tar-2Multiple stems are possible due to ambiguity in morpho-logical analysis.130NN+sg+nom+neuttheDETallocation of resources has completedNN+sg PREP NN+pl AUXV+sg VERB+pastpart?????????????NN+sg+gen+pl+masc????????VERB+perf+pass+part+neut+sg????????
?raspredelenie resursov zavershenoFigure 1: Aligned English-Russian sentence pairwith syntactic and morphological annotationget sentence, and lexicons LS and LT that supportthe operations described in the section above.
Leta sentence w1, ...wt, ...wn be the output of a MTsystem in the target language.
This sentence canbe converted into the corresponding stem set se-quence S1, ...St, ...Sn, applying the stemming op-eration.
Then the task is, for every stem set St inthe output sentence, to predict an inflection yt fromits inflection set It.
The predicted inflections shouldboth reflect the meaning conveyed by the source sen-tence, and comply with the agreement rules of thetarget language.
3Figure 1 shows an example of an aligned English-Russian sentence pair: on the source (English) side,POS tags and word dependency structure are indi-cated by solid arcs.
The alignments between En-glish and Russian words are indicated by the dot-ted lines.
The dependency structure on the Russianside, indicated by solid arcs, is given by a treelet MTsystem in our case (see Section 6.1), projected fromthe word dependency structure of English and wordalignment information.
Note that the Russian sen-tence displays agreement in number and gender be-tween the subject noun (raspredelenie) and the pred-icate (zaversheno); note also that resursov is in gen-itive case, as it modifies the noun on its left.5 Models for Inflection Prediction5.1 A Probabilistic ModelOur learning framework uses a Maximum EntropyMarkov model (McCallum et al, 2000).
The modeldecomposes the overall probability of a predictedinflection sequence into a product of local proba-bilities for individual word predictions.
The local3That is, assuming that the stem sequence that is output bythe MT system is correct.probabilities are conditioned on the previous k pre-dictions.
The model implemented here is of secondorder: at any decision point t we condition the prob-ability distribution over labels on the previous twopredictions yt?1 and yt?2 in addition to the given(static) word context from both the source and tar-get sentences.
That is, the probability of a predictedinflection sequence is defined as follows:p(y | x) =n?t=1p(yt | yt?1, yt?2, xt), yt ?
Itwhere xt denotes the given context at position tand It is the set of inflections corresponding to St,from which the model should choose yt.The features we constructed pair up predicates onthe context ( x?, yt?1, yt?2) and the target label (yt).In the suggested framework, it is straightforward toencode the morphological properties of a word, inaddition to its surface inflected form.
For example,for a particular inflected word form yt and its con-text, the derived paired features may include:?k ={1 if surface word yt is y?
and s?
?
St+10 otherwise?k+1 ={ 1 if Gender(yt) =?Fem?
and Gender(yt?1) =?Fem?0 otherwiseIn the first example, a given neighboring stem setSt+1 is used as a context feature for predicting thetarget word yt.
The second feature captures the gen-der agreement with the previous word.
This is possi-ble because our model is of second order.
Thus, wecan derive context features describing the morpho-logical properties of the two previous predictions.4Note that our model is not a simple multi-class clas-sifier, because our features are shared across mul-tiple target labels.
For example, the gender fea-ture above applies to many different inflected forms.Therefore, it is a structured prediction model, wherethe structure is defined by the morphological proper-ties of the target predictions, in addition to the wordsequence decomposition.5.2 Feature CategoriesThe information available for estimating the distri-bution over yt can be split into several categories,4Note that while we decompose the prediction task left-to-right, an appealing alternative is to define a top-down decompo-sition, traversing the dependency tree of the sentence.
However,this requires syntactic analysis of sufficient quality.131corresponding to feature source.
The first ma-jor distinction is monolingual versus bilingual fea-tures: monolingual features refer only to the context(and predicted label) in the target language, whilebilingual features have access to information in thesource sentences, obtained by traversing the wordalignment links from target words to a (set of) sourcewords, as shown in Figure 1.Both monolingual and bilingual features can befurther split into three classes: lexical, morpholog-ical and syntactic.
Lexical features refer to surfaceword forms, as well as their stems.
Since our modelis of second order, our monolingual lexical fea-tures include the features of a standard word trigramlanguage model.
Furthermore, since our model isdiscriminative (predicting word forms given theirstems), the monolingual lexical model can use stemsin addition to predicted words for the left and cur-rent position, as well as stems from the right con-text.
Morphological features are those that refer tothe features given in Table 1.
Morphological infor-mation is used in describing the target label as wellas its context, and is intended to capture morpho-logical generalizations.
Finally, syntactic featurescan make use of syntactic analyses of the sourceand target sentences.
Such analyses may be derivedfor the target language, using the pre-stemmed sen-tence.
Without loss of generality, we will use herea dependency parsing paradigm.
Given a syntacticanalysis, one can construct syntactic features; for ex-ample, the stem of the parent word of yt.
Syntacticfeatures are expected to be useful in capturing agree-ment phenomena.5.3 FeaturesTable 2 gives the full set of suggested features forRussian and Arabic, detailed by type.
For monolin-gual lexical features, we consider the stems of thepredicted word and its immediately adjacent words,in addition to traditional word bigram and trigramfeatures.
For monolingual morphological features,we consider the morphological attributes of the twopreviously predicted words and the current predic-tion; for monolingual syntactic features, we use thestem of the parent node.The bilingual features include the set of wordsaligned to the focus word at position t, where theyare treated as bag-of-words, i.e., each aligned wordFeature categories InstantiationsMonolingual lexicalWord stem st?1,st?2,st,st+1Predicted word yt, yt?1, yt?2Monolingual morphologicalf : POS, Person, Number, Gender, Tense f(yt?2),f(yt?1),f(yt)Neg, Det, Prep, Conj, ObjPron, PossPronMonolingual syntacticParent stem sHEAD(t)Bilingual lexicalAligned word set Al Alt, Alt?1, Alt+1Bilingual morph & syntacticf : POS, Person, Number, Gender, Tense f(Alt), f(Alt?1),Neg, Det, Prep, Conj, ObjPron, PossPron, f(Alt+1), f(AlHEAD(t))CompTable 2: The feature set suggested for English-Russian and English-Arabic pairsis assigned a separate feature.
Bilingual lexical fea-tures can refer to words aligned to yt as all as wordsaligned to its immediate neighbors yt?1 and yt+1.Bilingual morphological and syntactic features re-fer to the features of the source language, whichare expected to be useful for predicting morphol-ogy in the target language.
For example, the bilin-gual Det (determiner) feature is computed accord-ing to the source dependency tree: if a child of aword aligned to wt is a determiner, then the fea-ture value is assigned its surface word form (suchas a or the).
The bilingual Prep feature is com-puted similarly, by checking the parent chain of theword aligned to wt for the existence of a preposi-tion.
This feature is hoped to be useful for predict-ing Arabic inflected forms with a prepositional pre-fix, as well as for predicting case marking in Rus-sian.
The bilingual ObjPron and PossPron featuresrepresent any object pronoun of the word aligned towt and a preceding possessive pronoun, respectively.These features are expected to map to the object andpossessive pronoun features in Arabic.
Finally, thebilingual Compound feature checks whether a wordappears as part of a noun compound in the Englishsource.
f this is the case, the feature is assigned thevalue of ?head?
or ?dependent?.
This feature is rel-evant for predicting a genitive case in Russian anddefiniteness in Arabic.6 Experimental SettingsIn order to evaluate the effectiveness of the sug-gested approach, we performed reference experi-ments, that is, using the aligned sentence pairs of132Data Eng-Rus Eng-AraAvg.
sentlen Eng Rus Eng AraTraining 1M 470K14.06 12.90 12.85 11.90Development 1,000 1,00013.73 12.91 13.48 12.90Test 1,000 1,00013.61 12.84 8.49 7.50Table 3: Data set statistics: corpus size and averagesentence length (in words)reference translations rather than the output of anMT system as input.5 This allows us to evaluateour method with a reduced noise level, as the wordsand word order are perfect in reference translations.These experiments thus constitute a preliminary stepfor tackling the real task of inflecting words in MT.6.1 DataWe used a corpus of approximately 1 million alignedsentence pairs for English-Russian, and 0.5 millionpairs for English-Arabic.
Both corpora are from atechnical (software manual) domain, which we be-lieve is somewhat restricted along some morpho-logical dimensions, such as tense and person.
Weused 1,000 sentence pairs each for development andtesting for both language pairs.
The details of thedatasets used are given in Table 3.The sentence pairs were word-aligned usingGIZA++ (Och and Ney, 2000) and submitted to atreelet-based MT system (Quirk et al, 2005), whichuses the word dependency structure of the sourcelanguage and projects word dependency structure tothe target language, creating the structure shown inFigure 1 above.6.2 LexiconTable 4 gives some relevant statistics of the lexiconswe used.
For Russian, a general-domain lexicon wasavailable to us, consisting of about 80,000 lemmas(stems) and 9.4 inflected forms per stem.6 Limitingthe lexicon to word types that are seen in the train-ing set reduces its size substantially to about 14,000stems, and an average of 3.8 inflections per stem.We will use this latter ?domain-adapted?
lexicon inour experiments.5In this case, yt should equal wt, according to the task defi-nition.6The averages reported in Table 4 are by type and do notconsider word frequencies in the data.Source Stems Avg(| I |) Avg(| S |)Rus.
Lexicon 79,309 9.4Lexicon ?
Train 13,929 3.8 1.6Ara.
Lexicon ?
Train 12,670 7.0 1.7Table 4: Lexicon statisticsFor Arabic, as a full-size Arabic lexicon was notavailable to us, we used the Buckwalter morpholog-ical analyzer (Buckwalter, 2004) to derive a lexicon.To acquire the stemming and inflection operators, wesubmit all words in our training data to the Buckwal-ter analyzer.
Note that Arabic displays a high levelof ambiguity, each word corresponding to many pos-sible segmentations and morphological analyses; weconsidered all of the different stems returned by theBuckwalter analyzer in creating a word?s stem set.The lexicon created in this manner contains 12,670distinct stems and 89,360 inflected forms.For the generation of word features, we only con-sider one dominant analysis for any surface wordfor simplicity.
In case of ambiguity, we consideredonly the first (arbitrary) analysis for Russian.
ForArabic, we apply the following heuristic: use themost frequent analysis estimated from the gold stan-dard labels in the Arabic Treebank (Maamouri et al,2005); if a word does not appear in the treebank, wechoose the first analysis returned by the Buckwal-ter analyzer.
Ideally, the best word analysis shouldbe provided as a result of contextual disambiguation(e.g., (Habash and Rambow, 2005)); we leave thisfor future work.6.3 BaselineAs a baseline, we pick a morphological inflection ytat random from It.
This random baseline serves asan indicator for the difficulty of the problem.
An-other more competitive baseline we implementedis a word trigram language model (LM).
The LMswere trained using the CMU language modellingtoolkit (Clarkson and Rosenfeld, 1997) with defaultsettings on the training data described in Table 3.6.4 ExperimentsIn the experiments, our primary goal is to evaluatethe effectiveness of the proposed model using allfeatures available to us.
Additionally, we are inter-ested in knowing the contribution of each informa-tion source, namely of morpho-syntactic and bilin-gual features.
Therefore, we study the performance133of models including the full feature schemata as wellas models that are restricted to feature subsets ac-cording to the feature types as described in Section5.2.
The models are as follows: Monolingual-Word,including LM-like and stem n-gram features only;Bilingual-Word, which also includes bilingual lex-ical features;7 Monolingual-All, which has accessto all the information available in the target lan-guage, including morphological and syntactic fea-tures; and finally, Bilingual-All, which includes allfeature types from Table 2.For each model and language, we perform featureselection in the following manner.
The features arerepresented as feature templates, such as ?POS=X?,which generate a set of binary features correspond-ing to different instantiations of the template, as in?POS=NOUN?.
In addition to individual features, con-junctions of up to three features are also consideredfor selection (e.g., ?POS=NOUN & Number=plural?
).Every conjunction of feature templates consideredcontains at least one predicate on the prediction yt,and up to two predicates on the context.
The featureselection algorithm performs a greedy forward step-wise feature selection on the feature templates so asto maximize development set accuracy.
The algo-rithm is similar to the one described in (Toutanova,2006).
After this process, we performed some man-ual inspection of the selected templates, and finallyobtained 11 and 36 templates for the Monolingual-All and Bilingual-All settings for Russian, respec-tively.
These templates generated 7.9 million and9.3 million binary feature instantiations in the fi-nal model, respectively.
The corresponding num-bers for Arabic were 27 feature templates (0.7 mil-lion binary instantiations) and 39 feature templates(2.3 million binary instantiations) for Monolingual-All and Bilingual-All, respectively.7 Results and DiscussionTable 5 shows the accuracy of predicting word formsfor the baseline and proposed models.
We report ac-curacy only on words that appear in our lexicons.Thus, punctuation, English words occurring in thetarget sentence, and words with unknown lemmasare excluded from the evaluation.
The reported ac-curacy measure therefore abstracts away from the is-7Overall, this feature set approximates the information thatis available to a state-of-the-art statistical MT system.Model Eng-Rus Eng-AraRandom 31.7 16.3LM 77.6 31.7Monolingual Word 85.1 69.6Bilingual Word 87.1 71.9Monolingual All 87.1 71.6Bilingual All 91.5 73.3Table 5: Accuracy (%) results by modelsue of incomplete coverage of the lexicon.
Whenwe encounter these words in the true MT scenario,we will make no predictions about them, and simplyleave them unmodified.
In our current experiments,in Russian, 68.2% of all word tokens were in Cyril-lic, of which 93.8% were included in our lexicon.In Arabic, 85.5% of all word tokens were in Arabiccharacters, of which 99.1% were in our lexicon.8The results in Table 5 show that the suggestedmodels outperform the language model substantiallyfor both languages.
In particular, the contribution ofboth bilingual and non-lexical features is notewor-thy: adding non-lexical features consistently leadsto 1.5% to 2% absolute gain in both monolingualand bilingual settings in both language pairs.
Weobtain a particularly large gain in the Russian bilin-gual case, in which the absolute gain is more than4%, translating to 34% error rate reduction.
Addingbilingual features has a similar effect of gainingabout 2% (and 4% for Russian non-lexical) in ac-curacy over monolingual models.
The overall accu-racy is lower in Arabic than in Russian, reflectingthe inherent difficulty of the task, as indicated by therandom baseline (31.7 in Russian vs. 16.3 in Ara-bic).In order to evaluate the effectiveness of the modelin alleviating the data sparsity problem in morpho-logical generation, we trained inflection predictionmodels on various subsets of the training data de-scribed in Table 3, and tested their accuracy.
Theresults are given in Figure 2.
We can see that with asfew as 5,000 training sentences pairs, the model ob-tains much better accuracy than the language model,which is trained on data that is larger by a few ordersof magnitude.
We also note that the learning curve8For Arabic, the inflection ambiguity was extremely high:there were on average 39 inflected forms per stem set in ourdevelopment corpus (per token), as opposed to 7 in Russian.We therefore limited the evaluation of Arabic to those stems thathave up to 30 inflected forms, resulting in 17 inflected forms perstem set on average in the development data.1345055606570758085905 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100Training data size (x1,000)Accuracy(%)RUS-bi-wordRUS-bi-allARA-bi-wordARA-bi-allFigure 2: Accuracy, varying training data sizebecomes less steep as we use more training data,suggesting that the models are successfully learninggeneralizations.We have also manually examined some repre-sentative cases where the proposed model failed tomake a correct prediction.
In both Russian and Ara-bic, a very common pattern was a mistake in pre-dicting the gender (as well as number and person inArabic) of pronouns.
This may be attributed to thefact that the correct choice of the pronoun requirescoreference resolution, which is not available in ourmodel.
A more thorough analysis of the results willbe helpful to bring further improvements.8 Conclusions and Future WorkWe presented a probabilistic framework for mor-phological generation given aligned sentence pairs,incorporating morpho-syntactic information fromboth the source and target sentences.
The re-sults, using reference translations, show that the pro-posed models achieve substantially better accuracythan language models, even with a relatively smallamount of training data.
Our models using morpho-syntactic information also outperformed models us-ing only lexical information by a wide margin.
Thisresult is very promising for achieving our ultimategoal of improving MT output by using a special-ized model for target language morphological gener-ation.
Though this goal is clearly outside the scopeof this paper, we conducted a preliminary experi-ment where an English-to-Russian MT system wastrained on a stemmed version of the aligned data andused to generate stemmed word sequences, whichwere then inflected using the suggested framework.This simple integration of the proposed model withthe MT system improved the BLEU score by 1.7.The most obvious next step of our research, there-fore, is to further pursue the integration of the pro-posed model to the end-to-end MT scenario.There are multiple paths for obtaining further im-provements over the results presented here.
Theseinclude refinement in feature design, word analysisdisambiguation, morphological and syntactic anal-ysis on the source English side (e.g., assigning se-mantic role tags), to name a few.
Another area ofinvestigation is capturing longer-distance agreementphenomena, which can be done by implementing aglobal statistical model, or by using features fromdependency trees more effectively.ReferencesTim Buckwalter.
2004.
Buckwalter arabic morphological ana-lyzer version 2.0.Philip Clarkson and Roni Rosenfeld.
1997.
Statistical languagemodelling using the CMU cambridge toolkit.
In Eurospeech.Kevin Duh and Kathrin Kirchhoff.
2004.
Automatic learning oflanguage model structure.
In COLING.Ilknur Durgar El-Kahlout and Kemal Oflazer.
2006.
Initial ex-plorations in English to Turkish statistical machine transla-tion.
In NAACL workshop on statistical machine translation.Sharon Goldwater and David McClosky.
2005.
Improving sta-tistical MT through morphological analysis.
In EMNLP.Nizar Habash and Owen Rambow.
2005.
Arabic tokenization,part-of-speech tagging and morphological disambiguation inone fell swoop.
In ACL.Philipp Koehn and Kevin Knight.
2003.
Empirical methods forcompound splitting.
In EACL.Philipp Koehn.
2005.
Europarl: A parallel corpus for statisticalmachine translation.
In MT Summit.Young-Suk Lee.
2004.
Morphological analysis for statisticalmachine translation.
In HLT-NAACL.Mohamed Maamouri, Ann Bies, Tim Buckwalter, and HubertJin.
2005.
Arabic Treebank: Part 1 v 3.0.
Linguistic DataConsortium.Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira.2000.
Maximum entropy markov models for informationextraction and segmentation.
In ICML.Sonja Nie?en and Hermann Ney.
2004.
Statistical machinetranslation with scarce resources using morpho-syntactic in-formation.
Computational Linguistics, 30(2):181?204.Franz Josef Och and Hermann Ney.
2000.
Improved statisticalalignment models.
In ACL.Maja Popovic?
and Hermann Ney.
2004.
Towards the use ofword stems and suffixes for statistical machine translation.In LREC.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
Depen-dency tree translation: Syntactically informed phrasal SMT.In ACL.Hisami Suzuki and Kristina Toutanova.
2006.
Learning to pre-dict case markers in Japanese.
In COLING-ACL.Kristina Toutanova.
2006.
Competitive generative models withstructure learning for NLP classification tasks.
In EMNLP.135
