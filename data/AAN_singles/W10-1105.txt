Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 29?37,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAssessment of Utilityin Web Mining for the Domain of Public HealthPeter von Etter, Silja Huttunen, Arto Vihavainen,Matti Vuorinen and Roman YangarberDepartment of Computer ScienceUniversity of Helsinki, FinlandFirst.Last@cs.helsinki.fiAbstractThis paper presents ongoing work on applica-tion of Information Extraction (IE) technologyto domain of Public Health, in a real-worldscenario.
A central issue in IE is the qualityof the results.
We present two novel points.First, we distinguish the criteria for quality:the objective criteria that measure correctnessof the system?s analysis in traditional terms(F-measure, recall and precision), and, on theother hand, subjective criteria that measure theutility of the results to the end-user.Second, to obtain measures of utility, we buildan environment that allows users to interactwith the system by rating the analyzed con-tent.
We then build and compare several clas-sifiers that learn from the user?s responses topredict the relevance scores for new events.We conduct experiments with learning to pre-dict relevance, and discuss the results and theirimplications for text mining in the domain ofPublic Health.1 IntroductionWe describe an on-going project for text mining inthe domain of Public Health.
The aim of the projectis to build a system for providing decision supportto Public Health (PH) professionals and officials, inthe task of Epidemic Surveillance.Epidemic surveillance may be sub-divided intoindicator-based vs. event-based surveillance, (Hart-ley et al, 2010).
Whereas the former is based onstructured, quantitative data, which is collected, e.g.,from national or international clinical laboratoriesor databases, and is of reliable quality, the latteris much more noisy, and relies on ?alert and ru-mour scanning?, particularly from open-source me-dia, such as on-line news sites.
While the latterkind of information sources are less reliable over-all, they nonetheless constitute a crucial channel ofinformation in PH.
This is because the media are ex-tremely adept at picking up isolated cases and weaksignals?which may be indicative of emergence ofimportant events, such as an incipient epidemic orcritical change in a public-health situation?and inmany cases they can do so much more swiftly thanofficial channels.
National and supra-national (e.g.,European-level) Health Authorities require timelyinformation about threats posed to the public byemerging infectious diseases and epidemics.
There-fore, these Agencies rely on media-monitoring as amatter of routine, on a continual basis as part of theirday-to-day operations.The system described in this paper, PULS, is de-signed to support Epidemic Surveillance by moni-toring open-source media for reports about events ofpotential significance to Public Health (Yangarberand Steinberger, 2009).
We focus in this paper onnews articles mentioning incidents of infectious dis-eases.
The system does not make decisions, but pro-vides decision support, by filtering massive volumesof information and trying to identify those cases thatshould be brought to the attention of epidemic intel-ligence officers (EIO)?public health specialists en-gaged in epidemic surveillance.This is an inter-disciplinary effort.
The systembuilds on methods from text mining and computa-tional linguistics to identify the items of potentialinterest (Grishman et al, 2003).
The EIOs, on theother hand, are medical professionals, and are gen-erally not trained in computational methods.
There-fore the tools that they use must be intuitive and must29not overwhelm the user with volume or complexity.A convenient baseline for comparison is keyword-based search, as provided by search engines andnews aggregators.
Systems that rely on keyword-matching to find articles related to infectious threatsand epidemics quickly overwhelm the user with avast amount of news items, much of which is noise.We have tuned PULS, the ?Pattern-based Under-standing and Learning System,?
to support EpidemicSurveillance in several phases.
PULS is a collabo-rative effort with MedISys, a system for gatheringepidemic intelligence built by the European Com-mission (EC) at the Joint Research Centre (JRC)in Ispra, Italy.
First, MedISys finds news articlesfrom thousands of on-line sources around the world,identifies articles potentially relevant to EpidemicSurveillance, using a broad keyword-based Websearch, and sends them via an RSS feed to PULSon a continual basis.
Second, PULS employs ?fact-finding?
technology, Information Extraction (IE), todetermine exactly what happened in each article:who was affected by what disease/condition, whereand when?creating a structured record that is storedin the database.
Articles that do not trigger cre-ation of a database record are discarded.
A thirdcomponent then determines the relevance of the se-lected articles?and cases that they describe?to thedomain of Public Health, specifically to EpidemicSurveillance.Traditionally in IE research, performance hasbeen measured in terms of formal correctness?howaccurately the system is able to analyze the article(Hirschman, 1998).
In this paper we argue the needfor other measures of performance for text mining,using as a case study our application of Web miningto the domain of Public Health.
In the next section,we lay down criteria for judging quality, and presentthe approach taken in our system.
Section 3 out-lines the organisation of the system, and Section 4presents in detail our experiments with automatic as-signment of relevance scores.
In the final section wediscuss the results and outline next steps.2 Criteria for qualityIn this section we take a critical view at traditionalmeasures of quality, in text analysis in general, andIE in particular.
What defines quality most appropri-ately for our application, and how should we mea-sure quality?
We propose the following taxonomyof quality in our context:?
Objective: system?s perspective?
Correctness?
Confidence?
Subjective: user?s perspective?
Utility or relevance?
ReliabilityAt the top level, we distinguish objective vs. sub-jective measures.
Most IE research has focused oncorrectness over the last two decades, e.g., in theMUC and ACE initiatives (Hirschman, 1998; ACE,2004).
Correctness is a measure of how accuratelythe system extracts the semantics from an articleof text, in terms of matching the system?s answersto a set of answers pre-defined by human annota-tors.
In our context, a set of articles is annotatedwith a ?gold-standard?
set of database records, eachrecord containing fields like: the name of the dis-ease/infectious agent, the location/country of the in-cident, the date of the incident, the number of vic-tims, whether they are human or animal, whetherthey survived, etc.
Then the system?s response canbe compared to the gold standard and correctnesscan be computed in terms of recall and precision,F-measure, accuracy, etc.
?counting how many ofthe fields in each record were correctly extracted.This approach to quality is similar to the approachtaken in other areas of computational linguistics:how many structures in the text were correctly iden-tified, how many were missed, and how many spuri-ous structures were introduced.Confidence has been studied as well, to estimatethe probability of the correctness of the system?s an-swer, e.g., in (Culotta and McCallum, 2004).
Oursystem computes confidence using discourse-levelcues, (Huttunen et al, 2002): e.g., confidence de-creases as the distance between event trigger andevent attributes increases?the sentence that men-tions that someone has fallen ill or died is far fromthe mention of the disease.
Confidence also de-pends on uniqueness of attributes?e.g., if a doc-ument mentions only one country, the system has30more confidence that an event referring to this coun-try is correct.On the subjective side, utility, or relevance, askshow useful the result is to the user.
There are severalpoints to note.
First, it is clearly a highly subjectivemeasure, not easy to capture in exact terms.
Sec-ond, it is ?orthogonal?
to correctness in the sensethat from the user?s perspective utility matters irre-spective of correctness.
For example, an extractedcase can be 100% correct, yet have very low utilityto the user, (for the task of epidemic surveillance)?a perfectly extracted event that happened too longago would not matter in the current context.
Con-versely, every slot in the record may be extractederroneously, and yet the event may be of great im-portance and value to the user.
We focus specificallyon relevance vs. correctness.Given the current performance ?ceilings?
of 70-80% F-measure in state-of-the-art IE, what does cor-rectness of x% mean in practice?
It likely meansthat if x > y then a system achieving F-measurex is better to have than one achieving y.
But whatdoes it say about utility?
In the best case, correct-ness may be correlated with utility, in the worst caseit is independent of utility (e.g., if the system hap-pens to achieve high correctness on events from thepast, which have low relevance).
Since we are tar-geting a specific user base, the user?s perspectivemust be taken into account when estimating quality,not (only) the system?s perspective.
This implies theneed for automatic assignment of relevance scoresto analyzed events or documents.Finally, reliability measures whether the reportedevent is ?true?.
The relevance of extracted fact maybe high, but is it credible?
Can the information betrusted?
We list this criterion for quality for com-pleteness, since it is the ultimate goal of any surveil-lance process.
However, answering this requires agreat deal of knowledge external to the system, thatcan only be obtained by the human user through adetailed down-stream verification process.
The sys-tem may provide some support for determining reli-ability, e.g., by tracking the performance of differentinformation sources over time, since the reliabilityof the facts extracted from an article is related to thereliability of the source.
It may be possible to clas-sify Web-based sources according to their credibil-ity; some sources may habitually withhold informa-tion (for fear of impact to tourism, trade, etc.
); othersites may try to attract readership by exaggeratedclaims (e.g., tabloids).
On the other hand, clearlydisreputable sites may carry true information.
Thismeasure of quality is beyond the scope of this paper.3 The System: BackgroundPULS, the Pattern-based Understanding and Learn-ing System, is developed at the University ofHelsinki to extract factual information from plaintext.
PULS has been adapted to analyse texts forEpidemic Surveillance.1The components of PULS have been describedin detail previously, (Yangarber and Steinberger,2009; Steinberger et al, 2008; Yangarber et al,2007).
In several respects, it is similar to otherexisting systems for automated epidemic surveil-lance, viz., BioCaster (Doan et al, 2008), MedISysand PULS (Yangarber and Steinberger, 2009),HealthMap (Freifeld et al, 2008), and others (Lingeet al, 2009).PULS relies on EC-JRC?s MedISys for IR (in-formation retrieval)?MedISys performs a broadWeb search, using a set of boolean keyword-basedqueries, (Steinberger et al, 2008).
The result isa continuous stream of potentially relevant docu-ments, updated every few minutes.
Second, an IEcomponent, (Grishman et al, 2003; Yangarber andSteinberger, 2009), analyzes each retrieved docu-ment, to try to find events of potential relevanceto Public Health.
The system stores the struc-tured information about every detected event into adatabase.
The IE component uses a large set of lin-guistic patterns, which in turn depend on a large-scale public health ontology, similar to MeSH,2 thatcontains concepts for diseases and infectious agents,infectious vectors and animals, medical drugs, andgeographic locations.From each article, PULS?s pattern matching en-gine tries to extract a set of incidents, or ?facts?
?detailed information related to instances of diseaseoutbreak.
An incident is described by a set of fields,or attributes: location and country of the incident,disease name, the date of the incident, informationabout the victims?their type (people, animals, etc.
),1puls.cs.helsinki.fi/medical2www.nlm.nih.gov/mesh31number, whether they survived or died, etc.The result of IE is a populated database of ex-tracted items, that can be browsed and searched byany attribute, according to the user?s interests.
It iscrucial to note that the notion of a user?s focus orinterest is not the same as the notion of relevance,introduced above.
We take the view that the notionof relevance is shared among the entire PH commu-nity: an event is either relevant to PH or it is not.Note also, that this view is upheld by several classic,human-moderated PH surveillance systems, such asProMED-Mail3 or Canadian GPHIN.
User?s inter-est is individual, e.g., a user may have specific ge-ographic, or medical focus (e.g., only viral or tropi-cal illnesses), and given the structured database, s/hecan filter the content according to specific criteria.But that is independent of the shared notion of rele-vance to PH.
User focus can be exploited for targetedrecommendation, using techniques such as collabo-rative filtering; at present, this is beyond the scopeof our work.The crawler and IE components have been in op-eration and under refinement for some time.
We nextbuild a classifier to assign relevance scores to eachextracted event and matched document.4 Experimental SetupWe now present the work on automatic classificationof relevance scores.
In collaboration with the end-users, we defined guidelines for judging relevanceon a 6-point scale, summarized in Table 1.Criteria ScoreNew information, highly relevant 5Important updates, 4on-going developmentsReview of current events, 3potential risk of diseaseHistorical/non-current events 2Background informationNon-specific, non-factive events, 1secondary topics, scientific studieshypothetical riskUnrelated to PH 0Table 1: Guidelines for relevance scores in medical news3www.promedmail.orgNote, the separation between the ?high-relevance?
scores, 4 and 5, vs. the rest; thissplit is addressed in detail in Section 4.3.4.1 Discourse featuresIt is clear that these guidelines are highly subjec-tive, and cannot be encoded by rules directly.
Inorder to model the relevance judgements, we ex-tracted features?the discourse features?from thedocument that are indicative of, or mappable to,the relevance scores.
Discourse features try to cap-ture higher-order information, including complexand longer-range inter-dependencies and clues, in-volving the physical layout of the document, anddeeper semantic and conceptual information foundin the document.
Some examples of discourse fea-tures are:?
Relative-position, which is represented by anumber from zero to 1 indicating the propor-tion of the document one needs to read to reachthe event text;?
Disease-in-header is a binary value that indi-cates whether the disease is mentioned in theheadline or the first two sentences;?
Disease-to-trigger-distance indicates how farthe disease is from the trigger sentence (sameas for confidence computation);?
Recency is the number of days between the re-ported occurrence of the event and the publica-tion date;We compiled over two dozen discourse-level fea-tures.
It is clear that the discourse features do notdetermine the relevance scores, but provide weakindicators of relevance, so that probabilistic classi-fication is appropriate.
For example, a higher rel-ative position of an event probably indicates lowerrelevance, but there are often news summary arti-cles that gather many unrelated news together, andmay contain very important items anywhere in thearticle.4 A feature such as Victim-named, statingwhether the victim?s name is mentioned, often in-dicates lower-relevance events (obituaries, stories4Due to space limitations, we do not provide a detailed listof the discourse features.32about public personalities, etc.).
However, some-times news articles about disease outbreaks deliber-ately personify the victims, to give the reader a senseof their background, lifestyle, to speculate about thevictims?
common circumstances.We describe two classifiers we have built for rel-evance.
A Naive Bayes classifier (NB) was used asthe baseline.
We then tried to obtain improved per-formance with Support Vector Machines (SVM).4.2 DataThe dataset is the database of facts extracted by thesystem.
The system pre-assigns relevance to eachevent, and users have the option to accept or cor-rect the system?s relevance score, through the UserInterface, which also allows the users to correct er-roneous fills, e.g., if a country, disease name, etc.,was extracted incorrectly by the system.Along with the users, members of the develop-ment team also evaluated a sample of the extractedevents, and corrected relevance and erroneous fills.The developers are computer scientists and linguists,whereas the users are medics, and because they in-terpreted the guidelines differently this had an im-pact on the results, described in Tables 2 and 5.?Cleaned data?
: PULS?s user interface also per-mits users to correct incorrect fills in the events (inthe two rightmost columns in the tables).
This al-lowed us to obtain two parallel sets of exampleswith relevance labels: the raw examples, as theywere automatically extracted by the system, and the?cleaned?
examples, after users/developer correc-tions.
The raw set is more noisy, since it contains er-rors introduced by the system.
We used the cleanedexamples to train our classifiers, and tested them onboth the cleaned set and the raw set.
Testing againstthe cleaned set gives an ?idealized?
performance, (asif the IE system made no errors in analysis).
Trueperformance is expected be closer to testing on theraw set.In total, there were just under 1000 examples la-beled by the users and the developers (some exam-ples were labeled by both, since the system allowsmultiple users to attach different relevance judge-ments to the same example.
Most of the timeusers agreed on the relevance judgements, but non-developers were less likely to clean examples.
)4.3 Naive Bayes classifierInitially, we planned to perform regression to thecomplete [0?5] relevance scale.
However, thisproved problematic, since the amount of labeled datawas not sufficient to cover the continuum betweenhighly relevant and not-so-relevant items.
We there-fore decided instead to build a binary classifier.
Thisdecision is also justified in the context of our sys-tem?s user interface, which provides the users withtwo views:?
the Front Page View contains only high-relevance items (rated 4 or 5), in case the userwants to see only the most urgent items first;?
the Complete View shows the user all extracteditems, irrespective of relevance.
(The user canalways filter the database by relevance value.
)Thus, the relevance score is also used to guidea binary decision: whether to present a givenevent/article to the user on the Front-Page View.
TheNB classifier using the entire set of discourse fea-tures did not perform well, because the discoursefeatures we have implemented are inherently not in-dependent, which affects the performance of NB.To try to reduce the mutual dependence amongthe features, we added a simple, greedy feature-selection phase during training.
Feature selectionstarts by training a classifier on the full set of fea-tures, using leave-one-out (LOO) cross-validation toestimate the classifier?s performance.
In the nextphase, the algorithm in turn excludes the featuresone by one, and runs the LOO cross-validationagain, once with each feature excluded.
The featurewhose exclusion gives rise to the biggest increase inperformance is dropped out, and the selection step isrepeated with the reduced set of features.
We con-tinue to drop features until performance does not in-crease for several iterations; in our experiments, weused three steps beyond the top performance.
Wethen back up to the step that yielded peak perfor-mance.
The resulting subset of features is used totrain the final NB classifier.The NB classifier is implemented in R Language.Because relevance prediction is difficult for allevents, we also tried to predict the relevance of anarticle, making the simplifying assumption that thearticle is only as relevant as the first event found in33the article.5 The results are presented in Table 2.The rows labeled Dev only refer to the data sets la-beled by developers, and Users only to sets labeledby (non-developer) users.Testing on Number examplesClean Raw Clean RawEvent-levelDev only 76.96 76.66 560 510All 72.19 73.34 863 799Users only 70.38 66.53 303 289Document-levelDev only 80.41 79.00 291 281All 73.94 72.45 545 530Users only 65.82 67.09 238 232Table 2: Naive Bayes prediction accuracyThe event-level classification is shown in the topportion of the table.
Throughout, as expected, test-ing on the cleaned data usually gives slightly bet-ter (more idealized) performance estimates than test-ing on the raw.
Also, as expected, testing onthe first-only events (document-level) gives slightlybetter performance, since it?s a simpler problem?although there is less data to train/test on.It is important to observe that using data la-beled by developers gives significantly higher per-formance.
This is because coercing the users to fol-low the guidelines strictly is not possible, and theydeviate from the rules that they themselves helpedarticulate.
The rows labeled ?all?
show performancewhen all combined available data was used?labeledby both the developers and the users.This performance is quite good for a baseline.6The confusion matrices?for the developer-onlyevent-level raw data set?show the distribution oftrue/false positives/negatives.4.4 SVM ClassifierFor comparison, we built two additional classifiersusing the SVMLight Toolkit.7 We first used a linear5A manual check confirmed that there were no instanceswhere the first event in an article had lower relevance than asubsequent event.6Consider for comparison, that the correctness on a manu-ally constructed, non-hidden set of articles used for system de-velopment, is under 75% F-measure.7http://svmlight.joachims.org/True LabelsPredicted labels 4-5 0-3High-relevance 4-5 125 77Low-relevance 0-3 42 266Table 3: NB confusion matrixkernel as a baseline, and used a RBF kernel, whichis potentially more expressive.
The conditions fortesting the SVM classifiers were same as the onesfor the NB classifiers, and same datasets were usedas for the NB.As SVM with the RBF kernel can use non-linearseparating hyperplanes in the original feature spaceby using the kernel trick (Aizerman et al, 1964),we aimed to test whether it would provide an im-provement over the linear kernel.
(For more detaileddiscussions of SVM and different kernel functionsfor text classification, cf., for example, (Joachims,1998).
)To regularize the input for SVM, all feature val-ues were normalized to lie between 0 and 1 (forcontinuous-valued features), and set to 0 or 1 forbinary features.
Table 4 describes the accuracyachieved with the linear kernel.
Experiments labeledAll discourse features use the complete set of dis-course features (over 20 features).
Rows labeled Se-lected discourse features show results from trainingwith exactly same features as resulted from the fea-ture selection phase of NB.Event-level Document-levelClean Raw Clean RawAll discourse featuresDev only 75.33 77.17 76.87 76.56All 71.60 72.26 70.51 69.96Selected discourse features onlyDev only 76.07 77.95 77.94 77.62All 71.40 72.14 69.75 69.37Table 4: SVM prediction accuracy using linear kernelThe difference when training with selected dis-course features and all discourse features is notlarge, since SVM is able to distinguish between rel-evant and non-relevant features fairly well.
The re-sults from SVM using linear kernel appear compa-34rable with the results from the NB.In addition to using the discourse features, wealso tried using lexical features.
The lexical fea-tures for a given example?extracted event?is sim-ply the bag of words from the sentence containingthe event, plus the two surrounding sentences.
Toreduce data sparsity, the sentences are pre-processedby a lemmatizer, and passed through a named en-tity (NE) recognizer (Grishman et al, 2003), whichreplaces persons, organizations, locations and dis-ease names with a special token indicating the NE?sclass.
?Stop-word?
parts of speech were dropped?prepositions, conjunctions, and articles.Event-level Document-levelClean Raw Clean RawAll discourse featuresDev only 74.69 75.37 77.93 78.38All 69.58 70.26 71.56 71.25Selected discourse features onlyDev only 77.51 79.01 79.19 79.04All 72.02 72.84 72.59 72.30Lexical features onlyDev only 75.93 76.37 79.11 80.07All 73.28 73.47 74.53 74.71Lexical and selected discourse featuresDev only 78.87 79.24 82.66 81.83All 76.48 76.58 76.52 76.19Table 5: SVM prediction accuracy using RBF kernelThe performance of SVM with the RBF kernelis strongly dependent on the values of SVM pa-rameters C?the trade-off between training errorand margin?
and ?
?the kernel width (Joachims,1998).
We tuned these parameters manually bychecking a grid of values against a developmentdataset, and finding areas where the SVM performedwell.
These areas were then further investigated.
Af-ter trying 40 combinations, we set C as 10000 and ?to 0.001 for subsequent evaluations.
The results forSVM using RBF kernel are given in Table 5.High accuracy of lexical features alone was some-what surprising as lexical features consist only of thebag of words in the event-bearing sentence, plus thepreceding and the following sentences.
News arti-cles often have various pieces of information relatedto the event scattered around the document.
Forexample, the disease can appear only in the head-line, the location/country in the middle of the doc-ument, and the event-bearing sentence in a third lo-cation, (Huttunen et al, 2002).
Our lexical features,as presented here, are not capable of capturing suchlong-distance relationships.The observed difference in performance on rele-vance prediction between the data sets labeled by de-velopers vs. non-developer users, likely arises fromthe fact that developers follow the formal guidelinesmore strictly (being computer scientists).
Rows la-beled all show performance against data sets la-beled by real users, who work in different PH orga-nizations in several different countries, each groupof users intuitively following their own, subjectiveguidelines, despite the common guidelines agreed-upon for this project.
There may also be deviationwithin organizations.
For example, certain doctorsmay find specific diseases or locations more inter-esting, giving events containing them a high rele-vance, thus injecting personal preference into docu-ment relevance.5 Discussion and ConclusionsThe SVM performs somewhat better than the NaiveBayes classifier, though there is still much to be ex-plored and improved.
One odd effect is that some-times testing on the raw data gives slightly betterresults than testing on the clean data, though thisis probably not significant, since the SVM classi-fier is still not finely tuned (and the data containsome noise).
Using all discourse features performsslightly worse than using a reduced set of features?the same set of features that we obtained throughgreedy feature selection for NB.Although the lexical features alone seem to dosomewhat worse than the discourse features alone onevent-level classification, we still see that the lexicalfeatures contain a great deal of information (whichthe NB cannot use).
As expected, adding the dis-course features improves performance over lexicalfeatures alone, since discourse features capture in-formation about long-range dependencies that locallexical features do not.In forming splits for cross-validation or LOO, wemade sure not to split examples from the same doc-35ument across the training and test sets.
That is, fora given document, all events in it are either used fortraining or for testing, to avoid biasing the testing.To summarize, the points addressed in this paper:?
We have presented a language-technology-based approach to a problem in Public Health,specifically the problem of event-based epi-demic surveillance through monitoring on-linemedia.?
The user?s perspective needs to be taken intoaccount when estimating quality, not just thesystem?s perspective.
Utility to the user is atleast as important as (if not more importantthan) correctness.?
We have presented an operational system thatsuggests articles potentially relevant to the user,and assigns relevance scores to each extractedevent.?
For now, we assume the users share same no-tion of relevance of an event to Public Health.?
We have presented experiments and an initialevaluation of assignment of relevance scores.?
Experiments indicate that relevance appearsto be a tractable measure of quality, atleast in principle.
Marking document-levelrelevance?only for the first event in thedocument?appears to be easier.
However,making real users follow strict guidelines is dif-ficult in practice.On-going work includes refining the classificationapproaches, especially, using Bayesian networks, re-gression, using transductive SVMs to leverage unla-beled data, and exploring collaborative filtering toaddress users?
individual interests.AcknowledgmentsThis research was supported in part by: the Tech-nology Development Agency of Finland (TEKES),through the ContentFactory Project, and by theAcademy of Finland?s National Centre of Excel-lence ?Algorithmic Data Analysis (ALGODAN).?ReferencesACE.
2004.
Automatic content extraction.M.
A. Aizerman, E. A. Braverman, and L. Rozonoer.1964.
Theoretical foundations of the potential func-tion method in pattern recognition learning.
In Au-tomation and Remote Control, volume 25, pages 821?837.Aron Culotta and Andrew McCallum.
2004.
Confi-dence estimation for information extraction.
In Pro-ceedings of Human Language Technology Conferenceand North American Chapter of the Association forComputational Linguistics.Son Doan, Quoc Hung-Ngo, Ai Kawazoe, and Nigel Col-lier.
2008.
Global Health Monitor?a web-based sys-tem for detecting and mapping infectious diseases.
InProceedings of the International Joint Conference onNatural Language Processing (IJCNLP).C.C.
Freifeld, K.D.
Mandl, B.Y.
Reis, and J.S.
Brown-stein.
2008.
HealthMap: Global infectious diseasemonitoring through automated classification and visu-alization of internet media reports.
Journal of Ameri-can Medical Informatics Association, 15:150?157.Ralph Grishman, Silja Huttunen, and Roman Yangarber.2003.
Information extraction for enhanced access todisease outbreak reports.
Journal of Biomedical Infor-matics, 35(4):236?246.David Hartley, Noele Nelson, Ronald Walters, RayArthur, Roman Yangarber, Larry Madoff, Jens Linge,Abla Mawudeku, Nigel Collier, John Brownstein, Ger-main Thinus, and Nigel Lightfoot.
2010.
The land-scape of international event-based biosurveillance.Emerging Health Threats Journal, 3(e3).Lynette Hirschman.
1998.
Language understanding eval-uations: Lessons learned from muc and atis.
In Pro-ceedings of the First International Conference on Lan-guage Resources and Evaluation (LREC), pages 117?122, Granada, Spain, May.Silja Huttunen, Roman Yangarber, and Ralph Grishman.2002.
Complexity of event structure in informationextraction.
In Proceedings of the 19th InternationalConference on Computational Linguistics (COLING2002), Taipei, August.Thorsten Joachims.
1998.
Text categorization with su-port vector machines: Learning with many relevantfeatures.
In ECML: European Conference on MachineLearning, pages 137?142.J.P.
Linge, R. Steinberger, T.P.
Weber, R. Yangarber,E.
van der Goot, D.H. Al Khudhairy, and N.I.
Stil-ianakis.
2009.
Internet surveillance systems for earlyalerting of health threats.
Eurosurveillance Journal,14(13).Ralf Steinberger, Flavio Fuart, Erik van der Goot, CliveBest, Peter von Etter, and Roman Yangarber.
2008.36Text mining from the web for medical intelligence.
InDomenico Perrotta, Jakub Piskorski, Franoise Soulie?-Fogelman, and Ralf Steinberger, editors, Mining Mas-sive Data Sets for Security.
OIS Press, Amsterdam, theNetherlands.Roman Yangarber and Ralf Steinberger.
2009.
Auto-matic epidemiological surveillance from on-line newsin MedISys and PULS.
In Proceedings of IMED-2009: International Meeting on Emerging Diseasesand Surveillance, Vienna, Austria.Roman Yangarber, Clive Best, Peter von Etter, FlavioFuart, David Horby, and Ralf Steinberger.
2007.Combining information about epidemic threats frommultiple sources.
In Proceedings of the MMIESWorkshop, International Conference on Recent Ad-vances in Natural Language Processing (RANLP2007), Borovets, Bulgaria, September.37
