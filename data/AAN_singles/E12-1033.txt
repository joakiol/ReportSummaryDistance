Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 325?335,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsGeneralization Methods for In-Domain and Cross-Domain OpinionHolder ExtractionMichael Wiegand and Dietrich KlakowSpoken Language SystemsSaarland UniversityD-66123 Saarbru?cken, Germany{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.deAbstractIn this paper, we compare three differentgeneralization methods for in-domain andcross-domain opinion holder extraction be-ing simple unsupervised word clustering,an induction method inspired by distantsupervision and the usage of lexical re-sources.
The generalization methods areincorporated into diverse classifiers.
Weshow that generalization causes significantimprovements and that the impact of im-provement depends on the type of classifierand on how much training and test data dif-fer from each other.
We also address theless common case of opinion holders beingrealized in patient position and suggest ap-proaches including a novel (linguistically-informed) extraction method how to detectthose opinion holders without labeled train-ing data as standard datasets contain toofew instances of this type.1 IntroductionOpinion holder extraction is one of the most im-portant subtasks in sentiment analysis.
The ex-traction of sources of opinions is an essential com-ponent for complex real-life applications, suchas opinion question answering systems or opin-ion summarization systems (Stoyanov and Cardie,2011).
Common approaches designed to extractopinion holders are based on data-driven methods,in particular supervised learning.In this paper, we examine the role of general-ization for opinion holder extraction in both in-domain and cross-domain classification.
General-ization may not only help to compensate the avail-ability of labeled training data but also conciliatedomain mismatches.In order to illustrate this, compare for instance(1) and (2).
(1) Malaysia did not agree to such treatment of Al-Qaeda sol-diers as they were prisoners-of-war and should be accordedtreatment as provided for under the Geneva Convention.
(2) Japan wishes to build a $21 billion per year aerospace indus-try centered on commercial satellite development.Though both sentences contain an opinionholder, the lexical items vary considerably.
How-ever, if the two sentences are compared on the ba-sis of some higher level patterns, some similari-ties become obvious.
In both cases the opinionholder is an entity denoting a person and this en-tity is an agent1 of some predictive predicate (i.e.agree in (1) and wishes in (2)), more specifically,an expression that indicates that the agent utters asubjective statement.
Generalization methods ide-ally capture these patterns, for instance, they mayprovide a domain-independent lexicon for thosepredicates.
In some cases, even higher order fea-tures, such as certain syntactic constructions mayvary throughout the different domains.
In (1) and(2), the opinion holders are agents of a predictivepredicate, whereas the opinion holder her daugh-ters in (3) is a patient2 of embarrasses.
(3) Mrs. Bennet does what she can to get Jane and Bingley to-gether and embarrasses her daughters by doing so.If only sentences, such as (1) and (2), occur inthe training data, a classifier will not correctly ex-tract the opinion holder in (3), unless it obtainsadditional knowledge as to which predicates takeopinion holders as patients.1By agent we always mean constituents being labeled asA0 in PropBank (Kingsbury and Palmer, 2002).2By patient we always mean constituents being labeledas A1 in PropBank.325In this work, we will consider three differ-ent generalization methods being simple unsuper-vised word clustering, an induction method andthe usage of lexical resources.
We show that gen-eralization causes significant improvements andthat the impact of improvement depends on howmuch training and test data differ from each other.We also address the issue of opinion holders inpatient position and present methods including anovel extraction method to detect these opinionholders without any labeled training data as stan-dard datasets contain too few instances of them.In the context of generalization it is also impor-tant to consider different classification methodsas the incorporation of generalization may have avarying impact depending on how robust the clas-sifier is by itself, i.e.
how well it generalizes evenwith a standard feature set.
We compare two state-of-the-art learning methods, conditional randomfields and convolution kernels, and a rule-basedmethod.2 DataAs a labeled dataset we mainly use the MPQA2.0 corpus (Wiebe et al 2005).
We adhere tothe definition of opinion holders from previouswork (Wiegand and Klakow, 2010; Wiegand andKlakow, 2011a; Wiegand and Klakow, 2011b),i.e.
every source of a private state or a subjectivespeech event (Wiebe et al 2005) is considered anopinion holder.This corpus contains almost exclusively newstexts.
In order to divide it into different domains,we use the topic labels from (Stoyanov et al2004).
By inspecting those topics, we found thatmany of them can grouped to a cluster of newsitems discussing human rights issues mostly inthe context of combating global terrorism.
Thismeans that there is little point in considering everysingle topic as a distinct (sub)domain and, there-fore, we consider this cluster as one single domainETHICS.3 For our cross-domain evaluation, wewant to have another topic that is fairly differentfrom this set of documents.
By visual inspection,we found that the topic discussing issues regard-ing the International Space Station would suit ourpurpose.
It is henceforth called SPACE.3The cluster is the union of documents with the followingMPQA-topic labels: axisofevil, guantanamo, humanrights,mugabe and settlements.Domain # Sentences # Holders in sentence (average)ETHICS 5700 0.79SPACE 628 0.28FICTION 614 1.49Table 1: Statistics of the different domain corpora.In addition to these two (sub)domains, wechose some text type that is not even news textin order to have a very distant domain.
There-fore, we had to use some text not included in theMPQA corpus.
Existing text collections contain-ing product reviews (Kessler et al 2010; Topraket al 2010), which are generally a popular re-source for sentiment analysis, were not foundsuitable as they only contain few distinct opinionholders.
We finally used a few summaries of fic-tional work (two Shakespeare plays and one novelby Jane Austen4) since their language is notablydifferent from that of news texts and they con-tain a large number of different opinion holders(therefore opinion holder extraction is a meaning-ful task on this text type).
These texts make upour third domain FICTION.
We manually labeledit with opinion holder information by applying theannotation scheme of the MPQA corpus.Table 1 lists the properties of the different do-main corpora.
Note that ETHICS is the largest do-main.
We consider it our primary (source) domainas it serves both as a training and (in-domain) testset.
Due to their size, the other domains onlyserve as test sets (target domains).For some of our generalization methods, wealso need a large unlabeled corpus.
We use theNorth American News Text Corpus (LDC95T21).3 The Different Types of Generalization3.1 Word Clustering (Clus)The simplest generalization method that is con-sidered in this paper is word clustering.
By that,we understand the automatic grouping of wordsoccurring in similar contexts.
Such clusters areusually computed on a large unlabeled corpus.Unlike lexical features, features based on clustersare less sparse and have been proven to signif-icantly improve data-driven classifiers in relatedtasks, such as named-entity recognition (Turian et4available at: www.absoluteshakespeare.com/guides/{othello|twelfth night}/summary/{othello|twelfth night} summary.htmwww.wikisummaries.org/Pride and Prejudice326I.
Madrid, Dresden, Bordeaux, Istanbul, Caracas, Manila, ...II.
Toby, Betsy, Michele, Tim, Jean-Marie, Rory, Andrew, ...III.
detest, resent, imply, liken, indicate, suggest, owe, expect, ...IV.
disappointment, unease, nervousness, dismay, optimism, ...V. remark, baby, book, saint, manhole, maxim, coin, batter, ...Table 2: Some automatically induced clusters.ETHICS SPACE FICTION1.47 2.70 11.59Table 3: Percentage of opinion holders as patients.al., 2010).
Such a generalization is, in particular,attractive as it is cheaply produced.
As a state-of-the-art clustering method, we consider Brownclustering (Brown et al 1992) as implemented inthe SRILM-toolkit (Stolcke, 2002).
We induced1000 clusters which is also the configuration usedin (Turian et al 2010).5Table 2 illustrates a few of the clusters inducedfrom our unlabeled dataset introduced in Section(?)
2.
Some of these clusters represent locationor person names (e.g.
I.
& II.).
This exempli-fies why clustering is effective for named-entityrecognition.
We also find clusters that intuitivelyseem to be meaningful for our task (e.g.
III.
&IV.)
but, on the other hand, there are clusters thatcontain words that with the exception of their partof speech do not have anything in common (e.g.V.
).3.2 Manually Compiled Lexicons (Lex)The major shortcoming of word clustering is thatit lacks any task-specific knowledge.
The oppo-site type of generalization is the usage of manu-ally compiled lexicons comprising predicates thatindicate the presence of opinion holders, such assupported, worries or disappointed in (4)-(6).
(4) I always supported this idea.
holder:agent.
(5) This worries me.
holder:patient(6) He disappointed me.
holder:patientWe follow Wiegand and Klakow (2011b) whofound that those predicates can be best obtainedby using a subset of Levin?s verb classes (Levin,1993) and the strong subjective expressions of theSubjectivity Lexicon (Wilson et al 2005).
Forthose predicates it is also important to considerin which argument position they usually take anopinion holder.
Bethard et al(2004) found the5We also experimented with other sizes but they did notproduce a better overall performance.majority of holders are agents (4).
A certainnumber of predicates, however, also have opinionholders in patient position, e.g.
(5) and (6).Wiegand and Klakow (2011b) found that manyof those latter predicates are listed in one ofLevin?s verb classes called amuse verbs.
Whileon the evaluation on the entire MPQA corpus,opinion holders in patient position are fairly rare(Wiegand and Klakow, 2011b), we may wonderwhether the same applies to the individual do-mains that we consider in this work.
Table 3lists the proportion of those opinion holders (com-puted manually) based on a random sample of 100opinion holder mentions from those corpora.
Thetable shows indeed that on the domains from theMPQA corpus, i.e.
ETHICS and SPACE, thoseopinion holders play a minor role but there is a no-tably higher proportion on the FICTION-domain.3.3 Task-Specific Lexicon Induction (Induc)3.3.1 Distant Supervision with PrototypicalOpinion HoldersLexical resources are potentially much moreexpressive than word clustering.
This knowledge,however, is usually manually compiled, whichmakes this solution much more expensive.
Wie-gand and Klakow (2011a) present an intermedi-ate solution for opinion holder extraction inspiredby distant supervision (Mintz et al 2009).
Theoutput of that method is also a lexicon of predi-cates but it is automatically extracted from a largeunlabeled corpus.
This is achieved by collectingpredicates that frequently co-occur with prototyp-ical opinion holders, i.e.
common nouns such asopponents (7) or critics (8), if they are an agentof that predicate.
The rationale behind this isthat those nouns act very much like actual opin-ion holders and therefore can be seen as a proxy.
(7) Opponents say these arguments miss the point.
(8) Critics argued that the proposed limits were unconstitutional.This method reduces the human effort to specify-ing a small set of such prototypes.Following the best configuration reportedin (Wiegand and Klakow, 2011a), we extract 250verbs, 100 nouns and 100 adjectives from our un-labeled corpus (?2).3.3.2 Extension for Opinion Holders inPatient PositionThe downside of using prototypical opinionholders as a proxy for opinion holders is that it327anguish?, astonish, astound, concern, convince, daze, delight,disenchant?, disappoint, displease, disgust, disillusion, dissat-isfy, distress, embitter?, enamor?, engross, enrage, entangle?,excite, fatigue?, flatter, fluster, flummox?, frazzle?, hook?, hu-miliate, incapacitate?, incense, interest, irritate, obsess, outrage,perturb, petrify?, sadden, sedate?, shock, stun, tether?, troubleTable 4: Examples of the automatically extracted verbstaking opinion holders as patients (?
: not listed asamuse verb).is limited to agentive opinion holders.
Opinionholders in patient position, such as the ones takenby amuse verbs in (5) and (6), are not covered.Wiegand and Klakow (2011a) show that consid-ering less restrictive contexts significantly dropsclassification performance.
So the natural exten-sion of looking for predicates having prototypicalopinion holders in patient position is not effective.Sentences, such as (9), would mar the result.
(9) They criticized their opponents.In (9) the prototypical opinion holder opponents(in the patient position) is not a true opinionholder.Our novel method to extract those predicatesrests on the observation that the past participle ofthose verbs, such as shocked in (10), is very oftenidentical to some predicate adjective (11) havinga similar if not identical meaning.
For the predi-cate adjective, the opinion holder is, however, itssubject/agent and not its patient.
(10) He had shockedverb me.
holder:patient(11) I was shockedadj .
holder:agentInstead of extracting those verbs directly (10),we take the detour via their corresponding pred-icate adjectives (11).
This means that we collectall those verbs (from our large unlabeled corpus(?2)) for which there is a predicate adjective thatcoincides with the past participle of the verb.To increase the likelihood that our extractedpredicates are meaningful for opinion holder ex-traction, we also need to check the semantic typein the relevant argument position, i.e.
make surethat the agent of the predicate adjective (whichwould be the patient of the corresponding verb)is an entity likely to be an opinion holder.
Ourinitial attempts with prototypical opinion holderswere too restrictive, i.e.
the number of prototyp-ical opinion holders co-occurring with those ad-jectives was too small.
Therefore, we widen thesemantic type of this position from prototypicalopinion holders to persons.
This means that weallow personal pronouns (i.e.
I, you, he, she andwe) to appear in this position.
We believe that thisrelaxation can be done in that particular case, asadjectives are much more likely to convey opin-ions a priori than verbs (Wiebe et al 2004).An intrinsic evaluation of the predicates that wethus extracted from our unlabeled corpus is dif-ficult.
The 250 most frequent verbs exhibitingthis special property of coinciding with adjectives(this will be the list that we use in our experi-ments) contains 42% entries of the amuse verbs(?3.2).
However, we also found many other po-tentially useful predicates on this list that are notlisted as amuse verbs (Table 4).
As amuse verbscannot be considered a complete golden standardfor all predicates taking opinion holders as pa-tients, we will focus on a task-based evaluationof our automatically extracted list (?6).4 Data-driven MethodsIn the following, we present the two supervisedclassifiers we use in our experiments.
Both clas-sifiers incorporate the same levels of representa-tions, including the same generalization methods.4.1 Conditional Random Fields (CRF)The supervised classifier most frequently usedfor information extraction tasks, in general, areconditional random fields (CRF) (Lafferty et al2001).
Using CRF, the task of opinion holder ex-traction is framed as a tagging problem in whichgiven a sequence of observations x = x1x2 .
.
.
xn(words in a sentence) a sequence of output tagsy = y1y2 .
.
.
yn indicating the boundaries of opin-ion holders is computed by modeling the condi-tional probability P (x|y).The features we use (Table 5) are mostly in-spired by Choi et al(2005) and by the onesused for plain support vector machines (SVMs)in (Wiegand and Klakow, 2010).
They are orga-nized into groups.
The basic group Plain does notcontain any generalization method.
Each othergroup is dedicated to one specific generalizationmethod that we want to examine (Clus, Inducand Lex).
Apart from considering generalizationfeatures indicating the presence of generalizationtypes, we also consider those types in conjunctionwith semantic roles.
As already indicated above,semantic roles are especially important for the de-tection of opinion holders.
Unfortunately, the cor-328Group FeaturesPlainToken features: unigrams and bigramsPOS/chunk/named-entity features: unigrams, bi-grams and trigramsConstituency tree path to nearest predicateNearest predicateSemantic role to predicate+lexical form of predicateClusCluster features: unigrams, bigrams and trigramsSemantic role to predicate+cluster-id of predicateCluster-id of nearest predicateInducIs there predicate from induced lexicon within win-dow of 5 tokens?Semantic role to predicate, if predicate is contained ininduced lexiconIs nearest predicate contained in induced lexicon?LexIs there predicate from manually compiled lexiconswithin window of 5 tokens?Semantic role to predicate, if predicate is contained inmanually compiled lexiconsIs nearest predicate contained in manually compiledlexicons?Table 5: Feature set for CRF.responding feature from the Plain feature groupthat also includes the lexical form of the predicateis most likely a sparse feature.
For the opinionholder me in (10), for example, it would corre-spond to A1 shock.
Therefore, we introduce foreach generalization method an additional featurereplacing the sparse lexical item by a generaliza-tion label, i.e.
Clus: A1 CLUSTER-35265, Induc:A1 INDUC-PRED and Lex: A1 LEX-PRED.6For this learning method, we use CRF++.7 Wechoose a configuration that provides good perfor-mance on our source domain (i.e.
ETHICS).8For semantic role labeling we use SWIRL9, forchunk parsing CASS (Abney, 1991) and for con-stituency parsing Stanford Parser (Klein and Man-ning, 2003).
Named-entity information is pro-vided by Stanford Tagger (Finkel et al 2005).4.2 Convolution Kernels (CK)Convolution kernels (CK) are special kernel func-tions.
A kernel function K : X ?
X ?
R com-putes the similarity of two data instances xi andxj (xi ?
xj ?
X).
It is mostly used in SVMs thatestimate a hyperplane to separate data instancesfrom different classes H(~x) = ~w ?
~x + b = 0where w ?
Rn and b ?
R (Joachims, 1999).
In6Predicates in patient position are given the same gener-alization label as the predicates in agent position.
Speciallymarking them did not result in a notable improvement.7http://crfpp.sourceforge.net8The soft margin parameter ?c is set to 1.0 and all fea-tures occurring less than 3 times are removed.9http://www.surdeanu.name/mihai/swirlconvolution kernels, the structures to be comparedwithin the kernel function are not vectors com-prising manually designed features but the under-lying discrete structures, such as syntactic parsetrees or part-of-speech sequences.
Since they aredirectly provided to the learning algorithm, a clas-sifier can be built without taking the effort of im-plementing an explicit feature extraction.We take the best configuration from (Wiegandand Klakow, 2010) that comprises a combinationof three different tree kernels being two tree ker-nels based on constituency parse trees (one withpredicate and another with semantic scope) anda tree kernel encoding predicate-argument struc-tures based on semantic role information.
Theserepresentations are illustrated in Figure 1.
The re-sulting kernels are combined by plain summation.In order to integrate our generalization meth-ods into the convolution kernels, the input struc-tures, i.e.
the linguistic tree structures, have to beaugmented.
For that we just add additional nodeswhose labels correspond to the respective gener-alization types (i.e.
Clus: CLUSTER-ID, Induc:INDUC-PRED and Lex: LEX-PRED).
The nodesare added in such a way that they (directly) domi-nate the leaf node for which they provide a gener-alization.10 If several generalization methods areused and several of them apply for the same lex-ical unit, then the (vertical) order of the general-ization nodes is LEX-PRED  INDUC-PREDCLUSTER-ID.11 Figure 2 illustrates the predi-cate argument structure from Figure 1 augmentedwith INDUC-PRED and CLUSTER-IDs.For this learning method, we use theSVMLight-TK toolkit.12 Again, we tune theparameters to our source domain (ETHICS).135 Rule-based Classifiers (RB)Finally, we also consider rule-based classifiers(RB).
The main difference towards CRF and CKis that it is an unsupervised approach not requiringtraining data.
We re-use the framework by Wie-gand and Klakow (2011b).
The candidate set areall noun phrases in a test set.
A candidate is clas-sified as an opinion holder if all of the following10Note that even for the configuration Plain the trees arealready augmented with named-entity information.11We chose this order as it roughly corresponds to thespecificity of those generalization types.12disi.unitn.it/moschitti13The cost parameter?j (Morik et al 1999) was set to 5.329Figure 1: The different structures (left: constituency trees, right: predicate argument structure) derived fromSentence (1) for the opinion holder candidate Malaysia used as input for convolution kernels (CK).Figure 2: Predicate argument structure augmentedwith generalization nodes.conditions hold:?
The candidate denotes a person or group of persons.?
There is a predictive predicate in the same sentence.?
The candidate has a pre-specified semantic role in the eventthat the predictive predicate evokes (default: agent-role).The set of predicates is obtained from a given lex-icon.
For predicates that take opinion holders aspatients, the default agent-role is overruled.We consider several classifiers that differ in thelexicon they use.
RB-Lex uses the combination ofthe manually compiled lexicons presented in ?3.2.RB-Induc uses the predicates that have been au-tomatically extracted from a large unlabeled cor-pus using the methods presented in ?3.3.
RB-Induc+Lex considers the union of those lexicons.In order to examine the impact of modeling opin-ion holders in patient position, we also introducetwo versions of each lexicon.
AG just consid-ers predicates in agentive position while AG+PTalso considers predicates that take opinion hold-ers as patients.
For example, RB-InducAG+PTis a classifier that uses automatically extractedpredicates in order to detect opinion holders inboth agent and patient argument position, i.e.RB-InducAG+PT also covers our novel extractionmethod for patients (?3.3.2).The output of clustering will exclusively beevaluated in the context of learning-based meth-FeaturesInduc Lex Induc+LexDomains AG AG+PT AG AG+PT AG+PTETHICS 50.77 50.99 52.22 52.27 53.07SPACE 45.81 46.55 47.60 48.47 45.20FICTION 46.59 49.97 54.84 59.35 63.11Table 6: F-score of the different rule-based classifiers.ods, since there is no straightforward way of in-corporating this output into a rule-based classifier.6 ExperimentsCK and RB have an instance space that is differ-ent from the one of CRF.
While CRF producesa prediction for every word token in a sentence,CK and RB only produce a prediction for everynoun phrase.
For evaluation, we project the pre-dictions from RB and CK to word token level inorder to ensure comparability.
We evaluate the se-quential output with precision, recall and F-scoreas defined in (Johansson and Moschitti, 2010; Jo-hansson and Moschitti, 2011).6.1 Rule-based ClassifierTable 6 shows the cross-domain performance ofthe different rule-based classifiers.
RB-Lex per-forms better than RB-Induc.
In comparison to thedomains ETHICS and SPACE the difference islarger on FICTION.
Presumably, this is due to thefact that the predicates in Induc are extracted froma news corpus (?2).
Thus, Induc may slightly suf-fer from a domain mismatch.
A combination ofthe two classifiers, i.e.
RB-Lex+Induc, results ina notable improvement in the FICTION-domain.The approaches that also detect opinion holders aspatients (AG+PT) including our novel approach(?3.3.2) are effective.
A notable improvement can330Training Size (%)Features Alg.
5 10 20 50 100PlainCRF 32.14 35.24 41.03 51.05 55.13CK 42.15 46.34 51.14 56.39 59.52+ClusCRF 33.06 37.11 43.47 52.05 56.18CK 42.02 45.86 51.11 56.59 59.77+InducCRF 37.28 42.31 46.54 54.27 56.71CK 46.26 49.35 53.26 57.28 60.42+LexCRF 40.69 43.91 48.43 55.37 58.46CK 46.45 50.59 53.93 58.63 61.50+Clus+InducCRF 37.27 42.19 47.35 54.95 57.14CK 45.14 48.20 52.39 57.37 59.97+Clus+LexCRF 40.52 44.29 49.32 55.44 58.80CK 45.89 49.35 53.56 58.74 61.43+Lex+InducCRF 42.23 45.92 49.96 55.61 58.40CK 47.46 51.44 54.80 58.74 61.58AllCRF 41.56 45.75 50.39 56.24 59.08CK 46.18 50.10 54.04 58.92 61.44Table 7: F-score of in-domain (ETHICS) learning-based classifiers.only be measured on the FICTION-domain sincethis is the only domain with a significant propor-tion of those opinion holders (Table 3).6.2 In-Domain Evaluation ofLearning-based MethodsTable 7 shows the performance of the learning-based methods CRF and CK on an in-domainevaluation (ETHICS-domain) using differentamounts of labeled training data.
We carry outa 5-fold cross-validation and use n% of the train-ing data in the training folds.
The table shows thatCK is more robust than CRF.
The fewer trainingdata are used the more important generalizationbecomes.
CRF benefits much more from gener-alization than CK.
Interestingly, the CRF config-uration with the best generalization is usually asgood as plain CK.
This proves the effectivenessof CK.
In principle, Lex is the strongest general-ization method while Clus is by far the weakest.For Clus, systematic improvements towards nogeneralization (even though they are minor) canonly be observed with CRF.
As far as combina-tions are concerned, either Lex+Induc or All per-forms best.
This in-domain evaluation proves thatopinion holder extraction is different from named-entity recognition.
Simple unsupervised general-ization, such as word clustering, is not effectiveand popular sequential classifiers are less robustthan margin-based tree-kernels.Table 8 complements Table 7 in that it com-pares the learning-based methods with the bestrule-based classifier and also displays precisionand recall.
RB achieves a high recall, whereas thelearning-based methods always excel RB in pre-cision.14 Applying generalization to the learning-based methods results in an improvement of bothrecall and precision if few training data are used.The impact on precision decreases, however, themore training data are added.
There is always asignificant increase in recall but learning-basedmethods may not reach the level of RB eventhough they use the same resources.
This is aside-effect of preserving a much higher precision.It also explains why learning-based methods withgeneralization may have a lower F-score than RB.6.3 Out-of-Domain Evaluation ofLearning-based MethodsTable 9 presents the results of out-of-domain clas-sifiers.
The complete ETHICS-dataset is used fortraining.
Some properties are similar to the pre-vious experiments: CK always outperforms CRF.RB provides a high recall whereas the learning-based methods maintain a higher precision.
Sim-ilar to the in-domain setting using few labeledtraining data, the incorporation of generalizationincreases both precision and recall.
Moreover, acombination of generalization methods is betterthan just using one method on average, althoughLex is again a fairly robust individual generaliza-tion method.
Generalization is more effective inthis setting than on the in-domain evaluation us-ing all training data, in particular for CK, sincethe training and test data are much more differentfrom each other and suitable generalization meth-ods partly close that gap.There is a notable difference in precision be-tween the SPACE- and FICTION-domain (andalso the source domain ETHICS (Table 8)).
Westrongly assume that this is due to the distribu-tion of opinion holders in those datasets (Table 1).The FICTION-domain contains much more opin-ion holders, therefore the chance that a predictedopinion holder is correct is much higher.With regard to recall, a similar level of per-formance as in the ETHICS-domain can only beachieved in the SPACE-domain, i.e.
CK achievesa recall of 60%.
In the FICTION-domain, how-ever, the recall is much lower (best recall of CKis below 47%).
This is no surprise as the SPACE-domain is more similar to the source domain than14The reason for RB having a high recall is extensivelydiscussed in (Wiegand and Klakow, 2011b).331the FICTION-domain since ETHICS and SPACEare news texts.
FICTION contains more out-of-domain language.
Therefore, RB (which exclu-sively uses domain-independent knowledge) out-performs both learning-based methods includingthe ones incorporating generalization.
Similar re-sults have been observed for rule-based classifiersfrom other tasks in cross-domain sentiment anal-ysis, such as subjectivity detection and polarityclassification.
High-level information as it is en-coded in a rule-based classifier generalizes betterthan learning-based methods (Andreevskaia andBergler, 2008; Lambov et al 2009).We set up another experiment exclusively forthe FICTION-domain in which we combine theoutput of our best learning-based method, i.e.
CK,with the prediction of a rule-based classifier.
Thecombined classifier will predict an opinion holder,if either classifier predicts one.
The motivation forthis is the following: The FICTION-domain is theonly domain to have a significant proportion ofopinion holders appearing as patients.
We wantto know how much of them can be recognizedwith the best out-of-domain classifier using train-ing data with only very few instances of this typeand what benefit the addition of using various RBswhich have a clearer notion of these constructionsbrings about.
Moreover, we already observed thatthe learning-based methods have a bias towardspreserving a high precision and this may have asa consequence that the generalization features in-corporated into CK will not receive sufficientlylarge weights.
Unlike the SPACE-domain wherea sufficiently high recall is already achieved withCK (presumably due to its stronger similarity to-wards the source domain) the FICTION-domainmay be more severely affected by this bias andevidence from RB may compensate for this.Table 10 shows the performance of those com-bined classifiers.
For all generalization typesconsidered, there is, indeed, an improvement byadding information from RB resulting in a largeboost in recall.
Already the application of our in-duction approach Induc results in an increase ofmore than 8% points compared to plain CK.
Thetable also shows that there is always some im-provement if RB considers opinion holders as pa-tients (AG+PT).
This can be considered as someevidence that (given the available data we use)opinion holders in patient position can only be ef-fectively extracted with the help of RBs.
It is alsoCRF CKSize Feat.
Prec Rec F1 Prec Rec F110Plain 52.17 26.61 35.24 58.26 38.47 46.34All 62.85 35.96 45.75 63.18 41.50 50.1050Plain 59.85 44.50 51.05 59.60 53.50 56.39All 62.99 50.80 56.24 61.91 56.20 58.92100Plain 64.14 48.33 55.13 62.38 56.91 59.52All 64.75 54.32 59.08 63.81 59.24 61.44RB 47.38 60.32 53.07 47.38 60.32 53.07Table 8: Comparison of best RB with learning-basedapproaches on in-domain classification.Algorithms Generalization Prec Rec FCK (Plain) 66.90 41.48 51.21CK Induc 67.06 45.15 53.97CK+RBAG Induc 60.22 54.52 57.23CK+RBAG+PT Induc 61.09 58.14 59.58CK Lex 69.45 46.65 55.81CK+RBAG Lex 67.36 59.02 62.91CK+RBAG+PT Lex 68.25 63.28 65.67CK Induc+Lex 69.73 46.17 55.55CK+RBAG Induc+Lex 61.41 65.56 63.42CK+RBAG+PT Induc+Lex 62.26 70.56 66.15Table 10: Combination of out-of-domain CK and rule-based classifiers on FICTION (i.e.
distant domain).further evidence that our novel approach to extractthose predicates (?3.3.2) is effective.The combined approach in Table 10 not onlyoutperforms CK (discussed above) but also RB(Table 6).
We manually inspected the output ofthe classifiers to find also cases in which CK de-tect opinion holders that RB misses.
CK has theadvantage that it is not only bound to the relation-ship between candidate holder and predicate.
Itlearns further heuristics, e.g.
that sentence-initialmentions of persons are likely opinion holders.
In(12), for example, this heuristics fires while RBoverlooks this instance as to give someone a shareof advice is not part of the lexicon.
(12) She later gives Charlotte her share of advice on running ahousehold.7 Related WorkThe research on opinion holder extraction hasbeen focusing on applying different data-drivenapproaches.
Choi et al(2005) and Choi et al(2006) explore conditional random fields, Wie-gand and Klakow (2010) examine different com-binations of convolution kernels, while Johans-son and Moschitti (2010) present a re-ranking ap-proach modeling complex relations between mul-tiple opinions in a sentence.
A comparison of332SPACE (similar target domain) FICTION (distant target domain)CRF CK CRF CKFeatures Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1Plain 47.32 48.62 47.96 45.89 57.07 50.87 68.58 28.96 40.73 66.90 41.48 51.21+Clus 49.00 48.62 48.81 49.23 57.64 53.10 71.85 32.21 44.48 67.54 41.21 51.19+Induc 42.92 49.15 45.82 46.66 60.45 52.67 71.59 34.77 46.80 67.06 45.15 53.97+Lex 49.65 49.07 49.36 49.60 59.88 54.26 71.91 35.83 47.83 69.45 46.65 55.81+Clus+Induc 46.61 48.78 47.67 48.65 58.20 53.00 71.32 35.88 47.74 67.46 42.17 51.90+Lex+Induc 48.75 50.87 49.78 49.92 58.76 53.98 74.02 37.37 49.67 69.73 46.17 55.55+Clus+Lex 49.72 50.87 50.29 53.70 59.32 56.37 73.41 37.15 49.33 70.59 43.98 54.20All 49.87 51.03 50.44 51.68 58.76 54.99 72.00 37.44 49.26 70.61 44.83 54.84best RB 41.72 57.80 48.47 41.72 57.80 48.47 63.26 62.96 63.11 63.26 62.96 63.11Table 9: Comparison of best RB with learning-based approaches on out-of-domain classification.those methods has not yet been attempted.
Inthis work, we compare the popular state-of-the-artlearning algorithms conditional random fields andconvolution kernels for the first time.
All thesedata-driven methods have been evaluated on theMPQA corpus.
Some generalization methods areincorporated but unlike this paper they are neithersystematically compared nor combined.
The roleof resources that provide the knowledge of argu-ment positions of opinion holders is not coveredin any of these works.
This kind of knowledgeshould be directly learnt from the labeled train-ing data.
In this work, we found, however, thatthe distribution of argument positions of opinionholders varies throughout the different domainsand, therefore, cannot be learnt from any arbitraryout-of-domain training set.Bethard et al(2004) and Kim and Hovy (2006)explore the usefulness of semantic roles providedby FrameNet (Fillmore et al 2003).
Bethardet al(2004) use this resource to acquire labeledtraining data while in (Kim and Hovy, 2006)FrameNet is used within a rule-based classifiermapping frame-elements of frames to opinionholders.
Bethard et al(2004) only evaluate on anartificial dataset (i.e.
a subset of sentences fromFrameNet and PropBank (Kingsbury and Palmer,2002)).
The only realistic test set on which Kimand Hovy (2006) evaluate their approach are newstexts.
Their method is compared against a sim-ple rule-based baseline and, unlike this work, notagainst a robust data-driven algorithm.
(Wiegand and Klakow, 2011b) is similar to(Kim and Hovy, 2006) in that a rule-based ap-proach is used relying on the relationship towardspredictive predicates.
Diverse resources are con-sidered for obtaining such words, however, theyare only evaluated on the entire MPQA corpus.The only cross-domain evaluation of opinionholder extraction is reported in (Li et al 2007) us-ing the MPQA corpus as a training set and the NT-CIR collection as a test set.
A low cross-domainperformance is obtained and the authors concludethat this is due to the very different annotationschemes of those corpora.8 ConclusionWe examined different generalization methods foropinion holder extraction.
We found that for in-domain classification, the more labeled trainingdata are used, the smaller is the impact of gener-alization.
Robust learning methods, such as con-volution kernels, benefit less from generalizationthan weaker classifiers, such as conditional ran-dom fields.
For cross-domain classification, gen-eralization is always helpful.
Distant domainsare problematic for learning-based methods, how-ever, rule-based methods provide a reasonable re-call and can be effectively combined with thelearning-based methods.
The types of generaliza-tion that help best are manually compiled lexiconsfollowed by an induction method inspired by dis-tant supervision.
Finally, we examined the caseof opinion holders as patients and also presenteda novel automatic extraction method that provedeffective.
Such dedicated extraction methods areimportant as common labeled datasets (from thenews domain) do not provide sufficient trainingdata for these constructions.AcknowledgementsThis work was funded by the German Federal Ministryof Education and Research (Software-Cluster) undergrant no.
?01IC10S01?.
The authors thank AlessandroMoschitti, Benjamin Roth and Josef Ruppenhofer fortheir technical support and interesting discussions.333ReferencesSteven Abney.
1991.
Parsing By Chunks.
In RobertBerwick, Steven Abney, and Carol Tenny, editors,Principle-Based Parsing.
Kluwer Academic Pub-lishers, Dordrecht.Alina Andreevskaia and Sabine Bergler.
2008.
WhenSpecialists and Generalists Work Together: Over-coming Domain Dependence in Sentiment Tagging.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL/HLT), Columbus, OH,USA.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
Extract-ing Opinion Propositions and Opinion Holders us-ing Syntactic and Lexical Cues.
In Computing At-titude and Affect in Text: Theory and Applications.Springer-Verlag.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467?479.Yejin Choi, Claire Cardie, Ellen Riloff, and Sid-dharth Patwardhan.
2005.
Identifying Sourcesof Opinions with Conditional Random Fields andExtraction Patterns.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing(HLT/EMNLP), Vancouver, BC, Canada.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
JointExtraction of Entities and Relations for OpinionRecognition.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), Sydney, Australia.Charles.
J. Fillmore, Christopher R. Johnson, andMiriam R. Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16:235 ?
250.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics(ACL), Ann Arbor, MI, USA.Thorsten Joachims.
1999.
Making Large-Scale SVMLearning Practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods -Support Vector Learning.
MIT Press.Richard Johansson and Alessandro Moschitti.
2010.Reranking Models in Fine-grained Opinion Anal-ysis.
In Proceedings of the International Confer-ence on Computational Linguistics (COLING), Be-jing, China.Richard Johansson and Alessandro Moschitti.
2011.Extracting Opinion Expressions and Their Polari-ties ?
Exploration of Pipelines and Joint Models.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL), Portland,OR, USA.Jason S. Kessler, Miriam Eckert, Lyndsay Clarke,and Nicolas Nicolov.
2010.
The ICWSM JDPA2010 Sentiment Corpus for the Automotive Do-main.
In Proceedings of the International AAAIConference on Weblogs and Social Media DataChallange Workshop (ICWSM-DCW), Washington,DC, USA.Soo-Min Kim and Eduard Hovy.
2006.
ExtractingOpinions, Opinion Holders, and Topics Expressedin Online News Media Text.
In Proceedings ofthe ACL Workshop on Sentiment and Subjectivity inText, Sydney, Australia.Paul Kingsbury and Martha Palmer.
2002.
FromTreeBank to PropBank.
In Proceedings of theConference on Language Resources and Evaluation(LREC), Las Palmas, Spain.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of theAnnual Meeting of the Association for Computa-tional Linguistics (ACL), Sapporo, Japan.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of the InternationalConference on Machine Learning (ICML).Dinko Lambov, Gae?l Dias, and Veska Noncheva.2009.
Sentiment Classification across Domains.
InProceedings of the Portuguese Conference on Artifi-cial Intelligence (EPIA), Aveiro, Portugal.
Springer-Verlag.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press.Yangyong Li, Kalina Bontcheva, and Hamish Cun-ningham.
2007.
Experiments of Opinion Analy-sis on the Corpora MPQA and NTCIR-6.
In Pro-ceedings of the NTCIR-6 Workshop Meeting, Tokyo,Japan.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant Supervision for Relation Extrac-tion without Labeled Data.
In Proceedings of theJoint Conference of the Annual Meeting of the As-sociation for Computational Linguistics and the In-ternational Joint Conference on Natural LanguageProcessing of the Asian Federation of Natural Lan-guage Processing (ACL/IJCNLP), Singapore.Katharina Morik, Peter Brockhausen, and ThorstenJoachims.
1999.
Combining Statistical Learn-ing with a Knowledge-based Approach - A CaseStudy in Intensive Care Monitoring.
In Proceedingsthe International Conference on Machine Learning(ICML).Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-334ternational Conference on Spoken Language Pro-cessing (ICSLP), Denver, CO, USA.Veselin Stoyanov and Claire Cardie.
2011.
Auto-matically Creating General-Purpose Opinion Sum-maries from Text.
In Proceedings of Recent Ad-vances in Natural Language Processing (RANLP),Hissar, Bulgaria.Veselin Stoyanov, Claire Cardie, Diane Litman, andJanyce Wiebe.
2004.
Evaluating an Opinion An-notation Scheme Using a New Multi-PerspectiveQuestion and Answer Corpus.
In Proceedings ofthe AAAI Spring Symposium on Exploring Attitudeand Affect in Text, Menlo Park, CA, USA.Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.2010.
Sentence and Expression Level Annotationof Opinions in User-Generated Discourse.
In Pro-ceedings of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL), Uppsala,Sweden.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and Gen-eral Method for Semi-supervised Learning.
In Pro-ceedings of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL), Uppsala,Sweden.Janyce Wiebe, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing Subjective Language.
Computational Linguis-tics, 30(3).Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating Expressions of Opinions andEmotions in Language.
Language Resources andEvaluation, 39(2/3):164?210.Michael Wiegand and Dietrich Klakow.
2010.
Convo-lution Kernels for Opinion Holder Extraction.
InProceedings of the Human Language TechnologyConference of the North American Chapter of theACL (HLT/NAACL), Los Angeles, CA, USA.Michael Wiegand and Dietrich Klakow.
2011a.
Proto-typical Opinion Holders: What We can Learn fromExperts and Analysts.
In Proceedings of Recent Ad-vances in Natural Language Processing (RANLP),Hissar, Bulgaria.Michael Wiegand and Dietrich Klakow.
2011b.
TheRole of Predicates in Opinion Holder Extraction.
InProceedings of the RANLP Workshop on Informa-tion Extraction and Knowledge Acquisition (IEKA),Hissar, Bulgaria.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.
In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing (HLT/EMNLP), Vancouver, BC, Canada.335
