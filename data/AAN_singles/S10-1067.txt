Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 304?307,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsPKU_HIT: An Event Detection System Based on Instances Expansionand Rich Syntactic FeaturesShiqi Li1, Pengyuan Liu2, Tiejun Zhao1, Qin Lu3 and Hanjing Li11School of Computer Science and Technology,Harbin Institute of Technology, Harbin 150001, China{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn2Institute of Computational Linguistics,Peking University, Beijing 100871, Chinaliupengyuan@pku.edu.cn3Department of Computing,The Hong Kong Polytechnic University, Hong Kong, Chinacsluqin@comp.polyu.edu.hkAbstractThis paper describes the PKU_HIT systemon event detection in the SemEval-2010Task.
We construct three modules for thethree sub-tasks of this evaluation.
Fortarget verb WSD, we build a Na?veBayesian classifier which uses additionaltraining instances expanded from anuntagged Chinese corpus automatically.For sentence SRL and event detection, weuse a feature-based machine learningmethod which makes combined use ofboth constituent-based and dependency-based features.
Experimental results showthat the Macro Accuracy of the WSDmodule reaches 83.81% and F-Score ofthe SRL module is 55.71%.1 IntroductionIn this paper, we describe the system submittedto the SemEval-2010 Task 11 on event detectionin Chinese news sentences (Zhou, 2010).
Theobjective of the task is to detect and analyzebasic event contents in Chinese news sentences,similar to the frame semantic structure extractiontask in SemEval-2007.
However, this task is amore complex as it involves three interrelatedsubtasks: (1) target verb word sensedisambiguation (WSD), (2) sentence semanticrole labeling (SRL) and (3) event detection (ED).Therefore, the architecture of the system thatwe develop for the task consists of three modules:WSD, SRL and ED.
First, the WSD module is torecognize key verbs or verb phrases whichdescribe the basic event in a sentence, and thenselect an appropriate situation descriptionformula for the recognized key verbs (or verbphrases); Then, the SRL module anchors thearguments to suitable constituents in the sentence,and then label each argument with threefunctional tags, namely constituent type tag,semantic role tags and event role tag.
Finally, inthe ED module, complete situation description ofthe sentence can be achieved by combining theresults of the WSD module and the SRL module.For the WSD module, we consider the subtaskas a general WSD problem.
First of all, weautomatically extract many instances from anuntagged Chinese corpus using a heuristic ruleinspired by Yarowsky (1993).
Then we train aNa?ve Bayesian (NB) classifier based on both theextracted instances and the official training data.We then use the NB classifier to predict situationthe description formula and natural explanationof each target verb in testing data.For the SRL module, we use a rich syntacticfeature-based learning method.
As the state-of-the-art method in the field of SRL, feature-basedmethod represents a predicate-argument structure(PAS) by a flat vector using a set of linguisticfeatures.
Then PAS can be directly classified bymachine learning algorithms based on thecorresponding vectors.
In feature-based SRL, the304significance of syntactic information in SRL wasproven by (Punyakanok et al, 2005).
In ourmethod, we exploit a rich set of syntacticfeatures from two syntactic views: constituentand dependency.
As the two syntactic viewsfocus on different syntactic elements,constituent-based features and dependency-basedfeatures can complement each other in SRL tosome extent.
Finally, the ED module can bereadily implemented by combining the SRL andthe WSD result using some simply rules.2 System Description2.1 Target Verb WSDThe WSD module is based on a simple heuristicrule by which we can extract sense-labeledinstances automatically.
The heuristic ruleassumes that one sense per 3-gram which isproposed by us initially through investigating aChinese sense-tagged corpus STC (Wu et al,2006).
The assumption is similar to thecelebrated one sense per collocation supposition(Yarowsky, 1993), whereas ours has moreexpansibility.
STC is an ongoing project which isto build a sense-tagged corpus containing sense-tagged 1, 2 and 3 months of People?s Daily 2000now.
According to our investigation, given aspecific 3-gram (w-1wverbw1) to any target verb,on average, we expect to see the same label95.4% of the time.
Based on this observation, weconsider one sense per 3-gram (w-1wverbw1) or atleast we can extract instances with this pattern.For all the 27 multiple-sense target verbs inthe official training data, we found their 3-gram(w-1wverbw1) and extracted the instances with thesame 3-gram from a Chinese monolingual corpus?
the 2001 People?s Daily (about 116M bytes).We consider the same 3-gram instances shouldhave the same label.
Then an additional sense-labeled training corpus is built automatically inexpectation of having 95.4% precision at most.And this corpus has 2145 instances in total(official training data have 4608 instances).We build four systems to investigate the effectof our instances expansion using the Na?veBayesian classifier.
System configuration isshown in Table 1.
In column 1, BL meansbaseline, X means instance expansion, 3 and 15means the window size.
In column 2, wi is the i-th word relative to the target word, wi-1wi is the 2-gram of words, wj/j is the word with positioninformation (j?[-3,+3]).
In the last column, ?O?means using only the original training data and?O+A?
means using both the original andadditional training data.
Syntactic feature andparameter optimizing are not used in this module.System Features Window SizeTrainingDataBL_3wi, wi-1wi, wj/j?3 OX_3 ?3 O+ABL_15 ?15 OX_15 ?15 O+ATable 1: The system configuration2.2 Sentence SRL and Event DetectionWe use a feature-based machine learning methodto implement the SRL module in which threetags are labeled, namely the semantic role tag,the event role tag and the phrase type tag.
Weconsider the SRL task as a four-step pipeline: (1)parsing which generates a constituent parse treefor the input sentence; (2) pruning which filtersout many apparently impossible constituents(Xue and Palmer, 2004); (3) semantic roleidentification (SRI) which identifies theconstituent that will be the semantic role of apredicate in a sentence, and (4) semantic roleclassification (SRC) which determines the typeof identified semantic role.
The machine learningmethod takes PAS as the classification unitwhich consists of a target predicate and anargument candidate.
The SRI step utilizes abinary classifier to determine whether theargument candidate in the PAS is a real argument.Finally, in the SRC step, the semantic role tagand the event role tag of each identifiedargument can be obtained by two multi-valueclassifications on the SRI results.
The remainingphrase type tag can be directly extracted from theconstituent parsing tree.The selection of the feature set is the mostimportant factor for the feature-based SRLmethod.
In addition to constituent-based featuresand dependency-based features, we also considerWSD-based features.
To our knowledge, thecombined use of constituents-based syntacticfeatures and dependency-based syntactic featuresis the first attempts to use them both on thefeature level of SRL.
As a prevalent kind ofsyntactic features for SRL, constituent-basedfeatures have been extensively studied by manyresearchers.
In this module, we use 34constituent-based features, 35 dependency-basedfeatures, and 2 WSD-based features.
Among theconstituent-based features, 26 features aremanually selected from effective features provenby existing SRL studies and 8 new features are305defined by us.
Firstly, the 26 constituent-basedfeatures used by others are:y predicate (c1), path (c2), phrase type (c3),position (c4), voice (c5), head word (c6),predicate subcategorization (c7), syntacticframe (c8), head word POS (c9), partial path(c10), first/last word (c11/c12), first/last POS(c13/c14), left/right sibling type (c15/c16),left/right sibling head (c17/c18), left/rightsibling POS (c19/c20), constituent treedistance (c21), temporal cue words (c22),Predicate POS (c23), argument's parenttype(c24), argument's parent head (c25) andargument's parent POS (c26).And the 8 new features we define are:y Locational cue words (c27): a binary featureindicating whether the constituent containslocation cue word.y POS pattern of argument (c28): the left-to-right chain of POS tags of argument's children.y Phrase type pattern of argument (c29): theleft-to-right chain of phrase type labels ofargument's children.y Type of LCA and left child (c30): The phrasetype of the Lowest Common Ancestor (LCA)combined with its left child.y Type of LCA and right child (c31): The phrasetype of the LCA combined with its right child.y Three features: word bag of path (c32), wordbag of POS pattern (c33) and word bag of typepattern (c34), for generalizing three sparsefeatures: path (c7), POS pattern argument (c28)and phrase type pattern of argument (c29) bythe bag-of-words representation.Secondly, the selection of dependency-basedfeatures is similar to that of constituent-basedfeatures.
But dependency parsing lacksconstituent information.
If we want to usedependency-based features to label constituents,we should map a constituent to one or moreappropriate words in dependency trees.
Here weuse head word of a constituent to represent it independency parses.
The 35 dependency-basedfeatures we adopt are:y Predicate/Argument relation (d1/d2), relationpath (d3), POS pattern of predicate?s children(d4), relation pattern of predicate?s children(d5) , child relation set (d6), child POS set (d7),predicate/argument parent word (d8/d9),predicate/argument parent POS (d10/d11),left/right word (d12/d13), left/right POS(d14/d15), left/right relation (d16/d17),left/right sibling word (d18/d19), left/rightsibling POS (d20/d21), left/right siblingrelation (d22/d23), dep-exists (d24) and dep-type (d25), POS path (d26), POS path length(d27), relation path length (d28), high/lowsupport verb (d29/d30), high/low support noun(d31/d32) and LCA?s word/POS/relation(d33/d34/d35).In this work, the dependency parse trees aregenerated from the constituent parse trees using aconstituent-to-dependency converter (Marneffeet al, 2006).
The converter is suitable forsemantic analysis as it can retrieve the semantichead rather than the general syntactic head.Lastly, the 2 WSD-based features are:y Situation description formula (s1): predicate?ssituation description formula generated by theWSD module.y Natural explanation (s2): predicate?s naturalexplanation generated by the WSD module.3 Experimental Results and Discussion3.1 Target Verb WSDSystem Micro-A (%) Macro-A (%) RankBL_3 81.30 83.81 3/7X_3 79.82 82.58 4/7BL_15 79.23 82.18 5/7X_15 77.74 81.42 6/7Table 2: Official results of the WSD systemsTable 2 shows the official result of the WSDsystem.
BL_3 with window size three using theoriginal training corpus achieves the best resultin our submission.
It indicates the local featuresare more effective in our systems.
There are twopossible reasons why the performances of the Xsystem with instance expansion are lower thanthe BL system.
First, the additional instancesextracted based on 3-gram provide a few localfeatures but many topical features.
But, localfeatures are more effective for our systems asmentioned above.
The local feature relatedinformation that the classifier gets from theadditional instances is not sufficient.
Second, thegranularity of the WSD module is too small to bedistinguished by 3-grams.
As a result, theadditional corpus built upon 3-gram has moreexceptional instances (noises), and therefore itimpairs the performance of X_3 and X_15.Taking the verb ???
?
(belong to ) as anexample, it has two senses in the task, but bothsenses have the same natural explanation: ?????????????
(part of or belong to),which is always considered as the sense ingeneral SRL.
The difference between the twosenses is in their situation description formulas:?partof (x,y)+NULL?
vs. ?belongto (x,y)+NULL?.3063.2 Sentence SRL and Event DetectionIn the SRL module, we use the training dataprovided by SemEval-2010 to train the SVMclassifiers without any external resources.
Thetraining data contain 4,608 sentences, 100 targetpredicates and 13,926 arguments.
We use theSVM-Light Toolkit (Joachims, 1999) for theimplementation of SVM, and use the StanfordParser (Levy and Manning, 2003) as the parserand the constituent-to-dependency converter.
Weemploy the linear kernel for SVM and set theregularization parameter to the default valuewhich is the reciprocal of the average Euclideannorm of the training data.
The evaluation resultsof our SRL module on the official test data areshown in Table 3, where ?AB?, ?SR?, ?PT?
and?ER?
represent argument boundary, semantic roletag, phrase type tag, and event role tag.Tag Precision(%) Recall(%) F-Score(%)AB 73.10 66.83 69.82AB+SR 67.44 61.65 64.42AB+PT 61.78 56.48 59.01AB+ER 69.05 63.12 65.95Overall 58.33 53.32 55.71Table 3: Official results of the SRL systemIt is clear that ?AB?
plays an important role asthe labeling of the other three tags is directlybased on it.
Through analyzing the results, wefind that errors in the recognition of ?AB?
aremainly caused by two factors: the automaticconstituent parsing and the pruning algorithm.
Itis inevitable that some constituents andhierarchical relations are misidentified inautomatic parsing of Chinese.
These errors arefurther enlarged by the heuristic-based pruningalgorithm because the algorithm is built upon thegold-standard paring trees, and therefore a lot ofreal arguments are pruned out when using thenoisy automatic parses.
So the pruning algorithmis the current bottleneck of SRL in the evaluation.System Micro-A (%) Macro-A (%) RankBL_3 20.33 20.19 4/7X_3 20.05 20.23 5/7BL_15 20.05 20.22 6/7X_15 20.05 20.14 7/7Table 4: Official results of the ED systemsFrom the fact that the results of ?AB+SR?
and?AB+ER?
are close to that of ?AB?, it can beinferred that the SR and ER results should besatisfactory if the errors in ?AB?
are notpropagated.
Furthermore, the result of ?AB+PT?is low as the phrase types here is inconsistentwith those in Stanford Parser.
The problemshould be improved by a set of mapping rules.Finally, in the ED module, we combine theresults of WSD and SRL by filling variables ofthe situation description formula obtained by theWSD module with the arguments obtained by theSRL module according to their event role tags.Table 4 shows the final results which aregenerated by combining the results of WSD andSRL.
Obviously the reduced overall rankingcomparing to WSD is due to the SRL module.4 ConclusionsIn this paper, we propose a modular approach forthe SemEval-2010 Task on Chinese eventdetection.
Our system consists of three modules:WSD, SRL and ED.
The WSD module is basedon instances expansion, and the SRL module isbased on rich syntactic features.
Evaluationresults show that our system is good at WSD,semantic role tagging and event role tagging, butpoor at pruning and boundary detection.
In futurestudies, we will modify the pruning algorithm toreduce the bottleneck of the current system.AcknowledgmentsThis work is partially supported by the HongKong Polytechnic University under Grant No.
G-U297 and G-U596, and by the National NaturalScience Foundation of China under Grant No.60736014 and 60803094.ReferencesThorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods.Support Vector Learning, B. Sch?lkopf and C.Burges and A. Smola (ed), MIT Press.Roger Levy and Christopher D. Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank.Proceedings of ACL-2003.Vasin Punyakanok, Dan Roth, and Wentau Yih.
2005.The necessity of syntactic parsing for semantic rolelabeling.
Proceedings of IJCAI-2005.Yunfang Wu, Peng Jin, Yangsen Zhang, and ShiwenYu.
2006.
A Chinese corpus with word senseannotation.
Proceedings of ICCPOL-2006.David Yarowsky.
1993.
One sense per collocation.Proceedings of the ARPA Workshop on HumanLanguage Technology.Qiang Zhou.
2010.
SemEval-2010 task 11: Eventdetection in Chinese News Sentences.
Proceedingsof SemEval-2010.307
