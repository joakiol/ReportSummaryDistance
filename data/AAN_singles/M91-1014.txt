PRC PAKTUS :MUC-3 TEST RESULTS AND ANALYSI SCheryl KariyaPRC Inc .1500 Planning Research DriveMcLean, VA 22102email: kariya_cheryl@po.gis.prc .comFor natural language understanding researchers at PRC, preparing for MUC-3 was a usefu lendeavor in many respects .
The value of having a well-defined task, a large corpus, and a nautomated scoring program can hardly be overestimated .
This exercise has pointed out thestrengths of our system, confirmed our feelings about which aspects most need work, and taugh tus how to approach a task of this magnitude efficiently .
In this paper, we will discuss our testresults, the ways in which we prepared for the task, and lessons learned .RESULT STable 1 summarizes PRC's scores for MUC-3 Phase 2 .SLOT REC PRE OVG FALtemplate-id 88 61 39incident-date 51 60 0incident-type 69 78 0 1category 68 58 24 30indiv-perps 3 30 1 0org-perps 1 100 0perp-confidence 20 13 47 1 2phys-target-ids 9 32 1 2phys-target-num 26 66 6phys-target-types 10 67 0 0human-target-ids 2 42 0human-target-num 4 100 0human-target-types 2 70 0 0target-nationality 6 100 0 0instrument-types 0 * * 0incident-location 47 53 0phys-effects 18 24 41 2human-effects 5 21 14 1-------------------------MATCHED ONLY 32 53 20MATCHED/MISSING 28 53 20ALL TEMPLATES 28 36 46SET FILLS ONL YTable 1 : PRC Scores28 47 24 1Overall, we were pleased with PAKTUS's capabilities and performance, and we ar eexcited about planned improvements to the system .
It was particularly satisfying to observe that9 9only a few changes had to be made to the grammar, and that the lexicon structure and tools easil yaccomodated new additions .It came as no surprise to us that the discourse module was the component most in need o fdevelopment, as it is the most recent and least-developed part of the system .
Pieces for a fairlymajor restructuring are already in place, and discourse development will be one of our main effort sin the coming year.
To a large extent, our overgeneration of templates was due to the unfinishe dstate of the discourse component.
Currently, PAKTUS proposes one template for each sentenc ewhich contains an interesting-looking incident.
In discourse processing, the incident templates aremerged for the same incident type, if the date and location are not missing and not different .
Thiswas a temporary solution designed to prevent wild overgeneration .
As we achieve greater recall inthe other slots, those fills can be used in a more sophisticated merging strategy ; for the moment, asimple strategy was all that was feasible .The main limiting factor for PRC was the availability of people for development .
CPUcycles was also a factor, insofar as it limited the number of development tests that could be run in agiven amount of time .
It took from 4 to 6 hours to process 100 documents, depending on trac eoptions .We directed most of our energies to linguistic development, as shown in Table 2 below ,and except for the discourse component, the linguistic aspects of the task have essentially beencompleted.
Because we had less time remaining to devote to engineering issues, much of th einformation that PAKTUS produced has not yet been used in template-filling .
In using thelinguistic output to fill templates, we began with the two aspects of the template-filling task w econsidered the most important: identification of relevant incidents (template id) and identificatio nof incident type .
Our recall and precision scores for those slots reflect the amount of time we spenton them (see Table 1) .
We believe that, given more time to convert linguistic output to templat efills, we will be able to achieve comparable scores for the other slots as well .TIME SPENT ON DEVELOPMEN TFive PRC researchers participated in MUC-3 development .
Table 2 shows an estimate ofour level of effort for MUC-3 and a breakdown of tasks .HOURS%ofTOTAL%of LINGDEVELLING.
DEVEL.preprocessor 284 11 22grammar 154 6 1 2lexicon 457 18 35discourse 100 4 8linguistic troubleshooting 300 11 23TOTAL 1295 50 100ENGINEERING & TECHNICALTROUBLESHOOTINGsystem integration 150 6back-end engineering 300 1 1technical troubleshooting 308 1 2TOTAL 758 29100HOURS ,% of% of LINGTOTALDEVELMISC.technical direction 80 3other* 467TOTAL 547 21*learning emacs, scoring program, committee work, papers, presentation sTable 2: Breakdown of MUC-related hour sPAKTUS DEVELOPMEN TIn adapting PAKTUS (PRC's Adaptive Knowledge-based Text Understanding System) toany new domain, adjustments to each of the core system components must be made .
We brieflydiscuss the types and amounts of adjustments that were necessary in each component, and outlin esome of the new features that were added as we worked on MUC-3 .PreprocessorAs for any new message type, a new template specifying the format of the input stream hadto be built, so that the input could be separated into messages .
Methods for handling corpus-specific use of punctuation -- for example, [square] brackets and double dashes -- were developed .Header information (time and location) was saved for later processing .
Also, because time (date ofincident) played such an important role in the MUC-3 task, a new feature was added to th epreprocessor which bracketed time expressions in the text, calculated a date, and passed it on to th eparser as a symbol .
In general, adaptations to the preprocessor are only a small proportion of th eentire development effort .LexiconAdding the domain-specific lexicon required a considerable amount of time .
This isprimarily due to the volume of new items to be entered .
Especially time-consuming wasidentifying the terrorist organizations (due to variation in names, acronyms, translations) and theirassociated countries.
PAKTUS's automated tools for entering words, synonyms and compounds ,together with feature inheritance in the lexical and semantic networks, made actually enterin gindividual items easy .
Because the MUC-3 corpus contains large numbers of similar items (e .g .
,locations, terrorist organizations), facilities for batch-entering words were developed .
In additionto adding lexical items, a few features had to be added to categories of words ; for example, a slotwas added to words in the PERSON category to aid in classifying the TYPE of human target .Again, inheritance in the network made this process straightforward and fast.Another new feature for the lexicon is the use of heuristics for guessing at unknown word swhose roots are unknown .
(PAKTUS uses inflectional and derivational morphology if the root serg in the lexicon .)
Word regularities are identified using forward and reverse concordances of th eMUC-3 corpus, and exceptions to those regularities are entered in the lexicon .
An example of aheuristic used for MUC-3 is the guess that an unknown word ending in "-z" or "-o" is a Spanis hname.
We have thus far developed about two dozen heuristics.10 1GrammarThe only significant change made to the core grammar for MUC-3 involved timeexpressions .
A new arc was added to handle bracketed time expressions as adverbs, and the case-frame-applier was also adjusted.
Some additional work was done on apposition, but this was no tcorpus-specific development .
Altogether the grammar was well-suited to handle a MUC-3-lik etask, and changes were minimal .DiscoursePAKTUS's discourse component is currently fairly application-specific .
Many of thepatterns which identify important information (for this task) had to be written from scratch ,although a few were recycled from previous applications, such as MUC-2 .
The discoursecomponent, both for MUC-3 and in general, is, as we said above, the least-developed aspect o fPAKTUS, and the area in which we expect to make the most improvement in the coming year .One new feature already implemented is the addition of "word patterns," which use only lexicalinformation (but include conceptual associations), to supplement the discourse patterns, which us eparse output .
These word patterns accounted for roughly one-third of our recall .Future direction sWe put some effort into developing a pattern-based filter, to be used at the pre-parse stage ,for identifying relevant sentences and/or documents .
This filter was not used in the tests, but w eexpect to complete development and implement it for MUC-4 .
In addition, we plan to expand th ebracketing capabilities to include at least names, titles, and locations.
The major thrusts in thecoming year, however, will be :- to modify the discourse component to include more broadly-based, linguistic information; and- to develop more tools for analyzing our output .ReusabilityOne of the best aspects of the MUC-3 corpus is its generality .
Nearly all of thedevelopment we did for this effort can be reused for another application .
The exceptions would b esome domain-specific lexicon and some of the specific discourse patterns .TRAINING AND IMPROVEMEN TWe relied almost exclusively on the 100 messages in Test 1 for training after the Februaryinterim conference, as it had a reliable, consistent key .
Using Test 1, we ran approximately 30tests.
A few of these were run to determine the effect of different timing strategies on output; theothers, to test improvements in slot-filling from using lexical patterns, partial parses, run-ons, etc .The development corpus was used primarily for lexical development .
Two large tests were run onthe entire development corpus to locate a few coding errors and measure linguistic performance.The improvement in PAKTUS's linguistic performance between February and May 199 1can be seen in the following tables, derived from the test runs on the development corpus .102February resultstotal complete dparsespartialparsesrun-ons failed-----------------------------------------------------------------------------Total # of sentences 18016 7039 5460 4887 630% of sentences 100 39.1 30 .3 27 .1 3 .
5Total time (%) 100 12.7 41 .6 45 .7 N/AAvg time for parse 7.0 2.3 9.5 11 .7 N/A(in seconds)Avg time for preprocessor 0.4 0.3 0.4 0.5 N/A(in seconds)May resultstotal completedparsespartialparsesrun-ons failed---------------------------------------------------------------------------- -Total # of sentences 18584 8741 4263 5074 506% of sentences 100 47.0 22.9 27 .3 2 .
7Total time (%) 100 17.7 34.5 47 .9 N/AAvg time for parse 6.3 2.4 9.5 11 .1 N/A(in seconds)Avg time for preprocessor 2.5 2.1 2.9 3 .0 N/A(in seconds)The most significant changes is that the number and percentage of sentences that were full yparsed went up nearly 8% between February and May.
This reflects changes in all PAKTUScomponents except the discourse components .
Comparison between PRC's scores for the Test 1and Test 2 reflects the discourse improvements as well .WHAT WE LEARNE DAbout PAKTUSAs PAKTUS had never before been exercised on a scale as large as the MUC-3 corpus ,this was an opportunity to find out how it would hold up in a life-sized scenario .
We learned thatthe system architecture, knowledge representation and algorithms were more than adequate for th etask, that it was possible to do all we needed to do, with no major changes .
Further, wediscovered just how robust PAKTUS is .
We were pleasantly surprised, for example, tha tPAKTUS was able to parse 47% of the sentences in the development corpus, particularly sinc erelatively little time was spent on grammar development for MUC-3 .The MUC-3 corpus and automatic scoring tool made it possible for us to do extensiv eexperimentation on PAKTUS's timer.
We discovered several points in the parsing where speedcould be improved, while losing little or no important information .103Although we had been working on extensions to the preprocessor (e .g., document/sentencefiltering, word patterns) before the MUC-3 Phase 1 conference, participation in MUC-3 helpe dcrystallize our ideas on how best to use those extensions .
Further work showed us tha tpreprocessor output could be used in many profitable ways (e .g., helping to resolve anaphora) ,without compromising linguistic principles .Finally, as mentioned above, our participation in MUC-3 gave added impetus to thedevelopment of an improved discourse component .About the taskWorking on MUC-3 provided the PRC team of researchers valuable experience in large -scale system development .
Basically, we learned what needed to be done and how to divide th ework among ourselves efficiently .
We also, albeit somewhat belatedly, learned to make maximu muse of the tools provided us .
For example, rather late in the game, it surprised us that we had no tthought to use the development corpus as a source of information about perpetrator organizations !About evaluationTask design : Complete specification of a complex task to be performed is a non-trivialundertaking, requiring multiple iterations to resolve outstanding issues .
MUC-3 has clearl ydemonstrated that, in spite of excellent initial task design, unforeseen issues inevitably arise, andan ongoing mechanism to provide clarification is indispensable for such complex problems .Training: A valid evaluation must be representative of the domain and type of text o nwhich the system was trained.
The training corpus must therefore be sufficiently large and varie dto cover most of the issues which are likely to arise in a test .
(This is one of many areas whereMUC-3 was vastly superior to MUC-2 .)
Furthermore, for complex tasks such as MUC-3, it i scritical to have an authority available to judge the correctness of a system's response to the trainin gcorpus.
The development corpus keys, to which each site contributed 100 messages' worth ,provided such an "authority" .
The process of manually generating keys pointed out many issue swhich required task clarification, and thus reduced the risk of misinterpreting some aspects of th etask .Test Set : Selection of test messages should be done, as it was in MUC-3, from the corpu sbefore development begins .
How to ensure that the test set is representative of the corpus ,linguistically and in terms of content, is a question which it may be useful to address for MUC-4 ,as is the issue of the appropriate size of a test set .
The "answers" to the test set should be produce dindependently of the test, by an unbiased party (j the system developers).
Agreement by allparticipants to abide by the decisions of the unbiased party must be obtained before the test, an dmust be adhered to after the test .Metrics and Scoring : The automatic scoring program is a valuable tool for bot hdevelopment and testing .
It ensures a base level of impartiality in scoring, although thatimpartiality is mitigated by the fact that participants could assign themselves credit interactively ,possibly using different standards .
A possible solution, ultimately adopted for MUC-3, is to haveall results scored blindly by an unbiased party .The MUC-3 scoring program has been useful not only as a means for standardization i ntesting, but also as a development tool .
The fact that different metrics (e,g ., recall, precision ,overgeneration) were provided on a slot-by-slot basis gave us a clear picture of our progress .
Forviewing test results, this means of reporting highlights the strengths and weaknesses of differen taspects of the systems, rather than providing a single numeric score which masks the details .
I tthus provides a larger, fairer picture of the systems than would otherwise be possible .104
