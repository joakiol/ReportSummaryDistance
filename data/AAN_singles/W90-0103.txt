A Connectionist Treatment of Grammarfor Generation: Relying on EmergentsNigel WardComputer Science DivisionUniversity of  California at BerkeleyAbstractParallel Ireatment of syntactic considerations in generationpromises quality and speed.
Parallelism should be used not onlyfor simultaneous processing of several sub-parts of the output, buteven within single parts.
If beth types of parallelism are used withincremental generation it becomes unnecessary tobuild up and ma-nipulate representations of entence structure-- the syntactic formof the output can be emergent.FIG is a structured conneetionist generator bulk in this way.Constructions and their constituents are represented in the samenetwork which encodes world knowledge and lexical knowledge.Grammatical output results from synergy among many construe-tions simultaneously active at run-time.
FIG incorporates newways of handling constituency, word order and optional con-stituents; and simple ways to avoid the problems of instantiatienand binding.
Syntactic knowledge isexpressed in a simple, read-able form; this representation straightforwardly efines parts of thenetwork.1 In t roduct ionGeneration research as not yet fully identified the advan-tages offered by parallelism nor the techniques necessary totake advantage of it.
This is especially true for the syntacticaspects of generation.This paper presents a way to exploit parallelism for syn-tax in generation.
The key points are: Syntactic onstruc-tions are encoded in the same knowledge network as wordsand concepts.
Many constructions are active in parallel;there is synergy, and sometimes competition.
The syntacticform of the output emerges from interactions among con-structions at run-t ime--  explicit syntactic hoice and build-ing up of representations of syntactic structure are unneces-sary.To see that this approach works for syntactically non-trivial examples, consider that FIG's outputs include: "once1Thanks to Daniel Jurrafsky, Robert Wilensky, Dekai Wu, and TerryRegier.
This research was sponsored by the Defense Advanced ResearchProjects Agency (DoD), monitored by the Space and Naval Warfare Sys-tems Command under N00039-88-C-0292, and the Office of Naval Re-search under contract N00014-89-J-3205.
Anearly version of this paperappears inthe Proceedings of the 12th Cognitive Science Conference, Erl-baum, 1990.upon a time there lived an old man and an old woman,""one day the old man went into the hills to gather wood,"" a big peach bobbed down towards an old woman from up-stream," "an old woman gave a peach to an old man,""John broke a dish," "John made the cake vanish,"and "Mary was killed;" and when producing Japanese:"mukashi mukashi aru tokoro ni ojiisan to obaasan gasunde imashita," "aru hi ojiisan wa yama e shibakari niikimashita," "kawakami kara ookii momo ga donburikodonburako to obaasan e nagarete kimashita," "ojiisan wameeri ni momo o agemashita," and "meeri o koroshi-mashita.
"Section 2 discusses parallelism in syntax and presents thebasic proposal.
Section 3 presents a framework for connec-tionist generation, and Section 4 elaborates the proposal inthis framework.
Sections 5 through 8 discuss an implemen-tation of these ideas: Section 5 presents a representation forgrammatical knowledge, Section 6 explains how the pro-posal accounts for specific syntactic phenomena, Section 7presents an example of the generator in action, and Section8 discusses general implementation issues.
Section 9 sum-marizes.2 Para l le l  SyntaxThis section discusses two types of parallelism for syn-tax, proposes that a generator should have both of them, andsketches out the advantages of such an approach.Natural language generation research traditionally as-sumed that syntactic hoices are made in a fixed (and gen-erally top-down) order.
Yet, for incremental generation atleast, it is clear that a fixed order of decisions is not appro-priate.
This realization has led to generators which work onseveral parts of the input in parallel, simultaneously build-ing several sub-trees.
Recent work in this area includes(De Smedt 1990) and (Finkler & Neumann 1989).
I willrefer to this type of parallelism as 'part-wise' parallelism.A second kind of parallelism involves using several con-structions to generate ven one part of the output.
As faras I know, this 'within-part' parallelism has not been pro-posed in the generation literature.
It has proven useful in lin-guistics.
In Fillmore's Construction Grammar the syntactic15structure of sentences i  accounted for in terms of 'superim-position' of constructions (Fillmore 1989b).
It has also beenused in psycholingnistics, where analysis of speech errorssuggests hat even normal speech is the result of competing'plans' (Baars 1980).
More specifically, (Stemberger 1985)suggested that human speakers can be modeled as havingmany 'phrase structure units' being 'partially activated' si-multaneously.
That is, many syntactic alternatives for ex-pressing some piece of meaning are considered in parallel.I propose that a generator should exploit both part-wiseand within-part parallelism.Parallel generation is a good idea for several reasons.
1.It has been observed that part-wise parallelism is a goodway to improve the speed of response, specially for incre-mental generation.
2.
Part-wise parallelism is also usefulfor handling dependencies.
It is not always the case thatone part can be processed without consideration ofthe waythe surrounding utterance will turn out.
If the various partsare generated in parallel then knowledge about he proba-ble output for one part is available for consideration whenbuilding another part.
This can lead to better quality.
3.Given the possibility of constraints among the various yn-tactic choices involved in building an utterance, there is thepossibility that a 'first choice' will not work out when thelarger context is considered.
This suggests within-part par-allelism, so that a generator has available alternative waysto realize some information.
Given this it can find a set ofchoices satisfies all the dependencies, resulting in consis-tent and natural utterance.
4.
If a generator is indeed toconsider all the possible dependencies among choices, thenparallelism becomes necessary to cope with the amount ofcomputation necessary.
5 Parallelism is the natural way togenerate if the input is very complex (Ward 1989a).3 The FIG Approach to GenerationReduced to bare essentials, a generator's task is to getfrom concepts (what he speaker wants to express) to words(what he can say).
On this view, the key problem in genera-tion is computing the relevance (pertinence) of a particularword, given the concepts to express.
Syntactic and otherknowledge mediates this computation ofrelevance.Accordingly FIG is based on word choice - -  every otherconsideration is analyzed in terms of how it affects wordchoice.FIG is based on a large semantic network.
Words arenodes in the network, the activation they receive representsevidence for their elevance.
The basic FIG algorithm is:1. each node of the input is a source of activation2.
activation flows through the network3.
when the network settles, the most highly activatedword is selected and emitted4.
activation levels are updated to represent the new cur-rent state5.
steps 2 through 4 repeat until all of the input has beenconveyedThus FIG is an incremental generator.
Its network mustbe designed so that, when it settles, the node which is mosthighly activated corresponds to the best next word.
Thispaper discusses only the network structures which encodesyntactic knowledge.Elsewhere I argue that FIG points the way to accurateand flexible word choice (Ward 1988), producing natural-sounding output for machine translation (Ward 1989c), andmodeling the key aspects of the human language productionprocess (Ward 1989a).4 Conneetionist Syntax: OverviewIn FIG constructions and constituents also are representedas nodes in the knowledge network.
Their activation levelsrepresent their current relevance.
They interact with othernodes by means of activation flow.
Any number of construc-tions can be simultaneously active.
This handles part-wiseparallelism, competition, and superimposition.Syntactic considerations manifest themselves onlythrough their effects on the activation levels of words (di-rectly or indirectly).
An utterance is simply the result ofsuccessive word choices.
FIG does produce grammaticalsentences, most of the time, but their 'syntactic structure' isemergent, a side-effect ofexpressing the meaning.
Thus wecan say that he syntactic form of utterances is emergent inFIG 2.
This point will be illustrated repeatedly inSection 6.Mechanisms developed by linguists (and often adoptedby generation researchers), uch as unification, are not di-rected to the task of generation (or parsing) so much as tothe goal of explaining sentence structure.
Accounting forthe structure of sentences may be a worthwhile goal for lin-gnistics, but building syntactic structures i not necessaryfor language generation, as subsequent sections will show.The most common metaphor for generation is that ofmaking choices among alternatives.
For example, a gen-erator may choose among words for a concept, among waysto syntactically realize a constituent, and among conceptsto bind to a slot.
Given this metaphor, organizing choicesbecomes the key problem in generator design.
Attemptsto build parallel generators while retaining the notion ofexplicit choice run up against problems of sequencing thechoices or of doing bookkeeping so that he order of choicescan vary.
This appears to be difficult, judging by the gen-eral paucity of published outputs in descriptions of parallelgenerators.
On the other hand, relying on emergents means2post hoe examination of FIG output might make one think, for exam-ple, 'this exhibits the choice of the existential-there construction.'
In HGthere is indeed an inhibit link between the nodes ex-there and subj-pred,and so when generating the network tends to reach a state where only one ofthese is highly activated.
The most highly activated construction can havea strong effect on word choices, which is why the appearance of syntacticchoice arises.16(defp noun-phr(constituents(defp go-p(constituents(np-I(np-2(np-3(9-P-1(gp-2(gp-3(gp-4obl article ((article 1.2)))opt adjective((adJective .28)))obl noun ((cnoun .47))) ))Figure 1: Representation f the Eng~sh Noun-Phrase Constructionobl go-w ((go-w .2)))opt epart ((vparticle .6) (directionr .2)))opt'noun ((prep-phr .6) (destinationr .2)))opt verb ((purpose-clause .7) (purposer .2)))Figure 2: Representation f the Valence of "Go"))(defp ex-there(inhibit subj-pred passive)(constituents (et-i obl therew ((therew .5)))(et-2 obl verb ((verb .5)))(et-3 obl noun ((noun .3))) ))Figure3: Representation fthe Existential "Then" Constructionthere are no explicit choices to worry about, and thus thereare no problems of ordenng or bookkeeping at all(Ward1989b).In FIG all types of knowledge represented are uniformlyin the network, and interact freely at run time.
FIG not onlyallows this kind of interaction among various considerationswhen generating, it relies on it.
It relies on synergy amongconstructions in the same way that Construction Grammardoes.
It relies on synergy between semantic and syntacticconsiderations, a  seen below in Section 6.7.
It also enablesinteraction among lexical choices and syntactic onsidera-tions.5 Knowledge of SyntaxThis section presents FIG's representation f knowledge,first presenting it in a declarative form then showing howthat representation maps into network structures.Starting with this section I will be largely describing FIG-as-implemented, as of May 1990.
This is for the sake ofconcreteness.
The theory, however, is intended to applyto parallel generators in general.
Moreover, the syntacticknowledge presented in this section is purely illustrative.
Ido not claim that these represent the facts of English, northe best way to describe them in a grammar.
In particular,many generalizations are not captured.
The examples areintended simply to illustrate the representational tools andcomputational mechanisms available in FIG.
Many detailsare left unexplained for lack of space.Figure 1 shows FIG's definition of noun-phr, represent-ing the English noun-phrase construction.
This constructionhas three constituents: np-1, np-2, and np-3.
rip-1 and np-3 are obligatory, np-2 is optional.
Glossing over the detailsfor the moment, he list at the end of each constituent's defi-nition specifies how to realize the constituent.
For example,np-1, np-2, and np-3, should be realized as an article, ad-jective, and noun, respectively.Figure 2 shows the construction for the case frame of theword "go."
First comes go-w, for the word "go," which isobligatory.
Next come (optionally): a verb-particle r pre-senting direction (as in "go away" or "go back home" or"go down to the lake"), a prepositional phrase to expressthe destination, and a propose clause.Figure 3 shows the representation of the existential"there" construction, asin "there was a poor cobbler."
The'inhibit' field indicates that this construction is incompati-ble with the passive construction and also with subj-pred,the construction responsible for the basic SVO ordering ofEnglish.Figure 4 shows knowledge about when and where con-structions are relevant.
Bdetty, constructions are associatedwith words, with concepts, and with other constructions.Constructions are associated with the meanings they canexpress.
For example, ex-there is listed under the conceptintroductory, representing that this construction is appro-priate for introducing some character into the story, andpurpose-clause is listed as a way to express the purposerrelation.Constructions are associated with words.
For examplego-p is the 'valence' (case frame) of go-w and noun-phr isthe 'maximal' of cnoun.Constructions are also associated with other construc-tions.
For example, the fourth constituent of go-p subcat-egodzes for purpose-clause (Figure 2); and there are nega-tive associations among incompatible constructions, for ex-ample the 'inhibit' link between ex-there and subj-pred(Figure 3).Figure 5 shows a fragment of FIG's network, where thenumbers on the links are their weights.
This is partially17(defw peachw(smallcat cnoun) (expresses momoc)(defs cnoun (bigcat noun .4) (maximals(defw go-w (cat verb) (expresses ikuc)(grapheme (inf "go") (past "went")(defc introductoryc (properties persistent) (english(defr purposer (english (to2w .4) (purpose-clause .i))(grapheme "peach")(noun-phr .4)))(valence (go-p(pastp "gone")(english (consnt-initial; common-noun.2))(presp "going")) )(ex-there .2) ))(japanese (ni-w .6)))Figure 4: Some Knowledge Related to Constructions.5 ) )  )nou n.p hr ~'~"- - - - "~" ' - ,~, , /  \In-contextc 1 .~p-1  np-2 np-3 ,, .
4 7 \X adJecti:e %oun /X the-w a-w peachw ~ consnt-inltlal.5Figure 5: A Fragment of the Networkspecified by the knowledge shown in the previous figures.The mapping from s-expressions to network structures is notquite trivial.
For example, the link from noun to peaehwcomes from the statements that peachw has 'subcat' cnounand that cnoun has 'bigcat' noun.
Similarly, the link frompeaehw to noun-phr is inherited by peachw from the 'max-imals' information on cnoun.6 Various Syntactic Phenomena6.1 ConstituencyThe links described above suffice to handle constituency.Consider for example the fact that common ouns must bepreceded by articles in FIG's subset of English.
Supposethat peachw is activated, perhaps because a peache conceptis in the input.
Activation flows from peachw via noun-phr, rip-l, and article to a-w and the-w.In this way the relevance of a noun increases the rele-vancerating ofarticles.
Provided that other activation levelsare appropriate, this will cause some article to become themost highly activated word, and thus be selected and emit-ted.
Note that FIG does not first choose to say a noun, thendecide to say an article; rather the these 'decisions' emergeas activation levels settle.Any node can be mentioned by a constituent, thus con-structions can specify: which semantic elements to include(metonymies), what order to mention things in, what func-tion words to choose, and what inflections to use.6.2 SubcategorizationConsider the problem of specifying where a given con-cept should appear and what syntactic form it should take.In FIG this is handled by simultaneously activating a con-cept node and a syntactic construction orcategory node.
Forexample, the third constituent ofgo-p specifies that 'the di-rection of the going' be expressed as a 'verbal particle.'
Ac-tivation will thus flow to an appropriate word node, such asdownw, both via the concept filling the directionr slot andvia the syntactic ategory vparticle.
Thanks to this sort ofactivation flow FIG tends to select and emit an appropriateword in an appropriate form (Ward 1988).
Government, forexample, the way that some verbs govern case markers, ishandled in the same way.6.3 Word OrderIn an incremental connectionist generator, at each timethe activation level of a word must represent its current rele-vance.
In particular, words which are currently syntacticallyappropriate must be strongly activated.
In FIG the represen-tation of the current syntactic state is distributed across theconstructions.
There is no central process which plans ormanipulates word order; each construction simply operates18independently.
More highly activated constructions sendout more activation, and so have a greater effect.
But in theend, FIG just follows the simple rule, 'select and emit themost highly activated word.'
Thus word order is emergent.In FIG the current syntactic state is encoded in construc-tions' activation levels and 'cursors.'
The cursor of a con-struction points to the currently appropriate constituent andensures that it is relatively highly activated.
To be spe-cific, the cursor gives the location of a 'mask' specifying theweights of the links from the construction to constituents.The mask specifies a weight of 1.0 for the constituent un-der the cursor, and for subsequent constituents a weight pro-portional to their closeness to the cursor.
(Subsequent con-stituents must receive some activation so that there is part-wise parallelism.)
(For unordered constructions the weightson all construction-constituent linksare the same.
)For example, when the cursor of noun-phr points to np-1, articles receive a large proportion of the activation ofnoun-phr.
Thus, an article is likely to be the most highlyactivated word and therefore selected and emitted.
After anarticle is emitted the cursor is advanced to np-2, and so on.Advancing cursors is described in Section 6.5.In accordance with the intuition that a word is not trulyappropriate unless it is both syntactically and semanticallyappropriate, the activation level for words is given by theproduct (not the sum) of incoming syntactic and seman-tic activation, where 'syntactic activation' is activation re-ceived from constituents and syntactic ategories.
Theproblem with simply summing is that it results in the thenetwork often being in a state where many word-nodes havenearly equal activation, which makes the behavior is over-sensitive to minor changes in link weights.6.4 Optional ConstituentsWhen building a noun-phrase a generator should emit anadjective if semantically appropriate, otherwise it should ig-nore that option and emit a noun next.
FIG does this withoutadditional mechanism.To see this, suppose "the" has been emitted and the cursorof noun-pbr is on its second constituent, np-2.
As a resultadjectives get activation, via rip-2, and so to a lesser extentdo nouns via np-3.
There are two cases: If the input includesa concept linked (indirectly perhaps) to some adjective, thatadjective will receive activation from it.
In this case the ad-jective will receive more syntactic activation than any noundoes, and hence have more total activation, so it will be se-lected next.
If the input does not include any concept linkedto an adjective, then a noun will have more activation thanany adjective (since only the noun receives emantic activa-tion also), and so a noun will be selected next.Most generators use some syntax-driven procedure to in-spect semantics and decide xplicitly whether or not to real-ize an optional constituent.
In FIG, the decision to includeor to omit an optional constituent (or adjunct) is emergent- -  ff an adjective becomes highly activated it will be cho-sen, in the usual fashion, otherwise some other word, mostlikely a noun, will be.6.5 Updat ing ConstructionsRecall that FIG, after selecting and emitting a word, up-dates activation levels to represent the new state.
There areare several aspects to this.The cursors of constructions must advance as constituentsare completed.
The update mechanism can 'skip over' 'optconstituents, since, for example, ff there are no adjectives,the cursor of noun-phr should not remain stuck forever atthe second constituent.
More than one construction may beupdated after a word is output, for example, emitting anounmay cause updates to both the prep-phr construction andthe noun-phr construction.Constructions which are 'guiding' the output should bescored as more relevant.
Therefore the update processadds activation to those constructions whose cursors havechanged and sets temporary lower bounds on their activa-tion levels.
Thus, even though FIG does not make any syn-tactic plans, it tends to form a grammatical continuation ofwhatever it has already output.
After the last constituent ofa construction has been completed, the cursor is reset andthe lower bound is removed.Why is a separate update mechanism necessary?
Mostgenerators simply choose a construction and 'execute' itstraightforwardly.
However, in FIG no construction is ever'in control.'
For example, one construction may be stronglyactivating a verb, but activation from other constructionsmay 'interfere,' causing an adverbial, for example, to be in-terpolated.
Therefore constructions eed this kind of feed-back on what words have been output.6.6 No Instantiation or BindingIt is not obvious that notions of instanfiafion, binding, em-bedding, or recursion are essential for the description of nat-ural language.
Nor are mechanisms for these things essen-tial for the generation task, I conjecture.
This subsectionconsiders a problem which is usually handled with instanti-ation and shows how it can be handled more simply without.Consider the problem of generating utterances with mul-tiple 'copies,' for example, several noun phrases, or severaluses of "a".
Note that FIG as described so far would haveproblems with this.
For example since all words of cate-gory cnoun have links to noun-phr, that node might re-ceive more activation than appropriate, in cases when sev-eral nouns are active.
This could result in over-activation farticles, and thus premature output of "the," for example.In fact FIG uses a special rule for activation receivedacross inherited links: the maximum (not the sum) of theseamounts i  used.
For example, this rule applies to the 'max-imal' links from nouns to noun-phr, thus noun-phr effec-tively 'ignores' all but the most highly activated noun.
(Thiswas not shown in Figure 5.
)3_9F; q I .mm?l iFigure 6: An Input to FIG7 sp-1 ~noufsubj'pre i~go-w I old-womancl I jnp-3/old-womanwnoun-phr ~ np-1 ~ article ~'a'w \j the-w-~ in-contextwFigure 7: Selected Paths of Activation Flow Just Before Output of "the"An earlier version of FIG handled this by actually mak-ing copies.
For example, it would make a copy of noun-phr for each noun-expressible concept, and bind each copyto the appropriate concept, and to copies of a-w and the-w.
This worked but it made the program hard to extend.In particular, it was hard to choose weights uch that thenetwork would behave properly both before and after newnodes were inslantiated and linked in.6.7 Low-level CoherenceWords must stand in the correct relations to their neigh-bors.
For example, agenerator must not produce "the bigman went to the mountain" when the input calls for "theman went o the big mountain".
This is the problem of emit-ting the right adjective at the right time, or, in Other words,only emitting adjectives that stand in an appropriate r lationto the head noun.Most generators handle this easily with structure-mapping or pointer following.
For example, a syntax-directed generator may, whenever building a noun phrase,traverse the 'modified-by' pointer to find the item to turninto an adjective.
FIG, however, eschews tructure manip-ulation and pointer following.
Like all connectionist ap-proaches, therefore, it is potentially subject to problems withcrosstalk.The way to avoid this is to ensure that related concepts be-come highly activated together.
In the example, bige shouldbecome activated together with mountainc, not togetherwith old-mane.
Using a more elaborate terminology, thismeans that here should be some kind of 'focus of attention'(Chafe 1980), which successively 'lights up' groups of re-lated nodes.This condition is met in FIG, thanks to the links amongthe nodes of the input.
For example, ifmountaincl is linkedby a sizer link to bigel, then bigcl will tend to becomehighly activated whenever mountaincl is.
Thus, when old-mancl is the most highly activated concept-node, bigelwill only receive nergy from it indirectly (via an inverse-agentr link, a locationr link, and a sizer link) and thus willnot be activated sufficiently to interfere arly in the sen-tence.7 ExampleThis section describes how FIG produces "the old womanwent to a stream to wash clothes."
For this examplethe input is the set of nodes go-el, old-womancl, wash-elothescl, streamcl, and paste, linked together as shownin Figure 6.
(The names of the concepts have been an-glicized for the reader's convenience.)
(Boxes are drawnaround nodes in the input so that hey can be easily identi-fied in subsequent diagrams.
)Initially each node of the input has 11 units of activation.After activation flows, before any word is output, the mosthighly activated word node is the-w, primarily for the rea-sons shown in Figure 7.
Figure 8 shows the activation levelsof selected nodes.After "the" is emitted the update mechanism activatesnoun-phr and advances its cursor to np-2.
The most highlyactivated word becomes old-womanw, largely due to acti-vation from np-3.After "old woman" is emitted noun-phr is reset - -  thatis, the cursor is set back to np-1 and it thereby becomesready to guide production of another noun phrase.
Also,now the cursor on subj-pred advances to sp-2.
As a resultverbs, in particular go-w, become highly activated.20go-p- - -PATTERNS .
.
.
.
.
.
.
.
WORDS .
.
.
.
.
.
.
.
.
CONCEPTS-- -15.6 SUBJ-PRED 29.7 THE-W 19 .70LD-WOMANCiSP-i sp-2 21.0 A-W 15.0 IKUCi7.6 CAUSATIVEP 18 .50LD-WOMANW 14.0 KAWACICP-I cp-2 cp-3 13.3 STREAMW 13.2 SENTAKUCI6.6 NOUN-PHR 10.7 RIVERW ii.0 PASTCNP-I np-2 np-3 10.0 GO-W 8.3 VOWEL- IN IT IAL1.8 GO-P 7.5 WASH-CLOTHESW 6.1 CONSNT- INIT IALGP-i gp-2 gp-3 gp-4 3.9 TOiW 5.8 TOPICC1.4 PURPOSE-CLAUSE 3.2 MAKEW .. .
.
OTHER .. .
.
.PC-i pc-2 pc-3 2.9 TOWARDSW 13.4 CAUSER0.2 PREP-PHR 2.9 INTOW 10.4 AGENTRPP-i pp-2 2.5 TO2W 6.9 ARTICLE0.4 WITHW 4.5 NOUNFigure 8: Activation Levels of Selected Nodes Just Before Output of "the"destinationr-~gp-3 /pp-1  ~ prepositlo'~ ~tolwprep'phr ~ pp-2 noun \[ streamcl I~streamwFigure 9: Selected Paths of Activation Flow Just Before Output of "to"go-w is selected.
Because pastc has more activation thanpresentc, infinitivec and so on, go-w is inflected and emit-ted as "went" (the inflection mechanism is not described inthis paper), go-p's cursor advances to its second constituent,thus it activates directional particles, although there is no se-mantic input to any such word in this case.
tolw becomesthe most highly activated word, primarily for the reasonsshown in Figure 9.After "to" is emitted, the cursor of prep-phris advanced.The key path of activation flow is now from the second con-stituent of prep-phr to noun to streamw to noun-phr toarticle to a-w.
Thus a is selected.
The inflection mecha-nism produces "a" not "an" since consnt-initial is morehighly activated than vowel-initial.Then the cursor of noun-phr advances and "stream" isemitted.
After this the cursor of go-p advances to gp-4.From this constituent activation flows to purpose-clause,and in due course "to" and "wash clothes" are emitted.Now that all the nodes of the input are expressed, FIGends, having produced "the old woman went to a stream towash clothes.
"8 About the ImplementationI have used a connectionist model because it is a goodway to explore interactivity, parallelism, emergents, not be-cause of fondness for connectionism-for-its-own-sake.Thus I have not attempted to develop a distributed con-nectionist model.
Distributed models do have various ad-vantages, uch as elegant handling of generalizations andthe potential for learning.
Yet the current state of PDP tech-nology does not seem up to building an interactive model ofa complex task like language generation.
I therefore devel-oped FIG as a structured (localis0 connectionist system.I have also not attempted tomake FIG a 'pure' connec-tionist model.
For example, updating constructions is cur-rently done by a special process that goes in and changesactivation levels and moves the cursor.
(This process usesthe third elements in the constituent descriptions of Figures1-3, not previously discussed.)
FIG could be made more'pure' by doing this connectionistically, perhaps by addingnew nodes with special properties.
But this change wouldnot improve FIG's performance, since there seems no needfor the update process to interact with the other processes.A connectionist model of computation allows parallelismand emergents, but it certainly does not require them.
In-deed, other generators built using structured connection-ism (Kalita & Shastri 1987; Gasser 1988; Kitano 1989;Stolcke 1989) do not appear to exploit parallelism uch, nordo they exhibit emergent properties.
For example, Gasser'sCHIE relies heavily on winner-take-all subnetworks, whichcuts down on the amount of effective parallelism.
Also, farfrom exploiting emergents, CHIE uses 'neuron firings' tomodel syntactic hoices; these happen sequentially and the21exact order and timing of firings seems crucial.Currently FIG has about 350 nodes and 1000 links.
Be-fore each word choice, activation flows until the networksettles down, with cutoff after 9 cycles.
This takes about .2seconds per word on average, simulating parallel activationflow on a Symbolics 3670 (1.6 seconds on a Sun 3/140)~The correct operation of FIG depends on having correctlink weights.
I have no theory of weights, indeed rindingappropriate ones is still largely an empirical process.
How-ever there are regularities, for example, all 'inhibit' linkshave weight .7, almost all links from syntactic ategoriesto their members have weight .5, and so on.
Many of theweights have a rationale: for example, the link from rip-1to articles has a relatively high weight because articles getvery little activation from other sources.
No single weightis meaningful; the way it functions in context is.
For exam-ple, the exact weight of the link from the first constituent ofsubj-pred to noun is not crucial, as long as the product ofit and the weight on the agentr elation is appropriate.FIG's knowledge is, of course, very limited.
Adding newconcepts, words or constructions is generally straightfor-ward; they can be encoded by analogy to similar nodes, andusually the same link weights uffice.
Occasionally newnodes and links interact with other knowledge inthe systemin unforeseen ways, causing other nodes to get too muchor too little activation.
In these cases it is necessary to de-bug the network.
Sometimes trial-and-error experimenta-tion is required, but often the acceptable range of weightscan be determined by examination.
This is a kind of back-propagation by hand; it could doubtless be automated.9 SummaryI have proposed a new way to handle syntax for genera-tion.
The proposal also relies heavily on parallelism: part-wise parallelism, competition, and cooperation.
Also, syn-tactic considerations are used in parallel with lexical andworld knowledge and there is pervasive interaction amongthem.
This promises improved output quality without sacri-ricing speed, on parallel hardware.
The proposal also reliesheavily on emergents - - it does not make syntactic choicesnor build up representations of syntactic structure.
The net-work representations of linguistic knowledge affect wordchoice and order directly.This work is not traditional linguistics, artiricial intelli-gence, or connectionism, but uses techniques from all threefields.
I hope this will stimulate further work in empiricalcomputational linguistics, modeling human language pro-duction, and building useful parallel generation systems.ReferencesBaars, Bernard K. (1980).
The Competing Plans Hypoth-esis: an heuristic viewpoint on the causes of errors inspeech.
In Hans W. Dechert & Manfred Raupach, edi-tors, Temporal Variables in Speech.
Mouton.Chafe, Wallace L. (1980).
The Deployment of Conscious-ness in the Production of a Narrative.
In Wallace L. Chafe,editor, The Pear Stories.
Ablex.De Smedt, Koenrad J.MJ.
(1990).
Incremental SentenceGeneration: acomputer model of grammatical encoding.Technical Report 90-01, Nijmegen Institute for CognitionResearch and Information Technology.Fillmore, Charles (1989a).
The Mechanisms of"Construc-tion Grammar".
In Proceedings of the Berkeley LinguisticSociety, volume 15.Fillmore, Charles (1989b).
On Grammatical Constructions.course notes, UC Berkeley Linguistics DepartmenLFinkler, Wolfgang & Giinter Neumann (1989).
POPEL-HOW: A Distributed Parallel Model for Incremental Nat-ural Language Production with Feedback.
In Proceedingsof the Eleventh International Joint Conference on ArU~f-cial lntelligence.
Detroit.Gasser, Micheal (1988).
A Connectionist Model of Sen-tence Generation i a First and Second Language.
Tech-nical Report UCLA-AI-88-13, Los Angeles.Kalita, Jugal & Lokendra Shastri (1987).
Generation ofSimple Sentences in English Using the ConnectionistModel of Computation.
In 9th Cognitive Science Con-ference.
Lawrence Edbaum Associates.Kitano, Hiroaki (1989).
A Massively Parallel Model ofNatural Language Generation for Interpreting Telephony:Almost Concurrent Processing of Parsing and Genera-tion.
In Proceedings of the Second European Workshopon Natural Language Generation.Stemberger, J. P. (1985).
An Interactive Activation Modelof Language Production.
In Andrew W. Ellis, edi-tor, Progress in the Psychology of Language, Volume 1.Lawrence Erlbaum Associates.Stolcke, Andreas (1989).
Processing Unification-basedGrammars in a Connectionist Framework.
In l lth Cogni-tive Science Conference.
Lawrence Erlbaum Associates.Ward, Nigel (1988).
Issues in Word Choice.
In Proceedings12th COLING.
Budapest.Ward, Nigel (1989a).
Capturing Intuitions about HumanLanguage Production.
In Proceedings, Cognitive ScienceConference.
Lawrence Erlbaum Associates.
Ann Arbor.Ward, Nigel (1989b).
On the Ordering of Decisions in Ma-chine Translation.
In Proceedings of the Third AnnualConference of the Japanese Society for Artificial Intelli-gence, Tokyo.Ward, Nigel (1989c).
Towards Natural Machine Transla-tion.
In Proceedings of the EIC Workshop on ArtificialIntelligence, Tokyo.
Institute of Electronics, Information,and Communication E gineers.
Published as TechnicalResearch Report AI89-30.22
