Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22?31,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsBuilding a Large Annotated Corpus of Learner English:The NUS Corpus of Learner EnglishDaniel Dahlmeier1,2 and Hwee Tou Ng2,3 and Siew Mei Wu41SAP Technology and Innovation Platform, SAP Singapored.dahlmeier@sap.com2NUS Graduate School for Integrative Sciences and Engineering3Department of Computer Science, National University of Singapore{danielhe,nght}@comp.nus.edu.sg4Centre for English Language Communication, National University of Singaporeelcwusm@nus.edu.sgAbstractWe describe the NUS Corpus of Learner En-glish (NUCLE), a large, fully annotated cor-pus of learner English that is freely availablefor research purposes.
The goal of the cor-pus is to provide a large data resource for thedevelopment and evaluation of grammaticalerror correction systems.
Although NUCLEhas been available for almost two years, therehas been no reference paper that describes thecorpus in detail.
In this paper, we addressthis need.
We describe the annotation schemaand the data collection and annotation processof NUCLE.
Most importantly, we report onan unpublished study of annotator agreementfor grammatical error correction.
Finally, wepresent statistics on the distribution of gram-matical errors in the NUCLE corpus.1 IntroductionGrammatical error correction for language learnershas recently attracted increasing interest in the natu-ral language processing (NLP) community.
Gram-matical error correction has the potential to cre-ate commercially viable software tools for the largenumber of students around the world who arestudying a foreign language, in particular the largenumber of students of English as a Foreign Lan-guage (EFL).The success of statistical methods in NLP overthe last two decades can largely be attributed toadvances in machine learning and the availabilityof large, annotated corpora that can be used totrain and evaluate statistical models for various NLPtasks.
The biggest obstacle for grammatical errorcorrection has been that until recently, there was nolarge, annotated corpus of learner text that couldhave served as a standard resource for empirical ap-proaches to grammatical error correction (Leacocket al 2010).
The existing annotated learner corporawere all either too small or proprietary and not avail-able to the research community.
That is why wedecided to create the NUS Corpus of Learner En-glish (NUCLE), a large, annotated corpus of learnertexts that is freely available for research purposes.The corpus was built in collaboration with the Cen-tre for English Language Communication (CELC)at NUS.
NUCLE consists of about 1,400 student es-says from undergraduate university students at NUSwith a total of over one million words which arecompletely annotated with error tags and correc-tions.
All annotations and corrections have been per-formed by professional English instructors.
To thebest of our knowledge, NUCLE is the first annotatedlearner corpus of this size that is freely available forresearch purposes.
However, although the NUCLEcorpus has been available for almost two years now,there has been no reference paper that describes thedetails of the corpus.
That makes it harder for otherresearchers to start working with the NUCLE cor-pus.
In this paper, we address this need by giving adetailed description of the NUCLE corpus, includ-ing a description of the annotation schema, the datacollection and annotation process, and various statis-tics on the distribution of grammatical errors in thecorpus.
Most importantly, we report on an unpub-lished study of annotator agreement for grammaticalerror correction that was conducted prior to creating22Figure 1: The WAMP annotation interfacethe NUCLE corpus.
The study gives some insightsregarding the difficulty of the annotation task.The remainder of this paper is organized as fol-lows.
The next section explains the annotationschema that was used for labeling grammatical er-rors.
Section 3 reports the results of the inter-annotator agreement study.
Section 4 describes thedata collection and annotation process.
Section 5contains the error statistics.
Section 6 gives the re-lated work, and Section 7 concludes the paper.2 Annotation SchemaBefore starting the corpus creation, we had to de-velop a set of annotation guidelines.
This was donein a pilot study before the actual corpus was cre-ated.
Three instructors from CELC participated inthe pilot study.
The instructors annotated a small setof student essays that had been collected by CELCpreviously.
The annotation was performed using anin-house, online annotation tool, calledWriting, An-notation, and Marking Platform (WAMP), that wasdeveloped by the NUS NLP group specially for cre-ating the NUCLE corpus.
The annotation tool al-lows the annotators to work over the Internet usinga web browser.
Figure 1 shows a screen shot of theWAMP interface.
Annotators can browse through abatch of essays that has been assigned to them andperform the following tasks:?
Select arbitrary, contiguous text spans using thecursor to identify grammatical errors.?
Classify errors by choosing an error tag from adrop-down menu.?
Correct errors by typing the correction into atext box.?
Comment to give additional explanations ifnecessary.We wanted to impose as few constraints as possi-ble on the annotators and to give them an experiencethat would closely resemble their usual marking us-ing pen and paper.
Therefore, the WAMP annotationtool allows annotators to select arbitrary text spans,including overlapping text spans.After some annotation trials, we decided to usea tag set which had been developed by CELC ina previous study.
Some minor modifications weremade to the original tag set based on the feedbackof the annotators.
The result of the pilot study wasa tag set of 27 error categories which are groupedinto 13 categories.
The tag set is listed in Table 1.It is important to note that our annotation schemanot only labels each grammatical error with an errorcategory, but also requires an annotator to provide asuitable correction for the error as well.
The anno-tators were asked to provide a correction that wouldfix the grammatical error if the selected text spancontaining the grammatical error is replaced with thecorrection.
If multiple alternative text spans could beselected, the annotators were asked to select the min-imal text span so that minimal changes were made toarrive at the corrected text.We chose to use the tag set in Table 1 since thistag set was developed and used in a previous studyat CELC and was found to be a suitable tag set.
Fur-thermore, the tag set offers a reasonable compro-mise in terms of its complexity.
With 27 error cate-gories, it is sufficiently fine-grained to enable mean-ingful statistics for different error categories, yet notas complex as other tag sets that are much larger insize.3 Annotator AgreementHow reliably can human annotators agree onwhether a word or sentence is grammatically cor-rect?
The pilot annotation project gave us the op-portunity to investigate this question in a quantita-tive analysis.
Annotator agreement is also a mea-sure for how difficult a task is and serves as a test ofwhether humans can reliable perform the annotationtask with the given tag set.
During the pilot study,we randomly sampled 100 essays for measuring an-notator agreement.
These essays are part of the pilot23Error Tag Error Category Description / ExampleVerbsVt Verb Tense A university [had conducted ?
conducted] the surveylast year.Vm Verb modal No one [will ?
would] bother to consider a natural bal-ance.V0 Missing verb This [may ?
may be] due to a traditional notion thatboys would be the main labor force in a farm family.Vform Verb form Will the child blame the parents after he [growing ?grows] up?Subject-verb agreementSVA Subject-verb-agreement The boy [play ?
plays] soccer.Articles/determinersArtOrDet Article or Determiner From the ethical aspect, sex selection technology shouldnot be used in [non-medical ?
a non-medical] situa-tion.NounsNn Noun Number Sex selection should therefore be used for medical [rea-son ?
reasons] and nothing else.Npos Noun possessive The education of [mother?s ?
mothers] is a significantfactor in reducing son preference.PronounsPform Pronoun form 90% of couples seek treatment for family balancing rea-sons and 80% of [those ?
them] want girls.Pref Pronoun reference Moreover, children may find it hard to communicate with[his/her ?
their] parents.Word choiceWcip Wrong colloca-tion/idiom/prepositionSingapore, for example, has invested heavily [on ?
in]the establishment of BiopolisWa Acronyms Using acronyms without explaining what they stand for.Wform Word form Sex-selection may also result in [addition?
additional]stress for the family.Wtone Tone [Isn?t it ?
Is it not] what you always dreamed for?Sentence StructureSrun Runons, comma splice [Do spare some thought and time, we can make a dif-ference!
?
Do spare some thought and time.
We canmake a difference!]
(Should be split into two sentences)Smod Dangling modifier [Faced ?
When we are faced ] with the unprecedentedenergy crisis, finding an alternative energy resource hasnaturally become the top priority issue.Spar Parallelism The use of sex selection would prevent rather than [con-tributing ?
contribute] to a distorted sex ratio.Sfrag Fragment Although he is a student from the Arts faculty.Ssub Subordinate clause It is the wrong mindset of people that boys are more su-perior than girls [should ?
that should] be corrected.Table 1: NUCLE error categories.
Grammatical errors in the examples are printed in bold face in the form[<mistake>?
<correction>].24Error Tag Error Category Description / ExampleWord OrderWOinc Incorrect sentence form Why can [not we ?
we not] choose more intelligent andbeautiful babies?WOadv Adverb/adjective position It is similar to the murder of many valuable lives [onlybased ?
based only] on the couple?s own wish.TransitionsTrans Link words/phrases In the process of selecting the gender of the child, ethicalproblems arise [where ?
because] many innocent livesof unborn fetuses are taken away.MechanicsMec Punctuation, capitalization,spelling, typosThe [affect ?
effect] of that policy has yet to be felt.RedundancyRloc Local redundancy Currently, abortion is available to end a life only [becauseof ?
because] the fetus or embryo has the wrong sex.CitationCit Citation Poor citation practice.OthersOthers Other errors Any error that does not fit into any other category, but canstill be corrected.Um Unclear meaning The quality of the passage is so poor that it cannot becorrected.Table 1: NUCLE error categories (continued)data set and are not included in the official NUCLEcorpus.
The essays were then annotated by our threeannotators in a way that each essay was annotatedindependently by two annotators.
Four essays had tobe discarded as they were of very poor quality anddid not allow for any meaningful correction.
Thisleft us with 96 essays with double annotation.Comparing two sets of annotation is complicatedby the fact that the set of annotations that correctsan input text to a corrected output text is ambigu-ous (Dahlmeier and Ng, 2012).
In other words, it ispossible that two different sets of annotations pro-duce the same correction.
For example, one anno-tator could choose to select a whole phrase as oneerror, while the other annotator selects each wordindividually.
Our annotation guidelines ask annota-tors to select the minimum span that is necessary tocorrect the error, but we do not enforce any hard con-straints and different annotators can have a differentperception of where an error starts or ends.An especially difficult case is the annotation ofomission errors, for example missing articles.
Se-lecting a range of whitespace characters is difficultfor annotators, especially if the annotation tool isweb-based (as whitespace is variable in web pages).We asked annotators to select the previous or nextword and include them into the suggested correc-tion.
To change conduct survey to conduct a sur-vey, the annotator could change conduct to conducta, or change survey to a survey.
If we only com-pare the exact text spans selected by the annotatorswhen measuring agreement, these different ways toselect the context could easily cause us to concludethat the annotators disagree when they in fact agreeon the corrected phrase.
This would lead to an un-derestimation of annotator agreement.
To addressthis problem, we perform a simple text span nor-malization.
First, we ?grow?
the selected contextto align with whitespace boundaries.
For example,if an annotator just selected the last character e ofthe word use and provided ed as a correction, wegrow this annotation so that the whole word use isselected and used is the correction.
Second, we to-kenize the text and ?trim?
the context by removingtokens at the start and end that are identical in theoriginal and the correction.
Finally, the annotationsare ?projected?
onto the individual tokens they span,i.e., an annotation that spans a phrase of multiple to-25Source : This phenomenon opposes the real .Annotator A : This phenomenon opposes (the ?
 (ArtOrDet)) (real ?
reality (Wform)) .Annotator B : This phenomenon opposes the (real ?
reality (Wform)) .Table 2: Example of a sentence from the annotator agreement study with annotations from two different annotators.kens is broken up into multiple token-level annota-tions.
We align the tokens in the original text spanand the tokenized correction string using minimumedit distance.
Now, we can compare two annotationsin a more meaningful way at the token level.
Table 2shows a tokenized example sentence from the anno-tator agreement study with annotations from two dif-ferent annotators.
Annotator A and B agree that thefirst three words This, phenomenon, and opposes andthe final period are correct and do not need any cor-rection.
The annotators also agree that the word realis part of a word form (Wform) error and should bereplaced with reality.
However, they disagree withrespect to the article the: annotator A believes thereis an article error (ArtOrDet) and that the article hasto be deleted while annotator B believes that the ar-ticle is acceptable in this position.The example shows that annotator agreement canbe measured with respect to three different criteria:whether there is an error, what type of error it is,and how the error should be corrected.
Accordingly,we analyze annotator agreement under three differ-ent conditions:?
Identification Agreement of tagged tokens re-gardless of error category or correction.?
Classification Agreement of error category,given identification.?
Exact Agreement of error category and correc-tion, given identification.In the identification task, we are interested to seehow well annotators agree on whether something isa grammatical error or not.
In the example above,annotators A and B agree on 5 out of 6 tokens anddisagree on one token (the).
That results in an identi-fication agreement of 5/6 = 83%.
In the classifica-tion task, we investigate how well annotators agreeon the type of error, given that both have tagged thetoken as an error.
In the example, the classificationagreement is 100% as both annotator A and B taggedthe word real as a word form (Wform) error.
Finally,for the exact task, annotators are considered to agreeif they agree on the error category and the correctiongiven that they both have tagged the token as an er-ror.
In the example, the exact agreement is 100% asboth annotators give the same error category Wformand the same correction reality for the word real.
Weuse the popular Cohen?s Kappa coefficient (Cohen,1960) to measure annotator agreement between an-notators.
Cohen?s Kappa is defined as?
=Pr(a)?
Pr(e)1?
Pr(e)(1)where Pr(a) is the probability of agreement andPr(e) is the probability of chance agreement.
Wecan estimate Pr(a) and Pr(e) from the double an-notated essays through maximum likelihood estima-tion.
For two annotators A and B, the probability ofagreement isPr(a) =#agreed tokens#total tokens(2)where the number of agreed tokens is counted as de-scribed above, and the total number of tokens is thetotal token count of the subset of jointly annotateddocuments.
The probability of chance agreement iscomputed asPr(e) = Pr(A = 1, B = 1) + Pr(A = 0, B = 0)= Pr(A = 1)?
Pr(B = 1)+Pr(A = 0)?
Pr(B = 0)where Pr(A = 1) and Pr(A = 0) symbolize theevents of annotator A tagging a token as ?error?
or?no error?
respectively.
We make use of the factthat both annotators perform the task independently.Pr(A = 1) and Pr(A = 0) can be computedthrough maximum likelihood estimation.Pr(A = 1) =# annotated tokens of annotator A# total tokensPr(A = 0) =# unannotated tokens of annotator A# total tokens26Annotators Kappa-iden Kappa-class Kappa-exactA ?
B 0.4775 0.6206 0.5313A ?
C 0.3627 0.5352 0.4956B ?
C 0.3230 0.4894 0.4246Average 0.3877 0.5484 0.4838Table 3: Cohen?s Kappa for annotator agreement.The probabilities Pr(B = 1) and Pr(B = 0) arecomputed analogously.
The chance agreement forthis task is quite high, as the number of un-annotatedtokens is much higher than the number of annotatedtokens.
Cohen?s Kappa coefficients for the three an-notators and the average Kappa coefficient are listedin Table 3.
We observe that the Kappa scores arerelatively low and that there is a substantial amountof variability in the Kappa coefficients; annotator Aand B show a higher agreement with each other thanthey do with annotator C. According to Landis andKoch (1977), Kappa scores between 0.21 and 0.40are considered fair, and scores between 0.41 and0.60 are considered moderate.
The average Kappascore for identification can therefore only be consid-ered fair and the Kappa scores for classification andexact agreement are moderate.
Thus, an interestingresult of the pilot study was that annotators find itharder to agree on whether a word is grammaticallycorrect than agreeing on the type of error or how itshould be corrected.
The annotator agreement studyshows that grammatical error correction, especiallygrammatical error identification, is a difficult prob-lem.Our findings support previous research on an-notator agreement that has shown that grammati-cal error correction is a challenging task (Tetreaultand Chodorow, 2008; Lee et al 2009).
Tetreaultand Chodorow (2008) report a Kappa score of 0.63which in their words ?shows the difficulty of thistask and also show how two highly trained raterscan produce very different judgments.?
An interest-ing related work is (Lee et al 2009) which investi-gates the annotation of article and noun number er-rors.
The annotation is performed with either a sin-gle sentence context only or the five preceding sen-tences.
The agreement between annotators increaseswhen more context is given, from a Kappa score of0.55 to a Kappa score of 0.60.
Madnani et al(2011)and Tetreault et al(2010) propose crowdsourcing toovercome the problem of annotator variability.4 Data Collection and AnnotationThe main data collection for the NUCLE corpustook place between August and December 2009.
Wecollected a total of 2,249 student essays from 6 En-glish courses at CELC.
The courses are designed forstudents who need language support for their aca-demic studies.
The essays were written as courseassignments on a wide range of topics, like technol-ogy innovation or health care.
Some example ques-tion prompts are shown in Table 4.
All students areat a similar academic level, as they are all undergrad-uate students at NUS.
Students would typically haveto write two essay assignments during a course.
Thelength of each essay was supposed to be around 500words, although most essays were longer than the re-quired length.
From this data set, a team of 10 CELCEnglish instructors annotated 1,414 essays with over1.2 million words between October 2009 and April2010.
Due to budget constraints, we were unfortu-nately not able to perform double annotations for themain corpus.
Annotators were allowed to label anerror multiple times if the error could be assignedto more than one error tag, although we observedthat annotators did not make much use of this option.Minimal post-processing was done after the annota-tion process.
Annotators were asked to review somecorrections that appeared to contain annotation mis-takes, for example redundancy errors that did not re-move the annotated word.
The final results of theannotation exercise were a total of 46,597 error tags.The essays and the annotations were released as theNUCLE corpus through the NUS Enterprise R2Mportal in June 2011.
The link to the corpus can befound on the NUS NLP group?s website1.5 NUCLE Corpus StatisticsThis section provides basic statistics about the NU-CLE corpus and the collected annotations.
Thesestatistics already reveal some interesting insightsabout the nature of grammatical errors in learnertext.
In particular, we are interested in the follow-ing questions: how frequent are errors in the NU-CLE corpus and what are the most frequent error1www.comp.nus.edu.sg/?nlp/corpora.html27?Public spending on the aged should be limited so that money can be diverted to other areas of the country?s develop-ment.?
Do you agree?Surveillance technology such as RFID (radio-frequency identification) should not be used to track people (e.g., humanimplants and RFID tags on people or products).
Do you agree?
Support your argument with concrete examples.Choose a concept or prototype currently in research and development and not widely available in the market.
Presentan argument on how the design can be improved to enhance safety.
Remember to consider influential factors such ascost or performance when you summarize and rebut opposing views.
You will need to include very recently publishedsources in your references.Table 4: Example question prompts from the NUCLE corpus.NUS Corpus of Learner EnglishDocuments 1,414Sentences 59,871Word tokens 1,220,257Word types 30,492Error annotations 46,597# of sentences per document 42.34# of word tokens per document 862.98# of word tokens per sentence 20.38# of error annotations per document 32.95# of error annotations per 100 word tokens 3.82Table 5: Overview of the NUCLE corpuscategories?
The basic statistics of the NUCLE cor-pus are shown in Table 5.
In these statistics, wetreat multiple alternative annotations for the sameerror as separate errors, although it could be arguedthat these should be merged into a single error withmultiple alternative corrections.
Fortunately, onlyabout 1% of the errors are labeled with more thanone annotation.
We can see that grammatical errorsare very sparse, even in learner text.
In the NU-CLE corpus, there are 46,597 annotated errors for1,220,257 word tokens.
That makes an error densityof 3.82 errors per hundred words.
In other words,most of the word tokens in the corpus are grammat-ically correct.
This shows that the students whoseessays were used for the corpus already have a rel-ative high proficiency of English.
When we lookat the distribution of errors across documents, wecan make another interesting observation.
Figure 2shows a histogram of the number of error annota-tions per document.
The distribution appears non-Gaussian and is heavily skewed to the left with mostdocuments having less than 30 errors while somedocuments have significantly more errors than theaverage document.
That means that although gram-matical errors are rare in general, there are also doc-05101520253035400  20  40  60  80  100  120  140  160  180  200Number of documentsNumber of error annotationsHistogram: error annotation per documentFigure 2: Histogram of error annotations per documentin NUCLE.uments with many error annotations.
32 documentshave more than 100 error annotations and the highestnumber of error annotations in a document is 194.The mode, i.e., the most frequent value in the his-togram, is 15 which is to the left of the average of32.95.
A similar pattern can be observed when welook at the distribution of errors per sentence.
Fig-ure 3 shows a histogram of the number of error anno-tations per sentence in the NUCLE corpus.
For thishistogram, only the error annotations which start andend within sentence boundaries are considered (thisaccounts for 98.6% of all error annotations).
Sen-tence boundaries are determined automatically usingthe NLTK Punkt sentence splitter2.
The histogramshows that 57.64% of all sentences have zero errors,20.48% have exactly one error, and 10.66% have ex-actly two errors, and 11.21% of all sentences havemore than two errors.
Although the frequency de-creases quickly for higher error counts, the highestobserved number of error annotations for a sentenceis 28.2nltk.org28050001000015000200002500030000350000  5  10  15  20  25Number of sentencesNumber of error annotationsHistogram: error annotation per sentenceFigure 3: Histogram of error annotations per sentence inNUCLE.The skewed distribution of errors in the NUCLEcorpus is an interesting observation.
A possible ex-planation for the long tail of the distribution could bea ?rich-get-richer?
type of dynamics: if a learner hasmade a lot of mistakes in her essay so far, the chanceof her making more errors in the remainder of theessay increases, for example because she makes sys-tematic errors which are likely to be repeated.
Ex-plaining the cognitive processes that produce the ob-served error distribution is beyond the scope of thispaper, but it would certainly be an interesting ques-tion to investigate.So far, we have only been concerned with howmany errors learners make overall.
But it is alsoimportant to understand what types of errors lan-guage learners make.
Error categories that appearmore frequently should be addressed with higherpriority when creating an automatic error correctionsystem.
Figure 4 shows a histogram of error cate-gories.
Again, we can observe a skewed distribu-tion with a few error categories being very frequentand many error categories being comparatively in-frequent.
The top five error categories are wrongcollocation/idiom/preposition (Wcip) with 7,312 in-stances or 15.69% of all annotations, local redun-dancies (Rloc) (6,390 instances, 13.71%), article ordeterminer (ArtOrDet) (6,004 instances, 12.88%),noun number (Nn) (3,955 instances, 8.49%), andmechanics (Mec) (3,290 instances, 7.06%).
Thesetop five error categories account for 57.83% of all er-ror annotations.
The next 5 categories are verb tense(Vt) (3,288 instances, 7.06%) word form (Wform)010002000300040005000600070008000WcipRlocArtOrDetNn MecVt WformSVAOthersVformTransUm PrefSrunCit WOincWtoneSparVm V0 SsubWOadvNposSfragPformSmodWaNumberofannotationsError categoriesError categoriesFigure 4: Error categories histogram for the NUCLE cor-pus.
(2,241 instances, 4.81%), subject-verb agreement(SVA) (1,578 instances, 3.38%), other errors thatcould not be grouped into any of the error categories(1,532 instances, 3.29%), and Verb form (Vform)(1,531, 3.29%).
Together, the top 10 error cate-gories account for 79.66% of all annotated errors.A manual inspection showed that a large percentageof the local redundancy errors involve articles thatare deemed redundant by the annotator and shouldbe deleted.
These errors could also be consideredarticle or determiner errors.
For the Wcip errors,we observed that most Wcip errors are prepositionerrors.
This confirms that articles and prepositionsare the two most frequent error categories for EFLlearners (Leacock et al 2010).6 Related WorkIn this section, we compare NUCLE with otherlearner corpora.
While there were almost no an-notated learner corpora available for research pur-poses until recently, non-annotated learner corporahave been available for a while.
Two examples arethe International Corpus of Learner English (ICLE)(Granger et al 2002) and the Chinese Learner En-glish Corpus (Gui and Yang., 2003)3.
Rozovskayaand Roth (2010) annotated a portion of each of thesetwo learner corpora with error categories and correc-tions.
However, with 63,000 words, the annotateddata is small compared to NUCLE.3The Chinese Learner English Corpus contains annotationsfor error types but does not include corrections for the errors.29The Cambridge Learner Corpus (CLC) (Nicholls,2003) is possibly the largest annotated Englishlearner corpus.
Unfortunately, to our knowledge,the corpus is not freely available for research pur-poses.
A subset of the CLC was released in 2011by Yannakoudakis et al(2011).
The released dataset contains short essays written by students takingthe First Certificate in English (FCE) examination.The data set was also used in the recent HOO 2012shared task on preposition and determiner correction(Dale et al 2012).
Comparing the essays in the FCEdata set and NUCLE, we observe that the essays inthe FCE data set are shorter than the essays in NU-CLE and show a higher density of grammatical er-rors.
One reason for the higher number of errors (inparticular spelling errors) is most likely that the FCEdata was not collected from take-home assignmentswhere students have the chance to spell check theirwriting before submission.
But it could also meanthat the essays in FCE are from students with a lowerproficiency in English compared to NUCLE.
Withregards to the annotation schema, the CLC annota-tions include both the type of error (missing, unnec-essary, replacement, form) and the part of speech.As a result, the CLC tag set is large with 88 differ-ent error categories, far more than the 27 error cate-gories in NUCLE.Finally, the HOO 2011 shared task (Dale and Kil-garriff, 2011) released an annotated corpus of frag-ments from academic papers written by non-nativespeakers and published in a conference or work-shop of the Association for Computational Linguis-tics.
The corpus uses the annotation schema fromthe CLC.
Comparing the data set with NUCLE, theHOO 2011 data set is much smaller (about 20,000words for training and testing, respectively) and rep-resents a specific writing genre (NLP papers).
TheNUCLE corpus is much larger and covers a broaderrange of topics.7 ConclusionWe have presented the NUS Corpus of Learner En-glish (NUCLE), a large, annotated corpus of learnerEnglish.
The corpus contains over one millionwords which are completely annotated with gram-matical errors and corrections.
The NUCLE corpusis freely available for research purposes.
We havealso reported an inter-annotator agreement study forgrammatical error correction.
The study shows thatgrammatical error correction is a difficult task, evenfor humans.
The error statistics from the NUCLEcorpus show that learner errors are generally sparseand have a long-tail distribution.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesJ.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20(1):37?46.D.
Dahlmeier and H.T.
Ng.
2012.
Better evaluation forgrammatical error correction.
In Proceedings of HLT-NAACL, pages 568?572.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own:The HOO 2011 pilot shared task.
In Proceedings ofthe Generation Challenges Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 242?249.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
HOO2012: A report on the preposition and determiner errorcorrection shared task.
In Proceedings of the SeventhWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 54?62.S.
Granger, F. Dagneaux, E. Meunier, and M. Paquot.2002.
The International Corpus of Learner English.Presses Universitaires de Louvain, Louvain-la-Neuve,Belgium.S.
Gui and H. Yang.
2003.
Zhongguo Xuexizhe YingyuYuliaohu (Chinese Learner English Corpus).
Shang-hai Waiyu Jiaoyu Chubanshe.
In Chinese.J.R.
Landis and G.G Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Automated Grammatical Error Detection forLanguage Learners.
Morgan & Claypool Publishers.J.
Lee, J. Tetreault, and M. Chodorow.
2009.
Humanevaluation of article and noun number usage: Influ-ences of context and construction variability.
In Pro-ceedings of the Linguistic Annotation Workshop III(LAW3), pages 60?63.30N.
Madnani, J. Tetreault, M. Chodorow, and R. Ro-zovskaya.
2011.
They can help: using crowdsourc-ing to improve the evaluation of grammatical error de-tection systems.
In Proceedings of ACL:HLT, pages508?513.D.
Nicholls.
2003.
The Cambridge learner corpus: Errorcoding and analysis for lexicography and ELT.
In Pro-ceedings of the Corpus Linguistics 2003 Conference,pages 572?581.A.
Rozovskaya and D. Roth.
2010.
Annotating ESL er-rors: Challenges and rewards.
In Proceedings of theFifth Workshop on Innovative Use of NLP for BuildingEducational Applications, pages 28?36.J.
Tetreault and M. Chodorow.
2008.
Native judgmentsof non-native usage: Experiments in preposition errordetection.
In Proceedings of the Workshop on HumanJudgements in Computational Linguistics, pages 24?32.J.
Tetreault, E. Filatova, and M. Chodorow.
2010.
Re-thinking grammatical error annotation and evaluationwith the Amazon Mechanical Turk.
In Proceedingsof the Fifth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 45?48.H.
Yannakoudakis, T. Briscoe, and B. Medlock.
2011.A new dataset and method for automatically gradingESOL texts.
In Proceedings of ACL:HLT, pages 180?189.31
