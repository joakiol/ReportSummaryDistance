Weighted Rational Transductions and their Application to HumanLanguage ProcessingFernando Pereira Michael Riley Richard SproatAT&T Bel l  Laborator ies600 Mounta in  Ave.Mur ray  Hill, NJ  07974ABSTRACTWe present the concepts of weighted language, ~ansduction a d au-tomaton from algebraic automata theory as a general framework fordescribing and implementing decoding cascades in speech and lan-guage processing.
This generality allows us to represent uniformlysuch information sources as pronunciation dictionaries, languagemodels artd lattices, and to use uniform algorithms for building de-coding stages and for optimizing and combining them.
In particular,a single automata join algorithm can be used either to combine in-formation sources uch as a pronunciation dictionary and a context-dependency model during the construction of a decoder, or dynam-ically during the operation of the decoder.
Applications to speechrecognition and to Chinese text segmentation will be discussed.1.
In t roduct ionAs is well known, many problems in human language process-ing can be usefully analyzed in terms of the "noisy channel"metaphor: given an observation sequence o, find which in-tended message w is most likely to generate that observationsequence by maximizingP(w, o) = P(olw)P(w),where P(olw ) characterizes the transduction between in-tended messages and observations, and P(w) characterizesthe message generator.
More generally, the transduction be-tween messages and observations may involve several inter-mediate stagesP( so, sk )= P( sk Iso)P(so)P(sklso)=~,, ..... ,~_,P(sklsk-X)'"P(sllso) (1)where P(sk Iso) is the probability of transducing so to skthrough the intermediate stages, assuming that each step in thecascade is conditionally independent from the previous ones.Each s t is a sequence of units in an appropriaterepresentation.For instance, in speech recognition some of the intermediatestages might correspond to sequences of units like phones orsyllables.
A stralghtforwardbut useful observation is that anysuch a cascade can be factored at any intermediate stageP(sils~) = ~ P(s~lsk)P(skls.i) (2)SkFor computational reasons, sums and products in (1) are oftenreplaced by minirnizations and sums of negative log probabil-ities, yielding the approximationP(s0,sk) = P(sklso) + P(s0) (3)P(s ls0) rain,, ..... ,~_ ,E l< j_<kP(s~ls j= l )where X = - log X.
In this formulation, assuming the ap-proximation is reasonable, the most likely message so is theone minimizing P(s0, sk).Finally, each transduction i  such a cascade is often modeledby some finite-state device, for example a hidden Markovmodel.Although the above approach is widely used in speech andlanguage processing, usually the elements of the transductioncascade are built by "ad hoc" means, and commonalities be-tween them are not exploited.
We will here outline how thetheory of weighted rational languages and transductions canbe used as a general framework for transduction cascades.This theoretical foundation provides a rich set of operatorsfor combining cascade lements that generalizes the standardoperations on regular languages, uggests novel ways of com-bining models of different parts of the decoding process, andsupports uniform algorithms for transduction a d search at alllevels in the cascade.
In particular, we developed a genericjoin algorithm for combining any two consecutive l vels of acascade, a generic best-path search algorithm, and a genericinterleaving of join and search for building pruned joins.
Inaddition, general finite-state minimization techniques are alsoapplicable to all levels of a cascade.Weighted languages and transductions are generalizations ofthe standard notions of language and transduction i formallanguage theory \[1, 2\].
A weighted language isjust a mappingfrom strings over an alphabet to weights.
A weighted trans-duction is a mapping from pairs of strings over two alphabetsto weights.
For example, when weights represent proba-bilities and assuming appropriate normalization, a weightedlanguage is just a probability distribution over strings, anda weighted trarisduction a joint probability distribution overstring pairs.
The weighted rationallanguages andtransducersare those that can be represented by weighted finite-state ac-ceptors (WFSAs) and weighted finite-state ransducers (WF-STs), as described in more detail in the next section.
In thispaper we will be concerned with the weighted rational case,although some of the theory can be profitably extended beyondthe finite-state case \[3, 4\].262The notion of weighted rational transduction arises from thecombination of two ideas in automata theory: rational trans-ductions, used in many aspects of formal anguage theory \[2\],and weighted languages and automata, developed in patternrecognition \[5, 6\] and algebraic automata theory \[7, 8, 9\].Ordinary (unweighted) rational transductions have been suc-cessfully applied by researchers atXerox PARC \[10\] and atthe University of Paris 7 \[11\], among others, to several prob-lems in language processing, includifig morphological nal-ysis, dictionary compression and syntactic analysis.
HiddenMarkov Models and probabilistic finite-state language mod-els can be shown to be equivalent to WFSAs.
In algebraicautomata theory, rational series and rational transductions \[8\]are the algebraic ounterparts of WFSAs and WFSTs andgive the correct generalizations to the weighted case of thestandard algebraic operations on formal anguages and trans-ductions, such as union, concatenation, i tersection, restric-tion and composition.
We believe the work presented hereis among the first to apply these generalizations to human-language processing.Our first application is to speech recognition decoding.
Weshow that a conventional HMM decoder can be naturallyviewed as equivalent to a cascade of weighted transductions,and that our approach requires no modification whatsoeverwhen context dependencies cross higher-level unit boundaries(for instance, cross-word context-dependent models).Our second application is to the segmentation f Chinese textinto words, and the assignment of pronunciations to thosewords.
In Chinese orthography, most characters represent(monosyllabic) 'morphemes', and as in English, 'words' mayconsist of one or more morphemes.
Given that Chinese doesnot use whitespace to delimit words, it is necessary to recon-struct he grouping of characters into words.
This reconstruc-tion can also be thought of as a transduction problem.2.
TheoryIn the transduction cascade (1), each step corresponds to amapping from input-output pairs (r, s) to probabilities P(slr).More formally, steps in the cascade will be weighted trans-ductions T : 27 x F* ~ K where 27 and F* the sets ofstrings over the alphabets Z and F, and K is an appropriate s tof weights, for instance the real numbers between 0 and 1 inthe case of probabilities.
We will denote by T-  1 the inverseof T defined by T(t, s) = T(s, t).The right-most step of (1) is not a transduction, but rather aninformation source, in that case the language model.
We willrepresent such sources as weighted languages L : Z* ~ K.Given two transductions S : Z* x F* ~ K and T : F* x A* ---~K, we can define their composition S o T by(S o T)(r, t) = E S(r, s)T(s, t) (4)sEI"*For example, i fS  represents P(sk Isj) and T P(sj Isi) in (2),Figure 1: Recognition Cascadeis is clear that S o T represents P(sk Isi).A weighted transduction S : Z* ?
F* --.
K can be applied to aweighted language L : Z* ~ K to yield a weighted languageover F. It is convenient to abuse notation somewhat and useM o S for the result of the application, defined as(L o S)(t) = E L(s)S(s,t) (5)sEF ?Furthermore, if M is a weighted language over F, we canreverse apply S to M, written S o M = M o (S- : ) .
Forexample, i fS  represents P(sk \[so) and M represents P(so) in(1), then S o M represents P(po, pk).Finally, given two weighted languages M, N : Z* ~ K wedefine their intersection, also by convenient abuse of notationwritten M o N as:(M o N)(t) ='M(s)N(s) (6)In any cascade R1 o .
.
.
o Rm, with the Ri for 1 < i < mappropriate transductions and R1 and Rm transductions orlanguages, it is easy to see that the order of association ofthe o operators does not matter.
For example, if we haveL o S o T o M, we could either apply S to L, apply T tothe result and intersect the result with M, or compose S withT, reverse apply the result o M and intersect the result withL.
We are thus justified in our use of the same symbol forcomposition, application and intersection, and we will in therest of the paper use the term "(generalized) composition" forall of these operations.For a more concrete xample, consider the transduction cas-cade for speech recognition depicted in Figure 1, where A isthe transduction from acoustic observation sequences tophonesequences, D the transduction from phone sequences towordsequences (essentially a pronunciation dictionary) and M aweighted language representing the language model.
Given aparticular sequence of observations o, we can represent i  asthe trivial weighted language O that assigns 1 to o and 0 toany other sequence.
Then O o A represents he acoustic likeli-hoods of possible phone sequences that generate o,O o A o Dthe aeoustic-lexical likelihoods of possible word sequencesyielding o, and O o A o D o M the combined acoustic-lexical-linguistic probabilities of word sequences generating o. Theword string w with the highest weight (0 o A o D o M)(w)is precisely the most likely sentence hypothesis generating o.Exactly the same construction could have been carried outwith weights combined by rain and sum instead of sum andproduct in the definitions of application and intersection, and263Language Transductionsingletonscalingsumconcatenationpowerclosure{u}(v) = I i f fu = v(kL)(u) = kL(u)(L + M)(u) = L(u) + M(u)(LM)(w) = ~uv=~o L(u)M(v)L?
(e) = IL?
(u ~ e) = 0L n+l = LL"L* = ~k>o Lk{(u, v)}(w,z) = 1 i f fu = w and v = z(kT)(u, = kT( , v)(S + T)(u, v) = S(u, v) + T(u, v)(ST)(t, w) : E , , : t ,~.=w S(r, u)T(s, v)T?
(e, e) = 1= 0Tn + 1 : TT  nT* = ~k>o TkTable 1: Rational Operationsin that case the string w with the lowest weight (O o A o D oM)(w) 'would the best hypothesis.
More generally, the sumand product operations in (4), (5) and (6) can be replaced byany two operations forming an appropriate s miring \[7, 8, 9\],of which numeric addition and multiplication and numericminimum and addition are two examples 1Generalized composition is thus the main operation involvedin the construction and use of transduction cascades.
As wewill see in a moment, for rational languages and transductions,all instances of generalized composition are implemented bya uniform algorithm, the join of two weighted finite automata.In addition to those operations, weighted languages and trans-ductions can be constructed from simpler ones by the opera-tions shown in Table 1, which generalize in a straightforwardway the regular operations well-known from traditional au-tomata theory \[1\].
In fact, the rational languages and trans-ductions are exactly those that can be built from singletons byapplications of scaling, sum, concatenation a d closure.For example, assume that for each word w in a lexicon we aregiven a rational transduction D,o such that D~ (p, w) is theprobability that w is realized as the phone sequence p. Notethat this crucially allows for multiple pronunciations for w.Then the rational transduction (~ D,o) * gives the probabil-ities for realizations of word sequences as phone sequences(ignoring possible cross-word ependencies, which will bediscussed in the next section).Kleene's theorem states that regular languages are exactlythose representable by finite-state acceptors \[1\].
Its gener-alization to the weighted case and to transducers states thatweighted rational languages and transducers are exactly thosethat can be represented by finite automata \[8\].
Furthermore,all the operations on languages and transductions we havediscussed have finite-automata counterparts, which we haveimplemented.
Any cascade representable in terms of thoseoperations can thus be implemented directly as an appropri-ate combination of the programs implementing each of theoperations.lAdditional conditions to guarantee the existence of certain infinite sumsmay be necessary for certain semirings, for details ee \[7\] and \[8\].In the present setting, a K-weighted finite automaton.,4 con-sists of a finite set of states Qa and a finite set Aa of transitionss//~ qlq --, between states, where x is an element of the set oftransition labels AA and k E K is the transition weight.
Anassociative concatenation operation u ?
v must defined be-tween transition labels, with identity element ect.
As usual,each automaton has an initial state iA and a final state as-signment, which we represent as column vector of weightsFA indexed by states:.
A K-weighted finite automaton withAA = Z* is just a weighted finite-state acceptor (WFSA).
Onthe other hand, ifAA = Z* x F* with concatenation definedby (r, s) .
(u, v) = (ru, sv), we have a weightedfinite-statetransducer (WFST).As usual, we can define a path in an automa-ton .,4 as a sequence of connected transitions /3 =(q0, xl, kl, ql), ?
?., (qra-1, Xm, kin, qm).
Such a path has la-bel LA(p) = xl .
.
.
.
.
z,~, weight Wa(15) = k l  ' '  "krn andfinal weight F - W~ (p) = WA(pP)FA(qm).
We call ff reduced ifit is the empty path or i fx l  # e, and we write p ~,~ p' if k isthe sum of the weights of all reduced paths with label u fromq to q~.The language of automaton .,4 is defined asf~I~(~)where I.a(u) is the set of paths in .,4 with label u that start inthe initial state i.d.
Obviously, if .,4 is an acceptor, \[.A\] is aweighted language, and i fA is a transducer \[,4\]\] is a weightedtransduction.
The appropriate generalization fKleene's the-orem to weighted acceptors and transducers states that undermild conditions on the weights (which for instance are satis-fied by the rain, sum semiring), weighted rational languagesand transductions are exactly those defined by weighted au-tomata s outlined here \[8\].Weighted acceptors and transducers are thus faithful imple-mentations of rational anguages and transductions, and all2The usual notion of final state can be encoded this way by settingFA(q) = 1 ffq is final, FA(q) = 0 otherwise.264(a) ~ 1  ~ = ~ .
. "
o .~(b)oi:E/pi Oi:e../pi O/:Jpi(d) o~~- , ' (~  )Figure 2: Models as Automatathe operations on these described above have correspondingimplementations i  terms of algorithms on automata.
In par-ticular, generalized composition corresponds to the join oftwo automata.Given two automata ..4 and B and a new label set J ,  anda partial label join function ~:  A~ x An ~ J, we definetheir join by t~ as a new automaton C with label set J ,  statesQc = Q~ x Qt~, initial state ic = (i.a, it3), final weightsFc(q, q') = F~(q)Ft3(q) and transitions(p,p') (q, q')iff k = ab (7)~=y~z ,p ~.~q ,p' ~bq ~Different choices of t~ correspond to the instances of gen-eralized composition: for intersection, Aa = An = Z*,z = V~ z i f fz  = y = z; for composition, AA = Z* x F*,At3 = F* x A* and (z, z) = (z, y) ~ (y, z); and for appli-cation = AaZ*, As = Z* x F* and y = z ~ (z, y).
Thusjoin is the automata counterpart of generalized composition,and we will use the composition symbol indiferently in whatfollows to represent either composition or join.The operation between automata thus defined has a directdynamic-programming mplementation i  which reachablejoin states (q, q') are placed in a queue and extended in turnusng (7).
By organizing this queue according to the weightsof least-weight paths from the start state, we can combine joincomputation with search for lowest-weight paths, and subau-tomata of the join with states reachable by paths with weightswithin a beam of the best path.3.
Speech RecognitionIn our first application, we elaborate on how to describe aspeech recognizer as a transduction cascade.
Recall we de-compose the problem into a language, O, of acoustic observa-tion sequences, a transduction, A from acoustic observationsequences tophone sequences, a transduction, D from phonesequences to word sequences and a weighted language, M,specifying the language model (see Figure 1).
Each of thesecan be represented as a finite-state automaton (to some ap-proximation).The trivial automaton for the acoustic observation language,O, is defined for a given utterance as depicted in Figure 2a.Each state represents a fixed point in time ti, and each transi-tion has a label, oi, drawn from a finite alphabet that quantizesthe acoustic waveform between adjacent time points and is as-signed probability 1.0.The automaton for the acoustic observation sequence to phonesequence transduction, A, is defined in terms of phone models.A phone model is defined as a transducer f om a subsequenceof acoustic observation labels to a specific phone, and assignsto each subsequence a likelihood that the specified phoneproduced it.
Thus, different paths through a phone modelcorrespond to different acoustic realizations of the phone.Figure 2b depicts acommon topology for such a phone model.A is then defined as the closure of the sum ofthephone models.The automaton for the phone sequence toword sequence trans-duction, D, is defined similarly to that for A.
We define a wordmodel as a transducer f om a subsequence of phone labels toa specific word, which assigns to each subsequence a like-lihood that the specified word produced it.
Thus, differentpaths through aword model correspond to different phoneticrealizations of the word.
Figure 2c depicts a common topol-ogy for such a word model.
D is then defined as the closureof the sum of the phone models.Finally, the language model, M, is commonly an N-grammodel, encodable as a WFSA.
Combining these automata,(0  o A o D o M) (w)  is thus an automaton that assigns aprobability to each word sequence, and the highest-probabilitypath through that automaton estimates the most likely wordsequence for the given utterance.The finite-state modeling for speech recognition that we havejust described is hardly novel.
In fact, it is equivalent tothat presented in \[12\], in the sense that it generates the sameweighted language.
However, the transduction cascade ap-proach presented here allows one to view the computations innew ways.For instance, because composition, o, is associative, we seethat the computation of max,o(O o A o D o M) (w)  can beorganized in several ways.
A conventional integrated-search,speech recognizer computes maxw(O o (A o D o M) ) (w) .In other words, the phone, word, and language models are,in effect, compiled together into one large transducer whichis then applied to the input observation sequence \[12\].
Onthe other hand, one can use a more modular, staged compu-tation, maxw(((O o A) o D) o M) (w) .
In other words, firstthe acoustic observations are transduced into a phone latticerepresented asan automaton labeled by phones (phone recog-265nition).
'This lattice is in turn transduced into a word lattice(word recognition), which is then joined with the languagemodel (language model application) \[13\].The best approach may depend on the specific task, whichdetermines the size of intermediate r sults and the whetherfinite-state minimization is fruitful.
By having a generalpackage to manipulate these automata, we have been ableto experiment with various alternatives.
For many tasks, thecomplete; network, O o A o D o M, is too large to computeexplicitly, regardless of the order in which the operations areapplied.
The solution that is usually taken is to interleave thebest path computation with the composition operations and toretain only a portion of the intermediate r sults by discardingunpromising paths.So far, our presentation has used context-independent phonemodels.
In other words, the likelihoods assigned by a phonemodel in A assumed conditional independence from neigh-boring phones.
However, it has been shown that context-dependent phone models, which model a phone in the contextof its adjacent phones, are very effective for improving recog-nition performance \[14\].We can include context-dependent models, such as triphonemodels, in our presentation byexpanding our 'atomic models'in A to one for every phone in a distinct riphonic ontext.Each model will have the same form as in Figure 2b, butwill have different likelihoods for the different contexts.
Wecould also try to directly specify D in terms of the new units,but this is problematic.
First, even if each word in D hadonly one phonetic realization, we could not directly substituteits spelling in terms of context-dependent u its, since thecross-word units must be specified (because of the closureoperation).
In this case, a common approach is to eitheruse left (right) context-independent uni s at the word starts(ends), or to build a fully context-dependent l xicon, but havespecial computations that insure the correct models are used atword junctures.
In either case, this disallows use of phoneticnetworks as in Figure 2c.There is, however, anatural solution to these problems using aa finite-state ransduction.
We leave D as defined before, butinterpose a new transduction, C, between A and D, to convertbetween context-dependent andcontext-independent units.
Inother words, we now compute maxw (O o A o C o D o M) (w).The form of C for triphonic models is depicted in Figure 2d.For each context-dependent phone model, 7, which corre-sponds to the (context-independent) phone 7re in the context of7q and 7rr, there is a state qle in C for the biphone 7rlre, a stateqcr for 7rcTr~ and a transition from qtc to q~ with input label7 and output label 7rr.
We have constructed such a transducerand have been able to easily convert context-independent pho-netic networks into context-dependent ne works for certaintasks.
In those cases, we can implement full-context depen-dency with no special-purpose computations.4.
Chinese Text SegmentationOur second application is to text processing, namely the to-kenization of Chinese text into words, and the assignmentof pronunciations to those words.
In Chinese orthography,most characters epresent (monosyllabic) morphemes, and asin English, words may consist of one or more morphemes.Given that Chinese does not use whitespace todelimit words,it is necessary to 'reconstruct' the grouping of characters intowords.
For example, we want to say that the sentence \[\] 3~l~ ,~g~-~ "How do you say octopus in Japanese?
", con-sists of four words, namely \[\] 3~ ri4-wen2 'Japanese', ~ ,zhangl-yu2 'octopus', ~g~ zen3-mo 'how', and -~ shuol'say'.
The problem with this sentence is that \[\] ri4 is alsoa word (e.g.
a common abbreviation for Japan) as are 3~Y~ wen2-zhangl 'essay', and ~, yu2 'fish', so there is not aunique segmentation.The task of segmenting and pronouncing Chinese text is nat-urally thought of as a transduction problem.
The Chinesedictionary s is represented asa WFST D. The input alphabetis the set of Chinese characters, and the output alphabet is theunion of the set of Mandarin syllables with the set of part-of-speech labels.
A given word is represented asa sequenceof character-to-syllable transitions, terminated in an e-to-part-of-speech transition weighted by an estimate of the negativelog probability of the word.
For instance, the word ~,  'oc-topus' would be represented asthe sequence of transductions~:zhangllO.O ~:yu210.O c:noun/13.18.
A dictionary in thisform can easily be minimized using standard algorithms.An input sentence is represented asan unweighted acceptor S,with characters as transition labels.
Segmentation is then ac-complished by finding the lowest weight string in S o D*.
Theresult is a string with the words delimited by part-of-speechlabels and marked with their pronunciation.
For the exampleat hand, the best path is the correct segmentation, mappingthe input sequence \[\] 3~ c~ ~, c~ F~ c-~ ~ to the sequenceri4 wen2 noun zhangl yu2 noun zen3 mo adv shuo l verb.As is the case with English, no Chinese dictionary covers allof the words that one will encounter in Chinese text.
Forexample, many words that are derived via productive mor-phological processes are not generally to be found in the dic-tionary.
One such case in Chinese involves words derived viathe nominal plural affix r~l -men.
While some words in ~Iwill be found in the dictionary (e.g., /!
!~ tal-men 'they';~ ren2-men 'people'), many attested instances will not:for example, ~f~ jiang4-men '(military) generals', ~qingl-wal-men 'frogs'.
Given that the basic dictionary isrepresented asa finite-state automaton, it is a simple matterto augment the model just described with standard techniquesfrom finite-state morphology (\[15, 16\], inter alia).
For in-3We are currently using the 'Behavior Chinese-English Electronic Dic-tionary', Copyright Number 112366, from Behavior Design Corporation,R.O.C.
; we also wish to thank United Informaties, Inc., R.O.C.
for providingus with the Chinese text corpus that we used in estimating lexieal probabil-ities.
Finally we thank Dr. Jyun-Sheng Chang for kindly providing us withChinese personal name corpora.266stance, we can represent the fact that f \ ]  attaches to nouns byallowing e-transitions from the final states of noun entries, tothe initial state of a sub-transducer containing f \ ] .
However,for our purposes it is not sufficient merely to represent themorphological decomposition f (say) plural nouns, since wealso want to estimate the cost of the resulting words.
Forderived words that occur in our corpus we can estimate thesecosts as we would the costs for an underived ictionary en-try.
So, ~\ ]  jiang4-men '(military)generals' occurs andwe estimate its cost at 15.02; we include this word by allow-ing an e-transition between ~ and f~, with a cost chosen sothat the entire analysis o f~\ ]  ends up with a cost of 15.02.For non-occurring possible plural forms (e.g., ~/ /~f \ ]  nan2-gual-men 'pumpkins') we use the Good-Turing estimate (e.g.\[ 17\]), whereby the aggregate probability of previously unseenmembers of a construction is estimated as N1/N, where N isthe total number of observed tokens and N1 is the number oftypes observed only once; again, we arrange the automatonso that noun entries may transition to f \ ] ,  and the cost of thewhole (previously unseen) construction comes out with thevalue derived from the Good-Turing estimate.Another large class of words that are generally not to be foundin the dictionary are Chinese personal names: only famousnames like ~ j ,~  'Zhou Enlai' can reasonably be expectedto be in a dictionary, and even many of these are missing.
FullChinese personal names are formally simple, being alwaysof the form FAMILY+GIVEN.
The FAMILY name set is re-stricted: there are a few hundred single-character FAMILYnames, and about ten double-character ones.
Given namesare most commonly two characters long, occasionally one-character long: there are thus four possible name types.
Thedifficulty is that GIVEN names can consist, in principle, of anycharacter or pair of characters, o the possible GIVEN namesare limited only by the total number of characters, thoughsome characters are certainly far more likely than others.
Fora sequence of characters that is a possible name, we wish toassign a probabilityto that sequence qua name.
We use a vari-ant of an estimate proposed in \[18\].
Given a potential nameof the form F1 G1 G2, where F1 is a legal FAMILY name andG1 and G2 are Chinese characters, we estimate the probabil-ity of that name as the product of the probability of findingany name in text; the probability of F1 as a FAMILY name;the probability of the first character of a double GIVEN namebeing G1; the probability of the second character of a doubleGIVEN name being G2; and the probability of a name of theftyrm SINGLE-FAMILY+DOUBLE-GIVEN.
The first proba-bility is estimated from a count of names in a text database,whereas the last four probabilities are estimated from a largelist of personal names.
This model is easily incorporated intothe segmenter by building a transducer restricting the namesto the four licit types, with costs on the transitions for anyparticular name summing to an estimate of the cost of thatname.
This transducer is then summed with the transducerimplementing the dictionary and morphological rules, and thetransitive closure of the resulting transducer computed.References1.
M. A. Harrison, Introduction to Formal Language Theory.Reading, Massachussets: Addison-Wesley, 1978.2.
J. Berstel, Transductions and Context-Free Languages.
No.
38in LeitF~iden der angewandten Mathematik and MechanikLAMM, Stuttgart, Germany: Teubner StudienbOcher, 1979.3.
R. Teitelbaum, "Context-free error analysis by evaluation ofalgebraic power series," in Proc.
Fifth Annual A CM Symposiumon Theory of Computing, (Austin, Texas), pp.
196-199, 1973.4.
B. Lang, "A generative view of ill-formed input processing,"in ATR Symposium on Basic Research for Telephone Interpre-tation, (Kyotu, Japan), Dec. 1989.5.
A. Paz, Introduction to Probabilistic Automata.
Academic,1971.6.
T. R. Booth and R. A. Thompson, "Applying probability mea-sures to abstract languages," IEEE Trans.
Computers, vol.
C-22,pp.
442--450, May 1973.7.
S. Eilenberg, Automata, Languages, andMachines, vol.
A. SanDiego, California: Academic Press, 1974.8.
W. Kuich and A. Salomaa, Semirings, Automata, Languages.No.
5 in EATCS Monographs on Theoretical Computer Sci-ence, Berlin, Germany: Springer-Verlag, 1986.9.
J. Berstel and C. Reutenauer, Rational Series and Their Lan-guages.
No.
12 in EATCS Monographs on Theoretical Com-puter Science, Berlin, Germany: Spnnger-Verlag, 1988.10.
R. M. Kaplan and M. Kay, "Regular models of phonologicalrule systems;' Computational Linguistics, 1994.
To appear.11.
E. Roche, Analyse Syntaxique Transformationelle duFrancaispar Transducteurs et Lexique-Grammaire.
PhD thesis, Univer-sit6 Paris 7, 1993.12.
L. R. Bahl, E Jelinek, and R. Mercer, "A maximum likeli-hood approach to continuous speech recognition;' 1EEE Trans.PAMI, vol.
5, pp.
179-190, Mar.
1983.13.
A. Ljolje and M. D. Riley, "Optimal speech recognition us-ing phone recognition and lexical access;' in Proceedings ofICSLP, (Banff, Canada), pp.
313-316, Oct. 1992.14.
K.-E Lee, "Context dependentphonetic hidden Markov modelsfor continuous speech recognition," IEEE Trans.
ASSP, vol.
38,pp.
599--609, Apr.
1990.15.
K. Koskenniemi, Two-LeveI Morphology: a General Computa-tional Model for Word.Form Recognition and Production.
PhDthesis, University of Helsinki, Helsinki, 1983.16.
E. Tzoukermann a d M. Liberman, "A finite-state morpholog-ical processor for Spanish:' in COLING-90, Volume 3, pp.
3:277-286, COLING, 1990.17.
K. W. Church and W. Gale, "A comparison of the enhancedGood-Turing and deleted estimation methods for estimatingprobabilities of English bigrams," Computer Speech and Lan-guage, vol.
5, no.
1, pp.
19-54, 1991.18.
J.-S. Chang, S.-D. Chen, Y. Zheng, X.-Z.
Liu, and S.-J.Ke, "Large-corpus-based methods for Chinese personal namerecognition.
(In Chinese\]);' Journal of Chinese InformationProcessing, vol.
6, no.
3, pp.
7-15, 1992.267
