Proceedings of the 7th Workshop on Statistical Machine Translation, pages 450?459,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsPhrase Model Training for Statistical Machine Translation withWord Lattices of Preprocessing AlternativesJoern Wuebker and Hermann NeyHuman Language Technology and Pattern Recognition GroupRWTH Aachen UniversityAachen, Germany{wuebker,ney}@cs.rwth-aachen.deAbstractIn statistical machine translation, word latticesare used to represent the ambiguities in thepreprocessing of the source sentence, such asword segmentation for Chinese or morpholog-ical analysis for German.
Several approacheshave been proposed to define the probabilityof different paths through the lattice with ex-ternal tools like word segmenters, or by apply-ing indicator features.
We introduce a novellattice design, which explicitly distinguishesbetween different preprocessing alternativesfor the source sentence.
It allows us to makeuse of specific features for each preprocess-ing type and to lexicalize the choice of latticepath directly in the phrase translation model.We argue that forced alignment training canbe used to learn lattice path and phrase trans-lation model simultaneously.
On the news-commentary portion of the German?EnglishWMT 2011 task we can show moderate im-provements of up to 0.6% BLEU over a state-of-the-art baseline system.1 IntroductionThe application of statistical machine translation(SMT) to word lattice input was first introduced forthe translation of speech recognition output.
Ratherthan translating the single-best transcription, thespeech recognition system encodes all possible tran-scriptions and their probabilities within a word lat-tice, which is then used as input for the machinetranslation system (Ney, 1999; Matusov et al, 2005;Bertoldi et al, 2007).Since then, several groups have adapted this ap-proach to model ambiguities in representing thesource language with lattices and were able to re-port improvements over their respective baselines.The probabilities for different paths through the lat-tice are usually modeled by assigning probabilitiesto arcs as a byproduct of the lattice generation orby defining binary indicator features.
Applying thefirst method only makes sense if the lattice construc-tion is based on a single, comprehensive probabilis-tic method, like a Chinese word segmentation modelas is used by Xu et al (2005).
In applications likethe one described by Dyer et al (2008), where sev-eral different segmenters for Chinese are combinedto create the lattice, this is not possible.
Also, ourintuition suggests that simply defining indicator fea-tures for each of the segmenters may not be ideal, ifwe assume that there is not a single best segmenter,but rather that for different data instances a differentone works best.In this paper, we propose to model the latticepath implicitly within the phrase translation model.We introduce a novel lattice design, which explic-itly distinguishes between different ways of prepro-cessing the source sentence.
It enables us to definespecific binary features for each preprocessing typeand to learn lexicalized lattice path probabilities andthe phrase translation model simultaneously with aforced alignment training procedure.To train the phrase translation model, most state-of-the-art SMT systems rely on heuristic phrase ex-traction from a word-aligned training corpus.
Us-ing a modified version of the translation decoder to450force-align the training data provides a more consis-tent way of training.
Wuebker et al (2010) intro-duce a leave-one-out method which can overcomethe over-fitting effects inherent to this training pro-cedure (DeNero et al, 2006).
The authors report thisto yield both a significantly smaller phrase table andhigher translation quality than the heuristic phraseextraction.We argue that applying forced alignment train-ing helps to exploit the full potential of word latticetranslation.
The effects of the training on lattice in-put are analyzed on the news-commentary portion ofthe German?English WMT 2011 task.
Our resultsshow moderate improvements of up to 0.6% BLEUover the baseline.This paper is organized as follows: We will re-view related work in Section 2, describe the decoderin Section 3 and present our novel lattice design inSection 4.
The phrase training algorithm is intro-duced in Section 5, and Section 6 gives a detailedaccount of the experimental setup and discusses theresults.
Finally, our findings are summarized in Sec-tion 7.2 Related workWord lattices have been used for machine transla-tion of text in a variety of ways.
Dyer et al (2008)use it to encode different Chinese word segmenta-tions or Arabic morphological analyses.
For thephrase-based model, they report improvements ofup to 0.9% BLEU for Chinese?English and 1.6%BLEU for Arabic?English over the respective sin-gle best word segmented and morphologically ana-lyzed source.
These results are achieved without anexplicit way of modeling probabilities for differentpaths within the lattice.
The training of the phrasemodel is done by generating one version of the train-ing data for each segmentation method or morpho-logical analysis.
The word alignments are trainedseparately, and are then concatenated for phrase ex-traction.
Our work differs from (Dyer et al, 2008) inthat we explicitly distinguish the various preprocess-ing types in the lattice so that we can define specificpath features and lexicalize the lattice path probabil-ities within the phrase model.In (Xu et al, 2005) the probability of a segmen-tation, as given by the Chinese word segmentationmodel, and the translation model are combined intoa global decision rule.
This is done by weightingthe lattice edges with a source language model.
Theauthors report an improvement of 1.5% BLEU overtranslation of the single best segmentation with aphrase-based SMT system.Dyer (2009) introduces a maximum entropymodel for compound word splitting, which heuses to create word lattices for translation in-put.
He shows improvements in German-English,Hungarian-English and Turkish-English over state-of-the-art baselines.For the German?English WMT 2010 task, Hard-meier et al (2010) encode the morphological re-duction and decompounding of the German surfaceform as alternative paths in a word lattice.
Theyshow improvements of roughly 0.5% BLEU over thebaseline.
A binary indicator feature is added to thelog-linear framework for the alternative edges.
Ad-ditionally, they integrate long-range reorderings ofthe source sentence into the lattice, in order to matchthe word order of the English language, which yieldsanother improvement of up to 0.5% BLEU.Niehues and Kolss (2009) also use lattices to en-code different alternative reorderings of the sourcesentence which results in an improvement of2.0% BLEU over the baseline on the WMT 2008German?English task.Onishi et al (2010) propose a method of modelingparaphrases in a lattice.
They perform experimentson the English?Japanese and English?ChineseIWSLT 2007 tasks, and report improvements of1.1% and 0.9% BLEU over a paraphrase-augmentedbaseline.Schroeder et al (2009) generalize usage of latticesto combine input from multiple source languages.Factored translation models (Koehn and Hoang,2007) approach the idea of integrating annotationinto translation from the opposite direction.
Wherelattices allow the decoder to choose a single level ofannotation as translation source, factored models aredesigned to jointly translate several annotation lev-els (factors).
Thus, they are more suited to integratelow-level annotation that by itself does not providesufficient information for accurate translation, like451part-of-speech tags, gender, etc.
On the other hand,they require a one-to-one correspondence betweenthe factors, which makes them unsuitable to modelword segmentation or decompounding.The problem of performing real training for thephrase translation model has been approached in anumber of different ways in the past.
The first one,to the best of our knowledge, was the joint proba-bility phrase model presented by Marcu and Wong(2002).
It is shown to perform slightly inferior tothe standard heuristic phrase extraction from wordalignments by Koehn et al (2003).A detailed analysis of the inherent over-fittingproblems when training a generative phrase modelwith the EM algorithm is given in (DeNero et al,2006).
These findings are in principle confirmed byMoore and Quirk (2007) who, however, can showthat their model is less sensitive to reducing compu-tational resources than the state-of-the-art heuristic.Birch et al (2006) and DeNero et al (2008)present alternative training procedures for the jointmodel introduced by Marcu and Wong (2002),which are shown to improve its performance.In (Mylonakis and Sima?an, 2008) a phrase modelis described, whose training procedure is designedto counteract the inherent over-fitting problem by in-cluding prior probabilities based on Inversion Trans-duction Grammar and smoothing as learning objec-tive.
It yields a small improvement over a standardphrase-based baseline.Ferrer and Juan (2009) present an approach,where the phrase model is trained by a semi-hiddenMarkov model.In this work we apply the phrase training methodintroduced by Wuebker et al (2010), where thephrase translation model of a fully competitive SMTsystem is trained in a generative way.
The key toavoiding the over-fitting effects described by DeN-ero et al (2006) is their novel leave-one-out proce-dure.3 Decoding3.1 Phrase-based translationWe use a standard phrase-based decoder whichsearches for the best translation e?I?1 for a given inputsentence fJ1 by maximizing the posterior probabilitye?I?1 = argmaxI,eI1Pr(eI1|fJ1 ).
(1)Generalizing the noisy channel approach (Brownet al, 1990) and making use of the maximum ap-proximation (Viterbi), the decoder directly mod-els the posterior probability by a log-linear combi-nation of several feature functions hm(eI1, sK1 , fJ1 )weighted with scaling factors ?m, which results inthe decision rule (Och and Ney, 2004)e?I?1 = argmaxI,eI1,K,sK1{M?m=1?mhm(eI1, sK1 , fJ1 )}.
(2)Here, sK1 denotes the segmentation of eI1 and fJ1into K phrase-pairs and their alignment.
The fea-tures used are the language model, phrase translationand lexical smoothing models in both directions,word and phrase penalty and a simple distance-based reordering penalty.3.2 Lattice translationFor lattice input we generalize Equation 2 to alsomaximize over the set of sentences F(L) encodedby a given source word lattice L:e?I?1 =argmaxI,eI1,K,sK1 ,fJ1 ?F(L){M?m=1?mhm(eI1, sK1 , fJ1 )}(3)Note that in this formulation there are no prob-abilities assigned to the arcs of L. We define ad-ditional binary indicator features hm and lexical-ize path probabilities by encoding the path into theword identities.
To translate lattice input, we adaptthe standard phrase-based decoding algorithm as de-scribed in (Matusov et al, 2008).
The decoder keepstrack of the covered slots, which represent the topo-logical order of the nodes, rather than the coveredwords.
When expanding a hypothesis, it has to beverified that there is no overlap between the coverednodes and that a path exists from start to goal node,452Pakistans Streitkr?fte - wiederholt Ziel von Selbstmordattent?tern - sind demoralisiert .Pakistan Streit KraftStreit Kr?fteselbst Mord AttentatSelbst Mord Attent?ternsein demoralisierenLemmaSurfaceCompound0 1223 4 5 6 78 98 910 11 12 13 14Pakistans Streitkr?fte - wiederholt Ziel von Selbstmordattent?tern - sind demoralisiert .Pakistan Streit KraftStreit Kr?fteselbst Mord AttentatSelbst Mord Attent?ternsein demoralisierenLemmaSurfaceCompound0 1223 4 5 6 78 98 910 11 12 13 14Pakistans-wiederholt Ziel von-sind demoralisiert.- - .wiederholt Ziel vonFigure 1: Top: Slim lattice.
Bottom: Full lattice.
The sentence is taken from the training data.
The three layersSurface, Compound and Lemma are separated with dashed lines.
Nodes are labeled with slot information.
Slots areordered horizontally, layers vertically.which passes through all covered nodes.
In prac-tice, when considering a possible expansion cover-ing slots j?, ..., j??
with start and end states n?
andn?
?, we make sure that the following two conditionshold:?
n?
is reachable from the lattice node that cor-responds to the nearest already covered slot tothe left of j?.?
The node that corresponds to the nearest al-ready covered slot to the right of j??
is reachablefrom n?
?.It was noted by Dyer et al (2008) that the stan-dard distance-based reordering model needs to beredefined for lattice input.
We define the distortionpenalty as the difference in slot number.
Using theshortest path within the lattice is reported to havebetter performance in (Dyer et al, 2008), howeverwe did not implement it due to time constraints.4 Lattice designWe construct lattices from three different prepro-cessing variants of the German source side of thedata.
The surface form is the standard tokenizationof the source sentence.
The word compounds areproduced by the frequency-based compound split-ting method described in (Koehn and Knight, 2003),applied to the tokenized sentence.
From the com-pound split sentence we produce the lemma of theGerman words by applying the TreeTagger toolkit(Schmid, 1995).
Each of the different preprocess-ing variants is assigned a separate layer within thelattice.
For the phrase model, word identities are de-fined by both the word and its layer.
In this way, thephrase model can assign different scores to phrasesin different layers, allowing it to guide the search to-wards a specific layer for each word.
In practice, thisis done by annotating words with a unique identifierfor each layer.
For example, the word sein from thelemmatized layer will be written as LEM.sein withinboth the data and the phrase table.
If sein appears inthe surface form layer, it will be written as SUR.seinand is treated as a different word.
SUR is the identi-fier for the compound layer.We experiment with two different lattice designs.In the full lattice, all three layers are included foreach source word in surface form.
The slim latticeonly includes arcs for the lemma layer if it differsfrom the surface form, and only includes arcs for thecompound layer if it differs from both surface formand lemma.
Figure 1 shows a slim and a full latticefor the same training data sentence.For each layer, we add two indicator features tothe phrase table: One binary feature which is setto 1 if the phrase is taken from this layer, and onefeature which is equal to the number of words fromthis layer.
This results in six additional feature func-tions, whose weights are optimized jointly with thestandard features described in Section 3.1.
We will453denote them as layer features.5 Phrase translation model trainingTo train the phrase model, we use a modified versionof the translation decoder to force-align the trainingdata.
We apply the method described in (Wuebker etal., 2010), but with word lattices on the source side.To avoid over-fitting, we use their cross-validationtechnique, which is described as a low-cost alterna-tive to leave-one-out.
For cross-validation we seg-ment the training data into batches containing 5000sentences.
For each batch, the phrase table is up-dated by reducing the phrase counts by the localcounts produced by the current batch in the previ-ous training iteration.
For the first iteration, we per-form the standard phrase extraction separately foreach batch to produce the local counts.
Singletonphrases are assigned the probability ?(|f?
|+|e?|) withthe source and target phrase lengths |f?
| and |e?| andfixed ?
= e?5 (length-based leave-one-out).
Sen-tences for which the decoder is not able to find analignment are discarded (about 4% for our experi-ments).
To estimate the probabilities of the phrasemodel, we count all phrase pairs used in trainingwithin an n-best list (equally weighted).
The trans-lation probability for a phrase pair (f?
, e?)
is estimatedaspFA(e?|f?)
=CFA(f?
, e?)Cmon(f?
), (4)where CFA(f?
, e?)
is the count of the phrase pair(f?
, e?)
in the force-aligned training data.
In order tolearn the lattice path along with the phrase transla-tion probabilities, we make the following modifica-tion to the original formulation in (Wuebker et al,2010).
The denominator Cmon(f?)
is the count off?
in the target side of the training data, rather thanusing the real marginal counts.
This means that itis independent of the training procedure, and can becomputed by ignoring one side of the training dataand performing a simple n-gram count on the other.In this way the model learns to prefer lattice pathswhich are taken more often in training.
For exam-ple, if the phrase (LEM.Streit LEM.Kraft) is usedto align the sentence from Figure 1, Cmon(f?)
willbe increased for f?
= (SUR.Streitkr?fte) and f?
=(SPL.Streit SPL.Kr?fte) without affecting their jointcounts.
This leads to a lower probability for thesephrases, which is not the case if marginal countsare used.
Note that on the source side we have onetraining corpus for each lattice layer, which are con-catenated to compute Cmon(f?).
The size of the n-best lists used in this work is fixed to 20000.
Usingsmaller n-best lists was tested, but seems to have dis-advantages for the application to lattices.
After re-estimation of the phrase model, the feature weightsare optimized again.In order to achieve a good coverage of the train-ing data, we allow the decoder to generate backoffphrases.
If a source phrase consisting of a singleword does not have any translation candidates leftafter the bilingual phrase matching, one phrase pairis added to the translation candidates for each wordin the target sentence.
The backoff phrases are as-signed a fixed probability ?
= e?12.
Note that thisis smaller than the probability the phrase would beassigned according to the length-based leave-one-out heuristic, leading to a preference of singletonphrases over backoff phrases.
The lexical smooth-ing models are applied in the usual way to both sin-gleton and backoff phrases.
After each sentence, thebackoff phrases are discarded.
However, in the ex-periments for this work, introducing backoff phrasesonly increases the coverage from 95.8% to 96.2% ofthe sentences.6 Experimental evaluation6.1 Experimental setupOur experiments are carried out on the news-commentary portion of the German?English dataprovided for the EMNLP 2011 Sixth Workshopon Statistical Machine Translation (WMT 2011).
?We use newstest2008 as development set andnewstest2009 and newstest2010 as unseentest sets.
The word alignments are produced withGIZA++ (Och and Ney, 2003).
To optimize the log-linear parameters, the Downhill-Simplex algorithm(Nelder and Mead, 1965) is applied with BLEU (Pa-pineni et al, 2002) as optimization criterion.
The?http://www.statmt.org/wmt11454German EnglishSurface Compound LemmaTrain Sentences 136KRunning Words 3.4M 3.5M 3.3MVocabulary Size 118K 81K 52K 57Knewstest2008 Sentences 2051Running Words 48K 50K 50KVocabulary Size 10.3K 9.7K 7.3K 8.1KOOVs (Running Words) 3041 2092 1742 2070newstest2009 Sentences 2525Running Words 63K 66K 66KVocabulary Size 12.2K 11.4K 8.4K 9.4KOOVs (Running Words) 4058 2885 2400 2729newstest2010 Sentences 2489Running Words 62K 65K 62KVocabulary Size 12.3K 11.4K 8.5K 9.2KOOVs (Running Words) 4357 2952 2565 2742Table 1: Corpus Statistics for the WMT 2011 news-commentary data, the development set (newstest2008) andthe two test sets (newstest2009, newstest2010).
For the source side, three different preprocessing alternativesare included: Surface, Compound and Lemma.language model is a standard 4-gram LM with mod-ified Kneser-Ney smoothing (Chen and Goodman,1998) produced with the SRILM toolkit (Stolcke,2002).
It is trained on the full bilingual data andparts of the monolingual News crawl corpus pro-vided for WMT 2011.
Numbers are replaced witha single category symbol in a separate preprocess-ing step and we apply the long-range part-of-speechbased reordering rules proposed by (Popovic?
andNey, 2006).Table 1 shows statistics for the bilingual trainingdata and the development and test corpora for thethree different German preprocessing alternatives.It can be seen that both compound splitting andlemmatization reduce the vocabulary size and num-ber of out-of-vocabulary (OOV) words.
Results aremeasured in BLEU and TER (Snover et al, 2006),which are computed case-insensitively with a singlereference.6.2 Baseline experimentsTo get an overview over the effects of the differentpreprocessing alternatives for the German source,we built three baseline systems, one for each prepro-cessing type.
The phrase tables are extracted heuris-tically in the standard way from the word-alignedtraining data.
Additionally, we performed phrasetraining for the compound split version of the data.The results are shown in Table 2.
When movingfrom the Surface to the Compound layer, we observeimprovements of up to 1.0% in BLEU and 1.1% inTER.
Reducing the morphological richness further(Lemma) leads to a clear performance drop.
Appli-cation of phrase training on the compound split datayields a small degradation in TER on all data sets andin BLEU on newstest2010.
We assume that thisis due to the small size of the training data and itsheterogeneity, which makes it hard for the decoderto find good phrase alignments.6.3 Lattice experiments: Heuristic extractionWe generated both slim and full lattices for all datasets.
Similar to (Dyer et al, 2008), we concate-nate the three training data sets and their word align-ments to extract the phrases.
Note that this only pro-duces single-layer phrases.
It can be seen in Table2 that without the application of layer features theslim lattice slightly outperforms the full lattice.
In-455newstest2008 newstest2009 newstest2010BLEU[%]TER[%]BLEU[%]TER[%]BLEU[%]TER[%]Baseline Surface 19.5 64.6 18.6 64.4 20.6 62.8Compounds 20.5 63.5 19.1 63.5 21.1 61.9FA Compounds 20.5 63.9 19.1 63.8 20.9 62.3Lemma 19.2 65.4 18.2 65.2 19.9 63.9Slim Lattice without layer feat.
19.9 64.4 18.9 64.1 20.8 62.6(heuristic) with layer feat.
20.5 63.8 19.4 63.9 21.0 62.4Full Lattice without layer feat.
19.8 64.6 18.7 64.2 20.6 62.8(heuristic) with layer feat.
20.4 64.0 19.5 63.8 21.3 62.3Full Lattice without layer feat.
20.0 64.3 19.3 64.1 20.8 62.6(FA w/o layer feat.)
with layer feat.
20.2 64.3 19.1 64.2 20.7 62.8Full Lattice without layer feat.
20.5 63.7 19.5 63.6 21.3 62.1(FA w/ layer feat.)
with layer feat.
20.7 63.6 19.7 63.4 21.4 61.8Table 2: Results on the German-English WMT 2011 data.
Scores are computed case-insensitively for BLEU [%]and TER [%].
We evaluate performance of the baseline systems, one for each of the three different encodings, withboth slim and full lattices using heuristic phrase extraction and with full lattices using forced alignment phrase modeltraining (FA).
All lattice systems are evaluated with and without layer features.
The best scores in each column are inboldface, statistically significant improvement over the Compounds baseline is marked with blue color.troducing layer features boosts the performance forboth lattice types.
However, the performance in-crease is considerably larger for the full lattice sys-tems, which now outperform the slim lattice systemson newstest2009 and newstest2010.
Com-pared to the Compounds baseline, the full latticesystem with layer features shows a small improve-ment of up to 0.4% BLEU on newstest2009 andnewstest2010, but a degradation in TER.6.4 Lattice experiments: Phrase trainingThe experiments on phrase training are setup as fol-lows.
The phrase table is initialized with the stan-dard extraction and is identical to the one used forthe experiments in Section 6.3.
The log-linear scal-ing factors used in training are the optimized param-eters on the corresponding lattice, also taken fromthe experiments described in Section 6.3.
The forcedalignment procedure was run for one iteration.
Fur-ther iterations were tested, but did not give any im-provements.The phrase training was performed on the full lat-tice design.
The reason for this is that we want thesystem to learn all possible phrases.
Even if there isno difference in wording between the layers in train-ing, the additional phrases could be useful for un-seen test data.
The training was performed both withand without layer features.
The resulting systemswere also optimized with and without layer features,resulting in four different setups.From the results in Table 2 it is clear that phrasetraining without layer features does not have thedesired effect.
Even if we apply layer features tothe system trained without them, we do not reachthe performance of the best standard lattice system.We conclude that, without these indicator features,the standard lattice system does not produce goodphrase alignments.When the layer features are applied for both train-ing and translation, we observe improvements of upto 0.2% in BLEU and 0.5% in TER over the corre-sponding standard lattice system.
The gap betweenthe systems with and without layer features is muchsmaller than for the heuristically trained lattices.This indicates that our goal of encoding the best lat-tice path directly in the phrase model was at leastpartially achieved.
However, in order to exceed theperformance of our state-of-the-art baseline on bothmeasures, the layer features are still needed withinthe phrase training procedure and for translation.
Al-456source Das Warten hat gedauert mehr als NUM Minuten, was im Fall einer Stra?e, wowerden erwartet NUM Menschen, ist unverst?ndlich.reference The wait lasted more than NUM minutes, something incomprehensible for a racewhere you expect more than NUM people.lattice (heuristic) The wait has taken more than NUM minutes, which in the case of a street, whereNUM people are expected to be, can?t understand it.lattice (FA) The wait has taken more than NUM minutes, which in the case of a street, whereexpected NUM people, is incomprehensible.Figure 2: Example sentence from the newstest2009 data set.
The faulty phrase in the heuristic lattice translationis marked in boldface.together, our phrase trained lattice approach outper-forms the state-of-the-art baseline on all three datasets by up to 0.6% BLEU.
On newstest2009,this result is statistically significant with 95% confi-dence according to the bootstrap resampling methoddescribed by Koehn (2004).For a direct comparison between the heuristic andphrase-trained full lattice systems, we manually in-spected the optimized log-linear parameter valuesfor the layer features.
We observe that for the stan-dard lattices, paths through the lemmatized layer areheavily penalized.
In the phrase trained lattice setup,the penalty is much smaller.
As a result, the num-ber of words from the Lemma layer used for transla-tion of the newstest2009 data set is increased by49% from 1828 to 2715 words.
However, a manualinspection of the translations reveals that the mainimprovement seems to come from a better choiceof phrases from the Compound layer.
More specif-ically, the used phrases tend to be shorter ?
the av-erage phrase length of Compound layer phrases is1.5 words for both the baseline and the heuristic lat-tice system.
In the phrase trained lattice system, itis 1.3 words.
An example is given in Figure 2.
Wefocus on the end of the sentence, where the heuris-tic system uses the rather disfluent phrase (ist unver-st?ndlich.
# can?t understand it.
), whereas the forcedalignment trained system applies the three phrases(ist # is), (unverst?ndlich # incomprehensible) and(.
# .
).This effect can be explained by the leave-one-outprocedure.
As lemmatized phrases usually map toseveral phrases in the other layers, their count is gen-erally higher.
Application of leave-one-out, whichreduces the counts of all phrases extracted from thecurrent sentence by a fixed value, therefore has astronger penalizing effect on Surface and Compoundlayer phrases.
In the extreme case, phrases which aresingletons in the Compound layer are unlikely to beused at all in training, if the corresponding phrasein the Lemma layer has a higher count.
While thisrarely leads to the competing lemmatized phrasesbeing used in free translation, it allows for shorter,more general phrases from the more expressive lay-ers to be applied.
Indeed, the ?bad?
phrase (ist unver-st?ndlich.
# can?t understand it.)
from the examplein Figure 2 is a singleton.7 Conclusion and future workIn this work we apply a forced alignment phrasetraining technique to input word lattices in SMT forthe first time.
The goal of encoding better latticepath probabilities directly into the phrase model wasat least partially successful.
The proposed methodoutperforms our baseline by up to 0.6% BLEU.
Toachieve this, we presented a novel lattice design,which distinguishes between different layers, forwhich we can define separate indicator features.
Al-though these layer features are still necessary for thefinal system to improve over state-of-the-art perfor-mance, they are less important than in the heuristi-cally trained setup.One advantage of our approach is its adaptabilityto a variety of scenarios.
In future work, we planto apply it to additional language pairs.
Arabic andChinese on the source side, where the layers couldrepresent different word segmentations, seem a nat-ural choice.
We also hope to be able to leveragelarger training data sets.
As a natural extension weplan to allow learning of cross-layer phrases.
Fur-457ther, applying this framework to lattices modelingdifferent reorderings could be an interesting direc-tion.AcknowledgmentsThis work was partially realized as part of theQuaero Programme, funded by OSEO, French Stateagency for innovation, and also partially funded bythe European Union under the FP7 project T4MENet, Contract No.
249119.ReferencesN.
Bertoldi, R. Zens, and M. Federico.
2007.
Speechtranslation by confusion network decoding.
In Pro-ceedings of ICASSP 2007, pages 1297?1300, Hon-olulu, Hawaii, April.Alexandra Birch, Chris Callison-Burch, Miles Osborne,and Philipp Koehn.
2006.
Constraining the phrase-based, joint probability statistical translation model.
InProceedings of the Workshop on Statistical MachineTranslation, pages 154?157, Jun.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16:79?85, June.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Computer ScienceGroup, Harvard University, Aug.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why Generative Phrase Models UnderperformSurface Heuristics.
In Proceedings of the Workshopon Statistical Machine Translation, pages 31?38, NewYork City, June.John DeNero, Alexandre Buchard-C?t?, and Dan Klein.2008.
Sampling Alignment Structure under a BayesianTranslation Model.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 314?323, Honolulu, October.C.
Dyer, S. Muresan, and P. Resnik.
2008.
Generaliz-ing word lattice translation.
In Proceedings of AnnualMeeting of the Association for Computational Linguis-tics, pages 1012?1020, Columbus, Ohio, June.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for mt.
In Proceedingsof the 2009 Annual Conference of the North Ameri-can Chapter of the ACL, pages 406?414, Boulder, Col-orado, June.Jes?s-Andr?s Ferrer and Alfons Juan.
2009.
A phrase-based hidden semi-markov approach to machine trans-lation.
In Proceedings of European Association forMachine Translation (EAMT), Barcelona, Spain, May.European Association for Machine Translation.C.
Hardmeier, A. Bisazza, and M. Federico.
2010.
FBKat WMT 2010: Word Lattices for Morphological Re-duction and Chunk-based Reordering.
In Proceedingsof the Joint 5th Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 88?92, Uppsala, Swe-den, July.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 868?876.P.
Koehn and K. Knight.
2003.
Empirical Methodsfor Compound Splitting.
In Proceedings of EuropeanChapter of the ACL (EACL 2009), pages 187?194.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology - Volume 1, pages48?54, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
pages 388?395,Barcelona, Spain, July.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2002), July.E.
Matusov, H. Ney, and R. Schl?ter.
2005.
Phrase-based translation of speech recognizer word lattices us-ing loglinear model combination.
In Proceedings ofthe IEEE Automatic Speech Recognition and Under-standing Workshop (ASRU), pages 110?115, San Juan,Puerto Rico.Evgeny Matusov, Bj?rn Hoffmeister, and Hermann Ney.2008.
ASR Word Lattice Translation with ExhaustiveReordering is Possible.
In Interspeech, pages 2342?2345, Brisbane, Australia, September.Robert C. Moore and Chris Quirk.
2007.
An iteratively-trained segmentation-free phrase translation model forstatistical machine translation.
In Proceedings of theSecond Workshop on Statistical Machine Translation,pages 112?119, Prague, June.Markos Mylonakis and Khalil Sima?an.
2008.
PhraseTranslation Probabilities with ITG Priors and Smooth-ing as Learning Objective.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 630?639, Honolulu, October.458J.A.
Nelder and R. Mead.
1965.
The Downhill SimplexMethod.
Computer Journal, 7:308.H.
Ney.
1999.
Speech translation: Coupling of recog-nition and translation.
In Proceedings of IEEE Inter-national Conference on Acoustics, Speech, and SignalProcessing (ICASSP), pages 517?520, Phoenix, Ari-zona, USA, March.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 206?214, Athens, Greece,March.F.J.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449, De-cember.T.
Onishi, M. Utiyama, and E. Sumita.
2010.
Paraphraselattice for statistical machine translation.
In Proceed-ings of the ACL 2010 Conference Short Papers, pages1?5, Uppsala, Sweden, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thAnnual Meeting on Association for Computational Lin-guistics, pages 311?318, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.M.
Popovic?
and H. Ney.
2006.
POS-based Word Re-orderings for Statistical Machine Translation.
In Inter-national Conference on Language Resources and Eval-uation, pages 1278?1283.H.
Schmid.
1995.
Improvements in Part-of-Speech Tag-ging with an Application to German.
In Proceedingsof the ACL SIGDAT-Workshop, pages 47?50, Dublin,Ireland, March.Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009.Word lattices for multi-source translation.
In Proceed-ings of the 12th Conference of the European Chapterof the ACL, pages 719?727, Athens, Greece.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProc.
of AMTA, pages 223?231, Aug.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Spoken LanguageProcessing, volume 2, pages 901 ?
904, Denver, Col-orado, USA, September.Joern Wuebker, Arne Mauser, and Hermann Ney.
2010.Training phrase translation models with leaving-one-out.
In Proceedings of the 48th Annual Meeting of theAssoc.
for Computational Linguistics, pages 475?484,Uppsala, Sweden, July.Jia Xu, Evgeny Matusov, Richard Zens, and HermannNey.
2005.
Integrated chinese word segmentation instatistical machine translation.
In International Work-shop on Spoken Language Translation, pages 141?147,Pittsburgh, PA, USA, October.459
