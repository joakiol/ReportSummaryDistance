Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 132?136,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsIdentifying Sources of Inter-Annotator Variation:Evaluating Two Models of Argument AnalysisBarbara WhiteThe University of Western OntarioLondon, ON, Canada N6A 3K7bwhite6@uwo.caAbstractThis paper reports on a pilot study where twoModels of argument were applied to the Dis-cussion sections of a corpus of biomedical re-search articles.
The goal was to identifysources of systematic inter-annotator variationas diagnostics for improving the Models.
Inaddition to showing a need to revise bothModels, the results identified problems result-ing from limitations in annotator expertise.
Infuture work two types of annotators are re-quired: those with biomedical domain exper-tise and those with an understanding of rhe-torical structure.1 IntroductionGiven the vast and growing body of biomedicalresearch literature being published there is a needto develop automated text mining tools that willassist in filtering out the information most usefulto researchers.
Previous studies applying Argu-mentative Zoning (AZ) (Teufel et al 1999) andZone Analysis (ZA) (Mizuta et al 2005) haveshown that an analysis of the argumentativestructure of a text can be of use in InformationExtraction (IE).
As an alternative approach, itwas believed that Toulmin?s work on informallogic and argument structure (1958/2003) couldreflect the rhetorical strategies used by the au-thors of biomedical research articles.In order to compare and evaluate these ap-proaches two Models of argument were appliedto the same set of biomedical research articles.Inter-annotator agreement/disagreement betweenand within Models was examined.
Given thathuman-annotated data are ultimately to be usedfor machine learning purposes, there is growingrecognition of the need to analyze coder dis-agreements in order to differentiate between sys-tematic variation and noise (e.g.
Reidsma andCarletta 2008).
The goal of this study was toidentify systematic disagreements as diagnosticsfor improving the Models of argument.2 Annotation ProjectThe two Models of rhetoric (argument) in Tables1 and 2 were applied to a corpus of 12 articlesdownloaded at random from the BMC-series(BioMed Central) of journals.
The corpus cov-ered nine different domains, with a total of 400sentences; the three annotators worked inde-pendently.
Although the entire articles were readby the annotators, only the sentences in the Dis-cussion section were argumentatively catego-rized.
The annotators were the study coordinator(B, a PhD student in Computational Linguisticsand current author) and two fourth year under-graduate students from the Bachelor of MedicalSciences program at The University of WesternOntario (J and K).Coders annotated one article at a time, apply-ing each of the two Models; no sentence was al-lowed to be left unannotated.
In cases where anannotator was conflicted between categoriesguidelines for ?trumping?
were provided with theModels.
(For details on the Models, trumpingsystems, instructions to annotators, corpus dataand a sample annotated article please seewww.csd.uwo.ca/~mercer/White_Thesis09.pdf.
)The first model (Model 1) of argumentation tobe applied stems from work in AZ and ZA andwas adapted by White.
It focuses on the contentof a text, essentially differentiating ?new?
from?old?
information, and results from analysis (Ta-ble 1).
The second model is based on the con-cepts and language of Toulmin (1958/2003).
Jen-icek applied Toulmin to create a guide for writ-ing medical research articles (2006) and Graves(personal communications 2008, 2009) furtheradapted these ideas to work with our corpus(Model 2).
Its main focus is to identify ?Claims?being made by the authors, but it also differenti-ates between internal and external evidence, as132well as categories of explanation and implication(Table 2).Category SpecificationsCONTEXT (1) Background, accepted facts, previous work, motivationMETHOD (2) Methods, tools, processes, experimental designCURRENTRESULTS (3)Findings of currentexperimentRESULTSCOMPARED (4)Current results support orcontradict previous workANALYSIS (5)Possible interpretations orimplications of current orprevious results, significanceor limitations of their studyTable 1: Model 1 categories (White 2009)Category SpecificationsEXTRANEOUS (0)Statements extraneous toauthors?
argumentation,not related to a CLAIMCLAIM (1) Proposition put forward  based on analysis of resultsGROUNDS (2) Internal evidence from current studyWARRANT/BACKING (3)Understanding of theproblem, or data, fromother studiesQUALIFIER (4)Possible explanations forresults, comparisons withexternal evidencePROBLEM INCONTEXT (5)Implications for the field,future research directionsTable 2: Model 2 categories (Toulmin 1958,Jenicek 2006, Graves 2009)2.1   ResultsData were compiled on individual annotator?sargument category choices for each of the 400sentences, for each Model of rhetoric.
This al-lowed comparisons to be made between the twoModels, within Model by category, and betweenannotators.
Although the coders had differentbackgrounds, they were treated as equals i.e.there was no ?expert?
who served as a bench-mark.
There were three possible types of inter-annotator agreement: we all agreed on a choiceof category, we all differed, or two annotatorsagreed and the third disagreed.
This latter groupof two-way agreement (also implying two-wayvariation) was broken down into its three possi-bilities: J and K agreed, and differed from B(JK~B), J and B agreed, and differed from K(JB~K), or B and K agreed, and differed from J(BK~J) (Table 3).Model 1 Model 2All agree 242 60.50% 157 39.25%All disagree 15 3.75% 33 8.25%JK~B 32 8.00% 71 17.75%JB~K 42 10.50% 68 17.00%BK~J 69 17.25% 71 17.75%Total 400 100% 400 100%Table 3 Number of sentences in agreementgroupsThe overall (three-way) inter-annotator agree-ment was higher for Model 1 at 60.5%, withModel 2 at 39.25%.
All annotators were less fa-miliar with Model 2 than Model 1, and the for-mer had one more category, thus there was moreopportunity to disagree.
Although there is noguarantee that three-way agreement implies wewere all ?right?, it does suggest a shared under-standing of what the Model categories describe.On the other hand, there were instances of sen-tences under both Models where three differentcategories had been chosen but they could allseem to legitimately apply.
In addition, in sen-tences which are argumentatively and/or gram-matically complex, where one is forced to chooseonly one categorization, it is often difficult todecide which is the most appropriate.Given the difference in academic backgroundof the annotators, one hypothesis had been that Jand K would be more likely to agree with eachother and differ from B, the coder who was notknowledgeable in the biomedical sciences.
Ascan be seen in Table 3, however, this did not turnout to be the case.3    Sources of Inter-Annotator VariationIt was crucial to examine inter-annotator dis-agreements within each Model in order to deter-mine the categories that were particular sourcesof variation.
As a reference point for this, and forlooking at individual annotator preferences, Ipresent in Tables 4 and 5 the overall distributionof argument categories within Model.
These arecalculated on the basis of all 1200 annotationtokens (400 sentences * 3 annotators) across thecorpus.1333.1    Model 1Category Tokens PercentCONTEXT (1) 337 28.0%METHOD (2) 128 10.7%CURRENTRESULTS (3) 189 15.8%RESULTSCOMPARED (4) 114 9.5%ANALYSIS (5) 432 36.0%Total 1200 100%Table 4 Overall distribution by category ?Model 1The CONTEXT category was developed in orderto filter out background (?old?)
material.
Al-though this seemed straightforward, the resultsshowed that CONTEXT was the largest sourceof inter-annotator variation under Model 1: of the158 sentences that had some degree of inter-annotator variation, almost two-thirds (100) in-volved some variation between CONTEXT andanother category.
The primary reason for thiswas that frequently sentences in our corpus thatincluded category (1) material also included ma-terial suited to other categories (typicallyANALYSIS or RESULTS COMPARED) i.e.they were complex sentences.
There was alsointer-annotator disagreement between CUR-RENT RESULTS (3) and RESULTS COM-PARED (4); this was to be expected given thepotential overlap of content when discussing theauthors?
current study, especially in complexsentences.3.2    Model 2Category Tokens PercentEXTRANEOUS (0) 250 20.8%CLAIM (1) 185 15.4%GROUNDS (2) 218 18.2%WARRANT/BACKING (3) 215 18.0%QUALIFIER (4) 256 21.3%PROBLEM INCONTEXT (5) 76 6.3%Total 1200 100%Table 5 Overall distribution by category ?Model 2The EXTRANEOUS category had been devel-oped for sentences of a ?background?
nature,which did not fit into the Toulmin argumentstructure i.e.
they did not seem to relate directlyto any CLAIM.
Of the 243 sentences with somedegree of inter-annotator variation under Model2, 101 involved the EXTRANEOUS category.This variation a) showed that there were prob-lems in understanding argument structure, and b)reflected the differences in annotator preferences(Table 7).Model 2 is crucially a CLAIMS-based sys-tem, so variation between CLAIMS and othercategories is particularly significant, especiallysince it is assumed that this might be the cate-gory of greatest interest to biomedical research-ers.
There were 52 sentences which involvedsome variation between CLAIM (1) andQUALIFIER (4), a fact which revealed a need tomake clearer distinctions between these twocategories.
Many sentences in our corpus seemedto meet the specifications for both categories atthe same time i.e.
they were both an explanationand a conclusion.
There were 46 sentences in-volving some disagreement between (4) andWARRANT/BACKING (3).
The source of thisvariation seemed to be the difficulty decidingwhether the ?compare and contrast with externalevidence?
aspect of (4) or the straightforward?external evidence?
of (3) was more appropriatefor certain, especially complex, sentences.3.3    AnnotatorsUnder Model 1 the three annotator columnsshow a relatively similar distribution (Table 6).The exception is that J was less inclined to selectthe CONTEXT category, and more inclined toselect RESULTS COMPARED, than either B orK.Category B J K TotalCONTEXT (1) 121 92 124 337METHOD (2) 39 43 46 128CURRENTRESULTS (3) 59 67 63 189RESULTSCOMPARED (4) 36 57 21 114ANALYSIS (5) 145 141 146 432Total 400 400 400 1200Table 6 Category distribution by annotator ?Model 1Under Model 2 we see an extreme rangeamong annotators in the number of sentencesthey identified as EXTRANEOUS with J havingmore than twice as many as B (Table 7).
Thisdegree of annotator bias guaranteed that category134(0) would be involved in considerable inter-annotator disagreement.
The other notable skew-ing occurred in categories (1) and (4) where Band J shared similar numbers as opposed to K: Khad 91 sentences as CLAIM, almost twice asmany as B or J, and only 50 sentences asQUALIFIER, roughly half as many as B or J.Category B J K TotalEXTRANEOUS (0) 54 116 80 250CLAIM (1) 45 49 91 185GROUNDS (2) 86 61 71 218WARRANT/BACKING (3) 81 49 85 215QUALIFIER (4) 108 98 50 256PROBLEM INCONTEXT (5) 26 27 23 76Total 400 400 400 1200Table 7 Category distribution by annotator ?Model 2In addition to the systematic annotator prefer-ences discussed above there were instances of?errors?, choices which appear to be violations ofcategory specifications.
These may be the resultof haste or inattention, insufficient training or alack of understanding of the article?s content orthe Models.3.4    Corpus DataIt was assumed that longer sentences would bemore likely to be complex and thus more likelyto involve inter-annotator variation.
The resultsshowed that the articles with the smallest (19)and largest (31) average number of words persentence did exhibit this pattern: the formerranked highly in three-way annotator agreement(first under Model 1 and second under Model 2)and the latter second lowest under both Models.However, between these extremes there was noclear relationship between sentence length andoverall coder agreement under either Model.The most striking finding was the wide range ofthree-way coder agreement among the twelvearticles in the corpus: from 36% to 81% underModel 1 and 8% to 69% under Model 2.
The av-erages in Table 3 mask this source of inter-annotator variation.4    ConclusionThe problem of choosing a single argument cate-gory for a complex sentence was at the core ofmuch of the inter-annotator variation found un-der both Models.
The issue of sentences whichare rhetorically but not grammatically complexe.g.
those with a single tensed verb that seemedto qualify as both a CLAIM and a QUALIFIERunder Model 2 should be dealt with where possi-ble by revising the category specifications.
How-ever sentences that are grammatically complexshould be divided into clauses (one for eachtensed verb) as a pre-annotating process.
Al-though this creates more units and thus more op-portunities for coders to disagree, it is believedthat reducing uncertainty by allowing a differentargument category for each clause would beworth the trade-off.Although Model 1 had higher average three-way agreement at 60.5% than Model 2, this wasstill relatively poor performance.
As discussedabove the clear problem with this Model is theCONTEXT (1) category.
Research scientists arealways working within and building on previouswork ?
their own and others?
; thus ?old?
and?new?
information are inherently intertwined.Therefore this category needs to be revised, pos-sibly separating specific previous studies fromstatements related to the motivation for or goalsof the current experiment.
As discussed above,the EXTRANEOUS category of Model 2 needsto be redefined, and the CLAIM and QUALI-FIER categories must be clearly distinguished.Despite the relatively poor performance ofModel 2, with the above improvements it is be-lieved that a CLAIMS-based Model is still agood candidate for developing future IE tools.Annotator bias reflects the fact that coders didnot have sufficient understanding of rhetoricaltechniques and structure, but also the problemswith category specifications noted above.
Theextreme ?inter-article?
variation (Section 3.4)indicates that when texts are not clearly written,an annotator?s lack of knowledge of biomedicineand/or argument are even more problematic.Since the quality of writing in a corpus is a factorthat cannot be controlled ?team?
annotations arerecommended: a biomedical domain expertshould work together with an expert in rhetoric.It must be admitted, however, that even withimprovements to the Models of argument andusing annotators with more domain expertise,some degree of inter-annotator disagreement willinevitably occur as a result of individual differ-ences.
Ultimately annotators are making judg-ments ?
about texts and arguments that were cre-ated by others ?
that are somewhat subjective.135ReferencesMilos Jenicek.
2006.
How to read, understand, andwrite ?Discussion?
sections in medical articles: Anexercise in critical thinking.
Med Sci Monitor,12(6): SR28-SR36.Yoko Mizuta, Anna Korhonen, Tony Mullen andNigel Collier.
2005.
Zone Analysis in Biology Ar-ticles as a Basis for Information Extraction.
Inter-national Journal of Medical Informatics, 75(6):468-487.Dennis Reidsma and Jean Carletta.
2008.
ReliabilityMeasurement without Limits.
Computational Lin-guistics, 34(3): 319-326.Simone Teufel, Jean Carletta and Mark Moens.
1999.An annotation scheme for discourse-level argumen-tation in research articles.
Proceedings of theEighth Meeting of the European Chapter of the As-sociation for Computational Linguistics: 110-117.Stephen E. Toulmin.
1958/2003.
The Uses of Argu-ment.
Cambridge University Press, Cambridge,U.K.Barbara White.
2009.
Annotating a Corpus of Bio-medical Research Texts: Two Models of RhetoricalAnalysis.
PhD thesis, The University of WesternOntario, Canada.www.csd.uwo.ca/~mercer/White_Thesis09.pdf136
