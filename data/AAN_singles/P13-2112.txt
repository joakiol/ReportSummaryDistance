Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 634?639,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSimpler unsupervised POS tagging with bilingual projectionsLong Duong, 12 Paul Cook, 1 Steven Bird, 1 and Pavel Pecina21 Department of Computing and Information Systems, The University of Melbourne2 Charles University in Prague, Czech Republiclduong@student.unimelb.edu.au, paulcook@unimelb.edu.au,sbird@unimelb.edu.au, pecina@ufal.mff.cuni.czAbstractWe present an unsupervised approach topart-of-speech tagging based on projec-tions of tags in a word-aligned bilingualparallel corpus.
In contrast to the exist-ing state-of-the-art approach of Das andPetrov, we have developed a substantiallysimpler method by automatically identi-fying ?good?
training sentences from theparallel corpus and applying self-training.In experimental results on eight languages,our method achieves state-of-the-art re-sults.1 Unsupervised part-of-speech taggingCurrently, part-of-speech (POS) taggers are avail-able for many highly spoken and well-resourcedlanguages such as English, French, German, Ital-ian, and Arabic.
For example, Petrov et al (2012)build supervised POS taggers for 22 languages us-ing the TNT tagger (Brants, 2000), with an aver-age accuracy of 95.2%.
However, many widely-spoken languages ?
including Bengali, Javanese,and Lahnda ?
have little data manually labelledfor POS, limiting supervised approaches to POStagging for these languages.However, with the growing quantity of textavailable online, and in particular, multilingualparallel texts from sources such as multilin-gual websites, government documents and largearchives of human translations of books, news, andso forth, unannotated parallel data is becomingmore widely available.
This parallel data can beexploited to bridge languages, and in particular,transfer information from a highly-resourced lan-guage to a lesser-resourced language, to build un-supervised POS taggers.In this paper, we propose an unsupervised ap-proach to POS tagging in a similar vein to thework of Das and Petrov (2011).
In this approach,a parallel corpus for a more-resourced languagehaving a POS tagger, and a lesser-resourced lan-guage, is word-aligned.
These alignments are ex-ploited to infer an unsupervised tagger for the tar-get language (i.e., a tagger not requiring manually-labelled data in the target language).
Our ap-proach is substantially simpler than that of Dasand Petrov, the current state-of-the art, yet per-forms comparably well.2 Related workThere is a wealth of prior research on building un-supervised POS taggers.
Some approaches haveexploited similarities between typologically simi-lar languages (e.g., Czech and Russian, or Teluguand Kannada) to estimate the transition probabil-ities for an HMM tagger for one language basedon a corpus for another language (e.g., Hana et al,2004; Feldman et al, 2006; Reddy and Sharoff,2011).
Other approaches have simultaneouslytagged two languages based on alignments in aparallel corpus (e.g., Snyder et al, 2008).A number of studies have used tag projectionto copy tag information from a resource-rich toa resource-poor language, based on word align-ments in a parallel corpus.
After alignment, theresource-rich language is tagged, and tags are pro-jected from the source language to the target lan-guage based on the alignment (e.g., Yarowsky andNgai, 2001; Das and Petrov, 2011).
Das andPetrov (2011) achieved the current state-of-the-artfor unsupervised tagging by exploiting high con-fidence alignments to copy tags from the sourcelanguage to the target language.
Graph-based la-bel propagation was used to automatically producemore labelled training data.
First, a graph wasconstructed in which each vertex corresponds toa unique trigram, and edge weights represent thesyntactic similarity between vertices.
Labels werethen propagated by optimizing a convex functionto favor the same tags for closely related nodes634Model Coverage AccuracyMany-to-1 alignments 88% 68%1-to-1 alignments 68% 78%1-to-1 alignments: Top 60k sents 91% 80%Table 1: Token coverage and accuracy of many-to-one and 1-to-1 alignments, as well as the top60k sentences based on alignment score for 1-to-1alignments, using directly-projected labels only.while keeping a uniform tag distribution for un-related nodes.
A tag dictionary was then extractedfrom the automatically labelled data, and this wasused to constrain a feature-based HMM tagger.The method we propose here is simpler to thatof Das and Petrov in that it does not require con-vex optimization for label propagation or a featurebased HMM, yet it achieves comparable results.3 TagsetOur tagger exploits the idea of projecting tag infor-mation from a resource-rich to resource-poor lan-guage.
To facilitate this mapping, we adopt Petrovet al?s (2012) twelve universal tags: NOUN,VERB, ADJ, ADV, PRON (pronouns), DET (de-terminers and articles), ADP (prepositions andpostpositions), NUM (numerals), CONJ (conjunc-tions), PRT (particles), ?.?
(punctuation), and X(all other categories, e.g., foreign words, abbrevia-tions).
These twelve basic tags are common acrosstaggers for most languages.Adopting a universal tagset avoids the needto map between a variety of different, language-specific tagsets.
Furthermore, it makes it possi-ble to apply unsupervised tagging methods to lan-guages for which no tagset is available, such asTelugu and Vietnamese.4 A Simpler Unsupervised POS TaggerHere we describe our proposed tagger.
The keyidea is to maximize the amount of informationgleaned from the source language, while limit-ing the amount of noise.
We describe the seedmodel and then explain how it is successively re-fined through self-training and revision.4.1 Seed ModelThe first step is to construct a seed tagger fromdirectly-projected labels.
Given a parallel corpusfor a source and target language, Algorithm 1 pro-vides a method for building an unsupervised tag-ger for the target language.
In typical applications,the source language would be a better-resourcedlanguage having a tagger, while the target lan-guage would be lesser-resourced, lacking a taggerand large amounts of manually POS-labelled data.Algorithm 1 Build seed model1: Tag source side.2: Word align the corpus with Giza++ and re-move the many-to-one mappings.3: Project tags from source to target using the re-maining 1-to-1 alignments.4: Select the top n sentences based on sentencealignment score.5: Estimate emission and transition probabilities.6: Build seed tagger T.We eliminate many-to-one alignments (Step 2).Keeping these would give more POS-tagged to-kens for the target side, but also introduce noise.For example, suppose English and French werethe source and target language, respectively.
Inthis case alignments such as English laws (NNS)to French les (DT) lois (NNS) would be expected(Yarowsky and Ngai, 2001).
However, in Step 3,where tags are projected from the source to targetlanguage, this would incorrectly tag French les asNN.
We build a French tagger based on English?French data from the Europarl Corpus (Koehn,2005).
We also compare the accuracy and cov-erage of the tags obtained through direct projec-tion using the French Melt POS tagger (Denis andSagot, 2009).
Table 1 confirms that the one-to-onealignments indeed give higher accuracy but lowercoverage than the many-to-one alignments.
Atthis stage of the model we hypothesize that high-confidence tags are important, and hence eliminatethe many-to-one alignments.In Step 4, in an effort to again obtain higherquality target language tags from direct projection,we eliminate all but the top n sentences based ontheir alignment scores, as provided by the alignervia IBM model 3.
We heuristically set this cutoffto 60k to balance the accuracy and size of the seedmodel.1 Returning to our preliminary English?French experiments in Table 1, this process givesimprovements in both accuracy and coverage.21We considered values in the range 60?90k, but thischoice had little impact on the accuracy of the model.2We also considered using all projected labels for the top60k sentences, not just 1-to-1 alignments, but in preliminaryexperiments this did not perform as well, possibly due to thepreviously-observed problems with many-to-one alignments.635The number of parameters for the emission prob-ability is |V | ?
|T | where V is the vocabulary andT is the tag set.
The transition probability, on theother hand, has only |T |3 parameters for the tri-gram model we use.
Because of this differencein number of parameters, in step 5, we use dif-ferent strategies to estimate the emission and tran-sition probabilities.
The emission probability isestimated from all 60k selected sentences.
How-ever, for the transition probability, which has lessparameters, we again focus on ?better?
sentences,by estimating this probability from only those sen-tences that have (1) token coverage > 90% (basedon direct projection of tags from the source lan-guage), and (2) length > 4 tokens.
These cri-teria aim to identify longer, mostly-tagged sen-tences, which we hypothesize are particularly use-ful as training data.
In the case of our preliminaryEnglish?French experiments, roughly 62% of the60k selected sentences meet these criteria and areused to estimate the transition probability.
For un-aligned words, we simply assign a random POSand very low probability, which does not substan-tially affect transition probability estimates.In Step 6 we build a tagger by feeding the es-timated emission and transition probabilities intothe TNT tagger (Brants, 2000), an implementationof a trigram HMM tagger.4.2 Self training and revisionFor self training and revision, we use the seedmodel, along with the large number of target lan-guage sentences available that have been partiallytagged through direct projection, in order to builda more accurate tagger.
Algorithm 2 describesthis process of self training and revision, and as-sumes that the parallel source?target corpus hasbeen word aligned, with many-to-one alignmentsremoved, and that the sentences are sorted byalignment score.
In contrast to Algorithm 1, allsentences are used, not just the 60k sentences withthe highest alignment scores.We believe that sentence alignment score mightcorrespond to difficulty to tag.
By sorting the sen-tences by alignment score, sentences which aremore difficult to tag are tagged using a more ma-ture model.
Following Algorithm 1, we dividesentences into blocks of 60k.In step 3 the tagged block is revised by com-paring the tags from the tagger with those ob-tained through direct projection.
Suppose sourceAlgorithm 2 Self training and revision1: Divide target language sentences into blocksof n sentences.2: Tag the first block with the seed tagger.3: Revise the tagged block.4: Train a new tagger on the tagged block.5: Add the previous tagger?s lexicon to the newtagger.6: Use the new tagger to tag the next block.7: Goto 3 and repeat until all blocks are tagged.language word wsi is aligned with target languageword wtj with probability p(wtj |wsi ), T si is the tagfor wsi using the tagger available for the sourcelanguage, and T tj is the tag for wtj using the taggerlearned for the target language.
If p(wtj |wsi ) > S,where S is a threshold which we heuristically setto 0.7, we replace T tj by T si .Self-training can suffer from over-fitting, inwhich errors in the original model are repeatedand amplified in the new model (McClosky et al,2006).
To avoid this, we remove the tag ofany token that the model is uncertain of, i.e., ifp(wtj |wsi ) < S and T tj ?= T si then T tj = Null.
So,on the target side, aligned words have a tag fromdirect projection or no tag, and unaligned wordshave a tag assigned by our model.Step 4 estimates the emission and transitionprobabilities as in Algorithm 1.
In Step 5, emis-sion probabilities for lexical items in the previousmodel, but missing from the current model, areadded to the current model.
Later models thereforetake advantage of information from earlier mod-els, and have wider coverage.5 Experimental ResultsUsing parallel data from Europarl (Koehn, 2005)we apply our method to build taggers for the sameeight target languages as Das and Petrov (2011)?
Danish, Dutch, German, Greek, Italian, Por-tuguese, Spanish and Swedish ?
with English asthe source language.
Our training data (Europarl)is a subset of the training data of Das and Petrov(who also used the ODS United Nations datasetwhich we were unable to obtain).
The evaluationmetric and test data are the same as that used byDas and Petrov.
Our results are comparable totheirs, although our system is penalized by havingless training data.
We tag the source language withthe Stanford POS tagger (Toutanova et al, 2003).636Danish Dutch German Greek Italian Portuguese Spanish Swedish AverageSeed model 83.7 81.1 83.6 77.8 78.6 84.9 81.4 78.9 81.3Self training + revision 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4Table 2: Token-level POS tagging accuracy for our seed model, self training and revision, and the methodof Das and Petrov (2011).
The best results on each language, and on average, are shown in bold.0 5 10 15 20 25 30Iteration50607080Percentages Overall AccKnow AccOOV AccKnow tkn0 5 10 15 20 25 30Iteration7075808590Percentages Overall AccKnow AccOOV AccKnow tknFigure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion ofknown tokens for Italian (left) and Dutch (right).Table 2 shows results for our seed model, selftraining and revision, and the results reported byDas and Petrov.
Self training and revision im-prove the accuracy for every language over theseed model, and gives an average improvementof roughly two percentage points.
The averageaccuracy of self training and revision is on parwith that reported by Das and Petrov.
On individ-ual languages, self training and revision and themethod of Das and Petrov are split ?
each per-forms better on half of the cases.
Interestingly, ourmethod achieves higher accuracies on Germaniclanguages ?
the family of our source language,English?while Das and Petrov perform better onRomance languages.
This might be because ourmodel relies on alignments, which might be moreaccurate for more-related languages, whereas Dasand Petrov additionally rely on label propagation.Compared to Das and Petrov, our model per-forms poorest on Italian, in terms of percentagepoint difference in accuracy.
Figure 1 (left panel)shows accuracy, accuracy on known words, accu-racy on unknown words, and proportion of knowntokens for each iteration of our model for Italian;iteration 0 is the seed model, and iteration 31 isthe final model.
Our model performs poorly onunknown words as indicated by the low accuracyon unknown words, and high accuracy on knownwords compared to the overall accuracy.
The poorperformance on unknown words is expected be-cause we do not use any language-specific rulesto handle this case.
Moreover, on average for thefinal model, approximately 10% of the test datatokens are unknown.
One way to improve the per-formance of our tagger might be to reduce the pro-portion of unknown words by using a larger train-ing corpus, as Das and Petrov did.We examine the impact of self-training and re-vision over training iterations.
We find that forall languages, accuracy rises quickly in the first5?6 iterations, and then subsequently improvesonly slightly.
We exemplify this in Figure 1 (rightpanel) for Dutch.
(Findings are similar for otherlanguages.)
Although accuracy does not increasemuch in later iterations, they may still have somebenefit as the vocabulary size continues to grow.6 ConclusionWe have proposed a method for unsupervised POStagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substan-tially less-sophisticated (specifically not requiringconvex optimization or a feature-based HMM).The complexity of our algorithm is O(nlogn)compared to O(n2) for that of Das and Petrov637(2011) where n is the size of training data.3 Wemade our code are available for download.4In future work we intend to consider using alarger training corpus to reduce the proportion ofunknown tokens and improve accuracy.
Giventhe improvements of our model over that of Dasand Petrov on languages from the same familyas our source language, and the observation ofSnyder et al (2008) that a better tagger can belearned from a more-closely related language, wealso plan to consider strategies for selecting an ap-propriate source language for a given target lan-guage.
Using our final model with unsupervisedHMM methods might improve the final perfor-mance too, i.e.
use our final model as the ini-tial state for HMM, then experiment with differ-ent inference algorithms such as ExpectationMax-imization (EM), Variational Bayers (VB) or Gibbssampling (GS).5 Gao and Johnson (2008) compareEM, VB and GS for unsupervised English POStagging.
In many cases, GS outperformed othermethods, thus we would like to try GS first for ourmodel.7 AcknowledgementsThis work is funded by Erasmus MundusEuropean Masters Program in Language andCommunication Technologies (EM-LCT) andby the Czech Science Foundation (grant no.P103/12/G084).
We would like to thank ProkopisProkopidis for providing us the Greek Treebankand Antonia Marti for the Spanish CoNLL 06dataset.
Finally, we thank Siva Reddy and Span-dana Gella for many discussions and suggestions.ReferencesThorsten Brants.
2000.
TnT: A statistical part-of-speech tagger.
In Proceedings of the sixth con-ference on Applied natural language processing(ANLP ?00), pages 224?231.
Seattle, Washing-ton, USA.Dipanjan Das and Slav Petrov.
2011.
Unsu-pervised part-of-speech tagging with bilingualgraph-based projections.
In Proceedings of3We re-implemented label propagation from Das andPetrov (2011).
It took over a day to complete this step onan eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, butonly 15 minutes for our model.4https://code.google.com/p/universal-tagger/5We in fact have tried EM, but it did not help.
The overallperformance dropped slightly.
This might be because self-training with revision already found the local maximal point.the 49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies - Volume 1 (ACL 2011), pages600?609.
Portland, Oregon, USA.Pascal Denis and Beno?
?t Sagot.
2009.
Couplingan annotated corpus and a morphosyntactic lex-icon for state-of-the-art POS tagging with lesshuman effort.
In Proceedings of the 23rd Pa-cific Asia Conference on Language, Informationand Computation, pages 721?736.
Hong Kong,China.Anna Feldman, Jirka Hana, and Chris Brew.
2006.A cross-language approach to rapid creation ofnew morpho-syntactically annotated resources.In Proceedings of the Eight International Con-ference on Language Resources and Evaluation(LREC?06), pages 549?554.
Genoa, Italy.Jianfeng Gao and Mark Johnson.
2008.
A com-parison of bayesian estimators for unsupervisedhidden markov model pos taggers.
In Proceed-ings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP ?08,pages 344?352.
Association for ComputationalLinguistics, Stroudsburg, PA, USA.Jiri Hana, Anna Feldman, and Chris Brew.
2004.A resource-light approach to Russian morphol-ogy: Tagging Russian using Czech resources.In Proceedings of the 2004 Conference on Em-pirical Methods in Natural Language Process-ing (EMNLP ?04), pages 222?229.
Barcelona,Spain.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In Proceed-ings of the Tenth Machine Translation Summit(MT Summit X), pages 79?86.
AAMT, Phuket,Thailand.David McClosky, Eugene Charniak, and MarkJohnson.
2006.
Effective self-training for pars-ing.
In Proceedings of the main conference onHuman Language Technology Conference of theNorth American Chapter of the Association ofComputational Linguistics (HLT-NAACL ?06),pages 152?159.
New York, USA.Slav Petrov, Dipanjan Das, and Ryan McDonald.2012.
A universal part-of-speech tagset.
InProceedings of the Eight International Confer-ence on Language Resources and Evaluation(LREC?12), pages 2089?2096.
Istanbul, Turkey.Siva Reddy and Serge Sharoff.
2011.
Cross lan-guage POS Taggers (and other tools) for Indian638languages: An experiment with Kannada usingTelugu resources.
In Proceedings of the IJC-NLP 2011 workshop on Cross Lingual Infor-mation Access: Computational Linguistics andthe Information Need of Multilingual Societies(CLIA 2011).
Chiang Mai, Thailand.Benjamin Snyder, Tahira Naseem, Jacob Eisen-stein, and Regina Barzilay.
2008.
Unsupervisedmultilingual learning for POS tagging.
In Pro-ceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP?08), pages 1041?1050.
Honolulu, Hawaii.Kristina Toutanova, Dan Klein, Christopher D.Manning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic de-pendency network.
In Proceedings of the2003 Conference of the North American Chap-ter of the Association for Computational Lin-guistics on Human Language Technology - Vol-ume 1 (NAACL ?03), pages 173?180.
Edmon-ton, Canada.David Yarowsky and Grace Ngai.
2001.
Induc-ing multilingual POS taggers and NP bracketersvia robust projection across aligned corpora.
InProceedings of the Second Meeting of the NorthAmerican Chapter of the Association for Com-putational Linguistics on Language technolo-gies (NAACL ?01), pages 1?8.
Pittsburgh, Penn-sylvania, USA.639
