Multi-Speaker Language ModelingGang Ji and Jeff Bilmes ?SSLI Lab, Department of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195-2500{gang,bilmes}@ee.washington.eduAbstractIn conventional language modeling, the wordsfrom only one speaker at a time are repre-sented, even for conversational tasks such asmeetings and telephone calls.
In a conversa-tional or meeting setting, however, speakerscan have significant influence on each other.To recover such un-modeled inter-speaker in-formation, we introduce an approach for con-versational language modeling that considerswords from other speakers when predictingwords from the current one.
By augmenting anormal trigram context, our new multi-speakerlanguage model (MSLM) improves on bothSwitchboard and ICSI Meeting Recorder cor-pora.
Using an MSLM and a conditional mu-tual information based word clustering algo-rithm, we achieve a 8.9% perplexity reductionon Switchboard and a 12.2% reduction on theICSI Meeting Recorder data.1 IntroductionStatistical language models (LMs) are used in many ap-plications such as speech recognition, handwriting recog-nition, spelling correction, machine translation, and in-formation retrieval.
The goal is to produce a probabilitymodel over a word sequence P (W ) = P (w1, ?
?
?
, wT ).Conventional language models are often based on a fac-torized form P (W ) ?
?t P (wt|?
(ht)), where ht is thehistory for wt and ?
is a history mapping.The case of n-gram language modeling, where?
(ht) = wt?n+1 ?
?
?wt?1, is widely used.
Typically,n = 3, which yields a trigram model.
A refinement ofthis model is the class-based n-gram where the words arepartitioned into equivalence classes (Brown et al, 1992).
?This work was funded by NSF under grant IIS-0121396.In general, smoothing techniques are applied to lessenthe curse of dimensionality.
Among all methods, mod-ified Kneser-Ney smoothing (Chen and Goodman, 1998)is widely used because of its good performance.Modeling conversational language is a particularly dif-ficult task.
Even though conventional techniques workwell on read or prepared speech, situations such as tele-phone conversations or multi-person meetings pose greatresearch challenges due to disfluencies, and odd syn-tactic/discourse patterns.
Other difficulties include falsestarts, interruptions, and poor or unrepresented grammar.Most state-of-the-art language models consider wordstreams individually and treat different phrases indepen-dently.
In this work, we introduce multi-speaker lan-guage modeling (MSLM), which models the effects ona speaker of words spoken by other speakers partici-pating in the same conversation or meeting.
Our newmodel achieves initial perplexity reductions of 6.2% onSwitchboard-I, 5.8% on Switchboard Eval-2003, and10.3% on ICSI Meeting data.
In addition, we devel-oped a word clustering procedure (based on a standardapproach (Brown et al, 1992)) that optimizes conditionalword clusters.
Our class-based MSLMs using our newalgorithm yield improvements of 7.1% on Switchboard-I,8.9% on Switchboard Eval-2003, and 12.2% on meetings.A brief outline follows: Section 2 introduces multi-speaker language modeling.
Section 3 provides ini-tial evaluations on Switchboard and the ICSI Meetingdata.
Section 4 presents evaluations using our class-basedmulti-speaker language models, and Section 5 concludes.2 Multi-speaker Language ModelingIn a conversational setting, such as during a meeting ortelephone call, the words spoken by one speaker are af-fected not only by his or her own previous words butalso by other speakers.
Such inter-speaker dependency,however, is typically ignored in standard n-gram lan-guage models.
In this work, information (i.e., word to-kens) from other speakers (A) is used to better predictword tokens of the current speaker (W ).
When predict-ing wt, instead of using P (wt|w0, ?
?
?
, wt?1), the formP (wt|w0, ?
?
?
, wt?1; a0, ?
?
?
, at) is used.
Here at repre-sents a word spoken by some other speaker with appro-priate starting time (Section 3).
A straight-forward im-plementation is to extend the normal trigram model as:P (wt|?
(ht)) = P (wt|wt?1, wt?2, at).
(1)HiHiDiana ?
Take care?
ByeA:W:(a)SaturdayA:W: Saturdayon Sundayon 0.2s or Saturday0.4sC5:C4:C3:C1:C0:SaturdaySaturdaySaturday 1.1s SaturdayC2:on Sundayon or Saturday(b)Figure 1: Examples of phone conversation (a) and meet-ing (b).
(Frame sizes are not proportional to time scale.
)Figure 1 shows an example from Switchboard (a) andone from a meeting (b).
In (a), only two speakers are in-volved and the words from the current speaker, W , areaffected by the other speaker, A.
At the beginning of aconversation, the response to ?Hi?
is likely to be ?Hi?
or?Hello.?
At the end of the phone call, the response to?Take care?
might be ?Bye?, or ?You too?, etc.
In (b),we show a typical meeting conversation.
Speaker C2 isinterrupting C3 when C3 says ?Sunday?.
Because ?Sun-day?
is a day of the week, there is a high probability thatC2?s response is also a day of the week.
In our model, weonly consider two streams at a time, W and A. There-fore, when considering the probability of C2?s words, itis reasonable to collapse words from all other speakers(C0,C1,C3,C4, and C5) into one stream A as shown inthe figure.
This makes available to C2 the rest of themeeting to potentially condition on, although it does notdistinguish between different speakers.Our model, Equation 1, is different from most lan-guage modeling systems since our models condition onboth previous words and another potential factor A. Sucha model is easily represented using a factored languagemodel (FLM), an idea introduced in (Bilmes and Kirch-hoff, 2003; Kirchhoff et al, 2003), and incorporated intothe SRILM toolkit (Stolcke, 2002).
Note that a form ofcross-side modeling was used by BBN (Schwartz, 2004),where in a multi-pass speech recognition system the out-put of a first-pass from one speaker is used to prime wordsin the language model for the other speaker.3 Initial EvaluationWe evaluate MSLMs on three corpora: Switchboard-I, Switchboard Eval-2003, and ICSI Meeting data.
InSwitchboard-I, 6.83% of the words are overlapped intime, where we define w1 and w2 as being overlappedif s(w1) ?
s(w2) < e(w1) or s(w2) ?
s(w1) < e(w2),where s(?)
and e(?)
are the starting and ending time of aword.The ICSI Meeting Recorder corpus (Janin et al, 2003)consists of a number of meeting conversations with threeor more participants.
The data we employed has 32 con-versations, 35,000 sentences and 307,000 total words,where 8.5% of the words were overlapped.
As mentionedpreviously, we collapse the words from all other speakersinto one stream A as a conditioning set for W .
The dataconsists of all speakers taking their turn being W .To be used in an FLM, the words in each stream need tobe aligned at discrete time points.
Clearly, at should notcome from wt?s future.
Therefore, for each wt, we usethe closest previous A word in the past for at such thats(wt?1) ?
s(at) < s(wt).
Therefore, each at is usedonly once and no constraints are placed on at?s end time.This is reasonable since one can often predict a speaker?sword after it starts but before it completes.We score using the model P (wt|wt?1, wt?2, at).1 Dif-ferent back-off strategies, including different back-offpaths as well as combination methods (Bilmes and Kirch-hoff, 2003), were tried and here we present the best re-sults.
The backoff order (for Switchboard-I and Meeting)first dropped at, then wt?2, wt?1, ending with the uni-form distribution.
For Switchboard eval-2003, we useda generalized parallel backoff mechanism.
In all cases,modified Kneser-Ney smoothing (Chen and Goodman,1998) was used at all back-off points.Results on Switchboard-I and the meeting data em-ployed 5-fold cross-validation.
Training data for Switch-board eval-2003 consisted of all of Switchboard-I.
InSwitchboard eval-2003, hand-transcribed time marks areunavailable, so A was available only at the beginning ofutterances of W .2 Results (mean perplexities and stan-dard deviations) are listed in Table 1 (Switchboard-I andmeeting) and the |V | column in Table 3.Table 1: Perplexities from MSLM on Switchboard-I(swbd-I) and ICSI Meeting data (mr)data trigram four-gram mslm reductionswbd-I 73.2?0.4 73.7?0.4 68.5?0.3 6.2%mr 87.4?4.6 89.5?4.9 78.4?2.7 10.3%1In all cases, end of sentence tokens, </s>, were not scoredto avoid artificially small perplexities arising when wt = at =</s>, since P (</s>|</s>) yields a high probability value.2Having time-marks, say, via a forced alignment wouldlikely improve our results.In Table 1, the first column shows data set names.
Thesecond and third columns show our best baseline trigramand four-gram perplexities, both of which used interpo-lation and modified Kneser-Ney at every back-off point.The trigram outperforms the four-gram.
The fourth col-umn shows the perplexity results with MSLMs and thelast column shows the MSLM?s relative perplexity reduc-tion over the (better) trigram baseline.
This positive re-duction indicates that for both data sets, the utilization ofadditional information from other speakers can better pre-dict the words of the current speaker.
The improvement islarger in the highly conversational meeting setting sinceadditional speakers, and thus more interruptions, occur.3.1 AnalysisIt is elucidating at this point to identify when and howA-words can help predict W -words.
We thus computedthe log-probability ratio of P (wt|wt?1, wt?2, at) and thetrigram P (wt|wt?1, wt?2) evaluated on all test set tuplesof form (wt?2, wt?1wt, at).
When this ratio is large andpositive, conditioning on at significantly increases theprobability of wt in the context of wt?1 and wt?2.
Theopposite is true when the ratio is large and negative.
Toensure the significance of our results, we define ?large?to mean at least 101.5 ?
32, so that using at makes wt atleast 32 times more (or less) probable.
We chose 32 in adata-driven fashion, to be well above any spurious prob-ability differences due to smoothing of different models.At the first word of a phrase spoken by W , there area number of cases of A words that significantly increasethe probability of a W word relative to the trigram alone.This includes (in roughly decreasing order of probabil-ity) echos (e.g., when A says ?Friday?, W repeats it),greetings/partings (e.g., a W greeting is likely to followan A greeting), paraphrases (e.g., ?crappy?
followed by?ugly?, or ?Indiana?
followed by ?Purdue?
), is-a relation-ships (e.g., A saying ?corporation?
followed by W saying?dell?, A-?actor?
followed by W -?Swayze?, A-?name?followed by W -?Patricia?, etc.
), and word completions.On the other hand, some A contexts (e.g., laughter) sig-nificantly decrease the probability of many W words.Within a W phrase, other patterns emerge.
Inparticular, some A words significantly decrease theprobability that W will finish a commonly-used phrase.For example, in a trigram alone, p(bigger|and, bigger),p(forth|and, back), and p(easy|and, quick), all havehigh probability.
When also conditioning on A, someA words significantly decrease the probability offinishing such phrases.
For example, we find thatp(easy|and, quick, ?uh-hmm?)
p(easy|and, quick).A similar phenomena occurs for other com-monly used phrases, but only when A has utteredwords such as ?yeah?, ?good?, ?ok?, ?
[laugh-ter]?, ?huh?, etc.
While one possible explanationof this is just due to decreased counts, we foundthat for such phrases p(wt|wt?1, wt?2, at)minwt?3?S p4(wt|wt?1, wt?2, wt?3) where p4 is afour-gram, S = {w : C(wt, wt?1, wt?2, w) > 0}, andC is the 4-gram word count function for the switchboardtraining and test sets.
Therefore, our hypothesis isthat when W is in the process of uttering a predictablephrase and A indicates she knows what W will say, it isimprobable that W will complete that phrase.The examples above came from Switchboard-I, but wefound similar phenomena in the other corpora.4 Conditional Probability Clustering100 500 1000 1500 2000 |V|6870727476788082848688number of classesperplexitymr baselinemrswbd baselineswbdFigure 2: Class-based MSLM from MCMI clustering onSwitchboard-I (swbd) and ICSI Meeting (mr) data.Table 2: Three types of class-based MSLMs onSwitchboard-I (swbd) and ICSI Meeting (mr) corpora# of swbd mrclasses BROWN MMI MCMI BROWN MMI MCMI100 68.9?0.3 68.4?0.3 68.2?0.3 78.9?3.0 77.3?2.8 76.8?2.8500 68.9?0.3 68.3?0.3 67.9?0.3 78.7?3.1 77.1?2.8 76.7?2.81000 68.9?0.3 68.2?0.3 67.9?0.3 79.0?3.1 77.2?2.7 76.9?2.81500 69.0?0.3 68.2?0.3 68.0?0.3 79.6?3.1 77.4?2.7 77.4?2.72000 69.0?0.3 68.3?0.3 68.0?0.3 80.1?3.1 77.6?2.7 77.9?2.7|V | 68.5?0.3 78.3?2.7Table 3: Class-based MSLM on Switchboard Eval-2003size 100 500 1000 1500 2000 |V | 3-gram 4-gramppl 65.8 65.5 65.6 65.7 66.1 67.9 72.1 76.3% reduction 8.6 8.9 8.8 8.7 8.3 5.8 0 -5.8Class-based language models (Brown et al, 1992;Whittaker and Woodland, 2003) yield great benefits whendata sparseness abounds.
SRILM (Stolcke, 2002) canproduce classes to maximize the mutual informationbetween the classes I(C(wt);C(wt?1)), as describedin (Brown et al, 1992).
More recently, a method for clus-tering words at different positions was developed (Ya-mamoto et al, 2001; Gao et al, 2002).
Our goal isto produce classes that improve the scores P (wt|ht) =P (wt|wt?1, wt?2, C1(at)), what we call class-basedMSLMs.
In our case, the vocabulary for A is partitionedinto classes by either maximizing conditional mutualinformation (MCMI) I(wt;C(at)|wt?1, wt?2) or justmaximizing mutual information (MMI) I(wt;C(at)).While such clusterings can perform poorly under lowcounts, our results show further consistent improvements.Our new clustering procedures were implemented intothe SRILM toolkit.
When partitioned into smallerclasses, the A-tokens are replaced by their correspond-ing class IDs.
The result is then trained using the samefactored language model as before.
The resulting per-plexities for the MCMI case are presented in Figure 2,where the horizontal axis shows the number of A-streamclasses (the right-most shows the case before clustering),and the vertical axis shows average perplexity.
In bothdata corpora, the average perplexities decrease after ap-plying class-based MSLMs.
For both Switchboard-I andthe meeting data, the best result is achieved using 500classes (7.1% and 12.2% improvements respectively).To compare different clustering algorithms, resultswith the standard method of (Brown et al, 1992)(SRILM?s ngram-class) are also reported.
All the per-plexities for these three types of class-based MSLMs aregiven in Table 2.
For Switchboard-I, ngram-class doesslightly better than without clustering.
On the meetingdata, it even does slightly worse than no clustering.
OurMMI method does show a small improvement, and theperplexities are further (but not significantly) reduced us-ing our MCMI method (but at the cost of much morecomputation during development).We also show results on Switchboard eval-2003 in Ta-ble 3.
We compare an optimized four-gram, a three-gram baseline, and various numbers of cluster sizes us-ing our MCMI method and generalized backoff (Bilmesand Kirchhoff, 2003), which, (again) with 500 clusters,achieves an 8.9% relative improvement over the trigram.5 Discussions and ConclusionIn this paper, novel multi-speaker language modeling(MSLM) is introduced and evaluated.
After simplyadding words from other speakers into a normal trigramcontext, the new model shows a reasonable improvementin perplexity.
This model can be further improved whenclass-based cross-speaker information is employed.
Wealso presented two different criteria for this clustering.The more complex criteria gives similar results to thesimple one, presumably due to data sparseness.
Eventhough Switchboard and meeting data are different interms of topic, speaking style, and speaker number, onemight more robustly learn cross-speaker information bytraining on the union of these two data sets.There are a number of ways to extend this work.
First,our current approach is purely data driven.
One can imag-ine that higher level information (e.g., a dialog or otherspeech act) about the other speakers might be particularlyimportant.
Latent semantic analysis of stream A mightalso be usefully employed here.
Furthermore, more thanone word from stream A can be included in the contextto provide additional predictive ability.
With the meet-ing data, there may be a benefit to controlling for spe-cific speakers based on their degree of influence.
Alterna-tively, an MSLM might help identify the most influentialspeaker in a meeting by determining who most changesthe probability of other speakers?
words.Moreover, the approach clearly suggests that a multi-speaker decoder in an automatic speech recognition(ASR) system might be beneficial.
Once time marks foreach word are provided in an N -best list, our MSLMtechnique can be used for rescoring.
Additionally, sucha decoder can easily be specified using graphical mod-els (Bilmes and Zweig, 2002) in first-pass decodings.We wish to thank Katrin Kirchhoff and the anonymousreviewers for useful comments on this work.ReferencesJ.
Bilmes and K. Kirchhoff.
2003.
Factored languagemodels and generalized parallel backoff.
In HumanLanguage Technology Conference.J.
Bilmes and G. Zweig.
2002.
The graphical modelstoolkit: An open source software system for speechand time-series processing.
In Proc.
ICASSP, June.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-cer.
1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal Report TR-10-98, Computer Science Group, Har-vard University, August.J.
Gao, J. Goodman, G. Cao, and H. Li.
2002.
Exploringasymmetric clustering for statistical langauge model-ing.
In Proc.
of ACL, pages 183?190, July.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,and C. Wooters.
2003.
The ICSI Meeting Corpus.
InProc.
ICASSP, April.K.
Kirchhoff, J. Bilmes, S. Das, N. Duta, M. Egan, G. Ji,F.
He, J. Henderson, D. Liu, M. Noamany, P. Schone,R.
Schwartz, and D. Vergyri.
2003.
Novel approachesto arabic speech recognition: Report from the 2002Johns-Hopkins workshop.
In Proc.
ICASSP, April.R.
Schwartz.
2004.
Personal communication.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Spoken LanguageProcessing, September.E.
Whittaker and P. Woodland.
2003.
Language mod-elling for Russian and English using words and classes.Computer Speech and Language, pages 87?104.H.
Yamamoto, S. Isogai, and Y. Sagisaka.
2001.
Multi-class composite n-gram language model for spokenlanguage processing using multiple word clusters.
InProc.
of ACL, pages 531?538.
