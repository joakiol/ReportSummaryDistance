Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 491?498,Sydney, July 2006. c?2006 Association for Computational LinguisticsInterpreting Semantic Relations in Noun Compounds via Verb SemanticsSu Nam Kim?
and Timothy Baldwin???
Computer Science and Software EngineeringUniversity of Melbourne, Victoria 3010 Australiaand?
NICTA Victoria Research LabUniversity of Melbourne, Victoria 3010 Australia{snkim,tim}@csse.unimelb.edu.auAbstractWe propose a novel method for automat-ically interpreting compound nouns basedon a predefined set of semantic relations.First we map verb tokens in sentential con-texts to a fixed set of seed verbs usingWordNet::Similarity and Moby?sThesaurus.
We then match the sentenceswith semantic relations based on the se-mantics of the seed verbs and grammaticalroles of the head noun and modifier.
Basedon the semantics of the matched sentences,we then build a classifier using TiMBL.The performance of our final system at in-terpreting NCs is 52.6%.1 IntroductionThe interpretation of noun compounds (hereafter,NCs) such as apple pie or family car is a well-established sub-task of language understanding.Conventionally, the NC interpretation task is de-fined in terms of unearthing the underspecified se-mantic relation between the head noun and modi-fier(s), e.g.
pie and apple respectively in the caseof apple pie.NC interpretation has been studied in the con-text of applications including question-answeringand machine translation (Moldovan et al, 2004;Cao and Li, 2002; Baldwin and Tanaka, 2004;Lauer, 1995).
Recent work on the automatic/semi-automatic interpretation of NCs (e.g., Lapata(2002), Rosario and Marti (2001), Moldovan et al(2004) and Kim and Baldwin (2005)) has made as-sumptions about the scope of semantic relations orrestricted the domain of interpretation.
This makesit difficult to gauge the general-purpose utility ofthe different methods.
Our method avoids anysuch assumptions while outperforming previousmethods.In seminal work on NC interpretation, Finin(1980) tried to interpret NCs based on hand-codedrules.
Vanderwende (1994) attempted the auto-matic interpretation of NCs using hand-writtenrules, with the obvious cost of manual interven-tion.
Fan et al (2003) estimated the knowledgerequired to interpret NCs and claimed that perfor-mance was closely tied to the volume of data ac-quired.In more recent work, Barker and Szpakowicz(1998) used a semi-automatic method for NC in-terpretation in a fixed domain.
Lapata (2002)developed a fully automatic method but focusedon nominalizations, a proper subclass of NCs.1Rosario and Marti (2001) classified the nouns inmedical texts by tagging hierarchical informationusing neural networks.
Moldovan et al (2004)used the word senses of nouns based on the do-main or range of interpretation of an NC, leadingto questions of scalability and portability to noveldomains/NC types.
Kim and Baldwin (2005) pro-posed a simplistic general-purpose method basedon the lexical similarity of unseen NCs with train-ing instances.The aim of this paper is to develop an automaticmethod for interpreting NCs based on semantic re-lations.
We interpret semantic relations relative toa fixed set of constructions involving the modifierand head noun and a set of seed verbs for eachsemantic relation: e.g.
(the) family owns (a) caris taken as evidence for family car being an in-stance of the POSSESSOR relation.
We then at-tempt to map all instances of the modifier and headnoun as the heads of NPs in a transitive senten-tial context onto our set of constructions via lex-ical similarity over the verb, to arrive at an inter-pretation: e.g.
we would hope to predict that pos-sess is sufficiently similar to own that (the) familypossesses (a) car would be recognised as support-1With nominalizations, the head noun is deverbal, and inthe case of Lapata (2002), nominalisations are assumed tobe interpretable as the modifier being either the subject (e.g.child behavior) or object (e.g.
car lover) of the base verb ofthe head noun.491ing evidence for the POSSESSOR relation.
We usea supervised classifier to combine together the evi-dence contributed by individual sentential contextsof a given modifier?head noun combination, andarrive at a final interpretation for a given NC.Mapping the actual verbs in sentences to ap-propriate seed verbs is obviously crucial to thesuccess of our method.
This is particularly im-portant as there is no guarantee that we will findlarge numbers of modifier?head noun pairings inthe sorts of sentential contexts required by ourmethod, nor that we will find attested instancesbased on the seed verbs.
Thus an error in map-ping an attested verb to the seed verbs could resultin a wrong interpretation or no classification at all.In this paper, we experiment with the use of Word-Net (Fellbaum, 1998) and word clusters (based onMoby?s Thesaurus) in mapping attested verbs tothe seed verbs.
We also make use of CoreLex indealing with the semantic relation TIME and theRASP parser (Briscoe and Carroll, 2002) to de-termine the dependency structure of corpus data.The data source for our set of NCs is binaryNCs (i.e.
NCs with a single modifier) from theWall Street Journal component of the Penn Tree-bank.
We deliberately choose to ignore NCs withmultiple modifiers on the grounds that: (a) 88.4%of NC types in the Wall Street Journal componentof the Penn Treebank and 90.6% of NC types inthe British National Corpus are binary; and (b) weexpect to be able to interpret NCs with multiplemodifiers by decomposing them into binary NCs.Another simplifying assumption we make is to re-move NCs incorporating proper nouns since: (a)the lexical resources we employ in this researchdo not contain them in large numbers; and (b)there is some doubt as to whether the set of seman-tic relations required to interpret NCs incorporat-ing proper nouns is that same as that for commonnouns.The paper is structured as follows.
Section 2takes a brief look at the semantics of NCs and thebasic idea behind the work.
Section 3 details theset of NC semantic relations that is used in ourresearch, Section 4 presents an extended discus-sion of our approach, Section 5 briefly explains thetools we use, Section 6.1 describes how we gatherand process the data, Section 6.2 explains how wemap the verbs to seed verbs, and Section 7 andSection 8 present the results and analysis of ourapproach.
Finally we conclude our work in Sec-tion 9.2 MotivationThe semantic relation in NCs is the relation be-tween the head noun (denoted ?H?)
and the mod-ifier(s) (denoted ?M?).
We can find this semanticrelation expressed in certain sentential construc-tions involving the head noun and modifier.
(1) family carCASE: family owns the car.FORM: H own MRELATION: POSSESSOR(2) student protestCASE: protest is performed by student.FORM: M is performed by HRELATION: AGENTIn the examples above, the semantic relation(e.g.
POSSESSOR) provides an interpretation ofhow the head noun and modifiers relate to eachother, and the seed verb (e.g.
own) provides a para-phrase evidencing that relation.
For example, inthe case of family car, the family is the POSSES-SOR of the car, and in student protest, student(s)are the AGENT of the protest.
Note that voice is im-portant in aligning sentential contexts with seman-tic relations.
For instance, family car can be repre-sented as car is owned by family (passive) and stu-dent protest as student performs protest (active).The exact nature of the sentential context varieswith different synonyms of the seed verbs.
(3) family carCASE: Synonym=have/possess/belong toFORM: H own MRELATION: POSSESSOR(4) student protestCASE: Synonym=act/execute/doFORM: M is performed by HRELATION: AGENTThe verb own in the POSSESSOR relation hasthe synonyms have, possess and belong to.
Inthe context of have and possess, the form of re-lation would be same as the form with verb, own.However, in the context of belong to, family car492would mean that the car belongs to family.
Thus,even when the voice of the verb is the same(voice=active), the grammatical role of the headnoun and modifier can change.In our approach we map the actual verbs in sen-tences containing the head noun and modifiers toseed verbs corresponding to the relation forms.The set of seed verbs contains verbs representa-tive of each semantic relation form.
We chose twosets of seed verbs of size 57 and 84, to examinehow the coverage of actual verbs by seed verbs af-fects the performance of our method.
Initially, wemanually chose a set of 60 seed verbs.
We thenadded synonyms from Moby?s thesaurus for someof the 60 verbs.
Finally, we filtered verbs from thetwo expanded sets, since these verbs occur veryfrequently in the corpus (as this might skew theresults).
The set of seed verbs {have, own, pos-sess, belong to} are in the set of 57 seed verbs,and {acquire, grab, occupy} are added to the setof 84 seed verbs; all correspond to the POSSES-SOR relation.For each relation, we generate a set of con-structional templates associating a subset of seedverbs with appropriate grammatical relations forthe head noun and modifier.
Examples for POS-SESSOR are:S({have, own, possess}V,M SUBJ,H OBJ) (5)S({belong to}V,H SUBJ,M OBJ) (6)where V is the set of seed verbs, M is the modifierand H is the head noun.Two relations which do not map readily ontoseed verbs are TIME (e.g.
winter semester) andEQUATIVE (e.g.
composer arranger).
Here, werely on an independent set of contextual evidence,as outlined in Section 6.1.Through matching actual verbs attested in cor-pus data onto seed verbs, we can match sentenceswith relations (see Section 6.2).
Using this methodwe can identify the matching relation forms of se-mantic relations to decide the semantic relation forNCs.3 Semantic Relations in CompoundNounsWhile there has been wide recognition of the needfor a system of semantic relations with which toclassify NCs, there is still active debate as to whatthe composition of that set should be, or indeedRASP parserRaw SentencesModified SentencesFinal SentencesClassifierSemantic RelationPre?processingCollect Subj, Obj, PP, PPN, V, TFilter sentencesGet sentences with H,MVerb?Mappingmap verbs onto seed verbsMatch modified sentenceswrt relation formsMoby?s ThesaurusWordNet::SimilarityClassifier:TimblNoun CompoundFigure 1: System Architecturewhether it is reasonable to expect that all NCsshould be interpretable with a fixed set of semanticrelations.Based on the pioneering work on Levi (1979)and Finin (1980), there have been efforts in com-putational linguistics to arrive at largely task-specific sets of semantic relations, driven by theannotation of a representative sample of NCs froma given corpus type (Vanderwende, 1994; Barkerand Szpakowicz, 1998; Rosario and Marti, 2001;Moldovan et al, 2004).
In this paper, we use theset of 20 semantic relations defined by Barker andSzpakowicz (1998), rather than defining a new setof relations.
The main reasons we chose this setare: (a) that it clearly distinguishes between thehead noun and modifiers, and (b) there is cleardocumentation of each relation, which is vital forNC annotation effort.
The one change we maketo the original set of 20 semantic relations is to ex-clude the PROPERTY relation since it is too generaland a more general form of several other relationsincluding MATERIAL (e.g.
apple pie).4 MethodFigure 1 outlines the system architecture of ourapproach.
We used three corpora: the Browncorpus (as contained in the Penn Treebank), theWall Street Journal corpus (also taken from thePenn treebank), and the written component of theBritish National Corpus (BNC).
We first parsedeach of these corpora using RASP (Briscoe andCarroll, 2002), and identified for each verb to-ken the voice, head nouns of the subject andobject, and also, for each PP attached to thatverb, the head preposition and head noun of the493NP (hereafter, PPN).
Next, for our test NCs, weidentified all verbs for which the modifier andhead noun co-occur as subject, object, or PPN.We then mapped these verbs to seed verbs us-ing WordNet::Similarity and Moby?s The-saurus (see Section 5 for details).
Finally, we iden-tified the corresponding relation for each seed verband selected the best-fitting semantic relation us-ing a classifier.
To evaluate our approach, we builta classifier using TiMBL (Daelemans et al, 2004).5 ResourcesIn this section, we outline the tools and resourcesemployed in our method.As our parser, we used RASP, generating adependency representation for the most probableparse for each sentence.
Note that RASP also lem-matises all words in a POS-sensitive manner.To map actual verbs onto seed verbs,we experimented with two resources:WordNet::Similarity and Moby?s the-saurus.
WordNet::Similarity2 is an opensource software package that allows the userto measure the semantic similarity or related-ness between two words (Patwardhan et al,2003).
Of the many methods implemented inWordNet::Similarity, we report on resultsfor one path-based method (WUP, Wu and Palmer(1994)), one content-information based method(JCN, Jiang and Conrath (1998)) and two semanticrelatedness methods (LESK, Banerjee and Peder-sen (2003), and VECTOR, (Patwardhan, 2003)).We also used a random similarity-generatingmethod as a baseline (RANDOM).The second semantic resource we use for verb-mapping method is Moby?s thesaurus.
Moby?sthesaurus is based on Roget?s thesaurus, and con-tains 30K root words, and 2.5M synonyms and re-lated words.
Since the direct synonyms of seedverbs have limited coverage over the set of sen-tences used in our experiment, we also experi-mented with using second-level indirect synonymsof seed verbs.In order to deal with the TIME relation, we usedCoreLex (Buitelaar, 1998).
CoreLex is based on aunified approach to systematic polysemy and thesemantic underspecification of nouns, and derivesfrom WordNet 1.5.
It contains 45 basic CoreLextypes, systematic polysemous classes and 39,937nouns with tags.2www.d.umn.edu/ tpederse/similarity.htmlAs mentioned earlier, we built our supervisedclassifier using TiMBL.6 Data Collection6.1 Data ProcessingTo test our method, we extracted 2,166 NC typesfrom the Wall Street Journal (WSJ) component ofthe Penn Treebank.
We additionally extracted sen-tences containing the head noun and modifier inpre-defined constructional contexts from the amal-gam of: (1) the Brown Corpus subset containedin the Penn Treebank, (2) the WSJ portion of thePenn Treebank, and (3) the British National Cor-pus (BNC).
Note that while these pre-defined con-structional contexts are based on the contexts inwhich our seed verbs are predicted to correlatewith a given semantic relation, we instances of allverbs occurring in those contexts.
For example,based on the construction in Equation 5, we ex-tract all instances of S(Vi,M SUBJj ,H OBJj ) for allverbs Vi and all instances of NCj = (Mj ,Hj) inour dataset.Two annotators tagged the 2,166 NC types in-dependently at 52.3% inter-annotator agreement,and then met to discus all contentious annotationsand arrive at a mutually-acceptable gold-standardannotation for each NC.
The Brown, WSJ andBNC data was pre-parsed with RASP, and sen-tences were extracted which contained the headnoun and modifier of one of our 2,166 NCs in sub-ject or object position, or as (head) noun within theNP of an PP.
After extracting these sentences, wecounted the frequencies of the different modifier?head noun pairs, and filtered out: (a) all construc-tional contexts not involving a verb contained inWordNet 2.0, and (b) all NCs for which the modi-fier and head noun did not co-occur in at least fivesentential contexts.
This left us with a total of 453NCs for training and testing.
The combined totalnumber of sentential contexts for our 453 NCs was7,714, containing 1,165 distinct main verbs.We next randomly split the NC data into 80%training data and 20% test data.
The final numberof test NCs is 88; the final number of training NCsvaries depending on the verb-mapping method.As noted in Section 2, the relations TIME andEQUATIVE are not associated with seed verbs.
ForTIME, rather than using contextual evidence, wesimply flag the possibility of a TIME if the modifieris found to occur in the TIME class of CoreLex.
Inthe case of TIME, we consider coordinated occur-494ACTBENEFITHAVEUSEPLAYPERFORM......Seed verbsacceptactagree HOLD.....Verb?MappingMethodsAGENTBENEFICIARYCONTAINEROBJECTPOSSESSORINSTRUMENT.........Semantic RelationsOriginal verbsaccommodateFigure 2: Verb mappingrences of the modifier and head noun (e.g.
coachand player for player coach) as evidence for therelation.3 We thus separately collate statisticsfrom coordinated NPs for each NC, and from thiscompute a weight for each NC based on mutualinformation:TIME(NCi) = ?log2freq(coord(Mi, Hi))freqMi ?
freq(Hi)(7)where Mi and Hi are the modifier and head ofNCi, respectively, and freq(coord(Mi,Hi)) is thefrequency of occurrence of Mi and Hi in coordi-nated NPs.Finally, we calculate a normalised weight foreach seed verb by determining the proportion ofhead verbs each seed verb occurs with.6.2 Verb MappingThe sentential contexts gathered from corpusdata contain a wide range of verbs, not justthe seed verbs.
To map the verbs onto seedverbs, and hence estimate which semantic rela-tion(s) each is a predictor of, we experimentedwith two different methods.
First we used theWordNet::Similarity package to calculatethe similarity between a given verb and eachof the seed verbs, experimenting with the 5methods mentioned in Section 5.
Second, weused Moby?s thesaurus to extract both direct syn-onyms (D-SYNONYM) and a combination of directand second-level indirect synonyms of verbs (I-SYNONYM), and from this, calculate the closest-matching seed verb(s) for a given verb.Figure 2 depicts the procedure for mappingverbs in constructional contexts onto the seedverbs.
Verbs found in the various contexts in the3Note the order of the modifier and head in coordinatedNPs is considered to be irrelevant, i.e.
player and coach andcoach and player are equally evidence for an EQUATIVE inter-pretation for player coach (and coach player).accomplish achieve behave conduct ...ACTact conduct deadl with function perform playLEVEL=1LEVEL=2synonym in level1 synonym in level2 not found in level1Figure 3: Expanding synonyms# of SeedVB D-Synonyms D/I-Synonyms57 6,755(87.6%) 7,388(95.8%)84 6,987(90.6%) 7,389(95.8%)Table 1: Coverage of D and D/I-Synonymscorpus (on the left side of the figure) map onto oneor more seed verbs, which in turn map onto oneor more semantic relations.4 We replace all non-seed verbs in the corpus data with the seed verb(s)they map onto, potentially increasing the numberof corpus instances.Since direct (i.e.
level 1) synonyms fromMoby?s thesaurus are not sufficient to map allverbs onto seed verbs, we also include second-level (i.e.
level 2) synonyms, expanding from di-rect synonyms.
Table 1 shows the coverage ofsentences for test NCs, in which D indicates directsynonyms and I indicates indirect synonyms.7 EvaluationWe evaluated our method over both 17 semanticrelations (without EQUATIVE and TIME) and the full19 semantic relations, due to the low frequencyand lack of verb-based constructional contexts forEQUATIVE and TIME, as indicated in Table 2.
Notethat the test data set is the same for both sets ofsemantic relations, but that the training data inthe case of 17 semantic relations will not con-tain any instances for the EQUATIVE and TIME re-lations, meaning that all such test instances willbe misclassified.
The baseline for all verb map-ping methods is a simple majority-class classifier,which leads to an accuracy of 42.4% for the TOPICrelation.
In evaluation, we use two different val-ues for our method: Count and Weight.
Countis based on the raw number of corpus instances,while Weight employs the seed verb weight de-scribed in Section 6.1.4There is only one instance of a seed verb mapping tomultiple semantic relations, namely perform which corre-sponds to the two relations AGENT and OBJECT.495# of SR # SeedV Method WUP JCN RANDOM LESK VECTOR D-SYNONYM I-SYNONYM17 Baseline .423 .423 .423 .423 .423 .423 .42357 Count .324 .408 .379 .416 .466 .337 .337Weight .320 .408 .371 .416 .466 .337 .34284 Count .406 .470 .184 .430 .413 .317 .333Weight .424 .426 .259 .457 .526 .341 .40619 Baseline .409 .409 .409 .409 .409 .409 .40957 Count .315 .420 .384 .440 .466 .350 .337Weight .311 .420 .376 .440 .466 .350 .34284 Count .413 .470 .200 .414 .413 .321 .333Weight .439 .446 .280 .486 .526 .356 .393Table 2: Results with 17 relations and with 19 relations#of SR # SeedVB Method WUP JCN RANDOM LESK VECTOR17 Baseline .423 .423 .423 .423 .42357 Count .423 .385 .379 .413 .466Weight .423 .385 .379 .413 .46684 Count .325 .439 .420 .484 .466Weight .281 .393 .317 .476 .46619 Baseline .409 .409 .409 .409 .40957 Count .423 .397 .392 .413 .413Weight .423 .397 .392 .413 .50084 Count .333 .439 .425 .484 .413Weight .290 .410 .317 .484 .413Table 3: Results of combining the proposed method and with the method of Kim and Baldwin (2005)As noted above, we excluded all NCs for whichwe were unable to find at least 5 instances of themodifier and head noun in an appropriate senten-tial context.
This exclusion reduced the originalset of 2,166 NCs to only 453, meaning that theproposed method is unable to classify up to 80% ofNCs.
For real-world applications, a method whichis only able to arrive at a classification for 20% ofinstances is clearly of limited utility, and we needsome way of expanding the coverage of the pro-posed method.
This is achieved by adapting thesimilarity method proposed by Kim and Baldwin(2005) to our task, wherein we use lexical simi-larity to identify the nearest-neighbour NC for agiven NC, and classify the given NC according tothe classification for the nearest-neighbour.
Theresults for the combined method are presented inTable 3.8 DiscussionFor the basic method, as presented in Table 2, theclassifier produced similar results over the 17 se-mantic relations to the 19 semantic relations.
Us-ing data from Weight and Count for both 17 and19 semantic relations, the classifier achieved thebest performance with VECTOR (context vector-based distributional similarity), followed by JCNand LESK.
The main reason is that VECTOR ismore conservative than the other methods at map-ping (original) verbs onto seed verbs, i.e.
the aver-age number of seed verbs a given verb maps ontois small.
For the other methods, the semantics ofthe original sentences are often not preserved un-der verb mapping, introducing noise to the classi-fication task.Comparing the two sets of semantic relations(17 vs. 19), the set with more semantic rela-tions achieved slightly better performance in mostcases.
A detailed breakdown of the results re-vealed that TIME both has an above-average clas-sification accuracy and is associated with a rela-tively large number of test NCs, while EQUATIVEhas a below-average classification accuracy but isassociated with relatively few instances.While an increased number of seed verbs gener-ates more training instances under verb mapping,it is imperative that the choice of seed verbs bemade carefully so that they not introduce noiseinto the classifier and reducing overall perfor-mance.
Figure 4 is an alternate representation ofthe numbers from Table 2, with results for each in-dividual method over 57 and 84 seed verbs juxta-posed for each of Count andWeight.
From this, weget the intriguing result that Count generally per-forms better over fewer seed verbs, while Weightperforms better over more seed verbs.496WUP JCN RANDOM LESK VECTOR SYN?D SYN?D,IResult with CountVerb?mapping methodAccuracy(%)WUP JCN RANDOM LESK VECTOR SYN?D SYN?D,IVerb?mapping methodAccuracy(%)Result with Weight020406080100w/ 57 seed verbsw/ 84 seed verbs020406080100w/ 57 seed verbsw/ 84 seed verbsFigure 4: Performance with 57 vs. 84 seed verbs#of SR # SeedVB WUP LCH JCN LIN RANDOM LESK VECTOR17 Baseline .433 .433 .441 .441 .433 .477 .42857 .449 .421 .415 .337 .409 .469 .344Baseline .433 .433 .433 .433 .428 .438 .44484 .476 .416 .409 .349 .226 .465 .33319 Baseline .418 .418 .430 .430 .418 .477 .41357 .465 .418 .417 .341 .232 .462 .344Baseline .413 .413 .418 .418 .413 .438 .42684 .471 .413 .407 .348 .218 .465 .320Table 4: Results for the method of Kim and Baldwin (2005) over the test set used in this researchFor the experiment where we combine ourmethod with that of Kim and Baldwin (2005), aspresented in Table 3, we find a similar pattern ofresults to the proposed method.
Namely, VECTORand LESK achieve the best performance, with mi-nor variations in the absolute performance relativeto the original method but the best results for eachrelation set actually dropping marginally over theoriginal method.
This drop is not surprising whenwe consider that we use an imperfect method toidentify the nearest neighbour for an NC for whichwe are unable to find corpus instances in sufficientnumbers, and then a second imperfect method toclassify the instance.Compared to previous work, our method pro-duces reasonably stable performance when op-erated over the open-domain data with smallamounts of training data.
Rosario and Marti(2001) achieved about 60% using a neural net-work in a closed domain, Moldovan et al (2004)achieved 43% using word sense disambiguationof the head noun and modifier over open domaindata, and Kim and Baldwin (2005) produced 53%using lexical similarities of the head noun andmodifier (using the same relation set, but evaluatedover a different dataset).
The best result achievedby our system was 52.6% over open-domain data,using a general-purpose relation set.To get a better understanding of how ourmethod compares with that of Kim and Baldwin(2005), we evaluated the method of Kim and Bald-win (2005) over the same data set as used in thisresearch, the results of which are presented in Ta-ble 4.
The relative results for the different sim-ilarity metrics mirror those reported in Kim andBaldwin (2005).
WUP produced the best perfor-mance at 47-48% for the two relation sets, sig-nificantly below the accuracy of our method at53.3%.
Perhaps more encouraging is the resultthat the combined method?where we classify at-tested instances according to the proposed method,and classify unattested instances according to thenearest-neighbour method of Kim and Baldwin(2005) and the classifications from the proposedmethod?outperforms the method of Kim andBaldwin (2005).
That is, the combined methodhas the coverage of the method of Kim and Bald-win (2005), but inherits the higher accuracy of themethod proposed herein.
Having said this, the per-formance of the Kim and Baldwin (2005) methodover PRODUCT, TOPIC, LOCATION and SOURCE issuperior to that of our method.
In this sense,we believe that alternate methods of hybridisationmay lead to even better results.Finally, we wish to point out that the methodas presented here is still relatively immature, withconsiderable scope for improvement.
In its currentform, we do not weight the different seed verbs497based on their relative similarity to the originalverb.
We also use the same weight and frequencyfor each seed verb relative to a given relation, de-spite seed verbs being more indicative of a givenrelation and also potentially occurring more oftenin the corpus.
For instance, possess is more relatedto POSSESSOR than occupy.
Also possess occursmore often in the corpus than belong to.
As futurework, we intend to investigate whether allowancesfor these considerations can improve the perfor-mance of our method.9 ConclusionIn this paper, we proposed a method for au-tomatically interpreting noun compounds basedon seed verbs indicative of each semantic re-lation.
For a given modifier and head noun,our method extracted corpus instances of thetwo nouns in a range of constructional contexts,and then mapped the original verbs onto seedverbs based on lexical similarity derived fromWordNet::Similarity, and Moby?s The-saurus.
These instances were then fed into theTiMBL learner to build a classifier.
The best-performing method was VECTOR, which is a con-text vector distributional similarity method.
Wealso experimented with varying numbers of seedverbs, and found that generally the more seedverbs, the better the performance.
Overall, thebest-performing system achieved an accuracy of52.6% with 84 seed verbs and the VECTOR verb-mapping method.AcknowledgementsWewould like to thank the members of the Univer-sity of Melbourne LT group and the three anony-mous reviewers for their valuable input on this re-search.ReferencesTimothy Baldwin and Takaaki Tanaka.
2004.
Transla-tion by machine of compound nominals: Getting it right.In In Proceedings of the ACL 2004 Workshop on Multi-word Expressions: Integrating Processing, pages 24?31,Barcelona, Spain.Satanjeev Banerjee and Ted Pedersen.
2003.
Extended glossoverlaps as a measure of semantic relatedness.
In Pro-ceedings of the Eighteenth International Joint Conferenceon Artificial Intelligence, pages 805?810, Acapulco, Mex-ico.Ken Barker and Stan Szpakowicz.
1998.
Semi-automaticrecognition of noun modifier relationships.
In Proceed-ings of the 17th international conference on Computa-tional linguistics, pages 96?102, Quebec, Canada.Ted Briscoe and John Carroll.
2002.
Robust accurate statisti-cal annotation of general text.
In Proc.
of the 3rd Interna-tional Conference on Language Resources and Evaluation(LREC 2002), pages 1499?1504, Las Palmas, Canary Is-lands.Paul Buitelaar.
1998.
CoreLex: Systematic Polysemy andUnderspecification.
Ph.D. thesis, Brandeis University,USA.Yunbo Cao and Hang Li.
2002.
Base noun phrase translationusing web data and the EM algorithm.
In 19th Interna-tional Conference on Computational Linguistics, Taipei,Taiwan.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-tal van den Bosch.
2004.
TiMBL: Tilburg Memory BasedLearner, version 5.1, Reference Guide.
ILK Technical Re-port 04-02.James Fan, Ken Barker, and Bruce W. Porter.
2003.
Theknowledge required to interpret noun compounds.
In 7thInternational Joint Conference on Artificial Intelligence,pages 1483?1485, Acapulco, Mexico.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, USA.Timothy W. Finin.
1980.
The semantic interpretation ofcompound nominals.
Ph.D. thesis, University of Illinois,Urbana, Illinois, USA.Jay Jiang and David Conrath.
1998.
Semantic similar-ity based on corpus statistics and lexical taxonomy.
InProceedings on International Conference on Research inComputational Linguistics, pages 19?33.Su Nam Kim and Timothy Baldwin.
2005.
Automatic in-terpretation of noun compounds using WordNet similar-ity.
In Second International Joint Conference On NaturalLanguage Processing, pages 945?956, JeJu, Korea.Maria Lapata.
2002.
The disambiguation of nominalizations.Computational Linguistics, 28(3):357?388.Mark Lauer.
1995.
Designing Statistical Language Learn-ers: Experiments on Noun Compounds.
Ph.D. thesis,Macquarie University.Judith Levi.
1979.
The Syntax and Semantics of ComplexNominals.
New York: Academic Press.Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel An-tohe, and Roxana Girju.
2004.
Models for the seman-tic classification of noun phrases.
In HLT-NAACL 2004:Workshop on Computational Lexical Semantics, pages 60?67, Boston, USA.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-sen. 2003.
Using measures of semantic relatedness forword sense disambiguation.
In Proceedings of the FourthInternational Conference on Intelligent Text Processingand Computational Linguistics.Siddharth Patwardhan.
2003.
Incorporating dictionary andcorpus information into a context vector measure of se-mantic relatedness.
Master?s thesis, University of Min-nesota, USA.Barbara Rosario and Hearst Marti.
2001.
Classifying the se-mantic relations in noun compounds via a domain-specificlexical hierarchy.
In Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Processing,pages 82?90.Lucy Vanderwende.
1994.
Algorithm for automatic inter-pretation of noun sequences.
In Proceedings of the 15thConference on Computational linguistics, pages 782?788.Zhibiao Wu and Martha Palmer.
1994.
Verb semantics andlexical selection.
In 32nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 133 ?138.498
