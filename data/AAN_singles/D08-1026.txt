Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 244?253,Honolulu, October 2008. c?2008 Association for Computational LinguisticsIncorporating Temporal and Semantic Information with Eye Gaze forAutomatic Word Acquisition in Multimodal Conversational SystemsShaolin Qu Joyce Y. ChaiDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824{qushaoli,jchai}@cse.msu.eduAbstractOne major bottleneck in conversational sys-tems is their incapability in interpreting un-expected user language inputs such as out-of-vocabulary words.
To overcome this problem,conversational systems must be able to learnnew words automatically during human ma-chine conversation.
Motivated by psycholin-guistic findings on eye gaze and human lan-guage processing, we are developing tech-niques to incorporate human eye gaze for au-tomatic word acquisition in multimodal con-versational systems.
This paper investigatesthe use of temporal alignment between speechand eye gaze and the use of domain knowl-edge in word acquisition.
Our experiment re-sults indicate that eye gaze provides a poten-tial channel for automatically acquiring newwords.
The use of extra temporal and domainknowledge can significantly improve acquisi-tion performance.1 IntroductionInterpreting human language is a challenging prob-lem in human machine conversational systems dueto the flexibility of human language behavior.
Whenthe encountered vocabulary is outside of the sys-tem?s knowledge, conversational systems tend tofail.
It is desirable that conversational systems canlearn new words automatically during human ma-chine conversation.
While automatic word acquisi-tion in general is quite challenging, multimodal con-versational systems offer an unique opportunity toexplore word acquisition.
In a multimodal conversa-tional system where users can talk and interact witha graphical display, users?
eye gaze, which occursnaturally with speech production, provides a poten-tial channel for the system to learn new words auto-matically during human machine conversation.Psycholinguistic studies have shown that eye gazeis tightly linked to human language processing.
Eyegaze is one of the reliable indicators of what a per-son is ?thinking about?
(Henderson and Ferreira,2004).
The direction of eye gaze carries informa-tion about the focus of the user?s attention (Just andCarpenter, 1976).
The perceived visual context in-fluences spoken word recognition and mediates syn-tactic processing of spoken sentences (Tanenhaus etal., 1995).
In addition, directly before speaking aword, the eyes move to the mentioned object (Grif-fin and Bock, 2000).Motivated by these psycholinguistic findings, weare investigating the use of eye gaze for automaticword acquisition in multimodal conversation.
Par-ticulary, this paper investigates the use of tempo-ral information about speech and eye gaze and do-main semantic relatedness for automatic word ac-quisition.
The domain semantic and temporal in-formation are incorporated in statistical translationmodels for word acquisition.
Our experiments showthat the use of domain semantic and temporal infor-mation significantly improves word acquisition per-formance.In the following sections, we first describe the ba-sic translation models for word acquisition.
Then,we describe the enhanced models that incorporatetemporal and semantic information about speechand eye gaze for word acquisition.
Finally, wepresent the results of empirical evaluation.244(a) Raw gaze points (b) Processed gaze fixationsFigure 1: Domain scene with a user?s gaze fixations2 Related WorkWord acquisition by grounding words to visual en-tities has been studied in many language ground-ing systems.
For example, given speech paired withvideo images of single objects, mutual informationbetween audio and visual signals was used to acquirewords by associating acoustic phone sequences withthe visual prototypes (e.g., color, size, shape) of ob-jects (Roy and Pentland, 2002).
Generative mod-els were used to acquire words by associating wordswith image regions given parallel data of picturesand description text (Barnard et al, 2003).
Differ-ent from these works, in our work, the visual atten-tion foci accompanying speech are indicated by eyegaze.
Eye gaze is an implicit and subconscious in-put, which brings additional challenges in word ac-quisition.Eye gaze has been explored for word acquisitionin previous work.
In (Yu and Ballard, 2004), givenspeech paired with eye gaze information and videoimages, a translation model was used to acquirewords by associating acoustic phone sequences withvisual representations of objects and actions.
A re-cent investigation on word acquisition from tran-scribed speech and eye gaze in human machine con-versation was reported in (Liu et al, 2007).
In thiswork, a translation model was developed to asso-ciate words with visual objects on a graphical dis-play.
Different from these previous works, herewe investigate the incorporation of extra knowledge,specifically speech-gaze temporal information anddomain knowledge, with eye gaze to facilitate wordacquisition.3 Data CollectionWe recruited users to interact with a simplified mul-timodal conversational system to collect speech andeye gaze data.3.1 DomainWe are working on a 3D room decoration domain.Figure 1 shows the 3D room scene that was shownto the user in the experiments.
There are 28 3Dobjects (bed, chairs, paintings, lamp, etc.)
in theroom scene.
During the human machine conversa-tion, the system verbally asked the user a question(e.g., ?what do you dislike about the arrangementof the room??)
or issued a request (e.g., ?describethe left wall?)
about the room.
The user providedresponses by speaking to the system.During the experiments, users?
speech wasrecorded through an open microphone and users?eye gaze was captured by an Eye Link II eye tracker.Eye gaze data consists of the screen coordinates ofeach gaze point that was captured by the eye trackerat a sampling rate of 250hz.3.2 Data PreprocessingAs for speech data, we collected 357 spoken utter-ances from 7 users?
experiments.
The vocabularysize is 480, among which 227 words are nouns andadjectives.
We manually transcribed the collectedspeech.As for gaze data, the first step is to identify gazefixation from raw gaze points.
As shown in Fig-ure 1(a), the collected raw gaze points are very noisy.They can not be used directly for identifying gazefixated entities in the scene.
We processed the raw245gaze data to eliminate invalid and saccadic gazepoints.
Invalid gaze points occur when users lookoff the screen.
Saccadic gaze points occur duringballistic eye movements between gaze fixations.
Vi-sion studies have shown that no visual processingoccurs in the human mind during saccades (i.e., sac-cadic suppression) (Matin, 1974).
Since eyes do notstay still but rather make small, frequent jerky move-ments, we average nearby gaze points to better iden-tify gaze fixations.
The processed eye gaze fixationsare shown in Figure 1(b).1668 2096 32522692[19] [22] [ ] [10][11][10][11][10][11]This room has a chandelier2572 2872 3170 3528 3736speech streamgaze stream(ms)(ms)[fixated entity ID]ts tef: gaze fixation( [19] ?
bed_frame; [22] ?
door; [10] ?
bedroom; [11] ?
chandelier )Figure 2: Parallel speech and gaze streamsFigure 2 shows an excerpt of the collected speechand gaze fixation in one experiment.
In the speechstream, each word starts at a particular timestamp.In the gaze stream, each gaze fixation has a start-ing timestamp ts and an ending timestamp te.
Eachgaze fixation also has a list of fixated entities (3D ob-jects).
An entity e on the graphical display is fixatedby gaze fixation f if the area of e contains fixationpoint of f .Given the collected speech and gaze fixations, webuild parallel speech-gaze data set as follows.
Foreach spoken utterance and its accompanying gazefixations, we construct a pair of word sequence andentity sequence (w, e).
The word sequence w con-sists of only nouns and adjectives in the utterance.Each gaze fixation results in a fixated entity in theentity sequence e. When multiple entities are fix-ated by one gaze fixation due to the overlapping ofthe entities, the forefront one is chosen.
Also, wemerge the neighboring gaze fixations that containthe same fixated entities.
For the parallel speech andgaze streams shown in Figure 2, the resulting wordsequence is w = [room chandelier] and the entitysequence is e = [bed frame door chandelier].4 Translation Models for Automatic WordAcquisitionSince we are working on conversational systemswhere users interact with a visual scene, we considerthe task of word acquisition as associating wordswith visual entities in the domain.
Given the par-allel speech and gaze fixated entities {(w, e)}, weformulate word acquisition as a translation problemand use translation models to estimate word-entityassociation probabilities p(w|e).
The words with thehighest association probabilities are chosen as ac-quired words for entity e.4.1 Base Model IUsing the translation model I (Brown et al, 1993),where each word is equally likely to be aligned witheach entity, we havep(w|e) =1(l + 1)mm?j=1l?i=0p(wj |ei) (1)where l and m are the lengths of entity and wordsequences respectively.
This is the model used in(Liu et al, 2007) and (Yu and Ballard, 2004).
Werefer to this model as Model-1 throughout the restof this paper.4.2 Base Model IIUsing the translation model II (Brown et al, 1993),where alignments are dependent on word/entity po-sitions and word/entity sequence lengths, we havep(w|e) =m?j=1l?i=0p(aj = i|j,m, l)p(wj |ei) (2)where aj = i means that wj is aligned with ei.When aj = 0, wj is not aligned with any entity (e0represents a null entity).
We refer to this model asModel-2.Compared to Model-1, Model-2 considers the or-dering of words and entities in word acquisition.EM algorithms are used to estimate the probabilitiesp(w|e) in the translation models.5 Using Speech-Gaze TemporalInformation for Word AcquisitionIn Model-2, word-entity alignments are estimatedfrom co-occurring word and entity sequences in an246unsupervised way.
The estimated alignments are de-pendent on where the words/entities appear in theword/entity sequences, not on when those words andgaze fixated entities actually occur.
Motivated by thefinding that users move their eyes to the mentionedobject directly before speaking a word (Griffin andBock, 2000), we make the word-entity alignmentsdependent on their temporal relation in a new model(referred as Model-2t):p(w|e) =m?j=1l?i=0pt(aj = i|j, e,w)p(wj |ei) (3)where pt(aj = i|j, e,w) is the temporal alignmentprobability computed based on the temporal dis-tance between entity ei and word wj .We define the temporal distance between ei andwj asd(ei, wj) =????
?0 ts(ei) ?
ts(wj) ?
te(ei)te(ei) ?
ts(wj) ts(wj) > te(ei)ts(ei) ?
ts(wj) ts(wj) < ts(ei)(4)where ts(wj) is the starting timestamp (ms) of wordwj , ts(ei) and te(ei) are the starting and endingtimestamps (ms) of gaze fixation on entity ei.The alignment of word wj and entity ei is de-cided by their temporal distance d(ei, wj).
Basedon the psycholinguistic finding that eye gaze hap-pens before a spoken word, wj is not allowed tobe aligned with ei when wj happens earlier than ei(i.e., d(ei, wj) > 0).
When wj happens no earlierthan ei (i.e., d(ei, wj) ?
0), the closer they are, themore likely they are aligned.
Specifically, the tem-poral alignment probability of wj and ei in each co-occurring instance (w, e) is computed aspt(aj = i|j, e,w) ={0 d(ei, wj) > 0exp[?
?d(ei,wj)]?i exp[?
?d(ei,wj)]d(ei, wj) ?
0(5)where ?
is a constant for scaling d(ei, wj).
In ourexperiments, ?
is set to 0.005.An EM algorithm is used to estimate probabilitiesp(w|e) in Model-2t.
?5000 ?4000 ?3000 ?2000 ?1000 0 1000020406080100120140temporal distance of aligned word and entity (ms)alignmentcountFigure 3: Histogram of truly aligned word and entitypairs over temporal distance (bin width = 200ms)For the purpose of evaluation, we manually anno-tated the truly aligned word and entity pairs.
Fig-ure 3 shows the histogram of those truly alignedword and entity pairs over the temporal distance ofaligned word and entity.
We can observe in the fig-ure that 1) almost no eye gaze happens after a spo-ken word, and 2) the number of word-entity pairswith closer temporal distance is generally larger thanthe number of those with farther temporal distance.This is consistent with our modeling of the tempo-ral alignment probability of word and entity (Equa-tion (5)).6 Using Domain Semantic Relatedness forWord AcquisitionSpeech-gaze temporal alignment and occurrencestatistics sometimes are not sufficient to associatewords to an entity correctly.
For example, supposea user says ?there is a lamp on the dresser?
whilelooking at a lamp object on a table object.
Dueto their co-occurring with the lamp object, wordsdresser and lamp are both likely to be associatedwith the lamp object in the translation models.
Asa result, word dresser is likely to be incorrectly ac-quired for the lamp object.
For the same reason, theword lamp could be acquired incorrectly for the ta-ble object.
To solve this type of association prob-lem, the semantic knowledge about the domain andwords can be helpful.
For example, the knowledgethat the word lamp is more semantically related tothe object lamp can help the system avoid associat-247ing the word dresser to the lamp object.
Therefore,we are interested in investigating the use of semanticknowledge in word acquisition.On one hand, each conversational system has adomain model, which is the knowledge representa-tion about its domain such as the types of objectsand their properties and relations.
On the other hand,there are available resources about domain indepen-dent lexical knowledge (e.g., WordNet (Fellbaum,1998)).
The question is whether we can utilize thedomain model and external lexical knowledge re-source to improve word acquisition.
To address thisquestion, we link the domain concepts in the domainmodel with WordNet concepts, and define semanticrelatedness of word and entity to help the system ac-quire domain semantically compatible words.In the following sections, we first describe ourdomain modeling, then define the semantic related-ness of word and entity based on domain modelingand WordNet semantic lexicon, and finally describedifferent ways of using the semantic relatedness ofword and entity to help word acquisition.6.1 Domain ModelingWe model the 3D room decoration domain as shownin Figure 4.
The domain model contains all do-main related semantic concepts.
These concepts arelinked to the WordNet concepts (i.e., synsets in theformat of ?word#part-of-speech#sense-id?).
Each ofthe entities in the domain has one or more properties(e.g., semantic type, color, size) that are denoted bydomain concepts.
For example, the entity dresser 1has domain concepts SEM DRESSER and COLOR.These domain concepts are linked to ?dresser#n#4?and ?color#n#1?
in WordNet.Note that in the domain model, the domain con-cepts are not specific to a certain entity, they are gen-eral concepts for a certain type of entity.
Multipleentities of the same type have the same propertiesand share the same set of domain concepts.6.2 Semantic Relatedness of Word and EntityWe compute the semantic relatedness of a word wand an entity e based on the semantic similarity be-tween w and the properties of e. Specifically, se-mantic relatedness SR(e, w) is defined asSR(e, w) = maxi,jsim(s(cie), sj(w)) (6)?bed#n#1??picture#n#2?
?size#n#1??color#n#1?
?dresser#n#4?COLORbed_framedresser_1SIZESEM_DRESSER SEM_BED COLOREntities:Domainconcepts:WordNetconcepts:Dom ain ModelFigure 4: Domain model with domain concepts linked toWordNet synsetswhere cie is the i-th property of entity e, s(cie) is thesynset of property cie as designed in domain model,sj(w) is the j-th synset of word w as defined inWordNet, and sim(?, ?)
is the similarity score of twosynsets.We computed the similarity score of two synsetsbased on the path length between them.
The similar-ity score is inversely proportional to the number ofnodes along the shortest path between the synsets asdefined in WordNet.
When the two synsets are thesame, they have the maximal similarity score of 1.The WordNet-Similarity tool (Pedersen et al, 2004)was used for the synset similarity computation.6.3 Word Acquisition with Word-EntitySemantic RelatednessWe can use the semantic relatedness of word andentity to help the system acquire semantically com-patible words for each entity, and therefore improveword acquisition performance.
The semantic relat-edness can be applied for word acquisition in twoways: post process learned word-entity associationprobabilities by rescoring them with semantic relat-edness, or directly affect the learning of word-entityassociations by constraining the alignment of wordand entity in the translation models.6.3.1 Rescoring with semantic relatednessIn the acquired word list for an entity ei, eachword wj has an association probability p(wj |ei) thatis learned from a translation model.
We use the248semantic relatedness SR(ei, wj) to redistribute theprobability mass for each wj .
The new associationprobability is given by:p?
(wj |ei) =p(wj |ei)SR(ei, wj)?j p(wj |ei)SR(ei, wj)(7)6.3.2 Semantic alignment constraint intranslation modelWhen used to constrain the word-entity alignmentin the translation model, semantic relatedness can beused alone or used together with speech-gaze tempo-ral information to decide the alignment probabilityof word and entity.?
Using only semantic relatedness to constrainword-entity alignments in Model-2s, we havep(w|e) =m?j=1l?i=0ps(aj = i|j, e,w)p(wj |ei)(8)where ps(aj = i|j, e,w) is the alignment prob-ability based on semantic relatedness,ps(aj = i|j, e,w) =SR(ei, wj)?i SR(ei, wj)(9)?
Using semantic relatedness and temporal infor-mation to constrain word-entity alignments inModel-2ts, we havep(w|e) =m?j=1l?i=0pts(aj = i|j, e,w)p(wj |ei)(10)where pts(aj = i|j, e,w) is the alignmentprobability that is decided by both temporal re-lation and semantic relatedness of ei and wj ,pts(aj = i|j, e,w) =ps(aj = i|j, e,w)pt(aj = i|j, e,w)?i ps(aj = i|j, e,w)pt(aj = i|j, e,w)(11)where ps(aj = i|j, e,w) is the semantic align-ment probability in Equation (9), and pt(aj =i|j, e,w) is the temporal alignment probabilitygiven in Equation (5).EM algorithms are used to estimate p(w|e) inModel-2s and Model-2ts.7 Grounding Words to Domain ConceptsAs discussed above, based on translation models, wecan incorporate temporal and domain semantic in-formation to obtain p(w|e).
This probability onlyprovides a means to ground words to entities.
Inconversational systems, the ultimate goal of wordacquisition is to make the system understand the se-mantic meaning of new words.
Word acquisition bygrounding words to objects is not always sufficientfor identifying their semantic meanings.
Supposethe word green is grounded to a green chair object,so is the word chair.
Although the system is awarethat green is some word describing the green chair,it does not know that word green refers to the chair?scolor while the word chair refers to the chair?s se-mantic type.
Thus, after learning the word-entity as-sociations p(w|e) by the translation models, we needto further ground words to domain concepts of entityproperties.We further apply WordNet to ground words to do-main concepts.
For each entity e, based on asso-ciation probabilities p(w|e), we can choose the n-best words as acquired words for e. Those n-bestwords have the n highest association probabilities.For each word w acquired for e, the grounded con-cept c?e forw is chosen as the one that has the highestsemantic relatedness with w:c?e = argmaxi[maxjsim(s(cie), sj(w))](12)where sim(s(cie), sj(w)) is the semantic similarityscore defined in Equation (6).8 EvaluationWe evaluate word acquisition performance of differ-ent models on the data collected from our user stud-ies (see Section 3).8.1 Evaluation MetricsThe following metrics are used to evaluate the wordsacquired for domain concepts (i.e., entity properties){cie}.?
Precision?e?i # words correctly acquired for cie?e?i # words acquired for cie249?
Recall?e?i # words correctly acquired for cie?e?i # ground-truth1 words of cie?
F-measure2 ?
precision ?
recallprecision + recallThe metrics of precision, recall, and F-measureare based on the n-best words acquired for the entityproperties.
Therefore, we have different precision,recall, and F-measure when n changes.The metrics of precision, recall, and F-measureonly provide evaluation on the top n candidatewords.
To measure the acquisition performance onthe entire ranked list of candidate words, we definea new metric as follows:?
Mean Reciprocal Rank Rate (MRRR)MRRR =?e?Nei=11index(wie)?Nei=11i#ewhere Ne is the number of all ground-truthwords {wie} for entity e, index(wie) is the in-dex of word wie in the ranked list of candidatewords for entity e.Entities may have a different number of ground-truth words.
For each entity e, we calculate a Recip-rocal Rank Rate (RRR), which measures how closethe ranks of the ground-truth words in the candidateword list is to the best scenario where the top Newords are the ground-truth words for e. RRR is inthe range of (0, 1].
The higher the RRR, the betteris the word acquisition performance.
The average ofRRRs across all entities gives the Mean ReciprocalRank Rate (MRRR).Note that MRRR is directly based on the learnedword-entity associations p(w|e), it is in fact a mea-sure of grounding words to entities.1The ground-truth words were compiled and agreed upon bytwo human judges.8.2 Evaluation ResultsTo compare the effects of different speech-gazealignments on word acquisition, we evaluate the fol-lowing models:?
Model-1 ?
base model I without word-entityalignment (Equation (1)).?
Model-2 ?
base model II with positional align-ment (Equation (2)).?
Model-2t ?
enhanced model with temporalalignment (Equation (3)).?
Model-2s ?
enhanced model with semanticalignment (Equation (8)).?
Model-2ts ?
enhanced model with both tempo-ral and semantic alignment (Equation (10)).To compare the different ways of incorporatingsemantic relatedness in word acquisition as dis-cussed in Section 6.3.1, we also evaluate the follow-ing models:?
Model-1-r ?Model-1 with semantic relatednessrescoring of word-entity association.?
Model-2t-r ?
Model-2t with semantic related-ness rescoring of word-entity association.Figure 5 shows the results of models with differ-ent speech-gaze alignments.
Figure 6 shows the re-sults of models with semantic relatedness rescoring.In Figure 5 & 6, n-best means the top n word candi-dates are chosen as acquired words for each entity.The Mean Reciprocal Rank Rates of all models arecompared in Figure 7.8.2.1 Results of using different speech-gazealignmentsAs shown in Figure 5, Model-2 does not show aconsistent improvement compared to Model-1 whena different number of n-best words are chosen as ac-quired words.
This result shows that it is not veryhelpful to consider the index-based positional align-ment of word and entity for word acquisition.Figure 5 also shows that models consideringtemporal or/and semantic information (Model-2t,Model-2s, Model-2ts) consistently perform betterthan the models considering neither temporal nor2501 2 3 4 5 6 7 8 9 100.10.20.30.40.50.60.70.80.9n?bestprecisionModel?1Model?2Model?2tModel?2sModel?2ts(a) precision1 2 3 4 5 6 7 8 9 100.10.20.30.40.50.6n?bestrecallModel?1Model?2Model?2tModel?2sModel?2ts(b) recall1 2 3 4 5 6 7 8 9 100.20.250.30.350.40.450.50.55n?bestF?measureModel?1Model?2Model?2tModel?2sModel?2ts(c) F-measureFigure 5: Performance of word acquisition when different types of speech-gaze alignment are applied1 2 3 4 5 6 7 8 9 100.10.20.30.40.50.60.70.80.9n?bestprecisionModel?1Model?2tModel?1?rModel?2t?r(a) precision1 2 3 4 5 6 7 8 9 100.10.20.30.40.50.6n?bestrecallModel?1Model?2tModel?1?rModel?2t?r(b) recall1 2 3 4 5 6 7 8 9 100.20.250.30.350.40.450.50.55n?bestF?measureModel?1Model?2tModel?1?rModel?2t?r(c) F-measureFigure 6: Performance of word acquisition when semantic relatedness rescoring of word-entity association is appliedM?1 M?2 M?2t M?2s M?2ts M?1?r M?2t?r0.50.550.60.650.70.750.8ModelsMean Reciprocal Rank RateFigure 7: MRRRs achieved by different modelssemantic information (Model-1, Model-2).
AmongModel-2t, Model-2s, and Model-2ts, it is found thatthey do not make consistent differences.As shown in Figure 7, the MRRRs of differentmodels are consistent with their performances on F-measure.
A t-test has shown that the difference be-tween the MRRRs of Model-1 and Model-2 is notstatistically significant.
Compared to Model-1, t-tests have confirmed that MRRR is significantly im-proved by Model-2t (t = 2.27, p < 0.02), Model-2s(t = 3.40, p < 0.01), and Model-2ts(t = 2.60, p <0.01).
T-tests have shown no significant differencesamong Model-2t, Model-2s, and Model-2ts.8.2.2 Results of applying semantic relatednessrescoringFigure 6 shows that semantic relatedness rescor-ing improves word acquisition.
After semantic re-latedness rescoring of the word-entity associationslearned by Model-1, Model-1-r improves the F-measure consistently when a different number ofn-best words are chosen as acquired words.
Com-pared to Model-2t, Model-2t-r also improves the F-measure consistently.Comparing the two ways of using semantic relat-edness for word acquisition, it is found that rescor-ing word-entity association with semantic related-ness works better.
When semantic relatedness isused together with temporal information to constrainword-entity alignments in Model-2ts, word acqui-251Model Rank 1 Rank 2 Rank 3 Rank 4 Rank 5M-1 table(0.173) dresser(0.067) area(0.058) picture(0.053) dressing(0.041)M-2t table(0.146) dresser(0.125) dressing(0.061) vanity(0.051) fact(0.050)M-2t-r table(0.312) dresser(0.241) vanity(0.149) desk(0.047) area(0.026)Table 1: N-best candidate words acquired for the entity dresser 1 by different modelssition performance is not improved compared toModel-2t.
However, using semantic relatedness torescore word-entity association learned by Model-2t, Model-2t-r further improves word acquisition.As shown in Figure 7, the MRRRs of Model-1-r and Model-2t-r are consistent with their per-formances on F-measure.
Compared to Model-2t,Model-2t-r improves MRRR.
A t-test has confirmedthat this is a significant improvement (t = 1.97, p <0.03).
Compared to Model-1, Model-1-r signifi-cantly improves MRRR (t = 2.33, p < 0.02).
Thereis no significant difference between Model-1-r andModel-2t/Model-2s/Model-2ts.In Figures 5&6, we also notice that the recallof the acquired words is still comparably low evenwhen 10 best word candidates are chosen for eachentity.
This is mainly due to the scarcity of thosewords that are not acquired in the data.
Many ofthe words that are not acquired appear less than 3times in the data, which makes them unlikely to beassociated with any entity by the translation models.When more data is available, we expect to see higherrecall.8.3 An ExampleTable 1 shows the 5-best words acquired by differentmodels for the entity dresser 1 in the 3d room scene(see Figure 1).
In the table, each word is followed byits word-entity association probability p(w|e).
Thecorrectly acquired words are shown in bold font.As shown in the example, the baseline Model-1learned 2 correct words in the 5-best list.
Consid-ering speech-gaze temporal information, Model-2tlearned one more correct word vanity in the 5-bestlist.
With semantic relatedness rescoring, Model-2t-r further acquired word desk in the 5-best listbecause of the high semantic relatedness of worddesk and the type of entity dresser 1.
Although nei-ther Model-1 nor Model-2t successfully acquired theword desk in the 5-best list, the rank (=7) of the worddesk in Model-2t?s n-best list is much higher than therank (=21) in Model-1?s n-best list.9 ConclusionMotivated by the psycholinguistic findings, we in-vestigate the use of eye gaze for automatic word ac-quisition in multimodal conversational systems.
Par-ticularly, we investigate the use of speech-gaze tem-poral information and word-entity semantic related-ness to facilitate word acquisition.
Our experimentsshow that word acquisition is significantly improvedwhen temporal information is considered, which isconsistent with the previous psycholinguistic find-ings about speech and eye gaze.
Moreover, usingtemporal information together with semantic relat-edness rescoring further improves word acquisition.Eye tracking systems are no longer bulky sys-tems that prevent natural human machine commu-nication.
Display mounted gaze tracking systems(e.g., Tobii) are completely non-intrusive, can toler-ate head motion, and provide high tracking quality.Integrating eye tracking with conversational inter-faces is no longer beyond reach.
Recent works haveshown that eye gaze can facilitate spoken languageprocessing in conversational systems (Qu and Chai,2007; Prasov and Chai, 2008).
Incorporating eyegaze with automatic word acquisition provides an-other potential approach to improve the robustnessof human machine conversation.AcknowledgmentsThis work was supported by IIS-0347548 and IIS-0535112 from the National Science Foundation.The authors would like to thank Zahar Prasov for hiscontribution on data collection.
The authors wouldalso like to thank anonymous reviewers for theirvaluable comments and suggestions.ReferencesKobus Barnard, Pinar Duygulu, Nando de Freitas, DavidForsyth, David Blei, and Michael I. Jordan.
2003.252Matching words and pictures.
Journal of MachineLearning Research, 3:1107?1135.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Zenzi M. Griffin and Kathryn Bock.
2000.
What the eyessay about speaking.
Psychological Science, 11:274?279.JohnM.
Henderson and Fernanda Ferreira, editors.
2004.The interface of language, vision, and action: Eyemovements and the visual world.
New York: Taylor& Francis.Marcel A.
Just and Patricia A. Carpenter.
1976.
Eye fix-ations and cognitive processes.
Cognitive Psychology,8:441?480.Yi Liu, Joyce Y. Chai, and Rong Jin.
2007.
Au-tomated vocabulary acquisition and interpretation inmultimodal conversational systems.
In Proceedings ofthe 45th Annual Meeting of the Association of Compu-tational Linguistics (ACL).E.
Matin.
1974.
Saccadic suppression: a review and ananalysis.
Psychological Bulletin, 81:899?917.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity - measuring the relat-edness of concepts.
In Proceedings of the NineteenthNational Conference on Artificial Intelligence (AAAI-04).Zahar Prasov and Joyce Y. Chai.
2008.
What?s in agaze?
the role of eye-gaze in reference resolution inmultimodal conversational interfaces.
In Proceedingsof ACM 12th International Conference on IntelligentUser interfaces (IUI).Shaolin Qu and Joyce Y. Chai.
2007.
An explorationof eye gaze in spoken language processing for multi-modal conversational interfaces.
In Proceedings of theConference of the North America Chapter of the Asso-ciation of Computational Linguistics (NAACL).Deb K. Roy and Alex P. Pentland.
2002.
Learning wordsfrom sights and sounds, a computational model.
Cog-nitive Science, 26(1):113?146.Michael K. Tanenhaus, Michael J. Spivey-Knowiton,Kathleen M. Eberhard, and Julie C. Sedivy.
1995.
In-tegration of visual and linguistic information in spokenlanguage comprehension.
Science, 268:1632?1634.Chen Yu and Dana H. Ballard.
2004.
A multimodallearning interface for grounding spoken language insensory perceptions.
ACM Transactions on AppliedPerceptions, 1(1):57?80.253
