Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880?887,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Re-examination of Machine Learning Approachesfor Sentence-Level MT EvaluationJoshua S. Albrecht and Rebecca HwaDepartment of Computer ScienceUniversity of Pittsburgh{jsa8,hwa}@cs.pitt.eduAbstractRecent studies suggest that machine learn-ing can be applied to develop good auto-matic evaluation metrics for machine trans-lated sentences.
This paper further ana-lyzes aspects of learning that impact per-formance.
We argue that previously pro-posed approaches of training a Human-Likeness classifier is not as well correlatedwith human judgments of translation qual-ity, but that regression-based learning pro-duces more reliable metrics.
We demon-strate the feasibility of regression-basedmetrics through empirical analysis of learn-ing curves and generalization studies andshow that they can achieve higher correla-tions with human judgments than standardautomatic metrics.1 IntroductionAs machine translation (MT) research advances, theimportance of its evaluation also grows.
Efficientevaluation methodologies are needed both for facili-tating the system development cycle and for provid-ing an unbiased comparison between systems.
Tothis end, a number of automatic evaluation metricshave been proposed to approximate human judg-ments of MT output quality.
Although studies haveshown them to correlate with human judgments atthe document level, they are not sensitive enoughto provide reliable evaluations at the sentence level(Blatz et al, 2003).
This suggests that current met-rics do not fully reflect the set of criteria that peopleuse in judging sentential translation quality.A recent direction in the development of met-rics for sentence-level evaluation is to apply ma-chine learning to create an improved composite met-ric out of less indicative ones (Corston-Oliver et al,2001; Kulesza and Shieber, 2004).
Under the as-sumption that good machine translation will pro-duce ?human-like?
sentences, classifiers are trainedto predict whether a sentence is authored by a humanor by a machine based on features of that sentence,which may be the sentence?s scores from individ-ual automatic evaluation metrics.
The confidence ofthe classifier?s prediction can then be interpreted as ajudgment on the translation quality of the sentence.Thus, the composite metric is encoded in the confi-dence scores of the classification labels.While the learning approach to metric design of-fers the promise of ease of combining multiple met-rics and the potential for improved performance,several salient questions should be addressed morefully.
First, is learning a ?Human Likeness?
classi-fier the most suitable approach for framing the MT-evaluation question?
An alternative is regression, inwhich the composite metric is explicitly learned asa function that approximates humans?
quantitativejudgments, based on a set of human evaluated train-ing sentences.
Although regression has been con-sidered on a small scale for a single system as con-fidence estimation (Quirk, 2004), this approach hasnot been studied as extensively due to scalability andgeneralization concerns.
Second, how does the di-versity of the model features impact the learned met-ric?
Third, how well do learning-based metrics gen-eralize beyond their training examples?
In particu-lar, how well can a metric that was developed based880on one group of MT systems evaluate the translationqualities of new systems?In this paper, we argue for the viability of aregression-based framework for sentence-level MT-evaluation.
Through empirical studies, we firstshow that having an accurate Human-Likeness clas-sifier does not necessarily imply having a good MT-evaluation metric.
Second, we analyze the resourcerequirement for regression models for different sizesof feature sets through learning curves.
Finally, weshow that SVM-regression metrics generalize betterthan SVM-classification metrics in their evaluationof systems that are different from those in the train-ing set (by languages and by years), and their corre-lations with human assessment are higher than stan-dard automatic evaluation metrics.2 MT EvaluationRecent automatic evaluation metrics typically framethe evaluation problem as a comparison task: howsimilar is the machine-produced output to a set ofhuman-produced reference translations for the samesource text?
However, as the notion of similar-ity is itself underspecified, several different fami-lies of metrics have been developed.
First, simi-larity can be expressed in terms of string edit dis-tances.
In addition to the well-known word errorrate (WER), more sophisticated modifications havebeen proposed (Tillmann et al, 1997; Snover etal., 2006; Leusch et al, 2006).
Second, similar-ity can be expressed in terms of common word se-quences.
Since the introduction of BLEU (Papineniet al, 2002) the basic n-gram precision idea hasbeen augmented in a number of ways.
Metrics in theRouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para-phrasing into account; metrics such as METEOR(Banerjee and Lavie, 2005) and GTM (Melamed etal., 2003) calculate both recall and precision; ME-TEOR is also similar to SIA (Liu and Gildea, 2006)in that word class information is used.
Finally, re-searchers have begun to look for similarities at adeeper structural level.
For example, Liu and Gildea(2005) developed the Sub-Tree Metric (STM) overconstituent parse trees and the Head-Word ChainMetric (HWCM) over dependency parse trees.With this wide array of metrics to choose from,MT developers need a way to evaluate them.
Onepossibility is to examine whether the automatic met-ric ranks the human reference translations highlywith respect to machine translations (Lin and Och,2004b; Amigo?
et al, 2006).
The reliability of ametric can also be more directly assessed by de-termining how well it correlates with human judg-ments of the same data.
For instance, as a part of therecent NIST sponsored MT Evaluation, each trans-lated sentence by participating systems is evaluatedby two (non-reference) human judges on a five pointscale for its adequacy (does the translation retain themeaning of the original source text?)
and fluency(does the translation sound natural in the target lan-guage?).
These human assessment data are an in-valuable resource for measuring the reliability of au-tomatic evaluation metrics.
In this paper, we showthat they are also informative in developing bettermetrics.3 MT Evaluation with Machine LearningA good automatic evaluation metric can be seen asa computational model that captures a human?s de-cision process in making judgments about the ade-quacy and fluency of translation outputs.
Inferring acognitive model of human judgments is a challeng-ing problem because the ultimate judgment encom-passes a multitude of fine-grained decisions, and thedecision process may differ slightly from person toperson.
The metrics cited in the previous sectionaim to capture certain aspects of human judgments.One way to combine these metrics in a uniform andprincipled manner is through a learning framework.The individual metrics participate as input features,from which the learning algorithm infers a compos-ite metric that is optimized on training examples.Reframing sentence-level translation evaluationas a classification task was first proposed byCorston-Oliver et al (2001).
Interestingly, insteadof recasting the classification problem as a ?Hu-man Acceptability?
test (distinguishing good trans-lations outputs from bad one), they chose to developa Human-Likeness classifier (distinguishing out-puts seem human-produced from machine-producedones) to avoid the necessity of obtaining manu-ally labeled training examples.
Later, Kulesza andShieber (2004) noted that if a classifier provides a881confidence score for its output, that value can beinterpreted as a quantitative estimate of the inputinstance?s translation quality.
In particular, theytrained an SVM classifier that makes its decisionsbased on a set of input features computed from thesentence to be evaluated; the distance between inputfeature vector and the separating hyperplane thenserves as the evaluation score.
The underlying as-sumption for both is that improving the accuracy ofthe classifier on the Human-Likeness test will alsoimprove the implicit MT evaluation metric.A more direct alternative to the classification ap-proach is to learn via regression and explicitly op-timize for a function (i.e.
MT evaluation metric)that approximates human judgments in training ex-amples.
Kulesza and Shieber (2004) raised twomain objections against regression for MT evalua-tions.
One is that regression requires a large set oflabeled training examples.
Another is that regressionmay not generalize well over time, and re-trainingmay become necessary, which would require col-lecting additional human assessment data.
Whilethese are legitimate concerns, we show through em-pirical studies (in Section 4.2) that the additional re-source requirement is not impractically high, andthat a regression-based metric has higher correla-tions with human judgments and generalizes betterthan a metric derived from a Human-Likeness clas-sifier.3.1 Relationship between Classification andRegressionClassification and regression are both processes offunction approximation; they use training examplesas sample instances to learn the mapping from in-puts to the desired outputs.
The major difference be-tween classification and regression is that the func-tion learned by a classifier is a set of decision bound-aries by which to classify its inputs; thus its outputsare discrete.
In contrast, a regression model learnsa continuous function that directly maps an inputto a continuous value.
An MT evaluation metric isinherently a continuous function.
Casting the taskas a 2-way classification may be too coarse-grained.The Human-Likeness formulation of the problem in-troduces another layer of approximation by assum-ing equivalence between ?Like Human-Produced?and ?Well-formed?
sentences.
In Section 4.1, weshow empirically that high accuracy in the Human-Likeness test does not necessarily entail good MTevaluation judgments.3.2 Feature RepresentationTo ascertain the resource requirements for differentmodel sizes, we considered two feature models.
Thesmaller one uses the same nine features as Kuleszaand Shieber, which were derived from BLEU andWER.
The full model consists of 53 features: someare adapted from recently developed metrics; othersare new features of our own.
They fall into the fol-lowing major categories1:String-based metrics over references These in-clude the nine Kulesza and Shieber features as wellas precision, recall, and fragmentation, as calcu-lated in METEOR; ROUGE-inspired features thatare non-consecutive bigrams with a gap size of m,where 1 ?
m ?
5 (skip-m-bigram), and ROUGE-L(longest common subsequence).Syntax-based metrics over references We un-rolled HWCM into their individual chains of lengthc (where 2 ?
c ?
4); we modified STM so that it iscomputed over unlexicalized constituent parse treesas well as over dependency parse trees.String-based metrics over corpus Features inthis category are similar to those in String-basedmetric over reference except that a large English cor-pus is used as ?reference?
instead.Syntax-based metrics over corpus A large de-pendency treebank is used as the ?reference?
insteadof parsed human translations.
In addition to adap-tations of the Syntax-based metrics over references,we have also created features to verify the argumentstructures for certain syntactic categories.4 Empirical StudiesIn these studies, the learning models used for bothclassification and regression are support vector ma-chines (SVM) with Gaussian kernels.
All modelsare trained with SVM-Light (Joachims, 1999).
Ourprimary experimental dataset is from NIST?s 20031As feature engineering is not the primary focus of this pa-per, the features are briefly described here, but implementa-tional details will be made available in a technical report.882Chinese MT Evaluations, in which the fluency andadequacy of 919 sentences produced by six MT sys-tems are scored by two human judges on a 5-pointscale2.
Because the judges evaluate sentences ac-cording to their individual standards, the resultingscores may exhibit a biased distribution.
We normal-ize human judges?
scores following the process de-scribed by Blatz et al (2003).
The overall human as-sessment score for a translation output is the averageof the sum of two judges?
normalized fluency andadequacy scores.
The full dataset (6 ?
919 = 5514instances) is split into sets of training, heldout andtest data.
Heldout data is used for parameter tuning(i.e., the slack variable and the width of the Gaus-sian).
When training classifiers, assessment scoresare not used, and the training set is augmented withall available human reference translation sentences(4 ?
919 = 3676 instances) to serve as positive ex-amples.To judge the quality of a metric, we computeSpearman rank-correlation coefficient, which is areal number ranging from -1 (indicating perfect neg-ative correlations) to +1 (indicating perfect posi-tive correlations), between the metric?s scores andthe averaged human assessments on test sentences.We use Spearman instead of Pearson because itis a distribution-free test.
To evaluate the rela-tive reliability of different metrics, we use boot-strapping re-sampling and paired t-test to determinewhether the difference between the metrics?
correla-tion scores has statistical significance (at 99.8% con-fidence level)(Koehn, 2004).
Each reported correla-tion rate is the average of 1000 trials; each trial con-sists of n sampled points, where n is the size of thetest set.
Unless explicitly noted, the qualitative dif-ferences between metrics we report are statisticallysignificant.
As a baseline comparison, we report thecorrelation rates of three standard automatic metrics:BLEU, METEOR, which incorporates recall andstemming, and HWCM, which uses syntax.
BLEUis smoothed to be more appropriate for sentence-level evaluation (Lin and Och, 2004b), and the bi-gram versions of BLEU and HWCM are reportedbecause they have higher correlations than whenlonger n-grams are included.
This phenomenon has2This corpus is available from the Linguistic Data Consor-tium as Multiple Translation Chinese Part 4.00.050.10.150.20.250.30.350.445  50  55  60  65  70  75  80  85Correlation Coefficient withHuman Judgement(R)Human-Likeness Classifier Accuracy (%)Figure 1: This scatter plot compares classifiers?
ac-curacy with their corresponding metrics?
correla-tions with human assessmentsbeen previously observed by Liu and Gildea (2005).4.1 Relationship between ClassificationAccuracy and Quality of Evaluation MetricA concern in using a metric derived from a Human-Likeness classifier is whether it would be predic-tive for MT evaluation.
Kulesza and Shieber (2004)tried to demonstrate a positive correlation betweenthe Human-Likeness classification task and the MTevaluation task empirically.
They plotted the clas-sification accuracy and evaluation reliability for anumber of classifiers, which were generated as apart of a greedy search for kernel parameters andfound some linear correlation between the two.
Thisproof of concept is a little misleading, however, be-cause the population of the sampled classifiers wasbiased toward those from the same neighborhood asthe local optimal classifier (so accuracy and corre-lation may only exhibit linear relationship locally).Here, we perform a similar study except that wesampled the kernel parameter more uniformly (ona log scale).
As Figure 1 confirms, having an ac-curate Human-Likeness classifier does not necessar-ily entail having a good MT evaluation metric.
Al-though the two tasks do seem to be positively re-lated, and in the limit there may be a system that isgood at both tasks, one may improve classificationwithout improving MT evaluation.
For this set ofheldout data, at the near 80% accuracy range, a de-rived metric might have an MT evaluation correla-tion coefficient anywhere between 0.25 (on par with883unsmoothed BLEU, which is known to be unsuitablefor sentence-level evaluation) and 0.35 (competitivewith standard metrics).4.2 Learning CurvesTo investigate the feasibility of training regressionmodels from assessment data that are currentlyavailable, we consider both a small and a largeregression model.
The smaller model consists ofnine features (same as the set used by Kulesza andShieber); the other uses the full set of 53 featuresas described in Section 3.2.
The reliability of thetrained metrics are compared with those developedfrom Human-Likeness classifiers.
We follow a sim-ilar training and testing methodology as previousstudies: we held out 1/6 of the assessment dataset forSVM parameter tuning; five-fold cross validation isperformed with the remaining sentences.
Althoughthe metrics are evaluated on unseen test sentences,the sentences are produced by the same MT systemsthat produced the training sentences.
In later exper-iments, we investigate generalizing to more distantMT systems.Figure 2(a) shows the learning curves for the tworegression models.
As the graph indicates, evenwith a limited amount of human assessment data,regression models can be trained to be comparableto standard metrics (represented by METEOR in thegraph).
The small feature model is close to conver-gence after 1000 training examples3.
The modelwith a more complex feature set does require moretraining data, but its correlation began to overtakeMETEOR after 2000 training examples.
This studysuggests that the start-up cost of building even amoderately complex regression model is not impos-sibly high.Although we cannot directly compare the learningcurves of the Human-Likeness classifiers to those ofthe regression models (since the classifier?s trainingexamples are automatically labeled), training exam-ples for classifiers are not entirely free: human ref-erence translations still must be developed for thesource sentences.
Figure 2(c) shows the learningcurves for training Human-Likeness classifiers (interms of improving a classifier?s accuracy) using thesame two feature sets, and Figure 2(b) shows the3The total number of labeled examples required is closer to2000, since the heldout set uses 919 labeled examples.correlations of the metrics derived from the corre-sponding classifiers.
The pair of graphs show, es-pecially in the case of the larger feature set, that alarge improvement in classification accuracy doesnot bring proportional improvement in its corre-sponding metrics?s correlation; with an accuracy ofnear 90%, its correlation coefficient is 0.362, wellbelow METEOR.This experiment further confirms that judgingHuman-Likeness and judging Human-Acceptabilityare not tightly coupled.
Earlier, we have shown inFigure 1 that different SVM parameterizations mayresult in classifiers with the same accuracy rate butdifferent correlations rates.
As a way to incorpo-rate some assessment information into classificationtraining, we modify the parameter tuning process sothat SVM parameters are chosen to optimize for as-sessment correlations in the heldout data.
By incur-ring this small amount of human assessed data, thisparameter search improves the classifier?s correla-tions: the metric using the smaller feature set in-creased from 0.423 to 0.431, and that of the largerset increased from 0.361 to 0.422.4.3 GeneralizationWe conducted two generalization studies.
The firstinvestigates how well the trained metrics evaluatesystems from other years and systems developedfor a different source language.
The second studydelves more deeply into how variations in the train-ing examples affect a learned metric?s ability to gen-eralize to distant systems.
The learning models forboth experiments use the full feature set.Cross-Year Generalization To test how well thelearning-based metrics generalize to systems fromdifferent years, we trained both a regression-basedmetric (R03) and a classifier-based metric (C03)with the entire NIST 2003 Chinese dataset (using20% of the data as heldout4).
All metrics are thenapplied to three new datasets: NIST 2002 ChineseMT Evaluation (3 systems, 2634 sentences total),NIST 2003 Arabic MT Evaluation (2 systems, 1326sentences total), and NIST 2004 Chinese MT Evalu-ation (10 systems, 4470 sentences total).
The results4Here, too, we allowed the classifier?s parameters to betuned for correlation with human assessment on the heldout datarather than accuracy.884(a) (b) (c)Figure 2: Learning curves: (a) correlations with human assessment using regression models; (b) correlationswith human assessment using classifiers; (c) classifier accuracy on determining Human-Likeness.Dataset R03 C03 BLEU MET.
HWCM2002 Ara 0.466 0.384 0.423 0.431 0.4242002 Chn 0.309 0.250 0.269 0.290 0.2602004 Chn 0.602 0.566 0.588 0.563 0.546Table 1: Correlations for cross-year generalization.Learning-based metrics are developed from NIST2003 Chinese data.
All metrics are tested on datasetsfrom 2003 Arabic, 2002 Chinese and 2004 Chinese.are summarized in Table 1.
We see that R03 con-sistently has a better correlation rate than the othermetrics.At first, it may seem as if the difference betweenR03 and BLEU is not as pronounced for the 2004dataset, calling to question whether a learned met-ric might become quickly out-dated, we argue thatthis is not the case.
The 2004 dataset has manymore participating systems, and they span a widerrange of qualities.
Thus, it is easier to achieve ahigh rank correlation on this dataset than previousyears because most metrics can qualitatively discernthat sentences from one MT system are better thanthose from another.
In the next experiment, we ex-amine the performance of R03 with respect to eachMT system in the 2004 dataset and show that its cor-relation rate is higher for better MT systems.Relationship between Training Examples andGeneralization Table 2 shows the result of a gen-eralization study similar to before, except that cor-relations are performed on each system.
The rowsorder the test systems by their translation quali-ties from the best performing system (2004-Chn1,whose average human assessment score is 0.655 outof 1.0) to the worst (2004-Chn10, whose score is0.255).
In addition to the regression metric fromthe previous experiment (R03-all), we consider twomore regression metrics trained from subsets of the2003 dataset: R03-Bottom5 is trained from the sub-set that excludes the best 2003 MT system, and R03-Top5 is trained from the subset that excludes theworst 2003 MT system.We first observe that on a per test-system basis,the regression-based metrics generally have bettercorrelation rates than BLEU, and that the gap is aswide as what we have observed in the earlier cross-years studies.
The one exception is when evaluating2004-Chn8.
None of the metrics seems to correlatevery well with human judges on this system.
Be-cause the regression-based metric uses these individ-ual metrics as features, its correlation also suffers.During regression training, the metric is opti-mized to minimize the difference between its pre-diction and the human assessments of the trainingdata.
If the input feature vector of a test instanceis in a very distant space from training examples,the chance for error is higher.
As seen from theresults, the learned metrics typically perform betterwhen the training examples include sentences fromhigher-quality systems.
Consider, for example, thedifferences between R03-all and R03-Top5 versusthe differences between R03-all and R03-Bottom5.Both R03-Top5 and R03-Bottom5 differ from R03-all by one subset of training examples.
Since R03-all?s correlation rates are generally closer to R03-Top5 than to R03-Bottom5, we see that having seenextra training examples from a bad system is not asharmful as having not seen training examples from agood system.
This is expected, since there are manyways to create bad translations, so seeing a partic-885R03-all R03-Bottom5 R03-Top5 BLEU METEOR HWCM2004-Chn1 0.495 0.460 0.518 0.456 0.457 0.4442004-Chn2 0.398 0.330 0.440 0.352 0.347 0.3442004-Chn3 0.425 0.389 0.459 0.369 0.402 0.3692004-Chn4 0.432 0.392 0.434 0.400 0.400 0.3622004-Chn5 0.452 0.441 0.443 0.370 0.426 0.3262004-Chn6 0.405 0.392 0.406 0.390 0.357 0.3802004-Chn7 0.443 0.432 0.448 0.390 0.408 0.3922004-Chn8 0.237 0.256 0.256 0.265 0.259 0.1792004-Chn9 0.581 0.569 0.591 0.527 0.537 0.5352004-Chn10 0.314 0.313 0.354 0.321 0.303 0.3582004-all 0.602 0.567 0.617 0.588 0.563 0.546Table 2: Metric correlations within each system.
The columns specify which metric is used.
The rowsspecify which MT system is under evaluation; they are ordered by human-judged system quality, from bestto worst.
For each evaluated MT system (row), the highest coefficient in bold font, and those that arestatistically comparable to the highest are shown in italics.ular type of bad translations from one system maynot be very informative.
In contrast, the neighbor-hood of good translations is much smaller, and iswhere all the systems are aiming for; thus, assess-ments of sentences from a good system can be muchmore informative.4.4 DiscussionExperimental results confirm that learning fromtraining examples that have been doubly approx-imated (class labels instead of ordinals, human-likeness instead of human-acceptability) does nega-tively impact the performance of the derived metrics.In particular, we showed that they do not generalizeas well to new data as metrics trained from directregression.We see two lingering potential objections towarddeveloping metrics with regression-learning.
Oneis the concern that a system under evaluation mighttry to explicitly ?game the metric5.?
This is a con-cern shared by all automatic evaluation metrics, andpotential problems in stand-alone metrics have beenanalyzed (Callison-Burch et al, 2006).
In a learningframework, potential pitfalls for individual metricsare ameliorated through a combination of evidences.That said, it is still prudent to defend against the po-tential of a system gaming a subset of the features.For example, our fluency-predictor features are notstrong indicators of translation qualities by them-selves.
We want to avoid training a metric that as-5Or, in a less adversarial setting, a system may be perform-ing minimum error-rate training (Och, 2003)signs a higher than deserving score to a sentence thatjust happens to have many n-gram matches againstthe target-language reference corpus.
This can beachieved by supplementing the current set of hu-man assessed training examples with automaticallyassessed training examples, similar to the labelingprocess used in the Human-Likeness classificationframework.
For instance, as negative training ex-amples, we can incorporate fluent sentences that arenot adequate translations and assign them low over-all assessment scores.A second, related concern is that because the met-ric is trained on examples from current systems us-ing currently relevant features, even though it gener-alizes well in the near term, it may not continue tobe a good predictor in the distant future.
While pe-riodic retraining may be necessary, we see value inthe flexibility of the learning framework, which al-lows for new features to be added.
Moreover, adap-tive learning methods may be applicable if a smallsample of outputs of some representative translationsystems is manually assessed periodically.5 ConclusionHuman judgment of sentence-level translation qual-ity depends on many criteria.
Machine learning af-fords a unified framework to compose these crite-ria into a single metric.
In this paper, we havedemonstrated the viability of a regression approachto learning the composite metric.
Our experimentalresults show that by training from some human as-886sessments, regression methods result in metrics thathave better correlations with human judgments evenas the distribution of the tested population changes.AcknowledgmentsThis work has been supported by NSF Grants IIS-0612791 andIIS-0710695.
We would like to thank Regina Barzilay, RicCrabbe, Dan Gildea, Alex Kulesza, Alon Lavie, and MatthewStone as well as the anonymous reviewers for helpful commentsand suggestions.
We are also grateful to NIST for making theirassessment data available to us.ReferencesEnrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??sMa`rquez.
2006.
MT evaluation: Human-like vs. human ac-ceptable.
In Proceedings of the COLING/ACL 2006 MainConference Poster Sessions, Sydney, Australia, July.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: An auto-matic metric for MT evaluation with improved correlationwith human judgments.
In ACL 2005 Workshop on Intrinsicand Extrinsic Evaluation Measures for Machine Translationand/or Summarization, June.John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,Cyril Goutte, Alex Kulesza, Alberto Sanchis, and NicolaUeffing.
2003.
Confidence estimation for machine trans-lation.
Technical Report Natural Language EngineeringWorkshop Final Report, Johns Hopkins University.Christopher Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEU in machinetranslation research.
In The Proceedings of the ThirteenthConference of the European Chapter of the Association forComputational Linguistics.Simon Corston-Oliver, Michael Gamon, and Chris Brockett.2001.
A machine learning approach to the automatic eval-uation of machine translation.
In Proceedings of the 39thAnnual Meeting of the Association for Computational Lin-guistics, July.Thorsten Joachims.
1999.
Making large-scale SVM learningpractical.
In Bernhard Scho?elkopf, Christopher Burges, andAlexander Smola, editors, Advances in Kernel Methods -Support Vector Learning.
MIT Press.David Kauchak and Regina Barzilay.
2006.
Paraphrasing forautomatic evaluation.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, Main Confer-ence, New York City, USA, June.Philipp Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of the 2004 Confer-ence on Empirical Methods in Natural Language Processing(EMNLP-04).Alex Kulesza and Stuart M. Shieber.
2004.
A learning ap-proach to improving sentence-level MT evaluation.
In Pro-ceedings of the 10th International Conference on Theoreticaland Methodological Issues in Machine Translation (TMI),Baltimore, MD, October.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006.CDER: Efficient MT evaluation using block movements.
InThe Proceedings of the Thirteenth Conference of the Euro-pean Chapter of the Association for Computational Linguis-tics.Chin-Yew Lin and Franz Josef Och.
2004a.
Automatic evalu-ation of machine translation quality using longest commonsubsequence and skip-bigram statistics.
In Proceedings ofthe 42nd Annual Meeting of the Association for Computa-tional Linguistics, July.Chin-Yew Lin and Franz Josef Och.
2004b.
Orange: amethod for evaluating automatic evaluation metrics for ma-chine translation.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING 2004),August.Ding Liu and Daniel Gildea.
2005.
Syntactic features forevaluation of machine translation.
In ACL 2005 Workshopon Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, June.Ding Liu and Daniel Gildea.
2006.
Stochastic iterative align-ment for machine translation evaluation.
In Proceedingsof the Joint Conference of the International Conference onComputational Linguistics and the Association for Com-putational Linguistics (COLING-ACL?2006) Poster Session,July.I.
Dan Melamed, Ryan Green, and Joseph Turian.
2003.
Preci-sion and recall of machine translation.
In In Proceedings ofthe HLT-NAACL 2003: Short Papers, pages 61?63, Edmon-ton, Alberta.Franz Josef Och.
2003.
Minimum error rate training for statis-tical machine translation.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics, Philadel-phia, PA.Christopher Quirk.
2004.
Training a sentence-level machinetranslation confidence measure.
In Proceedings of LREC2004.Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-ciulla, and John Makhoul.
2006.
A study of translation editrate with targeted human annotation.
In Proceedings of the8th Conference of the Association for Machine Translationin the Americas (AMTA-2006).Christoph Tillmann, Stephan Vogel, Hermann Ney, HassanSawaf, and Alex Zubiaga.
1997.
Accelerated DP-basedsearch for statistical translation.
In Proceedings of the 5thEuropean Conference on Speech Communication and Tech-nology (EuroSpeech ?97).887
