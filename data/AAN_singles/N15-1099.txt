Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977?983,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Word Embedding Approach to Predicting the Compositionality ofMultiword ExpressionsBahar Salehi,?Paul Cook?and Timothy Baldwin??
NICTA Victoria Research LaboratoryDepartment of Computing and Information SystemsThe University of MelbourneVictoria 3010, Australia?
Faculty of Computer ScienceUniversity of New BrunswickFredericton, NB E3B 5A3, Canadabsalehi@student.unimelb.edu.au, paul.cook@unb.ca, tb@ldwin.netAbstractThis paper presents the first attempt to useword embeddings to predict the composition-ality of multiword expressions.
We considerboth single- and multi-prototype word em-beddings.
Experimental results show that, incombination with a back-off method basedon string similarity, word embeddings out-perform a method using count-based distribu-tional similarity.
Our best results are com-petitive with, or superior to, state-of-the-artmethods over three standard compositionalitydatasets, which include two types of multi-word expressions and two languages.1 IntroductionMultiword expressions (MWEs) are word combina-tions that display some form of idiomaticity (Bald-win and Kim, 2009), including semantic idiomatic-ity, wherein the semantics of the MWE (e.g.
ivorytower) cannot be predicted from the semantics ofthe component words (e.g.
ivory and tower).
Re-cent NLP work on semantic idiomaticity has focusedon the task of ?compositionality prediction?, in theform of a regression task whereby a given MWE ismapped onto a continuous-valued compositionalityscore, either for the MWE as a whole or for each ofits component words (Reddy et al, 2011; Schulte imWalde et al, 2013; Salehi et al, 2014b).Separately in NLP, there has been a recent surgeof interest in learning distributed representationsof word meaning, in the form of ?word embed-dings?
(Collobert and Weston, 2008; Mikolov et al,2013a) and composition over distributed representa-tions (Socher et al, 2012; Baroni et al, 2014).This paper is the first attempt to bring together thework on word embedding-style distributional analy-sis with compositionality prediction of MWEs.
Inthe context of compositionality prediction, our pri-mary research questions here are:RQ1: Are word embeddings superior to conven-tional count-based models of distributionalsimilarity?RQ2: How sensitive to parameter optimisation aredifferent word embedding approaches?RQ3: Are multi-prototype word embeddings empir-ically superior to single-prototype word em-beddings?We explore these questions relative to three compo-sitionality prediction datasets spanning two MWEconstruction types (noun compounds and verb par-ticle constructions) and two languages (English andGerman), and arrive at the following conclusions:(1) consistent with recent work over other NLPtasks, word embeddings are superior to count?based models of distributional similarity (and alsotranslation-based string similarity); (2) the resultsare relatively stable under parameter optimisationfor a given word embedding learning approach; and(3) based on two simple approaches to composition,single word embeddings are empirically slightly su-perior to multi-prototype word embeddings overall.9772 Related WorkRecent work on distributed approaches to distri-butional semantics has demonstrated their utilityin a wide range of NLP tasks, including identi-fying various morphosyntactic and semantic rela-tions (Mikolov et al, 2013a), dependency parsing(Bansal et al, 2014), sentiment analysis (Socher etal., 2013), named-entity recognition (Collobert andWeston, 2008; Passos et al, 2014), and machinetranslation (Zou et al, 2013; Devlin et al, 2014).Despite the wealth of research applying word em-beddings within NLP, they have not yet been consid-ered for predicting the compositionality of MWEs.Much prior work on MWEs has been tailoredto specific kinds of MWEs in particular languages(e.g.
English verb?noun combinations (Fazly et al,2009)).
There has however been recent interest inapproaches to MWEs that are more broadly applica-ble to a wider range of languages and MWE types(Brooke et al, 2014; Salehi et al, 2014b; Schneideret al, 2014).
Word embeddings could form the basisfor such an approach to predicting MWE composi-tionality.3 MethodologyIn this work, we estimate the compositionality ofan MWE based on the similarity between the ex-pression and its component words in vector space.We use three different vector-space models: (1) asimple count-based model of distributional similar-ity; (2) word embeddings based on WORD2VEC; and(3) a multi-sense skip-gram model that, unlike theprevious two models, is able to learn multiple em-beddings per target word (or MWE).
For all threemodels, we first greedily pre-tokenise the corpus torepresent each MWE as a single token, similarly toBaldwin et al (2003).
In this, we apply the con-straint that no language-specific pre-processing canbe applied to the training corpus, in order to makethe method maximally language independent.
Assuch, we cannot perform any form of lemmatisation,and MWE identification takes the form of simplestring match for concatenated instances of the com-ponent words, naively assuming that all occurrencesof that word combination are MWEs.
We detail eachof the distributional similarity methods below.3.1 Count-Based Distributional SimilarityOur first method for building vectors is that of Salehiet al (2014b): the top 50 most-frequent words inthe training corpus are considered to be stopwordsand discarded, and words with frequency rank 51?1051 are considered to be the content-bearing words,which form the dimensions for our vectors, in themanner of Sch?utze (1997).
To measure the similarityof the MWE vector and the component word vectors,we considered two different approaches.The first approach is based on Reddy et al (2011)and Schulte im Walde et al (2013).
The similar-ity between the MWE and each of its componentsis measured, and the overall compositionality ofthe MWE is computed by combining the similarityscores for the two components as follows:comp1(MWE) = ?sim(MWE,C1)+(1?
?
)sim(MWE,C2)where MWE is the vector associated with theMWE, Ciis the vector associated with the ith com-ponent word of the MWE, sim is a vector similarityfunction, and ?
?
[0, 1] is a weight parameter.We also experimented with the approach fromMitchell and Lapata (2010), where MWE is com-pared directly with a composed vector of the com-ponent words, based on vector addition:1comp2(MWE) = sim(MWE,C1+C2)For both comp1and comp2, we used cosine sim-ilarity as our similarity measure sim .3.2 WORD2VECOur second method is based on the recurrent neu-ral network language model (RNNLM) approach tolearning word embeddings of Mikolov et al (2013a)and Mikolov et al (2013b), using the WORD2VECpackage.2WORD2VEC uses a log-linear model in-spired by the original RNNLM approach of Mikolovet al (2010), in two forms: (1) a continuous bag-of-words (?CBOW?)
model, whereby all words ina context window are averaged in a single projec-tion layer; and (2) a continuous skip-gram model1We also experimented with vector multiplication, but foundit to perform poorly compared to the other approaches.2https://code.google.com/p/word2vec/978(?C-SKIP?
), whereby a given word in context is pro-jected onto a projection layer, and used to predict itsimmediate context (preceding and following words).WORD2VEC generates a vector of fixed dimension-ality d for each pre-tokenised word/MWE type withfrequency above a certain threshold in the trainingcorpus.
We again use comp1and comp2to estimatecompositionality from these vectors.3.3 Multi-Sense Skip-gram ModelOne potential shortcoming of WORD2VEC is that itgenerates a single word embedding for each word,irrespective of the relative polysemy of the word.Neelakantan et al (2014) proposed a method moti-vated by WORD2VEC, which efficiently learns mul-tiple embeddings per word/MWE.
We refer to thisapproach as the multi-sense skip-gram (MSSG)model.
We once again compose the resultant vec-tors with comp1and comp2, but modify the for-mulation slightly to handle the variable number ofvectors for each word/MWE, by searching over thecross-product of vectors in each sim calculation andtaking the maximum in each case.
We initially setthe number of embeddings to 2 in our MSSG exper-iments ?
in keeping with the findings in Neelakan-tan et al (2014) ?
but come back to examine theimpact of the number of embeddings on composi-tionality prediction in Section 5.4 DatasetsWe evaluate our methods over three datasets:3(1)English noun compounds (?ENCs?, e.g.
spellingbee and swimming pool); (2) English verb parti-cle constructions (?EVPCs?, e.g.
stand up and giveaway); and (3) German noun compounds (?GNCs?,e.g.
ahornblatt ?maple leaf?
and eidechse ?lizard?
).The ENC dataset consists of 90 binary Englishnoun compounds, and is annotated on a continu-ous [0, 5] scale for both overall compositionality andthe component-wise compositionality of each of themodifier and head noun (Reddy et al, 2011).
Thestate-of-the-art method for this dataset (Salehi etal., 2014b) is a supervised support vector regression3We also considered using the dataset from the DisCo sharedtask (Biemann and Giesbrecht, 2011), but ultimately excludedit because it includes different types of MWEs without indica-tion of the syntactic type of a given MWE, preventing us fromcarrying out construction-specific parameter tuning.model, trained over the distributional method fromSection 3.1 as applied to both English and 51 targetlanguages (under word and MWE translation).The EVPC dataset consists of 160 English verbparticle constructions, and is manually annotated forcompositionality on a binary scale for each of thehead verb and particle (Bannard, 2006).
In order totranslate the dataset into a regression task, we cal-culate the overall compositionality as the number ofannotations of entailment for the verb, divided bythe total number of verb annotations for that VPC.The state-of-the-art method for this dataset (Salehiet al, 2014b) is a linear combination of: (1) the dis-tributional method from Section 3.1; (2) the samemethod applied to 10 target languages (under wordand MWE translation, selecting the languages us-ing supervised learning); and (3) the string similaritymethod of Salehi and Cook (2013).The GNC dataset consists of 246 German nouncompounds, and is annotated on a continuous[1, 7] scale (von der Heide and Borgwaldt, 2009;Schulte im Walde et al, 2013).
The state-of-the-artmethod for this dataset is a distributional similaritymethod applied to part-of-speech tagged and lem-matised data (Schulte im Walde et al, 2013).5 ExperimentsFor all experiments, we train our models over rawtext Wikipedia corpora for either English or Ger-man, depending on the language of the dataset.The raw English and German corpora were prepro-cessed using the WP2TXT toolbox4to eliminateXML and HTML tags and hyperlinks, and punctu-ation was removed.
Finally, word-tokenisation wasperformed based on simple whitespace delimitation,after which we greedily identified all string occur-rences of the MWEs in each of our datasets and com-bined them into a single token.5The word embedding approaches are unable togenerate vector representations for tokens which oc-cur with frequency below a fixed cutoff.6In order to4http://wp2txt.rubyforge.org/5For English, a single model was trained over a corpus con-taining both ENC and EVPC tokens.6For a frequency threshold of 15, the total numbers ofENCs, EVPCs and GNCs for which we were unable to gener-ate word embeddings were 3, 0 and 25, respectively, in the lattercase, largely as a result of our simple tokenisation strategy and979Dataset Method comp1comp1+SS comp2comp2+SSENCWORD2VEC(d = 500,C-SKIP) .628 .761 .632 .761(d = 500,CBOW) .696 .786 .710 .791(d = 1000,C-SKIP) .636 .764 .648 .767(d = 1000,CBOW) .717 .789 .736 .796MSSG(d = 300, w = 5) .640 .764 .624 .759(d = 600, w = 5) .615 .758 .594 .758(d = 600, w = 10) .614 .749 .631 .756Distributional similarity .714String similarity .644State-of-the-art .744EVPCWORD2VEC(d = 500,C-SKIP) .289 .496 ?
?
(d = 500,CBOW) .293 .486 ?
?
(d = 1000,C-SKIP) .289 .504 ?
?
(d = 1000,CBOW) .289 .489 ?
?MSSG(d = 300, w = 5) .309 .506 ?
?
(d = 600, w = 5) .294 .498 ?
?
(d = 600, w = 10) .273 .494 ?
?Distributional similarity .165String similarity .385State-of-the-art .417GNCWORD2VEC(d = 500,C-SKIP) .393 .442 .321 .415(d = 500,CBOW) .400 .439 .361 .423(d = 1000,C-SKIP) .341 .411 .282 .394(d = 1000,CBOW) .371 .414 .349 .411MSSG(d = 300, w = 5) .181 .320 .122 .295(d = 600, w = 5) .202 .335 .146 .303(d = 600, w = 10) .155 .310 .101 .282Distributional Similarity .140String Similarity .372State-of-the-art .450Table 1: Pearson?s correlation (r) for the different methods over the three datasets; the state-of-the-art for each datasetis described in Section 4generate a compositionality prediction back-off forthe small numbers of MWEs in this category, we as-sign a default value, which is the mean of computedcompositionality scores for other instances.7As a baseline, we use the translation string simi-larity approach of Salehi and Cook (2013), includingthe cross-validation-based method for selecting the10 best languages to use for each dataset.
We furtherinclude a linear combination of the string similaritymethod with each of the various approaches basedon word embeddings.Table 1 shows the results for the various methods,lack of lemmatisation.7We also experimented with using the string similarity ap-proach as a back-off, which resulted in marginally lower resultsthan what is reported in Table 1.over a range of hyper-parameter settings for eachof WORD2VEC (vector dimensionality d; we alsopresent results for CBOW vs. C-SKIP) and MSSG(vector dimensionality d and window size w), in-formed by the experimental results in the respectivepublications.
Note that for EVPC, we don?t use thevector for the particle, in keeping with Salehi et al(2014b); as such, there are no results for comp2.
Forcomp1, ?
is set to 1.0 for EVPC, and 0.7 for bothENC and GNC, also based on the findings of Salehiet al (2014b).The results indicate that the approaches usingboth WORD2VEC and MSSG outperform simpledistributional and string similarity by a substantialmargin.
Further, over a variety of parameteriza-9801 2 3 4 5 6Number of prototypes0.10.20.30.40.50.60.7Correlation(r) ENC comp1ENC comp2EVPC comp1GNC comp1GNC comp2Figure 1: The effect of the number of prototypes on theresults with MSSGtions, they surpass the state-of-the-art methods forENC and EVPC; in the case of GNC, the best-performing method (WORD2VEC with d = 500 andC-SKIP) roughly matches the state-of-the-art.
Notethat in each case, the state-of-the-art is achievedusing varying levels of supervision over labelleddata (ENC and EVPC) or language-specific pre-processing (GNC), whereas the word embeddingmethods use no labelled data.
As such, the answerto RQ1 would appear to be a resounding yes.Looking to RQ2, the models are remarkablyinsensitive to hyper-parameter optimisation forEVPC, but there are slight deviations in the re-sults for ENC and GNC.
Having said that, theyare largely between the different word embeddingapproaches, and the results for a given approachunder different parameter settings is relatively sta-ble.
A large part of the cause of the drop in re-sults and greater parameter sensitivity over GNCis the lower token frequencies, through a combina-tion of the Wikipedia corpus being markedly smallerand our naive tokenisation strategy having low recallover German due to the richer morphology.
As such,the answer would appear to be a tentative ?relativelyinsensitive, assuming high token frequencies?.Finally, looking to RQ3, there was little separat-ing WORD2VEC and MSSG over ENC, but over theother two datasets, WORD2VEC had a clear advan-tage.
Given the high levels of polysemy observedin high frequency English verb particle construc-tions (Salehi et al, 2014a), this result for EVPC wasparticularly surprising, and suggests that, at leastunder our two basic forms of composition, multi-prototype word embeddings are at best equal to, andin many cases, inferior to, single-prototype wordembeddings.According to the results, the string similar-ity approach complements all word-embedding ap-proaches.
We hypothesise that this is because it isnot based on any corpus, and is thus not biased bythe frequency of token instances in the corpus.In Table 1, the number of embeddings for MSSGwas set to 2 prototypes, based on the default rec-ommendations of Neelakantan et al (2014).
To in-vestigate the impact of this parameter on our results,we retrained MSSG over the range [1, 6] and reranour experiments for each set of embeddings over thethree datasets (without string similarity, to isolatethe effect of the number of embeddings), as shownin Figure 1.
For both English datasets (ENC andEVPC), setting the number of prototypes to a valuehigher than 2 boosts the results slightly, with 5 pro-totypes appearing to be the optimal value.
For theGerman dataset (GNC), on the other hand, the bestresults are actually achieved for a single prototype.Further research is required to better understand thiseffect.6 ConclusionsWe presented the first approach to using word em-beddings to predict the compositionality of MWEs.We showed that this approach, in combination withinformation from string similarity, surpassed, orwas competitive with, the current state-of-the-art onthree compositionality datasets.
In future work weintend to explore the contribution of informationfrom word embeddings of a target expression and itscomponent words under translation into many lan-guages, along the lines of Salehi et al (2014b).AcknowledgementsWe thank the anonymous reviewers for their insight-ful comments and valuable suggestions.
NICTAis funded by the Australian government as repre-sented by Department of Broadband, Communica-tion and Digital Economy, and the Australian Re-search Council through the ICT Centre of Excel-981lence programme.ReferencesTimothy Baldwin and Su Nam Kim.
2009.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Damerau,editors, Handbook of Natural Language Processing.CRC Press, Boca Raton, USA, 2nd edition.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL-2003 Workshop on Multiword Expres-sions: Analysis, Acquisition and Treatment, pages 89?96, Sapporo, Japan.Colin James Bannard.
2006.
Acquiring Phrasal Lexiconsfrom Corpora.
Ph.D. thesis, University of Edinburgh.Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 809?815, Balti-more, USA.Marco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL 2014), pages 238?247, Baltimore, USA.Chris Biemann and Eugenie Giesbrecht.
2011.
Distri-butional semantics and compositionality 2011: Sharedtask description and results.
In Proceedings of theworkshop on distributional semantics and composi-tionality, pages 21?28, Portland, Oregon, USA.Julian Brooke, Vivian Tsang, Graeme Hirst, and FraserShein.
2014.
Unsupervised multiword segmentationof large corpora using prediction-driven decomposi-tion of n-grams.
In Proceedings of COLING 2014, the25th International Conference on Computational Lin-guistics: Technical Papers, pages 753?761, Dublin,Ireland.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceed-ings of the 25th International Conference on MachineLearning (ICML 2008), pages 160?167, Helsinki, Fin-land.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for statis-tical machine translation.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (ACL 2014), pages 1370?1380, Baltimore,USA.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identificationof idiomatic expressions.
Computational Linguistics,35(1):61?103.Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cer-nock`y, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proceedingsof the 11th Annual Conference of the InternationalSpeech Communication Association (INTERSPEECH2010), pages 1045?1048, Makuhari, Japan.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
In Proceedings of Workshop atthe International Conference on Learning Representa-tions, 2013, Scottsdale, USA.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2014), pages 1059?1069, Doha,Qatar.Alexandre Passos, Vineet Kumar, and Andrew McCal-lum.
2014.
Lexicon infused phrase embeddings fornamed entity resolution.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning, pages 78?86, Baltimore, USA.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality in com-pound nouns.
In Proceedings of IJCNLP, pages 210?218, Chiang Mai, Thailand.Bahar Salehi and Paul Cook.
2013.
Predictingthe compositionality of multiword expressions usingtranslations in multiple languages.
In Second JointConference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Confer-ence and the Shared Task: Semantic Textual Similarity,pages 266?275, Atlanta, Georgia, USA, June.Bahar Salehi, Paul Cook, and Timothy Baldwin.
2014a.Detecting non-compositional MWE components usingWiktionary.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1792?1797, Doha, Qatar, October.Bahar Salehi, Paul Cook, and Timothy Baldwin.
2014b.Using distributional similarity of multi-way transla-982tions to predict multiword expression compositional-ity.
In Proceedings of the 14th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 472?481, Gothenburg, Swe-den, April.Nathan Schneider, Emily Danchik, Chris Dyer, andNoah A. Smith.
2014.
Discriminative lexical seman-tic segmentation with gaps: Running the mwe gamut.Transactions of the Association of Computational Lin-guistics, 2:193?206.Sabine Schulte im Walde, Stefan M?uller, and StephenRoller.
2013.
Exploring vector space models to pre-dict the compositionality of German noun-noun com-pounds.
In Proceedings of the Second Joint Confer-ence on Lexical and Computational Semantics, pages255?265, Atlanta, USA.Hinrich Sch?utze.
1997.
Ambiguity Resolution in Lan-guage Learning.
CSLI Publications, Stanford, USA.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of the Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning 2012 (EMNLP-CoNLL2012), pages 1201?1211, Jeju Island, Korea.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP 2013),pages 1631?1642, Seattle, USA.Claudia von der Heide and Susanne Borgwaldt.
2009.Assoziationen zu Unter, Basis und Oberbegriffen.
Eineexplorative Studie.
In Proceedings of the 9th Nord-deutsches Linguistisches Kolloquium, pages 51?74.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceedingsof the 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1393?1398, Seattle,USA.983
