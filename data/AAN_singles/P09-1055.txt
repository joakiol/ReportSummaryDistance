Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 486?494,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA global model for joint lemmatization and part-of-speech predictionKristina ToutanovaMicrosoft ResearchRedmond, WA 98052kristout@microsoft.comColin CherryMicrosoft ResearchRedmond, WA 98052colinc@microsoft.comAbstractWe present a global joint model forlemmatization and part-of-speech predic-tion.
Using only morphological lexiconsand unlabeled data, we learn a partially-supervised part-of-speech tagger and alemmatizer which are combined using fea-tures on a dynamically linked dependencystructure of words.
We evaluate ourmodel on English, Bulgarian, Czech, andSlovene, and demonstrate substantial im-provements over both a direct transductionapproach to lemmatization and a pipelinedapproach, which predicts part-of-speechtags before lemmatization.1 IntroductionThe traditional problem of morphological analysisis, given a word form, to predict the set of all ofits possible morphological analyses.
A morpho-logical analysis consists of a part-of-speech tag(POS), possibly other morphological features, anda lemma (basic form) corresponding to this tag andfeatures combination (see Table 1 for examples).We address this problem in the setting where weare given a morphological dictionary for training,and can additionally make use of un-annotated textin the language.
We present a new machine learn-ing model for this task setting.In addition to the morphological analysis taskwe are interested in performance on two subtasks:tag-set prediction (predicting the set of possibletags of words) and lemmatization (predicting theset of possible lemmas).
The result of these sub-tasks is directly useful for some applications.1 Ifwe are interested in the results of each of these two1Tag sets are useful, for example, as a basis of sparsity-reducing features for text labeling tasks; lemmatization isuseful for information retrieval and machine translation froma morphologically rich to a morphologically poor language,where full analysis may not be important.subtasks in isolation, we might build independentsolutions which ignore the other subtask.In this paper, we show that there are strong de-pendencies between the two subtasks and we canimprove performance on both by sharing infor-mation between them.
We present a joint modelfor these two subtasks: it is joint not only in thatit performs both tasks simultaneously, sharing in-formation, but also in that it reasons about multi-ple words jointly.
It uses component tag-set andlemmatization models and combines their predic-tions while incorporating joint features in a log-linear model, defined on a dynamically linked de-pendency structure of words.The model is formalized in Section 5 and eval-uated in Section 6.
We report results on English,Bulgarian, Slovene, and Czech and show that jointmodeling reduces the lemmatization error by up to19%, the tag-prediction error by up to 26% and theerror on the complete morphological analysis taskby up to 22.6%.2 Task formalizationThe main task that we would like to solve isas follows: given a lexicon L which containsall morphological analyses for a set of words{w1, .
.
.
, wn}, learn to predict all morphologicalanalyses for other words which are outside of L.In addition to the lexicon, we are allowed to makeuse of unannotated text T in the language.
We willpredict morphological analyses for words whichoccur in T. Note that the task is defined on wordtypes and not on words in context.A morphological analysis of a word w consistsof a (possibly structured) POS tag t, together withone or several lemmas, which are the possible ba-sic forms of w when it has tag t. As an exam-ple, Table 1 illustrates the morphological analy-ses of several words taken from the CELEX lexi-cal database of English (Baayen et al, 1995) andthe Multext-East lexicon of Bulgarian (Erjavec,2004).
The Bulgarian words are transcribed in486Word Forms Morphological Analyses Tags Lemmastell verb base (VB), tell VB telltold verb past tense (VBD), tell VBD,VBN tellverb past participle (VBN), telltells verb present 3rd person sing (VBZ), tell VBZ telltelling verb present continuous (VBG), tell VBG,JJ telladjective (JJ), telling tellingizpravena adjective fem sing indef (A?FS-N), izpraven A?FS-N izpravenverb main part past sing fem pass indef (VMPS-SFP-N), izpravia VMPS-SFP-N izpraviaizpraviha verb main indicative 3rd person plural (VMIA3P), izpravia VMIA3P izpraviaTable 1: Examples of morphological analyses of words in English and Bulgarian.Latin characters.
Here by ?POS tags?
we meanboth simple main pos-tags such as noun or verb,and detailed tags which include grammatical fea-tures, such as VBZ for English indicating presenttense third person singular verb and A?FS-N forBulgarian indicating a feminine singular adjectivein indefinite form.
In this work we predict onlymain POS tags for the Multext-East languages, asdetailed tags were less useful for lemmatization.Since the predicted elements are sets, we useprecision, recall, and F-measure (F1) to evaluateperformance.
The two subtasks, tag-set predictionand lemmatization are also evaluated in this way.Table 1 shows the correct tag-sets and lemmas foreach of the example words in separate columns.Our task setting differs from most work on lemma-tization which uses either no or a complete rootlist(Wicentowski, 2002; Dreyer et al, 2008).2 We canuse all forms occurring in the unlabeled text T butthere are no guarantees about the coverage of thetarget lemmas or the number of noise words whichmay occur in T (see Table 2 for data statistics).Our setting is thus more realistic since it is whatone would have in a real application scenario.3 Related workIn work on morphological analysis using machinelearning, the task is rarely addressed in the formdescribed above.
Some exceptions are the work(Bosch and Daelemans, 1999) which presents amodel for segmenting, stemming, and taggingwords in Dutch, and requires the prediction ofall possible analyses, and (Antal van den Boschand Soudi, 2007) which similarly requires the pre-diction of all morpho-syntactically annotated seg-mentations of words for Arabic.
As opposed to2These settings refer to the availability of a set of wordforms which are possible lemmas; in the no rootlist setting,no other word forms in the language are given in addition tothe forms in the training set; in the complete rootlist setting,a set of word forms which consists of exactly all correct lem-mas for the words in the test set is given.our work, these approaches do not make use of un-labeled data and make predictions for each wordtype in isolation.In machine learning work on lemmatization forhighly inflective languages, it is most often as-sumed that a word form and a POS tag are given,and the task is to predict the set of correspondinglemma(s) (Mooney and Califf, 1995; Clark, 2002;Wicentowski, 2002; Erjavec and Dz?eroski, 2004;Dreyer et al, 2008).
In our task setting, we donot assume the availability of gold-standard POStags.
As a component model, we use a lemmatiz-ing string transducer which is related to these ap-proaches and draws on previous work in this andrelated string transduction areas.
Our transducer isdescribed in detail in Section 4.1.Another related line of work approaches the dis-ambiguation problem directly, where the task isto predict the correct analysis of word-forms incontext (in sentences), and not all possible anal-yses.
In such work it is often assumed that the cor-rect POS tags can be predicted with high accuracyusing labeled POS-disambiguated sentences (Er-javec and Dz?eroski, 2004; Habash and Rambow,2005).
A notable exception is the work of (Adleret al, 2008), which uses unlabeled data and amorphological analyzer to learn a semi-supervisedHMM model for disambiguation in context, andalso guesses analyses for unknown words using aguesser of likely POS-tags.
It is most closely re-lated to our work, but does not attempt to predictall possible analyses, and does not have to tacklea complex string transduction problem for lemma-tization since segmentation is mostly sufficient forthe focus language of that study (Hebrew).The idea of solving two related tasks jointly toimprove performance on both has been success-ful for other pairs of tasks (e.g., (Andrew et al,2004)).
Doing joint inference instead of taking apipeline approach has also been shown useful forother problems (e.g., (Finkel et al, 2006; Cohenand Smith, 2007)).4874 Component modelsWe use two component models as the basis ofaddressing the task: one is a partially-supervisedPOS tagger which is trained using L and the unla-beled text T; the other is a lemmatizing transducerwhich is trained from L and can use T. The trans-ducer can optionally be given input POS tags intraining and testing, which can inform the lemma-tization.
The tagger is described in Section 4.2 andthe transducer is described in Section 4.1.In a pipeline approach to combining the taggingand lemmatization components, we first predict aset of tags for each word using the tagger, and thenask the lemmatizer to predict one lemma for eachof the possible tags.
In a direct transduction ap-proach to the lemmatization subtask, we train thelemmatizer without access to tags and ask it topredict a single lemma for each word in testing.Our joint model, described in Section 5, is definedin a re-ranking framework, and can choose fromamong k-best predictions of tag-sets and lemmasgenerated from the component tagger and lemma-tizer models.4.1 Morphological analyserWe employ a discriminative character transduceras a component morphological analyzer.
The inputto the transducer is an inflected word (the source)and possibly an estimated part-of-speech; the out-put is the lemma of the word (the target).
Thetransducer is similar to the one described by Ji-ampojamarn et al (2008) for letter-to-phonemeconversion, but extended to allow for whole-wordfeatures on both the input and the output.
The coreof our engine is the dynamic programming algo-rithm for monotone phrasal decoding (Zens andNey, 2004).
The main feature of this algorithm isits capability to transduce many consecutive char-acters with a single operation; the same algorithmis employed to tag subsequences in semi-MarkovCRFs (Sarawagi and Cohen, 2004).We employ three main categories of features:context, transition, and vocabulary (rootlist) fea-tures.
The first two are described in detail by Ji-ampojamarn et al (2008), while the final is novelto this work.
Context features are centered arounda transduction operation such as es ?
e , as em-ployed in gives ?
give.
Context features includean indicator for the operation itself, conjoined withindicators for all n-grams of source context withina fixed window of the operation.
We also employ acopy feature that indicates if the operation simplycopies the source character, such as e ?
e. Tran-sition features are our Markov, or n-gram featureson transduction operations.
Vocabulary featuresare defined on complete target words, accordingto the frequency of said word in a provided unla-beled text T. We have chosen to bin frequencies;experiments on a development set suggested thattwo indicators are sufficient: the first fires for anyword that occurred fewer than five times, while asecond also fires for those words that did not oc-cur at all.
By encoding our vocabulary in a trie andadding the trie index to the target context trackedby our dynamic programming chart, we can ef-ficiently track these frequencies during transduc-tion.We incorporate the source part-of-speech tag byappending it to each feature, thus the context fea-ture es ?
e may become es ?
e, VBZ.
To en-able communication between the various parts-of-speech, a universal set of unannotated features alsofires, regardless of the part-of-speech, acting as aback-off model of how words in general behaveduring stemming.Linear weights are assigned to each of the trans-ducer?s features using an averaged perceptron forstructure prediction (Collins, 2002).
Note thatour features are defined in terms of the operationsemployed during transduction, therefore to cre-ate gold-standard feature vectors, we require notonly target outputs, but also derivations to pro-duce those outputs.
We employ a deterministicheuristic to create these derivations; given a gold-standard source-target pair, we construct a deriva-tion that uses only trivial copy operations untilthe first character mismatch.
The remainder ofthe transduction is performed with a single multi-character replacement.
For example, the deriva-tion for living ?
live would be l ?
l , i ?
i ,v ?
v , ing ?
e. For languages with morpholo-gies affecting more than just the suffix, one caneither develop a more complex heuristic, or deter-mine the derivations using a separate aligner suchas that of Ristad and Yianilos (1998).4.2 Tag-set prediction modelThe tag-set model uses a training lexicon L andunlabeled text T to learn to predict sets of tagsfor words.
It is based on the semi-supervised tag-ging model of (Toutanova and Johnson, 2008).
Ithas two sub-models: one is an ambiguity class488or a tag-set model, which can assign probabili-ties for possible sets of tags of words PTSM (ts|w)and the other is a word context model, which canassign probabilities PCM (contextsw|w, ts) to allcontexts of occurrence of word w in an unlabeledtext T. The word-context model is Bayesian andutilizes a sparse Dirichlet prior on the distributionsof tags given words.
In addition, it uses informa-tion on a four word context of occurrences of w inthe unlabeled text.Note that the (Toutanova and Johnson, 2008)model is a tagger that assigns tags to occurrencesof words in the text, whereas we only need to pre-dict sets of possible tags for word types, such asthe set {VBD, VBN} for the word told.
Their com-ponent sub-model PTSM predicts sets of tags andit is possible to use it on its own, but by also us-ing the context model we can take into accountinformation from the context of occurrence ofwords and compute probabilities of tag-sets giventhe observed occurrences in T. The two are com-bined to make a prediction for a tag-set of a testword w, given unlabeled text T, using Bayes rule:p(ts|w) ?
PTSM (ts|w)PCM (contextsw|w, ts).We use a direct re-implementation of the word-context model, using variational inference follow-ing (Toutanova and Johnson, 2008).
For the tag-set sub-model, we employ a more sophisticatedapproach.
First, we learn a log-linear classifier in-stead of a Naive Bayes model, and second, we usefeatures derived from related words appearing inT.
The possible classes predicted by the classifierare as many as the observed tag-sets in L. Thesparsity is relieved by adding features for individ-ual tags t which get shared across tag-sets contain-ing t.There are two types of features in the model:(i) word-internal features: word suffixes, capital-ization, existence of hyphen, and word prefixes(such features were also used in (Toutanova andJohnson, 2008)), and (ii) features based on re-lated words.
These latter features are inspired by(Cucerzan and Yarowsky, 2000) and are defined asfollows: for a word w such as telling, there is anindicator feature for every combination of two suf-fixes ?
and ?, such that there is a prefix p wheretelling= p?
and p?
exists in T. For example, if theword tells is found in T, there would be a featurefor the suffixes ?=ing,?=s that fires.
The suffixesare defined as all character suffixes up to lengththree which occur with at least 100 words.b o u n c e dVBD     VBN JJ  VBD  VBNb o u n c e rJJR NNbouncebouncer bounce?bouncbouncerboucerfbounce bouncebounced bounced b o u n c eVB     NN VBbounce bounce?
?
?Figure 1: A small subset of the graphical model.
Thetag-sets and lemmas active in the illustrated assignment areshown in bold.
The extent of joint features firing for thelemma bounce is shown as a factor indicated by the blue cir-cle and connected to the assignments of the three words.5 A global joint model for morphologicalanalysisThe idea of this model is to jointly predict the setof possible tags and lemmas of words.
In addi-tion to modeling dependencies between the tagsand lemmas of a single word, we incorporate de-pendencies between the predictions for multiplewords.
The dependencies among words are deter-mined dynamically.
Intuitively, if two words havethe same lemma, their tag-sets are dependent.
Forexample, imagine that we need to determine thetag-set and lemmas of the word bouncer.
The tag-set model may guess that the word is an adjectivein comparative form, because of its suffix, and be-cause its occurrences in T might not strongly in-dicate that it is a noun.
The lemmatizer can thenlemmatize the word like an adjective and come upwith bounce as a lemma.
If the tag-set model isfairly certain that bounce is not an adjective, butis a verb or a noun, a joint model which looks si-multaneously at the tags and lemmas of bouncerand bounce will detect a problem with this assign-ments and will be able to correct the tagging andlemmatization error for bouncer.The main source of information our joint modeluses is information about the assignments of allwords that have the same lemma l. If the tag-setmodel is better able to predict the tags of some ofthese words, the information can propagate to theother words.
If some of them are lemmatized cor-rectly, the model can be pushed to lemmatize theothers correctly as well.
Since the lemmas of testwords are not given, the dependencies between as-489signments of words are determined dynamicallyby the currently chosen set of lemmas.As an example, Figure 1 shows three sampleEnglish words and their possible tag-sets and lem-mas determined by the component models.
It alsoillustrates the dependencies between the variablesinduced by the features of our model active for thecurrent (incorrect) assignment.5.1 Formal model descriptionGiven a set of test words w1, .
.
.
wn and additionalword forms occurring in unlabeled data T, we de-rive an extended set of words w1, .
.
.
, wm whichcontains the original test words and additional re-lated words, which can provide useful informationabout the test words.
For example, if bouncer is atest word and bounce and bounced occur in T thesetwo words can be added to the set of test wordsbecause they can contribute to the classificationof bouncer.
The algorithm for selecting relatedwords is simple: we add any word for which thepipelined model predicts a lemma which is alsopredicted as one of the top k lemmas for a wordfrom the test set.We define a joint model over tag-sets and lem-mas for all words in the extended set, using fea-tures defined on a dynamically linked structureof words and their assigned analyses.
It is a re-ranking model because the tag-sets and possiblelemmas are limited to the top k options providedby the pipelined model.3 Our model is definedon a very large set of variables, each of whichcan take a large set of values.
For example, fora test set of size about 4,000 words for Slovene anadditional about 9,000 words from T were addedto the extended set.
Each of these words has acorresponding variable which indicates its tag-setand lemma assignment.
The possible assignmentsrange over all combinations available from the tag-ging and lemmatizer component models; using thetop three tag-sets per word and top three lemmasper tag gives an average of around 11.2 possibleassignments per word.
This is because the tag-sets have about 1.2 tags on average and we needto choose a lemma for each.
While it is not thecase that all variables are connected to each otherby features, the connectivity structure can be com-plex.More formally, let tsji denote possible tag-sets3We used top three tag-sets and top three lemmas for eachtag for training.for word wi, for j = 1 .
.
.
k. Also, let li(t)j de-note the top lemmas for word wi given tag t. Anassignment of a tag-set and lemmas to a word wiconsists of a choice of a tag-set, tsi (one of thepossible k tag-sets for the word) and, for each tagt in the chosen tag-set, a choice of a lemma outof the possible lemmas for that tag and word.
Forbrevity, we denote such joint assignment by tli.As a concrete example, in Figure 1, we can see thecurrent assignments for three words: the assignedtag-sets are shown underlined and in bolded boxes(e.g., for bounced, the tag-set {VBD,VBN} is cho-sen; for both tags, the lemma bounce is assigned).Other possible tag-sets and other possible lemmasfor each chosen tag are shown in greyed boxes.Our joint model defines a distribution over as-signments to all words w1, .
.
.
, wm.
The form ofthe model is as follows:P (tl1, .
.
.
, tlm) = eF (tl1,...,tlm)??
?tl?1,...,tl?meF (tl?1,...,tl?m)?
?Here F denotes the vector of features definedover an assignment for all words in the set and ?is a vector of parameters for the features.
Next wedetail the types of features used.Word-local features.
The aim of such features isto look at the set of all tags assigned to a word to-gether with all lemmas and capture coarse-graineddependencies at this level.
These features intro-duce joint dependencies between the tags and lem-mas of a word, but they are still local to the as-signment of single words.
One such feature is thenumber of distinct lemmas assigned across the dif-ferent tags in the assigned tag-set.
Another suchfeature is the above joined with the identity ofthe tag-set.
For example, if a word?s tag-set is{VBD,VBN}, it will likely have the same lemmafor both tags and the number of distinct lemmaswill be one (e.g., the word bounced), whereas if ithas the tags VBG, JJ the lemmas will be distinct forthe two tags (e.g.
telling).
In this class of featuresare also the log-probabilities from the tag-set andlemmatizer models.Non-local features.
Our non-local features look,for every lemma l, at all words which have thatlemma as the lemma for at least one of their as-signed tags, and derive several predicates on thejoint assignment to these words.
For example,using our word graph in the figure, the lemmabounce is assigned to bounced for tags VBD andVBN, to bounce for tags VB and NN, and tobouncer for tag JJR.
One feature looks at thecombination of tags corresponding to the differ-490ent forms of the lemma.
In this case this wouldbe [JJR,NN+VB-lem,VBD+VBN].
The feature alsoindicates any word which is exactly equal to thelemma with lem as shown for the NN and VB tagscorresponding to bounce.
Our model learns a neg-ative weight for this feature, because the lemmaof a word with tag JJR is most often a word withat least one tag equal to JJ.
A variant of thisfeature also appends the final character of eachword, like this: [JJR+r,NN+VB+e-lem,VBD+VBN-d].
This variant was helpful for the Slavic lan-guages because when using only main POS tags,the granularity of the feature is too coarse.
An-other feature simply counts the number of distinctwords having the same lemma, encouraging re-using the same lemma for different words.
An ad-ditional feature fires for every distinct lemma, ineffect counting the number of assigned lemmas.5.2 Training and inferenceSince the model is defined to re-rank candidatesfrom other component models, we need two differ-ent training sets: one for training the componentmodels, and another for training the joint modelfeatures.
This is because otherwise the accuracyof the component models would be overestimatedby the joint model.
Therefore, we train the com-ponent models on the training lexicons LTrain andselect their hyperparameters on the LDev lexicons.We then train the joint model on the LDev lexiconsand evaluate it on the LTest lexicons.
When apply-ing models to the LTest set, the component mod-els are first retrained on the union of LTrain andLDev so that all models can use the same amountof training data, without giving unfair advantageto the joint model.
Such set-up is also used forother re-ranking models (Collins, 2000).For training the joint model, we maximize thelog-likelihood of the correct assignment to thewords in LDev, marginalizing over the assign-ments of other related words added to the graph-ical model.
We compute the gradient approx-imately by computing expectations of featuresgiven the observed assignments and marginal ex-pectations of features.
For computing these ex-pectations we use Gibbs sampling to sample com-plete assignments to all words in the graph.4 We4We start the Gibbs sampler by the assignments found bythe pipeline method and then use an annealing schedule tofind a neighborhood of high-likelihood assignments, beforetaking about 10 complete samples from the graph to computeexpectations.use gradient descent with a small learning rate, se-lected to optimize the accuracy on the LDev set.For finding a most likely assignment at test time,we use the sampling procedure, this time using aslower annealing schedule before taking a singlesample to output as a guessed answer.For the Gibbs sampler, we need to sample anassignment for each word in turn, given the currentassignments of all other words.
Let us denote thecurrent assignment to all words except wi as tl?i.The conditional probability of an assignment tlifor word wi is given by:P (tli|tl?i) = eF (tli,tl?i)??
?tl?ieF (tl?i,tl?i)?
?The summation in the denominator is over allpossible assignments for word wi.
To computethese quantities we need to consider only the fea-tures involving the current word.
Because of thenature of the features in our model, it is possibleto isolate separate connected components whichdo not share features for any assignment.
If twowords do not share lemmas for any of their possi-ble assignments, they will be in separate compo-nents.
Block sampling within a component couldbe used if the component is relatively small; how-ever, for the common case where there are five ormore words in a fully connected component ap-proximate inference is necessary.6 Experiments6.1 DataWe use datasets for four languages: English, Bul-garian, Slovene, and Czech.
For each of the lan-guages, we need a lexicon with morphologicalanalyses L and unlabeled text.For English we derive the lexicon from CELEX(Baayen et al, 1995), and for the other lan-guages we use the Multext-East resources (Er-javec, 2004).
For English we use only open-classwords (nouns, verbs, adjectives, and adverbs), andfor the other languages we use words of all classes.The unlabeled data for English we use is the unionof the Penn Treebank tagged WSJ data (Marcus etal., 1993) and the BLLIP corpus.5 For the rest ofthe languages we use only the text of George Or-well?s novel 1984, which is provided in morpho-logically disambiguated form as part of Multext-East (but we don?t use the annotations).
Table 25The BLLIP corpus contains approximately 30 millionwords of automatically parsed WSJ data.
We used these cor-pora as plain text, without the annotations.491Lang LTrain LDev LTest Textws tl nf ws tl nf ws tl nfEng 5.2 1.5 0.3 7.4 1.4 0.8 7.4 1.4 0.8 320Bgr 6.9 1.2 40.8 3.8 1.1 53.6 3.8 1.1 52.8 16.3Slv 7.5 1.2 38.3 4.2 1.2 49.1 4.2 1.2 49.8 17.8Cz 7.9 1.1 32.8 4.5 1.1 43.2 4.5 1.1 43.0 19.1Table 2: Data sets used in experiments.
The number ofword types (ws) is shown approximately in thousands.
Alsoshown are average number of complete analyses (tl) and per-cent target lemmas not found in the unlabeled text (nf).details statistics about the data set sizes for differ-ent languages.We use three different lexicons for each lan-guage: one for training (LTrain), one for devel-opment (LDev), and one for testing (LTest).
Theglobal model weights are trained on the develop-ment set as described in section 5.2.
The lex-icons are derived such that very frequent wordsare likely to be in the training lexicon and lessfrequent words in the dev and test lexicons, tosimulate a natural process of lexicon construction.The English lexicons were constructed as follows:starting with the full CELEX dictionary and thetext of the Penn Treebank corpus, take all wordforms appearing in the first 2000 sentences (andare found in CELEX) to form the training lexi-con, and then take all other words occurring inthe corpus and split them equally between the de-velopment and test lexicons (every second wordis placed in the test set, in the order of first oc-currence in the corpus).
For the rest of the lan-guages, the same procedure is applied, startingwith the full Multext-East lexicons and the text ofthe novel 1984.
Note that while it is not possi-ble for training words to be included in the otherlexicons, it is possible for different forms of thesame lemma to be in different lexicons.
The sizeof the training lexicons is relatively small and webelieve this is a realistic scenario for application ofsuch models.
In Table 2 we can see the number ofwords in each lexicon and the unlabeled corpora(by type), the average number of tag-lemma com-binations per word,6 as well as the percentage ofword lemmas which do not occur in the unlabeledtext.
For English, the large majority of target lem-mas are available in T (with only 0.8% missing),whereas for the Multext-East languages around 40to 50% of the target lemmas are not found in T;this partly explains the lower performance on theselanguages.6The tags are main tags for the Multext-East languagesand detailed tags for English.Language Tag Model Tag Lem T+LEnglish none ?
94.0 ?full 89.9 95.3 88.9no unlab data 80.0 94.1 78.3Bulgarian none ?
73.2 ?full 87.9 79.9 75.3no unlab data 80.2 76.3 70.4Table 3: Development set results using different tag-setmodels and pipelined prediction.6.2 Evaluation of direct and pipelined modelsfor lemmatizationAs a first experiment which motivates our jointmodeling approach, we present a comparison onlemmatization performance in two settings: (i)when no tags are used in training or testing by thetransducer, and (ii) when correct tags are used intraining and tags predicted by the tagging modelare used in testing.
In this section, we report per-formance on English and Bulgarian only.
Compa-rable performance on the other Multext-East lan-guages is shown in Section 6.Results are presented in Table 3.
The experi-ments are performed using LTrain for training andLDev for testing.
We evaluate the models on tag-set F-measure (Tag), lemma-set F-measure(Lem)and complete analysis F-measure (T+L).
We showthe performance on lemmatization when tags arenot predicted (Tag Model is none), and when tagsare predicted by the tag-set model.
We can see thaton both languages lemmatization is significantlyimproved when a latent tag-set variable is used asa basis for prediction: the relative error reductionin Lem F-measure is 21.7% for English and 25%for Bulgarian.
For Bulgarian and the other Slaviclanguages we predicted only main POS tags, be-cause this resulted in better lemmatization perfor-mance.It is also interesting to evaluate the contributionof the unlabeled data T to the performance of thetag-set model.
This can be achieved by remov-ing the word-context sub-model of the tagger andalso removing related word features.
The resultsachieved in this setting for English and Bulgarianare shown in the rows labeled ?no unlab data?.
Wecan see that the tag-set F-measure of such modelsis reduced by 8 to 9 points and the lemmatizationF-measure is similarly reduced.
Thus a large por-tion of the positive impact tagging has on lemma-tization is due to the ability of tagging models toexploit unlabeled data.The results of this experiment show there arestrong dependencies between the tagging and492lemmatization subtasks, which a joint model couldexploit.6.3 Evaluation of joint modelsSince our joint model re-ranks candidates pro-duced by the component tagger and lemmatizer,there is an upper bound on the achievable perfor-mance.
We report these upper bounds for the fourlanguages in Table 4, at the rows which list m-bestoracle under Model.
The oracle is computed usingfive-best tag-set predictions and three-best lemmapredictions per tag.
We can see that the oracle per-formance on tag F-measure is quite high for alllanguages, but the performance on lemmatizationand the complete task is close to only 90 percentfor the Slavic languages.
As a second oracle wealso report the perfect tag oracle, which selectsthe lemmas determined by the transducer using thecorrect part-of-speech tags.
This shows how wellwe could do if we made the tagging model perfectwithout changing the lemmatizer.
For the Slaviclanguages this is quite a bit lower than the m-bestoracles, showing that the majority of errors of thepipelined approach cannot be fixed by simply im-proving the tagging model.
Our global model hasthe potential to improve lemma assignments evengiven correct tags, by sharing information amongmultiple words.The actual achieved performance for three dif-ferent models is also shown.
For comparison,the lemmatization performance of the direct trans-duction approach which makes no use of tags isalso shown.
The pipelined models select one-best tag-set predictions from the tagging model,and the 1-best lemmas for each tag, like the mod-els used in Section 6.2.
The model name lo-cal FS denotes a joint log-linear model whichhas only word-internal features.
Even with onlyword-internal features, performance is improvedfor most languages.
The the highest improvementis for Slovene and represents a 7.8% relative re-duction in F-measure error on the complete task.When features looking at the joint assignmentsof multiple words are added, the model achievesmuch larger improvements (models joint FS in theTable) across all languages.7 The highest overallimprovement compared to the pipelined approachis again for Slovene and represents 22.6% reduc-tion in error for the full task; the reduction is 40%7Since the optimization is stochastic, the results are av-eraged over four runs.
The standard deviations are between0.02 and 0.11.Language Model Tag Lem T+LEnglish tag oracle 100 98.9 98.7English m-best oracle 97.9 99.0 97.5English no tags ?
94.3 ?English pipelined 90.9 95.9 90.0English local FS 90.8 95.9 90.0English joint FS 91.7 96.1 91.0Bulgarian tag oracle 100 84.3 84.3Bulgarian m-best oracle 98.4 90.7 89.9Bulgarian no tags ?
73.2 ?Bulgarian pipelined 87.9 78.5 74.6Bulgarian local FS 88.9 79.2 75.8Bulgarian joint FS 89.5 81.0 77.8Slovene tag oracle 100 85.9 85.9Slovene m-best oracle 98.7 91.2 90.5Slovene no tags ?
78.4 ?Slovene pipelined 89.7 82.1 78.3Slovene local FS 90.8 82.7 80.0Slovene joint FS 92.4 85.5 83.2Czech tag oracle 100 83.2 83.2Czech m-best oracle 98.1 88.7 87.4Czech no tags ?
78.7 ?Czech pipelined 92.3 80.7 77.5Czech local FS 92.3 80.9 78.0Czech joint FS 93.7 83.0 80.5Table 4: Results on the test set achieved by joint andpipelined models and oracles.
The numbers represent tag-setprediction F-measure (Tag), lemma-set prediction F-measure(Lem) and F-measure on predicting complete tag, lemmaanalysis sets (T+L).relative to the upper bound achieved by the m-bestoracle.
The smallest overall improvement is forEnglish, representing a 10% error reduction over-all, which is still respectable.
The larger improve-ment for Slavic languages might be due to the factthat there are many more forms of a single lemmaand joint reasoning allows us to pool informationacross the forms.7 ConclusionIn this paper we concentrated on the task of mor-phological analysis, given a lexicon and unanno-tated data.
We showed that the tasks of tag pre-diction and lemmatization are strongly dependentand that by building state-of-the art models forthe two subtasks and performing joint inferencewe can improve performance on both tasks.
Themain contribution of our work was that we intro-duced a joint model for the two subtasks which in-corporates dependencies between predictions formultiple word types.
We described a set of fea-tures and an approximate inference procedure for aglobal log-linear model capturing such dependen-cies, and demonstrated its effectiveness on Englishand three Slavic languages.AcknowledgementsWe would like to thank Galen Andrew and Lucy Vander-wende for useful discussion relating to this work.493ReferencesMeni Adler, Yoav Goldberg, and Michael Elhadad.
2008.Unsupervised lexicon-based resolution of unknown wordsfor full morpholological analysis.
In Proceedings of ACL-08: HLT.Galen Andrew, Trond Grenager, and Christopher Manning.2004.
Verb sense and subcategorization: Using joint in-ference to improve performance on complementary tasks.In EMNLP.Erwin Marsi Antal van den Bosch and Abdelhadi Soudi.2007.
Memory-based morphological analysis and part-of-speech tagging of arabic.
In Abdelhadi Soudi, An-tal van den Bosch, and Gunter Neumann, editors, ArabicComputational Morphology Knowledge-based and Em-pirical Methods.
Springer.R.
H. Baayen, R. Piepenbrock, and L. Gulikers.
1995.
TheCELEX lexical database.Antal Van Den Bosch and Walter Daelemans.
1999.Memory-based morphological analysis.
In Proceedingsof the 37th Annual Meeting of the Association for Compu-tational Linguistics.Alexander Clark.
2002.
Memory-based learning of mor-phology with stochastic transducers.
In Proceedings ofthe 40th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 513?520.Shay B. Cohen and Noah A. Smith.
2007.
Joint morpholog-ical and syntactic disambiguation.
In EMNLP.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In ICML.M.
Collins.
2002.
Discriminative training methods for hid-den markov models: Theory and experiments with percep-tron algorithms.
In EMNLP.S.
Cucerzan and D. Yarowsky.
2000.
Language independentminimally supervised induction of lexical probabilities.
InProceedings of ACL 2000.Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing(EMNLP), pages 1080?1089, Honolulu, October.Tomaz?
Erjavec and Saa?o Dz?eroski.
2004.
Machine learn-ing of morphosyntactic structure: lemmatizing unknownSlovene words.
Applied Artificial Intelligence, 18:17?41.Tomaz?
Erjavec.
2004.
Multext-east version 3: Multilingualmorphosyntactic specifications, lexicons and corpora.
InProceedings of LREC-04.Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.Ng.
2006.
Solving the problem of cascading errors:Approximate bayesian inference for linguistic annotationpipelines.
In EMNLP.Nizar Habash and Owen Rambow.
2005.
Arabic tokeniza-tion, part-of-speech tagging and morphological disam-biguation in one fell swoop.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics.Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kon-drak.
2008.
Joint processing and discriminative trainingfor letter-to-phoneme conversion.
In Proceedings of ACL-08: HLT, pages 905?913, Columbus, Ohio, June.M.
Marcus, B. Santorini, and Marcinkiewicz.
1993.
Build-ing a large annotated coprus of english: the penn treebank.Computational Linguistics, 19.Raymond J. Mooney and Mary Elaine Califf.
1995.
Induc-tion of first-order decision lists: Results on learning thepast tense of english verbs.
Journal of Artificial Intelli-gence Research, 3:1?24.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learningstring-edit distance.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 20(5):522?532.Sunita Sarawagi and William Cohen.
2004.
Semimarkovconditional random fields for information extraction.
InICML.Kristina Toutanova and Mark Johnson.
2008.
A bayesianLDA-based model for semi-supervised part-of-speech tag-ging.
In nips08.Richard Wicentowski.
2002.
Modeling and Learning Mul-tilingual Inflectional Morphology in a Minimally Super-vised Framework.
Ph.D. thesis, Johns-Hopkins Univer-sity.R.
Zens and H. Ney.
2004.
Improvements in phrase-basedstatistical machine translation.
In HLT-NAACL, pages257?264, Boston, USA, May.494
