Retrieving Collocations From Korean TextSeonho Kim, Zooil Yang, Mansuk Song{pobi, zooil, mssong}@december.yonsei.ac.krDept.
of Computer Science,Yonsei University, Seoul, KoreaJung-Ho Ahnjungho@math.yonsei.ac.krDept.
of Mathematics,Yonsei University, Seoul, KoreaAbst ractThis paper describes a statistical methodologyibr automatically retrieving collocations fromPOS tagged Korean text using interrupted bi-grams.
The free order of Korean makes it hardto identify collocations.
We devised four statis-tics, 'frequency', 'randomness', 'condensation',and 'correlation' .to account for the more flexibleword order properties of Korean collocations.We extracted meaningful bigrams using an eval-uation ihnction and extended the bigrams ton-gram collocations by generating equivalencesets, a-covers.
We view a modeling problem forn-gram collocations as that for clustering of co-hesive words.1 In t roduct ionThere have been many theoretical nd appliedworks related to collocations.
A rapidly grow-ing awfilability of copora has attracted interestsm statistical methods for automatically extract-mg ?
:o\]loeations from textual corpora.
However,it is not easy to )dentify the central tendenciesof collocation distribution and the borderlinesof criteria are often fuzzy because the expres-sions can be of arbitrary lengths in a large va-riety of forms.
Getting reliable collocation pat-terns is particularly difficult in Korean whichallows arguments to scamble so freely.
Thispaper presents a statistical method using 'in-terrupted bigrams' for automatically retrieving~:ollocations and idiomatic expressions from Ko-rean text.
We suggest several statistics to ac-count for the more flexible word order.If the distribution of a random sample is un-known, we often try to make inferences aboutits properties described by suitably defined mea-sures.
For the properties of arbitrary collocationdistribution, four measure statistics: 'high fre-quency' ,  'condensat ion' ,  ' randomness' ,  and'correlat ion'  were devised.Given a morpheme, our system begins by re-trieving the frequency distributions of all bi-grams within window and then meaningful bi-grams are extracted.
We produce a-covers toextend them into n-gram collocations 1According to the definition of Kjellmer andCowie, a fossilized phrase is a sequence, wherethe occurrence of one word almost predicts therest of the phrase and one word predicts a verylimited number of words in a semi-fossilizedphrase (Kjellmer, 1995) (Cowie, 1981).
How-ever, in both fossilized and semi-fossilized typesthere is a high degree of cohesion among themembers of the phrases (Kjellmer, 1995).
Weconsider the cohesions as a-covers that are ob-tained by applying a fuzzy compatibility rela-tion, which satisfies ymmetry and reflexivity,to meaningful bigrams.
Namely, n-gram collo-cations could be interpreted as equivalent setsof the meaningful bigrams through partitioning.Here, a-covers mean the clustered sets of themeaningful bigrams.2 Re la ted  WorksIn determining properties of collocations, mostof corpus-based approaches accepted that thewords of a collocation have a particular statisti-cal distribution(Cruse, 1986).
Although previ-ous approaches have shown good results in re-trieving collocations and many properties havebeen identified, they depend heavily on the fre-quency factor.
(Choueka et al, 1983) proposed an algorithmfor retrieving only uninterrupted collocations, 2IBigrams and n-grams can be either adjacent mor-phemes or separated morphems by an arbitrary numberof other words.2In the case of an interrupted collocation, words canbe separated by an arbitrary number of words, whereas71sin(:e ,:hey assumed that a collocation is a se-,lu?
'n(:e of adjacent words that frequently ap-l:,(~ar t~)gether.
(Church and Hanks, 1989) de-lhw:(I ;t collocation as a pair of correlated words:m(i ,,se(t mutual information to evaluate such\],~xi(:a,1 (:orrelations of word pairs of length two.They retrieved interrupted word pairs, as well as,minterrupted word pairs.
(Haruno et al, 1996),:onstructed collocations by combining adjacentn-grams with high value of mutual information.
(Brei(lt, 1993)'s tudy was motivated by the factthan mutual information could not give realisticfigures to low fl'equencies and used t-score for asignificance test for V-N combinations.Martin noted that a span of 5 words on leftnnd right sides captures 95% of significant collo-(:ations in English (Martin, 1983).
Based on thisassumption, (Smadja, 1993) stored all bigramsof words along with their relative position, p (-5< p _~ 5).
He evaluated the lexical strength of aword pair using 'Z -score '  and the variance of itst)osil;ion distribution using ' .
sp read ' .
He defined~,.
(:()\]location as an arbitary, domain dependent,recurrent, and cohesive lexical cluster.
(Nagao and Mori, 1994) developed an algo-rithm tbr calculating adjacent n-grams to an ar-1)itrary large number of n. However, it was hardto find an efficient n and a lot of fragments wereobtained.
In Korean, statistics based on adja-cent n-grams is not sufficient o capture varioustypes of collocations.
(Shimohata et al, 1997)employed entropy value to filter out fragmentsof the adjacent n-gram model.
They evaluateddisorderness with the distribution of adjacentwords preceding and following a string.
Thestrings with a high value of entropy were ac-(:epted as collocations.
This disorderness i  ef-fi(:ient to eliminate fragments but can not han-(lle interrupted collocations.
In general, previ-ous ;studies on collocations have dealt with re-stricted types and depend on filtering measuresin a lexically point of view.3 Input  FormatIn this section, we discuss an input form rele-vant to Korean language structure and linguis-tic contents which would work well on an effi-mfinterrupted collocation is a sequence of words?
To~tvoid confusion of terms, we call a sequence of two wordsas ~ 'a ( l j acent  b igram'  and a sequence of n words as a?
ad?accnt  n -gram ~.72cient statistics.
Korean is one of agglutinativelanguages as well as a propositional language.An elementary node being called as ' eo jeo l '  isgenerally composed of a content word and func-tion words.
Namely, a word in English corre-sponds to a couple of morphemes in Korean.A key feature of Korean is that hmctionwords, such as propositions, endings, copula,auxiliary verbs, and particles, are highly devel-oped as independent morphemes, while they arerepresented as word order or inflections in En-glish.
Functional morphemes determine gram-matical relations, tense, modal, and aspect.In Korean, there are lots of multiple functionwords in a rigid forms.
They can be viewedas collocations.
For this reason, our system isdesigned at the morphological level.
A set oftwelve part of speech tags, { N, J, V, P, D, E,T, O, C, A, S, X } 3 was considered.Another feature is a free word order.
Sincethe words of a collocation appear in text withthe flexible ways, sufficient samples are requiredto compute accurate probabilities.
We allow po-sitional information to vary by using an inter-rupted bigram model.The basic input can be represented in (1).
Anobject k means a pair of morphemes (mi,mk)and mk corresponds to one of all possible mor-phemes, being able to co-occur with mi.
A vari-able j indicates the j-th position.
Xij denotesthe frequency of mk that occurs at the j-th po-sition before mi.Xi1 X12X21 X2~Xi  = .
.Xnl Xn2Given a predicateXll?
/ X210XnlO(1)morpheme as a base mor-pheme, the range of window is from -1 to -10.This distance constraint is for the characteris-tic of SOV language.
If a bigram includes anadverb morpheme, a larger window, from -20 to10 is used because the components often appearwidely separated from each other on text.
Inother cases, we considered the range from -5 to+5.
This distant constraints are for an efficientstatistics.An input data is transformed to a propertymatrix, T(Xi) as (2) that is a two dimensional3'Noun' , 'ad Ject ive ' , 'Verb ' , 'Postpos i t ion ' ,  'aDverb' ,'Ending' , 'pre-ending' ,  'cOpular ' ,  'Conjunct ion ' ,  'Auxil-iary verb',  'Suffix', 'etc.
'.Cn~,m,)(o}~1,o~ol) (drink,much)(3}~l,t--t ~ =) (drink,too)(Ot h l,gt) (~nk,~lcan)(3t,~l,OH ?J) (drink, everyday)(fl\[hl,~OI) (drink,boil)(OtJ, l ,~) (drink,,iot)(Ot~,l,@~l) (drink,t.bgether)(0~1,~ ==~) (drink, atittle)(OH,~) (drink,take)(OtA I , _~)  (drink, alittle)syntacticrelationVDVDVJVDWVDVDVDWVDpreferringposition1243213123Figure 1: meaningful bigrams of ~\[z\](drink) by Xtract~rr~:~y of k object.s, k = 1,2,...,n, on four vari-~t,|)les, V Frequency ~ VCondensation , V Randomness ,~md Vcorrelat io n.?F ?c ?R ?cRT(X i )= '.
.
.
.~F ~c ~R ~cR(2)~\]~) continue explanations, we begin by men-tioning the 'Xtrgct' tool by Smadja (Smadja,1993).
Our input form was designed in a simi-lar manner with 'Xtract'.
Smadja assumed thatthe components of a collocation should appeartogether in a relatively rigid way because of syn-tactic constraint.
Namely, a bigram pair (mi,'rnk), where mk Occurs at one(or several) spe-(:ific position around mi, would be a meaningfulbigrams for collocations.
The rigid word orderis related with the variance of frequency distri-bution of (mi, ink).
'Xtract' extracted the pairswhose variances are over a threshold and pulledout the interesting positions of them by stan-dar(lizz~tion of the frequency distributions.
Un-fbrtunately, the approach for English has sev-eral limitations to work 4 on Korean structureibr the following 'reasons:1.
For free order languages uch as Korean,words are widely distributed in text, sothat positional variance affects the over-tiltering of Useful bigrams.
Figure 1 showsthat there is no pair which contains ran-domly distributed morphems uch as func-tion words or nouns.
This indicates thatvery few pairs were produced when 'Xtract'is applied to Korean.4~We por ted  Smadja ' s  X t rac t  tool  into  a Korean  ver-sion.732.
Suppose that a meaning bigram, (mi, mk)prefers a position pj.
Then, the numberof concordances for condition probabilityP(mi, rnklpj) would be small, specially in afree order language.
As shown in Table 1,the model produced a lot of long meaning-less n-grams when compiling into n-grams.The precision value of Korean version ofXtract was estimated to be 40.4%.3.
The eliminated bigrams by the previousstage can appear again in n-gram colloca-tions.
When compiling, the model onlykeeps the words occupying the positionwith a probability greater than a giventhreshold from the concordances of (mi,ink, pj).
As one might imagine, the firststage could be useless.As stated above, in Korean, the effect of po-sition on collocations needs to be treated insome complex ways.
Korean collocations canbe divided into four types: 'idiom' 5, 'seman-tic collocation' 6, 'syntactic ollocation' 7 and'morphological collocation' s. Idioms and mor-phological collocations appear on text in a rigidway and word order but others do in the flex-ible ways.
From a consideration of these moreflexible collocations, we adopt an interrupted bi-gram model and suggest several statistics thatconsist with characteristics of Korean.4 AlgorithmThis section describes how properties are repre-sented as numerical values and how meaningfulobjects are retrieved.
In the first stage, we ex-tract meaningful interrupted bigrams based onfour properties.
Next, the meaningful bigramsare extended into n-gram collocations using aa-compatibility relation.It empirically showed that a Weibull distribu-tion (3) provides a close approximation of fre-quency distribution of bigrams.F(x)=l-~ -"*~ o<x<~ ~h~ ~>0,~>0 (3)5Idioms have no ambiguous  mean ing  but  requiresr igid pat terns  to preserve the  id iomat ic  mean ing .6The rep lacement  of some components  by o ther  wordsis more  free than  idioms.~The combinat ion  of words is af fected by select ionMrestr ic t ions  of predicate,  noun,  or adverb.s i t  cor responds  to mul t ip le  funct ion  word and  ap-pears  on a ad jacent  word  group.I ' r ( ' ( lII12'2n -grams.
.
.
.
.
_7_@(everyone).
Noun-~-(ob jeet  case) .
.
.
((u,d-))o--I(take)(@l-z,l))(drink) .
.. .
.
.
.
.
.
( ( ,~o\ ] ) ) (much)  (@l-z\]))(drink) .
Noun .
.
.
.
.
.-7-(two) ct-N(legs) ~\](at( locat ion case)) ~(a  little) ~ ~ ~(s t ra in )  x-l(stand) o-lx-I(with ~ ing)~(f ' r iend)  N'N (with) ?1t 71 .~- ~\].~-?l-(talk over) ~ x-I (~ ing)  ( (~- - I ) ) (a  l ittle) ~1 ~l (coffee) ~(ob jec t  case)(( - I -x l ) )(dr ink) ~(mod i fy ing  ending)  ~(dream)  .
.?
.
.
@(two) z\]~l-(hours) ~(ob jec t  case) ( (~Nl ) ) ( together )  ~(a lcoho l )  .~(object  case) ((nl-xl))(dr ink)~- (mod i (y ing  ending)  .2.
(he) .
.
.. .
.
.
.
N-el-(cola) ~(ob jec t  case) ( (~'N"N)) (a  little) (( - I -z l ) )(dr ink) o-I 71-~l(~ing) .
.
.
.
.. .
.
.
I-(I) ~(a l so )  "-t lN(baekl im(location)) ~lx-I(at) ( ( , l t~) ) (everyday)  ~ l~( tears )  ~(ob jec t  case)(@Pxl) ) (d ink)  ~ , I  ~(was  ~ ing)  71 ~l(beeause)  .
.
.
.?
.
.
L+(I) ~7\ ] (here)  -U-el(specially) z l l~(dawn)  oJ\](at) ((-N-})(fresh) -~-(modifying ending)  -~-(water) Y_(also)( (u \ [ -q) ) (dr ink)  .
Oc-~- .
~( that )  -~--~\[-(be unsu i tab le)  .
7~-?J-(most) .
~ N- ~z \ ] ( says )  ~r---~-(exercise) ~-(well)?
.
Verb .
.
Noun  .~-(object case) ((uJ-~-))(too) ~.~\]:o\](much) ((-~-z\]))(drink) -G-(modifying ending)  .
.
.
.
.?
.
.
Noun  Noun.
.
.
Noun ~-otix\](in) ((-~o\]))(boi l)  ~ (@\]-zi))(drink) .~ Noun .
.
.
.
.
.. .
.
.
.
-~-~1-7-.
(even though) .
( (~) ) (not )  (@},q)) (dr ink)  ,.~_(and) Noun Noun .
Verb End ing  Verb .
.Table 1: n-grams of =l-zl(drink) by Xt ract  (freq: freq of sentences)dist eval-2 O-1 O-3 X-3 O-I O-3 X-4 X-2 O-2 X-1 OThus, there are a lot of pairs with low frequencywhich interrupt to get reliable statistic.
Weclinfinated such pairs using median m that isa.
value such that P{X > m} > 1/2 to a fre-quency distribution F. If median is less than 3,we took the value 3 as a median.Any quantity that depends on not any un-known parameters of population distributionbut only the sample is called a statistic.
Weregarded four statistics relating to properties of(:ollocations as variables.
Before the further ex-plauation, consider Sm~, a sample space of mi asTable 2 whose cardinality \]Sm~l is n. Let one ob-.iect be (mi, mk) and its frequency distributionbe ./}k,,.rik2,''" ,fiklO and,::k+ be ~pl?_l like.Suppose that POS(mi) is J and POS(mk) "s?p'.4.1 P roper t iesThe properties which we considered are primar-ily concerned with the frequency and positionalinfbrmations of word pairs.
As we have em-phasized, the correlation between position and(:ollocation is very complicated in Korean.According to Breidt, MI or T-score thresh-olds work satisfactory as a filter for extractionof collocations, but filtered out at least half ofthe actual collocations (Breidt, 1993).
Gener-ally, assumed properties could not fully accounttbr collocations.
Therefore, in order to reduce ah)ss of infbrmation, the combination of observedvaxiables would be better than filtering.
We de-lined tbur variables for properties of collocations1.
Vi:2.
~:as follows.According to Benson's definition, a colloca-tion is a recurrent word combination (Ben-son et al, 1986).
We agree with this viewthat a word pair of high frequency wouldbe served as a collocation.
Vf statistic ofan object (mi, ink), is represented as (4).Here, standardization demands attention.The mean and standard deviation are cal-culated in the ' JP '  set which the object be-longs to.Vf = fik+--fijpf f i jp  'nE l  fi l+ A++JJP= %' = n , (4)a i j  P = 1=1 ( fd  f , jp )Intuitively, two words that prefer spe-cific positions must be related with eachother.
We seeked to recapture the ideawith the flexibility of word order.
Forthis, the concept of convergence on eachposition was employed.
In a free or-der language, a meaningful pair can oc-cur in text either with two distance orthree distance.
Let's consider two in-put vectors x, (0,1,0,0,0,1,0,0,1,0) and y,(0,0,0,1,1,1,0,0,0,0).
They have the samevariance but y would be more meaning-ful than x, because y can be interpretedas (0,0,0,0,3,0,0,0,0,0) within the free or-der framework.
Therefore, a spatial mask74:t. 14.:word pair(miami)(mi,m2)(mi,mk)(m~ ,m,~)totalPOS pair(J,P)(J,P)(a,P)(J,P)total frequency variable(position) distributionkl+ All k~2k2+ k=l k:2fik+ Akl fik2fin+ finl fin2f~++lJpunder a JP relation?
.
.
f i l l O?
?
?
fi210:?..
fik~o:?
.
.
finlof i+ltaP fi+MdP "'" fi+lolJPTable 2: all combinations of mi(1/2,1,1/2) was devised for convergence oneach position.
The calculation of conden-sation value rnikv at p-th position is:'lH, ikp4fiki+afika+fika p = 1f ikv { 1 "q'- ~ f ikp-l- f ikp41= - 2 p = 2...9fia 8 ~3fik 9 +4fik 1 o: 4' p= 10The mik,, is c, omputed by neighborhoodsthat are locdted in the border of thel)-th position.
The may_ALe_ is likely "'3 *~ kk+to represent :the condensation of (mi,ink) but it is; still deficient.
Intuitively,(0,1,1,1,0,3,2,0,0,0) would be less con-densed than (0,0,3,0,0,3,2,0,0,0).
There-fi)re, n' was designed for a penalty factor.Irtikp~.
= max (5)p=1,2 ..... lo ~n'f ia+',,' is the number of m, such that fikm 7 ~ 0ti)r 0 <_ m <_ 10, and it is a reverse propor-tion to the condensation.
Square root wasused tbr preventing the excessive influence pof  ' / t  .We were motivated by the idea that if a pairis randomly distributed in terms of posi-tion.
then it Would not be meaningful.
Es-pecially in tim case of flmction words, theyare likely to be randomly distributed over agiven morpheme but distributions of mean-in.gful pairs are not random, as shown inFigure 3.
A typical method for the check ofrandomness i to measure how far the givendistribution is away from a uniform dis-tribution.
In (6), fik means  the expectednumber of (mi, ink) at each position on thea,ssumption that the pair randomly occurs4.
Vat :at the position.
\]fikv-Tikl 71k can be viewedas an error rate at each position p basedon the assumption.
The big difference be-tween the expected number and the actualobserved frequency means that the distri-bution is not random.
One might thinkthat this concept is the same with one ofvariance.
However, note the denominator.This calculation is somewhat better thanvariance which depends on frequency.= ~ ( fikp -- f ik )2 v ,  .
(6)To become a meaningful bigram, a pairshould be syntactically valid.
We viewedthat if the frequency distribution of a pairkeeps the overall frequency distribution ofthe POS relation set which the pair be-longs to, then the pair would be syntac-tically valid.
To verify this idea, we de-pict the overall frequency distributions insome POS relations in Figure 2.
It showsthe frequency distributions of pairs whichare composed of postposition and predicatemorpheme.
It is quite interesting that allobjects have the similar form of frequencydistribution.
They have sharp peaks atthe first and third position.
Clearly, thisillustrates that a postposition has a highprobability of appearing at the first andthird position before a predicate.
We canconclude from this that pairs keeping theoverall frequency structure would be syn-tactically valid.
We used correlation coetti-cient for the structural similarity.
In thecase of a pair rnik, the correlation valuebetween (.fikl, f i k2 , ' " ,  .fiklo) and (./i+LI.lP-?
f i+2lJP,' '" ,.fi+loLJP) is evaluated.
Let x751400f requency  800120010006004002001600?
~ (be  deep)- -~- - .
-  ~ ,~.
(b e new)o}-~.
~; (be  beaut i fu l ).... : ......... ~ (rern a in ,be  left)?
~ .
- - -~-e - I  (be  heard ,droD in)----e---  ?,t~- ~ ( f o l ie  w ,  D o u r into)-~.
.
.
-~ ~ o( x I (d ro l~/ fa l l /aoar t )- o)AI (d r ink ).................. X-f (s tand  )(wear )?
O~ 71 ( regard)N ( increase ,c l im b)?
- F=t t )  (head  to)1 2 3 4 5 6 7 8 9 10?
pos i t ion d i s tance)Figure 2: Frequency  d is t r ibut ions  of  pa i rs  w i th  ' J P '  or  'VP '  POS re la t ionand y be two vectors whose componentsare mean corrected, xi - ~ for x, Yi - Y fory.
The correlation between two variables isstraightforward, if x and y is standardizedthrough dividing each of their elements bythe standard eviations, ax and ay, respec-tively.
Let x* be x/ax and y* be y/ay, thenthe correlation between x and y, VeT can berepresented as follows.Xt = (fikl, f ik2, ' ' ' ,  fiklo)Y' = (fi+llJP, fi+2WJP,"', fi+~olJP) (7)x Jy  *gcr = 10The ranks of bigrams by four measures i sum-marized in Figure 3.
It tells that each of themeasures comes up with our expectation.4.2 Eva luat ion  Funct ionin this section, we analyze the correlationsof fbur measures we defined and explain howto make an evaluation function for extractingmeaningful bigrams.
Table 3 shows the valuesof correlations which exist in the given mea-sures: V/, V~, Ve, VeT.
This explains that the de-fined measures have redundant parts.
We cansay that if a measure has the high values ofV~?
?VetV/ ?
vT ?,.1.0-0.495 1.0-0 .203 0.506 1.00.252 -0 .278 -0 .002 1.0Table 3: correlations between factorscorrelations between others, then it has a re-dundant part to be eliminated.
Since we don'tknow what factors are effective in determininguseful bigram, the concept of weights is morereliable than filtering.
We constructed an eval-uation function, which reflects the correlationsbetween the measures.First of all, we standardized four measures.Standardization gives an effect on adjustmentof value range according to its variability.
Thedegree of relationship between measurel andmeasure2 can be obtained by Cmeasurel,measure2which is {corr(measurel, measure2)} +,wherex + = x if x > 0, x + = 0 otherwise.
The evalu-ation function is concerned with the degrees ofrelationships of measures.f (Vf ,  Vr, Vc, Vcr) = Vf ?
?rVr +CoVe + ?crVcr(8)76iCb,.
: (1 - Cv .
,v f ) (1  - aCvA2'v  )(1 - aCvsv?r )?
.
.
.
.
(1 - Cv?
,vr ) (1  - a -~) (1  - a - -~)(/'or := (1 -- Cv~.~,vf)(1 - a vS"v~ )(1 -- a'~V~ 'v~ )where  a = 2 2(9)Here, the cor/stant a (~ 0.845) is for a com-pensation coefficient.
The minimum value of Cr,?c and ?c~ is 1/3 respectively, where Cv:,v~ =Cvj.,v~, = Cvf,v~ = 0 and all correlations of~., i,~:, and Vcr = 1.
On the contray, the max-lmum value of ?~, ?
?, and ?cr is 1 respectively,where Cv:,v, = Cvf,v~ = cv:,v~ = 0 and all cor-relations of Vr, Vc, Vcr = 0.
In other words, asthe coefficients ?~, ?~, and ?c~ get closer to 1,the correlations between measures reduce.As shown in (8) and (9), we agree that Vf isa.
primaryl factor of collocations.
Each coeffi-cient ?
indicates how much the property is re-flected in evaluation.
For example, in the caseof Cr, a -~ z~ is a portion which is related withthe property of condensation within random-ness.
Therefbre,i 1 - a - -~ corresponds to theremainder, when subtracting this portion fromrandomness.The threshold for evaluation was set by test-ing.
When the value for threshold was 0.5, goodresults were obtained but in noun morphems,a high value over 0.9 was required.
The pairsare selected as meaningful bigrams whose val-ues of the evaluation function are greater thanthe threshold.4.3 :Extending to n-gramsThe selected meaningful bigrams from the pre-vious step are extended into n-gram colloca-tions.
At the final step, the longest ones amongall (~-~:overs are Obtained as n-gram collocationsby eliminating substrings.
Here, n-gram collo-~:ations mean interrupted collocations as well asn-character strings.We regarded Cohesive clusters of the mean-ingful bigrams as n-gram collocations on the as-sumption that members in a collocation have ahigh degree of cohesion (Kjellmer, 1995).
Tofind cohesive chisters, a fuzzy compatibility re-la.tion R is appl!ed.
R on X x X, where X isthe set of all meaningful bigrams which contain; , ,  l)~se morpheme mi, means a cohesive relationa.nd partitions of' set X obtained by R corre-spond to n-gram collocations.
To say shortly,our problem hasshifted to clustering of a set X.A reason to employ the concept of fuzzy is thatequivalence sets defined by the relation may bemore desirable.A fuzzy compatability relation R(X,X) is rep-resented as a matrix by a membership function.The membership function of a fuzzy set A EX is denoted by #A : X ~ \[0, 1\] and maps ele-ments of a given set X into real numbers in \[0,1\].These two membership functions #A were usedto define the cohesive relation as follows.p(x)= I *ny l  .
: .
,~_  \ ] *ny lI*1 '~" J : -  lylD(p(x)\[ Ip(y)) = p(x)(log pry) - log p~x))if p(x)_(p(y)~A(X '  Y) ~ O(p(y)\[ Ip(x)) : p(y)(log p~x) - log p~y))i f  p(y)(_p(x)(10), , 2 J*nyl  (11)  /ZA I,x,Y)= T~ TLet Ixl and lYl be the frequency of concor-dances which contains the bigram pairs x and y,respectively.
IxAyl means how often two pairs xand y co-occur in the same concordances underthe distance constraint.
(10) is relative entropymeasure and (11) is dice coefficient.
This mea-sures are concerned with a lexical relation forcohesive degrees.To get equivalence sets, it is very importantto identify properties of the relation R we de-fined.
A relation which is reflexive, symmet-ric and transitive is called as an equivalencerelation or similarity relation.
In our case,the fuzzy cohesive relation, R is certainly re-flexive and symmetric.
If R(x, z) > ma, xyEymin\[R(x, y), R(y, z)\] is satisfied for all (x, z) eX 2, then R is transitive.
Generally, transitiveclosure is used for checking transitivity.
Thetransitive closure of a relation is defined as thesmallest fuzzy relation which is transitive andhas the fewest possible members with contain-ing the relation itself.Given a relation S(X,X), its max-min transi-tive closure ST(X, X) can be calculated by thefollowing algorithm consisted of three steps:1.
S I = SU (S o S) , o is a max-min compo-sition operator.2.
If S' # S, make S = S ' and go to Step 1.3.
Stop: S '= ST.If above algorithm terminates after the first iter-ation when applied to R, R satisfies transitivity.To verify its transitivity, above alogrithm wereemployed.
As a result, R did not satisfy transi-tivity.
It means that an element of X could be-77hmg to multiple (:lasses by R. This proves thatthe relation R is valid to explain collocations.A iuzzy binary relation R(X,X) which is re-th~xive and symmetric is called as a fuzzy com-pa.til)i\[ity relation and is usually referred to as~,.
(lunsi-e(tuivalence r lation.
When R is a fuzzycompa, tibility relation, compatibility classes are,l(,.fined in terms of a specified membership de-gre,'.
(~.
An a-compatibil ity class is a subset A ofX.
s,mh that it(x, y) > a for all x, y E A and thetnmily consisting of the compatibility classes iscalled as an a-cover of X to R in terms of a spe-cifi,: membership degree a.
An a-cover formspartitions of X and an element of X could be-long to multiple a-compatibil ity classes.
Here,we a.ccepted a-covers at 0.20 a-level in dice and(}.3{} in relative entropy.One might argue why we did not  directly ap-ply a\]\] bigrams to this stage with skipping theprevious stage.
We hope to deal with the com-t)arision in a later paper.5 Eva luat ionWe performed experiments for evaluation on328,859 sentences(8.5 million-morphemes) fromYonsei balanced copora.
250 morphemes wereselected for a test, such that frequency >_ 150.The morphemes have 8,064 pairs and 773 wereextracted as meaningful bigrams.
In the sec-ond stage, 3,490 disjoint a-compatibil ity classescorresponding to lexicMly cohesive clusters weregenera,ted.
698 longest n-gram collocations outof the a-compatibil ity classes were extracted byeliminating the fragments that can be subsumedin longer classes.The precision of extracted meaningful bigramwas 86.3% and 92% in the case of n-gram collo-cations.
We could take either o~-covers and thehmgest n-grams as n-gram collocations accord-ing to applications.Since unfortunately, there is no existingdatabase of collocations for evaluation, it is noteasy to compute precision values and recall val-ues as well.
We computed the precision valuesby hand.
As a different approach to Koreancollocations, (Lee et al, 1996) extracted inter-rupted bigrams using several filtering conditionsand at least the 90% of the results were adja-cent bigrams of length 1.
By this comparison,we may conclude that our approach is more flex-ible to deal with Korean word order.Figure 3 9 displays the changes of rank ac-cording to measures we considered.
It showsthat in contrast to other models, the proper-ties have been effective in retrieving colloca-tions which contain pairs of morphemes withrelatively low frequency.
Since the ranks of bi-grams in four measures came up with our expec-tation, if we could make more adequate valua-tion function, the precision would be improved.Table 4 shows some obtained meaningful bi-grams of 'o}.>\] (not)'.
There are a great deal ofexpressions relating negative sentences in Ko-rean.
The components of them occurs separatedin various ways.
When evaluating meaningflflbigrams, the coetticients for tile evaluation flmc-tion are as follows: Cr ~ 0.432, ?
(: v 0.490,?cr ~ 0.371 in the case of 'ol-q(not)' .
Thismeans that the influence of three other mea-sures is 1.284 times more than that of frequencymeasure in ' JP '  POS relation.We will illustrate all steps with a word,'~'(wear).
The results of the first stage, mean-ingful bigrams of '4_!
'(wear) m are shown in Fig-ure 4.
In the second stage, we calculated mem-bership grades of inputs using dice measure andrelative entropy measure.
As Figure 4 shows,dice measure looks unsatisfactory in such casesas the pair ' (~(object  case), ~o} (much))'.
Al-though the common frequency, 3 is a relativelyhigh in the aspect of the word with lower fre-quency, 'Nol ' (much),  the value of dice is low.Thus, we also tested relative entropy based onthe probability of low frequency.
Two measuresproduce similar results if all values in the levelset of R is considered instead of a specific valueof o~, but entropy measure produces more goodresults.Figure 4 and 5 show all o~-compatibilityclasses and the longest n-gram collocations of'~'(wear).
Through our method, various kindof collocations were extracted.
In Figure 4, theorder of components of a oe is by concordances.6 Conc lus ionIn this paper, we implemented measures whichreflect the four properties of collocation respec-9The meanings of pairs are not described in detailbecause the pairs including function words are hard totranslate into English.1?The word meaning corresponds to "put on(wear ortake on)" in English, but it uses for shoes or socks.78!mk mr poa- relationpost- not oo~mo.
:: ~o~:: Iof:~.tolf ~t oF q Jpt .oF i.~ JPJOFLf JP.~  =oFq Jp!o~ L-I Jpoll !O~M JPo~1 ~ ~ Io~-t.-I Jp~V l l l l d  I~ lmFrequency distr ibut ion22 20 17 26 40 48 17 15 6 4270 0 0 0 0 0 0 0 0 530 0 0 0 0 0 0 0 1 110 0 1 0 0 0 0 1 0 290 2 1 4 5 2 4 4 0 821 1 0 0 0 1 0 2 0 276 4 3 6 4 9 1 16 2 6024 31 34 35 38 46 24 48 10 130 0 0 0 I 7 1 0 0 023 24 25 27 24 55 17 13 1 019 30 23 30 15 19 19 58 1 11 0 1 0 1 3 0 0 0 00 0 2 0 1 3 1 0 0 06 7 11 13 5 25 3 3 0 01 0 0 6 1 2 1 1 0 0frec lentfreq std638 4.153 -012 -031 -0104 0.232 -0111 0.3303 1.79 -0209 1215 16 -17 -173 -01 2 -0randl  mnran istd36.3 0.490.0 2.574.7 1.977.7 2.152.9 1.161.9 1.422.9 -01.6 -153.0 1.14.9 -15.1 -123.3 -020.6 -09,6  -120.6 -0.~  Ih~Bml  | l lm lm~l l  | l lal~l l l lOl l l l  ~ I I~111I~I~I~ ~llrll~ll~l~I I I I~I I  I I~ I~ I~I~1 I1~1 ~ll lBml~nenmNI ~11~1~nenmil 111~1~O~1 llitai'aa B~I I I  Bill ~ I I I~ IEnml  t l l~ l l~evaluleval4.83.31.91.81.31.20.60.60.50.10.0-0 .6-0 .7-0 .7-0 .8Figure 3: Top 15 bigrams of 'o~'(not)  by our algorithmmi POSot?
J(white) J~ ~(shoes) N~-'~-(l~:~ots) N?=m~socks) N~-.~-~'J(rub~u shoes) ?
No11- (location) Poll * (location) P0~1- (location) P~ * (location) P~*(Iocation) PL *(modifying} pL *(modifying) EL *(modif~Rg} EL ,(modifying) E~ * (modifying) EL~ *(modlfy~ng) EL *(modifying) EL * (mod~fyirlg) EL *(modifying) E~*(subiec~) E> ~*(subject) P) |*(subiect) P~ F*(su~ect) P)~(su~ect) P2 Hsubject} P2}*(sul~ect) P?
m* (object) Pt-(object) P~" (object) Pt-(object) P~*(objecl) P?
*(object) P~-" (object) P-~- (object) P~"  (ObJ~:':t) P?
*(object) P~*(obj~ct\] P~-- (ob l~t )  P?
= funct ion  wordm~ POS of m= RelativeY~-~l-I{sneakets) N 4 5 1 0.22 0.062IS(leather) N 8 4 1 0.t7 0.172 P~(leat her} N 11 4 2 0.27 0.51-T~boots) N 8 11 1 0.11 0.04~(whlte) J 3 4 2 0.57 0.19-.~--~J~ ( mbl0er sPK.~s ) N 19 3 3 0.27 1.85.~O~(much) O 19 3 1 0.09 0.62-!
:1 (Y, hit e) J 19 4 2 0.17 0.78-r~(boots) N 19 11 2 0.13 0.10?o~shocks) N 19 8 3 0.22.
0.32.~-'~,-J~ (Veer shoes) N 19 3 3 0.27 1.85~Ol(much) D 50 3 1 0.04 0.94-~J,-I(sneaker s) N 50 5 2 0.07 0.92~(white) J 50 4 3 0.11 1.897 t-~(leather) N 50 4 1 0.04 0.63AJ~shoes) N 50 8 3 0.10 0.69~-  t-(boots) N 50 11 4 0.13 0.55?~shoc~.s) N 50 8 5 0.17 1.15.~.-~--'-'~ ( PJbber shoes) N 50 3 2 0.08 1.88HI * (location) P 50 19 14 0.41 0.71oF~_.~-(stfll) D 31 3 1 0.06 0.78?-Jo~l-I{sneakers) N 0.36tl:l (vA'~it e) J 0.51~il~shoes) N 0.51:~.
(~oots) N 0.09~-(Iocati~) P 0.10L *(modifying) E 0.22P~ol(much) D 3.03~-~i~sneakers) N 2.01~(white) J 2.067 P.~.,(leather) N 2.74~U~(shoes\] N 1.54::F~(boots) N 1.41?ol~(shacks) N 1.79~-.~-.~ (rubber shoes) N 3.03Hl*(Iocation) P 0.68L *(modifying) E 0,12~Hsubiect) P 0.22All c?--covers using dice measure" " ;  I.
(subject ) )-..:ltl (whit e) L (m odiNng) ....~-~ll.t(sneakers)-i.
( object ) ...~ (wear) ...?
..7 ~subject)... L (modifyirlg)..- ,~.J ~r(sl~S)-..~_J (wear)..-.-.2 ~sublect)... L (modifying)"-N u~(shoesl-II(object ) --.~ (wear).-.~gsub ject ) .
`~(~cat i~n)~(mod~fy~ng)~.~`1(~b ject )~Bj~(much)``~(we=)~.. .
.2~(sub ject ) .
.
.o l l ( Iocat ion) .
.
.
l l (ob ject ) .
.
.~Ol ( rnuch) .
.
.~(wear) .
.
.?
.-2~-.~ -~-(leather boots)..-~(wear)...?
.-2~-~ -.7-.~,.0eather boots) J(object)...~ (wear)..-?
.o~-~, ~ ~t~'(Ioather shoes).--~(wear)--.-..2}-~ ~J W(k3ather shoes)J.
(object)..-~(wear)......~(boots)..-~(wear)...- .
.
- .
?-~-(boots)t}Cobject)  ...~(wear)-.."'" L (modifying),..2 }-~ -7" '~(feather boots)t(object)..-~ (wear)..-... ~ (modifying).-o ~ ~ L~,j(leat her shoes)I(obJect)...~(wear)...?
.. L.. (modifying)"-~ (wear)...?
.- L (rnodifying)...ol (Iocation)..-J(object)...N (wear)..-... L (modifying)-..ol (Iocation)..-~(wear).-.... L. (modiNng..-~(object))...~ (wear)..."' i(object)"-~(wear)",.
.
.~OF(much) .
.
.~(wear ) .
.
.?
.
.~ ~(shoes)-..~ (wear)......~ ~(shoes) J(object).-.~ (wear)..--- o I-~.L~_ (sNI)-..~ (wear)-..?
..oj~:(shccks)...~J (wead ...?
..?~(socks)...~(Iocation)--.
L (modifying).-.-~-~-(boots)t~(object)...~ (wear)...?
..ol (location) ...~-~,~.AJ (rul0ber shoes)..-~ (wear)-.....ol (location) ... L (modl lying ) ...-.~-~(boots)i(object ) ...~ (wear) .-.?
.-oll(Iccation)...
L-(modlfytng).-.
?~(shocks)J(object),..~(wear).-....ol (Iocatlon)...?
(object)..-P~Ol(much)...~ (wear)...?
..N (Iocation)-..~ (wear)..-?
.
.N ( Iocat ion) .
.
.O~(shocks) .
.
.~(wear )  .,.-..N (Ioc~tion)--.~'gt(shocks)tl(object).--N (wear)...?
-.N(Iocation)...~(white)..-~-~.,J(rubber shoes)..-~(wear).-.?
.
.c~(Iocation).. .~(white).
.
.
.~.P~-C~(rubber shoes).i(object)...~ (wear).... .
.
o~( Iocat ion) .
.
-~(wh i te )L (mod i fy ing) .
.
.~-~.
.~( rubber  shoes)tl(object)...~(wear)..-?
..-~-.~t4(sneakers).-~ (wear)...?
.. -~-?l{sneakem)-B (object) .-.~J (wear) ..-- .M (whit e) .-.~-~--~ (rubber shces)...~ (wear)...-..~ (vANte)-.-~(wear).--.
- .~(v, /n i te) .
.
.~-~-tz l<sneakers) .
.
.~(wear) .
.
.---~ (whir e) .. -~-.~-~J-( sneaker s)iJ.
(object ) ..-~ (wear)..-.-.~ (white) L- (rnodiNng)...~ (wear) --.Figure 4: Meaningful bigrams and all c~-compatibility classes of '~'(wear)tively and the evaluation function which appro-priately combines the measures.
Our approachwas primarily focused on the subtle relation-shit)s between word positions and collocationsin a, free order language.We extracted meaningful bigrams using anew~luation flmction and extended them inton-grams by producing a-compatibility classes.The usefulness of our algorithm were illustratedby examples and tables.This method covered various range of colloca-tions, which the extracted collocation patternswere case frames, multiple functional words, se-lectional restrictions, semantic phrases, corn-79(Adverb,'o~Q') (Postposition,'o~') (Noun, 'o~q')(~\ ]~,  otq)(not only)(~-x\], o\]-q)(not only)(~t--~-~1, o~q)(simply ~ not)(~e-~Q, o\]-q)(but ~ not)(~ ,  olM)(never)(~V~I, ?l'q)(not necessarily)(~, o~q)(too)(~ol, obq)(not also)(~b, obq)(not)(2E, o~ )(not)(~-, o\]-w\] )(not)(~, o\]-q)(not also)(~-- ,  o\]M)(not because)(:~, o~q)(not ~ that)(~x~\], o\]-q)(not important)(~, ol-q)(not intend to)(?1t71, o\]-q)(not ~ that)(oil-r, o~q)(not he reason)Table 4: Examples of bigrams for negation expressionscollocations(Ionsest collocations)dice""Y b"':F~-'"~"" (S. .
.boots.
.
.wear)...Yb..o~l ..- ~-...-~...  ,~ ..- (S. .
.L .
.
.M.
.
.O.
.
.wear)?
.
.~b .
.O l l .
- .~ .
.
.~o l  ~ .
.
.
(S .
.
.L .
.
.O.
.
.much wear)- .
.2t.. .
~- ~ ~~- .
.
.~ .
.
-  (S-..M shoes+O.--wear)relative?
..~t--..oll-..-'7-@"-~ ""?
..71-...otl-.. c .
.
.
.
,~.
.
.
,Nol .
.
.~  ...?
..~F..L ~ ~t...~.... .
.~t---~.l ~ ' - l . "
~ ...K\] SameS : a proposition fora subject case0 : a proposit ion fora object caseL : a p ropos i t ion  fora Ioction caseM : for modify ingFigure 5: the longest n-gram collocations of '~'(wear)pound nouns, and idioms and it could be ap-plicable to other free order languages.With the development of recognition ofphrases, the input format and related distancebetween morphemes, the algorithm can be usedeffectively.
Also linguistic contents for statisti-cal constraints should be reflected in the system.We have plans to check how this algorithmwill work in English and to align bilingual col-locations for machine translation.Re ferencesBenson, M., Benson, E., Ilson.
R. 1986 TheBBI Combinatory Dictionary of English: AGuide to Word Combinations.
John Ben-jamins, Amsterdam and Philadelphia.Breidt, E. 1993 Extraction of V-N collocationsfiom text corpora: A feasibility study forGerman.
In the 1st ACL-Workshop on VeryLa.~ye Corpora.Choueka, Y., Klein, T., and Neuwitz, E. 1983.Automatic retrieval of frequent idiomatic andcollocationM expressions in a large corpus.
InJournal .for Literary and Linguistic Comput-ing, 4:34-38.Clmrch, K., and Hanks, P. 1989.
Word as-sociation norms, mutual information, andlexicography.
In Computational Linguistics,16(1):22-29.Cowie, A. P. 1981.
The treatment of colloca-tions and idioms in learner's dictionaries.
InApplied Linguistics, 2(3):223-235.Cruse, D. P. 1986 Lexical Semantics.
Cam-bridge University Press.Haruno, M., Ikehara, S., and Yamazaki.
T. 1996Learning bilingual collocations by word-levelsorting.
In Proceedings of the 16th COLING,525-530.Ikehara, S., Shirai, S., and Uchino, H. 1996A Statistical Method for Extracting Uninter-rupted and Interrupted Collocations.
In Pro-ceedings of the 16th COLING, 574-579.Kjellmer, G. 1995 A mint of phrases: CorpusLinguistics, 111-127.
Longman.Klir, J. G., and Yuan, B.
1995 Fuzzy SetsAnd Fuzzy Logic: Theory And Applications.Prentice-Hall.Martin, W., and Sterkenburg, V. P. 1983 Lexi-cography: principles and practiceNagao, M., and Mori, S. 1994.
A new methodof n-gram statistics tbr large number of n andautomatic extraction of words and phrasesfrom large text data of Japanese.
In Proceed-ings of the 15th COLING, 611-615.Ross, S. M. 1987 Introduction To Probability80IIAnd Statistics For Engineers And ScientistsJohn Wiley ~ SonsShimohata, S., Sugio, T., and Nagata, J.
1997.Retrieving collocations by co-occurrences andword order coristraints.
In the 35th AnnualMeeting qf ACL, 476-481.Sm~Ldja, F. 1993.
Retrieving collocations fromtext: Xtract.
In Computational Linguistics,19(1):143-177.Sm~tdja, F., MaKeown, K., and Hatzivas-silogtou, V. 1996.
Translating collocationstbr bilingual lexicons: A statistical approach.In Computational Linguistics, 22 (1): 1-38.Lee, KongJoo, Kim, Jaehoon Kim, Kim,Gih:hang.
1995.
Extracting Collocationsfronl Tagged Corpus in Korean.
In Proceed-ings of Korean Information Sience Society,22(2):623-626.81
