Bayesian Stratified Sampling to Assess Corpus UtilityJudith Hochberg, Clint Scovel, Timothy Thomas, and Sam HallGroup CIC-3, Los Alamos National LaboratoryMail Stop B265Los Alamos, NM 87545{judithh, jcs, trt, shall}@lanl.govAbst rac tThis paper describes a method for ask-ing statistical questions about a large textcorpus.
We exemplify the method byaddressing the question, "What percent-age of Federal Register documents arereal documents, of possible interest o atext researcher or analyst?"
We estimatean answer to this question by evaluating200 documents elected from a corpusof 45,820 Federal Register documents.Stratified sampling is used to reduce thesampling uncertainty of the estimatefrom over 3100 documents to fewer than11300.
The stratification is based on ob-served characteristics of real documents,while the sampling procedure incorpo-rates a Bayesian version of Neyrnan allo-cation.
A possible application of themethod is to establish baseline statisticsused to estimate recall rates for informa-tion retrieval systems.In t roduct ionThe traditional task in information retrieval isto find documents from a large corpus that arerelevant to a query.
In this paper we address arelated task: answering statistical questionsabout a corpus.
Instead of finding the docu-ments that match a query, we quantify the per-centage of documents that match it.The method is designed to address tatisti-cal questions that are:?
subjective: that is, informed readers maydisagree about which documents match thequery, and the same reader may make dif-ferent judgment at different times.
Thischaracteristic describes most queries of realinterest o text researchers.?
difficult: that is, one cannot define an al-gorithrn to reliably assess individual doc-uments, and thus the corpus as a whole.This characteristic follows naturally fromthe first.
It may be compounded by an in-sufficient understanding of a corpus, or  ashortcoming in one's tools for analyzing it.Statistical questions asked of small corporacan be answered exhaustively, by reading andscoring every document in the corpus.
Suchanswers will be subjective, since judgmentsabout the individual documents are subjective.For a large corpus, it is not feasible to read ev-ery document.
Instead, one must sample asubset of documents, then extrapolate the re-suits of the sample to the corpus as a whole.The conclusions that one draws from such asampling will have two components: the esti-mated answer to the question, and a confidenceinterval around the estimate.The method described in this paper com-bines traditional statistical sampling techniques(Cochran (1963), Kalton (1983)) with Bayes-ian analysis (Bayes (1763), Berger (1980)) toreduce this sampling uncertainty.
The methodis well-grounded in statistical theory, but itsapplication to textual queries is novel.
Onebegins by stratifying the data using objectivetests designed to yield relatively homogeneousstrata, within which most documents eithermatch or do not match the query.
Then onesamples randomly within each stratum, with thenumber of documents ampled per stratumdetermined through the analysis of a presam-pie.
A reader scores each selected ocument,and the results of the different strata are com-bined.
If the strata are well constructed, the re-suiting estimate about the corpus will have amuch smaller credibility interval (the Bayesianversion of a confidence interval) than onebased on a sample o f  the corpus as a whole.The method is well suited for subjectivequeries because it brings a human reader'ssubjective judgments to bear on individualdocuments.
The Bayesian approach that weapply to this problem allows a second oppor-tunity for the reader to influence the results ofthe sampling.
The reader can construct aprobability density that summarizes his or herprior expectations about each stratum.
Theseprior expectations are combined with pre-sampling results to determine the makeup ofthe final sample.
When the final sample is ana-lyzed, the prior expectations are again factoredin, influencing the estimated mean and the sizeof the credibility interval.
Thus different read-ers' prior expectations, and their judgments ofindividual documents, can lead to substantiiallydifferent results, which is consistent with thesubjective probability paradigm.In earlier work we used this method to ana-lyze medical records, asking, "What percentageof the patients are female?"
(Thomas et al(1995)).
The lack of a required gender fieldin the record format made this a subjectivequestion, especially for records that did notspecify the patient's gender at all, or gaveconflicting clues.
We stratified the corpus intoprobable male and female records based onlinguistic tests such as the number of femaleversus male pronouns in a record, then sam-pled within each stratum.
Stratification re-duced the sampling uncertainty for the ques-tion from fourteen percentage points (basedon an overall sample of 200 records) to five(based on a stratified sample of the same size).In this paper, we update the method andapply it to a new corpus, the Federal Register.The main change from Thomas et al (1995) isa greater focus on numerical methods as op-posed to parametric and forrnulaic calcula-tions.
For example, we use a non-parametricprior density instead of a beta density, andcombine posterior densities between strata us-ing a Monte Carlo simulation rather thanweighted means and variances.
Other differ-ences, such as a Bayesian technique for allocat-ing samples between strata, and a new methodfor determining the size of the credibility in-terval, are noted in the text.The Federal Register corpus is of generalinterest because it is part of the TIPSTER col-lection.
The question we addressed is likewiseof general interest: what percentage of docu-ments are of possible interest o a researcher,or to an analyst querying the corpus?
Anyonewho has worked with large text corpora willrecognize that not all documents are createdequal; identifying and filtering uninterestingdocuments can be a nuisance.
Estimating thepercentage of uninteresting documents in acorpus therefore helps determine its utility.The paper begins by describing the Fed-eral Register corpus and the corpus utilityquery.
It then describes two steps in finding astatistical answer to the query: first through anoverall sample of 200 documents from thecorpus, then through a stratified sample of200, then 400 documents.
The Conclusiontakes up the question of possible applicationdomains and implementation issues for themethod.1 DataThe text corpus used in this study was the Fed-eral Register.
Published by the United StatesGovernment, the Register contains the full textof all proposed and final Federal rules andregulations, notices of meetings and programs,and executive proclamations.
We used anelectronic version of the Register that was partof the 1997 TIPSTER collection distributed bythe Linguistic Data Consortium (http://www.ldc.upenn.edu/).
It consisted of 348 files, eachpurported to contain one issue of the Registerfor the years 1988 and 1989.
Each separaterule, regulation, etc.
within an issue was con-sidered a separate document and was bracketedwith SGML markup tags <DOC> and </DOC>.The corpus contained 45,820 such documents.There were systematic differences betweenthe corpus and the printed version of the Fed-eral Register.
The on-line version omittedpage numbers and boilerplate text seen in theprinted version.
The order of documents inthe two versions differed; for example, docu-ments within special Parts following the mainbody of the printed version were intermixedwith the main body of the on-line version.Other differences, uch as missing or repeateddocuments, were less systematic and appearedto be errors.Thus the TIPSTER corpus could in no waybe considered a perfect electronic version ofthe Federal Register.
Rather, it should beconsidered a realistic example of archivalrecords that are not extensively edited for thepurposes of information extraction research.2 The  QueryThe query we addressed in this paper grew outof an attempt o establish basic statistics forFederal Register documents.
When countingdocuments and determining their length, wenoticed that some purported documents (asjudged by <DOC> </DOC> bracketing) werenot what we came to define as real FederalRegister documents: documents describing theactivities of the federal government.
Besidesreal documents, the electronic Register con-tained pseudo-documents related to the useand publication of the paper version of  theRegister, such as tables of contents, indices,blank pages, and title pages.This discovery at first appeared to be amere nuisance.
We assumed that there was aneasy way to separate pseudo-documents fromreal documents, but could not find one.
Theharder we looked for a way to separate the twodocument types, the more we realized that thisdistinction had theoretical interest.
Determin-ing the percentage of real documents wouldserve to evaluate the true size of the corpus,and its usefulness for TIPSTER type applica-tions where documents relevant to topicqueries are expected to be returned.This query matched the two criteria setforth in the Introduction for applicability toour method.
As described above, there was noeasy way to separate real documents frompseudo-documents.
The query was also sub-jective, since readers might disagree about theclassification of particular documents.
For ex-ample, a document announcing classes on howto use the Federal Register could be consid-ered a real document (since notices of all sortsappear in the Register), or a pseudo-document(since it is promulgated bythe Register's officeand appears at regular intervals).
As anotherexample, readers might disagree about whicherratum documents are significant enough tobe considered real documents hemselves.3 Overa l l  es t imat ion  and  theBayesian approachWe will illustrate the Bayesian approach in thecontext of a straw man effort to estimate thepercentage of real documents without stratify-ing the corpus.
From the entire 45,820 docu-ment set, we sampled 200 documents at ran-dom.
Sampling was done with replacement(i.e., we did not remove sampled documentsfrom the population); however, no documentswere observed to be selected twice.
One of ourresearchers then reviewed the documents andjudged them as real documents versus pseudo-documents.
He did this by reading the firstfifty lines of each document.Of the 200 documents ampled, 187, or0.935, were judged to be real documents; thisserved as our initial estimate for the overallpercentage of real documents in the corpus.We then used Bayesian techniques to modifythis estimate based on our prior expectationsabout the population.
This was a three-stepprocess.
First, we calculated the binomial ike-lihood function corresponding to the samplingresults.
Second, we encoded our prior expec-tations in a likelihood function.
Third, wecombined the binomial and prior likelihoodfunctions to create a posterior probabilitydensity.
This posterior served as the basis forthe final estimate and credibility interval.3 .1  Binomial  l ikel ihood funct ionThe standard binomial ikelihood function as-sociated with the sampling result (187 realdocuments out of 200),200!
x 187 (l-x) 13, (1) f (x ) -  187!13!is graphed in Figure 1.
It shows, given eachpossible true percentage of real documents, thelikelihood that one would find 187 real docu-ments out of 200 sampled.
We evaluated thelikelihood function at a high degree of granu-larity -- at x intervals corresponding to fivesignificant digits -- so that we would later beable to map percentages of documents ontoexact numbers of documents.0.10~ 0.05o~ ,= 0%.0 o12 o., ) ,.oPossible true proportion of real documents in entire populationFigure 1.
Binomial ikelihood function given187 real documents out of 200 sampled3 .2  Pr iorWe chose a prior by inputting a personal like-lihood: one researcher's subjective opinionabout the population based on a first look atthe corpus.
The researcher's input consisted ofeleven likelihood values, at intervals of 0.1 onthe x axis, as shown in Figure 2.
These pointswere then splined to obtain a likelihood func-tion (Fig.
2; see Press et al (1988)) and nor-malized to obtain a probability density.
Theresulting density was discretized at five signifi-cant digits to match the granularity of the bi-nomial likelihood function.30.80::i J j0.0 0.2 0.4 0.6 0.8 1.0Possible b'ue proportion of real documentsFigure 2.
Prior personal likelihood forproportion of real documentsAn alternative to the above procedure is tochoose a prior from a parametric family suchas beta densities.
This approach simplifies.later calculations, as shown in Thomas et al(1995).
However, the non-parametric priorallows the researcher more freedom to choosea probability density that expresses his or herbest understanding of a population.3.3  Pos ter io rOnce the prior density was established, weapplied Bayes' theorem to calculate aposteriorprobability density for the population.
We didthis by multiplying binomial likelihood func-tion (Fig.
1) by the prior density (Fig.
2), thennormalizing.
The non-zero portion of the re-suiting posterior is graphed in Figure 3.Figure 3 contrasts this posterior densitywith the binomial likelihood function fromFigure 1, also normalized.
From a Bayesianperspective, the latter density implicitly factorsin the standard non-informative prior in whicheach possible percentage of real documentshas an equal probability: The informativeprior shifted the density slightly to the left.We used the posterior density to revise ourestimate of the percentage of real documentsin the population, and to quantify the uncer-tainty of this estimate.
The revised estimatewas the mean It, of the density, defined asl/ ,~kf(xk) ,  where l is the number of  pointsevaluated for the function (1,000,001).
Thisevaluated to 0.9257.
To quanti fythe uncer-tainty of this estimate, we found the 95% credi-bility interval surrounding it -- that is, therange on the x axis that contained 95% of thearea under the posterior density.oO .0.00020.00010.0~00.~Poslarlot based on- - -  ir~ormath~ ~ Inon-informative pnor I /17~'~, , - .
, .
.0.85 0.90 0.95Possible true proportion of reel documentsFigure 3.
Posterior probability density forproportion of real documentsThe traditional way to find this interval isto assume a normal distribution, compute thevariance c 2 of the posterior, defined aslf(xk)(Xk-l.t) 2, and set the credibility intervalk=lat .It 5:1.96 a.
This yielded a credibilityinterval between 0.8908 and 0.9606.
As analternative, we calculated the credibility intervalexactly, in a numerical fashion that yielded thetightest possible interval and thus somewhatreduced the final uncertainty of the estimate.To do so we moved outward from the peak ofthe density, summing under the curve until wereached a total probability of 0.95.
At eachstep outwards from the peak  we consideredprobability values to the fight and left andchose the larger of the two.
This method alsofinds a tighter interval than the  numericalmethod used in Thomas et al (1995), whichwas based on finding the left and right tailsthat each contained 0.025 of the density.The credibility interval found for the pos-terior probability density using the exactmethod is summarized in Table 1, in percent-ages of real documents and in numbers of realdocuments.
The document range was calcu-lated by multiplying the percentage range bythe number of documents in the corpus(45,820).
For comparison's sake the table in-cludes the parallel results obtained using aTablem,,Posterior based onwhich prior1.
Results from overall sampling (200 documents)Interval \] Size of credibilityinterval(in documents)m ,In percent real documents In number ofdocumentsInfor~rnative .
.
.
.
.
.
\[ 0.89029 < p < 0.95902 40793-43942 .
.
.
.
.
.
.
31490.89519 < p < 0.96374 41017-44158 Non-informative 3141non-informative prior.
The two intervals werealmost identical.
The non-informative priorled to a slightly smaller credibility interval thanthe informative prior, implying that the latterwas poorly chosen.
But regardless of the priorused, the size of the credibility interval, ex-pressed in numbers of documents, was over3100 documents.
This was a lot of uncertainty-- enough to taint any decision about the usageof documents in the on-line Federal Register.4 Reducing uncer ta in tystrat i f ied sampl ingusingWe performed a stratified sampling to reducethe uncertainty displayed in Table 1.
Thisprocess involved dividing the data into tworelatively homogeneous strata, one containingmostly real documents, the other mostlypseudo-documents, and combining samplingresults from the two strata.This approach is advantageous because thevariance of a binomial density, ~ (where nis the number sampled, and p the percentageof "yes" answers), shrinks dramatically for ex-treme values of p. Therefore, one can gener-ally reduce sampling uncertainty by combin-ing results from several homogeneous strata,rather than doing an overall sample from aheterogeneous population.As with our overall sample, we performedthe stratified sampling within the Bayesianframework.
The steps described in Section 3for the overall sample were repeated for eachstratum (with an additional step to allocatesamples to the strata), and the posteriors fromthe strata were combined for the final estimate.4 .1  Def in ing s t rata  and  al locat ing thesamplesWe divided the documents into two strata: ap-parent real documents, and apparent pseudo-documents.
The basis for the division was theobservation that most pseudo-documents wereof the following types:1.
Part dividers (title pages for subparts of anissue)2.
Title pages3.
Tables of contents4.
Reader Aids Sections5.
Instructions to insert illustrations not pre-sent in the electronic version6.
Null documents (no text material between<TEXT> and </TEXT> markers)7.
Other defective documents, uch as titles ofpresidential proclamations that were sepa-rated from the proclamation itself.We wrote a short Per1 script that recognizedpseudo-document types 1-4 using key phrases(e.g., />Part \[IVXM\]/ for Part dividers), andtypes 5-7 by their short length.
This test strat-ified the data into 3444 apparent pseudo-doc-uments and 42,376 apparent real documents.Exploration of the strata showed that thisstratification was not perfect -- indeed, if itwere, we could no longer call this query diffi-cult!
Some real documents were misclassifiedas pseudo-documents because they acciden-tally triggered the key phrase detectors.
Anerratum document correcting the incompre-hensible Register-ese error"<ITAG tagnum=68>BILLINGCODE 1505-01-D </ITAG>"was misclassified as a real document.
However,we will see that the stratification sufficed tosharply reduce the credibility interval.Before doing the stratified sampling, wehad to decide how many documents to samplefrom each stratum.
In a departure fromThomas et al (1995), we used a Bayesianmodification of Neyman allocation to do this.Traditional Neyman allocation requires a pre-sampling, of each stratum to determine its het-erogeneity; heterogeneous strata are thensampled more intensively.
In Newbold'sBayesian modification (1971), prior expecta-tions for each stratum are combined with pre-5sample results to create a posterior densiry foreach stratum.
These posteriors are then usedto determine the allocation.This technique therefore required creatingposterior densities for each stratum thatblended a prior density and a presample.
Ac-cordingly, we devised priors for the two strata-- apparent pseudo-documents, and apparentreal documents -- based on our exploratoryanalysis of the strata.
As in the overall analysis(Section 3.2), we splined the priors to five sig-nificant digits.
The original (unsplined) priorsare graphed in Figure 4.1.00.8 \0.60.40.2(0.00.00x- -- ~< apparent ps~udo-dooJments ~-\]O-- - -O  apparent real documents i\\\~ 0\~ .
/ /- ~ ..0-----0.
- - -O- - -O  - - -  O - - - .O-7 -~.O- - '~  ~ -  - -~ - - ~ - .
-x -  - -i i i i0.20  0 .40  0 .60  0 .80  1.00Possible true proporlion of real documentsFigure 4.
Prior likelihoods for proportion ofreal documents in the strataFor the presample, we randomly chose tendocuments from each stratum (with replace-ment) and read and scored them.
The pre-sample results were perfect -- all apparentpseudo-documents were pseudo-documents,and all apparent real documents were real.
Weapplied Bayes' theorem to calculate the poste-rior density for each stratum, multiplying thebinomial likelihood function associated withthe stratum's presample by the relevant priordensity, and normalizing.With these posteriors in hand, we wereready to determine the optimum allocationamong the strata.
Newbold (1971) gives thefraction q/allocated to each stratum i bycil/2Aill2(ni+l)l/2 (2)qi = kcjl/2 Ajll2(nj+ I ) I/2j= lwhere k is the number of strata, Ci is the cost ofsampling a stratum (assumed here to be 1), n i6is the number of documents in the presamplefor the stratum, and Ai isAi Hi 2 Pi (1-Pi)= (ni+2) (3)where Hi is the fraction of the overall popula-tion that comes from the ith stratum, and Pi isthe population mean for the posterior densityin the ith stratum.
The outcome of this proce-dure was an allocation of 15 apparent pseudo-documents and 185 apparent real documents.4 .2  Poster io rs  for each s t ra tumHaving already sampled ten documents fromeach stratum, we now sampled an additional 5apparent pseudo-documents and 175 apparentreal documents to make up the full sample.We chose documents randomly with replace-ment and judged each document subjectivelyas above.
To our surprise (knowing that thestratification was error-prone), this samplingagain gave perfect results: all apparent pseudo-documents were pseudo-documents, and allapparent real documents were real.We applied Bayes' theorem a final time toderive a new posterior probability density foreach stratum based on the results of the fullsample.
For each stratum, we multiplied thebinomial ikelihood function corresponding tothe full sampling results (0/15 and 185/185) bythe prior probability density for each stratum(i.e., the posterior density from the presample),then normalized.4~3 Combin ing  the  resu l t s :  MonteCar lo  s imu la t ionThe final step was to combine the two posted-ors to obtain an estimate and credibility inter-val for the population as a whole.
The tradi-tional approach would be to find the mean andvariance for each stratum's posterior andcombine these according to each stratum'sweight in the population.
Newbold (1971)k' b igives the weighted mean as i=~l~i n'-i' where biis the number of real documents found in stra-tum i out of ni sampled.
As an altemativetechnique, we used a Monte Carlo simulation(Shreider (1966)) to compute the density ofkthe fraction of real documents p = ~ Hi Pi.i=1We then used this density to provide a final es-timate and a corresponding credibility interval.The Monte Carlo simulation combined thetwo posteriors in proportion to the incidenceof real and pseudo-documents in the FederalRegister corpus.
Real documents constituted0.925 of the corpus, and pseudo-documentsthe remaining 0.075.
To perform the simula-tion, we randomly sampled both posteriordensities a million times.
For each pair ofpoints picked, we determined the weighted av-erage of the two points, and incremented thevalue of the corresponding point on the overalldensity by 10 -6 , or one millionth.
For exam-ple, if we picked 0.2 from the posterior for ap-parent pseudo-documents and 0.9 from theposterior for apparent real documents, then wewould increment the value of 0.8475(0.2*0.075 + 0.9*0.925) in the overall densityby 10 -6 .
At the end of the simulation, the totalarea of the density was 1.0.The resulting overall density is graphed inFigure 5 along with the posteriors.
Since thecorpus mostly contained apparent realdocuments, the combined ensity was closer tothat straatum's density.Using the same method as in section 3.3,we then found the exact 95% credibility inter-val for the combined density.
The results,summarized in Table 2, show a better than 3:1reduction from the overall sample, from 3138to 919 documents.
Table 2 also shows the re-sults obtained using a non-informative prior --that is, based on the sampled results alone,without any specific prior expectations.
Herewe clearly see the benefit of vigorously apply-ing the Bayesian approach, as the prior knowl-edge helps reduce the credibility interval byseven-tenths of a percent, or 325 documents.Discussion and Conclus ionBy sampling 200 documents, tratified accord-ing to observed ocument characteristics with aBayesian version of Neyman allocation, wehave addressed the question of how manyFederal Register documents are useful docu-ments that reflect he activities of the FederalO.Ot~200.
(X)150.00109qO.0.0005Postedor for~'~,~ent  i:~udo-documentst , t ,O'O00~.O0 0.20 0.40 0.60 0.8,0Possibte true proportion of real documentsPosledot forapparent reeJ documents \Mome Carlocombination \1.00Figure 5.
Posteriors after full sample,and Monte Carlo combination of posteriorsgovernment.
The answer was a credibility in-terval between 91% and 93%, or between41,768 and 42,687 documents.
This was asubstantially tighter estimate than could beobtained using either an overall sample, or astratified sample without prior expectations.This estimate was probably tight enough tobe useful in applications uch as comparingthe utility of different corpora.
If higher pre-cision were called for, the simplest way to fur-ther narrow the credibility interval would be toincrease the sample size.
In a follow-on exper-iment, it took less than a half hour to read anadditional 200 documents (this turned up twoincorrectly stratified documents, confirmingour expectations from exploratory analysis).The new data sharpened the posteriors, reduc-ing the combined credibility interval to 624documents, or 1.3 percentage points.
Furtherreductions could be obtained as desired.A final topic to address is When and howour technique may be used.
What types ofquestions are likely to be addressed, and whatare the implementation issues involved?TablePosterior based onwhich priorIn percent real documents, , , ,  - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
._ !nformat!ye 0.91157 < p < 0.93163Non-informative 0.91074 < p < 0.937892.
Results from stratified sampling (200 documents)Interval l Size of documentiI intervalIn number ofdocuments41,768-42,687 91941,730-42,974 1244We see two likely types of questions.
Aquestion may be asked for its own sake, as inthis paper or Thomas et al (1995).
Lookingfurther at the Federal Register corpus, otherfeasible questions using our method come tomind, such as:?
Has the amount of attention paid to theenvironment by the Federal governmentincreased??
What proportion of Federal affairs involvethe state of New Mexico?Users of other corpora could likewise posequestions relevant to their own interests.A question could also be asked not for itsown sake, but to establish abaseline statistic forinformation retrieval OR) recall rates.
Recall isthe percentage of relevant documents fiar aquery that an IR system actually finds.
To es-tablish recall, one must know how many rele-vant documents exist.
The standard techniquefor estimating this number is "pooling": identi-fying relevant documents from among thosereturned by all IR systems involved in a com-parison.
This method is used by the TRECprogram (Voorhees and Harman (1997)).
Ourmethod is a principled alternative to thismethod that is well-grounded in statistical the-ory, and, unlike pooling, is independent of anybiases present in current IR systems.Applying the method to a new question,whether for its own sake or to determine r call,involves developing a stratification test, con-structing a prior density for each stratum, per-forming the presample and full samples, andcombining the results.
Of these steps, stratifi-cation is the most important in reducing thecredibility interval.
In our work to date wehave achieved good results with stratificationtests that are conceptually and computationallysimple.
We suspect that when asking multiplequestions of the same corpus, it may even bepossible to automate the construction of strati-fication scripts.
Priors are easiest o constructif the strata are clean and well-understood.The appropriate amount of time to investin refining a stratification test and the associ-ated priors depends on the cost of evaluatingdocuments and the importance of a smallcredibility interval.
If documents are easy toevaluate, one might choose to put less time intostratification and priors construction, and re-duce the credibility interval by increasingsample size.
If one is restricted to a smallsample, then accurate stratification and goodpriors are more important.
If one requires anextremely tight confidence interval, then care-ful stratification and prior construction, and agenerous ample, are all recommended.AcknowledgmentsLos Alamos National Laboratory is operatedby the University of California for the UnitedStates Department of Energy under contractW-7405-ENG-36.
We thank the TIPSTERprogram and the Linguistic Data Consortiumfor making the Federal Register corpus avail-able, and Mike Cannon and Tony Wamock forhelpful discussions on statistical issues.Re ferencesBayes T. (1763) An essay towards the solving aproblem in the doctrine of chances.
Phy.
Trans.Roy.
Soc.
London 53: 370--418.
Reprinted byBarnard G.A.
(1958) Biometrika, 45, pp.
293-315.Berger J. O.
(1980) Statistical Decision Theory andBayesian Analysis.
Spnnger-Verlag, New York.Cochran W. G. (1963).
Sampling Techniques.
Wi-ley, New York.Kalton G. (1983) Introduction to Survey Sampling.Sage University Paper seres on Quantitative Ap-plications in the Social Sciences 07-035.
Sage,Beverly Hills, California.Newbold P. (1971) Optimum allocation in stratifiedtwo-phase sampling for proportions.
Biometrika,58, pp.
681-3.Press W. B., Teukolsky S., Vetterling W., and Flan-nery B.
(1988)Numerical Recipes in C: The Art ofScientific Computing.
Cambridge UniversityPress, Cambridge, England.Shreider Y.A.
(1966) The Monte Carlo Method.Pergamon Press, New York.Thomas T., Scovel C., Kruger C, and Shumate J.
(1995) Text to information: Sampling uncertaintyin an example from physician/patient counters.
In"proceedings of the Fourth Annual Symposium onDocument Analysis and Information Retrieval",Information Science Research Institute, Universityof Las Vegas, pp.
347-58.Voorhees E. and Harman D. (1997) Overview of theFifth Text REtrieval Conference (TREC-5).
In"Proceedings o f  the Fifth Text REtrievalConference (TREC-5)", E. Voorhees & D.
Harman,ed., NIST Special Publication 500-238, pp.
1-28.8
