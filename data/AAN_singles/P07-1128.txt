Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016?1023,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsCombining Multiple Knowledge Sources for Dialogue Segmentation inMultimedia ArchivesPei-Yun HsuehSchool of InformaticsUniversity of EdinburghEdinburgh, UK EH8 9WLp.hsueh@ed.ac.ukJohanna D. MooreSchool of InformaticsUniversity of EdinburghEdinburgh, UK EH8 9WLJ.Moore@ed.ac.ukAbstractAutomatic segmentation is important formaking multimedia archives comprehensi-ble, and for developing downstream infor-mation retrieval and extraction modules.
Inthis study, we explore approaches that cansegment multiparty conversational speechby integrating various knowledge sources(e.g., words, audio and video recordings,speaker intention and context).
In particu-lar, we evaluate the performance of a Max-imum Entropy approach, and examine theeffectiveness of multimodal features on thetask of dialogue segmentation.
We also pro-vide a quantitative account of the effect ofusing ASR transcription as opposed to hu-man transcripts.1 IntroductionRecent advances in multimedia technologies haveled to huge archives of audio-video recordings ofmultiparty conversations in a wide range of areasincluding clinical use, online video sharing ser-vices, and meeting capture and analysis.
While itis straightforward to replay such recordings, find-ing information from the often lengthy archives is amore challenging task.
Annotating implicit seman-tics to enhance browsing and searching of recordedconversational speech has therefore posed new chal-lenges to the field of multimedia information re-trieval.One critical problem is how to divide unstructuredconversational speech into a number of locally co-herent segments.
The problem is important for tworeasons: First, empirical analysis has shown that an-notating transcripts with semantic information (e.g.,topics) enables users to browse and find informationfrom multimedia archives more efficiently (Baner-jee et al, 2005).
Second, because the automaticallygenerated segments make up for the lack of explicitorthographic cues (e.g., story and paragraph breaks)in conversational speech, dialogue segmentationis useful in many spoken language understandingtasks, including anaphora resolution (Grosz and Sid-ner, 1986), information retrieval (e.g., as input forthe TREC Spoken Document Retrieval (SDR) task),and summarization (Zechner and Waibel, 2000).This study therefore aims to explore whether aMaximum Entropy (MaxEnt) classifier can inte-grate multiple knowledge sources for segmentingrecorded speech.
In this paper, we first evaluate theeffectiveness of features that have been proposed inprevious work, with a focus on features that can beextracted automatically.
Second, we examine otherknowledge sources that have not been studied sys-tematically in previous work, but which we expectto be good predictors of dialogue segments.
In ad-dition, as our ultimate goal is to develop an infor-mation retrieval module that can be operated in afully automatic fashion, we also investigate the im-pact of automatic speech recognition (ASR) errorson the task of dialogue segmentation.2 Previous WorkIn previous work, the problem of automatic dia-logue segmentation is often considered as similar tothe problem of topic segmentation.
Therefore, re-search has adopted techniques previously developed1016to segment topics in text (Kozima, 1993; Hearst,1997; Reynar, 1998) and in read speech (e.g., broad-cast news) (Ponte and Croft, 1997; Allan et al,1998).
For example, lexical cohesion-based algo-rithms, such as LCSEG (Galley et al, 2003), or itsword frequency-based predecessor TextTile (Hearst,1997) capture topic shifts by modeling the similarityof word repetition in adjacent windows.However, recent work has shown that LCSEG isless successful in identifying ?agenda-based conver-sation segments?
(e.g., presentation, group discus-sion) that are typically signalled by differences ingroup activity (Hsueh and Moore, 2006).
This isnot surprising since LCSEG considers only lexicalcohesion.
Previous work has shown that training asegmentation model with features that are extractedfrom knowledge sources other than words, such asspeaker interaction (e.g., overlap rate, pause, andspeaker change) (Galley et al, 2003), or partici-pant behaviors, e.g., note taking cues (Banerjee andRudnicky, 2006), can outperform LCSEG on similartasks.In many other fields of research, a variety of fea-tures have been identified as indicative of segmentboundaries in different types of recorded speech.For example, Brown et al (1980) have shown thata discourse segment often starts with relatively highpitched sounds and ends with sounds of pitch withina more compressed range.
Passonneau and Lit-man (1993) identified that topic shifts often occurafter a pause of relatively long duration.
Otherprosodic cues (e.g., pitch contour, energy) have beenstudied for their correlation with story segments inread speech (Tur et al, 2001; Levow, 2004; Chris-tensen et al, 2005) and with theory-based discoursesegments in spontaneous speech (e.g., direction-given monologue) (Hirschberg and Nakatani, 1996).In addition, head and hand/forearm movements areused to detect group-action based segments (Mc-Cowan et al, 2005; Al-Hames et al, 2005).However, many other features that we expect tosignal segment boundaries have not been studiedsystematically.
For instance, speaker intention (i.e.,dialogue act types) and conversational context (e.g.,speaker role).
In addition, although these featuresare expected to be complementary to one another,few of the previous studies have looked at the ques-tion how to use conditional approaches to model thecorrelation among features.3 Methodology3.1 Meeting CorpusThis study aims to explore approaches that can in-tegrate multimodal information to discover implicitsemantics from conversation archives.
As our goalis to identify multimodal cues of segmentation inface-to-face conversation, we use the AMI meetingcorpus (Carletta et al, 2006), which includes audio-video recordings, to test our approach.
In particu-lar, we are using 50 scenario-based meetings fromthe AMI corpus, in which participants are assignedto different roles and given specific tasks related todesigning a remote control.
On average, AMI meet-ings last 26 minutes, with over 4,700 words tran-spired.
This corpus includes annotation for dialoguesegmentation and topic labels.
In the annotation pro-cess, annotators were given the freedom to subdi-vide a segment into subsegments to indicate whenthe group was discussing a subtopic.
Annotatorswere also given a set of segment descriptions to beused as labels.
Annotators were instructed to add anew label only if they could not find a match in thestandard set.
The set of segment descriptions canbe divided to three categories: activity-based (e.g.,presentation, discussion), issue-based (e.g., budget,usability), and functional segments (e.g., chitchat,opening, closing).3.2 PreprocessingThe first step is to break a recorded meeting intominimal units, which can vary from sentence chunksto blocks of sentences.
In this study, we use spurts,that is, consecutive speech with no pause longer than0.5 seconds, as minimal units.Then, to examine the difference between the setof features that are characteristic of segmentation atboth coarse and fine levels of granularity, this studycharacterizes a dialogue as a sequence of segmentsthat may be further divided into sub-segments.
Wetake the theory-free dialogue segmentation annota-tions in the corpus and flatten the sub-segment struc-ture and consider only two levels of segmentation:top-level segments and all sub-level segments.1 We1We take the spurts which the annotators choose as the be-ginning of a segment as the topic boundaries.
On average,1017observed that annotators tended to annotate activity-based segments only at the top level, whereas theyoften included sub-topics when segmenting issue-based segments.
For example, a top-level interfacespecialist presentation segment can be divided intoagenda/equipment issues, user requirements, exist-ing products, and look and usability sub-level seg-ments.3.3 Intercoder AgreementTo measure intercoder agreement, we employ threedifferent metrics: the kappa coefficient, PK, andWD.
Kappa values measure how well a pair of an-notators agree on where the segments break.
PK isthe probability that two spurts drawn randomly froma document are incorrectly identified as belongingto the same segment.
WindowDiff (WD) calculatesthe error rate by moving a sliding window across thetranscript counting the number of times the hypoth-esized and reference segment boundaries are differ-ent.
While not uncontroversial, the use of these met-rics is widespread.
Table 1 shows the intercoderagreement of the top-level and sub-level segmenta-tion respectively.It is unclear whether the kappa values shown hereindicate reliable intercoder agreement.2 But giventhe low disagreement rate among codings in termsof the PK and WD scores, we will argue for the reli-ability of the annotation procedure used in this study.Also, to our knowledge the reported degree of agree-ment is the best in the field of meeting dialogue seg-mentation.3Intercoder Kappa PK WDTOP 0.66 0.11 0.17SUB 0.59 0.23 0.28Table 1: Intercoder agreement of annotations at thetop-level (TOP) and sub-level (SUB) segments.the annotators marked 8.7 top-level segments and 14.6 sub-segments per meeting.2In computational linguistics, kappa values over 0.67point to reliable intercoder agreement.
But Di Eugenio andGlass (2004) have found that this interpretation does not holdtrue for all tasks.3For example, Gruenstein et al(2005) report kappa(PK/WD) of 0.41(0.28/0.34) for determining the top-level and0.45(0.27/0.35) for the sub-level segments in the ICSI meetingcorpus.3.4 Feature ExtractionAs reported in Section 2, there is a wide range offeatures that are potentially characteristic of segmentboundaries, and we expect to find some of them use-ful for automatic recognition of segment boundaries.The features we explore can be divided into the fol-lowing five classes:Conversational Features: We follow Galley etal.
(2003) and extracted a set of conversational fea-tures, including the amount of overlapping speech,the amount of silence between speaker segments,speaker activity change, the number of cue words,and the predictions of LCSEG (i.e., the lexical co-hesion statistics, the estimated posterior probability,the predicted class).Lexical Features: We compile the list of wordsthat occur more than once in the spurts that havebeen marked as a top-level or sub-segment boundaryin the training set.
Each spurt is then represented asa vector space of unigrams from this list.Prosodic Features: We use the direct modellingapproach proposed in Shriberg and Stolcke (2001)and include maximum F0 and energy of the spurt,mean F0 and energy of the spurt, pitch contour (i.e.,slope) and energy at multiple points (e.g., the firstand last 100 and 200 ms, the first and last quarter,the first and second half) of a spurt.
We also includerate of speech, in-spurt silence, preceding and sub-sequent pauses, and duration.
The rate of speech iscalculated as both the number of words and the num-ber of syllables spoken per second.Motion Features: We measure the magnitudeof relevant movements in the meeting room usingmethods that detect movements directly from videorecordings in frames of 40 ms. Of special interest arethe frontal shots as recorded by the close up cameras,the hand movements as recorded by the overviewcameras, and shots of the areas of the room wherepresentations are made.
We then average the magni-tude of movements over the frames within a spurt asits feature value.Contextual Features: These include dialogue acttype4 and speaker role (e.g., project manager, mar-4In the annotations, each dialogue act is classified as oneof 15 types, including acts about information exchange (e.g.,Inform), acts about possible actions (e.g., Suggest), acts whoseprimary purpose is to smooth the social functioning (e.g., Be-positive), acts that are commenting on previous discussion (e.g.,1018keting expert).
As each spurt may consist of multipledialogue acts, we represent each spurt as a vector ofdialogue act types, wherein a component is 1 or 0depending on whether the type occurs in the spurt.3.5 Multimodal Integration Using MaximumEntropy ModelsPrevious work has used MaxEnt models for sentenceand topic segmentation and shown that conditionalapproaches can yield competitive results on thesetasks (Christensen et al, 2005; Hsueh and Moore,2006).
In this study, we also use a MaxEnt clas-sifier5 for dialogue segmentation under the typicalsupervised learning scheme, that is, to train the clas-sifier to maximize the conditional likelihood overthe training data and then to use the trained modelto predict whether an unseen spurt in the test setis a segment boundary or not.
Because continuousfeatures have to be discretized for MaxEnt, we ap-plied a histogram binning approach, which dividesthe value range intoN intervals that contain an equalnumber of counts as specified in the histogram, todiscretize the data.4 Experimental Results4.1 Probabilistic ModelsThe first question we want to address is whetherthe different types of characteristic multimodal fea-tures can be integrated, using the conditional Max-Ent model, to automatically detect segment bound-aries.
In this study, we use a set of 50 meet-ings, which consists of 17,977 spurts.
Among thesespurts, only 1.7% and 3.3% are top-level and sub-segment boundaries.
For our experiments we use10-fold cross validation.
The baseline is the re-sult obtained by using LCSEG, an unsupervised ap-proach exploiting only lexical cohesion statistics.Table 2 shows the results obtained by using thesame set of conversational (CONV) features usedin previous work (Galley et al, 2003; Hsueh andMoore, 2006), and results obtained by using all theavailable features (ALL).
The evaluation metrics PKand WD are conventional measures of error rates insegmentation (see Section 3.3).
In Row 2, we seeElicit-Assessment), and acts that allow complete segmentation(e.g., Stall).5The parameters of the MaxEnt classifier are optimized us-ing Limited-Memory Variable Metrics.TOP SUBError Rate PK WD PK WDBASELINE(LCSEG) 0.40 0.49 0.40 0.47MAXENT(CONV) 0.34 0.34 0.37 0.37MAXENT(ALL) 0.30 0.33 0.34 0.36Table 2: Compare the result of MaxEnt modelstrained with only conversational features (CONV)and with all available features (ALL).that using a MaxEnt classifier trained on the conver-sational features (CONV) alone improves over theLCSEG baseline by 15.3% for top-level segmentsand 6.8% for sub-level segements.
Row 3 showsthat combining additional knowledge sources, in-cluding lexical features (LX1) and the non-verbalfeatures, prosody (PROS), motion (MOT), and con-text (CTXT), yields a further improvement (of 8.8%for top-level segmentation and 5.4% for sub-levelsegmentation) over the model trained on conversa-tional features.4.2 Feature EffectsThe second question we want to address is whichknowledge sources (and combinations) are goodpredictors for segment boundaries.
In this round ofexperiments, we evaluate the performance of differ-ent feature combinations.
Table 3 further illustratesthe impact of each feature class on the error ratemetrics (PK/WD).
In addition, as the PK and WDscore do not reflect the magnitude of over- or under-prediction, we also report on the average number ofhypothesized segment boundaries (Hyp).
The num-ber of reference segments in the annotations is 8.7 atthe top-level and 14.6 at the sub-level.Rows 2-6 in Table 3 show the results of modelstrained with each individual feature class.
We per-formed a one-way ANOVA to examine the effectof different feature classes.
The ANOVA suggestsa reliable effect of feature class (F (5, 54) = 36.1;p < .001).
We performed post-hoc tests (TukeyHSD) to test for significant differences.
Analysisshows that the model that is trained with lexicalfeatures alone (LX1) performs significantly worsethan the LCSEG baseline (p < .001).
This isdue to the fact that cue words, such as okay andnow, learned from the training data to signal seg-1019TOP SUBHyp PK WD Hyp PK WDBASELINE 17.6 0.40 0.49 17.6 0.40 0.47(LCSEG)LX1 61.2 0.53 0.72 65.1 0.49 0.66CONV 3.1 0.34 0.34 2.9 0.37 0.37PROS 2.3 0.35 0.35 2.5 0.37 0.37MOT 96.2 0.36 0.40 96.2 0.38 0.41CTXT 2.6 0.34 0.34 2.2 0.37 0.37ALL 7.7 0.29 0.33 7.6 0.35 0.38Table 3: Effects of individual feature classes andtheir combination on detecting segment boundaries.ment boundaries, are often used for non-discoursepurposes, such as making a semantic contribution toan utterance.6 Thus, we hypothesize that these am-biguous cue words have led the LX1 model to over-predict.
Row 7 further shows that when all avail-able features (including LX1) are used, the com-bined model (ALL) yields performance that is sig-nificantly better than that obtained with individualfeature classes (F (5, 54) = 32.2; p < .001).TOP SUBHyp PK WD Hyp PK WDALL 7.7 0.29 0.33 7.6 0.35 0.38ALL-LX1 3.9 0.35 0.35 3.5 0.37 0.38ALL-CONV 6.6 0.30 0.34 6.8 0.35 0.37ALL-PROS 5.6 0.29 0.31 7.4 0.33 0.35ALL-MOTION 7.5 0.30 0.35 7.3 0.35 0.37ALL-CTXT 7.2 0.29 0.33 6.7 0.36 0.38Table 4: Performance change of taking out eachindividual feature class from the ALL model.Table 4 illustrates the error rate change (i.e., in-creased or decreased PK and WD score)7 that isincurred by leaving out one feature class from theALL model.
Results show that CONV, PROS, MO-TION and CTXT can be taken out from the ALLmodel individually without increasing the error ratesignificantly.8 Morevoer, the combined models al-6Hirschberg and Litman (1987) have proposed to discrimi-nate the different uses intonationally.7Note that the increase in error rate indicates performancedegradation, and vice versa.8Sign tests were used to test for significant differences be-tween means in each fold of cross validation.ways perform better than the LX1 model (p < .01),cf.
Table 3.This suggests that the non-lexical feature classesare complementary to LX1, and thus it is essentialto incorporate some, but not necessarily all, of thenon-lexical classes into the model.TOP SUBHyp PK WD Hyp PK WDLX1 61.2 0.53 0.72 65.1 0.49 0.66MOT 96.2 0.36 0.40 96.2 0.38 0.41LX1+CONV 5.3 0.27 0.30 6.9 0.32 0.35LX1+PROS 6.2 0.30 0.33 7.3 0.36 0.38LX1+MOT 20.2 0.39 0.49 24.8 0.39 0.47LX1+CTXT 6.3 0.28 0.31 7.2 0.33 0.35MOT+PROS 62.0 0.34 0.34 62.1 0.37 0.37MOT+CTXT 2.7 0.33 0.33 2.3 0.37 0.37Table 5: Effects of combining complementary fea-tures on detecting segment boundaries.Table 5 further illustrates the performance of dif-ferent feature combinations on detecting segmentboundaries.
By subtracting the PK or WD score inRow 1, the LX1 model, from that in Rows 3-6, wecan tell how essential each of the non-lexical classesis to be combined with LX1 into one model.
Resultsshow that CONV is the most essential, followed byCTXT, PROS and MOT.
The advantage of incorpo-rating the non-lexical feature classes is also shownin the noticeably reduced number of overpredictionsas compared to that of the LX1 model.To analyze whether there is a significant interac-tion between feature classes, we performed anotherround of ANOVA tests to examine the effect of LX1and each of the non-lexical feature classes on de-tecting segment boundaries.
This analysis showsthat there is a significant interaction effect on de-tecting both top-level and sub-level segment bound-aries (p < .01), suggesting that the performance ofLX1 is significantly improved when combined withany non-lexical feature class.
Also, among the non-lexical feature classes, combining prosodic featuressignificantly improves the performance of the modelin which the motion features are combined to detecttop-level segment boundaries (p < .05).10204.3 Degradation Using ASRThe third question we want to address here iswhether using the output of ASR will cause sig-nificant degradation to the performance of the seg-mentation approaches.
The ASR transcripts used inthis experiment are obtained using standard technol-ogy including HMM based acoustic modeling andN-gram based language models (Hain et al, 2005).The average word error rates (WER) are 39.1%.
Wealso applied a word alignment algorithm to ensurethat the number of words in the ASR transcripts isthe same as that in the human-produced transcripts.In this way we can compare the PK and WD metricsobtained on the ASR outputs directly with that onthe human transcripts.In this study, we again use a set of 50 meetingsand 10-fold cross validation.
We compare the per-formance of the reference models, which are trainedon human transcripts and tested on human tran-scripts, with that of the ASR models, which aretrained on ASR transcripts and tested on ASR tran-scripts.
Table 6 shows that despite the word recogni-tion errors, none of the LCSEG, the MaxEnt modelstrained with conversational features, and the Max-Ent models trained with all available features per-form significantly worse on ASR transcripts than onreference transcripts.
One possible explanation forthis, which we have observed in our corpus, is thatthe ASR system is likely to mis-recognize differentoccurences of words in the same way, and thus thelexical cohesion statistic, which captures the similar-ity of word repetition between two adjacency win-dows, is also likely to remain unchanged.
In addi-tion, when the models are trained with other featuresthat are not affected by the recognition errors, suchas pause and overlap, the negative impacts of recog-nition errors are further reduced to an insignificantlevel.5 DiscussionThe results in Section 4 show the benefits of includ-ing additional knowledge sources for recognizingsegment boundaries.
The next question to be ad-dressed is what features in these sources are mostuseful for recognition.
To provide a qualitative ac-count of the segmentation cues, we performed ananalysis to determine whether each proposed featureTOP SUBError Rate PK WD PK WDLCSEG(REF) 0.45 0.57 0.42 0.47LCSEG(ASR) 0.45 0.58 0.40 0.47MAXENT-CONV(REF) 0.34 0.34 0.37 0.37MAXENT-CONV(ASR) 0.34 0.33 0.38 0.38MAXENT-ALL(REF) 0.30 0.33 0.34 0.36MAXENT-ALL(ASR) 0.31 0.34 0.34 0.37Table 6: Effects of word recognition errors on de-tecting segments boundaries.discriminates the class of segment boundaries.
Pre-vious work has identified statistical measures (e.g.,Log Likelihood ratio) that are useful for determin-ing the statistical association strength (relevance) ofthe occurrence of an n-gram feature to target class(Hsueh and Moore, 2006).
Here we extend thatstudy to calculate the LogLikelihood relevance of allof the features used in the experiments, and use thestatistics to rank the features.Our analysis shows that people do speak and be-have differently near segment boundaries.
Someof the identified segmentation cues match previousfindings.
For example, a segment is likely to startwith higher pitched sounds (Brown et al, 1980; Ay-ers, 1994) and a lower rate of speech (Lehiste, 1980).Also, interlocutors pause longer than usual to makesure that everyone is ready to move on to a new dis-cussion (Brown et al, 1980; Passonneau and Lit-man, 1993) and use some conventional expressions(e.g., now, okay, let?s, um, so).Our analysis also identified segmentation cuesthat have not been mentioned in previous research.For example, interlocutors do not move around a lotwhen a new discussion is brought up; interlocutorsmention agenda items (e.g., presentation, meeting)or content words more often when initiating a newdiscussion.
Also, from the analysis of current di-alogue act types and their immediate contexts, wealso observe that at segment boundaries interlocu-tors do the following more often than usual: startspeaking before they are ready (Stall), give infor-mation (Inform), elicit an assessment of what hasbeen said so far (Elicit-assessment), or act to smoothsocial functioning and make the group happier (Be-positive).10216 Conclusions and Future WorkThis study explores the use of features from mul-tiple knowledge sources (i.e., words, prosody, mo-tion, interaction cues, speaker intention and role) fordeveloping an automatic segmentation componentin spontaneous, multiparty conversational speech.In particular, we addressed the following questions:(1) Can a MaxEnt classifier integrate the potentiallycharacteristic multimodal features for automatic di-alogue segmentation?
(2) What are the most dis-criminative knowledge sources for detecting seg-ment boundaries?
(3) Does the use of ASR tran-scription significantly degrade the performance of asegmentation model?First of all, our results show that a well perform-ing MaxEnt model can be trained with availableknowledge sources.
Our results improve on previouswork, which uses only conversational features, by8.8% for top-level segmentation and 5.4% for sub-level segmentation.
Analysis of the effectiveness ofthe various features shows that lexical features (i.e.,cue words) are the most essential feature class tobe combined into the segmentation model.
How-ever, lexical features must be combined with otherfeatures, in particular, conversational features (i.e.,lexical cohesion, overlap, pause, speaker change), totrain well performing models.In addition, many of the non-lexical featureclasses, including those that have been identified asindicative of segment boundaries in previous work(e.g., prosody) and those that we hypothesized asgood predictors of segment boundaries (e.g., mo-tion, context), are not beneficial for recognizingboundaries when used in isolation.
However, thesenon-lexical features are useful when combined withlexical features, as the presence of the non-lexicalfeatures can balance the tendency of models trainedwith lexical cues alone to overpredict.Experiments also show that it is possible to seg-ment conversational speech directly on the ASR out-puts.
These results encouragingly show that wecan segment conversational speech using featuresextracted from different knowledge sources, and inturn, facilitate the development of a fully automaticsegmentation component for multimedia archives.With the segmentation models developed and dis-criminative knowledge sources identified, a remain-ing question is whether it is possible to automat-ically select the discriminative features for recog-nition.
This is particularly important for prosodicfeatures, because the direct modelling approach weadopted resulted in a large number of features.
Weexpect that by applying feature selection methodswe can further improve the performance of auto-matic segmentation models.
In the field of machinelearning and pattern analysis, many methods and se-lection criteria have been proposed.
Our next stepwill be to examine the effectiveness of these meth-ods for the task of automatic segmentation.
Also, wewill further explore how to choose the best perform-ing ensemble of knowledge sources so as to facili-tate automatic selection of knowledge sources to beincluded.AcknowledgementThis work was supported by the EU 6th FWP IST In-tegrated Project AMI (Augmented Multi-party Inter-action, FP6-506811).
Our special thanks to WesselKraaij, Stephan Raaijmakers, Steve Renals, GabrielMurray, Jean Carletta, and the anonymous review-ers for valuable comments.
Thanks also to the AMIASR group for producing the ASR transcriptions,and to our research partners in TNO for generatingmotion features.ReferencesM.
Al-Hames, A. Dielmann, D. GaticaPerez, S. Reiter,S.
Renals, and D. Zhang.
2005.
Multimodal integra-tion for meeting group action segmentation and recog-nition.
In Proc.
of MLMI 2005.J.
Allan, J. Carbonell, G. Doddington, J. Yamron, andY.
Yang.
1998.
Topic detection and tracking pilotstudy: Final report.
In Proc.
of the DARPA BroadcastNews Transcription and Understanding Workshop.G.
M. Ayers.
1994.
Discourse functions of pitch range inspontaneous and read speech.
In Jennifer J. Venditti,editor, OSU Working Papers in Linguistics, volume 44,pages 1?49.S.
Banerjee and A. Rudnicky.
2006.
Segmenting meet-ings into agenda items by extracting implicit supervi-sion from human note-taking.
In Proc.
of IUI 2006.S.
Banerjee, C. Rose, and A. I. Rudnicky.
2005.
Thenecessity of a meeting recording and playback system,and the benefit of topic-level annotations to meeting1022browsing.
In Proc.
of the Tenth International Confer-ence on Human-Computer Interaction.G.
Brown, K. L. Currie, and J. Kenworthe.
1980.
Ques-tions of Intonation.
University Park Press.J.
Carletta et al 2006.
The AMI meeting corpus: A pre-announcement.
In Steve Renals and Samy Bengio, ed-itors, Springer-Verlag Lecture Notes in Computer Sci-ence, volume 3869.
Springer-Verlag.H.
Christensen, B. Kolluru, Y. Gotoh, and S. Renals.2005.
Maximum entropy segmentation of broadcastnews.
In Proc.
of ICASP, Philadelphia USA.B.
Di Eugenio and M. G. Glass.
2004.
The kappastatistic: A second look.
Computational Linguistics,30(1):95?101.M.
Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.2003.
Discourse segmentation of multi-party conver-sation.
In Proc.
of ACL 2003.B.
Grosz and C. Sidner.
1986.
Attention, intentions, andthe structure of discourse.
Computational Linguistics,12(3).A.
Gruenstein, J. Niekrasz, and M. Purver.
2005.
Meet-ing structure annotation: Data and tools.
In Proc.
ofthe SIGdial Workshop on Discourse and Dialogue.T.
Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,V.
Wan, R. Ordelman, and S. Renals.
2005.
Tran-scription of conference room meetings: An investiga-tion.
In Proc.
of Interspeech 2005.M.
Hearst.
1997.
TextTiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguis-tics, 25(3):527?571.J.
Hirschberg and D. Litman.
1987.
Now let?s talk aboutnow: identifying cue phrases intonationally.
In Proc.of ACL 1987.J.
Hirschberg andC.
H. Nakatani.
1996.
A prosodic anal-ysis of discourse segments in direction-giving mono-logues.
In Proc.
of ACL 1996.P.
Hsueh and J.D.
Moore.
2006.
Automatic topic seg-mentation and lablelling in multiparty dialogue.
In thefirst IEEE/ACM workshop on Spoken Language Tech-nology (SLT) 2006.H.
Kozima.
1993.
Text segmentation based on similaritybetween words.
In Proc.
of ACL 1993.I.
Lehiste.
1980.
Phonetic characteristics of discourse.In the Meeting of the Committee on Speech Research,Acoustical Society of Japan.G.
Levow.
2004.
Prosody-based topic segmentation formandarin broadcast news.
In Proc.
of HLT 2004.I.
McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud,M.
Barnard, and D. Zhang.
2005.
Automatic analysisof multimodal group actions in meetings.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence(PAMI), 27(3):305?317.R.
Passonneau and D. Litman.
1993.
Intention-basedsegmentation: Human reliability and correlation withlinguistic cues.
In Proc.
of ACL 1993.J.
Ponte and W. Croft.
1997.
Text segmentation by topic.In Proc.
of the Conference on Research and AdvancedTechnology for Digital Libraries 1997.J.
Reynar.
1998.
Topic Segmentation: Algorithms andApplications.
Ph.D. thesis, UPenn, PA USA.E.
Shriberg and A. Stolcke.
2001.
Direct modeling ofprosody: An overview of applications in automaticspeech processing.
In Proc.
International Conferenceon Speech Prosody 2004.G.
Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg.2001.
Integrating prosodic and lexical cues for auto-matic topic segmentation.
Computational Linguistics,27(1):31?57.K.
Zechner and A. Waibel.
2000.
DIASUMM: Flexi-ble summarization of spontaneous dialogues in unre-stricted domains.
In Proc.
of COLING-2000.1023
