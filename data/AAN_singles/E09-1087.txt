Proceedings of the 12th Conference of the European Chapter of the ACL, pages 763?771,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSemi-supervised Training for the Averaged Perceptron POS TaggerDrahom?
?ra ?johanka?
Spoustova?
Jan Hajic?
Jan Raab Miroslav SpoustaInstitute of Formal and Applied LinguisticsFaculty of Mathematics and Physics,Charles University Prague, Czech Republic{johanka,hajic,raab,spousta}@ufal.mff.cuni.czAbstractThis paper describes POS tagging exper-iments with semi-supervised training asan extension to the (supervised) averagedperceptron algorithm, first introduced forthis task by (Collins, 2002).
Experimentswith an iterative training on standard-sizedsupervised (manually annotated) dataset(106 tokens) combined with a relativelymodest (in the order of 108 tokens) un-supervised (plain) data in a bagging-likefashion showed significant improvementof the POS classification task on typo-logically different languages, yielding bet-ter than state-of-the-art results for Englishand Czech (4.12 % and 4.86 % relative er-ror reduction, respectively; absolute accu-racies being 97.44 % and 95.89 %).1 IntroductionSince 2002, we have seen a renewed interest inimproving POS tagging results for English, andan inflow of results (initial or improved) for manyother languages.
For English, after a relatively bigjump achieved by (Collins, 2002), we have seentwo significant improvements: (Toutanova et al,2003) and (Shen et al, 2007) pushed the resultsby a significant amount each time.11In our final comparison, we have also included the re-sults of (Gime?nez and Ma`rquez, 2004), because it has sur-passed (Collins, 2002) as well and we have used this tag-ger in the data preparation phase.
See more details below.Most recently, (Suzuki and Isozaki, 2008) published theirSemi-supervised sequential labelling method, whose resultson POS tagging seem to be optically better than (Shen et al,2007), but no significance tests were given and the tool is notavailable for download, i.e.
for repeating the results and sig-nificance testing.
Thus, we compare our results only to thetools listed above.Even though an improvement in POS taggingmight be a questionable enterprise (given that itseffects on other tasks, such as parsing or otherNLP problems are less than clear?at least for En-glish), it is still an interesting problem.
Moreover,the ?ideal?2 situation of having a single algorithm(and its implementation) for many (if not all) lan-guages has not been reached yet.
We have cho-sen Collins?
perceptron algorithm because of itssimplicity, short training times, and an apparentroom for improvement with (substantially) grow-ing data sizes (see Figure 1).
However, it is clearthat there is usually little chance to get (substan-tially) more manually annotated data.
Thus, wehave been examining the effect of adding a largemonolingual corpus to Collins?
perceptron, appro-priately extended, for two typologically differentlanguages: English and Czech.
It is clear howeverthat the features (feature templates) that the tag-gers use are still language-dependent.One of the goals is also to have a fast im-plementation for tagging large amounts of dataquickly.
We have experimented with various clas-sifier combination methods, such as those de-scribed in (Brill and Wu, 1998) or (van Halteren etal., 2001), and got improved results, as expected.However, we view this only as a side effect (yet, apositive one)?our goal was to stay on the turf ofsingle taggers, which are both the common groundfor competing on tagger accuracy today and alsosignificantly faster at runtime.3 Nevertheless, wehave found that it is advantageous to use them to(pre-)tag the large amounts of plain text data dur-2We mean easy to use for further research on problemsrequiring POS tagging, especially multilingual ones.3And much easier to (re)implement as libraries in proto-type systems, which is often difficult if not impossible withother people?s code.763Training data size (thousands of tokens)Accuracyon development data100 200 300 400 500 600 700 800 90096.096.597.097.598.0Figure 1: Accuracy of the original averaged per-ceptron, supervised training on PTB/WSJ (En-glish)ing the training phase.Apart from feeding the perceptron by variousmixtures of manually tagged (?supervised?)
andauto-tagged (?unsupervised?
)4 data, we have alsoused various feature templates extensively; for ex-ample, we use lexicalization (with the added twistof lemmatization, useful especially for Czech, aninflectionally rich language), ?manual?
tag clas-sification into large classes (again, useful espe-cially for Czech to avoid the huge, still-to-be-overcome data sparseness for such a language5),and sub-lexical features mainly targeted at OOVwords.
Inspired i.a.
by (Toutanova et al, 2003)and (Hajic?
and Vidova?-Hladka?, 1998), we also use?lookahead?
features (however, we still remainin the left-to-right HMM world ?
in this respectour solution is closer to the older work of (Hajic?and Vidova?-Hladka?, 1998) than to (Toutanova etal., 2003), who uses bidirectional dependenciesto include the right-hand side disambiguated tags,4For brevity, we will use the terms ?supervised?
and ?un-supervised?
data for ?manually annotated?
and ?
(automat-ically annotated) plain (raw) text?
data, respectively, eventhough these adjectives are meant to describe the process oflearning, not the data themselves.5As (Hajic?, 2004) writes, Czech has 4400 plausible tags,of which we have observed almost 2000 in the 100M cor-pus we have used in our experiments.
However, only 1100of them have been found in the manually annotated PDT 2.0corpus (the corpus on which we have based the supervisedexperiments).
The situation with word forms (tokens) is evenworse: Czech has about 20M different word forms, and theOOV rate based on the 1.5M PDT 2.0 data and measuredagainst the 100M raw corpus is almost 10 %.which we cannot.
)To summarize, we can describe our system asfollows: it is based on (Votrubec, 2006)?s imple-mentation of (Collins, 2002), which has been fedat each iteration by a different dataset consistingof the supervised and unsupervised part: precisely,by a concatenation of the manually tagged trainingdata (WSJ portion of the PTB 3 for English, mor-phologically disambiguated data from PDT 2.0 forCzech) and a chunk of automatically tagged unsu-pervised data.
The ?parameters?
of the trainingprocess (feature templates, the size of the unsu-pervised chunks added to the trainer at each itera-tion, number of iterations, the combination of tag-gers that should be used in the auto-tagging of theunsupervised chunk, etc.)
have been determinedempirically in a number of experiments on a de-velopment data set.
We should also note that as aresult of these development-data-based optimiza-tions, no feature pruning has been employed (seeSection 4 for details); adding (even lexical) fea-tures from the auto-tagged data did not give signif-icant accuracy improvements (and only made thetraining very slow).The final taggers have surpassed the currentstate-of-the-art taggers by significant margins (wehave achieved 4.12 % relative error reduction forEnglish and 4.86 % for Czech over the best pre-viously published results, single or combined),using a single tagger.
However, the best En-glish tagger combining some of the previous state-of-the-art ones is still ?optically?
better (yet notsignificantly?see Section 6).2 The perceptron algorithmWe have used the Morc?e6 tagger (Votrubec, 2006)as a main component in our experiments.
It is areimplementation of the averaged perceptron de-scribed in (Collins, 2002), which uses such fea-tures that it behaves like an HMM tagger and thusthe standard Viterbi decoding is possible.
Collins?GEN(x) set (a set of possible tags at any givenposition) is generated, in our case, using a mor-phological analyzer for the given language (essen-6The name ?Morc?e?
stands for ?MORfologie C?Es?tiny?
(?Czech morphology?, see (Votrubec, 2006)), since ithas been originally developed for Czech.
We keep thisname in this paper as the generic name of the aver-aged perceptron tagger for the English-language experi-ments as well.
We have used the version available athttp://ufal.mff.cuni.cz/morce/.764tially, a dictionary that returns all possible tags7for an input word form).
The transition and out-put scores for the candidate tags are based on alarge number of binary-valued features and theirweights, which are determined during iterativetraining by the averaged perceptron algorithm.The binary features describe the tag being pre-dicted and its context.
They can be derived fromany information we already have about the text atthe point of decision (respecting the HMM-basedoverall setting).
Every feature can be true or falsein a given context, so we can consider the true fea-tures at the current position to be the descriptionof a tag and its context.For every feature, the perceptron keeps itsweight coefficient, which is (in its basic version)an integer number, (possibly) changed at everytraining sentence.
After its final update, this in-teger value is stored with the feature to be laterretrieved and used at runtime.
Then, the task ofthe perceptron algorithm is to sum up all the co-efficients of true features in a given context.
Theresult is passed to the Viterbi algorithm as a tran-sition and output weight for the current state.8 Wecan express it asw(C, T ) =n?i=1?i.
?i(C, T ) (1)where w(C, T ) is the transition weight for tag Tin context C, n is the number of features, ?i is theweight coefficient of the ith feature and ?i(C, T )is the evaluation of the ith feature for context Cand tag T .
In the averaged perceptron, the val-ues of every coefficient are added up at each up-date, which happens (possibly) at each trainingsentence, and their arithmetic average is used in-stead.9 This trick makes the algorithm more re-sistant to weight oscillations during training (or,more precisely, at the end of it) and as a result, itsubstantially improves its performance.107And lemmas, which are then used in some of the fea-tures.
A (high recall, low precision) ?guesser?
is used forOOV words.8Which identifies unambiguously the corresponding tag.9Implementation note: care must be taken to avoid inte-ger overflows, which (at 100 iterations through millions ofsentences) can happen for 32bit integers easily.10Our experiments have shown that using averaging helpstremendously, confirming both the theoretical and practicalresults of (Collins, 2002).
On Czech, using the best featureset, the difference on the development data set is 95.96 % vs.95.02 %.
Therefore, all the results presented in the followingtext use averaging.The supervised training described in (Collins,2002) uses manually annotated data for the esti-mation of the weight coefficients ?.
The train-ing algorithm is very simple?only integer num-bers (counts and their sums for the averaging) areupdated for each feature at each sentence withimperfect match(es) found against the gold stan-dard.
Therefore, it can be relatively quickly re-trained and thus many different feature sets andother training parameters, such as the number ofiterations, feature thresholds etc.
can be con-sidered and tested.
As a result of this tuning,our (fully supervised) version of the Morc?e tag-ger gives the best accuracy among all single tag-gers for Czech and also very good results for En-glish, being beaten only by the tagger (Shen et al,2007) (by 0.10 % absolute) and (not significantly)by (Toutanova et al, 2003).3 The data3.1 The ?supervised?
dataFor English, we use the same data division of PennTreebank (PTB) parsed section (Marcus et al,1994) as all of (Collins, 2002), (Toutanova et al,2003), (Gime?nez and Ma`rquez, 2004) and (Shenet al, 2007) do; for details, see Table 1.data set tokens sentencestrain (0-18) 912,344 38,220dev-test (19-21) 131,768 5,528eval-test (22-24) 129,654 5,463Table 1: English supervised data set ?
WSJ partof Penn Treebank 3For Czech, we use the current standard PragueDependency Treebank (PDT 2.0) data sets (Hajic?et al, 2006); for details, see Table 2.data set tokens sentencestrain 1,539,241 91,049dev-test 201,651 11,880eval-test 219,765 13,136Table 2: Czech supervised data set ?
Prague De-pendency Treebank 2.03.2 The ?unsupervised?
dataFor English, we have processed the North Amer-ican News Text corpus (Graff, 1995) (without the765WSJ section) with the Stanford segmenter and to-kenizer (Toutanova et al, 2003).
For Czech, wehave used the SYN2005 part of Czech NationalCorpus (CNC, 2005) (with the original segmenta-tion and tokenization).3.3 GEN(x): The morphological analyzersFor English, we perform a very simple morpholog-ical analysis, which reduces the full PTB tagset toa small list of tags for each token on input.
The re-sulting list is larger than such a list derived solelyfrom the PTB/WSJ, but much smaller than a fulllist of tags found in the PTB/WSJ.11 The Englishmorphological analyzer is thus (empirically) opti-mized for precision while keeping as high recallas possible (it still overgenerates).
It consists of asmall dictionary of exceptions and a small set ofgeneral rules, thus covering also a lot of OOV to-kens.12For Czech, the separate morphological analyzer(Hajic?, 2004) usually precedes the tagger.
We usethe version from April 2006 (the same as (Spous-tova?
et al, 2007), who reported the best previousresult on Czech tagging).4 The perceptron feature setsThe averaged perceptron?s accuracy is determined(to a large extent) by the set of features used.
Afeature set is based on feature templates, i.e.
gen-eral patterns, which are filled in with concrete val-ues from the training data.
Czech and Englishare morphosyntactically very different languages,therefore each of them needs a different set offeature templates.
We have empirically testedhundreds of feature templates on both languages,taken over from previous works for direct compar-ison, inspired by them, or based on a combinationof previous experience, error analysis and linguis-tic intuition.In the following sections, we present the bestperforming set of feature templates as determinedon the development data set using only the super-vised training setting; our feature templates havethus not been influenced nor extended by the un-supervised data.1311The full list of tags, as used by (Shen et al, 2007), alsomakes the underlying Viterbi algorithm unbearably slow.12The English morphology tool is also downloadable as aseparate module on the paper?s accompanying website.13Another set of experiments has shown that there is not,perhaps surprisingly, a significant gain in doing so.4.1 English feature templatesThe best feature set for English consists of 30 fea-ture templates.
All templates predict the currenttag as a whole.
A detailed description of the En-glish feature templates can be found in Table 3.Context predicting whole tagTagsPrevious tagPrevious two tagsFirst letter of previous tagWord formsCurrent word formPrevious word formPrevious two word formsFollowing word formFollowing two word formsLast but one word formCurrent word affixesPrefixes of length 1-9Suffixes of length 1-9Current word featuresContains numberContains dashContains upper case letterTable 3: Feature templates for EnglishA total of 1,953,463 features has been extractedfrom the supervised training data using the tem-plates from Table 3.4.2 Czech feature templatesThe best feature set for Czech consists of 63 fea-ture templates.
26 of them predict current tag asa whole, whereas the rest predicts only some partsof the current tag separately (e.g., detailed POS,gender, case) to avoid data sparseness.
Such a fea-ture is true, in an identical context, for several dif-ferent tags belonging to the same class (e.g., shar-ing a locative case).
The individual grammaticalcategories used for such classing have been cho-sen on both linguistic grounds (POS, detailed fine-grained POS) and also such categories have beenused which contribute most to the elimination ofthe tagger errors (based on an extensive error anal-ysis of previous results, the detailed description ofwhich can be found in (Votrubec, 2006)).Several features can look ahead (to the rightof the current position) - apart from the obviousword form, which is unambiguous, we have used(in case of ambiguity) a random tag and lemma ofthe first position to the right from the current po-sition which might be occupied with a verb (basedon dictionary and the associated morphologicalguesser restrictions).A total of 8,440,467 features has been extractedfrom the supervised training data set.
A detaileddescription is included in the distribution down-loadable from the Morc?e website.7665 The (un)supervised training setupWe have extended the averaged perceptron setupin the following way: the training algorithm isfed, in each iteration, by a concatenation of thesupervised data (the manually tagged corpus) andthe automatically pre-tagged unsupervised data,different for each iteration (in this order).
Inother words, the training algorithm proper doesnot change at all: it is the data and their selection(including the selection of the way they are auto-matically tagged) that makes all the difference.The following ?parameters?
of the (unsuper-vised part of the) data selection had to be deter-mined experimentally:?
the tagging process for tagging the selecteddata?
the selection mechanism (sequential or ran-dom with/without replacement)?
the size to use for each iteration?
and the use and order of concatenation withthe manually tagged data.We have experimented with various settings toarrive at the best performing configuration, de-scribed below.
In each subsection, we comparethe result of our ,,winning?
configuration with re-sults of the experiments which have the selectedattributes omitted or changed; everything is mea-sured on the development data set.5.1 Tagging the plain dataIn order to simulate the labeled training events,we have tagged the unsupervised data simply bya combination of the best available taggers.
Forpractical reasons (to avoid prohibitive trainingtimes), we have tagged all the data in advance, i.e.no re-tagging is performed between iterations.The setup for the combination is as follows (theidea is simplified from (Spoustova?
et al, 2007)where it has been used in a more complex setting):1. run N different taggers independently;2. join the results on each position in the datafrom the previous step ?
each token thusends up with between 1 and N tags, a unionof the tags output by the taggers at that posi-tion;3. do final disambiguation (by a single tag-ger14).Tagger AccuracyMorc?e 97.21Shen 97.33Combination 97.44Table 4: Dependence on the tagger(s) used to tagthe additional plain text data (English)16Table 4 illustrates why it is advantageous to gothrough this (still)16 complicated setup against asingle-tagger bootstrapping mechanism, which al-ways uses the same tagger for tagging the unsu-pervised data.For both English and Czech, the selection oftaggers, the best combination and the best over-all setup has been optimized on the developmentdata set.
A bit surprisingly, the final setup is verysimilar for both languages (two taggers to tag thedata in Step 1, and a third one to finish it up).For English, we use three state-of-the-art tag-gers: the taggers of (Toutanova et al, 2003) and(Shen et al, 2007) in Step 1, and the SVM tag-ger (Gime?nez and Ma`rquez, 2004) in Step 3.
Werun the taggers with the parameters which wereshown to be the best in the corresponding papers.The SVM tagger needed to be adapted to acceptthe (reduced) list of possible tags.17For Czech, we use the Feature-based tagger(Hajic?, 2004) and the Morc?e tagger (with the newfeature set as described in section 4) in Step 1, andan HMM tagger (Krbec, 2005) in Step 3.
Thiscombination outperforms the results in (Spoustova?et al, 2007) by a small margin.5.2 Selection mechanism for the plain dataWe have found that it is better to feed the trainingwith different chunks of the unsupervised data ateach iteration.
We have then experimented with14This tagger (possibly different from any of theN taggersfrom Step 1) runs as usual, but it is given a minimal list of (atmost N ) tags that come from Step 2 only.15?Accuracy?
means accuracy of the semi-supervisedmethod using this tagger for pre-tagging the unsuperviseddata, not the accuracy of the tagger itself.16In fact, we have experimented with other taggercombinations and configurations as well?with the TnT(Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTag-ger (Schmid, 1994), with or without the Morc?e tagger in thepack; see below for the winning combination.17This patch is available on the paper?s website (see Sec-tion 7).767three methods of unsupervised data selection, i.e.generating the unsupervised data chunks for eachtraining iteration from the ,,pool?
of sentences.These methods are: simple sequential chopping,randomized data selection with replacement andrandomized selection without replacement.
Ta-ble 5 demonstrates that there is practically no dif-ference in the results.
Thus, we use the sequentialchopping mechanism, mainly for its simplicity.Method of data selection English CzechSequential chopping 97.44 96.21Random without replacement 97.44 96.20Random with replacement 97.44 96.21Table 5: Unsupervised data selection5.3 Joining the dataWe have experimented with various sizes of theunsupervised parts (from 500k tokens to 5M) andalso with various numbers of iterations.
The bestresults (on the development data set) have beenachieved with the unsupervised chunks containingapprox.
4 million tokens for English and 1 milliontokens for Czech.
Each training process consistsof (at most) 100 iterations (Czech) or 50 iterations(English); therefore, for the 50 (100) iterations weneeded only about 200,000,000 (100,000,000) to-kens of raw texts.
The best development data setresults have been (with the current setup) achievedon the 44th (English) and 33th (Czech) iteration.The development data set has been also used todetermine the best way to ?merge?
the manuallylabeled data (the PTB/WSJ and the PDT 2.0 train-ing data) and the unsupervised parts of the data.Given the properties of the perceptron algorithm,it is not too surprising that the best solution is toput (the full size of) the manually labeled data first,followed by the (four) million-token chunk of theautomatically tagged data (different data in eachchunk but of the same size for each iteration).
Itcorresponds to the situation when the trainer is pe-riodically ?returned to the right track?
by giving itthe gold standard data time to time.Figure 2 (English) and especially Figure 3(Czech) demonstrate the perceptron behavior incases where the supervised data precede the un-supervised data only in selected iterations.
A sub-set of these development results is also present inTable 6.0 10 20 30 40 5097.2097.2597.3097.3597.40IterationAccuracyon development dataEvery iterationEvery 4th iterationEvery 8th iterationEvery 16th iterationOnce at the beginning    No supervised dataFigure 2: Dependence on the inclusion of the su-pervised training data (English)English CzechNo supervised data 97.37 95.88Once at the beginning 97.40 96.00Every training iteration 97.44 96.21Table 6: Dependence on the inclusion of the su-pervised training data5.4 The morphological analyzers and theperceptron feature templatesThe whole experiment can be performed withthe original perceptron feature set described in(Collins, 2002) instead of the feature set describedin this article.
The results are compared in Table 7(for English only).Also, for English it is not necessary to use ourmorphological analyzer described in section 3.3(other variants are to use the list of tags derivedsolely from the WSJ training data or to give eachtoken the full list of tags found in WSJ).
It ispractically impossible to perform the unsupervisedtraining with the full list of tags (it would take sev-eral years instead of several days with the defaultsetup), thus we compare only the results with mor-phological analyzer to the results with the list oftags derived from the training data, see Table 8.It can be expected (some approximated exper-iments were performed) that the results with thefull list of tags would be very similar to the resultswith the morphological analyzer, i.e.
the morpho-logical analyzer is used mainly for technical rea-sons.
Our expectations are based mainly (but not7680 10 20 30 40 5095.695.795.895.996.096.196.2IterationAccuracyon development dataEvery iterationEvery 4th iterationEvery 8th iterationEvery 16th iterationOnce at the beginning    No supervised dataFigure 3: Dependence on the inclusion of the su-pervised training data (Czech)only) on the supervised training results, where theperformance of the taggers using the morpholog-ical analyzer output and using the full list of tagsare nearly the same, see Table 9.Feature set AccuracyCollins?
97.38Our?s 97.44Table 7: Dependence on the feature set used by theperceptron algorithm (English)GEN(x) AccuracyList of tags derived from train 97.13Our morphological analyzer 97.44Table 8: Dependence on the GEN(x)6 ResultsIn Tables 10 and 11, the main results (on the eval-test data sets) are summarized.
The state-of-theart taggers are using feature sets discribed in thecorresponding articles ((Collins, 2002), (Gime?nezand Ma`rquez, 2004), (Toutanova et al, 2003) and(Shen et al, 2007)), Morc?e supervised and Morc?esemi-supervised are using feature set desribed insection 4.For significance tests, we have used the pairedWilcoxon signed rank test as implemented in theR package (R Development Core Team, 2008)GEN(x) AccuracyList of tags derived from train 95.89Our morphological analyzer 97.17Full tagset 97.15Table 9: Supervised training results: dependenceon the GEN(x)Tagger accuracyCollins 97.07 %SVM 97.16 %Stanford 97.24 %Shen 97.33 %Morc?e supervised 97.23 %combination 97.48 %Morc?e semi-supervised 97.44 %Table 10: Evaluation of the English taggersTagger accuracyFeature-based 94.04 %HMM 94.82 %Morc?e supervised 95.67 %combination 95.70 %Morc?e semi-supervised 95.89 %Table 11: Evaluation of the Czech taggersin wilcox.test(), dividing the data into 100chunks (data pairs).6.1 EnglishThe combination of the three existing English tag-gers seems to be best, but it is not significantlybetter than our semi-supervised approach.The combination is significantly better than(Shen et al, 2007) at a very high level, but moreimportantly, Shen?s results (currently represent-ing the replicable state-of-the-art in POS tagging)have been significantly surpassed also by the semi-supervised Morc?e (at the 99 % confidence level).In addition, the semi-supervised Morc?e per-forms (on single CPU and development data set)77 times faster than the combination and 23 timesfaster than (Shen et al, 2007).6.2 CzechThe best results (Table 11) are statistically signif-icantly better than the previous results: the semi-supervised Morc?e is significantly better than both769the combination and the supervised (original) vari-ant at a very high level.7 DownloadWe decided to publish our system for wide use un-der the name COMPOST (Common POS Tagger).All the programs, patches and data files are avail-able at the website http://ufal.mff.cuni.cz/compostunder either the original data provider license, orunder the usual GNU General Public License, un-less they are available from the widely-known andeasily obtainable sources (such as the LDC, inwhich case pointers are provided on the downloadwebsite).The Compost website also contains easy-to-runLinux binaries of the best English and Czech sin-gle taggers (based on the Morc?e technology) as de-scribed in Section 6.8 Conclusion and Future WorkWe have shown that the ?right?18 mixture of su-pervised and unsupervised (auto-tagged) data cansignificantly improve tagging accuracy of the av-eraged perceptron on two typologically differentlanguages (English and Czech), achieving the bestknown accuracy to date.To determine what is the contribution of the in-dividual ?dimensions?
of the system setting, asdescribed in Sect.
5, we have performed exper-iments fixing all but one of the dimensions, andcompared their contribution (or rather, their losswhen compared to the best ?mix?
overall).
ForEnglish, we found that excluding the state-of-the-art-tagger (in fact, a carefully selected combina-tion of taggers yielding significantly higher qual-ity than any of them has) drops the resulting ac-curacy the most (0.2 absolute).
Significant yetsmaller drop (less than 0.1 percent) appears whenthe manually tagged portion of the data is not usedor used only once (or infrequently) in the inputto the perceptron?s learner.
The difference in us-ing various feature templates (yet al largely sim-ilar to what state-of-the-art taggers currently use)is not significant.
Similarly, the way the unsuper-vised data is selected plays no role, either; this dif-fers from the bagging technique (Breiman, 1996)where it is significant.
For Czech, the drop in ac-curacy appears in all dimensions, except the unsu-pervised data selection one.
We have used novelfeatures inspired by previous work but not used in18As empirically determined on the development data set.the standard perceptron setting yet (linguisticallymotivated tag classes in features, lookahead fea-tures).
Interestingly, the resulting tagger is betterthan even a combination of the previous state-of-the-art taggers (for English, this comparison is in-conclusive).We are working now on parallelization of theperceptron training, which seems to be possible(based i.a.
on small-scale preliminary experimentswith only a handful of parallel processes andspecific data sharing arrangements among them).This would further speed up the training phase, notjust as a nice bonus per se, but it would also allowfor a semi-automated feature template selection,avoiding the (still manual) feature template prepa-ration for individual languages.
This would in turnfacilitate one of our goals to (publicly) providesingle-implementation, easy-to-maintain state-of-the-art tagging tools for as many languages as pos-sible (we are currently preparing Dutch, Slovakand several other languages).19Another area of possible future work is moreprincipled tag classing for languages with largetagsets (in the order of 103), and/or addingsyntactically-motivated features; it has helpedCzech tagging accuracy even when only the ?in-trospectively?
defined classes have been added.
Itis an open question if a similar approach helpsEnglish as well (certain grammatical categoriescan be generalized from the current WSJ tagset aswell, such as number, degree of comparison, 3rdperson present tense).Finally, it would be nice to merge some of theapproaches by (Toutanova et al, 2003) and (Shenet al, 2007) with the ideas of semi-supervisedlearning introduced here, since they seem orthog-onal in at least some aspects (e.g., to replace therudimentary lookahead features with full bidirec-tionality).AcknowledgmentsThe research described here was supported by theprojects MSM0021620838 and LC536 of Ministryof Education, Youth and Sports of the Czech Re-public, GA405/09/0278 of the Grant Agency of theCzech Republic and 1ET101120503 of Academyof Sciences of the Czech Republic.19Available soon also on the website.770ReferencesThorsten Brants.
2000.
TnT - a Statistical Part-of-Speech Tagger.
In Proceedings of the 6th AppliedNatural Language Processing Conference, pages224?231, Seattle, WA.
ACL.Leo Breiman.
1996.
Bagging predictors.
Mach.Learn., 24(2):123?140.Eric Brill and Jun Wu.
1998.
Classifier Combinationfor Improved Lexical Disambiguation.
In Proceed-ings of the 17th international conference on Compu-tational linguistics, pages 191?195, Montreal, Que-bec, Canada.
Association for Computational Lin-guistics.CNC, 2005.
Czech National Corpus ?
SYN2005.
In-stitute of Czech National Corpus, Faculty of Arts,Charles University, Prague, Czech Republic.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In EMNLP ?02:Proceedings of the ACL-02 conference on Empiricalmethods in natural language processing, volume 10,pages 1?8, Philadelphia, PA.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool:A General POS Tagger Generator Based on SupportVector Machines.
In Proceedings of the 4th Interna-tional Conference on Language Resources and Eval-uation, pages 43?46, Lisbon, Portugal.David Graff, 1995.
North American News Text Cor-pus.
Linguistic Data Consortium, Cat.
LDC95T21,Philadelphia, PA.Jan Hajic?
and Barbora Vidova?-Hladka?.
1998.
Tag-ging Inflective Languages: Prediction of Morpho-logical Categories for a Rich, Structured Tagset.In Proceedings of the 17th international conferenceon Computational linguistics, pages 483?490.
Mon-treal, Quebec, Canada.Jan Hajic?.
2004.
Disambiguation of Rich Inflection(Computational Morphology of Czech).
Nakladatel-stv??
Karolinum, Prague.Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and MarieMikulova?.
2006.
Prague Dependency Treebankv2.0, CDROM, LDC Cat.
No.
LDC2006T01.
Lin-guistic Data Consortium, Philadelphia, PA.Pavel Krbec.
2005.
Language Modelling for SpeechRecognition of Czech.
Ph.D. thesis, UK MFF,Prague, Malostranske?
na?me?st??
25, 118 00 Praha 1.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1994.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational Linguistics, 19(2):313?330.R Development Core Team, 2008.
R: A Language andEnvironment for Statistical Computing.
R Foun-dation for Statistical Computing, Vienna, Austria.ISBN 3-900051-07-0.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof the 1st EMNLP, pages 133?142, New Brunswick,NJ.
ACL.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, page 9pp., Manchester, GB.Libin Shen, Giorgio Satta, and Aravind K. Joshi.
2007.Guided Learning for Bidirectional Sequence Classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 760?767, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Drahom?
?ra ?johanka?
Spoustova?, Jan Hajic?, JanVotrubec, Pavel Krbec, and Pavel Kve?ton?.
2007.The Best of Two Worlds: Cooperation of Statisticaland Rule-Based Taggers for Czech.
In Proceedingsof the Workshop on Balto-Slavonic Natural Lan-guage Processing 2007, pages 67?74, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proceedings of ACL-08: HLT, pages 665?673, Columbus, Ohio, June.Association for Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In NAACL ?03: Proceedings of the 2003 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics on HumanLanguage Technology, pages 173?180, Edmonton,Canada.
Association for Computational Linguistics.Hans van Halteren, Walter Daelemans, and Jakub Za-vrel.
2001.
Improving accuracy in word classtagging through the combination of machine learn-ing systems.
Computational Linguistics, 27(2):199?229.Jan Votrubec.
2006.
Morphological Tagging Basedon Averaged Perceptron.
In WDS?06 Proceedings ofContributed Papers, pages 191?195, Prague, CzechRepublic.
Matfyzpress, Charles University.771
