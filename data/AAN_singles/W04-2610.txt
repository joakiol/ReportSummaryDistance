Support Vector Machines Applied to the Classification of Semantic Relationsin Nominalized Noun PhrasesRoxana GirjuComputer Science DepartmentBaylor UniversityWaco, Texasgirju@cs.baylor.eduAna-Maria Giuglea, Marian Olteanu,Ovidiu Fortu, Orest Bolohan, andDan MoldovanDepartment of Computing ScienceUniversity of Texas at DallasDallas, Texasmoldovan@utdallas.eduAbstractThe discovery of semantic relations in textplays an important role in many NLP appli-cations.
This paper presents a method for theautomatic classification of semantic relationsin nominalized noun phrases.
Nominalizationsrepresent a subclass of NP constructions inwhich either the head or the modifier noun isderived from a verb while the other noun is anargument of this verb.
Especially designed fea-tures are extracted automatically and used in aSupport Vector Machine learning model.
Thepaper presents preliminary results for the se-mantic classification of the most representativeNP patterns using four distinct learning mod-els.1 Introduction1.1 Problem descriptionThe automatic identification of semantic relations in texthas become increasingly important in Information Ex-traction, Question Answering, Summarization, Text Un-derstanding, and other NLP applications.
This paper dis-cusses the automatic labeling of semantic relations innominalized noun phrases (NPs) using a support vectormachines learning algorithm.Based on the classification provided by the New Web-ster?s Grammar Guide (Semmelmeyer and Bolander1992) and our observations of noun phrase patterns onlarge text collections, the most frequently occurring NPlevel constructions are: (1) Compound Nominals consist-ing of two consecutive nouns (eg pump drainage - an IN-STRUMENT relation), (2) Adjective Noun constructionswhere the adjectival modifier is derived from a noun (egparental refusal - AGENT), (3) Genitives (eg tone of con-versation - a PROPERTY relation), (4) Adjective phrasesin which the modifier noun is expressed by a preposi-tional phrase which functions as an adjective (eg amuse-ment in the park - a LOCATION relation), and (5) Adjec-tive clauses where the head noun is modified by a relativeclause (eg the man who was driving the car - an AGENTrelation between man and driving).1.2 Previous work on the discovery of semanticrelationsThe development of large semantically annotated cor-pora, such as Penn Treebank2 and, more recently, Prop-Bank (Kingsbury, et al 2002), as well as semanticknowledge bases, such as FrameNet (Baker, Fillmore,and Lowe 1998), have stimulated a high interest in theautomatic acquisition of semantic relations, and espe-cially of semantic roles.
In the last few years, many re-searchers (Blaheta and Charniak 2000), (Gildea and Ju-rafsky 2002), (Gildea and Palmer 2002), (Pradhan etal.
2003) have focused on the automatic prediction of se-mantic roles using statistical techniques.
These statisticaltechniques operate on the output of probabilistic parsersand take advantage of the characteristic features of thesemantic roles that are then employed in a learning algo-rithm.While these systems focus on verb-argument semanticrelations, called semantic roles, in this paper we inves-tigate predicate-argument semantic relations in nominal-ized noun phrases and present a method for their auto-matic detection in open-text.1.3 ApproachWe approach the problem top-down, namely identify andstudy first the characteristics or feature vectors of eachnoun phrase linguistic pattern and then develop modelsfor their semantic classification.
The distribution of thesemantic relations is studied across different NP patternsand the similarities and differences among resulting se-mantic spaces are analyzed.
A thorough understandingof the syntactic and semantic characteristics of NPs pro-vides valuable insights into defining the most representa-tive feature vectors that ultimately drive the discriminat-ing learning models.An important characteristic of this work is that it re-lies heavily on state-of-the-art natural language process-ing and machine learning methods.
Prior to the discoveryof semantic relations, the text is syntactically parsed withCharniak?s parser (Charniak 2001) and words are seman-tically disambiguated and mapped into their appropriateWordNet senses.
The word sense disambiguation is donemanually for training and automatically for testing witha state-of-the-art WSD module, an improved version ofa system with which we have participated successfullyin Senseval 2 and which has an accuracy of 81% whendisambiguating nouns in open-domain.
The discovery ofsemantic relations is based on learning lexical, syntactic,semantic and contextual constraints that effectively iden-tify the most probable relation for each NP constructionconsidered.2 Semantic Relations in Nominalized NounPhrasesIn this paper we study the behavior of semantic relationsat the noun phrase level when one of the nouns is nom-inalized.
The following NP level constructions are con-sidered: complex nominals, genitives, adjective phrases,and adjective clauses.Complex NominalsLevi (Levi 1979) defines complex nominals (CNs) as ex-pressions that have a head noun preceded by one or moremodifying nouns, or by adjectives derived from nouns(usually called denominal adjectives).
Each sequence ofnouns, or possibly adjectives and nouns, has a particularmeaning as a whole carrying an implicit semantic rela-tion; for example, ?parental refusal?
(AGENT).The main tasks are the recognition, and the interpre-tation of complex nominals.
The recognition task dealswith the identification of CN constructions in text, whilethe interpretation of CNs focuses on the detection andclassification of a comprehensive set of semantic rela-tions between the noun constituents.GenitivesIn English there are two kinds of genitives; in one, themodifier is morphologically linked to the possessive clitic?s and precedes the head noun (s-genitive e.g.
?John?sconclusion?
), and in the second one the modifier is syn-tactically marked by the preposition of and follows thehead noun (of-genitive, e.g.
?declaration of indepen-dence?
).Adjective Phrases are prepositional phrases attached tonouns and act as adjectives (cf.
(Semmelmeyer andBolander 1992)).
Prepositions play an important roleboth syntactically and semantically ( (Dorr 1997).
Prepo-sitional constructions can encode various semantic re-lations, their interpretations being provided most of thetime by the underlying context.
For instance, the preposi-tion ?with?
can encode different semantic relations: (1) Itwas the girl with blue eyes (MERONYMY), (2) The babywith the red ribbon is cute (POSSESSION), (3) The womanwith triplets received a lot of attention (KINSHIP).The conclusion for us is that in addition to the nouns se-mantic classes, the preposition and the context play im-portant roles here.Adjective Clauses are subordinate clauses attached tonouns (cf.
(Semmelmeyer and Bolander 1992)).
Oftenthey are introduced by a relative pronoun/adverb (ie that,which, who, whom, whose, where) as in the following ex-amples: (1) Here is the book which I am reading (bookis the THEME of reading) (2) The man who was drivingthe car was a spy (man is the AGENT of driving).
Adjec-tive clauses are inherently verb-argument structures, thustheir interpretation consists of detecting the semantic rolebetween the head noun and the main verb in the relativeclause.
This is addressed below.3 Nominalizations and Mapping of NPsinto Grammatical Role Structures3.1 NominalizationsA further analysis of various examples of noun - nounpairs encoded by the first three major types of NP-levelconstructions shows the need for a different taxonomybased on the syntactic and grammatical roles the con-stituents have in relation to each other.
The criterion inthis classification splits the noun - noun examples (re-spectively, adjective - noun examples in complex nom-inals) into nominalizations and non-nominalizations.Nominalizations represent a particular subclass of NPconstructions that in general have ?a systematic corre-spondence with a clause structure?
(Quirk et al1985).The head or modifier noun is derived from a verb whilethe other noun (the modifier, or respectively, the head) isinterpreted as an argument of this verb.
For example, thenoun phrase ?car owner?
corresponds to ?he owns a car?.The head noun owner is morphologically related to theverb own.
Otherwise said, the interpretation of this classof NPs is reduced to the automatic detection and inter-pretation of semantic roles mapped on the correspondingverb-argument structure.As in (Hull and Gomez 1996), in this paper we usethe term nominalization to refer only to those senses ofthe nominalized nouns which are derived from verbs.For example, the noun ?decoration?
has three senses inWordNet 2.0: an ornament (#1), a medal (#2), and the actof decorating (#3).
Only the last sense is a nominaliza-tion.
However, there are more complex situations whenthe underlying verb has more than one sense that refers toan action/event.
This is the case of ?examination?
whichhas five senses of which four are action-related.
In thiscase, the selection of the correct sense is provided by thecontext.We are interested in answering the following ques-tions: (1) What is the best set of features that can capturethe meaning of noun - noun nominalization pairs for eachNP-level construction?
and (2) What is the semantic be-havior of nominalization constructions across NP levels?3.2 Taxonomy of nominalizationsDeverbal vs verbal noun.
(Quirk et al1985) generally classify nominalizationsbased on the morphological formation of the nominal-ized noun.
They distinguish between deverbal nouns, i.e.those derived from the underlying verb through word for-mation; e.g., ?student examination?, and verbal nouns,i.e.
those derived from the verb by adding the gerundsuffix ?-ing?
; e.g.
: ?cleaning woman?.
Most of the time,verbal nouns are derived from verbs which don?t have adeverbal correspondent.Table 1 shows the mapping of the first three major syn-tactic NP constructions to the grammatical role level.
Byanalyzing a large corpus, we have observed that Quirk?sgrammatical roles shown in Table 1 are not uniformly dis-tributed over the types of NP-constructions.
For example,the ? 	 fiffffifl?
pattern cannot be encodedby s-genitives (e.g., ?language teacher?, ?teacher of lan-guage?
).Some of the non-nominalization NP constructions canalso capture the arguments of a particular verb that ismissing (e.g., subject - object, subject - complement).The ?General?
subclass refers to all other types of noun- noun constructions that cannot be mapped on verb-argument relations (e.g., ?hundreds of dollars?).
Adjec-tive clauses are not part of Table 1 as they describe bydefault verb-argument relations (semantic roles).
Thusthey cannot be classified as nominalizations or non-nominalizations.Two other useful classifications for nominalizationsare: paraphrased vs. non-paraphrased, and the clas-sification according to the nominalized noun?s verb-argument underlying structures as provided by the Nom-Lex dictionary of English nominalizations (Macleod et al1998) discussed more later.Paraphrased vs non-Paraphrased.In most cases, the relation between the nominalized nounand the other noun argument can be captured from thesubcategorization properties of the underlying verb.
Oth-erwise said, most of the time, there is a systematic cor-respondence between the nominalized NP constructionand the predicate-argument structure of the correspond-ing verb in a clausal paraphrase (paraphrased nominal-ization).
The predicate-argument structure can be cap-tured by three grammatical roles: verb-subject, verb-object, and verb-complement.
We call the arguments ofthe verb that appear more frequently or are obligatory- frame arguments.
From this point of view the non-nominalized noun can be mapped on the verb-argumentframe or not.
Thus we can classify paraphrased nom-inalizations in framed and non-framed according to thepresence or absence of the non-nominalized noun in theframe of the verb.
The semantic classification of nom-inalizations involves first the detection of a nominaliza-tion, the selection of the correct sense of the root verb,and finally the detection of the semantic relationship withthe other noun.Besides the paraphrase nominalization, there is an-other type which occurs less frequently.
We call this typenon-paraphrased nominalization as its meaning is differ-ent from its most related paraphrase clause.
Examples:research budget, design contract, preparation booklet,publishing sub-industry and editing error.
An importantobservation is that the nominalized noun occurs most ofthe time on the first position in an NP construction.The criteria presented here consider also nominaliza-tions with adjectival modifiers such as ?parental refusal?.These adjectives are derived from nouns, so the con-struction is just a special case of nominalization betweennouns.NomLex classificationThe NomLex dictionary of nominalizations (Macleod etal.
1998) contains 1025 lexical entries and lists the verbsfrom which the nouns are derived.
This dictionary spec-ifies the complements allowed for a nominalization.
Themapping is done at a syntactic level only.
NomLex isused in the first phase of our algorithm in order to de-tect a possible nominalization and the corresponding rootverb.
The criterion of NomLex classification is based onthe verb-argument correspondence:a. Verb-nom: The nominalized noun represents the ac-tion/state of the verb (e.g., ?acquisition challenge?, ?de-positary receipt)?,b.
Subj-nom: The nominalized noun refers to the sub-ject of the verb (e.g., ?auto maker?, ?math teacher?
).This type is also called agential nominalization (Quirket al1985) as the nominalized noun captures informationabout both the subject and verb.c.
Obj-nom: The nominalized noun refers to the object ofthe verb (e.g., ?court order?, ?company employee?),d.
Verb-part: the nominalized noun is derived from acompositional verb (e.g., ?takeover target?
).3.3 Corpus Analysis at NP levelThe dataWe have assembled a corpus from the Wall Street Journalarticles from TREC-9.
Table 2 shows for each syntacticSyntactic Patterns Grammatical CNs Genitives Adjective ExampleRoles N N Adj-N ?s of PhraseDeverbal 	   ?heart massage?noun fffiflffi  !
#"$%   ?language teacher?fl& 	'%    ?smallpox vaccination?Nominalization())	'    ?boy?s application?Verbal*+'%-,.
"/0ffi(!)ff  ?cleaning woman?noun*1-,.
"/2ffifi ff  ?spending money?*+'%-,.
"/0ffi fl&  ?carving knife?fl&3*+-,.
"/2ffi   ?horse riding?Non-nominalization(!)ff ff   ?wind mill?())4+567  5  2   ?pine tree?General      ?hundreds of dollars?Table 1: Classification of noun phrase constructions on the types of grammatical roles (cf.
(Quirk et al1985)) neededfor semantic roles detection.
The classification is the result of our observations of nominalization patterns at nounphrase level.category the number of randomly selected sentences, thenumber of instances found in these sentences, and finallythe number of nominalized instances our group managedto annotate by hand.
The annotation of each exampleconsisted of specifying its feature vector and the most ap-propriate semantic relation as defined in (Moldovan et al2004).Inter-annotator AgreementThe annotators, four PhD students in Computational Se-mantics worked in groups of two, each group focusing onone half of the corpus to annotate.
Besides the type ofrelation, the annotators were asked to provide informa-tion about the order of the modifier and the head nounsin the syntactic constructions if applicable.
For example,?owner of the car?
and ?car of the owner?.The annotators were also asked to indicate if the in-stance was a nominalization and if yes, which of thenoun constituents was derived from a verb (e.g.
the headnoun nominalization ?student protest?, or the modifiernoun nominalization ?working woman??
cf.
(Quirk etal.1985)).The annotators?
agreement was measured using theKappa statistics (Siegel and Castellan 1988), one of themost frequently used measure of inter-annotator agree-ment for classification tasks:8:9<;=!>?;=!>$@?A;fi=)>$@?, whereBDC1E'FHGis the proportion of times the raters agree andBDC1E'IJGis the probability of agreement by chance.
TheK coefficient is 1 if there is a total agreement among theannotators, and 0 if there is no agreement other than thatexpected to occur by chance.For each construction, the corpus was split after agree-ment with an 80/20 training/testing ratio.
For each pat-tern, we computed the K coefficient only for those in-stances tagged with one of the 35 semantic relations (Kvalue for: NN (0.64), AdjN (0.70), s-genitive (0.69), of-genitive (0.73), adjective phrases (0.67), and adjectiveclauses (0.71)).
For each pattern, we also calculated thenumber of pairs that were tagged with OTHERS by bothannotators, over the number of examples classified in thiscategory by at least one of the judges, averaged by thenumber of patterns considered (agreement for OTHERS:75%).The K coefficient shows a good level of agreement forthe training and testing data on the set of 35 relations, tak-ing into consideration the task difficulty.
This can be ex-plained by the instructions the annotators received priorto annotation and by their expertise in lexical semantics.3.4 Distribution of Semantic RelationsEven noun phrase constructions are very productive al-lowing for a large number of possible interpretations, Ta-ble 3 shows that a relatively small set of 35 semantic re-lations covers a significant part of the semantic distribu-tion of these constructions on a large open-domain cor-pus.
Moreover, the distribution of these relations is de-pendent on the type of NP construction, each type en-coding a particular subset.
For example, in the case ofs-genitives, there were 13 relations found from the totalof 35 relations considered.
The most frequently occur-ring relations were AGENT, TEMPORAL, LOCATION, andTHEME.
By comparing the subsets of semantic relationsin each column we can notice that these semantic spaces(the set of semantic relations an NP construction can en-code) are not identical, proving our initial intuition thatthe NP constructions cannot be alternative ways of pack-ing the same information.
Table 3 also shows that there isa subset of semantic relations that can be fully encoded byall types of NP constructions.
The statistics about the an-notated nominalized examples are as follows (lines 3 and4 in Table 2): N-N (32.30%), Adj-N (30.80%), s-genitive(21.09%), of-genitive (21.8%), adjective phrase (40.5%).80% of the examples in adjective phrases (respectively in94% in s-genitives) had the nominalized noun on the headposition.This simple analysis leads to the important conclusionthat the NP constructions must be treated separately astheir semantic content is different.
We can draw fromhere the following conclusions:1.
Not all semantic relations can be encoded by all NPWall Street JournalCNs Genitives Adjective AdjectiveNN AdjN ?s of Phrases ClausesNo.
of sentences 7067 5381 50291 27067 14582 31568No.
of instances 5557 500 2990 4185 3502 6520 2No.
of annotated instances 2315 383 1816 3404 1341 563No.
of annotated nominalized instances 747 118 383 742 543 563No.
of annotated nominalized instances used in the learning task 312 118 383 344 297 563Table 2: Corpus statistics.syntactic constructions.2.
There are semantic relations that have preferences overparticular syntactic constructions.3.5 Model3.6 Support Vector MachinesSupport Vector Machines (SVM) have a strong mathe-matical foundation (Vapnik 1982) and have been appliedsuccessfully to text classification (Tong and Koller 2001),speech recognition, and other applications.
We appliedSVM to the semantic classification problem and obtainedencouraging results.SVM algorithms are a special class of hyperplaneclassifiers that use the information encoded in the dot-products of the transformed feature vectors as a simi-larity measure.
The similarity between two instances  and   is given as a function 8 ,8 E  G 9HE GHE G.
The Kernel function 8 is theinner product of the non-linear function 	 thatmaps the original feature vectors into real feature space.The function that provides the best classification is ofthe form: Eff G 9flfiffi!
#"%$A'&)( 8 E * +G.
The vec-tors  + for which the Lagrange multipliers( -,9/.arecalled support vectors.
Intuitively, they are the closest tothe separating hyperplane.
SVM provide good classifierswith few, well chosen training examples.In order to achieve classification in  classes,  021 ,a binary classifier is built for each pair of classes (a totalof 354 classifiers).
A voting procedure is then used to es-tablish the class of a new example.
For the experimentswith semantic relations, the simplest voting scheme hasbeen chosen; each binary classifier has one vote which isassigned to the class it chooses when it is run.
Then theclass with the largest number of votes is considered to bethe answer.
Using the specific nature of the semantic re-lation detection problem, new voting schemes can be de-signed, with good perspectives of improving the overallprecision.The software used in these experiments is the pack-age LIBSVM, http://www.csie.ntu.edu.tw/ 6 cjlin/libsvm/which implements the SVM algorithm described above.The choice of the kernel is the most difficult part ofapplying SVM algorithms as the performance of the clas-sifier might be enhanced with a judicious choice of thekernel.
We used in our experiments 4 types of generalkernels (linear, polynomial, radial-based and sigmoid),with good results.
All of them had nearly the same per-formance, with slight deviations between 2% and 4% ona reduced testing set.
However, remarkable is the factthat all classifiers, regardless of the kernel used, made thesame mistakes (misclassified the same examples - eg, aclassifier with 58% precision makes the same mistakes asone with 62% precision, plus some of its own, and thissituation occurred even when the two classifiers had dif-ferent kernels), while the overall precision seems to bearound to the same value during the coefficient tuning.This shows that the limitation is rather imposed by theclassification task than by the kernel type.3.7 Feature spaceThe key to a successful semantic classification of NP con-structions is the identification of their most specific lexi-cal, syntactic, semantic and contextual features.
We de-veloped algorithms for finding their values automatically.The values of these features are determined with the helpof some important resources mentioned below.ComLex (Grishman et al1994) is a computational lexi-con providing syntactic information for more than 38,000English headwords.
It contains detailed syntactic infor-mation about the attributes of each lexical item and thesubcategorization frames when words have arguments.This last feature is the most useful for our task as thesenses of verbs are clustered by the syntactic frames.
Wewill use ComLex in combination with VerbLeX to mapthe syntactic behaviors to verb semantic classes.VerbLeX is an in-house verb lexicon built by enrich-ing VerbNet (Kipper et al 2000) with verb synsetsfrom WordNet and verbs extracted from the semanticframes of FrameNet.
It contains information about thesemantic roles that can appear within a class of verbs to-gether with the selectional restrictions for their lexical re-alizations, syntactic subcategorization and WordNet verbsenses.
The syntactic information is less detailed thanin ComLex, but a mapping between these two resourceswill provide both the semantic and syntactic informationneeded for the task.
From the total of 13,213 verbs inthe extended VerbNet, 6,077 were distinct.
It also pro-vides a mapping from the FrameNet deep semantic rolesto general thematic roles (list defined in (Moldovan et al2004)), and use cases for VerbNet.No.
Semantic Frequency > ?ExamplesRelations CNs Genitives Adjective AdjectiveNN AdjN ?s of Phrases Clauses1 POSSESSION 0.46 1.16 1.06 0.37 0 0.23 ?stock holders?2 KINSHIP 0 0 0 0 0 03 ATTRIBUTE-HOLDER 1.37 6.97 1.41 8.24 1.82 0 ?intensity of intervention?4 AGENT 15.13 23.25 33.68 1.87 11.41 25.81 ?trading companies?5 TEMPORAL 0.92 0 26.24 1.50 11.87 6.27 ?date of purchase?6 DEPICTION-DEPICTED 0 0 0 0.75 0 0 ?evidence of cheating?7 PART-WHOLE 0 8.13 1.41 3.37 1.82 0 ?world consumer?8 IS-A (HYPERNYMY) 0 0 0 0 0 09 ENTAIL 0 0 0 0 0 010 CAUSE 0.46 0 0.35 1.12 0.91 0 ?fire destruction?11 MAKE/PRODUCE 0 3.48 0.35 3.00 0.91 0 ?computer?s maker?12 INSTRUMENT 0 0 0 0.75 0.45 0.46 ?oven cooking?13 LOCATION/SPACE 2.75 16.27 3.19 0.75 8.67 4.65 ?meeting in Philadelphia?14 PURPOSE 10.09 3.48 0 1.50 5.93 0 ?research budget?15 SOURCE 0 13.95 0 0.37 3.19 0.46 ?Japanese buyer?16 TOPIC 24.77 3.48 0 9.73 5.02 6.51 ?price discussion?17 MANNER 4.13 0 0 0 1.37 2.32 ?shock reaction?18 MEANS 0 0 0 0 0 019 ACCOMPANIMENT 0 0 0 0 0 020 EXPERIENCER 0 1.16 0.35 0 1.37 2.79 ?risk for the investor?21 RECIPIENT 0 0 0.35 0.37 0.45 0.23 ?ovations for the champions?22 FREQUENCY 0 8.13 0 0 0 0 ?daily jogging?23 INFLUENCE 0 0 0 0 0 024 ASSOCIATED WITH 0 1.16 0.71 1.12 1.82 0 ?designer?s attorney?25 MEASURE 0 0 0 5.24 0.45 0 ?5-mile running?26 SYNONYMY 0 0 0 0 0 027 ANTONYMY 0 0 0 0 0 028 PROBABILITY 0 0 0 0 0 0.46 ?chance that he is single?29 POSSIBILITY 0 0 0 0 0 030 CERTAINTY 0 0 0 0 0 031 THEME 25.22 4.65 22.34 43.82 32.87 38.14 ?use of cards?32 RESULT 5.04 0 0.35 3.00 0.45 1.62 ?ship construction?33 STIMULUS 0 0 0 0 0 2.32 ?the beautiful painting he saw?34 EXTENT 0 0 0 0 0 035 PREDICATE 0.46 0 0 0 0.45 0.70 ?sun king, as Louis XVI is called?OTHERS 4.58 4.65 8.15 13.11 8.67 6.98 ?editing error?Total no.
of examples 100   (218) 100   (86) 100   (282) 100   (267) 100   (219) 100   (430)Table 3: The distribution of the semantic relations on the annotated corpus after agreement.
The list of 35 semanticrelations was presented in (Moldovan et al 2004).
The percentages represent the number of examples that encode asemantic relation for a particular pattern.
The last row shows the number of examples covered by each pattern in theentire annotated corpus (1502 pairs).An essential aspect of our approach below is theword sense disambiguation (WSD) of the content words(nouns, verbs, adjectives and adverbs).
Using a state-of-the-art open-text WSD system, each word is mappedinto its corresponding WordNet 2.0 sense.
When dis-ambiguating each word, the WSD algorithm takes intoaccount the surrounding words, and this is one importantway through which context gets to play a role in the se-mantic classification of NPs.So far, we have identified and experimented with thefollowing NP features:1.
Semantic class of thenon-nominalized noun.
The non-nominalizednoun is classified into one of the 39 EuroWordNet nounsemantic classes.
VerbNet classes extended in VerbLeXcontain selectional restrictions for different semanticroles inside the verb frame.
These restrictions are pro-vided based on the EuroWordNet noun semantic classes.Example: ?computer maker?, where ?computer?
ismapped to the ABSTRACT noun category in EuroWord-Net.
We intend to map the EuroWordNet top nounsemantic classes into their WordNet correspondents.2.
Verb class for nominalized noun,or verb in adjective clauses maps thenominalizing verb into its VerbLeX class.
The intuitionbehind this feature is that semantic relations clusteraround specific VerbLeX verb classes.3.
Type of nominalization indicates theNomLex nominalization class.
For this experimentwe considered only examples that could be foundin NomLex.
By specifying subj-nom, obj-nom, andverb-nom types of nominalization, we reduce the list ofpossible semantic relations the verb can have with thenon-nominalized noun.
Example: ?computer maker?,where ?maker?
is an agential deverbal noun that capturesboth the subject (respectively, AGENT) and the verb.Thus, the noun ?computer?
can only map to object(respectively, THEME).4.
Verbal nominalization is a binary featureindicating whether the nominalized noun is gerundive ornot.
Chomsky (Chomsky 1970) showed that gerundivenominalizations have different behavior than derivednominalizations.
Example: ?woman worker?
vs ?work-ing woman?
; here ?working?
is a verbal nominal.5.
Semantic class of thecoordinating word.
This is a contextual fea-ture and can be either a noun (if the phrase that containsthe nominalization is attached to a noun) or a verb (ifthe phrase is an argument of the verb in the sentence).The feature value is either the VerbLeX class of the verbor the root of the noun in the WordNet hierarchy.
Thecoordinating word captures some properties present inthe noun phrase, properties that help to discriminatebetween various competing semantic relations.
Example:?Foreigners complain that they have limited access to[government procurement] in Japan.?
- the coordinatingword is ?access?
which is a psychological feature.6.
Position of the nominalized noundepicts the position of the nominalizing verb in the com-pound; ie, either head or modifier.
Example: ?workingwoman?, where the nominalized noun is the modifier,and ?computer maker?
where the nominalized noun isthe head noun.7.
In frame is a three-value feature indicatingwhether the compound has a paraphrase or if the peerin the compound is framed or not.
If the peer in theNP noun-noun pair is in the corresponding VerbLeXpredicate-argument frame, than the relation is capturedin the predicate-argument structure.
If it is not in theVerbLeX frame, but is an external argument (eg, LOCA-TION, TEMPORAL, MANNER, etc.
), then it is no-frame.Otherwise, there is no paraphrase that keeps the meaning,so the relation is not defined by the predicate-argumentframe.
Example: ?computer maker?
is framed whereas ?backyard composting?
is non-framed, and ?editingerror?
is no-paraphrase (has no paraphrase of typeverb-argument).8.
Relative pronoun/adverb applies onlyto adjective clauses and embeds information about thegrammatical and/or semantic role of the head noun inthe subordinate clause.
Example: ?the room where themeeting took place?
- the word where implies location.9.
Grammatical role of relativepronoun/adverb applies only to adjective clausesand specifies the grammatical role of the relative pro-noun/adverb, if one exists.
This feature depicts betterthe grammatical role played in the sentence by the headnoun.
We used for this purpose an in-house rule-basedgrammatical role detection module, which annotates thefollowing roles (cf.
(Quirk et al1985): subject, directobject, indirect object, subject complement (argumentfor copular verbs), object complement (second argumentfor complex transitive verbs), object oblique, free pred-icative, and approximates extent and temporal semanticroles.
Example: ?the man who gave Mary the book?
-Mary and the book are indirect object and, respectivelydirect object, so man cannot be THEME or RECIPIENT.10.
Voice.
This feature applies only to adjectiveclauses and indicates the voice of the verb in the relativeclause.
The voice plays an important role in the correla-tion between grammatical roles and semantic roles in asentence.
Example: ?the child that was taken to the zoo?- passive voice, so the child is in a THEME relation withthe verb take.Let?s consider an example of nominalization with itsfeatures.
?Several candidates have withdrawn their names fromconsideration after administration officials asked themfor their views on abortion and fetal-tissue transplants.
?The noun compound ?fetal-tissue#1 transplant#1?
isdetected as a nominalization as the noun ?transplant?
isderived from the verb ?to transplant#3?.
The features andtheir values are: Feature 1: semantic class for fetal-tissue:body-part; Feature 2: verb class for transplant: fill-9.8;Feature 3: type of nominalization: verb-nom; Feature 4:gerundive: no (0); Feature 5: semantic class for coordi-nating word (?view?)
= psychological feature#1; Feature6: position of the nominalized noun = second; Feature 7:in frame = yes.The in-house extended verb lexicon VerbLeX showsthe following semantic frame for the verb class fill-9.8: Agent[+animate] Destination[+location -region]Theme[+concrete] Body-part is a subcategory of con-crete.
Thus, for this example the semantic relation isTHEME.4 Overview of ResultsThe f-measure results obtained so far are summarized inTable 4.
They are divided in two categories, nominaliza-tions, and adjective clauses since the feature vectors differfrom one category to another.
We have compared the per-formance of SVM with three other learning algorithms:(1) semantic scattering (Moldovan et al 2004), (2) deci-sion trees (a C4.5 implementation), and (3) Naive Bayes.We considered as baseline semantic scattering which isa new learning model (Moldovan et al 2004) devel-oped in-house for the semantic classification of noun-noun pairs in NP constructions.
The semantic relationderives from the WordNet semantic classes of the twonouns participating in those constructions, as well as thesurrounding context provided by the WSD module.As expected, the results vary from pattern to pattern.SVM and Naive Bayes seem to perform better than othermodels for the nominalizations and adjective clauses.Overall, these results are very encouraging given thecomplexity of the problem.
By comparison with the base-line, the feature vector presented here gives better results.Syntactic Semantic Scattering Decision Naive Support VectorPattern (Baseline) Tree Bayes MachinesComplex NN     	  	Nominals AdjN   NA NA NAGenitives ?S   		 Of  	  	 Adjective Phrases    	   Adjective Clauses NA 	  Table 4: F-measure results for the semantic classification of NP patterns obtained with four learning models on acorpora with an 80/20 training/testing ratio.
?NA?
means not available.f1 f2 f3 f4 f5 f6 f7 f8 f9 f10Nominalized H M M M L H HAdjective Clauses M H H L HTable 5: The impact of each feature   on the overall performance; H-high (over 8%), M-medium(between 2% and8%), and L-low (below 2%).
Empty boxes indicate the absence of features.This explains in part our initial intuition that nominaliza-tion constructions at NP level have a different semanticbehavior than the NP non-nominalization patterns.We studied the influence of each feature on the per-formance, and since there are too many cases to discusswe only show in Table 5 the average impact as High,Medium, or Low.
This table also shows the features usedin each case.ReferencesC.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proceedings of COLLING/ACL.D.
Blaheta and E. Charniak.
2000.
Assigning functiontags to parsed text.
In Proceedings of the NAACL.E.
Charniak.
2001.
Immediate-head parsing for languagemodels.
In Proceedings of ACL, Toulouse, France.N.
Chomsky.
1970.
Remarks on Nominalization.
InReadings in English Transformational Grammar.
Ja-cobs, R. A., and Rosenbaum, P.S.
(eds), Ginn andCompany.B.
Dorr.
1997.
Large-scale dictionary construction forforeign language tutoring and interlingual machinetranslation.
In Machine Translation, 12(4).D.
Gildea and D. Jurafsky.
2002.
Automatic Labeling ofSemantic Roles.
In Computational Linguistics, 28(3).D.
Gildea and M. Palmer.
2002.
The Necessity of Parsingfor Predicate Argument Recognition.
In ACL.R.
Grishman, C. Macleod, and A. Meyers.
1994.
Comlexsyntax: Building a computational lexicon.
In Proceed-ings of COLING, Kyoto, Japan.R.
Hull and F. Gomez.
1996.
Semantic interpretation ofnominalizations.
In AAAI conference, Oregon.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class-basedconstruction of a verb lexicon.
In AAAI, Austin, Texas.P.
Kingsbury, M. Palmer, and M. Marcus.
2002.
AddingSemantic Annotation to the Penn TreeBank.
In Pro-ceedings of HLT, California.P.
Kingsbury and M. Palmer.
2002.
From Treebank toPropbank.
In Third International Conference on Lan-guage Resources and Evaluation, LREC-02, Las Pal-mas, Canary Islands.Judith Levi.
1979.
The Syntax and Semantics of Com-plex Nominals.
New York: Academic Press.C.
Macleod, R. Grishman, A. Meyers, L. Barrett, and R.Reeves.
1998.
Nomlex: A lexicon of nominalizations.In Proceedings of the 8th International Congress of theEuropean Association for Lexicography, Belgium.D.
Moldovan, A. Badulescu, M. Tatu, D. Antohe,and R. Girju.
2004.
Semantic Classification ofNon-nominalized Noun Phrases.
In Proceedings ofHLT/NAACL 2004 - Computational Lexical Semanticsworkshop, Boston, MA.S.
Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. Mar-tin, and D. Jurafsky.
2003.
Semantic role parsing:Adding semantic structure to unstructured text.
InICDM, Florida.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A comprehensive grammar of English language, Long-man, Harlow.S.
Siegel and N.J. Castellan.
1988.
Non Paramet-ric Statistics for the behavioral science.
New York:McGraw-Hill.M.
Semmelmeyer and D. Bolander.
1992.
The New Web-ster?s Grammar Guide.
Lexicon Publications, Inc.S.
Tong and D. Koller.
2001.
Support Vector MachineActive Learning with Applications to Text Classifica-tion.
In Journal of Machine Learning Research.V.
Vapnik.
1982.
Estimation of Dependences Based onEmpirical Data.
In Springer Verlag.
