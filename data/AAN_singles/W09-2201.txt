Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 1?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsCoupling Semi-Supervised Learning of Categories and RelationsAndrew Carlson1, Justin Betteridge1, Estevam R. Hruschka Jr.1,2 and Tom M. Mitchell11School of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213{acarlson,jbetter,tom.mitchell}@cs.cmu.edu2Federal University of Sao CarlosSao Carlos, SP - Brazilestevam@dc.ufscar.brAbstractWe consider semi-supervised learning ofinformation extraction methods, especiallyfor extracting instances of noun categories(e.g., ?athlete,?
?team?)
and relations (e.g.,?playsForTeam(athlete,team)?).
Semi-supervised approaches using a small numberof labeled examples together with many un-labeled examples are often unreliable as theyfrequently produce an internally consistent,but nevertheless incorrect set of extractions.We propose that this problem can be over-come by simultaneously learning classifiersfor many different categories and relationsin the presence of an ontology definingconstraints that couple the training of theseclassifiers.
Experimental results show thatsimultaneously learning a coupled collectionof classifiers for 30 categories and relationsresults in much more accurate extractionsthan training classifiers individually.1 IntroductionA great wealth of knowledge is expressed on the webin natural language.
Translating this into a struc-tured knowledge base containing facts about enti-ties (e.g., ?Disney?)
and relations between those en-tities (e.g.
CompanyIndustry(?Disney?, ?entertain-ment?))
would be of great use to many applications.Although fully supervised methods for learning toextract such facts from text work well, the costof collecting many labeled examples of each typeof knowledge to be extracted is impractical.
Re-searchers have also explored semi-supervised learn-ing methods that rely primarily on unlabeled data,Figure 1: We show that significant improvements in ac-curacy result from coupling the training of informationextractors for many inter-related categories and relations(B), compared with the simpler but much more difficulttask of learning a single information extractor (A).but these approaches tend to suffer from the fact thatthey face an under-constrained learning task, result-ing in extractions that are often inaccurate.We present an approach to semi-supervised learn-ing that yields more accurate results by coupling thetraining of many information extractors.
The intu-ition behind our approach (summarized in Figure 1)is that semi-supervised training of a single type ofextractor such as ?coach?
is much more difficult thansimultaneously training many extractors that covera variety of inter-related entity and relation types.In particular, prior knowledge about the relation-ships between these different entities and relations(e.g., that ?coach(x)?
implies ?person(x)?
and ?notsport(x)?)
allows unlabeled data to become a muchmore useful constraint during training.Although previous work has coupled the learningof multiple categories, or used static category rec-ognizers to check arguments for learned relation ex-1tractors, our work is the first we know of to couplethe simultaneous semi-supervised training of multi-ple categories and relations.
Our experiments showthat this coupling results in more accurate extrac-tions.
Based on our results reported here, we hy-pothesize that significant accuracy improvements ininformation extraction will be possible by couplingthe training of hundreds or thousands of extractors.2 Problem StatementIt will be helpful to first explain our use of commonterms.
An ontology is a collection of unary and bi-nary predicates, also called categories and relations,respectively.1 An instance of a category, or a cate-gory instance, is a noun phrase; an instance of a rela-tion, or a relation instance, is a pair of noun phrases.Instances can be positive or negative with respectto a specific predicate, meaning that the predicateholds or does not hold for that particular instance.A promoted instance is an instance which our algo-rithm believes to be a positive instance of some pred-icate.
Also associated with both categories and rela-tions are patterns: strings of tokens with placehold-ers (e.g., ?game against arg1?
and ?arg1 , head coachof arg2?).
A promoted pattern is a pattern believedto be a high-probability indicator for some predicate.The challenge addressed by this work is to learnextractors to automatically populate the categoriesand relations of a specified ontology with high-confidence instances, starting from a few seed pos-itive instances and patterns for each predicate anda large corpus of sentences annotated with part-of-speech (POS) tags.
We focus on extracting facts thatare stated multiple times in the corpus, which wecan assess probabilistically using corpus statistics.We do not resolve strings to real-world entities?
theproblems of synonym resolution and disambiguationof strings that can refer to multiple entities are leftfor future work.3 Related WorkWork on multitask learning has demonstrated thatsupervised learning of multiple ?related?
functionstogether can yield higher accuracy than learning thefunctions separately (Thrun, 1996; Caruana, 1997).Semi-supervised multitask learning has been shown1We do not consider predicates of higher arity in this work.to increase accuracy when tasks are related, allow-ing one to use a prior that encourages similar pa-rameters (Liu et al, 2008).
Our work also involvessemi-supervised training of multiple coupled func-tions, but differs in that we assume explicit priorknowledge of the precise way in which our multi-ple functions are related (e.g., that the values of thefunctions applied to the same input are mutually ex-clusive, or that one implies the other).In this paper, we focus on a ?bootstrapping?method for semi-supervised learning.
Bootstrap-ping approaches start with a small number of la-beled ?seed?
examples, use those seed examples totrain an initial model, then use this model to la-bel some of the unlabeled data.
The model isthen retrained, using the original seed examples plusthe self-labeled examples.
This process iterates,gradually expanding the amount of labeled data.Such approaches have shown promise in applica-tions such as web page classification (Blum andMitchell, 1998), named entity classification (Collinsand Singer, 1999), parsing (McClosky et al, 2006),and machine translation (Ueffing, 2006).Bootstrapping approaches to information extrac-tion can yield impressive results with little initialhuman effort (Brin, 1998; Agichtein and Gravano,2000; Ravichandran and Hovy, 2002; Pasca et al,2006).
However, after many iterations, they usu-ally suffer from semantic drift, where errors in label-ing accumulate and the learned concept ?drifts?
fromwhat was intended (Curran et al, 2007).
Couplingthe learning of predicates by using positive exam-ples of one predicate as negative examples for oth-ers has been shown to help limit this drift (Riloff andJones, 1999; Yangarber, 2003).
Additionally, ensur-ing that relation arguments are of certain, expectedtypes can help mitigate the promotion of incorrectinstances (Pas?ca et al, 2006; Rosenfeld and Feld-man, 2007).
Our work builds on these ideas to cou-ple the simultaneous bootstrapped training of multi-ple categories and multiple relations.Our approach to information extraction is basedon using high precision contextual patterns (e.g., ?ismayor of arg1?
suggests that arg1 is a city).
An earlypattern-based approach to information extraction ac-quired ?is a?
relations from text using generic con-textual patterns (Hearst, 1992).
This approach waslater scaled up to the web by Etzioni et al (2005).2Other research explores the task of ?open informa-tion extraction?, where the predicates to be learnedare not specified in advance (Shinyama and Sekine,2006; Banko et al, 2007), but emerge instead fromanalysis of the data.
In contrast, our approach re-lies strongly on knowledge in the ontology about thepredicates to be learned, and relationships amongthem, in order to achieve high accuracy.Chang et al (2007) present a framework forlearning that optimizes the data likelihood plusconstraint-based penalty terms than capture priorknowledge, and demonstrate it with semi-supervisedlearning of segmentation models.
Constraints thatcapture domain knowledge guide bootstrap learn-ing of a structured model by penalizing or disallow-ing violations of those constraints.
While similar inspirit, our work differs in that we consider learningmany models, rather than one structured model, andthat we are consider a much larger scale applicationin a different domain.4 Approach4.1 Coupling of PredicatesAs mentioned above, our approach hinges on the no-tion of coupling the learning of multiple functionsin order to constrain the semi-supervised learningproblem we face.
Our system learns four differenttypes of functions.
For each category c:1. fc,inst : NP (C)?
[0, 1]2. fc,patt : PattC(C)?
[0, 1]and for each relation r:1. fr,inst : NP (C)?NP (C)?
[0, 1]2. fr,patt : PattR(C)?
[0, 1]where C is the input corpus, NP (C) is the set ofvalid noun phrases in C, PattC(C) is the set of validcategory patterns in C, and PattR(C) is the set ofvalid relation patterns in C. ?Valid?
noun phrases,category patterns, and relation patterns are definedin Section 4.2.2.The learning of these functions is coupled in twoways:1.
Sharing among same-arity predicates accordingto logical relations2.
Relation argument type-checkingThese methods of coupling are made possible byprior knowledge in the input ontology, beyond thelists of categories and relations mentioned above.We provide general descriptions of these methodsof coupling in the next sections, while the details aregiven in section 4.2.4.1.1 Sharing among same-arity predicatesEach predicate P in the ontology has a list of othersame-arity predicates with which P is mutuallyexclusive, where mutuallyExclusive(P, P ?)
?
(P (arg1) ?
?P ?
(arg1)) ?
(P ?
(arg1) ?
?P (arg1)), and similarly for relations.
These mu-tually exclusive relationships are used to carry outthe following simple but crucial coupling: if predi-cate A is mutually exclusive with predicate B, A?spositive instances and patterns become negative in-stances and negative patterns for B.
For example,if ?city?, having an instance ?Boston?
and a pattern?mayor of arg1?, is mutually exclusive with ?scien-tist?, then ?Boston?
and ?mayor of arg1?
will becomea negative instance and a negative pattern respec-tively for ?scientist.?
Such negative instances andpatterns provide negative evidence to constrain thebootstrapping process and forestall divergence.Some categories are declared to be a subset ofone of the other categories being populated, wheresubset(P, P ?)
?
P (arg1) ?
P ?
(arg1), (e.g., ?ath-lete?
is a subset of ?person?).
This prior knowledgeis used to share instances and patterns of the subcat-egory (e.g., ?athlete?)
as positive instances and pat-terns for the super-category (e.g., ?person?
).4.1.2 Relation argument type-checkingThe last type of prior knowledge we use to couplethe learning of functions is type checking informa-tion which couples the learning of relations with cat-egories.
For example, the arguments of the ?ceoOf?relation are declared to be of the categories ?person?and ?company?.
Our approach does not promote apair of noun phrases as an instance of a relation un-less the two noun phrases are classified as belongingto the correct argument types.
Additionally, when arelation instance is promoted, the arguments becomepromoted instances of their respective categories.4.2 Algorithm DescriptionIn this section, we describe our algorithm, CBL(Coupled Bootstrap Learner), in detail.The inputs to CBL are a large corpus of POS-tagged sentences and an initial ontology with pre-3Algorithm 1: CBL AlgorithmInput: An ontology O, and text corpus COutput: Trusted instances/patterns for eachpredicateSHARE initial instances/patterns amongpredicates;for i = 1, 2, .
.
.
,?
doforeach predicate p ?
O doEXTRACT candidate instances/patterns;FILTER candidates;TRAIN instance/pattern classifiers;ASSESS candidates using classifiers;PROMOTE highest-confidence candidates;endSHARE promoted items among predicates;enddefined categories, relations, mutually exclusive re-lationships between same-arity predicates, subset re-lationships between some categories, seed instancesfor all predicates, and seed patterns for the cate-gories.
Categories in the input ontology also havea flag indicating whether instances must be propernouns, common nouns, or whether they can be ei-ther (e.g., instances of ?city?
are proper nouns).Algorithm 1 gives a summary of the CBL algo-rithm.
First, seed instances and patterns are sharedamong predicates using the available mutual exclu-sion, subset, and type-checking relations.
Then,for an indefinite number of iterations, CBL expandsthe sets of promoted instances and patterns for eachpredicate, as detailed below.CBL was designed to allow learning many pred-icates simultaneously from a large sample of textfrom the web.
In each iteration of the algorithm, theinformation needed from the text corpus is gatheredin two passes through the corpus using the MapRe-duce framework (Dean and Ghemawat, 2008).
Thisallows us to complete an iteration of the system in1 hour using a corpus containing millions of webpages (see Section 5.3 for details on the corpus).4.2.1 SharingAt the start of execution, seed instances and pat-terns are shared among predicates according to themutual exclusion, subset, and type-checking con-straints.
Newly promoted instances and patterns areshared at the end of each iteration.4.2.2 Candidate ExtractionCBL finds new candidate instances by usingnewly promoted patterns to extract the noun phrasesthat co-occur with those patterns in the text corpus.To keep the size of this set manageable, CBL lim-its the number of new candidate instances for eachpredicate to 1000 by selecting the ones that occurwith the most newly promoted patterns.
An analo-gous procedure is used to extract candidate patterns.Candidate extraction is performed for all predicatesin a single pass through the corpus using the MapRe-duce framework.The candidate extraction procedure has defini-tions for valid instances and patterns that limit ex-traction to instances that look like noun phrases andpatterns that are likely to be informative.
Here weprovide brief descriptions of those definitions.Category Instances In the placeholder of a cate-gory pattern, CBL looks for a noun phrase.
It usespart-of-speech tags to segment noun phrases, ignor-ing determiners.
Proper nouns containing prepo-sitions are segmented using a reimplementation ofthe Lex algorithm (Downey et al, 2007).
Cate-gory instances are only extracted if they obey theproper/common noun specification of the category.Category Patterns If a promoted category in-stance is found in a sentence, CBL extracts the pre-ceding words as a candidate pattern if they are verbsfollowed by a sequence of adjectives, prepositions,or determiners (e.g., ?being acquired by arg1?)
ornouns and adjectives followed by a sequence of ad-jectives, prepositions, or determiners (e.g., ?formerCEO of arg1?
).CBL extracts the words following the instance asa candidate pattern if they are verbs followed option-ally by a noun phrase (e.g., ?arg1 broke the home runrecord?
), or verbs followed by a preposition (e.g.,?arg1 said that?
).Relation Instances If a promoted relation pattern(e.g., ?arg1 is mayor of arg2?)
is found, a candi-date relation instance is extracted if both placehold-ers are valid noun phrases, and if they obey theproper/common specifications for their categories.Relation Patterns If both arguments from a pro-moted relation instance are found in a sentence then4the intervening sequence of words is extracted as acandidate relation pattern if it contains no more than5 tokens, has a content word, has an uncapitalizedword, and has at least one non-noun.4.2.3 Candidate FilteringCandidate instances and patterns are filtered tomaintain high precision, and to avoid extremely spe-cific patterns.
An instance is only considered for as-sessment if it co-occurs with at least two promotedpatterns in the text corpus, and if its co-occurrencecount with all promoted patterns is at least threetimes greater than its co-occurrence count with neg-ative patterns.
Candidate patterns are filtered in thesame manner using instances.All co-occurrence counts needed by the filteringstep are obtained with an additional pass throughthe corpus using MapReduce.
This implementa-tion is much more efficient than one that relies onweb search queries.
CBL typically requires co-occurrence counts of at least 10,000 instances withany of at least 10,000 patterns, which would require100 million hit count queries.4.2.4 Candidate AssessmentNext, for each predicate CBL trains a discretizedNa?
?ve Bayes classifier to classify the candidate in-stances.
Its features include pointwise mutual infor-mation (PMI) scores (Turney, 2001) of the candidateinstance with each of the positive and negative pat-terns associated with the class.
The current sets ofpromoted and negative instances are used as trainingexamples for the classifier.
Attributes are discretizedbased on information gain (Fayyad and Irani, 1993).Patterns are assessed using an estimate of the pre-cision of each pattern p:Precision(p) =?i?I count(i, p)count(p)where I is the set of promoted instances for thepredicate currently being considered, count(i, p) isthe co-occurrence count of instance i with pattern p,and count(p) is the hit count of the pattern p. Thisis a pessimistic estimate because it assumes that therest of the occurrences of pattern p are not with pos-itive examples of the predicate.
We also penalizeextremely rare patterns by thresholding the denomi-nator using the 25th percentile candidate pattern hitcount (McDowell and Cafarella, 2006).All of the co-occurrence counts needed for the as-sessment step are collected in the same MapReducepass as those required for filtering candidates.4.2.5 Candidate PromotionCBL then ranks the candidates according to theirassessment scores and promotes at most 100 in-stances and 5 patterns for each predicate.5 Experimental EvaluationWe designed our experimental evaluation to try toanswer the following questions: Can CBL iteratemany times and still achieve high precision?
Howhelpful are the types of coupling that we employ?Can we extend existing semantic resources?5.1 Configurations of the AlgorithmWe ran our algorithm in three configurations:?
Full: The algorithm as described in Section 4.2.?
No Sharing Among Same-Arity Predicates (NS):This configuration couples predicates only us-ing type-checking constraints.
It uses the fullalgorithm, except that predicates of the same ar-ity do not share promoted instances and patternswith each other.
Seed instances and patterns areshared, though, so each predicate has a small,fixed pool of negative evidence.?
No Category/Relation coupling (NCR): Thisconfiguration couples predicates using mutualexclusion and subset constraints, but not type-checking.
It uses the full algorithm, exceptthat relation instance arguments are not fil-tered or assessed using their specified categories,and arguments of promoted relations are notshared as promoted instances of categories.
Theonly type-checking information used is the com-mon/proper noun specifications of arguments forfiltering out implausible instances.5.2 Initial ontologyOur ontology contained categories and relations re-lated to two domains: companies and sports.
Ex-tra categories were added to provide negative evi-dence to the domain-related categories: ?hobby?
for?economic sector?
; ?actor,?
?politician,?
and ?scien-tist?
for ?athlete?
and ?coach?
; and ?board game?
for?sport?.
Table 1 lists each predicate in the leftmostcolumn.
Categories were started with 10?20 seed55 iterations 10 iterations 15 iterationsPredicate Full NS NCR Full NS NCR Full NS NCRActor 93 100 100 93 97 100 100 97 100Athlete 100 100 100 100 93 100 100 73 100Board Game 93 76 93 89 27 93 89 30 93City 100 100 100 100 97 100 100 100 100Coach 100 63 73 97 53 43 97 47 47Company 100 100 100 97 90 97 100 90 100Country 60 40 60 30 43 27 40 23 40Economic Sector 77 63 73 57 67 67 50 63 40Hobby 67 63 67 40 40 57 20 23 30Person 97 97 90 97 93 97 93 97 93Politician 93 93 97 73 53 90 90 53 87Product 97 87 90 90 87 100 97 90 77Product Type 93 93 90 70 73 97 77 80 67Scientist 100 90 97 97 63 97 93 60 100Sport 100 90 100 93 67 83 97 27 90Sports Team 100 97 100 97 70 100 90 50 100Category Average 92 84 89 82 70 84 83 63 79Acquired(Company, Company) 77 77 80 67 80 47 70 63 47CeoOf(Person, Company) 97 87 100 90 87 97 90 80 83CoachesTeam(Coach, Sports Team) 100 100 100 100 100 97 100 100 90CompetesIn(Company, Econ.
Sector) 97 97 80 100 93 67 97 63 60CompetesWith(Company, Company) 93 80 60 77 70 37 70 60 43HasOfficesIn(Company, City) 97 93 40 93 90 27 93 57 30HasOperationsIn(Company, Country) 100 95 50 100 97 40 90 83 13HeadquarteredIn(Company, City) 77 90 20 70 77 27 70 60 7LocatedIn(City, Country) 90 67 57 63 50 43 73 50 30PlaysFor(Athlete, Sports Team) 100 100 0 100 97 7 100 43 0PlaysSport(Athlete, Sport) 100 100 27 93 80 10 100 40 30TeamPlaysSport(Sports Team, Sport) 100 100 77 100 97 80 93 83 67Produces(Company, Product) 91 83 90 83 93 67 93 80 57HasType(Product, Product Type) 73 63 17 33 67 33 40 57 27Relation Average 92 88 57 84 84 48 84 66 42All 92 86 74 83 76 68 84 64 62Table 1: Precision (%) for each predicate.
Results are presented after 5, 10, and 15 iterations, for the Full, No Sharing(NS), and No Category/Relation Coupling (NCR) configurations of CBL .
Note that we expect Full and NCR toperform similarly for categories, but for Full to outperform NCR on relations and for Full to outperform NS on bothcategories and relations.6instances and 5 seed patterns.
The seed instanceswere specified by a human, and the seed patternswere derived from the generic patterns of Hearstfor each predicate (Hearst, 1992).
Relations werestarted with similar numbers of seed instances, andno seed patterns (it is less obvious how to gener-ate good seed patterns from relation names).
Mostpredicates were declared as mutually exclusive withmost others, except for special cases (e.g., ?hobby?and ?sport?
; ?university?
and ?sports team?
; and ?hasoffices in?
and ?headquartered in?
).5.3 CorpusOur text corpus was from a 200-million page webcrawl.
We parsed the HTML, filtered out non-English pages using a stop word ratio threshold, thenfiltered out web spam and adult content using a ?badword?
list.
The pages were then segmented into sen-tences, tokenized, and tagged with parts-of-speechusing the OpenNLP package.
Finally, we filteredthe sentences to eliminate those that were likely tobe noisy and not useful for learning (e.g., sentenceswithout a verb, without any lowercase words, withtoo many words that were all capital letters).
Thisyielded a corpus of roughly 514-million sentences.5.4 Experimental ProcedureWe ran each configuration for 15 iterations.
To eval-uate the precision of promoted instances, we sam-pled 30 instances from the promoted set for eachpredicate in each configuration after 5, 10, and 15 it-erations, pooled together the samples for each pred-icate, and then judged their correctness.
The judgedid not know which run an instance was sampledfrom.
We estimated the precision of the promotedinstances from each run after 5, 10, and 15 itera-tions as the number of correct promoted instancesdivided by the number sampled.
While samples of30 instances do not produce tight confidence inter-vals around individual estimates, they are sufficientfor testing for the effects in which we are interested.5.5 ResultsTable 1 shows the precision of each of the three al-gorithm configurations for each category and rela-tion after 5, 10, and 15 iterations.
As is apparentin this table, fully coupled training (Full) outper-forms training when coupling is removed betweencategories and relations (NCR), and also when cou-pling is removed among predicates of the same ar-ity (NS).
The net effect is substantial, as is appar-ent from the bottom row of Table 1, which showsthat the precision of Full outperforms NS by 6% andNCR by 18% after the first 5 iterations, and by aneven larger 20% and 22% after 15 iterations.
Thisincreasing gap in precision as iterations increase re-flects the ability of coupled learning to constrain thesystem to reduce the otherwise common drift asso-ciated with self-trained classifiers.Using Student?s paired t-test, we found that forcategories, the difference in performance betweenFull and NS is statistically significant after 5, 10,and 15 iterations (p-value < 0.05).2 No significantdifference was found between Full and NCR for cat-egories, but this is not a surprise, because NCR stilluses mutually exclusive and subset constraints.
Thesame test finds that the differences between Full andNS are significant for relations after 15 iterations,and the differences between Full and NCR are sig-nificant after 5, 10, and 15 iterations for relations.The worst-performing categories after 15 itera-tions of Full are ?country,?
?economic sector,?
and?hobby.?
The Full configuration of CBL promoted1637 instances for ?country,?
far more than the num-ber of correct answers.
Many of these are generalgeographic regions like ?Bayfield Peninsula?
and?Baltic Republics.?
In the ?hobby?
case, promotingpatterns like ?the types of arg1?
led to the categorydrifting into a general list of plural common nouns.
?Economic sector?
drifted into academic fields like?Behavioral Science?
and ?Political Sciences.?
Weexpect that the learning of these categories wouldbe significantly better if there were even more cat-egories being learned to provide additional negativeevidence during the filtering and assessment steps ofthe algorithm.At this stage of development, obtaining high re-call is not a priority because our intent is to createa continuously running and continuously improvingsystem; it is our hope that high recall will come withtime.
However, to very roughly convey the com-pleteness of the current results we show in Table 2the average number of instances promoted for cate-2Our selection of the paired t-test was motivated by the workof Smucker et al (2007), but the Wilcoxon signed rank testgives the same results.7Categories RelationsConfiguration Instances Prec.
Instances Prec.Full 970 83 191 84NS 1337 63 307 66NCR 916 79 458 42Table 2: Average numbers of promoted category and re-lation instances and estimates of their precision for eachconfiguration of CBL after 15 iterations.Figure 2: Extracted facts for two companies discoveredby CBL Full.
These two companies were extracted bythe learned ?company?
extractor, and the relations shownwere extracted by learned relation extractors.gories and relations for each of the three configura-tions of CBL after 15 iterations.
For categories, notsharing examples results in fewer negative examplesduring the filtering and assessment steps.
This yieldsmore promoted instances on average.
For relations,not using type checking yields higher relative recall,but at a much lower level of precision.Figure 2 gives one view of the type of informationextracted by the collection of learned category andrelation classifiers.
Note the initial seed examplesprovided to CBL did not include information abouteither company or any of these relation instances.35.6 Comparison to an Existing DatabaseTo estimate the capacity of our algorithm to con-tribute additional facts to publicly available seman-tic resources, we compared the complete lists of in-stances promoted during the Full 15 iteration runfor certain categories to corresponding lists in theFreebase database (Metaweb Technologies, 2009).Excluding the categories that did not have a di-rectly corresponding Freebase list, we computed foreach category: Precision ?
|CBLInstances| ?|Matches|, where Precision is the estimated pre-cision from our random sample of 30 instances,|CBLInstances| is the total number of instancespromoted for that category, and |Matches| is the3See http://rtw.ml.cmu.edu/sslnlp09 for re-sults from a full run of the system.Est.
CBL Freebase Est.
NewCategory Prec.
Instances Matches InstancesActor 100 522 465 57Athlete 100 117 54 63Board Game 89 18 6 10City 100 1799 1665 134Company 100 1937 995 942Econ.
Sector 50 1541 137 634Politician 90 962 74 792Product 97 1259 0 1221Sports Team 90 414 139 234Sport 97 613 134 461Table 3: Estimated numbers of ?new instances?
(correctinstances promoted by CBL in the Full 15 iteration runwhich do not have a match in Freebase) and the valuesused in calculating them.number of promoted instances that had an exactmatch in Freebase.
While exact matches may under-estimate the number of matches, it should be notedthat rather than make definitive claims, our intenthere is simply to give rough estimates, which areshown in Table 3.
These approximate numbers in-dicate a potential to use CBL to extend existing se-mantic resources like Freebase.6 ConclusionWe have presented a method of coupling the semi-supervised learning of categories and relations anddemonstrated empirically that the coupling forestallsthe problem of semantic drift associated with boot-strap learning methods.
We suspect that learningadditional predicates simultaneously will yield evenmore accurate learning.
An approximate compari-son with an existing repository of semantic knowl-edge, Freebase, suggests that our methods can con-tribute new facts to existing resources.AcknowledgmentsThis work is supported in part by DARPA, Google,a Yahoo!
Fellowship to Andrew Carlson, and theBrazilian research agency CNPq.
We also gratefullyacknowledge Jamie Callan for making available hiscollection of web pages, Yahoo!
for use of their M45computing cluster, and the anonymous reviewers fortheir comments.8ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In JCDL.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the web.
In IJCAI.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT.Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In WebDB Workshop at6th International Conference on Extending DatabaseTechnology.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28:41?75.Ming-Wei Chang, Lev-Arie Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In ACL.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In EMNLP.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual exclu-sion bootstrapping.
In PACLING.Jeffrey Dean and Sanjay Ghemawat.
2008.
Mapreduce:simplified data processing on large clusters.
Commun.ACM, 51(1):107?113.Doug Downey, Matthew Broadhead, and Oren Etzioni.2007.
Locating complex named entities in web text.In IJCAI.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web: an ex-perimental study.
Artif.
Intell., 165(1):91?134.Usama M. Fayyad and Keki B. Irani.
1993.
Multi-interval discretization of continuous-valued attributesfor classification learning.
In UAI.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In COLING.Qiuhua Liu, Xuejun Liao, and Lawrence Carin.
2008.Semi-supervised multitask learning.
In NIPS.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In NAACL.Luke K. McDowell and Michael Cafarella.
2006.Ontology-driven information extraction with on-tosyphon.
In ISWC.Metaweb Technologies.
2009.
Freebase data dumps.http://download.freebase.com/datadumps/.Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Names and similarities onthe web: fact extraction in the fast lane.
In ACL.Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Organizing and search-ing the world wide web of facts - step one: The one-million fact extraction challenge.
In AAAI.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In ACL.Ellen Riloff and Rosie Jones.
1999.
Learning dictionar-ies for information extraction by multi-level bootstrap-ping.
In AAAI.Benjamin Rosenfeld and Ronen Feldman.
2007.
Us-ing corpus statistics on entities to improve semi-supervised relation extraction from the web.
In ACL.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In HLT-NAACL.Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance tests forinformation retrieval evaluation.
In CIKM.Sebastian Thrun.
1996.
Is learning the n-th thing anyeasier than learning the first?
In NIPS.Peter D. Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In EMCL.Nicola Ueffing.
2006.
Self-training for machine trans-lation.
In NIPS workshop on Machine Learning forMultilingual Information Access.Roman Yangarber.
2003.
Counter-training in discoveryof semantic patterns.
In ACL.9
