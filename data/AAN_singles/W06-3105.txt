Proceedings of the Workshop on Statistical Machine Translation, pages 31?38,New York City, June 2006. c?2006 Association for Computational LinguisticsWhy Generative Phrase Models Underperform Surface HeuristicsJohn DeNero, Dan Gillick, James Zhang, Dan KleinDepartment of Electrical Engineering and Computer ScienceUniversity of California, BerkeleyBerkeley, CA 94705{denero, dgillick, jyzhang, klein}@eecs.berkeley.eduAbstractWe investigate why weights from generative mod-els underperform heuristic estimates in phrase-based machine translation.
We first propose a sim-ple generative, phrase-based model and verify thatits estimates are inferior to those given by surfacestatistics.
The performance gap stems primarilyfrom the addition of a hidden segmentation vari-able, which increases the capacity for overfittingduring maximum likelihood training with EM.
Inparticular, while word level models benefit greatlyfrom re-estimation, phrase-level models do not: thecrucial difference is that distinct word alignmentscannot all be correct, while distinct segmentationscan.
Alternate segmentations rather than alternatealignments compete, resulting in increased deter-minization of the phrase table, decreased general-ization, and decreased final BLEU score.
We alsoshow that interpolation of the two methods can re-sult in a modest increase in BLEU score.1 IntroductionAt the core of a phrase-based statistical machinetranslation system is a phrase table containingpairs of source and target language phrases, eachweighted by a conditional translation probability.Koehn et al (2003a) showed that translation qual-ity is very sensitive to how this table is extractedfrom the training data.
One particularly surprisingresult is that a simple heuristic extraction algorithmbased on surface statistics of a word-aligned trainingset outperformed the phrase-based generative modelproposed by Marcu and Wong (2002).This result is surprising in light of the reverse sit-uation for word-based statistical translation.
Specif-ically, in the task of word alignment, heuristic ap-proaches such as the Dice coefficient consistentlyunderperform their re-estimated counterparts, suchas the IBM word alignment models (Brown et al,1993).
This well-known result is unsurprising: re-estimation introduces an element of competition intothe learning process.
The key virtue of competitionin word alignment is that, to a first approximation,only one source word should generate each targetword.
If a good alignment for a word token is found,other plausible alignments are explained away andshould be discounted as incorrect for that token.As we show in this paper, this effect does not pre-vail for phrase-level alignments.
The central differ-ence is that phrase-based models, such as the onespresented in section 2 or Marcu and Wong (2002),contain an element of segmentation.
That is, they donot merely learn correspondences between phrases,but also segmentations of the source and target sen-tences.
However, while it is reasonable to sup-pose that if one alignment is right, others must bewrong, the situation is more complex for segmenta-tions.
For example, if one segmentation subsumesanother, they are not necessarily incompatible: bothmay be equally valid.
While in some cases, suchas idiomatic vs. literal translations, two segmenta-tions may be in true competition, we show that themost common result is for different segmentationsto be recruited for different examples, overfitting thetraining data and overly determinizing the phrasetranslation estimates.In this work, we first define a novel (but not rad-ical) generative phrase-based model analogous toIBM Model 3.
While its exact training is intractable,we describe a training regime which uses word-level alignments to constrain the space of feasiblesegmentations down to a manageable number.
Wedemonstrate that the phrase analogue of the Dice co-efficient is superior to our generative model (a re-sult also echoing previous work).
In the primarycontribution of the paper, we present a series of ex-periments designed to elucidate what re-estimationlearns in this context.
We show that estimates areoverly determinized because segmentations are used31in unintuitive ways for the sake of data likelihood.We comment on both the beneficial instances of seg-ment competition (idioms) as well as the harmfulones (most everything else).
Finally, we demon-strate that interpolation of the two estimates canprovide a modest increase in BLEU score over theheuristic baseline.2 Approach and Evaluation MethodologyThe generative model defined below is evaluatedbased on the BLEU score it produces in an end-to-end machine translation system from English toFrench.
The top-performing diag-and extractionheuristic (Zens et al, 2002) serves as the baseline forevaluation.1 Each approach ?
the generative modeland heuristic baseline ?
produces an estimated con-ditional distribution of English phrases given Frenchphrases.
We will refer to the distribution derivedfrom the baseline heuristic as ?H .
The distributionlearned via the generative model, denoted ?EM , isdescribed in detail below.2.1 A Generative Phrase ModelWhile our model for computing ?EM is novel, itis meant to exemplify a class of models that arenot only clear extensions to generative word align-ment models, but also compatible with the statisticalframework assumed during phrase-based decoding.The generative process we modeled produces aphrase-aligned English sentence from a French sen-tence where the former is a translation of the lat-ter.
Note that this generative process is opposite tothe translation direction of the larger system becauseof the standard noisy-channel decomposition.
Thelearned parameters from this model will be used totranslate sentences from English to French.
The gen-erative process modeled has four steps:21.
Begin with a French sentence f.1This well-known heuristic extracts phrases from a sentencepair by computing a word-level alignment for the sentence andthen enumerating all phrases compatible with that alignment.The word alignment is computed by first intersecting the direc-tional alignments produced by a generative IBM model (e.g.,model 4 with minor enhancements) in each translation direc-tion, then adding certain alignments from the union of the di-rectional alignments based on local growth rules.2Our notation matches the literature for phrase-based trans-lation: e is an English word, e?
is an English phrase, and e?I1 is asequence of I English phrases, and e is an English sentence.2.
Segment f into a sequence of I multi-wordphrases that span the sentence, f?
I1 .3.
For each phrase f?i ?
f?
I1 , choose a correspond-ing position j in the English sentence and es-tablish the alignment aj = i, then generate ex-actly one English phrase e?j from f?i.4.
The sequence e?j ordered by a describes an En-glish sentence e.The corresponding probabilistic model for this gen-erative process is:P (e|f) =?f?I1 ,e?I1,aP (e, f?
I1 , e?I1, a|f)=?f?I1 ,e?I1,a?(f?
I1 |f)?f?i?f?I1?
(e?j |f?i)d(aj = i|f)where P (e, f?
I1 , e?I1, a|f) factors into a segmentationmodel ?, a translation model ?
and a distortionmodel d. The parameters for each component of thismodel are estimated differently:?
The segmentation model ?(f?
I1 |f) is assumed tobe uniform over all possible segmentations fora sentence.3?
The phrase translation model ?
(e?j |f?i) is pa-rameterized by a large table of phrase transla-tion probabilities.?
The distortion model d(aj = i|f) is a discount-ing function based on absolute sentence posi-tion akin to the one used in IBM model 3.While similar to the joint model in Marcu and Wong(2002), our model takes a conditional form com-patible with the statistical assumptions used by thePharaoh decoder.
Thus, after training, the param-eters of the phrase translation model ?EM can beused directly for decoding.2.2 TrainingSignificant approximation and pruning is requiredto train a generative phrase model and table ?
suchas ?EM ?
with hidden segmentation and alignmentvariables using the expectation maximization algo-rithm (EM).
Computing the likelihood of the data3This segmentation model is deficient given a maximumphrase length: many segmentations are disallowed in practice.32for a set of parameters (the e-step) involves summingover exponentially many possible segmentations foreach training sentence.
Unlike previous attempts totrain a similar model (Marcu and Wong, 2002), weallow information from a word-alignment model toinform our approximation.
This approach allowedus to directly estimate translation probabilities evenfor rare phrase pairs, which were estimated heuristi-cally in previous work.In each iteration of EM, we re-estimate eachphrase translation probability by summing fractionalphrase counts (soft counts) from the data given thecurrent model parameters.
?new(e?j |f?i) =c(f?i, e?j)c(f?i)=?
(f,e)?f?I1 :f?i?f?I1?e?I1:e?j?e?I1?a:aj=i P (e, f?I1 , e?I1, a|f)?f?I1 :f?i?f?I1?e?I1?a P (e, f?I1 , e?I1, a|f)This training loop necessitates approximation be-cause summing over all possible segmentations andalignments for each sentence is intractable, requiringtime exponential in the length of the sentences.
Ad-ditionally, the set of possible phrase pairs grows toolarge to fit in memory.
Using word alignments, wecan address both problems.4 In particular, we candetermine for any aligned segmentation (f?
I1 , e?I1, a)whether it is compatible with the word-level align-ment for the sentence pair.
We define a phrase pairto be compatible with a word-alignment if no wordin either phrase is aligned with a word outside theother phrase (Zens et al, 2002).
Then, (f?
I1 , e?I1, a)is compatible with the word-alignment if each of itsaligned phrases is a compatible phrase pair.The training process is then constrained such that,when evaluating the above sum, only compatiblealigned segmentations are considered.
That is, weallow P (e, f?
I1 , e?I1, a|f) > 0 only for aligned seg-mentations (f?
I1 , e?I1, a) such that a provides a one-to-one mapping from f?
I1 to e?I1 where all phrase pairs(f?aj , e?j) are compatible with the word alignment.This constraint has two important effects.
First,we force P (e?j |f?i) = 0 for all phrase pairs not com-patible with the word-level alignment for some sen-tence pair.
This restriction successfully reduced the4The word alignments used in approximating the e-stepwere the same as those used to create the heuristic diag-andbaseline.total legal phrase pair types from approximately 250million to 17 million for 100,000 training sentences.However, some desirable phrases were eliminatedbecause of errors in the word alignments.Second, the time to compute the e-step is reduced.While in principle it is still intractable, in practicewe can compute most sentence pairs?
contributionsin under a second each.
However, some spuriousword alignments can disallow all segmentations fora sentence pair, rendering it unusable for training.Several factors including errors in the word-levelalignments, sparse word alignments and non-literaltranslations cause our constraint to rule out approx-imately 54% of the training set.
Thus, the reducedsize of the usable training set accounts for some ofthe degraded performance of ?EM relative to ?H .However, the results in figure 1 of the following sec-tion show that ?EM trained on twice as much dataas ?H still underperforms the heuristic, indicating alarger issue than decreased training set size.2.3 Experimental DesignTo test the relative performance of ?EM and ?H ,we evaluated each using an end-to-end translationsystem from English to French.
We chose this non-standard translation direction so that the examplesin this paper would be more accessible to a primar-ily English-speaking audience.
All training and testdata were drawn from the French/English section ofthe Europarl sentence-aligned corpus.
We tested onthe first 1,000 unique sentences of length 5 to 15 inthe corpus and trained on sentences of length 1 to 60starting after the first 10,000.The system follows the structure proposed inthe documentation for the Pharaoh decoder anduses many publicly available components (Koehn,2003b).
The language model was generated fromthe Europarl corpus using the SRI Language Model-ing Toolkit (Stolcke, 2002).
Pharaoh performed de-coding using a set of default parameters for weight-ing the relative influence of the language, translationand distortion models (Koehn, 2003b).
A maximumphrase length of three was used for all experiments.To properly compare ?EM to ?H , all aspects ofthe translation pipeline were held constant except forthe parameters of the phrase translation table.
In par-ticular, we did not tune the decoding hyperparame-ters for the different phrase tables.33Source 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743Iteration 2 0.3735 0.3851 0.3814iteration 3 0.3705 0.384 0.3827Iteration 4 0.3695 0.285 0.3801iteration 5 0.3705 0.284 0.3774interpSource 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743iteration 3 0.3705 0.384 0.3827iteration 3 0.3705 0.384 0.38270.360.370.380.390.4025k 50k 100kTraining sentencesBLEUHeuristicIteration 1iteration 30%20%40%60%80%100%0 10 20 30 40 50 60Sentence LengthSentences SkippedFigure 1: Statistical re-estimation using a generativephrase model degrades BLEU score relative to itsheuristic initialization.3 ResultsHaving generated ?H heuristically and ?EM withEM, we now compare their performance.
While themodel and training regimen for ?EM differ from themodel from Marcu and Wong (2002), we achievedresults similar to Koehn et al (2003a): ?EM slightlyunderperformed ?H .
Figure 1 compares the BLEUscores using each estimate.
Note that the expecta-tion maximization algorithm for training ?EM wasinitialized with the heuristic parameters ?H , so theheuristic curve can be equivalently labeled as itera-tion 0.Thus, the first iteration of EM increases the ob-served likelihood of the training sentences while si-multaneously degrading translation performance onthe test set.
As training proceeds, performance onthe test set levels off after three iterations of EM.
Thesystem never achieves the performance of its initial-ization parameters.
The pruning of our training regi-men accounts for part of this degradation, but not all;augmenting ?EM by adding back in all phrase pairsthat were dropped during training does not close theperformance gap between ?EM and ?H .3.1 AnalysisLearning ?EM degrades translation quality in largepart because EM learns overly determinized seg-mentations and translation parameters, overfittingthe training data and failing to generalize.
The pri-mary increase in richness from generative word-level models to generative phrase-level models isdue to the additional latent segmentation variable.Although we impose a uniform distribution oversegmentations, it nonetheless plays a crucial roleduring training.
We will characterize this phe-nomenon through aggregate statistics and transla-tion examples shortly, but begin by demonstratingthe model?s capacity to overfit the training data.Let us first return to the motivation behind in-troducing and learning phrases in machine transla-tion.
For any language pair, there are contiguousstrings of words whose collocational translation isnon-compositional; that is, they translate togetherdifferently than they would in isolation.
For in-stance, chat in French generally translates to cat inEnglish, but appeler un chat un chat is an idiomwhich translates to call a spade a spade.
Introduc-ing phrases allows us to translate chat un chat atom-ically to spade a spade and vice versa.While introducing phrases and parameterizingtheir translation probabilities with a surface heuris-tic allows for this possibility, statistical re-estimationwould be required to learn that chat should never betranslated to spade in isolation.
Hence, translating Ihave a spade with ?H could yield an error.But enforcing competition among segmentationsintroduces a new problem: true translation ambigu-ity can also be spuriously explained by the segmen-tation.
Consider the french fragment carte sur latable, which could translate to map on the table ornotice on the chart.
Using these two sentence pairsas training, one would hope to capture the ambiguityin the parameter table as:French English ?
(e|f)carte map 0.5carte notice 0.5carte sur map on 0.5carte sur notice on 0.5sur on 1.0... ... ...table table 0.5table chart 0.5Assuming we only allow non-degenerate seg-mentations and disallow non-monotonic alignments,this parameter table yields a marginal likelihoodP (f|e) = 0.25 for both sentence pairs ?
the intu-itive result given two independent lexical ambigu-34ities.
However, the following table yields a likeli-hood of 0.28 for both sentences:5French English ?
(e|f)carte map 1.0carte sur notice on 1.0carte sur la notice on the 1.0sur on 1.0sur la table on the table 1.0la the 1.0la table the table 1.0table chart 1.0Hence, a higher likelihood can be achieved by al-locating some phrases to certain translations whilereserving overlapping phrases for others, therebyfailing to model the real ambiguity that exists acrossthe language pair.
Also, notice that the phrase surla can take on an arbitrary distribution over any en-glish phrases without affecting the likelihood of ei-ther sentence pair.
Not only does this counterintu-itive parameterization give a high data likelihood,but it is also a fixed point of the EM algorithm.The phenomenon demonstrated above poses aproblem for generative phrase models in general.The ambiguous process of translation can be mod-eled either by the latent segmentation variable or thephrase translation probabilities.
In some cases, opti-mizing the likelihood of the training corpus adjustsfor the former when we would prefer the latter.
Wenext investigate how this problem manifests in ?EMand its effect on translation quality.3.2 Learned parametersThe parameters of ?EM differ from the heuristicallyextracted parameters ?H in that the conditional dis-tributions over English translations for some Frenchwords are sharply peaked for ?EM compared to flat-ter distributions generated by ?H .
This determinism?
predicted by the previous section?s example ?
isnot atypical of EM training for other tasks.To quantify the notion of peaked distributionsover phrase translations, we compute the entropy ofthe distribution for each French phrase according to5For example, summing over the first translation ex-pands to 17 (?
(map | carte)?
(on the table | sur la table)+?
(map | carte)?
(on | sur)?
(the table | la table)).it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-050 10 20 30 400 - .01.01 - .5.5 - 11 - 1.51.5 - 2> 2Entropy% Phrase TranslationsLearnedHeuristic1E-04 1E-02 1E+00 1E+02',de.lall 'leetlesMostCommon French PhrasesEntropyLearned HeuristicFigure 2: Many more French phrases have very lowentropy under the learned parameterization.the standard definition.H(?(e?|f?))
=?e??(e?|f?)
log2 ?(e?|f?
)The average entropy, weighted by frequency, for themost common 10,000 phrases in the learned tablewas 1.55, comparable to 3.76 for the heuristic table.The difference between the tables becomes muchmore striking when we consider the histogram ofentropies for phrases in figure 2.
In particular, thelearned table has many more phrases with entropynear zero.
The most pronounced entropy differencesoften appear for common phrases.
Ten of the mostcommon phrases in the French corpus are shown infigure 3.As more probability mass is reserved for fewertranslations, many of the alternative translations un-der ?H are assigned prohibitively small probabili-ties.
In translating 1,000 test sentences, for example,no phrase translation with ?(e?|f?)
less than 10?5 wasused by the decoder.
Given this empirical threshold,nearly 60% of entries in ?EM are unusable, com-pared with 1% in ?H .3.3 Effects on TranslationWhile this determinism of ?EM may be desirablein some circumstances, we found that the ambi-guity in ?H is often preferable at decoding time.35it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-050102030400 - .01 .01 - .5 .5 - 1 1 - 1.5 1.5 - 2 > 2Entropy% PhraseTranslations HeuristicLearned1E-04 1E-02 1E+00 1E+02 ',.ll 'n 'quequiplusl ' unionCommon French PhrasesEntropyLearned HeuristicFigure 3: Entropy of 10 common French phrases.Several learned distributions have very low entropy.In particular, the pattern of translation-ambiguousphrases receiving spuriously peaked distributions (asdescribed in section 3.1) introduces new tra slationerrors relative to the baseline.
We now investigateboth positive and negative effects of the learningprocess.The issue that motivated training a generativemodel is sometimes resolved correctly: for a wordthat translates differently alone than in the contextof an idiom, the translation probabilities can moreaccurately reflect this.
Returning to the previous ex-ample, the phrase table for chat has been correctedthrough the learning process.
The heuristic processgives the incorrect translation spade with 61% prob-ability, while the statistical learning approach givescat with 95% probability.While such examples of improvement are en-couraging, the trend of spurious determinism over-whelms this benefit by introducing errors in four re-lated ways, each of which will be explored in turn.1.
Useful phrase pairs can be assigned very lowprobabilities and therefore become unusable.2.
A proper translation for a phrase can be over-ridden by another translation with spuriouslyhigh probability.3.
Error-prone, common, ambiguous phrases be-come active during decoding.4.
The language model cannot distinguish be-tween different translation options as effec-tively due to deterministic translation modeldistributions.The first effect follows from our observation insection 3.2 that many phrase pairs are unusable dueto vanishingly small probabilities.
Some of the en-tries that are made unusable by re-estimation arehelpful at decoding time, evidenced by the factthat pruning the set of ?EM ?s low-scoring learnedphrases from the original heuristic table reducesBLEU score by 0.02 for 25k training sentences (be-low the score for ?EM ).The second effect is more subtle.
Consider thesentence in figure 4, which to a first approxima-tion can be translated as a series of cognates, asdemonstrated by the decoding that follows from theheuristic parameterization ?H .6 Notice also that thetranslation probabilities from heuristic extraction arenon-deterministic.
On the other hand, the translationsystem makes a significant lexical error on this sim-ple sentence when parameterized by ?EM : the useof caracte?rise in this context is incorrect.
This errorarises from a sharply peaked distribution over En-glish phrases for caracte?rise.This example illustrates a recurring problem: er-rors do not necessarily arise because a correct trans-lation is not available.
Notice that a preferable trans-lation of degree as degre?
is available under both pa-rameterizations.
Degre?
is not used, however, be-cause of the peaked distribution of a competingtranslation candidate.
In this way, very high prob-ability translations can effectively block the use ofmore appropriate translations at decoding time.What is furthermore surprising and noteworthy inthis example is that the learned, near-deterministictranslation for caracte?rise is not a common trans-lation for the word.
Not only does the statisticallearning process yield low-entropy translation dis-tributions, but occasionally the translation with un-desirably high conditional probability does not havea strong surface correlation with the source phrase.This example is not unique; during different initial-izations of the EM algorithm, we noticed such pat-6While there is some agreement error and awkwardness, theheuristic translation is comprehensible to native speakers.
Thelearned translation incorrectly translates degree, degrading thetranslation quality.36the situation varies to anla situation varie d ' uneHeuristically Extracted Phrase TableLearned Phrase Tableenormousimmensedegreedegr?situation varies tola varie d 'an enormousune immensedegreecaract?risethesituationcaracte?riseEnglish ?
(e|f)degree 0.998characterises 0.001characterised 0.001caracte?riseEnglish ?
(e|f)characterises 0.49characterised 0.21permeate 0.05features 0.05typifies 0.05degr e?English ?
(e|f)degree 0.49level 0.38extent 0.02amount 0.02how 0.01degre?English ?
(e|f)degree 0.64level 0.26extent 0.10Figure 4: Spurious determinism in the learned phrase parameters degrades translation quality.terns even for common French phrases such as deand ne.The third source of errors is closely related: com-mon phrases that translate in many ways dependingon the context can introduce errors if they have aspuriously peaked distribution.
For instance, con-sider the lone apostrophe, which is treated as a sin-gle token in our data set (figure 5).
The shape ofthe heuristic translation distribution for the phrase isintuitively appealing, showing a relatively flat dis-tribution among many possible translations.
Sucha distribution has very high entropy.
On the otherhand, the learned table translates the apostrophe tothe with probability very near 1.HeuristicEnglish ?H(e|f)our 0.10that 0.09is 0.06we 0.05next 0.05LearnedEnglish ?EM (e|f)the 0.99, 4.1 ?
10?3is 6.5 ?
10?4to 6.3 ?
10?4in 5.3 ?
10?4Figure 5: Translation probabilities for an apostro-phe, the most common french phrase.
The learnedtable contains a highly peaked distribution.Such common phrases whose translation dependshighly on the context are ripe for producing transla-tion errors.
The flatness of the distribution of ?H en-sures that the single apostrophe will rarely be usedduring decoding because no one phrase table entryhas high enough probability to promote its use.
Onthe other hand, using the peaked entry ?EM (the|?
)incurs virtually no cost to the score of a translation.The final kind of errors stems from interactionsbetween the language and translation models.
Theselection among translation choices via a languagemodel ?
a key virtue of the noisy channel frame-work ?
is hindered by the determinism of the transla-tion model.
This effect appears to be less significantthan the previous three.
We should note, however,that adjusting the language and translation modelweights during decoding does not close the perfor-mance gap between ?H and ?EM .3.4 ImprovementsIn light of the low entropy of ?EM , we could hope toimprove translations by retaining entropy.
There areseveral strategies we have considered to achieve this.Broadly, we have tried two approaches: combin-ing ?EM and ?H via heuristic interpolation methodsand modifying the training loop to limit determin-ism.The simplest strategy to increase entropy is tointerpolate the heuristic and learned phrase tables.Varying the weight of interpolation showed an im-provement over the heuristic of up to 0.01 for 100ksentences.
A more modest improvement of 0.003 for25k training sentences appears in table 1.In another experiment, we interpolated the out-put of each iteration of EM with its input, therebymaintaining some entropy from the initialization pa-rameters.
BLEU score increased to a maximum of0.394 using this technique with 100k training sen-tences, outperforming the heuristic by a slim marginof 0.005.We might address the determinization in ?EMwithout resorting to interpolation by modifying the37training procedure to retain entropy.
By imposing anon-uniform segmentation model that favors shorterphrases over longer ones, we hope to prevent theerror-causing effects of EM training outlined above.In principle, this change will encourage EM to ex-plain training sentences with shorter sentences.
Inpractice, however, this approach has not led to animprovement in BLEU.Another approach to maintaining entropy duringthe training process is to smooth the probabilitiesgenerated by EM.
In particular, we can use the fol-lowing smoothed update equation during the train-ing loop, which reserves a portion of probabilitymass for unseen translations.
?new(e?j |f?i) =c(f?i, e?j)c(f?i) + kl?1In the equation above, l is the length of the Frenchphrase and k is a tuning parameter.
This formula-tion not only serves to reduce very spiked probabili-ties in ?EM , but also boosts the probability of shortphrases to encourage their use.
With k = 2.5, thissmoothing approach improves BLEU by .007 using25k training sentences, nearly equaling the heuristic(table 1).4 ConclusionRe-estimating phrase translation probabilities usinga generative model holds the promise of improvingupon heuristic techniques.
However, the combina-torial properties of a phrase-based generative modelhave unfortunate side effects.
In cases of true ambi-guity in the language pair to be translated, parameterestimates that explain the ambiguity using segmen-tation variables can in some cases yield higher datalikelihoods by determinizing phrase translation esti-mates.
However, this behavior in turn leads to errorsat decoding time.We have also shown that some modest benefit canbe obtained from re-estimation through the blunt in-strument of interpolation.
A remaining challenge isto design more appropriate statistical models whichtie segmentations together unless sufficient evidenceof true non-compositionality is present; perhapssuch models could properly combine the benefits ofboth current approaches.Estimate BLEU?H 0.385?H phrase pairs that also appear in ?EM 0.365?EM 0.374?EM with a non-uniform segmentation model 0.374?EM with smoothing 0.381?EM with gaps filled in by ?H 0.374?EM interpolated with ?H 0.388Table 1: BLEU results for 25k training sentences.5 AcknowledgmentsWe would like to thank the anonymous reviewers fortheir valuable feedback on this paper.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
The mathematics ofstatistical machine translation: Parameter estimation.Computational Linguistics, 19(2), 1993.Philipp Koehn.
Europarl: A Multilingual Corpus forEvaluation of Machine Translation.
USC InformationSciences Institute, 2002.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
Sta-tistical phrase-based translation.
HLT-NAACL, 2003.Philipp Koehn.
Pharaoh: A Beam Search Decoder forPhrase-Based Statisical Machine Translation Models.USC Information Sciences Institute, 2003.Daniel Marcu and William Wong.
A phrase-based, jointprobability model for statistical machine translation.Conference on Empirical Methods in Natual LanguageProcessing, 2002.Franz Josef Och and Hermann Ney.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics, 29(1):19?51, 2003.Franz Josef Och, Christoph Tillmann, and Hermann Ney.Improved alignment models for statistical machinetranslation.
ACL Workshops, 1999.Andreas Stolcke.
Srilm ?
an extensible language model-ing toolkit.
Proceedings of the International Confer-ence on Statistical Language Processing, 2002.Richard Zens, Franz Josef Och and Hermann Ney.Phrase-Based Statistical Machine Translation.
AnnualGerman Conference on AI, 2002.38
