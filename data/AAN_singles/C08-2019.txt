Coling 2008: Companion volume ?
Posters and Demonstrations, pages 75?78Manchester, August 2008Using very simple statistics for review search: An explorationBo PangYahoo!
Researchbopang@yahoo-inc.comLillian LeeComputer Science Department, Cornell Universityllee@cs.cornell.eduAbstractWe report on work in progress on usingvery simple statistics in an unsupervisedfashion to re-rank search engine resultswhen review-oriented queries are issued;the goal is to bring opinionated or subjec-tive results to the top of the results list.
Wefind that our proposed technique performscomparably to methods that rely on sophis-ticated pre-encoded linguistic knowledge,and that both substantially improve the ini-tial results produced by the Yahoo!
searchengine.1 IntroductionOne important information need shared by manypeople is to find out about opinions and perspec-tives on a particular topic (Mishne and de Rijke,2006; Pang and Lee, 2008).
In fact, locating rel-evant subjective texts was a core task in the 2006and 2007 TREC Blog tracks (Ounis et al, 2006;Ounis et al, 2008).
Most participants considered atwo-phase re-ranking approach, where first topic-based relevancy search was employed, and thensome sort of filtering for subjectivity was applied;these filters were based on trained classifiers orsubjectivity lexicons.We propose an alternative approach to reviewsearch, one that is unsupervised and that doesnot rely on pre-existing dictionaries.
Rather, itin essence simply re-ranks the top k topic-basedc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.search results by placing those that have the leastidiosyncratic term distributions, with respect to thestatistics of the top k results, at the head of the list.The fact that it is the least, not the most, rare termswith respect to the search results that are most in-dicative of subjectivity may at first seem rathercounterintuitive; indeed, previous work has foundrare terms to be important subjectivity cues (Wiebeet al, 2004).
However, reviews within a given setof search results may tend to resemble each otherbecause they tend to all discuss salient attributes ofthe topic in question.2 AlgorithmDefine a search set as the top n webpages returnedin response to a review- or opinion-oriented queryby a high-quality initial search engine, in our case,the top 20 returned by Yahoo!.
As a question ofboth pragmatic and scientific value, we considerhow much information can be gleaned simply fromthe items in the search set itself; in particular, weask whether the subjective texts in the search setcan be ranked above the objective ones solely fromexamination of the patterns of term occurrencesacross the search-set documents.The idea we pursue is based in part on the as-sumption that the initial search engine is of rel-atively high quality, so that many of the search-set documents probably are, in fact, subjective.Therefore, re-ordering the top-ranked documentsby how much they resemble the other search-setdocuments in aggregate may be a good way toidentify the reviews.
Indeed, perhaps the reviewswill be similar to one another because they all tendto discuss salient features of the topic in question.75Suppose we have defined a search-set rarityfunction Rarityss(t) (see Section 2.1 below) thatvaries inversely with the number of documents inthe search set that contain the term t. Then, wedefine the idiosyncrasy score of a document d asthe average search-set rarity of the most commonterms it contains:I(d, k) =1k?t?k-commonest-terms(d)Rarityss(t) ,(1)where k-commonest-terms(d) is the k common-est terms in the search set that also occur in d. Forexample, when we set k to be the size of the vo-cabulary of d, the idiosyncrasy score is the aver-age search-set rarity of all the terms d contains.Then, to instantiate the similarity intuition outlinedabove, we simply rank by decreasing idiosyncrasy.The reason we look at just the top most com-mon terms is that the rarer terms might be noise.For example, terms that occur in just a few ofthe search-set documents might represent page- orsite-specific information that is irrelevant to thequery; but the presence of such terms does not nec-essarily indicate that the document in question isobjective.One potential problem with the approach out-lined above is the presence of stopwords, sinceall documents, subjective or objective, can be ex-pected to contain many of them.
Therefore, stop-word removal is indicated as an important pre-processing step.
As it turns out, the commonly-used InQuery stopword list (Allan et al, 2000)contains terms like ?less?
that, while uninforma-tive for topic-based information retrieval, may beimportant for subjectivity detection.
Therefore, weused a 102-item list1 based solely on frequencies inthe British National Corpus.2.1 Defining search-set rarityThere are various ways to define a search-set rar-ity function on terms.
Inspired by the efficacy ofthe inverse document frequency (IDF) in informa-tion retrieval, we consider several definitions forRarityss(t).
Let nss(t) be the number of docu-ments in the search set (not the entire corpus) thatcontain the term t. Due to space constraints, we1www.eecs.umich.edu/?qstout/586/bncfreq.htmlonly report results for:Rarityss(t)def=1nss(t),which is linearly increasing in 1/nss(t), (as befitsa measure of ?idiosyncrasy?).
The other defini-tions we considered were logarithmic or polyno-mial in 1/nss(t), and performed similarly to thelinear function.2.2 Comparison algorithmsOpinionFinder is a state-of-the-art publicly avail-able software package for sentiment analysis thatcan be applied to determining sentence-level sub-jectivity (Riloff and Wiebe, 2003; Wiebe andRiloff, 2005).
It employs a number ofpre-processing steps, including sentence splitting,part-of-speech tagging, stemming, and shallowparsing.
Shallow parsing is needed to identify theextraction patterns that the sentence classifiers in-corporate.We used OpinionFinder?s sentence-level output2to perform document-level subjectivity re-rankingas follows.
The result of running OpinionFinder?ssentence classifier is that each valid sentence3 isannotated with one of three labels: ?subj?, ?obj?,or ?unknown?.
First, discard the sentences labeled?unknown?.
Then, rank the documents by de-creasing percentage of subjective sentences amongthose sentences that are left.
In the case of ties, weuse the ranking produced by the initial search en-gine.We also considered a more lightweight wayto incorporate linguistic knowledge: score eachdocument according the percentage of adjectiveswithin the set of tokens it contains.
The motiva-tion is previous work suggesting that the presenceof adjectives is a strong indicator of the subjectiv-ity of the enclosing sentence (Hatzivassiloglou andWiebe, 2000; Wiebe et al, 2004).2There are actually two versions.
We used the accuracy-optimized version, as it outperformed the precision-optimizedversion.3OpinionFinder will only process documents in which allstrings identified as sentences by the system contain fewerthan 1000 words.
For the 31 documents in our dataset thatfailed this criterion, we set their score to 0.76p@1 p@2 p@3 p@4 p@5 p@10 p@S MAPSearch-engine baseline .536 .543 .541 .554 .554 .528 .538 .612OpinionFinder (accuracy version) .754 .717 .729 .725 .733 .675 .690 .768% of adjectives (type-based) .710 .703 .696 .681 .678 .625 .633 .715idiosyncrasy(linear), k = 50 .797 .783 .739 .717 .696 .613 .640 .729idiosyncrasy(linear), k = 100 .754 .783 .768 .739 .716 .630 .665 .743idiosyncrasy(linear), k = 200 .768 .761 .744 .746 .716 .623 .653 .731idiosyncrasy(linear), k = 300 .754 .761 .749 .736 .704 .614 .641 .724Table 1: Average search-set subjective-document precision results.
?S?
: number of subjective docu-ments.
Bold and underlining: best and second-best performance per column, respectively.3 EvaluationOur focus is on the quality of the documents placedat the very top ranks, since users often look onlyat the first page or first half of the first page ofresults (Joachims et al, 2005).
Hence, we reportthe precision of the top 1-5 and 10 documents, aswell as precision at the number of subjective doc-uments and mean average precision (MAP) for thesubjective documents.
All performance numbersare averages over the 69 search sets in our data,described next.Data Here, we sketch the data acquisition andlabeling process.
In order to get real user queriestargeted at reviews, we began with a randomly se-lected set of queries containing the word ?review?or ?reviews?4 from the the query log available athttp://www.sigkdd.org/kdd2005/kddcup/KDDCUPData.zip .
We created a search set foreach query by taking the top 20 webpages returnedby the Yahoo!
search engine and applying somepostprocessing.
Over a dozen volunteer annotatorsthen labeled the documents as to whether theywere subjective or objective according to a setof detailed instructions.
The end result wasover 1300 hand-labeled documents distributedacross 69 search sets, varying widely with re-spect to query topic.
Our dataset download siteis http://www.cs.cornell.edu/home/llee/data/search-subj.html .For almost every annotator, at least two of hisor her search sets were labeled by another personas well, so that we could measure pair-wise agree-4Subsequent manual filtering discarded some non-opinion-oriented queries, such as ?alternative medicine re-view volume5 numer1 pages 28 38 2000?.ment with respect to multiple queries.
On average,there was agreement on 88.2% of the documentsper search set, with the average Kappa coefficient(?)
being an acceptable 0.73, reflecting in part thedifficulty of the judgment.5 The lowest ?
occurson a search set with a 75% agreement rate.Results A natural and key baseline is the rankingprovided by the Yahoo!
search engine, which is ahigh-quality, industrial-strength system.
We con-sider this to be a crucial point of comparison.
Theresults are shown in the top line in Table 1.OpinionFinder clearly outperforms the initialsearch engine by a substantial margin, indicatingthat there are ample textual cues that can helpachieve better subjectivity re-ranking.The adjective-percentage baseline is also far su-perior to that of the search-engine baseline at allranks, but does not quite match OpinionFinder.
(Note that to achieve these results, we first dis-carded all terms contained in three or fewer of thesearch-set documents, since including such termsdecreased performance.)
Still, it is interesting tosee that it appears that a good proportion of theimprovements provided by OpinionFinder can beachieved using just adjective counts alone.We now turn to subjectivity re-ranking based onterm-distribution (idiosyncrasy) information.
For5One source of disagreement that stems from the specificsof our design is that we instructed annotators to mark ?salespitch?
documents as non-reviews, on the premise that al-though such texts are subjective, they are not valuable to auser searching for unbiased reviews.
(Note that this pol-icy presumably makes the dataset more challenging for au-tomated algorithms.)
There are several cases where only oneannotator identified this type of bias, which is not surprisingsince the authors of sales pitches may actively try to fool read-ers into believing the text to be unbiased.77consistency with the adjective-based method justdescribed, we first discarded all terms contained inthree or fewer of the search-set documents.As shown in Table 1, the idiosyncrasy-based al-gorithm posts results that are overall strongly su-perior to those of the initial, high-quality searchengine algorithm and also generally better than theadjective-percentage algorithm.
Note that thesephenomena hold for a range of values of k. Theoverall performance is also on par with Opin-ionFinder; for instance, according to the pairedt-test, the only statistically significant perfor-mance difference (.05 level) between the accuracy-emphasizing version of OpinionFinder and theidiosyncrasy-based algorithm for k = 100 is forprecision at 10.
In some sense, this is a striking re-sult: just looking at within-search-set frequenciesyields performance comparable to that of a methodthat utilizes rich linguistic knowledge and externalresources regarding subjectivity indicators.Another interesting observation is that term-distribution information seems to be more effectivefor achieving high precision at the very top ranks(precision at 1, 2, 3, and 4), whereas in contrast,relatively deep NLP seems to be more effective atachieving high precision at the ?lower?
top ranks,as demonstrated by the results for precision at 5,10, and the number of subjective documents, andfor MAP.
These results suggest that a combinationof the two methods could produce even greater im-provements.4 Concluding remarksWe considered the task of document-level sub-jectivity re-ranking of search sets, a task mod-eling a scenario in which a search engine isqueried to find reviews.
We found that our pro-posed term-distributional, idiosyncrasy-based al-gorithm yielded the best precision for the very topranks, whereas the more linguistically-oriented,knowledge-rich approach exemplified by Opinion-Finder gave the best results for precision at lowerranks.
It therefore seems that both types of infor-mation can be very valuable for the subjectivityre-ranking task, since they have somewhat com-plementary performance behaviors and both out-perform the initial search engine and an adjective-based approach.Our motivation that within a search set, reviewstend to resemble one another rather than differis reminiscent of intuitions underlying the use ofpseudo relevance feedback (PF) in IR (Ruthvenand Lalmas, 2003, Section 3.5).
Future work in-cludes comparison against PF methods and inves-tigation of ways to select the value of k.Acknowledgments We thank Eli Barzilay, Rich Caru-ana, Thorsten Joachims, Jon Kleinberg, Ravi Kumar, and thereviewers for their very useful help.
We are also very gratefulto our annotators, Mohit Bansal, Eric Breck, Yejin Choi, MattConnelly, Tom Finley, Effi Georgala, Asif-ul Haque, KersingHuang, Evie Kleinberg, Art Munson, Ben Pu, Ari Rabkin,Benyah Shaparenko, Ves Stoyanov, and Yisong Yue.
Thispaper is based upon work supported in part by the NSF un-der grant no.
IIS-0329064, a Yahoo!
Research Alliance gift,Google Anita Borg Memorial Scholarship funds, a CornellProvost?s Award for Distinguished Research, and an AlfredP.
Sloan Research Fellowship.
Any opinions, findings, andconclusions or recommendations expressed are those of theauthors and do not necessarily reflect the views or officialpolicies, either expressed or implied, of any sponsoring in-stitutions, the U.S. government, or any other entity.ReferencesAllan, James, Margaret E. Connell, W. Bruce Croft, Fang-Fang Feng, David Fisher, and Xiaoyan Li.
2000.
IN-QUERY and TREC-9.
In Proceedings of TREC, pages551?562.
NIST Special Publication 500-249.Hatzivassiloglou, Vasileios and Janyce Wiebe.
2000.
Effectsof adjective orientation and gradability on sentence subjec-tivity.
In Proceedings of COLING.Joachims, Thorsten, Laura Granka, Bing Pan, Helene Hem-brooke, and Geri Gay.
2005.
Accurately interpretingclickthrough data as implicit feedback.
In Proceedings ofSIGIR, pages 154?161.Mishne, Gilad and Maarten de Rijke.
2006.
A study of blogsearch.
In Proceedings of ECIR.Ounis, Iadh, Maarten de Rijke, Craig Macdonald, GiladMishne, and Ian Soboroff.
2006.
Overview of the TREC-2006 Blog Track.
In Proceedings of TREC.Ounis, Iadh, Craig Macdonald, and Ian Soboroff.
2008.
Onthe TREC Blog Track.
In Proceedings of ICWSM.Pang, Bo and Lillian Lee.
2008.
Opinion Mining and Sen-timent Analysis.
Foundations and Trends in InformationRetrieval series.
Now publishers.Riloff, Ellen and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceedingsof EMNLP.Ruthven, Ian and Mounia Lalmas.
2003.
A survey on theuse of relevance feedback for information access systems.Knowledge Engineering Review, 18(2):95?145.Wiebe, Janyce M. and Ellen Riloff.
2005.
Creating subjectiveand objective sentence classifiers from unannotated texts.In Proceedings of CICLing, number 3406 in LNCS, pages486?497.Wiebe, Janyce M., Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjec-tive language.
Computational Linguistics, 30(3):277?308,September.78
