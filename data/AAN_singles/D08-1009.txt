Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79?88,Honolulu, October 2008. c?2008 Association for Computational LinguisticsScaling Textual Inference to the WebStefan Schoenmackers, Oren Etzioni, and Daniel S. WeldTuring CenterUniversity of WashingtonComputer Science and EngineeringBox 352350Seattle, WA 98195, USAstef,etzioni,weld@cs.washington.eduAbstractMost Web-based Q/A systems work by find-ing pages that contain an explicit answer toa question.
These systems are helpless if theanswer has to be inferred from multiple sen-tences, possibly on different pages.
To solvethis problem, we introduce the HOLMES sys-tem, which utilizes textual inference (TI) overtuples extracted from text.Whereas previous work on TI (e.g., the lit-erature on textual entailment) has been ap-plied to paragraph-sized texts, HOLMES uti-lizes knowledge-based model construction toscale TI to a corpus of 117 million Web pages.Given only a few minutes, HOLMES doublesrecall for example queries in three disparatedomains (geography, business, and nutrition).Importantly, HOLMES?s runtime is linear inthe size of its input corpus due to a surprisingproperty of many textual relations in the Webcorpus?they are ?approximately?
functionalin a well-defined sense.1 Introduction and MotivationNumerous researchers have identified the Web asa rich source of answers to factual questions, e.g.,(Kwok et al, 2001; Brill et al, 2002), but often thedesired information is not stated explicitly even in atextual corpus as massive as the Web.
Consider thequestion ?What vegetables help prevent osteoporo-sis??
Since there is likely no sentence on the Webdirectly stating ?Kale prevents osteoporosis?, a sys-tem must infer that kale is an answer by combiningfacts from multiple sentences, possibly from differ-ent pages, which justify that conclusion: i.e., thatkale is a vegetable, kale contains calcium, and cal-cium helps prevent osteoporosis.Figure 1: The architecture of HOLMES.Textual Inference (TI) methods have advanced inrecent years.
For example, textual entailment tech-niques aim to determine whether one textual frag-ment (the hypothesis) follows from another (the text)(Dagan et al, 2005).
While most TI researchers havefocused on high-quality inferences from a smallsource text, we seek to utilize sizable chunks of theWeb corpus as our source text.
In order to do this,we must confront two major challenges.
The first isuncertainty: TI is an imperfect process, particularlywhen applied to the Web corpus, hence probabilisticmethods help to assess the confidence in inferences.The second challenge is scalability: how does infer-ence time scale given increasingly large corpora asinput?1.1 HOLMES: A Scalable TI SystemThis paper describes HOLMES, an implemented sys-tem, which addresses both challenges by carryingout scalable, probabilistic inference over groundassertions extracted from the Web.
The input toHOLMES is a conjunctive query, a set of inferencerules expressed as Horn clauses, and large sets ofground assertions extracted from theWeb, WordNet,and other knowledge bases.
As shown in Figure 1,HOLMES chains backward from the query, using theinference rules to construct a forest of proof treesfrom the ground assertions.
This forest is converted79into a Markov network (a form of Knowledge-Based Model Construction (KBMC) (Wellman etal., 1992)) and evaluated using approximate prob-abilistic inference.
HOLMES operates in an anytimefashion ?
if desired it can keep iterating: search-ing for more proofs, and elaborating the Markov net-work.HOLMES makes some important simplifying as-sumptions.
Specifically, we use simple groundtuples to represent extracted assertions (e.g.,contains(kale, calcium)).
Syntactic prob-lems (e.g., anaphora, relative clauses) and seman-tic challenges (e.g., quantification, counterfactuals,temporal qualification) are delegated to the extrac-tion system or simply ignored.
This paper focuseson scalability for this subset of the TI task.1.2 Summary of Experimental ResultsWe tested HOLMES on 183 million distinct groundassertions extracted from the Web by the TEX-TRUNNER system (Banko et al, 2007), coupledwith 159 thousand ground assertions from Word-Net (Miller et al, 1990), and a compact set of hand-coded inference rules.
Given a total of 55 to 145seconds, HOLMES was able to produce high-qualityinferences that doubled the number of answers toexample queries in three disparate domains: geog-raphy, business, and nutrition.We also evaluated how the speed of HOLMESscaled with the size of its input corpus.
In thegeneral case, logical inference over a Horn theory(needed in order to produce the probabilistic net-work) is polynomial in the number of ground asser-tions, and hence in the size of the textual corpus.1Unfortunately, this is prohibitive, since even low-order polynomial growth is fatal on a 117 million-page corpus, let alne the full Web.1.3 Why HOLMES Scales LinearlyFortunately, the Web?s long tail works in our favor.The relations we extract from text are approximatelypseudo-functional (APF), as we formalize in Sec-tion 3, and this property leads to runtime that scaleslinearly with the corpus.
To see the underlying in-tuition, consider the APF relation denoted by thephrase ?is married to;?
most of the time it maps aperson?s name to a small number of spousal names1In fact, it is P-complete ?
as hard as any polynomial-timeproblem.so this relation is APF.
Section 3 shows why thisAPF property ensures linear scaling, and Section 4demonstrates linear scaling in practice.2 An Overview of HOLMESHOLMES is a system designed to answer complexqueries over large, noisy knowledge bases.
As a mo-tivating example, we consider the question ?Whatvegetables help prevent osteoporosis??
As of thiswriting, Google has no pages explicitly stating ?kalehelps prevent osteoporosis?, making it challengingto return ?kale?
as an answer.
However, there arenumerous web pages stating that ?kale is high in cal-cium?
and others declaring that ?calcium helps pre-vent osteoporosis?.
If we could combine those factswe could easily infer that ?kale?
is an answer to thequestion ?What vegetables help prevent osteoporo-sis??
HOLMES was designed to make such infer-ences while accounting for uncertainty in the pro-cess.Given a query, expressed as a conjunctiveDatalog rule, HOLMES generates a probabilisticmodel using knowledge-based model construction(KBMC) (Wellman et al, 1992).
Specifically,HOLMES utilizes fast, logical inference to find thesubset of ground assertions and inference rules thatmay influence the answers to the query ?
enablingthe construction of a small and focused Markov net-work.
Since this graphical model is much smallerthan one incorporating all ground assertions, prob-abilistic inference will be much faster than if naivecompilation were used.Figure 1 summarizes the operation of HOLMES.As with many theorem provers or KBMC systems,HOLMES takes three inputs:1.
A set of knowledge bases ?
databases ofground relational assertions, each with anestimate of its probability, which can begenerated by TextRunner (Banko et al,2007) or Kylin (Wu and Weld, 2007).
Inour example, we would extract the as-sertions IsHighIn(kale, calcium) andPrevents(calcium, osteoporosis) fromthose sentences.2.
A domain theory ?
A set of probabilis-tic inference rules written as Markov logicHorn clauses, which can be used to de-rive new assertions.
The weight associ-ated with each clause specifies its reliability.80kaleis high incalcium(TextRunner : 0.39)kaleis high inmagnesium(TextRunner : 0.39) magnesiumhelps preventosteoporosis(TextRunner : 0.39) calciumhelps preventosteoporosis(TextRunner : 0.68) broccoliis high incalcium(TextRunner : 0.39)kalehelps preventosteoporosis(Inferred : 0.88) broccolihelps preventosteoporosis(Inferred : 0.49)kaleIS-Avegetable(WordNet : 0.9) broccoliIS-Avegetable(WordNet : 0.9)Inf.
Rule:Transitive-Throughhigh in Inf.
Rule:Transitive-Throughhigh in Inf.
Rule:Transitive-Throughhigh inkale matches the query(Inferred : 0.91) broccoli matches the query(Inferred : 0.58)Query Result Query ResultFigure 2: Partial proof ?tree?
(DAG) for the query ?Whatvegetables help prevent osteoporosis??
Rectangles de-pict ground assertions from a knowledge base, roundedboxes are inferred assertions, and shaded squared repre-sent the application of inference rules.
HOLMES convertsthis DAG into a Markov network in order to estimate theprobability of each node.In Section 2.3 we identify several domain-independent rules, but a user may (optionally)specify additional, domain-specific rules if de-sired.
In our example, we assume we are giventhe domain-specific rule: Prevents(X,Z) :-IsHighIn(X,Y) ?
Prevents(Y,Z)3.
A conjunctive query is specified as a Datalogrule.
For example, the question ?What vegeta-bles help prevent osteoporosis??
could be writ-ten as: query(X) :- IS-A(X,Vegetable)?
Prevents(X,osteoporosis)and returns a set of answers to the query, each withan associated probability.2.1 Basic OperationTo find these answers and their associated proba-bilities, HOLMES first finds all ground assertions inthe knowledge bases that are potentially relevant tothe query.
This is efficiently done using the infer-ence rules to chain backwards from the query.
Notethat the generated candidate answers, themselves,are less important than the associated proof trees.Furthermore, since HOLMES uses these ?trees?
(ac-tually, DAGs) to generate a probabilistic graphicalmodel, HOLMES seeks to find as many proof treesas possible for each query result ?
each may influ-ence the final belief in that result.
Figure 2 shows apartial proof tree for our example query.To handle uncertainty, HOLMES now constructs aground Markov network from the proof trees and theMarkov-logic-encoded inference rules.
Markov net-works (Pearl, 1988) model the joint distribution of aset of variables by creating an undirected graph withone node for each random variable, and represent-ing dependencies between variables with cliques inthe graph.
Each clique has a corresponding poten-tial function ?k, which returns a non-negative valuebased on the state of variables in the clique.
Theprobability of a state, x, is given byP (x) =1Z?
?k(x{k})where the partition functionZ is a normalizing term,and x{k} denotes the state of all the variables inclique k.HOLMES converts the proof trees into a Markovnetwork in a manner pioneered by the Markov Logicframework of Richardson and Domingos (2006).
ABoolean variable is created to represent the truth ofeach assertion in the proof forest.
Next, HOLMESadds edges to the Markov network to create a cliquecorresponding to each application of an inferencerule in the proof forest.Following the Markov Logic framework, the po-tential function of a clique has form ?
(x) = ew if allmember nodes are true (w denotes the weight of theinference rule), and ?
(x) = 1 otherwise.
The proba-bilities of leaf nodes are derived from the underlyingknowledge base,2 and inferred nodes are biased withan exponential prior.Finally, HOLMES computes the approximateprobability of each answer by running a variantof loopy belief propagation (Pearl, 1988) over theMarkov network.
In our experience this methodperforms well on networks derived from our Hornclause proof forest, but one could use Monte Carlotechniques or even exact methods if desired.Note that this architecture allows HOLMES tocombine information from multiple web pages to in-fer assertions not explicitly seen in the textual cor-pus.
Because this inference is done using a Markovnetwork, it correctly handles uncertain extractionsand probabilistic dependencies.
By using KBMC tocreate a custom, focused network for each query, the2In our experiments, ground assertions from WordNet geta uniformly high probability of correctness (0.9), but those ex-tracted from the Web are assigned probabilities derived fromredundancy statistics, following the intuition that frequently ex-tracted facts are more likely to be true (Etzioni et al, 2005).81amount of probabilistic inference is reduced to man-ageable proportions.2.2 Anytime, Incremental ExpansionBecause exact probabilistic inference is #P-complete, HOLMES uses approximate methods, buteven these techniques have problems if the Markovnetwork gets too large.
As a result, HOLMES createsthe network incrementally.
After the first proof treesare generated, HOLMES creates the model and per-forms approximate probabilistic inference.
If moretime is available then HOLMES searches for addi-tional proof trees and updates the network (Fig-ure 1).
This incremental process allows HOLMESto return initial results (with preliminary probabilityestimates) as soon as they are discovered.For efficiency, HOLMES exploits standard Data-log optimizations (e.g., it only expands proofs of re-cently added nodes and it uses an approximation tomagic sets (Ullman, 1989), rather than simple back-wards chaining).
For tractability, we also allow theuser to limit the number of transitive inference stepsfor any inference rule.HOLMES also includes a few enhancements fordealing with information extracted from natural lan-guage.
For example, HOLMES?s inference rules sup-port substring/regex matching of ground assertions,to accommodate simple variations in text.
HOLMESalso can be restricted to only operate over propernouns, which is useful for queries involving namedentities.2.3 Markov Logic Inference RulesHOLMES is given the following set of six domain-independent rules, which are similar to the up-ward monotone rules introduced by (MacCartneyand Manning, 2007).1.
Observed relations are likely to be true:R(X,Y) :- ObservedInCorpus(X, R, Y)2.
Synonym substitution preserves meaning:RTR(X?,Y) :- RTR(X,Y) ?
Synonym(X, X?)3.
RTR(X,Y?)
:- RTR(X,Y) ?
Synonym(Y, Y?)4.
Generalizations preserve meaning:RTR(X?,Y) :- RTR(X,Y) ?
IS-A(X, X?)5.
RTR(X,Y?)
:- RTR(X,Y) ?
IS-A(Y, Y?)6.
Transitivity of Part Meronyms:RTR(X,Y?)
:- RTR(X,Y) ?
Part-Of(Y, Y?
)where RTR matches ?
* in?
(e.g., ?born in?
).For example, if Q(X):-BornIn(X,?France?
),and we know from WordNet that Paris is inFrance, then by inference rule 6, we know thatBornIn(X,?Paris?)
will yield valid results forQ(X).
Although all of these rules contain at mosttwo relations in the body, HOLMES allows anarbitrary number of relations in the query and rulebodies.
However, we have found that even simplerules can dramatically improve some queries.We set the rule weights to capture the intuitionthat deeper inferences decrease the likelihood (asthere are more chances to make mistakes), whereasadditional, independent proof trees increase thelikelihood (as there is more supporting evidence).Specifically, in our experiments we set the prior oninferred facts to -0.75, the weight on rule 1 to 1.5,and the weights on all other rules to 0.6.At present, we define these weights manually, butwe expect to learn the parameter values in the future.3 Scaling Inference to the WebIf TI is applied to a corpus containing hundreds ofmillions or even billions of pages, its run time has tobe at most linear in the size of the corpus.
This sec-tion shows that under some reasonable assumptionsinference does scale linearly.We start our analysis with two simplifications.First, we assume that the number of distinct, groundassertions in the KBs, |A|, grows at most linearlywith the size of the textual corpus.
This is cer-tainly true for assertions extracted by TextRunnerand Kylin, and follows from our exclusion of textswith complex quantified sentences.
Our analysisnow proceeds to consider scaling with respect to |A|for a fixed query and set of inference rules.Our second assumption is that the size of everyproof tree is bounded by some constant, m. Thisis a strong assumption and one that depends on theprecise set of inference rules and pattern of groundassertions.
However, it holds in our experience, andif necessary could be enforced by terminating thesearch for proof trees at a certain depth, e.g., log(m).HOLMES?s knowledge-based model constructionhas two parts: construction of the proof forest andconversion of the forest into a Markov network.Since the Markov network is essentially isomorphicto the proof forest, the conversion will be O(|A|) ifthe forest is linear in size, which is ensured if thetime to construct the proof trees isO(|A|).
We show82this in the remainder of this section.Recall that HOLMES requires inference rules tobe function-free Horn clauses.
While this limits ex-pressivity to some degree, it provides a huge speedbenefit ?
logical inference over Horn clauses canbe done in polynomial time, whereas general propo-sitional inference (i.e., from grounded first-orderrules) is NP-complete.Alas, even low-order polynomial blowup is un-acceptable when the textual corpus reaches Webscale; we seek linear growth.
Intuitively, there aretwo places where polynomial expansion could causetrouble.
First, the number of different types of proofs(i.e., first order proofs) could grow too quickly, andsecondly, a given type of proof tree might applyto too many ground assertions (?tuples?
in databaselingo).
We treat these issues in turn.Under our assumptions, each proof tree can berepresented as an expression in relational algebrawith at most m equijoins (Ullman, 1989),3 eachstemming from the application of an inference rule.Since the number of rules is fixed, as is m, there area constant number of possible first-order proof trees.The bigger concern is that any one of these first-order trees might result in a polynomial number ofground trees; if so, the size of the ground forest(and corresponding Markov network) could growtoo quickly.
In fact, polynomial growth is a commonphenomena in database query evaluation.
Luckily,most relations in the Web corpus behave more fa-vorably.
We introduce a property of relations thatensures m-way joins, and therefore all proof treesup to size m, can be computed in O(|A|) time.The intuition is that most relations derived fromlarge corpora have a ?heavy-tailed?
distribution,wherein a few objects appear many times in a rela-tion, but most appear only once or twice, thus joinsinvolving rare objects lead to a small number of re-sults, and so the main limitation on scalability iscommon objects.
We now prove that if these com-mon objects account for a small enough fraction ofthe relation, then joins will still scale linearly.
Wefocus on binary relations, but these results can eas-ily be extended to relations of larger arity.3Note that an inference rule of the form H(X) :-R1(X,Y),R2(Y,Z) is equivalent to the algebraic expressionpiX(R1 ./ R2).
First a join is performed between R1 and R2testing for equality between values of Y ; then a projection elim-inates all columns besides X .Definition 1 A relation, R = {(xi, yi)} ?
X ?Y , is pseudo-functional (PF) in x with degree k, if?x ?
X : |{y|(x, y) ?
R}| ?
k. When the precisevariable and degree is irrelevant to discussion, wesimply say ?R is PF.
?An m-way equijoin over relations that are PF inthe join variables will have at most km ?
|R| results.Since km is constant for a given join and |R| scaleslinearly in the size of the textual corpus, proof treeconstruction over PF relations also scales linearly.However, due to their heavy-tailed distributions,most relations extracted from theWeb fit the pseudo-functional definition in most, but not all values ofX .
Fortunately, it turns out that in most cases these?bad?
values ofX are rare and hence don?t influencethe join size significantly.
We formalize this intu-ition by defining a class of approximately pseudo-functional (APF) relations and proving that joiningtwo APF relations produces at most a linear numberof results.Definition 2 A relation, R, is approximatelypseudo-functional (APF) in x with degree k, if Xcan be partitioned into two sets XG and XB suchthat for all x ?
XG R is PF with degree k and?x?XB|{y|(x, y) ?
R}| ?
k ?
log(|R|)Theorem 1.
If relation R1 is APF in y with de-gree k1 and R2 is APF in y with degree k2 thenthe relation Q = R1 ./ R2 has size at mostO(max(|R1|, |R2|)).Proof.
Since R1 and R2 are APF, we know thatY can be partitioned into four groups: YBB =YB1?YB2, YBG = YB1?YG2, YGB = YG1?YB2,YGG = YG1?YG2.4 We can show that each groupleads to at most O(|A|) entries in Q.
For y ?
YBBthere are at most k1 ?
k2 ?
log(|R1|) ?
log(|R2|) en-tries in Q.
The y ?
YGB and y ?
YBG lead to atmost k1 ?
k2 ?
log(|R2|) and k1 ?
k2 ?
log(|R1|)entries, respectively.
For y ?
YGG there are atmost k1 ?
k2 ?
max(|R1|, |R2|).
Summing the re-sults from the four partitions, we see that |Q| isO(max(|R1|, |R2|)), thus it is O(|A|).This theorem and proof can easily be extended to4YBB are the ?doubly bad?
values of y that violate the PFdefinition for both relations, YGG are the values that do not vio-late the PF definition for either relation, and YBG and YGB arethe values that violate it in only R1 or R2, resp.83an m-way equijoin, as long as each relation is APFin all arguments that are being joined.Theorem 2.
IfQ is the relation obtained by an equi-join over m relationsR1..m, each having size at mostO(|A|), and if all R1..m are APF in all argumentsthat they are joined in with degree at most kmax, andif?1?i?mlog(|Ri|) ?
|A|, then |Q| is O(|A|).The inequality in Theorem 2 relates the sizes ofthe relations (|R|), the join (m) and the number ofground assertions (|A|).
However, in many cases weare interested in much smaller values of m than theinequality enables.
We can relax the APF definitionto allow a broader, but still scalable, class ofm-way-APF relations.Corollary 3.
If Q is the relation obtained by an m-way join, and if each participating relation is APFin their joined variables with a bound of ki ?
m?|Ri|instead of ki ?
log(|Ri|), then the join is O(|A|).The final step in our scaling argument concernsprobabilistic inference, which is #P-Complete if per-formed exactly.
This is addressed in two ways.
First,HOLMES uses approximate methods, e.g., loopy be-lief propagation, which avoids the cost of exact in-ference ?
at the cost of reduced precision.
Sec-ondly, at a practical level, HOLMES?s incrementalconstruction of the graphical model (Figure 1) al-lows it to bound the size of the network by terminat-ing the search for additional proofs.4 Experimental ResultsThis section reports on measurements that confirmthat linear scaling with |A| occurs in practice, andthat HOLMES?s inference is not only scalable butalso improves precision/recall on sample queries ina diverse set of domains.
After describing the exper-imental domains and queries, Section 4.2 reports onthe boost to the area under the precision/recall curvefor a set of example queries in three domains: ge-ography, business, and nutrition.
Section 4.3 thenshows that APF relations are very common in theWeb corpus, and finally Section 4.4 demonstratesempirically that HOLMES?s inference time scaleslinearly with the number of pages in the corpus.4.1 Experimental SetupHOLMES utilized two knowledge bases in these ex-periments: TEXTRUNNER and WordNet.
TEX-TRUNNER contains approximately 183 million dis-tinct ground assertions extracted from over 117 mil-lion web pages, and WordNet contains 159 thousandmanually created IS-A, Part-Of, and Synonym asser-tions.In all queries, HOLMES utilizes the domain-independent inference rules described in Sec-tion 2.3.
HOLMES additionally makes use of twodomain-specific inference rules in the Nutritiondomain, to demonstrate the benefits of includingdomain-specific information.
Estimating the preci-sion and relative recall of HOLMES requires exten-sive and careful manual tagging of HOLMES output.To make this feasible, we restricted ourselves to aset of twenty queries in three domains, but made thedomains diverse to illustrate the broad scope of thesystem.We now describe each domain briefly.Geography: the query issued is: ?Who was born inone of the following countries??
More formally,Q(X) :- BornIn(X,{country}) where {country}is bound to each of the following nine countriesin turn {France, Germany, China, Thailand, Kenya,Morocco, Peru, Columbia, Guatemala}, yielding atotal of nine queries.Because Web text often refers to a person?sbirth city rather than birth country, this query il-lustrates how combining an ground assertion (e.g.,BornIn(Alberto Fujimori, Lima)) with back-ground knowledge (e.g., LocatedIn(Lima, Peru))enables the system to draw new conclusions (e.g.,BornIn(Alberto Fujimori, Peru)).Business: we issued the following two queries.1) Which companies are acquiring software com-panies?
Formally, Q(X) :- Acquired(X, Y)?
Develops(Y, ?software?)
This query testsHOLMES?s ability to scalably join a large number ofassertions from multiple pages.2) Which companies are headquartered in theUSA?
Q(X) :- HeadquarteredIn(X, ?USA?)?
IS-A(X, ?company?
)Answering this query comprehensively requiresHOLMES to combine a join (over the relations Head-quarteredIn and IS-A) with transitive inference onPartOf (e.g., Seattle is PartOf Washington which isPartOf the USA) and on IS-A (e.g., Microsoft IS-Asoftware company which IS-A company).
The IS-A assertions came from both TEXTRUNNER (usingpatterns from (Hearst, 1992)) and WordNet.8400.20.40.60.810 1000 2000 3000 4000 5000Estimated RecallPrecisionBaselineHolmes Increase in AuCFigure 3: PR Curve for BornIn(X, {country}).
Inferenceboosts the Area under the PR Curve (AuC) by 102 %.Domain Increase Total Inferencein AuC TimeGeography +102% 55 sBusiness +2,643% 145 sNutrition +5,595% 64 sTable 1: Improvement in the AuC of HOLMES over theBASELINE and total inference time taken by HOLMES.Results are summed over all queries in the geography,business, and nutrition domains.
Inference time mea-sured on unoptimized prototype.Nutrition: the nine queries issued are instancesof ?What foods prevent disease??
Where a food isa member of one of the classes: fruit, vegetable, orgrain, and a disease is one of: anemia, scurvy, orosteoporosis.
More formally, Q(X, {disease}) :-Prevents(X, {disease}) ?
IS-A(X, {food})Our experiments in the nutrition domain utilizedtwo domain-specific inference rules in addition tothe ones presented in Section 2.3:Prevents(X,Y):-HighIn(X,Z) ?
Prevents(Z,Y)Prevents(X,Y):-Contains(X,Z) ?
Prevents(Z,Y)4.2 Effect of Inference on RecallTo measure the cost and benefit of HOLMES?s in-ference we need to define a baseline for compar-ison.
Answering the conjunctive queries in thebusiness and nutrition domains requires computingjoins, which TEXTRUNNER does not do.
Thus, wedefined a baseline system, BASELINE, which hasaccess to the underlying Knowledge Bases (KBs)(TEXTRUNNER and WordNet), and the ability tocompute joins using information explicitly stated ineither KB, but does not have the ability to infer newassertions.We compared HOLMES with BASELINE in allthree domains.
Figure 3 depicts the combined pre-cision/relative recall curves for the nine Geographyqueries.
HOLMES yields substantially higher re-call (the shaded region) at modestly lower preci-sion, doubling the area under the precision/recallcurve (AuC).
The other precision/recall curves alsoshowed a slight drop in precision for substantialgains in recall.
Table 1 summarizes the results, alongwith the total runtime needed for inference.
Becauserelations in the business domain are much largerthan in the other domains (i.e., 100x ground asser-tions), inference is slower in this domain.We note that inference is particularly helpful withrarely mentioned instances.
However, inference canlead to errors when the proof tree contains joins ongeneric terms (e.g., ?company?)
or common extrac-tion errors (e.g., ?LLC?
as a company name).
Thisis a key area for future work.4.3 Prevalence of APF RelationsTo determine the prevalence of APF relations inWebtext, we examined a sample of 500 binary relationsselected randomly from TEXTRUNNER?s ground as-sertions.
The surface forms of the relations and ar-guments may misrepresent the true properties of theunderlying concepts, so to better estimate the trueproperties we merged synonymous values as givenby Resolver (Yates and Etzioni, 2007) or the mostfrequent sense of the word in WordNet.
For exam-ple, we would consider BornIn(baby, hospital)and BornAt(infant, infirmary) to represent thesame concept, and so would merge them into oneinstance of the ?Born In?
relation.
The largest two re-lations had over 1.25 million unique instances each,and 52% of the relations had more than 10,000 in-stances.For each relation R, we first found all instancesof R extracted by TEXTRUNNER and merged allsynonymous instances as described above.
Then,for each argument of R we computed the smallestvalue, Kmin, such that R is APF with degree Kmin.Since many interesting assertions can be inferred bysimply joining two relations, we also considered thespecial case of 2-way joins using Corollary 3.
Wecomputed the smallest value, K2./, such that the re-lation is two-way-APF with degree K2./.Figure 4 shows the fraction of relations withKmin andK2./ of at mostK as a function of varying850%20%40%60%80%100%0 1000 2000 3000 4000 5000 6000Degree of Approximate Pseudo-FunctionalityAPFAPF for two-way joinFigure 4: Prevalence of APF relations in Web text.
Thex-axis depicts the degree of pseudo-functionality, e.g.,Kmin and K2./, (see definition 2); the y-axis lists thepercent of relations that are APF with that degree.
Re-sults are averaged over both arguments.values of K. The results are averaged over both ar-guments of each binary relation.
For arbitrary joinsin this KB, 80% of the relations are APF with de-gree less than 496; for 2-way joins (like the ones inour inference rules and test queries), 80% of the rela-tions are APF with degree less than 65.
These resultsindicate that the majority of relations TEXTRUNNERextracted from text are APF, and so we can expectHOLMES?s techniques will allow efficient inferenceover most relations.While Theorem 2 guarantees that joins over thoserelations will beO(|R|), that notation hides a poten-tially large constant factor of Kminm.
Fortunatelythe constant factor is significantly smaller in prac-tice.
To see why, we re-examine the proof: the largefactor comes from assuming that all of R?s first ar-guments which meet the PF definition are associatedwith exactly Kmin distinct second arguments.
How-ever, in our corpus 83% of first arguments are as-sociated with only one second argument.
Clearly,our worst-case analysis substantially over-estimatesinference time for most queries.
Moreover, in ad-ditional experiments (omitted due to space limita-tions), measured join sizes grew linearly in the sizeof the corpus, but were on average two to three or-ders of magnitude smaller than the bounds given inthe theory.
This observation held across relationswith different sizes and values of Kmin.While the results in Figure 4 may vary for othersets of relations, we believe the general trendshold.
This is promising for Question Answering andTextual Inference systems, since if true it impliesR2 = 0.9881R2 = 0.9808R2 = 0.99310204060801001201401600% 20% 40% 60% 80% 100%Fraction of CorpusGeographyBusinessNutritionFigure 5: The effects of corpus size on total inferencetime.
We see approximately linear growth in all domains,and display the best fit lines and coefficient of determina-tion (R2) of each.that combining information frommultiple differencesource is feasible, and can allow such systems to in-fer answers not explicitly seen in any source.4.4 Scalability of Inference SpeedSince the previous subsection showed that most re-lations are APF in their arguments, our theory pre-dicts HOLMES?s inference will scale linearly.
Wetested this hypothesis empirically by running infer-ence over the test queries in our three domains, whilevarying the number of pages in the textual corpus.Figure 5 shows how the inference time HOLMESused to answer all queries in each domain scaleswith KB size.
For these queries, and several oth-ers we tested (not shown here), inference time growslinearly with the size of the KB.
Based on these re-sults we believe that HOLMES can provide scalableinference over a wide variety of domains.5 Related WorkTextual Entailment systems are given two textualfragments, text T and hypothesis H , and attempt todecide if the meaning of H can be inferred fromthe meaning of T (Dagan et al, 2005).
Whilemany approaches have addressed this problem, ourwork is most closely related to that of (Raina et al,2005; MacCartney and Manning, 2007; Tatu andMoldovan, 2006; Braz et al, 2005), which convertthe inputs into logical forms and then attempt to?prove?
H from T plus a set of axioms.
For in-stance, (Braz et al, 2005) represents T , H , and aset of rewrite rules in a description logic framework,and determines entailment by solving an integer lin-86ear program derived from that representation.These approaches and related ones (e.g.,(Van Durme and Schubert, 2008)) use highlyexpressive representations, enabling them to ex-press negation, temporal information, and more.HOLMES?s representation is much simpler?Markov Logic Horn Clauses for inference rulescoupled with a massive database of ground asser-tions.
However, this simplification allows HOLMESto tackle a ?text?
of enormously larger size: 117million Web pages versus a single paragraph.
A sec-ond, if smaller, difference stems from the fact thatinstead of determining whether a single hypothesissentence, H , follows from the text, HOLMES tries tofind all consequents that match a conjunctive query.HOLMES is also related to open-domain question-answering systems such as Mulder (Kwok et al,2001), AskMSR (Brill et al, 2002), and others(Harabagiu et al, 2000; Brill et al, 2001).
How-ever, these Q/A systems attempt to find individualdocuments or sentences containing the answer.
Theyoften perform deep analysis on promising texts, andback off to shallower, less reliable methods if thosefail.
In contrast, HOLMES utilizes TI and attemptsto combine information from multiple different sen-tences in a scalable way.While its ability to combine information frommultiple sources is promising, HOLMES has severallimitations these Q/A systems do not have.
SinceHOLMES relies on an information extraction sys-tem to convert sentences into ground predicates,any limitations of the IE system will be propagatedto HOLMES.
Additionally, the logical representa-tion HOLMES uses limits the reasoning and typesof questions it can answer.
HOLMES is geared to-wards answering questions which are naturally ex-pressed as properties and relations of entities, and isnot well suited to answering more abstract or openended questions.
Although we have demonstratedthat HOLMES is scalable, further work is needed tomake it to run at interactive speeds.Finally, research in statistical relational learningsuch as MLNs (Richardson and Domingos, 2006),RMNs (Taskar et al, 2002), and others (Getoorand Taskar, 2007) have studied techniques for com-bining logical and probabilistic inference.
Our in-ference rules are more restrictive than those al-lowed in MLNs, but this trade-off allows us to ef-ficiently scale inference to large, open domain cor-pora.
By constructing only cliques for satisfied in-ference rules, HOLMES explicitly models the intu-ition behind LazySAT inference (Singla and Domin-gos, 2006) as used in MLNs.
I.e., most Horn clauseinference rules will be trivially satisfied since theirantecedents will be false, so we only need to worryabout ones where the antecedent is true.6 ConclusionsThis paper makes three main contributions:1.
We introduce and evaluate the HOLMES sys-tem, which leverages KBMC methods in orderto scale a class of TI methods to the Web.2.
We define the notion of Approximately Pseudo-Functional (APF) relations and prove that, fora APF relations, HOLMES?s inference time in-creases linearly with the size of the input cor-pus.
We show empirically that APF relationsappear to be prevalent in our Web corpus (Fig-ure 4), and that HOLMES?s runtime does scalelinearly with the size of its input (Figure 5), tak-ing only a few CPU minutes when run over 183million distinct ground assertions.3.
We present experiments demonstrating that, fora set of queries in the domains of geography,business, and nutrition, HOLMES substantiallyimproves the quality of answers (measured byAuC) relative to a ?no inference?
baseline.In the future, we plan more extensive tests to char-acterize when HOLMES?s inference is helpful.
Wealso hope to examine in what cases jointly perform-ing extraction and inference (as opposed to perform-ing them separately) is feasible at scale.
Finally, weplan to examine methods for HOLMES to learn bothrule weights and new inference rules.AcknowledgementsWe thank the following for helpful comments onprevious drafts: Fei Wu, Michele Banko, Mausam,Doug Downey, and Alan Ritter.
This research wassupported in part by NSF grants IIS-0535284, IIS-0312988, and IIS-0307906, ONR grants N00014-08-1-0431 and N00014-06-1-0147, CALO grant 03-000225, the WRF / TJ Cable Professorship as wellas gifts from Google.
The work was performed atthe University of Washington?s Turing Center.87ReferencesM.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Procs.
of IJCAI.R.
Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-mons.
2005.
An inference model for semantic en-tailment in natural language.
In Proceedings of theNational Conference on Artificial Intelligence (AAAI),pages 1678?1679.E.
Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.2001.
Data-intensive question answering.
In Procs.of Text REtrieval Conference (TREC-10), pages 393?400.Eric Brill, Susan Dumais, and Michele Banko.
2002.
Ananalysis of the AskMSR question-answering system.In EMNLP ?02: Proceedings of the ACL-02 conferenceon Empirical methods in natural language processing,pages 257?264, Morristown, NJ, USA.
Association forComputational Linguistics.I.
Dagan, O. Glickman, and B. Magnini.
2005.
ThePASCAL Recognising Textual Entailment Challenge.Proceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment, pages 1?8.O.
Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: An experimental study.
Artificial Intelligence,165(1):91?134.L.
Getoor and B. Taskar.
2007.
Introduction to StatisticalRelational Learning.
MIT Press.S.
Harabagiu, M. Pasca, and S. Maiorano.
2000.
Exper-iments with open-domain textual question answering.In Procs.
of the COLING-2000.M.
Hearst.
1992.
Automatic Acquisition of Hyponymsfrom Large Text Corpora.
In Procs.
of the 14th In-ternational Conference on Computational Linguistics,pages 539?545, Nantes, France.C.C.T.
Kwok, O. Etzioni, and D.S.
Weld.
2001.
Scal-ing question answering to the Web.
Proceedings ofthe 10th international conference on World Wide Web,pages 150?161.B.
MacCartney and C.D.
Manning.
2007.
Natural Logicfor Textual Inference.
In Workshop on Textual Entail-ment and Paraphrasing.G.
Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
Miller.
1990.
Introduction to wordnet: An on-linelexical database.
International Journal of Lexicogra-phy, 3(4):235?312.Judea Pearl.
1988.
Probabilistic reasoning in intelli-gent systems: networks of plausible inference.
MorganKaufmann Publishers Inc. San Francisco, CA, USA.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via learning andabductive reasoning.
In Proceedings of AAAI 2005.AAAI Press.M.
Richardson and P. Domingos.
2006.
Markov LogicNetworks.
Machine Learning, 62(1-2):107?136.Parag Singla and Pedro Domingos.
2006.
Memory-efficient inference in relational domains.
In AAAI.B.
Taskar, P. Abbeel, and D. Koller.
2002.
Discrimi-native probabilistic models for relational data.
Eigh-teenth Conference on Uncertainty in Artificial Intelli-gence (UAI02).Marta Tatu and Dan Moldovan.
2006.
A logic-basedsemantic approach to recognizing textual entailment.In Proceedings of the COLING/ACL on Main confer-ence poster sessions, pages 819?826, Morristown, NJ,USA.
Association for Computational Linguistics.J.
Ullman.
1989.
Database and knowledge-base systems.Computer Science Press.B.
Van Durme and L.K.
Schubert.
2008.
Open knowl-edge extraction through compositional language pro-cessing.
In Symposium on Semantics in Systems forText Processing.M.
Wellman, J. Breese, and R. Goldman.
1992.
Fromknowledge bases to decision models.
The KnowledgeEngineering Review, 7(1):35?53.F.
Wu and D. Weld.
2007.
Autonomously semantifyingWikipedia.
In Proceedings of the ACM Sixteenth Con-ference on Information and Knowledge Management(CIKM-07), Lisbon, Porgugal.A.
Yates and O. Etzioni.
2007.
Unsupervised resolutionof objects and relations on the Web.
In Procs.
of HLT.88
