Improved Hidden Markov Modeling forSpeaker-Independent Continuous Speech RecognitionXuedong Huang, Fil Alleva, Satoru HayamizuHsiao-Wuen Hon, Mei-Yuh Hwang, Kai-Fu LeeSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213AbstractThis paper eports recent efforts to further improve the perfor-mance of the Sphinx system for speaker-independent co tin-uous speech recognition.
The recognition error ate is signifi-cantly reduced with incorporation ofadditional dynamic fea-tures, semi-continuous hidden Markov models, and speakerclustering.
For the June 1990 (RM2) evaluation test set, theerror rates of our current system are 4.3% and 19.9% forword-pair grammar and no grammar respectively.IntroductionThis paper reports our recent effort to further improve theaccuracy of the Sphinx System \[10\].
We choose here toadhere to the basic architecture of the Sphinx System, andto use the standard Resource Management task and trainingcorpus.
Possible improvements could be resulted from thefollowing categories:?
Incorporate additional dynamic features.?
Improve HMM probabilistic representation.?
Cluster training speakers to provide multiple models.?
Introduce discfiminant transformations to improve dis-crimination.?
Extend corrective training for semi-continuous models.?
Improve allophonetic models of coarticulation.This paper will report advances on the first five categories.Improved allophonic modeling has been reported in \[13\].Our first improvement involves the incorporation of dy-namic features computed from the LPC cepstrum.
Previousversions of the Sphinx system have used first order differ-enced cepstrum and power.
Here, we experimented withsecond and third order differenced cepstrum and power.
Wealso experimented with incorporation fboth 40 msec and 80msec differenced cepstrum, as well as the difference derivedfrom compressed speech \[3\].
These additional feature setsare incorporated in the multi-codebook framework.
The bestcombination reduced errors by 17% over the baseline results.Our second technique mploys the semi-continuous hid-den Markov model (HMM) \[8\].
Multiple-codebook semi-continuous models are extended to our current Sphinx ver-sion.
Both diagonal and full covariance Gaussian models areinvestigated.
We found that the best variants of both modelsreduced error rate of discrete HMMs by 10-20%.Due to smoothing abilities of the semi-continuous model,we were able to train multiple sets of models for differentspeakers.
We investigated automatic speaker clustering andexplicit male/female clustered models.
In both cases, modelsof all the speaker clusters are simultaneously active, withthe restriction that no between-cluster ransitions are allowed.Thus, the system retains peaker-independent characteristics.By using multiple model sets with the semi-continuous HMM,the error ate is further educed by 10-15%.We experimented with two variants of linear discriminanttransformations.
The first attempted touse a single transfor-marion to separate all triphone states.
The second attemptedto shift the mean vectors of the semi-continuous mixtures, soas to separate confusable words.
However, neither methodproduced any improvement.Finally, we investigated corrective training for semi-continuous models.
At the time of this writing, we haveonly applied our discrete corrective training algorithm \[15\] tosemi-continuous models.
We found that this method is effec-tive if top- 1 (or discrete HMM) decoding isused.
However, ifthe recognition algorithm considers top N codewords, whilethe corrective training uses only the top 1 codeword, the re-suits degrade considerably.
Thus, corrective training is notused in this evaluation.In the next five sections, we describe these techniques.We will measure improvements based on our baseline systemas reported in \[11\], and evaluated on the 600 sentences thatcomprise the February and October 1989 test sets.
Next, asummary of all the improvements will be provided for thetuning (February and October 1989) sets, as well as the newRM2 test set (480 sentences from 4 speakers).
The last sectioncontains our conclusion and outlines our future work.Dynamic FeaturesTemporal changes in the spectra re believed to play an im-portant role in human perception.
One way to capture thisinformation is to use delta coefficients or differenced coeffi-cients \[4, 14\] that measure the change of coefficients over time.Temporal information is particularly suitable for HMMs, sinceHMMs assume ach frame is independent of the past, andthese dynamic features broaden the scope of a frame.327In the past, the Sphinx system has utilized three codebookscontaining:?
12 LPC cepstrum coefficients.?
12 differenced LPC cepstrum coefficients (40 msec.
dif-ference)?
Power and differenced power (40 msec.
).We experimented with a number of new measures of spec-tral dynamics, including:?
Second order differencing (cepstrum and power).?
Third order differencing (cepstrum and power).?
Multiple window differencing (40 msec.
and 80 msec).?
Differencing from temporally compressed speech.The first set of coefficients i incorporated in a new code-book, whose parameters are second order differences of thecepstrum coefficient.
The second order difference for framen is the difference between +l and n-1 first order differen-tial coefficients.
We incorporated this as a fourth codebook,and evaluated the new system using the word pair grammar(perplexity 60) on the February and October 1989 test sets(600 sentences).
We found that second order differencing re-duced errors from 6.9% to 6.2%.
Second order differencingof power (used as another parameter in the power codebook)further educed errors to 6.0%.We attempted to extend this idea to third-order differenc-ing, taking the difference of adjacent second-order differen-tial coefficients.
But we found that performance deterioratedslightly.
We conclude that there is little information beyondsecond-order differences.Next, we incorporated both 40 msec.
and 80 msec.
dif-ferences, which represent short-term and long-term spectraldynamics, respectively.
We hoped that these two sources ofinformation are more complementary than redundant.
We firstincorporated the two as separate codebooks (making a totalof five codebooks), which reduced errors from 6.0% to 5.9%.We then incorporated the two into one codebook, weightedby their variances.
This further educed errors to 5.7%.
Webelieve the latter approach gave better performance becausethe correlation between the 40 msec.
and 80 msec.
differ-ences violated the codebook independence assumption of themulti-codebook approach.Finally, we tried to incorporate a variable measure of spec-tral dynamics.
Instead of taking static differences, we takedifferences that depend on "acoustic segments."
We definedacoustic segments by using the variable frame rate method\[16\].
Speech is segmented according to theEuclidean distanceof the cepstral coefficients.
A segment boundary is placed be-tween frames whose distance xceeds a pre-set threshold.
Thethreshold is chosen so that the ratio of frames to segments iabout 2.5 to 1.
Each segment is then averaged into a singlecepstral (and power) vector.
The differential measure for seg-ment n is computed by subtracting the averaged cepstrum ofsegment n- 1 from that of n+l.
Then, the compressed cepstrumis expanded back to its original frame length, by duplicatingthe compressed frames, so that its length matches that of theother code sequences.
This provides more acoustic ontext forframes that are in stationary regions.
We used this codebookinstead of the second order differences, and found that errorsincreased to over 7%.
One explanation for this phenomenonis that this type of compression-expansion increased frame-to-frame correlation, which makes HMMs less appropriatemodels.Thus, the final configuration i volves four codebooks, eachwith 256 entries, that use:?
12 LPC cepstrum coefficients.?
12 40-msec differenced LPC cepstrum coefficients and12 80-msec differenced LPC cepstrum coefficients.?
12 second-order differenced power.?
Power, 40-msec differenced power, second-order differ-enced power.This configuration reduced an original error rate of 6.9% to5.7%, a 17% error rate reduction.
A summary of dynamicfeature results is give in Table 1.Systems \[ Error RateBaseline I 6.9%Additional dynamic features 5.7%Table 1: Improvements u ing additional dynamic features.Semi-Continuous HMMsSemi-continuous hidden Markov models mutually optimizethe VQ codebook and HMM parameters under a unifiedprobabilistic framework \[7, 8, 6\].
Here, each VQ code-word is regarded as a continuous probability density func-tion.
Intuitively, from the discrete HMM point of view, semi-continuous HMMs integrate quantization accuracy into theHMM, and robustly estimate the discrete output probabilitiesby considering multiple codeword candidates in VQ proce-dure.
From the continuous mixture HMM point of view,semi-continuous HMMs employ a shared mixture of contin-uous output probability densities for each individual HMM.Shared mixtures ubstantially reduce the number of free pa-rameters and computational complexity in comparison withthe continuous mixture HMM, while maintaining reasonablyits modeling power.
For the semi-continuous model, appropri-ate acoustic representation a d probability density functionsis crucial to the recognition accuracy.
With appropriately cho-sen acoustic parameters and probability density functions, thesemi-continuous HMM can greatly enhance the robustness incomparison with the discrete HMM \[8\].We first performed exploratory semi-continuous experi-ments on our three-codebook system.
The semi-continuousHMM was extended to accommodate multiple feature front-end \[8, 6\].
All codebook means and covariance matrices arereestimated together with the HMM parameters except he328power covariance matrices, which are fixed.
In an early ex-periment on the June 88 test set, we found that full covarianceHMMs outperformed diagonal covariance semi-continuousHMMs (with an error reduction of 10% in comparison withthe diagonal semi-continuous models, and 20% error reduc-tion in comparison with the discrete HMM).
However, onthe present uning set, the full covariance semi-continuousHMMs did not give us any improvement.
This is probablybecause the correlation among our acoustic parameters is notvery strong, so that the diagonal covariance assumption isrelatively valid.
When three codebooks are used, the diago-nal semi-continuous model reduced error rate of the discreteHMM by 13%.
Results using three codebooks are shown inTable 2.\[ Models I Error Rate \[I DiscreteHMM \[ 6.9%Semi-continuous HMM 6.0%Table 2: Discrete and semi-continuous results for three code-book systems.Another advantage touse the semi-continuous HMM is thatit requires less training data in comparison with the discreteHMM.
Therefore, given current raining data set, more de-tailed models can be employed to improve the recognitionaccuracy.
One way to increase the number of parameters ito use speaker-clustered models as shown in the followingsection.Speaker ClusteringIn the past, we have experimented with speaker clustering asa means of shaker adaptation \[12\]; however, we found thatclustering fragmented the training data, and actually degradedperformance.
In that experiment, no smoothing across clus-ter was performed.
We now rectify this problem with twodifferent approaches.The first approach uses discrete models, and smoothes themusing deleted interpolation between correct cluster and otherclusters.
We clustered the speakers based on similarity oftheir allophonic HMMs \[5\].
To perform recognition, onerecognition network is generated for each speaker cluster.
Allnetworks are run in parallel, and the best overall scoring path ischosen as the recognized sentence.
Note that his is a speaker-independent approach, as no a priori cluster selection takesplace.
With two and three clusters, this approach reducederrors by about 6%.The second approach smoothes the resulting models bysemi-continuous HMMs.
Because multi-codewords are usedin Forward-Backward training for semi-continuous models,more models can be trained robustly.
Thus, smoothing takesplace only within-cluster, and not between-cluster.
For thisstudy, we simply used male and female as the two clusters.No interpolation between clustered models is used.
The bestoverall scoring path with clustered models is chosen as therecognized sentence.
For three-codebook systems, the errorreduction of clustered semi-continuous HMMs is over 10% incomparison with the semi-continuous HMM, and over 20%in comparison with the clustered iscrete HMM.Finally, we combined the four-codebook front-end with thespeaker-clustered semi-continuous HMMs.
The results areshown in Table 3.
The combined error reduction here is 17%in comparison with the discrete HMM.I Systems I Error Rate \[I DiscreteHMM I 5-7% \]Semi-continuous HMM 4.7%Table 3: Four codebook results: discrete HMMs vs. speaker-clustered, semi-continuous HMMs.Discriminant TransformationsTwo variants of linear discriminant transformation were ex-perimented.
First, the classes to be discriminated are definedas triphone states.
The Viterbi segmented data are used tocompute within- and between-class means and covariancematrices.
Here, 7 continuous frames are treated as one vec-tor for discriminate transformation.
The transformed vectorcorresponding to top three-frame eigenvalues are divided intothree vectors for three-codebook generation.
Several vari-ations of the approach were experimented.
However, theaverage recognition accuracy is not improved.Next, we experimented with a unified linear discrimi-nant transformation to find appropriate features for semi-continuous hidden Markov modeling.
We used word levelsupervision to estimate the confusion covariance matrices.This extends the technique suggested by \[9, 2\] to the semi-continuous HMM.
Both within- and confusion-covariancematrices for each VQ codeword are weighted with the semi-continuous HMM posterior probabilities.
We investigatedboth codeword-dependent a d codeword-independent dis-criminant transformations with different parameters.
Unfor-tunately, the final word accuracy is still about he same as ourbest semi-continuous HMM.Results of the unified discriminat ransformation werepromising.
We think more experiments are needed to fullyunderstand the problem.Corrective TrainingPreviously, we have applied the IBM corrective training algo-rithm \[1\] to continuous speech training \[15\].
This approachbasically involved generation of misrecognitions and near-misses for each training sentence, and then modifying theHMM parameters to discriminate the correct sentence fromthe misrecognitions and near-misses.For discrete models, this method rewards codewords thatcontribute to the correct alignment, and punishes those thatcontributeto misrecognitions and near-misses.
However, witha semi-continuous system, several codewords are accountablefor each frame alignment.
At the time of this writing, we have329only used a simple xtension of our algorithm: for the purposeof corrective training, only the top semi-continuous candidate(rather than top 4 or 6) was used.This technique ssentially uses top-1 correction and top-4decoding.
We found that this technique increased errors sub-stantially, presumably due to the mismatch between the cor-rective and decoding stages.
In a second experiment, both top-1 correction and decoding were applied (although ypotheseswere generated with a top-4 system), significant improve-ments were observed (an error reduction of 10-15%).
How-ever, the improvement was less than that of the 4-codebooksemi-continuous HMM.
Thus, for evaluation purposes, weopted to bypass the corrective training stage.In order to reap maximum benefit from corrective training,we will need to implement a consistent algorithm for semi-continuous corrective training.
We also believe that an N-best algorithm \[17\] for hypothesizing near-misses will helpsignificantly.The results on these speakers are better than the tuning set.The error reduction of our current system is about 40% incomparison with the baseline system.
We believe this can bepartially be attributed tothe better modeling of female speech.Previously, speaker-independent models were trained with 1/3female speech.
With separated male/female models, femaleresults improved substantially.Speaker Word-Pair Grammar No GrammarError Rate Error RateBJW 3.1% 18.6%JLS 4.8% 21.3%JRM 5.8% 24.0%LPN 3.6% 15.7%Average 4.3% 19.9%Table 6: Results with RM2 test set.Summary of ResultsWithout corrective training, our previous best results was6.9% error rate on the 600 sentence tuning set (with cor-rective training, this was reduced to 5.7%).
We will refer tothe 6.9% error rate system as the "baseline" system.
Table 4shows our progress with the techniques described in this pa-per.
This represented a 32% error rate reduction from thebaseline system.
We believe with proper implementation fcorrective training, another 10% or more reduction will bepossible.Systems Error RateBaseline 6.9%+2nd order diff.
cepstrum 6.2%+2nd order diff.
power 6.0%+80ms 1st diff.
order cepstrum 5.7%+Semi-continuous clustered model 4.7%Table 4: Improvements of various techniques using the word-pair grammar.Since our intermediate r sults were only evaluated on theword-pair system, we do not have detailed results for the no-grammar system.
The baseline and final system results areshown in Table 5.
The improvements introduced here led toa 28% error reduction.Finally, we evaluated the above system on the June 90(RM2) test set, which consists of 480 sentences spoken byfour speakers.
The evaluation results are shown in Table 6.Systems Error RateBaseline 27.1%Final system 19.5%Table 5: Improvements u ing no grammar.ConclusionsIn this paper, we have presented several techniques that sub-stantially reduced Sphinx's error rate.
These techniquesinclude: dynamic features, semi-continuous HMMs, andspeaker clustering.
We have also found that discriminanttransformations and dynamic features based on variable frameanalysis did not improve recognition.
We also obtained is-appointing results using a compromised corrective trainingalgorithm.In the future, we expect o further extend some of theseareas.
We will investigate other methods for automatical pa-rameter selection.
We will extend speaker clustering to amuch larger number of clusters (on a larger database).
Cor-rective training could be improved by using N-Best sentencehypotheses, as well as by using a consistent algorithm forsemi-continuous learning.
Finally, we hope to further investi-gate discriminant methods, and learn whether they are limitedto small vocabularies, ordiscover new variations that improveour large-vocabulary system.We believe the improvement of basic speech research isessential for further progress of the spoken language systems.We hope extensions of the above areas of research will furthernarrow the gap of man-machine communication.References\[1\] Bahl, L., Brown, P., De Souza, P., and Mercer, R. A NewAlgorithm for the Estimation of Hidden Markov ModelParameters.
in: IEEE International Conference onAcoustics, Speech, and Signal Processing.
1988.\[2\] Doddington, G. Phonetically Sensitive Discriminantsfor Improved Speech Recognition.
in: IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing.
1989.\[3\] Furui, S. On the Use of Hierarchical Spectral Dynamicsin Speech Recognition.
in."
IEEE International Con-330ference on Acoustics, Speech, and Signal Processing.1990, pp.
789-792.\[4\] Furui, S. Speaker-Independent Isolated Word Recogni-tion Using Dynamic Features of Speech Spectrum.
IEEETransactions on Acoustics, Speech, and Signal Pro-cessing, vol.
ASSP-34 (1986), pp.
52-59.\[5\] Hayamizu, S., Lee, K., and Hon, H. Description ofAcoustic Variations by Hidden Markov Models with TreeStructure.
in: International Conference on SpokenLanguage Processing.
1990.\[6\] Huang, X., Ariki, ?., and Jack, M. Hidden MarkovModels for Speech Recognition.
Edinburgh UniversityPress, Edinburgh, U.K., 1990.\[7\] Huang, X. and Jack, M. Semi-Continuous HiddenMarkov Models for Speech Signals.
Computer Speechand Language, vol.
3 (1989), pp.
239-252.\[8\] Huang, X., Lee, K., and Hon, H. On Semi-ContinuousHidden Markov Modeling.
in: IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing.
1990.\[9\] Hunt, M. A Comparison of Several Acoustic Represen-tations for Speech Recognition with Degraded and Un-degraded Speech.
in: IEEE International Conferenceon Acoustics, Speech, and Signal Processing.
1989.\[10\] Lee, K. Automatic Speech Recognition: The Devel-opment of the SPHINX System.
Kluwer AcademicPublishers, Boston, 1989.\[11\] Lee, K. Hidden Markov Models : Past, Present, andFuture.
in: Proceedings ofEurospeech.
1989.\[12\] Lee, K. Large-Vocabulary Spealrer-lndependent Co tin-uous Speech Recognition: The SPHINX System.
Com-puter Science Department, Carnegie Mellon University,April 1988.\[13\] Lee, K., Hayamizu, S., Hon, H., Huang, C., Swartz,J., and Weide, R. Allophone Clustering for ContinuousSpeech Recognition.
in: IEEE International Confer-ence on Acoustics, Speech, and Signal Processing.1990.\[14\] Lee, K., H.W., H., and Reddy, R. An Overview of theSPHINX Speech Recognition System.
IEEE Transac-tions on Acoustics, Speech, and Signal Processing,January 1990.\[15\] Lee, K. and Mahajan, S. Corrective and ReinforcementLearning for Speaker-Independent Continuous SpeechRecognition.
Computer Speech and Language, vol.
(1990), p..\[16\] Russell, M., Ponting, K., Peeling, S., Browning, S.,Bridle, J., Moore, R., Galiano, I., and Howell, P. TheARM Continuous Speech Recognition System.
in: IEEEInternational Conference on Acoustics, Speech, andSignal Processing.
1990, pp.
69-72.\[17\] Schwartz, R. and Chow, Y.
The Optimal N-Best Al-gorithm: An Efficient Procedure for Finding MultipleSentence Hypotheses.
in: IEEE International Con-ference on Acoustics, Speech, and Signal Processing.1990.331
