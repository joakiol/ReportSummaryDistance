An Information-Theory-Based Feature Type Analysis for theModelling of Statistical ParsingSUI Zhifang ?
?, ZHAO Jun ?, Dekai WU ??
Hong Kong University of Science & TechnologyDepartment of Computer ScienceHuman Language Technology CenterClear Water Bay, Hong Kong?
Peking UniversityDepartment of Computer Science & TechnologyInstitute of Computational LinguisticsBeijing, Chinasuizf@icl.pku.edu.cn, zhaojun@cs.ust.hk, dekai@cs.ust.hkAbstractThe paper proposes an information-theory-based method for feature types analysis inprobabilistic evaluation modelling forstatistical parsing.
The basic idea is that weuse entropy and conditional entropy tomeasure whether a feature type grasps someof the information for syntactic structureprediction.
Our experiment quantitativelyanalyzes several feature types?
power forsyntactic structure prediction and draws aseries of interesting conclusions.1  IntroductionIn the field of statistical parsing, variousprobabilistic evaluation models have beenproposed where different models use differentfeature types [Black, 1992] [Briscoe, 1993][Brown, 1991] [Charniak, 1997] [Collins, 1996][Collins, 1997] [Magerman, 1991] [Magerman,1992] [Magerman, 1995] [Eisner, 1996].
How toevaluate the different feature types?
effects forsyntactic parsing?
The paper proposes aninformation-theory-based feature types analysismodel, which uses the measures of predictiveinformation quantity, predictive informationgain, predictive information redundancy andpredictive information summation toquantitatively analyse the different contextualfeature types?
or feature types combination?spredictive power for syntactic structure.In the following, Section 2 describes theprobabilistic evaluation model for syntactic trees;Section 3 proposes an information-theory-basedfeature type analysis model; Section 4introduces several experimental issues; Section 5quantitatively analyses the different contextualfeature types or feature types combination in theview of information theory and draws a series ofconclusion on their predictive powers forsyntactic structures.2  The probabilistic evaluation modelfor statistical syntactic parsingGiven a sentence, the task of statistical syntacticparsing is to assign a probability to eachcandidate parsing tree that conforms to thegrammar and select the one with highestprobability as the final analysis result.
That is:)|(maxarg STPTTbest =  (1)where S denotes the given sentence, T denotesthe set of all the candidate parsing trees thatconform to the grammar, P(T|S) denotes theprobability of parsing tree T for the givensentence S.The task of probabilistic evaluation model insyntactic parsing is the estimation of P(T|S).
Inthe syntactic parsing model which uses rule-based grammar, the probability of a parsing treecan be defined as the probability of thederivation which generates the current parsingtree for the given sentence.
That is,?
?==?===niiiniiinShrPSrrrrPSrrrPSTP1112121),|(),,,,|()|,,,()|((2)Where, 121 ,,, ?irrr   denotes a derivation rulesequence, hi denotes the partial parsing treederived from 121 ,,, ?irrr  .In order to accurately estimate the parameters,we need to select some feature typesmFFF ,,, 21  , depending on which we candivide the contextual condition Shi ,  forpredicting rule ri into some equivalence classes,that is, ],[, ,,, 21 ShSh iFFFi m????
??
 , so that?
?==?niiiniii ShrPShrP11]),[|(),|(  (3)According to the equation of (2) and (3), wehave the following equation:?=?niii ShrPSTP1]),[|()|(  (4)In this way, we can get a unite expression ofprobabilistic evaluation model for statisticalsyntactic parsing.
The difference among thedifferent parsing models lies mainly in that theyuse different feature types or feature typecombination to divide the contextual conditioninto equivalent classes.
Our ultimate aim is todetermine which combination of feature types isoptimal for the probabilistic evaluation model ofstatistical syntactic parsing.
Unfortunately, thestate of knowledge in this regard is very limited.Many probabilistic evaluation models have beenpublished inspired by one or more of thesefeature types [Black, 1992] [Briscoe, 1993][Charniak, 1997] [Collins, 1996] [Collins, 1997][Magerman, 1995] [Eisner, 1996], butdiscrepancies between training sets, algorithms,and hardware environments make it difficult, ifnot impossible, to compare the modelsobjectively.
In the paper, we propose aninformation-theory-based feature type analysismodel by which we can quantitatively analysethe predictive power of different feature types orfeature type combinations for syntactic structurein a systematic way.
The conclusion is expectedto provide reliable reference for feature typeselection in the probabilistic evaluationmodelling for statistical syntactic parsing.3 The information-theory-basedfeature type analysis model for statisticalsyntactic parsingIn the prediction of stochastic events, entropyand conditional entropy can be used to evaluatethe predictive power of different feature types.To predict a stochastic event, if the entropy ofthe event is much larger than its conditionalentropy on condition that a feature type isknown, it indicates that the feature type graspssome of the important information for thepredicted event.According to the above idea, we build theinformation-theory-based feature type analysismodel, which is composed of four concepts:predictive information quantity, predictiveinformation gain, predictive informationredundancy and predictive informationsummation.z Predictive Information Quantity (PIQ));( RFPIQ , the predictive information quantityof feature type F to predict derivation rule R, isdefined as the difference between the entropy ofR and the conditional entropy of R on conditionthat the feature type F is known.???
?=?=RrFf rPfPrfPrfPFRHRHRFPIQ,)()(),(log),()|()();((5)Predictive information quantity can be used tomeasure the predictive power of a feature type infeature type analysis.z Predictive Information Gain (PIG)For the prediction of rule R,PIG(Fx;R|F1,F2,...,Fi), the predictive informationgain of taking Fx as a variant model on top of abaseline model employing F1,F2,...,Fi as featuretype combination, is defined as the differencebetween the conditional entropy of predicting Rbased on feature type combination F1,F2,...,Fiand the conditional entropy of predicting Rbased on feature type combination F1,F2,...,Fi,Fx.)6(),,,(),,(),,,(),,,,(log),,,,(),,,|(),,|(),,|;(1111111111rffPffPfffPrfffPrfffPFFFRHFFRHFFRFPIGiixixiRrFfFfFfxixiiixxxii?=?=????
?If ),,,|;(),,,|;( 2121 iyix FFFRFPIGFFFRFPIG  > ,then Fx is deemed to be more informative thanFy for predicting R on top of F1,F2,...,Fi, nomatter whether PIQ(Fx;R) is larger thanPIQ(Fy;R) or not.z Predictive Information Redundancy(PIR)Based on the above two definitions, we canfurther draw the definition of predictiveinformation redundancy as follows.PIR(Fx,{F1,F2,...,Fi};R) denotes the redundantinformation between feature type Fx and featuretype combination {F1,F2,...,Fi} in predicting R,which is defined as the difference betweenPIQ(Fx;R) and PIG(Fx;R|F1,F2,...,Fi).
That is,),,,|;();()};,,,{,(2121ixxixFFFRFPIGRFPIQRFFFFPIR?=(7)Predictive information redundancy can beused as a measure of the redundancy betweenthe predictive information of a feature type andthat of a feature type combination.z Predictive Information Summation (PIS)PIS(F1,F2,...,Fm;R), the predictive informationsummation of feature type combinationF1,F2,...,Fm, is defined as the total informationthat F1,F2,...,Fm can provide for the prediction ofa derivation rule.
Exactly,?=?+=miiimFFRFPIGRFPIQRFFFPIS211121),,|;();();,,,((8)4 Experimental Issues4.1 The classification of the featuretypesThe predicted event of our experiment is thederivation rule to extend the current non-terminal node.
The feature types for predictioncan be classified into two classes, history featuretypes and objective feature types.
In thefollowing, we will take the parsing tree shown inFigure-1 as the example to explain theclassification of the feature types.In Figure-1, the current predicted event is thederivation rule to extend the framed non-terminal node VP, the part connected by thesolid line belongs to history feature types, whichis the already derived partial parsing tree,representing the structural environment of thecurrent non-terminal node.
The part framed bythe larger rectangle belongs to the objectivefeature types, which is the word sequencecontaining the leaf nodes of the partial parsingtree rooted by the current node, representing thefinal objectives to be derived from the currentnode.4.2 The corpus used in the experimentThe experimental corpus is derived from PennTreeBank[Marcus,1993].
We semi-automatically assign a headword and a POS tagto each non-terminal node.
80% of the corpus(979,767 words) is taken as the training set, usedfor estimating the various co-occurrenceprobabilities, 10% of the corpus (133,814 words)is taken as the testing set, used to calculatepredictive information quantity, predictiveinformation gain, predictive informationredundancy and predictive informationsummation.
The other 10% of the corpus(133,814 words) is taken as the held-out set.
Thegrammar rule set is composed of 8,126 CFGrules extracted from Penn TreeBank.SV PV PN N PPierreN N PVinkenM DwillV BjoinD TtheN NboardINasD TaJJnonexecut iveN NdirectorN N PNov.C D29..N P N P N PP PN PFigure-1: The classification of feature types4.3 The smoothing method used in theexperimentIn the information-theory-based feature typeanalysis model, we need to estimate jointprobability ),,,,( 21 rfffP i .
Let F1,F2,...,Fi bethe feature type series selected till now,RrFfFfFf ii ????
,,,, 2211  , we use ablended probability ),,,,(~ 21 rfffP i  toapproximate probability ),,,,( 21 rfffP i  inorder to solve the sparse data problem[Bell,1992].?=?
?++=ijjjirfffPwrPwrPwrfffP121001121),,,,()()(),,,,(~(9)In the above formula,??
?=RrrcrP?1 )?(1)((10)?
?=RrrcrcrP?0 )?
()()(                (11)where )(rc is the total number of time that r hasbeen seen in the corpus.According to the escape mechanism in [Bell,1992], we define the weights wk )1( ik ?<?
inthe formula (9) as follows.iiiksskkewikeew?=???
?= ?+=11,)1(1       (12)where ek denotes the escape probability ofcontext ),,,( 21 kfff   , that is, the probabilityin which (f1 , f2 , ... , fk , r) is unseen in the corpus.In such case, the blending model has to escapeto the lower contexts to approximate),,,,( 21 rfffP k .
Exactly, escape probability isdefined as????????=?
?= ???
?1,00,)?,,...,,()?,,...,,(?21?21kikrfffcrfffdeRrkRrkk    (13)where??
?=>=0)?,,...,,(,00)?,,...,,(,1)?,,...,,(212121rfffcifrfffcifrfffdkkk  (14)In the above blending model, a specialprobability ??
?=RrrcrP?1 )?
(1)(  is used, where allderivation rules are given an equal probability.As a result, 0),,,,(~ 21 >rfffP i  as long as0)?(?>?
?Rrrc .5 The information-theory-basedfeature type analysisThe experiments led to a number of interestingconclusions on the predictive power of variousfeature types and feature type combinations,which is expected to provide reliable referencefor the modelling of probabilistic parsing.5.1 The analysis to the predictiveinformation quantities of lexical featuretypes, part-of-speech feature types andconstituent label feature typesz GoalOne of the most important variation in statisticalparsing over the last few years is that statisticallexical information is incorporated into theprobabilistic evaluation model.
Some statisticalparsing systems show that the performance isimproved after the lexical information is added.Our research aims at a quantitative analysis ofthe differences among the predictive informationquantities provided by the lexical feature types,part-of-speech feature types and constituentlabel feature types from the view of informationtheory.z DataThe experiment is conducted on the historyfeature types of the nodes whose structuraldistance to the current node is within 2.In Table-1, ?Y?
in PIQ(X of Y; R) representsthe node, ?X?
represents the constitute label, theheadword or POS of the headword of the node.In the following, the units of PIQ are bits.z ConclusionAmong the feature types in the same structuralposition of the parsing tree, the predictiveinformation quantity of lexical feature type islarger than that of part-of-speech feature type,and the predictive information quantity of part-of-speech feature type is larger than that of theconstituent label feature type.Table-1: The predictive information quantity of the history feature type candidatesPIQ(X of Y; R) X= constituent label X= headword X= POS ofthe headwordY= the current node 2.3609 3.7333 2.7708Y= the parent 1.1598 2.3253 1.1784Y= the grandpa 0.6483 1.6808 0.6612Y= the first right brother of the current node 0.4730 1.1525 0.7502Y= the first left brother of the current node 0.5832 2.1511 1.2186Y= the second right brother of the current node 0.1066 0.5044 0.2525Y= the second left brother of the current node 0.0949 0.6171 0.2697Y= the first right brother of the parent 0.1068 0.3717 0.2133Y= the first left brother of the parent 0.2505 1.5603 0.61455.2 The analysis to the influence of thestructural relation and the structuraldistance to the predictive informationquantities of the history feature typesz Goal:In this experiment, we wish to find out theinfluence of the structural relation and structuraldistance between the current node and the nodethat the given feature type related to has to thepredictive information quantities of these featuretypes.z Data:In Table-2, SR represents the structural relationbetween the current node and the node that thegiven feature type related to.
SD represents thestructural distance between the current node andthe node that the given feature type related to.Table-2: The predictive information quantity of the selected history feature typesPIQ(constituent labelof Y; R)SR= parent relation SR= brother relation SR= mixed parent andbrother relation0.5832(Y= the first left brother)SD=1 1.1598(Y= the parent)0.4730(Y= the first right brother)0.2505(Y= the first left brotherof the parent)0.0949(Y= the second left brother)SD=2 0.6483(Y= the grandpa)0.1066(Y= the second right brother)0.1068(Y= the first rightbrother of the parent)z ConclusionAmong the history feature types which have thesame structural relation with the current node(the relations are both parent-child relation, orboth brother relation, etc), the one which hascloser structural distance to the current node willprovide larger predictive information quantity;Among the history feature types which have thesame structural distance to the current node, theone which has parent relation with the currentnode will provide larger predictive informationquantity than the one that has brother relation ormixed parent and brother relation to the currentnode (such as the parent's brother node).5.3 The analysis to the predictiveinformation quantities of the historyfeature types and the objective featuretypesz GoalMany of the existing probabilistic evaluationmodels prefer to use history feature types otherthan objective feature types.
We select some ofhistory feature types and objective feature types,and quantitatively compare their predictiveinformation quantities.z DataThe history feature type we use here is theheadword of the parent, which has the largestpredictive information quantity among all thehistory feature types.
The objective feature typesare selected stochastically, which are the firstword and the second word in the objective wordsequence of the current node (Please see 4.1 andFigure-1 for detailed descriptions on the selectedfeature types).Table-3: The predictive information quantity of the selected history and objective feature typesClass Feature type PIQ(Y;R)History feature type Y= headword of the parent 2.3253Y= the first word in the objective word sequence 3.2398Objective feature typeY= the second word in the objective word sequence 3.0071z ConclusionEither of the predictive information quantity ofthe first word and the second word in theobjective word sequence is larger than that ofthe headword of the parent node which has thelargest predictive information quantity among allof the history feature type candidates.
That is tosay, objective feature types may have largerpredictive power than that of the history featuretype.5.4 The analysis to the predictiveinformation quantities of the objectivefeatures types selected respectively on thephysical position information, theheuristic information of headword andmodifier, and the exact headwordinformationz GoalNot alike the structural history feature types, theobjective feature types are sequential.
Generally,the candidates of the objective feature types areselected according to the physical position.However, from the linguistic viewpoint, thephysical position information can hardly graspthe relations between the linguistic structures.Therefore, besides the physical positioninformation, our research try to select theobjective feature types respectively according tothe exact headword information and the heuristicinformation of headword and modifier.
Throughthe experiment, we hope to find out whatinfluence the exact headword information, theheuristic information of headword and modifier,and the physical position information haverespectively to the predictive informationquantities of the feature types.z Data:Table-4 gives the evidence for the claim.Table-4: the predictive information quantity of the selected objective feature typesthe information used to select the objectivefeature typesPIQ(Y;R)the physical position information 3.2398(Y= the first word in the objective word sequence)Heuristic information 1: determine whether aword has the possibility to act as the headword ofthe current constitute according to its POS3.1401(Y= the first word in the objective word sequencewhich has the possibility to act as the headword ofthe current constitute)Heuristic information 2: determine whether aword has the possibility to act as the modifier ofthe current constitute according to its POS3.1374(Y= the first word in the objective word sequencewhich has the possibility to act as the modifier of thecurrent constitute)Heuristic information 3: given the currentheadword, determine whether a word has thepossibility to modify the headword2.8757(Y= the first word in the objective word sequencewhich has the possibility to modify the headword)the exact headword information 3.7333(Y= the headword of the current constitute)z ConclusionThe predictive information quantity of theheadword of the current node is larger than thatof a feature type selected according to theselected heuristic information of headword ormodifier, and larger than that of a feature typeselected according to the physical positions; Thepredictive information quantity of a feature typeselected according to the physical positions islarger than that of a feature types selectedaccording to the selected heuristic informationof headword or modifier.5.5 The selection of the feature typecombination which has the optimalpredictive information summationz Goal:We aim at proposing a method to select thefeature types combination that has the optimalpredictive information summation for prediction.z ApproachWe use the following greedy algorithm to selectthe optimal feature type combination.In building a model, the first feature type tobe selected is the feature type which has thelargest predictive information quantity for theprediction of the derivation rule among all of thefeature type candidates, that is,);(maxarg1 RFPIQF iFi ?
?=    (15)Where ?
is the set of candidate feature types.Given that the model has selected feature typecombination jFFF ,,, 21  , the next featuretype to be added into the model is the featuretype which has the largest predictive informationgain in all of the feature type candidate exceptjFFF ,,, 21  , on condition that jFFF ,,, 21 is known.
That is,)16(),,,|;( 21},,2,1{1 maxarg jijFFFiFiFj FFFRFPIGF ??
?+ =z Data:Among the feature types mentioned above, theoptimal feature type combination (i.e.
the featuretype combination with the largest predictiveinformation summation) which is composed of 6feature types is, the headword of the currentnode (type1), the headword of the parent node(type2), the headword of the grandpa node(type3), the first word in the objective wordsequence(type4), the first word in the objectiveword sequence which have the possibility to actas the headword of the current constitute(type5),the headword of the right brother node(type6).The cumulative predictive informationsummation is showed in Figure-201234567type1 type2 type3 type4 type5 type6feature typecummulativepredictinginformationsummationFigure-2: The cumulative predictive information summation of the feature type combinations6 ConclusionThe paper proposes an information-theory-basedfeature type analysis method, which not onlypresents a series of heuristic conclusion on thepredictive power of the different feature typesand feature type combination for syntacticparsing, but also provides a guide for themodeling of syntactic parsing in the view ofmethodology, that is, we can quantitativelyanalyse the different contextual feature types orfeature types combination's effect for syntacticstructure prediction in advance.
Based on theseanalysis, we can select the feature type or featuretypes combination that has the optimalpredictive information summation to build theprobabilistic parsing model.However, there are still some questions to beanswered in this paper.
For example, what is thebeneficial improvement in the performance afterusing this method in a real parser?
Whether theimprovements in PIQ will lead to theimprovement of parsing accuracy or not?
In thefollowing research, we will incorporate theseconclusions into a real parser to see whether theparsing accuracy can be improved or not.Another work we will do is to do someexperimental analysis to find the impact of datasparseness on feature type analysis, which iscritical to the performance of real systems.The proposed feature type analysis methodcan be used in not only the probabilisticmodelling for statistical syntactic parsing, butalso language modelling in more general fields[WU, 1999a] [WU, 1999b].ReferencesBell, T.C., Cleary, J.G., Witten,I.H.
1992.
TextCompression, PRENTICE HALL, EnglewoodCliffs, New Jersey 07632, 1992Black, E., Jelinek, F.,Lafferty, J.,Magerman, D.M.,Mercer, R. and Roukos, S. 1992.
Towardshistory-based grammars: using richer models ofcontext in probabilistic parsing.
In Proceedings ofthe February 1992 DARPA Speech and NaturalLanguage Workshop, Arden House, NY.Brown, P., Jelinek, F., & Mercer, R. 1991.
Basicmethod of probabilistic context-free grammars.IBM internal Report, Yorktown Heights, NY.T.Briscoe and J. Carroll.
1993.
Generalized LRparsing of natural language (corpora) withunification-based grammars.
ComputationalLinguistics, 19(1): 25-60Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statics.
InProceedings of the Fourteenth National Conferenceon Artificial Intelligence, AAAI Press/MIT Press,Menlo Park.Stanley F. Chen and Joshua Goodman.
1999.
AnEmpirical Study of Smoothing Techniques forLanguage Modeling.
Computer Speech andLanguage, Vol.13, 1999Michael John Collins.
1996.
A new statisticalparser based on bigram lexical dependencies.
InProceedings of the 34th Annual Meeting of theACL.Michael John Collins.
1997.
Three generativelexicalised models for statistical parsing.
InProceedings of the 35th Annual Meeting of theACL.J.Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
InProceedings of COLING-96, pages 340-345Joshua Goodman.
1998.
Parsing Inside-Out.
PhD.Thesis, Harvard University, 1998Magerman, D.M.
and Marcus, M.P.
1991.
Pearl: aprobabilistic chart parser.
In Proceedings of theEuropean ACL Conference, Berlin, Germany.Magerman, D.M.
and Weir, C. 1992.
Probabilisticprediction and Picky chart parsing.
In Proceedingsof the February 1992 DARPA Speech and NaturalLanguage Workshop, Arden House, NY.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33thAnnual Meeting of the ACL.Mitchell P. Marcus, Beatrice Santorini & Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn treebank.Computational Linguistics 19, pages 313-330C.
E. Shannon.
1951.
Prediction and Entropy ofPrinted English.
Bell System Technical Journal,1951Dekai,Wu, Sui Zhifang, Zhao Jun.
1999a.
AnInformation-Based Method for Selecting FeatureTypes for Word Prediction.
Proceedings ofEurospeech'99, Budapest HungaryDekai, Wu, Zhao Jun, Sui Zhifang.
1999b.
AnInformation-Theoretic Empirical Analysis ofDependency-Based Feature Types for WordPrediction Models.
Proceedings of EMNLP'99,University of Maryland, USA
