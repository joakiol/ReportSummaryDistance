Language Model Based Arabic Word SegmentationYoung-Suk Lee     Kishore Papineni      Salim RoukosIBM T. J. Watson Research CenterYorktown Heights, NY 10598Ossama Emam    Hany HassanIBM Cairo Technology Development CenterP.O.Box 166, El-Ahram, Giza, EgyptAbstractWe approximate Arabic?s richmorphology by a model that a wordconsists of a sequence of morphemes inthe pattern prefix*-stem-suffix* (*denotes zero or more occurrences of amorpheme).
Our method is seeded by asmall manually segmented Arabic corpusand uses it to bootstrap an unsupervisedalgorithm to build the Arabic wordsegmenter from a large unsegmentedArabic corpus.
The algorithm uses atrigram language model to determine themost probable morpheme sequence for agiven input.
The language model isinitially estimated from a small manuallysegmented corpus of about 110,000words.
To improve the segmentationaccuracy, we use an unsupervisedalgorithm for automatically acquiringnew stems from a 155 million wordunsegmented corpus, and re-estimate themodel parameters with the expandedvocabulary and training corpus.
Theresulting Arabic word segmentationsystem achieves around 97% exact matchaccuracy on a test corpus containing28,449 word tokens.
We believe this is astate-of-the-art performance and thealgorithm can be used for many highlyinflected languages provided that one cancreate a small manually segmentedcorpus of the language of interest.1   IntroductionMorphologically rich languages likeArabic present significant challenges to manynatural language processing applicationsbecause a word often conveys complexmeanings decomposable into severalmorphemes (i.e.
prefix, stem, suffix).
Bysegmenting words into morphemes, we canimprove the performance of natural languagesystems including machine translation (Brownet al 1993) and information retrieval (Franz,M.
and McCarley, S. 2002).
In this paper, wepresent a general word segmentation algorithmfor handling inflectional morphology capableof segmenting a word into a prefix*-stem-suffix* sequence, using a small manuallysegmented corpus and a table ofprefixes/suffixes of the language.
We do notaddress Arabic infix morphology where manystems correspond to the same root with variousinfix variations; we treat all the stems of acommon root as separate atomic units.
The useof a stem as a morpheme (unit of meaning) isbetter suited than the use of a root for theapplications we are considering in informationretrieval and machine translation (e.g.
differentstems of the same root translate into differentEnglish words.)
Examples of Arabic words andtheir segmentation into prefix*-stem-suffix* aregiven in Table 1, where '#' indicates amorpheme being a prefix, and '+' a suffix.1 As1 Arabic is presented in both native and Buckwaltertransliterated Arabic whenever possible.
All nativeArabic is to be read from right-to-left, and transliteratedArabic is to be read from left-to-right.
The convention ofshown in Table 1, a word may include multipleprefixes, as in   ????
(l: for, Al: the),  or multiplesuffixes, as in   ?????
(t: feminine singular, h: his).A word may also consist only of a stem, as in?????
(AlY, to/towards).The algorithm implementation involves (i)language model training on a morpheme-segmented corpus, (ii) segmentation of inputtext into a sequence of morphemes using thelanguage model parameters, and (iii)unsupervised acquisition of new stems from alarge unsegmented corpus.
The only linguisticresources required include  a small manuallysegmented corpus ranging from 20,000 wordsto 100,000 words, a table of prefixes andsuffixes of the language and  a largeunsegmented corpus.In Section 2, we discuss related work.
InSection 3, we describe the segmentationalgorithm.
In Section 4, we discuss theunsupervised algorithm for new stemacquisition.
In Section 5, we presentexperimental results.
In Section 6, wesummarize the paper.2   Related WorkOur work adopts major components of thealgorithm from (Luo & Roukos 1996):language model (LM) parameter estimationfrom a segmented corpus and inputsegmentation on the basis of LM probabilities.However, our work diverges from their workin two crucial respects: (i) new technique ofcomputing all possible segmentations of aword into prefix*-stem-suffix* for decoding,and  (ii) unsupervised algorithm for new stemacquisition based on a stem candidate'ssimilarity to stems occurring in the trainingcorpus.
(Darwish 2002) presents a  supervisedtechnique which identifies the root of anArabic word by stripping away the prefix andthe suffix of the word on the basis of manuallyacquired dictionary of word-root pairs and thelikelihood that a prefix and a suffix wouldoccur with the template from which the root isderived.
He reports 92.7% segmentationaccuracy on a 9,606 word evaluation corpus.His technique pre-supposes at most one prefixand one suffix per stem regardless of the actualnumber and meanings of prefixes/suffixesassociated with the stem.
(Beesley 1996)presents a finite-state morphological analyzerfor Arabic, which displays the root, pattern,and prefixes/suffixes.
The analyses are basedon manually acquired lexicons and rules.Although his analyzer is comprehensive in thetypes of knowledge it presents, it has beencriticized for their extensive development timeand lack of robustness, cf.
(Darwish 2002).marking a prefix with '#" and a suffix with '+' will beadopted throughout the paper.
(Yarowsky and Wicentowsky 2000)presents a minimally supervised morphologicalanalysis with a  performance of over 99.2%accuracy for the 3,888 past-tense test cases inEnglish.
The core algorithm lies in theestimation of a probabilistic alignmentbetween inflected forms and root forms.
Theprobability estimation is based on the lemmaalignment by frequency ratio similarity amongdifferent inflectional forms derived from thesame lemma, given a table of inflectionalparts-of-speech, a list of the canonical suffixesfor each part of speech, and a list of thecandidate noun, verb and adjective roots of thelanguage.
Their algorithm does not handlemultiple affixes per word.
(Goldsmith 2000) presents an unsupervisedtechnique based on the expectation-maximization algorithm and minimumdescription length to segment exactly onesuffix per word, resulting in an F-score of 81.8for suffix identification in English according to(Schone and Jurafsky 2001).
(Schone andJurafsky 2001) proposes an unsupervisedalgorithm capable of automatically inducingthe morphology of inflectional languages usingonly text corpora.
Their algorithm combinescues from orthography, semantics, andcontextual information to inducemorphological relationships in German, Dutch,and English, among others.
They report F-scores between 85 and 93 for suffix analysesand between 78 and 85 for circumfix analysesin these languages.
Although their algorithmcaptures prefix-suffix combinations orcircumfixes, it does not handle the multipleaffixes per word we observe in Arabic.2Words            Prefixes                 Stems             SuffixesArabic    Translit.
Arabic  Translit.
Arabic    Translit.
Arabic   Translit.?????????????
?
AlwlAyAt  #??
Al#       ????
wlAy      ??
+    +At???????????
HyAth           ??????
HyA  ?
+?
+    +t +h?????????????
llHSwl  #?#  ??
l# Al#    ??????
HSwl?????
AlY           ?????
AlYTable 1  Segmentation of Arabic Words into Prefix*-Stem-Suffix*3  Morpheme Segmentation3.1 Trigram Language ModelGiven an Arabic sentence, we use a trigramlanguage model on morphemes to segment itinto a sequence of morphemes {m1, m2, ?,mn}.The input to the morpheme segmenter is asequence of Arabic tokens ?
we use atokenizer that looks only at white space andother punctuation, e.g.
quotation marks,parentheses, period, comma, etc.
A sample ofa manually segmented corpus is given below2.Here multiple occurrences of prefixes andsuffixes per word are marked with anunderline.????
# ???
???????
????
??
??
?
?# ????
# ????
??
# ?
??+???
??
????
# ???
??????
???
?+????
???
????
# ??
#???
???+?
+?
????
+????
??
???
#???
#?# ???
# ?????
?# ??????
??
????
??+????
# ?
?????
?# ???
????
?
#?
???
??
????
????
??????
+?????
.?????
??
??????
#  ??
????
?
?#?# ?# ????????
???????
?????
????
# ?????
# ????
???
?
?# ???????
??
????
?+??
+ ???
????
???
#?
# ???????
?????????+????
???
?w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al#Awl fy jA}z +p Al# nmsA Al# EAm Al#mADy Ely syAr +p fyrAry $Er b# AlAm fybTn +h ADTr +t +h Aly Al# AnsHAb mn Al#tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al#fHwS +At Al# Drwry +p Hsb mA A$Ar fryq2 A manually segmented Arabic corpus containing about140K word tokens has been provided by LDC(http://www.ldc.upenn.edu).
We divided this corpus intotraining and the development test sets as described inSection 5.jAgwAr.
w# s# y# Hl sA}q Al# tjArb fyjAgwAr Al# brAzyly lwsyAnw bwrty mkAnAyrfAyn fy Al# sbAq gdA Al# AHd Al*y s#y# kwn Awly xTw +At +h fy EAlm sbAq +AtAlfwrmwlAMany instances of prefixes and suffixes inArabic are meaning bearing and correspond toa word in English such as pronouns andprepositions.
Therefore, we choose asegmentation into multiple prefixes andsuffixes.
Segmentation into one prefix  and onesuffix per word, cf.
(Darwish 2002), is not veryuseful for applications like statistical machinetranslation, (Brown et al 1993), for which anaccurate word-to-word alignment between thesource and the target languages is critical forhigh quality translations.The trigram language model probabilitiesof morpheme sequences, p(mi|mi-1, mi-2), areestimated from the morpheme-segmentedcorpus.
At token boundaries, the morphemesfrom previous tokens constitute the histories ofthe current morpheme in the trigram languagemodel.
The trigram model is smoothed usingdeleted interpolation with the bigram andunigram models, (Jelinek 1997), as in (1):(1) p(m3 | m1 ,m2) =  ?3 p(m3 |m1 ,m2) + ?2p(m3 |m2) + ?3 p(m3), where ?1+?2 +?3 = 1.A small morpheme-segmented corpusresults in a relatively high out of vocabularyrate for the stems.
We describe below anunsupervised acquisition of new stems from alarge unsegmented Arabic corpus.
However,we first describe the segmentation algorithm.3.2  Decoder for Morpheme Segmentation3We take the unit of decoding to be a sentencethat has been tokenized using white space andpunctuation.
The task of a decoder is to findthe morpheme sequence which maximizes thetrigram probability of the input sentence, as in(2):(2)  SEGMENTATIONbest = Argmax IIi=1, Np(mi|mi-1mi-2), N = number of morphemes inthe input.Search algorithm for (2) is informallydescribed for each word token as follows:Step 1: Compute all possible segmentations ofthe token  (to be elaborated in 3.2.1).Step 2: Compute the trigram language modelscore of each segmentation.
For somesegmentations of a token, the stem may be anout of vocabulary item.
In that case, we use an?UNKNOWN?
class in the trigram languagemodel with the model probability given byp(UNKNOWN|mi-1, mi-2) * UNK_Fraction, whereUNK_Fraction is 1e-9 determined on empiricalgrounds.
This allows us to segment new wordswith a high accuracy even with a relativelyhigh number of unknown stems in thelanguage model vocabulary, cf.
experimentalresults in Tables 5 & 6.Step 3: Keep the top N highest scoredsegmentations.3.2.1  Possible Segmentations of  a WordPossible segmentations of a word token arerestricted to those derivable from a table ofprefixes and suffixes of the language fordecoder speed-up and improved accuracy.Table 2 shows examples of atomic (e.g.
??,??)
and multi-component (e.g.
??????
,???????
)prefixes and suffixes, along with theircomponent morphemes in native Arabic.33 We have acquired the prefix/suffix table from a 110Kword manually segmented LDC corpus (51 prefixes & 72suffixes) and from IBM-Egypt (additional 14 prefixes &122 suffixes).
The performance improvement by theadditional prefix/suffix list ranges from 0.07% to 0.54%according to the manually segmented training corpussize.
The smaller the manually segmented corpus size is,the bigger the performance improvement by addingadditional prefix/suffix list is.Prefixes          Suffixes??
??
?
?# ??+??????
?#  ?
?# ????
?+   ???
??+???????
?#  ?#   ?
?# ?????+??
+ ?
?Table 2  Prefix/Suffix TableEach token is assumed to have the structureprefix*-stem-suffix*, and is compared againstthe prefix/suffix table for segmentation.
Givena word token, (i) identify all of the matchingprefixes and suffixes from the table, (ii) furthersegment each matching prefix/suffix at eachcharacter position, and (iii) enumerate allprefix*-stem-suffix* sequences derivable from(i) and (ii).Table 3 shows all of its possiblesegmentations of the token ???????
(wAkrrhA; 'and I repeat it'),4 where ?
indicatesthe null prefix/suffix and the Seg Score is thelanguage model probabilities of eachsegmentation S1 ... S12.
For this token, thereare two matching prefixes #?
(w#) and#??
(wA#) from the prefix table, and twomatching suffixes ?+(+A) and ?
?+(+hA)from the suffix table.
S1, S2, & S3 are thesegmentations given the null prefix ?
andsuffixes ?, +A, +hA.
S4, S5, & S6 are thesegmentations given the prefix w# and suffixes?, +A, +hA.
S7, S8, & S9 are thesegmentations given the prefix wA# andsuffixes ?, +A, +hA.
S10, S11, & S12 are thesegmentations given the prefix sequence w#A# derived from the prefix wA# and  suffixes?, +A, +hA.
As illustrated by S12, derivationof sub-segmentations of the matchingprefixes/suffixes enables the system to identifypossible segmentations which would have beenmissed otherwise.
In this case, segmentationincluding the derived prefix sequence??+???
# ?# ?
(w# A# krr +hA) happens tobe the correct one.3.2.2.
Prefix-Suffix FilterWhile the number of possible segmentations ismaximized by sub-segmenting matching4 A sentence in which the token occurs is as follows:  ????????????
????????
????
??
?????
?????
?????
??
????????
???????
(qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fyAlm$tqAt AlnfTyp.
)4prefixes and suffixes, some of illegitimate sub-segmentations are filtered out on the basis ofthe knowledge specific to the manuallysegmented corpus.
For instance, sub-segmentation of the suffix hA into +h +A isruled out because there is no suffix sequence+h +A in the training corpus.
Likewise, sub-segmentation of the prefix Al into A# l# isfiltered out.
Filtering out improbableprefix/suffix sequences improves thesegmentation accuracy, as shown in Table 5.Prefix Stem Suffix Seg ScoresS1 ?
wAkrrhA ?
2.6071e-05S2 ?
wAkrrh +A 1.36561e-06S3 ?
wAkrr +hA 9.45933e-07S4 w# AkrrhA ?
2.72648e-06S5 w# Akrrh +A 5.64843e-07S6 w# Akrr +hA 4.52229e-05S7 wA# krrhA ?
7.58256e-10S8 wA# krrh +A 5.09988e-11S9 wA# krr +hA 1.91774e-08S10 w# A# krrhA ?
7.69038e-07S11 w# A# krrh +A 1.82663e-07S12 w# A# krr +hA 0.000944511Table 3 Possible Segmentations of???????
(wAkrrhA)4  Unsupervised Acquisition  of  NewStemsOnce the seed segmenter is developed on thebasis of a manually segmented corpus,  theperformance may be improved by iterativelyexpanding the stem vocabulary  and retrainingthe language model on a large automaticallysegmented Arabic corpus.Given a small manually segmented corpusand a large unsegmented corpus, segmenterdevelopment proceeds as follows.Initialization: Develop the seed segmenterSegmenter0 trained on the manually segmentedcorpus Corpus0, using the language modelvocabulary, Vocab0, acquired from Corpus0.Iteration: For i = 1 to N, N = the number ofpartitions of the unsegmented corpusi.
Use Segmenteri-1 to segment Corpusi.ii.
Acquire new stems from the newlysegmented Corpusi.
Add the new stems toVocabi-1, creating an expanded vocabularyVocabi.iii.
Develop Segmenteri trained on Corpus0through Corpusi with Vocabi.Optimal Performance Identification:Identify the Corpusi and Vocabi, which resultin the best performance, i.e.
system trainingwith Corpusi+1 and Vocabi+1 does not improvethe performance any more.Unsupervised acquisition of new stemsfrom an automatically segmented new corpusis a three-step process: (i)  select new stemcandidates on the basis of a frequencythreshold, (ii) filter out new stem candidatescontaining a sub-string with a high likelihoodof being a prefix, suffix, or prefix-suffix.
Thelikelihood of a sub-string being a prefix, suffix,and prefix-suffix of a token is computed as in(5) to (7), (iii) further filter out new stemcandidates on the basis of contextualinformation, as in (8).
(5)  Pscore = number of tokens with prefix P /number of tokens starting with sub-string P(6)  Sscore = number of tokens with suffix S /number of tokens ending with sub-string S(7)  PSscore = number of tokens with prefix Pand suffix S / number of tokens starting withsub-string P and ending with  sub-string SStem candidates containing a sub-string with ahigh prefix, suffix, or prefix-suffix likelihoodare filtered out.
Example sub-strings with theprefix, suffix, prefix-suffix likelihood 0.85 orhigher in a 110K word manually segmentedcorpus are given in Table 4.
If a token startswith the sub-string ???
(sn), and end with  ???
(hA), the sub-string's likelihood of being theprefix-suffix of the token is 1.
If a token startswith the sub-string  ????
(ll), the sub-string'slikelihood of being the prefix of the token is0.945, etc.Arabic Transliteration      Score???
+  stem # ???
sn# stem+hA      1.0?+ stem # ?????
Al# stem+p      0.984stem # ????
ll# stem      0.945?
?+  stem         stem+At      0.889Table 4 Prefix/Suffix Likelihood Score5(8) Contextual Filter: (i) Filter out stems co-occurring with prefixes/suffixes not present inthe training corpus.
(ii) Filter out stems whoseprefix/suffix distributions are highlydisproportionate to those seen in the trainingcorpus.According to (8), if a stem is followed bya potential suffix +m, not present in thetraining corpus, then it is filtered out as anillegitimate stem.
In addition, if a stem ispreceded by a prefix and/or followed by asuffix with a significantly higher proportionthan that observed in the training corpus, it isfiltered out.
For instance, the probability forthe suffix +A to follow a stem is less than 50%in the training corpus regardless of the stemproperties, and therefore, if a candidate stem isfollowed by +A with the probability of over70%, e.g.
mAnyl +A, then it is filtered out asan illegitimate stem.5  Performance EvaluationsWe present experimental results illustrating theimpact of three factors on segmentation errorrate: (i) the base algorithm, i.e.
language modeltraining and decoding, (ii) language modelvocabulary and training corpus size, and (iii)manually segmented training corpus size.Segmentation error rate is defined in (9).
(9)  (number of incorrectly segmented tokens /total number of tokens)  x  100Evaluations have been performed on adevelopment test corpus containing 28,449word tokens.
The test set is extracted from20001115_AFP_ARB.0060.xml.txt through20001115_AFP_ARB.0236.xml.txt of theLDC Arabic Treebank: Part 1 v 2.0 Corpus.Impact of the core algorithm and theunsupervised stem acquisition has beenmeasured on segmenters developed from 4different sizes of manually segmented seedcorpora: 10K, 20K, 40K, and 110K words.The experimental results are shown inTable 5.
The baseline performances areobtained by assigning each token the mostfrequently occurring segmentation in themanually segmented training corpus.
Thecolumn headed by '3-gram LM' indicates theimpact of the segmenter using only trigramlanguage model probabilities for decoding.Regardless of the manually segmented trainingcorpus size, use of  trigram language modelprobabilities reduces the word error rate of thecorresponding baseline by approximately 50%.The column headed by '3-gram LM + PSFilter' indicates the impact of the corealgorithm plus Prefix-Suffix Filter discussed inSection 3.2.2.
Prefix-Suffix Filter reduces theword error rate ranging from 7.4% for thesmallest (10K word) manually segmentedcorpus to 21.8% for the largest (110K word)manually segmented corpus ?- around 1%absolute reduction for all segmenters.
Thecolumn headed by '3-gram LM + PS Filter +New Stems' shows the impact of unsupervisedstem acquisition from a 155 million wordArabic corpus.
Word error rate reduction dueto the unsupervised stem acquisition is 38% forthe segmenter developed from the 10K wordmanually segmented corpus and 32% for thesegmenter developed from 110K wordmanually segmented corpus.Language model vocabulary size (LM VOCSize) and the unknown stem ratio (OOV ratio)of various segmenters is given in Table 6.
Forunsupervised stem acquisition, we have set thefrequency threshold at 10 for every 10-15million word corpus, i.e.
any new morphemesoccurring more than 10 times in a 10-15million word corpus are considered to be newstem candidates.
Prefix, suffix, prefix-suffixlikelihood score to further filter out illegitimatestem candidates was set at 0.5 for thesegmenters developed from 10K, 20K, and40K manually segmented corpora, whereas itwas set at 0.85 for the segmenters developedfrom a 110K manually segmented corpus.Both the frequency threshold and the optimalprefix, suffix, prefix-suffix likelihood scoreswere determined on empirical grounds.Contextual Filter stated in (8) has been appliedonly to the segmenter developed from 110Kmanually segmented training corpus.5Comparison of Tables 5 and 6 indicates a highcorrelation between the segmentation error rateand the unknown stem ratio.5 Without the Contextual Filter, the  error rate of thesame segmenter is 3.1%.6Manually SegmentedTraining Corpus SizeBaseline  3-gram LM  3-gram LM +PS Filter3-gram LM + PSFilter + New Stems10K Words    26.0%        14.7%            13.6%          8.5%20K Words       19.7%        9.1%            8.0%          5.9%40K Words        14.3%        7.6%            6.5%          5.1%110K Words        11.0%        5.5%            4.3%           2.9%Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate3-gram LM  3-gram LM + PS Filter + New Stems Manually SegmentedTraining Corpus Size     LM VOC Size      OOV Ratio    LM VOC Size      OOV Ratio10K Words           2,496          20.4%          22,964           7.8%20K Words           4,111          11.4%          25,237           5.3%40K Words           5,531            9.0%          21,156           4.7%110K Words           8,196            5.8%          25,306           1.9%Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio3-gram LM + PS Filter + New Stems Manually SegmentedTraining Corpus Size   Unknown Stem          Alywm     Other Errors  Total # of Errors10 K Words    1,844  (76.9%)        98 (4.1%)     455 (19.0%)          2,39720 K Words    1,174  (71.1%)        82 (5.0%)     395 (23.9%)          1,65140 K Words    1,005  (69.9%)        81 (5.6%)     351 (24.4%)          1,437110 K Words       333  (39.6%)        82 (9.8%)     426 (50.7%)             841Table 7 Segmentation Error AnalysesTable 7 gives the error analyses of foursegmenters according to three factors: (i)errors due to unknown stems, (ii) errorsinvolving  ??????????
(Alywm), and (iii) errors due toother factors.
Interestingly, the segmenterdeveloped from a 110K manually segmentedcorpus has the lowest percentage of ?unknownstem?
errors at 39.6% indicating that ourunsupervised acquisition of new stems isworking well, as well as suggesting to use alarger unsegmented corpus for unsupervisedstem acquisition.??????????
(Alywm) should be segmenteddifferently depending on its part-of-speech tocapture the semantic ambiguities.
If it is anadverb or a proper noun, it is segmented as??????????
'today/Al-Youm', whereas if it is a noun,it is segmented as ??
# ??????
'the day.'
Propersegmentation of   ??????????
primarily requires itspart-of-speech information, and cannot beeasily handled by morpheme trigram modelsalone.Other errors include over-segmentation offoreign words such as  ???????????????
(bwtyn) as  ?#??????????
and  ?????????????
(lytr)  'litre' as ?
# ?# ?????
.These errors are attributed to the segmentationambiguities of these tokens:  ???????????????
isambiguous between ' ???????????????
(Putin)' and '?#??????????
(by aorta)'.
?????????????
is ambiguousbetween ' ?????????????
(litre)' and ' ?
# ?# ?????
(for himto harm)'.
These errors may also be correctedby incorporating part-of-speech informationfor disambiguation.To address the segmentation ambiguityproblem, as illustrated by ' ???????????????
(Putin)' vs.' ?
# ??????????
(by aorta)', we have developed ajoint model for segmentation and part-of-speech tagging for which the bestsegmentation of an input sentence is obtainedaccording to the formula (10), where ti is thepart-of-speech of morpheme mi, and N is thenumber of morphemes in the input sentence.
(10) SEGMENTATIONbest = Argmax ?i=1,Np(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti)By using the joint model, the segmentationword error rate of the best performingsegmenter has been reduced by about 10%7from 2.9% (cf.
the last column of Table 5) to2.6%.5  Summary and Future WorkWe have presented a robust word segmentationalgorithm which segments a word into aprefix*-stem-suffix* sequence, along withexperimental results.
Our Arabic wordsegmentation system implementing thealgorithm achieves around 97% segmentationaccuracy on a development test corpuscontaining 28,449 word tokens.
Since thealgorithm can identify any number of prefixesand suffixes of a given token, it is generallyapplicable to various language familiesincluding agglutinative languages (Korean,Turkish, Finnish), highly inflected languages(Russian, Czech) as well as semitic languages(Arabic, Hebrew).Our future work includes (i) applicationof the current technique to other highlyinflected languages, (ii) application of theunsupervised stem acquisition technique onabout 1 billion word unsegmented Arabiccorpus, and (iii) adoption of a novelmorphological analysis technique to handleirregular morphology, as realized in Arabicbroken plurals  ?????????
(ktAb) 'book' vs.
????????
(ktb) 'books'.AcknowledgmentThis work was partially supported by theDefense Advanced Research Projects Agencyand monitored by SPAWAR under contract No.N66001-99-2-8916.
The views and findingscontained in this material are those of theauthors and do not necessarily reflect theposition of policy of the Government and noofficial endorsement should be inferred.
Wewould like to thank Martin Franz for discussionson language model building, and his help withthe use of ViaVoice language model toolkit.ReferencesBeesley, K. 1996.
Arabic Finite-StateMorphological Analysis and Generation.Proceedings of COLING-96, pages 89?
94.Brown, P., Della Pietra, S., Della Pietra, V.,and Mercer, R. 1993.
The mathematics  ofstatistical machine translation:  ParameterEstimation.
Computational  Linguistics,19(2): 263?311.Darwish, K. 2002.
Building a Shallow  ArabicMorphological Analyzer in  One  Day.Proceedings of the  Workshop onComputational  Approaches to SemiticLanguages,  pages 47?54.Franz, M. and McCarley, S. 2002.
ArabicInformation Retrieval at IBM.
Proceedingsof TREC 2002, pages 402?
405.Goldsmith, J.
2000.
Unsupervised  learningof  the morphology of a natural  language.Computational Linguistics, 27(1).Jelinek, F. 1997.
Statistical Methods forSpeech Recognition.
The MIT Press.Luo, X. and Roukos, S. 1996.
An IterativeAlgorithm to Build Chinese LanguageModels.
Proceedings of ACL-96, pages139?143.Schone, P. and Jurafsky, D. 2001.Knowledge-Free Induction of  InflectionalMorphologies.
Proceedings  of  NorthAmerican Chapter of  Association forComputational  Linguistics.Yarowsky, D. and Wicentowski, R. 2000.Minimally supervised morphologicalanalysis by multimodal alignment.Proceedings of ACL-2000, pages 207?
216.Yarowsky, D, Ngai G. and Wicentowski, R.2001.
Inducting Multilingual Text  AnalysisTools via Robust Projection  across AlignedCorpora.
Proceedings of  HLT 2001, pages161?168.8
