Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 356?363, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsWBI-NER: The impact of domain-specific features on the performance ofidentifying and classifying mentions of drugsTim Rockta?schel Torsten Huber Michael Weidlich Ulf LeserHumboldt-Universita?t zu BerlinKnowledge Management in BioinformaticsUnter den Linden 6Berlin, 10099, Germany{trocktae,thuber,weidlich,leser}@informatik.hu-berlin.deAbstractNamed entity recognition (NER) systemsare often based on machine learning tech-niques to reduce the labor-intensive devel-opment of hand-crafted extraction rules anddomain-dependent dictionaries.
Nevertheless,time-consuming feature engineering is oftenneeded to achieve state-of-the-art performance.In this study, we investigate the impact of suchdomain-specific features on the performanceof recognizing and classifying mentions ofpharmacological substances.
We compare theperformance of a system based on general fea-tures, which have been successfully appliedto a wide range of NER tasks, with a systemthat additionally uses features generated fromthe output of an existing chemical NER tooland a collection of domain-specific resources.We demonstrate that acceptable results can beachieved with the former system.
Still, our ex-periments show that using domain-specific fea-tures outperforms this general approach.
Oursystem ranked first in the SemEval-2013 Task9.1: Recognition and classification of pharma-cological substances.1 IntroductionThe accurate identification of drug mentions in textis an important prerequisite for many applications, in-cluding the retrieval of information about substancesin drug development (e.g.
Roberts and Hayes (2008)),the identification of adverse drug effects (e.g.
Lea-man et al(2010)) and the recognition of drug-druginteractions (e.g.
Thomas et al(2011)).
Given thatmost of the information related to drug research iscovered by medical reports and pharmacological pub-lications, computational methods for information ex-traction should be used to support this task.The SemEval-2013 Task 9.1 competition1 (Segura-Bedmar et al 2013) aims at a fair assessment on thestate-of-the-art of tools that recognize and classifymentions of pharmacological substances in naturallanguage texts ?
a task referred to as drug namedentity recognition (NER).
The goal of participatingteams is to recreate the gold annotation on a held-out part of an annotated corpus.
Four classes of en-tities have to be identified: Drug, DrugN, Groupand Brand.
Entities of class Drug denote any kindof drug that is approved for use in humans, whereasDrugN denotes substances that are not approved.Group are terms describing a group of drugs andBrand stands for drug names introduced by a phar-maceutical company.The aim of this study is to examine whether it isworthwhile to implement domain-specific featuresfor supporting drug NER.
The question we attemptto answer is whether such features really help in iden-tifying and classifying mentions of drugs or whethera mostly domain-independent feature set, which canbe applied to many other tasks, achieves a compara-ble performance.2 Related workVarious NER systems for identifying differentclasses of chemical entities, including mentions ofdrugs, trivial names and IUPAC terms, have been pro-posed.1http://www.cs.york.ac.uk/semeval-2013/task9/(accessed 2013-04-29)356Klinger et al(2008) trained a conditional randomfield (CRF) (Lafferty et al 2001) for extracting men-tions of IUPAC and IUPAC-like entities.
They reportan F1 measure of 85.6% on a hand-annotated corpusconsisting of MEDLINE abstracts.Segura-Bedmar et al(2008) introduced Drug-NER, which is based on UMLS MetaMap Trans-fer (MMTx) and nomenclature rules by the WorldHealth Organization International NonproprietaryNames (INNs).
Their system extracts and classifiesmentions of drugs and achieves a precision of 99.1%and a recall of 99.8% on a silver-standard corpus.OSCAR (Open-Source Chemistry Analysis Rou-tines) (Corbett and Murray-Rust, 2006; Jessop et al2011) extracts mentions of a wide range of chemi-cals using a maximum entropy Markov model (Mc-Callum et al 2000).
It achieves an F1 of 83.2% ona corpus consisting of PubMed abstracts and 80.7%on a corpus consisting of chemistry papers (Corbettand Copestake, 2008).Hettne et al(2009) compiled Jochem (the jointchemical dictionary) from ChemIDplus, ChEBI,DrugBank, PubChem, HMDB, KEGG, MeSH andCAS Registry IDs.
Jochem was used with Peregrine(Schuemie et al 2007), a dictionary-based NER tool,achieving an F1 of 50% on the SCAI corpus (Kola?riket al 2008).We developed ChemSpot (Rockta?schel et al2012), a system for extracting mentions of variouskinds of chemicals from text.
We applied a CRFfor extracting mentions of IUPAC entities based onthe work of Klinger et al(2008) and used Jochem(Hettne et al 2009) with an adapted matching-mechanism for identifying trivial names, drugs andbrands.
ChemSpot v1.0 achieved an overall F1 of68.1% on the SCAI corpus.
In the meantime, we haveworked on several enhancements (see Section 3.1).The SemEval-2013 Task 9.1 poses new challengeson NER tools.
Instead of targeting all kinds of chem-icals, it focuses on drugs, i.e., pharmacological sub-stances that affect humans and are used for adminis-tration.
Moreover, entities need to be classified intothe four categories mentioned above.3 MethodsOur approach is based on a linear-chain CRF withmostly domain-independent features commonly ap-plied to NER tasks.
In addition, we employ vari-ous domain-specific features derived from the out-put of ChemSpot?s components, as well as Jochem,the PHARE ontology (Coulet et al 2011) and theChEBI ontology (De Matos et al 2010).
In thefollowing, we first explain extensions to ChemSpot.Subsequently, we give a brief introduction to linear-chain CRFs before describing the general anddomain-specific features used by our system.
Finally,we explain the experimental setup and discuss ourresults.3.1 Improvements of ChemSpotTo improve ChemSpot?s chemical NER performance,we extend it by two components and modify itsmatch-expansion mechanism.The first addition is a pattern-based taggerfor chemical formulae.
In its basic form it ex-tracts mentions matching the regular expression(S N?(\+|-)?
)+ where S denotes a chemicalsymbol and N a natural number greater one.2 Thispattern is augmented by filters to comply with othernaming conventions, such as correct grouping ofcompounds with parentheses.The second extension targets ambiguous ab-breviations.
For example, the abbreviation ?DAG?could denote ?diacylglycerol?
or ?directed acyclicgraph?.
We use ABBREV, an algorithm proposedby Schwartz and Hearst (2003), for extracting suchabbreviations and their definitions (e.g.
?diacylglyc-erol (DAG)?).
Note that the position of the long form(LF) and short form (SF) is interchangeable.
To dis-ambiguate between chemical and non-chemical ab-breviations, we apply the following two rules to thementions extracted by ChemSpot: (1) For a givenpair of LF and SF, we check whether the LF wasfound to be a chemical but the SF was not.
In thiscase we add a new annotation for every occurrenceof the SF in the document.
(2) Contrary to that, ifonly the SF was tagged as a chemical but the LF wasnot, we assume that the abbreviation does not referto a chemical and remove all annotations of the SFin the document.ChemSpot?s match-expansion often leads to theextraction of non-chemical suffixes corresponding toverbs, e.g., ?-induced?, ?-enriched?
or ?-mediated?.2By convention, 1 is ommited (e.g.
CO2 instead of C1O2).357yiyi?1yi?2 yi+1 yi+2??
?
?
?xFigure 1: A factor graph for a 1st-order linear-chainCRF (2nd-order with dashed edges).
Note that foreach feature function fj in the factor ?, the sameweight ?j is used at all other positions (gray) in thesequence (parameter tying).To tackle this issue we stop the expansion at tokenswhose part-of-speech tag refers to a verb form.
Fur-thermore, we integrated OPSIN (Lowe et al 2011)to normalize entity mentions to InChI strings.The current v1.5 release of ChemSpot achieves anoverall F1 of 74.2% on the SCAI corpus, improvingthe performance by 6.1 percentage points (pp) F1compared to ChemSpot v1.0.3.2 Linear-chain conditional random fieldsContrary to the hybrid strategy used in ChemSpot,we follow a purely machine learning based approachfor drug NER in this work.
NER can be formulatedas a sequence labeling task where the goal is to finda sequence of labels y = {y1, .
.
.
, yn} given a se-quence x = {x1, .
.
.
, xn} of observed input tokens.Labels commonly follow the IOB format, where Bdenotes a token at the beginning of an entity men-tion, I denotes the continuation of a mention and Ocorresponds to tokens that are not part of a mention.Extracting entity mentions from a tokenized text xthen amounts to finding y?
= arg maxy p(y |x).Linear-chain CRFs are well-known discriminativeundirected graphical models that encode the condi-tional probability p(y |x) of a set of input variablesx and a sequence of output variables y (see Wallach(2004) or Klinger and Tomanek (2007) for an intro-duction).
In the case of NER, x is a sequence of ntokens and y a sequence of n corresponding labels.Linear-chain CRFs of order k factorize p(y |x) intoa product of factors ?, globally normalized by aninput-dependent partition function Z(x):p(y |x) =1Z(x)n?i=1?
(yi?k, .
.
.
, yi?1, yi,x, i).A factor ?
is commonly defined as the exponen-tial function of a sum of weighted feature functions{f1, .
.
.
, fm}:1Z(x)n?i=1exp?
?m?j=1?jfj(yi?k, .
.
.
, yi?1, yi,x, i)??
.The feature function weights {?j} can be learnedfrom training data and are shared across all positionsi (parameter tying).The factorization of a CRF can be illustrated bya factor graph (Kschischang et al 2001).
A factorgraph is a bipartite graph, where one set of nodescorresponds to random variables and the other to fac-tors.
Each factor is connected to the variables of itsdomain, making the factorization of the model ap-parent.
Figure 1 shows a factor graph for a segmentof a linear-chain CRF of order one and two respec-tively.
In a linear-chain CRF of order k, the label ofa token at position i is connected via feature func-tions of a factor to the input sequence x as well asthe previous k labels.
For example, in a first-orderlinear-chain CRF, one of the feature functions couldbe f[O?B,capital](yi?1, yi,x, i), which evaluates to 1 ifyi?1 = O, yi = B and xi starts with a capital letter(otherwise it yields 0).
Multiplication with the weight?
[O?B,capital] yields an unnormalized local score thatindicates how favorable a transition from O to B isprovided that the token at position i starts with a capi-tal letter.
Note that the terms feature function and fea-ture are often used synonymously and the case differ-entiation for different labels is implicit.
In this worka feature fcapital denotes its corresponding set of first-order feature functions {f[s?t,capital](yi?1, yi,x, i) |(s, t) ?
{(B,B), (B,I), (B,O), (I,B), (I,I), (I,O),(O,B), (O,O)}}3 (similarly for second-order).We employ MALLET (McCallum, 2002) as under-lying CRF implementation and use a second-orderlinear-chain CRF with offset conjunctions of ordertwo.
Offset conjunctions of order k adds featuresaround a window of k to the features at a partic-ular position, providing more contextual informa-tion.
Akin to Klinger et al(2008), we perform fine-grained tokenization, splitting at special charactersand transitions from alphanumeric characters to dig-its.
An exemplary tagging sequence is shown in Ta-ble 1.3Transitions from O to I are invalid.358i 0 1 2 3 4 5 6 7 8 9 10y O B-DrugN O B-DrugN I-DrugN I-DrugN O B-Group O O Ox Both ibogaine and 18 - MC ameliorate opioid withdrawal signs .Table 1: Example label sequence for the tokenized sentence MedLine.d110.s4 of the training corpus.Feature Class DescriptionFCfCHEMSPOT part of a prediction by ChemSpotfIUPAC part of an IUPAC entityfFORMULA part of a chemical formulafDICTIONARY part of a dictionary matchfABBREV part of a chemical abbreviationFJfJOCHEM dictionaries in JochemfPREFIX frequent chemical prefixfSUFFIX frequent chemical suffixFOfPHARE PHARE ontologyfCHEBDESCS #descendants in ChEBI ontologyfCHEBDEPTH average depth in ChEBI ontologyFGfKLINGER see Klinger et al(2008)fBANNER see Leaman and Gonzalez (2008)fABNER see Settles (2005)FFfUPPERCASESENT.
part of an upper-case sentencefPREVWINDOW text of preceding four tokensfNEXTWINDOW text of succeeding four tokensTable 2: Overview of features used for identifyingand classifying mentions of pharmacological sub-stances.Note that the sequence-labeling approach in thedescribed form cannot cope with discontinuous en-tity mentions.
Since only a tiny fraction (?
0.3%) ofentities in the training corpus are discontinuous, wesimply neglect these for training and tagging.3.3 Feature setsAn overview of the features used by our system isshown in Table 2.
Our first two submissions for theSemEval-2013 Task 9.1 differ only in that they usedifferent subsets of these features.
Run 1 employs afeature set assembled from common general featuresused for biomedical NER (FG ?
FF ), whereas Run2 additionally uses features tailored for extractingmentions of chemicals (FC ?
FJ ?
FO).3.3.1 Run 1: general features FG and FFWe employ a union of common, rather domain-independent features published by Klinger et al(2008), Settles (2005) and Leaman and Gonzalez(2008).
Note that these feature sets have been suc-cessfully applied to a wide range of different biomed-ical NER tasks, e.g., identifying mentions of DNAsequences, genes, diseases, mutations, IUPAC terms,cell lines and cell types.
They encompass morpho-logical, syntactic and orthographic features, such asthe text of the token itself, token character n-gramsof length 2 and 3, prefixes and suffixes of length 2, 3,and 4, characters left and right to a token and part-of-speech tags.
Furthermore, they contain various regu-lar expressions that capture, for instance, whether atoken starts with a capital letter or contains digits.In addition, we employ features based on NERexamples in FACTORIE (McCallum et al 2009).Specifically, we use the text of the four precedingand succeeding tokens and whether a token is part ofa sentence that contains only upper-case characters.The latter is commonly the case for headlines, whichlikely contain an entity mention.3.3.2 Run 2: domain-specific featuresIn addition to the features of Run 1, we use pre-dictions of our improved version of ChemSpot, aswell as features derived from Jochem, PHARE andChEBI.ChemSpot-based features FC: When a token ispart of a mention extracted by one of ChemSpot?scomponents (i.e.
IUPAC entity, chemical formula,dictionary match or chemical abbreviation), we usethe name of the respective component as feature.
Inaddition, we determine whether a token is part of anentity predicted by ChemSpot after match-expansion,boundary-correction and resolution of overlappingentities.
Using the output of ChemSpot as featuresfor our system could be framed as stacking (seeWolpert (1992)).Jochem-based features FJ : For every dictionarycontained in Jochem, we check whether a token ispart of an entity in that dictionary and use the name ofthe dictionary as feature.
Furthermore, we compile359a list of frequent chemical suffixes and prefixes oflength three from Jochem.Ontology-based features FO: It is often hard todetermine whether a mention refers to a specificchemical entity or rather an abstract term denotinga group of chemicals.
To distinguish between thesetwo cases, we calculate the average depth and thenumber of descendants of a term in the ChEBI on-tology and use the binned count as feature.
The ideabehind these features is that the specificity of an en-tity correlates positively with its depth in the ontol-ogy (e.g.
leaf nodes are likely specific chemicals)and negatively with the number of descendants (i.e.having few descendants indicates a specific entity).Further ontology-based features are derived fromPHARE, which consists of 200 curated relations.
Ifpossible, we map a token to a term in that ontologyand use its label as feature.3.4 ExperimentsWe perform document-level 10-fold cross-validation(CV) on the training corpus to measure the impactof domain-specific features.
To ensure comparabilitybetween Run 1 and Run 2, we use the same splits forevaluation.
Furthermore, we train models on the com-plete training corpus and evaluate on the test corpusof DDI Task 9.1 for each run respectively.
In addi-tion, we train a third model based on the best featureset determined with CV and use the entity mentionsof the Task 9.2 test corpus, which also contains anno-tations of drug-drug interactions, as additional train-ing data (Run 3).
Following the SemEval-2013 Task9.1 metrics, we evaluate exact matching performance(correct entity boundaries) and strict matching per-formance (correct boundaries and correct type).4 ResultsTable 3 shows micro-average CV results for identi-fying and classifying mentions of pharmacologicalsubstances in the training corpus.
The performancevaries drastically between different entity classes re-gardless of the feature set, e.g., Run 1 achieves an F1of 91.0% for Drug, but only 15.9% F1 for DrugN.Run 2 outperforms Run 1 for entities of classDrug (+1.2 pp F1) and DrugN (+4.9 pp F1), butyields a lower performance for Brand (?0.9 ppF1) and no change for Group entities.
Overall, theRun 1 Run 2P R F1 P R F1 ?F1Drug 92.1 89.9 91.0 92.0 92.3 92.2 +1.2DrugN 54.7 9.3 15.9 62.4 12.5 20.8 +4.9Group 87.2 82.5 84.8 87.3 82.3 84.8 0.0Brand 87.8 70.8 78.4 87.1 69.8 77.5 ?0.9Exact 93.9 86.9 90.3 94.5 89.0 91.7 +1.4Strict 90.3 83.6 86.8 90.3 85.1 87.6 +0.8Table 3: Document-level 10-fold cross-validationmicro-average results on the training corpus.micro-average F1 measure increases by 0.8 pp forstrict matching and 1.4 pp F1 for exact matching.The performance on the test corpus (see Table 4)is drastically lower compared to CV results (e.g.
17.6pp F1 for strict evaluation of Run 1).
Except for enti-ties of class Group, using domain-specific featuresleads to a superior performance for identifying andclassifying mentions of pharmacological substances.Run 2 outperforms Run 1 by 1.6 pp F1 for strict eval-uation and 5.9 pp F1 for exact evaluation.
Using en-tity mentions of the Task 9.2 test corpus as additionaltraining data (Run 3) further boosts the performanceby 0.7 pp F1 for strict evaluation.5 DiscussionOur results show a clear performance advantagewhen using domain-specific features tailored foridentifying mentions of chemicals.
CV results andresults on the test corpus show an increase in preci-sion and recall for exact matching and an increasein recall for strict matching.
The considerably higherrecall for exact matching can be attributed to a highercoverage of chemical entities by features that exploitdomain-knowledge.It is striking that the performance for DrugN enti-ties is extremely low compared to the other classes.We believe that this might be due to two reasons.First, entities of this class are underrepresented inthe training corpus (?
3%).
Since machine learningbased methods tend to favor the majority class, it islikely that many DrugN entities were classified asmentions of one of the much larger classes Drug (?64%) or Group (?
23%).
This can be confirmed bythe large differences between strict and exact match-ing results shown in Table 3 and Table 4.360Run 1 Run 2 Run 3# P R F1 P R F1 ?F1 P R F1 ?F1Drug 351 74.2 79.5 76.8 72.9 85.2 78.6 +1.8 73.6 85.2 79.0 +0.4DrugN 121 25.0 4.1 7.1 35.7 8.3 13.4 +6.3 31.4 9.1 14.1 +0.7Group 155 77.3 74.8 76.1 78.1 73.5 75.7 ?0.4 79.2 76.1 77.6 +1.9Brand 59 76.2 81.4 78.7 77.8 83.1 80.3 +1.6 81.0 86.4 83.6 +3.3Exact 686 82.1 72.9 77.2 85.6 80.8 83.1 +5.9 85.5 81.3 83.3 +0.2Strict 686 73.6 65.3 69.2 73.0 68.8 70.8 +1.6 73.4 69.8 71.5 +0.7DrugBank (Strict) 304 86.9 85.2 86.0 87.3 86.2 86.8 +0.8 88.1 87.5 87.8 +1.0MEDLINE (Strict) 382 60.8 49.5 54.5 60.5 55.0 57.6 +3.1 60.7 55.8 58.1 +0.5Table 4: Results on the test corpus.
?F1 denotes the F1 pp difference to the preceding Run and # the numberof annotated mentionsSecond, DrugN denotes substances that have an ef-fect on humans, but are not approved for medical use?
a property that is rarely stated along with the entitymention and can thus often only be determined withdomain-knowledge.We think it is also important to point to the largedifference between results obtained by 10-fold CVon the training corpus and test results (e.g.
up to 17.6pp F1 for Run 1).
One reason might be the large frac-tion (?
83%) of entity mentions that appear morethan once in the training corpus compared to pre-sumably many unseen entities in the test corpus.
For10-fold CV this means that an entity in the evaluationfold has already been seen with a high probability inone of the nine training folds, yielding results thatoverestimate the generalization performance.
More-over, our results indicate that identifying and clas-sifying pharmacological substances is much harderfor MEDLINE documents than for DrugBank docu-ments with a difference of up to 31.5 pp F1 (cf.
thelast two rows of Table 4).
Hence, another apparentreason for the performance differences is the sub-stantial skew in the ratio of DrugBank to MEDLINEdocuments in the training corpus (roughly 4:1) com-pared to the test corpus (roughly 1:1).
Since both setsof documents stem from different resources, this canbe referred to as domain-adaptation problem.In additional experiments we found that thegeneral-purpose chemical NER tool ChemSpotachieves an F1 of 65.5% for exact matching on thetest corpus.
This is 17.8 pp F1 below our best resultsobtained with a machine learning based system (cf.Run 3) that is able to exploit properties of the task-specific annotations of the corpora.6 ConclusionWe described our contribution to the SemEval-2013Task 9.1: Recognition and classification of pharma-cological substances.
We found that a system basedon rather general features commonly used for a widerange of biomedical NER tasks yields competetiveresults.
Implementing this system needed no domain-adaptation and its performance could be sufficient forapplications building upon drug NER.
Nevertheless,adding domain-specific features boosts the perfor-mance considerably.
Further improvements can beachieved by using entity annotations of the Task 9.2test corpus as additional training data.We identified two limitations of our approach.First, we found that entities of the minority class(DrugN) are very hard to classify correctly.
Second,differences between DrugBank and MEDLINE docu-ments probably cause a domain-adaptation problem.For future work, one could investigate whether thelatter can be addressed by domain-adaptation tech-niques (e.g.
Satpal and Sarawagi (2007)).
To copewith DrugN entities, one could implement featuresderived from those resources that were used by theannotators for deciding whether a substance is ap-proved for use in humans, e.g., Drugs@FDA4 andthe WHO ATC5 classification system.4http://www.accessdata.fda.gov/scripts/cder/drugsatfda/ (accessed 2013-04-29)5http://www.whocc.no/atc_ddd_index/ (accessed2013-04-29)361AcknowledgementsWe thank Philippe Thomas for preparing a simplified for-mat of the corpora.
We thank him and Roman Klinger forfruitful discussions.Funding: Tim Rockta?schel is funded by the German Fed-eral Ministry of Economics and Technology (BMWi)[KF2205209MS2], Torsten Huber and Michael Weidlichare funded by the German Federal Ministry of Educationand Research (BMBF) [0315746].ReferencesPeter Corbett and Ann Copestake.
2008.
Cascaded classi-fiers for confidence-based chemical named entity recog-nition.
BMC Bioinf., 9(Suppl 11):S4.Peter Corbett and Peter Murray-Rust.
2006.
High-throughput identification of chemistry in life sciencetexts.
In Proc.
of CompLife 2006, pages 107?118.Adrien Coulet, Yael Garten, Michel Dumontier, Russ B.Altman, Mark A. Musen, and Nigam H. Shah.
2011.
In-tegration and publication of heterogeneous text-minedrelationships on the Semantic Web.
J. Biomed.
Seman-tics, 2(Suppl 2):S10.Paula De Matos, Rafael Alca?ntara, Adriano Dekker, Mar-cus Ennis, Janna Hastings, Kenneth Haug, InmaculadaSpiteri, Steve Turner, and Christoph Steinbeck.
2010.Chemical Entities of Biological Interest: an update.
Nu-cleic Acids Res., 38:D249?D254.Kristina M. Hettne, Rob H. Stierum, Martijn J. Schuemie,Peter J.M.
Hendriksen, Bob J.A.
Schijvenaars, ErikM.
Van Mulligen, Jos Kleinjans, and Jan A. Kors.
2009.A dictionary to identify small molecules and drugs infree text.
Bioinformatics, 25(22):2983?2991.David M. Jessop, Sam E. Adams, Egon L. Willighagen,Lezan Hawizy, and Peter Murray-Rust.
2011.
OS-CAR4: a flexible architecture for chemical text-mining.J.
Cheminf., 3(1):41.Roman Klinger, Corinna Kola?rik, Juliane Fluck, MartinHofmann-Apitius, and Christoph M. Friedrich.
2008.Detection of IUPAC and IUPAC-like chemical names.In Proc.
of ISMB.
Bioinformatics, volume 24, pagesi268?i276.Roman Klinger and Katrin Tomanek.
2007.
ClassicalProbabilistic Models and Conditional Random Fields.Algorithm Engineering Report TR07-2-013.
Depart-ment of Computer Science, Dortmund University ofTechnology.
ISSN 1864-4503.Corinna Kola?rik, Roman Klinger, Christoph M. Friedrich,Martin Hofmann-Apitius, and Juliane Fluck.
2008.Chemical names: terminological resources and corporaannotation.
In Proc.
of the Workshop on Building andevaluating resources for biomedical text mining, pages51?58.Frank R. Kschischang, Brendan J. Frey, and Hans-AndreaLoeliger.
2001.
Factor graphs and the sum-productalgorithm.
IEEE Trans.
Inf.
Theory, 47(2):498?519.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields : Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProc.
of ICML-2001, pages 282?289.Robert Leaman and Graciela Gonzalez.
2008.
BANNER:an executable survey of advances in biomedical namedentity recognition.
In Proc.
of Pac Symp Biocomput,pages 652?663.Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-nie Skariah, Jian Yang, and Graciela Gonzalez.
2010.Towards internet-age pharmacovigilance: extracting ad-verse drug reactions from user posts to health-relatedsocial networks.
In Proc.
of Workshop BioNLP, pages117?125.
ACL.Daniel M. Lowe, Peter T. Corbett, Peter Murray-Rust, andRobert C. Glen.
2011.
Chemical name to structure:Opsin, an open source solution.
J. Chem.
Inf.
Model.,51(3):739?753.Andrew K. McCallum.
2002.
MALLET: AMachine Learning for Language Toolkit.http://mallet.cs.umass.edu.Andrew K. McCallum, Dayne Freitag, and Fernando.Pereira.
2000.
Maximum entropy Markov models forinformation extraction and segmentation.
In Proc.
ofICML-2000, pages 591?598.Andrew K. McCallum, Karl Schultz, and Sameer Singh.2009.
FACTORIE: Probabilistic programming via im-peratively defined factor graphs.
In Proc.
of NeuralInformation Processing Systems (NIPS).Phoebe M. Roberts and William S. Hayes.
2008.
Infor-mation needs and the role of text mining in drug de-velopment.
In Proc.
of Pac Symp Biocomput, pages592?603.Tim Rockta?schel, Michael Weidlich, and Ulf Leser.
2012.ChemSpot: A Hybrid System for Chemical Named En-tity Recognition.
Bioinformatics, 28(12):1633?1640.Sandeepkumar Satpal and Sunita Sarawagi.
2007.
Do-main adaptation of conditional probability modelsvia feature subsetting.
In Knowledge Discovery inDatabases: PKDD 2007, pages 224?235.
Springer.Martijn J. Schuemie, Rob Jelier, and Jan A. Kors.
2007.Peregrine: Lightweight gene name normalization bydictionary lookup.
In Proc.
of the Second BioCreativeChallenge, volume 2, pages 131?133.Ariel S. Schwartz and Marti A. Hearst.
2003.
A sim-ple algorithm for identifying abbreviation definitionsin biomedical text.
In Proc.
of Pac Symp Biocomput,volume 8, pages 451?462.Isabel Segura-Bedmar, Paloma Mart?
?nez, and Mar??aHerrero-Zazo.
2013.
Semeval-2013 task 9: Extractionof drug-drug interactions from biomedical texts.
In362Proc.
of the 7th International Workshop on SemanticEvaluation (SemEval 2013).Isabel Segura-Bedmar, Paloma Mart?
?nez, and Mar??aSegura-Bedmar.
2008.
Drug name recognition and clas-sification in biomedical texts.
A case study outliningapproaches underpinning automated systems.
DrugDiscovery Today, 13(17-18):816?823.Burr Settles.
2005.
ABNER: an open source tool forautomatically tagging genes, proteins and other entitynames in text.
Bioinformatics, 21(14):3191?3192.Philippe Thomas, Mariana Neves, Ille?s Solt, DomonkosTikk, and Ulf Leser.
2011.
Relation extraction for drug-drug interactions using ensemble learning.
In Proc.of the 1st Challenge Task on Drug-Drug InteractionExtraction 2011, pages 11?18.Hanna M. Wallach.
2004.
Conditional Random Fields: AnIntroduction.
Technical Report MS-CIS-04-21.
Depart-ment of Computer and Information Science, Universityof Pennsylvania.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5(2):241?259.363
