Automatic Detection and Language Identification of Multilingual DocumentsMarco Lui?
?, Jey Han Lau?
and Timothy Baldwin???
Department of Computing and Information SystemsThe University of Melbourne?
NICTA Victoria Research Laboratory?
Department of PhilosophyKing?s College Londonmhlui@unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.netAbstractLanguage identification is the task of automat-ically detecting the language(s) present in adocument based on the content of the docu-ment.
In this work, we address the problemof detecting documents that contain text frommore than one language (multilingual docu-ments).
We introduce a method that is able todetect that a document is multilingual, iden-tify the languages present, and estimate theirrelative proportions.
We demonstrate the ef-fectiveness of our method over synthetic data,as well as real-world multilingual documentscollected from the web.1 IntroductionLanguage identification is the task of automaticallydetecting the language(s) present in a documentbased on the content of the document.
Languageidentification techniques commonly assume that ev-ery document is written in one of a closed set ofknown languages for which there is training data,and is thus formulated as the task of selecting themost likely language from the set of training lan-guages.
In this work, we remove this monolingualassumption, and address the problem of languageidentification in documents that may contain textfrom more than one language from the candidate set.We propose a method that concurrently detects that adocument is multilingual, and estimates the propor-tion of the document that is written in each language.Detecting multilingual documents has a varietyof applications.
Most natural language processingtechniques presuppose monolingual input data, soinclusion of data in foreign languages introducesnoise, and can degrade the performance of NLP sys-tems (Alex et al., 2007; Cook and Lui, 2012).
Au-tomatic detection of multilingual documents can beused as a pre-filtering step to improve the quality ofinput data.
Detecting multilingual documents is alsoimportant for acquiring linguistic data from the web(Scannell, 2007; Abney and Bird, 2010), and hasapplications in mining bilingual texts for statisticalmachine translation from online resources (Resnik,1999; Nie et al., 1999; Ling et al., 2013).
There hasbeen particular interest in extracting text resourcesfor low-density languages from multilingual webpages containing both the low-density language andanother language such as English (Yamaguchi andTanaka-Ishii, 2012; King and Abney, 2013).
Kingand Abney (2013, p1118) specifically mention theneed for an automatic method ?to examine a mul-tilingual document, and with high accuracy, list thelanguages that are present in the document?.We introduce a method that is able to detect multi-lingual documents, and simultaneously identify eachlanguage present as well as estimate the propor-tion of the document written in that language.
Weachieve this with a probabilistic mixture model, us-ing a document representation developed for mono-lingual language identification (Lui and Baldwin,2011).
The model posits that each document is gen-erated as samples from an unknown mixture of lan-guages from the training set.
We introduce a Gibbssampler to map samples to languages for any givenset of languages, and use this to select the set of lan-guages that maximizes the posterior probability ofthe document.27Transactions of the Association for Computational Linguistics, 2 (2014) 27?40.
Action Editor: Kristina Toutanova.Submitted 1/2013; Revised 7/2013; Published 2/2014.
c?2014 Association for Computational Linguistics.Our method is able to learn a language identi-fier for multilingual documents from monolingualtraining data.
This is an important property as thereare no standard corpora of multilingual documentsavailable, whereas corpora of monolingual docu-ments are readily available for a reasonably largenumber of languages (Lui and Baldwin, 2011).
Wedemonstrate the effectiveness of our method empir-ically, firstly by evaluating it on synthetic datasetsdrawn from Wikipedia data, and then by applying itto real-world data, showing that we are able to iden-tify multilingual documents in targeted web crawlsof minority languages (King and Abney, 2013).Our main contributions are: (1) we present amethod for identifying multilingual documents, thelanguages contained therein and the relative propor-tion of the document in each language; (2) we showthat our method outperforms state-of-the-art meth-ods for language identification in multilingual doc-uments; (3) we show that our method is able to es-timate the proportion of the document in each lan-guage to a high degree of accuracy; and (4) we showthat our method is able to identify multilingual doc-uments in real-world data.2 BackgroundMost language identification research focuses onlanguage identification for monolingual documents(Hughes et al., 2006).
In monolingual LangID, thetask is to assign each documentD a unique languageLi ?
L. Some work has reported near-perfect accu-racy for language identification of large documentsin a small number of languages (Cavnar and Tren-kle, 1994; McNamee, 2005).
However, in order toattain such accuracy, a large number of simplifyingassumptions have to be made (Hughes et al., 2006;Baldwin and Lui, 2010a).
In this work, we tacklethe assumption that each document is monolingual,i.e.
it contains text from a single language.In language identification, documents are mod-eled as a stream of characters (Cavnar and Trenkle,1994; Kikui, 1996), often approximated by the cor-responding stream of bytes (Kruengkrai et al., 2005;Baldwin and Lui, 2010a) for robustness over vari-able character encodings.
In this work, we followBaldwin and Lui (2010a) in training a single modelfor languages that naturally use multiple encodings(e.g.
UTF8, Big5 and GB encodings for Chinese), asissues of encoding are not the focus of this research.The document representation used for languageidentification generally involves estimating the rel-ative distributions of particular byte sequences, se-lected such that their distributions differ betweenlanguages.
In some cases the relevant sequencesmay be externally specified, such as function wordsand common suffixes (Giguet, 1995) or grammati-cal word classes (Dueire Lins and Gonc?alves, 2004),though they are more frequently learned from la-beled data (Cavnar and Trenkle, 1994; Grefenstette,1995; Prager, 1999a; Lui and Baldwin, 2011).Learning algorithms applied to language identi-fication fall into two general categories: Bayesianclassifiers and nearest-prototype (Rocchio-style)classifiers.
Bayesian approaches include Markovprocesses (Dunning, 1994), naive Bayes methods(Grefenstette, 1995; Lui and Baldwin, 2011; Tiede-mann and Ljubes?ic?, 2012), and compressive mod-els (Teahan, 2000).
The nearest-prototype methodsvary primarily in the distance measure used, includ-ing measures based on rank order statistics (Cav-nar and Trenkle, 1994), information theory (Bald-win and Lui, 2010a), string kernels (Kruengkrai etal., 2005) and vector space models (Prager, 1999a;McNamee, 2005).Language identification has been applied in do-mains such as USENET messages (Cavnar andTrenkle, 1994), web pages (Kikui, 1996; Mar-tins and Silva, 2005; Liu and Liang, 2008), websearch queries (Ceylan and Kim, 2009; Bosca andDini, 2010), mining the web for bilingual text(Resnik, 1999; Nie et al., 1999), building minor-ity language corpora (Ghani et al., 2004; Scannell,2007; Bergsma et al., 2012) as well as a large-scale database of Interlinear Glossed Text (Xia et al.,2010), and the construction of a large-scale multilin-gual web crawl (Callan and Hoy, 2009).2.1 Multilingual DocumentsLanguage identification over documents that containtext from more than one language has been identifiedas an open research question (Hughes et al., 2006).Common examples of multilingual documents areweb pages that contain excerpts from another lan-guage, and documents from multilingual organiza-tions such as the European Union.28English French Italian German Dutch Japanesecharacter the pour di auf voo ?byte 74 68 65 20 70 6F 75 7 20 64 69 20 20 61 75 66 76 6F 6 E3 81 AFTable 1: Examples of per-language byte sequences selected by information gain.The Australiasian Language Technology Work-shop 2010 hosted a shared task where participantswere required to predict the language(s) present in aheld-out test set containing monolingual and bilin-gual documents (Baldwin and Lui, 2010b).
Thedataset was prepared using data from Wikipedia, andbilingual documents were produced using a segmentfrom a page in one language, and a segment from thesame page in another language.
We use the datasetfrom this shared task for our initial experiments.To the authors?
knowledge, the only other work todirectly tackle identification of multiple languagesand their relative proportions in a single document isthe LINGUINI system (Prager, 1999a).
The systemis based on a vector space model, and cosine simi-larity between a feature vector for the test documentand a feature vector for each language Li, computedas the sum of feature vectors for all the documentsfor language Li in the training data.
The elementsin the feature vectors are frequency counts overbyte n-grams (2?n?5) and words.
Language iden-tification for multilingual documents is performedthrough the use of virtual mixed languages.
Prager(1999a) shows how to construct vectors representa-tive of particular combinations of languages inde-pendent of the relative proportions, and proposes amethod for choosing combinations of languages toconsider for any given document.Language identification in multilingual docu-ments could also be performed by application of su-pervised language segmentation algorithms.
Givena system that can segment a document into la-beled monolingual segments, we can then extractthe languages present as well as the relative propor-tion of text in each language.
Several methods forsupervised language segmentation have been pro-posed.
Teahan (2000) proposed a system based ontext compression that identifies multilingual docu-ments by first segmenting the text into monolingualblocks.
Rehurek and Kolkus (2009) perform lan-guage segmentation by computing a relevance scorebetween terms and languages, smoothing across ad-joining terms and finally identifying points of transi-tion between high and low relevance, which are in-terpreted as boundaries between languages.
Yam-aguchi and Tanaka-Ishii (2012) use a minimum de-scription length approach, embedding a compressivemodel to compute the description length of text seg-ments in each language.
They present a linear-timedynamic programming solution to optimize the lo-cation of segment boundaries and language labels.3 MethodologyLanguage identification for multilingual documentsis a multi-label classification task, in which a doc-ument can be mapped onto any number of labelsfrom a closed set.
In the remainder of this paper,we denote the set of all languages by L. We de-note a document D which contains languages Lxand Ly as D ?
{Lx, Ly}, where Lx, Ly ?
L.We denote a document that does not contain a lan-guage Lx by D ?
{Lx}, though we generally omitall the languages not contained in the document forbrevity.
We denote classifier output using .
; e.g.D .
{La, Lb} indicates that document D has beenpredicted to contain text in languages La and Lb.3.1 Document Representation and FeatureSelectionWe represent each document D as a frequency dis-tribution over byte n-gram sequences such as thosein Table 1.
Each document is converted into a vectorwhere each entry counts the number of times a par-ticular byte n-gram is present in the document.
Thisis analogous to a bag-of-words model, where the vo-cabulary of ?words?
is a set of byte sequences thathas been selected to distinguish between languages.The exact set of features is selected from thetraining data using Information Gain (IG), aninformation-theoretic metric developed as a split-ting criterion for decision trees (Quinlan, 1993).
IG-based feature selection combined with a naive Bayesclassifier has been shown to be particularly effectivefor language identification (Lui and Baldwin, 2011).293.2 Generative Mixture ModelsGenerative mixture models are popular for text mod-eling tasks where a mixture of influences governs thecontent of a document, such as in multi-label doc-ument classification (McCallum, 1999; Ramage etal., 2009), and topic modeling (Blei et al., 2003).Such models normally assume full exchangeabilitybetween tokens (i.e.
the bag-of-words assumption),and label each token with a single discrete label.Multi-label text classification, topic modeling andour model for language identification in multilingualdocuments share the same fundamental representa-tion of the latent structure of a document.
Each la-bel is modeled with a probability distribution overtokens, and each document is modeled as a proba-bilistic mixture of labels.
As presented in Griffithsand Steyvers (2004), the probability of the ith token(wi) given a set of T labels z1?
?
?zT is modeled as:P (wi) =T?j=1P (wi|zi = j)P (zi = j) (1)The set of tokens w is the document itself, whichin all cases is observed.
In the case of topic model-ing, the tokens are words and the labels are topics,and z is latent.
Whereas topic modeling is gener-ally unsupervised, multi-label text classification isa supervised text modeling task, where the labelsare a set of pre-defined categories (such as RUBBER,IRON-STEEL, TRADE, etc.
in the popular Reuters-21578 data set (Lewis, 1997)), and the tokens areindividual words in documents.
z is still latent, butconstrained in the training data (i.e.
documents arelabeled but the individual words are not).
Some ap-proaches to labeling unseen documents require thatz for the training data be inferred, and methods fordoing this include an application of the Expectation-Maximization (EM) algorithm (McCallum, 1999)and Labeled LDA (Ramage et al., 2009).The model that we propose for language identifi-cation in multilingual documents is similar to multi-label text classification.
In the framework of Equa-tion 1, each per-token label zi is a language and thevocabulary of tokens is not given by words but ratherby specific byte sequences (Section 3.1).
The keydifference with multi-label text classification is thatwe use monolingual (i.e.
mono-label) training data.Hence, z is effectively observed for the training data(since all tokens must share the same label).
To inferz for unlabeled documents, we utilize a Gibbs sam-pler, closely related to that proposed by Griffiths andSteyvers (2004) for LDA.
The sampling probabilityfor a label zi for token w in a document d is:P (zi = j|z?i, w) ?
?
(w)j ?
?
(d)j (2)?
(w)j = P (wi|zi = j, z?i, w?i)?
(d)j = P (zi = j|z?i)In the LDA model, ?
(d)j is assumed to have a Dirich-let distribution with hyperparameter ?, and the worddistribution for each topic ?
(w)j is also assumed tohave a Dirichlet distribution with hyperparameter?.
Griffiths (2002) describes a generative model forLDA where both ?
(w)j and ?
(d)j are inferred fromthe output of a Gibbs sampler.
In our method, weestimate ?
(w)j using maximum likelihood estima-tion (MLE) from the training data.
Estimating ?
(w)jthrough MLE is equivalent to a multinomial NaiveBayes model (McCallum and Nigam, 1998):??
(w)j =n(w)j + ?n(.
)j +W?
(3)where n(w)j is the number of times word w occurswith label j, and n(.
)j is the total number of wordsthat occur with label j.
By setting ?
to 1, we obtainstandard Laplacian smoothing.
Hence, only ??
(d)j isupdated at each step in the Gibbs sampler:??
(d)j =n(d)?i,j + ?n(d)?i + T?
(4)where n(d)?i,j is the number of tokens in document dthat are currently mapped to language j, and n(d)?i isthe total number of tokens in document d. In bothcases, the current assignment of zi is excluded fromthe count.
T is the number of languages (i.e.
the sizeof the label set).
For simplicity, we set ?
to 0.
Wenote that in the LDA model, ?
and ?
influence thesparsity of the solution, and so it may be possibleto tune these parameters for our model as well.
Weleave this as an avenue for further research.303.3 Language Identification in MultilingualDocumentsThe model described in Section 3.2 can be used tocompute the most likely distribution to have gen-erated an unlabeled document over a given set oflanguages for which we have monolingual trainingdata, by letting the set of terms w be the byte n-gramsequences we selected using per-language informa-tion gain (Section 3.1), and allowing the labels z torange over the set of all languages L. Using train-ing data, we compute ??
(w)j (Equation 3), and thenwe infer P (Lj |D) for each Lj ?
L for the unla-beled document, by running the Gibbs sampler untilthe samples for zi converge and then tabulating ziover the whole d and normalizing by |d|.
Naively,we could identify the languages present in the doc-ument by D .
{Lx if ?
(zi = Lx|D)}, but closely-related languages tend to have similar frequency dis-tributions over byte n-gram features, and hence it islikely that some tokens will be incorrectly mapped toa language that is similar to the ?correct?
language.We address this issue by finding the subset of lan-guages ?
from the training set L that maximizesP (?|D) (a similar approach is taken in McCallum(1999)).
Through an application of Bayes?
theorem,P (?|D) ?
P (D|?
)?P (?
), noting that P (D) is anormalizing constant and can be dropped.
We as-sume that P (?)
is constant (i.e.
any subset of lan-guages is equally likely, a reasonable assumption inthe absence of other evidence), and hence maximizeP (D|?).
For any given D = w1?
?
?wn and ?, weinfer P (D|?)
from the output of the Gibbs sampler:P (D|?)
=N?i=1P (wi|?)
(5)=N?i=1?j?
?P (wi|zi = j)P (zi = j) (6)where both P (wi|zi = j) and P (zi = j) are esti-mated by their maximum likelihood estimates.In practice, exhaustive evaluation of the powersetof L is prohibitively expensive, and so we greed-ily approximate the optimal ?
using Algorithm 1.
Inessence, we initially rank all the candidate languagesby computing the most likely distribution over thefull set of candidate languages.
Then, for each ofthe top-N languages in turn, we consider whetherAlgorithm 1 DetectLang(L,D)LN ?
top-N z ?
L by P (z|D)??
{Lu}for each Lt ?
LN do??
?
?
?
Ltif P (D|?)
+ t < P (D|??)
then??
?
?end ifend for??
?
\ {Lu}return D .
?to add it to ?.
?
is initialized with Lu, a dummylanguage with a uniform distribution over terms (i.e.P (w|Lu) = 1|w| ).
A language is added if it improvesP (D|?)
by at least t. The threshold t is requiredto suppress the addition of spurious classes.
Addinglanguages gives the model additional freedom to fitparameters, and so will generally increase P (D|?
).In the limit case, adding a completely irrelevant lan-guage will result in no tokens being mapped to thea language, and so the model will be no worse thanwithout the language.
The threshold t is thus used tocontrol ?how much?
improvement is required beforeincluding the new language in ?.3.4 Benchmark ApproachesWe compare our approach to two methods forlanguage identification in multilingual documents:(1) the virtual mixed languages approach (Prager,1999a); and (2) the text segmentation approach (Ya-maguchi and Tanaka-Ishii, 2012).Prager (1999a) describes LINGUINI, a languageidentifier based on the vector-space model com-monly used in text classification and information re-trieval.
The document representation used by Prager(1999a) is a vector of counts across a set of charac-ter sequences.
Prager (1999a) selects the feature setbased on a TFIDF-like approach.
Terms with occur-rence count m < n?
k are rejected, where m is thenumber of times the term occurs in the training data(the TF component), n is the number of languages inwhich the term occurred (the IDF component, where?document?
is replaced with ?language?
), and k is aparameter to control the overall number of terms se-lected.
In Prager (1999a), the value of k is reportedto be optimal in the region 0.3 to 0.5.
In practice,31the value of k indirectly controls the number of fea-tures selected.
Values of k are not comparable acrossdatasets as m is not normalized for the size of thetraining data, so in this work we do not report thevalues of k and instead directly select the top-N fea-tures, weighted by mn .
In LINGUINI, each languageis modeled as a single pseudo-document, obtainedby concatenating all the training data for the givenlanguage.
A document is then classified accordingto the vector with which it has the smallest angle;this is implemented by finding the language vectorwith the highest cosine with the document vector.Prager (1999a) also proposes an extension to theapproach to allow identification of bilingual docu-ments, and suggests how this may be generalized toany number of languages in a document.
The gistof the method is simple: for any given pair of lan-guages, the projection of a document vector ontothe hyperplane containing the language vectors ofthe two languages gives the mixture proportions ofthe two languages that minimizes the angle with thedocument vector.
Prager (1999a) terms this projec-tion a virtual mixed language (VML), and showshow to find the angle between the document vec-tor and the VML.
If this angle is less than that be-tween the document vector and any individual lan-guage vector, the document is labeled as bilingual inthe two languages from which the mixed vector wasderived.
The practical difficulty presented by thisapproach is that exhaustively evaluating all possiblecombinations of languages is prohibitively expen-sive.
Prager (1999a) addresses this by arguing that inmultilingual documents, ?the individual componentlanguages will be close to d (the document vector)?
probably closer than most or all other languages?.Hence, language mixtures are only considered forcombinations of the top m languages.Prager (1999a) shows how to obtain the mixturecoefficients for bilingual VMLs, arguing that theprocess generalizes.
Prager (1999b) includes thecoefficients for 3-language VMLs, which are muchmore complex than the 2-language variants.
Us-ing a computer algebra system, we verified the an-alytic forms of the coefficients in the 3-languageVML.
We also attempted to obtain an analytic formfor the coefficients in a 4-language VML, but thesewere too complex for the computer algebra systemto compute.
Thus, our evaluation of the VML ap-proach proposed by Prager (1999a) is limited to 3-language VMLs.
Neither Prager (1999a) nor Prager(1999b) include an empirical evaluation over mul-tilingual documents, so to the best of our knowl-edge this paper is the first empirical evaluation ofthe method on multilingual documents.
As no refer-ence implementation of this method is available, wehave produced our own implementation, which wehave made freely available.1The other benchmark we consider in this paper isthe method for text segmentation by language pro-posed by Yamaguchi and Tanaka-Ishii (2012) (here-after referred to as SEGLANG).
The actual task ad-dressed by Yamaguchi and Tanaka-Ishii (2012) is todivide a document into monolingual segments.
Thisis formulated as the task of segmenting a documentD = x1, ?
?
?
, x|D| (where xi denotes the ith char-acter of D and |D| is the length of the document)by finding a list of boundaries B = [B1, ?
?
?
, B|B|]where each Bi indicates the location of a languageboundary as an offset from the start of the document,resulting in a list of segments X = [X0, ?
?
?
, X|B|].For each segment Xi, the system predicts Li, thelanguage associated with the segment, producing alist of labellings L = [L0, ?
?
?
, L|B|], with the con-straint that adjacent elements in L must differ.
Ya-maguchi and Tanaka-Ishii (2012) solve the problemof determining X and L for an unlabeled text us-ing a method based on minimum description length.They present a dynamic programming solution tothis problem, and analyze a number of parametersthat affect the overall accuracy of the system.
Giventhis method to determine X and L, it is then triv-ial to label an unlabeled document according toD .
{Lx if ?Lx ?
L}, and the length of each seg-ment in X can then be used to determine the pro-portions of the document that are in each language.In this work, we use a reference implementation ofSEGLANG kindly provided to us by the authors.Using the text segmentation approach ofSEGLANG to detect multilingual documents differsfrom LINGUINI and our method primarily in thatLINGUINI and our method fragment the documentinto small sequences of bytes, and discard informa-tion about the relative order of the fragments.
Thisis in contrast to SEGLANG, where this information1https://github.com/saffsd/linguini.py32System PM RM FM P?
R?
F?Benchmark .497 .467 .464 .833 .826 .829Winner .718 .703 .699 .932 .931 .932SEGLANG .801 .810 .784 .866 .946 .905LINGUINI .616 .535 .513 .713 .688 .700Our method .753 .771 .748 .945 .922 .933Table 2: Results on the ALTW2010 dataset.?Benchmark?
is the benchmark system proposed bythe shared task organizers.
?Winner?
is the highest-F?
system submitted to the shared task.is utilized in the sequential prediction of labels forconsecutive segments of text, and is thus able tomake better use of the locality of text (since there arelikely to be monolingual blocks of text in any givenmultilingual document).
The disadvantage of this isthat the underlying model becomes more complexand hence more computationally expensive, as weobserve in Section 5.3.5 EvaluationWe seek to evaluate the ability of each method:(1) to correctly identify the language(s) present ineach test document; and (2) for multilingual doc-uments, to estimate the relative proportion of thedocument written in each language.
In the first in-stance, this is a classification problem, and the stan-dard notions of precision (P), recall (R) and F-score(F) apply.
Consistent with previous work in lan-guage identification, we report both the document-level micro-average, as well as the language-levelmacro-average.
For consistency with Baldwin andLui (2010a), the macro-averaged F-score we reportis the average of the per-class F-scores, rather thanthe harmonic mean of the macro-averaged precisionand recall; as such, it is possible for the F-scoreto not fall between the precision and recall values.As is common practice, we compute the F-score for?
= 1, giving equal importance to precision andrecall.2 We tested the difference in performancefor statistical significance using an approximate ran-domization procedure (Yeh, 2000) with 10000 iter-ations.
Within each table of results (Tables 2, 3 and2Intuitively, it may seem that the maximal precision and re-call should be achieved when precision and recall are balanced.However, because of the multi-label nature of the task and vari-able number of labels assigned to a given document by our mod-els, it is theoretically possible and indeed common in our resultsfor the maximal macro-averaged F-score to be achieved whenmacro-averaged precision and recall are not balanced.4), all differences between systems are statisticallysignificant at a p < 0.05 level.To evaluate the predictions of the relative propor-tions of a document D written in each detected lan-guageLi, we compare the topic proportion predictedby our model to the gold-standard proportion, mea-sured as a byte ratio as follows:gs(Li|D) =length of Li part of D in byteslength of D in bytes (7)We report the correlation between predicted and ac-tual proportions in terms of Pearson?s r coefficient.We also report the mean absolute error (MAE) overall document?language pairs.4 Experiments on ALTW2010Our first experiment utilizes the ALTW2010 sharedtask dataset (Baldwin and Lui, 2010b), a syntheticdataset of 10000 bilingual documents3 generatedfrom Wikipedia data, introduced in the ALTW2010shared task,4 The dataset is organized into training,development and test partitions.
Following standardmachine learning practice, we train each system us-ing the training partition, and tune parameters usingthe development partition.
We then report macro andmicro-averaged precision, recall and F-score on thetest partition, using the tuned parameters.The results on the ALTW2010 shared task datasetare summarized in Table 2.
Each of the three sys-tems we compare was re-trained using the trainingdata provided for the shared task, with a slight dif-ference: in the shared task, participants were pro-vided with multilingual training documents, but thesystems targeted in this research require monolin-gual training data.
We thus split the training doc-uments into monolingual segments using the meta-data provided with the dataset.
The metadata wasonly published after completion of the task and wasnot available to task participants.
For comparison,we have included the benchmark results publishedby the shared task organizers, as well as the scoreattained by the winning entry (Tran et al., 2010).3With a small number of monolingual documents, formedby randomly selecting the two languages for a given docu-ment independently, leaving the possibility of the same two lan-guages being selected.4http://comp.mq.edu.au/programming/task_description/33We tune the parameters for each system using thedevelopment partition of the dataset, and report re-sults on the test partition.
For LINGUINI, there is asingle parameter k to be tuned: the number of fea-tures per language.
We tested values between 10000and 50000, and selected 46000 features as the opti-mal value.
For our method, there are two parametersto be tuned: (1) the number of features selected foreach language, and (2) the threshold t for includinga language.
We tested features-per-language countsbetween 30 and 150, and found that adding featuresbeyond 70 per language had minimal effect.
Wetested values of the threshold t from 0.01 to 0.15,and found the best value was 0.14.
For SEGLANG,we introduce a threshold t on the minimum propor-tion of a document (measured in bytes) that mustbe labeled by a language before that language is in-cluded in the output set.
This was done because ourinitial experiments indicate that SEGLANG tends toover-produce labels.
Using the development data,we found the best value of t was 0.10.We find that of the three systems tested, two out-perform the winning entry to the shared task.
Thisis more evident in the macro-averaged results thanin the micro-averaged results.
In micro-averagedterms, our method is the best performer, whereason the macro-average, SEGLANG has the high-est F-score.
This suggests that our method doeswell on higher-density languages (relative to theALTW2010 dataset), and poorly on lower-densitylanguages.
This also accounts for the higher micro-averaged precision but lower micro-averaged recallfor our method as compared to SEGLANG.
The im-proved macro-average F-score of SEGLANG comesat a much higher computational cost, which in-creases dramatically as the number of languages isincreased.
In our testing on a 16-core worksta-tion, SEGLANG took almost 24 hours to process theALTW2010 shared task test data, compared to 2minutes for our method and 40 seconds for LIN-GUINI.
As such, SEGLANG is poorly suited to de-tecting multilingual documents where a large num-ber of candidate languages is considered.The ALTW2010 dataset is an excellent startingpoint for this research, but it predominantly containsbilingual documents, making it difficult to assess theability of systems to distinguish multilingual docu-ments from monolingual ones.
Furthermore, we areunable to use it to assess the ability of systems todetect more than 2 languages in a document.
To ad-dress these shortcomings, we construct a new datasetin a similar vein.
The dataset and experiments per-formed on it are described in the next section.5 Experiments on WIKIPEDIAMULTITo fully test the capabilities of our model, we gen-erated WIKIPEDIAMULTI, a dataset that containsa mixture of monolingual and multilingual docu-ments.
To allow for replicability of our results andto facilitate research in language identification, wehave made the dataset publicly available.5 WIKI-PEDIAMULTI is generated using excerpts from themediawiki sources of Wikipedia pages downloadedfrom the Wikimedia foundation.6 The dumps weused are from July?August 2010.To generate WIKIPEDIAMULTI, we first normal-ized the raw mediawiki documents.
Mediawiki doc-uments typically contain one paragraph per line, in-terspersed with structural elements.
We filtered eachdocument to remove all structural elements, andonly kept documents that exceeded 2500 bytes afternormalization.
This yielded a collection of around500,000 documents in 156 languages.
From thisinitial document set (hereafter referred to as WI-KICONTENT), we only retained languages that hadmore than 1000 documents (44 languages), and gen-erated documents for WIKIPEDIAMULTI as follows:1. randomly select the number of languages K(1?K?5)2. randomly select a set of K languages S ={Li?L for i = 1?
?
?K} without replacement3.
randomly select a document for each Li?Sfrom WIKICONTENT without replacement4.
take the top 1K lines of the document5.
join the K sections into a single document.As a result of the procedure, the relative propor-tion of each language in a multilingual documenttends not to be uniform, as it is conditioned on thelength of the original document from which it wassourced, independent of the otherK?1 for the otherlanguages that it was combined with.
Overall, theaverage document length is 5500 bytes (standard de-viation = 3800 bytes).
Due to rounding up in taking5http://www.csse.unimelb.edu.au/?tim/6http://dumps.wikimedia.org34System PM RM FM P?
R?
F?SEGLANG .809 .975 .875 .771 .975 .861LINGUINI .853 .772 .802 .838 .774 .805Our method .962 .954 .957 .963 .955 .959Table 3: Results on the WIKIPEDIAMULTI dataset.the top 1k lines (step 4), documents with higher Ktend to be longer (6200 bytes for K = 5 vs 5100bytes for K = 1).The WIKIPEDIAMULTI dataset contains training,development and test partitions.
The training parti-tion consists of 5000 monolingual (i.e.
K = 1) doc-uments.
The development partition consists of 5000documents, 1000 documents for each value of Kwhere 1?K?5.
The test partition contains 200 doc-uments for each K, for a total of 1000 documents.There is no overlap between any of the partitions.5.1 Results over WIKIPEDIAMULTIWe trained each system using the monolingual train-ing partition, and tuned parameters using the devel-opment partition.
For LINGUINI, we tested featurecounts between 10000 and 50000, and found thatthe effect was relatively small.
We thus use 10000features as the optimum value.
For SEGLANG, wetested values for threshold t between 0.01 and 0.20,and found that the maximal macro-averaged F-scoreis attained when t = 0.06.
Finally, for our methodwe tested features-per-language counts between 30and 130 and found the best performance with 120features per language, although the actual effect ofvarying this value is rather small.
We tested valuesof the threshold t for adding an extra language to?
from 0.01 to 0.15, and found that the best resultswere attained when t = 0.02.The results of evaluating each system on thetest partition are summarized in Table 3.
In thisevaluation, our method clearly outperforms bothSEGLANG and LINGUINI.
The results on WIKI-PEDIAMULTI and ALTW2010 are difficult to com-pare directly due to the different compositions of thetwo datasets.
ALTW2010 is predominantly bilin-gual, whereas WIKIPEDIAMULTI contains docu-ments with text in 1?5 languages.
Furthermore, theaverage document in ALTW2010 is half the lengthof that in WIKIPEDIAMULTI.
Overall, we observethat SEGLANG has a tendency to over-label (despitethe introduction of the t parameter to reduce this ef-fect), evidenced by high recall but lower precision.LINGUINI is inherently limited in that it is only ableto detect up to 3 languages per document, causingrecall to suffer on WIKIPEDIAMULTI.
However, italso tends to always output 3 languages, regardlessof the actual number of languages in the document,hurting precision.
Furthermore, even on ALTW2010it has lower recall than the other two systems.6 Estimating Language ProportionsIn addition to detecting multiple languages withina document, our method also estimates the relativeproportions of the document that are written in eachlanguage.
This information may be useful for detect-ing documents that are candidate bitexts for trainingmachine translation systems, since we may expectlanguages in the document to be present in equalproportions.
It also allows us to identify the pre-dominant language of a document.A core element of our model of a document isa distribution over a set of labels.
Since each la-bel corresponds to a language, as a first approxima-tion, we take the probability mass associated witheach label as a direct estimate of the proportion ofthe document written in that language.
We examinethe results for predicting the language proportionsin the test partition of WIKIPEDIAMULTI.
Mappinglabel distributions directly to language proportionsproduces excellent results, with a Pearson?s r valueof 0.863 and an MAE of 0.108.Although labels have a one-to-one correspon-dence with languages, the label distribution doesnot actually correspond directly to the language pro-portion, because the distribution estimates the pro-portion of byte n-gram sequences associated witha label and not the proportion of bytes directly.The same number of bytes in different languagescan produce different numbers of n-gram sequences,because after feature selection not all n-gram se-quences are retained in the feature set.
Hereafter,we refer to each n-gram sequence as a token, and theaverage number of tokens produced per byte of textas the token emission rate.We estimate the per-language token emission rate(Figure 1) using the training partition of WIKIPE-DIAMULTI.
To improve our estimate of the lan-guage proportions, we correct our label distribution35Original text the cat in the hatn-gram features??????
?he : 2 the : 2hat : 1 in : 1th : 1 the : 1hat : 1 he c : 1in t : 1 n th : 1??????
?Emission rate #bytes#tokens = 1812 = 1.5 bytes/tokenFigure 1: Example of calculating n-gram emissionrate for a text string.using estimates of the per-language token emissionrate RLi in bytes per token for Li?L.
Assume thata document D of length |D| is estimated to containK languages in proportions Pi for i = 1?
?
?K.
Thecorrected estimate for the proportion of Li is:Prop(Li) =Pi ?RLi?Kj=1 (Pj ?RLj )(8)Note that the |D| term is common to the numeratorand denominator and has thus been eliminated.This correction improves our estimates of lan-guage proportions.
After correction, the Pearson?sr rises to 0.981, and the MAE is reduced to 0.024.The improvement is most noticeable for language?document pairs where the proportion of the docu-ment in the given language is about 0.5 (Figure 2).7 Real-world Multilingual DocumentsSo far, we have demonstrated the effectiveness ofour proposed approach using synthetic data.
Theresults have been excellent, and in this section wevalidate the approach by applying it to a real-worldtask that has recently been discussed in the lit-erature.
Yamaguchi and Tanaka-Ishii (2012) andKing and Abney (2013) both observe that in tryingto gather linguistic data for ?non-major?
languagesfrom the web, one challenge faced is that documentsretrieved often contain sections in another language.SEGLANG (the solution of Yamaguchi and Tanaka-Ishii (2012)) concurrently detects multilingual doc-uments and segments them by language, but the ap-proach is computationally expensive and has a ten-dency to over-label (Section 5).
On the other hand,the solution of King and Abney (2013) is incom-plete, and they specifically mention the need for anautomatic method ?to examine a multilingual docu-ment, and with high accuracy, list the languages thatare present in the document?.
In this section, weshow that our method is able to fill this need.
WeSystem P R FBaseline 0.719 1.00 0.837SEGLANG 0.779 0.991 0.872LINGUINI 0.729 0.981 0.837Our method 0.907 0.916 0.912Table 4: Detection accuracy for English-languageinclusion in web documents from targeted webcrawls for low-density languages.make use of manually-annotated data kindly pro-vided to us by Ben King, which consists of 149 doc-uments containing 42 languages retrieved from theweb using a set of targeted queries for low-densitylanguages.
Note that the dataset described in Kingand Abney (2013) was based on manual confirma-tion of the presence of English in addition to the low-density language of primary interest; our datasetcontains these bilingual documents as well as mono-lingual documents in the low-density language of in-terest.
Our purpose in this section is to investigatethe ability of automatic systems to select this subsetof bilingual documents.
Specifically, given a col-lection of documents retrieved for a target language,the task is to identify the documents that contain textin English in addition to the target language.
Thus,we re-train each system for each target language, us-ing only training data for English and the target lan-guage.
We reserve the data provided by Ben Kingfor evaluation, and train our methods using data sep-arately obtained from the Universal Declaration ofHuman Rights (UDHR).
Where UDHR translationsfor a particular language were not available, we useddata from Wikipedia or from a bible translation.
Ap-proximately 20?80 kB of data were used for eachlanguage.
As we do not have suitable developmentdata, we made use of the best parameters for eachsystem from the experiments on WIKIPEDIAMULTI.We find that all 3 systems are able to detect thateach document contains the target language with100% accuracy.
However, systems vary in their abil-ity to detect if a document also contains English inaddition to the target language.
The detection accu-racy for English-language inclusion is summarizedin Table 4.7 For comparison, we include a heuristicbaseline based on labeling all documents as contain-7Note that Table 2 and Table 3 both report macro and micro-averaged results across a number of languages.
In contrast Ta-ble 4 only reports results for English, and the values are notdirectly comparable to our earlier evaluation.360.2 0.4 0.6 0.8 1.0Actual Proportion0.20.40.60.81.0PredictedProportionPearson's r: 0.863MAE: 0.108(a) without emission rate correction0.2 0.4 0.6 0.8 1.0Actual Proportion0.20.40.60.81.0PredictedProportionPearson's r: 0.981MAE: 0.0241(b) with emission rate correctionFigure 2: Scatterplot of the predicted vs. actual language proportions in a document for the test partition ofWIKIPEDIAMULTI (predictions are from our method; each point corresponds to a document-language pair).ing English.
We find that, like the heuristic base-line, SEGLANG and LINGUINI both tend to over-label documents, producing false positive labels ofEnglish, resulting in increased recall at the expenseof precision.
Our method produces less false pos-itives (but slightly more false negatives).
Overall,our method attains the best F for detecting En-glish inclusions.
Manual error analysis suggests thatthe false negatives for our method generally occurwhere a relatively small proportion of the documentis written in English.8 Future WorkDocument segmentation by language could be ac-complished by a combination of our method and themethod of King and Abney (2013), which could becompared to the method of Yamaguchi and Tanaka-Ishii (2012) in the context of constructing corporafor low-density languages using the web.
Anotherarea we have identified in this paper is the tuningof the parameters ?
and ?
in our model (currently?
= 0 and ?
= 1), which may have some effect onthe sparsity of the model.Further work is required in dealing with cross-domain effects, to allow for ?off-the-shelf?
languageidentification in multilingual documents.
Previouswork has shown that it is possible to generate a docu-ment representation that is robust to variation acrossdomains (Lui and Baldwin, 2011), and we intend toinvestigate if these results are also applicable to lan-guage identification in multilingual documents.
An-other open question is the extension of the genera-tive mixture models to ?unknown?
language identi-fication (i.e.
eliminating the closed-world assump-tion (Hughes et al., 2006)), which may be possiblethrough the use of non-parametric mixture modelssuch as Hierarchical Dirichlet Processes (Teh et al.,2006).9 ConclusionWe have presented a system for language identifi-cation in multilingual documents using a generativemixture model inspired by supervised topic model-ing algorithms, combined with a document represen-tation based on previous research in language iden-tification for monolingual documents.
We showedthat the system outperforms alternative approachesfrom the literature on synthetic data, as well as onreal-world data from related research on linguisticcorpus creation for low-density languages using theweb as a resource.
We also showed that our systemis able to accurately estimate the proportion of thedocument written in each of the languages identi-fied.
We have made a full reference implementationof our system freely available,8 as well as the syn-thetic dataset prepared for this paper (Section 5), inorder to facilitate the adoption of this technology andfurther research in this area.8https://github.com/saffsd/polyglot37AcknowledgmentsWe thank Hiroshi Yamaguchi for making a referenceimplementation of SEGLANG available to us, andBen King for providing us with a collection of real-world multilingual web documents.
This work wassubstantially improved as a result of the insightfulfeedback received from the reviewers.NICTA is funded by the Australian Governmentas represented by the Department of Broadband,Communications and the Digital Economy and theAustralian Research Council through the ICT Cen-tre of Excellence program.ReferencesSteven Abney and Steven Bird.
2010.
The humanlanguage project: building a universal corpus of theworld?s languages.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 88?97.
Association for Computational Lin-guistics.Beatrice Alex, Amit Dubey, and Frank Keller.
2007.Using foreign inclusion detection to improve parsingperformance.
In Proceedings of the Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,Czech Republic.Timothy Baldwin and Marco Lui.
2010a.
Languageidentification: The long and the short of the matter.
InProceedings of Human Language Technologies: The11th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL HLT 2010), pages 229?237, Los Angeles,USA.Timothy Baldwin and Marco Lui.
2010b.
Multilin-gual language identification: ALTW 2010 shared taskdataset.
In Proceedings of the Australasian LanguageTechnology Workshop 2010 (ALTW 2010), pages 5?7,Melbourne, Australia.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific Twittercollections.
In Proceedings the Second Workshop onLanguage in Social Media (LSM2012), pages 65?74,Montre?al, Canada.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet allocation.
Journal of MachineLearning Research, 3:993?1022.Alessio Bosca and Luca Dini.
2010.
Language identi-fication strategies for cross language information re-trieval.
In Working Notes of the Cross Language Eval-uation Forum (CLEF).Jamie Callan and Mark Hoy, 2009.
ClueWeb09Dataset.
Available at http://boston.lti.cs.cmu.edu/Data/clueweb09/.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of theThird Symposium on Document Analysis and Informa-tion Retrieval, pages 161?175, Las Vegas, USA.Hakan Ceylan and Yookyung Kim.
2009.
Languageidentification of search engine queries.
In Proceedingsof the Joint Conference of the 47th Annual Meetingof the ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages1066?1074, Singapore.Paul Cook and Marco Lui.
2012. langid.py for bet-ter language modelling.
In Proceedings of the Aus-tralasian Language Technology Association Workshop2012, pages 107?112, Dunedin, New Zealand.Rafael Dueire Lins and Paulo Gonc?alves.
2004.
Au-tomatic language identification of written texts.
InProceedings of the 2004 ACM Symposium on AppliedComputing (SAC 2004), pages 1128?1133, Nicosia,Cyprus.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical Report MCCS 940-273, ComputingResearch Laboratory, New Mexico State University.Rayid Ghani, Rosie Jones, and Dunja Mladenic.
2004.Building minority language corpora by learning togenerate web search queries.
Knowledge and Infor-mation Systems, 7(1):56?83.Emmanuel Giguet.
1995.
Categorisation according tolanguage: A step toward combining linguistic knowl-edge and statistical learning.
In Proceedings of the4th International Workshop on Parsing Technologies(IWPT-1995), Prague, Czech Republic.Gregory Grefenstette.
1995.
Comparing two languageidentification schemes.
In Proceedings of AnalisiStatistica dei Dati Testuali (JADT), pages 263?268,Rome, Italy.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Thomas Griffiths.
2002.
Gibbs sampling in the gener-ative model of latent Dirichlet allocation.
TechnicalReport, Stanford University.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006.
Recon-sidering language identification for written languageresources.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC 2006), pages 485?488, Genoa, Italy.38Genitiro Kikui.
1996.
Identifying the coding systemand language of on-line documents on the internet.
InProceedings of the 16th International Conference onComputational Linguistics (COLING ?96), pages 652?657, Kyoto, Japan.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents usingweakly supervised methods.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1110?1119, At-lanta, Georgia.Canasai Kruengkrai, Prapass Srichaivattana, VirachSornlertlamvanich, and Hitoshi Isahara.
2005.
Lan-guage identification based on string kernels.
In Pro-ceedings of the 5th International Symposium on Com-munications and Information Technologies (ISCIT-2005), pages 896?899, Beijing, China.David D. Lewis.
1997.
The Reuters-21578 data set.available at http://www.daviddlewis.com/resources/testcollections/reuters21578/.Wang Ling, Guang Xiang, Chris Dyer, Alan Black, andIsabel Trancoso.
2013.
Microblogs as parallel cor-pora.
In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 176?186, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Jicheng Liu and Chunyan Liang.
2008.
Text Categoriza-tion of Multilingual Web Pages in Specific Domain.In Proceedings of the 12th Pacific-Asia Conference onAdvances in Knowledge Discovery and Data Mining,PAKDD?08, pages 938?944, Osaka, Japan.Marco Lui and Timothy Baldwin.
2011.
Cross-domainfeature selection for language identification.
In Pro-ceedings of the 5th International Joint Conference onNatural Language Processing (IJCNLP 2011), pages553?561, Chiang Mai, Thailand.Bruno Martins and Ma?rio J. Silva.
2005.
Language iden-tification in web pages.
In Proceedings of the 2005ACM symposium on Applied computing, pages 764?768, Santa Fe, USA.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for Naive Bayes text classifi-cation.
In Proceedings of the AAAI-98 Workshop onLearning for Text Categorization, pages Available asTechnical Report WS?98?05, AAAI Press., Madison,USA.Andrew Kachites McCallum.
1999.
Multi-label textclassification with a mixture model trained by EM.
InProceedings of AAAI 99 Workshop on Text Learning.Paul McNamee.
2005.
Language identification: a solvedproblem suitable for undergraduate instruction.
Jour-nal of Computing Sciences in Colleges, 20(3):94?101.Jian-Yun Nie, Michel Simard, Pierre Isabelle, andRichard Durand.
1999.
Cross-language informationretrieval based on parallel texts and automatic min-ing of parallel texts from the web.
In Proceedingsof 22nd International ACM-SIGIR Conference on Re-search and Development in Information Retrieval (SI-GIR?99), pages 74?81, Berkeley, USA.John M. Prager.
1999a.
Linguini: language identificationfor multilingual documents.
In Proceedings the 32ndAnnual Hawaii International Conference on SystemsSciences (HICSS-32), Maui, Hawaii.John M. Prager.
1999b.
Linguini: Language identifica-tion for multilingual documents.
Journal of Manage-ment Information Systems, 16(3):71?101.John Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, USA.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: Asupervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2009), pages 248?256, Singapore.Radim Rehurek and Milan Kolkus.
2009.
LanguageIdentification on the Web: Extending the DictionaryMethod.
In Proceedings of Computational Linguis-tics and Intelligent Text Processing, 10th InternationalConference (CICLing 2009), pages 357?368, MexicoCity, Mexico.Philip Resnik.
1999.
Mining the Web for bilingual text.In Proceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 527?534,College Park, USA.Kevin P Scannell.
2007.
The Cru?bada?n Project: Cor-pus building for under-resourced languages.
In Build-ing and Exploring Web Corpora: Proceedings of the3rd Web as Corpus Workshop, pages 5?15, Louvain-la-Neuve, Belgium.W.
J. Teahan.
2000.
Text Classification and Seg-mentation Using Minimum Cross-Entropy.
In Pro-ceedings the 6th International Conference ?Recherched?Information Assistee par Ordinateur?
(RIAO?00),pages 943?961, Paris, France.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581.Jo?rg Tiedemann and Nikola Ljubes?ic?.
2012.
Efficientdiscrimination between closely related languages.
InProceedings of the 24th International Conference onComputational Linguistics (COLING 2012), pages2619?2634, Mumbai, India.Giang Binh Tran, Dat Ba Nguyen, and Bin ThanhKieu.
2010.
N-gram based approach for mul-tilingual language identification.
poster.
available39at http://comp.mq.edu.au/programming/task_description/VILangTek.pdf.Fei Xia, Carrie Lewis, and William D. Lewis.
2010.
Lan-guage ID for a thousand languages.
In LSA AnnualMeeting Extended Abstracts, Baltimore,USA.Hiroshi Yamaguchi and Kumiko Tanaka-Ishii.
2012.Text segmentation by language using minimum de-scription length.
In Proceedings the 50th AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 969?978, Jeju Is-land, Korea.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 18th International Conference on Compu-tational Linguistics (COLING 2000), pages 947?953,Saarbru?cken, Germany.40
