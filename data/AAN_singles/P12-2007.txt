Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33?37,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsHead-Driven Hierarchical Phrase-based TranslationJunhui Li Zhaopeng Tu?
Guodong Zhou?
Josef van GenabithCentre for Next Generation LocalisationSchool of Computing, Dublin City University?
Key Lab.
of Intelligent Info.
ProcessingInstitute of Computing Technology, Chinese Academy of Sciences?School of Computer Science and TechnologySoochow University, China{jli,josef}@computing.dcu.ietuzhaopeng@ict.ac.cn gdzhou@suda.edu.cnAbstractThis paper presents an extension of Chi-ang?s hierarchical phrase-based (HPB) model,called Head-Driven HPB (HD-HPB), whichincorporates head information in translationrules to better capture syntax-driven infor-mation, as well as improved reordering be-tween any two neighboring non-terminals atany stage of a derivation to explore a largerreordering search space.
Experiments onChinese-English translation on four NIST MTtest sets show that the HD-HPB model signifi-cantly outperforms Chiang?s model with aver-age gains of 1.91 points absolute in BLEU.1 IntroductionChiang?s hierarchical phrase-based (HPB) transla-tion model utilizes synchronous context free gram-mar (SCFG) for translation derivation (Chiang,2005; Chiang, 2007) and has been widely adoptedin statistical machine translation (SMT).
Typically,such models define two types of translation rules:hierarchical (translation) rules which consist of bothterminals and non-terminals, and glue (grammar)rules which combine translated phrases in a mono-tone fashion.
Due to lack of linguistic knowledge,Chiang?s HPB model contains only one type of non-terminal symbol X , often making it difficult to se-lect the most appropriate translation rules.1 Whatis more, Chiang?s HPB model suffers from limitedphrase reordering combining translated phrases in amonotonic way with glue rules.
In addition, once a1Another non-terminal symbol S is used in glue rules.glue rule is adopted, it requires all rules above it tobe glue rules.One important research question is therefore howto refine the non-terminal category X using linguis-tically motivated information: Zollmann and Venu-gopal (2006) (SAMT) e.g.
use (partial) syntacticcategories derived from CFG trees while Zollmannand Vogel (2011) use word tags, generated by ei-ther POS analysis or unsupervised word class in-duction.
Almaghout et al (2011) employ CCG-based supertags.
Mylonakis and Sima?an (2011) uselinguistic information of various granularities suchas Phrase-Pair, Constituent, Concatenation of Con-stituents, and Partial Constituents, where applica-ble.
Inspired by previous work in parsing (Char-niak, 2000; Collins, 2003), our Head-Driven HPB(HD-HPB) model is based on the intuition that lin-guistic heads provide important information about aconstituent or distributionally defined fragment, asin HPB.
We identify heads using linguistically mo-tivated dependency parsing, and use their POS torefine X.
In addition HD-HPB provides flexible re-ordering rules freely mixing translation and reorder-ing (including swap) at any stage in a derivation.Different from the soft constraint modelingadopted in (Chan et al, 2007; Marton and Resnik,2008; Shen et al, 2009; He et al, 2010; Huang etal., 2010; Gao et al, 2011), our approach encodessyntactic information in translation rules.
However,the two approaches are not mutually exclusive, aswe could also include a set of syntax-driven featuresinto our translation model.
Our approach maintainsthe advantages of Chiang?s HPB model while at thesame time incorporating head information and flex-33??/NROuzhou??/NNbaguo??/ADlianming??/VVzhichi??/NRmeiguo?
?/NNlichangrootEight European countries jointly support America?s standFigure 1: An example word alignment for a Chinese-English sentence pair with the dependency parse tree forthe Chinese sentence.
Here, each Chinese word is at-tached with its POS tag and Pinyin.ible reordering in a derivation in a natural way.
Ex-periments on Chinese-English translation using fourNIST MT test sets show that our HD-HPB modelsignificantly outperforms Chiang?s HPB as well as aSAMT-style refined version of HPB.2 Head-Driven HPB Translation ModelLike Chiang (2005) and Chiang (2007), our HD-HPB translation model adopts a synchronous con-text free grammar, a rewriting system which gen-erates source and target side string pairs simulta-neously using a context-free grammar.
Instead ofcollapsing all non-terminals in the source languageinto a single symbol X as in Chiang (2007), given aword sequence f ij from position i to position j, wefirst find heads and then concatenate the POS tagsof these heads as f ij?s non-terminal symbol.
Specif-ically, we adopt unlabeled dependency structure toderive heads, which are defined as:Definition 1.
For word sequence f ij , wordfk (i ?
k ?
j) is regarded as a head if it is domi-nated by a word outside of this sequence.Note that this definition (i) allows for a word se-quence to have one or more heads (largely due tothe fact that a word sequence is not necessarily lin-guistically constrained) and (ii) ensures that headsare always the highest heads in the sequence from adependency structure perspective.
For example, theword sequence ouzhou baguo lianming in Figure 1has two heads (i.e., baguo and lianming, ouzhou isnot a head of this sequence since its headword baguofalls within this sequence) and the non-terminal cor-responding to the sequence is thus labeled as NN-AD.
It is worth noting that in this paper we onlyrefine non-terminal X on the source side to head-informed ones, while still usingX on the target side.According to the occurrence of terminals intranslation rules, we group rules in the HD-HPBmodel into two categories: head-driven hierarchicalrules (HD-HRs) and non-terminal reordering rules(NRRs), where the former have at least one terminalon both source and target sides and the later have noterminals.
For rule extraction, we first identify ini-tial phrase pairs on word-aligned sentence pairs byusing the same criterion as most phrase-based trans-lation models (Och and Ney, 2004) and Chiang?sHPB model (Chiang, 2005; Chiang, 2007).
Weextract HD-HRs and NRRs based on initial phrasepairs, respectively.2.1 HD-HRs: Head-Driven Hierarchical RulesAs mentioned, a HD-HR has at least one terminalon both source and target sides.
This is the sameas the hierarchical rules defined in Chiang?s HPBmodel (Chiang, 2007), except that we use head POS-informed non-terminal symbols in the source lan-guage.
We look for initial phrase pairs that containother phrases and then replace sub-phrases with POStags corresponding to their heads.
Given the wordalignment in Figure 1, Table 1 demonstrates the dif-ference between hierarchical rules in Chiang (2007)and HD-HRs defined here.Similar to Chiang?s HPB model, our HD-HPBmodel will result in a large number of rules causingproblems in decoding.
To alleviate these problems,we filter our HD-HRs according to the same con-straints as described in Chiang (2007).
Moreover,we discard rules that have non-terminals with morethan four heads.2.2 NRRs: Non-terminal Reordering RulesNRRs are translation rules without terminals.
Givenan initial phrase pair on the source side, there arefour possible positional relationships for their targetside translations (we use Y as a variable for non-terminals on the source side while all non-terminalson the target side are labeled as X):?
Monotone ?Y ?
Y1Y2, X ?
X1X2?;?
Discontinuous monotone?Y ?
Y1Y2, X ?
X1 .
.
.
X2?;?
Swap ?Y ?
Y1Y2, X ?
X2X1?;?
Discontinuous swap?Y ?
Y1Y2, X ?
X2 .
.
.
X1?.34phrase pairs hierarchical rule head-driven hierarchical rulelichang, stand X?lichang, standNN?lichang,X?standmeiguo lichang1, America?s stand1 X?meiguo X1, America?s X1NN?meiguo NN1,X?America?s X1zhichi meiguo, support America?s X?zhichi meiguo, support America?sVV-NR?zhichi meiguo,X?support America?szhichi meiguo1 lichang,support America?s1 standX?X1 lichang,X1 standVV?VV-NR1 lichang,X?X1 standTable 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs.
Indexed underlines indicate sub-phrasesand corresponding non-terminal symbols.
The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)POS tags of the corresponding word sequence in the source language.Merging two neighboring non-terminals into asingle non-terminal, NRRs enable the translationmodel to explore a wider search space.
During train-ing, we extract four types of NRRs and calculateprobabilities for each type.
To speed up decoding,we currently (i) only use monotone and swap NRRsand (ii) limit the number of non-terminals in a NRRto 2.2.3 Features and DecodingGiven e for the translation output in the target lan-guage, s and t for strings of terminals and non-terminals on the source and target side, respectively,we use a feature set analogous to the default featureset of Chiang (2007), including:?
Phd-hr (t|s) and Phd-hr (s|t), translation probabili-ties for HD-HRs;?
Plex (t|s) and Plex (s|t), lexical translation proba-bilities for HD-HRs;?
Ptyhd-hr = exp (?1), rule penalty for HD-HRs;?
Pnrr (t|s), translation probability for NRRs;?
Ptynrr = exp (?1), rule penalty for NRRs;?
Plm (e), language model;?
Ptyword (e) = exp (?|e|), word penalty.Our decoder is based on CKY-style chart parsingwith beam search and searches for the best deriva-tion bottom-up.
For a source span [i, j], it appliesboth types of HD-HRs and NRRs.
However, HD-HRs are only applied to generate derivations span-ning no more than K words ?
the initial phraselength limit used in training to extract HD-HRs ?while NRRs are applied to derivations spanning anylength.
Unlike in Chiang?s HPB model, it is pos-sible for a non-terminal generated by a NRR to beincluded afterwards by a HD-HR or another NRR.3 ExperimentsWe evaluate the performance of our HD-HPB modeland compare it with our implementation of Chiang?sHPB model (Chiang, 2007), a source-side SAMT-style refined version of HPB (SAMT-HPB), and theMoses implementation of HPB.
For fair compari-son, we adopt the same parameter settings for ourHD-HPB and HPB systems, including initial phraselength (as 10) in training, the maximum number ofnon-terminals (as 2) in translation rules, maximumnumber of non-terminals plus terminals (as 5) onthe source, beam threshold ?
(as 10?5) (to discardderivations with a score worse than ?
times the bestscore in the same chart cell), beam size b (as 200)(i.e.
each chart cell contains at most b derivations).For Moses HPB, we use ?grow-diag-final-and?
toobtain symmetric word alignments, 10 for the max-imum phrase length, and the recommended defaultvalues for all other parameters.We train our model on a dataset with ?1.5M sen-tence pairs from the LDC dataset.2 We use the2002 NIST MT evaluation test data (878 sentencepairs) as the development data, and the 2003, 2004,2005, 2006-news NIST MT evaluation test data(919, 1788, 1082, and 616 sentence pairs, respec-tively) as the test data.
To find heads, we parse thesource sentences with the Berkeley Parser3 (Petrovand Klein, 2007) trained on Chinese TreeBank 6.0and use the Penn2Malt toolkit4 to obtain (unlabeled)dependency structures.We obtain the word alignments by running2This dataset includes LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T063http://code.google.com/p/berkeleyparser/4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/35GIZA++ (Och and Ney, 2000) on the corpus in bothdirections and applying ?grow-diag-final-and?
re-finement (Koehn et al, 2003).
We use the SRI lan-guage modeling toolkit to train a 5-gram languagemodel on the Xinhua portion of the Gigaword corpusand standard MERT (Och, 2003) to tune the featureweights on the development data.For evaluation, the NIST BLEU script (version12) with the default settings is used to calculate theBLEU scores.
To test whether a performance differ-ence is statistically significant, we conduct signifi-cance tests following the paired bootstrap approach(Koehn, 2004).
In this paper,?**?
and?*?
de-note p-values less than 0.01 and in-between [0.01,0.05), respectively.Table 2 lists the rule table sizes.
The full rule ta-ble size (including HD-HRs and NRRs) of our HD-HPB model is ?1.5 times that of Chiang?s, largelydue to refining the non-terminal symbol X in Chi-ang?s model into head-informed ones in our model.It is also unsurprising, that the test set-filtered ruletable size of our model is only ?0.7 times that of Chi-ang?s: this is due to the fact that some of the refinedtranslation rule patterns required by the test set areunattested in the training data.
Furthermore, the ruletable size of NRRs is much smaller than that of HD-HRs since a NRR contains only two non-terminals.Table 3 lists the translation performance withBLEU scores.
Note that our re-implementation ofChiang?s original HPB model performs on a par withMoses HPB.
Table 3 shows that our HD-HPB modelsignificantly outperforms Chiang?s HPB model withan average improvement of 1.91 in BLEU (and sim-ilar improvements over Moses HPB).Table 3 shows that the head-driven scheme out-performs a SAMT-style approach (for each test setp < 0.01), indicating that head information is moreeffective than (partial) CFG categories.
Taking lian-ming zhichi in Figure 1 as an example, HD-HPBlabels the span VV, as lianming is dominated byzhichi, effecively ignoring lianming in the transla-tion rule, while the SAMT label is ADVP:AD+VV5which is more susceptible to data sparsity.
In addi-tion, SAMT resorts to X if a text span fails to satisifypre-defined categories.
Examining initial phrases5the constituency structure for lianming zhichi is (VP (ADVP(AD lianming)) (VP (VV zhichi) ...)).System Total MT 03 MT 04 MT 05 MT 06 Avg.HPB 39.6 2.8 4.7 3.3 3.0 3.4HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2Table 2: Rule table sizes (in million) of different mod-els.
Note: 1) For HD-HPB, the rule sizes separated by /indicate HD-HRs and NRRs, respectively; 2) Except for?Total?, the figures correspond to rules filtered on the cor-responding test set.System MT 03 MT 04 MT 05 MT 06 Avg.Moses HPB 32.94* 35.16 32.18 29.88* 32.54HPB 33.59 35.39 32.20 30.60 32.95HD-HPB 35.50** 37.61** 34.56** 31.78** 34.86SAMT-HPB 34.07 36.52** 32.90* 30.66 33.54HD-HR+Glue 34.58** 36.55** 33.84** 31.06 34.01Table 3: BLEU (%) scores of different models.
Note:1) SAMT-HPB indicates our HD-HPB model with non-terminal scheme of Zollmann and Venugopal (2006);2) HD-HR+Glue indicates our HD-HPB model replac-ing NRRs with glue rules; 3) Significance tests forMoses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glueare done against HPB.extracted from the SAMT training data shows that28% of them are labeled as X.In order to separate out the individual contribu-tions of the novel HD-HRs and NRRs, we carry outan additional experiment (HD-HR+Glue) using HD-HRs with monotonic glue rules only (adjusted to re-fined rule labels, but effectively switching off the ex-tra reordering power of full NRRs).
Table 3 showsthat on average more than half of the improvementover HPB (Chiang and Moses) comes from the re-fined HD-HRs, the rest from NRRs.Examining translation rules extracted from thetraining data shows that there are 72,366 types ofnon-terminals with respect to 33 types of POS tags.On average each sentence employs 16.6/5.2 HD-HRs/NRRs in our HD-HPB model, compared to15.9/3.6 hierarchical rules/glue rules in Chiang?smodel, providing further indication of the impor-tance of NRRs in translation.4 ConclusionWe present a head-driven hierarchical phrase-based(HD-HPB) translation model, which adopts head in-formation (derived through unlabeled dependencyanalysis) in the definition of non-terminals to bet-ter differentiate among translation rules.
In ad-36dition, improved and better integrated reorderingrules allow better reordering between consecutivenon-terminals through exploration of a larger searchspace in the derivation.
Experimental results onChinese-English translation across four test setsdemonstrate significant improvements of the HD-HPB model over both Chiang?s HPB and a source-side SAMT-style refined version of HPB.AcknowledgmentsThis work was supported by Science Foundation Ire-land (Grant No.
07/CE/I1142) as part of the Cen-tre for Next Generation Localisation (www.cngl.ie)at Dublin City University.
It was also partiallysupported by Project 90920004 under the NationalNatural Science Foundation of China and Project2012AA011102 under the ?863?
National High-Tech Research and Development of China.
Wethank the reviewers for their insightful comments.ReferencesHala Almaghout, Jie Jiang, and Andy Way.
2011.
CCGcontextual labels in hierarchical phrase-based SMT.
InProceedings of EAMT 2011, pages 281?288.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of ACL 2007, pages33?40.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL 2000, pages 132?139.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL 2005, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Yang Gao, Philipp Koehn, and Alexandra Birch.
2011.Soft dependency constraints for reordering in hierar-chical phrase-based translation.
In Proceedings ofEMNLP 2011, pages 857?868.Zhongjun He, Yao Meng, and Hao Yu.
2010.
Maxi-mum entropy based phrase reordering for hierarchicalphrase-based translation.
In Proceedings of EMNLP2010, pages 555?563.Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of EMNLP 2010, pages 138?147.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL 2003, pages 48?54.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proceedings of ACL-HLT 2008, pages 1003?1011.Markos Mylonakis and Khalil Sima?an.
2011.
Learninghierarchical translation structure with linguistic anno-tations.
In Proceedings of ACL-HLT 2011, pages 642?652.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL2000, pages 440?447.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL2003, pages 160?167.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACL2007, pages 404?411.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proceedings of EMNLP 2009, pages72?80.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of NAACL 2006 - Workshop on StatisticalMachine Translation, pages 138?141.Andreas Zollmann and Stephan Vogel.
2011.
A word-class approach to labeling PSCFG rules for machinetranslation.
In Proceedings of ACL-HLT 2011, pages1?11.37
