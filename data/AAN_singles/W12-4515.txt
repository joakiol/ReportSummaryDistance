Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122?128,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsBART goes multilingual: The UniTN / Essex submission to the CoNLL-2012Shared TaskOlga Uryupina?
Alessandro Moschitti?
Massimo Poesio??
?University of Trento?
University of Essexuryupina@gmail.com, moschitti@disi.unitn.it, massimo.poesio@unitn.itAbstractThis paper describes the UniTN/Essex sub-mission to the CoNLL-2012 Shared Task onthe Multilingual Coreference Resolution.
Wehave extended our CoNLL-2011 submission,based on BART, to cover two additional lan-guages, Arabic and Chinese.
This paper fo-cuses on adapting BART to new languages,discussing the problems we have encounteredand the solutions adopted.
In particular, wepropose a novel entity-mention detection algo-rithm that might help identify nominal men-tions in an unknown language.
We also dis-cuss the impact of basic linguistic informationon the overall performance level of our coref-erence resolution system.1 IntroductionA number of high-performance coreference resolu-tion (CR) systems have been created for English inthe past decades, implementing both rule-based andstatistical approaches.
For other languages, how-ever, the situation is far less optimistic.
For Ro-mance and German languages, several systems havebeen developed and evaluated, in particular, at theSemEval-2010 track 1 on Multilingual CoreferenceResolution (Recasens et al, 2010).
For other lan-guages, individual approaches have been proposed,covering specific subparts of the task, most com-monly pronominal anaphors (cf., for example, (Iidaand Poesio, 2011; Arregi et al, 2010) and many oth-ers).Two new languages, Arabic and Chinese, havebeen proposed for the CoNLL-2012 shared task(Pradhan et al, 2012).
They present a challeng-ing problem: the systems are required to pro-vide entity mention detection (EMD) and design aproper coreference resolver for both languages.
AtUniTN/Essex, we have focused on these parts of thetask, relying on a modified version of our last-yearsubmission for English.Most state-of-the-art full-scale coreference reso-lution systems rely on hand-written rules for themention detection subtask.1 For English, such rulesmay vary from corpus to corpus, reflecting specificsof particular guidelines (e.g.
whether nominal pre-modifiers can be mentions, as in MUC, or not, as inmost other corpora).
However, for each corpus, suchheuristics can be adjusted in a straightforward way.Creating a robust rule-based EMD module for a newlanguage, on the contrary, is a challenging issue thatrequires substantial linguistic knowledge.In this paper, we advocate a novel approach, re-casting parse-based EMD as a statistical problem.We consider a node-filtering model that does not relyon any linguistic expertise in a given language.
In-stead, we use tree kernels (Moschitti, 2008; Mos-chitti, 2006) to induce a classifier for mention NP-nodes automatically from the data.Another issue to be solved when designing acoreference resolution system for a new languageis a possible lack of relevant linguistic information.Most state-of-the-art CR algorithms rely on rela-tively advanced linguistic representations of men-tions.
This can be seen as a remarkable shift1Statistical EMD approaches have been proved useful forACE-style coreference resolution, where mentions are basicunits belonging to a restricted set of semantic types.122from knowledge-lean approaches of the late nineties(Harabagiu and Maiorano, 1999).
In fact, modernsystems try to account for complex coreference linksby incorporating lexicographic and world knowl-edge, for example, using WordNet (Harabagiu et al,2001; Huang et al, 2009) or Wikipedia (Ponzettoand Strube, 2006).
For languages other than English,however, even the most basic properties of mentionscan be intrinsically difficult to extract.
For example,Baran and Xue (2011) have shown that a complex al-gorithm is needed to identify the number propertyof Chinese nouns.Both Arabic and Chinese have long linguistic tra-ditions and therefore most grammar studies rely onterminology that can be very confusing for an out-sider.
For example, several works on Arabic (Hoyt,2008) mention that nouns can be made definite withthe suffix ?Al-?, but this is not a semantic, but syn-tactic definiteness.
Without any experience in Ara-bic, one can hardly decide how such ?syntactic defi-niteness?
might affect coreference.In the present study, we have used the informa-tion provided by the CoNLL organizers to try andextract at least some linguistic properties of men-tions for Arabic and Chinese.
We have run severalexperiments, evaluating the impact of such very ba-sic knowledge on the performance level of a coref-erence resolution system.The rest of the paper is organized as follows.
Inthe next section we briefly describe the general ar-chitecture and the system for English, focusing onthe adjustments made after the last year competition.Section 3 is devoted to new languages: we first dis-cuss our EMD module and then describe the proce-dures for extracting linguistic knowledge.
Section 4discusses the impact of our solutions to the perfor-mance level of a coreference resolver.
The officialevaluation results are presented in Section 5.2 BARTOur CoNLL submission is based on BART (Versleyet al, 2008).
BART is a modular toolkit for corefer-ence resolution that supports state-of-the-art statisti-cal approaches to the task and enables efficient fea-ture engineering.
BART has originally been createdand tested for English, but its flexible modular archi-tecture ensures its portability to other languages anddomains.The BART toolkit has five main components: pre-processing pipeline, mention factory, feature extrac-tion module, decoder and encoder.
In addition, anindependent LanguagePlugin module handles all thelanguage specific information and is accessible fromany component.The architecture is shown in Figure 1.
Each mod-ule can be accessed independently and thus adjustedto leverage the system?s performance on a particularlanguage or domain.The preprocessing pipeline converts an input doc-ument into a set of linguistic layers, representedas separate XML files.
The mention factory usesthese layers to extract mentions and assign theirbasic properties (number, gender etc).
The fea-ture extraction module describes pairs of mentions{Mi,Mj}, i < j as a set of features.
At themoment we have around 45 different feature ex-tractors, encoding surface similarity, morpholog-ical, syntactic, semantic and discourse informa-tion.
Note that no language-specific informationis encoded in the extractors explicitly: a language-independent representation, provided by the Lan-guage Plugin, is used to compute feature val-ues.
For CoNLL-2012, we have created two addi-tional features: lemmata-match (similar to stringmatch, but uses lemmata instead of tokens) andnumber-agreement-du (similar to commonlyused number agreement features, but supports dualnumber).The encoder generates training examples througha process of sample selection and learns a pairwiseclassifier.
Finally, the decoder generates testing ex-amples through a (possibly distinct) process of sam-ple selection, runs the classifier and partitions thementions into coreference chains.2.1 Coreference resolution in EnglishThe English track at CoNLL-2012 can be consideredan extension of the last year?s CoNLL task.
Newdata have been added to the corpus, including twoadditional domains, but the annotation guidelines re-main the same.We have therefore mainly relied on the CoNLL-2011 version of our system (Uryupina et al, 2011)for the current submission, providing only minor ad-justments.
Thus, we have modified our preprocess-123POS TaggerMergerMention TaggerParserPreprocessingMentionFactoryCoreferencechains(entities)Language PluginUnannotatedTextFeatureExtractorLearnerMachineEncoder/DecoderFigure 1: BART architectureing pipeline to operate on the OntoNotes NE-types,mapping them into MUC types required by BART.This allows us to participate in the closed track, asno external material is used any longer.Since last year, we have continued with our exper-iments on multi-objective optimization, proposed inour CoNLL-2011 paper (Uryupina et al, 2011).
Wehave extended the scope of our work to cover differ-ent machine learning algorithms and their parame-ters (Saha et al, 2011).
For CoNLL-2012, we havere-tested all the solutions of our optimization exper-iments, picking the one with the highest score on thecurrent development set.Finally, our recent experiments on domain se-lection (Uryupina and Poesio, 2012) suggest that,at least for some subparts of OntoNotes, a sys-tem might benefit from training a domain-specificmodel.
We have tested this hypothesis on theCoNLL-2012 data and have consequently traineddomain-specific classifiers for the nw and bc do-mains.3 Coreference resolution in Arabic andChineseWe have addressed two main issues when develop-ing our coreference resolvers for Arabic and Chi-nese: mention detection and extraction of relevantlinguistic properties of our mentions.3.1 Mention detectionMention detection is rarely considered to be a sepa-rate task.
Only very few studies on coreference reso-lution report on their EMD techniques.
Existing cor-pora of coreference follow different approaches tomention annotation: this includes defining mentionboundaries (basic vs. maximal NPs), alignment pro-cedures (strict vs. relaxed with manually annotatedminimal spans vs. relaxed with automatically ex-tracted heads), the position on singleton and/or non-referential mentions (annotated vs. not).The CoNLL-2011/2012 guidelines take a verystrict view on mention boundaries: only the maxi-mal spans are annotated and no approximate match-ing is allowed.
Moreover, the singleton mentions(i.e.
not participating in coreference relations) arenot marked.
This makes the mention detection taskfor OntoNotes extremely challenging, especially forthe two new languages: on the one hand, one hasto provide exact boundaries; on the other hand, it ishard to learn such information explicitly, as not allthe candidate mentions are annotated.Most CoNLL-2011 systems relied on hand-written rules for the mention detection subtask.
Thiswas mainly possible due to the existence of well-studied and thoroughly documented head-detectionrules for English, available as a description for reim-plementing (Collins, 1999) or as a downloadablepackage.
Consider the following example:(1) ..((the rising price)NP2 of (food)NP3)NP1 ..124In this fragment, three nominal phrases can be iden-tified, with the first one (?the rising price of food?
)spanning over the two others (?the rising price?)
and(?food?).
According to the OntoNotes annotationguidelines, the second noun phrase cannot be a men-tion, because it is embedded in an upper NP and theyshare the same head noun.
The third noun phrase, onthe contrary, could be a mention?even though it?sembedded in another NP, their heads are different.Most CoNLL-2011 participants used as a backbonea heuristic discarding embedded noun phrases.For less-known languages, however, this heuris-tic is only applicable as long as we can compute anNP?s head reliably.
Otherwise it?s hard to distinguishbetween candidate mentions similar to NP1 and toNP2 in the example above.A set of more refined heuristics is typically ap-plied to discard or add some specific types of men-tions.
For example, several studies (Bergsma andYarowsky, 2011) have addressed the issue of detect-ing expletive pronouns in English.
Again, in the ab-sence of linguistic expertise, one can hardly engi-neer such heuristics for a new language manually.We have investigated the possibility of learn-ing mention boundaries automatically from theOntoNotes data.
We recast the problem as an NP-node filtering task: we analyze automatically com-puted parse trees and consider all the NP-nodes to becandidate instances to learn a classifier of correct vs.incorrect mention nodes.
Clearly, this approach can-not account for mentions that do not correspond toNP-nodes.
However, as Table 1 shows, around 85-89% of all the mentions, both for Arabic and Chi-nese, are NP-nodes.train developmentNP-nodes % NP-nodes %Arabic 24068 87.23 2916 87.91Chinese 88523 85.96 12572 88.52Table 1: NP-nodes in OntoNotes for Arabic and Chinese:total numbers and percentage of mentions.We use tree kernels (Moschitti, 2008; Moschitti,2006) to induce a classifier that labels an NP nodeand a part of the parse tree that surrounds it as?mention.
Two integer parameters control the se-lection of the relevant part of the parse tree, allowingfor pruning the nodes that are far above or far belowthe node of interest.Our classifier is supposed to decide whether anNP-node is a mention of a real-world object.
Suchmentions, however, are annotated in OntoNotes aspositive instances only when they corefer with someother mentions.
The classifier works as a preproces-sor for a CR system and therefore has no informationthat would allow it to discriminate between single-ton vs. non-singleton mentions.
One can investigatepossibilities for joint EMD and CR to alleviate theproblem.
We have adopted a simpler solution: wetune a parameter (cost factor) that controls the pre-cision/recall trade-off to bias the classifier stronglytowards recall.We use a small subset (1-5%) of the training datato train the EMD classifier.
We tune the EMD pa-rameters to optimize the overall performance: werun the classifier to extract mentions for the wholetraining and development sets, run the coreferenceresolver and record the obtained result (CoNLLscore).
The whole set of parameters to be tunedcomprise: the size of the training set for EMD, theprecision-recall trade-off, and two pruning thresh-olds.3.2 Extracting linguistic propertiesAll the features implemented in BART use somekind of linguistic information from the mentions.For example, the number-agreement featurefirst extracts the number properties of individualmentions.
For a language supported by BART, suchproperties are computed by the MentionFactory.
Fora new language, they should be provided as a part ofthe mention representation computed by some ex-ternal preprocessing facilities.
The only obligatorymention property is its span?
the sequence of rel-evant token ids?all the properties discussed beloware optional.The following properties have been extracted fornew languages directly from the CoNLL table:?
sentence id?
sequence of lemmata?
speaker (Chinese only)Coordinations have been determined by analyz-ing the sequence of PoS tags: any span containing125a coordinate conjunction is a coordination.
They arealways considered plural and unspecified for gender,their heads correspond to their entire spans.For non-coordinate NPs, we extract the headnouns using simple heuristics.
In Arabic, the firstnoun in a sequence is a head.
In Chinese, the lastone is a head.
If no head can be found through thisheuristic, we try the same method, but allow for pro-nouns to be heads, and, as a default, consider thewhole span to be the head.Depending on the PoS tag of the head noun, weclassify a mention as an NE, a pronoun or a nomi-nal (default).
For named entities, no further mentionproperties have been extracted.We have compiled lists of pronouns for both Ara-bic and Chinese from the training and developmentdata.
For Arabic, we use gold PoS tags to classifypronouns into subtypes, person, number and gender.For Chinese, no such information is available, so wehave consulted several grammar sketches and lists ofpronouns on the web.
We do not encode clusivity2and honorifics.3For Arabic, we extract the gender and numberproperties of nominals in the following way.
First,we have processed the gold PoS tags to create a listof number and gender affixes.
We compute the prop-erties of our mentions by analyzing the affixes oftheir heads.
In a number of constructions, however,the gender is not marked explicitly, so we have com-piled a gender dictionary for Arabic lemmata on thetraining and development data.
If the gender can-not be computed from affixes, we look it up in thedictionary.Finally, we have made an attempt at computingthe definiteness of nominal expressions.
For Arabic,we consider as definites all mentions with definitehead nouns (prefixed with ?Al?)
and all the idafaconstructs with a definite modifier.4 We could notcompute definiteness for Chinese reliably.2In some dialects of Chinese, a distinction is made betweenthe first person plural inclusive (?you and me?)
and the firstperson exclusive (?me and somebody else?)
pronouns.3In Chinese, different pronouns should be used address-ing different persons, reflecting the relative social status of thespeaker and the listener.4Idafa-constructs are syntactic structures, conveying, veryroughly speaking, genitive semantics, commonly used in Ara-bic.
Their accurate analysis requires some language-specificprocessing.4 Evaluating the impact of kernel-basedmention detection and basic linguisticknowledgeTo adopt our system to new languages, we have fo-cused on two main issues: EMD and extraction oflinguistic properties.
In this section we discuss theimpact of each factor on the overall performance.Table 2 summarizes our evaluation experiments.
Allthe figures reported in this section are CoNLL scores(averages of MUC, B3 and CEAFe) obtained on thedevelopment data.To evaluate the impact of our kernel-based EMD(TKEMD), we compare its performance against twobaselines.
The lower bound, ?allnp?, considers allthe NP-nodes in a parse tree to be candidate men-tions.
The upper bound, ?goldnp?
only considersgold NP-nodes to be candidate mentions.
Note thatthe upper bound does not include mentions that donot correspond to NP-nodes at all (around 12% ofall the mentions in the development data, cf.
Table 1above).We have created three versions of our corefer-ence resolver, using different amounts of linguisticknowledge.
The baseline system (Table 2, first col-umn) relies only on mention spans.
The system it-self is a reimplementation of Soon et al (2001), but,clearly, only the string-matching feature can be com-puted without specifying mention properties.A more advanced version of the system (secondcolumn) uses the same model and the same featureset, but relies on mention properties, extracted as de-scribed in Section 3.2 above.
The final version (thirdcolumn) makes use of all the features implementedin BART.
We run a greedy feature selection algo-rithm, starting from the string matching and addingfeatures one by one, until the performance stops in-creasing.For Chinese, our EMD approach has proved to beuseful, bringing around 1.5-2% improvement overthe ?allnp?
baseline for all the versions of the coref-erence resolver.
The module for extracting mentionproperties has only brought a moderate improve-ment.
This is not surprising, as we have not beenable to extract many relevant linguistic properties,especially for nominals.
We believe that an improve-ment can be achieved on the Chinese data by incor-porating more linguistic information.126baseline +linguistics +linguistics+featuresArabicallnp 45.47 46.15 46.32TKEMD 46.98 47.44 49.07goldnp 51.08 63.27 64.55Chineseallnp 50.72 51.04 51.40TKEMD 53.10 53.33 53.53goldnp 57.78 57.30 57.98Table 2: Evaluating the impact of EMD and linguisticknowledge: CoNLL F-score.For Arabic, the linguistic properties could poten-tially be very helpful: on gold NPs, our linguisticallyrich system outperforms its knowledge-lean coun-terpart by 13 percentage points.
Unfortunately, thisimprovement is mirrored only partially on the fullyautomatically acquired mentions.5 Official resultsTable 3 shows the official results obtained by oursystem at the CoNLL-2012 competition.Metric Recall Precision F-scoreEnglishMUC 61.00 60.78 60.89BCUBED 63.59 68.48 65.95CEAF (M) 52.44 52.44 52.44CEAF (E) 41.42 41.64 41.53BLANC 67.40 72.83 69.65ArabicMUC 41.33 41.66 41.49BCUBED 65.77 69.23 67.46CEAF (M) 50.82 50.82 50.82CEAF (E) 42.43 42.13 42.28BLANC 65.58 70.56 67.69ChineseMUC 45.62 63.13 52.97BCUBED 59.17 80.78 68.31CEAF (M) 52.40 52.40 52.40CEAF (E) 48.47 34.52 40.32BLANC 68.72 80.76 73.11Table 3: BART performance at CoNLL-2012: official re-sults on the test set.6 ConclusionIn this paper we have discussed our experimentson adapting BART to two new languages, Chineseand Arabic, for the CoNLL-2012 Shared Task onthe Multilingual Coreference Resolution.
Our teamhas some previous experience with extending BARTto cover languages other than English, in particular,Italian and German.
For those languages, however,most of our team members had at least an advancedknowledge, allowing for more straightforward engi-neering and error analysis.
Both Arabic and Chi-nese present a challenge: they require new mentiondetection algorithms, as well as special language-dependent techniques for extracting mention prop-erties.For Arabic, we have proposed several simple ad-justments to extract basic morphological informa-tion.
As our experiments show, this can potentiallylead to a substantial improvement.
The progress,however, is hindered by the mention detection qual-ity: even though our TKEMD module outperformsthe lower bound baseline, there is still a lot ofroom for improvement, that can be achieved aftera language-aware error analysis.For Chinese, the subtask of extracting relevant lin-guistic information has turned out to be very chal-lenging.
We believe that, by elaborating on themethods for assigning linguistic properties to nomi-nal mentions and combining them with the TKEMDmodule, one can boost the performance level of acoreference resolver.7 AcknowledgmentsThe research described in this paper has been par-tially supported by the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der the grants #247758: ETERNALS ?
TrustworthyEternal Systems via Evolving Software, Data andKnowledge, and #288024: LIMOSINE ?
Linguis-tically Motivated Semantic aggregation engiNes.127ReferencesOlatz Arregi, Klara Ceberio, Arantza D?
?az De Illar-raza, Iakes Goenaga, Basilio Sierra, and Ana Zelaia.2010.
A first machine learning approach to pronom-inal anaphora resolution in Basque.
In Proceedingsof the 12th Ibero-American conference on Advances inartificial intelligence, IBERAMIA?10, pages 234?243,Berlin, Heidelberg.
Springer-Verlag.Elizabeth Baran and Nianwen Xue.
2011.
Singular orplural?
Exploiting parallel corpora for Chinese num-ber prediction.
In Proceedings of the Machine Trans-lation Summit XIII.Shane Bergsma and David Yarowsky.
2011.
NADA:A robust system for non-referential pronoun detec-tion.
In Proceedings of the Discourse Anaphora andAnaphor Resolution Colloquium, Faro, Portugal, Oc-tober.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Sanda Harabagiu and Steven Maiorano.
1999.Knowledge-lean coreference resolution and its rela-tion to textual cohesion and coherence.
In Proceed-ings of the ACL Workshop On The Relation Of Dis-course/Dialogue Structure And Reference.Sanda Harabagiu, Ra?zvan Bunescu, and Steven Maio-rano.
2001.
Text and knowledge mining for coref-erence resolution.
In Proceedings of the 2nd Meetingof the North American Chapter of the Association forComputational Linguistics, pages 55?62.Frederick Hoyt.
2008.
The Arabic noun phrase.
InThe Encyclopedia of Arabic Language and Linguis-tics.
Leiden:Brill.Zhiheng Huang, Guangping Zeng, Weiqun Xu, and AsliCelikyilmaz.
2009.
Effectively exploiting WordNetin semantic class classification for coreference resolu-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Ryu Iida and Massimo Poesio.
2011.
A cross-lingualILP solution to zero anaphora resolution.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics, pages 804?813.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Proceedings of European Conference on MachineLearning, pages 318?329.Alessandro Moschitti.
2008.
Kernel methods, syntax andsemantics for relational text categorization.
In Pro-ceeding of the International Conference on Informa-tion and Knowledge Management, NY, USA.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, pages 192?199.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of theSixteenth Conference on Computational Natural Lan-guage Learning (CoNLL 2012), Jeju, Korea.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.Anto`nia Mart?
?, Mariona Taule?, Ve?ronique Hoste,Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 Task 1: Coreference resolution inmultiple languages.
In Proceedings of the 5thInternational Workshop on Semantic Evaluations(SemEval-2010), Uppsala, Sweden.Sriparna Saha, Asif Ekbal, Olga Uryupina, and MassimoPoesio.
2011.
Single and multi-objective optimiza-tion for feature selection in anaphora resolution.
InProceedings of the International Joint Conference onNatural Language Processing.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistic, 27(4):521?544.Olga Uryupina and Massimo Poesio.
2012.
Domain-specific vs. uniform modeling for coreference resolu-tion.
In Proceedings of the Language Resources andEvaluation Conference.Olga Uryupina, Sriparna Saha, Asif Ekbal, and Mas-simo Poesio.
2011.
Multi-metric optimization forcoreference: The UniTN / IITP / Essex submission tothe 2011 CONLL shared task.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning (CoNLL 2011).Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: a modular toolkit for coreference resolution.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics on Human Lan-guage Technologies, pages 9?12.128
