A Stochastic Japanese Morphological Analyzer Using aForward-DP Backward-A* N-Best  Search Algor i thmMasa.aki NAGATANTT  Network Information Systems l~,~bor~ttorics1-2356 Take, Yokosuka-Shi, Kanagaw~t, 238-03 Japan(tel) 4-81-468-59-2796(fax) +81-468-59-3428(e-mail) nagata@nttnly.ntt .
j l )AbstractWe present a novel method for segmenting the inputsentence into words and assigning parts of speech tothe words.
It consists of a statistical language modeland an efficient wo-pa~qs N-best search algorithm.
Thealgorithm does not require delimiters between words.Thus it is suitable for written Japanese.
q'he proposedJapanese morphological nalyzer achieved 95. l% recalland 94.6% precision for open text when it was trainedand tested on the ATI'?
Corpus.1 Introduct ionIn recent years, we have seen a fair number of l)al)ers re-porting accuracies ofmore than 95% for English part ofspeech tagging with statistical language modeling tech-niques \[2-4, 10, 11\].
On the other hand, there are fewworks on stochastic Japanese morphological nalysis\[9, 12, 14\], and they don't seem to have convinced theJapanese NLP community that the statistically-basedteclmiques are superior to conventional rule-based tech-niques uch as \[16, 17\].We show in this paper that we can buihl a stochasticJapanese morphological nalyzer that offers approxi-mately 95% accuracy on a statistical language model-ing technique and an efficient two-pass N-best searchstrategy.We used tile simple tri-POS model as the taggingmodel for Japanese.
Probability estimates were ob-tained after training on the ATI{ l)ialogue Database\[5\], whose word segmentation a d part of speech tagassignment were laboriously performed by hand.We propose a novel search strategy for getting theN best morphological nalysis hypotheses for the in-put sentence.
It consists of the forward dynamic pro-gramming search and the backward A* search.
Theproposed algorithm amalgamates and extends threewell-known algorithms in different fields: the MinimumConnective-Cost Method \[7\] for Japanese morphologi-cal analysis, Extended Viterbi Algorithm for charac-ter recognition \[6\], and "l~'ee-Trellis N-Best Search forspeech recognition \[15\].We also propose a novel method for handling un-known words uniformly within the statistical pproach.Using character trigrams ms tim word model, it gener-ates the N-best word hypotheses that match the left-most substrings starting at a given position in the inputsenten  ce.Moreover, we propose a novel method for evaluat-ing the performance of morphological analyzers.
Un-like English, Japanese does not place spaces betweenwords.
It is difficult, even for native Japanese, to placeword boundaries consistently because of the aggluti-native nature of the language.
Thus, there were nostandard performance metrics.
We applied bracketingaccuracy measures \[1\], which is originally used for En-glish parsers, to Japanese morphological nalyzers.
Wealso slightly extended the original definition to describethe accuracy of tile N-best candidates.In the following sections, we first describe the tech-niques used in the proposed morphological nalyzer,we then explain the cwduation metrics and show thesystem's performance by experimental results.2 Tagging Model2.1 Tr i -POS Mode l  and  Re la t ive  F re -quency  Tra in ingWe used the tri-POS (or triclass, tri-tag, tri-Ggrametc.)
model ~Ls tile tagging model for Japanese.
Con-sider a word segmentation f the input sentence W =wl w2.
.
.
w,~ and a sequence of tags T = t i ts .
.
,  t,, ofthe same length.
The morphological analysis tmsk cauI)e formally defined ,~ finding a set of word segmen-tat.ion and parts of speech ~ssignment that maximizethe joint probability of word sequence arm tag sequenceP(W, 7').
In the tri-POS model, the joint probability isapproximated bythe product of parts of speech trigramprobabilities P(t i l t i_2,t i_ l )  and word output probabil-ities for given part of speech P(wl\]ll):r(w,:r) = \]~ r(tdt,_o.,t,_x)r'(w, lt4 (1)i=1201In practice, we consider sentence boundaries ~s specialsymbols as follows.P(W,T) = P(ql#)P(wtltt)P(t,.
l#, tl)P(w21t~)~I P(tilti_2,ti_l)P(willi)P(#\[t,,_l,?,,) (2)i=3where "#" indicates the sentence boundary marker.
Ifwe have some tagged text available, we can estimate theprobabilities P(tdti_2,ti_l ) and P(wiltl) by comput-ing the relative frequencies of the corresponding eventson this data:.N(ti_2, ti-1, tl)P(tifti-2'ti-t) = f(qltl-2'ti-x) - iV ( t i _ .
.
, , t i _ , )(3)P(wilti) = f(wilt,) -- N(w,t) ('1)N(t)where f indicates the relative frequency, N(w, t) is t!,enumber of times a given word w appears with tag l, aidN(li_2,ti-l,tl) is the number of times that sequer~cel~ i _2t i _ l l  i appears in the text.
It is inevitable to s~ irerfrom sparse-data problem in the part of speech tag tri-gram probability I .
To handle open text, trigram Frol:-ability is smoothed by interpolated estimation, wi~ichsimply interpolates trlgram, bigram, unigram, and ze-rogram relative frequencies\[8\],P ( t i lq_ , ,q_ l  ) = qaf(tilt,_2,q_, )A-q2f(tdti_l) + qtf(ti) + qoV (5)where f indicates the relative.frequency and V is auniform probability that each tag will occur.
The non-negative weights qi satisfy q3 + q~ + q1 + q0 = 1, andthey are adjusted so as to make the observed ata mostprobable after the adjustment by using EM algorithm ~-.2.2 Order Reduct ion  and RecursiveTracingIn order to understand the search algorithm describedin the next section, we will introduce the second orderHMM and extended Viterbi algorithm \[6\].
Consideringthe combined state sequence U = ltl'tt2.., ttn, whereul = tl and ui = ti-tli, we haveP(uilui_l) = P(tilti_=,ti_l) (6)Substituting Equation (6) into Equation (l), we havelWe used 120 part of speedl tags.
In the ATR Corpus, 26parts of speech, 13 conjugation types, and 7 conjugation formsare defined.
Out of 26, 5 parts of speech ave conjugation.
Sincewe used a list of part of speech, conjugation type, and conjuga-tion form as a tag, there are 119 tags in the A'IT?
Corpus.
Weadded the sentence boundary marker to them.aTo handle open text, word output probahility P( lo i l t i )  mustalso be smoothed.
Tiffs problem is discussed in a later section *Lsthe unknown word problem.Equationmodel.
'ill I .
.
.
Wiwe haveP(W,r) = 1\].
P(mlui-a)P(wilti) (7)i=1(7) have the same form as the first orderConshler the partial word sequence HI/ =and the partial tag sequence Ti = t l .
.
.
t l ,F(w~,~) = ?
)(w,_, ,~-, )P(~d,.
-~)p(wdtO (8)Equation (8) suggests that, to find the maxlmmnP(I,Vi,7\]) for each ul, we need only to: remember themaximum P(W?_I, 7\]_1), extend each of these prob-abilities to every ul by computing Eqnation (8), andselect the m;uxinmm P(~/Vi,Ti) for each ui.
'thus, byincreasing i by 1 to n, selecting the u.  ttlat maximizeP(W.,7\]~), and backtracing the sequence leading tothe nmxinmm probability, we can get the optimal tagseqnence.3 Search  S t ra tegyThe search algorithm consists of a forward dynamicprogramming search and a backward A* search.
First,a linear time dynamic programming is used for record-ing the scores of all partial paths in a table 3.
A back-ward A* algorithm based tree search is then used toextend the partial paths.
Partial paths extended in thebackward tree search are ranked by their correspond-ing fill path scores, which are cmnputed by addingthe scores of backward partial path scores to the cotresponding best possihle scores of the remaining pathswhich are prerecorded in the forward search.
Since thescore of the incomplete portion of a path is exactlyknown, the backward search is admissible.
That is, thetop-N candidates are exact.3.1 The Forward DP SearchTable 1 shows the two data structures used in our al-gorithm.
The st,'t, cture parse stores tile informationof a word and the best partial path up to the word.Parse .s ta r t  and parse.end are the indices of tilestart and end positions of the word in the sentence.Parse.pos is tile part of speech tag, which is a list ofpart of speech, conjugation type, and conjugation formin our system for Japanese.
Parse .n th -order -~tateis a list of the last two parts of speech tags includ-ing that of the current word.
This slot correspondsto the combined state in the second order IIMM.Parse .prob-so - fa r  is the score of the best partialpath from the beginning of the sentence to the word.Parse.prev?ous i  the pointer to the (best) previousparse structure as in conventional Viterbi decoding,which is not necessary if we use the backward N bestsearch.~ln fact, we use two tables, pa~se-\].
ist and path-~ap.
Thereason is described later.202The structure word represents the word informationin the dictionary including its lexical form, part ofspeech tag, and word output probability given tt,e partof speech.Table h Data structures for the N best algorithmstartendpeanth-order-stateprob-ao-farpreviousparse  strttctul'e"tim beginning pasition of the wordthe end position of the wordpart of speech tag of the worda list of the la-'~t two parts (,f speechthe b,~t partial path score from the starta pointer to previous parse strllettll'eword structureform \] lexical f,.
'-n{ of the wordl)Oa \[ part of speech tag of the wordprob _ word outlmt probabilityBefore explaining tim forward search, we will de-fine some flmctions and tables used in the algo-rithm.
In the forward search, we use a table calledparse-list, whose key is the end position of theparse  structure, and wlm,se value is a list of parsestructures that have the best partial path scores foreach combined state at the end position.
Functionreg is ter - to -parse - l i s t  registers a parse  structureagainst the parse-list and maintains the best par-tim parses.
Function get -parse - l i s t  returns a listof parse  structnres at the specified position.
We alsouse the fimetion l e l tmost -subst r ings  which returnsa list of word  structures in the dictionary whose lexicalform matches the substrings tarting at the.
specifiedposition in the input sentence.funct ion ~orward-paae (string)begini n i t ia l -a tepO ; It Pods spec ia l  symbols at both ends.for iffil to length(s t r ing)  doforeach parse in get -parse - l i s t ( i )  doforeach word ill l e f tmost -subat r ings(a t r ing , i )  ,I(7poa-ngrma : -  append(parse.nth-order-stato ,l i s t  (word.poa))if (traneprob(poe-ngrtm) > O) thennew-parse :'.
make-parseO ;new-parse.mtart  :~ i ;new-parse.end : -  i + length(word.form);hey-pares,poe :- word.pea;new-parae.nth-order-mtate :~ rest(pos-ngram) ;naw-paree.preb-ee- far  :-  parae.prob-so- far* transprob(pos-ngram) * word.prob;new-parse.previous := paras;reg ie ter -parse - to -parae- l t s t  (new-parse) ;reg is ter -paree- to -path -ma p (new-parse) ;endifelldendendf inn l -e tQp( ) ;  i/ Randlan t r tmai t ion  to tho e~d symbol.endFigure h The forward DP search algorithmFigure 1 shows the central part of the forward dy-namic programming search algorithm.
It starts fromthe beg,string of tim inlmt sentence, and proceeds char-attar by character.
At each point in tim sentence, itlooks up the combination of the best partial parsesending at the point and word hypotheses tarting atthat point.
If tim connection of a partial parse anda word llypothesis is allowed by the tagging model, anew continuation parse is made and registered in theparse - l i s t .
The partial path score for the new con-titular,on parse is the product of the best partial pathscore up to the poi,g, the trigram probability of thelast three parts of speech tags and the word outputprobability for LIfe part of speech 4.3 .2  The  Backward  A* SearchThe backward search uses a table called path-map,whose key is the end position of tile parse  structure,and whose value is a list of parse  structures that havethe best partial path scores for each distinct combin~ties of the start position and the combined state.
Thedilference 1)etween parse - l i s t  and path-map is thatpath-map is classi/ied by tim start position of the lastword in addition to tim combined state.This distinction is crucial for the proposed N bestalgorithm, l"or tim tbrward search to tind a parse thatmaximizes Equation (1), it is the parts of speech se-quence that matters.
For the backward N-best search,how(wet, we want N most likely word segmentation andpart of speech sequence.
Parse-list may shadow lessprobable candidates that have the same part of speechsc:qnence for the best scoring candidate, but differ intim segmentaL,on of the last word.
As shown in Figure1, path-map is made during the forward search by thefunction reg is ter -parse - to -path -map,  which regis-ters a parse  structure to path-map and maintains thebest partial parses in the table's criteria.Now we describe the central part of tim backwardA* search algorithm.
But we assume that the readersknow the A* algorithm, and exphtin only the way weapplied the algorithm to the problem.We consider a parse structure ,~q a state in A*search.
Two slates are e(plat if their parse  structureshave the same start position, end position, and com-bined state.
The backward search starts at the end ofthe input, sentence, and backtracks to the beginning ofthe sentence using tim path-map.Initial states are obtained by looking up the entriesof tim sentence nd position of the path-map.
The suc-cessor states are obtained by first, looking u 1) tim en-tries of the path-map at the start position of the cur-rent parse,  then cbecldng whether they satisfy the con-straint of the combined state transition in the secondorder IIMM, aim whether the transition is allowed bythe tagging model.
The combined state transition con-straint means that tim part of speech sequence in theparse .n th -order -s ta te  of the current parse,  ignor-4 In Figure 1, function transprob returns the probability ofgiven trlgraln.
Functions i n i t ia l - s tep  and f ina l - s tep  treat\ [be  t l 'a l iS l t \ [ons  I%L sltl i l l~llce \] ,Ol l l |dl l l ' ieg,203ing the last element, equals that of tile previous parse,ignoring the first element.The state transition cost of the backward search isthe product of the part of speech trigram probabilityand the word output probability.
Tile score estimateof the remaining portion of a path is obtained from theparse .prob-so -~ar  slot in the parse structure.The backward search generates the N best hypothe-ses sequentially and there is no need to preset N. Thecomplexity of the backward search is significantly lessthan that of the forward search.4 Word  Mode lTo handle open text, we have to cope with unknownwords.
Since Japanese do not put spaces betweenwords, we have to identify unknown words at first.
Todo this, we can look at the spelling (character sequence)that may constitute a word, or look at the context oidentify words that are acceptable in this context.Once word hypotheses for unknown words are gener-ated, the proposed N-best algorithm will find tile mostlikely word segmentation a d part of speech assignmenttaking into account he entire sentence.
Therefore, wecan formalize the unknown word problem as (letermin-ing the span of an unknown word, assigning its part ofspeech, and estimating its probability given its part ofspeech.Let us call a computational model that determinesthe probability of any word hypothesis given its lexi-cal form and its part of speech the "word model".
Theword model must account for morphology and word for-marion to estimate the part of speech and tile probabil-ity of a word hypothesis.
For tile first approxinmtion,we used the character trigram of each part of sl)eech asthe word model.Let C = cic~.., c,~ denote the sequence of n charac-ters that constitute word zv whose part of speech is t.We approximate the probability of the word given partof speech P(wlt ) by tile trigram probabilities,p(,,,Iz) = P,(C) - -  f',(~,l#, #)~',(~1#, <)uIX P,(~,lc+-=, ~ -~)r,(#1c.._l, ..,)i=3(9)where special symbol "#" indicates ttle word boundarymarker.
Character trigram probabilities are estimatedfrom the training corpus by computing relative fre-quency of character bigram and trigram that appearedin words tagged as t.Pt(cilci-2, q - i )  = f,(c~l~-=, ?,-~) = Nt(ci_2, Ci_l, ci)N,(c~_.~, i)(lO)where Nt(ci_2,ci_~,ci) is tile total number of timescharacter trigram ci_2ci_~el appears in words taggedas t in the training corpus.
Note that the charactertrigram probabilities reflect the frequency of word to-kens in tile training corpus.
Since there are more than3,000 characters in Japanese, trigram probabilities aresmoothed by interpolated estimation to cope with thesparse-data problem.It is ideal to make this character trigram model forall open clmss categories, llowever, the amount of train-ing data is too small for low frequency categories if wedivide it by part of speech tags.
Therefore, we madetrigram models only for tile 4 most frequent parts ofspeech that are open categories and have no conju~gation.
They are common noun, proper noun, sahenno/ln5~ and nun lera l .> (est imate-paxt-of-spoech ~ )  ; Hiyako Hotel( C ~ \ ]  2.7621915641723623E-7)(~f~/~ 6.3406095003694205E-9)(~ l~\ ]  5,840424519473811E-19)(~(~ 5.7364195413101E-29))> (est imate-part-of-speech ~ I 9 9 4 )((~()~ 1,8053860295767367E-6)(~M o.s1224s6sls404~zE-17)(~-Jdrl:~"~%~ 2.288684007246524E-17)(~,~ 7.sos~s3~aso211e-20)); p roper  noun; common noun; sa_han noun; numeral; numeral; proper noun; common noun; sahen nounFigure 2: N-best Tags for Unknown WordsFigure 2 show two examples of part of speech estima-tion for unknown words.
Each trigram model returnsa probability if the input string is a word belonging tothe category.
In both examples, the correct categoryhas the largest probability.> (get -lef tmo st-subst riags-uit h-word-model( ( i~ 4) -~M 2.519457'597358691E-7)(~ ~;~f:~"~j~ 2.3449215070189967E-8)(~ ~tlfj/~l~i\] 7.02439907471337451{-9)(~\]i,~, '1  2.375650975098567E-9)(',l~J~ "4.
)'~;'~$il 5.706S'(4990251415E-IO)(t~.~ '~j~cj~,\] 4.735628004876359E-13)(~,~ ~$~1 8.9289423481071831{-14)( i~b  ~)'~,~i l  7.266613344265452E-14)(~1~,~ \[~i~ 6.866d9949613207E-16)( , l~b 'RlJlIiG,~I 2.45302390s251351sE-17))Figure 3: N-Best Word lIylmthesesFigure 3 shows the N-best word hypotheses gener-ated by using tile character trigram models.
A wordhypothesis is a list of word boundary, part of speechassignment, and word probability that matches tile left-most substrings starting at a given position in tile inputsentence.
In the forward search, to handle unknownwords, word hypotheses are generated at every posi-tion in addition to the ones generated by the functionleftmost-subs~;r ings,  which are the words found illtile dictionary, llowever, ill our system, we limited thentunl)er of word hyl)otheses generated at each positionto 10, for efficiency reasons.aA noun tlmt can be used a~s a verb when it is followed by aforlna,\] verb "s~tr~t",2045 Eva luat ion  MeasuresWe applied the performance measures for Englishparsers \[1\] to Japanese morphological analyzers.
Thebasic idea is that morphological nalysis for a sentencecan be thought of as a set of labeled brackets, where abracket corresponds to word segmentation and its la-.bel corresponds to part of speech.
We then comparethe brackets contained in the system's output to thebrackets contained in the standard analysis.
For theN-best candidate, we will make the union of t\],e brack-ets contained in each candidate, and compare thenr tothe brackets in the standard.For comparison, we court{, the number of I)rackctsin the standard data (Std), the number of brackets inthe system output (Sys), and the nunlber of match-ing brackets (M).
We then calculate the nleasurcs ofrecall (= M/Std) and precision (= M/Sys).
We alsoconnt the number of crossings, which is tile mmtber ofc,'mes where a bracketed sequence from the standarddata overlaps a bracketed sequence from tile systemoutput, but neither sequence is completely coutainedin the other.We defined two equaiity criteria of brackets forcounting tim number of matching brackets.
Two brack-ets are unlabeled-bracket-equal if the boundaries of thetwo brackets are tile same.
Two brackets are labeled-bracket.equal if the labels of the brackets ark the samein addition to unlabeled-I)racket-equal.
In comparingthe consistency of the word segmentations of two brack-clings, wllich we call structure-consistency, we countthe measures (recall, precision, crossings) by unlabeled-bracket-equal.
In comparing the consistency of partof speech assignment in addition to word segmenta-tion, which we call label-consistency, we couut them bylabeled-bracket-equal.-31.90894138309038-38 .
S9433~3fi658235fi~b/tRllDll~iil-ill!lll,Til~l t-J-/lJ/ltlDil, l .
l~k  o I i fd~)-43, I0367483fi46801Figure 4: N-Best Morphological Analysis hypothesesFor example, Figure 4 shows a sample of N-hest anal-ysls hypotheses, where the first candidate is the correctanalysis a.
For the second candhlate, since there are !
)})rackets in tim correct data (Std=9), 11 brackets in thesecond candidate (Sys=l l ) ,  and 8 nlatciiing brackets(M=8), tile recall and precision with respect to labelconsistency are 8/9 and 8/11, respectively.
For the top6Probabilities m'e in liiltura\] log b~se .two candidates, since tliere ;ire 12 distinct brackets intile systems otll.litlt and 9 Inatehing brackets, tile re-call and precision with respect o hal)el consistency are9/9 aud 9/12, respeetiwqy.
For the third candidate,since the correct data and the third candidate differin just one part of Sl)eech tag, the recall and precisionwittl respect o structure consistency are 9/9 and 9/9,respectiw>ly.6 Exper imentTable 2: The aillount of training and test data ~_~ trahling texts closed test open 10 o 0 Sentences / -1~5 " 10{i0 13899 Words ' 149059 13176 \[Characters _ 267,122 \[ 9422~ 98997We used the NI'I~ Dialogue Databaae\[5\] to train andtest the proposed morphological nalysis method.
It isa corpus of approxiumtely 800,000 words whose wordsegmentatio,l and part of speech tag assigmnent werelaboriously performed by hand.
In tiffs experilneut, weonly used one fourth of the A'Ft~.
Corl)us , a portion ofthe keyl)oard dialogues in the conference registrationdomain.
First, we selected 1,000 test sentences for allopen test, arid used I.he others for training.
Tile corpuswas divided into 90% R)r training and 10% for test-ing.
We then selected 1,000 sentences from tile traiu-ing set and used them for a closed test.
The number ofsentences, words, and characters for each test set andtraining texts are shown iu 'Pable 2.The training texts contained 6580 word types and6945 tag trigram types.
There were 247 unknownword types and 213 unknown tag trigram types in timopen test senteuces.
Thus, both part of speech tri-gralrl l)robabilities alld word output probabilities mustbe snioothed to handle open texts.Table 3: Perccld.
;ige of words correctly segmented andtagged: raw part o\[' speech bigram aud trigrmnI 2 I 98'{l% I 8 9 .
7 ' ~ ~  \[90.7% \[ 0.007 \[I :~\[ os.,~:~ \[ 8 a .
~ .
s ~  \] 84.a% \[ o.m2 \]I '~  9a.
'2% I 7 s .
~ / o  I 79.6% I o.o15 Ik 5 I <~l''i''i?
I > i n ' / ~ %  I r(~.o~ I o.o~s_lFirst, as a I)reliminary experiment, we compared tileperforn)ances of part of speech bigram and trigram.Table 3 shows the percentages of words correctly seg-mented and tagged, tested on the closed test sentences.The trigram model achiew;d 97.5% recall and 97.8%precision flu" the top candidate, while tile bigram modelachiew.
'd 96.2% recall and 96.6% precision.
Althoughboth tagging models sllow very high l)erformanee, tile20,5trigram model outperformed tile bigram model in everymetric.We then tested the proposed system, which usessmoothed part of speech trigram with word model, onthe open test sentences.
Table 4 shows tile percentagesof words correctly segmented and tagged.
In Table 4,label consistency 2 represents the accuracy of segmen-tation and tagging ignoring the difference in conjuga-tion form.For open texts, tile morphological nalyzer achieved95.1% recall and 94.6% precision for the top candidate,and 97.8% recall and 73.2% precision for the 5 bestcandidates.
This performance is very encouraging, andis comparable to the state-of-the-art stochastic taggerfor English \[2-4, 10, 11\].Since the segmentation accuracy of the proposed sys-tem is relatively high (97.7% recall and 97.2% precisionfor the top candidate) compared to the morphologi-cal analysis accuracy, it is likely that we can improvethe part of speech assignment accuracy by refining thestatistically-based tagging model.
We find a fair num-ber of tagging errors happened in conjugation forms.We assume that this is caused by the fact that theJapanese tag set used in tile ATR.
Corpus is not de-tailed enough to capture the complicated Japanese verbmorphology.1009590$580q75906560Hogpho loq lca l  Ana ly l t s  Accuracy  fo r  N - I les t  Sentencesr iw  t r tq ram (c losed  ce :~t)  -a-.--raw b i t / tam , a ~ t , ?
~a~othed  t r t~ram wi th  Iopen  text )  -o , .
.Imoothed  t r lq ram wi  word  a lo t le l  l opes  te~t )  -~ ....raw m wi th  word  moOel  l apen  text}  ~.
-iraw t r  r l ra  w i thout  word  nloc~el lopes  text )  -.12-::-.. .
.
.
-+ .
.
.
.
.
.
.
.
.
.
.
.
., :  .
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I I , I2 3 4R inkFigure 5: Tile percentage of sentences correctly seg-mented and tagged.Figure 5 shows tile percentage of sentences (notwords) correctly segmented and tagged.
For open texts,the sentence accuracy of the raw part of speech trigramwithout word model is 62.7% for the top candidate and70.4% for the top-5, while that of smoothed trigramwith word model is 66.9% for the top and 80.3% for thetop-5.
We can see that, by smoothing tile part ofsllecchtrigram and by adding word model to handle unknownwords, the accuracy and robustness of the morpholog-ical analyzer is significantly improved.
Ilowever, tilesentence accuracy for closed texts is still significantlybetter that that for ol)en texts.
It is clear that moreresearch as to be done on the smoothing problem.7 Discuss ionMorphological analysis is an important practical prob-lem with potential apl)lication in many areas includingkana-to-kanji conversion 7, speech recognition, charac-ter recognition, speech synthesis, text revision support,information retrieval, and machine translation.Most conventional Japanese morphological nalyzersuse rule-based heuristic searches.
They usually use aconnectivity rnatrix (part-of-sl)eech-pair grammar) ,asthe language model.
To rank the morphological nal-ysis hypotheses, they usually use heuristics uch asLongest Match Method or Least Bunsetsu's NumberMethod \[16\].There are some statistically-based approaches toJapanese morphological nalysis.
The tagging modelspreviously used are either part of speech I)igram \[9, 14\]or Character-based IIMM \[12\].Both hem'istic-based and statistically-based ap-proaches use t.he Minimum Connective-Cost Method\[7\], which is a linear time dynamic programming algo-rithm that finds the morphological hypothesis that hastile minimal connective cost (i.e.
bigram-ba~sed cost)as derived by certain criteria.q'o handle unknown words, most Japanese morpho-logical analyzers u,~e character type heuristics \[17\],which is "a string of the same character type is likely toconstitute a word".
There is one stochastic approachthat uses bigram of word formation unit \[13\].
tlowever,it does not learn probabilities from training texts, butlearns them fi'om machine readable dictionaries, andthe model is not incorporated in working morphologi-cal analyzers, as fitr as the author knows.The unique features of the proposed Japanese mor-phological analyzer is that it can find tile exact N mostlikely hyl)otheses using part of speech trigram, and itcan handle unlmown words using character trigram.The algoril.hm can naturally be extended to handle anyhigher order Markov models.
Moreover, it can nat-nrally be extended to handle lattice-style input thatis often used as t.he output of speech recognition andcharacter ecognition systems, by extending the func-tion ( le f tmost -subat r inga)  so as to return a list ofwords in the dictionary that matches the substrings intile input lattice stm'ting at the specified p(xqition.For future wot'k, we have to study the most effectiveway of generating word hypotheses that can handh.
?
un:known words.
Currently, we are limiting the number ofword hypotheses to reduce ambiguity at tile cost of ac-curacy.
We have also to study tile word model for opencategories thai, have conjugation, because the training7Kana- to -kan j i  convers ion  i s  a pop~alar  J I Lpanese  inputmethod on computer using ASCII keyboard.
Phonetic tranHerip-tion by Roams (ASCII) characters are input and converted fir, stto the Japanese syllabary hiragana which is then converted toorthographic trm~scrlption ncluding Chinese character kanjl.206Table 4: The percentage of words correctly segmented and tagged: smoothed trigram with word modelsmoothed trigram with word model (open text)lal)el consistencyrecall precision crossings95.1% 94.6% 0.01396.5% 88.0% 0.02397.3% 82.1% 0.03197.6% 77.4% 0.0'1697.8% 73.2% 0.061label consistency 2recall precision crossings95.9% 95.4% (J.01397.0% 90.3% 0.02397.6% 85.1% 0.03197.9% 80.7% 0.04698.1% 77.1% 0.060structure consistencyrecall precision97.7% 97.2%98.2% 94.4%98.5% 91.7%98.7% 89.6%98.8% 87.9%crossings0.0130.0220.0290.0440.056data gets too small to make trigrams if we divide it bytags.
We will probably have to tie some parameters tosolve the insufficient data problem.Moreover, we have to study the method to adapt hesystem to a new domain.
To develop an m~supervisedlearning method, like the forward-backward algorithmfor IIMM, is an urgent goal, since we can't always ex-pect the availability of manually segmented and taggeddata.
We can think of an EM algorithm by replacingmaximization with summation in the extended Viterbialgorithm, but we don't know how to handle unknownwords in this algorithm.8 Conc lus ionWe have developed a stochastic Japanese morphologi-cal analyzer.
It uses a statistical tagging model and anefficient two-pass earch algorithm to llnd the N bestmorphological nalysis hypotheses for the input sen-tence.
Its word segmentation a d tagging accuracy isapproxlmatcly 95%, which is comparable to the star.e-of-the-art stochastic tagger for English.\[7\]\[8\]\[9\]\[10\]\[11\]\[13\]Re ferences\[14\]\[1\] Black, E. et al: "A Procedure for Quantit.a-tively Comparing the Syntactic Coverage of En-glish Grammars", I)AIH'A Speech an(I Nalm'a\]Language Workshop, pp.306-311, Morgan Kauf- \[15\]mann, 1991.\[2\] Charniak, E., Ilendrickson, C., Jacol)son, N.,and Perkowitz, M.: "Equations for Part-of~SpeechTagging", AAAI-93, I)1).784-789, 1993.
\[16\]\[3\] Church, K.: "A Stochastic Part of Speech Taggerand Noun Phrase Parser for English", ANLP-88,pp.136-143, 1988.\[4\] Cutting, D., Kupiec, J., Pederseu, J., and Sibnn,P.
: "A Practical Part-of-Speech Tagger", ANLP- \[17\]92, pp.133-140, 1992.\[5\] Ehara, T., Ogura, K. and Morimoto, T.: "h'rl~Dialogue Database," 1CSLP-90, pp.1093-1096,1990.\[6\] IIe, Y.: "Extended Viterbi Algorithm for SecondOrder Ilidden Markov Process", ICI'R-88, pp.718-720, 1988.Ilisamitsu, T. and Nitta, Y.: "MorphologicalAnalysis by Minimum Connetivc-Cost Method",'\['echnical Report S/GNLC 90-8, IEICE, pp.17-24,1990 (in Japanese).Jelinek, F.: "Self-organized language modeling forspeech recognition", IBM Report, 1985 (Reprintedin Readings in Speech Recognition, 1)i).450-506).Matsunobu, E., lIitaka, T., and Yoshida, S.: "Syn-t.actic Analysis by Stochastic P, UNSETSU Gram-mar", Technical Rel)ort SIGNL 56-3, IPSJ, 1986(in Japanese).Meriahlo, 1t.
: "Tagging Text with a ProbabilisticModer', ICASSP-9I, pp.809-812, 1991.Meteer, M. W., Schwartz, R. and Weischedel, R.:"I'OST: Using l)robal)ilities in Language Process-ing', lJCAI-9 t, pp.960-965, 1991.Murakaini, J. and Sagayama, S.: "llidden MarkovModel applied to Morphological Analysis", 45thNational Meeting of the IPSJ, Vol.3, pp.161-162,1992 (in Japanese).Nagai, I1.
and llital?a, T.: "Japanese Word For-marion Model and Its Evahmtion", Trans IPSJ,Vol.34, No.9, pp.1944-1955, 1993 (in Japanese).Sakai, S.: "Morphological Category l~';igram: ASingle Language Model for botl, Spoken l,anguageand Text", ISS1)-93, I)1).87-90, 1993.Soong, F. K. aml lluang E.: "A Tree-TrellisBased Fast Search for Finding the N Best Sen-tence llypotheses in Continuous Speech Recogni-tion", ICASS P-9 I, pp.705-708, 1991.Yoshinmra, K, llitaka, T., and Yoshida, S.: "Mor-phological Analysis of Non-marked-off JapaneseSentences hy the Least llUNSETSU's NumberMethod", 't'rans.
I1'$3, Vol.24, No.l, pp.40-46,19811 (in Japanese).Yoshimura, K., Takeuchi, M., Tsuda, K.and Shudo, K.: "Morphological Analysis ofJapanese Sent.ences Containing Unknown Words",Trans.
IPSJ, Vol.30, No.3, pp.294-301, 1989 (inJapanese).207
