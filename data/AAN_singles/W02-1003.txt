An Incremental Decision List LearnerJoshua GoodmanMicrosoft ResearchOne Microsoft WayRedmond, WA 98052joshuago@microsoft.comAbstractWe demonstrate a problem with the stan-dard technique for learning probabilisticdecision lists.
We describe a simple, in-cremental algorithm that avoids this prob-lem, and show how to implement it effi-ciently.
We also show a variation that addsthresholding to the standard sorting algo-rithm for decision lists, leading to similarimprovements.
Experimental results showthat the new algorithm produces substan-tially lower error rates and entropy, whilesimultaneously learning lists that are overan order of magnitude smaller than thoseproduced by the standard algorithm.1 IntroductionDecision lists (Rivest, 1987) have been used for avariety of natural language tasks, including accentrestoration (Yarowsky, 1994), word sense disam-biguation (Yarowsky, 2000), finding the past tense ofEnglish verbs (Mooney and Califf, 1995), and sev-eral other problems.
We show a problem with thestandard algorithm for learning probabilistic deci-sion lists, and we introduce an incremental algorithmthat consistently works better.
While the obvious im-plementation for this algorithm would be very slow,we also show how to efficiently implement it.
Thenew algorithm produces smaller lists, while simul-taneously substantially reducing entropy (by about40%), and error rates (by about 25% relative.
)Decision lists are a very simple, easy to understandformalism.
Consider a word sense disambiguationtask, such as distinguishing the financial sense of theword ?bank?
from the river sense.
We might want thedecision list to be probabilistic (Kearns and Schapire,1994) so that, for instance, the probabilities can bepropagated to an understanding algorithm.
The de-cision list for this task might be:IF ?water?
occurs nearby, output ?river: .95?, ?fi-nancial: .05?ELSE IF ?money?
occurs nearby, output ?river: .1?,?financial: .9?ELSE IF word before is ?left?, output ?river: .8?,?financial: .2?ELSE IF ?Charles?
occcurs nearby, output ?river:.6?, ?financial: .4?ELSE output ?river: .5?, ?financial: .5?The conditions of the list are checked in order, andas soon as a matching rule is found, the algorithmoutputs the appropriate probability and terminates.If no other rule is used, the last rule always triggers,ensuring that some probability is always returned.The standard algorithm for learning decision lists(Yarowsky, 1994) is very simple.
The goal is to min-imize the entropy of the decision list, where entropyrepresents how uncertain we are about a particular de-cision.
For each rule, we find the expected entropyusing that rule, then sort all rules by their entropy,and output the rules in order, lowest entropy first.Decision lists are fairly widely used for many rea-sons.
Most importantly, the rule outputs they produceare easily understood by humans.
This can make de-cision lists useful as a data analysis tool: the decisionlist can be examined to determine which factors aremost important.
It can also make them useful whenthe rules must be used by humans, such as when pro-ducing guidelines to help doctors determine whethera particular drug should be administered.
Decisionlists also tend to be relatively small and fast and easyAssociation for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
17-24.Proceedings of the Conference on Empirical Methods in Naturalto apply in practice.Unfortunately, as we will describe, the standard al-gorithm for learning decision lists has an importantflaw: it often chooses a rule order that is suboptimalin important ways.
In particular, sometimes the al-gorithm will use a rule that appears good ?
has loweraverage entropy ?
in place of one that is good ?
low-ers the expected entropy given its location in the list.We will describe a simple incremental algorithm thatconsistently works better than the basic sorting al-gorithm.
Essentially, the algorithm builds the list inreverse order, and, before adding a rule to the list,computes how much the rule will reduce entropy atthat position.
This computation is potentially veryexpensive, but we show how to compute it efficientlyso that the algorithm can still run quickly.2 The AlgorithmsIn this section, we describe the traditional algorithmfor decision list learning in more detail, and then mo-tivate our new algorithm, and finally, describe ournew algorithm and variations on it in detail.
For sim-plicity only, we will state all algorithms for the binaryoutput case; it should be clear how to extend all ofthe algorithms to the general case.2.1 Traditional AlgorithmDecision list learners attempt to find models thatwork well on test data.
The test data consists of a se-ries of inputs x1, ..., xn, and we are trying to predictthe corresponding results y1, ..., yn.
For instance, ina word sense disambiguation task, a given xicouldrepresent the set of words near the word, and yicould represent the correct sense of the word.
Givena model D which predicts probabilities PD(y|x),the standard way of defining how well D works isthe entropy of the model on the test data, definedas?ni=1?log2PD(yi|xi).
Lower entropy is better.There are many justifications for minimizing entropy.Among others, the ?true?
probability distribution hasthe lowest possible entropy.
Also, minimizing train-ing entropy corresponds to maximizing the probabil-ity of the training data.Now, consider trying to learn a decision list.
As-sume we are given a list of possible questions,q1, ..., qn.
In our word sense disambiguation ex-ample, the questions might include ?Does the word?water?
occur nearby,?
or more complex ones, suchas ?does the word ?Charles?
occur nearby and is theword before ?river.??
Let us assume that we havesome training data, and that the system has two out-puts (values for y), 0 and 1.
Let C(qi, 0) be thenumber of times that, when qiwas true in the train-ing data, the output was 0, and similarly for C(qi, 1).Let C(qi) be the total number of times that qiwastrue.
Now, given a test instance, x, y for which qi(x)is true, what probability would we assign to y = 1?The simplest answer is to just use the probability inthe training data,C(qi, 1)/C(qi).
Unfortunately, thistends to overfit the training data.
For instance, if qiwas true only once in the training data, then, depend-ing on the value for y that time, we would assign aprobability of 1 or 0.
The former is clearly an over-estimate, and the latter is clearly an underestimate.Therefore, we smooth our estimates (Chen and Good-man, 1999).
In particular, we used the interpolatedabsolute discounting method.
Since both the tradi-tional algorithm and the new algorithm use the samesmoothing method, the exact smoothing techniquewill not typically affect the relative performance ofthe algorithms.
Let C(0) be the total number of ysthat were zero in the training, and let C(1) be the to-tal number of ys that were one.
Then, the ?unigram?probability y is P (y) = C(y)C(0)+C(1).
Let N(qi) be thenumber of non-zero ys for a given question.
In par-ticular, in the two class case, N(qi) will be 0 if therewere no occurences of the question qi, 1 if trainingsamples for qialways had the same value, and 2 ifboth 1 and 0 values occurred.
Now, we pick somevalue d (using heldout data) and discount all countsby d. Then, our probability distribution isP (y|qi) =???
(C(qi,y)?d)C(qi)+dN(qi)C(qi)P (y) if C(qi, y) > 0dN(qi)C(qi)P (y) otherwiseNow, the predicted entropy for a question qiis justentropy(qi) = ?P (0|qi)log2P (0|qi)?P (1|qi)log2P (1|qi)The typical training algorithm for decision lists isvery simple.
Given the training data, compute thepredicted entropy for each question.
Then, sort thequestions by their predicted entropy, and output adecision list with the questions in order.
One of thequestions should be the special question that is al-ways TRUE, which returns the unigram probability.Any question with worse entropy than TRUE willshow up later in the list than TRUE, and we willnever get to it, so it can be pruned away.2.2 New AlgorithmConsider two weathermen in Seattle in the winter.Assume the following (overly optimistic) model ofSeattle weather.
If today there is no wind, then to-morrow it rains.
On one in 50 days, it is windy, and,the day after that, the clouds might have been sweptaway, leading to only a 50% chance of rain.
So,overall, we get rain on 99 out of 100 days.
The lazyweatherman simply predicts that 99 out of 100 days,it will rain, while the smart weatherman gives the trueprobabilities (i.e.
100% chance of rain tomorrow ifno wind today, 50% chance of rain tomorrow if windtoday.
)Consider the entropy of the two weathermen.The lazy weatherman always says ?There is a 99%chance of rain tomorrow; my average entropy is?.99?
log2.99 ?
.01 ?
log2.01 = .081 bits.?
Thesmart weatherman, if there is no wind, says ?100%chance of rain tomorrow; my entropy is 0 bits.?
Ifthere is wind, however, the smart weatherman says,?50% chance of rain tomorrow; my entropy is 1 bit.
?Now, if today is windy, who should we trust?
Thesmart weatherman, whose expected entropy is 1 bit,or the lazy weatherman, whose expected entropy is.08 bits, which is obviously much better.The decision list equivalent of this is as follows.Using the classic learner, we learn as follows.
Wehave three questions: if TRUE then predict rain withprobability .99 (expected entropy = .081).
If NOWIND then predict rain with probability 1 (expectedentropy = 0).
If WIND then predict rain with proba-bility 1/2 (expected entropy = 1).
When we sort theseby expected entropy, we get:IF NO WIND, output ?rain: 100%?
(entropy 0)ELSE IF TRUE, output ?rain: 99%?
(entropy .081)ELSE IF WIND, output ?rain: 50%?
(entropy 1)Of course, we never reach the third rule, and on windydays, we predict rain with probabiliy .99!The two weathermen show what goes wrong witha naive algorithm; we can easily do much better.
Forthe new algorithm, we start with a baseline ques-tion, the question which is always TRUE and pre-list = { TRUE }dofor each question qientropyReduce(i) =entropy(list)?
entropy(prepend(qi, list))l = i such that entropyReduce(i) is largestif entropyReduce(l) <  thenreturn listelselist = prepend(ql, list)Figure 1: New Algorithm, Simple Versiondicts the unigram probabilities.
Then, we find thequestion which if asked before all other questionswould decrease entropy the most.
This is repeateduntil some minimum improvement, , is reached.1Figure 1 shows the new algorithm; the notationentropy(list) denotes the training entropy of a poten-tial decision list, and entropy(prepend(qi, list)) indi-cates the training entropy of list with the question ?Ifqithen output p(y|qi)?
prepended.Consider the Parable of the Two Weathermen.
Thenew learning algorithm starts with the baseline: IfTRUE then predict rain with probability 99% (en-tropy .081).
Then it prepends the rule that reducesthe entropy the most.
The entropy reduction fromthe question ?NO WIND?
is .081?
.99 = .08, whilethe entropy for the question ?WIND?
is 1 bit forthe new question, versus .5 ?
1 + .5 ?
?log2.01 =.5 + .5 ?
6.64 = 3.82, for the old, for a reductionof 2.82 bits, so we prepend the ?WIND?
question.Finally, we learn (at the top of the list), that if ?NOWIND?, then rain 100%, yielding the following de-cision list:IF NO WIND, output ?rain: 100%?
(entropy 0)ELSE IF WIND, output ?rain: 50%?
(entropy 1)ELSE IF TRUE, output ?rain: 99%?
(entropy .081)Of course, we never reach the third rule.Clearly, this decision list is better.
Why did ourentropy sorter fail us?
Because sometimes a smartlearner knows when it doesn?t know, while a dumbrule, like our lazy weatherman who ignores the wind,doesn?t know enough to know that in the current sit-1This means we are building the tree bottom up; it would beinteresting to explore building the tree top-down, similar to adecision tree, which would probably also work well.list = {TRUE}for each training instance xj, yjinstanceEnt(j) = ?log2p(yj)for each question qi// Now we compute entropyReduce(i) =// entropy(TRUE)?
entropy(qi,TRUE)entropyReduce(i) = 0for each xj, yjsuch that qi(xj)entropyReduce(i) += log2p(yj)?
log2p(yj|qi)dol = argmaxientropyReduce(i)if entropyReduce(l) <  thenreturn listelselist = prepend(ql, list)for each xj, yjsuch that ql(xj)for each k such that qk(xj)entropyReduce(k) += instanceEnt(j)instanceEnt(j) = ?log2p(yj|ql)for each k such that qk(xj)entropyReduce(k) ?= instanceEnt(j)Figure 2: New Algorithm, Efficient Versionuation, the problem is harder than usual.2.2.1 EfficiencyUnfortunately, the algorithm of Figure 1, if imple-mented in a straight-forward way, will be extremelyinefficient.
The problem is the inner loop, whichrequires computing entropy(prepend(qi, list)).
Thenaive way of doing this is to run all of the trainingdata through each possible decision list.
In practice,the actual questions tend to be pairs or triples of sim-ple questions.
For instance, an actual question mightbe ?Is word before ?left?
and word after ?of???
Thus,the total number of questions can be very large, andrunning all the data through the possible new decisionlists for each question would be extremely slow.Fortunately, we can precompute entropyReduce(i)and incrementally update it.
In order to do so, we alsoneed to compute, for each training instance xj, yjtheentropy with the current value of list.
Furthermore,we store for each question qithe list of instancesxj, yjsuch that qi(xj) is true.
With these changes,the algorithm runs very quickly.
Figure 2 gives theefficient version of the new algorithm.for each question qicompute entropy(i)list = questions sorted by entropy(i)remove questions worse than TRUEfor each training instance xj, yjinstanceEnt(j) = ?log2p(yj)for each question qiin list in reverse orderentropyReduce = 0for each xj, yjsuch that qi(xj)entropyReduce +=instanceEnt(j)?
log2p(yj|qi)if entropyReduce < remove qifrom listelsefor each xj, yjsuch that qi(xj)instanceEnt(j) = log2p(yj|qi)Figure 3: Compromise: Delete Bad QuestionsNote that this efficient version of the algorithmmay consume a large amount of space, because of theneed to store, for each question qi, the list of traininginstances for which the question is true.
There are anumber of speed-space tradeoffs one can make.
Forinstance, one could change the update loop fromfor each xj, yjsuch that qi(xj)tofor each xj, yjif qi(xj) then ...There are other possible tradeoffs.
For instance, typ-ically, each question qiis actually written as a con-junction of simple questions, which we will denoteQij.
Assume that we store the list of instances thatare true for each simple question Qij, and that qiis ofthe form Qi1&Qi2&...&QiI.
Then we can write anupdate loop in which we first find the simple questionwith the smallest number of true instances, and loopover only these instances when finding the instancesfor which qiis true:k = argminjnumber instances such that Qijfor each xj, yjsuch that Qik(xj)if qi(xj) then ...2.3 Compromise AlgorithmNotice the original algorithm can actually allow ruleswhich make things worse.
For instance, in our lazyweatherman example, we built this decision list:IF NO WIND, output ?rain: 100%?
(entropy 0)ELSE IF TRUE, output ?rain: 99%?
(entropy .081)ELSE IF WIND, output ?rain: 50%?
(entropy 1)Now, the second rule could simply be deleted, and thedecision list would actually be much better (althoughin practice we never want to delete the ?TRUE?
ques-tion to ensure that we always output some probabil-ity.)
Since the main reason to use decision lists is be-cause of their understandability and small size, thisoptimization will be worth doing even if the full im-plementation of the new algorithm is too complex.The compromise algorithm is displayed in Figure 3.When the value of  is 0, only those rules that improveentropy on the training data are included.
When thevalue of  is ?
?, all rules are included (the stan-dard algorithm).
Even when a benefit is predicted,this may be due to overfitting; we can get furtherimprovements by setting the threshold to a highervalue, such as 3, which means that only rules thatsave at least three bits ?
and thus are unlikely to leadto overfitting ?
are added.3 Previous WorkThere has been a modest amount of previous workon improving probabilistic decision lists, as well asa fair amount of work in related fields, especially intransformation-based learning (Brill, 1995).First, we note that non-probabilistic decision listsand transformation-based learning (TBL) are actu-ally very similar formalisms.
In particular, as ob-served by Roth (1998), in the two-class case, theyare identical.
Non-probabilistic decision lists learnrules of the form ?If qithen output y?
while TBLsoutput rules of the form ?If qiand current-class isy?, change class to y?.
Now, in the two class case, arule of the form ?If qiand current-class is y?, changeclass to y?
is identical to one of the form ?If qichangeclass to y?, since either way, all instances for whichqiis TRUE end up with value y.
The other differencebetween decision lists and TBLs is the list ordering.With a two-class TBL, one goes through the rulesfrom last-to-first, and finds the last one that applies.With a decision list, one goes through the list in or-der, and finds the first one that applies.
Thus in thetwo-class case, simply by changing rules of the form?If qiand current-class is y?, change class to y?
to?If qioutput y?, and reversing the rule order, we canchange any TBL to an equivalent non-probabilisticdecision list, and vice-versa.
Notice that our incre-mental algorithm is analogous to the algorithm usedby TBLs: in TBLs, at each step, a rule is added thatminimizes the training data error rate.
In our prob-abilistic decision list learner, at each step, a rule isadded that minimizes the training data entropy.Roth notes that this equivalence does not hold inan important case: when the answers to questionsare not static.
For instance, in part-of-speech tagging(Brill, 1995), when the tag of one word is changed, itchanges the answers to questions for nearby words.We call such problems ?dynamic.
?The near equivalence of TBLs and decision lists isimportant for two reasons.
First, it shows the connec-tion between our work and previous work.
In partic-ular, our new algorithm can be thought of as a prob-abilistic version of the Ramshaw and Marcus (1994)algorithm, for speeding up TBLs.
Just as that al-gorithm stores the expected error rate improvementof each question, our algorithm stores the expectedentropy improvement.
(Actually, the Ramshaw andMarcus algorithm is somewhat more complex, be-cause it is able to deal with dynamic problems suchas part-of-speech tagging.)
Similarly, the space-efficient algorithm using compound questions atthe end of Section 2.2.1 can be thought of as astatic probabilistic version of the efficient TBL ofNgai and Florian (2001).The second reason that the connection to TBLs isimportant is that it shows us that probabilistic de-cision lists are a natural way to probabilize TBLs.Florian et al (2000) showed one way to make prob-abilistic versions of TBLs, but the technique is some-what complicated.
It involved conversion to a deci-sion tree, and then further growing of the tree.
Theirtechnique does have the advantage that it correctlyhandles the multi-class case.
That is, by using adecision tree, it is relatively easy to incorporate thecurrent state, while the decision list learner ignoresthat state.
However, this is not clearly an advantage?
adding extra dependencies introduces data sparse-ness, and it is an empirical question whether depen-dencies on the current state are actually helpful.
Ourprobabilistic decision lists can thus be thought of asa competitive way to probabilize TBLs, with the ad-vantage of preserving the list-structure and simplicityof TBL, and the possible disadvantage of losing thedependency on the current state.Yarowsky (1994) suggests two improvements tothe standard algorithm.
First, he suggests an op-tional, more complex smoothing algorithm than theone we applied.
His technique involves estimatingboth a probability based on the global probabilitydistribution for a question, and a local probability,given that no questions higher in the list were TRUE,and then interpolating between the two probabilities.He also suggests a pruning technique that eliminates90% of the questions while losing 3% accuracy; aswe will show in Section 4, our technique or varia-tions eliminate an even larger percentage of ques-tions while increasing accuracy.
Yarowsky (2000)also considered changing the structure of decisionlists to include a few splits at the top, thus combiningthe advantages of decision trees and decision lists.The combination of this hybrid decision list and theimproved smoothing was the best performer for par-ticipating systems in the 1998 senseval evaluation.Our technique could easily be combined with thesetechniques, presumably leading to even better results.However, since we build our decision lists from lastto first, rather than first to last, the local probability isnot available as the list is being built.
But there is noreason we could not interpolate the local probabilityinto a final list.
Similarly, in Yarowsky?s technique,the local probability is also not available at the timethe questions are sorted.Our algorithm can be thought of as a natural prob-abilistic version of a non-probabilistic decision listlearner which prepends rules (Webb, 1994).
Onedifficulty that that approach has is ranking rules.
Inthe probabilistic framework, using entropy reductionand smoothing seems like a natural solution.4 Experimental Results and DiscussionIn this section, we give experimental results, showingthat our new algorithm substantially outperforms thestandard algorithm.
We also show that while accu-racy is competitive with TBLs, two linear classifiersare more accurate than the decision list algorithms.Many of the problems that probabilistic decisionlist algorithms have been used for are very similar: ina given text context, determine which of two choicesis most appropriate.
Accent restoration (Yarowsky,1994), word sense disambiguation (Yarowsky, 2000),and other problems all fall into this framework, andtypically use similar feature types.
We thus choseone problem of this type, grammar checking, andbelieve that our results should carry over at leastto these other, closely related problems.
In partic-ular, we chose to use exactly the same training, test,problems, and feature sets used by Banko and Brill(2001a; 2001b).
These problems consisted of try-ing to guess which of two confusable words, e.g.?their?
or ?there?, a user intended.
Banko and Brillchose this data to be representative of typical machinelearning problems, and, by trying it across data sizesand different pairs of words, it exhibits a good deal ofdifferent behaviors.
Banko and Brill used a standardset of features, including words within a window of2, part-of-speech tags within a window of 2, pairs ofword or tag features, and whether or not a given wordoccurred within a window of 9.
Altogether, they had55 feature types.
They used all features of each typethat occurred at least twice in the training data.We ran our comparisons using 7 different algo-rithms.
The first three were variations on the stan-dard probabilistic decision list learner.
In particular,first we ran the standard sorted decision list learner,equivalent to the algorithm of Figure 3, with a thresh-old of negative infinity.
That is, we included all rulesthat had a predicted entropy at least as good as theunigram distribution, whether or not they would ac-tually improve entropy on the training data.
We callthis ?Sorted: ??.?
Next, we ran the same learnerwith a threshold of 0 (?Sorted: 0?
): that is, we in-cluded all rules that had a predicted entropy at leastas good as the unigram distribution, and that wouldat least improve entropy on the training data.
Thenwe ran the algorithm with a threshold of 3 (?Sorted:3?
), in an attempt to avoid overfitting.
Next, we ranour incremental algorithm, again with a threshold ofreducing training entropy by at least 3 bits.In addition to comparing the various decision listalgorithms, we also tried several other algorithms.First, since probabilistic decision lists are probabilis-tic analogs of TBLs, we compared to TBL (Brill,1995).
Furthermore, after doing our research on de-cision lists, we had several successes using simplelinear models, such as a perceptron model and a max-imum entropy (maxent) model (Chen and Rosenfeld,1999).
For the perceptron algorithm, we used a varia-tion that includes a margin requirement, ?
(Zaragozawj= 0for 100 iterations or until no changefor each training instance xj, yjif q(xj) ?
wj?
yj< ?wj+= q(xj)?
yjFigure 4: Perceptron Algorithm with Margin1M 10M 50MSorted: ??
14.27% 8.88% 6.23%Sorted: 0 13.16% 8.43% 5.84%Sorted: 3 10.23% 6.30% 3.94%Incremental: 3 10.80% 6.33% 4.09%Transformation 10.36% 5.14% 4.00%Maxent 8.60% 4.42% 2.62%Perceptron 8.22% 3.96% 2.65%Figure 5: Geometric Mean of Error Rate acrossTraining Sizesand Herbrich, 2000).
Figure 4 shows this incrediblysimple algorithm.
We use q(xj) to represent the vec-tor of answers to questions about input xj; wjis aweight vector; we assume that the output, yjis -1 or+1; and ?
is a margin.
We assume that one of thequestions is TRUE, eliminating the need for a sepa-rate threshold variable.
When ?
= 0, the algorithmreduces to the standard perceptron algorithm.
Theinclusion of a non-zero margin and running to con-vergence guarantees convergence for separable datato a solution that works nearly as well as a linearsupport vector machine (Krauth and Mezard, 1987).Given the extreme simplicity of the algorithm and thefact that it works so well (not just compared to thealgorithms in this paper, but compared to several oth-ers we have tried), the perceptron with margin is ourfavorite algorithm when we don?t need probabilities,and model size is not an issue.Most of our algorithms have one or more parame-ters that need to be tuned.
We chose 5 additional con-fusable word pairs for parameter tuning and choseparameter values that worked well on entropy anderror rate across data sizes, as measured on these 5additional word pairs.
For the smoothing discountvalue we used 0.7.
For thresholds for both the sortedand the incremental learner, we used 3 bits.
For theperceptron algorithm, we set ?
to 20.
For TBL?s min-imum number of errors to fix, the traditional value of1M 10M 50MSorted: ??
1065 10388 38893Sorted: 0 831 8293 31459Sorted: 3 45 462 1999Incremental: 3 21 126 426Transformation 15 77 244Maxent 1363 12872 46798Perceptron 1363 12872 46798Figure 6: Geometric Mean of Model Sizes acrossTraining Sizes 1M 10M 50MSorted: ??
0.91 0.70 0.55Sorted: 0 0.81 0.64 0.47Sorted: 3 0.47 0.43 0.29Incremental: 3 0.49 0.36 0.25Maxent 0.44 0.27 0.18Figure 7: Arithmetic Mean of Entropy across Train-ing Sizes2 worked well.
For the maxent model, for smooth-ing, we used a Gaussian prior with 0 mean and 0.3variance.
Since sometimes one learning algorithmis better at one size, and worse at another, we triedthree training sizes: 1, 10 and 50 million words.In Figure 5, we show the error rates of each algo-rithm at different training sizes, averaged across the10 words in the test set.
We computed the geomet-ric mean of error rate, across the ten word pairs.
Wechose the geometric mean, because otherwise, wordswith the largest error rates would disproportionatelydominate the results.
Figure 6, shows the geometricmean of the model sizes, where the model size is thenumber of rules.
For maxent and perceptron mod-els, we counted size as the total number of features,since these models store a value for every feature.For Sorted: ??
and Sorted: 0, the size is similar toa maxent or perceptron model ?
almost every rule isused.
Sorted: 3 drastically reduces the model size ?by a factor of roughly 20 ?
while improving perfor-mance.
Incremental: 3 is smaller still, by about anadditional factor of 2 to 5, although its accuracy isslightly worse than Sorted: 3.
Figure 7 shows the en-tropy of each algorithm.
Since entropy is logarthmic,we use the arithmetic mean.Notice that the traditional probabilistic decisionlist learning algorithm ?
equivalent to Sorted: ???
always has a higher error rate, higher entropy, andlarger size than Sorted: 0.
Similarly, Sorted: 3 haslower entropy, higher accuracy, and smaller modelsthan Sorted: 0.
Finally, Incremental: 3 has slightlyhigher error rates, but slightly lower entropies, and1/2 to 1/5 as many rules.
If one wants a probabilisticdecision list learner, this is clearly the algorithm touse.
However, if probabilities are not needed, thenTBL can produce lower error rates, with still fewerrules.
On the other hand, if one wants either the low-est entropies or highest accuracies, then it appearsthat linear models, such as maxent or the perceptronalgorithm with margin work even better, at the ex-pense of producing much larger models.Clearly, the new algorithm works very well whensmall size and probabilities are needed.
It wouldbe interesting to try combining this algorithm withdecision trees in some way.
Both Yarowsky (2000)and Florian et al (2000) were able to get improve-ments on the simple decision list structure by addingadditional splits ?
Yarowsky by adding them at theroot, and Florian et al by adding them at the leaves.Notice however that the chief advantage of decisionlists over linear models is their compact size and un-derstandability, and our techniques simultaneouslyimprove those aspects; adding additional splits willalmost certainly lead to larger models, not smaller.It would also be interesting to try more sophisticatedsmoothing techniques, such as those of Yarowsky.We have shown that a simple, incremental algo-rithm for learning probabilistic decision lists can pro-duce models that are significantly more accurate,have significantly lower entropy, and are significantlysmaller than those produced by the standard sortedlearning algorithm.
The new algorithm comes at thecost of some increased time, space, and complexity,but variations on it, such as the sorted algorithm withthresholding, or the techniques of Section 2.2.1, canbe used to trade off space, time, and list size.
Over-all, given the substantial improvements from this al-gorithm, it should be widely used whenever the ad-vantages ?
compactness and understandability ?
ofprobabilistic decision lists are needed.ReferencesM.
Banko and E. Brill.
2001a.
Mitigating the paucity ofdata problem.
In HLT.M.
Banko and E. Brill.
2001b.
Scaling to very very largecorpora for natural language disambiguation.
In ACL.E.
Brill.
1995.
Transformation-based error-driven learn-ing and natural language processing: A case study inpart-of-speech tagging.
Comp.
Ling., 21(4):543?565.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech and Language, 13:359?394.S.F.
Chen and R. Rosenfeld.
1999.
A gaussian prior forsmoothing maximum entropy models.
Technical Re-port CMU-CS-99-108, Computer Science Department,Carnegie Mellon University.R.
Florian, J. C. Henderson, and G. Ngai.
2000.
Coaxingconfidences out of an old friend: Probabilistic classifi-cations from transformation rule lists.
In EMNLP.M.
Kearns and R. Schapire.
1994.
Efficient distribution-free learning of probabilistic concepts.
Computer andSystem Sciences, 48(3):464?497.W.
Krauth and M. Mezard.
1987.
Learning algorithmswith optimal stability in neural networks.
Journal ofPhysics A, 20:745?752.R.J.
Mooney and M.E.
Califf.
1995.
Induction of first-order decision lists: Results on learning the past tenseof English verbs.
In International Workshop on Induc-tive Logic Programming, pages 145?146.G.
Ngai and R. Florian.
2001.
Transformation-basedlearning in the fast lane.
In NA-ACL, pages 40?47.L.
Ramshaw and M. Marcus.
1994.
Exploring the statis-tical derivation of transformational rule sequences forpart-of-speech tagging.
In Proceedings of the Balanc-ing Act Workshop on Combining Symbolic and Statis-tical Approaches to Language, pages 86?95.
ACL.R.
Rivest.
1987.
Learning decision lists.
Machine Learn-ing, 2(3):229?246.Dan Roth.
1998.
Learning to resolve natural languageambiguities: A unified approach.
In AAAI-98.G.
Webb.
1994.
Learning decision lists by prepending in-ferred rules, vol.
b.
In Second Singapore InternationalConference on Intelligent Systems, pages 280?285.David Yarowsky.
1994.
Decision lists for lexical ambi-guity resolution: Application to accent restoration inspanish and french.
In ACL, pages 88?95.David Yarowsky.
2000.
Hierarchical decision lists forword sense disambiguation.
Computers and the Hu-manities, 34(2):179?186.Hugo Zaragoza and Ralf Herbrich.
2000.
The perceptronmeets reuters.
In Workshop on Machine Learning forText and Images at NIPS 2001.
