Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 403?414,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOpen-Domain Fine-Grained Class Extraction from Web Search QueriesMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractThis paper introduces a method for extract-ing fine-grained class labels (?countries withdouble taxation agreements with india?)
fromWeb search queries.
The class labels are morenumerous and more diverse than those pro-duced by current extraction methods.
Alsoextracted are representative sets of instances(singapore, united kingdom) for the class la-bels.1 IntroductionMotivation: As more semantic constraints areadded, concepts like companies become more spe-cific, e.g., companies that are in the software busi-ness, and have been started in a garage.
Thesets of instances associated with the classes becomesmaller; the class labels used to concisely describethe meaning of more specific concepts tend to be-come longer.
In fact, fine-grained class labels suchas ?software companies started in a garage?
are of-ten complex noun phrases, since they must somehowsummarize multiple semantic constraints.
AlthoughWeb users are interested in both coarse (e.g., ?com-panies?)
and fine-grained (e.g., ?software compa-nies started in a garage?)
class labels, virtually allclass labels acquired from text by previous extrac-tion methods (Etzioni et al 2005; Van Durme andPas?ca, 2008; Kozareva and Hovy, 2010; Snow etal., 2006) exhibit little syntactic diversity.
Indeed,instances and class labels that are relatively com-plex nouns are known to be difficult to detect andpick out precisely from surrounding text (Downeyet al 2007).
This and other challenges associatedwith large-scale extraction from Web text (Etzioniet al 2011) cause the extracted class labels to usu-ally follow a rigid modifiers-plus-nouns format.
Theformat covers nouns (?companies?)
possibly pre-ceded by one or many modifiers (?software com-panies?, ?computer security software companies?
).Examples of actual extractions include ?europeancities?
(Etzioni et al 2005), ?strong acids?
(Pan-tel and Pennacchiotti, 2006), ?prestigious privateschools?
(Van Durme and Pas?ca, 2008), ?aquaticbirds?
(Kozareva and Hovy, 2010).As an alternative to extracting class labels fromtext, some methods simply import them fromhuman-curated resources, for example from the setof categories encoded in Wikipedia (Remy, 2002).As a result, class labels potentially exhibit highersyntactic diversity.
The modifiers-plus-nouns for-mat (?computer security software companies?)
isusually still the norm.
But other formats are possi-ble: ?software companies based in london?, ?soft-ware companies of the united kingdom?.
Vocab-ulary coverage gaps remain a problem, with manyrelevant class labels (?software companies of texas?
?software companies started in a garage?, ?soft-ware companies that give sap training?)
still miss-ing.
There is a need for methods that more ag-gressively identify fine-grained class labels, beyondthose extracted by previous methods or encoded inexisting, manually-created resources.
Such class la-bels increase coverage, for example in scenarios thatenrich Web search results with instances availablefor the class labels specified in the queries.Contributions: The contributions of this paper aretwofold.
First, it proposes a weakly-supervised403method to assemble a large vocabulary of class la-bels from queries.
The class labels include fine-grained class labels (?countries with double taxa-tion agreements with india?, ?no front license platestates?)
that are difficult to extract from text byprevious methods for open-domain information ex-traction.
Second, the method acquires representa-tive instances (singapore, united kingdom; arizona,new mexico) that belong to fine-grained class labels(?countries with double taxation agreements withindia?, ?no front license plate states?).
Both classlabels and their instances are extracted from Websearch queries.2 Extraction from Queries2.1 Extraction of Class LabelsOverview: Given a set of arbitrary Web searchqueries as input, our method produces a vocabularyof fine-grained class labels.
For this purpose, it: a)selects an initial vocabulary of class labels, as a sub-set of input queries that are likely to correspond tosearch requests for classes; b) expands the vocabu-lary, by generating a large, noisy set of other pos-sible class labels, through replacements of ngramswithin initial class labels with their similar phrases;c) restricts the generated class labels to those thatmatch the syntactic structure of class labels withinthe initial vocabulary; and d) further restricts thegenerated class labels to those that appear within thelarger set of arbitrary Web search queries.Initial Vocabulary of Class Labels: Out of a setof arbitrary search queries available as input, thequeries in the format ?list of ..?
are selected as theinitial vocabulary of class labels.
The prefix ?listof?
is discarded from each query.
Thus, the query?list of software companies that use linux?
gives theclass label ?software companies that use linux?.Generation via Phrase Similarities: As a prerequi-site to generating class labels, distributionally simi-lar phrases (Lin and Pantel, 2002; Lin and Wu, 2009;Pantel et al 2009) and their scores are collected inadvance.
A phrase is represented as a vector of itscontextual features.
A feature is a word, collectedfrom windows of three words centered around theoccurrences of the phrase in sentences across Webdocuments (Lin and Wu, 2009).
In the contextualvector of a phrase, the weight of a feature is thepointwise-mutual information (Lin and Wu, 2009)between the phrase P and the feature F .
The dis-tributional similarity score between two phrases isthe cosine similarity between the contextual vectorsof the two phrases.
The lists of most distribution-ally similar phrases of a phrase P are thus compiledoffline, by ranking the similar phrases of P in de-creasing order of their similarity score relative to P .Each class label from the initial vocabulary is ex-panded into a set of generated, candidate class la-bels.
To this effect, every ngram P within a givenclass label is replaced with each of the distribution-ally similar phrases, if any, available for the ngram.As shown later in the experimental section, the ex-pansion can increase the vocabulary by a factor of100.Approximate Syntactic Filtering: The set of gen-erated class labels is noisy.
The set is filtered, byretaining only class labels whose syntactic structurematches the syntactic structure of some class label(s)from the initial vocabulary.
The syntactic structureis loosely approximated at surface rather than syn-tactic level.
A generated class label is retained, ifits sequence of part of speech tags matches the se-quence of part of speech tags of one of the class la-bels from the initial vocabulary.
As an additionalconstraint, the sequence must contain one tag cor-responding to a common noun in plural form, i.e.,NNS.
Otherwise, the class label is discarded.Query Filtering: Generated class labels that passprevious filters are further restricted.
They are inter-sected with the set of arbitrary Web search queriesavailable as input.
Generated class labels that arenot full queries are discarded.2.2 Extraction of InstancesOverview: Our method mines instances of fine-grained class labels from queries.
In a nutshell, itidentifies queries containing two types of informa-tion simultaneously.
First, the queries contain an in-stance (marvin gaye) of the more general class labels(?musicians?)
from which the fine-grained class la-bels (?musicians who have been shot?)
can be ob-tained.
Second, the queries contain the constraintsadded by the fine-grained class labels (?...
shot?)
ontop of the more general class labels.Instances of General Class Labels: Follow-ing (Ponzetto and Strube, 2007), the Wikipedia cate-gory network is refined into a hierarchy that discards404non-IsA (thematic) edges, and retains only IsA (sub-sumption) edges from the network (Ponzetto andStrube, 2007).
Instances, i.e., titles of Wikipediaarticles, are propagated upwards to all their ances-tor categories.
The class label ?musicians?
wouldbe mapped into madonna, marvin gaye, jon bon jovietc.
The mappings from each ancestor category, toall its descendant instances in the Wikipedia hierar-chy, represent our mappings from more general classlabels to instances.Decomposition of Fine-Grained Class Labels: Afine-grained class label (e.g., ?musicians who havebeen shot?)
is effectively decomposed into pairs oftwo pieces of information.
The first piece is a moregeneral class label (?musicians?
), if any occurs init.
The second piece is a bag of words, collectedfrom the remainder of the fine-grained class labelafter discarding stop words.
Note that the standardset of stop words is augmented with auxiliary verbs(e.g., does, has, is, would), determiners, conjunc-tions, prepositions, and question wh-words (Radevet al 2005) (e.g., where, how).
In the first pieceof each pair, the general class label is then replacedwith each of its instances.
This produces multiplepairs of a candidate instance and a bag of words, foreach fine-grained class label.
As an illustration, theclass labels ?musicians who have been shot?
and?automobiles with remote start?
are decomposedinto pairs like <madonna, {shot}>, <marvin gaye,{shot}>; and <buick lacrosse, {remote, start}>,<nissan versa, {remote, start}>, respectively.Matching of Candidate Instances: A decomposedclass label is retained, if there are matching queriesthat contain the candidate instance, the bag of words,and optionally stop words.
Otherwise, the decom-posed class label is discarded.
The word matching isperformed after word stemming (Porter, 1980).
Theaggregated frequency of the matching queries is as-signed as the score of the candidate instance for thefine-grained class label:Score(I, C) =?Q(Freq(Q)|Match(Q,< I,C >)) (1)For example, the score of the candidate instancemarvin gaye for the class label ?musicians who havebeen shot?, is the sum of the frequencies of thematching queries ?marvin gaye is shot?, ?when wasmarvin gaye shot?, ?why marvin gaye was shot?etc.
Similarly, the score of buick lacrosse for ?au-tomobiles with remote start?
is given by the aggre-gated frequencies of the queries ?buick lacrosse re-mote start?, ?how to remote start buick lacrosse?,?remote start for buick lacrosse?.
Candidate in-stances of a class label are ranked in decreasing or-der of their scores.3 Experimental SettingWeb Textual Data: The experiments rely on a sam-ple of 1 billion queries in English submitted by usersof a Web search engine.
Each query is accompa-nied by its frequency of occurrence.
Also availableis a sample of around 200 million Web documentsin English.Phrase Similarities: Web documents are used inthe experiments only to construct a phrase similar-ity repository following (Lin and Wu, 2009; Pantelet al 2009).
The repository contains ranked listsof the top 1000 phrases, computed to be the mostdistributionally similar to each of around 16 millionphrases.Text Pre-Processing: The TnT tagger (Brants,2000) assigns part of speech tags to words in classlabels.Instances: To collect mappings from Wikipedia cat-egories (as more general class labels) to titles of de-scendant Wikipedia articles (as instances), a snap-shot of Wikipedia articles was intersected with theWikipedia category hierarchy from (Ponzetto andStrube, 2007).
The mappings connect a total of1,535,083 instances to a total of 108,756 class la-bels.4 Evaluation of Class Labels4.1 Evaluation ProcedureExperimental Runs: Human-compiled informationavailable within Wikipedia serves as the source ofdata for two baseline runs.
The set of all categories,listed in Wikipedia for any of its articles, corre-sponds to the set of class labels ?acquired?
in runRwc.
Categories used for internal Wikipedia book-keeping (Ponzetto and Strube, 2007) are discarded.Their names contain one of the words article(s), cat-egory(ies), indices, pages, redirects, stubs, or tem-plates.
Similarly, the titles of Wikipedia articles withthe prefix ?List of ..?
(e.g., ?List of automobile man-ufacturers of Germany?)
form the set of class labels405?acquired?
in run Rwl.
The prefix ?List of?
is dis-carded.For completeness, a third baseline run, Rdc, cor-responds to class labels extracted from Web docu-ments.
The class labels are noun phrases C that fillextraction patterns equivalent to ?C such as I?.
Thepatterns are matched to document sentences.
Theboundaries of the class labels C are approximatedfrom part of speech tags of sentence words (VanDurme and Pas?ca, 2008).
The patterns were pro-posed in (Hearst, 1992).
They were employedwidely in subsequent methods (Etzioni et al 2005;Kozareva et al 2008; Wu et al 2012), which ex-tract class labels precisely from the set of class la-bels C produced by the extraction patterns.
Evenmethods using queries as a textual data source stillextract class labels from documents using the sameextraction patterns (Pas?ca, 2010).
Therefore, fromthe point of view of evaluating class labels, run Rdcis a valid representative of previous extraction meth-ods, including (Etzioni et al 2005; Kozareva et al2008; Van Durme and Pas?ca, 2008; Pas?ca, 2010; Wuet al 2012).Besides the baseline runs, three experimental runsare considered.
In run Rql, the queries starting withthe prefix ?list of?
form the set of class labels.
Theprefix ?list of?
is discarded from each query.
In runRqg, the class labels are generated via phrase sim-ilarities, starting from Rql as an initial set of classlabels.
Run Rqa represents an ablation experiment.It is created from Rqg, by limiting the expansion ofa given class label via distributional similarities toonly one, rather than multiple, phrases within theclass label.
Note that, by design, none of the classlabels that appear in Rql also appear in runs Rqa orRqg.
Therefore, the intersection between Rql, on onehand, and Rqa and Rqg, on the other hand, is theempty set.All data, including the class labels extracted in allexperimental runs, is converted to lower case.4.2 Relative Coverage of Class LabelsCoverage Over Entire Sets: Table 1 illustrates theoverall coverage of the various experimental runs.The table takes all class labels into account, relativeto the Wikipedia-based runs as reference sets: Rwc(Wikipedia categories), in the upper part of the table;and Rwl (Wikipedia List-Of categories), in the lowerCounts CvgA B |A| |B| |A?B| |A?B||A|vs.
Wikipedia categories:Rwc Rdc 295,587 2,884,390 15,011 0.051Rql 295,587 1,649,261 21,979 0.074Rqa 295,587 33,073,741 33,502 0.113Rqg 295,587 134,235,151 43,935 0.148Rql?Rqg 295,587 135,884,412 65,914 0.222vs.
Wikipedia categories that are queries:Rwc?Q Rdc 126,318 2,884,390 14,840 0.117Rql 126,318 1,649,261 21,979 0.173Rqa 126,318 33,073,741 33,502 0.265Rqg 126,318 134,235,151 43,935 0.347Rql?Rqg 126,318 135,884,412 65,914 0.521vs.
Wikipedia List-Of categories:Rwl Rdc 134,840 2,884,390 8,099 0.060Rql 134,840 1,649,261 26,446 0.196Rqa 134,840 33,073,741 16,204 0.120Rqg 134,840 134,235,151 20,021 0.148Rql?Rqg 134,840 135,884,412 46,467 0.344vs.
Wikipedia List-Of categories that are queries:Rwl?Q Rdc 47,442 2,884,390 7,985 0.168Rql 47,442 1,649,261 24,821 0.523Rqa 47,442 33,073,741 16,204 0.341Rqg 47,442 134,235,151 20,021 0.422Rql?Rqg 47,442 135,884,412 44,842 0.945Table 1: Coverage of class labels extracted by variousexperimental runs, relative to class labels available inWikipedia before and after intersecting them with a largeset of arbitrary queries (A = reference set, relative towhich coverage is computed; B = measured set, for whichcoverage is computed relative to the reference set; |A| =size of set A; Q = set of input queries)part of the table.
Note that the number of class labelsextracted by the individual run shown in the secondcolumn (B) is shown in the fourth column (|B|).
Inparticular, there are around 1.6 million unique ?listof ..?
queries, from which class labels are collectedin run Rql.During the computation of coverage, the refer-ence set, and the set for which coverage is beingcomputed, are intersected.
Intersection relies onstrict string matching.
All words, including punc-tuation, must match exactly in order for a class la-bel to be part of the intersection.
The referencesets are intersected with the set of all Web searchqueries Q used in the experiments.
Coverage is com-puted both before and after intersection.
Less thanhalf (126,318 of 295,587) of the class labels, for406the reference set Rwc; and about a third (47,442 of134,840) for Rwl; appear in the set Q of all queries.Three conclusions can be drawn from the re-sults.
First, query-based runs vastly outperformWikipedia-based runs in terms of absolute coverage.Run Rql contains around 5 and 12 times more classlabels, than Rwc and Rwl respectively.
On top ofthat, generating class labels via phrase similaritiesfurther increases the class label count by about 20times for Rqa, and 80 times for Rqg.
Second, query-based runs Rqa and Rqg surpass the document-basedrun Rdc.
Third, higher class label counts translateinto higher relative coverage.
In the upper part ofthe table, run Rwl contains 3.9% (relative to Rwc)and 7.1% (relative to Rwc?Q) of the reference set.But the relative coverage doubles for Rql at 7.4%(relative to Rwc) and 17.3% (relative to Rwc?Q).Coverage again doubles for Rqg at 14.8% (relativeto Rwc) and 34.7% (relative to Rwc?Q).
The unionof query-based initial and generated class labels isRql?Rqg.
The union contains about a quarter (i.e.,22.2%) or half (52.1%) of the reference set Rwc, de-pending on whether the reference set is intersectedwith the set of all queries or not.
In the lower part ofthe table, more than 90% of the queries in the refer-ence set Rwl that are also queries are found amongthe class labels collectively extracted in the query-based runs.
Note that, since Rql is disjoint from Rqaand Rqg, none of the class labels already in Rql canbe ?re-discovered?
(generated) again in Rqa or Rqg.Therefore, by experimental design, relative coveragescores of Rql may be relatively difficult to surpass byRqa or Rqg taken individually.Diversity: Class labels restricted to those that havethe format ?..
that/which/who ..?
are relatively morespecific, e.g., ?grocery stores that double coupons inomaha?, ?airlines which fly from santa barbara?,?writers who were doctors?.
The most frequenthead phrases of such restricted class labels offer anidea about how diverse the class labels are.
Thecounts of class labels for the most frequent headphrases are in the order of 10?s in the case of Rwl vs.10,000?s for Rqg.
In comparison, none of the classlabels of run Rdc have this format.
The lack of suchclass labels in run Rdc, and their smaller proportionin run Rwl vs. Rqg, suggest that class labels extractedby the proposed method exhibit higher lexical andsyntactic diversity than previous methods do.Tag (Value): Examples of Class Labelscorrect (1.0): angioplasty specialists in kolkata, goodthings pancho villa did, eating disorders inpatient unitsin the uk nhs specialist servicesquestionable (0.5): picture framers adelaide cbd, sideeffects bicalutamide, different eating disorders, privatehospitals treat kidney stones ukincorrect (0.0): al hirschfield theatre hours, value ofberkshire hathaway shares, remove spaces in cobol,dogs with loss of appetite, 1999 majorca openTable 2: Correctness tags manually assigned to class la-bels containing one of the (underlined) target phrases, ex-tracted by various runs4.3 Precision of Class LabelsEvaluation Metric: Class labels being evaluated aremanually assigned a correctness tag.
A class label isdeemed correct, if it is grammatically well-formedand describes a relevant concept that embodies some(unspecified) set of instances that share similar prop-erties; questionable, if it is relevant but not well-formed; or incorrect.
A questionable class label isnot well-formed because it lacks necessary linkingparticles (e.g., the prepositions of or for in ?side ef-fects bicalutamide?
), or contains undesirable mod-ifiers (?different eating disorders?).
Examples ofcorrect and incorrect class labels are ?angioplastyspecialists in kolkata?
and ?al hirschfield theatrehours?
respectively.To compute the precision score, the correctnesstags are converted to numeric values, as shown inTable 2: correct to 1; questionable to 0.5; and in-correct to 0.
Precision over a list of class labels ismeasured as the sum of the correctness values of theclass labels in the list, divided by the size of the list.Precision Relative to Target Phrases: The preci-sion of the class labels in each run is determined sim-ilarly to how relative coverage was computed ear-lier.
More precisely, the precision is computed overthe class labels whose names contain each phrasefrom the set of 75 target phrases from (Alfonsecaet al 2010).
For each phrase, and for each run,a random sample of at most 50 of the class labelsthat match the phrase is selected for evaluation.
Thesamples taken for each run, corresponding to thesame phrase, are combined into a merged list.
Thisproduces one merged list for each phrase, for a totalof 75 merged lists.
The precision score over a target407phrase is the precision score over its sample of classlabels.The last two columns of Table 3 capture the pre-cision scores for the class labels.
The scores arecomputed in two ways: averaged over the (variable)subsets of target phrases for which some matchingclass label(s) exist, in the last but one column, e.g.,over 19 of the 75 target phrases for Rwc; and aver-aged over the entire set of 75 target phrases, in thelast column.
The former does not penalize a runfor not being able to extract any class labels con-taining a particular target phrase, whereas the latterdoes penalize.
Naturally, precision scores over theentire set of target phrases decrease when coverageis lower, for runs Rwc, Rwl and, to a lesser extent,Rdc and Rql.
But even after ignoring target phraseswith no matching class labels, precision scores inthe last but one column in Table 3 reveal importantproperties of the experimental runs.
First, betweenthe two Wikipedia-based runs, Rwl has perfect classlabels, whereas as many as 1 in 4 class labels ofrun Rwc are marked as incorrect during the evalu-ation.
Second, the class labels collected from ?listof ..?
queries in run Rql correspond to relevant, well-formed concepts in 80% of the cases.
Third, the gen-eration of class labels via phrase similarities (Rqg)greatly increases coverage as shown earlier.
The in-crease comes at the expense of lowering precisionfrom 80% to 72%.
However, the phrases from ini-tial queries that are expanded via distributional sim-ilarities can be limited from multiple to only one, byswitching from Rqg to Rqa.
This gives higher preci-sion for Rqa than for Rqg.As a complement to Table 3, the graphs in Fig-ure 1 offer a more detailed view into the precisionof class labels.
The figure covers a Wikipedia-basedrun (Rwc) and two query-based runs (Rql, Rqg).
Thegraphs show the precision scores, over each of the75 target phrases.
Among target phrases for whichsome matching class labels exist in the respectiverun, the target phrases with the lowest precisionscores are robotics (score of 0.15) and karlsruhe(0.33), for Rwc; carotid arteries and kidney stones,both with a score of 0.00 because their matchingclass labels are all incorrect, for Rdc; african pop-ulation and chester arthur, both with a score of 0.00because their matching class labels are all incorrect,for Rql; and arlene martel (0.00) and right to voteRun Target Phrases Precision of Class LabelsOver Target PhrasesAll Matched Cvg Over Matched Over AllRwc 75 19 0.253 0.756 0.191Rwl 75 15 0.200 1.000 0.200Rdc 75 35 0.467 0.834 0.389Rql 75 48 0.640 0.800 0.512Rqa 75 70 0.933 0.868 0.810Rqg 75 73 0.973 0.724 0.705Table 3: Precision of class labels that match (i.e., whosenames contain) each target phrase, computed as an av-erage over (variable) subsets of target phrases for whichsome matching class label(s) exist, and as an average overthe entire set of 75 target phrases00.10.20.30.40.50.60.70.80.91aaa1adelaidecbd5americanfascism10antarcticregion15baquba20boulder colorado25chester arthur30contemporaryart35eatingdisorders40halogens45juan carlos50luckyali55phosphorus60rouen65u.s.70wlan75PrecisionPhrasePer-Phrase Precision for Run Rwc00.10.20.30.40.50.60.70.80.91aaa1adelaidecbd5americanfascism10antarcticregion15baquba20boulder colorado25chester arthur30contemporaryart35eatingdisorders40halogens45juan carlos50luckyali55phosphorus60rouen65u.s.70wlan75PrecisionPhrasePer-Phrase Precision for Run Rql00.10.20.30.40.50.60.70.80.91aaa1adelaidecbd5americanfascism10antarcticregion15baquba20boulder colorado25chester arthur30contemporaryart35eatingdisorders40halogens45juan carlos50luckyali55phosphorus60rouen65u.s.70wlan75PrecisionPhrasePer-Phrase Precision for Run Rdc00.10.20.30.40.50.60.70.80.91aaa1adelaidecbd5americanfascism10antarcticregion15baquba20boulder colorado25chester arthur30contemporaryart35eatingdisorders40halogens45juan carlos50luckyali55phosphorus60rouen65u.s.70wlan75PrecisionPhrasePer-Phrase Precision for Run RqgFigure 1: Precision scores for runs Rwc, Rql, Rdc andRqg , over class labels that match (i.e., contain) each ofthe 75 target phrases(0.25), for Rqg.Precision over Samples of Class Labels: The pre-cision is separately computed over a random sampleof 400 class labels per experimental run.
The sam-ples are selected from the set of all class labels ex-tracted by the respective run.
The precision scoresare: 0.759 for Rwc; 1.000 for Rwl; 0.806 for Rdc;0.811 for Rql; 0.856 for Rqa; and 0.711 for Rqg.
Thescores are in line with scores computed earlier overthe target phrases, in the fourth column of Table 3.Discussion: As noted in (Ponzetto and Strube,2007), Wikipedia organizes its articles and cate-gories into a category network that mixes IsA (sub-sumption) edges with non-IsA (thematic) edges.Whenever an edge in Wikipedia is not IsA, the par-408Longest Class LabelsRwl: [japanese army and navy members in military orpolitic services in proper japan korea manchuria occu-pied china and nearest areas in previous times and pa-cific war epoch(1930-40s), mental disorders as definedby the diagnostic and statistical manual of mental dis-orders and the international statistical classification ofdiseases and related health problems,..]Rqg: [differences between transformational lead-ership and transactional leadership, things to do inllanfairpwllgwyngyllgogerychwyrndrobwllllantysilio-gogogoch, philosophical differences between thomasjefferson and alexander hamilton, musculoskeletalmanifestations of human immunodeficiency virusinfection,..]Table 4: Longest class labels extracted by runs Rwl andRqgent category may not be a relevant concept that de-scribes some set of instances that share similar prop-erties.
Such categories are not good class labels,and therefore are marked as incorrect.
Examples in-clude the class labels ?austrian contemporary art?,?1999 majorca open?
and ?u.s.
route 30?, listed inWikipedia as categories of the instances vienna bien-nale, 1999 majorca open and squirrel hill tunnel re-spectively.
This affects the precision scores for Rwcin Table 3.
It also affects the coverage values rela-tive to Rwc in Table 1.
Ideally, high-precision exper-imental runs would not extract any incorrect class la-bels that happen to appear in Rwc, for example ?aus-trian contemporary art?.
But the coverage relativeto Rwc would artificially penalize such runs, for notextracting the incorrect class labels from Rwc.As a proxy for estimating class label complexity,Table 4 shows the longest class labels derived fromWikipedia (Rwl) vs. generated from queries (Rqg).Class labels derived from Web search queries maybe semantically overlapping.
Examples are ?writerswho killed themselves?
vs. ?writers who committedsuicide?.
The overlap is desirable, since differentWeb users may request the same information via dif-ferent queries.
The same phenomenon has been ob-served in other information extraction tasks.
It alsoaffects manually-created resources like Wikipedia.The continuous manual refinements to Wikipediacontent still cannot prevent the occurrence of du-plicate class labels among Wikipedia List-Of cate-gories.
The duplicates are present in run Rwl.
Exam-Target Class Labels007 movie actors, .308 weapons, actors with obsessivecompulsive disorder, antibiotics for multiple sclerosis,astronauts in space station, automobiles with remotestart, beatles songs of love, beetles that bite, compa-nies with sustainable competitive advantage, countrieswith double taxation agreements with india, criminalswho have been executed, daft punk live albums, dal-las medical companies, direct democracy states, elec-tronic companies in electronic city bangalore, expen-sive brands of shoes, eye diseases in cats, f1 car com-panies, fwd sports cars, garden landscaping maga-zines, heliskiing resorts, hell in a cell wrestlers, hol-idays celebrated in sydney, ibf weight classes, ibiza2011 djs, immunology scientists, jewelry manufactur-ing companies, kanye west songs on youtube, kingstonupon thames supermarkets, latin military ranks, lud-hiana newspapers, maastricht treaty countries, mu-sicians who have been shot, no front license platestates, non-profit organizations in nashville tennessee,organic chocolate companies, plants which are used inhomeopathy, programming languages for server sideprogramming, qatar chemical companies, qld privateschools, real estate companies in virginia beach vir-ginia, respiratory infection antibiotics, serial killerswith antisocial personality disorder, singers with curlyhair, telecommunications companies in the philip-pines, trains from la to san diego, visual basic databasemanagement systems, warmblood colors, washingtonuniversity basketball players, world heritage sites innorthern irelandTable 5: Set of 50 class labels, used in the evaluation ofextracted instancesples are ?formula one drivers that never qualified fora race?
vs. ?formula one drivers who never quali-fied for a race?
; or ?goaltenders who have scoreda goal in a nhl game?
vs. ?goaltenders who havescored a goal in an nhl game?.
Some of the lexi-cal differences among class labels are due to unde-sirable misspellings.
Again, similar problems occa-sionally affect existing Wikipedia categories: ?no-bel laureates who endorse barack obama?
vs. ?no-bel laureates who endorse barrack obama?.5 Evaluation of Instances5.1 Evaluation ProcedureTarget Set of Class Labels: The target set for evalu-ation is shown in Table 5.
Initially, a random sampleof 100 class labels is selected from all class labels in409Tag (Value): Examples of Instancescorrect (1.0): countries with double taxation agree-ments with india: thailand; hell in a cell wrestlers:brock lesnar; ibiza 2011 djs: dimitri from paris; he-liskiing resorts: valle nevadoquestionable (0.5): 007 movie actors: david niven;kanye west songs on youtube: the good life; holidayscelebrated in sydney: waitangi dayincorrect (0.0): electronic companies in electroniccity bangalore: bank of baroda; garden landscapingmagazines: marquis; immunology scientists: rosalindfranklinTable 6: Correctness tags manually assigned to instancesextracted from queries for various class labelsrun Rqg.
Class labels deemed incorrect, as well asclass labels for which no instances are extracted, aremanually removed from the sample.
Out of the re-maining class labels, a smaller random sample of 50of the remaining class labels is retained, for the pur-pose of evaluating the quality of instances extractedfor various class labels.Evaluation Metric: The evaluation computes theprecision of the ranked list of instances extracted foreach target class label.
To remove any undesirablebias towards higher-ranked instances, the ranked listis sorted alphabetically, then each instance is as-signed one of the correctness tags from Table 6.Instances are deemed questionable, if they wouldbe correct for a rather obscure interpretation of theclass label.
For example, david niven is an actor inone of the spoofs rather than main releases of the007 movie.
Instances that would be correct if a fewwords were dropped or added are also deemed ques-tionable: the good life is not one of the ?kanye westsongs on youtube?
but good life is.To compute the precision score over a ranked listof instances, the correctness tags are converted tonumeric values.
Precision at some rank N in the listis measured as the sum of the correctness values ofthe instances extracted up to rank N, divided by thenumber of instances extracted up to rank N.5.2 Precision of InstancesPrecision: Precision scores in Table 7 vary acrosstarget class labels.
For some class labels, the ex-tracted instances are noisy enough that scores arebelow 0.50 at ranks 10 and higher.
This is the casefor ?electronic companies in electronic city banga-Target Class Label Precision of Instances@1 @5 @10 @50007 movie actors 1.00 1.00 0.85 0.85actors with obsessive compul-sive disorder0.00 0.60 0.70 0.70antibiotics for multiple sclerosis 0.50 0.60 0.55 0.58astronauts in space station 1.00 0.70 0.85 0.83automobiles with remote start 1.00 1.00 0.75 0.75beatles songs of love 0.00 0.50 0.65 0.52beetles that bite 1.00 0.80 0.50 0.56companies with sustainablecompetitive advantage1.00 1.00 0.80 0.88countries with double taxationagreements with india1.00 1.00 1.00 0.90criminals who have been exe-cuted1.00 1.00 0.90 0.82daft punk live albums 0.50 0.40 0.35 0.35dallas medical companies 0.00 0.70 0.65 0.54direct democracy states 1.00 1.00 0.90 0.86electronic companies in elec-tronic city bangalore1.00 0.40 0.40 0.42expensive brands of shoes 1.00 1.00 0.90 0.92eye diseases in cats 0.50 0.50 0.35 0.35f1 car companies 1.00 1.00 0.80 0.30fwd sports cars 1.00 1.00 1.00 1.00garden landscaping magazines 0.00 0.10 0.15 0.06heliskiing resorts 1.00 1.00 1.00 1.00hell in a cell wrestlers 1.00 1.00 1.00 0.92holidays celebrated in sydney 1.00 0.70 0.75 0.75... ... ... ... ...Average over 50 class labels 0.80 0.80 0.76 0.71Table 7: Precision at various ranks in the ranked lists ofinstances extracted from queries, for various target classlabels and as an average over the entire set of 50 targetclass labelslore?
and ?daft punk live albums?, and especiallyfor ?garden landscaping magazines?
which has theworst precision.
On the other hand, instances ex-tracted for ?companies with sustainable competitiveadvantage?
or ?criminals who have been executed?have high precision across all ranks.
As an aver-age over all target class labels, precision is 0.76 atrank 10, and 0.71 at rank 50.
Although there is roomfor improvement, we find these accuracy levels to beencouragingly good, especially at rank 50.
As a re-minder, instances are extracted from noisy queries,and for class labels as fine-grained as those acquiredand used in our experiments.
Some of the extractedranked lists of instances are shown in Table 8.410Target Class Label Extracted Instancescountries withdouble taxationagreements withindia[singapore, malaysia, mauritius,kenya, australia, united king-dom, cyprus, turkey, thailand, ger-many,..]direct democracystates[california, oregon, nevada, wis-consin, louisiana, arizona, ver-mont, alaska, illinois, michigan,..]fwd sports cars [scion tc, ford probe, honda pre-lude, nissan 200sx, lotus elan, mit-subishi fto, dodge srt-4, mitsubishigto, volvo c30, toyota celica,..]garden landscap-ing magazines[front, contemporary, gallery,edge, view, chelsea, wallpaper,expo, wizard, sunset,..]holidays cele-brated in sydney[halloween, australia day, anzacday, independence day, waitangiday, melbourne cup, hogmanay,rotuma day, solstice, yule,..]Table 8: Ranked lists of instances extracted for a sampleof class labelsIn additional experiments, the same evaluationprocedure is applied to output from two previous ex-traction methods.
The first method starts by inter-nally generating a small set of seed instances for aclass label given as input (Wang and Cohen, 2009).A set expansion module then expands the seed setinto a longer, ranked list of instances.
The instancesare extracted from unstructured and semi-structuredtext within Web documents.
The documents are ac-cessed via the search interface of a general-purposeWeb search engine (cf.
(Wang and Cohen, 2009)for more details).
The second method extracts in-stances of class labels using the extraction patternsproposed in (Hearst, 1992).
As such, it is similarto (Kozareva et al 2008; Van Durme and Pas?ca,2008; Wu et al 2012).
The method correspondsto the run Rdc described earlier, where the rela-tive ranking of instances and class labels uses theco-occurrence of instances and class labels withinqueries (Pas?ca, 2010).
For the purpose of the eval-uation, when no instances are available for a targetclass label, the class label is generalized into iter-atively shorter phrases containing fewer modifiers,until some instances are available for the shorterphrase.
For example, target class labels like actorswith obsessive compulsive disorder, beatles songs oflove, garden landscaping magazines do not have anyinstances extracted by the second method.
There-fore, the instances evaluated for the second methodfor these target class labels are collected from theinstances of the more general actors, beatles songs,landscaping magazines.
Without the generalization,the target class label would receive no credit dur-ing the evaluation, and the two previous methodswould have lower precision scores.
Over the 50 tar-get class labels, the precision of the two methods is0.11 and 0.27 at rank 5; 0.06 and 0.25 at rank 10;0.05 and 0.22 at rank 20; and 0.05 and 0.20 at rank50.
The results confirm that, as explained earlier,previous methods for open-domain information ex-traction have limited ability to extract instances offine-grained class labels.Discussion: Earlier errors in the acquisition of theclass label affect the usefulness of any instances thatmay be subsequently extracted for them.
The ex-periments require candidate instances to appear inWikipedia.
This may improve precision, at the ex-pense of not extracting instances that are not yet inWikipedia (Lin et al 2012).6 Related WorkPrevious methods for extracting classes of instancesfrom text acquire sets of instances that are eacheither unlabeled (Pennacchiotti and Pantel, 2009;Jain and Pennacchiotti, 2010; Shi et al 2010),or associated with a class label (Banko et al2007; Wang and Cohen, 2009).
The sets of in-stances and/or class labels may be organized asflat sets or hierarchically, relative to inferred hier-archies (Kozareva and Hovy, 2010) or existing hier-archies such as WordNet (Snow et al 2006; Davi-dov and Rappoport, 2009) or the category networkwithin Wikipedia (Wu and Weld, 2008; Ponzettoand Navigli, 2009).
Semi-structured text from Webdocuments is a complementary resource to unstruc-tured text, for the purpose of extracting relations ingeneral (Cafarella et al 2008), and classes and in-stances in particular (Talukdar et al 2008; Dalvi etal., 2012).With previous methods, the vocabulary of classlabels potentially produced for any instance is con-fined to a closed set provided manually as in-put (Wang and Cohen, 2009; Carlson et al 2010).The closed set is often derived from resources likeWikipedia (Talukdar and Pereira, 2010; Lin et al4112012; Hoffart et al 2013) or Freebase (Pantel etal., 2012).
Alternatively, the vocabulary is not aclosed set, but instead is acquired along with theinstances (Pantel and Pennacchiotti, 2006; Snowet al 2006; Banko et al 2007; Van Durme andPas?ca, 2008; Kozareva and Hovy, 2010).
In the lat-ter case, the extracted class labels take the form ofhead nouns preceded by modifiers.
Examples are?cities?, ?european cities?
(Etzioni et al 2005);?artists?, ?strong acids?
(Pantel and Pennacchiotti,2006); ?outdoor activities?, ?prestigious privateschools?
(Van Durme and Pas?ca, 2008); ?methate-rians?, ?aquatic birds?
(Kozareva and Hovy, 2010).In contrast, the class labels extracted in our methodexhibit greater syntactic diversity and are finer-grained.
In addition, they are not constrained to aparticular set of categories available in resources likeWikipedia.Fine-grained class labels roughly correspond toqueries submitted in typed search (Demartini et al2009) or entity search (Balog et al 2010) or list-seeking questions (?name the circuit judges in thecayman islands that are british?).
But our focus ison generating, rather than answering such queriesor, more generally, attempting to deeply understandtheir semantics (Li, 2010).
Phrase similarities canbe derived with any methods, using documents (Linand Wu, 2009) or search queries (Jain and Pennac-chiotti, 2010).Whether Web search queries are a useful textualdata source for open-domain information extractionhas been investigated in several tasks.
Examples arecollecting unlabeled sets of similar instances (Jainand Pennacchiotti, 2010), ranking of class labelsalready extracted from text (Pas?ca, 2010), extract-ing attributes of instances (Alfonseca et al 2010)and identifying the occurrences in queries of in-stances of several types, where the types are de-fined in a manually-created resource (Pantel et al2012).
Comparatively, we show that queries are use-ful in identifying possible class labels, not only re-ranking them; and even in populating the class labelswith relevant, albeit small, sets of corresponding in-stances.As automatically-extracted class labels becomefiner-grained, they more clearly illustrate a phe-nomenon that received little attention.
Namely, classlabels of an instance, on one hand, and relations link-ing the instance with other instances and classes, onthe other hand, are not mutually exclusive piecesof knowledge.
Their extraction does not necessar-ily require different, dedicated techniques.
Quitethe opposite, class labels serve in text as nothingmore than convenient lexical representations, or lex-ical shorthands, of relations linking instances withother instances.
The class labels ?no front licenseplate states?
and ?states with no front license platerequirement?
are applicable to arizona.
If so, it isbecause arizona is a state, and states require the in-stallation of license plates on vehicles, and the re-quirement does not apply to the front of vehiclesin the case of arizona.
The connection betweenclass labels and relations has been judiciously ex-ploited in (Nastase and Strube, 2008).
In that study,relations encoded implicitly within Wikipedia cat-egories are transformed into explicit relations.
Asan example, the explicit relation that deconstruct-ing harry is directed by woody allen is obtainedfrom the fact that deconstructing harry is listed un-der ?movies directed by woody allen?
in Wikipedia.Ours is the first approach to examine the potentialfor extracting relations from search queries, whererelations are compactly and loosely folded into therespective class labels.
A variety of methods addressthe more general task of acquisition of open-domainrelations from documents, e.g., (Zhu et al 2009;Carlson et al 2010; Fader et al 2011; Lao et al2011).7 ConclusionThe approach introduced in this paper exploitsknowledge loosely encoded within Web searchqueries.
It acquires a vocabulary of class labels thatare finer grained than in previous literature.
Theclass labels have precision comparable to that ofclass labels derived from human-created knowledgerepositories.
Furthermore, representative instancesare extracted from queries for the fine-grained classlabels, at encouraging levels of accuracy.
Currentwork explores the use of noisy syntactic features toincrease the accuracy of extracted class labels; theextraction of instances from evidence in multiple,rather than single queries; the expansion of extractedinstances into larger sets; and the conversion of fine-grained class labels into relations among classes.412ReferencesE.
Alfonseca, M. Pas?ca, and E. Robledo-Arnuncio.
2010.Acquisition of instance attributes via labeled and re-lated instances.
In Proceedings of the 33rd Interna-tional Conference on Research and Development in In-formation Retrieval (SIGIR-10), pages 58?65, Geneva,Switzerland.K.
Balog, M. Bron, and M. de Rijke.
2010.
Category-based query modeling for entity search.
In Proceed-ings of the 32nd European Conference on InformationRetrieval (ECIR-10), pages 319?331, Milton Keynes,United Kingdom.M.
Banko, Michael J Cafarella, S. Soderland, M. Broad-head, and O. Etzioni.
2007.
Open information ex-traction from the Web.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence(IJCAI-07), pages 2670?2676, Hyderabad, India.T.
Brants.
2000.
TnT - a statistical part of speech tagger.In Proceedings of the 6th Conference on Applied Natu-ral Language Processing (ANLP-00), pages 224?231,Seattle, Washington.M.
Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.2008.
WebTables: Exploring the power of tables onthe Web.
In Proceedings of the 34th Conference onVery Large Data Bases (VLDB-08), pages 538?549,Auckland, New Zealand.A.
Carlson, J. Betteridge, R. Wang, E. Hruschka, andT.
Mitchell.
2010.
Coupled semi-supervised learn-ing for information extraction.
In Proceedings of the3rd ACM Conference on Web Search and Data Mining(WSDM-10), pages 101?110, New York.B.
Dalvi, W. Cohen, and J. Callan.
2012.
Websets: Ex-tracting sets of entities from the Web using unsuper-vised information extraction.
In Proceedings of the5th ACM Conference on Web Search and Data Mining(WSDM-12), pages 243?252, Seattle, Washington.D.
Davidov and A. Rappoport.
2009.
Enhancementof lexical concepts using cross-lingual Web mining.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-09), pages 852?861, Singapore.G.
Demartini, T. Iofciu, and A. de Vries.
2009.
Overviewof the INEX 2009 Entity Ranking track.
In INitiativefor the Evaluation of XML Retrieval Workshop, pages254?264, Brisbane, Australia.D.
Downey, M. Broadhead, and O. Etzioni.
2007.
Locat-ing complex named entities in Web text.
In Proceed-ings of the 20th International Joint Conference on Ar-tificial Intelligence (IJCAI-07), pages 2733?2739, Hy-derabad, India.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theWeb: an experimental study.
Artificial Intelligence,165(1):91?134.O.
Etzioni, A. Fader, J. Christensen, S. Soderland, andMausam.
2011.
Open information extraction: Thesecond generation.
In Proceedings of the 22nd In-ternational Joint Conference on Artificial Intelligence(IJCAI-11), pages 3?10, Barcelona, Spain.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identifyingrelations for open information extraction.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (EMNLP-11), pages1535?1545, Edinburgh, Scotland.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th In-ternational Conference on Computational Linguistics(COLING-92), pages 539?545, Nantes, France.J.
Hoffart, F. Suchanek, K. Berberich, and G. Weikum.2013.
YAGO2: a spatially and temporally enhancedknowledge base from Wikipedia.
Artificial Intelli-gence, 194:28?61.A.
Jain and M. Pennacchiotti.
2010.
Open entity ex-traction from Web search query logs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (COLING-10), pages 510?518,Beijing, China.Z.
Kozareva and E. Hovy.
2010.
A semi-supervisedmethod to learn and construct taxonomies using theweb.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing(EMNLP-10), pages 1110?1118, Cambridge, Mas-sachusetts.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Semanticclass learning from the Web with hyponym patternlinkage graphs.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics (ACL-08), pages 1048?1056, Columbus, Ohio.N.
Lao, T. Mitchell, and W. Cohen.
2011.
Random walkinference and learning in a large scale knowledge base.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-11), pages 529?539, Edinburgh, Scotland.X.
Li.
2010.
Understanding the semantic struc-ture of noun phrase queries.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics (ACL-10), pages 1337?1345, Up-psala, Sweden.D.
Lin and P. Pantel.
2002.
Concept discovery from text.In Proceedings of the 19th International Conferenceon Computational linguistics (COLING-02), pages 1?7, Taipei, Taiwan.D.
Lin and X. Wu.
2009.
Phrase clustering for discrim-inative learning.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Linguis-tics (ACL-IJCNLP-09), pages 1030?1038, Singapore.413T.
Lin, Mausam, and O. Etzioni.
2012.
No noun phraseleft behind: Detecting and typing unlinkable enti-ties.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL-12), pages 893?903, Jeju Island, Korea.V.
Nastase and M. Strube.
2008.
Decoding Wikipediacategories for knowledge acquisition.
In Proceedingsof the 23rd National Conference on Artificial Intelli-gence (AAAI-08), pages 1219?1224, Chicago, Illinois.M.
Pas?ca.
2010.
The role of queries in ranking la-beled instances extracted from text.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (COLING-10), pages 955?962, Bei-jing, China.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging generic patterns for automatically harvesting se-mantic relations.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Computa-tional Linguistics (COLING-ACL-06), pages 113?120,Sydney, Australia.P.
Pantel, E. Crestan, A. Borkovsky, A. Popescu, andV.
Vyas.
2009.
Web-scale distributional similarity andentity set expansion.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-09), pages 938?947, Singapore.P.
Pantel, T. Lin, and M. Gamon.
2012.
Mining entitytypes from query logs via user intent modeling.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-12), pages563?571, Jeju Island, Korea.M.
Pennacchiotti and P. Pantel.
2009.
Entity extrac-tion via ensemble semantics.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-09), pages 238?247,Singapore.S.
Ponzetto and R. Navigli.
2009.
Large-scale taxonomymapping for restructuring and integrating Wikipedia.In Proceedings of the 21st International Joint Confer-ence on Artificial Intelligence (IJCAI-09), pages 2083?2088, Pasadena, California.S.
Ponzetto and M. Strube.
2007.
Deriving a large scaletaxonomy from Wikipedia.
In Proceedings of the 22ndNational Conference on Artificial Intelligence (AAAI-07), pages 1440?1447, Vancouver, British Columbia.M.
Porter.
1980.
An algorithm for suffix stripping.
Pro-gram, 14(3):130?137.D.
Radev, W. Fan, H. Qi, H. Wu, and A. Grewal.
2005.Probabilistic question answering on the Web.
Journalof the American Society for Information Science andTechnology, 56(3):571?583.M.
Remy.
2002.
Wikipedia: The free encyclopedia.
On-line Information Review, 26(6):434.S.
Shi, H. Zhang, X. Yuan, and J. Wen.
2010.Corpus-based semantic class mining: Distributionalvs.
pattern-based approaches.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics (COLING-10), pages 993?1001, Beijing,China.R.
Snow, D. Jurafsky, and A. Ng.
2006.
Semantic tax-onomy induction from heterogenous evidence.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics (COLING-ACL-06), pages 801?808, Sydney, Australia.P.
Talukdar and F. Pereira.
2010.
Experiments in graph-based semi-supervised learning methods for class-instance acquisition.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL-10), pages 1473?1481, Uppsala,Sweden.P.
Talukdar, J. Reisinger, M. Pas?ca, D. Ravichandran,R.
Bhagat, and F. Pereira.
2008.
Weakly-supervisedacquisition of labeled class instances using graph ran-dom walks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing(EMNLP-08), pages 582?590, Honolulu, Hawaii.B.
Van Durme and M. Pas?ca.
2008.
Finding cars, god-desses and enzymes: Parametrizable acquisition of la-beled instances for open-domain information extrac-tion.
In Proceedings of the 23rd National Confer-ence on Artificial Intelligence (AAAI-08), pages 1243?1248, Chicago, Illinois.R.
Wang and W. Cohen.
2009.
Automatic set instanceextraction using the Web.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics (ACL-IJCNLP-09), pages 441?449, Singa-pore.F.
Wu and D. Weld.
2008.
Automatically refining theWikipedia infobox ontology.
In Proceedings of the17th World Wide Web Conference (WWW-08), pages635?644, Beijing, China.W.
Wu, , H. Li, H. Wang, and K. Zhu.
2012.
Probase:a probabilistic taxonomy for text understanding.
InProceedings of the 2012 International Conference onManagement of Data (SIGMOD-12), pages 481?492,Scottsdale, Arizona.J.
Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen.
2009.
Stat-Snowball: a statistical approach to extracting entity re-lationships.
In Proceedings of the 18th World WideWeb Conference (WWW-09), pages 101?110, Madrid,Spain.414
