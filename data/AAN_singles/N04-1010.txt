Acquiring Hyponymy Relations from Web DocumentsKeiji Shinzato Kentaro TorisawaSchool of Information Science,Japan Advanced Institute of Science and Technology (JAIST)1-1 Asahidai, Tatsunokuchi, Nomi-gun, Ishikawa, 923-1292 JAPAN{skeiji,torisawa}@jaist.ac.jpAbstractThis paper describes an automatic method foracquiring hyponymy relations from HTMLdocuments on the WWW.
Hyponymy relationscan play a crucial role in various natural lan-guage processing systems.
Most existing ac-quisition methods for hyponymy relations relyon particular linguistic patterns, such as ?NPsuch as NP?.
Our method, however, does notuse such linguistic patterns, and we expectthat our procedure can be applied to a widerange of expressions for which existing meth-ods cannot be used.
Our acquisition algo-rithm uses clues such as itemization or listingin HTML documents and statistical measuressuch as document frequencies and verb-nounco-occurrences.1 IntroductionThe goal of this work is to become able to automaticallyacquire hyponymy relations for a wide range of wordsor phrases from HTML documents on the WWW.
We donot use particular lexicosyntactic patterns, as previous at-tempts have (Hearst, 1992; Caraballo, 1999; Imasumi,2001; Fleischman et al, 2003; Morin and Jacquemin,2003; Ando et al, 2003).
The frequencies of use for suchlexicosyntactic patterns are relatively low, and there canbe many words or phrases that do not appear in such pat-terns even if we look at a large number of texts.
The effortof searching for other clues indicating hyponymy rela-tions is thus significant.
We try to acquire hyponymy re-lations by combining three different types of clue obtain-able from a wide range of words or phrases.
The first typeof clue is inclusion in itemizations or lists found in typi-cal HTML documents on the WWW.
The second consistsof statistical measures such as the document frequency(df) and the inverse document frequency (idf), which are?
Car Specification?
Toyota?
Honda?
NissanFigure 1: An example of itemizationpopular in the IR literature.
The third is verb-noun co-occurrence in normal corpora.In our acquisition, we made the following assumptions.Assumption A Expressions included in the same item-ization or listing in an HTML document are likelyto have a common hypernym.Assumption B Given a set of hyponyms that have acommon hypernym, the hypernym appears in manydocuments that include the hyponyms.Assumption C Hyponyms and their hypernyms are se-mantically similar.Our acquisition process computes a common hyper-nym for expressions in the same itemizations.
It pro-ceeds as follows.
First, we download a large numberof HTML documents from the WWW and extract a setof natural language expressions that are listed in thesame itemized region of documents.
Consider the item-ization in Fig.
1.
We extract the set of expressions,{Toyota,Honda,Nissan} from it.
From Assumption A,we can treat these expressions as candidates of hyponymsthat have a common hypernym such as ?company?.
Wecall such expressions in the same itemization hyponymcandidates.
Particularly, a set of the hyponym candi-dates extracted from a single itemization or list is calleda hyponym candidate set (HCS).
For the example docu-ment, we would treat Toyota, Honda, and Nissan as hy-ponym candidates, and regard them as members of thesame HCS.We then download documents that include at least onehyponym candidate by using an existing search engine,and pick up a noun that appears in the documents andthat has the largest score.
The score was designed sothat words appearing in many downloaded documents arehighly ranked, according to Assumption B.
We call theselected noun a hypernym candidate for the given hy-ponym candidates.Note that if we download documents including ?Toy-ota?
or ?Honda?, many will include the word ?company?,which is a true hypernym of Toyota.
However, wordswhich are not hypernyms, but which are closely associ-ated with Toyota or Honda (e.g., ?price?)
will also be in-cluded in many of the downloaded documents.
The nextstep of our procedure is designed to exclude such non-hypernyms according to Assumption C. We compute thesimilarity between hypernym candidates and hyponymcandidates in an HCS, and eliminate the HCS and its hy-pernym candidate from the output if they are not seman-tically similar.
For instance, if the previous step of ourprocedure produces ?price?
as a hypernym candidate forToyota and Honda, then the hypernym candidate and thehyponym candidates are removed from the output.
Weempirically show that this helps to improve overall preci-sion.Finally, we further elaborate computed hypernym can-didates by using additional heuristic rules.
Though weadmit that these rules are rather ad hoc, they worked wellin our experiments.We have tested the effectiveness of our methodsthrough a series of experiments in which we used HTMLdocuments downloaded from actual web sites.
We ob-served that our method can find a significant number ofhypernyms that (at least some of) alternative hypernymacquisition procedures cannot acquire, at least, when onlya rather small amount of texts are available.In this paper, Section 2 describes our acquisition al-gorithm.
Section 3 gives our experimental results whichwe obtained using Japanese HTML documents, and Sec-tion 4 discusses the benefit obtained through our methodbased on a comparison with alternative methods.2 Acquisition AlgorithmOur acquisition algorithm consists of four steps, as ex-plained in this section.Step 1 Extraction of hyponym candidates from itemizedexpressions in HTML documents.Step 2 Selection of a hypernym candidate with respectto df and idf.Step 3 Ranking of hypernym candidates and HCSsbased on semantic similarities between hypernymand hyponym candidates.<UL><LI>Car Specification</LI><UL><LI>Toyota</LI><LI>Honda</LI><LI>Nissan</LI></UL></UL>Figure 2: An example of HTML documentsStep 4 Application of a few additional heuristics to elab-orate computed hypernym candidates and hyponymcandidates.2.1 Step 1: Extraction of hyponym candidatesThe objective of Step 1 is to extract an HCS, which is a setof hyponym candidates that may have a common hyper-nym, from the itemizations or lists in HTML documents.Many methods can be used to do this.
Our approach isa simple one.
Each expression in an HTML documentcan be associated with a path, which specifies both theHTML tags that enclose the expression and the order ofthe tags.
Consider the HTML document in Figure 2.
Theexpression ?Car Specification?
is enclosed by the tags<LI>,</LI> and <UL>,</UL>.
If we sort these tagsaccording to their nesting order, we obtain a path (UL, LI)and this path specifies the information regarding the placeof the expression.
We write ?
(UL, LI),Car Specification?if (UL, LI) is a path for the expression ?Car Specifica-tion?.
We can then obtain the following paths for the ex-pressions from the document.?
(UL, LI),Car Specification?,?
(UL, UL, LI),Toyota?,?
(UL, UL, LI),Honda?,?
(UL, UL, LI),Nissan?Basically, our method extracts the set of expressionsassociated with the same path as an HCS 1.
In the aboveexample, we can obtain the HCS {Toyota,Honda, ?
?
?
}.We extract an itemization only when its size is n and3 < n < 20.
This is because the processing of largeitemizations (particularly the downloading of the relateddocuments) is time-consuming, and small itemizationsare often used to obtain a proper layout in HTML doc-uments2.1We actually need to distinguish different occurrences of thetags in some cases to prevent distinct itemizations from beingrecognized as a single itemization.2We found some words that are often inserted into an item-ization but do not have common semantic properties with otheritems in the same itemization, during the experiments using adevelopment set.
????
(links)?
and ????
(help)?
are ex-amples of such words.
We prepared a list of such words con-sisting of 70 items, and removed them from the HCSs obtainedin Step 1.2.2 Step 2: Selection of a hypernym candidate by dfand idfIn Step 1, we can obtain a set of hyponym candidates,an HCS, that may have a common hypernym.
In Step2, we select a common hypernym candidate for an HCS.First, we prepare two sets of documents.
We randomlyselect a large number of HTML documents and downloadthem.
We call this set of documents a global documentset.
We assume this document set indicates the generaltendencies of word frequencies.
Then we download thedocuments including each hyponym candidate in a givenHCS.
This document set is called a local document set,and we use it to know the strength of the association ofnouns with the hyponym candidates.Let us denote a given HCS as C, a local documentset obtained from all the items in C as LD(C), and aglobal document set as G. We also assume that N isa set of words, which can be candidates of hypernym3.A hypernym candidate, denoted as h(C), for C is ob-tained through the following formula, where df (n,D) isthe number of documents that include a noun n in a doc-ument set D.h(C) = argmaxn?N{df (n,LD(C)) ?
idf (n,G)}idf (n,G) = log |G|df (n,G)The score has a large value for a noun that appears in alarge number of documents in the local document set andis found in a relatively small number of documents in theglobal document set.In general, nouns strongly associated with many itemsin a given HCS tend to be selected through the above for-mula.
Since hyponym candidates tend to share a commonsemantic property, and their hypernym is one of the wordsstrongly associated with the common property, the hyper-nym is likely to be picked up through the above formula.Note that a process of generalization is performed auto-matically by treating all the hyponym candidates in anHCS simultaneously.
That is, words strongly connectedwith only one hyponym candidate (for instance, ?Lexus?for Toyota) have relatively low score values since we ob-tain statistical measures from all the local document setsfor all the hyponym candidates in an HCS.Nevertheless, this scoring method is a weak method inone sense.
There could be many non-hypernyms that are3In our experiments, N is a set consisting of 37,639 words,each of which appeared more than 500 times in 33 years ofJapanese newspaper articles (Yomiuri newspaper 1987-2001,Mainichi newspaper 1991-1999 and Nikkei newspaper 1983-1990; 3.01 GB in total).
We excluded 116 nouns that we ob-served never be hypernyms from N .
An example of such nounis ??
(I)?.
We found them in the experiments using a develop-ment set.strongly associated with many of the hyponym candidates(for instance, ?price?
for Toyota and Honda).
Such non-hypernyms are dealt with in the next step.An evident alternative to this method is to usetf (n,LD(C)), which is the frequency of a noun n in thelocal document set, instead of df (n,LD(C)).
We triedusing this method in our experiments, but it produced lessaccurate results, as we show in Section 3.2.3 Step 3: Ranking of hypernym candidates andHCSs by semantic similarityThus, our procedure can produce pairs consisting of ahypernym candidate and an HCS, which are denoted by{?h(C1), C1?, ?h(C2), C2?, ?
?
?
, ?h(Cm), Cm?
}.Here, C1, ?
?
?
, Cm are HCSs, and h(Ci) is a common hy-pernym candidate for hyponym candidates in an HCS Ci.In Step 3, our procedure ranks these pairs by using thesemantic similarity between h(Ci) and the items in Ci.The final output of our procedure is the top k pairs in thisranking after some heuristic rules are applied to it in Step4.
In other words, the procedure discards the remainingm ?
k pairs in the ranking because they tend to includeerroneous hypernyms.As mentioned, we cannot exclude non-hypernyms thatare strongly associated with hyponym candidates fromthe hypernym candidate obtained by h(C).
For exam-ple, the value of h(C) may be a non-hypernym ?price?,rather than ?company?, when C = {Toyota,Honda}.The objective of Step 3 is to exclude such non-hypernymsfrom the output of our procedure.
We expect such non-hypernyms to have relatively low semantic similarities tothe hyponym candidates, while the behavior of true hy-pernyms should be semantically similar to the hyponyms.If we rank the pairs of hypernym candidates and HCSsaccording to their semantic similarities, the low rankedpairs are likely to have an erroneous hypernym candidate.We can then obtain relatively precise hypernyms by dis-carding the low ranked pairs.The similarities are computed through the followingsteps.
First, we parse all the texts in the local documentset, and check the argument positions of verbs where hy-ponym candidates appear.
(To parse texts, we use a down-graded version of an existing parser (Kanayama et al,2000) throughout this work.)
Let us denote the frequencyof the hyponym candidates in an HCS C occupying anargument position p of a verb v as fhypo(C, p, v).
As-sume that all possible argument positions are denoted as{p1, ?
?
?
, pl} and all the verbs as {v1, ?
?
?
, vm}.
We thendefine the co-occurrence vector of hyponym candidatesas follows.hypov(C) = ?fhypo(C, p1, v1), fhypo(C, p2, v1), ?
?
?
,fhypo(C, pl?1, vm), fhypo(C, pl, vm)?In the same way, we can define the co-occurrence vec-tor of a hypernym candidate n.hyperv(n) = ?f(n, p1, v1), ?
?
?
, f(n, pl, vm)?Here, f(n, p, v) is the frequency of a noun n occupyingan argument position p of a verb v obtained from the pars-ing results of a large number of documents - 33 years ofJapanese newspaper articles (Yomiuri newspaper 1987-2001, Mainichi newspaper 1991-1999, and Nikkei news-paper 1990-1998; 3.01 GB in total) - in our experimentalsetting.The semantic similarities between hyponym candi-dates in C and a hypernym candidate n are then computedby a cosine measure between the vectors:sim(n,C) = hypov(C) ?
hyperv(n)|hypov(C)||hyperv(n)|Our procedure sorts the hypernym-HCS pairs{?h(Ci), Ci?
}mi=1 using the valuesim(h(Ci), Ci) ?
df (h(Ci),LD(Ci)) ?
idf (h(Ci), G)Note that we consider not only the similarity but also thedf ?
idf score used in Step 2 in the sorting.An evident alternative to the above method is the al-gorithm that re-ranks the top j hypernym candidates ob-tained by df ?
idf for a given HCS by using the samescore.
However, we found no significant improvementwhen this alternative was used in our experiments, as welater explain.2.4 Step 4: Application of other heuristic rulesThe procedure described up to now can produce a hyper-nym for hyponym candidates with a certain precision.
Wefound, though, that we can improve accuracy by using afew more heuristic rules, which are listed below.Rule 1 If the number of documents that include a hyper-nym candidate is less than the sum of the numbers ofthe documents that include an item in the HCS, thendiscard both the hypernym candidate and the HCSfrom the output.Rule 2 If a hypernym candidate appears as substrings ofan item in its HCS and it is not a suffix of the item,then discard both the hypernym candidate and theHCS from the output.
If a hypernym candidate isa suffix of its hyponym candidate, then half of themembers of an HCS must have the hypernym can-didate as their suffixes.
Otherwise, discard both thehypernym candidate and its HCS from the output.Rule 3 If a hypernym candidate is an expression belong-ing to the category of place names, then replace it by?place name?.In general, we can expect that a hypernym is used ina wider range of contexts than those of its hyponyms,and that the number of documents including the hyper-nym candidate should be larger than the number of webdocuments including hyponym candidates.
This justifiesRule 1.
We use the hit counts given by an existing searchengine as the number of documents including an expres-sion.As for Rule 2, note that Japanese is a headfinal language, and a semantic head of a com-plex noun phrase is the last noun.
Considerthe following two Japanese complex nouns.amerika-eiga / nihon-eiga(American) (movie) / (Japanese) (movie)Apparently an American movie is a kind of movie as isa Japanese movie.
There are many multi-word expres-sions whose hypernyms are their suffixes, and if someexpressions share a common suffix, it is likely to be theirhypernym.
However, if a hypernym candidate appears ina position other than as a suffix of a hyponym candidate,the hypernym candidate is likely to be an erroneous one.In addition, if a hypernym candidate is a common suffixof only a small portion of an HCS, then the HCS tendsnot to have semantic uniformity, and such a hypernymcandidate should be eliminated from the output.
(We em-pirically determined ?one-half?
as a threshold in our ex-periments on the development set.
)As for Rule 3, in our experiments on a development set,we found that our procedure could not provide precise hy-pernyms for place names such as ?Kyoto?
and ?Tokyo?.In the case of Kyoto and Tokyo, our procedure produced?Japan?
as a hypernym candidate.
Although ?Japan?
isconsistent with most of our assumptions regarding hy-pernyms, it is a holonym of Kyoto and Tokyo, but theirhypernym.
In general, when a set of place names is givenas an HCS, the procedure tends to produce the name ofthe region or area that includes all the places designatedby the hyponym candidates.
We then added the rule to re-place such place names by the expression ?place name,?which is a true hypernym in many of such cases 4.Recall that we obtained the ranked pairs of an HCS andits common hypernym in Step 3.
By applying the aboverules, some pairs are removed from the ranked pairs, orare modified.
For some given integer k, the top k pairs ofthe obtained ranked pairs become the final output of ourprocedure, as mentioned before.3 Experimental ResultsWe downloaded about 8.71 ?
105 HTML documents(10.4 GB with HTML tags), and extracted 9.02 ?
104HCSs through the method described in Section 2.1.
We4To judge if a hypernym candidate is a place name, weused the output of a morphological analyzer (Matsumoto et al,1993).0204060801000  500  1000  1500  2000  2500accuracy [%]# of hypernym hyponym pairsProposed Method (Step 4)Step 3Step 2Step 2 (tf)Figure 3: Contribution of each step0204060801000  500  1000  1500  2000  2500accuracy [%]# of hypernym hyponym pairsProposed Method-Step 3-Step 4-Rule 1-Rule 2-Rule 3Figure 4: Contribution of each step and rulerandomly picked 2,000 HCSs from among the extractedHCS as our test set.
The test set contained 13,790 hy-ponym candidates.
(Besides these HCSs, we used a de-velopment set consisting of about 4,000 HCSs to developour algorithm.)
For each single hyponym candidate, wedownloaded the top 100 documents in the ranking pro-duced by a search engine5 as a local document set if theengine found more than 100 documents.
Otherwise, allthe documents were downloaded.
(Note that a local doc-ument set for an HCS may contain more than 100 doc-uments.)
As a global document set, we used the down-loaded 1.00 ?
106 HTML documents (1.26 GB withoutHTML tags).Fig.
3 shows the accuracy of hypernymsobtained after Steps 2, 3, and 4.
We as-sumed each step produced the sorted pairs ofan HCS and a hypernym, which are denoted by{?h(C1), C1?, ?h(C2), C2?, ?
?
?
, ?h(Cm), Cm?}.
Thesorting was done by the score sim(h(Ci), Ci) ?df (h(Ci),LD(Ci)) ?
idf (h(Ci), G) after Steps 3 and4, as described before, while the output of Step 2 wassorted by the df ?
idf score.
In addition, we assumed5The search engine ?goo?.
(http://www.goo.ne.jp)0204060801000  500  1000  1500  2000  2500accuracy [%]# of hypernym hyponym pairsProposed MethodRe-ranking of top 2 hypernym candidatesRe-ranking of top 3 hypernym candidatesRe-ranking of top 4 hypernym candidatesRe-ranking of top 5 hypernym candidatesFigure 5: Contribution of re-rankingeach step produced only the top 200 pairs in the sortedpairs.
(Since the output of Step 4 is the final output, thismeans that we also assumed that only the top 200 pairsof a hypernym and an HCS would be produced as finaloutput with our procedure.
In other words, the remaining1,800 (=2,000-200) pairs were discarded.
)The resulting hypernyms were checked by the authorsaccording to the definition of the hypernym given inMiller et al, 1990, i.e., we checked if the expression ?ahyponym candidate is a kind of a hypernym candidate.
?is acceptable.
Then, we computed the precision, which isthe ratio of the correct hypernym-hyponym pairs againstall the pairs obtained from the top n pairs of an HCS andits hypernym candidate.
The x-axis of the graph indicatesthe number of hypernym-hyponym pairs obtained fromthe top n pairs of an HCS and its hypernym candidate,while the y-axis indicates the precision.More precisely, the curve for Step i plots the followingpoints, where 1 ?
j ?
200.?j?k=1|Ck|,?jk=1 correct(Ck, h(Ck))?jk=1 |Ck|?correct(Ck, h(Ck)) indicates the number of hyponymcandidates in Ck that are true hyponyms of h(Ck).
Notethat after Step 4, the precision reached about 75% for 701hyponym candidates, which was slightly more than 5%of all the given hyponym candidates.
For 1398 hyponymcandidates (about 10% of all the candidates), the preci-sion was about 61%.Another important point is that ?Step 2 (tf)?
in thegraph refers to an alternative to our Step 2 procedure; i.e.,the Step 2 procedure in which df (h(C),LD(C)) was re-placed by tf (h(C),LD(C)).
One can see the Step 2 pro-cedure with df works better than that with tf .Table 1 shows some examples of the acquired HCSsand their common hypernyms.
Recall that a common suf-fix of an HCS is a good candidate to be a hypernym.
Theexamples were taken from cases where a common suffixhypernym?hyponym?,hyponym .*???
.
* hypernym,hyponym .*??
(?|?)?
hypernym,hyponym .*????
.
* hypernym,hyponym .*???
.
* hypernym,hyponym .*?
(?
|?)?
.
* hypernym,hyponym .*?????
.
* hypernym,hyponym .
* (?
|??)
.
* hypernymThe hypernym and hyponym may be bracketed by?
?or ?
?.Figure 6: lexicosyntactic patternsof an HCS was not produced as a hypernym.
This listis actually the output of Step 3, and shows which HCSsand their hypernym candidates were eliminated/modifiedfrom the output in Step 4 and which rule was fired toeliminate/modify them.Next, we eliminated some steps from the whole pro-cedure.
Figure 4 shows the accuracy when one of thesteps was eliminated from the procedure.
?-Step X?
or?-Rule X?
refers to the accuracies obtained through theprocedure from which step X or rule X were eliminated.Note that both graphs indicate that every step and rulecontributed to the improvement of the precision.Figure 5 compares our method and an alternativemethod, which was the algorithm that re-ranks the top jhypernym candidates for a given HCS by using the scoresim(h,C) ?
df (h,LD(C)) ?
idf (h,G), where h is a hy-pernym candidate, in Step 3.
(Recall that our algorithmuses the score only for sorting pairs of HCSs and their hy-pernym.
In other words, we do not re-rank the hypernymcandidates for a single HCS.)
We found no significant im-provement when the alternative was used.4 Comparison with alternative methodsWe have shown that our assumptions are effective for ac-quiring hypernyms.
However, there are other alternativemethods applicable under our settings.
We evaluated thefollowings methods and compared the results with thoseof our procedure.Alternative 1 Compute the non-null suffixes that areshared by the maximum number of hyponym can-didates, and regard the longest as a hypernym can-didate.Alternative 2 Extract hypernyms for hyponym candi-dates by looking at the captions or titles of the item-izations from which hyponym candidates are ex-tracted.Alternative 3 Extract hypernyms by using lexicosyntac-tic patterns.Alternative 4 Combinations of Alternative 1-3.The evaluation method for Alternative 1 and Alterna-tive 2 is the same as the one for our method.
We simplyjudged if the produced hypernyms are acceptable or not.But we used different evaluation method for the otheralternatives.
We checked if the correct hypernyms pro-duced by our method can be found by these alternatives.This is simply for the sake of easiness of the evaluation.Note that we evaluated Alternative 1 and Alternative 2 inthe second evaluation scheme when they are combinedand are used as a part of Alternative 4.More detailed explanations on the alternative methodsare given below.Alternative 1 Recall that Japanese is a head final lan-guage, and we have explained that common suffixes ofhyponym candidates are good candidates to be commonhyponyms.
Alternative 1 computes a hypernym candidateaccording to this principle.Alternative 2 This method uses the captions of theitemizations, which are likely to contain a hypernym ofthe items in the itemization.
We manually found cap-tions or titles that are in the position such that they canexplain the content of the itemization, and picked up thecaption closest to the itemization and the second closestto it.
Then, we checked if the picked-up captions includedthe proper hypernyms.
Note that the precision obtainedby this method is just an upper bound of real performancebecause we do not have a method to extract hypernymsfrom captions at least at the current stage of our research.Alternative 3 We prepared the lexicosyntactic patternsin Fig.
6, which are similar to the ones used in the pre-vious studies of hypernym acquisition in Japanese (Ima-sumi, 2001; Ando et al, 2003).
One difference from theprevious studies was that we used a regular expressioninstead of a parser.
This may have caused some errors,but our patterns were more generous than those used inthe previous studies, and did not miss the expressionsmatched to the patterns from the previous studies.
Inother words, the accuracy obtained with our patterns wasan upper bound on the performance obtained by the previ-ous proposal.
Another difference was that the procedurewas given correct pairs of a hypernym and a hyponymcomputed beforehand using our proposed method, and itonly checked whether given pairs could be found by us-ing the lexicosyntactic patterns from given texts.
In otherwords, this alternative method checked if the lexicosyn-tactic patterns could find the hypernym-hyponym pairssuccessfully obtained by our procedure.
The texts usedwere local document sets from which our procedure com-puted a hypernym candidate.
If our procedure has betterfigures than this method, this means that our procedurecan produce hypernyms that cannot be acquired by pat-terns, at least, from a rather small number of texts (i.e., amaximum of 100 documents per hyponym candidate).0204060801000  500  1000  1500  2000accuracy [%]# of hypernym hyponym pairsProposed MethodAlternative 1Alternative 2Alternative 3Alternative 4Figure 7: Comparison with alternative methodsAlternative 4 We also compared our procedure withthe combination of all the above methods: Alternative4.
Again, we checked whether the combination couldfind the correct hypernym-hyponym pairs provided byour method.
The difference between the precision of ourmethod and that of Alternative 4 reflects the number ofhypernym-hyponym pairs that our method could acquireand that Alternative 4 could not.
We assumed that for agiven HCS a hypernym was successfully acquired if oneof the above methods could find the correct hypernym.
Inother words, the performance of Alternative 4 would beachieved only when there were a technique to combinethe output of the above methods in an optimal way.Figure 7 shows the comparison between our procedureand the alternative methods.
We plotted the graph as-suming the pairs of hypernym candidates and hyponymcandidates were sorted in the same order as the order ob-tained by our procedure6.
The results suggest that ourmethod can acquire a significant number of hypernymsthat the alternative methods cannot obtain, when we gaverather small amount of texts, a maximum of 100 docu-ments per hyponym candidate, as in our current experi-mental settings.
There is possibility that the difference,particularly the difference from the peformance of Alter-native 3, becomes smaller when we give more texts to thealternative methods.
But the comparison in such settingsis actually a difficult task because of the time required fordownloading.
It is our possible future work.5 Concluding Remarks and Future WorkWe have proposed a method for acquiring hyponymy re-lations from Web documents, and have shown its effec-tiveness through experimental results.
We also showed6More precisely, we sorted only the hyponym candidates inthe order used by our procedure for sorting, and attached thehypernym candidates produced by each alternative to the hy-ponym candidates.that our method could find a significant number of hy-ponymy relations that alternative methods could not, atleast when the amount of documents used was rathersmall.The first goal of our future work is to further improvethe precision of our method.
One possible approach willbe to combine our methods with alternative techniques,which were actually examined in our experiments.
Oursecond goal is to extend our method so that it can han-dle multi-word hypernyms.
Currently, our method pro-duces just ?company?
as a hypernym of ?Toyota?.
If wecan obtain a multi-word hypernym such as ?automobilemanufacturer,?
it can provide more useful information tovarious types of natural language processing systems.ReferencesMaya Ando, Satoshi Sekine, and Shun Ishizaki.
2003.Automatic extraction of hyponyms from newspaper us-ing lexicosyntactic patterns.
In IPSJ SIG Technical Re-port 2003-NL-157, pages 77?82.
in Japanese.Sharon A. Caraballo.
1999.
Automatic construction ofa hypernym-labeled noun hierarchy from text.
In Pro-ceedings of 37th Annual Meeting of the Association forComputational Linguistics, pages 120?126.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online ques-tion answering: Answering questions before they areasked.
In Proceedings of the 41st Annural Meeting ofthe Association for Computational Linguistics, pages1?7.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th International Conference on Computational Lin-guistics, pages 539?545.Kyosuke Imasumi.
2001.
Automatic acquisition of hy-ponymy relations from coordinated noun phrases andappositions.
Master?s thesis, Kyushu Institute of Tech-nology.Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi,and Jun?ichi Tsujii.
2000.
A hybrid Japanese parserwith hand-crafted grammar and statistics.
In Proceed-ings of COLING 2000, pages 411?417.Yuji Matsumoto, Sadao Kurohashi, Takehito Utsuro, Hi-roshi Taeki, and Makoto Nagao.
1993.
JapaneseMorphological Analyzer JUMAN user?s manual.
inJapanese.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to wordnet: An on-line lexical database.Journal of Lexicography, 3(4):235?244.Emmanuel Morin and Christian Jacquemin.
2003.
Auto-matic acquisition and expansion of hypernym links.
InComputer and the Humanities 2003. forthcoming.Table 1: Examples of the acquired pairs of a hypernym candidate and HCS.Rank Hypernyms Rank Fired Hypernymsby Hyponym candidate sets obtained in by Rules obtained inStep4 Step3 Step3 1 2 3 Step4??
(murder)*,??
(arson)*,??
(rape)*,???
(burglary)*,??
?
?29 ????
(burgle robbery)*,????
(theft without breaking-in)*, (crime) 68 ?
?
?
(crime)?????
(robbery without breaking-in)*????
(Moscow)*,???
(Kiev)*,?????
(Tashkend)*,??69????
(Minsk)*,????
(Tbilisi)*,??????
(Dushanbe)*, ??
?169 ?
?
+ (place?????
(Bishkek)*,????
(Astana)*,?????
(Kishinev)*, (Russia)name)????
(Erevan)*,???
(Baku)*,??????
(Ashkhabad)*?????
(Seguignol)*,????
(Yasuo Fujii)*,????
(Yuji Goshima)*,????
(Tomotaka Tamaki)*,??
?
?78 ????
(Hiroki Fukutome)*,????
(Keiichi Hirano)*, (player) 196 ?
?
?
(player)?????
(Sheldon)*,????
(Kazuhiko Shiotani)*,(These are baseball players.)????????
(wireless card),?????????(security),??
?
?81 ?????
(radio),???????
(a kind of instrument), (wireless) 200 ?
?
?
(wireless)????????
(a kind of department)????
(Diapensia lapponica)*,?????
(Sasa kurilensis),?
?116 ????????
(Rhododendron aureum)*, (flower) 280 ?
?
?
(flower)????????
(Polygonatum lasianthum)*????
(shiitake mushroom)*,???????
(Hericium ramosum)*,?????
(Pseudocolus schellenbergiae)*, ???
??
?127 ????????
(Rhodophyllus murraii)*, (mash- 306 ?
?
?
(mash-??????
(Amanita virgineoides Bas)*, room) room)?????
(Pseudocolus schellenbergiae)*139??
(music),??
(movie),???
(cartoon), ??
?324 ?
?
???????
(encounter),???
(artiste) (web site) (web site)?????
(Ryunosuke Akutagawa),????
(Tsugi Takano),????
(Bokusui Wakayama),?????
(Motojiro Kajii),????
(Roka Tokutomi),?????
(Yuriko Miyamoto),150????
(Soseki Natume),?????
(Kantaro Tanaka), ?
?343 ?
?
????????
(Doppo Kunikida),????
(Kyusaku Yumeno), (work) (work)?????????
(William Blake),???
(Kan Kikuchi),????????
(parse error)(These are novelists.)????
(May Day),?????
(Christmas Day),?????
(Easter),??
(the New Year),???
(All Saints?
Day),???
(Epifania),???
?172 ?????
(Emancipation Day),?????
(Immacolata concezione), (Japan) 391 ?
?
+ (place????????
(Stefano?s Day),?????
(Ferragosto) name)(These are national holidays in Italy.)?????
(mother)*,??
(warm current)*,??
(cloud drift)*,??
?
?184 ???
(blue sky girl)*,??????
(beauty has guilt)* (movie) 416 ?
?
?
(movie)(These are Japanese movies.)???
(group of galaxies),??????
(member),???
????????
(Andromeda Galaxy)*,???
(The Galaxy)*, (galaxy) 10 ?
+ ?
??????
(local group of galaxies)????
(Brazil),?????
(Philippine),??
(Korea),????
(India),????
(U.S.A.),??
(Thailand), ?
?80 + ?
+ ???
(China),???
(Peru),???????
(Australia), (Japan)??????
(Argentina),????
(Spain)?*?
indicates a hyponym candidate that is a true hyponym of the provided hypernym candidate.?+?
in the ?Fired Rules?
column indicates a firing rule, while ???
specifies the rule that doesn?t fire.
