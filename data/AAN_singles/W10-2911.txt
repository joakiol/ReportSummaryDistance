Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77?87,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsType Level Clustering Evaluation: New Measuresand a POS Induction Case StudyRoi Reichart1?
Omri Abend2??
Ari Rappoport21ICNC 2Institute of Computer ScienceHebrew University of Jerusalem{roiri|omria01|arir}@cs.huji.ac.ilAbstractClustering is a central technique in NLP.Consequently, clustering evaluation is ofgreat importance.
Many clustering algo-rithms are evaluated by their success intagging corpus tokens.
In this paper wediscuss type level evaluation, which re-flects class membership only and is inde-pendent of the token statistics of a partic-ular reference corpus.
Type level evalua-tion casts light on the merits of algorithms,and for some applications is a more naturalmeasure of the algorithm?s quality.We propose new type level evaluationmeasures that, contrary to existing mea-sures, are applicable when items are pol-ysemous, the common case in NLP.
Wedemonstrate the benefits of our measuresusing a detailed case study, POS induc-tion.
We experiment with seven leadingalgorithms, obtaining useful insights andshowing that token and type level mea-sures can weakly or even negatively corre-late, which underscores the fact that thesetwo approaches reveal different aspects ofclustering quality.1 IntroductionClustering is a central machine learning technique.In NLP, clustering has been used for virtually ev-ery semi- and unsupervised task, including POStagging (Clark, 2003), labeled parse tree induction(Reichart and Rappoport, 2008), verb-type clas-sification (Schulte im Walde, 2006), lexical ac-quisition (Davidov and Rappoport, 2006; Davi-dov and Rappoport, 2008), multilingual document?
* Both authors equally contributed to this paper.?
Omri Abend is grateful to the Azrieli Foundation forthe award of an Azrieli Fellowship.clustering (Montavlo et al, 2006), coreference res-olution (Nicolae and Nicolae, 2006) and namedentity recognition (Elsner et al, 2009).
Conse-quently, the methodology of clustering evaluationis of great importance.
In this paper we focuson external clustering evaluation, i.e., evaluationagainst manually annotated gold standards, whichexist for almost all such NLP tasks.
External eval-uation is the dominant form of clustering evalu-ation in NLP, although other methods have beenproposed (see e.g.
(Frank et al, 2009)).In this paper we discuss type level evaluation,which evaluates the set membership structure cre-ated by the clustering, independently of the tokenstatistics of the gold standard corpus.
Many clus-tering algorithms are evaluated by their successin tagging corpus tokens (Clark, 2003; Nicolaeand Nicolae, 2006; Goldwater and Griffiths, 2007;Gao and Johnson, 2008; Elsner et al, 2009).
How-ever, in many cases a type level evaluation is thenatural one.
This is the case, for example, whena POS induction algorithm is used to compute atag dictionary (the set of tags that each word cantake), or when a lexical acquisition algorithm isused for constructing a lexicon containing the setof frames that a verb can participate in, or when asense induction algorithm computes the set of pos-sible senses of each word.
In addition, even whenthe goal is corpus tagging, a type level evaluationis highly valuable, since it may cast light on therelative or absolute merits of different algorithms(as we show in this paper).Clustering evaluation has been extensively in-vestigated (Section 3).
However, the discussioncenters around the monosemous case, where eachitem belongs to exactly one cluster, although pol-ysemy is the common case in NLP.The contribution of the present paper is as fol-lows.
First, we discuss the issue of type level eval-uation and explain why even in the monosemouscase a token level evaluation presents a skewed77picture (Section 2).
Second, we show for thecommon polysemous case why adapting existinginformation-theoretic measures to type level eval-uation is not natural (Section 3).
Third, we pro-pose new mapping-based measures and algorithmsto compute them (Section 4).
Finally, we performa detailed case study with part-of-speech (POS)induction (Section 5).
We compare seven lead-ing algorithms, showing that token and type levelmeasures can weakly or even negatively correlate.This shows that type level evaluation indeed re-veals aspects of a clustering solution that are notrevealed by the common tagging-based evaluation.Clustering is a vast research area.
As far as weknow, this is the first NLP paper to propose typelevel measures for the polysemous case.2 Type Level Clustering EvaluationThis section motivates why both type and tokenlevel external evaluations should be done, even inthe monosemous case.Clustering algorithms compute a set of inducedclusters (a clustering).
Some algorithms directlycompute a clustering, while some others producea tagging of corpus tokens from which a clusteringcan be easily derived.
A clustering is monosemousif each item is allowed to belong to a single clusteronly, and polysemous otherwise.
An external eval-uation is one which is based on a comparison of analgorithm?s result to a gold standard.
In this paperwe focus solely on external evaluation, which isthe most common evaluation approach in NLP.Token and type level evaluations reflect differ-ent aspects of a clustering.
External token levelevaluation assesses clustering quality according tothe clustering?s accuracy on a given manually an-notated corpus.
This is certainly a useful evalua-tion measure, e.g.
when the purpose of the cluster-ing algorithm is to annotate a corpus to serve asinput to another application.External type level evaluation views the com-puted clustering as a set membership structure andevalutes it independently of the token statistics inthe gold standard corpus.
There are two maincases in which this is useful.
First, a type levelevaluation can be the natural one in light of theproblem itself.
For example, if the purpose ofthe clustering algorithm is to automatically builda lexicon (e.g., VerbNet (Kipper et al, 2000)),then the lexicon structure itself should be evalu-ated.
Second, it may be valuable to decouple cor-pus statistics from the induced clustering when thelatter is to be used for annotating corpora that ex-hibit different statistics.
In other words, if we eval-uate an algorithm that will be invoked on a diverseset of corpora having different token statistics, atype level evaluation might provide a better picture(or at least a complementary one) on the quality ofthe clustering algorithm.To motivate type level evaluation, consider POSinduction, which exemplifies both cases above.Clearly, a word form may belong to several partsof speech (e.g., ?contrast?
is both a noun and averb, ?fast?
is both an adjective and an adverb,?that?
can be a determiner, conjunction and adverb,etc.).
As an evaluation of a POS induction algo-rithm, it is natural to evaluate the lexicon it gener-ates, even if the main goal is to annotate a corpus.The lexicon lists the possible POS tags for eachword, and thus its evaluation is a polysemous typelevel one.Even if we ignore polysemy, type level evalua-tion is useful for a POS induction algorithm usedto tag a corpus.
There are POS classes whosemembers are very frequent, e.g., determiners andprepositions.
Here, a very small number of wordtypes usually accounts for a large portion of corpustokens.
For example, in the WSJ Penn Treebank(Marcus et al, 1993), there are 43,740 word typesand over 1M word tokens.
Of the types, 88 aretagged as prepositions.
These types account foronly 0.2% of the types, but for as many as 11.9%of the tokens.
An algorithm which is accurate onlyon prepositions would do much better in a tokenlevel evaluation than in a type level one.This phenomenon is not restricted to preposi-tions or English.
In the WSJ corpus, determinersaccount for 0.05% of the types but for 9.8% of thetokens.
In the German NEGRA corpus (Brants,1997), the article class (both definite and indefi-nite) accounts for 0.04% of the word types and for12.5% of the word tokens, and the coordinatingconjunctions class accounts for 0.05% of the wordtypes but for 3% of the tokens.The type and token behavior differences resultfrom the Zipfian distribution of word tokens toword types (Mitzenmacher, 2004).
Since the wordfrequency distribution is Zipfian, any clustering al-gorithm that is accurate only on a small number offrequent words (not necessarily members of a par-ticular class) would perform well in a token levelevaluation but not in a type one.
For example,78the most frequent 100 word types (regardless ofPOS class) in WSJ (NEGRA) account for 43.9%(41.3%) of the tokens in the corpus.
These wordsappear in 32 out of the 34 non-punctuation POSclasses in WSJ and in 38 out of the 51 classes inNEGRA.Other natural language entities also demonstrateZipfian distribution of tokens to types.
For exam-ple, the distribution of syntactic categories in parsetree constituents is Zipfian, as shown in (Reichartand Rappoport, 2008) for English, German andChinese corpora.
Thus, the distinction between to-ken and type level evaluation is important also forgrammar induction algorithms.It may be argued that a token level evaluationis sufficient since it already reflects type informa-tion.
In this paper we demonstrate that this is notthe case, by showing that they correlate weakly oreven negatively in an important NLP task.3 Existing Clustering EvaluationMeasuresClustering evaluation is challenging.
Many mea-sures have been proposed in the past decades(Pfitzner et al, 2008).
In this section, we brieflysurvey the three main types: mapping based,counting pairs, and information theoretic mea-sures, and motivate our decision to focus on thefirst in this paper.Mapping based measures are based on a post-processing step in which each induced cluster ismapped to a gold class (or vice versa).
The stan-dard mappings are greedy many-to-one (M-1) andgreedy one-to-one (1-1).
Several measures whichrely on these mappings were proposed.
The mostcommon and perhaps the simplest one is accu-racy, which computes the fraction of items cor-rectly clustered under the mapping.
Other mea-sures include: L (Larsen, 1999), D (Van Dongen,2000), misclassification index (MI) (Zeng et al,2002), H (Meila and Heckerman, 2001), clusteringF-measure (Fung et al, 2003) and micro-averagedprecision and recall (Dhillon et al, 2003).
In Sec-tion 4 we show why existing mapping-based mea-sures cannot be applied to the polysemous typecase and present new mapping-based measures forthis case.Counting pairs measures are based on a com-binatorial approach which examines the numberof data element pairs that are clustered similarlyin the reference and proposed clustering.
Amongthese are Rand Index (Rand, 1971), Adjusted RandIndex (Hubert and Arabie, 1985), ?
statistic (Hu-bert and Schultz, 1976), Jaccard (Milligan et al,1983), Fowlkes-Mallows (Fowlkes and Mallows,1983) and Mirkin (Mirkin, 1996).
Schulte imWalde (2006) used such a measure for type levelevaluation of monosemous verb type clustering.Meila (2007) described a few problems withsuch measures.
A serious one is that their valuesare unbounded, making it hard to interpret theirresults.
This can be solved by adjusting their val-ues to lie in [0, 1], but even adjusted measures suf-fer from severe distributional problems, limitingtheir usability in practice.
We thus do not addresscounting pairs measures in this paper.Information-theoretic (IT) measures.
ITmeasures assume that the items in the dataset aretaken from a known distribution (usually the uni-form distribution), and thus the gold and inducedclusters can be treated as random variables.
Thesemeasures utilize a co-occurrence matrix I betweenthe gold and induced clusters.
We denote the in-duced clustering by K and the gold clustering byC.
Iij contains the number of items in the in-tersection of the i-th gold class and the j-th in-duced cluster.
When assuming the uniform dis-tribution, the probability of an event (a gold classc or an induced cluster k) is its relative size, sop(c) = ?|K|k=1IckN and p(k) =?|C|c=1IckN (N is thetotal number of clustered items).Under this assumption we define the entropiesand the conditional entropies:H(C) = ?
P|C|c=1P|K|k=1 IckN logP|K|k=1 IckNH(C|K) = ?
P|K|k=1P|C|c=1IckN logIckP|C|c=1 IckH(K) and H(K|C) are defined similarly.In Section 5 we use two IT measures for tokenlevel evaluation, V (Rosenberg and Hirschberg,2007) and NVI (Reichart and Rappoport, 2009)(a normalized version of VI (Meila, 2007)).
Theappealing properties of these measures have beenextensively discussed in these references; see also(Pfitzner et al, 2008).
V and NVI are defined asfollows:h =(1 H(C) = 01 ?
H(C|K)H(C) H(C) 6= 0c =(1 H(K) = 01 ?
H(K|C)H(K) H(K) 6= 0V = 2hch + c79NV I(C, K) =(H(C|K)+H(K|C)H(C) H(C) 6= 0H(K) H(C) = 0In the monosemous case (type or token), the ap-plication of the measures described in this sectionto type level evaluation is straightforward.
In thepolysemous case, however, they suffer from seri-ous shortcomings.Consider a case in which each item is assignedexactly r gold clusters and each gold cluster hasthe exact same number of items (i.e., each has asize of l?r|C| , where l is the number of items).
Now,consider an induced clustering where there are |C|induced clusters (|K| = |C|) and each item is as-signed to all induced clusters.
The co-occurrencematrix in this case should have identical values inall its entries.
Even if we allow the weight eachitem contributes to the matrix to depend on its goldand induced entry sizes, the situation will remainthe same.
This is because all items have the exactsame entry size and both gold and induced cluster-ings have uniform cluster sizes.In this case, the random variables defined by theinduced and gold clustering assignments are in-dependent (this easily follows from the definitionof independent events, since the joint probabilityis the multiplication of the marginals).
Hence,H(K|C) = H(K) and H(C|K) = H(C), andboth V and NVI obtain their worst possible val-ues1.
However, the score should surely depend onr (the size of each word?s gold entry).
Specifi-cally, when r = |C| we get that the induced andgold clusterings are identical.
This case should notget the worst score, and it should definitely scorehigher than the case in which r = 1, where K isdramatically different from C.The problem can in theory be solved by pro-viding the number of clusters per item as an inputto the algorithm.
However, in NLP this is unre-alistic (even if the total number of clusters can beprovided) and the number should be determinedby the algorithm.
We therefore do not considerIT-based measures in this paper, deferring them tofuture work.4 Mapping Based Measures forPolysemous Type EvaluationIn this section we present new type level evalu-ation measures for the polysemous case.
As we1V values are in [0, 1], 0 being the worst.
NVI obtains itshighest and worst possible value, 1 + log(|K|)H(C) .show below, these measures do not suffer from theproblems discussed for IT measures in Section 3.All measures are mapping-based: first, a map-ping between the induced and gold clusters is per-formed, and then a measure E is computed.
Asis common in the clustering evaluation literature(Section 3), we use M-1 and 1-1 greedy mappings,defined to be those that maximize the correspond-ing measure E.Let C = {c1, ..., cn} be the set of gold classesand K = {k1, ..., km} be the set of induced clus-ters.
Denote the number of words types by l. LetAi ?
C, Bi ?
K, i = 1...l be the set of goldclasses and set of induced clusters for each word.The polysemous nature of task is reflected by thefact that Ai and Bi are subsets, rather than mem-bers, of C and K respectively.Our measures address quality from two persec-tives, that of the individual items clustered (Sec-tion 4.1) and that of the clusters (Section 4.2).Item-based measures especially suit evaluation ofclustering quality for the purpose of lexicon induc-tion, and have no counterpart in the monosemouscase.
Cluster-based measures are a direct general-ization of existing mapping based measures to thepolysemous case.The difficulty in designing item-based andcluster-based measures is that the number of clus-ters assigned to each item is determined by theclustering algorithm.
Below we show how to over-come this.4.1 Item-Based EvaluationFor a given mapping h : K ?
C, denoteh(Bi) = {h(x) : x ?
Bi}.
A fundamental quan-tity for item-based evaluation is the number of cor-rect clusters for each item (word type) under thismapping, denoted by IMi (IM stands for ?itemmatch?
):IMi = |Ai ?
h(Bi)|The total item match IM is defined to be:IM = ?li=1 IMi =?li=1 |Ai ?
h(Bi)|In the monosemous case, IM is normalized bythe number of items, yielding an accuracy score.Applying a similar definition in the polysemouscase, normalizing instead by the total number ofgold clusters assigned to the items, can be easilymanipulated.
Even a clustering which has the cor-rect number of induced clusters (equal to the num-ber of gold classes) but which assigns each item to80all induced clusters, receives a perfect score underboth greedy M-1 and 1-1 mappings.
This holds forany induced clustering for which ?i, Ai ?
h(Bi).Note that using a mapping from C to K (or acombination of both directions) would exhibit thesame problem.To overcome the problem, we use the harmonicaverage of two normalized terms (F-score).
Weuse two average variants, micro and macro.
Macroaverage computes the total number of matchesover all words and normalizes in the end.
Recall(R), Precision (P) and their harmonic average (F-score) are accordingly defined:R = IMPli=1 |Ai|P = IMPli=1 |h(Bi)|MacroI = 2RPR + P == 2IMPli=1 |Ai| +Pli=1 |h(Bi)|= F (h) ?lXi=1IMiF (h) is a constant depending on h. As all itemsare equally weighted, those with larger gold andinduced entries have more impact on the measure.The micro average, aiming to give all items anequal status, first computes an F-score for eachitem and then averages over them.
Hence, eachitem contributes at most 1 to the measure.
ThisMicroI measure is given by:Ri = IMi|Ai| Pi =IMi|h(Bi)| Fi =2RiPiRi+Pi =2IMi|Ai|+|h(Bi)|MicroI = 1llXi=1Fi =1llXi=12IMi|Ai| + |h(Bi)|== 1llXi=1wi(h) ?
IMiWhere wi(h) is a weight depending on h butalso on i.For both measures, the maximum score is 1.
Itis obtained if and only if Ai = h(Bi) for every i.In 1-1 mapping, when the number of inducedclusters is larger than the number of gold clus-ters, some of the induced clusters are not mapped.To preserve the nature of 1-1 mapping that pun-ishes for excessive clusters2, we define |h(Bi)| tobe equal to |Bi| even for these unmapped clusters.Recall that any induced clustering in which?i, Ai ?
h(Bi) gets the best score under a greedymapping with the accuracy measure.
In MacroIand MicroI the obtained recalls are perfect, but theprecision terms reflect deviation from the correctsolution.2And to allow us to compute it accurately, see below.In the example in Section 3 showing an unrea-sonable behavior of IT-based measures, the scoredepends on r for both MacroI and MicroI.
Withour new measures, recall is always 1, but precisionis rn .
This is true both for 1-1 and M-1 mappings.Hence, the new measures show reasonable behav-ior in this example for all r values.MicroI was used in (Dasgupta and Ng, 2007)with a manually compiled mapping.
Their map-ping was not based on a well-defined scheme buton a heuristic.
Moreover, providing a manualmapping might be impractical when the number ofclusters is large, and can be inaccurate, especiallywhen the clustering is not of very high quality.In the following we discuss how to compute the1-1 and M-1 greedy mappings for each measure.1-1 Mapping.
We compute h by finding themaximal weighted matching in a bipartite graph.In this graph one side represents the induced clus-ters, the other represents the gold classes andthe matchings correspond to 1-1 mappings.
Theproblem can be efficiently solved by the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957).To be able to use this technique, edge weightsmust not depend upon h. In 1-1 mapping,|h(Bi)| = |Bi|, and therefore F (h) = F andwi(h) = wi.
That is, both quantities are inde-pendent of h3.
For MacroI, the weight on the edgebetween the s-th gold class and the j-th inducedcluster is: W (esj) =?li=1 F ?
Is?AiIj?Bi .
ForMicroI it is: W (esj) =?li=1 wi ?
Is?AiIj?Bi .Is?Ai is 1 if s ?
Ai and 0 otherwise.M-1 Mapping.
There are two problems in ap-plying the bipartite graph technique to finding anM-1 mapping.
First, under such mapping wi(h)and F (h) do depend on h. The problem maybe solved by selecting some constant weightingscheme.
However, a more serious problem alsoarises.Consider a case in which an item x has a goldentry {C1} and an induced entry {K1, K2}.
Saythe chosen mapping mapped both K1 and K2 toC1.
By summing over the graph?s edges selectedby the mapping, we add weight (F (h) for MacroIand wi(h) for MicroI) both to the edge betweenK1 and C1 and to the edge between K2 and C1.However, the item?s IMi is only 1.
This prohibits3Consequently, the increase in MacroI and MicroI follow-ing an increase of 1 in an item?s gold/induced intersection size(IMi) is independent of h.81the use of the bipartite graph method for the M-1case.Since we are not aware of any exact method forsolving this problem, we use a hill-climbing al-gorithm.
We start with a random mapping and arandom order on the induced clusters.
Then weiterate over the induced clusters and map each ofthem to the gold class which maximizes the mea-sure given that the rest of the mapping remainsconstant.
We repeat the process until no improve-ment to the measure can be obtained by changingthe assignment of a single induced cluster.
Sincethe score depends on the initial random mappingand random order, we repeat this process severaltimes and choose the maximum between the ob-tained scores.4.2 Cluster-Based EvaluationThe cluster-based evaluation measures we proposeare a direct generalization of existing monose-mous mapping based measures to the polysemoustype case.For a given mapping h : K ?
C, we define h?
:Kh ?
C. Kh is defined to be a clustering whichis obtained by performing set union between everytwo clusters in K that are mapped to the same goldcluster.
The resulting h?
is always 1-1.
We denote|Kh| = mh.Our motivation for using h?
in the definition ofthe measures instead of h is to stay as close aspossible to accuracy, the most common mapping-based measure in the monosemous case.
M-1(monosemous) accuracy does not punish for split-ing classes.
For instance, in a case where there isa gold cluster ci and two induced clusters k1 andk2 such that ci = k1 ?
k2, the M-1 accuracy is thesame as in the case where there is one cluster k1such that ci = k1.
M-1 accuracy, despite its in-difference to splitting, was shown to reflect betterthan 1-1 accuracy the clustering?s applicability forsubsequent applications (at least in some contexts)(Headden III et al, 2008).Recall that in item-based evaluation, IMi mea-sures the intersection between the induced andgold entries of each item.
Therefore, the set unionoperation is not needed for that case, since whenan item appears in two induced clusters that aremapped to the same gold cluster, its IMi is in-creased only by 1.A fundamental quantity for cluster-based eval-uation is the intersection between each inducedcluster and the gold class to which it is mapped.We denote this value by CMj (CM stands for?cluster match?
):CMj = |kj ?
h?
(kj)|The total intersection (CM ) is accordingly de-fined to be:CM = ?mhj=1 CMj =?mhj=1 |kj ?
h?
(kj)|As with the item-based evaluation (Section 4.1),using CM or a derived accuracy as a measure isproblematic.
A clustering that assigns n inducedclasses to each word (n is the number of goldclasses) will get the highest possible score underevery greedy mapping (1-1 or M-1), as will anyclustering in which ?i, Ai ?
h(Bi).As in the item-based evaluation, a possible so-lution is based on defining recall, precision and F-score measures, computed either in the micro or inthe macro level.
The macro cluster-based measureturns out to be identical to the macro item-basedmeasure MacroI4.The following derivation shows the equivalencefor the 1-1 case.
The M-1 case is similar.
We notethat h = h?
in the 1-1 case and we therefore ex-change them in the definition of CM .
It is enoughto show that CM = IM , since the denominator isthe same in both cases:CM = Pmj=1 |kj ?
h(kj)| == Pmj=1Pli=1 Ii?kj Ii?h(kj) == Pli=1Pmj=1 Ii?kj Ii?h(kj) == Pli=1 |Ai ?
h(Bi)| = IMThe micro cluster-based measures are defined:Rj = CMj|h?
(kj)| Pj =CMj|kj | Fj =2RjPjRj+PjThe micro cluster measure MicroC is obtainedby taking a weighted average over the Fj?s:MicroC = ?k?Kh|k|N?
FkWhere N?
= ?z?Kh |z| is the number of clus-tered items after performing the set union andincluding repetitions.
If, in the 1-1 case wherem > n, an induced cluster is not mapped, we de-fine Fk = 0.
A definition of the measure usinga reverse mapping (i.e., from C to K) would haveused a weighted average with weights proportionalto the gold classes?
sizes.4Hence, we have six type level measures: MacroI (whichis equal to MacroC), MicroI, and MicroC, each of which intwo versions, M-1 and 1-1.82The definition of h?
causes a similar computa-tional difficulty as in the M-1 item-based mea-sures.
Consequently, we apply a hill climbingalgorithm similar to the one described in Sec-tion 4.1.The 1-1 mapping is computed using the samebipartite graph method described in Section 4.1.The graph?s vertices correspond to gold and in-duced clusters and an edge?s weight is the F-scorebetween the class and cluster corresponding to itsvertices times the cluster?s weight (|k|/N?
).5 Evaluation of POS Induction ModelsAs a detailed case study for the ideas presentedin this paper, we apply the various measures forthe POS induction task, using seven leading POSinduction algorithms.5.1 Experimental SetupPOS Induction Algorithms.
We experimentedwith the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwa-ter and Griffiths, 2007), GJ08 (Gao and Johnson,2008), and GVG09 (Van Gael et al, 2009) (threemodels).
Additional recent good results for vari-ous variants of the POS induction problem are de-scribed in e.g., (Smith and Eisner, 2004; Grac?a etal., 2009).Clark03 and ARR10 are monosemous algo-rithms, allowing a single cluster for each wordtype.
The other algorithms are polysemous.
Theyperform sequence labeling where each token istagged in its context, and different tokens (in-stances) of the same type (word form) may receivedifferent tags.Data Set.
All models were tested on sections2-21 of the PTB-WSJ, which consists of 39832sentences, 950028 tokens and 39546 unique types.Of the tokens, 832629 (87.6%) are not punctuationmarks.Evaluation Measures.
Type level evaluationused the measures MacroI (which is equal toMacroC), MicroI and MicroC both with greedy1-1 and M-1 mappings as described in Section 4.The type level gold (induced) entry is defined tobe the set of all gold (induced) clusters with whichit appears.For the token level evaluation, six measures areused (see Section 3): accuracy with M-1 and 1-1mappings, NVI, V, H(C|K) and H(K|C), using eas the logarithm?s base.
We use the full WSJ POStags set excluding punctuation5.Punctuation.
Punctuation marks occupy alarge volume of the corpus tokens (12.4% in ourexperimental corpus), and are easy to cluster.Clustering punctuation marks thus greatly inflatestoken level results.
To study the relationship be-tween type and token level evaluations in a fo-cused manner, we excluded punctuation from theevaluation (they are still used during training, soalgorithms that rely on them are not harmed).Number of Induced Clusters.
The numberof gold POS tags in WSJ is 45, of which 11 arepunctuation marks.
Therefore, for the ARR10 andClark03 models, 34 clusters were induced.
ForGJ08 we received the output with 45 clusters.
TheiHMM models of GVG09 determine the numberof clusters automatically (resulting in 47, 91 and192 clusters, see below).
For GG07, our com-puting resources did not enable us to induce 45clusters and we therefore used 176.
Our focus inthis paper is to study the type vs. token distinctionrather than to provide a full scope comparison be-tween algorithms, for which more clustering sizeswould need to be examined.Configurations.
We ran the ARR10 taggerwith the configuration detailed in (Abend et al,2010).
For Clark03, we ran his neyessenmorphmodel7 10 times (using an unknown words thresh-old of 5) and report the average score for eachmeasure.
The models of GVG09 were run in thethree configurations reported in their paper: onewith a Dirichlet process prior and fixed parame-ters, another with a Pittman-Yore prior with fixedparameters, and a third with a Dirichlet processprior with parameters learnt from the data.
All fivemodels were run in an optimal configuration.We obtained the code of Goldwater and Grif-fiths?
BHMM model and ran it for 10K iterationswith an annealing technique for parameter estima-tion.
That was the best parameter estimation tech-nique available to us.
This is the first time that thismodel is evaluated on such a large experimentalcorpus, and it performed well under these condi-tions.The output of the model of GJ08 was sent tous by the authors.
The model was run on sec-5We use all WSJ tokens in the training stage, but omitpunctuation marks during evaluation.6The 17 most frequent tags cover 94% of the word in-stances and more than 99% of the word types in the WSJgold standard tagging.7www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html83tions 2-21 of the WSJ-PTB using significantlyinferior computing resources compared to thoseused for producing the results reported in theirpaper.
While this model cannot be compared tothe aforementioned six models due to the subopti-mal configuration, we evaluate its output using ourmeasures to get a broader variety of experimentalresults8.5.2 Results and DiscussionTable 1 presents the scores of the compared mod-els under all evaluation measures (six token level,six type level).
What is important here to noteare the differences between type and token levelevaluations for the algorithms.
We are mainlyinterested in two things: (1) seeing how relativerankings change in the two evaluation types, thusshowing that the two types are not highly corre-lated and are both useful; and (2) insights gainedby using a type level evaluation in addition to theusual token level one.Note that the table should not be used to deducewhich algorithm is the ?best?
for the task, even ac-cording to a single evaluation type.
This is be-cause, as explained above, the algorithms do notinduce the same number of clusters and this affectstheir results.Results indicate that type level evaluation re-veals aspects of the clustering quality that are notexpressed in the token level.
For the Clark03model the disparity is most apparent.
While inthe token level it performs very well (better thanthe polysemous algorithms for the 1-1, V and NVItoken level measures), in the type level it is thesecond worst in the item-based 1-1 scores and theworst in the M-1 scores.Here we have a clear demonstration of the valueof type level evaluation.
The Clark03 algorithmis assessed as excellent using token level evalua-tion (second only to ARR10 in M-1, 1-1, V andNVI), and only a type level one shows its rela-tively poor type performance.
Although readersmay think that this is natural due to the algorithm?smonosemous nature, this is not the case, since themonosemous ARR10 generally ranked first in thetype level measures (more on this below).The disparity is also observed for polysemousalgorithms.
The GG07 model?s token level scoresare mediocre, while in the type level MicroC 1-18We would like to thank all authors for sending us thedata.measure this model is the best and in the type levelMicroI and MacroI 1-1 measures it is the secondbest.Monosemous vs. polysemous algorithms.
Thetable shows that the ARR10 model achieves thebest results in most type and token level evalua-tion measures.
The fact that this monosemous al-gorithm outperforms the polysemous ones, evenin a type level evaluation, may seem strange atfirst sight but can be explained as follows.
Pol-ysemous tokens account for almost 60% of thecorpus (565K out of 950K), so we could expectthat a monosemous algorithm should do badly ina token-level evaluation.
However, for most of thepolysemous tokens the polysemy is only weaklypresent in the corpus9, so it is hard to detect evenfor polysemous algorithms.
Regarding types, pol-ysemous types constitute only 16.6% of the cor-pus types, so a monosemous algorithm which isquite good in assigning types to clusters has a goodchance of beating polysemous algorithms in a typelevel evaluation.Hence, monosemous POS induction algorithmsare not at such a great disadvantage relative to pol-ysemous ones.
This observation, which was fullymotivated by our type level case study, might beused to guide future work on POS induction, andit thus serves as another demonstration for the util-ity of type level evaluation.Hill climbing algorithm.
For the type levelmeasures with greedy M-1 mapping, we used thehill-climbing algorithm described in Section 4.Recall that the mapping to which our algorithmconverges depends on its random initialization.We therefore ran the algorithm with 10 differ-ent random initializations and report the obtainedmaximum for MacroI, MicroI and MicroC in Ta-ble 1.
The different initializations caused very lit-tle fluctuation: not more than 1% in the 9 (7) bestruns for the item-based (MicroC) measures.
Wetake this result as an indication that the obtainedmaximum is a good approximation of the globalmaximum.We tried to improve the algorithm by selectingan intelligent initialization heuristic.
We used theM-1 mapping obtained by mapping each inducedcluster to the gold class with which it has the high-9Only about 27% of the tokens are instances of words thatare polysemous but not weakly polysemous (we call a wordweakly polysemous if more than 95% of its instances (tokens)are tagged by the same tag).84Token Level Evaluation Type Level EvaluationMacroI MicroI MicroCM-1 1-1 NVI V H(C|K) H(K|C) M-1 1-1 M-1 1-1 M-1 1-1ARR10 0.675 0.588 0.809 0.608 1.041 1.22 0.579 0.444 0.596 0.455 0.624 0.403Clark03 0.65 0.484 0.887 0.586 1.04 1.441 0.396 0.301 0.384 0.288 0.463 0.347GG07 0.5 0.415 0.989 0.479 1.523 1.241 0.497 0.405 0.461 0.398 0.563 0.445GVG09(1) 0.51 0.444 1.033 0.477 1.471 1.409 0.513 0.354 0.436 0.352 0.486 0.33GVG09(2) 0.591 0.484 0.998 0.529 1.221 1.564 0.637 0.369 0.52 0.373 0.548 0.32GVG09(3) 0.668 0.368 1.132 0.534 0.978 2.18 0.736 0.280 0.558 0.276 0.565 0.199GJ08* 0.605 0.383 1.09 0.506 1.231 1.818 0.467 0.298 0.446 0.311 0.561 0.291Table 1: Token level (left columns) and type level (right columns) results for seven POS inductionalgorithms (rows) (see text for details).
Token and type level performance are weakly correlated andcomplement each other as evaluation measures.
ARR10, a monosemous algorithm, yields the best resultsin most measures.
(GJ08* results are different from those reported in the original paper because it wasrun with weaker computing resources than those used there.
)est weight edge in the bipartite graph.
Recall fromSection 4.1 that this is a reasonable approximationof the greedy M-1 mapping.
Again, we ran it forthe three type level measures for 10 times with arandom update order on the induced clusters.
Thishad only a minor effect on the final scores.Number of clusters.
Previous work (Reichartand Rappoport, 2009) demonstrated that in datasets where a relatively small fraction of the goldclasses covers most of the items, it is reasonableto choose this number to be the number of inducedclusters.
In our experimental data set, this number(the ?prominent cluster number?)
is around 17 (seeSection 5.1).
Up to this number, increasing thenumber of clusters is likely to have a positive ef-fect on token level M-1, 1-1, H(C|K), and H(K|C)scores.
Inducing a larger number of clusters, how-ever, is likely to positively affect M-1 and H(C|K)but to have a negative effect on 1-1 and H(K|C).This tendency is reflected in Table 1.
For theGG07 model the number of induced clusters, 17,approximates the number of prominent clustersand is lower than the number of induced clus-ters of the other models.
This is reflected byits low token level M-1 and H(C|K) performanceand its high quality H(K|C) and NVI token levelscores.
The GVG (1)-(3) models induced 47, 91and 192 clusters respectively.
This might explainthe high token level M-1 and H(C|K) performanceof GVG(3), as well as its high M-1 type levelperformance, compared to its mediocre scores inother measures.The item based measures.
The table indicatesthat there is no substantial difference between thetwo item based type level scores with 1-1 map-ping.
The definitions of MacroI and MicroI implythat if |Ai|+ |h(Bi)| (which equals |Ai|+ |Bi| un-der a 1-1 mapping) is constant for all word types,then a clustering will score equally on both 1-1type measures.
Indeed, in our experimental cor-pus 83.4% of the word types have one POS tag,12.5% have 2, 3.1% have 3 and only 1% of thewords have more.
Therefore, |Ai| is roughly con-stant.
The ARR10 and Clark03 models assign aword type to a single cluster.
For the other models,the number of clusters per word type is generallysimilar to that of the gold standard.
Consequently,|Bi| is roughly constant as well, which explainsthe similar behavior of the two measures.Note that for other clustering tasks |Ai| may notnecessarily be constant, so the MacroI and MicroIscores are not likely to be as similar under the 1-1mapping.6 SummaryWe discussed type level evaluation for polysemousclustering, presented new mapping-based evalu-ation measures, and applied them to the evalua-tion of POS induction algorithms, demonstratingthat type level measures provide value beyond thecommon token level ones.We hope that type level evaluation in generaland the proposed measures in particular will beused in the future for evaluating clustering perfor-mance in NLP tasks.ReferencesOmri Abend, Roi Reichart and Ari Rappoport, 2010.Improved Unsupervised POS Induction through Pro-totype Discovery.
ACL ?10.Thorsten Brants, 1997.
The NEGRA Export Format.CLAUS Report, Saarland University.85Alexander Clark, 2003.
Combining Distributional andMorphological Information for Part of Speech In-duction.
EACL ?03.Sajib Dasgupta and Vincent Ng, 2007.
Unsu-pervised Part-of-Speech Acquisition for Resource-Scarce Languages.
EMNLP-CoNLL ?07.Dmitry Davidov, Ari Rappoport, 2006.
EfficientUnsupervised Discovery of Word Categories us-ing Symmetric Patterns and High Frequency Words.COLING-ACL ?06.Dmitry Davidov, Ari Rappoport.
2008.
UnsupervisedDiscovery of Generic Relationships Using PatternClusters and its Evaluation by Automatically Gen-erated SAT Analogy Questions.
ACL ?08I.
S. Dhillon, S. Mallela, and D. S. Modha, 2003.
In-formation Theoretic Co-clustering.
KDD ?03Micha Elsner, Eugene Charniak, and Mark Johnson,2009.
Structured Generative Models for Unsuper-vised Named-Entity Clustering.
NAACL ?09.Stella Frank, Sharon Goldwater, and Frank Keller,2009.
Evaluating Models of Syntactic CategoryAcquisition without Using a Gold Standard.
Proc.31st Annual Conf.
of the Cognitive Science Society,2576?2581.E.B Fowlkes and C.L.
Mallows, 1983.
A Method forComparing Two Hierarchical Clusterings.
Journalof American statistical Association,78:553-569.Benjamin C. M. Fung, Ke Wang, and Martin Ester,2003.
Hierarchical Document Clustering using Fre-quent Itemsets.
SIAM International Conference onData Mining ?03.Jianfeng Gao and Mark Johnson, 2008.
A Compar-ison of Bayesian Estimators for Unsupervised Hid-den Markov Model POS Taggers.
EMNLP ?08.Sharon Goldwater and Tom Griffiths, 2007.
FullyBayesian Approach to Unsupervised Part-of-SpeechTagging.
ACL ?07.Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-nando Pereira, 2009.
Posterior vs. Parameter Spar-sity in Latent Variable Models.
NIPS ?09.William P. Headden III, David McClosky and EugeneCharniak, 2008.
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction.
COLING?08.L.
Hubert and J. Schultz, 1976.
Quadratic Assignmentas a General Data Analysis Strategy.
British Journalof Mathematical and Statistical Psychology, 29:190-241.L.
Hubert and P. Arabie, 1985.
Comparing Partitions.Journal of Classification, 2:193-218.Maurice Kandall and Jean Dickinson, 1990.
RankCorrelation Methods.
Oxford University Press, NewYork.Karin Kipper, Hoa Trang Dang and Martha Palmer,2000.
Class-Based Construction of a Verb Lexicon.AAAI ?00.Harold W. Kuhn, 1955.
The Hungarian Method forthe Assignment Problem.
Naval Research LogisticsQuarterly, 2:83-97.Bjornar Larsen and Chinatsu Aone, 1999.
Fast and ef-fective text mining using linear-time document clus-tering.
KDD ?99.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313-330.Marina Meila and David Heckerman, 2001.
An Ex-perimental Comparison of Model-based ClusteringMethods.
Machine Learning, 42(1/2):9-29.Marina Meila, 2007.
Comparing Clustering ?
an In-formation Based Distance.
Journal of MultivariateAnalysis, 98:873-895.C.W Milligan, S.C Soon and L.M Sokol, 1983.
TheEffect of Cluster Size, Dimensionality and the Num-ber of Clusters on Recovery of True Cluster Struc-ture.
IEEE transactions on Pattern Analysis andMachine Intelligence, 5:40-47.Boris G. Mirkin, 1996.
Mathematical Classificationand Clustering.
Kluwer Academic Press.Michael Mitzenmacher , 2004.
A Brief History ofGenerative Models for Power Law and LognormalDistributions .
Internet Mathematics, 1(2):226-251.Soto Montalvo, Raquel Martnez, Arantza Casillas, andVctor Fresno, 2006.
Multilingual Document Clus-tering: an Heuristic Approach Based on CognateNamed Entities.
ACL ?06.James Munkres, 1957.
Algorithms for the Assignmentand Transportation Problems.
Journal of the SIAM,5(1):32-38.Cristina Nicolae and Gabriel Nicolae, 2006.
BEST-CUT: A Graph Algorithm for Coreference Resolu-tion.
EMNLP ?06.Darius M. Pfitzner, Richard E. Leibbrandt and DavidM.W Powers, 2008.
Characterization and Evalua-tion of Similarity Measures for Pairs of Clusterings.Knowledge and Information Systems: An Interna-tional Journal, DOI 10.1007/s10115-008-0150-6.William Rand, 1971.
Objective Criteria for the Evalu-ation of Clustering Methods.
Journal of the Ameri-can Statstical Association, 66(336):846-850.86Roi Reichart and Ari Rappoport, 2008.
UnsupervisedInduction of Labeled Parse Trees by Clustering withSyntactic Features.
COLING ?08.Roi Reichart and Ari Rappoport, 2009.
The NVI Clus-tering Evaluation Measure.
CoNLL ?09.Andrew Rosenberg and Julia Hirschberg, 2007.
V-Measure: A Conditional Entropy-based ExternalCluster Evaluation Measure.
EMNLP ?07.Sabine Schulte im Walde, 2006.
Experiments onthe Automatic Induction of German Semantic VerbClasses.
Computational Linguistics, 32(2):159-194.Noah A. Smith and Jason Eisner, 2004.
AnnealingTechniques for Unsupervised Statistical LanguageLearning.
ACL ?04.Stijn Van Dongen, 2000.
Performance Criteria forGraph Clustering and Markov Cluster Experiments.Technical report CWI, AmsterdamJurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-mani, 2009.
The Infinite HMM for UnsupervisedPOS Tagging.
EMNLP ?09.Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, andGuang R. Gao, 2002.
An Adaptive Meta-clusteringApproach: Combining the Information from Differ-ent Clustering Results .
CSB 00:27687
