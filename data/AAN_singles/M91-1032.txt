UNISYS :DESCRIPTION OF THE UNISYS SYSTEM USED FOR MUC- 3Carl Weir, Tim Finin, Robin McEntire, and Barry Sil kUnisys Center for Advanced Information Technolog yPaoli, Pennsylvani aweir@prc .unisys.com215-648-236 9INTRODUCTIO NThis paper describes the Unisys MUC-3 text understanding system, a system based upon a three -tiered approach to text processing in which a powerful knowledge-based form of information retrievalplays a central role .
This knowledge-based form of information retrieval makes it possible to define a neffective level of text analysis that falls somewhere between what is possible with standard keyword-base dinformation retrieval techniques and deep linguistic analysis .The Unisys Center for Advanced Information Technology (CAIT) has a long-standing commitment toNLP research and development, with the Pundit NLP system developed at CAIT serving as the Center' sprimary research vehicle [3] .
The Unisys MUC-3 system, however, consists primarily of components thatare less than 7 months old and still in a developmental stage .
Although the three-tiered processing ap-proach that the MUC-3 system's architecture is based upon includes Pundit as its third level of (linguistic )analysis, the incorporation of Pundit into the MUC-3 system was not achieved in time for the final MUC- 3test in May, 1991 .
A decision was made to focus on the development of a knowledge-based informatio nretrieval component, and this precluded the integration of Pundit into the prototype) The Unisys MUC- 3system without its linguistic analysis component is depicted in Figure 1 .
This is the version of the syste mthat was actually used in the MUC-3 test .APPROACH AND SYSTEM DESCRIPTIO NThe Unisys MUC-3 system's architecture consists of five main processing components, three of whic hrepresent levels of text understanding .
An initial preprocessing component transforms texts into a nappropriate format for the text understanding components to manipulate .
The three text understandin gcomponents engage in (1) standard keyword-based information retrieval, (2) knowledge-based informatio nretrieval, and (3) linguistic analysis .
2 A final, template generation component gathers together all th efacts extracted from a given text and builds template data structures out of them .
These five componentsare described in more detail below .A Message Pre-processing ComponentThe Unisys MUC-3 system's message pre-processing component is a special, low-level processor whichparses texts into their component parts and generates output in a form compatible with the KBIR Drule processing system (i .e ., as a set of Prolog terms) .
This processor is a special C program which wasgenerated using an Application Specific Language called MFPL (Message Format Processing Language )[6] .
MFPL was specifically designed as a high-level language for processing the formatted portions o felectronic messages .
In addition to producing a representation of the text in Prolog terms, this modul eidentifies and encodes sentence boundaries, paragraph boundaries, and the standard formatted portion sof the text (e .g ., date, time, location, etc .)
.1 1f funding is available, we plan to add Pundit to the system in time for the MUC-4 conference (June, 1992) .2 As pointed out in the introduction, the Pundit NLP system could not be integrated into the MUC-3 system in time fo rthe final MUC-3 test run.2120 .
n5O ID000 01.
T101PZan1 ID 12.
DATE07.101091 .
TIM5011.3110KNOWLEDGE-BASED I R(KB?RD) <Even tLocatorRulesMessag ePreprocessin(MFPL);>;>0T1-00C1-00990000ll, 7 .10109 (1P1) -- (TUT]0L01AL11( 0PPICIO10 is on TSATu1 .105 ?120700D 10 51 n 1Y Yo STI0NIL LIMATIO11 (110) 2510 Vs AG15 0111101ITF ma CAF L31.11-01 S 0IL PIP0LI110, COLauuA?0 Oh.IL PIPGI55 .Figure 1: This diagram depicts the version of the Unisys MUC-3 system used in the final MUC-3 test run .The third level of text-understanding in the three-tiered approach to text-processing described in this pape r(linguistic analysis provided by the Pundit NLP system) was not incorporated in time for the test, and istherefore not represented in the diagram .A Keyword-Based Information Retrieval ComponentThe keyword analysis component of the Unisys MUC-3 system predicts when various types of terroristacts (bombings, murders, kidnappings, and so forth) have been referred to in a text .
The probability ofan act of a given type having occurred is determined by a search for words, word stems, and pairs ofwords and pairs of word stems, that are associated with types of acts .'
The probability of such a word (o rword stem, or word pair or stem pair) occurring in a text for which an act of a given type is associated i sdetermined as follows .The frequency of presence for a given word W (or word stem .
.
.)
in texts for which a terrorist ac tof a given type T occurs is computed (f (W, T)), as is the presence of the word in any text at all in thecomplete corpus (f(W,C)) .
The probability of the word appearing in texts for which a terrorist act of agiven type occursf(W,T)f (T,C)and the probability of the word occurring in any textf(W,C)IC Iare calculated, and these two values are used to determine the conditional probability of the word (o rword stem .
.
.)
predicting the given type of terrorist act .P(W, T3 The keyword analysis system uses a rule-driven word-stemmer based on one developed by Chris Paice (Landcaster, UK)213Only words with relatively high probabilities of predicting a given type of terrorist act are searched for in atext, and words that do not occur frequently enough in the text corpus based on some empirically-derive dthreshold are not used .Training the keyword-based analysis component .
A database of key words, two-word phrases ,word stems and two-stem phrases was compiled from the DEV corpus using a collection of GAWK scripts .After some experimentation, we decided not to use the word stem and stem-pair data in the final test ,because it was not making any positive difference in the system's event detection performance .
Currently,an event class, T, is predicted for a text if it contains any single word (or two-word phrase), W, whereP(W,T) > .65 or if it contains two words (or two two-word phrases) W1 and W2 where P(W1i T) > .55and P(W2 , T) > .55 .
Further experimental variation of the scoring algorithm should result in continue denhancements to this component's event detection capabilities .A Knowledge-based Information Retrieval Component (KBIRD )Once a set of terrorist acts have been predicted, the task of generating templates describing those act sfalls to the knowledge-based information retrieval component called KBIRD .KBIRD is a rule-based system for concept-spotting in free text [2, 7] .
KBIRD rules are forward-chainin ghorn clauses whose antecedents are constituents discovered and recorded in a chart data structure andwhose consequents are newly inferred constituents?concepts (or facts)?to be added to the chart .
Theantecedents and consequents of KBIRD rules can include arbitrary Prolog goals just as in Definite Claus eGrammars [5] .It is tempting to think of a set of KBIRD rules as implementing a kind of bottom up chart parser, butthere are several interesting differences .
One distinctive feature is that the concepts that KBIRD rule sinfer are associated with a specific region of text, a region which is the maximal cumulative span of th eregions of text associated with each expression in a given rule's antecedent .
Moreover, these regions ca nbe explicitly reasoned about by subsequent KBIRD rules .In typical natural language parsers, there is an implicit constraint that adjacent constituents in a rul emust be realized by contiguous strings of text in the input .
KBIRD allows one to write rules which specifyother constraints on the relative positions of the strings which realize rule constituents .
The anteceden tof a KBIRD rule may consist of several facts (words or concepts) that are the arguments of operator sillustrated below.
New operators are easy to define .Antecedent Format Operator Descriptio nA - BA is contiguous with B.A , BA is in the same text as B .A .. BA is in the same sentence as B .A .
.
> BA is in the same sentence as and precedes B.A .
.
.
BA is in the same paragraph as B .A .
.+ BA is in the same region as B .KBIRD rules are compiled into a combination of Prolog backward chaining rules and forward chainin grules in Pfc [1] .
A simple optimizer is applied to the output of this compilation process to improv eperformance .
KBIRD has many additional features which are inherited from the Pfc rule language, suchas the ability to write non-monotonic rules which specify that no occurrence of a certain constituent o rconcept be found in a given region .214Some examples of KBIRD rules are shown below .
The first rule states that if the wordstem "MURDER*"has been found in the text, then a fact should be added to the factbase stating that a potential murde revent has been found.
The second rule illustrates KBIRD's ability to recognize phrases, asserting that ifthe string "ARMY OF NATIONAL LIBERATION" is discovered, a fact should be added to the factbase statin gthat a terrorist organization exists in the text at the same location as the string .
The third rule illustratesthe use of operations on concepts derived from the text, asserting that if a terrorist event E is found inthe same sentence as a potential victim V, then a fact should be added to the factbase indicating that Vis the actual victim of E.1.
"MURDER*" __> potential_murder_event .2.
"ARMY"""OF""NATIONAL""LIBERATION" ==> terrorist_organization .3. terrorist_event(E) .
.
potential_victim(V) __> victim(E,V) .Several additional features of the KBIRD rule language should be mentioned, all of which appear in th efollowing, more complex rule used to infer individual perpetrators :generic_perpetrator(A)OP ,["unlikely_perpetrator(Name)] ,{get_4u11_text_at_loc(P,Name) }_=> potential_ind_perpetrator(A, Name).In the first clause of the antecedent of this rule, the text location index associated with the concep tgeneric..perpetrator(A) is bound to the logic variable P with the ?
operator.
This allows the locatio nto be explicitly constrainted later in the rule .
If a clause is enclosed in square brackets, as is the casefor the second clause of the antecedent, then its location is ignored .
This condition also shows the use ofthe tilde (") as a negation operator .
Thus, this second clause specifies that it is not the case that Nam ehas been determined to be an "unlikely perpetrator" anywhere else in the text .
The final clause of th eantecedent in this rule is enclosed in curly brackets, which indicates that it is a Prolog constraint whichmust be met?this clause is used to extract the actual text associated with the concept bound to the logi cvariable P .A Template GeneratorThe Template Generator has three tasks : to select the actual templates to be produced as output, t ochoose between candidate slot fillers if more than one has been found, and to print the template in th eproper format .Template Selection.
The process of determining which template structures to build out of the fact sinferred by KBIRD begins by determining if any events at all have been predicted .
If no event has beenpredicted, then an "irrelevant template" is created .
If several events of the same type have been created ,the template generator will attempt to merge them using a set of heuristics which hypothesize that twoevent descriptions refer to the same event .
Some of the general heuristics used for merging events of thesame class are :?
Merge two events if there is a significant overlap in the text regions found by the event locator rules.?
Merge two events if they share human targets whose scores are above a certain threshold .?
Merge two events if they share physical targets whose scores are above a certain threshold .215Slot Filler Selection.
After merging events, the template generator must select the final slot fille rvalues .
The KBIRD rules which propose slot fillers attach a score (an integer between 0 and 100) to eac hcandidate which represents the system's confidence in that value .
If multiple candidate fillers exist for agiven template, several general heuristics are used to select among them :?
Candidate slot values with scores below a given threshold are dropped from consideration .?
A set of synonymous expressions are dropped in favor of their canonical expression .?
If one candidate expression is a substring of another, then the shorter one is dropped .?
A generic description (e .g ., vehicles) is dropped in favor of one or more subsumed ones (e .g ., ambu-lance, truck) .?
If a slot can only take a single value then the candidate receiving the highest value is selected .A Linguistic Analysis Component (Pundit )The Pundit natural language processing system has been under development at Unisys for the last fiv eyears and is capable of performing a detailed linguistic analysis of an input text .
Unlike KBIRD, Punditabstracts away from the actual strings used to convey information in a text at the very beginning of it sanalysis process by determining to which syntactic properties and domain concepts the lexical items i nthe text correspond.
These syntactic properties and domain concepts are then processed without muc hattention being paid to their physical location in the text .
In KBIRD, on the other hand, everything thatis manipulated, even concepts that have been asserted, are explicitly associated with regions of text .A key capability that the deeper linguistic processing of Pundit can provide is the determination o fthe grammatical and thematic roles of expressions in a text .
Thus, it can determine that in the sentence"Castellar is the second mayor that has been murdered in Colombia in the last 3 days" that Castellar is thesubject of the copular verb in the matrix clause, and that Castellar should inherit properties asserted ofthe predicate nominal argument .
It can also recognize the passive voice of the relative/subordinate claus eheaded by that and thus that it is Castellar that has been murdered (as the second mayor) in Columbia.It would be possible to build a KBIRD rulebase that performs the sort of detailed linguistic analysis no wbeing performed by Pundit .
Merging KBIRD and Pundit in this way would minimize the complication sof integrating the text analyses that they perform .
However, such a merger would very likely reduce th emodularity of the three-tiered approach to text processing that we have been following .AN EXTENDED EXAMPLEIn this section, we illustrate in a more concrete fashion how the Unisys MUC-3 system goes abou tprocessing messages by examining in more detail what happens during the processing of a specific text ,message TST1-MUC3-0099 in the MUC-3 corpus (see Figure 2) .
Our discussion will proceed through th evarious processing phases that have been identified .Phase One : Message Pre-processingIn this phase, the message is parsed (by a special low-level processor) into its components and outpu tin a form compatible with the KBIRD rule processing system.
This processor is a special C progra mgenerated by MFPL, the ASL mentioned earlier in this paper .
This phase produces text input of thefollowing sort to the Prolog portion of the system, including default (header) information about the dat eand location .216TST1-MUC3-0099LIMA, 25 OCT 89 (EFE) - [TEXT] POLICE HAVE REPORTED THAT TERRORISTS TONIGH TBOMBED THE EMBASSIES OF THE PAC AND THE SOVIET UNION .
THE BOMBS CAUSE DDAMAGE BUT NO INJURIES.A CAR-BOMB EXPLODED IN FRONT OF THE PRC EMBASSY, WHICH IS?IN THE LIMA RES-IDENTIAL DISTRICT OF SAN ISIDRO .
MEANWHILE, TWO BOMBS WERE THROWN AT AUSSR EMBASSY VEHICLE THAT WAS PARKED IN FRONT OF THE EMBASSY LOCATED I NORRANTIA DISTRICT, NEAR SAN ISIDRO .POLICE SAID THE ATTACKS WERE CARRIED OUT ALMOST SIMULTANEOUSLY AN DTHAT THE BOMBS BROKE WINDOWS AND DESTROYED THE TWO VEHICLES .NO ONE HAS CLAIMED RESPONSIBILITY FOR THE ATTACKS SO FAR .
POLICE SOURCES ,HOWEVER, HAVE SAID THE ATTACKS COULD HAVE BEEN CARRIED OUT BY TH EMAOIST "SHINING PATH" GROUP OR THE GUEVARIST "TUPAC AMARU REVOLUTION-ARY MOVEMENT" (MRTA) GROUP .
THE SOURCES ALSO SAID THAT THE SHINING PATHHAS ATTACKED SOVIET INTERESTS IN PERU IN THE PAST .IN JULY 1989 THE SHINING PATH BOMBED A BUS CARRYING NEARLY 50 SOVIE TMARINES INTO THE PORT OF EL CALLAO .
FIFTEEN SOVIET MARINES WERE WOUNDED .SOME 3 YEARS AGO TWO MARINES DIED FOLLOWING A SHINING PATH BOMBING OF AMARKET USED BY SOVIET MARINES .IN ANOTHER INCIDENT 3 YEARS AGO, A SHINING PATH MILITANT WAS KILLED B YSOVIET EMBASSY GUARDS INSIDE THE EMBASSY COMPOUND .
THE TERRORIST WASCARRYING DYNAMITE .THE ATTACKS TODAY COME AFTER SHINING PATH ATTACKS DURING WHICH LEAS T10 BUSES WERE BURNED THROUGHOUT LIMA ON 24 OCT .Figure 2 : Message TST1-MUC3-0099 .msg(id,"TST1-MUC3-0099") .msg(loc,"LIMA") .msg(date,[2&,"OCT",89]) .msg(src,"EFE") .msg(type,'TEXT') .msg(text,['POLICE,'HAVE','REPORTED','THAT','TERRORISTS','TONIGHT' ,Phase Two: Keyword analysi sIn the second phase, the keyword analysis component predicts three event classes?bombings with aprobability of 87%, attacks with a probability of 66%, and murders with a probability of 63% .
Figure 3shows the particular words and word pairs which gave rise to these predicted event types .
The last colum nin this table contains triples consisting of a probability, a word or two-word phrase, and its location i nthe text .
Given our current thresholds, the murder prediction was judged to be too weak for furthe rconsideration .Phase Three : KBIRD processingKBIRD examines the text word by word and applies forward chaining rules whenever their pre-conditionsare met .
KBIRD's task is to take the event classes predicted by the keyword analysis stage and try t opredict additional event classes as well as instantiate the predicted types with individual events .
Eventinstances are associated with particular regions within the text .
When an event instance is created ,additional rules will be triggered to look for values to fill each of the instance's slots .217Sent J Type Prob ~ Keys2 bombing 55 [55,DAMAGE,20 :21 ]2 bombing 71 [57,THE,BOMBS,17:19] [71,DAMAGE,BUT,20 :22]3 bombing 79 [79,EXPLODED,27 :29 ]3 bombing 82 [82,EXPLODED,IN,27 :29]4 bombing 77 [54,THROWN,51 :52] [77,PARKED,59 :60]4 bombing 84 [84,TWO,BOMBS,48 :50]5 bombing 80 [80,WINDOWS,88 :89]5 bombing 71 [57,THE,BOMBS,85:87] [71,WINDOWS,AND,88 :90 ]5 attack 66 [66,ATTACKS,WERE,77 :99]8 bombing 87 [56,BUS,167 :168] [87,CALLAO,178 :179]8 bombing 75 [63,A,BUS,166:168] [75,PORT,OF,175 :177]10 murder 63 [63,KILLED,BY,217 :219 ]11 bombing 69 [69,DYNAMITE,231 :232]Figure 3: Keyword analysis predicts the likely occurrence of bombings and attacks in this message .Predicting Additional Event Types.
In some cases the co-occurrence of an instance of some even ttype predicted by the keyword-based analysis component with words or inferred concepts that have bee ndetected in a text will allow KBIRD to infer additional event types .
For example, the following KBIR Drule, which was triggered in the processing of message 0099, asserts that the occurrence of "BURNED" inthe active voice in a message for which an instance of a bombing event has been discovered is enough t opredict the likely occurrence of an arson event .
"BURNED" < .
be_word(W) ,actual_event('BOMBING',_,_) ,["predicted_event_type('ARSON') ]__> probable_event('ARSON') .Locating Events .
The process of instantiating event types, or locating events, is initiated in KBIR Dthrough a class of locator rules, which attempt to find "hot spots" in the text which seem to be discussin gevents of the predicted type .
The following locator rules were used to detect bombing, attack, and arso ninstances in this message :[probable_event('BOMBING')] ,"BOMBED"CI < .
"be_word(W) ,"BOMBED"II .
.> [potential_physical_target(_,_,_)] ,{gen_event_id(ID) }__> bombing (ID,'BOMBED') ,syntax(ID,active) .Paraphrase : If the occurrence of a bombing event is likely and the word "bombed" occurs in the activevoice (no preceding "be" word) with a potential physical target to its right in the same sentence, theninfer an instance of a bombing event .
[probable_event('BOMBING')] ,["THE"""ATTACK*"]"be_word(W)""CARRIED"""OUT" .
.
[bomb_device] ,{gen._ event _id (ID) }__> bombing(ID,'BE CARRIED OUT WITH BOMB') ,syntax(ID,active) .Paraphrase : If the occurrence of a bombing event is likely and the phrase "The attack was carriedout" occurs (or a variant with some other "be" word), and in the same sentence somewhere a bom bdevice is mentioned, then infer an instance of a bombing event .21 8[probable_ event ('ATTACK ')] ,["THE"""ATTACKS"]"be_word(W)""CARRIED"""OUT" .
.
["bomb_device] ,(gen_ event _id (ID )__> attack(ID,'BE CARRIED OUT') ,syntax(ID,active) .Paraphrase : If the occurrence of an attack is likely and the phrase "The attack was carried out "occurs (or a variant with some other "be" word), and no mention is made of a bomb device in thesame sentence, then infer an instance of an attack event .
[probable_event('ARSON')] ,"BURNED" < .
be_word(W) < .
.
[potential_physicaltarget{gen_eventid(ID)}arson(ID,'BE BURNED') ,syntax(ID,passive) .Paraphrase : If the occurrence of an arson event is likely and the word "burned" occurs in the passivevoice (with a "be" word to its left) with a mention of a potential physical target somewhere to theleft in the same sentence, then infer an instance of an arson event .Although the rule above for detecting an instance of an attack event will initially fire as the words in themessage are examined sequentially by KBIRD and the phrase "THE ATTACK WAS CARRIED OUT "is encountered, the attack event instance that has been created will eventually be retracted when, in th esame sentence, the description of a bomb device is encountered ("THE BOMBS") .
On the other hand, thesecond rule for inferring instances of bombing events will suddenly have all of its antecedent constraint smet when this latter phrase is encountered, and so it will fire to create a new instance of a bombing .Locating perpetrator ids and orgs .
The following two rules are triggered when, in the first sentenceof 0099, the word "TERRORISTS" is encountered .
The latter rule licenses the inference that "TERROR-ISTS" describes a potential perpetrator .
"TERRORISTS" __> generic_perpetrator('TERRORIST') .generic_perpetrator(A)IP ,["unlikely_perpetrator(Name)] ,{get_full_text_at_loc(P,Name) }__> potential_ind_perpetrator(A, Name).Later, in the fourth paragraph of the text, the following rules are used to infer that the known guerrill aorganizations "SHINING PATH" and "TUPAC AMARU REVOLUTIONARY MOVEMENT" have bee nencountered .
"SHINING"""PATH" __> organization('GUERRILLA') ."
TUPAC """ IMARU"""REVOLUTIONARY"""MOVEMENT" ==> organization('GUERRILLA') .Locating a Physical Target .
In processing the first three paragraphs of the text, a number of rule sfire to trigger the recognition of potential physical targets .
Embassies and vehicles are frequent physica ltargets, and so the following inference rules have been written to capture essential information abou tthem :"EMBASSIES" __> structure('DIPLOMAT OFFICE OR RESIDENCE','PLURAL') .
"VEHICLE"__> vehicle(1) .
"VEHICLES" __> vehicle('PLURAL') .219vehicle(Q) __> structure('TRANSPORT VERICLE',Q) .structure(Type,Quantity)CP, {get_lull_text_at_loc(P,Text) }==> structure(Text,Type,Quantity) .structure(Text,Type,Quantity )==> potential_physical_target(Text,Type,Quantity) .Detecting Event Instances, Revisited .
The discovery of a physical target satisfies the last of theantecedent constraints for the arson and the first bombing event locator rules mentioned earlier, and s oactual events (event instances) can now be inferred by them .
Actual events are represented in the char tas facts of the following sort :chart(actual_event(BOMBING,E1,bombing),6 :7) .chart(actnal_event(BOMBING,E3,bombing),90 :91) .chart(actual_event(BOMBING,E4,bombing),78 :81) .chart(actual_event(BOMBING,E5,bombing),78 :81) .chart(actual_event(BOMBING,E6,bombing),165 :166) .chart(actual_event(IRSON,E7,arson),246 :248) .Multiple bombing instances are created because of the many different ways in which the various rule sfor inferring bombing instances can be satisfied .
It will be the job of the template generator to detect an dmerge references to the same event .Generating Slot Values .
Once an event instance has been asserted, KBIRD will begin to infer tm pclauses, which will later be written to a file to serve as input to the template generator for filling templat eslots .
Each clause has as one of its parameters a score that indicates how likely it is to be an appropriateslot value .
The following rules illustrate how a perpetrator that is a terrorist is favored in a bombin gevent .actual_event(_,ID,bombing) .
.
potential_ind_perpetrator('TERRORIST',P)__> tmp(ID, slot05, P, kbird, 95) .actual_event(_,ID,_) .
.
potential_ind_perpetrator(_,P )==> tmp(ID, alot05, P, kbird, 50) .Similarly, the following rules illustrate how, in templates representing bombing events, organization sthat have been identified as guerrilla groups are favored over drug cartels and military groups as likel yvalues for the perpetrator ORG slot .actual_event(_,ID,bombing) .
.organization( G, 'GUERRILLA' )__> tmp(ID, slot06, [G,'GUERRILL?
'], kbird, 85) .actual_event(_,ID,bombing) .
.organization( G, 'DRUGGIES' )__> tmp(ID, slot06, [G,'REBELS'], kbird, 77) .actual_event(_,ID,bombing) .
.organization( G, 'MILITARY' )__> tmp(ID, slot06, [G,'MILITARY'], kbird, 35) .220Phase Four: Template generatorFigure 4 contains the arson and bombing templates produced by the template generator for messageTST1-MUC3-0099 .
Several bombing events were located, but they were all merged by the templat egenerator into a single representation .Correct Filler(if different)#Descriptio n0message id1template id2date of incident3type of incident4category of incident5perpetrator: id of indi v6perpetrator: id of org(s)7perpetrator: confidenc e8physical target : id(s)9physical target : total num10 physical target : type(s )11 human target : id(s )12 human target : total num13 human target : type(s )14 target : foreign nation(s )15 instrument : type(s)16 location of incident17 effect on physical target18 effect on human target(s)# Description0message i d1template id2date of incident3type of incident4category of incident5perpetrator: id of indi v6perpetrator: id of org(s )7perpetrator: confidence8physical target : id(s )9physical target : total num10 physical target : type(s )11 human target : id(s )12 human target : total num13 human target : type(s )14 target : foreign nation(s )15 instrument: type(s )16 location of incident17 effect on physical targe t18 effect on human target(s)Filler use dTST1-MUC3-0099125 OCT 89ARSO NTERRORIST ACT"SHINING PATH"CLAIMED OR ADMITTED :"SHINING PATH""BUSES"1 0TRANSPORT VEHICLE : "BUSES "sPERU: LIMA (DEPARTMENT) :LIMA (CITY )SOME DAMAGE : "BUSES"Filler usedTST1-MUC3-0099201JUL89-31JUL89BOMBINGTERRORIST AC T"TERRORISTS ""SHINING PATH "CLAIMED OR ADMITTED :"SHINING PATH ""VEHICLES ""EMBASSIES "PLURA LTRANSPORT VEHICLE:"VEHICLES"DIPLOMAT OFFICE O RRESIDENCE : "EMBASSIES "PERU : CALLAO (PORT )DESTROYED : "EMBASSIES"DESTROYED : "VEHICLES"REPORTED AS FACT:"SHINING PATH "Correct Filler (if different)24OCT89-25OCT89.- This conjunct ok."TUPAC AMARU .
.
.
"REPORTED AS FACT : "TERRORISTS "POSSIBLE: "SHINING PATH "POSSIBLE: "TUPAC .
.
.
""VEHICLE "4- This conjunct ok.TRANSPORT VEHICLE:"VEHICLE "??
This conjunct ok .USS RPERU: LIMA (CITY) :ORRANTIA (DISTRICT )SOME DAMAGE: EMBASSIE S4- This conjunct ok.Figure 4: Arson and bombing templates generated by the Unisys MUC-3 system for TST1-MUC3-0099 .The arson template generated by the system was almost completely correct .
The only problem wasthat the perpetrator confidence reported for "SHINING PATH" was CLAIMED OR ADMITTED andnot REPORTED AS FACT .
In the bombing template generated by the system, the date was incorrectl yidentified as being a span of time in July instead of a span of time in October .
The July inference was221based on information in the fifth paragraph .
The system also failed to report the TUPAC AMARU groupas a perpetrator ORG value, even though the group was identified in the text .
An uninteresting bug inthe template generator caused this error .
Finally, rules for inferring that the physical targets belonged t oforeign nations were not sensitive enough to be activated .CONCLUSIONSThe value of the three-tiered approach realized in the Unisys MUC-3 system is two-fold .
First, theterrorist domain is sufficiently well-defined that a deep linguistic analysis is often unneccessary, and usin glinguistic analysis sparingly provides a dramatic improvement in robustness and processing time .
Second ,in the MUC-3 evaluation task we have discovered that a small amount of modeling effort, i .e ., writingKBIRD rules, produces a significant improvement in our ability to extract pertinent information .
SinceKBIRD is a forward chaining rule-driven methodology, the creation, modification and removal of rules i sa very easy and intuitive process .The three-tiered approach of combining traditional information retrieval and linguistic analysis tech-niques with the type of analysis that our knowledge-based information retrieval system, KBIRD, provide soffers significant advantages to solving common text processing problems .
The modularity of this approachallows us to utilize advances made in keyword analysis and NLP technology with relative ease .REFERENCES[1] Tim Finin, Rich Fritzson, and Dave Matuzsek .
Adding forward chaining and truth maintenance t oprolog.
In Fifth IEEE Conference on Artificial Intelligence Application, pages 123-130, March 1989 .
[2] Tim Finin, Robin McEntire, Carl Weir, and Barry Silk .
A three-tiered approach to natural languag etext retrieval .
In Proceedings of the AAAI workshop on Natural Language Text Retrieval, Los Angeles ,July 1991 .
[3] L .
Hirschman, M. Palmer, J .
Dowding, D .
Dahl, M. Linebarger, R .
Passonneau, F .-M. Lang, C .
Ball ,and C. Weir .
The PUNDIT natural-language processing system .
In AI Systems in Government Conf.Computer Society of the IEEE, March 1989 .
[4] Chris Paice .
Another stemmer .
SIGIR Forum, Fall 1990 .
[5] Fernando C.N.
Pereira and David H.D.
Warren .
Definite clause grammars for language analysis?asurvey of the formalism and a comparison with augmented transition networks .
Artificial Intelligence,13(3):231-278, 1980 .
[6] Bob Pollack .
Message format processing language .
Manual, Unisys Center for Advanced InformationTechnology, August 1989 .
Version 2 .1 .
[7] Carl Weir, Tim Finin, Barry Silk, Marcia Linebarger, and Robin McEntire .
Knowledge-based strategie sfor robust text-understanding.
The Eighth Annual Intelligence Community AI/Advance Computin gSymposium, March 1991 .222
