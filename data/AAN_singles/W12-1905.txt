NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 31?38,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsExploiting Partial Annotations with EM TrainingDirk Hovy, Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Marina del Rey, CA 90292{dirkh, hovy}@isi.eduAbstractFor many NLP tasks, EM-trained HMMs arethe common models.
However, in order to es-cape local maxima and find the best model, weneed to start with a good initial model.
Re-searchers suggested repeated random restartsor constraints that guide the model evolu-tion.
Neither approach is ideal.
Restarts aretime-intensive, and most constraint-based ap-proaches require serious re-engineering or ex-ternal solvers.
In this paper we measure the ef-fectiveness of very limited initial constraints:specifically, annotations of a small number ofwords in the training data.
We vary the amountand distribution of initial partial annotations,and compare the results to unsupervised andsupervised approaches.
We find that partialannotations improve accuracy and can reducethe need for random restarts, which speeds uptraining time considerably.1 IntroductionWhile supervised learning methods achieve goodperformance in many NLP tasks, they are inca-pable of dealing with missing annotations.
For mostnew problems, however, missing data is the norm,which makes it impossible to train supervised mod-els.
Unsupervised learning techniques can makeuse of unannotated data and are thus well-suited forthese problems.For sequential labeling tasks (POS-tagging, NE-recognition), EM-trained HMMs are the most com-mon unsupervised model.
However, running vanillaforward-backward-EM leads to mediocre results,due to various properties of the training method(Johnson, 2007).
Running repeated restarts withrandom initialization can help escape local maxima,but in order to find the global optimum, we need torun a great number (100 or more) of them (Ravi andKnight, 2009; Hovy et al, 2011).
However, thereis another solution.
Various papers have shown thatthe inclusion of some knowledge greatly enhancesperformance of unsupervised systems.
They intro-duce constraints on the initial model and the param-eters.
This directs the learning algorithm towards abetter parameter configuration.
Types of constraintsinclude ILP-based methods (Chang et al, 2007;Chang et al, 2008; Ravi and Knight, 2009), and pos-terior regularization (Grac?a et al, 2007; Ganchev etal., 2010).
While those approaches are powerful andyield good results, they require us to reformulate theconstraints in a certain language, and either use anexternal solver, or re-design parts of the maximiza-tion step.
This is time-consuming and requires a cer-tain expertise.One of the most natural ways of providing con-straints is to annotate a small amount of data.
Thiscan either be done manually, or via simple heuris-tics, for example, if some words?
parts of speechare unambiguous.
This can significantly speed uplearning and improve accuracy of the learned mod-els.
These partial annotations are a common tech-nique for semi-supervised learning.
It requires nochanges to the general framework, or the use of ex-ternal solvers.While this well-known, it is unclear exactly howmuch annotation, and annotation of what, is most ef-fective to improve accuracy.
To our knowledge, nopaper has investigated this aspect empirically.
We31Inputs: I went to the showwalk on waterPartial Annotations: I went to the:DET show:NNwalk on:sense5 waterFigure 1: In partial annotation, words are replaced bytheir labelexplore the use of more unlabeled data vs. partialannotation of a small percentage.
For the secondcase, we investigate how much annotation we needto achieve a particular accuracy, and what the bestdistribution of labels is.
We test our approach ona POS-tagging and word sense disambiguation taskfor prepositions.We find that using partial annotations improvesaccuracy and reduces the effect of random restarts.This indicates that the same accuracy can be reachedwith fewer restarts, which speeds up training timeconsiderably.Our contributions are:?
we show how to include partial annotations inEM training via parameter tying?
we show how the amounts and distribution ofpartial annotations influence accuracy?
we evaluate our method on an existing data set,comparing to both supervised and unsupervisedmethods on two tasks2 Preliminaries2.1 Partial AnnotationsWhen training probabilistic models, more con-straints generally lead to improved accuracy.
Themore knowledge we can bring to bear, the more weconstrain the number of potential label sequencesthe training algorithm has to consider.
They alsohelp us to find a good initial model: it has to explainthose fixed cases.The purest form of unsupervised learning as-sumes the complete lack of annotation.
However,in many cases, we can use prior knowledge to labelwords in context based on heuristics.
It is usuallynot the case that all labels apply to all observations.If we know the alphabet of labels we use, we of-ten also know which labels are applicable to whichobservations.
This is encoded in a dictionary.
ForPOS-tagging, it narrows the possible tags for eachword?irrespective of context?down to a manageableset.
Merialdo (1994) showed how the amount ofavailable dictionary information is correlated withperformance.
However, dictionaries list all applica-ble labels per word, regardless of context.
We canoften restrict the applicable label for an observationin a specific context even more.
We extend this toinclude constraints applied to some, but not all in-stances.
This allows us to restrict the choice for anobservation to one label.
We substitute the word incase by a special token with just one label.
Based onsimple heuristics, we can annotate individual wordsin the training data with their label.
For example, wecan assume that ?the?
is always a determiner.
Thisis a unigram constraint.
We can expand those con-straints to include a wider context.
In a sentence like?I went to the show?, we know that NN is the onlyapplicable tag for ?show?, even if a dictionary liststhe possible tags NN and VB.
In fact, we can makethat assumption for all words with a possible POStag of NN that follow ?the?.
This is an n-gram con-straint.Partial annotations provide local constraints.They arise from a number of different cases:?
simple heuristics that allow the disambiguationof some words in context (such as words after?the?
being nouns)?
when we can leverage annotated data from adifferent task?
manual labeling of a few instancesWhile the technique is mainly useful for problemswhere only few labeled examples are available, wemake use of a corpus of annotated data.
This allowsus to control the effect of the amount and type ofannotated data on accuracy.We evaluate the impact of partial annotations ontwo tasks: preposition sense disambiguation andPOS tagging.2.2 Preposition Sense DisambiguationPrepositions are ubiquitous and highly ambiguous.Disambiguating prepositions is thus a challengingand interesting task in itself (see SemEval 2007 task,32(Litkowski and Hargraves, 2007)).
There are threeelements in the syntactic structure of prepositionalphrases, namely the head word h (usually a noun,verb, or adjective), the preposition p, and the objectof the preposition, o.
The triple (h, p, o) forms asyntactically and semantically constrained structure.This structure is reflected in dependency parses as acommon construction.Tratz and Hovy (2009) show how to use the de-pendency structure to solve it.
Their method out-performed the previous state-of-the-art (which useda window-based approach) by a significant margin.Hovy et al (2011) showed how the sequential na-ture of the problem can be exploited in unsupervisedlearning.
They present various sequential modelsand training options.
They compare a standard bi-gram HMM and a very complex model that is de-signed to capture mutual constraints.
In contrast tothem, we use a trigram HMM, but move the preposi-tion at the end of the observed sequence, to conditionit on the previous words.
As suggested there, we useEM with smoothing and random restarts.2.3 Unsupervised POS-taggingMerialdo (1994) introduced the task of unsupervisedPOS tagging using a dictionary.
For each word,we know the possible labels in general.
The modelhas to learn the labels in context.
Subsequent work(Johnson, 2007; Ravi and Knight, 2009; Vaswani etal., 2010) has expanded on this in various ways, withaccuracy between 86% and 96%.
In this paper, wedo not attempt to beat the state of the art, but rathertest whether our constraints can be applied to a dif-ferent task and data set.3 Methodology3.1 DataFor PSD, we use the SemEval task data.
It con-sists of a training (16k) and a test set (8k) of sen-tences with sense-annotated prepositions followingthe sense inventory of The Preposition Project, TPP(Litkowski, 2005).
It defines senses for each of the34 most frequent English prepositions.
There are onaverage 9.76 senses per preposition (between 2 and25).
We combine training and test and use the an-notations from the training data to partially label ourcorpus.
The test data remains unlabeled.
We use theWordNet lexicographer senses as labels for the argu-ments.
It has 45 labels for nouns, verbs, and adjec-tives and is thus roughly comparable to the prepo-sitions sense granularity.
It also allows us to con-struct a dictionary for the arguments from WordNet.Unknown words are assumed to have all possiblesenses applicable to their respective word class (i.e.all noun senses for words labeled as nouns, etc).
Weassume that pronouns other than ?it?
refer to people.For the POS-tagged data, we use the Brown cor-pus.
It contains 57k sentences and about 1, 16mwords.
We assume a simplified tag set with 38 tagsand a dictionary that lists all possible tags for eachword.
For the partial annotations, we label every oc-currence of ?the?, ?a?, and ?an?
as DET, and the nextword with possible tag NN as NN.
Additional con-straints label all prepositions as ?P?
and all forms of?be?
as ?V?.
We train on the top two thirds and teston the last third.For both data sets, we converted all words tolower case and replaced numbers by ?
@?.3.2 Modelsw1w2l1l2walk water onw3l3Figure 2: PSD: Trigram HMM with preposition as lastelementFor POS-tagging, we use a standard bigramHMM without back-off.For PSD, we use a trigram HMM, but move thepreposition at the end of the observed sequence, tocondition it on the previous words (see Figure 2).Since not all prepositions have the same set of la-bels, we train individual models for each preposi-tion.
We can thus learn different parameter settingsfor the different prepositions.We use EM with smoothing and random restartsto train our models.
For smoothing,  is added toeach fractional count before normalization at eachiteration to prevent overfitting (Eisner, 2002a).
We33set  to 0.01.
We stop training after 40 iterations,or if the perplexity change between iterations wasless than 0.0001.
We experimented with differentnumbers of random restarts (none, 10, 50, and 100).3.3 Dealing with Partial AnnotationsThe most direct way to constrain a specific word toonly one label is to substitute it for a special to-ken that has only that label.
If we have a partiallyannotated example ?walk on-sense5 water?
as in-put (see Figure 1), we add an emission probabilityP (word = label |tag = label) to our model.However, this is problematic in two ways.
Firstly,we have effectively removed a great number ofinstances where ?on?
should be labeled ?sense5 ?from our training data, and replaced them with an-other token: there are now fewer instances fromwhich we collect C(on|sense5 ).
The fractionalcounts for our transition parameters are not af-fected by this, but the counts for emission param-eter are skewed.
We thus essentially siphon prob-ability mass from P (on|sense5 ) and move it toP (on : sense5 |sense5 ).
Since the test data nevercontains labels such as sense5 , our partial annota-tions have moved a large amount of probability massto a useless parameter: we are never going to useP (on : sense5 |sense5 ) during inference!Secondly, since EM tends to find uniform distri-butions (Johnson, 2007), other, rarer labels will alsohave to receive some probability.
The counts for la-bels with partial annotations are fixed, so in order touse the rare labels (for which we have no partial an-notations), their emission counts need to come fromunlabeled instances.
Say sense1 is a label for whichwe have no partial annotations.
Every time EM col-lects emission counts from a word ?on?
(and not alabeled version ?on:sensen?
), it assigns some of itto P (on|sense1 ).
Effectively, we thus assign toomuch probability mass to the emission of the wordfrom rare labels.The result of these two effects is the inverse ofwhat we want: our model will use the label withthe least partial annotations (i.e., a rare label) dis-proportionately often during inference, while the la-bels for which we had partial annotations are rarelyused.
The resulting annotation has a low accuracy.We show an example of this in Section 5.The solution to this problem is simple: param-eter tying.
We essentially have to link each par-tial annotation to the original word that we replaced.The observed word ?on?
and the partial annotation?on : sense5 ?
should behave the same way duringtraining.
This way, our emission probabilities forthe word ?on?
given a label (say, ?sense5 ?)
takethe information from the partial annotations into ac-count.
This technique is also described in Eisner(2002b) for a phonological problem with similarproperties.
Technically, the fractional counts we col-lect for C(on : sense5 |sense5 ) should also countfor C(on|sense5 ).
By tying the two parameters to-gether, we achieve exactly that.
This way, we canprevent probability mass from being siphoned awayfrom the emission probability of the word, and anundue amount of probability mass from being as-signed to rare labels.4 Experiments4.1 How Much Annotation Is Needed?In order to test the effect of partial annotations onaccuracy, we built different training sets.
We variedthe amount of partial annotations from 0 to 65% inincrements of 5%.
The original corpus we use con-tains 67% partial annotations, so we were unable togo beyond this number.
We created the different cor-pora by randomly removing the existing annotationsfrom our corpus.
Since this is done stochastically,we ran 5 trials for each batch and averaged the re-sults.We also test the effect more unsupervised data hason the task.
Theoretically, unsupervised methodsshould be able to exploit additional training data.
Weuse 27k examples extracted from the prepositionalattachment corpus from Ratnaparkhi et al (1994).4.2 What Kind of Annotation Is Needed?We can assume that not only the quantity, but alsothe distribution of the partial annotations makes adifference.
Given that we can only annotate a cer-tain percentage of the data, how should we best dis-tribute those annotations among instances to max-imize accuracy?
In order to test this, we hold theamount of annotated data fixed, but vary the labelswe use.
We choose one sense and annotate only theinstances that have that sense, while leaving the restunlabeled.34Ideally, one would like to examine all subsets ofannotations, from just a single annotation to all butone instances of the entire training data.
This wouldcover the spectrum from unsupervised to supervised.It is unlikely that there is a uniform best number thatholds for all problems within this immense searchspace.
Rather, we explore two very natural cases,and compare them to the unsupervised case, for var-ious numbers of random restarts:1. all partial annotations are of the same sense2.
one labeled example of each sense5 ResultsSystem Acc.
(%)semi-supervised w/o param tying 4.73MFS baseline 40.00unsupervised (Hovy et al, 2011) 55.00semi-supervised, no RR 63.18semi-supervised, 10 RR 63.12semi-supervised, 50 RR 63.16semi-supervised, 100 RR 63.22semi-supervised, addtl.
data, no RR 62.67semi-supervised, addtl.
data, 10 RR 62.47semi-supervised, addtl.
data, 50 RR 62.58semi-supervised, addtl.
data, 100 RR 62.58supervised (Hovy et al, 2010) 84.50Table 1: Accuracy of various PSD systems.
Baseline ismost frequent sense.Table 1 shows the results for the PSD systems wetested.
Since not all test sets are the same size, we re-port the weighted average over all prepositions.
Forsignificance tests, we use two-tailed t-tests over thedifference in accuracy at p < 0.001.The difference between our models and the base-line as well as the best unsupervised models inHovy et al (2011) are significant.
The low accu-racy achieved without parameter tying underscoresthe importance of this technique.
We find that thedifferences between none and 100 random restartsare not significant if partial annotations are used.Presumably, the partial annotations provide a strongenough constraint to overcome the effect of the ran-dom initializations.
I.e., the fractional counts fromthe partial annotations overwhelm any initial param-eter settings and move the model to a more advanta-geous position in the state space.
The good accuracyfor the case with no restarts corroborates this.505560657075808590951000 5 10 15 20 25 30 35 40 45 50 55 60 65accuracy(%)amount of annotated prepositions (%)Figure 3: Accuracy for PSD systems improves linearlywith amount of partial annotations.
Accuracies abovedotted line improve significantly (at p < 0.001) over un-supervised approach (Hovy et al, 2011)Figure 3 shows the effect of more partial anno-tations on PSD accuracy.
Using no annotations atall, just the dictionary, we achieve roughly the sameresults as reported in Hovy et al (2011).
Each incre-ment of partial annotations increases accuracy.
Ataround 27% annotated training examples, the differ-ence starts to be significant.
This shows that unsu-pervised training methods can benefit from partialannotations.
When adding more unsupervised data,we do not see an increase in accuracy.
In this case,the algorithm failed to make use of the additionaltraining data.
This might be because the two datasets were not heterogenous enough, or because thenumber of emission parameters grew faster than theamount of available training examples.
A possible,yet somewhat unsatisfying explanation is that whenwe increase the overall training data, we reduce thepercentage of labeled data (here to 47%; the resultwas comparable to the one observed in our ablationstudies).
It seems surprising, though, that the modeldoes not benefit from the additional data1.
More ag-gressive smoothing might help alleviate that prob-lem.The results on the distribution of partial annota-tion are shown in Figure 4.
Using only the most1Note that similar effects were observed by (Smith and Eis-ner, 2005; Goldwater and Griffiths, 2007).350102030405060708090100all 1st 2nd 3rd 4th 5th one each53.5548.7744.7143.0045.6549.6963.12accuracy(%)senses usedFigure 4: Labeling one example of each sense yields bet-ter results than all examples of any one sense.
Sensesordered by frequencyfrequent sense, accuracy drops to 49.69%.
Whilethis is better than the baseline which simply assignsthis sense to every instance, it is a steep drop.
Weget better results using just one annotated exampleof each sense (53.55%).System Acc.
(%)(Merialdo, 1994) 86.60random baseline 62.46unsupervised, no RR 82.77semi-supervised, DET+NN 88.51semi-supervised, DET+NN+P 88.97semi-supervised, DET+NN+P+V 87.07Table 2: Accuracy of various POS systems.
Randombaseline averaged over 10 runs.The results for POS tagging confirm our previ-ous findings.
The random baseline chooses for eachword one of the possible tags.
We averaged the re-sults over 10 runs.
The difference in accuracy be-tween both the baseline and the unsupervised ap-proach as well as the unsupervised approach and anyof the partial annotations are significant.
However,the drop in accuracy when adding the last heuris-tic points to a risk: partial annotation with heuris-tics can introduce errors and offset the benefits ofthe constraints.
Careful selection of the right heuris-tics and the tradeoff between false positives they in-troduce and true positives they capture can alleviatethis problem.6 Related ResearchUnsupervised methods have great appeal forresource-poor languages and new tasks.
They havebeen applied to a wide variety of sequential label-ing tasks, such as POS tagging, NE recognition, etc.The most common training technique is forward-backward EM.
While EM is guaranteed to improvethe data likelihood, it can get stuck in local max-ima.
Merialdo (1994) showed how the the initializedmodel influences the outcome after a fixed numberof iterations.
The importance is underscored suc-cinctly by Goldberg et al (2008).
They experimentwith various constraints.The idea of using partial annotations has beenexplored in various settings.
Druck et al (2008)present an approach to label features instead ofinstances for discriminative probabilistic models,yielding substantial improvements.
They also studythe effectiveness of labeling features vs. labeling in-stances.
Rehbein et al (2009) study the utility ofpartial annotations as precursor to further, humanannotation.
Their experiments do not extend to un-supervised training.
Tsuboi et al (2008) used datathat was not full annotated.
However, their settingis in principle supervised, only few words are miss-ing.
Instead of no labels, those words have a limitednumber of possible alternatives.
This works well fortasks with a small label alphabet or data where anno-tators left multiple options for some words.
In con-trast, we start out with unannotated data and assumethat some words can be labeled.
Gao et al (2010)present a successful word alignment approach thatuses partial annotations.
These are derived fromhuman annotation or heuristics.
Their method im-proves BLEU, but requires some modification of theEM framework.7 Conclusion and Future WorkIt is obvious, and common knowledge, that provid-ing some annotation to an unsupervised algorithmwill improve accuracy and learning speed.
Surpris-ingly, however, our literature search did not turn upany papers stating exactly how and to what degreethe improvements appear.
We therefore selected a36very general training method, EM, and a simple ap-proach to include partial annotations in it using pa-rameter tying.
This allows us to find more stablestarting points for sequential labeling tasks than ran-dom or uniform initialization.
We find that we wouldneed a substantial amount of additional unlabeleddata in order to boost accuracy.
In contrast, we canget significant improvements by partially annotatingsome instances (around 27%).
Given that we canonly annotate a certain percentage of the data, it isbest to distribute those annotations among all appli-cable senses, rather than focus on one.
This obviatesthe need for random restarts and speeds up training.This work suggests several interesting new av-enues to explore.
Can one integrate this procedureinto a large-scale human annotation effort to ob-tain a kind of active learning, suggesting which in-stances to annotate next, until appropriate stoppingcriteria are satisfied (Zhu et al, 2008)?
Can onedetermine upper bounds for the number of randomrestarts given the amount of annotations?AcknowledgementsWe would like to thank Victoria Fossum, KevinKnight, Zornitsa Kozareva, and Ashish Vaswani forinvaluable discussions and advice.
We would alsolike to thank the reviewers who provided us withhelpful feedback and suggestions.
Research sup-ported in part by Air Force Contract FA8750-09-C-0172 under the DARPA Machine Reading Program.ReferencesMing-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 280?287, Prague, Czech Republic.
Associ-ation for Computational Linguistics.Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, andDan Roth.
2008.
Learning and inference with con-straints.
In Proceedings of the 23rd national confer-ence on Artificial intelligence, pages 1513?1518.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using gener-alized expectation criteria.
In Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 595?602.
ACM.Jason Eisner.
2002a.
An interactive spreadsheet forteaching the forward-backward algorithm.
In Pro-ceedings of the ACL-02 Workshop on Effective toolsand methodologies for teaching natural language pro-cessing and computational linguistics-Volume 1, pages10?18.
Association for Computational Linguistics.Jason Eisner.
2002b.
Parameter estimation for prob-abilistic finite-state transducers.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, pages 1?8.
Association for Com-putational Linguistics.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
The Journal of MachineLearning Research, 11:2001?2049.Qin Gao, Nguyen Bach, and Stephan Vogel.
2010.
Asemi-supervised word alignment algorithm with par-tial manual alignments.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetricsMATR, pages 1?10.
Association for Computa-tional Linguistics.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.Em can find pretty good hmm pos-taggers (when givena good start).
In Proceedings of ACL.Sharon Goldwater and Thomas Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In ANNUAL MEETING-ASSOCIATION FORCOMPUTATIONAL LINGUISTICS, volume 45, page744.Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar.
2007.Expectation maximization and posterior constraints.Advances in Neural Information Processing Systems,20:569?576.Dirk Hovy, Stephen Tratz, and Eduard Hovy.
2010.What?s in a Preposition?
Dimensions of Sense Dis-ambiguation for an Interesting Word Class.
In Coling2010: Posters, pages 454?462, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.Dirk Hovy, Ashish Vaswani, Stephen Tratz, David Chi-ang, and Eduard Hovy.
2011.
Models and trainingfor unsupervised preposition sense disambiguation.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 323?328, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 296?305.Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word-Sense Disambiguation of Prepo-sitions.
In Proceedings of the 4th International37Workshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.Ken Litkowski.
2005.
The preposition project.http://www.clres.com/prepositions.html.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational linguistics,20(2):155?171.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.1994.
A maximum entropy model for prepositionalphrase attachment.
In Proceedings of the workshop onHuman Language Technology, pages 250?255.
Asso-ciation for Computational Linguistics.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP:Volume 1-Volume 1, pages 504?512.
Association forComputational Linguistics.Ines Rehbein, Josef Ruppenhofer, and CarolineSporleder.
2009.
Assessing the benefits of par-tial automatic pre-labeling for frame-semanticannotation.
In Proceedings of the Third LinguisticAnnotation Workshop, pages 19?26.
Association forComputational Linguistics.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 354?362.Association for Computational Linguistics.Stephen Tratz and Dirk Hovy.
2009.
Disambiguation ofPreposition Sense Using Linguistically Motivated Fea-tures.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, Companion Volume: Student Research Work-shop and Doctoral Consortium, pages 96?100, Boul-der, Colorado, June.
Association for ComputationalLinguistics.Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, ShinsukeMori, and Yuji Matsumoto.
2008.
Training condi-tional random fields using incomplete annotations.
InProceedings of the 22nd International Conference onComputational Linguistics, volume 1, pages 897?904.Association for Computational Linguistics.Ashish Vaswani, Adam Pauls, and David Chiang.
2010.Efficient optimization of an mdl-inspired objectivefunction for unsupervised part-of-speech tagging.
InProceedings of the ACL 2010 Conference Short Pa-pers, pages 209?214.
Association for ComputationalLinguistics.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008.Multi-criteria-based strategy to stop active learningfor data annotation.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics-Volume 1, pages 1129?1136.
Association for Compu-tational Linguistics.38
