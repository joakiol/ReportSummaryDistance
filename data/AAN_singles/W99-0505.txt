Towards a Meaning-Full Comparison of Lexieal ResourcesKenneth C LltkowskaCL Research9208 Gue RoadDamascus, MD 20872ken@clres cornh t tp / /www tires tomAbstractThe mapping from WordNet o Hector senses m Senseval provides a "gold standard" against wluch tojudge our ability to compare lexlcal resources The "gold standard" is provided through aword overlap analysis(with and without a stop list) for flus mapping, achieving at most a 36 percent correct mapping (inflated by 9percent from "empty" assignments) An alternaUve componenttal analysis of the defimtaons, using syntacUc,collocatmnal, and semantac component and relation identification (through the use ofdefimng patterns integratedseamlessly mto the parsing thclaonary), provides an almost 41 percent correct mapping, with an additaonal 4percent by recogmzmg semantic omponents not used in the Senseval mapping Defimtion sets of the Sensevalwords from three pubhshed thclaonanes and Dorr's lextcal knowledge base were added to WordNet and the Hectordatabase to exanune the nature of the mapping process between defimtton sets of more and less sco\[~e Thetecbauques described here consUtute only an maaal implementation f the componenUal nalysis approach andsuggests that considerable further improvements can be acluevedIntroductionThe difficulty of companng lemcal resources,long a s~gnfficant challenge in computauonalhnguistlcs (Atlans, 1991), came to the fore in therecent Senseval competatton (IOlgarnff, 1998), whensome systems that relied heavily on the WordNet(Miller, et al 1990) sense inventory were faced withthe necessity of using another sense inventory(Hecto0 A hasty solutaon to the problem was the "development of a map between the two inventories,but some part~cipants expressed concerns that use offlus map may have degraded their performance toanunknown degreeAlthough there were disclaimers about heWordNet-Hector map, it nonetheless tands as ausable gold standard for efforts to compare lexicalresources Moreover, we have a usable baseline (aword overlap method suggested m (Lesk, 1986))against which to compare whether we are able tomake improvements m the mapping (since flusmethod has been shown to perform not as well asexpected (Krovetz, 1992))We first describe the lextcal resources used mthe study (Hector, WordNet, other dicUonanes, and alex~cal knowledge base), first characterizing them interms ofpolysemy and the types of leracalmformaUon each contmns (syntacUc properties andfeatures, emantac components and relaUons, andcollocaUonal properties) We then present results ofperfornung the word overlap analysis of the 18 verbsused m Senseval, analyzing the definitions mWordNet and Hector We then expand our analysisto include other dictionaries We describe ourmethods of analysis, particularly the methods ofparsing defimtaons and identff)qng semanticrelations (semrels) based on defimng patterns,essentially takang first steps m Implementing theprogram described by Atkms and focusmg on the useof"meamng" full mformataon rather than statisticalmformaUon We identify the results that have beenachieved thus far and outline further steps that mayadd more "meanmg" to the analysisIAll analyses described m this paper were performedautomatically using functlonahty incorporated mDIMAP (Dictionary Maintenance Programs)(available for immediate download at (CL Research,1999a)) This includes automatac extracuon ofWordNet reformation for the selected words(mtegrated mDIMAP) Hector defimtlons wereuploaded into DIMAP dicUonanes after use of aconversmn program Defimtlons for other30The Lexical  ResourcesTlus analysis focuses on the mmn verb sensesused In Senseval (not ichoms and phrases),specifically the followmgAMAZE, BAND, BET, BOTHER, BURY, CALCULATE,CONSUME, DERIVE, FLOAT, HURDLE, INVADE,PROMISE, SACK, SANCTION, SCRAP, SEIZE,SHAKE, SLIGHTThe Hector database used In Senseval consists of atree of senses, each of which contains defimttons,syntactic properties, example usages, and "clues"(collocational information about he syntactic andsemantic enwronment in wluch a word appears inthe spectfic sense) The WordNet database contmnssynonyms (synsets), perhaps a defimtton or exampleusages (gloss), some syntactic mformaUon (verbframes), hypernyms, hyponyms, and some othersemrels (ENTAILS, CAUSES)To extend our analysis In order to look at otherissues of lexacal resource comparison, we haveincluded the defirauons or leracal information fromthe following additional sources?
Webster's 3 ra New International Dictionary (W3)?
Oxford Advanced l.earners D~ctlonary (OALD)?
American Hentage DlcUonary (AI-ID)?
Dorr's Lexacal Knowledge Base (Dorr)We used only the defimuons from W3, OALD, andAHD (which also contmn sample usages and somecollocattonal information m the form of usage notes,not used at the present tame) Dorr's databasecontains thematic grids wluch characterize thethematic roles of obligatory and optional semanuccomponents, frequently identifying accompanyingpreposmons (Olsen, et al  1998)The following table identities the number ofsenses and average overall polysemy for each ofthese resourcesdictionaries were entered by handWordamazebandbetbotherburycalculateconsumedenvefloathurdleinvadepronusesacksanctionscrapseizeshakeshghtAveragePolysemyooo1 2 4 23 1 I I  44 2 5 57 6 9 712 6 14 55 5 10 96 6 8 86 5 15 516 4 41 142 1 4 36 2 10 55 4 7 44 4 6 32 2 5 23 1 3 311 6 21 138 8 37 171 1 6 3O1 22 41 34 48 13 13 13 210 51 03 13 22 01 11 07 17 12I 057 37 120 62 34 22Word Over lap Analys isWe first estabhsh a baseline for automaticreplication of the lexicographer's mappmg fromWordNet 1 6 to Hector, using a s~mple word overlapanalysis mular to (Lesk, 1986) The lextcographermapped the 66 WordNet senses (each synset mwhich a test occurred) Into 102 Hector senses Atotal of 86 assignments were made, 9 WordNetsenses were gwen no assignments, 40 recewedexactly one, and 17 senses received 2 or 3asssgnments The WordNet senses contained 348words (about half of wluch were common wordsappeanng on our stop list, which contained 165words, mostly preposmons, pronouns, andconjunctions) The Hector senses elected m theword overlap analysis contained about 960 words (allHector senses contained 1878 words)We performed a strict word overlap analysts(with and wsthout a stop hst) between tile definlUonsin WordNet and the Hector senses, that is, we didnot attempt to ldenttfy root forms of Inflected wordsWe took each word m a WordNet sense anddetermined whether ~t appeared in a Hector sense,we selected a Hector sense based on the highestpercentage ofwords over all Hector senses An31empty selection was made ff all the words in theWordNet sense did not appear in any Hector sense,only content words were considered when the stophst was usedFor example, for bet, WordNet sense 2 (stake(money) on the outcome of an issue) mapped intoHector sense 4 ((of a person) to risk (a sum of moneyor property) m thts way) In this case, there was anoverlap on two words (money, 039 in the Hectordefimtlon (0 13 of its 15 words) without he stop listWhen the stop list was invoked, there was an overlapof only one word (money, 0 07 of the Hectordefimtion) In this case, the lexicographer had madethree assignments (Hector senses 2, 3, and 4), ourscoring method treated flus as only 1 out of 3 correct(not using the relaxed method employed in Sensevalof treating flus as completely correct)Without he stop hst, our selections matched thelexicographer's in 28 of 86 cases (32 6%), using thestop list, we were successful in 31 of 86 cases(36 1%) The improvement arising when the stoplist was used is deceptive, where 8 cases were due toempty assignments ( o that only 23 cases, 26 7%,were due to matching content words) Overall, only41 content words were involved in these 23 successeswhen the stop list was used, an average of I 8content wordsTo summanze the word overlap analysis (1)despite a ncher set of defimtions in Hector, 9 of 66WordNet senses (13 6%) could not be assigned, (2)despite the greater detail in Hector senses comparedto WordNet senses (2 8 times as many words), only1 8 content words participated in the assignments,and (3) therefore, the defimng vocabulary betweenthese two definition sets seems to be somewhatdivergent Although it might appear as if the wordoverlap analysis does not perform well, this is notthe case The analysis provides abroad overview ofthe defimuon companson process between twodefinmon sets and frames a deeper analysis of thedifferences Moreover, it appears that the accuracyof a "gold standard" mapping is not cruciallyimportant The quality of the mapping may helpframe the subsequent analysis more precisely, but itseems ufficient that any reasonable mapping willsuffice This will be discussed further afterpresenting the results of the componentlal nalysis ofthe defimtlons32Meaning-Full Analysis of DefinitionsThe deeper analysis of the mapping between twodefimtion sets relies primarily on two major steps(1) parsing definitions and using defimng patterns toidentify semrels present m the definitions and (2)relaxing values to these relations by allowing"synonymic" substitution (using WordNet) Thus,for example, ffwe identify hypernyms or instrumentsfrom parsing adefimtion, we would say that hedefimtions are "equal" not just ffthe hypernym orinstrument is the same word, but also Lf thehypernyms or instruments are members of the samesynsetThis approach is based on the finding(Litkowski, 1978) that a dictionary induces asemantic network where nodes represent "concepts"that may be lexicahzed and verbalized in more thanone way This finding implies, in general, theabsence of true synonyms, and instead the kind of"concept" embodied in WordNet synsets (withseveral lexical items and phraseologles) A slmdarapproach, parsing defimtlons and relaxing semrelvalues, was followed in (Dolan, 1994) for clnstenngrelated senses w~thin a single dictionaryThe ideal toward which this approach strives isa complete identification of the meamng componentsincluded in a defimtion The meaning componentscan include syntactic features and charactenstlcs(including subcategonzation patterns), semantmcomponents (realized through identification ofsemrels), selectional restrictions, and coUocationalspecificationsThe first stage of the analysis parses thedefinitions (CL Research, 1999b, Litkowski, toappear) and uses the parse results to extract (viadefining patterns) semrels Since definitions havemany idiosyncrasies (that do not follow ordinarytext), an important first step in this stage ispreprocessmg the definition text to put it into asentence frame that facilitates the extraction ofsemrels 22Note that the stop hst is not applicable to thedefinition parsing The parser is a full-scalesentence parser, where prepositmns and other wordson the stop list are necessary for successful parsingMoreover, inclusion of the prepositions i  cmcml tothe method, since they are the bearers of muchsemrel informationThe extractmn of semrels examines the parseresults, a e,  a tree whose mtermedaate nodesrepresent non-ternunals and whose leaves representthe lextcal atems that compnse the defimuons, whereany node may also include annotations such ascharacterizations of number and tense For all nounor verb defimttons, flus includes Identification of thehead noun (with recogmtton of"empty" heads) orverb, for verbs, we signal whether the defimtaoncontmned any selecttonal restnctmus (that as,pamcular parenthesazed xpressaons) for the subjectand object We then exanune preposattonal phrasesIn the defimUon and deterrmne whether we have a"defining pattern" for the preposaUon whach we canuse as mdacaUve of a partacular semrel We alsoidentify adverbs m the parse tree and look these upin WordNet o adentffy an adjecuve synset fromwluch they are derived (if one is gwen)The defimng pattems are actually part of thedictionary used by the parser That is, we do nothave to develop specafic routines to look for specLficpatterns A defimng pattern ~s a regular expressaonthat arlaculates a syntactac pattern to be matchedThus, to recograze a "manner" semrel, we have thefoUowmg entry for "m"m(dpat((~ rep0 l(det(0)) adj manner(0)st(manner))))This allows us to recognize "m" as possibly gwmgrise to a "manner" component, where we recogmze"m" (the tdde, which allows us to specify partacularelements before the "m" as well), vath a noun phrasethat consasts of 0 or 1 determiner, an adjectwe, andthe lateral "manner" The '0 ?
after the detenmnerand the hteral mdacate that these words are notcopied into the value for a "manner" role, so that thevalue to the "manner" semrel becomes only theadjectwe that as recogmzedThe second stage of the analysis uses thepopulated lexacal database to compare senses andmake the selectaons This process follows thegeneral methodology used m Senseval (Lltkowska, toappear) Specifically, m the defimtaon comparison,we first exanune xclusaon cntena to rule outspecific mappings These criteria include syntacUcproperUes (e g,  a verb sense that Is only transluvecannot map into one that Is only mtransRave) andcollocataonal propertaes (eg,  a sense that is usedwith a parUcle cannot map into one that uses adifferent particle) At the present tune, these areused only rmmmally33We next score each viable sense based on rotssemrels We increment the score ff the senses have acommon hypernym or If a sense's hypernyms belong?
to the same synset as the other sense's hypernyms Ifa parUcular sense con~ns a large number ofsynonyms (that as, no differentiae on the hypernym)and they overlap consaderably m the synsets theyevoke, the score can be increased substanUallyCurrently, we add 5 points for each match 3We increment the score based on commonsemrels In tins amtml tmplementaUon, wehavedefimng patterns (usually qmte nummal) forrecogmzmg Instrument, means, location, purpose,source, manner, has-constituents, has-members,is-part-of, locale, and goal 4 We Increment thescore by 2 points when we have a common semreland then by another 5 points when the value Is~dentacal orm the same synsetAfter all possable increments othe scores havebeen made, we then select he sense(s) w~th thelughest score Finally, we compare our selecuonwith that of the gold standard to assess our mappingover all sensesAnother way an wluch our methodology followsthe Senseval process as that at proceedsincrementally Thus, ~t ms not necessary to have a"final" perfect parse and mapping rouUne We canmake conUnual refinements atany stage of theprocess and exarmne the overall effect As mSenseval, we may make changes to deal wath aparticular phenomenon with the result hat overallperformance d chnes, but w~th a sounder basis formaking subsequent amprovementsResults of Componential AnalysisThe "gold standard" analysis Involves mapping66 WordNet senses with 348 words into 102 Hectorsenses with 1878 words Using the methoddescribed above, we obtained 35 out of 86 correct3At the present tame, we use WordNet o adentffysemreis We envaslon usmg the full semanlacnetwork created by parsing all a dlcUonary'sdefimtaons Thas would include a richer set ofsemrels than currently included m WordNet4The defimng patterns are developed by hand Wehave onlyJust begun this effort, so the current set mssomewhat Impoverishedmappmgs (407%), a shght improvement over the 31correct assignments u mg the stop-last word overlaptechmque However, as mentioned above, the stop-hst techmque had aclueved 8 of its successes bymatclung null assignments Consadered on tlusbasins, ~t seems that the componentaal analysistechmque provides ubstantial ~mprovement Inaddition, our technique "erred" on 4 cases by malangassagnments where none were made by theleracographer We suggest that these cases docon~n some common elements of meaning and mayconceivably not be construed as errorsThe mapping from WordNet o Hector hadrelatavely few empty mappings, enses for wtuch Itwas not possable to make an assignment These arethe cases where at appears that the chetmnanes donot overlap and thus prowde a tentative mdacataon fwhere two dictionaries may have different coverageThe cases of multiple assignments mchcate thedegree ofamblgmty m the mapping The average mboth darecUons between Hector and WordNet weredonunated by the mabdaty to obtain gooddascnnunatton for the word "semze" Thus, tlusmethod identifies individual words where the&scnnunatwe ablhty needs to be further efined?
Perhaps more importantly, the componentmlanalysis method exploits consaderably more WordNet - Hector?
mformauon than the word overlap methodsWhereas the stop-hst word overlap mapping was?
based on only 41 content words, the componenual ~approach (In the selected mappings) had 228 hits in ~ .~?
developing ats scores, with only a small number of ~ .~ ~defining patternsComparison of DictionariestelO ~30'3We next exanuned the nature of themterrelalaons between parrs of chctaonanes w~thoutuse of a "gold standard" to assess the process ofmapping For t/us purpose, we mapped m both&recttons between the paars {WordNet, Hector},{W3, OALD}, and {W3, AHD We exanune Dorr'slexacal knowledge base for the amphcatlons It mayhave m the mapping processNeither WordNet nor Hector are properlyv~ewed as chcuonanes, ince there was no mtenuonto pubhsh them as such WordNet "glosses" aregenerally smaller (53 words per sense) compared toHector (184 words per sense), whach contains manywords specff3nng selectmnal restnct~ons on thesubject and object of the verbs Hector was usedprimarily for a large-scale sense tagging projectThe three formal d~ctmnanes were subject torigorous pubhslung and style standards Theaverage number of words per sense were 87(OALD), 7 1 (AHD), and 9 9 (W3), w~th an averageof 3 4, 62, and 120 senses per wordEach table shows the average number of sensesbeing mapped, the average number of assignments mthe target dlCtmnary, the average number of sensesfor which no assagnment could be made, the averagenumber of mulUple assignments per word, and theaverage score of the assignments hat were madeWN-Hector 37 47 06 17 119Hector-WN 57 64 14 22 113These points are further emphasized m themapping between W3 and OALD, where thedisparity between the empty and mulUpleassagnments indicate that we are mapping betweendictionaries qmte disparate This tends to be thecase not only for the enUre set of words, but also isevident for individual words where there is aconsiderable d~spanty mthe number of senses,wtuch then dominate the overall dlspanty Thus, forexample, W3 has 41 defimUons for "float", whileOALD has 10 We tend to be unable to find thespecific sense m going from W3 to OALD, because atis likely that we have many more specific defimtlonsthat are not present In the other direction, we arehkely to have considerable ambiguity and multipleassignmentsW3-OALDOALD-W3W3 - OALD120 78 60 18 9934 60 07 32 8634ABetween W3 and AHD, there ss less overalldaspanty between the defimtaon sets, although sinceW3 Is tmabndged, we stall have a relatavely lughnumber of senses m W3 that do not appear to bepresent m AHD Finally, It should be noted that thescores for the published ictaonanes tend to be alittle lower than for WordNet and Hector Tlusreflects the hkehhood that we have not extracted asmuch mformataon as we dad m parsing andanalyzmg the defimtaon sets used m SensevalW3 - AHDoJ?
'q OW3-AHD 120 115 40 36 90AHD-W3 6 2 9 1 1 2 4 1 9 1We next considered Dorr's lexacal database Wefirst transformed her theta grids ?
to  syntacticspectflcataons (transttave or lntransmttve) andidenttficataon f semreis (e g, where she Identifiedan instr component, we added such a semrel to theDIMAP sense) We were able to identify a mappmgfrom WordNet o her senses for two words ("float"and "shake") for wluch Dorr has several entriesHowever, smce she has considerably more semanuccomponents han we are currently able to recogmze,we dad not pursue this avenue any further at flustimeMore important than just mappmg between twowords, Dorr's data mdacates the posstbday of furtherexploitation of a richer set of semanUc omponentsSpectfically, as reported m (Olsen, et al 1998), mdescnbmg procedures for automatically acqumngthematic grids for Mandann Chinese, ~t was notedthat "verbs that incorporate hemaUc elements mtheir meamng would not allow that element toappear m the complement s ructure" Thus, by usmgDorr's thematic grids when verb are parsed mdefimtaons, it ~s possible to ~dentffy where partacularsemantac components are lexicahzed and whichothers are transnutted through to the themaUc grid(complement or subcategonzataon pattern) for thedefimendumThe transmiss~on f semantic omponents tothethematic gnd ~s also reflected overtly m manydefimtlons For example, shake has one definition,"to bnng to a specified condatton by or as ffbyrepeated qmck jerky movements" We would thusexpect that the thematac grid for this defimtaonshould include a "goal" And, ?deed, Dorr'sdatabase has two senses whch reqmre a "goal" aspart of their thematic grid Smularly, for manydefimtaons m the sample set, we ~dentLfied a sourcedefimng pattern based on the word "from,"frequently, the object of the preposmon was the word"source" ttseff, mdacatmg that the subcategonzaUon,properties of the defimendum should ?elude asource componentDiscussionWlule the improvement m mapping by using thecomponentaal analysis techmque (over the wordoverlap methods) is modest, we consider theseresults qmte slgmficant m wew of the very smallnumber of defimng patterns we have ImplementedMost of the improvement stems from the wordsubstatuUon pnnclple described earlier (as ewdencedby the preponderance of 5 point scores) Thistechmque also provides a mechamsm for bnngmgback the stop words, wz,  the preposmons, wluch arethe careers of mformatmn about semrels (the 2 pointscores )The more general conclusion (from the wordsubsutuuon) is that the success arises from no longerconsidenng a defimtmn m ~solation The propercontext for a word and its defimtions consists not.lUSt of the words that make up the definition, butalso the total semantac network represented bythedictaonaryWe have aclueved our results by explomng onlya small part of that network We have moved only afew steps ?
to  that network beyond the mdawdualwords and their definitions We would expect hatfurther expansmn, first by the add?on of further and~mproved semrel defining patterns, and second,through the identaficataon fmore pnmmve semanuccomponents, will add considerably toour abflay tomap between lexacal resources We also expect~mprovements from consideration fothertechniques, uch as attempts at ontology ahgnment(Hovy, 1998)Although tile definition analysis provlded herewas performed on definmons with?
a stogielanguage, the vanous meamng componentsmmmmmmmm35correspond to those used in an Interhngua The useof the exUncuon method (developed morder tocharactenze v rbs m another language, Clunese) canfrmtfully be applied here as wellTwo further observaUons about lus process canbe made The first is that rchance on a well-established semantic network such as WordNet ,s notnecessary The componenUal nalysis method reheson the local neighborhood fwords m thedefimUons, not on the completeness of the networkIndeed, the network ~tsel?
can be bootstrapped basedon the parsing results The method can work vathany semanUc network or ontology and may be usedto refine or flesh out the network or ontologyThe second observation is that it is not necessaryto have a well-estabhshed "gold standard" Anymapping vail do All that Is necessary is for anymvesugator (lemcographer or not) to create ajudgmental mappmg The methods employed herecan then quanufy ttus mapping based on a wordoverlap analysis and then further examine tt basedon the componenaal nalysis The componenUalanalysis method can then be used to exanuneunderlying subtleUes and nuances tn the defimUous,wluch a lemcographer or analyst can then examinem further detail to assess the mappingFuture WorkTlus work has marked the first ume that all thenecessary mfrastructure has been combmed tn arudimentary form Because of its rudimentary status,the opportumUes for improvement are quiteextensive In addlUon, there are many opportumUesfor using the techmques descnbed here m furtherNLP apphcatlonsFirst, the techmques described here haveimmediate apphcabtllty as part of a lexicographer'sworkstaUon When defimUons are parsed andsemrels are zdenttfied, the resulUng data structurescan be apphed against a corpus of instances forparUcular words (as m Senseval) for improvingword-sense disamblguaUon The techmques willalso permit comparing an entry vath Itself todeternune the mterrelattonshtps among ~tsdefimUons and of companng the defimUons of two"synonyms" to deternune the amount of overlapbetween them on a defimtlon by defimUon bas~sAlthough the analys,s here has focused on theparsing of defimUous, the development of defimngpatterns clearly extends to generalized text parsingsince the defimng patterns have been incorporatedmto the same chcttonary used for parsing free text,the patterns can be used threctly to identify thepresence of parUcular semrels among sentenualconsUtuents We are working to integrate th~sfuncUonahty into our word-sense &sambiguaUontechruques (both the defimng patterns and thesemrels) Even further, mt seems that matclungdefimng patterns in free text can be used for lextcalacquisition Textual matenal that contains thesepatterns could concewably be flagged as providingdefimUonal matenal which can then be compared toemstmg defimUons to assess whether their use tscous,stent vath these defimUons, and ff not, at leastto flag the inconsistencyThe tecluuques descnbed here can be appheddirectly to the fields of ontology development andanalysis of ternunologlcal databases For ontoiogles,vath or w~thout defimuons, the methods employedcan be used to compare ntries m dai'erentontologles based pnmanly on the relattous m theontology, both luerarclucal nd other Forternunologlcal databases, the methods descnbed herecan be used to exanune the set of conceptualrelaUons lmphed by the defimtmus The defimuonparsing wall facd~tate he development of thetermmolog~ca I network tn the pamcular fieldcovered by the databaseThe componenUal nalysts methods result m aricher semantic network that can be used m otherapphcattous Thus, for example, ~t s possible toextend the leracal chatmng methods described m(Green, 1997), which are based on the semrels usedm WordNet The semrels developed with thecomponenttal analysis method would provideadditional detad available for apphcauon of lexlcalcohesion methods In particular, addtUonal relattouswould penmt some structunng wmthm the individualleracal chams, rather than just consldenng eachcham as an amorphous set (Green, 1999)Finally, we are currently investigating the use ofthe componenUal nalysts techmque for mformauonextracUon The techmque identifies (fromdefimtlous) lots that can be used as slots or fields mtemplate generataon Once these slots are identified,we wall be attemptmg toextract slot values fromItems m large catalog databases (mdhons of items)36In conclusion, it would seem that, instead of apaucity of tnformation allovang us to comparelexmal resources, by bnngmg m the full semanticnetwork of the lexicon, we are overwhelmed with aplethora of dataAcknowledgmentsI would like to thank Bonnie Dorr, ChnstianeFellbaum, Steve Green, Ed Hovy, RameshKnshnamurthy, Bob Krovetz, Thomas Potter, LucyVanderwende, and an anonymous reviewer for theircomments on an earlier draft of this paperLttkowskl, K C (to appear) SENSEVAL TheCL Research Expenence Computers and theHumamttesMtller, G A, Beckwlth, R, Fellbaum, C,Gross, D, & Miller, K J (1990) Introduction toWordNet An on-hne lexical database lnternatwnalJournal of Lexicography, 3(4), 235-244Olsen, M B, Dorr, B J, & Thomas, S C(1998, 28-31 October) Enhancmg AutomaticAcqulsmon of Thematic Structure in a Large-ScaleLexacon for Mandann Chinese Tlurd Conference ofthe Association for Machine Translation m theAmericas, AMTA-98 Langhorne, PAReferencesAtlans, B T S (1991) Bmldmga lexicon Thecontribution of lexicography lnternattonal Journalof Lextcography, 4(3), 167-204CL Research (1999a) CL Research Demoshttp//www clres com/Demo htmlCL Research (1999b) Dmtlonary ParsingProject http//www clres com/dpp htmlDolan, W B (1994, 5-9 Aug) Word SenseAmblguation Chistenng Related SensesCOLING-94, The 15th International Conference onComputational Linguistics Kyoto, JapanGreen, S J (1997) Automatically generatinghypertext by computing semantic smulanty \[Dlss\],Toronto, Canada Umverstty of TorontoGreen, S J (Sjgreen@mn mq edu au) (1999, 1June) (Rich semantic networks)Hovy, E (1998, May) Combining andStandardizing Large-Scale, Practical Ontologms forMachine Translation and Other Uses LanguageResources and Evaluation Conference Granada,SpamKalgarnff, A (1998) SENSEVAL Home Pagehttp//www itn bton ac uk/events/senseval/Krovetz, R (1992, June) Sense-Linking m aMachine Readable Dictionary 30th Annual Meetingof the Association for Computational Lmgu~sticsNewark, Delaware Association for ComputationalLmgtusticsLesk, M (1986) Automatic SenseDlsamblguation Using Machine ReadableDmttonanes How to Tell a Pine Cone from an IceCream Cone Proceechngs ofSIGDOCLttkowski, K C (1978) Models of the semanticstructure of dictionaries American Journal ofComputattonal Lmgutsttcs, Atf 81, 25-7437
