Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1382?1391, Dublin, Ireland, August 23-29 2014.From neighborhood to parenthood: the advantages of dependencyrepresentation over bigrams in Brown clusteringSimon?SusterUniversity of GroningenNetherlandss.suster@rug.nlGertjan van NoordUniversity of GroningenNetherlandsg.j.m.van.noord@rug.nlAbstractWe present an effective modification of the popular Brown et al.
1992 word clustering algorithm,using a dependency language model.
By leveraging syntax-based context, resulting clusters arebetter when evaluated against a wordnet for Dutch.
The improvements are stable across parameterssuch as number of clusters, minimum frequency and granularity.
Further refinement is possiblethrough dependency relation selection.
Our approach achieves a desired clustering quality withless data, resulting in a decrease in cluster creation times.1 IntroductionSemi-supervised approaches have been successful in various areas of natural language processing.
Amonga plethora of clustering techniques, Brown clustering (Brown et al., 1992) is popular for its conceptualsimplicity, available implementations (Liang, 2005; Stolcke, 2002), and because the resulting wordclusters can be helpful for several tasks.
Clusters are used as syntactic and semantic generalizations ofwords, requiring fewer model parameters.Brown clustering (section 2) groups words based on shared context.
However, only immediatelyadjacent words are taken into account as recognized e.g.
by Koo et al.
(2008), Sagae and Gordon (2009),and Grave et al.
(2013).
For example, even though verbs constitute an informative context for object nouns,they are rarely considered in Brown clustering, unlike in dependency-based clustering.
The differencebetween the contexts can be illustrated with the following example:The method repeatedly samples the databigram contextsdependency contextsThe bigram context thus fails to capture the relation between the object data and the predicate samples, aswell as the one between the subject method and the predicate.
Furthermore, the dependency representationrightly ignores some of the less informative contexts coming from immediately adjacent words.
Forexample, there is no relation between the predicate samples and the article the to the right.It might be preferable therefore to induce word clusters based on the dependency relations in whichthe words occur.
In section 3, we present how this relates to Brown clustering, and we modify the codeby Percy Liang, so that dependency clustering can be used.
We evaluate clusters in a wordnet-basedsimilarity experiment.
Dependency clustering yields superior clusters for Dutch across different settingsof parameters such as number of clusters, frequency threshold and level of granularity.
Selecting specificdependency relation labels and using data obtained from them as input to clustering further improves theclustering quality.
The proposed adaptation of Brown clustering does not change the complexity of theThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1382algorithm, and?although we assume that syntactically parsed text is available?it requires much less datafor a desired level of clustering quality.2 The Brown clustering algorithmBrown clustering (Brown et al., 1992) is an agglomerative algorithm that induces a hierarchical clusteringof words.
It takes a tokenized corpus and groups words into k clusters identified by bit strings, representingpaths in the induced binary tree in which the leaves are word clusters.
Prefixes of the paths can be used toachieve clusters of coarser granularity (Sun et al., 2011; Turian et al., 2010).
The obtained clusters containwords that are semantically related, or are paradigmatic or orthographic variants.1The algorithm starts by putting k most frequent words into distinct clusters.
Then, the k+1thmostfrequent word is assigned to a new cluster, and two among the resulting k+1 clusters are merged, i.e.
thepair that maximizes the average mutual information of the current clustering.
This process is repeateduntil all words have been merged.
The resulting k clusters are then merged to build the binary tree.
Theversion of the algorithm optimized for speed runs in O(k2|V|), with |V| the vocabulary size.Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008;Candito and Crabb?e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turianet al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013),unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al.,2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi etal., 2013).
Recently, multilingual clustering has also been proposed (T?ackstr?om et al., 2012; Faruqui andDyer, 2013).Among the most frequently recognized limitations (cf.
Koo et al.
(2008); Chrupala (2011)) are a) thehard nature of the clustering, b) relatively long running time2and c) insensitivity to wider context.
Ourmethod attempts to overcome the final disadvantage.
As it requires less data, it also reduces the runningtime.Leveraging syntactic context for word representations has been explored, among others, in Lin (1998)on distributional thesauri; Haffari et al.
(2011) on combining Brown clusters and word groupings from splitnon-terminals; Sagae and Gordon (2009) on using unlexicalized syntactic context in hierarchical clustering;Van de Cruys (2010) and Pad?o and Lapata (2007) on comparison of window- and syntactic-based wordspace models; and Boyd-Graber and Blei (2008) on syntactic topic models.The work closest to ours is that of Grave et al.
(2013).
The authors show that clusters obtained fromdependency trees outperform standard Brown clustering when used as features in super-sense taggingand NER.
Their focus is on a generalization of Brown clustering with Hidden Markov models (extendingMarkov chains to trees), allowing the creation of soft clusters.3Learning and inference are done withonline expectation-maximization and belief propagation.Whereas Grave et al.
focus on new learning methods for clustering with HMMs on dependency trees,we take an in-depth look at parameters and choices that are standardly considered using the (Brown etal., 1992) algorithm.
We show that the advantage of dependency clustering can be observed throughoutdifferent parametrizations of cluster capacity, granularity level, frequency thresholding and other criteria(section 6), and that the advantage is roughly constant for varying amounts of input data.
Finally, weprovide new insight in the advantage of selective dependency clustering, in which the data obtained onlyfrom specific dependency relations lead to better clusters.
Our approach constitutes a straightforwardextension of Brown clustering, and only required a simple modification of the Brown clustering code.1We are using the term semantic relatedness in its broadest possible scope.
Words or clusters are semantically related whenthey have any kind of semantic relation: synonymy, meronymy, antonymy, hypernymy etc.
(Turney and Pantel, 2010).2Although coarser clustering (k<1000) can mean more practical running times, as the clustering depends quadratically on k.3This approach allows to capture homonymy/polysemy, with the idea that when a word representation is needed, it canbe obtained in a context-sensitive way (Huang et al., 2011; Nepal and Yates, 2014).
This is certainly an important advantageover Brown clustering in which the mapping between a word and a cluster is deterministic; however, it comes with its owndisadvantages: creating context-sensitive representations requires (potentially) costly inference; furthermore, HMM-basedclustering does not build nor lends itself easily to a hierarchy, which is often exploited during feature creation in supervisedlearning to control cluster granularity (see the end of section 5.2)13833 Extension of the Brown clusteringThe bigram language model underlying Brown clustering takes the probability of a sentence as theproduct of probabilities of words based on immediately preceding words.
In contrast, we replace thisby a dependency language model (DLM), which defines the probability of a sentence over dependencytrees (Shen et al., 2008).
This probability can be factorized in different ways (Chen et al., 2012; Charniak,2001; Popel and Mare?cek, 2010), but the common idea is that a word is conditioned on some history,where the link between the two is a dependency.
In practice, the history can include the immediate parentof the word, which can be either a lexical head or the artificial root node, as well as siblings between thechild and the parent.
Our take on DLM is similar to Charniak (2001) and Popel and Mare?cek (2010): theprobability of a word is conditioned simply on its parent.
This is the same view as taken by Grave etal.
(2013).The Brown clustering objective is to find such a deterministic clustering function C mapping each wordfrom the vocabulary V to one of K clusters that maximizes the likelihood of the data.
The likelihood of asequence of word tokens, w = ?wi?mi=1, with each wi?
V , factors asL(w; C) =m?i=1p(C(wi)|C(wi?1))p(wi|C(wi)), (3.1)where C(w0) is a special start-of-sequence symbol.
As shown by Brown et al.
(1992), by taking thenegative logarithm and using the ML estimates, the equation 3.1 is decomposed to the negative entropy ofthe sequence w and mutual information between adjacent clusters.
Since the entropy is independent ofthe clustering function, the objective amounts to finding such C that maximizes the mutual information.For dependency clustering, we change the cluster transition probability so that conditioning is on thecluster of the parent of the word at position i, instead of on the cluster of the previous word:L?
(w; C) =m?i=1p(C(wi)|C(wpi(i)))p(wi|C(wi)), (3.2)where i ranges over all children in a tree and pi is a function from the children to their unique parents(which include the special root of the tree).
Calculation of the mutual information changes only tothe extent that count tables no longer represent adjacency relationship (bigrams) between words butparenthood (child?parent relation).4 Evaluation taskWe evaluate our word clusters by following the method of Van de Cruys (2010) for evaluating vector spacemodels.
The method is based on a wordnet for Dutch and assumes that two semantically related wordsalso occur close to each other in the wordnet hierarchy.4We use Cornetto (Vossen et al., 2013), whichincludes more than 92,000 form-POS pairs described in terms of lexical units, synsets and other criteria.For calculating similarity scores, we treat Cornetto as a digraph, with nodes constituting synsets and arcsconstituting hypernymic relations, and adopt the Lin similarity measure (Lin, 1998)5in combination withthe ontological variant of Information Content6.Evaluation is guided by a list of 10,000 most frequent words from SoNaR, a 500M-word referencecorpus for Dutch.7Every word is compared to other words in the same cluster, and the average similarityfor all comparisons is taken as the final score.
The described method is well suited for measuringintracluster quality, yet useful information about word similarity is available also by looking at neighboring4For English, several semantic similarity datasets are available (such as WordSimilarity-353 (Finkelstein et al., 2001)), someof which can identify the type of relatedness captured.
We are not aware of such datasets for Dutch.5Which is a function of the IC of the least common subsumer of two synsets and the IC of individual synsets.
The scoreranges between 0 and 1.6Which is the negative logarithm of (|L|+ 1)?1((|Ls|/|Ss|) + 1), where L are the leaves of the hierarchy, Lsare the leavesreachable from a synset s, and Ssare the subsumers of s (S?anchez et al., 2011).7http://lands.let.ru.nl/projects/SoNaR1384clusters in the binary tree.
This intercluster quality, according to which clusters that are close in thebinary tree are more similar than clusters that are far apart, can be captured indirectly by evaluating usingdifferent bit substrings.
In this way, when a substring is used, two or more semantically related, butisolated clusters are merged, which should result in a drop in clustering quality (semantic relatednesstends to ?dissolve?
when merging).For both standard and dependency Brown clustering, the same set of sentences is used.
From SoNaR,we sampled sentences amounting to roughly 46M words, which is comparable to the count for Englishdatasets of Koo et al.
(2008) and Turian et al.
(2010).
The sentence length was restricted to five or morewords to exclude noisy text.
Corpus annotation was removed.For dependency clustering, the dataset was lemmatized and parsed with the Alpino parser (Van Noord,2006), an HPSG parser with a maxent disambiguation component, achieving labeled dependency accuracyof around 90.5 for Dutch.8The parsing accuracy is likely to be lower on our dataset, but we expect thiseffect to be small since Alpino has been shown to be relatively insensitive to domain shifts comparedto some entirely data-driven parsers (Plank and van Noord, 2010).
For default clustering, we only usefirst-order dependencies produced by the parser.
The bilexical counts (head and dependent regardless ofthe relation label) serve as input for dependency clustering.5 Experiments and ResultsThe main parameter for word clustering is the number of clusters k, which we set to either 1000 or 3200,9except when measuring clustering capacity, for which smaller values of k are used.
Additionally, we limitthe minimum frequency of words in clustering to three, unless stated otherwise.
The vocabulary size fork=1000 clustering with applied frequency threshold is around 237,000.
We use a paired t-test to check forstatistical significance of observed differences in means.5.1 Cluster examplesIn Table 1, we show both the versatility of dependency clusters by dividing the examples in five groups(A?E), and the similarity of clusters within group.
The longer the common bit substring between clusters,the closer they are in the hierarchy.
Group A includes words describing professions or people?s rolesand functions.
Group B lists personal pronouns, including reflexive pronouns (B2), where substantialdifferentiation exists with many singleton clusters.
Clusters are capable of grouping orthographic variants(D1; email and e-mail) and diminutives (sms DIM, corresponding to Dutch smsje).
Because first and lastnames are extremely common in our corpus, clustering creates fine-grained distinctions between these(C).
C1 groups names of presidents, whereas C2 and C3 distinguish between feminine and masculinenames.
Measurable concepts are included in E.5.2 Cluster qualityTable 2 presents the general quality of standard and dependency clustering.
The results for 1000 and 3200clusters (in the latter we use a higher frequency threshold for faster computation) show that we obtaina higher similarity score for 3200 clusters compared to 1000, and a more marked difference betweenstandard and dependency clustering in the case of k=3200 (?=0.019).
We also looked at how manywords from the frequency list were evaluated successfully.
The recall depends on the success of mappingbetween words and synsets as well as the success of finding the word in one of the clusters.
The latterfactor influences the recall to a much lesser degree, as almost all words are found in the clustering.For 3200 clusters with the minimum frequency set to fifty, approximately 5000 words are successfullyevaluated, whereas for 1000 clusters, this number is around 7000.10These numbers are not affected bythe type of clustering (standard or dependency).8Strictly speaking, the output of lemmatization is root forms.
We perform this preprocessing step to increase the number oftimes that a word is successfully matched in the wordnet hierarchy and evaluated.9Which are standardly encountered throughout the literature.
For k above 3200, the algorithm falls short of practicality oncurrent hardware assuming a single-core implementation.10The difference between the figures occurs because of a different frequency threshold.1385Group Cluster id Most frequent words LeftA1 001010001011100aannemer,contractor,huis arts,family doctor,bakker,baker,notaris,lawyer,apotheker,pharmacist,makelaarestate agent+57A2 001010001011011analist,analyst,criticus,reviewer,waarnemer,observer,kenner,expert,commentator,commentator,mens recht organisatiehuman rights organization+8A3 0010100010111110ondernemer,entrepreneur,zakenman,businessman,bedrijf leider,manager,zelfstandige,self-employed,koopman,merchant,starterstarter+18B1 011101111011110mijme0B2 01110111101110zichzelf,him/herself,mezelf,myself,jezelf,yourself,onszelf,ourselves,mijzelf,myself,uzelfyourself0B3 01110111101101hemhim0B4 01110111101100henthem0C1 00110010010Bush,Bush,Obama,Obama,Clinton,Clinton,Poetin,Putin,Chirac,Chirac,SarkozySarkozy+95C2 0011000111010Sarah, Kim, Nathalie, Justine, Kirsten, Tia, Eline+12C3 0011000111011David, Jimmy, Benjamin, Samuel, Tommy, Sean+98D1 001011100010101email, mail, sms, sms DIM, e-mail, mail DIM+13D2 001011100010100telefoon,telephone,satelliet,satellite,telefonie,telephony,telefoon lijn,telephone line,Explorer,Explorer,muziek speler,music player,iTunesiTunes+7E 001000010110101inkomen,income,energie verbruik,energy consumption,minimum loon,minimum wage,cholesterol,cholesterol,opleidingsniveau,level of education,IQ,IQ,alcohol gehaltealcohol content+32Table 1: Example dependency clusters obtained from a run with number of clusters set to 3200 andminimum frequency to 50.
The underlined part of the bit string indicates the longest common substringwithin one group.
English translation of the Dutch original is given in italics and is left out when clearfrom the original.
Column Left indicates the remaining number of (less frequent) words in the cluster.k Brown DepBrown ?1000 0.191 0.196 +.005*3200 0.279 0.298 +.019**Table 2: Lin similarity scores for standard Brown clustering and dependency Brown clustering (DepBrown),with k the number of clusters.
?=DepBrown ?
Brown.
Frequency threshold of 50 is used for clusteringwith k = 3200.
*: statistically significant with p < 0.05, **: statistically significant with p < 0.001.Results for four different clustering parametrizations are shown in Table 3.
One way of controlling thegranularity is to choose the number of output clusters k. As shown in the table under CAP (?capacity?
),dependency clustering achieves a better quality regardless of the choice of k, and in general, choosinga smaller k decreases quality, which is compatible with the observations of Turian et al.
(2010) in theirchunking experiments.An effect similar to that of controlling capacity can be achieved by making use of the fact that theinduced structure is a hierarchy.11By choosing a path prefix length that is shorter than the maximumlength, we control the cluster granularity (denoted in the table as PREF-*).
For different tasks, differentpath prefixes might be appropriate (Sun et al., 2011; Koo et al., 2008; Miller et al., 2004).
For example,one might prefer coarser distinctions (i.e.
shorter bit strings) in parsing, while finer granularity might benecessary to obtain effective representations of proper names in NER.
We ran the experiment with prefixlength ranging from one to eighteen, and show a selection of four settings in the table.
Across the board,dependency clustering yields better results than standard clustering.
Naturally, with shorter prefixes thequality decreases, which is explained by increasing word population in the clusters, with more and more11The parameter k needs to be chosen before clustering, whereas the hierarchical structure can be exploited during featurepreparation based on already existing clusters.1386Setting k min Brown DepBrown ?CAP200 10 0.148 0.157 +.009400 10 0.169 0.175 +.006600 10 0.182 0.191 +.009800 10 0.191 0.205 +.014PREF-16 1000 10 0.2 0.215 +.015PREF-12 1000 10 0.187 0.202 +.015PREF-8 1000 10 0.159 0.168 +.009PREF-4 1000 10 0.114 0.127 +.013FREQ1000 5 0.196 0.204 +.0081000 10 0.202 0.216 +.0141000 20 0.206 0.221 +.0151000 30 0.209 0.224 +.0151000 50 0.216 0.227 +.011NOUNS 1000 3 0.272 0.279 +.007Table 3: Lin similarity scores for standard Brown clustering and dependency Brown clustering (DepBrown),with k the number of clusters, min the minimum frequency of words.
CAP: varying k, fixed min; FREQ:varying min, fixed k; NOUNS: evaluating only nouns, PREF-n: size of bit-string prefix, ?=DepBrown ?Brown.
All the results reported for DepBrown are significantly different from Brown with p < 0.001.distant (both hierarchically and semantically) clusters being merged.By inspecting individual clusters, we observe that frequent words in a cluster exhibit clear semanticrelatedness, but that rare words are often semantically quite unrelated.12This is confirmed by our resultsin which the quality of the clustering improves approximately logarithmically with frequency thresholdincreasing (FREQ).
The margin between standard and dependency clustering is also increasing as weincrease the threshold.
In practice, Brown clusters appear to be equally useful with a high frequencythreshold (Owoputi et al., 2013) as without thresholding (Koo et al., 2008; Turian et al., 2010).We also investigate the quality of nouns only, to facilitate the comparison to Van de Cruys (2010).
Weobserve a considerable gain in quality when only nouns are used compared to using all parts of speech?
the Lin score is increased by 0.08.
In the noun-only evaluation, dependency clustering achieves ahigher score (0.279) than standard clustering (0.272).
Van de Cruys (2010) shows that syntactic vectorspace models outperform window-based models, which is confirmed by our finding for word clusteringas well.
In his work, syntactic vector space models yield a 0.04 advantage in Lin score, whereas ourdependency clusters achieve a less marked advantage, reaching up to 0.019 in Lin score.
A possibleexplanation for this difference is that in his evaluation an average over only five most similar nouns istaken, whereas we impose no such restriction.
We would like to point out that our work does not aim tocompare and discuss the merits of clustering and vector space models as possible techniques for obtainingword representations, but rather to provide a comprehensive comparison of standard Brown clustering andits dependency extension.5.3 Learning curvesFigure 5.2 shows the amount of data needed to achieve a certain quality of clustering.
For clustering onten thousand sentences the similarity score is around 0.14, with a higher score for standard clustering.For each subsequent addition of data, dependency clustering outperforms standard clustering.
In order toachieve the highest score attained by standard clustering (0.19), resulting from clustering on 2.4 millionsentences (41 million words), dependency clustering requires only slightly more than 500 thousandsentences (8.5 million words).
This observation is advantageous especially because less data means12Although cf.
Turian et al.
(2010) who show that Brown clustering has a superior representation for rare words than neuralword embeddings in their experiment.1387?
= 1.9Mllllll ll l l lll l0.140.160.180.2010K 50K 100K 500K 1M ALL(2.7M)Number of sentencesAveragequality(Lin)lStandard BrownDepBrown (this paper)Figure 1: Learning curves for standard and dependency Brown clustering with 1000 clusters and afrequency threshold of 3.
Dashed line displays the difference in amount of data needed for DepBrown toachieve the best quality of Brown.
Using all, 2.7 million sentences from the corpus (ALL) corresponds to46 million words.shorter running time for clustering as the number of word types is reduced.5.4 Refinement of dependency clustersOur dependency clustering described in the previous sections operates on words appearing in all depen-dency relations.
We now investigate whether selecting only a particular dependency relation?i.e.
usingas the input both parent and child words from that dependency relation?leads to clusters with highersemantic relatedness.
Each relation can be characterized as either a first- or a second-order relation.13A second-order relation is between two words with an intervening preposition, e.g.
between a verb anda noun of a directional complement introduced by a preposition, such as in the Dutch ?eten achter pc?
(?eating at the computer?
).14We ran clustering for each of the forty-five dependency relations separatelyand measured the quality of each resulting clustering.
The cumulative baseline that does not distinguishbetween dependency relations is given as ALL for first-order relations in Table 4.
This is the same resultas reported on the first line in Table 2.
The addition of second-order dependencies does not change theclustering quality of the baseline (0.196) but increases the number of types.In the upper part of Table 4, we list six relations leading to clustering quality above the baseline.13The experiments in previous sections included only first-order relations.14The preposition should be seen only as an implicit link between two words and is not included in the input data for clustering.For the example fragment only ?eating?
and ?computer?
constitute the data instance actually used by the algorithm.1388Type Ord-1 Ord-2 DepBrown PopulationOBJ2  0.238 1,622LD  0.233 2,419PC  0.211 21,157LD  0.208 12,149OBJ1  0.203 108,037SU  0.199 79,844ALL  0.196 495,479ALL   0.196 559,908SU+OBJ1  0.202 156,645Table 4: Lin similarity scores for dependency Brown clustering (DepBrown) per type of dependency rela-tion.
Ord-1: first-order relation; Ord-2: second-order relation (with intervening preposition); Population:number of word types in the clustering.Two conclusions can be drawn from the results on these relations.
First, some dependency relationscontribute better context that leads to increased semantic relatedness compared to clustering withoutrelation selection.
Second, both first- and second-order relations appear among the relations outperformingthe baseline.
The highest score from the top six relations is achieved by taking words exclusively from thesecond-order secondary object (OBJ2) relation.
However, relatively few word types are included in theclusters.
The same is true for the first-order directional complements (LD).
Of course, clustering withonly one of these relations would have quite limited applicability if used in a supervised NLP task dueto the low number of word types.
However, the main point we want to make here is that these relationsyield semantically superior clusters and demonstrate that syntactic functions truly merit further attentionin learning semantic clusters using syntax.
The remaining four among the top six relations are morefrequent relations, and lead to clusterings with higher number of word types.
These are the second-orderprepositional complement (PC) and directional complement (LD) relations, and the first-order directobject (OBJ1) and subject (SU) relations.
Finally, the setting SU+OBJ1 joins words obtained from subjectand direct object relations, and achieves a quality that falls between the values obtained for the tworelations separately, yet still increases the number of word types.6 Conclusion and future workWe have presented a detailed study on a simple extension of Brown clustering with a dependency languagemodel.
In the first part, we have consolidated the advantage of dependency clustering over standardBrown clustering in a series of experiments, including cluster capacity, granularity level, frequencythresholding, amount of data and other.
In the second part, we put forward the idea of selective clusteringusing data obtained only from specific dependency relations.
Several relations lead to a clustering withimproved intracluster similarity.
We make the code as well as the induced clusters freely available athttps://github.com/rug-compling/dep-brown-cluster.Our findings from the selective clustering warrant the development of more complex models capable ofincluding syntactic functions for obtaining semantic clusters.
We reserve this work for the future.
We findit interesting to apply dependency Brown clustering to languages of different families and compare it inthis setting to the standard Brown clustering.
The future work further includes a study of the effect ofdependency clusters in downstream tasks.
Another important point is the effect of parser accuracy on thequality of obtained clusters.AcknowledgmentsThanks to C?a?gr?
C?
?oltekin, Gregory Mills, Olga Yeroshina and the anonymous reviewers for valuablesuggestions, and to Percy Liang for implementation-related comments.1389ReferencesJordan Boyd-Graber and David M. Blei.
2008.
Syntactic topic models.
In NIPS.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-basedn-gram models of natural language.
Computational Linguistics, 18(4):467?479.Marie Candito and Beno?
?t Crabb?e.
2009.
Improving generative statistical parsing with semi-supervised wordclustering.
In IWPT.Eugene Charniak.
2001.
Immediate-head parsing for language models.
In ACL.Wenliang Chen, Min Zhang, and Haizhou Li.
2012.
Utilizing dependency language models for graph-baseddependency parsing models.
In ACL.Grzegorz Chrupala.
2011.
Efficient induction of probabilistic word classes with LDA.
In IJCNLP.Manaal Faruqui and Chris Dyer.
2013.
An information theoretic approach to bilingual word clustering.
In ACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2001.
Placing search in context: The concept revisited.
In WWW.Edouard Grave, Guillaume Obozinski, and Francis Bach.
2013.
Hidden Markov tree models for semantic classinduction.
In CoNLL.Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar.
2011.
An ensemble model that combines syntactic andsemantic clustering for discriminative dependency parsing.
In ACL.Fei Huang, Alexander Yates, Arun Ahuja, and Doug Downey.
2011.
Language models as representations forweakly-supervised nlp tasks.
In CoNLL.Terry Koo, Xavier Carreras, and Michael Collins.
2008.
Simple Semi-supervised Dependency Parsing.
In Pro-ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio.Percy Liang.
2005.
Semi-supervised learning for natural language.
Master?s thesis, Massachusetts Institute ofTechnology.Dekang Lin.
1998.
An information-theoretic definition of similarity.
In ICML.Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.
Name tagging with word clusters and discriminativetraining.
In HLT-NAACL.Saeedeh Momtazi, Sanjeev Khudanpur, and Dietrich Klakow.
2010.
A comparative study of word co-occurrencefor term clustering in language model-based sentence retrieval.
In ACL-HLT.Anjan Nepal and Alexander Yates.
2014.
Factorial Hidden Markov models for learning representations of naturallanguage.
In ICLR.Gertjan Van Noord.
2006.
At Last Parsing Is Now Operational.
In TALN.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conversational text with word clusters.
In HLT-NAACL.Sebastian Pad?o and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Computa-tional Linguistics, 33:161?199.Barbara Plank and Alessandro Moschitti.
2013.
Embedding semantic similarity in tree kernels for domain adapta-tion of relation extraction.
In ACL.Barbara Plank and Gertjan van Noord.
2010.
Grammar-driven versus data-driven: Which parsing system is moreaffected by domain shifts?
In NLPLING Workshop.Kashyap Popat, Balamurali A.R, Pushpak Bhattacharyya, and Gholamreza Haffari.
2013.
The haves and thehave-nots: Leveraging unlabelled corpora for sentiment analysis.
In ACL.Martin Popel and David Mare?cek.
2010.
Perplexity of n-gram and dependency language models.
In TSD.Kenji Sagae and Andrew S. Gordon.
2009.
Clustering words by syntactic similarity improves dependency parsingof predicate-argument structures.
In IWPT.1390David S?anchez, Montserrat Batet, and David Isern.
2011.
Ontology-based information content computation.Knowledge-Based Systems, 24(2):297?303.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
A new string-to-dependency machine translation algorithmwith a target dependency language model.
In ACL.Yongzhe Shi, Wei-Qiang Zhang, Jia Liu, and Michael Johnson.
2013.
Rnn language model with word clusteringand class-based output layer.
EURASIP Journal on Audio, Speech, and Music Processing, (1).Andreas Stolcke.
2002.
SRILM?an extensible language modeling toolkit.
In ICSLP.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.
Semi-supervised relation extraction with large-scale wordclustering.
In HLT-ACL.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkoreit.
2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In HLT-NAACL.Ivan Titov and Alexandre Klementiev.
2012.
A Bayesian approach to unsupervised semantic role induction.
InEACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In ACL.Peter D. Turney and Patrick Pantel.
2010.
From frequency to meaning: vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.Tim van de Cruys.
2010.
Mining for Meaning: The Extraction of Lexico-semantic Knowledge from Text.
Ph.D.thesis, University of Groningen.Piek Vossen, Isa Maks, Roxanne Segers, Hennie van der Vliet, Marie-Francine Moens, Katja Hofmann, ErikTjong Kim Sang, and Maarten de Rijke, editors, 2013.
Cornetto: A Combinatorial Lexical Semantic Databasefor Dutch.
Springer.1391
