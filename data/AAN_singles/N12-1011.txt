2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103?111,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsExploring Content Features for Automated Speech ScoringShasha Xie, Keelan Evanini, Klaus ZechnerEducational Testing Service (ETS)Princeton, NJ 08541, USA{sxie,kevanini,kzechner}@ets.orgAbstractMost previous research on automated speechscoring has focused on restricted, predictablespeech.
For automated scoring of unrestrictedspontaneous speech, speech proficiency hasbeen evaluated primarily on aspects of pro-nunciation, fluency, vocabulary and languageusage but not on aspects of content and topi-cality.
In this paper, we explore features repre-senting the accuracy of the content of a spokenresponse.
Content features are generated us-ing three similarity measures, including a lex-ical matching method (Vector Space Model)and two semantic similarity measures (LatentSemantic Analysis and Pointwise Mutual In-formation).
All of the features exhibit moder-ately high correlations with human proficiencyscores on human speech transcriptions.
Thecorrelations decrease somewhat due to recog-nition errors when evaluated on the output ofan automatic speech recognition system; how-ever, the additional use of word confidencescores can achieve correlations at a similarlevel as for human transcriptions.1 IntroductionAutomated assessment of a non-native speaker?sproficiency in a given language is an attractive ap-plication of automatic speech recognition (ASR) andnatural language processing (NLP) technology; thetechnology can be used by language learners forindividual practice and by assessment providers toreduce the cost of human scoring.
While muchresearch has been done about the scoring of re-stricted speech, such as reading aloud or repeatingsentences verbatim (Cucchiarini et al, 1997; Bern-stein et al, 2000; Cucchiarini et al, 2000; Witt andYoung, 2000; Franco et al, 2000; Bernstein et al,2010b), much less has been done about the scor-ing of spontaneous speech.
For automated scor-ing of unrestricted, spontaneous speech, most auto-mated systems have estimated the non-native speak-ers?
speaking proficiency primarily based on low-level speaking-related features, such as pronuncia-tion, intonation, rhythm, rate of speech, and fluency(Cucchiarini et al, 2002; Zechner et al, 2007; Chenet al, 2009; Chen and Zechner, 2011a), although afew recent studies have explored features based onvocabulary and grammatical complexity (Zechner etal., 2007; Bernstein et al, 2010a; Bernstein et al,2010b; Chen and Zechner, 2011b).To date, little work has been conducted on au-tomatically assessing the relatively higher-level as-pects of spontaneous speech, such as the contentand topicality, the structure, and the discourse in-formation.
Automated assessment of these aspectsof a non-native speaker?s speech is very challeng-ing for a number of reasons, such as the short lengthof typical responses (approximately 100 words fora typical 1 minute response, compared to over 300words in a typical essay/written response), the spon-taneous nature of the speech, and the presence ofdisfluencies and possible grammatical errors.
More-over, the assessment system needs text transcriptsof the speech to evaluate the high level aspects, andthese are normally obtained from ASR systems.
Therecognition accuracy of state-of-the-art ASR sys-tems on non-native spontaneous speech is still rel-atively low, which will sequentially impact the re-103liability and accuracy of automatic scoring systemsusing these noisy transcripts.
However, despite thesedifficulties, it is necessary for an automated assess-ment system to address the high level informationof a spoken response in order to fully cover all as-pects that are considered by human raters.
Thus, inthis paper we focus on exploring features to repre-sent the high-level aspect of speech mainly on theaccuracy of the content.As a starting point, we consider approaches thathave been used for the automated assessment of con-tent in essays.
However, due to the qualitative dif-ferences between written essays and spontaneousspeech, the techniques developed for written textsmay not perform as well on spoken responses.
Still,as a baseline, we will evaluate the content featuresused for essay scoring on spontaneous speech.
Inaddition to a straightforward lexical Vector SpaceModel (VSM), we investigate approaches using twoother similarity measures, Latent Semantic Analysis(LSA) and Pointwise Mutual Information (PMI), inorder to represent the semantic-level proficiency ofa speaker.
All of the content features are analyzedusing both human transcripts and speech recognizeroutput, so we can have a better understanding of theimpact of ASR errors on the performance of the fea-tures.
As expected, the results show that the per-formance on ASR output is lower than when hu-man transcripts are used.
Therefore, we propose im-proved content features that take into account ASRconfidence scores to emphasize responses whose es-timated word accuracy is comparatively higher thanothers.
These improved features can obtain similarperformance when compared to the results using hu-man transcripts.This paper is organized as follows.
In the nextsection we introduce previous research on auto-mated assessment of content in essays and spokenresponses.
The content features we generated andthe model we used to build the final speaking scoresare described in Sections 3 and Section 4, respec-tively.
In Section 5 we show the performance ofall our proposed features.
Finally, we conclude ourwork and discuss potential future work in Section 6.2 Related WorkMost previous research on assessment of non-nativespeech has focused on restricted, predictable speech;see, for example, the collection of articles in (Es-kenazi et al, 2009).
When assessing spontaneousspeech, due to relatively high word error rates ofcurrent state-of-the-art ASR systems, predominantlyfeatures related to low-level information have beenused, such as features related to fluency, pronuncia-tion or prosody (Zechner et al, 2009).For scoring of written language (automated essayscoring), on the other hand, several features relatedto the high level aspects have been used previously,such as the content and the discourse information.In one approach, the lexical content of an essay wasevaluated by using a VSM to compare the wordscontained in each essay to the words found in a sam-ple of essays from each score category (Attali andBurstein, 2006).
In addition, this system also usedan organization feature measuring the difference be-tween the ideal structure of an essay and the actualdiscourse elements found in the essay.
The featuresdesigned for measuring the overall organization ofan essay assumed a writing strategy that includedan introductory paragraph, at least a three-paragraphbody with each paragraph in the body consisting ofa pair of main point, supporting idea elements, anda concluding paragraph.
In another approach, thecontent of written essays were evaluated using LSAby comparing the test essays with essays of knownquality in regard of their degree of conceptual rele-vance and the amount of relevant content (Foltz etal., 1999).There has been less work measuring spoken re-sponses in terms of the higher level aspects.
In(Zechner and Xi, 2008), the authors used a contentfeature together with other features related to vocab-ulary, pronunciation and fluency to build an auto-mated scoring system for spontaneous high-entropyresponses.
This content feature was the cosine wordvector product between a test response and the train-ing responses which have the highest human score.The experimental results showed that this featuredid not provide any further contribution above abaseline of only using non-content features, andfor some tasks the system performance was evenslightly worse after including this feature.
However,104we think the observations about the content featuresused in this paper were not reliable for the followingtwo reasons: the number of training responses waslimited (1000 responses), and the ASR system had arelatively high Word Error Rate (39%).In this paper, we provide further analysis on theperformance of several types of content features.Additionally, we used a larger amount of trainingdata and a better ASR system in an attempt to extractmore meaningful and accurate content features.3 Automatic Content ScoringIn automatic essay scoring systems, the content of anessay is typically evaluated by comparing the wordsit contains to the words found in a sample of es-says from each score category (1-4 in our experi-ments), where the scores are assigned by trained hu-man raters.
The basic idea is that good essays willresemble each other in their word choice, as willpoor essays.
We follow this basic idea when extract-ing content features for spoken responses.3.1 Scoring FeaturesFor each test spoken response, we calculate its simi-larity scores to the sample responses from each scorecategory.
These scores indicate the degree of simi-larity between the words used in the test responseand the words used in responses from different scorepoints.
Using these similarity scores, 3 content fea-tures are generated in this paper:?
Simmax: the score point which has the high-est similarity score between test response andscore vector?
Sim4: the similarity score to the responseswith the highest score category (4 in our ex-periments).?
Simcmb: the linear combination of the similar-ity scores to each score category.4?i=1wi ?
Simi (1)where wi is scaled to [-1, 1] to imply its positiveor negative impact.3.2 Similarity MeasuresThere are many ways to calculate the similarity be-tween responses.
A simple and commonly usedmethod is the Vector Space Model, which is alsoused in automated essay scoring systems.
Under thisapproach, all the responses are converted to vectors,whose elements are weighted using TF*IDF (termfrequency, inverse document frequency).
Then, thecosine similarity score between vectors can be usedto estimate the similarity between the responses thevectors originally represent.Other than this lexical matching method, we alsotry two additional similarity measures to better cap-ture the semantic level information: Latent SemanticAnalysis (Landauer et al, 1998) and a corpus-basedsemantic similarity measure based on pointwise mu-tual information (Mihalcea et al, 2006).
LSA hasbeen widely used for computing document similar-ity and other information retrieval tasks.
Under thisapproach, Singular Value Decomposition (SVD) isused to analyze the statistical relationship between aset of documents and the words they contain.
A m?nword-document matrix X is first built, in which eachelement Xij represents the weighted term frequencyof word i in document j.
The matrix is decomposedinto a product of three matrices as follows:X = U?V T (2)where U is an m?m matrix of left-singular vectors,?
is an m?n diagonal matrix of singular values, andV is the n?
n matrix of right-singular vectors.The top ranked k singular values in ?
are kept,and the left is set to be 0.
So ?
is reformulated as ?k.The original matrix X is recalculated accordingly,Xk = U?kVT (3)This new matrix Xk can be considered as asmoothed or compressed version of the original ma-trix.
LSA measures the similarity of two documentsby calculating the cosine between the correspondingcompressed column vectors.PMI was introduced to calculate the semanticsimilarity between words in (Turney, 2001).
It isbased on the word co-occurrence on a large corpus.Given two words, their PMI is computed using:PMI(w1, w2) = log2p(w1&w2)p(w1) ?
p(w2)(4)105This indicates the statistical dependency between w1and w2, and can be used as a measure of the semanticsimilarity of two words.Given the word-to-word similarity, we can calcu-late the similarity between two documents using thefollowing function,sim(D1, D2) =12(?w?
{D1} (maxSim(w,D2) ?
idf(w))?w?
{D1} idf(w)+?w?
{D2}(maxSim(w,D1) ?
idf(w)?w?
{D2} idf(w)))(5)maxSim(w,Di) = maxwj?
{Di}PMI(w,wj)(6)For each word w in document D1, we find the wordin document D2 which has the highest similarityto w. Similarly, for each word in D2, we iden-tify the most similar words in D1.
The similarityscore between two documents is then calculated bycombining the similarity of the words they contain,weighted by their word specificity (i.e., IDF values).In this paper, we use these three similarity mea-sures to calculate the similarity between the test re-sponse and the training responses for each score cat-egory.
Using the VSM method, we convert all thetraining responses in one score category into one bigvector, and for a given test response we calculate itscosine similarity to this vector as its similarity to thatcorresponding score point vector.
For the other simi-larity measures, we calculate the test response?s sim-ilarity to each of the training responses in one scorecategory, and report the average score as its similar-ity to this score point.
We also tried using this av-erage similarity score for the VSM method, but ourexperimental results showed that this average scoreobtained lower performance than using one big vec-tor generated from all the training samples due todata sparsity.
After the similarity scores to each ofthe four score categories are computed, the contentfeatures introduced in Section 3.1 are then extractedand are used to evaluate the speaking proficiency ofthe speaker.4 System ArchitectureThis section describes the architecture of our auto-mated content scoring system, which is shown inFigure 1.
First, the test taker?s voice is recorded,and sent to the automatic speech recognition system.Second, the feature computation module takes theoutput hypotheses from the speech recognizer andgenerates the content features.
The last componentconsiders all the scoring features, and produces thefinal score for each spoken response.Feature?ComputationRecognized?Words?Scoring?Computation?Moduleand?UtterancesFeaturesSpeechScoringModelSpeech?RecognizerScoring?ModelAudioFilesSpeaking?ScoresFigure 1: Architecture of the automated content scoringsystem.While we are using human transcripts of spokenresponses as a baseline in this paper, we want to notethat in an operational system as depicted in this fig-ure, the scoring features are computed and extractedusing the hypotheses from the ASR system, whichexhibits a relatively high word error rate.
Theserecognition errors will sequentially impact the pro-cess of calculating the similarity and computing thecontent scores, and decrease the performance of thefinal speaking scores.
In order to improve the systemperformance in this ASR condition, we explore theuse of word confidence scores from the ASR systemduring feature generation.
In particular, the similar-ity scores between the test response and each scorecategory are weighted using the recognition confi-dence score of the response, so that the scores canalso contain information related to its acoustic accu-racy.
The confidence score for one response is theaverage value of all the confidence scores for eachword contained in the response.
In Section 5, wewill evaluate the performance of our proposed con-tent features using both human transcripts and ASRoutputs, as well as the enhanced content features us-106ing ASR confidence scores.5 Experimental Results5.1 DataThe data we use for our experiments are from theTest of English as a Foreign Language R?
internet-based test (TOEFL iBT) in which test takers respondto several stimuli using spontaneous speech.
Thisdata set contains 24 topics, of which 8 are opinion-based tasks, and 16 are contextual-based tasks.
Theopinion-based tasks ask the test takers to provideinformation or opinions on familiar topics basedon their personal experience or background knowl-edge.
The purpose of these tasks is to measure thespeaking ability of examinees independent of theirability to read or listen to English language.
Thecontextual-based tasks engage reading, listening andspeaking skills in combination to mimic the kindsof communication expected of students in campus-based situations and in academic courses.
Test tak-ers read and/or listen to some stimulus materials andthen respond to a question based on them.
For eachof the tasks, after task stimulus materials and/or testquestions are delivered, the examinees are allowed ashort time to consider their response and then pro-vide their responses in a spontaneous manner withineither 45 seconds (for the opinion-based tasks) or 60seconds (for the contextual-based tasks).For each topic, we randomly select 1800 re-sponses for training, and 200 responses as develop-ment set for parameter tuning.
Our evaluation datacontains 1500 responses from the same English pro-ficiency test, which contain the same 24 topics.
Allof these data are scored on a 0-4 scale by expert hu-man raters.
In our automated scoring system, we usea filtering model to identify responses which shouldhave a score of 0, such as responses with a technicaldifficulty (e.g., equipment problems, high ambientnoise), responses containing uncooperative behaviorfrom the speakers (e.g., non-English speech, whis-pered speech).
So in this paper we only focused onthe responses with scores of 1-4.
Statistics for thisdata set are shown in Table 1.
As the table shows,the score distributions are similar across the train-ing, development, and evaluation data sets.5.2 Speech recognizerWe use an ASR system containing a cross-wordtriphone acoustic model trained on approximately800 hours of spoken responses from the same En-glish proficiency test mentioned above and a lan-guage model trained on the corresponding tran-scripts, which contain a total of over 5 millionwords.
The Word Error Rate (WER) of this systemon the evaluation data set is 33%.5.3 Evaluation metricTo measure the quality of the developed features, weemploy a widely used metric, the Pearson correla-tion coefficient (r).
In our experiments, we use thevalue of the Pearson correlation between the featurevalues and the human proficiency scores for eachspoken response.5.4 Feature performance on transcriptsIn Section 3.1, we introduced three features derivedfrom the similarity between the test responses andthe training responses for each score point.
We firstbuild the training samples for each topic, and thencompare the test responses with their correspondingmodels.
Three similarity measures are used for cal-culating the similarity scores, VSM, LSA, and thePMI-based method.
In order to avoid the impact ofrecognition errors, we first evaluate these similaritymethods and content features using the human tran-scripts.
The Pearson correlation coefficients on theevaluation data set for this experiment are shown inTable 2.
The parameters used during model build-ing, such as the weights for each score category inthe feature Simcmb and the number of topics k inLSA, are all tuned on the development set, and ap-plied directly on the evaluation set.The correlations show that even the simple vec-tor space model can obtain a good correlation of0.48 with the human rater scores.
The featureSimcmb performs the best across almost all the testsetups, since it combines the information from allscore categories.
The PMI-based features outper-form the other two similarity methods when evalu-ated both on all responses or only on the contextual-based topics.
We also observe that the correlationson contextual-based tasks are much higher than onopinion-based tasks.
The reason for this is that107Table 1: Summary statistics of training, development and evaluation data set.Data sets Responses Speakers score avg score sdScore distribution (percentage %)1 2 3 4Train 43200 8000 2.63 0.79 1750 (4.1) 15128 (35.0) 20828 (48.2) 4837 (11.2)Dev 4800 3760 2.61 0.79 215 (4.5) 1719 (35.8) 2295 (47.8) 499 (10.4)Eval 1500 250 2.57 0.81 95 (6.3) 549 (36.6) 685 (45.7) 152 (10.1)Table 2: Pearson correlations of the content features using human transcripts.VSM LSA PMISimmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 SimcmbALL 0.46 0.32 0.48 0.32 0.38 0.45 0.18 0.51 0.53Contextual 0.50 0.51 0.58 0.36 0.55 0.57 0.21 0.57 0.62Opinion 0.37 0.03 0.25 0.29 0.14 0.22 0.06 0.42 0.51the contextual-based tasks are more constrained tothe materials provided with the test item, whereasthe opinion-based tasks are relatively open-ended.Therefore, it is easier for the similarity measures totrack the content, the topics, or the vocabulary usageof the contextual-based topics.
Overall, the best cor-relations are obtained using the feature combiningthe similarity scores to each score category and thePMI-based methods to calculate the similarity.
Here,the Pearson correlations are 0.53 for all responses,and 0.62 for the contextual-based tasks only.We also investigated whether additional perfor-mance gains could be achieved by combining infor-mation from the three different content features tobuild a single overall content score, since the threefeatures may measure disparate aspects of the re-sponse.
The combination model we use is mul-tiple regression, in which the score assigned to atest response is estimated as a weighted linear com-bination of a selected set of features.
The fea-tures are the similarity values to each score category(Simi, i ?
{1, 2, 3, 4}), calcuated using the threesimilairty measures.
In total we have 12 contentfeatures.
The regression model is also built on thedevelopment set, and tested on the evaluation set.The correlation for the final model is 0.60 on allresponses, which is significantly better than the in-dividual models (0.48 for VSM, 0.45 for LSA, and0.53 for PMI).
Compared to results reported in pre-vious work on similar speech scoring tasks but mea-suring other aspects of speech, our correlation re-sults are very competitive (Zechner and Xi, 2008;Zechner et al, 2009).5.5 Feature Performance on ASR outputThe results shown in the previous section were ob-tained using human transcripts of test responses, andwere reported in order to demonstrate the meaning-fulness of the proposed features.
However, in prac-tical automated speech scoring systems, the onlyavailable text is the output of the ASR system, whichmay contain a large number of recognition errors.Therefore, in this section we show the performanceof the content features extracted using ASR hy-potheses.
Note that we still use the human tran-scripts of the training samples to train the models,the parameter values and the regression weights;however, we only use ASR output of the evaluationdata for testing the feature performance.
These cor-relations are shown in Table 3.Compared to the results in Table 2, we find thatthe VSM and LSA methods are very robust to recog-nition errors, and we only observe slight correlationdecreases on these features.
However, the decreasefor the PMI-based method is quite large.
A possi-ble reason for this is that this method is based onword-to-word similarity computed on the trainingdata, so the mismatch between training and evalu-ation set likely has a great impact on the computa-tion of the similarity scores, since we train on humantranscripts, but test using ASR hypotheses.
Likelyfor the same reason, the regression model combiningall the features does not provide any further contri-bution to the correlation result (0.44 when evaluated108Table 3: Pearson correlations of the content features using ASR output.VSM LSA PMISimmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 SimcmbALL 0.43 0.34 0.48 0.30 0.37 0.43 0.11 0.24 0.42Contextual 0.49 0.53 0.58 0.34 0.54 0.57 0.16 0.31 0.53Opinion 0.30 0.05 0.07 0.25 0.12 0.15 0.05 0.17 0.27Table 4: Pearson correlations of the content features using ASR output with confidence scores.VSM LSA PMISimmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 SimcmbALL 0.43 0.36 0.48 0.30 0.40 0.45 0.11 0.39 0.51Contextual 0.49 0.55 0.58 0.34 0.57 0.59 0.16 0.46 0.59Opinion 0.30 0.24 0.25 0.25 0.18 0.20 0.05 0.32 0.40on all responses).In Section 4, we proposed using ASR confidencescores during feature extraction to introduce acous-tic level information and, thus, penalize responsesfor which the ASR output is less likely to be correct.Under this approach, all similarity scores are mul-tiplied by the average word confidence score con-tained in the test response.
The performance of theseenhanced features is provided in Table 4.
Comparedto the scores in Table 3, the enhanced features per-form better than the basic features that do not takethe confidence scores into consideration.
Using thisapproach, we can improve the correlation scores formost of the features, especially for the PMI-basedfeatures.
These features had lower correlations be-cause of the recognition errors, but with the con-fidence scores, they outperform the other featureswhen evaluated both on all responses or only oncontextual-based responses.
Note that the correla-tions for feature Simmax remains the same becausethe same average confidence scores for each test re-sponse is multiplied by the similarity scores to eachof the score points, so the score point obtaining thehighest similarity score is the same whether the con-fidence scores are considered or not.
The correlationof the regression model also improves from 0.44 to0.51 when the confidence scores are included.
Over-all, the best correlations for the individual similarityfeatures with the confidence scores are very close tothose obtained using human transcripts, as shown inTables 2 and 4: the difference is 0.53 vs. 0.51 forall responses, and 0.62 vs. 0.59 for contextual-basedtasks only.Because all models and parameter values aretrained on human transcripts, this experimentalsetup might not be optimal for using ASR outputs.For instance, the regression model does not outper-form the results of individual features using ASRoutputs, although the confidence scores help im-prove the overall correlation scores.
We expect thatwe can obtain better performance by using a regres-sion model trained on ASR transcripts, which canbetter model the impact of noisy data on the features.In our future work, we will build sample responsesfor each score category, tune the parameter values,and train the regression model all on ASR hypothe-ses.
We hope this can solve the mismatch problemduring training and evaluation, and can provide useven better correlation results.6 Conclusion and Future WorkMost previous work on automated scoring of spon-taneous speech used features mainly related to low-level information, such as fluency, pronunciation,prosody, as well as a few features measuring aspectssuch as vocabulary diversity and grammatical accu-racy.
In this paper, we focused on extracting con-tent features to measure the speech proficiency inrelatively higher-level aspect of spontaneous speech.Three features were computed to measure the sim-ilarity between a test response and a set of sam-ple responses representing different levels of speak-ing proficiency.
The similarity was calculated usingdifferent methods, including the lexical matching109method VSM, and two corpus-based semantic simi-larity measures, LSA and PMI.
Our experimental re-sults showed that all the features obtained good cor-relations with human proficiency scores if there areno recognition errors in the text transcripts, with thePMI-based method performing the best over threesimilarity measures.
However, if we used ASR tran-scripts, we observed a marked performance drop forthe PMI-based method.
Although we found thatVSM and LSA were very robust to ASR errors, theoverall correlations for the ASR condition were notas good as using human transcripts.
To solve thisproblem, we proposed to use ASR confidence scoresto improve the feature performance, and achievedsimilar results as when using human transcripts.As we discussed in Section 5, all models weretrained using human transcripts, which might de-crease the performance when these models are ap-plied directly to the ASR outputs.
In our futurework, we will compare models trained on humantranscripts and on ASR outputs, and investigatewhether we should use matching data for trainingand evaluation, or whether we should not introducenoise during training in order to maintain the validityof the models.
We will also investigate whether thecontent features can provide additional informationfor automated speech scoring, and help build betterscoring systems when they are combined with othernon-content features, such as the features represent-ing fluency, pronunciation, prosody, vocabulary di-versity information.
We will also explore generatingother features measuring the higher-level aspects ofthe spoken responses.
For example, we can extractfeatures assessing the responses?
relatedness to thestimulus of an opinion-based task.
For contextual-based tasks, the test takers are asked to read or lis-ten to some stimulus material, and answer a ques-tion based on this information.
We can build modelsusing these materials to check the correctness andrelatedness of the spoken responses.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater R?
v.2.
The Journal of Technology,Learning, and Assessment, 4(3):3?30.Jared Bernstein, John De Jong, David Pisoni, and BrentTownshend.
2000.
Two experiments on automaticscoring of spoken language proficiency.
In Proceed-ings of Integrating Speech Tech.
in Learning (InSTIL).Jared Bernstein, Jian Cheng, and Masanori Suzuki.2010a.
Fluency and structural complexity as predic-tors of L2 oral proficiency.
In Proceedings of Inter-speech.Jared Bernstein, Alistair Van Moere, and Jian Cheng.2010b.
Validating automated speaking tests.
Lan-guage Testing, 27(3):355?377.Lei Chen and Klaus Zechner.
2011a.
Applying rhythmfeatures to automatically assess non-native speech.
InProceedings of Interspeech.Miao Chen and Klaus Zechner.
2011b.
Computing andevaluating syntactic complexity features for automatedscoring of spontaneous non-native speech.
In Pro-ceedings of ACL-HLT.Lei Chen, Klaus Zechner, and Xiaoming Xi.
2009.
Im-proved pronunciation features for construct-driven as-sessment of non-native spontaneous speech.
In Pro-ceedings of NAACL-HLT.Catia Cucchiarini, Helmer Strik, and Lou Boves.
1997.Automatic evaluation of Dutch pronunciation by usingspeech recognition technology.
In IEEE Workshop onAuotmatic Speech Recognition and Understanding.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learners?fluency by means of automatic speech recognitiontechnology.
Journal of the Acoustical Society of Amer-ica, 107:989?999.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2002.Quantitative assessment of second language learners?fluency: comparisons between read and spontaneousspeech.
Journal of the Acoustical Society of America,111(6):2862?2873.Maxine Eskenazi, Abeer Alwan, and Helmer Strik.
2009.Spoken language technology for education.
SpeechCommunication, 51(10):831?1038.Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.1999.
The Intelligent Essay Assessor: Applications toeducational technology.
Interactive multimedia Elec-tronic Journal of Computer-Enhanced Learning, 1(2).Horacio Franco, Leonardo Neumeyer, Vassilios Di-galakis, and Orith Ronen.
2000.
Combination of ma-chine scores for automatic grading of pronunciationquality.
Speech Communication, 30(1-2):121?130.Thomas K Landauer, Peter W. Foltz, and Darrell Laham.1998.
Introduction to Latent Semantic Analysis.
Dis-course Processes, 25:259?284.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings ofthe American Association for Artificial Intelligence,September.110Peter D. Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofECML.Silke M. Witt and Steve J.
Young.
2000.
Phone-level pronunciation scoring and assessment for interac-tive language learning.
Speech Communication, 30(1-2):95?108.Klaus Zechner and Xiaoming Xi.
2008.
Towards auto-matic scoring of a test of spoken language with het-erogeneous task types.
In Proceedings of the ThirdWorkshop on Innovative Use of NLP for Building Ed-ucational Applications.Klaus Zechner, Derrick Higgins, and Xiaoming Xi.2007.
SpeechraterTM: A construct-driven approachto score spontaneous non-native speech.
In Proceed-ings of the 2007 Workshop of the International SpeechCommunication Association (ISCA) Special InterestGroup on Speech and Language Technology in Edu-cation (SLaTE).Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken En-glish.
Speech Communication, 51(10):883?895.111
