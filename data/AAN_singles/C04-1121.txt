Sentiment classification on customer feedback data: noisy data, large featurevectors, and the role of linguistic analysisMichael GamonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052mgamon@microsoft.comAbstractWe demonstrate that it is possible to performautomatic sentiment classification in the very noisydomain of customer feedback data.
We show thatby using large feature vectors in combination withfeature reduction, we can train linear supportvector machines that achieve high classificationaccuracy on data that present classificationchallenges even for a human annotator.
We alsoshow that, surprisingly, the addition of deeplinguistic analysis features to a set of surface levelword n-gram features contributes consistently toclassification accuracy in this domain.1 IntroductionSoftware companies typically receive highvolumes of electronic customer feedback everyday, some of it in the form of elicited surveys,some of it in the form of unsolicited comments,suggestions, criticism.
In order to react to thatfeedback quickly, and to direct it to the appropriatechannels inside the company, it is desirable toprovide intelligent and automatic classification ofthe feedback along two dimensions:What is the feedback about?Is the feedback positive or negative?The first question is addressed by text miningtools.
Automatic sentiment classification addressesthe second question.
Text mining tools can helpmake large quantities of feedback moremanageable by splitting them into clusters basedon keywords or topics.
Sentiment analysis, whichis the focus of this paper, adds a second dimensionto the analysis.
It makes it possible to focus thetext mining on areas in need of improvement(negative feedback) or on areas of success(positive feedback).Sentiment classification is a special case of textcategorization, where the criterion of classificationis the attitude expressed in the text, rather than the?content?
or topic.
Faced with the task of having toautomatically classify a piece of text as expressingpositive or negative sentiment, a reasonable firstapproach would consist of paying special attentionto words that tend to express a positive or negativeattitude.
Pang et al (2002) have demonstrated,however, that this is not as straightforward as onemay think, given that sentiment is often expressedin more subtle and indirect ways.The literature on sentiment classification can bedivided into approaches that rely on semanticresources, such as a sentiment or affect lexicon(Nasukawa and Yi 2003, Subasic and Huettner2001), or a large scale knowledge base (Liu et al2003) on the one hand, and approaches that try tolearn patterns directly from tagged data, withoutadditional resources (Dave et al2003, Pang et al2003).
Much research is also being directed atacquiring affect lexica automatically (Turney 2002,Turney and Littman 2002).There is also a considerable amount of researchon classification of text as ?subjective?
or?objective?
(Wiebe et al2001, Yu andHatzivassiloglou 2003), a task that is not relevantfor the processing of very brief pieces of directcustomer feedback.In many studies, research on sentimentclassification is conducted on review-type data,such as movie or restaurant reviews.
These dataoften consist of relatively well-formed, coherentand at least paragraph-length pieces of text.
Theresults we present in this paper are based oncustomer feedback data from web surveys, which,as we will discuss below, are particularly noisy andfragmentary.For our purpose of automatic classification ofcustomer feedback, we decided to use machine-learning directly on the customer feedback, insteadof relying on additional semantic resources of anykind.
This decision was motivated by practicalconsiderations: first, the customer feedback datawe are facing are often very short and sometimesvery incoherent.
This makes it seem unlikely that adetailed semantic resource would be of particularhelp.
Second, we believe that an appropriatelychosen machine-learning technique will be able todraw its own conclusions from the distribution oflexical elements in a piece of feedback.We conducted our sentiment classificationexperiments using support vector machines.Support vector machines (SVMs) have a goodtrack record in text classification (Joachims 1998,Dumais et al 1998), they can be trained using alarge number of features, and both training andclassification for linear SVMs are fast withoptimized learning algorithms.
For ourexperiments we use John Platt?s SequentialMinimal Optimization (SMO) tool (Platt 1999).
Inthe absence of any evidence that would suggest amore complicated kernel function such as apolynomial or an RBF kernel, we have decided totrain linear SVMs for our classification task (seealso the results in Joachims 1998).The procedure, as is standard in supervisedmachine learning tasks, consists of training aclassifier on pretagged training data and thenevaluating the performance of the classifier on aheld-out set of test data.The two main questions we wanted to assesswith our experiments are:1. which features and feature sets arerelevant for sentiment classification oncustomer feedback?2.
what is the maximum classificationaccuracy that can be achieved on thisdata set?2 DataOur data consists of 11399 feedback items froma Global Support Services survey, and 29485feedback items from a Knowledge Base survey fora total of 40884 items.
We excluded pieces offeedback without any verbatim from the data.Along with the verbatim, customers provided anumeric satisfaction score on a scale from 1 (notsatisfied) to 4 (very satisfied) for each of thosepieces of feedback.
The numeric score served asthe target tag in our experiments, making itunnecessary to perform any costly humanevaluation and tagging.
The distribution of itemsacross numerical scores is given in Table 1.Category 1 2 3 4Numberof documents8596 9060 14573 8655Table 1: number of documents in eachsatisfaction categoryThe data is extremely noisy, and a humanevaluation of a random set of 200 pieces offeedback could only assign a positive or negativesentiment to 117 (58.5%) items, the rest was eitherbalanced (16 cases or 8%), expressed no sentiment(50 cases or 25%), or too incoherent or random tobe classified (17 cases or 8.5%).
Amongst the 117classifiable cases, the human evaluator assignedthe category ?positive?
: to 26 cases (or 22.2%) andthe category ?negative?
to 91 cases (or 77.8%).After automatic sentence breaking into onesentence per line, the individual files contained anaverage of 2.56 lines.
For our experiments we splitthe data 90/10 into training and held-out test data.We performed 10-fold cross validation for each ofthe experiments reported in this paper.For each of the various classification tasks, wetrained a linear SVM using the standard settings ofthe SMO tool, and calculated accuracy, precisionand recall numbers on the held-out test data,averaging them across the 10-fold cross validation.3 Features3.1 Feature vectorsWe experimented with a range of differentfeature sets.
Most importantly, we wanted toestablish whether we would gain any significantadvantage in the sentiment classification task byusing features based on deep linguistic analysis orwhether surface-based features would suffice.Previous results in authorship attribution and styleclassification experiments had indicated thatlinguistic features contribute to the overallaccuracy of the classifiers, although our nullhypothesis based on a review of the relevantliterature for sentiment classification was that wewould not gain much by using these features.
Thesurface features we used were lemma unigrams,lemma bigrams, and lemma trigrams.For the linguistic features, we performed alinguistic analysis of the data with the NLPWinnatural language processing system developed inMicrosoft Research (an overview can be found inHeidorn 2000).
NLPWin provides us with a phrasestructure tree and a logical form for each string,from which we can extract an additional set offeatures:?
part-of-speech trigrams?
constituent specific length measures(length of sentence, clauses,adverbial/adjectival phrases, and nounphrases)?
constituent structure in the form of contextfree phrase structure patterns for eachconstituent in a parse tree.
Example:DECL::NP VERB NP (a declarativesentence consisting of a noun phrase averbal head and a second noun phrase)?
Part of speech information coupled withsemantic relations (e.g.
?Verb - Subject -Noun?
indicating a nominal subject to averbal predicate)?
Logical form features provided byNLPWin, such as transitivity of apredicate, tense information etc.For each of these features, except for the lengthfeatures, we extract a binary value, correspondingto the presence or absence of that feature in a givendocument.
Using binary values forpresence/absence as opposed to frequency values ismotivated by the rather extreme brevity of thesedocuments.3.2 Feature reductionFeature reduction is an important part ofoptimizing the performance of a (linear) classifierby reducing the feature vector to a size that doesnot exceed the number of training cases as astarting point.
Further reduction of vector size canlead to more improvements if the features are noisyor redundant.Reducing the number of features in the featurevector can be done in two different ways:?
reduction to the top ranking n featuresbased on some criterion of?predictiveness??
reduction by elimination of sets offeatures (e.g.
elimination of linguisticanalysis features etc.
)Experimenting with the elimination of featuresets provides an answer to the question as to whichqualitative sets of features play a significant role inthe classification taskOf course these methods can also be combined,for example by eliminating sets of features andthen taking the top ranking n features from theremaining set.We used both techniques (and theircombinations) in our experiments.
The measure of?predictiveness?
we employed is log likelihoodratio with respect to the target variable (Dunning1993).In the experiments described below, n (in the ntop-ranked features) ranged from 1000 to 40,000.The different feature set combinations we usedwere:?
?all features??
?no linguistic features?
(only wordngrams)?
?surface features?
(word ngrams,function word frequencies and POSngrams)?
?linguistic features only?
(no wordngrams)4 ResultsGiven the four different rankings associated byusers with their feedback, we experimented withtwo distinct classification scenarios:1. classification of documents as belongingto category 1 versus category 42. classification of documents as belongingto categories 1 or 2 on the one hand, and3 or 4 on the otherTwo additional scenarios can be envisioned.
Inthe first, two classifiers (?1 versus 2/3/4?
and ?4versus 1/2/3?)
would be trained and their voteswould be combined either through weightedprobability voting or other classifier combinationmethods (Dietterich 1997).
A second possibilityis to learn a three-way distinction ?1 versus 2/3versus 4?.
In this paper we restrict ourselves tothe scenarios 1 and 2 above.
Initial experimentssuggest that the combination of two classifiersyields only minimal improvements.4.1 Classification of category 1 versuscategory 4Figure 1 below illustrates the accuracy of the ?1versus 4?
classifier at different feature reductioncutoffs and with different feature sets.
Theaccuracy differences are statistically significant atthe .99 confidence level, based on the 10fold crossvalidation scenario.
Figure 2and Figure 3 show theF1-measure for target value 4 (?good sentiment?
)and target value 1 (?bad sentiment?)
respectively.The baseline for this experiment is 50.17%(choosing category 4 as the value for the targetfeature by default).Accuracy peaks at 77.5% when the top 2000features in terms of log likelihood ratio are used,and when the feature set is not restricted, i.e.
whenthese top 2000 features are drawn from linguisticand surface features.
We will return to the role oflinguistic features in section 4.4.F1-measure for both target 4 (Figure 2) andtarget 1 (Figure 3) exhibit a similar picture, againwe achieve maximum performance by using thetop 2000 features from the complete pool offeatures.Accuracy 1 versus 4707172737475767778798020k 10k 5k 2k 1knumber of featuresaccuracyall featuresno linguistic featuressurface featureslinguistic features onlyFigure 1: Accuracy of the 1 versus 4 classifierTarget 4: F-measure 1 vs 4 classifier707172737475767778798020k 10k 5k 2k 1knumber of featuresF-measureall featuresno linguistic featuressurface featureslinguistic features onlyFigure 2: F1-measure for target category 4Target 1: F-measure 1 vs 4 classifier707172737475767778798020k 10k 5k 2k 1knumber of featuresF-measureall featuresno linguistic featuressurface featureslinguistic features onlyFigure 3: F1-measure for target category 14.2 Classification of categories 1 and 2 versus3 and 4Accuracy and F1-measure results for the ?1/2versus 3/4?
task are shown in Figure 4, Figure 5and Figure 6.
Again, the accuracy differences arestatistically significant.
The baseline in thisscenario is at 56.81% (choosing category 3/4 forthe target feature by default).
Classificationaccuracy is lower than in the ?1 versus 4?
scenario,as can be expected since the fuzzy categories 2 and3 are included in the training and test data.Similarly to the ?1 versus 4?
classification,accuracy is maximal at 69.48% when the top 2000features from the complete feature set are used.The F1-measure for the target value 1/2 peaks atthe same feature reduction cutoff, whereas the F1-measure for the target value 3/4 benefits from moredrastic feature reduction to a set of only the top-ranked 1000 features.Accuracy 1/2 versus 3/4656667686970717273747520k 10k 5k 2k 1knumber of featuresaccuracyall featuresno linguistic featuressurface featureslinguistic features onlyFigure 4: Accuracy of the 1/2 versus 3/4 classifierTarget 3/4: F-measure 1/2 versus 3/4 classifier7171.57272.57373.57474.57575.57620k 10k 5k 2k 1knumber of featuresF-measureall featuresno linguistic featuressurface featureslinguistic features onlyFigure 5: F1-measure for target category 3/4Target 1/2: F-measure 1/2 versus 3/4 classifier555657585960616263646520k 10k 5k 2k 1knumber of featuresF-measureall featuresno linguistic featuressurface featureslinguistic features onlyFigure 6: F1-measure for target category 1/24.3 Results compared to human classificationThe numbers reported in the previous sectionsare substantially lower than results that have beenreported on other data sets such as movie orrestaurant reviews.
Pang et al (2002), for example,report a maximum accuracy of 82.9% on moviereviews.
As we have observed in section 2, thedata that we are dealing with here are extremelynoisy.
Recall that on a random sample of 200pieces of feedback even a human evaluator couldonly assign a sentiment classification to 117 of thedocuments, the remaining 83 being either balancedin their sentiment, or too unclear or too short to beclassifiable at all.
In order to assess performance ofour classifiers on ?cleaner?
data, we used the 117humanly classifiable pieces of customer feedbackas a test set for the best performing classifierscenario.
For that purpose, we retrained both ?1versus 4?
and ?1/2 versus 3/4?
classifiers with thetop-ranked 2000 features on our data set, with thehumanly evaluated cases removed from thetraining set.
Results are shown in Table 2, thebaseline in this experiment is at 77.78% (choosingthe ?bad?
sentiment as a default).1 versus 4using top 2kfeatures1/2 versus 3/4using top 2kfeaturesAccuracy 85.47 69.23F-measure?good?74.62 58.14F-measure?bad?89.82 75.67Table 2: Results of the two best classifiers onhumanly classifiable dataAccuracy of 85.47% as achieved by the ?1versus 4?
scenario is in line with accuracy numbersreported for less noisy domains.4.4 The role of linguistic analysis featuresFigure 1 through Figure 6 also show the effect ofeliminating whole feature sets from the trainingprocess.
A result that came as a surprise to us is thefact that the presence of very abstract linguisticanalysis features based on constituent structure andsemantic dependency graphs improves theperformance of the classifiers.
The only exceptionto this observation is the F1-measure for the?good?
sentiment case in the ?1/2 versus 3/4?scenario (Figure 5), where the different feature setsyield very much similar performance across thefeature reduction spectrum, with the ?no linguisticfeatures?
even outperforming the other feature setsby a very small margin (0.18%).
While theimprovement in practice may be too small towarrant the overhead of linguistic analysis, it isvery interesting from a linguistic point of view thateven in a domain as noisy as this one, there seemto be robust stylistic and linguistic correlates withsentiment.
Note that in the ?1 versus 4?
scenariowe can achieve classification accuracy of 74.5% byusing only linguistic features (Figure 1), withoutthe use of any word n-gram features (or any otherword-based information) at all.
This clearlyindicates that affect and style are linked in a moresignificant way than has been previously suggestedin the literature.4.5 Relevant featuresGiven that linguistic features play a consistentrole in the experiments described here, weinspected the models to see which features play aparticularly big role as indicated by theirassociated weights in the linear svm.
This isparticularly interesting in light of the fact that inprevious research on sentiment classification,affect lexica or other special semantic resourceshave served as a source for features (see referencesin section 1).
When looking at the top 100weighted features in the best classifier (?1 versus4?
), we found an interesting mix of the obvious,and the not-so-obvious.
Amongst the obviously?affect?-charged terms and features in the top 100are:+Neg1, unable to, thanks, the good, easy to, easeof, lack of, not find, not work, no help, muchaccurate, a simpleOn the other hand, there are many features thatcarry high weights, but are not what one wouldintuitively think of as a typical affect indicator:try the, of, off, ++Univ2, ADV PRON PREP3,NP::PRON:CHAR 4 , @@Adj Props Verb TsubPron5, AUXP::VERB, yourWe conclude from this inspection of individualfeatures that within a specific domain it is notnecessarily advisable to start out with a resourcethat has been geared towards containingparticularly affect-charged terminology.
See Panget al (2002) for a similar argument.
As ournumbers and feature sets suggest, there are manyterms (and grammatical patterns) associated withsentiment in a given domain that may not fall intoa typical affect class.We believe that these results show that as withmany other classification tasks in the machinelearning literature, it is preferable to start withoutan artificially limited ?hand-crafted?
set offeatures.
By using large feature sets which arederived from the data, and by paring down thenumber of features through a feature reductionprocedure if necessary, relevant patterns in the datacan be identified that may not have been obviousto the human intuition.5 ConclusionWe have shown that in the very noisy domain ofcustomer feedback, it is nevertheless possible toperform sentiment classification.
This can beachieved by using large initial feature vectorscombined with feature reduction based on log1this semantic feature indicates a negated context.2Universal quantification.3part of speech trigram.4An NP consisting of a pronoun followed by apunctuation character.5An adjectival semantic node modified by a verbalproposition and a pronominal subject.
This is in fact therepresentation for a copular construction of the form?pronoun be adjective to verb...?
as in ?I am happy toreport...?likelihood ratio.
A second, more surprising result isthat the use of abstract linguistic analysis featuresconsistently contributes to the classificationaccuracy in sentiment classification.
While resultslike this have been reported in the area of styleclassification (Baayen et al 1996, Gamon 2004),they are noteworthy in a domain where stylisticmarkers have not been considered in the past,indicating the need for more research into thestylistic correlations of affect in text.6 AcknowledgementsWe thank Anthony Aue and Eric Ringger(Microsoft Research) and Hang Li (MicrosoftResearch Asia) for helpful comments anddiscussions, and Chris Moore (Microsoft ProductSupport Services UK) for the initial request forsentiment classification based on the needs ofSupport Services at Microsoft.
Thanks also go toKarin Berghoefer of the Butler-Hill group formanually annotating a subset of the data.ReferencesHarald Baayen, Hans van Halteren, and FionaTweedie.
1996.
Outside the Cave of Shadows:Using Syntactic Annotation to EnhanceAuthorship Attribution.
Literary and LinguisticComputing 11(3): 121-131.Thomas G. Dietterich (1997): ?Machine-learningresearch: Four current directions?.
In: AIMagazine, 18 (4), pp.97-136.Susan Dumais, John Platt, David Heckerman,Mehran Sahami (1998): ?Inductive LearningAlgorithms and Representations for TextCategorization?.
Proceedings of CIKM-98, pp.148-155.Ted Dunning.
1993.
Accurate Methods for theStatistics of Surprise and Coincidence.Computational Linguistics 19: 61-74.Aidan Finn and Nicholas Kushmerick (2003):?Learning to classify documents according togenre?.
IJCAI-03 Workshop on ComputationalApproaches to Text Style and Synthesis.Michael Gamon (2004): ?Linguistic correlates ofstyle: authorship classification with deeplinguistic analysis features?.
Paper to bepresented at COLING 2004.George Heidorn.
(2000): ?Intelligent WritingAssistance.?
In R. Dale, H. Moisl and H.Somers, eds., Handbook of Natural LanguageProcessing.
Marcel Dekker.Thorsten Joachims (1998): ?Text Categorizationwith Support Vector Machines: Learning withMany Relevant Features?.
Proceedings of ECML1998, pp.
137-142.Kushal Dave, Steve Lawrence and David M.Pennock (2003): ?Mining the Peanut Gallery:Opinion Extraction and Semantic Classificationof Product Reviews?.
In: Proceedings of theTwelfth International World Wide WebConference, pp.
519-528.Hugo Liu, Henry Lieberman and Ted Selker(2003): ?A Model of Textual Affect Sensingusing Real-World Knowledge?.
In: Proceedingsof the Seventh Conference on Intelligent UserInterfaces, pp.
125-132.Tetsuya Nasukawa and Jeonghee Yi (2003):?Sentiment Analysis: Capturing FavorabilityUsing Natural Language Processing?.
In:proceedings of the International Conference onKnowledge Capture, pp.
70-77.Bo Pang, Lillian Lee and ShivakumarVaithyanathan (2002): ?Thumbs up?
SentimentClassification using Machine LearningTechniques?.
Proceedings of EMNLP 2002, pp.79-86.John Platt (1999): ?Fast training of SVMs usingsequential minimal optimization?.
In: B.Schoelkopf, C. Burges and A. Smola (eds.
)?Advances in Kernel Methods: Support VectorLearning?, MIT Press, Cambridge, MA, pp.
185-208.Pero Subasic and Alison Huettner (2001): ?AffectAnalysis of Text Using Fuzzy SemanticTyping?.
In: Proceedings of the Tenth IEEEInternational Conference on Fuzzy Systems, pp.483-496.Ljup?o Todorovski and Sa?o D?eroski (2003):?Combining Classifiers with Meta DecisionTrees?.
In: Machine Learning, 50, pp.223-249.Peter D. Turney (2002): ?Thumbs up or thumbsdown?
Semantic orientation applied tounsupervised classification of reviews?.
In:Proceedings of ACL 2002, pp.
417-424.Peter D. Turney and M. L. Littman (2002):?Unsupervised lLearning of SemanticOrientation from a Hundred-Billion-WordCorpus.?
Technical report ERC-1094 (NRC44929), National research Council of Canada.Janyce Wiebe, Theresa Wilson and Matthew Bell(2001): ?Identifying Collocations forRecognizing Opinions?.
In: Proceedings of theACL/EACL Workshop on Collocation.Hong Yu and Vasileios Hatzivassiloglou (2003):?Towards Answering pinion Questions:Separating Facts from Opinions and Identifyingthe Polarity of Opinion Sentences?.
In:Proceedings of EMNLP 2003.
