Proceedings of the SIGDIAL 2013 Conference, pages 117?121,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsDialogue Act Recognition inSynchronous and Asynchronous ConversationsMaryam Tavafi?, Yashar Mehdad?, Shafiq Joty?, Giuseppe Carenini?, Raymond Ng?
?Department of Computer Science, University of British Columbia, Vancouver, Canada?Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar?
{tavafi, mehdad, carenini, rng}@cs.ubc.ca ?sjoty@qf.org.qaAbstractIn this work, we study the effectiveness ofstate-of-the-art, sophisticated supervisedlearning algorithms for dialogue act mod-eling across a comprehensive set of differ-ent spoken and written conversations in-cluding: emails, forums, meetings, andphone conversations.
To this aim, we com-pare the results of SVM-multiclass andtwo structured predictors namely SVM-hmm and CRF algorithms.
Extensive em-pirical results, across different conversa-tional modalities, demonstrate the effec-tiveness of our SVM-hmm model for di-alogue act recognition in conversations.1 IntroductionRevealing the underlying conversational struc-ture in dialogues is important for detecting thehuman social intentions in spoken conversationsand in many applications including summariza-tion (Murray, 2010), dialogue systems and di-alogue games (Carlson, 1983) and flirt detec-tion (Ranganath, 2009).
As an additional example,Ravi and Kim (2007) show that dialogue acts canbe used for analyzing the interaction of students ineducational forums.Recently, there have been increasing interestsfor dialogue act (DA) recognition in spoken andwritten conversations, which include meetings,phone conversations, emails and blogs.
However,most of the previous works are specific to one ofthese domains.
There are potentially useful fea-tures and algorithms for each of these domains,but due to the underlying similarities betweenthese types of conversations, we aim to identify adomain-independent DA modeling approach thatcan achieve good results across all types of con-versations.
Such a domain-independent dialogueact recognizer makes it possible to automaticallyrecognize dialogue acts in a wide variety of con-versational data, as well as in conversations span-ning multiple domains/modalities; for instance aconversation that starts in a meeting and then con-tinues via email.While previous work in DA modeling has fo-cused on studying only one (Carvalho, 2005;Shrestha, 2004; Ravi, 2007; Ferschke, 2012; Kim,2010a; Sun, 2012) or, in a few cases, a couple ofconversational domains (Jeong, 2009; Joty, 2011),in this paper, we analyze the performance of su-pervised DA modeling on a comprehensive setof different spoken and written conversations thatincludes: emails, forums, meetings, and phoneconversations.
More specifically, we comparethe performance of three state-of-the-art, sophis-ticated machine learning algorithms, which in-clude SVM-multiclass and two structured predic-tors SVM-hmm and Conditional Random Fields(CRF) for DA modeling.
We present an exten-sive set of experiments studying the effectivenessof DA modeling on different types of conversa-tions such as emails, forums, meeting, and phonediscussions.
The experimental results show thatthe SVM-hmm algorithm outperforms other su-pervised algorithms across all datasets.2 Related WorkThere have been several studies on superviseddialogue act (DA) modeling.
To the best ofour knowledge, none of them compare the per-formance of DA recognition on different syn-chronous (e.g., meeting and phone) and asyn-chronous (e.g., email and forum) conversations.Most of the works analyze DA modeling in a spe-cific domain.
Carvalho and Cohen (2005) proposeclassifying emails into their dialogue acts accord-ing to two ontologies for nouns and verbs.
Theontologies are used for determining the speechacts of each single email with verb-noun pairs.Shrestha and McKeown (2004) also study the117problem of DA modeling in email conversationsconsidering the two dialogue acts of question andanswer.
Likewise, Ravi and Kin (2007) presenta DA recognition method for detecting questionsand answers in educational discussions.
Ferschkeet al(2012) apply DA modeling to Wikipedia dis-cussions to analyze the collaborative process ofediting Wikipedia pages.
Kim et al(2010a) studythe task of supervised classification of dialogueacts in one-to-one online chats in the shopping do-main.All these previous studies focus on DA recog-nition in one or two domains, and do not sys-tematically analyze the performance of differentdialog act modeling approaches on a compre-hensive set of conversation domains.
As far aswe know, the present work is the first that pro-poses domain-independent supervised DA model-ing techniques, and analyzes their effectiveness ondifferent modalities of conversations.3 Dialogue Act Recognition3.1 Conversational structureAdjacent utterances in a conversation have astrong correlation in terms of their dialogue acts.As an example, if speaker 1 asks a question tospeaker 2, it is a high probability that the next ut-terance of the conversation would be an answerfrom speaker 2.
Therefore, the conversationalstructure is a paramount factor that should be takeninto account for automatic DA modeling.
The con-versational structure differs in spoken and writtendiscussions.
In spoken conversations, the discus-sion between the speakers is synchronized.
Thespeakers hear each other?s ideas and then statetheir opinions.
So the temporal order of the ut-terances can be considered as the conversationalstructure in these types of conversations.
How-ever, in written conversations such as email andforum, authors contribute to the discussion in dif-ferent order, and sometimes they do not pay atten-tion to the content of previous posts.
Therefore,the temporal order of the conversation cannot beused as the conversational structure in these do-mains, and appropriate techniques should be usedto extract the underlying structure in these conver-sations.To this aim, when reply links are available inthe dataset, we use them to capture the conversa-tion structure.
To obtain a conversational structurethat is often even more refined than the reply links,we build the Fragment Quotation Graph.
To thisend, we follow the procedure proposed by Joty etal.
(2011) to extract the graph structure of a thread.3.2 FeaturesIn defining the feature set, we have two primarycriteria, being domain independent and effective-ness in previous works.
Lexical features such asunigrams and bigrams have been shown to be use-ful for the task of DA modeling in previous stud-ies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi,2007; Carvalho, 2005).
In addition, unigrams havebeen shown to be the most effective among thetwo.
So, as the lexical feature, we include the fre-quency of unigrams in our feature set.Moreover, length of the utterance is anotherbeneficial feature for DA recognition (Ferschke,2012; Shrestha, 2004; Joty, 2011), which we addto our feature set.
The speaker of an utterancehas shown its utility for recognizing speech acts(Sun, 2012; Kim, 2010a; Joty, 2011).
Sun andMorency (2012) specifically employ a speaker-adaptation technique to demonstrate the effective-ness of this feature for DA modeling.
We alsoinclude the relative position of a sentence in apost for DA modeling since most of previous stud-ies (Ferschke, 2012; Kim, 2010a; Joty, 2011)prove the efficiency of this feature.3.3 AlgorithmsSince most top performing DA models use su-pervised approaches (Carvalho, 2005; Shrestha,2004; Ravi, 2007; Ferschke, 2012; Kim, 2010a),to analyze the performance of DA modeling on acomprehensive set of different spoken and writtenconversations, we compare the state-of-the-art su-pervised algorithms.We employ three state-of-the-art, sophisticatedsupervised learning algorithms:SVM-hmm predicts labels for the examplesin a sequence (Tsochantaridis, 2004).
Thisapproach uses the Viterbi algorithm to find thehighest scoring tag sequence for a given obser-vation sequence.
Being a Hidden Markov Model(HMM), the model makes the Markov assump-tion, which means that the label of a particularexample is assigned only by considering thelabel of the previous example.
This approach isconsidered an SVM because the parameters of themodel are trained discriminatively to separate thelabel of sequences by a large margin.118CRF is a probabilistic framework to label andsegment sequence data (Lafferty, 2001).
Themain advantage of CRF over HMM is that it re-laxes the assumption of conditional independenceof observed data.
HMM is a generative modelthat assigns a joint distribution over label andobservation sequences.
Whereas, CRF defines theconditional probability distribution over label se-quences given a particular observation sequence.SVM-multiclass is a generalization of binarySVM to a multiclass predictor (Crammer, 2001).The SVM-multiclass does not consider thesequential dependency between the examples.4 CorporaGathering conversational corpora for DA model-ing is an expensive and time-consuming task.
Dueto the privacy issues, there are few available con-versational datasets.For asynchronous conversations, we use avail-able corpora for email and forum discussions.
Forsynchronous domains we employ available cor-pora in multi-party meeting and phone conversa-tions.BC3 (Email): As the labeled dataset for emailconversations, we use BC3 (Ulrich, 2008), whichcontains 40 threads from W3C corpus.
TheBC3 corpus is annotated with twelve domain-independent dialogue acts, which are mainlyadopted from the MRDA tagset, and it has beenused in several previous works (e.g., (Joty, 2011)).CNET (Forum): As the labeled forum dataset,we use the available CNET corpus, which is an-notated with eleven domain-independent dialogueacts in a post-level (Kim et al2010b).
This corpusconsists of 320 threads and a total of 1332 posts,which are mostly from technical forums.MRDA (Meeting): ICSI-MRDA dataset isused as labeled data for meeting conversation,which contains 75 meetings with 53 unique speak-ers (Shriberg, 2004).
The ICSI-MRDA dataset re-quires one general tag per sentence followed byvariable number of specific tags.
There are 11general tags and 39 specific tags in the annotationscheme.
We reduce their tagset to the eleven gen-eral tags to be consistent with the other datasets.SWBD (Phone): In addition to multi-partymeeting conversations, we also report our experi-mental results on Switchboard-DAMSL (SWBD),which is a large-scale corpus containing telephonespeech (Jurafsky, 1997).
This corpus is annotatedwith the SWBD-DAMSL tagset, which consists of220 tags.
We use the mapping table presented byJeong (2009) to reduce the tagset to 16 domain-independent dialogue acts.All the available corpora are annotated with di-alogue acts at the sentence-level.
The only excep-tion is the CNET forum dataset, on which we ap-ply DA classification at the post-level.5 Experiments and Results5.1 Experimental settingsIn our experiments, we use the SVM-hmm1 andSVM-multiclass2 packages developed with theSVM-light software.
We use the Mallet package3for the CRF algorithm.
The results of supervisedclassifications are compared to the baseline, whichis the majority class of each dataset.
We apply5-fold cross-validation for the supervised learn-ing methods to each dataset, and compare the re-sults of different methods using micro-averagedand macro-averaged accuracies.5.2 ResultsTable 1 shows the results of supervised classifi-cation on different conversation modalities.
Weobserve that SVM-hmm and CRF classifiers out-perform SVM-multiclass classifier in all conversa-tional domains.
Both SVM-hmm and CRF classi-fiers consider the sequential structure of conversa-tions, while this is ignored in the SVM-multiclassclassifier.
This shows that the sequential structureof the conversation is beneficial independently ofthe conversational modality.
We can also observethat the SVM-hmm algorithm results in the highestperformance in all datasets.
As shown in (Altun,2003), generalization performace of SVM-hmmis superior to CRF.
This superiority also appliesto the DA modeling task across all the conversa-tional modalities.
However, as it was investigatedby Keerthi and Sundararajan (2007), the discrep-ancy in the performance of these methods mayarise from different feature functions that thesetwo methods use, and they might perform simi-larly when they use the same feature functions.Comparing the results across different datasets,we can also note that the largest improvementof SVM-hmm and CRF is on the SWBD, the1http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html2http://svmlight.joachims.org/svm_multiclass.html3http://mallet.cs.umass.edu119Corpus Baseline SVM-multiclass SVM-hmm CRFMicro Macro Micro Macro Micro Macro Micro MacroBC3 69.56 8.34 73.57 (4.01) 8.34 (0) 77.75 (8.19) 18.20 (9.86) 72.18 (2.62) 14.9 (6.56)CNET 36.75 9.09 34.8 (-1.95) 9.3 (0.21) 58.7 (21.95) 17.1 (8.01) 40.3 (3.55) 11.5 (2.41)MRDA 66.47 9.09 66.47 (0) 9.09 (0) 80.5 (14.03) 32.4 (23.31) 77.8 (11.33) 22.9 (13.81)SWBD 46.44 6.25 46.5 (0.06) 6.25 (0) 74.32 (27.88) 30.13 (23.88) 73.04 (26.6) 24.05 (17.8)Table 1: Results of supervised DA modeling; columns are micro-averaged and macro-averaged accura-cies with difference with baseline in parentheses.phone conversation dataset.
Moreover, super-vised DA recognition on synchronous conversa-tions achieves a better performance than on asyn-chronous conversations.
We can argue that this isdue to the less complex sequential structure of syn-chronous conversations.
A lower macro-averagedaccuracy in asynchronous conversations (i.e., fo-rums and emails) can be justified in the same way.By looking at the results in asynchronous con-versations, we observe a larger improvement ofmicro-averaged accuracy over the CNET corpus.This might be due to two reasons: i) the DA tagsetsin both corpora are different (i.e., no overlap intagsets); and ii) the conversational structure in fo-rums and emails is different.5.3 DiscussionWe analyze the strengths and weakness of super-vised DA modeling with SVM-hmm in differentconversations individually.BC3: SVM-hmm succeeds in classifying mostof the statement and yes-no question speech acts inthe BC3 corpus.
However, it does not show a highaccuracy for classifying polite mechanisms suchas ?thanks?
and ?regards?.
Through the error anal-ysis, we observed that in most of these cases theerror arose from the voting algorithm.
Moreover,the improvement of supervised DA modeling onthe BC3 corpus is smaller than the other datasets.This may suggest that email conversation is a chal-lenging domain for DA recognition.CNET: The inventory of dialogue acts in theCNET dataset can be considered as two groups ofquestion and answer dialogue acts, and we wouldneed more sophisticated features in order to clas-sify the posts into the fine-grained dialogue acts.The SVM-hmm succeeds in predicting the labelsof question-question and answer-answer dialogueacts, but it performs poorly for the other labels.The improvement of DA modeling over the base-line is significant for this dataset.
To further im-prove the performance, a hierarchical DA classifi-cation can be applied.
In this way, the posts wouldbe classified into question and non-question dia-logue acts in the first level.MRDA: SVM-hmm performs well for pre-dicting the classes of statement, floor holder,backchannel, and wh-question.
Floor holders andbackchannels are mostly the short utterances suchas ?ok?, ?um?, and ?so?, and we believe the lengthand unigrams features are very effective for pre-dicting these dialogue acts.
On the other hand,SVM-hmm fails in predicting the other types ofquestions such as rhetorical questions and open-ended questions by classifying them as statements.Arguably by adding more sophisticated featuressuch as POS tags, SVM-hmm would perform bet-ter for classifying these speech acts.SWBD: The improvement of supervised DArecognition on the SWBD is higher than the otherdomains.
Supervised DA classification correctlypredicts most of the classes of statement, reject re-sponse, wh-question, and backchannel.
However,SVM-hmm cannot predict some specific dialogueacts of phone conversations such as self-talk andsignal-non-understanding.
There are a few utter-ances in the corpus with these dialogue acts, andmost of them are classified as statements.6 Conclusion and Future WorkWe have studied the effectiveness of sophisticatedsupervised learning algorithms for DA modelingacross a comprehensive set of different spoken andwritten conversations.
Through an extensive ex-periment, we have shown that our proposed SVM-hmm algorithm with the domain-independent fea-ture set can achieve high results on different syn-chronous and asynchronous conversations.In future, we will incorporate other lexical andsyntactic features in our supervised framework.We also plan to augment our feature set withdomain-specific features like prosodic features forspoken conversations.
We will also investigate theperformance of our domain-independent approachin a semi-supervised framework.120ReferencesCongkai Sun and Louise-Philippe Morency.
2012.
Di-alogue Act Recognition using Reweighted SpeakerAdaptation.
13th Annual SIGdial Meeting on Dis-course and Dialogue.Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca.1997.
Switchboard SWBD-DAMSL labeling projectcoder?s manual, draft 13.
Technical report, Univ.
ofColorado Institute of Cognitive Science.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng, and Hannah Carvey.
2004.
The ICSI Meet-ing Recorder Dialog Act (MRDA) Corpus.
HLT-NAACL SIGDIAL Workshop.Gabriel Murray, Giuseppe Carenini, and Raymond T.Ng.
2010.
Generating and validating abstracts ofmeeting conversations: a user study.
INLG?10.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and struc-tured output spaces.
Proceedings of the 21st Inter-national Conference on Machine Learning (ICML).Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.2008.
A publicly available annotated corpus forsupervised email summarization.
EMAIL?08 Work-shop.
AAAI.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
Intl.
Conf.
on Machine Learning.Koby Crammer and Yoram Singer.
2001.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
Journal of Machine Learning Re-search.Lari Carlson.
1983.
Dialogue Games: An Approach toDiscourse Analysis.
D. Reidel.Lokesh Shrestha and Kathleen McKeown.
2004.
De-tection of question-answer pairs in email conversa-tions.
Proceedings of the 20th Biennial Int.
Conf.
onComputational Linguistics.Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee.2009.
The Semi-supervised speech act recognitionin emails and forums.
Proceedings of the 2009 Conf.Empirical Methods in Natural Language Processing.Oliver Ferschke, Iryna Gurevych, and Yevgen Cheb-otar.
2012.
Behind the Article: Recognizing Dia-log Acts in Wikipedia Talk Pages.
Proceedings ofthe 13th Conference of the European Chapter of theACL.Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.2009.
Its not you, its me: Detecting flirting and itsmisperception in speed-dates.
EMNLP-09.S.
S. Keerthi and S. Sundararajan.
2007.
CRF versusSVM-Struct for sequence labeling.
Technical report,Yahoo Research.Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.2011.
Unsupervised modeling of dialog acts inasynchronous conversations.
IJCAI.Su N. Kim, Lawrence Cavedon, and Timothy Baldwin.2010a.
Classifying dialogue acts in one-on-one livechats.
EMNLP?10.Su N. Kim, Li Wang, and Timothy Baldwin.
2010b.Tagging and linking web forum posts.
Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning, CoNLL ?10.Sujith Ravi and Jihie Kim.
2007.
Profiling studentinteractions in threaded discussions with speech actclassifiers.
AIED?07, LA, USA.Vitor R. Carvalho and William W. Cohen.
2005.
Onthe collective classification of email "speech acts".Proceedings of the 31st Annual Int.
ACM SIGIRConf.
on Research and Development in InformationRetrieval.Yasemin Altun and Ioannis Tsochantaridis and ThomasHofmann.
2003.
Hidden Markov Support VectorMachines.
Proceedings of the 20th InternationalConference on Machine Learning.7 Appendix A.
Frequency of DialogueActs in the CorporaTag Dialogue Acts Email(BC3) Forum(CNET) Meeting(MRDA) Phone(SWBD)A Accept response 2.07% ?
?
6.96%AA Acknowledge and appreciate 1.24% ?
?
2.12%AC Action motivator 6.09% ?
?
0.38%P Polite mechanism 6.97% ?
?
0.12%QH Rhetorical question 0.75% ?
0.34% 0.25%QO Open-ended question 1.32% ?
0.17% 0.3%QR Or/or-clause question 1.10% ?
?
0.2%QW Wh-question 2.29% ?
1.63% 0.95%QY Yes-no question 6.75% ?
4.75% 2.62%R Reject response 1.06% ?
?
1.03%S Statement 69.56% ?
66.47% 46.44%U Uncertain response 0.79% ?
?
0.15%Z Hedge ?
?
?
11.55%B Backchannel ?
?
14.44% 26.62%D Self-talk ?
?
?
0.1%C Signal-non-understanding ?
?
?
0.14%FH Floor holder ?
?
7.96% ?FG Floor grabber ?
?
2.96% ?H Hold ?
?
0.76% ?QRR Or clause after yes-no question ?
?
0.38% ?QR Or question ?
?
0.2% ?QQ Question-question ?
27.92% ?
?QA Question-add ?
11.67% ?
?QCN Question-confirmation ?
3.89% ?
?QCC Question-correction ?
0.36% ?
?AA Answer-answer ?
36.75% ?
?AD Answer-add ?
8.84% ?
?AC Answer-confirmation ?
0.36% ?
?RP Reproduction ?
0.71% ?
?AO Answer-objection ?
1.07% ?
?RS Resolution ?
7.78% ?
?O Other ?
0.71% ?
?Table 2: Dialogue act categories and their relativefrequency.Table 2 indicates the dialogue acts of each cor-pus and their relative frequencies in that dataset.The table shows that the distribution of dialogueacts in the datasets are not balanced.
Most of theutterances in the datasets are labeled as statements.Consequently, during the classification step, mostof the utterances are labeled as the statement dia-logue act.
This always affects the performance ofa classifier in dealing with low frequency classes.A possible approach to tackle this problem is tocluster the correlative dialogue acts into the samegroup and apply a DA modeling approach in a hi-erarchical manner.121
