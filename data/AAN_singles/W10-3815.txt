Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 118?126,COLING 2010, Beijing, August 2010.Deep Syntax Language Models and Statistical Machine TranslationYvette GrahamNCLTDublin City Universityygraham@computing.dcu.ie josef@computing.dcu.ieJosef van GenabithCNGLDublin City UniversityAbstractHierarchical Models increase the re-ordering capabilities of MT systemsby introducing non-terminal symbols tophrases that map source language (SL)words/phrases to the correct positionin the target language (TL) translation.Building translations via discontiguousTL phrases increases the difficulty of lan-guage modeling, however, introducing theneed for heuristic techniques such as cubepruning (Chiang, 2005), for example.An additional possibility to aid languagemodeling in hierarchical systems is to usea language model that models fluency ofwords not using their local context in thestring, as in traditional language models,but instead using the deeper context ofa word.
In this paper, we explore thepotential of deep syntax language mod-els providing an interesting comparisonwith the traditional string-based languagemodel.
We include an experimental evalu-ation that compares the two kinds of mod-els independently of any MT system to in-vestigate the possible potential of integrat-ing a deep syntax language model into Hi-erarchical SMT systems.1 IntroductionIn Phrase-Based Models of Machine Translationall phrases consistent with the word alignmentare extracted (Koehn et al, 2003), with shorterphrases needed for high coverage of unseen dataand longer phrases providing improved fluency intarget language translations.
Hierarchical Mod-els (Chiang, 2007; Chiang, 2005) build on Phrase-Based Models by relaxing the constraint thatphrases must be contiguous sequences of wordsand allow a short phrase (or phrases) nested withina longer phrase to be replaced by a non-terminalsymbol forming a new hierarchical phrase.
Tra-ditional language models use the local context ofwords to estimate the probability of the sentenceand introducing hierarchical phrases that generatediscontiguous sequences of TL words increasesthe difficulty of computing language model proba-bilities during decoding and require sophisticatedheuristic language modeling techniques (Chiang,2007; Chiang, 2005).Leaving aside heuristic language modeling fora moment, the difficulty of integrating a tradi-tional string-based language model into the de-coding process in a hierarchical system, highlightsa slight incongruity between the translation modeland language model in Hierarchical Models.
Ac-cording to the translation model, the best way tobuild a fluent TL translation is via discontiguousphrases, while the language model can only pro-vide information about the fluency of contiguoussequences of words.
Intuitively, a language modelthat models fluency between discontiguous wordsmay be well-suited to hierarchical models.
Deepsyntax language models condition the probabilityof a word on its deep context, i.e.
words linked toit via dependency relations, as opposed to preced-ing words in the string.
During decoding in Hi-erarchical Models, words missing a context in thestring due to being preceded by a non-terminal,might however be in a dependency relation witha word that is already present in the string and118this context could add useful information aboutthe fluency of the hypothesis as its constructed.In addition, using the deep context of a wordprovides a deeper notion of fluency than the lo-cal context provides on its own and this might beuseful to improve such things as lexical choice inSMT systems.
Good lexical choice is very im-portant and the deeper context of a word, if avail-able, may provide more meaningful informationand result in better lexical choice.
Integratingsuch a model into a Hierarchical SMT system isnot straightforward, however, and we believe be-fore embarking on this its worthwhile to evalu-ate the model independently of any MT system.We therefore provide an experimental evaluationof the model and in order to provide an interestingcomparison, we evaluate a traditional string-basedlanguage model on the same data.2 Related WorkThe idea of using a language model based on deepsyntax is not new to SMT.
Shen et al (2008) usea dependency-based language model in a stringto dependency tree SMT system for Chinese-English translation, using information from thedeeper structure about dependency relations be-tween words, in addition to the position of thewords in the string, including information aboutwhether context words were positioned on the leftor right of a word.
Bojar and Hajic?
(2008) use adeep syntax language model in an English-Czechdependency tree-to-tree transfer system, and in-clude three separate bigram language models: areverse, direct and joint model.
The model in ourevaluation is similar to their direct bigram model,but is not restricted to bigrams.Riezler and Maxwell (2006) use a trigram deepsyntax language model in German-English depen-dency tree-to-tree transfer to re-rank decoder out-put.
The language model of Riezler and Maxwell(2006) is similar to the model in our evaluation,but differs in that it is restricted to a trigram modeltrained on LFG f-structures.
In addition, as lan-guage modeling is not the main focus of theirwork, they provide little detail on the languagemodel they use, except to say that it is based on?log-probability of strings of predicates from rootto frontier of target f-structure, estimated frompredicate trigrams in English f-structures?
(Rie-zler and Maxwell, 2006).
An important prop-erty of LFG f-structures (and deep syntactic struc-tures in general) was possibly overlooked here.F-structures can contain more than one path ofpredicates from the root to a frontier that in-clude the same ngram, and this occurs when theunderlying graph includes unary branching fol-lowed by branching with arity greater than one.In such cases, the language model probability asdescribed in Riezler and Maxwell (2006) is incor-rect as the probability of these ngrams will be in-cluded multiple times.
In our definition of a deepsyntax language model, we ensure that such du-plicate ngrams are omitted in training and testing.In addition, Wu (1998) use a bigram deep syntaxlanguage model in a stochastic inversion transduc-tion grammar for English to Chinese.
None of therelated research we discuss here has included anevaluation of the deep syntax language model theyemploy in isolation from the MT system, however.3 Deep SyntaxThe deep syntax language model we describe isnot restricted to any individual theory of deepsyntax.
For clarity, however, we restrict our ex-amples to LFG, which is also the deep syntaxtheory we use for our evaluation.
The LexicalFunctional Grammar (LFG) (Kaplan and Bres-nan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrym-ple, 2001) functional structure (f-structure) is anattribute-value encoding of bi-lexical labeled de-pendencies, such as subject, object and adjunctfor example, with morpho-syntactic atomic at-tributes encoding information such as mood andtense of verbs, and person, number and case fornouns.
Figure 1 shows the LFG f-structure for En-glish sentence ?Today congress passed Obama?shealth care bill.
?1Encoded within the f-structure is a directedgraph and our language model uses a simplifiedacyclic unlabeled version of this graph.
Figure1(b) shows the graph structure encoded within thef-structure of Figure 1(a).
We discuss the simpli-fication procedure later in Section 5.1Morpho-syntactic information/ atomic features are omit-ted from the diagram.119(a) ??????????
?PRED passSUBJ[PRED congress]OBJ????
?PRED billSPEC[POSS[PRED Obama]]MOD[PRED careMOD[PRED health]]????
?ADJ[PRED today]???????????
(b) <s>passtoday congress bill</s> </s> obama care</s> health</s>Figure 1: ?Today congress passed Obama?s health care bill.
?4 Language ModelWe use a simplified approximation of the deepsyntactic structure, de, that encodes the unlabeleddependencies between the words of the sentence,to estimate a deep syntax language model prob-ability.
Traditional string-based language mod-els combine the probability of each word in thesentence, wi, given its preceding context, the se-quence of words from w1 to wi?1, as shown inEquation 1.p(w1, w2, ..., wl) =l?i=1p(wi|w1, ..., wi?1) (1)In a similar way, a deep syntax language modelprobability combines the probability of each wordin the structure, wi, given its context within thestructure, the sequence of words from wr, thehead of the sentence, to wm(i), as shown in Equa-tion 2, with function m used to map the index of aword in the structure to the index of its head.
2p(de) =l?i=1p(wi|wr, ..., wm(m(i))wm(i)) (2)In order to combat data sparseness, we applythe Markov assumption, as is done in traditionalstring-based language modeling, and simplify theprobability by only including a limited length ofhistory when estimating the probability of each2We refer to the lexicalized nodes in the dependencystructure as words, alternatively the term predicate can beused.word in the structure.
For example, a trigram deepsyntax language model conditions the probabilityof each word on the sequence of words consistingof the head of the head of the word followed bythe head of the word as follows:p(de) =l?i=1P (wi|wm(m(i)) , wm(i)) (3)In addition, similar to string-based languagemodeling, we add a start symbol, <s>, at theroot of the structure and end symbols, </s>, atthe leaves to include the probability of a word be-ing the head of the sentence and the probabilityof words occurring as leaf nodes in the structure.Figure 2(a) shows an example of how a trigramdeep syntax language model probability is com-puted for the example sentence in Figure 1(a).5 Simplified Approximation of the DeepSyntactic RepresentationWe describe the deep syntactic structure, de, asan approximation since a parser is employed toautomatically produce it and there is therefore nocertainty that we use the actual/correct deep syn-tactic representation for the sentence.
In addi-tion, the function m requires that each node in thestructure has exactly one head, however, structure-sharing can occur within deep syntactic structuresresulting in a single word legitimately having twoheads.
In such cases we use a simplification ofthe graph in the deep syntactic structure.
Fig-ure 3 shows an f-structure in which the subject120(a) Deep Syntax LM (b) Traditional LMp(e) ?
p( pass | <s>)?
p(e) ?
p( passed | today congress )?p( today | <s> pass )?
p( today | <s>)?p(</s> | pass today )?p( congress | <s> pass )?
p( congress | <s> today )?p(</s> | pass congress )?p( bill | <s> pass )?
p( bill | health care )?p( obama | pass bill )?
p( obama | congress passed )?p(</s> | bill obama )?p( care | pass bill )?
p( care | s health )?p( health | bill care )?
p( health | ?
s )?p(</s> | care health )p( ?
| passed Obama )?p( s | obama ?
)?p( .
| care bill )?p(</s> | bill .
)Figure 2: Example Comparison of Deep Syntax and Traditional Language Modelsof both like, be and president is hillary.
In oursimplified structure, the dependency relations be-tween be and hillary and president and hillary aredropped.
We discuss how we do this later in Sec-tion 6.
Similar to our simplification for structuresharing, we also simplify structures that containcycles by discarding edges that cause loops in thestructure.6 ImplementationSRILM (Stolcke, 2002) can be used to computea language model from ngram counts (the -readoption of the ngram-count command).
Implemen-tation to train the language model, therefore, sim-ply requires accurately extracting counts from thedeep syntax parsed training corpus.
To simplifythe structures to acyclic graphs, nodes are labeledwith an increasing index number via a depth firsttraversal.
This allows each arc causing a loop inthe graph or argument sharing to be identified bya simple comparison of index numbers, as the in-dex number of its start node will be greater thanthat of its end node.
The algorithm we use toextract ngrams from the dependency structures isstraightforward: we simply carry out a depth-firsttraversal of the graph to construct paths of wordsthat stretch from the root of the graph to words????????????????
?PRED likeSUBJ 1:[PRED Hillary]XCOMP???????????
?PRED beSUBJ 1XCOMP-PRED[PRED presidentSUBJ 1]ADJ??
?PRED atOBJ[PRED U.N.SPEC[PRED the]]???????????????????????????????
?<s>likehillary be</s> president at</s> U.N.the</s>Figure 3: ?Hillary liked being president at theU.N.?121????????????
?PRED agreeSUBJ[PRED nobody]XCOMP???????
?PRED withOBJ?????
?PRED pointADJ????
?COORD and{[PRED two],[PRED three]}???????????????????????????????
?<s>agreenobody with</s> pointandtwo three</s> </s>Figure 4: ?Nobody agreed with points two andthree.
?at the leaves and then extract the required orderngrams from each path.
As mentioned earlier,some ngrams can belong to more than one path.Figure 4 shows an example structure containingunary branching followed by binary branching inwhich the sequence of symbols and words ?<s>agree with point and?
belong to the path endingin two </s> and three </s>.
In order to ensurethat only distinct ngrams are extracted we assigneach word in the structure a unique id numberand include this in the extracted ngrams.
Pathsare split into ngrams and duplicate ngrams result-ing from their occurrence in more than one pathare discarded.
Its also possible for ngrams to le-gitimately be repeated in a deep structure, and insuch cases we do not discard these ngrams.
Legit-imately repeating ngrams are easily identified asthe id numbers attached to words will be differ-ent.7 Deep Syntax and Lexical Choice inSMTCorrect lexical choice in machine translation isextremely important and PB-SMT systems relyon the language model to ensure, that when twophrases are combined with each other, that themodel can rank combined phrases that are flu-ent higher than less fluent combinations.
Con-ditioning the probability of each word on itsdeep context has the potential to provide amore meaningful context than the local contextwithin the string.
A comparison of the proba-bilities of individual words in the deep syntaxmodel and traditional language model in Figure2 clearly shows this.
For instance, let us con-sider how the language model in a German toEnglish SMT system is used to help rank thefollowing two translations today congress passed... and today convention passed ... (the wordKongress in German can be translated into ei-ther congress or convention in English).
Inthe deep syntax model, the important compet-ing probabilities are (i) p(congress|<s>pass)and (ii) p(convention|<s>pass), where (i)can be interpreted as the probability of theword congress modifying pass when pass isthe head of the entire sentence and, simi-larly (ii) the probability of the word conven-tion modifying pass when pass is the head ofthe entire sentence.
In the traditional string-based language model, the equivalent compet-ing probabilities are (i) p(congress|<s>today),the probability of congress following today whentoday is the start of the sentence and (ii)p(convention|<s>today), probability of con-vention following today when today is the startof the sentence, showing that the deep syntaxlanguage model is able to use more meaningfulcontext for good lexical choice when estimatingthe probability of words congress and conventioncompared to the traditional language model.In addition, the deep syntax language modelwill encounter less data sparseness problems forsome words than a string-based language model.In many languages words occur that can legiti-mately be moved to different positions within thestring without any change to dependencies be-tween words.
For example, sentential adverbsin English, can legitimately change position ina sentence, without affecting the underlying de-pendencies between words.
The word today in?Today congress passed Obama?s health bill?122can appear as ?Congress passed Obama?s healthbill today?
and ?Congress today passed Obama?shealth bill?.
Any sentence in the training cor-pus in which the word pass is modified by todaywill result in a bigram being counted for the twowords, regardless of the position of today withineach sentence.In addition, some surface form words such asauxiliary verbs for example, are not representedas predicates in the deep syntactic structure.
Forlexical choice, its not really the choice of auxiliaryverbs that is most important, but rather the choiceof an appropriate lexical item for the main verb(that belongs to the auxiliary verb).
Omitting aux-iliary verbs during language modeling could aidgood lexical choice, by focusing on the choice ofa main verb without the effect of what auxiliaryverb is used with it.For some words, however, the probability in thestring-based language model provides as good ifnot better context than the deep syntax model, butonly for the few words that happen to be precededby words that are important to its lexical choice,and this reinforces the idea that SMT systems canbenefit from using both a deep syntax and string-based language model.
For example, the proba-bility of bill in Figures 2(a) and 2(b) is computedin the deep syntax model as p(bill| <s> pass)and in the string-based model using p(bill|healthcare), and for this word the local context seems toprovide more important information than the deepcontext when it comes to lexical choice.
The deepmodel nevertheless adds some useful information,as it includes the probability of bill being an argu-ment of pass when pass is the head of a sentence.In traditional language modeling, the specialstart symbol is added at the beginning of a sen-tence so that the probability of the first word ap-pearing as the first word of a sentence can beincluded when estimating the probability.
Withsimilar motivation, we add a start symbol to thedeep syntactic representation so that the probabil-ity of the head of the sentence occurring as thehead of a sentence can be included.
For exam-ple, p(be| <s>) will have a high probability asthe verb be is the head of many sentences of En-glish, whereas p(colorless| <s>) will have a lowprobability since it is unlikely to occur as the head.We also add end symbols at the leaf nodes in thestructure to include the probability of these wordsappearing at that position in a structure.
For in-stance, a noun followed by its determiner such asp(</s> |attorney a) would have a high probabil-ity compared to a conjunction followed by a verbp(</s> |and be).8 EvaluationWe carry out an experimental evaluation to inves-tigate the potential of the deep syntax languagemodel we describe in this paper independently ofany machine translation system.
We train a 5-gram deep syntax language model on 7M Englishf-structures, and evaluate it by computing the per-plexity and ngram coverage statistics on a held-out test set of parsed fluent English sentences.
Inorder to provide an interesting comparison, wealso train a traditional string-based 5-gram lan-guage model on the same training data and testit on the same held-out test set of English sen-tences.
A deep syntax language model comes withthe obvious disadvantage that any data it is trainedon must be in-coverage of the parser, whereas astring-based language model can be trained on anyavailable data of the appropriate language.
Sinceparser coverage is not the focus of our work, weeliminate its effects from the evaluation by select-ing the training and test data for both the string-based and deep syntax language models on the ba-sis that they are in fact in-coverage of the parser.8.1 Language Model TrainingOur training data consists of English sentencesfrom the WMT09 monolingual training corpuswith sentence length range of 5-20 words that arein coverage of the parsing resources (Kaplan et al,2004; Riezler et al, 2002) resulting in approxi-mately 7M sentences.
Preparation of training andtest data for the traditional language model con-sisted of tokenization and lower casing.
Parsingwas carried out with XLE (Kaplan et al, 2002)and an English LFG grammar (Kaplan et al,2004; Riezler et al, 2002).
The parser producesa packed representation of all possible parses ac-cording to the LFG grammar and we select onlythe single best parse for language model trainingby means of a disambiguation model (Kaplan et123Corpus Tokens Ave. Tokens Vocabper Sent.strings 138.6M 19 345KLFG lemmas/predicates 118.4M 16 280KTable 1: Language model statistics for string-based and deep syntax language models, statistics are forstring tokens and LFG lemmas for the same set of 7.29M English sentencesal., 2004; Riezler et al, 2002).
Ngrams were auto-matically extracted from the f-structures and low-ercased.
SRILM (Stolcke, 2002) was used to com-pute both language models.
Table 1 shows statis-tics on the number of words and lemmas used totrain each model.8.2 TestingThe test set consisted of 789 sentences selectedfrom WMT09 additional development sets3 con-taining English Europarl text and again was se-lected on the basis of sentences being in-coverageof the parsing resources.
SRILM (Stolcke, 2002)was used to compute test set perplexity and ngramcoverage statistics for each order model.Since the deep syntax language model adds endof sentence markers to leaf nodes in the structures,the number of (so-called) end of sentence markersin the test set for the deep syntax model is muchhigher than in the string-based model.
We there-fore also compute statistics for each model whenend of sentence markers are omitted from trainingand testing.
4 In addition, since the vast majorityof punctuation is not represented as predicates inLFG f-structures, we also test the string-based lan-guage model when punctuation has been removed.8.3 ResultsTable 2 shows perplexity scores and ngram cover-age statistics for each order and type of languagemodel.
Note that perplexity scores for the string-based and deep syntax language models are notdirectly comparable because each model has a dif-ferent vocabulary.
Although both models train onan identical set of sentences, the data is in a dif-ferent format for each model, as the string-based3test2006.en and test2007.en4When we include end of sentence marker probabilitieswe also include them for normalization, and omit them fromnormalization when their probabilities are omitted.model is trained on surface form tokens, whereasthe deep syntax model uses lemmas.
Ngram cov-erage statistics provide a better comparison.Unigram coverage for all models is high withall models achieving close to 100% coverage onthe held-out test set.
Bigram coverage is high-est for the deep syntax language model when eosmarkers are included (94.71%) with next high-est coverage achieved by the string-based modelthat includes eos markers (93.09%).
When eosmarkers are omitted bigram coverage goes downslightly to 92.44% for the deep syntax model andto 92.83% for the string-based model, and whenpunctuation is also omitted from the string-basedmodel, coverage goes down again to 91.57%.Trigram coverage statistics for the test set main-tain the same rank between models as in the bi-gram coverage, from highest to lowest as follows:DS+eos at 64.71%, SB+eos at 58.75%, SB-eosat 56.89%, DS-eos at 53.67%, SB-eos-punc at53.45%.
For 4-gram and 5-gram coverage a sim-ilar coverage ranking is seen, but with DS-eos(4gram at 17.17%, 5gram at 3.59%) and SB-eos-punc (4gram at 20.24%, 5gram at 5.76%) swap-ping rank position.8.4 DiscussionNgram coverage statistics for the DS-eos andSB-eos-punc models provide the fairest com-parison, with the deep syntax model achiev-ing higher coverage than the string-based modelfor bigrams (+0.87%) and trigrams (+0.22%),marginally lower coverage coverage of unigrams(-0.02%) and lower coverage of 4-grams (-3.07%)and 5-grams (2.17%) compared to the string-based model.Perplexity scores for the deep syntax modelwhen eos symbols are included are low (79 for the5gram model) and this is caused by eos markers1241-gram 2-gram 3-gram 4-gram 5-gramcov.
ppl cov.
ppl cov.
ppl cov.
ppl cov.
pplSB-eos 99.61% 1045 92.83% 297 56.89% 251 23.32% 268 7.19% 279SB-eos-punc 99.58% 1357 91.57% 382 53.45% 327 20.24% 348 5.76% 360DS-eos 99.56% 1005 92.44% 422 53.67% 412 17.17% 446 3.59% 453SB+eos 99.63% 900 93.09% 227 58.75% 194 25.48% 207 8.35% 215DS+eos 99.70% 211 94.71% 77 64.71% 73 29.86% 78 8.75% 79Table 2: Ngram coverage and perplexity (ppl) on held-out test set.
Note: DS = deep syntax, SB string-based, eos = end of sentence markersin the test set in general being assigned relativelyhigh probabilities by the model, and since severaloccur per sentence, the perplexity increases whenthe are omitted (453 for the 5gram model).Tables 3 and 4 show the most frequently en-countered trigrams in the test data for each typeof model.
A comparison shows how different thetwo models are and highlights the potential of thedeep syntax language model to aid lexical choicein SMT systems.
Many of the most frequently oc-curring trigram probabilities for the deep syntaxmodel are for arguments of the main verb of thesentence, conditioned on the main verb, and in-cluding such probabilities in a system could im-prove fluency by using information about whichwords are in a dependency relation together ex-plicitely in the model.
In addition, a frequent tri-gram in the held-out data is <s> be also, wherethe word also is a sentential adverb modifyingbe.
Trigrams for sentential adverbs are likely tobe less effected by data sparseness in the deepsyntax model compared to the string-based modelwhich could result in the deep syntax model im-proving fluency with respect to combinations ofmain verbs and their modifying adverbs.
The mostfrequent trigram in the deep syntax test set is <s>and be, in which the head of the sentence is theconjunction and with argument be.
In this type ofsyntactic construction in English, its often the casethat the conjunction and verb will be distant fromeach other in the sentence, for example: Nobodywas there except the old lady and without thinkingwe quickly left.
(where was and and are in a de-pendency relation).
Using a deep syntax languagemodel could therefore improve lexical choice forsuch words, since they are too distant for a string-3-gram No.
Occ.
Prob.<s> and be 42 0.1251<s> be this 21 0.0110<s> must we 19 0.0347<s> would i 19 0.0414<s> be in 17 0.0326<s> be that 14 0.0122be debate the 13 0.0947<s> be debate 13 0.0003<s> can not 12 0.0348<s> and president 11 0.0002<s> would like 11 0.0136<s> would be 11 0.0835<s> be also 10 0.0075Table 3: Most frequent trigrams in test set for deepsyntax modelbased model.9 ConclusionsWe presented a comparison of a deep syntaxlanguage and traditional string-based languagemodel.
Results showed that the deep syntax lan-guage model achieves similar ngram coverage tothe string-based model on a held out test set.We highlighted the potential of integrating sucha model into SMT systems for improving lexicalchoice by using a deeper context for probabilitiesof words compared to a string-based model.ReferencesBojar, Ondr?ej, Jan Hajic?.
2008.
Phrase-Based andDeep Syntactic English-to-Czech Statistical Ma-chine Translation.
In Proceedings of the third Work-1253-gram No.
Occ.
Prob.mr president , 40 0.5385<s> this is 25 0.1877by the european 20 0.0014the european union 18 0.1096<s> it is 16 0.1815the european parliament 15 0.0252would like to 15 0.4944<s> i would 15 0.0250<s> that is 14 0.1094i would like 14 0.0335and gentlemen , 13 0.1005ladies and gentlemen 13 0.2834<s> we must 12 0.0120should like to 12 0.1304i should like 11 0.0089, ladies and 11 0.5944, it is 10 0.1090Table 4: Most frequent trigrams in test set forstring-based modelshop on Statistical Machine Translation, Columbus,Ohio.Bresnan, Joan.
2001.
Lexical-Functional Syntax.,Blackwell Oxford.Chiang, David.
2007.
Hierarchical Phrase-basedModels of Translation In Computational Linguis-tics, No.
33:2.Chiang, David.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 263-270,Ann Arbor, Michigan.Dalrymple, Mary.
2001.
Lexical Functional Gram-mar, Academic Press, San Diego, CA; London.Kaplan, Ronald, Stefan Riezler, Tracy H. King, JohnT.
Maxwell, Alexander Vasserman.
2004.
Speedand Accuracy in Shallow and Deep Stochastic Pars-ing.
In Proceedings of Human Language Tech-nology Conference/North American Chapter of theAssociation for Computational Linguistics Meeting,Boston, MA.Kaplan, Ronald M., Tracy H. King, John T. Maxwell.2002.
Adapting Existing Grammars: the XLE Ex-perience.
In Proceedings of the 19th InternationalConference on Computational Linguistics (COL-ING) 2002, Taipei, Taiwan.Kaplan, Ronald M. 1995.
The Formal Architecture ofLexical Functional Grammar.
In Formal Issues inLexical Functional Grammar, ed.
Mary Dalrymple,pages 7-28, CSLI Publications, Stanford, CA.Kaplan, Ronald M., Joan Bresnan.
1982.
LexicalFunctional Grammar, a Formal System for Gram-matical Represenation.
In J. Bresnan, editor, TheMental Representation of Grammatical Relations,173-281, MIT Press, Cambridge, MA.Koehn, Philipp, Hieu Hoang.
2007.
Factored Trans-lation Models.
Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, 868-876.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicoli Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.Annual Meeting of the Association for Computa-tional Linguistics, demonstration sessionKoehn, Philipp 2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe tenth Machine Translation Summit.Koehn, Philipp, Franz Josef Och, Daniel Marcu.
2003.Statistical Phrase-based Translation.
In Proceed-ings of Human Language Technology and NorthAmerican Chapter of the Association for Computa-tional Linguistics Conference, 48-54.Riezler, Stefan, John T. Maxwell III.
2006.
Grammat-ical Machine Translation.
In Proceedings of HLT-ACL, pages 248-255, New York.Riezler, Stefan, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, Mark Johnson.2002.
Parsing the Wall Street Journal using LexicalFunctional Grammar and Discriminitive EstimationTechniques .
(grammar version 2005) In Proceed-ings of the 40th ACL, Philadelphia.Shen, Libin, Jinxi Xu, Ralph Weischedel.
2008.A New String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
Proceedings of ACL-08: HLT, pages 577-585.Stolcke, Andreas.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing, Denver, Colorado.Dekai, Wu, Hongsing Wong.
1998.
Machine Trans-lation with a Stochastic Grammatical Channel.
InProceedings of the 36th ACL and 17th COLING,Montreal, Quebec.126
