Proceedings of NAACL HLT 2009: Short Papers, pages 245?248,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsQuadratic Features and Deep Architectures for ChunkingJoseph Turian and James Bergstra and Yoshua BengioDept.
IRO, Universite?
de Montre?alAbstractWe experiment with several chunking models.Deeper architectures achieve better gener-alization.
Quadratic filters, a simplificationof a theoretical model of V1 complex cells,reliably increase accuracy.
In fact, logisticregression with quadratic filters outperformsa standard single hidden layer neural network.Adding quadratic filters to logistic regressionis almost as effective as feature engineering.Despite predicting each output label indepen-dently, our model is competitive with onesthat use previous decisions.1 IntroductionThere are three general approaches to improvingchunking performance: engineer better features,improve inference, and improve the model.Manual feature engineering is a common direc-tion.
One technique is to take primitive featuresand manually compound them.
This technique iscommon, and most NLP systems use n-gram basedfeatures (Carreras and Ma`rquez, 2003; Ando andZhang, 2005, for example).
Another approach islinguistically motivated feature engineering, e.g.Charniak and Johnson (2005).Other works have looked in the direction ofimproving inference.
Rather than predicting eachdecision independently, previous decisions can beincluded in the inference process.
In this work,we use the simplest approach of modeling eachdecision independently.The third direction is by using a better model.
Ifmodeling capacity can be added without introducingtoo many extra degrees of freedom, generalizationcould be improved.
One approach for compactlyincreasing capacity is to automatically induceintermediate features through the composition ofnon-linearities, for example SVMs with a non-linearkernel (Kudo and Matsumoto, 2001), inducingcompound features in a CRF (McCallum, 2003),neural networks (Henderson, 2004; Bengio and Le-Cun, 2007), and boosting decision trees (Turian andMelamed, 2006).
Recently, Bergstra et al (2009)showed that capacity can be increased by addingquadratic filters, leading to improved generalizationon vision tasks.
This work examines how wellquadratic filters work for an NLP task.
Compared tomanual feature engineering, improved models areappealing because they are less task-specific.We experiment on the task of chunking (Sang andBuchholz, 2000), a syntactic sequence labeling task.2 Sequence labelingBesides Collobert and Weston (2008), previouswork on sequence labeling usually use previousdecisions in predicting output labels.
Here we donot take advantage of the dependency between suc-cessive output labels.
Our approach predicts eachoutput label independently of the others.
This allowsus to ignore inference during training: The modelmaximizes the conditional likelihood of each outputlabel independent of the output label of other tokens.We use a sliding window approach.
The outputlabel for a particular focus token xi is predictedbased upon k?
tokens before and after xi.
The entirewindow is of size k = 2 ?
k?
+ 1.
Nearly all work onsequence labeling uses a sliding window approach(Kudo and Matsumoto, 2001; Zhang et al, 2002;245204 204 204 204 204 204 204150 150 150 150 150 150 150tokwinoutin40023?h?q?qFigure 1: Illustration of our baseline I-T-W-O model (seeSecs.
4 and 5.1).
The input layer comprises seven tokenswith 204 dimensions each.
Each token is passed througha shared 150-dimensional token feature extractor.
These7 ?
150 features are concatenated and 400 features areextracted from them in the window layer.
These 400 fea-tures are the input to the final 23-class output prediction.Feature extractors ?q and ?h are described in Section 3.Carreras and Ma`rquez, 2003; Ando and Zhang,2005, for example).
We assume that each token xcan be transformed into a real-valued feature vector?
(x) with l entries.
The feature function will bedescribed in Section 4.A standard approach is as follows: We firstconcatenate the features of k tokens into one vector[?(xi?k?
), .
.
.
, ?(xi+k?)]
of length k ?
l entries.
We canthen pass [?(xi?k?
), .
.
.
, ?(xi+k?)]
to a feature extractorover the entire window followed by an outputlog-linear layer.Convolutional architectures can help when thereis a position-invariant aspect to the input.
In machinevision, parameters related to one part of the imageare sometimes restricted to be equal to parametersrelated to another part (LeCun et al, 1998).
Aconvolutional approach to sequence labeling is asfollows: At the lowest layer we extract features fromindividual tokens using a shared feature extractor.These higher-level individual token features are thenconcatenated, and are passed to a feature extractorover the entire window.In our baseline approach, we apply one convolu-tional layer of feature extraction to each token (onetoken layer), followed by a concatenation, followedby one layer of feature extraction over the entirewindow (one window layer), followed by a 23-Doutput prediction using multiclass logistic regres-sion.
We abbreviate this architecture as I-T-W-O(inputtokenwindowoutput).
See Figure 1 foran illustration of this architecture.3 Quadratic feature extractorsThe most common feature extractor in the literatureis a linear filter h followed by a non-linear squashing(activation) function ?
:f (x) = ?
(h(x)), h(x) = b +Wx.
(1)In our experiments, we use the softsign squash-ing function ?
(z) = z/(1 + |z|).
n-class lo-gistic regression predicts ?
(h(x)), where softmax?i(z) = exp(zi)/?k exp(zk).
Rust et al (2005) arguesthat complex cells in the V1 area of visual cortexare not well explained by Eq.
1, but are insteadbetter explained by a model that includes quadraticinteractions between regions of the receptive field.Bergstra et al (2009) approximate the model ofRust et al (2005) with a simpler model of theform given in Eq.
2.?
In this model, the pre-squashtransformation q includes J quadratic filters:f (x) = ?
(q(x)), q(x) =?????????
?b +Wx +??
J?j=1(V jx)2??????????
(2)where b,W, and V1 .
.
.VJ are tunable parameters.In the vision experiments of Bergstra et al(2009), using quadratic filters improved the gen-eralization of the trained architecture.
We wereinterested to see if the increased capacity wouldalso be beneficial in language tasks.
For our logisticregression (I-O) experiments, the architecture isspecifically I?
?qO, i.e.
output O is the softmax?
applied to the quadratic transform q of the inputI.
Like Bergstra et al (2009), in architectures withhidden layers, we apply the quadratic transform qin all layers except the final layer, which uses lineartransform h. For example, I-T-W-O is specificallyI??qT??qW?
?hO, as shown in Figure 1.
Futurework will explore if generalization is improved byusing q in the final layer.4 FeaturesHere is a detailed description of the types of featureswe use, with number of dimensions:?
embeddings.
We map each word to a real-valued50-dimensional embedding.
These embeddingswere obtained by Collobert and Weston (2008), and?
Bergstra et al (2009) do not use a sqrt in Eq.
2.
We found thatsqrt improves optimization and gives better generalization.246were induced based upon a purely unsupervisedtraining strategy over the 631 million words in theEnglish Wikipedia.?
POS-tag.
Part-of-speech tags were assigned auto-matically, and are part of the CoNLL data.
45 dim.?
label frequencies.
Frequency of each labelassigned to this word in the training and validationdata.
From Ando and Zhang (2005).
23 dim.?
type(first character).
The type of the first charac-ter of the word.
type(x) = ?A?
if x is a capital letter,?a?
if x is a lowercase letter, ?n?
if x is a digit, and xotherwise.
From Collins (2002).
20 dim.?
word length.
The length of the word.
20 dim.?
compressed word type.
We convert each char-acter of the word into its type.
We then remove anyrepeated consecutive type.
For example, ?Label-making?
?
?Aa-a?.
From Collins (2002).
46 dim.The last three feature types are based upon ortho-graphic information.
There is a combined total of204 features per token.5 ExperimentsWe follow the conditions in the CoNLL-2000shared task (Sang and Buchholz, 2000).
Of the 8936training sentences, we used 1000 randomly sampledsentences (23615 words) for validation.5.1 Training detailsThe optimization criterion used during training isthe maximization of the sum (over word positions)of the per-token log-likelihood of the correct deci-sion.
Stochastic gradient descent is performed usinga fixed learning rate ?
and early stopping.
Gradientsare estimated using a minibatch of 8 examples.
Wefound that a learning rate of 0.01, 0.0032, or 0.001was most effective.In all our experiments we use a window sizeof 7 tokens.
In preliminary experiments, smallerwindows yielded poorer results, and larger oneswere no better.
Layer sizes of extracted featureswere chosen to optimize validation F1.5.2 ResultsWe report chunk F-measure (F1).
In some tableswe also report Acc, the per-token label accuracy,post-Viterbi decoding.Figure 2 shows that using quadratic filters reliablyimproves generalization on all architectures.
Forthe I-T-W-O architecture, quadratic filters increase91.5%92%92.5%93%93.5%94%94.5%95%0  1  2  4  8  1691.5%92%92.5%93%93.5%94%94.5%95%# of quadratic filtersI-T-W-O (baseline)I-W-O (1 hidden layer NN)I-O (LogReg)Figure 2: Validation F1 (y-axis) as we vary the numberof quadratic filters (x-axis), over different model archi-tectures.
Both architecture depth and quadratic filtersimprove validation F1.Architecture #qf Acc F1I-O 16 96.45 93.94I-W(400)-O 4 96.66 94.39I-T(150)-W(566)-O 2 96.85 94.77I-T(150)-W(310)-W(310)-O 4 96.87 94.82Table 1: Architecture experiments on validation data.The first column describes the layers in the architecture.
(The architecture in Figure 1 is I-T(150)-W(400)-O.
)The second column gives the number of quadratic filters.For each architecture, the layer sizes and number ofquadratic filters are chosen to maximize validation F1.Deeper architectures achieve higher F1 scores.validation F1 by an absolute 0.31.
Most surpris-ingly, logistic regression with 16 filters achievesF1=93.94, which outperforms the 93.83 of a stan-dard (0 filter) single hidden layer neural network.With embeddings as the only features, logregwith 0 filters achieves F1=85.36.
By adding allfeatures, we can raise the F1 to 91.96.
Alternately,by adding 16 filters, we can raise the F1 to 91.60.
Inother words, adding filters is nearly as effective asour manual feature engineering.Table 1 shows the result of varying the overallarchitecture.
Deeper architectures achieve higherF1 scores.
Table 2 compares the model as we lesionoff different features.
POS tags and the embeddingswere the most important features.We applied our best model overall (I-T-W-W-Oin Table 1) to the test data.
Results are shown in247Feature set Acc F1default 96.81 94.69no orthographic features 96.84 94.62no label frequencies 96.77 94.58no POS tags 96.60 94.22no embeddings 96.40 93.97only embeddings 96.18 93.53Table 2: Results on validation of varying the feature set,for the architecture in Figure 1 with 4 quadratic filters.NP F1 Prc Rcl F1AZ05 94.70 94.57 94.20 94.39KM01 94.39 93.89 93.92 93.91I-T-W-W-O 94.44 93.72 93.91 93.81CM03 94.41 94.19 93.29 93.74SP03 94.38 - - -Mc03 93.96 - - -AZ05- - 93.83 93.37 93.60ZDJ02 93.89 93.54 93.60 93.57Table 3: Test set results for Ando and Zhang (2005), Kudoand Matsumoto (2001), our I-T-W-W-O model, Carrerasand Ma`rquez (2003), Sha and Pereira (2003), McCallum(2003), Zhang et al (2002), and our best I-O model.AZ05- is Ando and Zhang (2005) using purely supervisedtraining, not semi-supervised training.
Scores are nounphrase F1, and overall chunk precision, recall, and F1.Table 3.
We are unable to compare to Collobert andWeston (2008) because they use a different trainingand test set.
Our model predicts all labels in thesequence independently.
All other works in Table 3use previous decisions when making the currentlabel decision.
Our approach is nonetheless compet-itive with approaches that use this extra information.6 ConclusionsMany NLP approaches underfit important linguisticphenomena.
We experimented with new techniquesfor increasing chunker model capacity: addingdepth (automatically inducing intermediate featuresthrough the composition of non-linearities), andincluding quadratic filters.
Higher accuracy wasachieved by deeper architectures, i.e.
ones withmore intermediate layers of automatically tuned fea-ture extractors.
Although they are a simplification ofa theoretical model of V1 complex cells, quadraticfilters reliably improved generalization in all archi-tectures.
Most surprisingly, logistic regression withquadratic filters outperformed a single hidden layerneural network without.
Also, with logistic regres-sion, adding quadratic filters was almost as effectiveas manual feature engineering.
Despite predictingeach output label independently, our model iscompetitive with ones that use previous decisions.AcknowledgmentsThank you to Ronan Collobert, Le?on Bottou, andNEC Labs for access to their word embeddings, andto NSERC and MITACS for financial support.ReferencesR.
Ando and T. Zhang.
A high-performance semi-supervised learning method for text chunking.
In ACL,2005.Y.
Bengio and Y. LeCun.
Scaling learning algorithmstowards AI.
In Large Scale Kernel Machines.
2007.J.
Bergstra, G. Desjardins, P. Lamblin, and Y. Bengio.Quadratic polynomials learn better image features.
TR1337, DIRO, Universite?
de Montre?al, 2009.X.
Carreras and L. Ma`rquez.
Phrase recognition byfiltering and ranking with perceptrons.
In RANLP, 2003.E.
Charniak and M. Johnson.
Coarse-to-fine n-best pars-ing and MaxEnt discriminative reranking.
In ACL, 2005.M.
Collins.
Ranking algorithms for named entity extrac-tion: Boosting and the voted perceptron.
In ACL, 2002.R.
Collobert and J. Weston.
A unified architecture fornatural language processing: Deep neural networks withmultitask learning.
In ICML, 2008.J.
Henderson.
Discriminative training of a neuralnetwork statistical parser.
In ACL, 2004.T.
Kudo and Y. Matsumoto.
Chunking with supportvector machines.
In NAACL, 2001.Y.
LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradientbased learning applied to document recognition.
IEEE,86(11):2278?2324, November 1998.A.
McCallum.
Efficiently inducing features of condi-tional random fields.
In UAI, 2003.N.
Rust, O. Schwartz, J.
A. Movshon, and E. Simoncelli.Spatiotemporal elements of macaque V1 receptive fields.Neuron, 46(6):945?956, 2005.E.
T. Sang and S. Buchholz.
Introduction to theCoNLL-2000 shared task: Chunking.
In CoNLL, 2000.F.
Sha and F. C. N. Pereira.
Shallow parsing withconditional random fields.
In HLT-NAACL, 2003.J.
Turian and I. D. Melamed.
Advances in discriminativeparsing.
In ACL, 2006.T.
Zhang, F. Damerau, and D. Johnson.
Text chunkingbased on a generalization of Winnow.
JMLR, 2, 2002.248
