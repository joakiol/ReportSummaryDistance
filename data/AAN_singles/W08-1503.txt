Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 9?16Manchester, August 2008An Integrated Dialog Simulation Technique for Evaluating Spoken DialogSystemsSangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae LeeDepartment of Computer Science and EngineeringPohang University of Computer Science and Technology(POSTECH)San 31, Hyoja-Dong, Pohang, 790-784, Korea{hugman, lcj80, getta, gblee}@postech.ac.krAbstractThis paper proposes a novel integrated dialogsimulation technique for evaluating spoken di-alog systems.
Many techniques for simulat-ing users and errors have been proposed foruse in improving and evaluating spoken dia-log systems, but most of them are not easilyapplied to various dialog systems or domainsbecause some are limited to specific domainsor others require heuristic rules.
In this pa-per, we propose a highly-portable technique forsimulating user intention, utterance and Au-tomatic Speech Recognition (ASR) channels.This technique can be used to rapidly build adialog simulation system for evaluating spo-ken dialog systems.
We propose a novel userintention modeling and generating method thatuses a linear-chain conditional random field, adata-driven domain specific user utterance sim-ulation method, and a novel ASR channel sim-ulation method with adjustable error recogni-tion rates.
Experiments using these techniqueswere carried out to evaluate the performanceand behavior of previously developed dialogsystems designed for navigation dialogs, andit turned out that our approach is easy to set upand shows the similar tendencies of real users.1 IntroductionEvaluation of spoken dialog systems is essential for de-veloping and improving the systems and for assessingtheir performance.
Normally, humans are used to eval-uate the systems, but training and employing humanevaluators is expensive.
Furthermore, qualified humanusers are not always immediately available.
These in-evitable difficulties of working with human users cancause huge delay in development and assessment ofc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.spoken dialog systems.
To avoid the problems that re-sult from using humans to evaluate spoken dialog sys-tems, developers have widely used dialog simulation,in which a simulated user interacts with a spoken dia-log system.Many techniques for user intention, utterance and er-ror simulation have been proposed.
However, previ-ously proposed simulation techniques cannot be eas-ily applied to evaluate various dialog systems, becausesome of these techniques are specially designed to workwith their own dialog systems, some require heuristicrules or flowcharts, and others try to build user sidedialog management systems using specialized dialogmanaging methods.
These problems motivated us todevelop dialog simulation techniques which allow de-velopers to build dialog simulation systems rapidly foruse in evaluating various dialog systems.To be successful, a simulation approach should notdepend on specific domains or rules.
Also it should notbe coupled to a specific dialog management method.Furthermore, successful dialog simulation should fullysupport both user simulation and environment simula-tion.
In user simulation, it must be capable of simu-lating both user intentions and user utterances, becauseuser utterances are essential for testing the language un-derstanding component of the dialog system.
In addi-tion to user simulation, environment simulation such asASR channel simulation is desirable because it allowsdevelopers to test the dialog system in various acousticenvironments.In this paper, we propose novel dialog simulationtechniques which satisfy these requirements.
We in-troduce a new user intention simulation method basedon the sequential graphical model, and a user utterancesimulator which can generate diverse natural user utter-ances.
The user intention and utterance simulators areboth fully data-driven approaches; therefore they havehigh domain- and language portability.
We also proposea novel Automatic Speech Recognizer (ASR) channelsimulator which allows the developers to set the de-sired speech recognition performance level.
Througha case study, we showed that our approach is feasible insuccessful dialog simulation to evaluate spoken dialog9systems.This paper is structured as follows.
We first provide abrief introduction of other dialog simulation techniquesand their differences from our approach in Section 2.We then introduce the overall architecture and the de-tailed methods of intention, utterance and ASR channelsimulation in Section 3.
Experiments to test the simula-tion techniques, and a case study are described in Sec-tion 4.
We conclude with a brief summary and suggestdirections for future work in Section 5.2 Related WorksDialog simulation techniques can be classified accord-ing to the purpose of the simulation.
One of the pur-poses is to support the refinement of dialog strategies.Some techniques use large amounts of simulated datafor a systematic exploration of the dialog state spacein the framework of reinforcement learning (Schatz-mann et al, 2005; Schatzmann et al, 2007a).
Othertechniques use simulation techniques to investigate andimprove the target dialog strategies by examining theresults heuristically or automatically (Chung, 2004;Rieser and Lemon, 2006; Torres et al, 2008).
A sec-ond purpose of dialog simulation techniques is to eval-uate the dialog system itself qualitatively.
Eckert et al,(1997) and Lo?pez-Co?zar et., (2003; 2006) used a dialogsimulation to evaluate whole dialog systems.Dialog simulation techniques can also be classifiedaccording to the layers of the simulation.
Typically, di-alog simulation can be divided into three layers: userintention, user surface (utterance) and error simulation.Some studies have focused only on the intention levelsimulation (Rieser and Lemon, 2006; Schatzmann etal., 2007b; Cuayahuitl et al, 2005).
The main purposeof those approaches was to collect and examine inten-tion level dialog behavior for automatically learning di-alog strategies.
In this case, surface and error simula-tions were neglected or simply accessed normally.Another approach is to simulate both user intentionand surface.
In this approach, user utterance generationis designed to express a given intention.
Chung (2004)tried to use the natural language generation moduleof (Seneff, 2002) to generate this surface.
He used aspeech synthesizer to generate user utterances.
Lo?pez-Co?zar et., (2003; 2006) collected real human utter-ances, and selected and played the voice to provide in-put for the spoken dialog system.
Both Chung (2004)and Lo?pez-Co?zar et., (2003; 2006) used rule based in-tention simulation.
They used real ASR to recognizethe synthesized or played voice; hence, ASR channelsimulation is not needed in their techniques.
Schefflerand Young (2000; 2001) used the lattices which are de-rived from the grammars used by the recognition en-gine, but generated user utterances by associating thelattice edges with intentions.
During utterance gener-ation, they simulated errors in recognition and under-standing by probabilistic substitution on the selection ofthe edge.
Schatzmann et al, (2007a; 2007b) proposed astatistical model for user utterance generation and errorsimulation using agenda based intention simulation.The existing rule-based techniques for simulating in-tentions or surfaces are not appropriate in the sense ofportability criteria.
In addition, specific dialog manag-ing techniques based user simulators (e.g., (Torres etal., 2008)) are not desirable because it is not easy toimplement these techniques for other developers.
An-other important criterion for evaluating dialog simula-tion techniques for use in evaluating spoken dialog sys-tems is the range of simulation layers.
Simulations thatare restricted to only the intention level are not suffi-cient to evaluate the whole dialog system.
Domain andlanguage independent techniques for simulating bothintentions and utterances are needed, and ASR channelsimulation is desirable for evaluating the spoken dia-log systems accurately because human-machine dialogis heavily influenced by speech recognition errors.3 Dialog Simulation Architecture forDialog System Evaluation3.1 Overall ArchitectureTypical spoken dialog systems deal with the dialog be-tween a human user and a machine.
Human users ut-ter spoken language to express their intention, which isrecognized, understood and managed by ASR, SpokenLanguage Understanding (SLU) and Dialog Manager(DM).
Conventionally, ASR has been considered to bea component of dialog systems.
However, in this re-search, we do not include a real ASR module in the di-alog system component because a real ASR takes onlyfixed level of speech as an input.
To use real voices,we must either collect real human speech or generatevoices using a speech synthesizer.
However, both ap-proaches have limitations.
When recording and play-ing real human voices, the cost of data collection ishigh and the simulator can simulate only the behav-ior of the humans who were recorded.
When using aspeech synthesizer, the synthesizer can usually generatethe speech of one person, on a limited variety of speechbehaviors; this means that the dialog system cannot beevaluated under various conditions.
Also, in both ap-proaches, freely adjusting the speech recognition per-formance level is difficult.
In this research, instead ofusing real speech we simulate the ASR channel and addnoises to a clean utterance from the user simulator tomimic the speech recognition result.The overall architecture of our dialog simulation sep-arates the user simulator into two levels: user intentionsimulator and utterance simulator (Fig.
1).
The userintention simulator accepts the discourse circumstanceswith system intention as input and generates the nextuser intention.
The user utterance simulator constructsa corresponding user sentence to express the given userintention.
The simulated user sentence is fed to theASR channel simulator, which then adds noises to theutterance.
This noisy utterance is passed to a dialog sys-10Dialog SystemUser SimulatorASRChannelSimulator SLUDialog ManagerUser Intention SimulatorUser Utterance SimulatorSystem IntentionDialog LogsEvaluator EvaluationResults= Simulated User = = System =Figure 1: Overall architecture of dialog simulationtem which consists of a SLU and a DM.
The dialog sys-tem understands the user utterance, manages dialog andpasses the system intention to the user simulator.
Usersimulator, ASR channel simulator and dialog system re-peat the conversation until the user simulator generatesan end to the dialog.After finishing simulating one dialog successfully,this dialog is stored in Dialog Logs.
If the dialog logscontain enough dialogs, the evaluator uses the logs toevaluate the performance of the dialog system.3.2 User Intention SimulationThe task of user intention simulation is to generate sub-sequent user intentions given current discourse circum-stances.
The intention is usually represented as ab-stracted user?s goals and information on user?s utter-ance (surface).
In other words, generating the user?snext semantic frame from the current discourse statusconstitutes the user intention simulation.Dialog is basically sequential behavior in which par-ticipants use language to interact with each other.
Thismeans that intentions of the user or the system are natu-rally embedded in a sequential structure.
Therefore, inintention modeling we must consider how to model thissequential property.
Also, we must understand that theuser?s intention depends not only on previous n-gramuser and system intentions, but also on diverse dis-course circumstances, including dialog goal, the num-ber of items, and the number of filled component slots.Sophisticated user intention modeling should be able toreflect the discourse information.To satisfy the sequential property and use richinformation for user intention modeling, we usedlinear-chain Conditional Random Field (CRF) model(Lafferty et al, 2001) for user intention modeling.Let Y,X be random vectors, ?
= {?k} ?
RK be aparameter vector, and {fk(y, y?,xt)}Kk=1be a set ofreal-valued feature functions.
Then a linear-chain CRFis a distribution of p(y|x) that takes the formUI1DI1UI2DI2UItDItUIt+1DIt+1?Figure 2: Conditional Random Fields for user intentionmodeling.
UIt: User Intention ; DIt: Discourse Infor-mation for the tth user turnp(y|x) =1Z(x)exp{K?k=1?kfk(yt, yt?1,xt)} (1)where Z(x) is an instance-specific normalization func-tion.Z(x) =?yexp{K?k=1?kfk(yt, yt?1,xt)}CRF is an undirected graphical model that defines asingle log-linear distribution over the joint probabilityof an entire label sequence given a particular observa-tion sequence.
This single distribution removes the per-state normalization requirement and allows entire statesequences to be accounted for at once.
This property iswell suited to model the entire sequence of intentions ina dialog.
Also, CRF is a conditional model, and not ajoint model (such as the Hidden Markov Model).
Arbi-trary facts can be captured to describe the observationin the form of indicator functions.
This means that CRFallows us to use rich discourse information to model in-tentions.CRF has states and observations in each time line.We represent the user intention as state and discourseinformation as observations in CRF (Fig.
2).
We rep-resent the state as a semantic frame.
For example inthe semantic frame representing the user intention forthe utterance ?I want to go to city hall?
(Fig.
3), dia-log act is a domain-independent label of an utterance atthe level of illocutionary force (e.g.
statement, request,wh question) and main goal is the domain-specific usergoal of an utterance (e.g.
give something, tell purpose).Component slots represent named entities in the utter-ance.
We use the cartesian product of each slot of se-mantic frame to represent the state of the utterance inour CRF model.
In this example, the state symbol is?request?search loc?loc name?.For the observation, we can use various discourseevents because CRF allows using rich information byinterpreting each event as an indicator function.
Be-cause we pursue the portable dialog simulation tech-nique, we separated the features of the discourse in-formation into those that are domain independent andthose that are domain dependent.
Domain independent11I want to go to city hall.requestsearch_loccityhallI/PRP want/VB to/TO go/VB to/TO [loc_name]/[loc_name]PRP, VB, TO, VB, TO, [loc_name]I, want, to, go, to, [loc_name]Structure PRP ?
VB ?
TO ?
VB ?
TO ?
[loc_name]I ?
want ?
to ?
go ?
to ?
[loc_name]Semantic Frame for User Inention SimulationPreprocessing Information for User Utterance SimulationStructure TagsWord Vocabularyprocessed  utteranceGeneration Target for User Utterance SimulationWord Sequenceraw user utterance dialog_act main_goal component.
[loc_name]Figure 3: Example of semantic frame for user inten-tion, and preprocessing and generation target for userutterance simulation.features include discourse information which is not rel-evant to the specific dialog domain and system.
For ex-ample, previous system acts in Fig.
4 are not dependenton specific dialog domain.
The actual values of pre-vious system acts could be dependent on each dialogdomain and system, but the label itself is independentbecause every dialog system has system parts and corre-sponding system acts.
In contrast, domain specific dis-course information exists for each dialog system.
Forexample, in the navigation domain (Fig.
4), the cur-rent position of the user or the user?s favorite restau-rant could be very important for generating the user?sintention.
This information is dependent on the spe-cific domain and system.
We handle these features as?OTHER INFO?.We trained the user intention model using dialog ex-amples of human-machine.
One training example con-sists of a sequence of user intentions and discourse in-formation features in a given dialog.
We collected train-ing examples and trained the intention model using atypical CRF training method, a limited-memory quasi-Newton code for unconstrained optimization (L-BFGS)of (Liu and Nocedal, 1989).To generate user intentions given specific discoursecircumstances, we calculate the probability of a se-quence of user intentions from the beginning of thedialog to the corresponding turn.
For example, sup-pose that we need to generate user intention at thethird turn (UI3) (Fig.
2).
We have previously sim-ulated user intentions UI1and UI2using DI1andDI2.
In this case, we calculate the probability ofUI1?
UI2?
UI3given DI1, DI2and DI3.
No-tice that DI3contains discourse information at the thirdturn: it includes previous system intention, attributesand other useful information.
Using the algorithm (Fig.5) we generate the user intention at turn t. The proba-bility of P (UI1, UI2, .
.
.
, UIt|DI1, DI2, .
.
.
, DIt) iscalculated using the equation (1).
In the genera-tion of user intention at t turn, we do not select theUItwhich has higher probability.
Instead, we se-lect UItrandomly based on the probability distributionPREV_1_SYS_ACT previous system action.Ex) PREV_1_SYS_ACT=confirmPREV_1_SYS_ACT_ATTRIBUTES previous system mentioned attributes.Ex) PREV_1_SYS_ACT_attributes=city_namePREV_2_SYS_ACT previous system action.Ex) PREV_2_SYS_ACT=confirmPREV_2_SYS_ACT_ATTRIBUTES previous system mentioned attributes.Ex) PREV_2_SYS_ACT_attributes=city_nameSYSTEM_HOLDING_COMP_SLOT system recognized component slot.Ex) SYSTEM_HOLDING_COMP_SLOT=loc_nameOTHER_INFO other useful domain dependent informationEx) OTHER_INFO(user_fav_rest)=gajokjungDomain Independent FeaturesDomain Dependent FeaturesFigure 4: Example feature design for navigation do-mainUI t   ?
user intention at t turnS  ?
user intentions set (UI t  ?
S )UI 1 , UI 2 , ?
, UI t-1  ?
already simulated user intention sequenceDI 1 , DI 2 , ?
, DI t  ?
discourse information from 1 to t  turnFor each UI t  in SCalculate P( UI 1 , UI 2 , ?, UI t |DI 1 , DI 2 , ?, DI t )UI t  ?
random user intention from P( UI 1 , UI 2 , ?, UI t |DI 1 , DI 2 , ?, DI t )Figure 5: User intention generation algorithmP (UI1, UI2, .
.
.
, UIt|DI1, DI2, .
.
.
, DIt) because wewant to generate diverse user intention sequence giventhe same discourse context.
If we select UItwhich hashighest probability, user intention simulator always re-turns the same user intention sequence.3.3 User Utterance SimulationUtterance simulation generates surface level utteranceswhich express a given user intention.
For example, ifusers want to go somewhere and provide place nameinformation, we need to generate corresponding utter-ances (e.g.
?I want to go to [place name] or ?Let?s go to[place name]?).
We approach the task of user utterancesimulation by assuming that the types of structures andthe vocabulary are limited when we make utterances toexpress certain context and intention in a specific do-main, and that humans express their intentions by re-combining and re-aligning these structures and vocabu-laries.To model this process, we need to collect the types ofstructures and vocabularies.
For this, we need to definethe context space.
We define the structure and vocabu-lary space as a production of dialog act and main goal.In an example of semantic frame for the utterance ?Iwant to go to city hall?
(Fig.
3), the structure and vocab-ulary (SV) space ID is ?request # search loc?, which isproduced by the dialog act and the main goal.
We col-lect structure tags, which consist of a part of speechtag, a component slot tag, and a vocabulary that cor-responds to SV space.
For example (Fig.
3), structure121.
Repeat generate  S t  based on PSV(S t+1 |S t ),    until S T  = <setence_end> , where S t  ?
S  ,   t=1,2,3,?.T .2.
Generate W t  based on PSV (W t |S t ), where    t =1,2,3,..,T  , W t  ?
V3.
The generation word sequence W ={W1,W2,..,WT} is inserted    into the set of generated utterance U4.
Repeat 1  to 3  for Max_Generation_Number    times, Max_Generation_Number is given by developers1.Rescore the utterance U k  in the set of U  by the measure2.Select top n-bestFirst Phase ?
Generating Structures and Words given SV spaceSecond Phase ?
Selection by measureFigure 6: Algorithm of user utterance simulationtags include PRP, VB, TO, VB as a part of speech tagand [loc name] as a component slot tag.
The vocab-ulary includes I, want, to, go, and [loc name].
In thevocabulary, every named-entity word is replaced withits category name.In this way, we can collect the structure tags and vo-cabulary for each SV space from the dialog logs.
Forthe given SV space, we estimate probability distribu-tions for statistical user utterance simulation using atraining process.
For each space, we estimate tag tran-sition probability PSV(St+1|St) and collect structuretags set SSVand vocabularies VSV.We devised a two-phase user utterance generation al-gorithm (Fig.
6).
Symbols are as follows.
The detailexplanation of Fig.
6 will be followed in the next sub-sections.?
SSV: structure tag set for given SV?
VSV: vocabularies for given SV?
Si: structure tag, i = 0, ..., T, Si?
SSV?
Wi: word, i = 0, ..., T, Wi?
VSV?
Wseq: generated word sequence.
Wseq=(W1,W2, ...,WT)?
Uk: k-th sampled utterance,k = 1, ..., Max Sampling Number, Uk?
U3.3.1 First Phase - Generating Structure andWord SequenceWe generate the structure tag S1based on the prob-ability of PSV(S1| < sentence start >) and thenS1influences the generating of S2after PSV(S2|S1).In this way, a structure tag chain is generated sequen-tially based on the structure tag transition probabilityPSV(St+1|St) until the last generated structure tag STis < sentence end >.
We assume that the currentstructure tag has a first order Markov property, whichmeans that the structure tag is only influenced by theprevious structure tag.
After the structure tags aregenerated, the emission probability PSV(Wt|St)(w =1, .
.
.
, T ) is used to generate the word sequence giventhe tag sequence.
We iterate the process of generatingstructures and word sequences sufficient times to gen-erate many different structure tags and word sequenceswhich may occur in real human expressions.
Select-ing natural utterances from the generated utterances re-quires an automatic evaluation metric.3.3.2 Second Phase - Selection by the BLEUmeasureTo measure the naturalness of the generated utter-ances, we use the BLEU (Bilingual Evaluation Under-study) score (Papineni et al, 2001) which is widelyused for automatic evaluation in Statistical MachineTranslation (SMT).
In SMT, translated candidate sen-tences are evaluated by comparing semantically equiv-alent reference sentences which have been translatedby a human.
Evaluation of the user utterance gener-ation shares the same task of evaluation in SMT.
Wecan evaluate the naturalness of generated utterances bycomparing semantically equivalent reference utterancescollected by humans.
Therefore, the BLEU score canbe adopted successfully to measure the naturalness ofthe utterances.The BLEU score is the geometric mean of the n-gramprecisions with a brevity penalty.
The original BLEUmetric is used to evaluate translated sentences by com-paring them to several reference sentences.
We mod-ified the BLEU metric to compare one generated ut-terance with several reference utterances.
To rescorethe generated utterances, we used the Structure andWord interpolated BLEU score (SWB).
After the firstphase, we obtain generated utterances which have bothstructure and word sequence.
To measure the natu-ralness of a generated utterance, we check both struc-tural and lexical naturalness.
We calculated Struc-ture Sequence BLEU score using the generated struc-ture tags sequences instead of words sequences with thereference structure tag sequences of the SV space in theBLEU calculation process.
The Word Sequence BLEUis calculated by measuring BLEU score using the gener-ated words sequence with the reference word sequencesof the SV space.
SWB is calculated as:SWB = ?
?
Structure Sequence BLEU+(1?
?)
?
Word Sequence BLEUIn this study, we set ?
= 0.5.
Using SWB, we selectthe top 20-best generated utterances and return a corre-sponding generated utterance by selecting one of themrandomly.3.4 ASR channel SimulationASR channel simulation generates speech recognitionerrors which might occur in the real speech recognitionprocess.
In this study, we simulate the ASR channel andmodify the generated clean utterance to a speech rec-ognized erroneous utterance.
Successful ASR channelsimulation techniques should have the following prop-erties: the developer should be able to set the simu-lated word error rate (WER) between 0% ?
100%; thesimulated errors should be generated based on realistic13phone-level and word-level confusions; and the tech-nique should be easily adapted to new tasks, at low cost.Our ASR channel simulation approach is designedto satisfy these properties.
The proposed ASR channelsimulation method involved four steps: 1) Determiningerror position 2) Generating error types on error markedwords.
3) Generating ASR errors such as substitution,deletion and insertion errors, and 4) Rescoring and se-lecting simulated erroneous utterances (Fig.
7 for Ko-rean language example).In the first step, we used the WER to determine thepositions of erroneous words.
For each word, we ran-domly generate a number between 0 and 1.
If this num-ber is between 0 and WER, we mark the word ErrorWord (1); otherwise we mark the word Clean Word (0).In the second step, we generate ASR error types for theerror marked words based on the error type distribution.In the third step, we generate various types of ASR er-ror.
In the case of deletion error, we simply delete theerror marked word from the utterance.
In the case ofinsertion error, we select one word from the pronunci-ation dictionary randomly, and insert it before the errormarked word.
In the case of substitution error, we use amore complex process to select a substitutable word.To select a substitutable word, we compare themarked error word with the words from pronunciationdictionary which are similar in syllable sequence andphoneme sequence.
First, we convert the final wordsequence from the user simulator into a phoneme se-quence using a Grapheme-to-Phoneme (G2P) module(Lee et al, 2006).
Then, we extract a part of thephoneme sequence which is similar to the error markedword from the entire phoneme sequence of the ut-terance.
The reason for extracting a target phonemesequence corresponding to one word from the entirephoneme sequence is that the G2P results vary betweenthe boundaries of words.
Then, we separate the markedword into syllables and compare their syllable-levelsimilarity to other words in the pronunciation dictio-nary.
We calculate a similarity score which interpolatessyllable and phoneme level similarity using followingequations.Similarity = ?
?
Syllable Alignment Score+(1?
?)
?
Phone Alignment ScoreWe used the dynamic global alignment algorithm of(Needleman and Wunsch, 1970) for both syllable andphoneme sequence alignment.
This alignment algo-rithm requires a weight matrix.
As a weight matrix,we used a vowel confusion matrix which is based onthe manner of articulation.
We consider the position(back/front, high/mid/low) of the tongue and the shape(round/flat) of the lips.
We select candidate wordswhich have higher similarity than an arbitrary thresh-old ?
and replace the error marked word with a randomword from this set.
We repeat steps 1 to 3 many times(usually 100) to collect error added utterances.In the fourth step, we rescore the error added utter-si-chung e ga go sip eo (I want to go to city hall)si-cheong e ga go sip eo0 1 1 0 1 0- del sub - sub -Generating Error Types and PositionsGenerating Candidate Lists of Noisy Utterance si-cheong - geo-gi go si eosi-cheong - ga-ja go seo eosi-cheong - gat go seu eosi-cheong - geot go sil eoSelecting Noisy Utterance si-cheong gat go seo eoError GenerationRanking with LM score1-Step2-Step3-Step4-StepFigure 7: Example of ASR channel simulationances using the language model (LM) score.
This LMis trained using a domain corpus which is usually usedin ASR.
We select top n-best erroneous utterances (weset n=10) and choose one of them randomly.
This utter-ance is the final result of ASR channel simulator, and isfed into the dialog system.4 ExperimentsWe proposed a method that user intention, utterance andASR channel simulation to rapidly assemble a simula-tion system to evaluate dialog systems.
We conducteda case study for the navigation domain Korean spokendialog system to test our simulation method and exam-ine the dialog behaviors using the simulator.
We used100 dialog examples from real user and dialog systemto train user intention and utterance simulator.
We usedthe SLU method of (Jeong and Lee, 2006), and dia-log management method of (Kim et al, 2008) to buildthe dialog system.
After trained user simulator, we per-form simulation to collect 5000 dialog samples for eachWER settings (WER = 0 ?
40 %).To verify the user intention and utterance simula-tion quality, we let two human judges to evaluate 200randomly chosen dialogs and 1031 utterances from thesimulated dialog examples (WER=0%).
At first, theyevaluate a dialog with three scale (1: Unnatural, 2: Pos-sible, 3: Natural), then evaluate the utterances of a dia-log with three scale (1: Unclear, 2: Understandable, 3:Natural).The inter evaluator agreement (kappa) is 0.45 and0.58 for dialog and utterance evaluation respectively,which show the moderate agreement (Fig.
8).
Bothjudges show the positive reactions for the quality of userintention and utterance, the simulated dialogs can bepossibly occurred, and the quality of utterance is closeto natural human utterance.We also did regression analysis with the results ofhuman evaluation and the SWB score to find out therelationship between SWB and human judgment.
Fig.9 shows the result of polynomial regression (order 3)result.
It shows that ?Unclear?
utterance might have 0.514Human 1 Human 2 Average KappaDialog 2.38 2.22 2.30 0.45Utterance 2.74 2.67 2.71 0.58Figure 8: Human evaluation results on dialog and utter-ance0.340.440.540.640.740.840.941 1.5 2 2.5 3SWB ScoreAverage human evaluation for user utterancesFigure 9: Relationship between SWB score and humanjudgment?
0.7 SWB score, ?Possible?
and ?Natural?
simulatedutterance might have over 0.75.
It means that we cansimulate good user utterance if we constrain the usersimulator with the threshold around 0.75 SWB score.To assess the ASR channel simulation quality, wecompared how SLU of utterances was affected byWER.
SLU was quantified according to sentence er-ror rate (SER) and concept error rate (CER).
Comparedto WER set by the developer, measured WER was thesame, SER increased more rapidly, and CER increasedmore slowly (Fig.
10).
This means that our simula-tion framework models SLU errors effective as well asspeech recognition errors.Fig.
11 shows the overall dialog system behaviors us-ing the user simulator and ASR channel simulator.
Asthe WER rate increased, dialog system performance de-creased and dialog length increased.
This result is sim-ilar as observed to the dialog behaviors in real human-0 10 20 30 40 50WER measured 0 10 19.71 29 39.06 49.21SER 0 33.28 56.6 70.91 81.29 88CER 1.9087 10.1069 18.3183 26.1619 34.4322 41.7550102030405060708090100errrate(%)Figure 10: Relationship between given WER and mea-sured other error rates.
X-axis = WER fixed by ASRchannel(%)7.78.18.58.99.30.500.600.700.800.901.000 5 10 15 20 25 30 35 40Avg.
DialogLength(turns)SLUaccuracyandTCRWord Error Rate (%)SLUTCRLengthFigure 11: Dialog simulation result on navigation do-mainmachine dialog.5 ConclusionThis paper presented novel and easy to build dialog sim-ulation methods for use in evaluation of spoken dia-log systems.
We proposed methods of simulating utter-ances and user intentions to replace real human users,and introduced an ASR channel simulation method thatacts as a real speech recognizer.
We introduce a methodof simulating user intentions which is based on the CRFsequential graphical model, and an utterance simulatorthat generates user utterances.
Both user intention andutterance simulators use a fully data-driven approach;therefore, they have high domain- and language porta-bility.
We also proposed a novel ASR channel sim-ulator which allows the developers to set the speechrecognition performance level.
We applied our meth-ods to evaluate a navigation domain dialog system; ex-perimental results show that the simulators successfullyevaluated the dialog system, and that simulated inten-tion, utterance and errors closely match to those ob-served in real human-computer dialogs.
We will applyour approach to other dialog systems and bootstrap newdialog system strategy for the future works.6 AcknowledgementThis research was supported by the Intelligent RoboticsDevelopment Program, one of the 21st Century FrontierR&D Programs funded by the Ministry of KnowledgeEconomy of Korea.ReferencesChung, G. 2004.
Developing a flexible spoken dialogsystem using simulation.
Proc.
ACL, pages 63?70.Cuayahuitl, H., S. Renals, O.
Lemon, and H. Shi-modaira.
2005.
Human-Computer Dialogue Sim-ulation Using Hidden Markov Models.
Automatic15Speech Recognition and Understanding, 2005 IEEEWorkshop on, pages 100?105.Eckert, W., E. Levin, and R. Pieraccini.
1997.
Usermodeling for spoken dialogue system evaluation.Automatic Speech Recognition and Understanding,1997.
Proceedings., 1997 IEEE Workshop on, pages80?87.Jeong, M. and G. Lee.
2006.
Jointly PredictingDialog Act and Named Entity for Statistical Spo-ken Language Understanding.
Proceedings of theIEEE/ACL 2006 workshop on spoken language tech-nology (SLT).Kim, K., C. Lee, S Jung, and G. Lee.
2008.
Aframe-based probabilistic framework for spoken di-alog management using dialog examples.
In the 9thsigdial workshop on discourse and dialog (sigdial2008), To appear.Lafferty, J.D., A. McCallum, and F.C.N.
Pereira.
2001.Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
Pro-ceedings of the Eighteenth International Conferenceon Machine Learning table of contents, pages 282?289.Lee, J., S. Kim, and G.G.
Lee.
2006.
Grapheme-to-Phoneme Conversion Using Automatically ExtractedAssociative Rules for Korean TTS System.
In NinthInternational Conference on Spoken Language Pro-cessing.
ISCA.Liu, D.C. and J. Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45(1):503?528.Lo?pez-Co?zar, R., A.
De la Torre, JC Segura, and AJ Ru-bio.
2003.
Assessment of dialogue systems bymeans of a new simulation technique.
Speech Com-munication, 40(3):387?407.Lo?pez-Co?zar, Ramo?n, Zoraida Callejas, and MichaelMctear.
2006.
Testing the performance of spoken di-alogue systems by means of an artificially simulateduser.
Artif.
Intell.
Rev., 26(4):291?323.Needleman, SB and CD Wunsch.
1970.
A generalmethod applicable to the search for similarities in theamino acid sequence of two proteins.
J Mol Biol,48(3):443?53.Papineni, K., S. Roukos, T. Ward, and WJ Zhu.2001.
BLEU: a method for automatic evaluation ofMT.
Research Report, Computer Science RC22176(W0109-022), IBM Research Division, TJ WatsonResearch Center, 17.Rieser, V. and O.
Lemon.
2006.
Cluster-Based UserSimulations for Learning Dialogue Strategies.
InNinth International Conference on Spoken LanguageProcessing.
ISCA.Schatzmann, J., K. Georgila, and S. Young.
2005.Quantitative Evaluation of User Simulation Tech-niques for Spoken Dialogue Systems.
In 6th SIGdialWorkshop on Discourse and Dialogue.
ISCA.Schatzmann, J., B. Thomson, and S. Young.
2007a.Error simulation for training statistical dialogue sys-tems.
Automatic Speech Recognition & Understand-ing, 2007.
ASRU.
IEEE Workshop on, pages 526?531.Schatzmann, J., B. Thomson, and S. Young.
2007b.Statistical User Simulation with a Hidden Agenda.Proc.
SIGDial, Antwerp, Belgium.Scheffler, K. and S. Young.
2000.
Probabilistic simula-tion of human-machine dialogues.
Proc.
of ICASSP,2:1217?1220.Scheffler, K. and S. Young.
2001.
Corpus-based dia-logue simulation for automatic strategy learning andevaluation.
Proc.
NAACL Workshop on Adaptationin Dialogue Systems, pages 64?70.Seneff, S. 2002.
Response planning and generationin the Mercury flight reservation system.
ComputerSpeech and Language, 16(3):283?312.Torres, Francisco, Emilio Sanchis, and EncarnaSegarra.
2008.
User simulation in a stochastic di-alog system.
Comput.
Speech Lang., 22(3):230?255.16
