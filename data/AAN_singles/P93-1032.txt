AUTOMATIC  ACQUIS IT ION OF A LARGESUBCATEGORIZAT ION D ICT IONARY FROM CORPORAChr i s topher  D.  Mann ingXerox PARC and Stanford UniversityStanford Univers i tyDept.
of Linguistics, Bldg.
100Stanford,  CA 94305-2150, USAInternet:  manning@csli.stanford.eduAbst rac tThis paper presents a new method for producinga dictionary of subcategorization frames from un-labelled text corpora.
It is shown that statisticalfiltering of the results of a finite state parser un-ning on the output of a stochastic tagger produceshigh quality results, despite the error rates of thetagger and the parser.
Further, it is argued thatthis method can be used to learn all subcategori-zation frames, whereas previous methods are notextensible to a general solution to the problem.INTRODUCTIONRule-based parsers use subcategorization nforma-tion to constrain the number of analyses that aregenerated.
For example, from subcategorizationalone, we can deduce that the PP in (1) must bean argument of the verb, not a noun phrase mod-ifier:(1) John put \[Nethe cactus\] [epon the table\].Knowledge of subcategorization also aids text ger-eration programs and people learning a foreignlanguage.A subcategorization frame is a statement ofwhat types of syntactic arguments a verb (or ad-jective) takes, such as objects, infinitives, that-clauses, participial clauses, and subcategorizedprepositional phrases.
In general, verbs and ad-jectives each appear in only a small subset of allpossible argument subcategorization frames.A major bottleneck in the production of high-coverage parsers is assembling lexical information,?Thanks to Julian Kupiec for providing the tag-ger on which this work depends and for helpful dis-cussions and comments along the way.
I am alsoindebted for comments on an earlier draft to MartiHearst (whose comments were the most useful!
), Hin-rich Schfitze, Penni Sibun, Mary Dalrymple, and oth-ers at Xerox PARC, where this research was completedduring a summer internship; Stanley Peters, and thetwo anonymous ACL reviewers.such as subcategorization information.
In earlyand much continuing work in computational lin-guistics, this information has been coded labori-ously by hand.
More recently, on-line versionsof dictionaries that provide subcategorization in-formation have become available to researchers(Hornby 1989, Procter 1978, Sinclair 1987).
Butthis is the same method of obtaining subcatego-rizations - painstaking work by hand.
We havesimply passed the need for tools that acquire lex-ical information from the computational linguistto the lexicographer.Thus there is a need for a program that can ac-quire a subcategorization dictionary from on-linecorpora of unrestricted text:1.
Dictionaries with subcategorization nformationare unavailable for most languages (only a fewrecent dictionaries, generally targeted at non-native speakers, list subcategorization frames).2.
No dictionary lists verbs from specialized sub-fields (as in I telneted to Princeton), but thesecould be obtained automatically from texts suchas computer manuals.3.
Hand-coded lists are expensive to make, and in-variably incomplete.4.
A subcategorization dictionary obtained auto-matically from corpora can be updated quicklyand easily as different usages develop.
Diction-aries produced by hand always ubstantially lagreal language use.The last two points do not argue against he useof existing dictionaries, but show that the incom-plete information that they provide needs to besupplemented with further knowledge that is bestcollected automatically) The desire to combinehand-coded and automatically learned knowledge1A point made by Church and Hanks (1989).
Ar-bitrary gaps in listing can be smoothed with a pro-gram such as the work presented here.
For example,among the 27 verbs that most commonly cooccurredwith from, Church and Hanks found 7 for which this235suggests that we should aim for a high precisionlearner (even at some cost in coverage), and thatis the approach adopted here.DEF IN IT IONS ANDDIFF ICULT IESBoth in traditional grammar and modern syntac-tic theory, a distinction is made between argu-ments and adjuncts.
In sentence (2), John is anargument and in the bathroom is an adjunct:(2) Mary berated John in the bathroom.Arguments fill semantic slots licensed by a particu-lar verb, while adjuncts provide information aboutsentential slots (such as time or place) that can befilled for any verb (of the appropriate aspectualtype).While much work has been done on the argu-ment/adjunct distinction (see the survey of dis-tinctions in Pollard and Sag (1987, pp.
134-139)),and much other work presupposes this distinction,in practice, it gets murky (like many things inlinguistics).
I will adhere to a conventional no-tion of the distinction, but a tension arises inthe work presented here when judgments of argu-ment/adjunct status reflect something other thanfrequency of cooccurrence - since it is actuallycooccurrence data that a simple learning programlike mine uses.
I will return to this issue later.Different classifications of subcategorizationframes can be found in each of the dictionariesmentioned above, and in other places in the lin-guistics literature.
I will assume without discus-sion a fairly standard categorization of subcatego-rization frames into 19 classes (some parameter-ized for a preposition), a selection of which areshown below:IVTVDTVTHATNPTHATINFNPINFINGP(prep)Intransitive verbsTransitive verbsDitransitive verbsTakes a finite ~hal complementDirect object and lhaL complementInfinitive clause complementDirect object and infinitive clauseTakes a participial VP complementPrepositional phrase headed by prepNP-P(prep) Direct object and PP headed by prepsubcategorization frame was not listed in the Cobuilddictionary (Sinclair 1987).
The learner presented herefinds a subcategorization involving from for all but oneof these 7 verbs (the exception being ferry which wasfairly rare in the training corpus).PREVIOUS WORKWhile work has been done on various sorts of col-location information that can be obtained fromtext corpora, the only research that I am awareof that has dealt directly with the problem of theautomatic acquisition of subcategorization framesis a series of papers by Brent (Brent and Berwick1991, Brent 1991, Brent 1992).
Brent and Bet-wick (1991) took the approach of trying to gen-erate very high precision data.
2 The input washand-tagged text from the Penn Treebank, andthey used a very simple finite state parser whichignored nearly all the input, but tried to learnfrom the sentences which seemed least likely tocontain false triggers - mainly sentences with pro-nouns and proper names.
3 This was a consistentstrategy which produced promising initial results.However, using hand-tagged text is clearly nota solution to the knowledge acquisition problem(as hand-tagging text is more laborious than col-lecting subcategorization frames), and so, in morerecent papers, Brent has attempted learning sub-categorizations from untagged text.
Brent (1991)used a procedure for identifying verbs that wasstill very accurate, but which resulted in extremelylow yields (it garnered as little as 3% of the in-formation gained by his subcategorization learnerrunning on tagged text, which itself ignored a hugepercentage of the information potentially avail-able).
More recently, Brent (1992) substituted avery simple heuristic method to detect verbs (any-thing that occurs both with and without the suffix-ing in the text is taken as a potential verb, andevery potential verb token is taken as an actualverb unless it is preceded by a determiner or apreposition other than to.
4 This is a rather sim-plistic and inadequate approach to verb detection,with a very high error rate.
In this work I will usea stochastic part-of-speech tagger to detect verbs(and the part-of-speech of other words), and willsuggest hat this gives much better results.
5Leaving this aside, moving to either this last ap-proach of Brent's or using a stochastic tagger un-dermines the consistency of the initial approach.Since the system now makes integral use of ahigh-error-rate component, s it makes little sense2That is, data with very few errors.3A false trigger is a clause in the corpus that onewrongly takes as evidence that a verb can appear witha certain subcategorization frame.4Actually, learning occurs only from verbs in thebase or -ing forms; others are ignored (Brent 1992,p.
8).SSee Brent (1992, p. 9) for arguments against usinga stochastic tagger; they do not seem very persuasive(in brief, there is a chance of spurious correlations, andit is difficult to evaluate composite systems).SOn the order of a 5% error rate on each token for236for other components to be exceedingly selectiveabout which data they use in an attempt o avoidas many errors as possible.
Rather, it would seemmore desirable to extract as much information aspossible out of the text (even if it is noisy), andthen to use appropriate statistical techniques tohandle the noise.There is a more fundamental reason to thinkthat this is the right approach.
Brent and Ber-wick's original program learned just five subcat-egorization frames (TV, THAT, NPTHAT, INF andNPINF).
While at the time they suggested that "weforesee no impediment to detecting many more,"this has apparently not proved to be the case (inBrent (1992) only six are learned: the above plusDTV).
It seems that the reason for this is that theirapproach as depended upon finding cues that arevery accurate predictors for a certain subcategori-zation (that is, there are very few false triggers),such as pronouns for NP objects and to plus afinite verb for infinitives.
However, for many sub-categorizations there just are no highly accuratecues/  For example, some verbs subcategorize forthe preposition in, such as the ones shown in (3):(3) a.
Two women are assisting the police intheir investigation.b.
We chipped in to buy her a new TV.c.
His letter was couched in conciliatoryterms.But the majority of occurrences of in after a verbare NP modifiers or non-subcategorized locativephrases, such as those in (4).
s(4) a.
He gauged support for a change in theparty leadership.b.
He built a ranch in a new suburb.c.
We were traveling along in a noisy heli-copter.There just is no high accuracy cue for verbs thatsubcategorize for in.
Rather one must collectcooccurrence statistics, and use significance test-ing, a mutual information measure or some otherform of statistic to try and judge whether a partic-ular verb subcategorizes for in or just sometimesthe stochastic tagger (Kupiec 1992), and a presumablyhigher error rate on Brent's technique for detectingverbs,rThis inextensibility is also discussed by Hearst(1992).SA sample of 100 uses of /n from the New YorkTimes suggests that about 70% of uses are in post-verbal contexts, but, of these, only about 15% are sub-categorized complements (the rest being fairly evenlysplit between NP modifiers and time or place adjunctPPs).appears with a locative phrase.
9 Thus, the strat-egy I will use is to collect as much (fairly accurate)information as possible from the text corpus, andthen use statistical filtering to weed out false cues.METHODOne month (approximately 4 million words) of theNew York Times newswire was tagged using a ver-sion of Julian Kupiec's stochastic part-of-speechtagger (Kupiec 1992).
l?
Subcategorization learn-ing was then performed by a program that pro-cessed the output of the tagger.
The program hadtwo parts: a finite state parser ran through thetext, parsing auxiliary sequences and noting com-plements after verbs and collecting histogram-typestatistics for the appearance of verbs in variouscontexts.
A second process of statistical filteringthen took the raw histograms and decided the bestguess for what subcategorization frames each ob-served verb actually had.The  f in i te  state parserThe finite state parser essentially works as follows:it scans through text until it hits a verb or auxil-iary, it parses any auxiliaries, noting whether theverb is active or passive, and then it parses com-plements following the verb until something recog-nized as a terminator of subcategorized argumentsis reached) 1 Whatever has been found is enteredin the histogram.
The parser includes a simple NPrecognizer (parsing determiners, possessives, ad-jectives, numbers and compound nouns) and vari-ous other rules to recognize certain cases that ap-peared frequently (such as direct quotations in ei-ther a normal or inverted, quotation first, order).The parser does not learn from participles incean NP after them may be the subject rather thanthe object (e.g., the yawning man).The parser has 14 states and around 100 transi-tions.
It outputs a list of elements occurring afterthe verb, and this list together with the record ofwhether the verb is passive yields the overall con-text in which the verb appears.
The parser skips tothe start of the next sentence in a few cases wherethings get complicated (such as on encountering a9One cannot just collect verbs that always appearwith in because many verbs have multiple subcatego-rization frames.
As well as (3b), chip can also just bea IV: John chipped his tooth.1?Note that the input is very noisy text, includingsports results, bestseller lists and all the other vagariesof a newswire.aaAs well as a period, things like subordinating con-junctions mark the end of subcategorized arguments.Additionally, clausal complements such as those intro-duced by that function both as an argument and as amarker that this is the final argument.237conjunction, the scope of which is ambiguous, ora relative clause, since there will be a gap some-where within it which would give a wrong observa-tion).
However, there are many other things thatthe parser does wrong or does not notice (such asreduced relatives).
One could continue to refinethe parser (up to the limits of what can be recog-nized by a finite state device), but the strategy hasbeen to stick with something simple that worksa reasonable percentage of the time and then tofilter its results to determine what subcategoriza-tions verbs actually have.Note that the parser does not distinguish be-tween arguments and adjuncts.
12 Thus the frameit reports will generally contain too many things.Indicative results of the parser can be observed inFig.
1, where the first line under each line of textshows the frames that the parser found.
Becauseof mistakes, skipping, and recording adjuncts, thefinite state parser records nothing or the wrongthing in the majority of cases, but, nevertheless,enough good data are found that the final subcate-gorization dictionary describes the majority of thesubcategorization frames in which the verbs areused in this sample.F i l te r ingFiltering assesses the frames that the parser found(called cues below).
A cue may be a correct sub-categorization for a verb, or it may contain spuri-ous adjuncts, or it may simply be wrong due to amistake of the tagger or the parser.
The filteringprocess attempts to determine whether one can behighly confident hat a cue which the parser notedis actually a subcategorization frame of the verbin question.The method used for filtering is that suggestedby Brent (1992).
Let Bs be an estimated upperbound on the probability that a token of a verbthat doesn't take the subcategorization frame swill nevertheless appear with a cue for s. If a verbappears m times in the corpus, and n of thosetimes it cooccurs with a cue for s, then the prob-ability that all the cues are false cues is boundedby the binomial distribution:m m!n (m- - B , )  m- -i=nThus the null hypothesis that the verb does nothave the subcategorization frame s can be rejectedif the above sum is less than some confidence levelC (C = 0.02 in the work reported here).Brent was able to use extremely low values forB~ (since his cues were sparse but unlikely to be12Except for the fact that it will only count the firstof multiple.
PPs as an argument.false cues), and indeed found the best performancewith values of the order of 2 -8 .
However, using myparser, false cues are common.
For example, whenthe recorded subcategorization is __ NP PP(of), itis likely that the PP should actually be attachedto the NP rather than the verb.
Hence I haveused high bounds on the probability of cues be-ing false cues for certain triggers (the used val-ues range from 0.25 (for WV-P(of)) to 0.02).
Atthe moment, the false cue rates B8 in my systemhave been set empirically.
Brent (1992) discussesa method of determining values for the false cuerates automatically, and this technique or somesimilar form of automatic optimization could prof-itably be incorporated into my system.RESULTSThe program acquired a dictionary of 4900 subcat-egorizations for 3104 verbs (an average of 1.6 perverb).
Post-editing would reduce this slightly (afew repeated typos made it in, such as acknowl-ege, a few oddities such as the spelling garonteeas a 'Cajun' pronunciation of guarantee and a fewcases of mistakes by the tagger which, for example,led it to regard lowlife as a verb several times bymistake).
Nevertheless, this size already comparesfavorably with the size of some production MTsystems (for example, the English dictionary forSiemens' METAL system lists about 2500 verbs(Adriaens and de Braekeleer 1992)).
In general,all the verbs for which subcategorization frameswere determined are in Webster's (Gove 1977) (theonly noticed exceptions being certain instances ofprefixing, such as overcook and repurchase), buta larger number of the verbs do not appear inthe only dictionaries that list subcategorizationframes (as their coverage of words tends to be morelimited).
Examples are fax, lambaste, skedaddle,sensationalize, and solemnize.
Some idea of thegrowth of the subcategorization dictionary can behad from Table 1.Table 1.
Growth of subcategorization dictionaryWords Verbs in Subcats SubcatsProcessed subcat learned learned(million) dictionary per verb1.2 1856 2661 1.432.9 2689 4129 1.534.1 3104 4900 1.58The two basic measures of results are the in-formation retrieval notions of recall and precision:How many of the subcategorization frames of theverbs were learned and what percentage of thethings in the induced dictionary are correct?
Ihave done some preliminary work to answer thesequestions.238In the mezzanine, a man came with two sons and one baseball glove, like so many others there, in case,\[p(with)\]OKIvof course, a foul ball was hit to them.
The father sat throughout the game with the\[pass,p(to)\] \[p(throughout)\]?KTv *IVglove on, leaning forward in anticipation like an outfielder before every pitch.
By the sixth inning, he*P(forward)appeared exhausted from his exertion.
The kids didn't seem to mind that the old man hogged the\[xcomp,p( from)\] \[inf\] \[that\] \[np\]*XCOMP OKINF OKTHAT OKTvglove.
They had their hands full with hot dogs.
Behind them sat a man named Peter and his son\[that\]*TV-XCOMP *IV OK DTVPaul.
They discussed the merits of Carreon over McReynolds in left field, and the advisability of\[np,p(of)\]OKTVreplacing Cone with Musselman.
At the seventh-inning stretch, Peter, who was born in Austria butOKTv-v(with ) OKTVcame to America at age 10, stood with the crowd as "Take Me Out to the Ball Game" was played.
The?KP(to) OKIvfans sang and waved their orange caps.\[np\]OKIv OKTvOKTvFigure 1.
A randomly selected sample of text from the New York Times, with what the parser could extractfrom the text on the second line and whether the resultant dictionary has the correct subcategorization forthis occurrence shown on the third line (OK indicates that it does, while * indicates that it doesn't).For recall, we might ask how many of the usesof verbs in a text are captured by our subcate-gorization dictionary.
For two randomly selectedpieces of text from other parts of the New YorkTimes newswire, a portion of which is shown inFig.
1, out of 200 verbs, the acquired subcatego-rization dictionary listed 163 of the subcategori-zation frames that appeared.
So the token recallrate is approximately 82%.
This compares with abaseline accuracy of 32% that would result fromalways guessing TV (transitive verb) and a per-formance figure of 62% that would result from asystem that correctly classified all TV and THATverbs (the two most common types), but whichgot everything else wrong.We can get a pessimistic lower bound on pre-cision and recall by testing the acquired diction-ary against some published dictionary.
13 For this13The resulting figures will be considerably lowerthan the true precision and recall because the diction-ary lists subcategorization frames that do not appearin the training corpus and vice versa.
However, thisis still a useful exercise to undertake, as one can at-tain a high token success rate by just being able toaccurately detect he most common subcategorizationtest, 40 verbs were selected (using a random num-ber generator) from a list of 2000 common verbs.
14Table 2 gives the subcategorizations listed in theOALD (recoded where necessary according to myclassification of subcategorizations) and those inthe subcategorization dictionary acquired by myprogram in a compressed format.
Next to eachverb, listing just a subcategorization frame meansthat it appears in both the OALD and my subcat-egorization dictionary, a subcategorization framepreceded by a minus sign ( - )  means that the sub-categorization frame only appears in the OALD,and a subcategorization frame preceded by a plussign (+) indicates one listed only in my pro-gram's subcategorization dictionary (i.e., one thatis probably wrong).
15 The numbers are the num-ber of cues that the program saw for each subcat-frames.14The number 2000 is arbitrary, but was chosenfollowing the intuition that one wanted to test theprogram's performance on verbs of at least moderatefrequency.15The verb redesign does not appear in the OALD,so its subcategorization e try was determined by me,based on the entry in the OALD for design.239egorization frame (that is in the resulting subcat-egorization dictionary).
Table 3 then summarizesthe results from the previous table.
Lower boundsfor the precision and recall of my induced subcat-egorization dictionary are approximately 90% and43% respectively (looking at types).The aim in choosing error bounds for the filter-ing procedure was to get a highly accurate dic-tionary at the expense of recall, and the lowerbound precision figure of 90% suggests that thisgoal was achieved.
The lower bound for recall ap-pears less satisfactory.
There is room for furtherwork here, but this does represent a pessimisticlower bound (recall the 82% token recall figureabove).
Many of the more obscure subcategoriza-tions for less common verbs never appeared in themodest-sized learning corpus, so the model had nochance to master them.
16Further, the learned corpus may reflect languageuse more accurately than the dictionary.
TheOALD lists retire to NP and retire from NP assubeategorized PP complements, but not retire inNP.
However, in the training corpus, the colloca-tion retire in is much more frequent han retireto (or retire from).
In the absence of differentialerror bounds, the program is always going to takesuch more frequent collocations as subeategorized.Actually, in this case, this seems to be the rightresult.
While in can also be used to introduce alocative or temporal adjunct:(5) John retired from the army in 1945.if in is being used similarly to to so that the twosentences in (6) are equivalent:(6) a. John retired to Malibu.b.
John retired in Malibu.it seems that in should be regarded as a subcatego-rized complement of retire (and so the dictionaryis incomplete).As a final example of the results, let us discussverbs that subcategorize for from (of.
fn.
1 andChurch and Hanks 1989).
The acquired subcate-gorization dictionary lists a subcategorization in-volving from for 97 verbs.
Of these, 1 is an out-right mistake, and 1 is a verb that does not appearin the Cobuild dictionary (reshape).
Of the rest,64 are listed as occurring with from in Cobuild and31 are not.
While in some of these latter casesit could be argued that the occurrences of fromare adjuncts rather than arguments, there are alsoa6For example, agree about did not appear in thelearning corpus (and only once in total in another twomonths of the New York Times newswire that I exam-ined).
While disagree about is common, agree aboutseems largely disused: people like to agree with peoplebut disagree about opics.Table 2.
Subcategorizations for 40 randomly se-lected verbs in OALD and acquired subcategori-zation dictionary (see text for key).agree: INF:386, THAT:187, P(lo):101, IV:77,P(with):79, p(on):63, -P(about), --WHaih --TVannoy: --TVassign: TV-P(t0):19, NPINF:ll, --TV-P(for),--DTV, +TV:7att r ibute:  WV-P(to):67, +P(to):12become: IV:406, XCOMP:142, --PP(Of)bridge: WV:6, +P(between):3burden:  WV:6, TV-P(with):5calculate: THAT:I 1, TV:4, - -WH, --NPINF,--PP(on)chart: TV:4, +DTV:4chop: TV:4, --TV-P(Up), --TV-V(into)depict: WV-P(as):10, IV:9, --NPINGdig: WV:12, P(out):8, P(up):7, --IV, --TV-P (in), --TV-P (0lit), --TV-P (over), --TV-P (up),--P(for)drill: Tv-P(in):I4, TV:14, --IV, --P(FOR)emanate:  P(from ):2employ: TV:31,--TV-P(on),--TV-P(in),--TV-P(as), --NPINFencourage: NPINF:IO8, TV:60, --TV-P(in)exact: --TV, --TV-PP(from)exclaim: THAT:10,--IV,--P0exhaust: TV:12exploit: TV:11fascinate: TV:17f l avor :  TV:8, --TV-PP(wiih)heat: IV:12, TV:9, --TV-P(up), --P(up)leak: P(out):7, --IV, --P(in), --IV, - -TV-P(tO)lock: TV:16, TV-P(in):16, --IV, --P(), --TV-P(together), --TV-P(up), --TV-P(out), --TV-P(away)mean: THAT:280, TV:73, NPINF:57, INF:41,ING:35, --TV-PP (to), --POSSING, --TV-PP (as)--DTV, --TV-PP (for)occupy: TV:17, --TV-P(in), --TV-P(with)prod: TV:4, Tv-e(into):3, --IV, --P(AT),--NPINFredesign: TV:8, --TV-P (for), --TV-P(as),--NPINFreiterate: THAT:13, --TVremark:  THAT:7, --P(on), --P(upon), --IV,+IV:3,retire: IV:30, IV:9, --P(from), --P(t0),--XCOMP, +e(in):38shed: TV:8, --TV-P (on)sift: P(through):8, --WV, --TV-P(OUT)strive: INF:14, P(for):9, --P(afler),-e  (against), -P  (with), --IVtour: TV:9, IV:6, --P(IN)troop: --IV, -P0 ,  \[TV: trooping the color\]wallow: P( in) :2, - - IV, -P(about) , -P(around)water: WV:13,--IV,--WV-P(down), -}-THAT:6240Table 3.
Comparison of results with OALDSubcategorization framesWord Right Wrong Out of Incorrectagree: 6 8all: 0 1annoy: 0 1assign: 2 1 4 Tvattribute: 1 1 1 P(/o)become: 2 3bridge: 1 1 1 wv-P(belween)burden: 2 2calculate: 2 5chart: 1 1 1 DTVchop: 1 3depict: 2 3dig: 3 9drill: 2 4emanate: 1 1employ: 1 5encourage: 2 3exact: 0 2exclaim: 1 3exhaust: 1 1exploit: 1 1fascinate: 1 1flavor: 1 2heat: 2 4leak: 1 5lock: 2 8mean: 5 10occupy: 1 3prod: 2 5redesign: 1 4reiterate: 1 2remark: 1 1 4 IVretire: 2 1 5 P(in)shed: 1 2sift: 1 3strive: 2 6tour: 2 3troop: 0 3wallow: 1 4water: 1 1 3 THAT60 7 139Precision (percent right of ones learned): 90%Recall (percent of OALD ones learned): 43%some unquestionable omissions from the diction-ary.
For example, Cobuild does not list that forbidtakes from-marked participial complements, butthis is very well attested in the New York Timesnewswire, as the examples in (7) show:(7) a.
The Constitution appears to forbid thegeneral, as a former president who cameto power through a coup, from taking of-fice.b.
Parents and teachers are forbidden fromtaking a lead in the project, and .
.
.Unfortunately, for several reasons the resultspresented here are not directly comparable withthose of Brent's systems.
17 However, they seemsto represent at least a comparable l vel of perfor-mance.FUTURE DIRECT IONSThis paper presented one method of learning sub-categorizations, but there are other approachesone might try.
For disambiguating whether a PPis subcategorized by a verb in the V NP PP envi-ronment, Hindle and Rooth (1991) used a t-scoreto determine whether the PP has a stronger asso-ciation with the verb or the preceding NP.
Thismethod could be usefully incorporated into myparser, but it remains a special-purpose t chniquefor one particular ease.
Another research direc-tion would be making the parser stochastic as well,rather than it being a categorical finite state de-vice that runs on the output of a stochastic tagger.There are also some linguistic issues that re-main.
The most troublesome case for any Englishsubcategorization learner is dealing with prepo-sitional complements.
As well as the issues dis-cussed above, another question is how to representthe subcategorization frames of verbs that take arange of prepositional complements (but not all).For example, put can take virtually any locativeor directional PP complement, while lean is morechoosy (due to facts about the world):l~My system tries to learn many more subcatego-rization frames, most of which are more difficult todetect accurately than the ones considered in Brent'swork, so overall figures are not comparable.
The re-call figures presented in Brent (1992) gave the rateof recall out of those verbs which generated at leastone cue of a given subcategorization rather than outof all verbs that have that subcategorization (pp.
17-19), and are thus higher than the true recall rates fromthe corpus (observe in Table 3 that no cues were gen-erated for infrequent verbs or subcategorization pat-terns).
In Brent's earlier work (Brent 1991), the errorrates reported were for learning from tagged text.
Noerror rates for running the system on untagged textwere given and no recall figures were given for eithersystem.241(8) a. John leaned against he wallb.
*John leaned under the tablec.
*John leaned up the chuteThe program doesn't yet have a good way of rep-resenting classes of prepositions.The applications of this system are fairly obvi-ous.
For a parsing system, the current subcate-gorization dictionary could probably be incorpo-rated as is, since the utility of the increase in cov-erage would almost undoubtedly outweigh prob-lems arising from the incorrect subcategorizationframes in the dictionary.
A lexicographer wouldwant to review the results by hand.
Nevertheless,the program clearly finds gaps in printed diction-aries (even ones prepared from machine-readablecorpora, like Cobuild), as the above example withforbid showed.
A lexicographer using this programmight prefer it adjusted for higher recall, even atthe expense of lower precision.
When a seeminglyincorrect subcategorization frame is listed, the lex-icographer could then ask for the cues that led tothe postulation of this frame, and proceed to verifyor dismiss the examples presented.A final question is the applicability of the meth-ods presented here to other languages.
Assumingthe existence of a part-of-speech lexicon for an-other language, Kupiec's tagger can be triviallymodified to tag other languages (Kupiec 1992).The finite state parser described here dependsheavily on the fairly fixed word order of English,and so precisely the same technique could only beemployed with other fixed word order languages.However, while it is quite unclear how Brent'smethods could be applied to a free word order lan-guage, with the method presented here, there is aclear path forward.
Languages that have free wordorder employ either case markers or agreement af-fixes on the head to mark arguments.
Since thetagger provides this kind of morphological knowl-edge, it would be straightforward to write a similarprogram that determines the arguments of a verbusing any combination ofword order, case markingand head agreement markers, as appropriate forthe language at hand.
Indeed, since case-markingis in some ways more reliable than word order, theresults for other languages might even be betterthan those reported here.CONCLUSIONAfter establishing that it is desirable to be able toautomatically induce the subcategorization framesof verbs, this paper examined a new technique fordoing this.
The paper showed that the techniqueof trying to learn from easily analyzable piecesof data is not extendable to all subcategorizationframes, and, at any rate, the sparseness of ap-propriate cues in unrestricted texts suggests thata better strategy is to try and extract as much(noisy) information as possible from as much ofthe data as possible, and then to use statisticaltechniques to filter the results.
Initial experimentssuggest that this technique works at least as well aspreviously tried techniques, and yields a methodthat can learn all the possible subcategorizationframes of verbs.REFERENCESAdriaens, Geert, and Gert de Braekeleer.
1992.Converting Large On-line Valency Dictionariesfor NLP Applications: From PROTON Descrip-tions to METAL Frames.
In Proceedings ofCOLING-92, 1182-1186.Brent, Michael R. 1991.
Automatic Acquisi-tion of Subcategorization Frames from UntaggedText.
In Proceedings of the 29th Annual Meetingof the ACL, 209-214.Brent, Michael R. 1992.
Robust Acquisition ofSubcategorizations from Unrestricted Text: Un-supervised Learning with Syntactic Knowledge.MS, John Hopkins University, Baltimore, MD.Brent, Michael R., and Robert Berwick.
1991.Automatic Acquisition of SubcategorizationFrames from Free Text Corpora.
In Proceedingsof the ~th DARPA Speech and Natural LanguageWorkshop.
Arlington, VA: DARPA.Church, Kenneth, and Patrick Hanks.
1989.Word Association Norms, Mutual Information,and Lexicography.
In Proceedings of the 27th An-nual Meeting of the ACL, 76-83.Gove, Philip B.
(ed.).
1977.
Webster's eventhnew collegiate dictionary.
Springfield, MA: G. &C. Merriam.Hearst, Marti.
1992.
Automatic Acquisition ofHyponyms from Large Text Corpora.
In Pro-ceedings of COLING-92, 539-545.Hindle, Donald, and Mats Rooth.
1991.
Struc-tural Ambiguity and Lexical Relations.
In Pro-ceedings of the 291h Annual Meeting of the ACL,229-236.Hornby, A. S. 1989.
Oxford Advanced Learner'sDictionary of Current English.
Oxford: OxfordUniversity Press.
4th edition.Kupiec, Julian M. 1992.
Robust Part-of-SpeechTagging Using a Hidden Markov Model.
Com-puter Speech and Language 6:225-242.Pollard, Carl, and Ivan A. Sag.1987.
Information-Based Syntax and Semantics.Stanford, CA: CSLI.Procter, Paul (ed.).
1978.
Longman Dictionaryof Contemporary English.
Burnt Mill, Harlow,Essex: Longman.Sinclair, John M.
(ed.).
1987.
Collins CobuildEnglish Language Dictionary.
London: Collins.242
