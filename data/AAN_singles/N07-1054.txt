Proceedings of NAACL HLT 2007, pages 428?435,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsBuilding and Rening Rhetorical-Semantic Relation ModelsSasha Blair-Goldensohn andGoogle, Inc.76 Ninth AvenueNew York, NYsasha@google.comKathleen R. McKeown?
and Owen C.
Rambow??
Department of Computer Science?
Center for Computational Learning SystemsColumbia University{kathy,rambow}@cs.columbia.eduAbstractWe report results of experiments whichbuild and refine models of rhetorical-semantic relations such as Cause and Con-trast.
We adopt the approach of Marcuand Echihabi (2002), using a small set ofpatterns to build relation models, and ex-tend their work by refining the trainingand classification process using parame-ter optimization, topic segmentation andsyntactic parsing.
Using human-annotatedand automatically-extracted test sets, wefind that each of these techniques results inimproved relation classification accuracy.1 IntroductionRelations such as Cause and Contrast, which we callrhetorical-semantic relations (RSRs), may be sig-naled in text by cue phrases like because or how-ever which join clauses or sentences and explicitlyexpress the relation of constituents which they con-nect (Example 1).
In other cases the relation may beimplicitly expressed (2).1Example 1 Because of the recent accounting scan-dals, there have been a spate of executive resigna-tions.Example 2 The administration was once again be-set by scandal.
After several key resignations ...1The authors would like to thank the four anonymous re-viewers for helpful comments.
This work was supported by theDefense Advanced Research Projects Agency (DARPA) underContract No.
HR0011-06-C-0023.
Any opinions, findings andconclusions or recommendations expressed in this material arethose of the authors and do not necessarily reflect the views ofDARPA.The first author performed most of the research reported inthis paper while at Columbia University.In this paper, we examine the problem of detect-ing such relations when they are not explicitly sig-naled.
We draw on and extend the work of Marcuand Echihabi (2002).
Our baseline model directlyimplements Marcu and Echihabi?s approach, opti-mizing a set of basic parameters such as smoothingweights, vocabulary size and stoplisting.
We thenfocus on improving the quality of the automatically-mined training examples, using topic segmenta-tion and syntactic heuristics to filter out traininginstances which may be wholly or partially in-valid.
We find that the parameter optimization andsegmentation-based filtering techniques achieve sig-nificant improvements in classification performance.2 Related WorkRhetorical and discourse theory has a long traditionin computational linguistics (Moore and Wiemer-Hastings, 2003).
While there are a number of differ-ent relation taxonomies (Hobbs, 1979; McKeown,1985; Mann and Thompson, 1988; Martin, 1992;Knott and Sanders, 1998), many researchers havefound that, despite small differences, these theorieshave wide agreement in terms of the core phenom-ena for which they account (Hovy and Maier, 1993;Moser and Moore, 1996).Work on automatic detection of rhetorical and dis-course relations falls into two categories.
Marcuand Echihabi (2002) use a pattern-based approachin mining instances of RSRs such as Contrast andElaboration from large, unannotated corpora.
Wediscuss this work in detail in Section 3.
Otherwork uses human-annotated corpora, such as theRST Bank (Carlson et al, 2001), used by Soricutand Marcu (2003), the GraphBank (Wolf and Gib-son, 2005), used by Wellner et al (2006), or ad-hoc annotations, used by (Girju, 2003; Baldridgeand Lascarides, 2005).
In the past year, the ini-428tial public release of the Penn Discourse TreeBank(PDTB) (Prasad et al, 2006) has significantly ex-panded the discourse-annotated corpora available toresearchers, using a comprehensive scheme for bothimplicit and explicit relations.Some work in RSR detection has enlisted syntac-tic analysis as a tool.
Marcu and Echihabi (2002) fil-ter training instances based on Part-of-Speech (POS)tags, and Soricut and Marcu (2003) use syntac-tic features to identify sentence-internal RST struc-ture.
Lapata and Lascarides (2004) focus theirwork syntactically, analyzing temporal links be-tween main and subordinate clauses.
Sporleder andLascarides (2005) extend Marcu and Echihabi?s ap-proach with the addition of a number of features,including syntactic features based on POS and ar-gument structure, as well as lexical and other sur-face features.
They report that, when working withsparse training data, this richer feature set, combinedwith a boosting-based algorithm, achieves more ac-curate classification than Marcu and Echihabi?s sim-pler, word-pair based approach (we describe the lat-ter in the next section).3 The M&E FrameworkWe model two RSRs, Cause and Contrast, adopt-ing the definitions of Marcu and Echihabi (2002)(henceforth M&E) for their Cause-Explanation-Evidence and Contrast relations, respectively.
Inparticular, we follow their intuition that in buildingan automated model it is best to adopt a higher-levelview of relations (cf.
(Hovy and Maier, 1993)),collapsing the finer-grained distinctions that holdwithin and across relation taxonomies.M&E use a three-stage approach common in cor-pus linguistics: collect a large set of class instances(instance mining), analyze them to create a modelof differentiating features (model building), and usethis model as input to a classication step whichdetermines the most probable class of unknown in-stances.The intuition of the M&E model is to apply a setof RSR-associated cue phrase patterns over a largetext corpus to compile a training set without the costof human annotation.
For instance, Example 1 willmatch the Cause-associated pattern ?Because of W1, W2 .
?, where W1 and W2 stand for non-emptystrings containing word tokens.
In the aggregate,such instances increase the prior belief that, e.g.,a text span containing the word scandals and onecontaining resignations are in a Cause relation.
Acritical point is that the cue words themselves (e.g.,because) are discarded before extracting these wordpairs; otherwise these cue phrases themselves wouldlikely be the most distinguishing features learned.More formally, M&E build up their modelthrough the three stages mentioned above as fol-lows: In instance mining, for each RSR r they com-pile an instance set Ir of (W1,W2) spans whichmatch a set of patterns associated with r. Inmodel building, features are extracted from these in-stances; M&E extract a single feature, namely thefrequency of token pairs derived from taking thecartesian product of W1 = {w1...wn} ?
W2 ={wn+1...wm} = {(w1, wn+1)...(wn, wm)} overeach span pair instance (W1,W2) ?
I; these pairfrequencies are tallied for each RSR into a frequencytable Fr.
Then in classication, the most likely re-lation r between two unknown-relation spans W1and W2 can be determined by a na?
?ve Bayesianclassifier as argmaxr?R P (r|W1,W2), where theprobability P (r|W1,W2) is simplified by assum-ing the independence of the individual token pairsto: ?
(wi,wj)?W1,W2 P ((wi, wj)|r).
The frequencycounts Fr are used as maximum likelihood estima-tors of P ((wi, wj)|r).4 TextRelsTextRels is our implementation of the M&E frame-work, and serves as our platform for the experimentswhich follow.For instance mining, we use a set of cue phrasepatterns derived from published lists (e.g., (Marcu,1997; Prasad et al, 2006)) to mine the Gigawordcorpus of 4.7 million newswire documents2 for re-lation instances.
We mine instances of the Causeand Contrast RSRs discussed earlier, as well as aNoRel ?relation?.
NoRel is proposed by M&E asa default model of same-topic text across which nospecific RSR holds; instances are extracted by tak-ing text span pairs which are simply sentences fromthe same document separated by at least three inter-vening sentences.
Table 1 lists a sample of our ex-2distributed by the Linguistic Data Consortium429Type Sample Patterns Instances Instances, M&ECause BOS Because W1 , W2 EOSBOS W1 EOS BOS Therefore , W2 EOS.926,654 889,946Contrast BOS W1 , but W2 EOSBOS W1 EOS BOS However , W2 EOS.3,017,662 3,881,588NoRel BOS W1 EOS (BOS EOS){3,} BOS W2 EOS 1,887,740 1,000,000Table 1: RSR types, sample extraction patterns, number of training instances used in TextRels, and numberof training instances used by M&E.
BOS and EOS are sentence beginning/end markers.traction patterns and the total number of training in-stances per relation; in addition, we hold out 10,000instances of each type, which we divide evenly intodevelopment and training sets.For model building, we compile the training in-stances into token-pair frequencies.
We implementseveral parameters which control the way these fre-quencies are computed; we discuss these parametersand their optimization in the next section.For classication, we implement three binaryclassifiers (for Cause vs Contrast, Cause vs NoReland Contrast vs NoRel) using the nai?ve Bayesianframework of the M&E approach.
We implementseveral classification parameters, which we discussin the next section.5 Parameter OptimizationOur first set of experiments examine the impact ofvarious parameter settings in TextRels, using classi-fication accuracy on a development set as our heuris-tic.
We find that the following parameters havestrong impacts on classification:?
Tokenizing our training instances using stem-ming slightly improves accuracy and also reducesmodel size.?
Laplace smoothing is as accurate as Good-Turing, but is simpler to implement.
Our experi-ments find peak performance with 0.25 ?
value, i.e.the frequency assumed for unseen pairs.?
Vocabulary size of 6,400 achieves peak perfor-mance; tokens which are not in the most frequent6,400 stems (computed over Gigaword) are replacedby an UNK pseudo-token before F is computed.?
Stoplisting has a negative impact on accuracy;we find that even the most frequent tokens contributeuseful information to the model; a stoplist size ofzero achieves peak performance.?
Minimum Frequency cutoff is imposed to dis-card from F token pair counts with a frequency of< 4; results degrade slightly below this value, anddiscarding this long tail of rare pair counts signifi-cantly shrinks model size.Classif./Pdtb Auto Auto-SM&ETestSet Opt Seg Opt Seg Opt SegCau/Con 59.1 61.1 69.8 69.7 70.3 70.6 87Cau/NR 75.2 74.3 72.7 73.5 71.2 72.3 75Con/NR 67.4 69.7 70.7 71.3 68.2 70.0 64Table 2: Classifier accuracy across PDTB, Autoand Auto-S test sets for the parameter-optimizedclassifier (?Opt?)
and the same classifier trained onsegment-constrained instances (?Seg?).
Accuracyfrom M&E is reported for reference, but we note thatthey use a different test set so the comparison is notexact.
Baseline in all cases is 50%.To evaluate the performance of our three binaryclassifiers using these optimizations, we follow theprotocol of M&E.
We present the classifier for, e.g.,Cause vs NoRel with an equal number of span-pairinstances for each RSR (as in training, any patterntext has been removed).
We then determine the ac-curacy of the classifier in predicting the actual RSRof each instance; in all cases we use an equal num-ber of input pairs for each RSR so random baselineis 50 %.
We carry out this evaluation over two dif-ferent test sets.The first set (?PDTB?)
is derived from the PennDiscourse TreeBank (Prasad et al, 2006).
We ex-tract ?Implicit?
relations, i.e.
text spans from adja-cent sentences between which annotators have in-ferred semantics not marked by any surface lexi-cal item.
To extract test instances for our CauseRSR, we take all PDTB Implicit relations markedwith ?Cause?
or ?Consequence?
semantics (344 to-tal instances); for our Contrast RSR, we take in-stances marked with ?Contrast?
semantics (293 to-430tal instances).3 PDTB marks the two ?Arguments?of these relationship instances, i.e.
the text spans towhich they apply; these are used as test (W1,W2)span pairs for classification.
We test the perfor-mance on PDTB data using 280 randomly selectedinstances each from the PDTB Cause and Contrastsets, as well as 280 randomly selected instancesfrom our test set of automatically extracted NoRelinstances (while there is a NoRel relation includedin PDTB, it is too sparse to use in this testing, with53 total examples).The second test set (?Auto?)
uses the 5,000 testinstances of each RSR type automatically extractedin our instance mining process.Table 2 lists the accuracy for the optimized(?Opt?)
classifier over the Auto and PDTB test sets4.
(The ?Seg?
columns and ?Auto-S?
test set are ex-plained in the next section.
)We also list for reference the accuracy reportedby M&E; however, their training and test sets arenot the same so this comparison is inexact, al-though their test set is extracted automatically in thesame manner as ours.
In the Cause versus Contrastcase, their reported performance exceeds ours sig-nificantly; however, in a subset of their experimentswhich test Cause versus Contrast on instances fromthe human annotated RSTBank corpus (Carlson etal., 2001) where no cue phrase is present, they re-port only 63% accuracy over a 56% baseline (thebaseline is > 50% because the number of input ex-amples is unbalanced).Since we also experience a drop in performancefrom the automatically derived test set to the human-annotated test set (the PDTB in our case), we fur-ther examined this issue.
Our goal was to see if thelower accuracy on the PDTB examples is due to (1)the inherent difficulty of identifying implicit rela-tion spans or (2) something else, such as the corpus-switching effect due to our model being trained and3Note that we are using the initial PDTB release, in whichonly three of 24 data sections have marked Implicit relations, sothat the number of such examples will presumably grow in thenext release.4We do not provide pre-optimization baseline accuracy be-cause this would be arbitrarily depend on how sub-optimally weselect values select parameter values.
For instance, by using aVocabulary Size of 3,200 (rather than 6,400) and a Laplace ?value of 1, the mean accuracy of the classifiers on the Auto testset drops from 71.6 to 70.5; using a Stoplist size of 25 (ratherthan 0) drops this number to 67.3.tested on different corpora (Gigaword and PDTB,respectively).
To informally test this, we testedagainst explicitly cue-phrase marked examples gath-ered from PDTB.
That is, we used the M&E-stylemethod for mining instances, but we gathered themfrom the PDTB corpus.
Interestingly, we found that(1) appears to be the case: for the Cause vs.
Contrast(68.7%), Cause vs. NoRel (73.0%) and (Contrast vs.NoRel (71.0%) classifiers, the performance patternswith the Auto test set rather than the results from thePDTB Implicit test set.
This bolsters the argumentthat ?synthetic?
implicit relations, i.e.
those createdby stripping of originally present cue phrases, can-not be treated as fully equivalent to ?organic?
onesannotated by a human judge but which are not ex-plicitly indicated by a cue phrase.
Sporleder andLascarides (To Appear) recently investigated this is-sue in greater detail, and indeed found that such syn-thetic and organic instances appear to have impor-tant differences.6 Using Topic SegmentationIn our experiments with topic segmentation, we aug-mented the instance mining process to take accountof topic segment boundaries.
The intuition here isthat all sentence boundaries should not be treatedequally during RSR instance mining.
That is, wewould like to make our patterns recognize that somesentence boundaries indicate merely an orthographicbreak without a switch in topic, while others canseparate quite distinct topics.
Sometimes the lattertype are marked by paragraph boundaries, but theseare unreliable markers since they may be used quitedifferently by different authors.Instead, we take the approach of adding topic seg-ment boundary markers to our corpus, which we canthen integrate into our RSR extraction patterns.
Inthe case of NoRel, our assumption in our originalpatterns is that the presence of at least three inter-vening sentences is a sufficient heuristic for findingspans which are not joined by one of the other RSRs;we add the constraint that sentences in a NoRel re-lation be in distinct topical segments, we can in-crease model quality.
Conversely, for two-sentenceCause and Contrast instances, we add the constraintthat there must not be an intervening topic segmentboundary between the two sentences.431Before applying these segment-augmented pat-terns, we must add boundary markers to our cor-pus.
While the concept of a topic segment canbe defined at various granularities, we take a goal-oriented view and aim to identify segments with amean length of approximately four sentences, rea-soning that these will be long enough to excludesome candidate NoRel instances, yet short enough toexclude a non-trivial number of Contrasts and Causeinstances.
We use an automatic topic segmentationtool, LCSeg (Galley et al, 2003) setting parame-ters so that the derived segments are of the approx-imate desired length.
Using these parameters, LC-Seg produces topic segments with a mean length of3.51 sentences over Gigaword, as opposed to 1.54sentences for paragraph boundaries.
Using a sim-ple metric that assumes ?correct?
segment bound-aries always occur at paragraph boundaries, LCSegachieves 76% precision.We rerun the instance mining step of TextRelsover the segmented training corpus, after adding thesegment-based constraints mentioned above to ourpattern set.
Although our constraints reduce theoverall number of instances available in the corpus,we extract for training the same number of instancesper RSR as listed in Table 1 (our non-segment-constrained training set does not use all instancesin the corpus).
Using the optimal parameter set-tings determined in the previous section, we buildour models and classifiers based on these segment-constrained instances.To evaluate the classifiers built on the segment-constrained instances, we can essentially follow thesame protocol as in our Parameter Optimization ex-periments.
However, we must choose whether touse a held-out test set taken from the segment-constrained instances (?Auto-S?)
or the same testset as used to evaluate our parameter optimization,i.e.
the (?Auto?)
test set from unsegmented trainingdata.
We decide to test on both.
On the one hand,segmentation is done automatically, so it is realisticthat given a ?real world?
document, we can computesegment boundaries to help our classification judg-ments.
On the other hand, testing on unsegmentedinput allows us to compare more directly to the num-bers from our previous section.
Further, for taskswhich would apply RSR models outside of a single-document context (e.g., for assessing coherence ofa synthesized abstract), a test on unsegmented inputmay be more relevant.
Table 2 shows the results forthe ?Seg?
classifiers on both Auto test sets, as wellas the PDTB test set.We observe that the performance of the classi-fiers is indeed impacted by training on the segment-constrained instances.
On the PDTB test data, per-formance using the segment-trained classifiers im-proves in two of three cases, with a mean improve-ment of 1.2%.
However, because of the small sizeof this set, this margin is not statistically significant.On the automatically-extracted test data, thesegment-trained classifier is the best performer inall three cases when using the segmented test data;while the margin is not statistically significant for asingle classifier, the overall accurate-inaccurate im-provement is significant (p < .05) using a Chi-squared test.
On the unsegmented test data, thesegment-trained classifiers are best in two of threecases, but the overall accurate-inaccurate improve-ment does not achieve statistical significance.
Weconclude tentatively that a classifier trained on ex-amples gleaned with topic-segment-augmented pat-terns performs more accurately than our baselineclassifier.7 Using SyntaxWhether or not we use topic segmentation to con-strain our training instances, our patterns rely onsentence boundaries and cue phrase anchors to de-marcate the extents of the text spans which formour RSR instances.
However, an instance whichmatches such a pattern often contains some amountof text which is not relevant to the relation in ques-tion.
Consider:Example 3 Wall Street investors, citing a drop inoil prices because weakness in the automotivesector, sold off shares in GM today.In this case, a syntactically informed analysiscould be used to extract the constituents in the cause-effect relationship from within the boldfaced nomi-nal clause only, i.e.
as ?a drop in oil prices?
and?weakness in the automotive sector.?
However, theoutput of our instance mining process simply splitsthe string around the cue phrase ?because of?
andextracts the entire first and second parts of the sen-tence as the constituents.
Of course, this may be for432the best; in this case there is an implicit Cause rela-tionship between the NP headed by drop and the soldVP which our pattern-based rules inadvertently cap-ture; our experiments here test whether such noise ismore helpful than hurtful.Recognizing the potential complexity of usingsyntactic phenomena, we reduce the dimensions ofthe problem.
First, we focus on single-sentence in-stances; this means we analyze only Cause and Con-trast patterns, since NoRel uses only multi-sentencepatterns.
Second, within the Cause and Contrast in-stances, we narrow our investigation to the most pro-ductive pattern of each type (in terms of training in-stances extracted), given that different syntactic phe-nomena may be in play for different patterns.
Thetwo patterns we use are ?W1 because W2?
for Cause(accounts for 54% of training instances) and ?W1, but W2?
for Contrast (accounts for 41% of train-ing instances).
Lastly, we limit the size of our train-ing set because of parsing time demands.
We usethe Collins parser (Collins, 1996) to parse 400,000instances each of Cause and Contrast for our fi-nal results.
Compared with our other models, thisis approximately 43% of our total Cause instancesand 13% of our total Contrast instances.
For theNoRel model, we use a randomly selected subset of400,000 instances from our training set.
For all rela-tions, we use the non-segment-constrained instanceset as the source of these instances.7.1 Analyzing and Classifying Syntactic ErrorsTo analyze the possible syntactic bases for the typeof over-capturing behavior shown in Example 3, wecreate a small development set of 100 examples eachfrom Cause and Contrast training examples which fitthe criteria just mentioned.
We then manually iden-tify and categorize any instances of over-capturing,labeling the relation-relevant and irrelevant spans.We find that 75% of Cause and 58% of Contrastexamples contain at least some over-capturing; weobserve several common reasons for over-capturingthat we characterize syntactically.
For example, amatrix clause with a verb of saying should not bepart of the RSR.
Using automatic parses of these in-stances created by we then design syntactic filteringheuristics based on a manual examination of parsetrees of several examples from our development set.For Contrast, we find that using the coordinat-ing conjunction (CC) analysis of but, we can use astraightforward rule which limits the extent of RSRspans captured to the conjuncts/children of the CCnode, e.g.
by capturing only the boldfaced clausesin the following example:Example 4 For the past six months, managementhas been revamping positioning and strategy, butalso scaling back operations.This heuristic successfully cuts out the irrelevanttemporal relative clause, retaining the relevant VPswhich are being contrasted.
Note that the heuris-tic is not perfect; ideally the adverb also would befiltered here, but this is more difficult to generalizesince contentful adverbials, e.g.
strategically shouldnot be filtered out.For the because pattern, we capture the right-hand span as any text in child(ren) nodes of the be-cause IN node.
We extend the left-hand span onlyas far as the first phrasal (e.g.
VP) or finite clause(e.g.
SBAR) node above the because node.
Analyz-ing Example 3, the heuristic correctly captures theright-hand span; however, to the left of because, theheuristic cuts too much, and misses the key noundrop.7.2 Error Analysis: Evaluating the HeuristicsThe first question we ask is, how well do ourheuristics work in identifying the actual correctRSR extents?
We evaluate this against the PennDiscourse TreeBank (PDTB), restricting ourselvesto discourse-annotated but and because sentenceswhich match the RSR patterns which are the sub-ject of our syntactic filtering.
Since the PDTBis annotated on the same corpus as Penn Tree-Bank (PTB), we separately evaluate the perfor-mance of our heuristics using gold-standard PTBparses (?PDTB-Gold?)
versus the trees generated byCollins?
parser (?PDTB-Prs?).
We extract our testdata from the PDTB data corresponding to section23 of PTB, i.e.
the standard testing section, so thatthe difference between the gold-standard and realparse trees is meaningful.
Section 23 contains 60annotated instances of but and 52 instances of be-cause which we can use for this purpose.
We definethe measurement of accuracy here in terms of word-level precision/recall.
That is, the set of words fil-tered by our heuristics are compared to the ?correct?433Heuristic PDTB-Prs PDTB-GoldContrast 89.6 / 73.0 / 80.5 79.0 / 80.6 / 79.8Cause 78.5 / 78.8 / 78.6 87.3 / 79.5 / 83.2Table 3: Precision/Recall/F-measure of syntacticheuristics under various data sets and settings as de-scribed in Section 7.2.words to cut, i.e.
those which the annotated RSR ex-tents exclude.
The results of this analysis are shownin Table 3.We performed an analysis of our heuristics onSection 24 of the PDTB.
In that section, there are 74relevant sentences: 20 sentences with because, and54 sentences with but.
Exactly half of all sentences(37) have no problems in the application of theheuristics (7 because sentences, 30 but sentences).Among the remaining sentences, the main source ofproblems is that our heuristics do not always removematrix clauses with verbs of saying (15 cases total, 8of which are because sentences).
For the but clauses,our heuristics removed the subject in 12 cases wherethe PDTB did not do so.
Additionally, the heuristicfor but sentences does not correctly identify the sec-ond conjunct in five cases (choosing instead a paren-thetical, for instance).In looking at our syntactic heuristics for theCause relationship, we see that they indeed elimi-nate the most frequent source of discrepancies withthe PDTB, namely the false inclusion of a matrixclause of saying, resulting in 15 out of 20 perfectanalyses.We also evaluate the difference in performancebetween the PDTB-Gold and PDTB-Prs perfor-mance to determine to what extent using a parser(as opposed to the Gold Standard) degrades the per-formance of our heuristics.
We find that in Sec-tion 24, 13 out of 74 sentences contain a parsingerror in the relevant aspects, but the effects are typ-ically small and result from well-known parser is-sues, mainly attachment errors.
As we can see in Ta-ble 3, the heuristic performance using an automaticparser degrades only slightly, and as such we can ex-pect an automatic parser to contribute to improvingRSR classification (as indeed it does).Pdtb Test Set Auto Test SetU Syn P U Syn PCau/Con 59.6 60.5 54.5 66.3 65.8 60.8Cau/NR 72.2 74.9 52.6 70.3 70.2 57.3Con/NR 61.6 60.2 52.2 69.4 69.8 56.8Table 4: Classifier accuracy for the Unfiltered (U),Syntactically Filtered (Syn), and POS (P) modelsdescribed in Section 7.3, over PDTB and Auto testsets.
Baseline in all cases is 50%.7.3 Classification EvaluationWe evaluate the impact of our syntactic heuristics onclassification over the Auto and PDTB test sets usingthe same instance set of 400,000 training instancesper relation.
However, each applies different filtersto the instances I before computing the frequenciesF (all other parameters use the same values; theseare set slightly differently than the optimized val-ues discussed earlier because of the smaller train-ing sets).
In addition to an Unfiltered baseline, weevaluate Filtered models obtained with our syntac-tic heuristics for Cause and Contrast.
To provide anadditional point of comparison, we also evaluate thePart-of-Speech based filtering heuristic described byMarcu and Echihabi, which retains only nouns andverbs.
Unlike the other filters, the POS-based filter-ing is applied to the NoRel instances as well as theCause and Contrast instances.
Table 4 summarizesthe results of the classifying the PDTB and Auto testsets with these different models.Before we examine the results, we note that thesyntactic heuristic cuts a large portion of trainingdata out.
In terms of the total sum of frequencies inFcause, i.e.
the word pairs extracted from all causeinstances, the syntactic filtering cuts out nearly half.With this in mind, we see that while the syntac-tic filtering achieves slightly lower mean accuracy ascompared to the Unfiltered baseline on the Auto testset, the pairs it does keep appear to be used more ef-ficiently (the differences are significant).
Even withthis reduced training set, the syntactic heuristic im-proves performance in two out of three cases on thePDTB test set, including a 2.7 percent improvementfor the Cause vs NoRel classifier.
However, due tothe small size of the PDTB test set, none of thesedifferences is statistically significant.We posit that bias in the Auto set may cause this434difference in performance across training sets; spansin the Auto set are not true arguments of the rela-tion in the PDTB sense, but nonetheless occur reg-ularly with the cue phrases used in instance miningand thus are more likely to be present in the test set.Lastly, we observe that the POS-based filteringdescribed by M&E performs uniformly poorly.
Wehave no explanation for this at present, given thatM&E?s results with this filter appear promising.8 ConclusionIn this paper, we analyzed the problem of learning amodel of rhetorical-semantic relations.
Building onthe work of Marcu and Echihabi, we first optimizedseveral parameters of their model, which we foundto have significant impact on classification accuracy.We then focused on the quality of the automatically-mined training examples, analyzing two techniquesfor data filtering.
The first technique, based on au-tomatic topic segmentation, added additional con-straints on the instance mining patterns; the sec-ond used syntactic heuristics to cut out irrelevantportions of extracted training examples.
While thetopic-segmentation filtering approach achieves sig-nificant improvement and the best results overall,our analysis of the syntactic filtering approach indi-cates that refined heuristics and a larger set of parseddata can further improve those results.
We wouldalso like to experiment with combining the two ap-proaches, i.e.
by applying the syntactic heuristicsto an instance set extracted using topic segmenta-tion constraints.
We conclude that our experimentsshow that these techniques can successfully refineRSR models and improve our ability to classify un-known relations.ReferencesJason Baldridge and Alex Lascarides.
2005.
Probabilistic head-driven parsing for discourse structure.
In CoNLL 2005.L.
Carlson, D. Marcu, and M.E.
Okurowski.
2001.
Building adiscourse-tagged corpus in the framework of rhetorical struc-ture theory.
In Eurospeech 2001 Workshops.M.
Collins.
1996.
A new statistical parser based on bigramlexical dependencies.
In ACL 1996.M.
Galley, K.R.
McKeown, E. Fosler-Lussier, and H. Jing.2003.
Discourse segmentation of multi-party conversation.In ACL 2003.R.
Girju.
2003.
Automatic detection of causal relations forquestion answering.
In ACL 2003 Workshops.Jerry R. Hobbs.
1979.
Coherence and coreference.
CognitiveScience, 3(1):67?90.E.
Hovy and E. Maier.
1993.
Parsimonious or profligate: Howmany and which discourse structure relations?
UnpublishedManuscript.A.
Knott and T. Sanders.
1998.
The classification of coherencerelations and their linguistic markers: An exploration of twolanguages.
Journal of Pragmatics, 30(2):135?175.M.
Lapata and A. Lascarides.
2004.
Inferring sentence-internaltemporal relations.
In HLT 2004.W.C.
Mann and S.A. Thompson.
1988.
Rhetorical structuretheory: Towards a functional theory of text organization.Text, 8(3):243?281.D.
Marcu and A. Echihabi.
2002.
An unsupervised approach torecognizing discourse relations.
In ACL 2002.D.
Marcu.
1997.
The Rhetorical Parsing, Summarization andGeneration of Natural Language Texts.
Ph.D. thesis, Uni-versity of Toronto, Department of Computer Science.J.
Martin.
1992.
English Text: System and Structure.
JohnBenjamins.K.R.
McKeown.
1985.
Text generation: Using discoursestrategies and focus constraints to generate natural lan-guage text.
Cambridge University Press.J.D.
Moore and P. Wiemer-Hastings.
2003.
Discourse incomputational linguistics and artificial intelligence.
InM.A.
Gernbacher A.G. Graesser and S.R.
Goldman, ed-itors, Handbook of Discourse Processes, pages 439?487.Lawrence Erlbaum Associates.M.G.
Moser and J.D.
Moore.
1996.
Toward a synthesis of twoaccounts of discourse structure.
Computational Linguistics,22(3):409?420.R.
Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, andB.
Webber.
2006.
The penn discourse treebank 1.0. anno-tation manual.
Technical Report IRCS-06-01, University ofPennsylvania.R.
Soricut and D. Marcu.
2003.
Sentence level discourse pars-ing using syntactic and lexical information.
In HLT-NAACL2003.C.
Sporleder and A. Lascarides.
2005.
Exploiting linguisticcues to classify rhetorical relations.
In RANLP 2005.C.
Sporleder and A. Lascarides.
To Appear.
Using automat-ically labelled examples to classify rhetorical relations: Anassessment.
Natural Language Engineering.B.
Wellner, J. Pustejovsky, C. Havasi, R. Sauri, andA.
Rumshisky.
2006.
Classification of discourse coherencerelations: An exploratory study using multiple knowledgesources.
In SIGDial 2006.F.
Wolf and E. Gibson.
2005.
Representing discourse coher-ence: A corpus-based analysis.
Computational Linguistics,31(2):249?287.435
