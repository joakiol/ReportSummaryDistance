Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 8?15,Portland, Oregon, June 23, 2011. c?2011 Association for Computational LinguisticsTowards Multi-Document Summarization of Scientific Articles:MakingInteresting Comparisons with SciSummNitin AgarwalLanguage Technologies InstituteCarnegie Mellon Universitynitina@cs.cmu.eduRavi Shankar ReddyLanguage Technologies Resource CenterIIIT-Hyderabad, Indiakrs reddy@students.iiit.ac.inKiran GvrLanguage Technologies Resource CenterIIIT-Hyderabad, Indiakiran gvr@students.iiit.ac.inCarolyn Penstein Rose?Language Technologies InstituteCarnegie Mellon Universitycprose@cs.cmu.eduAbstractWe present a novel unsupervised approach tothe problem of multi-document summariza-tion of scientific articles, in which the doc-ument collection is a list of papers cited to-gether within the same source article, other-wise known as a co-citation.
At the heart ofthe approach is a topic based clustering offragments extracted from each co-cited arti-cle and relevance ranking using a query gen-erated from the context surrounding the co-cited list of papers.
This analysis enables thegeneration of an overview of common themesfrom the co-cited papers that relate to the con-text in which the co-citation was found.
Wepresent a system called SciSumm that em-bodies this approach and apply it to the 2008ACL Anthology.
We evaluate this summa-rization system for relevant content selectionusing gold standard summaries prepared onprinciple based guidelines.
Evaluation withgold standard summaries demonstrates thatour system performs better in content selec-tion than an existing summarization system(MEAD).
We present a detailed summary ofour findings and discuss possible directionsfor future research.1 IntroductionIn this paper we present a novel, unsupervised ap-proach to multi-document summarization of scien-tific articles.
While the field of multi-document sum-marization has achieved impressive results with col-lections of news articles, summarization of collec-tions of scientific articles is a strikingly differentproblem.
Multi-document summarization of newsarticles amounts to synthesizing details about thesame story as it has unfolded over a variety of re-ports, some of which contain redundant information.In contrast, each scientific article tells its own re-search story.
Even with papers that address similarresearch questions, the argument being made is dif-ferent.
Instead of collecting and arranging detailsinto a single, synthesized story, the task is to abstractaway from the specific details of individual papersand to find the common threads that unite them andmake sense of the document collection as a whole.Another challenge with summarization of scien-tific literature becomes clear as one compares alter-native reviews of the same literature.
Each reviewauthor brings their own unique perspective and ques-tions to bear in their reading and presentation of thatliterature.
While this is true of other genres of doc-uments that have been the target of multi-documentsummarization work in the past, we don?t find queryoriented approaches to multi-document summariza-tion of scientific articles.
One contribution of thiswork is a technical approach to query oriented multi-document summarization of scientific articles thathas been evaluated in comparison with a competi-tive baseline that is not query oriented.
The evalu-ation demonstrates the advantage of the query ori-ented approach for this type of summarization.We present a system called SciSumm that sum-marizes document collections that are composed oflists of papers cited together within the same sourcearticle, otherwise known as a co-citation.
Using thecontext of the co-citation in the source article, wegenerate a query that allows us to generate a sum-mary in a query-oriented fashion.
The extracted por-8tions of the co-cited articles are then assembled intoclusters that represent the main themes of the arti-cles that relate to the context in which they werecited.
Our evaluation demonstrates that SciSummachieves higher quality summaries than the MEADsummarization system (Radev, 2004).The rest of the paper is organized as follows.
Wepresent an overview of relevant literature in Section2.
The end-to-end summarization pipeline has beendescribed in Section 3 .
Section 4 presents an eval-uation of summaries generated from the system.
Weend the paper with conclusions and some interestingfurther research directions in Section 5.2 Literature ReviewWe begin our literature review by thinking aboutsome common use cases for multi-document sum-marization of scientific articles.First consider that as a researcher reads a scien-tific article, she/he encounters numerous citations,most of them citing the foundational and seminalwork that is important in that scientific domain.
Thetext surrounding these citations is a valuable re-source as it allows the author to make a statementabout her viewpoint towards the cited articles.
Atool that could provide a small summary of the col-lection of cited articles that is constructed specifi-cally to relate to the claims made by the author cit-ing them would be useful.
It might also help theresearcher determine if the cited work is relevant forher own research.As an example of such a co-citation consider thefollowing citation sentenceVarious machine learning approaches have beenproposed for chunking (Ramshaw and Marcus,1995; Tjong Kim Sang, 2000a; Tjong Kim Sang etal.
, 2000; Tjong Kim Sang, 2000b; Sassano andUtsuro, 2000; van Halteren, 2000).Now imagine the reader trying to determine aboutwidely used machine learning approaches for nounphrase chunking.
Instead of going through theseindividual papers, it would be more useful to getthe summary of the topics in all those papers thattalk about the usage of machine learning methods inchunking.2.1 Overview of Multi-DocumentSummarizationAn exhaustive summary of recent work in summa-rization is out of the scope for this paper.
Hence, wereview only the most relevant approaches in summa-rization to our current work.
As most recent work inmulti-document summarization has been extractive,and in our observation, scientific articles contain thetype of information that we would want in a sum-mary, we follow this convention.
This allows us toavoid the complexities of natural language genera-tion based approaches in abstractive summarization.Multi-document summarization is an extension ofsingle document summarization in which the the-matically important textual fragments are extractedfrom multiple comparable documents, e.g., news ar-ticles describing the same event.
The techniques notonly need to address identification and removal ofredundant information but also inclusion of uniqueand novel contributions.
Various graph based (Maniet al, 1997) and centroid clustering based meth-ods (Radev et al, 2000) have been proposed toaddress the problem of multi-document summa-rization.
Both of these methods identify commonthemes present in a document collection using a sen-tence similarity metric.2.2 Summarization of Scientific ArticlesSurprisingly, not many approaches to the problem ofsummarization of scientific articles have been pro-posed in the past.
One exception is Teufel andMoens (2002), who view summarization as a clas-sification task in which they use a Naive Bayes clas-sifier to assign a rhetorical status to each sentencein an article and thus divide the whole article intoregions with a specific argumentation status (e.g.categories such as AIM, CONTRAST and BACK-GROUND).
In our proposed approach, we are tryingto identify reoccurring topic themes that are com-mon across the articles and may appear under a va-riety of rhetorical headings.Nanba and colleagues (1999) argue in their workthat a co-citation frequently implies a consistentviewpoint towards the cited articles.
Similarly, forarticles cited within different sentences, textual sim-ilarity between the articles is inversely proportionalto the size of the sentential gap between the citations.9Figure 1: SciSumm summarization pipelineIn our work we make use of this insight by gen-erating a query to focus our multi-document sum-mary from the text closest to the citation.
Qazvinianand colleagues (2008) present a summarization ap-proach that can be seen as the converse of what weare working to achieve.
Rather than summarizingmultiple papers cited in the same source article, theysummarize different viewpoints expressed towardsthe same article from different citing articles.
Someof the insights they use in their work also apply toour problem.
They used a clustering approach overdifferent citations for the same target article for dis-covery of different ways of thinking about that ar-ticle.
Citation text has been already shown to con-tain important concepts about the article that mightbe absent from other important sections of an ar-ticle e.g.
an Abstract (Mohammad et al, 2009) .Template based generation of summaries possess-ing similar hierarchical topic structure as the RelatedWork section in an article has also been proposed(Hoang et al, 2010).
In our work, we consider a flattopic structure in the form of topic clusters.
Morespecifically, we discover the comparable attributesof the co-cited articles using Frequent Term BasedClustering (Beil et al, 2002).
The clusters gener-ated in this process contain a set of topically relatedtext fragments called tiles, which are extracted fromthe set of co-cited articles.
Each cluster is indexedwith a label, which is a frequent term set present inthe tile.
We take this to be an approximation of adescription for the topic represented by the cluster.3 System Overview of the SciSummSummarization SystemA high level overview of our system?s architecture ispresented in Figure 1 .
The system provides a webbased interface for viewing and summarizing re-search articles in the ACL Anthology corpus, 2008.The summarization proceeds in three main stages.First, a user may retrieve a collection of articles ofinterest by entering a query.
SciSumm responds byreturning a list of relevant articles.
The user can con-tinue to read an article of interest as shown in Figure2.
The co-citations in the paper are highlighted inbold and italics to mark them as points of interest forthe user.
If a user clicks on a co-citation, SciSummresponds by generating a query from the local con-text of the co-citation and uses it to rank the clustersgenerated.As an example consider the following citationsentence ?Various machine learning approacheshave been proposed for chunking (Ramshaw andMarcus, 1995; Tjong Kim Sang, 2000a; Tjong KimSang et al , 2000; Tjong Kim Sang, 2000b; Sassanoand Utsuro, 2000; van Halteren, 2000)?.
If the userclicks on this co-citation, SciSumm generates a listof clusters and ranks them for relevance.
Most of thetop ranked cluster labels thus generated are shown inFigure 3 along with the cluster content of the highestranked cluster labelled as Phrase, Noun.
The labelsindex into the corresponding cluster.
An exampleof such cluster is displayed in Figure 4.
The clus-ter has a label Chunk and contains tiles from twoof the three papers discussing about a topic identi-10Figure 2: Interface to read a paper.
The sentences containing co-citations are automatically highlighted and contain a?More?
button beside them letting the user elaborate on the sentence.fied by this label.
In this specific example the topicwas not shared by tiles present in the third paper.The words highlighted are interesting terms whichare either part of the label of the cluster or showa low IDF (Inverse Document Frequency) amongstthe tiles generated from the co-cited papers.
Thesewords are presented as hyper-links to the search in-terface and can be further used as search queries forfinding articles on related topics.3.1 System DescriptionSciSumm has four primary modules that are centralto the functionality of the system, as displayed inFigure 1.
First, the Text Tiling module takes care ofobtaining tiles of text relevant to the citation context.It uses the Texttiling algorithm (Hearst, 1997), tosegment the co-cited papers into text tiles based ontopic shifts identified using a term overlap measurecomputed between fixed-length blocks of text.
Next,the clustering module is used to generate labelledclusters using the text tiles extracted from the co-cited papers.
The labels provide a conveniently com-prehensible and yet terse description of each cluster.We have used a Frequent Term Based Clustering al-gorithm (Beil et al, 2002) for clustering.
The clus-ters are ordered according to relevance with respectto the generated query.
This is accomplished by theRanking Module.
Finally, the summary presenta-tion module is used to display the ranked clustersobtained from the ranking module.
Alongside theclusters, an HTML pane also shows the labels of allthe clusters.
Having such a bird?s-eye view of all thecluster labels helps the user to quickly navigate to aninteresting topic.
The entire pipeline is used in real-time to generate topic clusters which are useful forgenerating snippet summary and more exploratoryanalysis.In the following sections, we discuss each of themain modules in detail.3.2 TexttilingThe Text Tiling module uses the TextTiling algo-rithm (Hearst, 1997) for segmenting the text of eacharticle.
Each such segment obtained by the TextTil-ing algorithm has been referred as a text tile.
Wehave used these text tiles as the basic unit for oursummary since individual sentences are too shortto stand on their own.
Once computed, text tilesare used to identify the context associated with aco-citation.
The intuition is that an embedded co-citation in a text tile is connected with the topic dis-tribution of the tile.
We use important text from thistile to rank the text clusters generated using FrequentTerm based text clustering.11Figure 3: Clusters generated in response to a user click on the co-citation.
The list of clusters in the left pane gives abird-eye view of the topics which are present in the co-cited papers3.3 Frequent Term Based ClusteringThe clustering module employs Frequent TermBased Clustering (Beil et al, 2002).
For each co-citation, we use this clustering technique to clusterall the of the extracted text tiles generated by seg-menting each of the co-cited papers.
We settled onthis clustering approach for the following reasons:?
Text tile contents coming from different papersconstitute a sparse vector space, and thus thecentroid based approaches would not work verywell.?
Frequent Term based clustering is extremelyfast in execution time as well as and relativelyefficient in terms of space requirements.?
A frequent term set is generated for each clus-ter which gives a comprehensible description ofthe cluster.Frequent Term Based text clustering uses a groupof frequently co-occurring terms called a frequentterm set.
Each frequent term set indexes to a cor-responding cluster.
The frequent term set has theproperty that it occurs at least once in each of thedocuments present in the cluster.
The algorithm usesthe first k term sets if all the documents in the doc-ument collections are clustered.To discover all thepossible candidates for clustering, i.e., term sets, weused the Apriori algorithm (Agrawal et al, 1994),which identifies the sets of terms that are relativelyfrequent.
We use entropy measure to score each fre-quent term set as discovered from the Apriori algo-rithm.
The entropy overlap of a cluster Ci, EO(Ci)is calculated as follows:EO(Ci) =?DjCi?1fj.ln(1fj)where Dj is the jth document which gets binnedin the cluster Ci, fj is the number of clusters whichcontain Dj .
A smaller value means that the doc-ument Dj is contained in few other clusters Ci.EO(Ci) increases monotonically as fj increases.We thus rank the clusters with their correspondingEO(Ci) and then pick a cluster with the smallestentropy overlap EO(Ci) .
Once a cluster is chosento be included in the final clustering, we remove thedocuments present in chosen cluster from other can-didate clusters.
This results in a hard clustering ofdocuments.
We also remove term set correspond-ing to Ci from the list of candidate frequent termsets and then again recompute the EO(Ci) ?s for theclusters.
We continue this re-scoring and selectinga candidate cluster until the final clustering does notcompletely exhaust the entire document collection.3.4 Cluster RankingThe ranking module uses cosine similarity betweenthe query and the centroid of each cluster to rank allthe clusters generated by the clustering module.
Thecontext of a co-citation is restricted to the text of thetile in which the co-citation is found.
In this waywe attempt to leverage the expert knowledge of theauthor as it is encoded in the local context of the co-citation in our process of automatically ranking theclusters in terms of importance.12Figure 4: Example of a summary generated by our system.
We can see that the clusters are cross cutting acrossdifferent papers, thus giving the user a multi-document summary.4 EvaluationIn a typical evaluation of a multi-document sum-marization system, gold standard summaries areevaluated against fixed length generated summaries.Summarization conferences such as DUC have com-petitions where different summarization systemscompete on a standard task of generating summariesfor a publicly available dataset.
The summaries gen-erated using each individual summarization systemare then evaluated against the summaries preparedby human annotators.
Summarization of scientificarticle is a novel task and hence no test collection ofgold standard summaries exist.
Thus, it was neces-sary to prepare our own evaluation corpus, consist-ing of gold standard multi-document summaries fora set of randomly selected co-citations.4.1 Experimental SetupAn important target user population for multi-document summarization of scientific articles isgraduate students.
Hence to get a measure of howwell the summarization system is performing, weasked 2 graduate students who have been workingin the computational linguistics community to cre-ate gold standard summaries of a fixed length (8 sen-tences ?
200 words) for ten different randomly se-lected co-citations.
The students were given guide-lines to prepare summaries based on the design goalsof the SciSumm system, but not any of its technicaldetails.
Thus, for 10 co-citations, we obtained twodifferent gold standard summaries.
For ROUGE-1the average score between each pair of gold standardsummaries was 0.518 (Min = 0.388, Max = 0.686).Similarly for ROUGE-2 the average score was 0.242(Min = 0.119, Max=0.443).
While these scores donot have a well-calibrated meaning to them, theygive an indication of the complexity of the task.Since the annotators were creating extractive sum-maries which could justify the co-citation, they hadto pay special attention to the section where the co-citation came from.
One can consider this similar tothe sense making process a reader might go throughwhen using the citing paper as a lens through whichto interpret the cited literature.Note that while SciSumm provides users withan interactive interface that supports navigation be-tween documents, the gold standard summaries arestatic.
Thus, our evaluation is designed to measurethe quality of the content selection when taking intoconsideration the citation context.
This would also13help us to evaluate the influence exerted by the ci-tation context in the gold standard summaries.
Infuture work, we will evaluate the usability of theSciSumm system using a task based evaluation.In the absence of any other multi-document sum-marization systems in the domain of scientific ar-ticle summarization, we used a widely used andfreely available multi-document summarization sys-tem called MEAD (Radev, 2004) as our baseline.MEAD uses centroid based summarization to cre-ate informative clusters of topics.
We use the de-fault configuration of MEAD in which MEAD useslength, position and centroid for ranking each sen-tence.
We did not use query focussed summariza-tion with MEAD.
We evaluate its performance withthe same gold standard summaries we use to evalu-ate SciSumm.
For generating a summary from oursystem we used sentences from the tiles which getsclustered in the top ranked cluster.
When that entirecluster is exhausted we move on to the next highlyranked cluster.
In this way we prepare a summarycomprising of 8 sentences.4.2 ResultsFor measuring performance of the two summariza-tion systems (SciSumm and MEAD), we computethe ROUGE metric based on the 2 * 10 gold standardsummaries that were manually created.
ROUGEhas been traditionally used to compute the perfor-mance based on the N-gram overlap (ROUGE-N)between the summaries generated by the system andthe target gold summaries.
For our evaluation weused two different versions of the ROUGE met-ric, namely ROUGE-1 and ROUGE-2, which cor-respond to measures of the unigram and bigramoverlap respectively.
We computed four metrics inorder to measure SciSumm?s performance, namelyROUGE-1 F-measure, ROUGE-1 Recall, ROUGE-2 F-measure, and ROUGE-2 Recall.
To measure thestatistical significance of this result, we carried outa Student T-Test, the results of which are presentedin the results section.
The t-test results displayedin Table 1 show that our systems performs signif-icantly better than MEAD on three of the metrics(p < .05).
On two additional metrics, SciSummperforms marginally better (p < .1).This shows that using the query generated outof the co-citation is useful for content selectionTable 1: Average ROUGE results.
* represents improve-ment significant at p < .05, ?
at p < .01.Metric MEAD SciSummROUGE-1 F-measure 0.3680 0.5123 ?ROUGE-1 Recall 0.4168 0.5018ROUGE-1 Precision 0.3424 0.5349 ?ROUGE-2 F-measure 0.1598 0.3303 *ROUGE-2 Recall 0.1786 0.3227 *ROUGE-2 Precision 0.1481 0.3450 ?from cited papers.
Intuitively, this makes sense aseach researcher would have a unique perspectivewhen reviewing scientific literature.
Co-citationscan be considered as micro-reviews which summa-rizes the thread unifying the research presented ineach of the cited papers.
This provides evidence thatthe co-citation context provides useful informationfor forming an effective query to focus the multi-document summary to reflect the perspective of theauthor of the citing paper.5 Conclusions and Future WorkIn this work, we proposed the first unsupervised ap-proach to the problem of multi-document summa-rization of scientific articles that we know of.
Inthis approach, the document collection is a list ofpapers cited together within the same source arti-cle, otherwise known as a co-citation.
The summaryis presented in the form of topic labeled clusters,which provide easy navigation based on the user?stopic of interest.
Another contribution is a techni-cal approach to query oriented multi-document sum-marization of scientific articles that has been evalu-ated in comparison with a competitive baseline thatis not query oriented.
Our evaluation shows that theSciSumm approach to content selection outperformsanother multi-document summarization system forthis summarization task.Our long term goal is to expand the capabilitiesof SciSumm to generate literature surveys of largerdocument collections from less focused queries.This more challenging task would require more con-trol over filtering and ranking in order to avoid gen-erating summaries that lack focus.
To this end, afuture improvement that we plan to use a variant onMMR (Maximum Marginal Relevance) (Carbonell14et al, 1998), which can be used to optimize the di-versity of selected text tiles as well as the relevancebased ordering of clusters in order to put a more di-verse set of observations from the co-cited articlesat the fingertips of users.
A natural extension wouldalso be to discover the nature of citations to gen-erate improved summaries.
Non-explicit citations(Qazvinian et al, 2010) which could be used to gen-erate similar topic clusters.Another important direction is to refine the inter-action design through task-based user studies.
As wecollect more feedback from students and researchersthrough this process, we will use the insights gainedto achieve a more robust and effective implementa-tion.
We also plan to leverage research in informa-tion visualization to enhance the usability of the sys-tem.6 AcknowledgementsThis work was supported by NSF EEC-064848 andNSF EEC-0935127.ReferencesAgrawal R. and Srikant R. 1994.
Fast Algorithm forMining Association Rules In Proceedings of the 20thVLDB Conference Santiago, Chile, 1994Baxendale, P. 1958.
Machine-made index for technicalliterature - an experiment.
IBM Journal of Researchand DevelopmentBeil F., Ester M. and Xu X 2002.
Frequent-Term basedText Clustering In Proceedings of SIGKDD ?02 Ed-monton, Alberta, CanadaCarbonell J. and Goldstein J.
1998.
The Use of MMR,Diversity-Based Reranking for Reordering Documentsand Producing Summaries In Research and Develop-ment in Information Retrieval, pages 335?336Councill I. G. , Giles C. L. and Kan M. 2008.
ParsCit:An open-source CRF reference string parsing pack-age INTERNATIONAL LANGUAGE RESOURCESAND EVALUATION European Language ResourcesAssociationEdmundson, H.P.
1969.
New methods in automatic ex-tracting.
Journal of ACM.Hearst M.A.
1997 TextTiling: Segmenting text intomulti-paragraph subtopic passages In proceedings ofLREC 2004, Lisbon, Portugal, May 2004Joseph M. T. and Radev D. R. 2007.
Citation analysis,centrality, and the ACL AnthologyKupiec J. , Pedersen J. , Chen F. 1995.
A training doc-ument summarizer.
In Proceedings SIGIR ?95, pages68-73, New York, NY, USA.
28(1):114?133.Luhn, H. P. 1958.
IBM Journal of Research Develop-ment.Mani I. , Bloedorn E. 1997.
Multi-Document Summa-rization by graph search and matching In AAAI/IAAI,pages 622-628.
[15, 16].Nanba H. , Okumura M. 1999.
Towards Multi-paperSummarization Using Reference Information In Pro-ceedings of IJCAI-99, pages 926?931 .Paice CD.
1990.
Constructing Literature Abstracts byComputer: Techniques and Prospects InformationProcessing and Management Vol.
26, No.1, pp, 171-186, 1990Qazvinian V. , Radev D.R 2008.
Scientific Papersummarization using Citation Summary Networks InProceedings of the 22nd International Conference onComputational Linguistics, pages 689?696 Manch-ester, August 2008Radev D. R .
, Jing H. and Budzikowska M. 2000.Centroid-based summarization of multiple documents:sentence extraction, utility based evaluation, and userstudies In NAACL-ANLP 2000 Workshop on Auto-matic summarization, pages 21-30, Morristown, NJ,USA.
[12, 16, 17].Radev, Dragomir.
2004.
MEAD - a platform for multi-document multilingual text summarization.
In pro-ceedings of LREC 2004, Lisbon, Portugal, May 2004.Teufel S. , Moens M. 2002.
Summarizing ScientificArticles - Experiments with Relevance and RhetoricalStatus In Journal of Computational Linguistics, MITPress.Mohammad, Saif and Dorr, Bonnie and Egan, Melissaand Hassan, Ahmed and Muthukrishan, Pradeep andQazvinian, Vahed and Radev, Dragomir and Zajic,David 2009.
Using citations to generate surveys ofscientific paradigms In Proceedings of Human Lan-guage Technologies:The 2009 Annual Conference ofthe North American Chapter of the Association forComputational LinguisticsQazvinian, Vahed and Radev, Dragomir R. 2010.
Identi-fying non-explicit citing sentences for citation-basedsummarization In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-ticsHoang, Cong Duy Vu and Kan, Min-Yen 2010.
Towardsautomated related work summarization In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics: Posters15
