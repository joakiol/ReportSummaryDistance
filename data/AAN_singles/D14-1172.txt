Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1643?1653,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAssessing the Impact of Translation Errorson Machine Translation Quality with Mixed-effects ModelsMarcello Federico, Matteo Negri, Luisa Bentivogli, Marco TurchiFBK - Fondazione Bruno KesslerVia Sommarive 18, 38123 Trento, Italy{federico,negri,bentivogli,turchi}@fbk.euAbstractLearning from errors is a crucial aspect ofimproving expertise.
Based on this no-tion, we discuss a robust statistical frame-work for analysing the impact of differenterror types on machine translation (MT)output quality.
Our approach is based onlinear mixed-effects models, which allowthe analysis of error-annotated MT out-put taking into account the variability in-herent to the specific experimental settingfrom which the empirical observations aredrawn.
Our experiments are carried outon different language pairs involving Chi-nese, Arabic and Russian as target lan-guages.
Interesting findings are reported,concerning the impact of different errortypes both at the level of human perceptionof quality and with respect to performanceresults measured with automatic metrics.1 IntroductionThe dominant statistical approach to machinetranslation (MT) is based on learning from largeamounts of parallel data and tuning the result-ing models on reference-based metrics that canbe computed automatically, such as BLEU (Pap-ineni et al., 2001), METEOR (Banerjee and Lavie,2005), TER (Snover et al., 2006), GTM (Turianet al., 2003).
Despite the steady progress in thelast two decades, especially for few well resourcedtranslation directions having English as target lan-guage, this way to approach the problem is quicklyreaching a performance plateau.
One reason isthat parallel data are a source of reliable informa-tion but, alone, limit systems knowledge to ob-served positive examples (i.e.
how a sentenceshould be translated) without explicitly modellingany notion of error (i.e.
how a sentence shouldnot be translated).
Another reason is that, as adevelopment and evaluation criterion, automaticmetrics provide a holistic view of systems?
be-haviour without identifying the specific issues of atranslation.
Indeed, the global scores returned byMT evaluation metrics depend on comparisons be-tween translation hypotheses and reference trans-lations, where the causes and the nature of the dif-ferences between them are not identified.To cope with these issues and define systemimprovement priorities, the focus of MT evalua-tion research is gradually shifting towards profil-ing systems?
behaviour with respect to various ty-pologies of errors (Vilar et al., 2006; Popovi?c andNey, 2011; Farr?us et al., 2012, inter alia).
Thisshift has enriched the traditional MT evaluationframework with a new element, that is the actualerrors done by a system.
Until now, most of theresearch has focused on the relationship (i.e.
thecorrelation) between two elements of the frame-work: humans and automatic evaluation metrics.As a new element of the framework, which be-comes a sort of ?evaluation triangle?, the analy-sis of error annotations opens interesting researchproblems related to the relationships between: i)error types and human perception of MT qualityand ii) error types and the sensitivity of automaticmetrics.Besides motivating further investigation on met-rics featuring high correlation with human judge-ments (a well-established MT research sub-field,which is out of the scope of this paper), connectingthe vertices of this triangle raises new challengingquestions such as:(1) Which types of MT errors have the high-est impact on human perception of translationquality?
Surprisingly, little prior work focusedon this side of the triangle.
Error annotationshave been considered to highlight strengths andweaknesses of MT engines or to investigate theinfluence of different error types on post-editors?work.
However, the direct connection between er-1643rors and users?
preferences has been only partiallyunderstood, mainly from a descriptive standpointand through rudimentary techniques unsuitable todraw clear-cut conclusions or reliable inferences.
(2) To which types of errors are different MTevaluation metrics more sensitive?
This side ofthe triangle has been even less explored.
For in-stance, little has been done to understand whichautomatic metric is more suitable to assess sys-tem improvements with respect to a specific issue(e.g.
word order or morphology) or to shed lighton the joint impact of different error types on per-formance results calculated with different metrics.To answer these questions, we propose a ro-bust statistical framework to analyse the im-pact of different error types, alone and in com-bination, both on human perception of quality andon MT evaluation metrics?
results.
Our analysisis carried out by employing linear mixed-effectsmodels, a generalization of linear regression mod-els suited to model responses with fixed and ran-dom effects.
Experiments are performed on datacovering three translation directions (English toChinese, Arabic and Russian).
For each direc-tion, two automatic translations were collected foraround 400 sentences and were manually evalu-ated by expert translators through absolute qualityjudgements and error annotation.Building on the advantages offered by linearmixed-effects models, our main contributions in-clude:?
A rigorous method, novel to MT error anal-ysis research, to relate MT issues to humanpreferences and MT metrics?
results;?
The application of such method to threetranslation directions having English assource and different languages as target;?
A number of findings, specific to each lan-guage direction, which are out of the reach ofthe few simpler methods proposed so far.Overall, our study has clear practical implica-tions for MT systems?
development and evalu-ation.
Indeed, the proposed statistical analysisframework represents an ideal instrument to: i)identify translation issues having the highest im-pact on human perception of quality and ii) choosethe most appropriate evaluation metric to measureprogress towards their solution.2 Related WorkError analysis, as a way to identify systems?
weak-nesses and define priorities for their improvement,is gaining increasing interest in the MT com-munity (Popovi?c and Ney, 2011; Popovic et al.,2013).
Along this direction, the initial efforts todevelop error taxonomies covering different levelsof granularity (Flanagan, 1994; Vilar et al., 2006;Farr?us Cabeceran et al., 2010; Stymne and Ahren-berg, 2012; Lommel et al., 2014) have been re-cently complemented by investigations on how toexploit error annotations for diagnostic purposes.Error annotations of sentences produced by differ-ent MT systems, in different target languages anddomains, have been used to determine the qual-ity of translations according to the amount of er-rors encountered (Popovic et al., 2013), to designnew automatic metrics that take into considera-tion human annotations (Popovic, 2012; Bojar etal., 2013), and to train classifiers that can auto-matic identify fine-grained errors in the MT output(Popovi?c and Ney, 2011).
The impact of edit op-erations on post-editors?
productivity, which im-plicitly connects the severity of different errors tohuman activity, has also been studied (Temnikova,2010; O?Brien, 2011; Blain et al., 2011), butfew attempts have been made to explicitly modelhow fine-grained errors impact on human qualityjudgements and automatic metrics.Recently, the relation between different errortypes, their frequency, and human quality judge-ments has been investigated from a descriptivestandpoint in (Lommel et al., 2014; Popovi?c et al.,2014).
In both works, however, the underlying as-sumption that the most frequent error has also thelargest impact on quality perception is not verified(in general and, least of all, across language pairs,domains, MT systems and post-editors).
Anotherlimitation of the proposed (univariate) analysis liesin the fact that it exclusively focuses on error typestaken in isolation.
This simplification excludes thepossibility that humans, when assigning a globalquality score to a translation, may be influencednot only by the error types but also by their inter-action.
The implications of such possibility callfor a multivariate analysis capable to model alsoerror interactions.In (Kirchhoff et al., 2013), a statistically-grounded approach based on conjoint analysis hasbeen used to investigate users?
reactions to dif-ferent types of translation errors.
According to1644their results, word order is the most dispreferrederror type, and the count of the errors in a sen-tence is not a good predictor of users?
prefer-ences.
Though more sophisticated than methodsbased on rough error counts, the conjoint modelis bound to several constraints that limit its us-ability.
In particular, the application of conjointanalysis in this context requires to: i) operate withsemi-automatically created (hence artificial) datainstead of real MT output, ii) manually define dif-ferent levels of severity for each error type (e.g.high/medium/low), and iii) limit the number of er-ror types considered to avoid the explosion of allpossible combinations.
Finally, the conjoint anal-ysis framework is not able to explicitly model vari-ance in the translated sentences, the human anno-tators, and the SMT systems used to translate thesource sentences.
Our claim is that avoiding anypossible bias introduced by these factors should bea priority in the analysis of empirical observationsin a given experimental setting.So far, the relation between errors and auto-matic metrics has been analysed by measuring thecorrelation between single or total error frequen-cies and automatic scores (Popovi?c and Ney, 2011;Farr?us et al., 2012).
Using two different error tax-onomies, both works show that the sum of the er-rors has a high correlation with BLEU and TERscores.
Similar to the aforementioned works ad-dressing the impact of MT errors on human per-ception, these studies disregard error interactions,and their possible impact on automatic scores.To overcome these issues, we propose a ro-bust statistic analysis framework based on mixed-effects models, which have been successfully ap-plied to several NLP problems such as sentimentanalysis (Greene and Resnik, 2009), automaticspeech recognition (Goldwater et al., 2010), andspoken language translation (Ruiz and Federico,2014).
Despite their effectiveness, the use ofmixed-effects models in the MT field is rather re-cent and limited to the analysis of human post-editions (Green et al., 2013; L?aubli et al., 2013).In both studies, the goal was to evaluate the im-pact of post-editing on the quality and productivityof human translation assuming an ANOVA mixedmodel for a between-subject design, in which hu-man translators either post-edited or translated thesame texts.
Our scenario is rather different as weemploy mixed models to measure the influence ofdifferent MT error types - expressed as continu-ous fixed effects - on quality judgements and auto-matic quality metrics.
Mixed models, having thecapability to absorb random variability due to thespecific experimental set-up, provide a robust mul-tivariate method to efficiently analyse the impor-tance of error types.Finally, differently from all previous works, ouranalysis is run on language pairs having Englishas source and languages distant from English (interm of morphology and word-order) as target.3 Mixed-effects ModelsMixed-effects models - or simply mixed models- like any regression model, express the relation-ship between a response variable and some co-variates and/or contrast factors.
They enhanceconventional models by complementing fixed ef-fects with so-called random effects.
Random ef-fects are introduced to absorb random variabilityinherent to the specific experimental setting fromwhich the observations are drawn.
In general, ran-dom effects correspond to covariates that are not -or cannot be - exhaustively observed in an experi-ment, e.g.
the human annotators and the evaluatedsystems.
Hence, mixed models permit to elegantlycope with experimental design aspects that hinderthe applicability of conventional regression mod-els.
These are, in particular, the use of repeatedand/or clustered observations that introduce corre-lations in the response variable that clearly violatethe independence and homoscedasticity assump-tions of conventional linear, ANOVA, and logis-tic regression models.
Significance testing withmixed models is in general more powerful, i.e.
lessprone to Type II Errors, and also permits to reducethe chance of Type I Errors in within-subject de-signs, which are prone to the ?fallacy of language-as-a-fixed-effect?
(Clark, 1973).Random effects can be directly associated tothe regression model parameters, as random in-tercepts and random slopes, and have the sameform of the generic error component of the model,i.e.
normally distributed with zero mean and un-known variance.
As random effects introduce hid-den variables, mixed models are trained with Ex-pectation Maximization, while significance testingis performed via likelihood-ratio (LR) tests.In this work we employ mixed linear models tomeasure the influence of different MT error types,expressed as continuous fixed effects, on quality1645judgements or on automatic quality metrics.1We illustrate mixed linear models (Baayen etal., 2008) by referring to our analysis, which ad-dresses the relationships between a quality metric(y) and different types of errors (e.g.
A, B, andC)2observed at the sentence level.
For the sake ofsimplicity, we assume to have balanced repeatedobservations for one single crossed effect.
That is,we have i ?
{1, .
.
.
, I} MT systems (our groups)each of which translated the same j ?
{1, .
.
.
, J}test sentences.
Our response variable yij- a nu-meric quality score - is computed on each (sen-tence, system) pair, and we aim to investigate itsrelationship with error statistics available for eachMT output, namely Aij, Bijand Cij.
A (possible)linear mixed model for our study would be:yij= ?0+ ?1Aij+ ?2Bij+ ?3Cij+ (1)b0,i+ b1,iAij+ b2,iBij+ b3,iCi+ ijThe model is split into two lines on purpose.
Thefirst line shows the fixed effect component, that isintercept (?0) and slopes (?1, ?2, ?3) for each errortype.
The second line specifies the random struc-ture of the model, which includes random inter-cept and slopes for each MT system and the resid-ual error.
Borrowing the notation from (Greenet al., 2013), we conveniently rewrite (1) in thegroup-wise arranged matrix notation:yi= xTi?
+ zTibi+ i(2)where yiis the J ?
1 vector of responses, xiis theJ?p design matrix of covariates (including the in-tercept) with fixed coefficients ?
?
Rp?1, z is therandom structure matrix defined by J ?
q covari-ates with random coefficients bi?
Rq?1, and iisthe vector of residuals (in our example, p = 4 andq = 4).
By packing together vectors and matricesindexed over groups i, we can rewrite the modelin a general form (Baayen et al., 2008), which canrepresent any possible crossed-effects and randomstructures defined over them allowing, at the sametime, for a compact model specification:y = XT?
+ ZTb+  (3) ?
N (0, ?2I), b ?
N (0, ?2?
), b ?
1Although mixed ordinal models (Tutz and Hennevogl,1996) are in principle more appropriate to target qualityjudgements, in our preliminary investigations mixed linearmodels showed a significantly higher predictive power.2Here, A, B and C represent three generic error classes.Their actual number in a given experimental setting will de-pend on the granularity of the reference error taxonomy.where ?
is the relative variance-covariance q ?
qmatrix of the random effects (now q = 4I), ?2is the variance of the per-observation term , thesymbol ?
denotes independence of random vari-ables, andN indicates the multivariate normal dis-tribution.
While b, ?, and ?
are estimated via max-imum likelihood, the single random intercept andslope values for each group are calculated subse-quently.
They are referred to as Best Linear Un-biased Predictors (BLUPS) and, formally, are notparameters of the model.The significance of the contribution of each sin-gle parameter (e.g.
single entries of ?)
to thegoodness of fit can be tested via likelihood ratio.In this way, both the fixed and random effect struc-ture of the model can be investigated with respectto its actual necessity to the model.4 DatasetFor our analysis we used a dataset that coversthree translation directions, corresponding to En-glish to Chinese, Arabic, and Russian.
An inter-national organization provided us a set of Englishsentences together with their translation producedby two anonymous MT systems.
For each evalu-ation item (source sentence and two MT outputs)three experts were asked to assign quality scores tothe MT outputs, and a fourth expert was asked toannotate translation errors.
The four experts, whowere all professional translators native in the ex-amined target languages, were carefully trained toget acquainted with the evaluation guidelines andthe annotation tool specifically developed for theseevaluation tasks (Girardi et al., 2014).
The anno-tation process was carried out in parallel by all an-notators over one week, resulting in a final datasetcomposed of 312 evaluation items for the ENZHdirection, 393 for ENAR, and 437 for ENRU.4.1 Quality JudgementsQuality judgements were collected by asking thethree experts to rate each automatic translationaccording to a 1-5 Likert scale, where 1 means?incomprehensible translation?
and 5 means ?per-fect translation?.
The distribution of the collectedannotations with respect to each quality score isshown in Figure 1.
As we can see, this distri-bution reflects different levels of perceived qual-ity across languages.
ENZH, for instance, has thehighest number of low quality scores (1 and 2),while ENRU has the highest number of high qual-16460%?20%?40%?60%?80%?100%?ENZH?
ENAR?
ENRU?5?4?3?2?1?Figure 1: Distribution of quality scores.ity scores (4 and 5).Table 1 shows the average of all the qual-ity scores assigned by each annototator as wellas the average score obtained for each MT sys-tem.
These values demonstrate the variabilityof annotators and systems.
A particularly highvariability among human judges is observed forthe ENAR language direction (also reflected bythe inter-annotator agreement scores discussed be-low), while ENZH shows the highest variabilitybetween systems.
As we will see in ?5.1, we suc-cessfully cope with this variability by consideringsystems and annotators as random effects, whichallow the regression models to abstract from thesedifferences.Ann1 Ann2 Ann3 Sys1 Sys2ENZH 2.38 2.69 2.21 2.29 2.56ENAR 2.76 2.77 1.84 2.39 2.53ENRU 2.82 2.72 2.96 2.87 2.79Table 1: Average quality scores per annotator andper system.Inter-annotator agreement was computed usingthe Fleiss?
kappa coefficient (Fleiss, 1971), and re-sulted in 22.70% for ENZH, 5.24% for ENAR, and21.80% for ENRU.
While for ENZH and ENRUthe results fall in the range of ?fair?
agreement(Landis and Koch, 1977), for ENAR only ?slight?agreement is reached, reflecting the higher anno-tators?
variability evidenced in Table 1.A more fine-grained agreement analysis is pre-sented in Figure 2, where the kappa values aregiven for each score class.
In general we no-tice a lower agreement on the intermediate qualityscores, while annotators tend to agree on very badand, even more, on good translations.
In partic-ular, we see that the agreement for ENAR is sys-tematically lower than the values measured for theother languages on all the score classes.-??0.1?0?0.1?0.2?0.3?0.4?0.5?0.6?0.7?1?
2?
3?
4?
5?FLEISS'?KAPPA?QUALITY?SCORES?ENZH?ENAR?ENRU?Figure 2: Class specific inter-annotator agreement.4.2 Error AnnotationThis evaluation task was carried out by one ex-pert for each language direction, who was asked toidentify the type of errors present in the MT outputand to mark their position in the text.
Since the fo-cus of our work is the analysis method rather thanthe definition of an ideal error taxonomy, for thedifficult language directions addressed we optedfor the following general error classes, partiallyoverlapping with (Vilar et al., 2006): i) reorderingerrors, ii) lexicon errors (including wrong lexicalchoices and extra words), iii) missing words, iv)morphology errors.Figure 3 shows the distribution of the errors interms of affected tokens (words) for each errortype.
Since token counts for Chinese are not word-based but character-based, for readability purposesthe number of errors counted for Chinese trans-lations have been divided by 2.5.
Note also thatmorphological errors annotated for ENZH involveonly 13 characters and thus are not visible in theplot.
The total number of errors amounts to 16,320characters for ENZH, 4,926 words for ENAR, and5,965 words for ENRU.This distribution highlights some differencesbetween languages directions.
For example, trans-lations into Arabic and Russian present severalmorphology errors, while word reordering is themost frequent issue for translations into Chinese.As we will see in ?5.1, error frequency does notgive a direct indication of their impact on trasla-tion quality judgements.4.3 Automatic MetricsIn our investigation we consider three popular au-tomatic metrics: sentence-level BLEU (Lin andOch, 2004), TER (Snover et al., 2006), and GTM(Turian et al., 2003).
We compute all automaticscores by relying on a single reference and by16470?500?1000?1500?2000?2500?3000?3500?4000?ENZH?
ENAR?
ENRU?LEX?MISS?MORPH?REO?Figure 3: Distribution of error types.means of standard packages.
In particular, auto-matic scores on Chinese are computed at the char-acter level.
Moreover, as we use metrics as re-sponse variables for our regression models, wecompute all metrics at the sentence level.
Theoverall mean scores for all systems and languagesare reported in Table 2.
Differences in systems?performance can be observed for all languagepairs; as we will observe in ?5.2 such variabilityexplains the effectiveness of considering the MTsystems as a random effect.BLEU TER GTMSys1 Sys2 Sys1 Sys2 Sy1 Sys2ENZH 27.95 44.11 64.52 48.13 62.15 72.30ENAR 19.63 25.25 68.83 63.99 47.20 52.33ENRU 27.10 31.07 60.89 54.41 53.74 56.41Table 2: Overall automatic scores per system.5 ExperimentsTo assess the impact of translation errors on MTquality we perform two sets of experiments.
Thefirst set (?5.1) addresses the relation between er-rors and human quality judgements.
The sec-ond set (?5.2) focuses on the relation between er-rors and automatic metrics.
In both cases, be-fore measuring the impact of different errors onthe response variable (respectively quality judge-ments and metrics), we validate the effectivenessof mixed linear models by comparing their predic-tion capability with other methods.In all experiments, error counts of each categorywere normalized into percentages with respect tothe sentence length and mapped in a logarithmicscale.
In this way, we basically assume that theimpact of errors tends to saturate above a giventhreshold, hypothesis that also results in better fitsby our models.3Notice that while the chosen log-3In other words, we assume that human sensitivity to er-10 base is easy to interpret, linear models can im-plicitly adjust it.
Our analysis makes use of mixedlinear models incorporating, as fixed effects, thefour types of errors (lex, miss, morph and reo) andtheir pairwise interactions (the product of the sin-gle error log counts), while their random struc-ture depends on each specific experiment.
Forthe experiments we rely on the R language (RCore Team, 2013) implementation of linear mixedmodel in the lme4 library (Bates et al., 2014).We assess the quality of our mixed linear mod-els (MLM) by comparing their prediction capabil-ity with a sequence of simpler linear models in-cluding only fixed effects.
In particular, we builtfive univariate models and two multivariate mod-els.
The univariate models use as covariates, re-spectively, the sum of all error types (baseline),and each of the four types of errors (lex, miss,morph and reo).
The two multivariate models in-clude all the four error types, considering themwithout interactions (FLM w/o Interact.)
and withinteractions (FLM).Prediction performance is computed in terms ofMean Absolute Error (MAE),4which we estimateby averaging over 1,000 random splits of the datain 90% training and 10% test.
In particular, for thehuman quality classes we pick the integer between1-5 that is closest to the predicted value.5.1 Errors vs. Quality JudgementsThe response variable we target in this experimentis the quality score produced by human annotators.Our measurements follow a typical within-subjectdesign in which all the 3 annotators are exposedto the same conditions (levels of the independentvariables), corresponding in our case to perfectlybalanced observations from 2 MT systems and Nsentences.
This setting results in repeated or clus-tered observations (thus violating independence)corresponding to groups which naturally identifypossible random effects,5namely the annotators(3 levels with 2xN observations each), the systems(2 levels and 3xN observations each), and the sen-rors follows a log-scale law: e.g.
more sensitive to variationsin the interval [1-10] that in the interval [30-40].4MAE is calculated as the average of the absolute errors|fi?
yi|, where fiis the prediction of the model and yithetrue value for the ithinstance.
As it is a measure of error,lower MAE scores indicate that our predictions are closer tothe true values of each test instance.5In all our experiments, random effects are limited to ran-dom shifts since preliminary experiments also including ran-dom slopes did not provide consistent results.1648Model ENZH ENAR ENRUbaseline 0.58 0.73 0.67lex 0.67 0.78 0.72miss 0.72 0.89 0.74morph 0.72 0.89 0.74reo 0.70 0.82 0.76FLM w/o Interact.
0.59 0.77 0.65FLM 0.57 0.72 0.63MLM 0.53 0.61 0.61Table 3: Prediction capability of human judge-ments (MAE).tences (N levels with 6 observations each).
In prin-ciple, such random effects permit to remove sys-tematic biases of individual annotators, single sys-tems and even single sentences, which are mod-elled as random variables sampled from distinctpopulations.Table 3 shows a comparison of the predictioncapability of the mixed model6with simpler ap-proaches.
While the good performance achievedby our strong baseline cannot be outperformedby separately counting the number of errors of asingle type, lower MAE results are obtained bymethods based on multivariate analysis.
Amongthem, FLM brings the first consistent improve-ments over the baseline by considering error in-teractions, while MLM leads to the lowest MAEdue to the addition of random effects.
The impor-tance of random effects is particularly evidencedby ENAR (12 points below the baseline).
Indeed,as discussed in ?4.1, for this language combina-tion human annotators show the lowest agreementscore.
This variability, which hides the smallerdifferences in systems?
behaviour, demonstratesthe importance of accounting for the erratic fac-tors that might influence empirical observations ina given setting.
The good performance achievedby MLM, combined with their high descriptivepower,7motivates their adoption in our study.Concerning the analysis of error impact, Ta-ble 4 shows the statistically significant coefficientsfor the full-fledged MLM models for each trans-lation direction.
By default, all reported coeffi-cients have p-values ?
10?4, while those markedwith ?
and ?
have respectively p-values ?
10?3and ?
10?2.
Slope coefficients basically show6Note that the mixed model used in prediction does not in-clude the random effect on sentences since the training sam-ples do not guarantee sufficient observations for each test sen-tence.7Note that the strong baseline used for comparison is notcapable to describe the contribution of the different errortypes.Error ENZH ENAR ENRUIntercept 4.29 3.79?4.21lex -1.27 -0.96 -1.12miss -1.76 -0.90 -1.30morph -0.48?-0.83 -0.51reo -1.01 -0.75 -0.18lex:miss 1.00 0.39 0.68lex:morph - 0.29 0.32lex:reo 0.50 0.21 -miss:morph - 0.35 -miss:reo 0.54 0.33 -morph:reo - 0.37 -Table 4: Effect of translation errors on MT qual-ity perception on all judged sentences.
Reportedcoefficients (?)
are all statistically significant withp ?
10?4, except those marked with?
(p ?
10?3),and?
(p ?
10?2).the impact of different error types (alone and incombination) on human quality scores.
Those thatare not statistically significant are omitted as theydo not increase the fitting capability of our model.As can be seen from the table, such impact variesacross the different language combinations.
Whilefor ENZH and ENRU miss is the error havingthe highest impact (highest decrement with respectto the intercept), the most problematic error forENAR is lex.
It is interesting to observe that pos-itive values for error combinations indicate thattheir combined impact is lower that the sum of theimpact of the single errors.
For instance, while forENZH a one-step increment in lex and miss errorswould respectively cause a reduction in the humanjudgement of 1.27 and 1.76, their occurrence inthe same sentence would be discounted by 1.00.This would result in a global judgement of 2.26(4.29 -1.27 -1.76 +1.00) instead of 1.26.
Whilefor ENAR this phenomenon can be observed forall error combinations, such discount effects arenot always significant for the other two languagepairs.
The existence of discount effects of variousmagnitude associated to the different error com-binations is a novel finding made possible by theadoption of mixed-effect models.Another interesting observation is that, in con-trast with the common belief that the most fre-quent errors have the highest impact on humanquality judgements, our experiments do not re-veal such strict correlation (at least for the exam-ined language pairs).
For instance, for ENZH andENRU the impact of miss errors is higher than theimpact of other more frequent issues.1649BLEU score TER GTMModel ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRUbaseline 12.4 9.8 12.2 15.7 13.4 14.4 9.8 10.6 11.5lex 12.9 10.4 13.0 16.3 13.8 14.9 9.7 10.9 12.1miss 13.8 10.5 14.1 17.3 14.2 16.4 10.5 11.1 13.2morph 13.9 10.3 13.6 17.5 13.8 16.3 10.5 10.9 13.1reo 13.7 10.5 14.0 17.4 14.1 16.3 10.4 11.1 13.1FLM w/o Interact.
12.9 9.9 12.2 16.3 13.5 14.4 9.7 10.7 11.7FLM 12.3 9.7 12.1 15.6 13.4 14.3 9.4 10.6 11.6MLM 10.8 9.5 12.0 14.7 13.0 14.2 8.9 10.5 11.6Table 5: Prediction capability of BLEU score, TER and GTM (MAE).5.2 Errors vs. Automatic MetricsIn this experiment, the response variable is an au-tomatic metric which is computed on a sample ofMT outputs (which are again perfectly balancedover systems and sentences) and a set of referencetranslations.
As no subjects are involved in the ex-periment, random variability is assumed to comefrom the involved systems, the tested sentences,and the unknown missing link between the covari-ates (error types) and the response variable whichis modelled by the residual noise.
Notice that,in this case, the random effect on the sentencesalso incorporates in some sense the randomnessof the corresponding reference translations, whichare themselves representatives of larger samples.The prediction capability of the mixed model,in comparison with the simpler ones, is reportedin Table 5.
Also in this case, the low MAEachieved by the baseline is out of the reach of uni-variate methods.
Again, small improvements arebrought by FLM when considering error interac-tions, whereas the most visible gains are achievedby MLM due to their control of random effects.This is more evident for some language combina-tions and can be explained by the differences insystems?
performance, a variability factor easilyabsorbed by random effects.
Indeed, the largestMAE decrements over the baseline are always ob-served for ENZH (for which the overall mean re-sults reported in Table 2 show the largest dif-ferences) and the smallest decrements relate tolanguage/metric combinations where systems?
be-haviour is more similar (e.g.
ENRU/GTM).Concerning the analysis of error impact, Table6 shows how different error types (alone and incombination) influence performance results mea-sured with automatic metrics.
To ease interpre-tation of the reported figures we also show Pear-son and Spearman correlations of each set of coef-ficients (excluding intercept estimates) with theircorresponding coefficients reported in Table 4.
Infact, our primary interest in this experiment is tosee which metrics show a sensitivity to specific er-ror types similar to human perception.
As we cansee, the coefficients for each metric significantlyvary depending on the language, for the simplereason that also the distribution and co-occurrenceof errors vary significantly across the different lan-guages and MT systems.
Remarkably, for sometranslation directions, some of the metrics showa sensitivity to errors that is very similar to thatof human judges.
In particular, BLEU for ENZHand ENAR, and GTM for ENZH show a very highcorrelation with the human sensitivity to transla-tion errors, with Pearson correlation coefficient ?0.97.
For ENRU, the best Pearson correlation isinstead achieved by TER (-0.78).Besides these general observations, a closerlook at the reported scores brings additional find-ings.
In three cases (BLEU for ENZH, GTM forENZH and ENAR) the analysed metrics are mostsensitive to the same error type that has the high-est influence on human judgements (according toTable 4, these are miss for ENZH and ENRU, lexfor ENAR).
On the contrary, in one case (TER forENZH) the analysed metric is insensitive to the er-ror type (miss) which has the highest impact on hu-man quality scores.
From a practical point of view,these remarks provide useful indications about theappropriateness of each metric to highlight the de-ficiencies of a specific system and to measure im-provements targeting specific issues.
As a rule ofthumb, for instance, to measure improvements ofan ENZH system with respect to missing words,it would be more advisable to use BLEU or GTMinstead of TER.88Note that this conclusion holds for our data sample, inwhich different types of errors co-occur and only one refer-ence translation is available.
In such conditions, our regres-sion model shows that TER is not influenced by miss errors ina statistically significant way.
This does not mean that TERis insensitive to missing words when occurring in isolation,1650BLEU score TER GTMError ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRUIntercept 60.55238.45?51.73 32.41252.25?33.4?83.57?60.11?75.38lex -18.78 -9.25 -16.57 16.87 9.66 18.45 -13.63 -7.60 -16.13miss -23.20 -10.41 -6.75 - - 8.24 -14.87 - -5.98morph - -9.97 -12.65 - 8.90 11.41 - -6.60 -10.42reo -13.27 -7.62 -10.57 14.44 9.81 6.39 -7.29 -5.50 -7.03lex:miss 14.37 4.97?- - - - 8.24?- -lex:morph - - 5.27?- - -5.22?- - 4.92lex:reo 8.57 3.57?5.40?-7.24?-4.35?- 5.46 3.22?3.652miss:morph - 4.44?- - - - - - -miss:reo 6.74?- 4.30 - - -6.38?5.07?- 4.71?morph:reo - 3.81?- - -4.97?- - 2.57?-Pearson 0.98 0.97 0.70 -0.58 -0.78 -0.78 0.98 0.78 0.74Spearman 0.97 0.91 0.73 -0.57 -0.59 -0.80 0.97 0.59 0.76Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla-tion with their corresponding effects on human quality scores (from Table 4).
Reported coefficients (?
)are statistically significant with p ?
10?4, except those marked with?
(p ?
10?3),?
(p ?
10?2) and2(p ?
10?1).Similar considerations also apply to the analysisof the impact of error combinations.
The same dis-count effects that we noticed when analysing theimpact of errors?
co-occurrence on human percep-tion (?5.1) are evidenced, with different degrees ofsensitivity, by the automatic metrics.
While someof them substantially reflect human response (e.g.BLEU and GTM for ENZH), in some cases weobserve either the insensitivity to specific combi-nations (mostly for ENAR), or a higher sensitivitycompared to the values measured for human as-sessors (mostly for ENRU, where the impact ofmiss:reo combinations is discounted - hence un-derestimated - by all the metrics).Despite such small differences, the coherence ofour results with previous findings (?5.1) suggeststhe reliability of the applied method.
Complet-ing the picture along the side of the MT evalua-tion triangle which connects error annotations andautomatic metrics, our findings contribute to shedlight on the existing relationships between transla-tion errors, their interaction, and the sensitivity ofwidely used automatic metrics.6 ConclusionWe investigated the MT evaluation triangle (hav-ing as corners automatic metrics, human qualityjudgements and error annotations) along the twoless explored sides, namely: i) the relation be-tween MT errors and human quality judgementsbut that TER becomes less sensitive to such errors when theyco-occur with other types of errors.
Overall, our experimentsshow that when MT outputs contain more than one error type,automatic metrics show different levels of sensitivity to eachspecific error type.and ii) the relation between MT errors and auto-matic metrics.
To this aim we employed a ro-bust statistical analysis framework based on lin-ear mixed-effects models (the first contribution ofthe paper), which have a higher descriptive powerthan simpler methods based on the raw count oftranslation errors and are less artificial comparedto previous statistically-grounded approaches.Working on three translation directions havingChinese, Arabic and Russian as target (our secondcontribution), we analysed error-annotated trans-lations considering the impact of specific errors(alone and in combination) and accounting for thevariability of the experimental set-up that origi-nated our empirical observations.
This led us tointeresting findings specific to each language pair(third contribution).
Concerning the relation be-tween MT errors and quality judgements, we haveshown that: i) the frequency of errors of a giventype does not correlate with human preferences,ii) errors having the highest impact can be pre-cisely isolated and iii) the impact of error inter-actions is often subject to measurable and previ-ously unknown ?discount?
effects.
Concerning therelation between MT errors and automatic met-rics (BLEU, TER and GTM), our analysis evi-denced significant differences in the sensitivity ofeach metric to different error types.
Such differ-ences provide useful indications about the mostappropriate metric to assess system improvementswith respect to specific weaknesses.
If learningfrom errors is a crucial aspect of improving exper-tise, our method and the resulting empirical find-ings represent a significant contribution towards a1651more informed approach to system development,improvement and evaluation.AcknowledgementsThis work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).ReferencesHarald R. Baayen, Douglas J. Davidson, and Dou-glas M. Bates.
2008.
Mixed-effects modeling withcrossed random effects for subjects and items.
Jour-nal of memory and language, 59(4):390?412.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Douglas Bates, Martin Maechler, Ben Bolker, andSteven Walker, 2014. lme4: Linear mixed-effectsmodels using Eigen and S4.
R package version 1.1-6.Fr?ed?eric Blain, Jean Senellart, Holger Schwenk, MirkoPlitt, and Johann Roturier.
2011.
Qualitative analy-sis of post-editing for high quality machine transla-tion.
In Asia-Pacific Association for Machine Trans-lation (AAMT), editor, Machine Translation SummitXIII, Xiamen (China), 19-23 sept.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Herbert H. Clark.
1973.
The language-as-fixed-effectfallacy: A critique of language statistics in psycho-logical research.
Journal of verbal learning and ver-bal behavior, 12(4):335?359.Mireia Farr?us, Marta R. Costa-juss`a, and MajaPopovi?c.
2012.
Study and correlation analysis oflinguistic, perceptual, and automatic machine trans-lation evaluations.
J.
Am.
Soc.
Inf.
Sci.
Technol.,63(1):174?184, January.Mireia Farr?us Cabeceran, Marta Ruiz Costa-Juss`a,Jos?e Bernardo Mari?no Acebal, Jos?e Adri?anRodr?
?guez Fonollosa, et al.
2010.
Linguistic-basedevaluation criteria to identify statistical machinetranslation errors.
In Proceedings of the 14thAnnual Conference of the European Association forMachine Translation (EAMT).Mary Flanagan.
1994.
Error classification for mt eval-uation.
In Technology Partnerships for Crossing theLanguage Barrier: Proceedings of the First Confer-ence of the Association for Machine Translation inthe Americas, pages 65?72.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5).Christian Girardi, Luisa Bentivogli, Mohammad AminFarajian, and Marcello Federico.
2014.
Mt-equal:a toolkit for human assessment of machine trans-lation output.
In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics: System Demonstrations, pages 120?123, Dublin, Ireland, August.
Dublin City Univer-sity and Association for Computational Linguistics.Sharon Goldwater, Daniel Jurafsky, and Christopher D.Manning.
2010.
Which words are hard to rec-ognize?
prosodic, lexical, and disfluency factorsthat increase speech recognition error rates.
SpeechCommunication, 52(3):181?200.Spence Green, Jeffrey Heer, and Christopher D. Man-ning.
2013.
The efficacy of human post-editing forlanguage translation.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems, pages 439?448.
ACM.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, NAACL ?09, pages 503?511, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner.2013.
A conjoint analysis framework for evaluatinguser preferences in machine translation.
MachineTranslation, pages 1?17.Richard J. Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33 (1):159?174.Samuel L?aubli, Mark Fishel, Gary Massey, MaureenEhrensberger-Dow, and Martin Volk.
2013.
Assess-ing Post-Editing Efficiency in a Realistic TranslationEnvironment.
In Michel Simard Sharon O?Brienand Lucia Specia (eds.
), editors, Proceedings of MTSummit XIV Workshop on Post-editing Technologyand Practice, pages 83?91, Nice, France.Chin-Yew Lin and Franz Josef Och.
2004.
Orange:a method for evaluating automatic evaluation met-rics for machine translation.
In Proceedings of Col-ing 2004, pages 501?507, Geneva, Switzerland, Aug23?Aug 27.
COLING.Arle Lommel, Aljoscha Burchardt, Maja Popovi?c, KimHarris, Eleftherios Avramidis, and Hans Uszkoreit.16522014.
Using a new analytic measure for the anno-tation and analysis of mt errors on real data.
InProceedings of the 17th Conference of the Euro-pean Association for Machine Translation (EAMT),Dubrovnik, Croatia, June.Sharon O?Brien.
2011.
Cognitive Explorations ofTranslation.
Bloomsbury Studies in Translation.Bloomsbury Academic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Research ReportRC22176, IBM Research Division, Thomas J. Wat-son Research Center.Maja Popovi?c and Hermann Ney.
2011.
Towards au-tomatic error analysis of machine translation output.Comput.
Linguist., 37(4):657?688, December.Maja Popovic, Eleftherios Avramidis, Aljoscha Bur-chardt, Sabine Hunsicker, Sven Schmeier, CindyTscherwinka, David Vilar, and Hans Uszkoreit.2013.
Learning from human judgments of machinetranslation output.
In Proceedings of the MT SummitXIV.
Proceedings of MT Summit XIV.Maja Popovi?c, Arle Lommel, Aljoscha Burchardt,Eleftherios Avramidis, and Hans Uszkoreit.
2014.Relations between different types of post-editing op-erations, cognitive effort and temporal effort.
InProceedings of the 17th Conference of the Euro-pean Association for Machine Translation (EAMT),Dubrovnik, Croatia, June.Maja Popovic.
2012.
Class error rates for evaluationof machine translation output.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 71?75, Montr?eal, Canada, June.
Associ-ation for Computational Linguistics.R Core Team, 2013.
R: A Language and Environmentfor Statistical Computing.
R Foundation for Statis-tical Computing, Vienna, Austria.Nick Ruiz and Marcello Federico.
2014.
Assessing theImpact of Speech Recognition Errors on MachineTranslation Quality.
In 11th Conference of the As-sociation for Machine Translation in the Americas(AMTA), Vancouver, BC, Canada.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In 5th Conference of the Association for MachineTranslation in the Americas (AMTA), Boston, Mas-sachusetts, August.Sara Stymne and Lars Ahrenberg.
2012.
Onthe practice of error analysis for machine trans-lation evaluation.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Thierry Declerck,Mehmet Uur Doan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey, may.
European Language ResourcesAssociation (ELRA).Irina Temnikova.
2010.
Cognitive evaluation approachfor a controlled language post-editing experiment.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Bente Maegaard, Joseph Mariani, JanOdijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Eval-uation (LREC?10), Valletta, Malta, may.
EuropeanLanguage Resources Association (ELRA).Joseph P. Turian, I. Dan Melamed, and Luke Shen.2003.
Evaluation of machine translation and itsevaluation.
In Proceedings of the MT Summit IX.Gerhard Tutz and Wolfgang Hennevogl.
1996.
Ran-dom effects in ordinal regression models.
Computa-tional Statistics & Data Analysis, 22(5):537?557.David Vilar, Jia Xu, Luis Fernando dHaro, and Her-mann Ney.
2006.
Error analysis of statistical ma-chine translation output.
In Proceedings of the FifthInternational Conference on Language Resourcesand Evaluation (LREC?06), pages 697?702.1653
