Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 27?36,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsA Demographic Analysis of Online Sentiment during Hurricane IreneBenjamin Mandel?, Aron Culotta?, John Boulahanis+,Danielle Stark+, Bonnie Lewis+, Jeremy Rodrigue+?Department of Computer Science and Industrial Technology+Department of Sociology and Criminal JusticeSoutheastern Louisiana UniversityHammond, LA 70402AbstractWe examine the response to the recent nat-ural disaster Hurricane Irene on Twitter.com.We collect over 65,000 Twitter messages re-lating to Hurricane Irene from August 18th toAugust 31st, 2011, and group them by loca-tion and gender.
We train a sentiment classi-fier to categorize messages based on level ofconcern, and then use this classifier to investi-gate demographic differences.
We report threeprincipal findings: (1) the number of Twit-ter messages related to Hurricane Irene in di-rectly affected regions peaks around the timethe hurricane hits that region; (2) the level ofconcern in the days leading up to the hurri-cane?s arrival is dependent on region; and (3)the level of concern is dependent on gender,with females being more likely to express con-cern than males.
Qualitative linguistic vari-ations further support these differences.
Weconclude that social media analysis providesa viable, real-time complement to traditionalsurvey methods for understanding public per-ception towards an impending disaster.IntroductionIn 2011, natural disasters cost the United Statesmore than 1,000 lives and $52 billion.
The num-ber of disasters costing over $1 billion in 2011(twelve) is more than in the entire decade of the1980s.1 As the number of people living in disaster-prone areas grows, it becomes increasingly impor-tant to have reliable, up-to-the-minute assessmentsof emergency preparedness during impending disas-1?Record year for billion-dollar disasters?, CBS News, De-cember 7, 2011.ters.
Understanding issues such as personal risk per-ception, preparedness, and evacuation plans helpspublic agencies better tailor emergency warnings,preparations, and response.Social scientists typically investigate these issuesusing polling data.
The research shows significantdemographic differences in response to governmentwarnings, personal risk assessment, and evacuationdecisions (Perry and Mushkatel, 1986; Perry andLindell, 1991; Goltz et al, 1992; Fothergill et al,1999; West and Orr, 2007; Enarson, 1998).
For ex-ample, Fothergill et al (1999) find that minoritiesdiffer in their risk perception and in their response toemergency warnings, with some groups having fa-talistic sentiments that lead to greater fear and lesspreparedness.
Goltz et al (1992) find that peoplewith lower income and education, Hispanics, andwomen all expressed greater fear of earthquakes.This past research suggests governments couldbenefit by tailoring their messaging and response toaddress the variability between groups.
While sur-vey data have advanced our knowledge of these is-sues, they have two major drawbacks for use indisaster research.
First, most surveys rely on re-sponses to hypothetical scenarios, for example byasking subjects if they would evacuate under cer-tain scenarios.
This hypothetical bias is well-known(Murphy et al, 2005).
Second, surveys are often im-practical in disaster scenarios.
In a rapidly-changingenvironment, governments cannot wait for a time-consuming survey to be conducted and the resultsanalyzed before making warning and response de-cisions.
Additionally, survey response rates shortlybefore or after a disaster are likely to be quite low, ascitizens are either without power or are busy prepar-ing or rebuilding.
Thus, it is difficult to collect data27during the critical times immediately before and af-ter the disaster.In this paper, we investigate the feasibility ofassessing public risk perception using social me-dia analysis.
Social media analysis has recentlybeen used to estimate trends of interest such asstock prices (Gilbert and Karahalios, 2010), moviesales (Asur and Huberman, 2010), political mood(O?Connor et al, 2010a), and influenza rates (Lam-pos and Cristianini, 2010; Culotta, 2010; Culotta,2012).
We apply a similar methodology here to as-sess the public?s level of concern toward an impend-ing natural disaster.As a case study, we examine attitudes towardHurricane Irene expressed on Twitter.com.
We col-lect over 65,000 Twitter messages referencing Hur-ricane Irene between August 18th and August 31st,2011; and we train a sentiment classifier to annotatemessages by level of concern.
We specifically lookat how message volume and sentiment varies overtime, location, and gender.Our findings indicate that message volume in-creases over the days leading up to the hurricane,and then sharply decreases following its dispersal.The timing of the increase and subsequent decreasein messages differs based on the location relative tothe storm.
There is also an increasing proportion ofconcerned messages leading up to Hurricane Irene?sarrival, which then decreases after Irene dissipation.A demographic analysis of the proportion of con-cerned messages shows significant differences bothby region and gender.
The gender differences in par-ticular are supported by previous survey results fromthe social science literature (West and Orr, 2007).These results suggest that social media analysis is aviable technology for understanding public percep-tion during a hurricane.The remainder of the paper is organized as fol-lows: First, we describe the data collection method-ology, including how messages are annotated withlocation and gender.
Next, we present sentimentclassification experiments comparing various classi-fiers, tokenization procedures, and feature sets.
Fi-nally, we apply this classifier to the entire messageset and analyze demographic variation in levels ofconcern.Data CollectionIrene became a tropical storm on August 20th, 2011,and hit the east coast of the United States betweenAugust 26th and 28th.
This hurricane provides acompelling case to investigate for several reasons.First, Irene affected many people in many states,meaning that regional differences in responses canbe investigated.
Second, there was considerable me-dia and political attention surrounding HurricaneIrene, leading to it being a popular topic on socialnetwork sites.
Third, the fact that there was fore-warning of the hurricane means that responses to itcan be evaluated over time.Twitter is a social networking site that allowsusers to post brief, public messages to their follow-ers.
Using Twitter?s API2, we can sample many mes-sages as well as their meta-data, such as time, loca-tion, and user name.
Also, since Twitter can be usedon smart phones with batteries, power outages dueto natural disasters will presumably have less of aneffect on the volume of messages.Using Twitter?s sampling API (?spritzer?
), wesample approximately uniformly from all messagesbetween August 18 and August 31.
We then per-form keyword filtering to collect messages contain-ing the words ?Irene?
or ?Hurricane?, or the hashtag?#Irene?.
During the period of August 18th to Au-gust 31st, messages containing these keywords areoverwhelmingly related to Hurricane Irene and notsome other event.
This results in 65,062 messages.Inferring LocationIn order to determine the location of the messagesender, we process the user-reported location datafrom that user?s profile.
Since not all users enter ac-curate location data, we search for specific keywordsin order to classify the messages by state.
For exam-ple, if the location data contains a token ?VT?
or?Vermont,?
it is labeled as coming from Vermont.
(See Appendix A for more details.)
The locationswe consider are the 13 states directly affected byHurricane Irene, plus Washington DC.
These loca-tions are then grouped into 3 regions.
First, the NewEngland region consists of the states of Connecticut,Massachusetts, Rhode Island, New Hampshire, Ver-mont, and Maine.
Second, the Middle States region2http://dev.twitter.com28L?M?N?H?K?B?A?C?D?E?F?G?I?J?O?0?0.005?0.01?0.015?0.02?0.025?0.03?8/18/11?12:00?AM?8/18/11?2:00?PM?8/19/11?4:00?AM?8/19/11?6:00?PM?8/20/11?8:00?AM?8/20/11?10:00?PM?8/21/11?12:00?PM?8/22/11?2:00?AM?8/22/11?4:00?PM?8/23/11?6:00?AM?8/23/11?8:00?PM?8/24/11?10:00?AM?8/25/11?12:00?AM?8/25/11?2:00?PM?8/26/11?4:00?AM?8/26/11?6:00?PM?8/27/11?8:00?AM?8/27/11?10:00?PM?8/28/11?12:00?PM?8/29/11?2:00?AM?8/29/11?4:00?PM?8/30/11?6:00?AM?8/30/11?8:00?PM?8/31/11?10:00?AM?Message?Propor?n??New?England???Middle?States???Upper?South?A:?8?22?5:00am???Irene?becomes?a?Cat.?1?hurricane?B:?8?22?8:30pm???Irene?becomes?a?Cat.?2?hurricane?C:?8?23?1:51pm???Strong?earthquake?hits?near?Richmond,?VA.??Earlier?on?8?23,?Irene?had?been?forecast?to?hit?East?Coast?;?FEMA?held?press?conference.?D:?8?24?8:00am???Irene?becomes?a?Cat.?3?hurricane??E:?8?25?5:00am???Hurricane?and?Tropical?Storm?Watches?Issued?for?coast?in?SC,?NC?F:?8?25?5:00pm???New?Hurricane?Watches?issued?for?coastal?areas?from?VA?to?NJ.?G:?8?26?5:00am???Hurr.?Watches?in?NC?to?NJ?upgraded?to?Warnings;?new?Watches?for?NY?coast?H:?8?26?2:00pm???Irene?weakens?a?li?e,?Tropical?Storm?force?winds?arriving?along?NC?coast?I:?8?27?8:00am???Center?of?Irene?makes?landfall?at?Cape?Lookout,?NC?as?a?Cat.?1?Hurricane?J:?8?27?7:00pm???Irene?re?emerges?over?Atlan??Ocean?at?NC/VA?coastal?border?K:?8?27?11:00pm???Irene?drenching?Mid?Atlan??states?L:?8?28?11:00am???Irene?now?Tropical?Storm;?over?Southeastern?NY;?Southern?New?England?M:?8?28?5:00pm???Center?of?Irene?nearing?northern?New?England?N:?8?28?8:00pm???Major?flooding?occurring?in?parts?of?New?England?O:?8?29?5:00am??
?Remnants?of?Irene?moving?into?Quebec?and?Newfoundland;?Major?flooding?con?ues?in?parts?of?Northeast ?Figure 1: Results from Hurricane Irene Twitter data showing the influence of disaster-related events on the numberof messages from each region.
The y-axis is the proportion of all Irene-related messages from each region that wereposted during each hour.consists of New York, New Jersey, and Pennsylva-nia.
Third, the Upper South region consists of NorthCarolina, Virginia, Maryland, Delaware, and Wash-ington DC.Of the messages that we collect between Au-gust 18th and August 31st, 15,721 are identifiedas belonging to one of the directly affected areas.Grouped into regions, we find that 2,424 are fromNew England, 8,665 are from the Middle-States re-gion, and 4,632 are from the Upper South region.Figure 1 displays the messages per hour fromeach of the three regions.
The y-axis is normalizedover all messages from that region ?
e.g., a valueof 0.02 for New England means that 2% of all mes-sages from New England over the 10 day span wereposted in that hour.
This allows us to see which timeperiods were the most active for that region.
Indeed,we see that the spikes occur in geographical orderof the hurricane?s path, from the South, to the Mid-Atlantic region, and finally New England.
Addition-ally, Figure 1 is marked with data points indicatingwhich events were occurring at that time.There are several obvious limitations of this ap-proach (as explored in Hecht et al (2011)).
For ex-ample, users may enter false location information,have an outdated profile, or may be posting mes-sages from a different location.
Assuming these is-sues introduce no systemic bias, aggregate analysesshould not be significantly impacted (as supportedby the observed trends in Figure 1).Inferring GenderTo determine the gender of the message sender, weprocess the name field from the user?s profile ob-tained from the Twitter API.
The U.S. Census Bu-reau provides a list of the most popular male andfemale names in the United States.
The lists con-tain over 1,000 of the most common male namesand over 4,000 of the most common female names.After removing names that can be either male or fe-male (for example, Chris or Dana), we match thefirst name of the user to the list of names obtainedfrom the census.
Users that cannot be classified insuch a manner are labeled as unsure.
The data con-tains a total of 60,808 distinct users, of which 46%are assigned a gender (of those, 55% are female,45%male).
We find that many of the unlabeled usersare news agencies.
A similar methodology is used byMislove et al (2011).
As with geographic inference,29Total Sample 8/18/2011-8/31/2011 25,253,444Matching Irene Keywords 65,062Female-indicative names 16,326Male-indicative names 13,597Mid-Atlantic states 8,665Upper-South states 4,632New England states 2,424Table 1: Number of messages in sample for each filter.we make no attempt to model any errors introducedby this process (e.g., users providing false names).Table 1 displays statistics of the overall dataset.
Asample of 100 messages revealed no misattributedlocation or gender information.Sentiment ClassificationIn this section, we describe experiments applyingsentiment classification to assess the level of con-cern of each message.
Our goal is not to investigatenew sentiment classification techniques, but insteadto determine whether existing, well-known methodsare applicable to this domain.
While there is an ex-tensive literature in sentiment classification technol-ogy (Pang and Lee, 2008), binary classification us-ing a bag-of-words assumption has been shown toprovide a strong baseline, so that is the approach weuse here.
We also evaluate the impact of lexicons andtokenization strategies.We define ?concerned?
messages to be thoseshowing some degree of apprehension, fear, or gen-eral concern regarding Hurricane Irene.
Examples ofunconcerned messages include links to news reportsor messages expressing an explicit lack of concern.The idea is to assess how seriously a particular groupis reacting to an impeding disaster.To train the classifier, we sample 408 messagesfrom the 66,205 message collection and manuallyannotate them as concerned or unconcerned.
The fi-nal training set contains 170 concerned messages.Examples are shown in Table 2.
To estimate inter-annotator agreement, we had a second annotatorsample 100 labeled messages (50 concerned, 50unconcerned) for re-annotation.
The inter-annotatoragreement is 93% (Cohen?s kappa ?
= .86).Examples of concerned messageswonderful, praying tht this hurricane goesback out to sea.Im actually scared for this hurricane...This hurricane is freaking me out.hope everyone is #safe during #ireneExamples of unconcerned messagesfor the very latest on hurricane irenelike our fb page ...am i the only one who doesn?t give ashit about this hurricane?
?tropical storm irene?s track threatenssouth florida - miamiherald.comTable 2: Examples of concerned and unconcerned mes-sages from the training set.Tokenization and featuresWe train a simple bag-of-words classifier, where thebasic feature set is the list of word frequencies ineach message.
Given the brevity and informalityof Twitter messages, tokenization choices can havea significant impact on classification accuracy.
Weconsider two alternatives:?
Tokenizer0: The tokenizer of O?Connor etal.
(2010b), which does very little normaliza-tion.
Punctuation is preserved (for the purposeof identifying semantics such as emoticons),URLs remain intact, and text is lower-cased.?
Tokenizer1: A simple tokenizer that removesall punctuation and converts to lowercase.We also consider two feature pruning options:?
Stop Words: Remove words matching a list of524 common English words.?
Frequency Pruning: Remove words occurringfewer than 2 times in the labeled data.We also consider the following features:?
Worry lexicon: We heuristically create a smalllexicon containing words expressing worry ofsome kind, based on a brief review of the data.3We replace all such tokens with a WORRIEDfeature.3The words are afraid, anxiety, cautious, die, died, nervous,pray, prayers, prayin, praying, safe, safety, scared, scary, terri-fied, thoughts, worried, worry, worrying30Classifier Acc Pr Re F1MaxEnt 84.27 ?
2.0 90.15 70.00 78.81Dec.
Tree 81.35 ?
1.8 79.72 67.06 72.84Naive Bayes 78.63 ?
2.2 75.78 71.76 73.72Worry Lex.
79.41 95.74 52.94 68.18Table 3: Average accuracy (with standard error) andmicro-averaged precision, recall, and F1 for the three sen-timent classifiers, using their best configurations.
The dif-ference in accuracy between MaxEnt and the other clas-sifiers is statistically significant (paired t-test, p < 0.01).?
Humor lexicon: Similarly, we create a smalllexicon containing words expressing humor.4We replace all such tokens with a HUMORfeature.?
Emoticon: Two common emoticons ?:)?
and?:(?
are detected (prior to tokenization in thecase of Tokenizer 1).Finally, we consider three classifiers: MaxEnt(i.e., logistic regression), Naive Bayes, and a De-cision Tree (ID3) classifier, as implemented inthe MALLET machine learning toolkit (McCallum,2002).
We use all the default settings, except we setthe maximum decision tree depth to 50 (after pre-liminary results suggested that the default size of 4was too small).Enumerating the possible tokenization, features,and classifier choices results in 192 possible sys-tem configurations.
For each configuration, 10-foldcross-validation is performed on the labeled trainingdata.
Table 3 reports the results for each classifierusing its best configuration.
The configuration To-kenizer1/Remove Stop Words/Freq.
Pruning/Worrylexicon/Humor lexicon/Emoticons was the best con-figuration for both MaxEnt and Naive Bayes.
Deci-sion Tree differed only in that its best configurationdid not use Frequency Pruning.
Table 3 also com-pares to a simple baseline that classifies messagesas concerned if they contain any of the words in theworry lexicon (while accuracy is competitive, recallis quite low).MaxEnt exhibits the best accuracy, precision, andF1; Naive Bayes has slightly better recall.
Table 4provides a summary of the numerical impact each4The words are lol, lmao, rofl, rotfl, ha, haha.System Configuration Avg Acc Max AccTokenizer0 77.78 81.10Tokenizer1 80.59 84.27Keep Stop Words 77.99 81.34Remove Stop Words 80.38 84.27No Freq.
Pruning 79.67 83.29Freq.
Pruning 78.71 84.27No Worry lexicon 77.62 81.82Worry lexicon 80.76 84.27No Humor Lexicon 79.15 83.78Humor Lexicon 79.23 84.27No Emoticons 79.26 84.27Emoticons 79.11 84.27Table 4: Summary of the impact of various tokenizationand feature choices.
The second and third columns list theaverage and maximum accuracy over all possible systemconfigurations with that setting.
All results use the Max-Ent classifier and 10-fold cross-validation.
Tokenizer1,Remove Stop Words, and Worry Lexicon result in thelargest improvements in accuracy.configuration choice has.
Using MaxEnt, we com-pute the accuracy over every possible system config-uration, then average the accuracies to obtain eachrow.
Thus, the Tokenizer1 row reports the averageaccuracy over all configurations that use Tokenizer1.Additionally, we report the highest accuracy of anyconfiguration using that setting.
These results in-dicate that Tokenizer1, Remove Stop Words, andWorry Lexicon result in the largest accuracy gains.Thus, while some unsupervised learning researchhas suggested that only light normalization shouldbe used for social media text analysis (O?Connor etal., 2010b), for this supervised learning task it ap-pears that more aggressive normalization and featurepruning can improve accuracy.We select the best performing MaxEnt classifierfor use in subsequent experiments.
First we retrainthe classifier on all the labeled data, then use itto label all of the unlabeled data from the original65,062 messages.
To estimate performance on thisnew data, we sample 200 additional documents ofthis testing data and manually label them (35 posi-tive, 165 negative).
We find that the automated clas-sifications are accurate in 86% of these documents.Many of the remaining errors appear to be diffi-cult cases.
For example, consider the message: ?1stan earthquake, now a hurricane?
Damn NY do you31miss me that bad??
The classifier labels this as con-cerned, but the message is likely intended to be hu-morous.
In another message (?#PrayForNYC andeveryone that will experience Hurricane Irene?
), ahashtag #PrayForNYC complicates tokenization, sothe word ?pray?
(highly correlated with concern) isnot detected, resulting in a false negative.Demographic AnalysisWe next apply this classifier to assess the demo-graphic determinants of concerned messages.
Byclassifying all remaining messages, we can analyzetrends in sentiment over time by gender and region.Figure 2 displays the total number of messagesby day as well as the subset (and percentage) thatare classified as concerned.
Consulting the timelinein Figure 1, we see that the peak volume occurs onAugust 27th, the day the eye of the hurricane makeslandfall.
The percentage of messages labeled as con-cerned actually peaks a day earlier, on August 26th.Geographic AnalysisWe first make several observations concerning Fig-ure 1, which does not use the sentiment classifier,but only displays message volume.
There appears tobe a regional difference in when message volumepeaks.
Data point C in the figure, which marks thetime around 2pm on August 23rd, represents the firstnoticeable spike in message count, particularly in theUpper South region.
Two important events were oc-curring around this time period.
First, the strongestearthquake to hit the Eastern United States sinceWWII (measured as 5.8 on the Richter scale) oc-curs near Richmond, Virginia.
Also on August 23rd,a few hours prior to the earthquake, FEMA holds apress conference regarding the impeding threat thatHurricane Irene will pose to East Coast states.
Itappears likely that the combination of these eventsleads to the increase in messages on August 23rdas revealed in the figure.
In fact, in examining someof the messages posted on Twitter during that timeperiod, we notice some people commenting on theunlikeliness that two natural disasters would hit theregion in such a narrow time frame.Also in Figure 1, we see that the frequency ofTwitter messages relating to Hurricane Irene foreach region increases greatly over roughly the pe-0?5?10?15?20?25?0?5000?10000?15000?20000?25000?Aug?21?Aug?22?Aug?23?Aug?24?Aug?25?Aug?26?Aug?27?Aug?28?Aug?29?Aug?30?Aug?31?%?concerned?#?messages?Irene?Messages?
Concerned?Irene?Messages?
%?Concerned?Figure 2: Total number of Twitter messages related toHurricane Irene, as well as the count and percentage clas-sified as concerned by the sentiment classifier.riod of August 25th to August 28th, before decreas-ing later on August 28th and beyond.
The increaseand decrease roughly parallel the approach of Hurri-cane Irene toward and then beyond each region.
Datapoint I represents the time (August 27th at 8am)when the center of Hurricane Irene makes landfallon the North Carolina coast.
This point representsthe highest message count for the Upper South re-gion.
Later on August 27th, as the hurricane movesnorth toward New Jersey and then New York, wesee the peak message count for the Middle Statesregion (Data point K).
Finally, on August 28th inthe late morning, as Hurricane Irene moves into theNew England region, we see that the New Englandregions peak message count occurs (Data Point L).With the sentiment classifier from the previoussection, we can perform a more detailed analysisof the regional differences than can be performedusing message volume alone.
Figure 3 applies thesentiment classifier to assess the proportion of mes-sages from each region that express concern.
Figure3 (top) shows the raw percentage of messages fromeach region by day, while the bottom figure showsthe proportion of messages from each region that ex-press concern.
While the New England region hasthe lowest volume of messages, on many days it hasthe highest proportion of concerned messages.Comparing regional differences in aggregateacross all 10 days would be misleading ?
after thehurricane passes a region, it is expected that the levelof concern should decrease.
Indeed, these aggregateregional differences are not statistically significant(NE=15.59%, MID=15.4%, SOUTH=15.69%).
In-stead, for each day we compare the levels of concern320?2?4?6?8?10?12?14?16?Aug?21?Aug?22?Aug?23?Aug?24?Aug?25?Aug?26?Aug?27?Aug?28?Aug?29?Aug?30?Aug?31?%?messages?NE?MID?SOUTH?0?5?10?15?20?25?30?Aug?22?Aug?23?Aug?24?Aug?25?Aug?26?Aug?27?Aug?28?Aug?29?Aug?30?%?concerned?NE?MID?SOUTH?Figure 3: Message proportion and percent classified asconcerned by the sentiment classifier, by region.for each region, testing for significance using a Chi-squared test.
Two days show significant differences:August 25 and August 27.
On both days, the propor-tion of concerned messages in New England is sig-nificantly higher (p < 0.05) than that of the South-ern region (August 25: NE=21.6%, SOUTH=14.6%;August 26: NE=18.5%, SOUTH=15.1%).
It is diffi-cult to directly attribute causes to these differences,although on August 25, a Hurricane Watch was is-sued for the New England area, and on August 27that Watch was upgraded to a Warning.
It is alsopossible that states that experience hurricanes morefrequently express lower levels of concern.
Furthersociological research is necessary to fully addressthese differences.Gender AnalysisWe apply a similar analysis to assess the differ-ences in levels of concern by gender.
Figure 4 showsthat for roughly the period between August 24thand August 29th, messages written by females aremore likely to express concern than those writtenby males.
Over the entire period, 18.7% of female-authored messages are labeled as concerned, whileover the same period 13.9% of male-authored mes-sages are labeled as concerned.
We perform a Chi-0?5?10?15?20?25?30?Aug?21?Aug?22?Aug?23?Aug?24?Aug?25?Aug?26?Aug?27?Aug?28?Aug?29?Aug?30?Aug?31?%?messages?male?female?0?5?10?15?20?25?Aug?21?Aug?22?Aug?23?Aug?24?Aug?25?Aug?26?Aug?27?Aug?28?Aug?29?Aug?30?Aug?31?%?concerned?
male?female?Figure 4: Message proportion and percent classified asconcerned by the sentiment classifier, by gender.squared test over the entire period, and find that gen-der differences in concern are significant (p < .01).We conclude that messages attributed to female au-thors are significantly more likely to be classified asconcerned than messages authored by males.In order to assess a possible gender bias in ourclassifier, we examine the proportion of concern formales and females in the labeled training set.
Wefind that of the original 408 labeled messages, 69are from males, 112 are from females, and 227 can-not be determined.
24 male messages, or 34.8%, aremarked as concerned.
In contrast, 57 female mes-sages, or 50.9%, are marked as concerned.
88 of theundetermined gender messages, or 38.9%, are con-cerned.
We therefore down-sample the female mes-sages from our labeled training set until the propor-tion of female-concerned messages matches that ofmale-concerned messages.
Repeating our classifica-tion experiments shows no significant difference inthe relative proportions of messages labeled as con-cerned by gender.
We therefore conclude that thetraining set is not injecting a gender bias in the clas-sifier.33Female: i my safe praying this everyone died jadabutistillloveu brenda who love t me thank school petsretweet respects all please here so stayneverapologizefor wine sleep rainbow prayers lordMale: http co de en el hurac media breaking larooftoproofing track obama jimnorton gay ron blamessmem change seattle orkaan becomes disaster zona zanlean vivo por es location dolphinNew England: boston MAirene ct vt ri england sundayconnecticut malloy ma vermont tropical maine wtnhmassachusetts haven rhode VTirene va powerCThurricane cambridge mass lls gilsimmonsmbta gunna storm slut NHireneMiddle States: nyc ny nj nycmayorsoffice york jerseymta brooklyn zone nytmetro va ryan philly shutdc mayor city manhattan lls new subways conteam longisland bloomberg evacuation evacuateyorkers catskills queensSouth: nc dc va lls earthquake raleigh marylanddmv ncwx virginia ncirene richmond isabelle perdueisabel mdhurricane bout carolina capitalweather sniperrva norfolk goin feeds nycmayorsoffice baltimore ilmmema tho aintTable 5: Top 30 words for each demographic ranked byInformation Gain.Qualitative AnalysisIn Table 5 we provide a brief qualitative analy-sis by displaying the top 30 words for each demo-graphic obtained using Information Gain (Manningand Schtze, 1999), a method of detecting featuresthat discriminate between document classes.
To pro-vide some of the missing context: ?jada?
refers tothe divorce of celebrities Will Smith and Jada Pin-kett; ?hurac?
refers to the Spanish word Huraca?n;?smem?
stands for Social Media for EmergencyManagement; ?dolphin?
refers to a joke that was cir-culated referencing the hurricane; ?lls?
is an abbre-viation for ?laughing like shit?.Some broad trends appear: male users tend to ref-erence news, politics, or jokes; the Middle Statesreference the evacuation of New York City, and theSouth refers back to other disasters (the earthquake,the sniper attacks of 2002, Hurricane Isabel).Related WorkRecent research has investigated the effectiveness ofsocial media for crisis communication (Savelyev etal., 2011) ?
indeed, the U.S. Federal EmergencyManagement Agency now uses Twitter to dissem-inate information during natural disasters (Kalish,2011).
Other work has examined the spread offalse rumors during earthquakes (Mendoza et al,2010) and tsunamis (Acar and Muraki, 2011) andcharacterized social network dynamics during floods(Cheong and Cheong, 2011), fires (Vieweg et al,2010), and violence (Heverin and Zach, 2010).While some of this past research organizes messagesby topic, to our knowledge no work has analyzeddisaster sentiment or its demographic determinants.Survey research by West and Orr (2007) con-cluded that women may feel more vulnerable dur-ing hurricanes because they are more likely to havechildren and belong to a lower socio-economic class.Richer people, they find, tend to have an easier timedealing with natural disasters like hurricanes.
Thesereasons might explain our finding that women aremore likely on Twitter to show concern than menabout Hurricane Irene.
West and Orr also find dif-ferences in regional perceptions of vulnerability be-tween coastal areas and non-coastal areas.
Our loca-tion annotation must be more precise before we canperform a similar analysis.More generally, our approach can be considereda type of computational social science, an emergingarea of study applying computer science algorithmsto social science research (Lazer et al, 2009; Hop-kins and King, 2010).Conclusion and Future WorkOur results show that analyzing Twitter messagesrelating to Hurricane Irene reveals differences insentiment depending on a person?s gender or loca-tion.
We conclude that social media analysis is a vi-able complement to existing survey methodologies,providing real-time insight into public perceptionsof a disaster.
Future directions include investigatinghow to account for classifier error in hypothesis test-ing (Fuller, 1987), adjusting classification propor-tions using quantification methods (Forman, 2007),as well as applying the approach to different disas-ters and identifying additional sentiment classes ofinterest.
Finally, it will be important to infer a greatervariety of demographic attributes and also to adjustfor the demographic bias inherent in social media.34ReferencesAdam Acar and Yuya Muraki.
2011.
Twitter for crisiscommunication: lessons learned from Japan?s tsunamidisaster.
International Journal of Web Based Commu-nities, 7(3):392?402.S.
Asur and B.
A. Huberman.
2010.
Predicting the futurewith social media.
In Proceedings of the ACM Inter-national Conference on Web Intelligence.France Cheong and Christopher Cheong.
2011.
So-cial media data mining: A social network analysis oftweets during the 2010?2011 Australian floods.
InPACIS 2011 Proceedings.Aron Culotta.
2010.
Towards detecting influenza epi-demics by analyzing Twitter messages.
In Workshopon Social Media Analytics at the 16th ACM SIGKDDConference on Knowledge Discovery and Data Min-ing.Aron Culotta.
2012.
Lightweight methods to estimateinfluenza rates and alcohol sales volume from Twittermessages.
Language Resources and Evaluation, Spe-cial Issue on Analysis of Short Texts on the Web.
toappear.Elaine Enarson.
1998.
Through women?s eyes: A gen-dered research agenda for disaster social science.
Dis-asters, 22(2):157?73.George Forman.
2007.
Quantifying counts, costs, andtrends accurately via machine learning.
Technical re-port, HP Laboratories, Palo Alto, CA.A.
Fothergill, E.G.
Maestas, and J.D.
Darlington.
1999.Race, ethnicity and disasters in the united states: A re-view of the literature.
Disasters, 23(2):156?73, Jun.W.A.
Fuller.
1987.
Measurement error models.
Wiley,New York.Eric Gilbert and Karrie Karahalios.
2010.
Widespreadworry and the stock market.
In Proceedings of the 4thInternational AAAI Conference on Weblogs and SocialMedia, Washington, D.C., May.J.D.
Goltz, L.A. Russell, and L.B.
Bourque.
1992.
Initialbehavioral response to a rapid onset disaster: A casestudy.
International Journal of Mass Emergencies andDisasters, 10(1):43?69.Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.2011.
Tweets from justin bieber?s heart: the dynamicsof the location field in user profiles.
In Proceedings ofthe 2011 annual conference on Human factors in com-puting systems, CHI ?11, pages 237?246, New York,NY, USA.T.
Heverin and L. Zach.
2010.
Microblogging forcrisis communication: Examination of Twitter use inresponse to a 2009 violent crisis in Seattle-Tacoma,Washington area.
In Proceedings of the Seventh Inter-national Information Systems for Crisis Response andManagement Conference, Seattle, WA.Daniel J. Hopkins and Gary King.
2010.
A methodof automated nonparametric content analysis for so-cial science.
American Journal of Political Science,54(1):229?247.Brian Kalish.
2011.
FEMA will use social mediathrough all stages of a disaster.
Next Gov, February.Vasileios Lampos and Nello Cristianini.
2010.
Trackingthe flu pandemic by monitoring the social web.
In 2ndIAPR Workshop on Cognitive Information Processing(CIP 2010), pages 411?416.David Lazer, Alex Pentland, Lada Adamic, SinanAral, Albert-Laszlo Barabasi, Devon Brewer, NicholasChristakis, Noshir Contractor, James Fowler, MyronGutmann, Tony Jebara, Gary King, Michael Macy,Deb Roy, and Marshall Van Alstyne.
2009.
Computa-tional social science.
Science, 323(5915):721?723.Chris Manning and Hinrich Schtze.
1999.
Founda-tions of Statistical Natural Language Processing.
MITPress, Cambridge, MA, May.Andrew Kachites McCallum.
2002.
MAL-LET: A machine learning for language toolkit.http://mallet.cs.umass.edu.Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.2010.
Twitter under crisis: Can we trust what we RT?In 1st Workshop on Social Media Analytics (SOMA?10), July.Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka Onnela, , and J. Niels Rosenquist.
2011.
Un-derstanding the demographics of twitter users.
InProceedings of the Fifth International AAAI Con-ference on Weblogs and Social Media (ICWSM?11),Barcelona, Spain.James Murphy, P. Allen, Thomas Stevens, and DarrylWeatherhead.
2005.
A meta-analysis of hypotheticalbias in stated preference valuation.
Environmental andResource Economics, 30(3):313?325.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010a.From Tweets to polls: Linking text sentiment to publicopinion time series.
In International AAAI Conferenceon Weblogs and Social Media, Washington, D.C.Brendan O?Connor, Michel Krieger, and David Ahn.2010b.
Tweetmotif: Exploratory search and topic sum-marization for twitter.
In ICWSM.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1?2):1?135.R.W.
Perry and M.K.
Lindell.
1991.
The effects of eth-nicity on decision-making.
International journal ofmass emergencies and disasters, 9(1):47?68.R.W.
Perry and A.H. Mushkatel.
1986.
Minority citizensin disasters.
University of Georgia Press, Athens, GA.35Alexander Savelyev, Justine Blanford, and Prasenjit Mi-tra.
2011.
Geo-twitter analytics: Applications in cri-sis management.
In 25th International CartographicConference, pages 1?8.Sarah Vieweg, Amanda L. Hughes, Kate Starbird, andLeysia Palen.
2010.
Microblogging during two nat-ural hazards events: what twitter may contribute to sit-uational awareness.
In Proceedings of the 28th inter-national conference on Human factors in computingsystems, pages 1079?1088, New York, NY, USA.Darrell M. West and Marion Orr.
2007.
Race, gender,and communications in natural disasters.
The PolicyStudies Journal, 35(4).Appendix A: Location String MatchingThe following strings were matched against the userlocation field of each message to determine the loca-tion of the message.
Matches were case insensitive,except for abbreviations (e.g., VT must be capital-ized to match).Vermont, VT, Maine, ME, New Hampshire,Rhode Island, RI, Delaware, DE, Connecticut, CT,Maryland, MD, Baltimore, North Carolina, NC,Massachusetts, MA, Boston, Mass, W Virginia,West Virginia, Virginia, VA, RVA, DC, D.C., PA,Philadelphia, Pittsburgh, Philly, New Jersey, At-lantic City, New York, NY, NYC, Long Island, Man-hattan, Brooklyn, Staten Island, The Bronx, Queens,NY, N.Y.36
