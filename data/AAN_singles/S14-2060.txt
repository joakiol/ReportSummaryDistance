Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356?360,Dublin, Ireland, August 23-24, 2014.IUCL: Combining Information Sources for SemEval Task 5Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K?ublerIndiana UniversityBloomington, IN, USA{alexr,leviking,liucan,md7,skuebler}@indiana.eduAbstractWe describe the Indiana University sys-tem for SemEval Task 5, the L2 writ-ing assistant task, as well as some exten-sions to the system that were completedafter the main evaluation.
Our team sub-mitted translations for all four languagepairs in the evaluation, yielding the topscores for English-German.
The systemis based on combining several informationsources to arrive at a final L2 translationfor a given L1 text fragment, incorporatingphrase tables extracted from bitexts, an L2language model, a multilingual dictionary,and dependency-based collocational mod-els derived from large samples of target-language text.1 IntroductionIn the L2 writing assistant task, we must translatean L1 fragment in the midst of an existing, nearlycomplete, L2 sentence.
With the presence of thisrich target-language context, the task is rather dif-ferent from a standard machine translation setting,and our goal with our design was to make effec-tive use of the L2 context, exploiting collocationalrelationships between tokens anywhere in the L2context and the proposed fragment translations.Our system proceeds in several stages: (1) look-ing up or constructing candidate translations forthe L1 fragment, (2) scoring candidate transla-tions via a language model of the L2, (3) scoringcandidate translations with a dependency-drivenword similarity measure (Lin, 1998) (which wecall SIM), and (4) combining the previous scoresin a log-linear model to arrive at a final n-bestlist.
Step 1 models transfer knowledge betweenThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/the L1 and L2; step 2 models facts about the L2syntax, i.e., which translations fit well into the lo-cal context; step 3 models collocational and se-mantic tendencies of the L2; and step 4 gives dif-ferent weights to each of the three sources of in-formation.
Although we did not finish step 3 intime for the official results, we discuss it here, asit represents the most novel aspect of the system ?namely, steps towards the exploitation of the richL2 context.
In general, our approach is language-independent, with accuracy varying due to the sizeof data sources and quality of input technology(e.g., syntactic parse accuracy).
More featurescould easily be added to the log-linear model, andfurther explorations of ways to make use of target-language knowledge could be promising.2 Data SourcesThe data sources serve two major purposes for oursystem: For L2 candidate generation, we use Eu-roparl and BabelNet; and for candidate rankingbased on L2 context, we use Wikipedia and theGoogle Books Syntactic N-grams.Europarl The Europarl Parallel Corpus (Eu-roparl, v7) (Koehn, 2005) is a corpus of pro-ceedings of the European Parliament, contain-ing 21 European languages with sentence align-ments.
From this corpus, we build phrase tablesfor English-Spanish, English-German, French-English, Dutch-English.BabelNet In the cases where the constructedphrase tables do not contain a translation for asource phrase, we need to back off to smallerphrases and find candidate translations for thesecomponents.
To better handle sparsity, we extendlook-up using the multilingual dictionary Babel-Net, v2.0 (Navigli and Ponzetto, 2012) as a way tofind translation candidates.356Wikipedia For German and Spanish, we use re-cent Wikipedia dumps, which were converted toplain text with the Wikipedia Extractor tool.1Tosave time during parsing, sentences longer than 25words are removed.
The remaining sentences arePOS-tagged and dependency parsed using MateParser with its pre-trained models (Bohnet, 2010;Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013).To keep our English Wikipedia dataset to a man-ageable size, we choose an older (2006), smallerdump.
Long sentences are removed, and the re-maining sentences are POS-tagged and depen-dency parsed using the pre-trained Stanford Parser(Klein and Manning, 2003; de Marneffe et al.,2006).
The resulting sizes of the datasets are(roughly): German: 389M words, 28M sentences;Spanish: 147M words, 12M sentences; English:253M words, 15M sentences.
Dependencies ex-tracted from these parsed datasets serve as trainingfor the SIM system described in section 3.3.Google Books Syntactic N-grams For English,we also obtained dependency relationships for ourword similarity statistics using the arcs dataset ofthe Google Books Syntactic N-Grams (Goldbergand Orwant, 2013), which has 919M items, eachof which is a small ?syntactic n-gram?, a termGoldberg and Orwant use to describe short de-pendency chains, each of which may contain sev-eral tokens.
This data set does not contain the ac-tual parses of books from the Google Books cor-pus, but counts of these dependency chains.
Weconverted the longer chains into their component(head, dependent, label) triples and then collatedthese triples into counts, also for use in the SIMsystem.3 System DesignAs previously mentioned, at run-time, our systemdecomposes the fragment translation task into twoparts: generating many possible candidate transla-tions, then scoring and ranking them in the target-language context.3.1 Constructing Candidate TranslationsAs a starting point, we use phrase tables con-structed in typical SMT fashion, built with thetraining scripts packaged with Moses (Koehn etal., 2007).
These scripts preprocess the bitext, es-timate word alignments with GIZA++ (Och and1http://medialab.di.unipi.it/wiki/Wikipedia_ExtractorNey, 2000) and then extract phrases with thegrow-diag-final-and heuristic.At translation time, we look for the givensource-language phrase in the phrase table, and ifit is found, we take all translations of that phraseas our candidates.When translating a phrase that is not found inthe phrase table, we try to construct a ?syntheticphrase?
out of the available components.
Thisis done by listing, combinatorially, all ways todecompose the L1 phrase into sub-phrases of atleast one token long.
Then for each decomposi-tion of the input phrase, such that all of its compo-nents can be found in the phrase table, we gen-erate a translation by concatenating their target-language sides.
This approach naively assumesthat generating valid L2 text requires no reorder-ing of the components.
Also, since there are 2n?1possible ways to split an n-token phrase into sub-sequences (i.e., each token is either the first tokenin a new sub-sequence, or it is not), we performsome heuristic pruning at this step, taking onlythe first 100 decompositions, preferring those builtfrom longer phrase-table entries.
Every phrase inthe phrase table, including these synthetic phrases,has both a ?direct?
and ?inverse?
probability score;for synthetic phrases, we estimate these scores bytaking the product of the corresponding probabili-ties for the individual components.In the case that an individual word cannot befound in the phrase table, the system attempts tolook up the word in BabelNet, estimating the prob-abilities as uniformly distributed over the availableBabelNet entries.
Thus, synthetic phrase tableentries can be constructed by combining phrasesfound in the training data and words available inBabelNet.For the evaluation, in cases where an L1 phrasecontained words that were neither in our train-ing data nor BabelNet (and thus were simply out-of-vocabulary for our system), we took the firsttranslation for that phrase, without regard to con-text, from Google Translate, through the semi-automated Google Docs interface.
This approachis not particularly scalable or reproducible, butsimulates what a user might do in such a situation.3.2 Scoring Candidate Translations via a L2Language ModelTo model how well a phrase fits into the L2 con-text, we score candidates with an n-gram lan-357guage model (LM) trained on a large sample oftarget-language text.
Constructing and queryinga large language model is potentially computa-tionally expensive, so here we use the KenLMLanguage Model Toolkit and its Python interface(Heafield, 2011).
Here our models were trainedon the Wikipedia text mentioned previously (with-out filtering long sentences), with KenLM set to5-grams and the default settings.3.3 Scoring Candidate Translations viaDependency-Based Word SimilarityThe candidate ranking based on the n-gram lan-guage model ?
while quite useful ?
is based onvery shallow information.
We can also rank thecandidate phrases based on how well each of thecomponents fits into the L2 context using syntacticinformation.
In this case, the fitness is measured interms of dependency-based word similarity com-puted from dependency triples consisting of thethe head, the dependent, and the dependency la-bel.
We slightly adapted the word similarity mea-sure by Lin (1998):SIM(w1, w2) =2 ?
c(h, d, l)c(h,?, l) + c(?, d, l)(1)where h = w1and d = w2and c(h, d, l)is the frequency with which a particular(head, dependent, label) dependency tripleoccurs in the L2 corpus.
c(h,?, l) is the fre-quency with which a word occurs as a headin a dependency labeled l with any dependent.c(?, d, l) is the frequency with which a wordoccurs as a dependent in a dependency labeledl with any head.
In the measure by Lin (1998),the numerator is defined as the information of alldependency features that w1and w2share, com-puted as the negative sum of the log probability ofeach dependency feature.
Similarly, the denom-inator is computed as the sum of information ofdependency features for w1and w2.To compute the fitness of a word wifor itscontext, we consider a set D of all words that aredirectly dependency-related to wi.
The fitness ofwiis thus computed as:FIT (wi) =?DwjSIM(wi, wj)|D|(2)The fitness of a phrase is the average word sim-ilarity over all its components.
For example, thefitness of the phrase ?eat with chopsticks?
wouldbe computed as:FIT (eat with chopsticks) =FIT (eat) + FIT (with) + FIT (chopsticks)3(3)Since we consider the heads and dependentsof a target phrase component, these may be situ-ated inside or outside the phrase.
Both cases areincluded in our calculation, thus enabling us toconsider a broader, syntactically determined localcontext of the phrase.
By basing the calculation ona single word?s head and dependents, we attemptto avoid data sparseness issues that we might getfrom rare n-gram contexts.Back-Off Lexical-based dependency triples suf-fer from data sparsity, so in addition to computingthe lexical fitness of a phrase, we also calculate thePOS fitness.
For example, the POS fitness of ?eatwith chopsticks?
would be computed as follows:FIT (eat/VBG with/IN chopsticks/NNS) =FIT (VBG) + FIT (IN) + FIT (NNS)3(4)Storing and Caching The large vocabularyand huge number of combinations of our(head, dependent, label) triples poses an effi-ciency problem when querying the dependency-based word similarity values.
Thus, we storedthe dependency triples in a database with aPython programming interface (SQLite3) andbuilt database indices on the frequent query types.However, for frequently searched dependencytriples, re-querying the database is still inefficient.Thus, we built a query cache to store the recently-queried triples.
Using the database and cache sig-nificantly speeds up our system.This database only stores dependency triplesand their corresponding counts; the dependency-based similarity value is calculated as needed, foreach particular context.
Then, these FIT scoresare combined with the scores from the phrase ta-ble and language model, using weights tuned byMERT.358system acc wordacc oofacc oofwordaccrun2 0.665 0.722 0.806 0.857SIM 0.647 0.706 0.800 0.852nb 0.657 0.717 0.834 0.868Figure 1: Scores on the test set for English-German; here next-best is CNRC-run1.system acc wordacc oofacc oofwordaccrun2 0.633 0.72 0.781 0.847SIM 0.359 0.482 0.462 0.607best 0.755 0.827 0.920 0.944Figure 2: Scores on the test set for English-Spanish; here best is UEdin-run2.3.4 Tuning Weights with MERTIn order to rank the various candidate translations,we must combine the different sources of infor-mation in some way.
Here we use a familiar log-linear model, taking the log of each score ?
the di-rect and inverse translation probabilities, the LMprobability, and the surface and POS SIM scores ?and producing a weighted sum.
Since the originalscores are either probabilities or probability-like(in the range [0, 1]), their logs are negative num-bers, and at translation time we return the trans-lation (or n-best) with the highest (least negative)score.This leaves us with the question of how toset the weights for the log-linear model; in thiswork, we use the ZMERT package (Zaidan, 2009),which implements the MERT optimization algo-rithm (Och, 2003), iteratively tuning the featureweights by repeatedly requesting n-best lists fromthe system.
We used ZMERT with its defaultsettings, optimizing our system?s BLEU scoreson the provided development set.
We chose, forconvenience, BLEU as a stand-in for the word-level accuracy score, as BLEU scores are maxi-mized when the system output matches the refer-ence translations.4 ExperimentsIn figures 1-4, we show the scores on this year?stest set for running the two variations of our sys-tem: run2, the version without the SIM exten-sions, which we submitted for the evaluation, andSIM, with the extensions enabled.
For compar-ison, we also include the best (or for English-German, next-best) submitted system.
We see heresystem acc wordacc oofacc oofwordaccrun2 0.545 0.682 0.691 0.800SIM 0.549 0.687 0.693 0.800best 0.733 0.824 0.905 0.938Figure 3: Scores on the test set for French-English;here best is UEdin-run1.system acc wordacc oofacc oofwordaccrun2 0.544 0.679 0.634 0.753SIM 0.540 0.676 0.635 0.753best 0.575 0.692 0.733 0.811Figure 4: Scores on the test set for Dutch-English;here best is UEdin-run1.that the use of the SIM features did not improvethe performance of the base system, and in thecase of English-Spanish caused significant degra-dation, which is as of yet unexplained, though wesuspect difficulties parsing the Spanish test set, asfor all of the other language pairs, the effects ofadding SIM features were small.5 ConclusionWe have described our entry for the initial run-ning of the ?L2 Writing Assistant?
task and ex-plained some possible extensions to our base log-linear model system.In developing the SIM extensions, we facedsome interesting software engineering challenges,and we can now produce large databases of depen-dency relationship counts for various languages.Unfortunately, these extensions have not yet ledto improvements in performance on this particu-lar task.
The databases themselves seem at leastintuitively promising, capturing interesting infor-mation about common usage patterns of the tar-get language.
Finding a good way to make useof this information may involve computing somemeasure that we have not yet considered, or per-haps the insights captured by SIM are covered ef-fectively by the language model.We look forward to future developments aroundthis task and associated applications in helpinglanguage learners communicate effectively.359ReferencesBernd Bohnet and Jonas Kuhn.
2012.
The best ofboth worlds ?
A graph-based completion model fortransition-based parsers.
In Proceedings of the 13thConference of the European Chapter of the Associ-ation for Computational Linguistics (EACL), pages77?87, Avignon, France.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (COLING), pages 89?97, Beijing,China.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
InProceedings of LREC-06, Genoa, Italy.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large cor-pus of English books.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),pages 241?247, Atlanta, GA.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation, pages 187?197, Edinburgh, Scot-land, United Kingdom, July.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL-2003,pages 423?430, Sapporo, Japan.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, Prague, Czech Republic.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In International Conference onMachine Learning (ICML), volume 98, pages 296?304.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 440?447, HongKong.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.Wolfgang Seeker and Jonas Kuhn.
2013.
Morphologi-cal and syntactic case in statistical dependency pars-ing.
Computational Linguistics, 39(1):23?55.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.360
