Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 145?148,Prague, June 2007. c?2007 Association for Computational LinguisticsFBK-irst: Lexical Substitution Task ExploitingDomain and Syntagmatic CoherenceClaudio Giuliano and Alfio Gliozzo and Carlo StrapparavaFBK-irst, I-38050, Povo, Trento, ITALY{giuliano, gliozzo, strappa}@itc.itAbstractThis paper summarizes FBK-irst participa-tion at the lexical substitution task of theSEMEVAL competition.
We submitted twodifferent systems, both exploiting synonymlists extracted from dictionaries.
For eachword to be substituted, the systems rank theassociated synonym list according to a simi-larity metric based on Latent Semantic Anal-ysis and to the occurrences in the Web 1T5-gram corpus, respectively.
In particular,the latter system achieves the state-of-the-artperformance, largely surpassing the baselineproposed by the organizers.1 IntroductionThe lexical substitution (Glickman et al, 2006a) canbe regarded as a subtask of the lexical entailment,in which for a given word in context the system isasked to select an alternative word that can be re-placed in that context preserving the meaning.
Lex-ical Entailment, and in particular lexical reference(Glickman et al, 2006b)1 , is in turn a subtask of tex-tual entailment, which is formally defined as a rela-tionship between a coherent text T and a languageexpression, the hypothesis H .
T is said to entail H ,denoted by T ?
H , if the meaning of H can be in-ferred from the meaning of T (Dagan et al, 2005;Dagan and Glickman., 2004).
Even though this no-tion has been only recently proposed in the computa-tional linguistics literature, it attracts more and moreattention due to the high generality of its settings andto the usefulness of its (potential) applications.1In the literature, slight variations of this problem have beenalso referred to as sense matching (Dagan et al, 2006).With respect to lexical entailment, the lexical sub-stitution task has a more restrictive criterion.
Infact, two words can be substituted when meaning ispreserved, while the criterion for lexical entailmentis that the meaning of the thesis is implied by themeaning of the hypothesis.
The latter condition is ingeneral ensured by substituting either hyperonymsor synonyms, while the former is more rigid becauseonly synonyms are in principle accepted.Formally, in a lexical entailment task a system isasked to decide whether the substitution of a par-ticular term w with the term e in a coherent textHw = H lwHr generates a sentence He = H leHrsuch that Hw ?
He, where H l and Hr denote theleft and the right context of w, respectively.
Forexample, given the source word ?weapon?
a systemmay substitute it with the target synonym ?arm?, inorder to identify relevant texts that denote the soughtconcept using the latter term.A particular case of lexical entailment is recog-nizing synonymy, where both Hw ?
He and He ?Hw hold.
The lexical substitution task at SEMEVALaddresses exactly this problem.
The task is not easysince lists of candidate entailed words are not pro-vided by the organizers.
Therefore the system isasked first to identify a set of candidate words, andthen to select only those words that fit in a particu-lar context.
To promote unsupervised methods, theorganizers did not provide neither labeled data fortraining nor dictionaries or list of synonyms explain-ing the meanings of the entailing words.In this paper, we describe our approach to theLexical Substitution task at SEMEVAL 2007.
Wedeveloped two different systems (named IRST1-lsaand IRST2-syn in the official task ranking), both ex-ploiting a common lists of synonyms extracted fromdictionaries (i.e.
WordNet and the Oxford Dictio-145nary) and ranking them according to two differentcriteria:Domain Proximity: the similarity between eachcandidate entailed word and the context of theentailing word is estimated by means of a co-sine between their corresponding vectors in theLSA space.Syntagmatic Coherence: querying a large corpus,the system finds all occurrences of the targetsentence, in which the entailing word is substi-tuted with each synonym, and it assigns scoresproportional to the occurrence frequencies.Results show that both methods are effective.
Inparticular, the second method achieved the best per-formance in the competition, defining the state-of-the-art for the lexical substitution task.2 Lexical Substitution SystemsThe lexical substitution task is a textual entailmentsubtask in which the system is asked to provide oneor more terms e ?
E ?
syn(w) that can be sub-stituted to w in a particular context Hw = H lwHrgenerating a sentence He = H leHr such that bothHw ?
He and He ?
Hw hold, where syn(w) is theset of synonyms lemmata obtained from all synset inwhich w appears in WordNet and H l and Hr denotethe left and the right context of w, respectively.The first step, common to both systems, consistsof determining the set of synonyms syn(w) for eachentailing word (see Section 2.1).
Then, each systemranks the extracted lists according to the criteria de-scribed in Section 2.2 and 2.3.2.1 Used Lexical ResourcesFor selecting the synonym candidates we used twolexical repositories: WordNet 2.0 and the OxfordAmerican Writer Thesaurus (1st Edition).
For eachtarget word, we simply collect all the synonyms forall the word senses in both these resources.We exploited two corpora for our systems: theBritish National Corpus for acquiring the LSA spacefor ranking with domain proximity measure (Sec-tion 2.2) and the Web 1T 5-gram Version 1 corpusfrom Google (distributed by Linguistic Data Consor-tium)2 for ranking the proposed synonyms accord-ing to syntagmatic coherence (Section 2.3).2Available from http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13.No other resources were used and the sense rank-ing in WordNet was not considered at all.
Thereforeour system is fully unsupervised.2.2 Domain ProximitySemantic Domains are common areas of human dis-cussion, such as Economics, Politics, Law (Magniniet al, 2002).
Semantic Domains can be describedby DMs (Gliozzo, 2005), by defining a set of termclusters, each representing a Semantic Domain, i.e.a set of terms having similar topics.
A DM is repre-sented by a k ?
k?
rectangular matrix D, containingthe domain relevance for each term with respect toeach domain.DMs can be acquired from texts by exploitingterm clustering algorithms.
The degree of associ-ation among terms and clusters, estimated by thelearning algorithm, provides a domain relevancefunction.
For our experiments we adopted a clus-tering strategy based on Latent Semantic Analy-sis (LSA) (Deerwester et al, 1990), following themethodology described in (Gliozzo, 2005).The input of the LSA process is a Term by Docu-ment matrix T of the frequencies in the whole cor-pus for each term.
In this work we indexed all lem-matized terms.
The so obtained matrix is then de-composed by means of a Singular Value Decompo-sition, identifying the principal components of T.Once a DM has been defined by the matrix D, theDomain Space is a k?
dimensional space, in whichboth texts and terms are associated to Domain Vec-tors (DVs), i.e.
vectors representing their domainrelevance with respect to each domain.
The DV ~t?ifor the term ti ?
V is the ith row of D, whereV = {t1, t2, .
.
.
, tk} is the vocabulary of the cor-pus.
The DVs for texts are obtained by mapping thedocument vectors ~dj , represented in the vector spacemodel, into the vectors ~d?j in the Domain Space, de-fined byD(~dj) = ~dj(IIDFD) = ~d?j (1)where IIDF is a diagonal matrix such that iIDFi,i =IDF (wi) and IDF (wi) is the Inverse DocumentFrequency of wi.
The similarity among both textsand terms in the Domain Space is then estimated bythe cosine operation.To implement our lexical substitution criterion weranked the candidate entailed words according totheir domain proximity, following the intuition thatif two words can be substituted in a particular con-text, then the entailed word should belong to the146same semantic domain of the context in which theentailing word is located.The intuition above can be modeled by estimatingthe similarity in the LSA space between the pseudodocument, estimated by Equation 1, formed by allthe words in the context of the entailing word (i.e.the union of H l and Hr), and each candidate en-tailed word in syn(w).2.3 Syntagmatic CoherenceThe syntagmatic coherence criterion is based on thefollowing observation.
If the entailing word w inits context Hw = H lwHr is actually entailed bya word e, then there exist some occurrences on theWEB of the expression He = H leHr, obtainedby replacing the entailing word with the candidateentailed word.
This intuition can be easily imple-mented by looking for occurrences of He in the Web1T 5-gram Version 1 corpus.Figure 1 presents pseudo-code for the synonymscoring procedure.
The procedure takes as input theset of candidate entailed words E = syn(w) for theentailing word w, the context Hw in which w oc-curs, the length of the n-gram (2 6 n 6 5) and thetarget word itself.
For each candidate entailed wordei, the procedure ngrams(Hw, w, ei, n) is invokedto substitute w with ei in Hw, obtaining Hei , and re-turns the set Q of all n-grams containing ei.
For ex-ample, all 3-grams obtained replacing ?bright?
withthe synonym ?intelligent?
in the sentence ?He wasbright and independent and proud.?
are ?He was in-telligent?, ?was intelligent and?
and ?intelligent andindependent?.
The maximum number of n-gramsgenerated is ?5n=2 n. Each candidate synonym isthen assigned a score by summing all the frequen-cies in the Web 1T corpus of the so generated n-grams3.
The set of synonyms is ranked accordingthe so obtained scores.
However, candidates whichappear in longer n-grams are preferred to candidatesappearing in shorter ones.
Therefore, the ranked listcontains first the candidate entailed words appearingin 5-grams, if any, then those appearing in 4-grams,and so on.
For example, a candidate e1 that appearsonly once in 5-grams is preferred to a candidate e2that appears 1000 times in 4-grams.
Note that thisstrategy could lead to an output list with repetitions.3Note that n-grams with frequency lower than 40 are notpresent in the corpus.1: Given E, the set of candidate synonyms2: Given H , the context in which w occurs3: Given n, the length of the n-gram4: Given w, the word to be substituted5: E?
?
?6: for each ei in E do7: Q?
ngrams(H,w, ei, n)8: scorei ?
09: for each qj in Q do10: Get the frequency fj of qj11: scorei ?
scorei + fj12: end for13: if scorei > 0 then add the pair {scorei, ei}in E?14: end for15: Return E?Figure 1: The synonym scoring procedure3 EvaluationThere are basically two scoring methodologies: (i)BEST, which scores the best substitute for a givenitem, and (ii) OOT, which scores for the best 10 sub-stitutes for a given item, and systems do not benefitfrom providing less responses4 .BEST.
Table 1 and 2 report the performance for thedomain proximity and syntagmatic coherence rank-ing.
Please note that in Table 2 we report both theofficial score and a score that takes into account justthe first proposal of the systems, as the usual in-terpretation of BEST score methodology would sug-gest5.OOT.
Table 4 and 5 report the performance for thedomain proximity and syntagmatic coherence rank-ing, scoring for the 10 best substitutes.
The resultsare quite good especially in the case of syntagmaticcoherence ranking.Baselines.
Table 3 displays the baselines respec-tively for the BEST and OOT using WordNet 2.1as calculated by the task organizers.
They pro-pose many baseline measures, but we report only the4The task proposed a third scoring measure MW that scoresprecision and recall for detection and identification of multi-words in the input sentences.
However our systems were notdesigned for this functionality.
For the details of all scoringmethodologies please refer to the task description documents.5We misinterpreted that the official scorer divides anywaythe figures by the number of proposals.
So for the competitionwe submitted the oot result file without cutting the words afterthe first one.147P R Mode P Mode Rall 8.06 8.06 13.09 13.09Table 1: BEST results for LSA ranking (IRST1-lsa)P R Mode P Mode Rall 12.93 12.91 20.33 20.33all (official) 6.95 6.94 20.33 20.33Table 2: BEST results for Syntagmatic ranking(IRST2-syn)WordNet one, as it is the higher scoring baseline.
Wecan observe that globally our systems perform quitegood with respect to the baselines.4 ConclusionIn this paper we reported a detailed description ofthe FBK-irst systems submitted to the Lexical En-tailment task at the SEMEVAL 2007 evaluation cam-paign.
Our techniques are totally unsupervised, asthey do not require neither the availability of sensetagged data nor an estimation of sense priors, notconsidering the WordNet sense order information.Results are quite good, as in general they signifi-cantly outperform all the baselines proposed by theorganizers.
In addition, the method based on syn-tagmatic coherence estimated on the WEB outper-forms, to our knowledge, the other systems sub-mitted to the competition.
For the future, we planto avoid the use of dictionaries by adopting termsimilarity techniques to select the candidate entailedwords and to exploit this methodology in some spe-cific applications such as taxonomy induction andontology population.AcknowledgmentsClaudio Giuliano is supported by the X-Mediaproject (http://www.x-media-project.org), sponsored by the European Commissionas part of the Information Society Technologies(IST) programme under EC grant number IST-FP6-026978.
Alfio Gliozzo is supported by FIRB-IsraelP R Mode P Mode RWN BEST 9.95 9.95 15.28 15.28WN OOT 29.70 29.35 40.57 40.57Table 3: WordNet BaselinesP R Mode P Mode Rall 41.23 41.20 55.28 55.28Table 4: OOT results for LSA ranking (IRST1-lsa)P R Mode P Mode Rall 69.03 68.90 58.54 58.54Table 5: OOT results for Syntagmatic ranking(IRST2-syn)research project N. RBIN045PXH.ReferencesI.
Dagan and O. Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
In proceedings of the PASCAL Workshopon Learning Methods for Text Understanding and Min-ing, Grenoble.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The pas-cal recognising textual entailment challenge.
Proceed-ings of the PASCAL Challenges Workshop on Recog-nising Textual Entailment.I.
Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,and C. Strapparava.
2006.
Direct word sense match-ing for lexical substitution.
In Proceedings ACL-2006,pages 449?456, Sydney, Australia, July.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society of InformationScience.O.
Glickman, I. Dagan, M. Keller, S. Bengio, andW.
Daelemans.
2006a.
Investigating lexical substi-tution scoring for subtitle generation tenth conferenceon computational natural language learning.
In Pro-ceedings of CoNLL-2006.O.
Glickman, E. Shnarch, and I. Dagan.
2006b.
Lexicalreference: a semantic matching subtask.
In proceed-ings of EMNLP 2006.A.
Gliozzo.
2005.
Semantic Domains in Computa-tional Linguistics.
Ph.D. thesis, ITC-irst/University ofTrento.B.
Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.2002.
The role of domain information in wordsense disambiguation.
Natural Language Engineer-ing, 8(4):359?373.148
