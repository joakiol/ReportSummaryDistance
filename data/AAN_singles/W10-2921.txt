Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 172?181,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsImproved Natural Language Learning viaVariance-Regularization Support Vector MachinesShane BergsmaUniversity of Albertasbergsma@ualberta.caDekang LinGoogle, Inc.lindek@google.comDale SchuurmansUniversity of Albertadale@cs.ualberta.caAbstractWe present a simple technique for learn-ing better SVMs using fewer training ex-amples.
Rather than using the standardSVM regularization, we regularize towardlow weight-variance.
Our new SVM ob-jective remains a convex quadratic func-tion of the weights, and is therefore com-putationally no harder to optimize than astandard SVM.
Variance regularization isshown to enable dramatic improvementsin the learning rates of SVMs on three lex-ical disambiguation tasks.1 IntroductionDiscriminative training is commonly used in NLPand speech to scale the contribution of differentmodels or systems in a combined predictor.
Forexample, discriminative training can be used toscale the contribution of the language model andtranslation model in machine translation (Och andNey, 2002).
Without training data, it is often rea-sonable to weight the different models equally.
Wepropose a simple technique that exploits this intu-ition for better learning with fewer training exam-ples.
We regularize the feature weights in a Sup-port Vector Machine (Cortes and Vapnik, 1995) to-ward a low-variance solution.
Since the new SVMquadratic program is convex, it is no harder to op-timize than the standard SVM objective.When training data is generated through hu-man effort, faster learning saves time and money.When examples are labeled automatically, throughuser feedback (Joachims, 2002) or from tex-tual pseudo-examples (Smith and Eisner, 2005;Okanohara and Tsujii, 2007), faster learning canreduce the lag before a new system is useful.We demonstrate faster learning on lexical dis-ambiguation tasks.
For these tasks, a system pre-dicts a label for a word in text, based on theword?s context.
Possible labels include part-of-speech tags, named-entity types, and word senses.A number of disambiguation systems make pre-dictions with the help of N-gram counts from aweb-scale auxiliary corpus, typically via a search-engine (Lapata and Keller, 2005) or N-gram cor-pus (Bergsma et al, 2009).
When discriminativetraining is used to weigh the counts for classifi-cation, many of the learned feature weights havesimilar values.
Good weights have low variance.For example, consider the task of prepositionselection.
A system selects the most likely prepo-sition given the context, and flags a possible errorif it disagrees with the user?s choice:?
I worked in Russia from 1997 to 2001.?
I worked in Russia *during 1997 to 2001.Bergsma et al (2009) use a variety of web countsto predict the correct preposition.
They have fea-tures for COUNT(in Russia from), COUNT(Russiafrom 1997), COUNT(from 1997 to), etc.
If these arehigh, from is predicted.
Similarly, they have fea-tures for COUNT(in Russia during), COUNT(Russiaduring 1997), COUNT(during 1997 to).
These fea-tures predict during.
All counts are in the logdomain.
The task has thirty-four different prepo-sitions to choose from.
A 34-way classifier istrained on examples of correct preposition usage;it learns which context positions and sizes are mostreliable and assigns feature weights accordingly.A very strong unsupervised baseline, however,is to simply weight all the count features equally.In fact, in Bergsma et al (2009), the supervisedapproach requires over 30,000 training examplesbefore it outperforms this baseline.
In contrast,we show that by regularizing a classifier towardequal weights, a supervised predictor outperformsthe unsupervised approach after only ten exam-ples, and does as well with 1000 examples as thestandard classifier does with 100,000.172Section 2 first describes a general multi-classSVM.
We call the base vector of informationused by the SVM the attributes.
A standardmulti-class SVM creates features for the cross-product of attributes and classes.
E.g., the attributeCOUNT(Russia during 1997) is not only a featurefor predicting the preposition during, but also forpredicting the 33 other prepositions.
The SVMmust therefore learn to disregard many irrelevantfeatures.
We observe that this is not necessary,and develop an SVM that only uses the relevantattributes in the score for each class.
Building onthis efficient framework, we incorporate varianceregularization into the SVM?s quadratic program.We apply our algorithms to three tasks: prepo-sition selection, context-sensitive spelling correc-tion, and non-referential pronoun detection (Sec-tion 4).
We reproduce Bergsma et al (2009)?sresults using a multi-class SVM.
Our new mod-els achieve much better accuracy with fewer train-ing examples.
We also exceed the accuracy of areasonable alternative technique for increasing thelearning rate: including the output of the unsuper-vised system as a feature in the SVM.Variance regularization is an elegant addition tothe suite of methods in NLP that improve perfor-mance when access to labeled data is limited.
Sec-tion 5 discusses some related approaches.
Whilewe motivate our algorithm as a way to learn betterweights when the features are counts from an aux-iliary corpus, there are other potential uses of ourmethod.
We outline some of these in Section 6,and note other directions for future research.2 Three Multi-Class SVM ModelsWe describe three max-margin multi-class classi-fiers and their corresponding quadratic programs.Although we describe linear SVMs, they can beextended to nonlinear cases in the standard wayby writing the optimal function as a linear combi-nation of kernel functions over the input examples.In each case, after providing the general tech-nique, we relate the approach to our motivatingapplication: learning weights for count features ina discriminative web-scale N-gram model.2.1 Standard Multi-Class SVMWe define a K-class SVM following Crammer andSinger (2001).
This is a generalization of binarySVMs (Cortes and Vapnik, 1995).
We have a set{(x?1, y1), ..., (x?M , yM )} of M training examples.Each x?
is an N -dimensional attribute vector, andy ?
{1, ...,K} are classes.
A classifier, H , mapsan attribute vector, x?, to a class, y. H is parame-terized by a K-by-N matrix of weights, W:HW(x?)
=Kargmaxr=1{W?r ?
x?}
(1)where W?r is the rth row of W. That is, the pre-dicted label is the index of the row of W that hasthe highest inner-product with the attributes, x?.We seek weights such that the classifier makesfew errors on training data and generalizes wellto unseen data.
There are KN weights to learn,for the cross-product of attributes and classes.The most common approach is to train K sep-arate one-versus-all binary SVMs, one for eachclass.
The weights learned for the rth SVM pro-vide the weights W?r in (1).
We call this approachOvA-SVM.
Note in some settings various one-versus-one strategies may be more effective thanone-versus-all (Hsu and Lin, 2002).The weights can also be found using a singleconstrained optimization (Vapnik, 1998; Westonand Watkins, 1998).
Following the soft-marginversion in Crammer and Singer (2001):minW,?1,...,?M12K?i=1||W?i||2 + Cm?i=1?isubject to : ?i ?0?r 6= yi, W?yi ?
x?i ?
W?r ?
x?i ?1 ?
?i (2)The constraints require the correct class to bescored higher than other classes by a certain mar-gin, with slack for non-separable cases.
Minimiz-ing the weights is a form of regularization.
Tuningthe C-parameter controls the emphasis on regular-ization versus separation of training examples.We call this the K-SVM.
The K-SVM out-performed the OvA-SVM in Crammer and Singer(2001), but see Rifkin and Klautau (2004).
Thepopularity of K-SVM is partly due to convenience;it is included in popular SVM software like SVM-multiclass1 and LIBLINEAR (Fan et al, 2008).Note that with two classes, K-SVM is less effi-cient than a standard binary SVM.
A binary classi-fier outputs class 1 if (w?
?
x?
> 0) and class 2 other-wise.
The K-SVM encodes a binary classifier usingW?1 = w?
and W?2 = ?w?, therefore requiring twicethe memory of a binary SVM.
However, both bi-nary and 2-class formulations have the same solu-tion (Weston and Watkins, 1998).1http://svmlight.joachims.org/svm multiclass.html1732.1.1 Web-Scale N-gram K-SVMK-SVM was used with N-gram models in Bergsmaet al (2009).
For preposition selection, attributeswere web counts of patterns filled with 34 preposi-tions, corresponding to the 34 classes.
Each prepo-sition serves as the filler of each context pattern.Fourteen patterns were used for each filler: all five5-grams, four 4-grams, three 3-grams, and two 2-grams spanning the position to be predicted.
Thereare N = 14?34 = 476 total attributes, and thereforeKN = 476 ?
34 = 16184 weights in W.This K-SVM classifier can potentially exploitvery subtle information.
Let W?in and W?beforebe weights for the classes in and before.
Noticesome of the attributes weighted in the inner prod-ucts W?before ?
x?
and W?in ?
x?
will be for counts ofthe preposition after.
Relatively high counts for acontext with after should deter us from choosingin more than from choosing before.
These cor-relations can be encoded in the classifier via thecorresponding weights on after-counts in W?in andW?before.
How useful are these correlations andhow much training data is needed before they canbe learned and exploited effectively?We next develop a model that, for each class,only scores those attributes deemed to be directlyrelevant to the class.
Our experiments thus empir-ically address these questions for different tasks.2.2 SVM with Class-Specific AttributesSuppose we can partition our attribute vectors intosub-vectors that only include attributes that we de-clare as relevant to the corresponding class: x?
=(x?1, ..., x?K).
We develop a classifier that onlyuses the class-specific attributes in the score foreach class.
The classifier uses an N -dimensionalweight vector, w?, which follows the attribute par-tition, w?
= (w?1, ..., w?K).
The classifier is:Hw?(x?)
=Kargmaxr=1{w?r ?
x?r} (3)We call this classifier the CS-SVM (an SVM withClass-Specific attributes).The weights can be determined using the follow(soft-margin) optimization:minw?,?1,...,?m12 w?T w?
+ Cm?i=1?isubject to : ?i ?0?r 6= yi, w?yi ?
x?iyi ?
w?r ?
x?ir ?1 ?
?i (4)There are several advantages to this formula-tion.
Foremost, rather than having KN weights,it can have only N .
For linear classifiers, thenumber of examples needed to reach optimumperformance is at most linear in the number ofweights (Vapnik, 1998; Ng and Jordan, 2002).
Infact, both the total number and number of activefeatures per example decrease by K. Thus this re-duction saves far more memory than what couldbe obtained by an equal reduction in dimensional-ity via pruning infrequent attributes.Also, note that unlike the K-SVM (Section 2.1),in the binary case the CS-SVM is completely equiv-alent (thus equally efficient) to a standard SVM.We will not always a priori know the class as-sociated with each attribute.
Also, some attributesmay be predictive of multiple classes.
In suchcases, we can include ambiguous attributes in ev-ery sub-vector (needing N+D(K-1) total weightsif D attributes are duplicated).
In the degeneratecase where every attribute is duplicated, CS-SVMis equivalent to K-SVM; both have KN weights.2.2.1 Optimization as a Binary SVMWe could solve the optimization problem in (4)directly using a quadratic programming solver.However, through an equivalent transformationinto a binary SVM, we can take advantage of effi-cient, custom SVM optimization algorithms.We follow Har-Peled et al (2003) in transform-ing a multi-class example into a set of binaryexamples, each specifying a constraint from (4).We extend the attribute sub-vector correspondingto each class to be N -dimensional.
We do thisby substituting zero-vectors for all the other sub-vectors in the partition.
The attribute vector for therth class is then z?r = (0?, ..., 0?, x?r, 0?, ..., 0?).
This isknown as Kesler?s Construction and has a long his-tory in classification (Duda and Hart, 1973; Cram-mer and Singer, 2003).
We then create binary rankconstraints for a ranking SVM (Joachims, 2002)(ranking SVMs reduce to standard binary SVMs).We create K instances for each multi-class exam-ple (x?i, yi), with the transformed vector of the trueclass, z?yi , assigned a higher-rank than all the other,equally-ranked classes, z?
{r 6=yi}.
Training a rank-ing SVM using these constraints gives the sameweights as solving (4), but allows us to use effi-cient, custom SVM software.2 Note the K-SVM2One subtlety is whether to use a single slack, ?i, for allK-1 constraints per example i (Crammer and Singer, 2001),or a different slack for each constraint (Joachims, 2002).
Us-174can also be trained this way, by including everyattribute in every sub-vector, as described earlier.2.2.2 Web-Scale N-gram CS-SVMReturning to our preposition selection example, anobvious attribute partition for the CS-SVM is toinclude as attributes for predicting preposition ronly those counts for patterns filled with preposi-tion r. Thus x?in will only include counts for con-text patterns filled with in and x?before will onlyinclude counts for context patterns filled with be-fore.
With 34 sub-vectors and 14 attributes in each,there are only 14 ?
34 = 476 total weights.
In con-trast, K-SVM had 16184 weights to learn.It is instructive to compare the CS-SVM in (3) tothe unsupervised SUMLM approach in Bergsma etal.
(2009).
That approach can be written as:H(x?)
= Kargmaxr=1{1?
?
x?r} (5)where 1?
is an N -dimensional vector of ones.
Thisis CS-SVM with all weights set to unity.
Thecounts for each preposition are simply summed,and whichever one scores the highest is taken asthe output (actually only a subset of the counts areused, see Section 4.1).
As mentioned earlier, thissystem performs remarkably well on several tasks.2.3 Variance Regularization SVMsSuppose we choose our attribute partition well andtrain the CS-SVM on a sufficient number of exam-ples to achieve good performance.
It is a reason-able hypothesis that the learned weights will bepredominantly positive.
This is because each sub-vector x?r was chosen to only include attributesthat are predictive of class r. Unlike the classifierin (1) which weighs positive and negative evidencetogether for each class, in CS-SVM, negative evi-dence only plays a roll as it contributes to the scoreof competing classes.If all the attributes are equally important, theweights should be equal, as in the unsupervisedapproach in (5).
If some are more important thanothers, the training examples should reflect thisand the learner can adjust the weights accord-ingly.3 In the absence of this training evidence, itis reasonable to bias the classifier toward an equal-weight solution.ing the former may be better as it results in a tighter boundon empirical risk (Tsochantaridis et al, 2005).3E.g., the true preposition might be better predicted by thecounts of patterns that tend to include the preposition?s gram-matical object, i.e., patterns that include more right-context.Rather than the standard SVM regularizationthat minimizes the norm of the weights as in (4),we therefore regularize toward weights that havelow variance.
More formally, we can regard theset of weights, w1, ..., wN , as the distribution of adiscrete random variable, W .
We can calculate themean and variance of this variable from its distri-bution.
We seek a variable that has low variance.We begin with a more general objective andthen explain how a specific choice of covariancematrix, C, minimizes the variance of the weights.We propose the regularizer:minw?,?1,...,?m12 w?TCw?
+Cm?i=1?isubject to : ?i ?0?r 6= yi, w?yi ?
x?iyi ?
w?r ?
x?ir ?1 ?
?i (6)where C is a normalized covariance matrix suchthat?i,j Ci,j = 0.
This ensures uniform weightvectors receive zero regularization penalty.
Sinceall covariance matrices are positive semi-definite,the quadratic program (QP) remains convex in w?,and thus amenable to general purpose QP-solvers.Since the unsupervised system in (5) has zeroweight variance, the SVM learned in (6) should doas least as well as (5) as we tune the C-parameteron development data.
That is, as C approacheszero, variance minimization becomes the sole ob-jective of (6), and uniform weights are produced.We use covariance matrices of the form:C = diag(p?)
?
p?p?T (7)where diag(p?)
is the matrix constructed by puttingp?
on the main diagonal.
Here, p?
is an arbitraryN -dimensional weighting vector, such that p ?0 and ?i pi = 1. p?
dictates the contribution ofeach wi to the mean and variance of the weightsin w?.
It is easy to see that?i,j Ci,j =?i pi ?
?i?j pipj = 0.We now show that w?T (diag(p?)
?
p?p?T )w?
ex-presses the variance of the weights in w?
with re-spect to the probability weighting p?.
The varianceof a random variable with mean E[W ] = ?
is:Var[W ] = E[(W ?
?
)2] = E[W 2] ?
E[W ]2The mean of the weights using probability weight-ing p?
is E[W ] = w?T p?
= p?w?.
Also, E[W 2] =w?T diag(p?)w?.
Thus:Var[W ] = w?T diag(p?)w?
?
(w?T p?)(p?w?
)= w?T (diag(p?)
?
p?p?
)w?175In our experiments, we deem each weight to beequally important to the variance calculation, andset pi = 1N ,?i = 1, .
.
.
, N .The goal of the regularization in (6) using Cfrom (7) can be regarded as directing the SVM to-ward a good unsupervised system, regardless ofthe constraints (training examples).
In some un-supervised systems, however, only a subset of theattributes are used.
In other cases, distinct subsetsof weights should have low variance, rather thanminimizing the variance across all weights.
Thereare examples of these situations in Section 4.We can account for these cases in our QP.
Weprovide separate terms in our quadratic functionfor the subsets of w?
that should have low vari-ance.
Suppose we create L subsets of w?
: ?
?1, ...?
?L,where ?
?j is w?
with elements set to zero that are notin subset j.
We then minimize 12(?
?T1 C1?
?1 + ...
+??TLCL??L).
If the terms in subset j have low vari-ance, Cj = C from (7) is used.
If the subset corre-sponds to attributes that are not a priori known tobe useful, an identity matrix can instead be used,Cj = I, and these weights will be regularized to-ward zero as in a standard SVM.4Variance regularization therefore exploits extraknowledge by the system designer.
The designerdecides which weights should have similar values,and the SVM is biased to prefer this solution.One consequence of being able to regularizedifferent subsets of weights is that we can also ap-ply variance regularization to the standard multi-class SVM (Section 2.1).
We can use an identityCi matrix for all irrelevant weights, i.e., weightsthat correspond to class-attribute pairs where theattribute is not directly relevant to the class.
In ourexperiments, however, we apply variance regular-ization to the more efficient CS-SVM.We refer to a CS-SVM trained using the varianceminimization quadratic program as the VAR-SVM.2.3.1 Web-Scale N-gram VAR-SVMIf variance regularization is applied to all weights,attributes COUNT(in Russia during), COUNT(Russiaduring 1997), and COUNT(during 1997 to) will beencouraged to have similar weights in the score forclass during.
Furthermore, these will be weightedsimilarly to other patterns, filled with other prepo-sitions, used in the scores for other classes.4Weights must appear in ?1 subsets (possibly only in theCj = I subset).
Each occurs in at most one in our experi-ments.
Note it is straightforward to express this as a singlecovariance matrix regularizer over w?
; we omit the details.Alternatively, we could minimize the varianceseparately over all 5-gram patterns, then over all4-gram patterns, etc., or over all patterns with afiller in the same position.
In our experiments, wetook a very simple approach: we minimized thevariance of all attributes that are weighted equallyin the unsupervised baselines.
If a feature is not in-cluded in a baseline, it is regularized toward zero.3 Experimental DetailsWe use the data sets from Bergsma et al (2009).These are the three tasks where web-scale N-gramcounts were previously used as features in a stan-dard K-SVM.
In each case a classifier makes a de-cision for a particular word based on the word?ssurrounding context.
The attributes of the classi-fier are the log counts of different fillers occurringin the context patterns.
We retrieve counts fromthe web-scale Google Web 5-gram Corpus (Brantsand Franz, 2006), which includes N-grams oflength one to five.
We apply add-one smoothingto all counts.
Every classifier also has bias fea-tures (for every class).
We simply include, whereappropriate, attributes that are always unity.We use LIBLINEAR (Fan et al, 2008) to trainK-SVM and OvA-SVM, and SVMrank (Joachims,2006) to train CS-SVM.
For VAR-SVM, we solvethe primal form of the quadratic program directlyin CPLEX (2005), a general optimization package.We vary the number of training examples foreach classifier.
The C-parameters of all SVMs aretuned on development data.
We evaluate using ac-curacy: the percentage of test examples that areclassified correctly.
We also provide the accuracyof the majority-class baseline and best unsuper-vised system, as defined in Bergsma et al (2009).As an alternative way to increase the learningrate, we augment a classifier?s features using theoutput of the unsupervised system: For each class,we include one feature for the sum of all counts (inthe unsupervised system) that predict that class.We denote these augmented systems with a + asin K-SVM+ and CS-SVM+.4 Applications4.1 Preposition SelectionPreposition errors are common among new En-glish speakers (Chodorow et al, 2007).
Systemsthat can reliably identify these errors are neededin word processing and educational software.176Training ExamplesSystem 10 100 1K 10K 100KOvA-SVM 16.0 50.6 66.1 71.1 73.5K-SVM 13.7 50.0 65.8 72.0 74.7K-SVM+ 22.2 56.8 70.5 73.7 75.2CS-SVM 27.1 58.8 69.0 73.5 74.2CS-SVM+ 39.6 64.8 71.5 74.0 74.4VAR-SVM 73.8 74.2 74.7 74.9 74.9Table 1: Accuracy (%) of preposition-selectionSVMs.
Unsupervised accuracy is 73.7%.In our experiments, a classifier must choose thecorrect preposition among 34 candidates, usingcounts for filled 2-to-5-gram patterns.
We use100K training, 10K development, and 10K testexamples.
The unsupervised approach sums thecounts of all 3-to-5-gram patterns for each prepo-sition.
We therefore regularize the variance of the3-to-5-gram weights in VAR-SVM, and simultane-ously minimize the norm of the 2-gram weights.4.1.1 ResultsThe majority-class is the preposition of; it occursin 20.3% of test examples.
The unsupervised sys-tem scores 73.7%.
For further perspective on theseresults, note Chodorow et al (2007) achieved 69%with 7M training examples, while Tetreault andChodorow (2008) found the human performancewas around 75%.
However, these results are notdirectly comparable as they are on different data.Table 1 gives the accuracy for different amountsof training data.
Here, as in the other tasks, K-SVMmirrors the learning rate in Bergsma et al (2009).There are several distinct phases among the rela-tive ranking of the systems.
For smaller amountsof training data (?1000 examples) K-SVM per-forms worst, while VAR-SVM is statistically sig-nificantly better than all other systems, and al-ways exceeds the performance of the unsupervisedapproach.5 Augmenting the attributes with sumcounts (the + systems) strongly helps with fewerexamples, especially in conjunction with the moreefficient CS-SVM.
However, VAR-SVM clearlyhelps more.
We noted earlier that VAR-SVM isguaranteed to do as well as the unsupervised sys-tem on the development data, but here we confirmthat it can also exploit even small amounts of train-ing data to further improve accuracy.CS-SVM outperforms K-SVM except with 100K5Significance is calculated using a ?2 test over the test setcorrect/incorrect contingency table.Training ExamplesSystem 10 100 1K 10K 100KCS-SVM 86.0 93.5 95.1 95.7 95.7CS-SVM+ 91.0 94.9 95.3 95.7 95.7VAR-SVM 94.9 95.3 95.6 95.7 95.8Table 2: Accuracy (%) of spell-correction SVMs.Unsupervised accuracy is 94.8%.examples, while OvA-SVM is better than K-SVMfor small amounts of data.6 K-SVM performs bestwith all the data; it uses the most expressive repre-sentation, but needs 100K examples to make useof it.
On the other hand, feature augmentationand variance regularization provide diminishingreturns as the amount of training data increases.4.2 Context-Sensitive Spelling CorrectionContext-sensitive spelling correction, or real-worderror/malapropism detection (Golding and Roth,1999; Hirst and Budanitsky, 2005), is the task ofidentifying errors when a misspelling results in areal word in the lexicon, e.g., using site when sightor cite was intended.
Contextual spell checkers areamong the most widely-used NLP technology, asthey are included in commercial word processingsoftware (Church et al, 2007).For every occurrence of a word in a pre-definedconfusion set (e.g.
{cite, sight, cite}), the clas-sifier selects the most likely word from the set.We use the five confusion sets from Bergsma et al(2009); four are binary and one is a 3-way classi-fication.
We use 100K training, 10K development,and 10K test examples for each, and average ac-curacy across the sets.
All 2-to-5 gram counts areused in the unsupervised system, so the varianceof all weights is regularized in VAR-SVM.4.2.1 ResultsOn this task, the majority-class baseline is muchhigher, 66.9%, and so is the accuracy of the top un-supervised system: 94.8%.
Since four of the fivesets are binary classifications, where K-SVM andCS-SVM are equivalent, we only give the accuracyof the CS-SVM (it does perform better on the one3-way set).
VAR-SVM again exceeds the unsuper-vised accuracy for all training sizes, and generally6Rifkin and Klautau (2004) argue OvA-SVM is as goodas K-SVM, but this is ?predicated on the assumption that theclasses are ?independent?,?
i.e., that examples from class 0are no closer to class 1 than to class 2.
This is not true of thistask (e.g.
x?before is closer to x?after than x?in, etc.
).177Training ExamplesSystem 10 100 1KCS-SVM 59.0 71.0 84.3CS-SVM+ 59.4 74.9 84.5VAR-SVM 70.2 76.2 84.5VAR-SVM+FreeB 64.2 80.3 84.5Table 3: Accuracy (%) of non-referential detectionSVMs.
Unsupervised accuracy is 80.1%.performs as well as the augmented CS-SVM+ us-ing an order of magnitude less training data (Ta-ble 2).
Differences from ?1K are significant.4.3 Non-Referential Pronoun DetectionNon-referential detection predicts whether the En-glish pronoun it refers to a preceding noun (?itlost money?)
or is used as a grammatical place-holder (?it is important to...?).
This binary clas-sification is a necessary but often neglected stepfor noun phrase coreference resolution (Paice andHusk, 1987; Bergsma et al, 2008; Ng, 2009).Bergsma et al (2008) use features for the countsof various fillers in the pronoun?s context patterns.If it is the most common filler, the pronoun islikely non-referential.
If other fillers are common(like they or he), it is likely a referential instance.For example, ?he lost money?
is common on theweb, but ?he is important to?
is not.
We use thesame fillers as in previous work, and preprocessthe N-gram corpus in the same way.The unsupervised system picks non-referentialif the difference between the summed count ofit fillers and the summed count of they fillers isabove a threshold (note this no longer fits (5),with consequences discussed below).
We thusseparately minimize the variance of the it patternweights and the they pattern weights.
We use 1Ktraining, 533 development, and 534 test examples.4.3.1 ResultsThe most common class is referential, occurringin 59.4% of test examples.
The unsupervised sys-tem again does much better, at 80.1%.Annotated training examples are much harderto obtain for this task and we experiment with asmaller range of training sizes (Table 3).
The per-formance of VAR-SVM exceeds the performanceof K-SVM across all training sizes (bold accura-cies are significantly better than either CS-SVM for?100 examples).
However, the gains were notas large as we had hoped, and accuracy remainsworse than the unsupervised system when not us-ing all the training data.
When using all the data,a fairly large C-parameter performs best on devel-opment data, so regularization plays less of a role.After development experiments, we speculatedthat the poor performance relative to the unsuper-vised approach was related to class bias.
In theother tasks, the unsupervised system chooses thehighest summed score.
Here, the difference in itand they counts is compared to a threshold.
Sincethe bias feature is regularized toward zero, then,unlike the other tasks, using a low C-parameterdoes not produce the unsupervised system, so per-formance can begin below the unsupervised level.Since we wanted the system to learn this thresh-old, even when highly regularized, we removedthe regularization penalty from the bias weight,letting the optimization freely set the weight tominimize training error.
With more freedom, thenew classifier (VAR-SVM+FreeB) performs worsewith 10 examples, but exceeds the unsupervisedapproach with 100 training points.
Althoughthis was somewhat successful, developing betterstrategies for bias remains useful future work.5 Related WorkThere is a large body of work on regularization inmachine learning, including work that uses posi-tive semi-definite matrices in the SVM quadraticprogram.
The graph Laplacian has been used toencourage geometrically-similar feature vectors tobe classified similarly (Belkin et al, 2006).
An ap-pealing property of these approaches is that theyincorporate information from unlabeled examples.Wang et al (2006) use Laplacian regularizationfor the task of dependency parsing.
They regular-ize such that features for distributionally-similarwords have similar weights.
Rather than penal-ize pairwise differences proportional to a similar-ity function, we simply penalize weight variance.In the field of computer vision, Tefas et al(2001) (binary) and Kotsia et al (2009) (multi-class) also regularize weights with respect to a co-variance matrix.
They use labeled data to find thesum of the sample covariance matrices from eachclass, similar to linear discriminant analysis.
Wepropose the idea in general, and instantiate witha different C matrix: a variance regularizer overw?.
Most importantly, our instantiated covariancematrix does not require labeled data to generate.In a Bayesian setting, Raina et al (2006) model178feature correlations in a logistic regression clas-sifier.
They propose a method to construct a co-variance matrix for a multivariate Gaussian prioron the classifier?s weights.
Labeled data for other,related tasks is used to infer potentially correlatedfeatures on the target task.
Like in our results, theyfound that the gains from modeling dependenciesdiminish as more training data is available.We also mention two related online learning ap-proaches.
Similar to our goal of regularizing to-ward a good unsupervised system, Crammer et al(2006) regularize w?
toward a (different) target vec-tor at each update, rather than strictly minimizing||w?||2.
The target vector is the vector learned fromthe cumulative effect of previous updates.
Dredzeet al (2008) maintain the variance of each weightand use this to guide the online updates.
However,covariance between weights is not considered.We believe new SVM regularizations in gen-eral, and variance regularization in particular, willincreasingly be used in combination with relatedNLP strategies that learn better when labeled datais scarce.
These may include: using more-generalfeatures, e.g.
ones generated from raw text (Milleret al, 2004; Koo et al, 2008), leveraging out-of-domain examples to improve in-domain classifi-cation (Blitzer et al, 2007; Daume?
III, 2007), ac-tive learning (Cohn et al, 1994; Tong and Koller,2002), and approaches that treat unlabeled data aslabeled, such as bootstrapping (Yarowsky, 1995),co-training (Blum and Mitchell, 1998), and self-training (McClosky et al, 2006).6 Future WorkThe primary direction of future research will beto apply the VAR-SVM to new problems and tasks.There are many situations where a system designerhas an intuition about the role a feature will play inprediction; the feature was perhaps added with thisrole in mind.
By biasing the SVM to use featuresas intended, VAR-SVM may learn better with fewertraining examples.
The relationship between at-tributes and classes may be explicit when, e.g.,a rule-based system is optimized via discrimina-tive learning, or annotators justify their decisionsby indicating the relevant attributes (Zaidan et al,2007).
Also, if features are a priori thought tohave different predictive worth, the attribute val-ues could be scaled such that variance regulariza-tion, as we formulated it, has the desired effect.Other avenues of future work will be to extendthe VAR-SVM in three directions: efficiency, rep-resentational power, and problem domain.While we optimized the VAR-SVM objective inCPLEX, general purpose QP-solvers ?do not ex-ploit the special structure of [the SVM optimiza-tion] problem,?
and consequently often train intime super-linear with the number of training ex-amples (Joachims et al, 2009).
It would be usefulto fit our optimization problem to efficient SVMtraining methods, especially for linear classifiers.VAR-SVM?s representational power could be ex-tended by using non-linear SVMs.
Kernels canbe used with a covariance regularizer (Kotsia etal., 2009).
Since C is positive semi-definite, thesquare root of its inverse is defined.
We can there-fore map the input examples using (C?
12 x?
), andwrite an equivalent objective function in terms ofkernel functions over the transformed examples.Also, since structured-prediction SVMs buildon the multi-class framework (Tsochantaridis etal., 2005), variance regularization can be incor-porated naturally into more complex predictiontasks, such as parsers, taggers, and aligners.VAR-SVM may also help in new domains whereannotated data is lacking.
VAR-SVM should bestronger cross-domain than K-SVM; regulariza-tion with domain-neutral prior-knowledge can off-set domain-specific biases.
Learned weight vec-tors from other domains may also provide cross-domain regularization guidance.7 ConclusionWe presented variance-regularization SVMs, anapproach to learning that creates better classi-fiers using fewer training examples.
Variance reg-ularization incorporates a bias for known goodweights into the SVM?s quadratic program.
TheVAR-SVM can therefore exploit extra knowledgeby the system designer.
Since the objective re-mains a convex quadratic function of the weights,the program is computationally no harder to opti-mize than a standard SVM.
We also demonstratedhow to design multi-class SVMs using only class-specific attributes, and compared the performanceof this approach to standard multi-class SVMs onthe task of preposition selection.While variance regularization is most helpful ontasks with many classes and features, like prepo-sition selection, it achieved gains on all our taskswhen training with smaller sample sizes.
It shouldbe useful on a variety of other NLP problems.179ReferencesMikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: A geometric frame-work for learning from labeled and unlabeled exam-ples.
JMLR, 7:2399?2434.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Distributional identification of non-referentialpronouns.
In ACL-08: HLT.Shane Bergsma, Dekang Lin, and Randy Goebel.2009.
Web-scale N-gram models for lexical disam-biguation.
In IJCAI.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT.Thorsten Brants and Alex Franz.
2006.
The GoogleWeb 1T 5-gram Corpus Version 1.1.
LDC2006T13.Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In ACL-SIGSEM Workshop on Prepo-sitions.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with Golombcoding.
In EMNLP-CoNLL.David Cohn, Les Atlas, and Richard Ladner.
1994.
Im-proving generalization with active learning.
Mach.Learn., 15(2):201?221.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Mach.
Learn., 20(3):273?297.CPLEX.
2005.
IBM ILOG CPLEX 9.1. www.ilog.com/products/cplex/.Koby Crammer and Yoram Singer.
2001.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
JMLR, 2:265?292.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.JMLR, 3:951?991.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
JMLR, 7:551?585.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In ACL.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
InICML.Richard O. Duda and Peter E. Hart.
1973.
PatternClassification and Scene Analysis.
John Wiley &Sons.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLIN-EAR: A library for large linear classification.
JMLR,9:1871?1874.Andrew R. Golding and Dan Roth.
1999.
A Winnow-based approach to context-sensitive spelling correc-tion.
Mach.
Learn., 34(1-3):107?130.Sariel Har-Peled, Dan Roth, and Dav Zimak.
2003.Constraint classification for multiclass classificationand ranking.
In NIPS.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lexicalcohesion.
Nat.
Lang.
Eng., 11(1):87?111.Chih-Wei Hsu and Chih-Jen Lin.
2002.
A comparisonof methods for multiclass support vector machines.IEEE Trans.
Neur.
Networks, 13(2):415?425.Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu.
2009.
Cutting-plane training ofstructural SVMs.
Mach.
Learn., 77(1):27?59.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In KDD.Thorsten Joachims.
2006.
Training linear SVMs inlinear time.
In KDD.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In ACL-08: HLT.Irene Kotsia, Stefanos Zafeiriou, and Ioannis Pitas.2009.
Novel multiclass classifiers based on the min-imization of the within-class variance.
IEEE Trans.Neur.
Networks, 20(1):14?34.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACMTrans.
Speech and Language Processing, 2(1):1?31.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InHLT-NAACL.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In HLT-NAACL.Andrew Y. Ng and Michael I. Jordan.
2002.
Discrim-inative vs. generative classifiers: A comparison oflogistic regression and naive bayes.
In NIPS.Vincent Ng.
2009.
Graph-cut-based anaphoricity de-termination for coreference resolution.
In NAACL-HLT.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In ACL.Daisuke Okanohara and Jun?ichi Tsujii.
2007.
A dis-criminative language model with pseudo-negativesamples.
In ACL.180Chris D. Paice and Gareth D. Husk.
1987.
Towards theautomatic recognition of anaphoric features in En-glish text: the impersonal pronoun ?it?.
ComputerSpeech and Language, 2:109?132.Rajat Raina, Andrew Y. Ng, and Daphne Koller.
2006.Constructing informative priors using transfer learn-ing.
In ICML.Ryan Rifkin and Aldebaro Klautau.
2004.
In defenseof one-vs-all classification.
JMLR, 5:101?141.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: training log-linear models on unlabeleddata.
In ACL.Anastasios Tefas, Constantine Kotropoulos, and Ioan-nis Pitas.
2001.
Using support vector machines toenhance the performance of elastic graph matchingfor frontal face authentication.
IEEE Trans.
PatternAnal.
Machine Intell., 23:735?746.Joel R. Tetreault and Martin Chodorow.
2008.
Theups and downs of preposition error detection in ESLwriting.
In COLING.Simon Tong and Daphne Koller.
2002.
Support vec-tor machine active learning with applications to textclassification.
JMLR, 2:45?66.Ioannis Tsochantaridis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large mar-gin methods for structured and interdependent out-put variables.
JMLR, 6:1453?1484.Vladimir N. Vapnik.
1998.
Statistical Learning The-ory.
John Wiley & Sons.Qin Iris Wang, Colin Cherry, Dan Lizotte, and DaleSchuurmans.
2006.
Improved large margin depen-dency parsing via local constraints and Laplacianregularization.
In CoNLL.Jason Weston and Chris Watkins.
1998.
Multi-classsupport vector machines.
Technical Report CSD-TR-98-04, Department of Computer Science, RoyalHolloway, University of London.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In ACL.Omar Zaidan, Jason Eisner, and Christine Piatko.2007.
Using ?annotator rationales?
to improve ma-chine learning for text categorization.
In NAACL-HLT.181
