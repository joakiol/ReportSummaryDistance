Comparative Experiments on Disambiguating Word Senses:An Illustration of the Role of Bias in Machine LearningRaymond J .
MooneyDepar tment  of Computer  SciencesUniversity of TexasAust in,  TX  78712-1188mooney@cs.utexas.eduAbst rac tThis paper describes an experimental compari-son of seven different learning algorithms on theproblem of learning to disambiguate he meaningof a word from context.
The algorithms testedinclude statistical, neural-network, decision-tree,rule-based, and case-based classification tech-niques.
The specific problem tested involves dis-ambiguating six senses of the word "line" usingthe words in the current and proceeding sentenceas context.
The statistical and neural-networkmethods perform the best on this particular prob-lem and we discuss a potential reason for this ob-served difference.
We also discuss the role of biasin machine learning and its importance in explain-ing performance differences observed on specificproblems.In t roduct ionRecent research in empirical (corpus-based) natu-ral language processing has explored a number ofdifferent methods for learning from data.
Threegeneral approaches are statistical, neural-network,and symbolic machine learning and numerous spe-cific methods have been developed under eachof these paradigms (Wermter, Riloff, & Scheler,1996; Charniak, 1993; Reilly & Sharkey, 1992).An important question is whether some methodsperform significantly better than others on partic-ular types of problems.
Unfortunately, there havebeen very few direct comparisons of alternativemethods on identical test data.A somewhat indirect comparison of apply-ing stochastic ontext-free grammars (Periera &Shabes, 1992), a transformation-based method(Brill, 1993), and inductive logic program-ming (Zelle & Mooney, 1994) to parsing theATIS (Airline Travel Information Service) cor-pus from the Penn Treebank (Marcus, Santorini,& Marcinkiewicz, 1993) indicates fairly similarperformance for these three very different meth-ods.
Also, comparisons of Bayesian, information-retrieval, neural-network, and case-based methodson word-sense disambiguation have also demon-strated similar performance (Leacock, Towell, &Voorhees, 1993b; Lehman, 1994).
However, ina comparison of neural-network and decision-treemethods on learning to generate the past tenseof an English verb, decision trees performed sig-nificantly better (Ling & Marinov, 1993; Ling,1994).
Subsequent experiments on this problemhave demonstrated that an inductive logic pro-gramming method produces even better resultsthan decision trees (Mooney & Califf, 1995).In this paper, we present direct comparisonsof a fairly wide range of general learning algo-rithms on the problem of discriminating six sensesof the word "line" from context, using data as-sembled by Leacock et al (1993b).
We comparea naive Bayesian classifier (Duda & Hart, 1973),a perceptron (Rosenblatt, 1962), a decision-treelearner (Quinlan, 1993), a k nearest-neighbor clas-sifier (Cover & Hart, 1967), logic-based DNF (dis-junctive normal form) and CNF (conjunctive nor-mal form) learners (Mooney, 1995) and a decision-list learner (Rivest, 1987).
Tests on all methodsused identical training and test sets, and ten sep-arate random trials were run in order to measureaverage performance and allow statistical testingof the significance of any observed ifferences.
Onthis particular task, we found that the Bayesianand perceptron methods perform significantly bet-ter than the remaining methods and discuss a po-tential reason for this observed ifference.
We alsodiscuss the role of bias in machine learning and itsimportance in explaining the observed ifferencesin the performance of alternative methods on spe-cific problems.Background on  Mach ine  Learn ingand  B iasResearch in machine learning over the last tenyears has been particularly concerned with exper-imental comparisons and the relative performanceof different classification methods (Shavlik & Di-82etterich, 1990; Kulikowski & Weiss, 1991; Langley,1996).
In particular, the UCI Machine LearningData Repository (Merz, Murphy, & Aha, 1996)was assembled to facilitate mpirical comparisons.Experimental comparisons ofdifferent methods onvarious benchmark problems have generally foundrelatively small differences in predictive accuracy(Mooney, Shavlik, Towell, & Gove, 1989; Fisher &McKusick, 1989; Weiss & Kapouleas, 1989; Atlas,Cole, Conner, EI-Sharkawi, Marks, Muthusamy,& Bernard, 1990; Dietterich, Hild, & Bakiri, 1990;Kulikowski & Weiss, 1991; Shavlik, Mooney, &Towell, 1991; Holte, 1993).
However, on specificproblems, certain methods can demonstrate a sig-nificant advantage.
For example, on the problemof detecting promoter sequences in DNA (whichindicate the start of a new gene), neural-networkand similar methods perform significantly betterthan symbolic induction methods (Towell, Shav-lik, & Noordewier, 1990; Baffes & Mooney, 1993).On the other hand, as mentioned in the introduc-tion, symbolic induction methods perform signifi-cantly better than neural-networks on the problemof learning to generate the past tense of an Englishverb (Ling & Marinov, 1993; Ling, 1994; Mooney& Califf, 1995).It is generally agreed that the philosophicalproblem of induction (Hume, 1748) means thatno inductive algorithm is universally better thanany other.
It can be proven that when averagedover a uniform distribution of all possible classi-fication problems, the generalization performance(predictive accuracy on unseen examples) of anyinductive algorithm is zero.
This has been calledthe "Conservation Law for Generalization Perfor-mance" (Schaffer, 1994) or a "no free lunch" the-orem (Wolpert, 1992).
However, averaging overa uniform distribution of all possible functions iseffectively equivalent to assuming a "random uni-verse" in which the past is not predictive of thefuture.
If all problems are not equally likely, theexpected generalization performance over a distri-bution of real-world problems can of course be pos-itive (Rao, Gordon, & Spears, 1995).In machine learning, bias refers to "any ba-sis for choosing one generalization ver another,other than strict consistency with the instances"(Mitchell, 1980).
Decision-tree methods havea bias for simple decision trees, rule inductionmethods have a bias for simple DNF expressions,neural-network methods have a bias for linearthreshold functions, 1and naive Bayes has a biasfor functions which respect conditional indepen-dence of features.
The more the bias of a certain1 Although multi-layer networks with sufficient hid-den can represent arbitrary nonlinear functions, theywill tend to learn a linear function if one exists that isconsistent with the training data.learning algorithm fits the characteristics of a par-ticular problem, the better it will perform on thatproblem.
Most learning algorithms have some sortof "Occam's razor" bias in which hypotheses thatcan be represented with fewer bits in some particu-lar representation language are preferred (Blumer,Ehrenfeucht, Haussler, gz Warmuth, 1987).
How-ever, the compactness with which different repre-sentation languages (e.g.
decision trees, DNF, lin-ear threshold networks) can represent particularfunctions can vary dramatically (e.g.
see Pagalloand Haussler (1990)).
Therefore, different biasescan perform better or worse on specific problems.One of the main goals of machine learning is tofind biases that perform well on the distributionof problems actually found in the real world.As an example, consider the advantage neural-networks have on the promoter recognition prob-lem mentioned earlier.
There are several potentialsites where hydrogen bonds can form between theDNA and a protein and if enough of these bondsform, promoter activity can occur.
This is rep-resented most compactly using an M-of-N classi-fication function which returns true if any subsetof size M of N specified features are present inan example (Fisher & McKusick, 1989; MurphyPazzani, 1991; Baffes ~ Mooney, 1993).
A sin-gle linear threshold unit can easily represent suchfunctions, whereas a DNF expression requires "Nchoose M" terms to represent them.
Therefore,the difference in their ability to compactly rep-resent such functions explains the observed per-formance difference between rule induction andneural-networks onthis problem.
2Of course picking the right bias or learning al-gorithm for a particular task is a difficult problem.A simple approach is to automate the selection ofa method using internal cross-validation (Schaffer,1993).
Another approach is to use meta-learningto learn a set of rules (or other classifier) that pre-dicts when a learning algorithm will perform beston a domain given features describing the problem(Aha, 1992).
A recent special issue of the MachineLearning journal on "Bias Evaluation and Selec-tion" introduced by Gordon and desJardins (1995)presents current research in this general area.Learn ing  to  D isambiguate  WordSensesSeveral recent research projects have taken acorpus-based approach to lexical disambiguation(Brown, Della-Pietra, Della-Pietra, & Mercer,1991; Gale, Church, & Yarowsky, 1992b; Leacocket al, 1993b; Lehman, 1994).
The goal is to learn2This explanation was originally presented byShavlik et al (1991).83to use surrounding context o determine the senseof an ambiguous word.
Our tests are based on thecorpus assembled by Leacock et al (1993b).
Thetask is to disambiguate the word "line" into oneof six possible senses (text, formation, division,phone, cord, product) based on the words occur-ring in the current and previous entence.
The cor-pus was assembled from the 1987-89 Wall StreetJournal and a 25 million word corpus from theAmerican Printing House for the Blind.
Sentencescontaining "line" were extracted and assigned asingle sense from WordNet (Miller, 1991).
Thereare a total of 4,149 examples in the full corpus un-equally distributed across the six senses.
Due tothe use of the Wall Street Journal, the "product"sense is more than 5 times as common as any ofthe others.
Previous tudies have first sampled thedata so that all senses were equally represented.Leacock et al (1993b), Leacock, Towell,and Voorhees (1993a) and Voorhees, Leacock,and Towell (1995) present results on a Bayesianmethod (Gale, Church, & Yarowsky, 1992a), acontent vector method from information retrieval(Salton, Wong, & Yang, 1975), and a neural net-work trained using backpropagation (Rumelhart,Hinton, & Williams, 1986).
The neural networkarchitecture that performed at least as well as anyother contained no hidden units, so was effectivelyequivalent to a perceptron.
On the six-sense tasktrained on 1,200 examples and averaged over threerandom trials, they report the following general-ization accuracies: Bayesian, 71%; content vec-tors, 72%; neural nets, 76%.
None of these differ-ences were statistically significant given the smallnumber of trials.In these studies, the data for the content-vector and neural-network methods was first re-duced by ignoring case and reducing words tostems (e.g.
computer(s), computing, computa-tion(al), etc.
are all conflated to the featurecomput) and removing a set of about 570 high-frequency stopwords (e.g.
the, by, you, etc.).
Sim-ilar preprocessing was performed for the currentexperiments, but we can not guarantee identicalresults.
The result was a set of 2,094 examplesequally distributed across the six senses whereeach example was described using 2,859 binaryfeatures each representing the presence or absenceof a particular word stem in the current or imme-diately preceding sentence.Learn ing  A lgor i thms TestedThe current experiments test a total of sevendifferent learning algorithms with quite dif-ferent biases.
This section briefly describeseach of these algorithms.
Except for C4.5,which uses the C code provided by Quin-lan (1993), all of these methods are imple-mented in Common Lisp and available on-line athttp ://www.
cs.
ut exas.
edu/users/ml/ml-progs .html.All systems were run on a Sun SPARCstation 5with 40MB of main memory.The simplest algorithms tested were a naiveBayesian classifier which assumes conditional in-dependence of features and a k nearest-neighborclassifier, which assigns a test example to themajority class of the 3 closest training examples(using Hamming distance to measure closeness)(Duda ~ Hart, 1973; Kulikowski & Weiss, 1991).Initial results indicated that k nearest neighborwith k--3 resulted in slightly better performancethan k--1.
Naive Bayes is intended as a simplerepresentative of statistical methods and nearestneighbor as a simple representative of instance-based (case-based, exemplar) methods (CoverHart, 1967; Aha, Kibler, ~ Albert, 1991).Since the previous results of Leacock et al(1993b) indicated that neural networks did notbenefit from hidden units on the "line" disam-biguation data, we employed a simple perceptron(Rosenblatt, 1962) as a representative connection-ist method.
The implementation learns a separateperceptron for recognizing each sense and assignsa test case to the sense indicated by the perceptronwhose output most exceeds its threshold.
In thecurrent experiments, there was never a problemwith convergence during training.As a representative of decision-tree methods,we chose C4.5 (Quinlan, 1993), a system that iseasily available and included in most recent exper-imental comparisons in machine learning.
All pa-rameters were left at their default values.
We alsotested C4.5-RULES, a variant of C4.5 in which de-cision trees are translated into rules and pruned;however, its performance was slightly inferior tothe base C4.5 system on the "line" corpus; there-fore, its results are not included.Finally, we tested three simple logic-based in-duction algorithms that employ different represen-tations of concepts: DNF, CNF, and decision lists.Most rule-based methods, e.g.
Michalski (1983),induce a disjunctive set of conjunctive rules andtherefore represent concepts in DNF.
Some recentresults have indicated that representing conceptsin CNF (a conjunction of disjunctions) frequentlyperforms omewhat better (Mooney, 1995).
Someconcepts are more compactly represented in CNFcompared to DNF and vice versa.
Therefore,both representations are included.
Finally, deci-sion lists (Rivest, 1987) are ordered lists of con-junctive rules, where rules are tested in order andthe first one that matches an instance is used toclassify it.
A number of effective concept-learningsystems have employed decision lists (Clark84Niblett, 1989; Quinlan, 1993; Mooney & Califf,1995) and they have already been successfully ap-plied to lexical disambiguation (Yarowsky, 1994).All of the logic-based methods are variationsof the FOIL algorithm for induction of first-orderfunction-free Horn clauses (Quinlan, 1990), ap-propriately simplified for the propositional case.They are called PFoIL-DNF,  PFOlL-CNF, andPFoIL-DLIsT.
The algorithms are greedy cov-ering (separate-and-conquer) methods that usean information-theoretic heuristic to guide a top-down search for a simple definition consistent withthe training data.
PFo lL -DNF (PFolL-CNF)learns a separate DNF (CNF) description for eachsense using the examples of that sense as posi-tive instances and the examples of all other sensesas negative instances.
Mooney (1995) describesPFoIL-DNF and PFo lL -CNF in more detail andPFoIL-DLIsT is based on the first-order decision-list learner described by Mooney and Califf (1995).Exper imentsIn order to evaluate the performance ofthese sevenalgorithms, direct multi-trial comparisons on iden-tical training and test sets were run on the "line"corpus.
Such head-to-head comparisons of meth-ods are unfortunately relatively rare in the empiri-cal natural-language literature, where papers gen-erally report results of a single method on a singletraining set with, at best, indirect comparisons toother methods.Exper imenta l  MethodologyLearning curves were generated by splitting thepreprocessed "line" corpus into 1,200 training ex-amples and 894 test cases, training all methodson an increasingly larger subset of the trainingdata and repeatedly testing them on the testset.
Learning curves are fairly common in ma-chine learning but not in corpus-based languageresearch.
We believe they are important sincethey reveal how algorithms perform with varyingamounts of training data and how their perfor-mance improves with additional training.
Resultson a fixed-sized training set gives only one datapoint on the learning curve and leaves the possi-bility that differences between algorithms are hid-den due to a ceiling effect, in which there aresufficient training examples for all methods toreach near Bayes-optimal performance, s LearningaBayes-optimal performance is achieved by alwayspicking the category with the maximum probabilitygiven all of its features.
This requires actually knowingthe conditional probability of each category given eachof the exponentially large number of possible instancedescriptions.curves generally follow a power law where predic-tive accuracy climbs fairly rapidly and then lev-els off at an asymptotic level.
A learning curvecan reveal whether the performance of a system isapproaching an asymptote or whether additionaltraining data would likely result in significant im-provement.
Since gathering annotated trainingdata is an expensive time-consuming process, it isimportant to understand the performance ofmeth-ods given varying amounts of training data.In addition to measuring eneralization accu-racy, we also collected ata on the CPU time takento train and test each method for each training-set size measured on the learning curve.
This pro-vides information on the computational resourcesrequired by each method, which may also be usefulin deciding between them for particular applica-tions.
It also provides data on how the algorithmscales by providing information on how trainingtime grows with training-set size.Finally, all results are averaged over ten ran-dom selections of training and test sets.
The per-formance of a system can vary a fair bit from trialto trial, and a difference in accuracy on a sin-gle training set may not indicate an overall per-formance advantage.
Unfortunately, most resultsreported in empirical natural-language r searchpresent only a single or very small number of tri-Ms. Running multiple trials also allows for sta-tistical testing of the significance of any resultingdifferences in average performance.
We employa simple two-tailed, paired t-test to compare theperformance of two systems for a given training-set size, requiring significance at the 0.05 level.Even more sophisticated statistical analysis of theresults is perhaps warranted.Experimental  ResultsThe resulting learning curves are shown in Fig-ure 1 and results on training and testing time areshown in Figures 2 and 3.
Figure 3 presents thetime required to classify the complete set of 894test examples.With respect to accuracy, naive Bayes andperceptron perform significantly better (p _< 0.05)than all other methods for all training-set sizes.Naive Bayes and perceptron are not significantlydifferent, except at 1,200 training examples wherenaive Bayes has a slight advantage.
Note that theresults for 1,200 training examples are compara-ble to those obtained by Leacock et al (1993b) forsimilar methods.
PFOIL-DLIsT is always signifi-cantly better than PFoIL-DNF and PFoIL-CNFand significantly better than 3 Nearest Neighborand C4.5 at 600 and 1,200 training examples.
C4.5and 3 Nearest Neighbor are always significantlybetter than PFoIL-DNF and PFoIL -CNF but8580 I I I I IOoo~70605040302O~ ,d ?o oE}"  ~ .
.
.
.
.
.
.
.
.
.
.
~ .
:=....... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
: : : : .
.
,  ............Naive Bayes oPercept ron  -4 - - .PFOIL -DL IST  -E~--C4 .5  --x- .....3 Nearest Neighbor -~-- -PFOIL -DNF -~-- -PFOIL -CNF  -~-  --10 I0 200I I I I400 600 800 1000Numl:~rofTraining ExamplesFigure 1: Accuracy at Disambiguating "Line"1200not significantly different from each other.
Finally,PFoIL-DNF is significantly better than PFOIL-CNF at 600 and 1,200 training examples.With respect o training time, virtually all dif-ferences are significant.
The logic-based inductionmethods are slowest, C4.5 and perceptron inter-mediate, and naive Bayes the fastest.
Since it juststores examples, training time for Nearest Neigh-bor is always zero.
In general, connectionist meth-ods are much slower to train than alternative tech-niques (Shavlik et al, 1991); however, in this casea simple perceptron converges quite rapidly.With respect o testing time, the symbolic in-duction methods are fastest and almost indistin-guishable from zero in Figure 3 since they onlyneed to test a small subset of the features.
4All visible differences in the graph are significant.Naive Bayes is the slowest; both it and percep-tron have the constant overhead of computing aweighted function over all of the almost 3,000 fea-tures.
Nearest neighbor grows linearly with thenumber of training instances as expected; moresophisticated indexing methods can reduce this tologarithmic expected time (Friedman, Bentley,Finkel, 1977).
54C4.5 suffers a small constant overhead ue to theC code having to read the test data in from a separatefile.5It should be noted that the implementation ofnearest neighbor was optimized to handle sparse bi-nary vectors by only including and comparing the lea-D iscuss ion  o f  Resu l tsNaive Bayes and perceptron are similar in thatthey both employ a weighted combination of allfeatures.
The decision-tree and logic-based ap-proaches all attempt to find a combination of a rel-atively small set of features that accurately predictclassification.
After training on 1,200 examples,the symbolic structures learned for the line corpusare relatively large.
Average sizes are 369 leavesfor C4.5 decision trees, 742 literals for PFoIL-DLIST decision lists, 841 literals for PFoIL-DNFformulae, and 1197 literals for PFo IL -CNF for-mulae.
However, many nodes or literals can testthe same feature and the last two results includethe total literal count for six separate DNF orCNF formulae (one for each sense).
Therefore,each discrimination is clearly only testing a rel-atively small fraction of the 2,859 available fea-tures.
Nearest neighbor bases its classifications onall features; however, it weights them all equally.Therefore, differential weighting is apparently nec-essary for high-performance on this problem.
Al-ternative instance-based methods that weight fea-tures based on their predictive ability have alsobeen developed (Aha et al, 1991).
Therefore, ourresults indicate that lexical disambiguation is per-haps best performed using methods that combineweighted evidence from all of the features rathertures actually present in the examples.
Without thisoptimization, testing would have been several ordersof magnitude slower.86I -o0600050004000300020001000| i | | IPFOIL-DNF oPFOIL-CNF -+---PFOIL -DL IST  -E~--Perceptron -x.
.....C4.5 --~---Naive Bayes -~---3 Nearest Neighbor -~---?
t !?
!fo .~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.200 400 600 800Training ExamplesFigure 2: Training Time for "Line" Corpus1000 1200than making a decision by testing only a smallsubset of highly predictive features.Among the other methods tested, decisionlists seem to perform the best.
The ordering ofrules employed in a decision list in order to sim-plify the representation a d perform conflict reso-lution apparently gives it an advantage over othersymbolic methods on this task.
In addition to theresults reported by Yarowsky (1994) and Mooneyand Califf (1995), it provides evidence for theutility of this representation for natural-languageproblems.With respect o training time, the symbolicmethods are significantly slower since they aresearching for a simple declarative r presentation fthe concept.
Empirically, the time complexity formost methods are growing somewhat worse thanlinearly in the number of training examples.
Theworst in this regard are PFoIL-DNF and PFoIL-C N F which have a worst-case complexity of O (n2)(Mooney, 1995).
However, all of the methods areable to process fairly large sets of data in reason-able time.With respect to testing time, the symbolicmethods perform the best since they only needto test a small number of features before makinga decision.
Therefore, in an application where re-sponse time is critical, learned rules or decisiontrees could provide rapid classification with onlya modest decrease in accuracy.
Not surprisingly,there is a trade-off between training time and test-ing time, the symbolic methods pend more effortduring training compressing the representation fthe learned concept resulting in a simpler descrip-tion that is quicker to test.Future ResearchThe current results are for only one simple en-coding of the lexical disambiguation problem intoa feature vector representing an unordered set ofword stems.
This paper has focused on explor-ing the space of possible algorithms rather thanthe space of possible input representations.
Al-ternative ncodings which exploit positional infor-mation, syntactic word tags, syntactic parse trees,semantic information, etc.
should be tested to de-termine the utility of more sophisticated represen-tations.
In particular, it would be interesting tosee if the accuracy ranking of the seven algorithmsis affected by a change in the representation.Similar comparisons of a range of algorithmsshould also be performed on other natural an-guage problems such as part-of-speech tagging(Church, 1988), prepositional phrase attachment(Hindle & Rooth, 1993), anaphora resolution(Anoe ~ Bennett, 1995), etc..
Since the require-ments of individual tasks vary, different algorithmsmay be suitable for different sub-problems in nat-ural language processing.87o"0 ?
:00 ?
?/)3503002502001501 O050I I I I I~.
Naive Bales o J3 Nearest Neighbor --0--- |Perceptron -m-- |C4.5 ---x ...... "PFOIL-DNF -~--- 1PFOIL-CNF -~-'- ..-"1"PFOIL-DLIST -~- -- .
.
- '"I ~ .G  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
\ [ \ ]  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
'~:~:::':"-; ............ ~" ........... ii"21"2 .................. x 2~...~?.~?.~?..~..~..~...C~..~..~:..~..~..~?..2...~...~.....2..2...Z/~...2:200 400 600 800 1000 1200Training ExamplesFigure 3: Testing Time for "Line" CorpusConclusionsThis paper has presented fairly comprehensive ex-periments comparing seven quite different empiri-cal methods on learning to disambiguate words incontext.
Methods that employ a weighted com-bination of a large set of features, such as sim-ple Bayesian and neural-network methods, wereshown to perform better than alternative meth-ods such as decision-tree, rule-based, and instance-based techniques on the problem of disambiguat-ing the word "line" into one of six possible sensesgiven the words that appear in the current andprevious entence as context.
Although differentlearning algorithms can frequently perform quitesimilarly, they all have specific biases in their rep-resentation ofconcepts and therefore can illustrateboth strengths and weaknesses in particular appli-cations.
Only rigorous experimental comparisonstogether with a qualitative analysis and explana-tion of their results can help determine the appro-priate methods for particular problems in naturallanguage processing.AcknowledgementsThis research was partially supported by theNational Science Foundation through grant IRI-9310819.
I would also like to thank Goeff Towellfor providing access to the "line" corpus.ReferencesAha, D. W. (1992).
Generalizing from case stud-ies: A case study.
In Proceedings of the NinthInternational Conference on Machine Learn-ing, pp.
1-10 Aberdeen, Scotland.Aha, D. W., Kibler, D., & Albert, M. K. (1991).Instance-based learning algorithms.
Ma-chine Learning, 6(1), 37-66.Anoe, C., ~ Bennett, S. W. (1995).
Evaluat-ing automated and manual acquisition ofanaphora resolution strategies.
In Proceed-ings of the 33rd Annual Meeting of the As-sociation for Computational Linguistics, pp.122-129 Cambridge, MA.Atlas, L., Cole, R., Conner, J., E1-Sharkawi, M.,Marks, R., Muthusamy, Y., & Bernard, E.(1990).
Performance comparisons betweenbackpropagation networks and classificationtrees on three real-world applications.
InTouretzky, D. S.
(Ed.
), Advances in NeuralInformation Processing Systems 2.
MorganKaufmann, San Mateo, CA.Baffes, P., ~ Mooney, R. (1993).
Symbolic revisionof theories with M-of-N rules.
In Proceedingsof the Thirteenth International Joint Con-ference on Artificial Intelligence, pp.
1135-1140 Chambery, France.Blumer, A., Ehrenfeucht, A., Haussler, D., & War-muth, M. (1987).
Occam's razor.
Informa-tion Processing Letters, 24, 377-380.88Brill, E. (1993).
Automatic grammar inductionand parsing free text: A transformation-based approach.
In Proceedings of the 31stAnnual Meeting of the Association for Com-putational Linguistics, pp.
259-265 Colum-bus, Ohio.Brown, P., Della-Pietra, S., Della-Pietra, V., &Mercer, R. (1991).
Word sense disambigua-tion using statistical methods.
In Proceed-ings of the 29th Annual Meeting of the As-sociation for Computational Linguistics, pp.264-270.Charniak, E. (1993).
Statistical Language Learn-ing.
MIT Press.Church, K. (1988).
A stochastic parts programand noun phrase parser for unrestricted text.In Proceedings of the Second Conference onApplied Natural Language Processing.
Asso-ciation for Computational Linguistics.Clark, P., & Niblett, T. (1989).
The CN2 induc-tion algorithm.
Machine Learning, 3, 261-284.Cover, T. M., & Hart, P. E. (1967).
Nearest neigh-bor pattern classification.
IEEE Transac-tions on Information Theory, 13, 21-27.Dietterich, T. G., Hild, H., & Bakiri, G. (1990).A comparative study of ID3 and backprop-agation for English text-to-speech mapping.In Proceedings of the Seventh InternationalConference on Machine Learning, pp.
24-31Austin, TX.Duda, R. O., & Hart, P. E. (1973).
Pattern Clas-sification and Scene Analysis.
Wiley, NewYork.Fisher, D. H., & McKusick, K. B.
(1989).
An em-pirical comparison of ID3 and backpropaga-tion.
In Proceedings of the Eleventh Interna-tional Joint Conference on Artificial Intelli-gence, pp.
788-793 Detroit, MI.Friedman, J., Bentley, J., & Finkel, R. (1977).
Analgorithm for finding best matches in loga-rithmic expected time.
A CM Transactionson Mathematical Software, 3 (3), 209-226.Gale, W., Church, K., & Yarowsky, D. (1992a).
Amethod for disambiguating word senses in alarge corpus.
Computers and the Humani-ties, 26, 415-439.Gale, W., Church, K. W., & Yarowsky, D. (1992b).Estimating upper and lower bounds on theperformance of word-sense disambiguationprograms.
In Proceedings of the 30th An-nual Meeting of the Association for Com-putational Linguistics, pp.
249-256 Newark,Delaware.Gordon, D. F., & desJardins, M. (1995).
Evalua-tion and selection of biases in machine learn-ing.
Machine Learning, 20(1/2), 5-22.Hindle, D., & Rooth, M. (1993).
Structural am-biguity and lexical relations.
ComputationalLinguistics, 19(1), 103-120.Holte, R. C. (1993).
Very simple classificationrules perform well on most commonly useddatasets.
Machine Learning, 11(1), 63-90.Hume, D. (1748).
An Inquiry Concerning HumanUnderstanding Reprinted 1955.
Liberal ArtsPress, New York.Kulikowski, C. A., & Weiss, S. M. (1991).Computer Systems That Learn - Classifica-tion and Prediction Methods from Statistics,Neural Nets, Machine Learning, and ExpertSystems.
Morgan Kaufmann, San Mateo,CA.Langley, P. (1996).
Elements of Machine Learning.Morgan Kaufmann, San Francisco, CA.Leacock, C., Towell, G., & Voorhees, E. (1993a).Corpus-based statistical sense resolution.
InProceedings of the ARPA Workshop on Hu-man Language Technology.Leacock, C., Towell, G., & Voorhees, E. (1993b).Towards building contextual representationsof word senses using statistical models.
InProceedings of the SIGLEX Workshop: Ac-quisition of Lexical Knowledge from Text,pp.
10-20.
Association for ComputationalLinguistics.Lehman, J. F. (1994).
Toward the essential natureofsatistical knowledge insense resolution.
InProceedings of the Twelfth National Confer-ence on Artificial Intelligence, pp.
734-741Seattle, WA.Ling, C. X.
(1994).
Learning the past tense ofEnglish verbs: The symbolic pattern asso-ciator vs. connectionist models.
Journal ofArtificial Intelligence Research, 1, 209-229.Ling, C. X., & Marinov, M. (1993).
Answering theconnectionist challenge: A symbolic modelof learning the past tense of English verbs.Cognition, 49(3), 235-290.Marcus, M., Santorini, B., & Marcinkiewicz, M.(1993).
Building a large annotated corpus ofEnglish: The Penn treebank.
ComputationalLinguistics, 19(2), 313-330.Merz, C., Murphy, P. M., & Aha, D. W. (1996).Repository of machine learning databaseshttp://w~, ics.
uci.
edu/'mlearn/mlrepos itory, html.Department of Information and ComputerScience, University of California, Irvine, CA.Michalski, R. S. (1983).
A theory and method-ology of inductive learning.
In Michalski,89R.
S., Carbonell, J. G., & Mitchell, T.
M.(Eds.
), Machine Learning: An Artificial In-telligence Approach, pp.
83-134.
Tioga.Miller, G. (1991).
WordNet: An on-line lexicaldatabase.
International Journal of Lexicog-raphy, 3(4).Mitchell, T. (1980).
The need for biases in learninggeneralizations.
Tech.
rep. CBM-TR-117,Rutgers University.
Reprinted in Readingsin Machine Learning, J. W. Shavlik and T.G.
Dietterich (eds.
), Morgan Kaufman, SanMateo, CA, 1990.Mooney, R. J.
(1995).
Encouraging experimentalresults on learning CNF.
Machine Learning,I9(1), 79-92.Mooney, R. J., & Califf, M. E. (1995).
Induction offirst-order decision lists: Results on learningthe past tense of English verbs.
Journal ofArtificial Intelligence Research, 3, 1-24.Mooney, R. J., Shavlik, J. W., Towell, G., &Gove, A.
(1989).
An experimental compar-ison of symbolic and connectionist learningalgorithms.
In Proceedings of the EleventhInternational Joint Conference on Artifi-cial Intelligence, pp.
775-780 Detroit, MI.Reprinted in Readings in Machine Learning,J.
W. Shavlik and T. G. Dietterich (eds.
),Morgan Kaufman, San Mateo, CA, 1990.Murphy, P. M., & Pazzani, M. J.
(1991).
ID2-of-3: Constructive induction of M-of-N con-cepts for discriminators in decision trees.In Proceedings of the Eighth InternationalWorkshop on Machine Learning, pp.
183-187 Evanston, IL.Pagallo, G., & Haussler, D. (1990).
Boolean fea-ture discovery in empirical earning.
Ma-chine Learning, 5, 71-100.Periera, F., & Shabes, Y.
(1992).
Inside-outsidereestimation from partially bracketed cor-pora.
In Proceedings of the 30th An-nual Meeting of the Association for Com-putational Linguistics, pp.
128-135 Newark,Delaware.Quinlan, J. R. (1993).
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann, San Ma-teo,CA.Quinlan, J.
(1990).
Learning logical definitionsfrom relations.
Machine Learning, 5(3),239-266.Rao, R. B., Gordon, D., & Spears, W. (1995).For every generalization action is there re-ally an equal an opposite reaction?
Analy-sis of the conservation law for generalizationperformance.
In Proceedings of the TwelfthInternational Conference on Machine Learn-ing, pp.
471-479 San Francisco, CA.
MorganKaufman.Reilly, R. G., & Sharkey, N. E.
(Eds.).
(1992).Connectionist Approaches to Natural Lan-guage Processing.
Lawrence Erlbaum andAssociates, Hilldale, NJ.Rivest, R. L.. (1987).
Learning decision lists.
Ma-chine Learning, 2(3), 229-246.Rosenblatt, F. (1962).
Principles of Neurodynam-ies.
Spartan, New York.Rumelhart, D. E., Hinton, G. E., & Williams,J.
R. (1986).
Learning internal representa-tions by error propagation.
In Rumelhart,D.
E., & McClelland, J. L.
(Eds.
), ParallelDistributed Processing, Vol.
I, pp.
318-362.MIT Press, Cambridge, MA.Salton, G., Wong, A., & Yang, C. S. (1975).A vector space model for automatic index-ing.
Communications of the Association forComputing Machinery, 18(11), 613-620.Schaffer, C. (1993).
Selecting a classificationmethod by cross-validation.
Machine Learn-ing, 13(1), 135-143.Schaffer, C. (1994).
A conservation law for gener-alization performance.
In Proceedings of theEleventh International Conference on Ma-chine Learning, pp.
259-265 San Francisco,CA.
Morgan Kaufman.Shavlik, J. W., & Dietterich, T. G.
(Eds.).
(1990).Readings in Machine Learning.
MorganKaufmann, San Mateo,CA.Shavlik, J. W., Mooney, R. J., g~ Towell, G. G.(1991).
Symbolic and neural earning algo-rithms: An experimental comparison.
Ma-chine Learning, 6, 111-143.
Reprintedin Readings in Knowledge Acquisition andLearning, B. G. Buchanan and D. C.
Wilkins(eds.
), Morgan Kaufman, San Mateo, CA,1993.Towell, G. G., Shavlik, J. W., & Noordewier,M.
O.
(1990).
Refinement of approximatedomain theories by knowledge-based artifi-cial neural networks.
In Proceedings of theEighth National Conference on Artificial In-telligence, pp.
861-866 Boston, MA.Voorhees, E., Leacock, C., & Towell, G. (1995).Learning context to disambiguate wordsenses.
In Petsche, T., Hanson, S., & Shav-lik, J.
(Eds.
), Computational Learning The-ory and Natural Learning Systems, Vol.
3,pp.
279-305.
MIT Press, Cambridge, MA.Weiss, S. M., & Kapouleas, I.
(1989).
An empir-ical comparison of pattern recognition, eu-ral nets, and machine learning classification90methods.
In Proceedings of the EleventhInternational Joint Conference on ArtificialIntelligence, pp.
781-787 Detroit, MI.Wermter, S., Riloff, E., & Scheler, G.
(Eds.).(1996).
Symbolic, Connectionist, and Sta-tistical Approaches to Learning for Natu-ral Language Processing.
Springer Verlag,Berlin.
in press.Wolpert, D. H. (1992).
On the connection betweenin-sample testing and generalization error.Complex Systems, 6, 47-94.Yarowsky, D. (1994).
Decision lists for lexicalambiguity resolution: Application to accentrestoration in Spanish and French.
In Pro-ceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics,pp.
88-95 Las Cruces, NM.Zelle, J. M., & Mooney, R. J.
(1994).
Inducing de-terministic Prolog parsers from treebanks: Amachine learning approach.
In Proceedingsof the Twelfth National Conference on Arti-ficial Intelligence, pp.
748-753 Seattle, WA.91
