Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 88?95, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsAn Expectation Maximization Approach to Pronoun ResolutionColin Cherry and Shane BergsmaDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8{colinc,bergsma}@cs.ualberta.caAbstractWe propose an unsupervised ExpectationMaximization approach to pronoun reso-lution.
The system learns from a fixedlist of potential antecedents for each pro-noun.
We show that unsupervised learn-ing is possible in this context, as the per-formance of our system is comparable tosupervised methods.
Our results indicatethat a probabilistic gender/number model,determined automatically from unlabeledtext, is a powerful feature for this task.1 IntroductionCoreference resolution is the process of determin-ing which expressions in text refer to the same real-world entity.
Pronoun resolution is the important yetchallenging subset of coreference resolution where asystem attempts to establish coreference between apronominal anaphor, such as a third-person pronounlike he, she, it, or they, and a preceding noun phrase,called an antecedent.
In the following example, apronoun resolution system must determine the cor-rect antecedent for the pronouns ?his?
and ?he.?
(1) When the president entered the arena with hisfamily, he was serenaded by a mariachi band.Pronoun resolution has applications across manyareas of Natural Language Processing, particularlyin the field of information extraction.
Resolving apronoun to a noun phrase can provide a new inter-pretation of a given sentence, giving a Question An-swering system, for example, more data to consider.Our approach is a synthesis of linguistic and sta-tistical methods.
For each pronoun, a list of an-tecedent candidates derived from the parsed corpusis presented to the Expectation Maximization (EM)learner.
Special cases, such as pleonastic, reflex-ive and cataphoric pronouns are dealt with linguisti-cally during list construction.
This allows us to trainon and resolve all third-person pronouns in a largeQuestion Answering corpus.
We learn lexicalizedgender/number, language, and antecedent probabil-ity models.
These models, tied to individual words,can not be learned with sufficient coverage from la-beled data.
Pronouns are resolved by choosing themost likely antecedent in the candidate list accord-ing to these distributions.
The resulting resolutionaccuracy is comparable to supervised methods.We gain further performance improvement by ini-tializing EM with a gender/number model derivedfrom special cases in the training data.
This modelis shown to perform reliably on its own.
We alsodemonstrate how the models learned through our un-supervised method can be used as features in a su-pervised pronoun resolution system.2 Related WorkPronoun resolution typically employs some com-bination of constraints and preferences to selectthe antecedent from preceding noun phrase candi-dates.
Constraints filter the candidate list of improb-able antecedents, while preferences encourage se-lection of antecedents that are more recent, frequent,etc.
Implementation of constraints and preferencescan be based on empirical insight (Lappin and Le-ass, 1994), or machine learning from a reference-88annotated corpus (Ge et al, 1998).
The majorityof pronoun resolution approaches have thus far re-lied on manual intervention in the resolution pro-cess, such as using a manually-parsed corpus, ormanually removing difficult non-anaphoric cases;we follow Mitkov et al?s approach (2002) with afully-automatic pronoun resolution method.
Pars-ing, noun-phrase identification, and non-anaphoricpronoun removal are all done automatically.Machine-learned, fully-automatic systems aremore common in noun phrase coreference resolu-tion, where the method of choice has been deci-sion trees (Soon et al, 2001; Ng and Cardie, 2002).These systems generally handle pronouns as a subsetof all noun phrases, but with limited features com-pared to systems devoted solely to pronouns.
Kehlerused Maximum Entropy to assign a probability dis-tribution over possible noun phrase coreference re-lationships (1997).
Like his approach, our systemdoes not make hard coreference decisions, but re-turns a distribution over candidates.The above learning approaches require anno-tated training data for supervised learning.
Cardieand Wagstaff developed an unsupervised approachthat partitions noun phrases into coreferent groupsthrough clustering (1999).
However, the partitionsthey generate for a particular document are not use-ful for processing new documents, while our ap-proach learns distributions that can be used on un-seen data.
There are also approaches to anaphoraresolution using unsupervised methods to extractuseful information, such as gender and number (Geet al, 1998), or contextual role-knowledge (Beanand Riloff, 2004).
Co-training can also leverageunlabeled data through weakly-supervised referenceresolution learning (Mu?ller et al, 2002).
As an alter-native to co-training, Ng and Cardie (2003) use EMto augment a supervised coreference system withunlabeled data.
Their feature set is quite different, asit is designed to generalize from the data in a labeledset, while our system models individual words.
Wesuspect that the two approaches can be combined.Our approach is inspired by the use of EM inbilingual word alignment, which finds word-to-wordcorrespondences between a sentence and its transla-tion.
The prominent statistical methods in this fieldare unsupervised.
Our methods are most influencedby IBM?s Model 1 (Brown et al, 1993).3 Methods3.1 Problem formulationWe will consider our training set to consist of(p, k, C) triples: one for each pronoun, where p isthe pronoun to be resolved, k is the pronoun?s con-text, and C is a candidate list containing the nouns pcould potentially be resolved to.
Initially, we take kto be the parsed sentence that p appears in.C consists of all nouns and pronouns that precedep, looking back through the current sentence and thesentence immediately preceding it.
This small win-dow may seem limiting, but we found that a cor-rect candidate appeared in 97% of such lists in alabeled development text.
Mitkov et al also limitcandidate consideration to the same window (2002).Each triple is processed with non-anaphoric pronounhandlers (Section 3.3) and linguistic filters (Sec-tion 3.4), which produce the final candidate lists.Before we pass the (p, k, C) triples to EM, wemodify them to better suit our EM formulation.There are four possibilities for the gender and num-ber of third-person pronouns in English: masculine,feminine, neutral and plural (e.g., he, she, it, they).We assume a noun is equally likely to corefer withany member of a given gender/number category, andreduce each p to a category label accordingly.
Forexample, he, his, him and himself are all labeled asmasc for masculine pronoun.
Plural, feminine andneutral pronouns are handled similarly.
We reducethe context term k to p?s immediate syntactic con-text, including only p?s syntactic parent, the parent?spart of speech, and p?s relationship to the parent, asdetermined by a dependency parser.
Incorporatingcontext only through the governing constituent wasalso done in (Ge et al, 1998).
Finally, each candi-date in C is augmented with ordering information,so we know how many nouns to ?step over?
beforearriving at a given candidate.
We will refer to this or-dering information as a candidate?s j term, for jump.Our example sentence in Section 1 would create thetwo triples shown in Figure 1, assuming the sentencebegan the document it was found in.3.2 Probability modelExpectation Maximization (Dempster et al, 1977) isa process for filling in unobserved data probabilisti-cally.
To use EM to do unsupervised pronoun reso-89his: p = masc k = p?s familyC = arena (0), president (1)he: p = masc k = serenade pC = family (0), masc (1), arena (2),president (3)Figure 1: EM input for our example sentence.j-values follow each lexical candidate.lution, we phrase the resolution task in terms of hid-den variables of an observed process.
We assumethat in each case, one candidate from the candidatelist is selected as the antecedent before p and k aregenerated.
EM?s role is to induce a probability dis-tribution over candidates to maximize the likelihoodof the (p, k) pairs observed in our training set:Pr(Dataset) =?
(p,k)?DatasetPr(p, k) (1)We can rewrite Pr(p, k) so that it uses a hidden can-didate (or antecedent) variable c that influences theobserved p and k:Pr(p, k) =?c?CPr(p, k, c) (2)Pr(p, k, c) = Pr(p, k|c)Pr(c) (3)To improve our ability to generalize to future cases,we use a na?
?ve Bayes assumption to state that thechoices of pronoun and context are conditionally in-dependent, given an antecedent.
That is, once weselect the word the pronoun represents, the pronounand its context are no longer coupled:Pr(p, k|c) = Pr(p|c)Pr(k|c) (4)We can split each candidate c into its lexical com-ponent l and its jump value j.
That is, c = (l, j).If we assume that l and j are independent, and thatp and k each depend only on the l component of c,we can combine Equations 3 and 4 to get our finalformulation for the joint probability distribution:Pr(p, k, c) = Pr(p|l)Pr(k|l)Pr(l)Pr(j) (5)The jump term j, though important when resolvingpronouns, is not likely to be correlated with any lex-ical choices in the training set.Table 1: Examples of learned pronoun probabilities.Word (l) masc fem neut plurcompany 0.03 0.01 0.95 0.01president 0.94 0.01 0.03 0.02teacher 0.19 0.71 0.09 0.01This results in four models that work together todetermine the likelihood of a given candidate.
ThePr(p|l) distribution measures the likelihood of a pro-noun given an antecedent.
Since we have collapsedthe observed pronouns into groups, this models aword?s affinity for each of the four relevant gen-der/number categories.
We will refer to this as ourpronoun model.
Pr(k|l) measures the probability ofthe syntactic relationship between a pronoun and itsparent, given a prospective antecedent for the pro-noun.
This is effectively a language model, gradinglexical choice by context.
Pr(l) measures the prob-ability that the word l will be found to be an an-tecedent.
This is useful, as some entities, such as?president?
in newspaper text, are inherently morelikely to be referenced with a pronoun.
Finally,Pr(j) measures the likelihood of jumping a givennumber of noun phrases backward to find the cor-rect candidate.
We represent these models with ta-ble look-up.
Table 1 shows selected l-value entriesin the Pr(p|l) table from our best performing EMmodel.
Note that the probabilities reflect biases in-herent in our news domain training set.Given models for the four distributions above,we can assign a probability to each candidate inC according to the observations p and k; that is,Pr(c|p, k) can be obtained by dividing Equation 5by Equation 2.
Remember that c = (l, j).Pr(c|p, k) =Pr(p|l)Pr(k|l)Pr(l)Pr(j)?c?
?C Pr(p|l?)Pr(k|l?)Pr(l?)Pr(j?
)(6)Pr(c|p, k) allows us to get fractional counts of(p, k, c) triples in our training set, as if we had actu-ally observed c co-occurring with (p, k) in the pro-portions specified by Equation 6.
This estimationprocess is effectively the E-step in EM.The M-step is conducted by redefining our mod-els according to these fractional counts.
For exam-ple, after assigning fractional counts to candidates90according to Pr(c|p, k), we re-estimate Pr(p|l) withthe following equation for a specific (p, l) pair:Pr(p|l) =N(p, l)N(l)(7)where N() counts the number of times we see agiven event or joint event throughout the training set.Given trained models, we resolve pronouns byfinding the candidate c?
that is most likely for thecurrent pronoun, that is c?
= argmaxc?CPr(c|p, k).Because Pr(p, k) is constant with respect to c,c?
= argmaxc?CPr(p, k, c).3.3 Non-anaphoric PronounsNot every pronoun in text refers anaphorically to apreceding noun phrase.
There are a frequent num-ber of difficult cases that require special attention,including pronouns that are:?
Pleonastic: pronouns that have a grammaticalfunction but do not reference an entity.
E.g.
?Itis important to observe it is raining.??
Cataphora: pronouns that reference a futurenoun phrase.
E.g.
?In his speech, the presidentpraised the workers.??
Non-noun referential: pronouns that refer to averb phrase, sentence, or implicit concept.
E.g.
?John told Mary they should buy a car.
?If we construct them na?
?vely, the candidate listsfor these pronouns will be invalid, introducing noisein our training set.
Manual handling or removalof these cases is infeasible in an unsupervised ap-proach, where the input is thousands of documents.Instead, pleonastics are identified syntactically us-ing an extension of the detector developed by Lap-pin and Leass (1994).
Roughly 7% of all pronounsin our labeled test data are pleonastic.
We detectcataphora using a pattern-based method on parsedsentences, described in (Bergsma, 2005b).
Futurenouns are only included when cataphora are iden-tified.
This approach is quite different from Lap-pin and Leass (1994), who always include all fu-ture nouns from the current sentence as candidates,with a constant penalty added to possible cataphoricresolutions.
The cataphora module identifies 1.4%of test data pronouns to be cataphoric; in each in-stance this identification is correct.
Finally, we knowof no approach that handles pronouns referring toverb phrases or implicit entities.
The unavoidableerrors for these pronouns, occurring roughly 4% ofthe time, are included in our final results.3.4 Candidate list modificationsIt would be possible for C to include every nounphrase in the current and previous sentence, but per-formance can be improved by automatically remov-ing improbable antecedents.
We use a standard set ofconstraints to filter candidates.
If a candidate?s gen-der or number is known, and does not match the pro-noun?s, the candidate is excluded.
Candidates withknown gender include other pronouns, and nameswith gendered designators (such as ?Mr.?
or ?Mrs.?
).Our parser also identifies plurals and some genderedfirst names.
We remove from C all times, dates, ad-dresses, monetary amounts, units of measurement,and pronouns identified as pleonastic.We use the syntactic constraints from BindingTheory to eliminate candidates (Haegeman, 1994).For the reflexives himself, herself, itself and them-selves, this allows immediate syntactic identificationof the antecedent.
These cases become unambigu-ous; only the indicated antecedent is included in C.We improve the quality of our training set by re-moving known noisy cases before passing the setto EM.
For example, we anticipate that sentenceswith quotation marks will be problematic, as otherresearchers have observed that quoted text requiresspecial handling for pronoun resolution (Kennedyand Boguraev, 1996).
Thus we remove pronounsoccurring in the same sentences as quotes from thelearning process.
Also, we exclude triples wherethe constraints removed all possible antecedents, orwhere the pronoun was deemed to be pleonastic.Performing these exclusions is justified for training,but in testing we state results for all pronouns.3.5 EM initializationEarly in the development of this system, we wereimpressed with the quality of the pronoun modelPr(p|l) learned by EM.
However, we found we couldconstruct an even more precise pronoun model forcommon words by examining unambiguous cases inour training data.
Unambiguous cases are pronounshaving only one word in their candidate list C. Thiscould be a result of the preprocessors described in91Sections 3.3 and 3.4, or the pronoun?s position inthe document.
A PrU (p|l) model constructed fromonly unambiguous examples covers far fewer wordsthan a learned model, but it rarely makes poor gen-der/number choices.
Furthermore, it can be obtainedwithout EM.
Training on unambiguous cases is sim-ilar in spirit to (Hindle and Rooth, 1993).
We foundin our development and test sets that, after applyingfilters, roughly 9% of pronouns occur with unam-biguous antecedents.When optimizing a probability function that is notconcave, the EM algorithm is only guaranteed tofind a local maximum; therefore, it can be helpfulto start the process near the desired end-point in pa-rameter space.
The unambiguous pronoun modeldescribed above can provide such a starting point.When using this initializer, we perform our ini-tial E-step by weighting candidates according toPrU (p|l), instead of weighting them uniformly.
Thisbiases the initial E-step probabilities so that a strongindication of the gender/number of a candidate fromunambiguous cases will either boost the candidate?schances or remove it from competition, dependingon whether or not the predicted category matchesthat of the pronoun being resolved.To deal with the sparseness of the PrU (p|l) dis-tribution, we use add-1 smoothing (Jeffreys, 1961).The resulting effect is that words with few unam-biguous occurrences receive a near-uniform gen-der/number distribution, while those observed fre-quently will closely match the observed distribution.During development, we also tried clever initializersfor the other three models, including an extensivelanguage model initializer, but none were able to im-prove over PrU (p|l) alone.3.6 Supervised extensionEven though we have justified Equation 5 with rea-sonable independence assumptions, our four mod-els may not be combined optimally for our pronounresolution task, as the models are only approxima-tions of the true distributions they are intended torepresent.
Following the approach in (Och and Ney,2002), we can view the right-hand-side of Equa-tion 5 as a special case of:exp(?1 log Pr(p|l) + ?2 log Pr(k|l)+?3 log Pr(l) + ?4 log Pr(j))(8)where ?i : ?i = 1.
Effectively, the log proba-bilities of our models become feature functions ina log-linear model.
When labeled training data isavailable, we can use the Maximum Entropy princi-ple (Berger et al, 1996) to optimize the ?
weights.This provides us with an optional supervised ex-tension to the unsupervised system.
Given a smallset of data that has the correct candidates indicated,such as the set we used while developing our unsu-pervised system, we can re-weight the final modelsprovided by EM to maximize the probability of ob-serving the indicated candidates.
To this end, wefollow the approach of (Och and Ney, 2002) veryclosely, including their handling of multiple correctanswers.
We use the limited memory variable met-ric method as implemented in Malouf?s maximumentropy package (2002) to set our weights.4 Experimental Design4.1 Data setsWe used two training sets in our experiments, bothdrawn from the AQUAINT Question Answeringcorpus (Vorhees, 2002).
For each training set, wemanually labeled pronoun antecedents in a corre-sponding key containing a subset of the pronounsin the set.
These keys are drawn from a collectionof complete documents.
For each document, all pro-nouns are included.
With the exception of the super-vised extension, the keys are used only to validatethe resolution decisions made by a trained system.Further details are available in (Bergsma, 2005b).The development set consists of 333,000 pro-nouns drawn from 31,000 documents.
The devel-opment key consists of 644 labeled pronouns drawnfrom 58 documents; 417 are drawn from sentenceswithout quotation marks.
The development set andits key were used to guide us while designing theprobability model, and to fine-tune EM and smooth-ing parameters.
We also use the development key aslabeled training data for our supervised extension.The test set consists of 890,000 pronouns drawnfrom 50,000 documents.
The test key consists of1209 labeled pronouns drawn from 118 documents;892 are drawn from sentences without quotationmarks.
All of the results reported in Section 5 aredetermined using the test key.924.2 Implementation DetailsTo get the context values and implement the syntac-tic filters, we parsed our corpora with Minipar (Lin,1994).
Experiments on the development set indi-cated that EM generally began to overfit after 2 it-erations, so we stop EM after the second iteration,using the models from the second M-step for test-ing.
During testing, ties in likelihood are broken bytaking the candidate closest to the pronoun.The EM-produced models need to be smoothed,as there will be unseen words and unobserved (p, l)or (k, l) pairs in the test set.
This is because prob-lematic cases are omitted from the training set, whileall pronouns are included in the key.
We han-dle out-of-vocabulary events by replacing words orcontext-values that occur only once during trainingwith a special unknown symbol.
Out-of-vocabularyevents encountered during testing are also treatedas unknown.
We handle unseen pairs with additivesmoothing.
Instead of adding 1 as in Section 3.5, weadd ?p = 0.00001 for (k, l) pairs, and ?w = 0.001for (p, l) pairs.
These ?
values were determined ex-perimentally with the development key.4.3 Evaluation schemeWe evaluate our work in the context of a fully auto-matic system, as was done in (Mitkov et al, 2002).Our evaluation criteria is similar to their resolutionetiquette.
We define accuracy as the proportion ofpronouns correctly resolved, either to any coreferentnoun phrase in the candidate list, or to the pleonas-tic category, which precludes resolution.
Systemsthat handle and state performance for all pronounsin unrestricted text report much lower accuracy thanmost approaches in the literature.
Furthermore, au-tomatically parsing and pre-processing texts causesconsistent degradation in performance, regardless ofthe accuracy of the pronoun resolution algorithm.
Tohave a point of comparison to other fully-automaticapproaches, note the resolution etiquette score re-ported in (Mitkov et al, 2002) is 0.582.5 Results5.1 Validation of unsupervised methodThe key concern of our work is whether enoughuseful information is present in the pronoun?s cat-egory, context, and candidate list for unsupervisedlearning of antecedents to occur.
To that end, ourfirst set of experiments compare the pronoun resolu-tion accuracy of our EM-based solutions to that of aprevious-noun baseline on our test key.
The resultsare shown in Table 2.
The columns split the resultsinto three cases: all pronouns with no exceptions;all cases where the pronoun was found in a sentencecontaining no quotation marks (and therefore resem-bling the training data provided to EM); and finallyall pronouns excluded by the second case.
We com-pare the following methods:1.
Previous noun: Pick the candidate from the fil-tered list with the lowest j value.2.
EM, no initializer: The EM algorithm trainedon the test set, starting from a uniform E-step.3.
Initializer, no EM: A model that ranks candi-dates using only a pronoun model built fromunambiguous cases (Section 3.5).4.
EM w/ initializer: As in (2), but using the ini-tializer in (3) for the first E-step.5.
Maxent extension: The models produced by(4) are used as features in a log-linear modeltrained on the development key (Section 3.6).6.
Upper bound: The percentage of cases with acorrect answer in the filtered candidate list.For a reference point, picking the previous noun be-fore applying any of our candidate filters receives anaccuracy score of 0.281 on the ?All?
task.Looking at the ?All?
column in Table 2, we seeEM can indeed learn in this situation.
Starting fromuniform parameters it climbs from a 40% baselineto a 60% accurate model.
However, the initializercan do slightly better with precise but sparse gen-der/number information alone.
As we hoped, com-bining the initializer and EM results in a statisticallysignificant1 improvement over EM with a uniformstarting point, but it is not significantly better thanthe initializer alone.
The advantage of the EM pro-cess is that it produces multiple models, which canbe re-weighted with maximum entropy to reach ourhighest accuracy, roughly 67%.
The ?
weights thatachieve this score are shown in Table 3.Maximum entropy leaves the pronoun modelPr(p|l) nearly untouched and drastically reduces the1Significance is determined throughout Section 5 using Mc-Nemar?s test with a significance level ?
= 0.05.93Table 2: Accuracy for all cases, all excluding sen-tences with quotes, and only sentences with quotes.Method All No?
?
Only?
?1 Previous noun 0.397 0.399 0.3912 EM, no initializer 0.610 0.632 0.5493 Initializer, no EM 0.628 0.642 0.5874 EM w/ initializer 0.632 0.663 0.5465 Maxent extension 0.669 0.696 0.5936 Upper bound 0.838 0.868 0.754influence of all other models (Table 3).
This, com-bined with the success of the initializer alone, leadsus to believe that a strong notion of gender/numberis very important in this task.
Therefore, we im-plemented EM with several models that used onlypronoun category, but none were able to surpass theinitializer in accuracy on the test key.
One factorthat might help explain the initializer?s success isthat despite using only a PrU (p|l) model, the ini-tializer also has an implicit factor resembling a Pr(l)model: when two candidates agree with the categoryof the pronoun, add-1 smoothing ensures the morefrequent candidate receives a higher probability.As was stated in Section 3.4, sentences with quo-tations were excluded from the learning process be-cause the presence of a correct antecedent in the can-didate list was less frequent in these cases.
This isvalidated by the low upper bound of 0.754 in theonly-quote portion of the test key.
We can see thatall methods except for the previous noun heuris-tic score noticeably better when ignoring those sen-tences that contain quotation marks.
In particular,the difference between our three unsupervised solu-tions ((2), (3) and (4)) are more pronounced.
Muchof the performance improvements that correspondto our model refinements are masked in the overalltask because adding the initializer to EM does notimprove EM?s performance on quotes at all.
Devel-oping a method to construct more robust candidatelists for quotations could improve our performanceon these cases, and greatly increase the percentageof pronouns we are training on for a given corpus.Table 3: Weights set by maximum entropy.Model Pr(p|l) Pr(k|l) Pr(l) Pr(j)Lambda 0.931 0.056 0.070 0.167Table 4: Comparison to SVM.Method AccuracyPrevious noun 0.398EM w/ initializer 0.664Maxent extension 0.708SVM 0.7145.2 Comparison to supervised systemWe put our results in context by comparing ourmethods to a recent supervised system.
The compar-ison system is an SVM that uses 52 linguistically-motivated features, including probabilistic gen-der/number information obtained through webqueries (Bergsma, 2005a).
The SVM is trainedwith 1398 separate labeled pronouns, the same train-ing set used in (Bergsma, 2005a).
This data isalso drawn from the news domain.
Note the su-pervised system was not constructed to handle allpronoun cases, so non-anaphoric pronouns were re-moved from the test key and from the candidate listsin the test key to ensure a fair comparison.
As ex-pected, this removal of difficult cases increases theperformance of our system on the test key (Table 4).Also note there is no significant difference in per-formance between our supervised extension and theSVM.
The completely unsupervised EM system per-forms worse, but with only a 7% relative reductionin performace compared to the SVM; the previousnoun heuristic shows a 44% reduction.5.3 Analysis of upper boundIf one accounts for the upper bound in Table 2, ourmethods do very well on those cases where a cor-rect answer actually appears in the candidate list: thebest EM solution scores 0.754, and the supervisedextension scores 0.800.
A variety of factors result inthe 196 candidate lists that do not contain a true an-tecedent.
21% of these errors arise from our limitedcandidate window (Section 3.1).
Incorrect pleonas-tic detection accounts for another 31% while non-94noun referential pronouns cause 25% (Section 3.3).Linguistic filters (Section 3.4) account for most ofthe remainder.
An improvement in any of these com-ponents would result in not only higher final scores,but cleaner EM training data.6 ConclusionWe have demonstrated that unsupervised learning ispossible for pronoun resolution.
We achieve accu-racy of 63% on an all-pronoun task, or 75% whena true antecedent is available to EM.
There is nowmotivation to develop cleaner candidate lists andstronger probability models, with the hope of sur-passing supervised techniques.
For example, incor-porating antecedent context, either at the sentenceor document level, may boost performance.
Further-more, the lexicalized models learned in our system,especially the pronoun model, are potentially pow-erful features for any supervised pronoun resolutionsystem.ReferencesDavid L. Bean and Ellen Riloff.
2004.
Unsupervised learningof contextual role knowledge for coreference resolution.
InHLT-NAACL, pages 297?304.Adam L. Berger, Stephen A. Della Pietra, and Vincent J. DellaPietra.
1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22(1):39?71.Shane Bergsma.
2005a.
Automatic acquisition of gender infor-mation for anaphora resolution.
In Proceedings of the 18thConference of the Canadian Society for Computational Intel-ligence (Canadian AI 2005), pages 342?353, Victoria, BC.Shane Bergsma.
2005b.
Corpus-based learning for pronom-inal anaphora resolution.
Master?s thesis, Departmentof Computing Science, University of Alberta, Edmonton.http://www.cs.ualberta.ca/?bergsma/Pubs/thesis.pdf.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 19(2):263?312.Claire Cardie and Kiri Wagstaff.
1999.
Noun phrase corefer-ence as clustering.
In Proceedings of the 1999 Joint SIGDATConference on Empirical Methods in Natural Language Pro-cessing and Very Large Corpora, pages 82?89.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Maxi-mum likelihood from incomplete data via the EM algorithm.Journal of the Royal Statistical Society, 39(1):1?38.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A statisticalapproach to anaphora resolution.
In Proceedings of the SixthWorkshop on Very Large Corpora, pages 161?171.L.
Haegeman.
1994.
Introduction to Government & Bindingtheory: Second Edition.
Basil Blackwell, Cambridge, UK.Donald Hindle and Mats Rooth.
1993.
Structural ambiguityand lexical relations.
Computational Linguistics, 19(1):103?120.Harold Jeffreys, 1961.
Theory of Probability, chapter 3.23.
Ox-ford: Clarendon Press, 3rd edition.Andrew Kehler.
1997.
Probabilistic coreference in informa-tion extraction.
In Proceedings of the Second Conference onEmpirical Methods in Natural Language Processing, pages163?173.Christopher Kennedy and Branimir Boguraev.
1996.
Anaphorafor everyone: Pronominal anaphora resolution without aparser.
In Proceedings of the 16th Conference on Compu-tational Linguistics, pages 113?118.Shalom Lappin and Herbert J. Leass.
1994.
An algorithm forpronominal anaphora resolution.
Computational Linguis-tics, 20(4):535?561.Dekang Lin.
1994.
Principar - an efficient, broad-coverage,principle-based parser.
In Proceedings of COLING-94,pages 42?48, Kyoto, Japan.Robert Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedings of theSixth Conference on Natural Language Learning (CoNLL-2002), pages 49?55.Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002.A new, fully automatic version of Mitkov?s knowledge-poorpronoun resolution method.
In Proceedings of the ThirdInternational Conference on Computational Linguistics andIntelligent Text Processing, pages 168?186.Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002.
Ap-plying co-training to reference resolution.
In Proceedingsof the 40th Annual Meeting of the Association for Computa-tional Linguistics, pages 352?359.Vincent Ng and Claire Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Proceedings ofthe 40th Annual Meeting of the Association for Computa-tional Linguistics, pages 104?111.Vincent Ng and Claire Cardie.
2003.
Weakly supervised nat-ural language learning without redundant views.
In HLT-NAACL 2003: Proceedings of the Main Conference, pages173?180.Franz J. Och and Hermann Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machine trans-lation.
In Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics, pages 295?302,Philadelphia, PA, July.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.2001.
A machine learning approach to coreference resolu-tion of noun phrases.
Computational Linguistics, 27(4):521?544.Ellen Vorhees.
2002.
Overview of the TREC 2002 question an-swering track.
In Proceedings of the Eleventh Text REtrievalConference (TREC).95
