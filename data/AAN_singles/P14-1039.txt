Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 413?423,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsThat?s Not What I Meant!Using Parsers to Avoid Structural Ambiguities in Generated TextManjuan Duan and Michael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210, USA{duan,mwhite}@ling.osu.eduAbstractWe investigate whether parsers can beused for self-monitoring in surface real-ization in order to avoid egregious errorsinvolving ?vicious?
ambiguities, namelythose where the intended interpretationfails to be considerably more likely thanalternative ones.
Using parse accuracyin a simple reranking strategy for self-monitoring, we find that with a state-of-the-art averaged perceptron realizationranking model, BLEU scores cannot beimproved with any of the well-knownTreebank parsers we tested, since theseparsers too often make errors that humanreaders would be unlikely to make.
How-ever, by using an SVM ranker to combinethe realizer?s model score together withfeatures from multiple parsers, includingones designed to make the ranker more ro-bust to parsing mistakes, we show that sig-nificant increases in BLEU scores can beachieved.
Moreover, via a targeted man-ual analysis, we demonstrate that the SVMreranker frequently manages to avoid vi-cious ambiguities, while its ranking errorstend to affect fluency much more oftenthan adequacy.1 IntroductionRajkumar & White (2011; 2012) have recentlyshown that some rather egregious surface realiza-tion errors?in the sense that the reader wouldlikely end up with the wrong interpretation?canbe avoided by making use of features inspired bypsycholinguistics research together with an other-wise state-of-the-art averaged perceptron realiza-tion ranking model (White and Rajkumar, 2009),as reviewed in the next section.
However, one isapt to wonder: could one use a parser to checkwhether the intended interpretation is easy to re-cover, either as an alternative or to catch additionalmistakes?
Doing so would be tantamount to self-monitoring in Levelt?s (1989) model of languageproduction.Neumann & van Noord (1992) pursued the ideaof self-monitoring for generation in early workwith reversible grammars.
As Neumann & vanNoord observed, a simple, brute-force way to gen-erate unambiguous sentences is to enumerate pos-sible realizations of an input logical form, thento parse each realization to see how many inter-pretations it has, keeping only those that havea single reading; they then went on to devise amore efficient method of using self-monitoring toavoid generating ambiguous sentences, targeted tothe ambiguous portion of the output.
We mightquestion, however, whether it is really possibleto avoid ambiguity entirely in the general case,since Abney (1996) and others have argued thatnearly every sentence is potentially ambiguous,though we (as human comprehenders) may notnotice the ambiguities if they are unlikely.
Tak-ing up this issue, Khan et al (2008)?building onChantree et al?s (2006) approach to identifying?innocuous?
ambiguities?conducted several ex-periments to test whether ambiguity could be bal-anced against length or fluency in the context ofgenerating referring expressions involving coordi-nate structures.
Though Khan et al?s study waslimited to this one kind of structural ambiguity,they do observe that generating the brief variantswhen the intended interpretation is clear instanti-ates Van Deemter?s (2004) general strategy of onlyavoiding vicious ambiguities?that is, ambigui-ties where the intended interpretation fails to beconsiderably more likely than any other distractorinterpretations?rather than trying to avoid all am-biguities.In this paper, we investigate whether Neumann& van Noord?s brute-force strategy for avoid-413ing ambiguities in surface realization can be up-dated to only avoid vicious ambiguities, extend-ing (and revising) Van Deemter?s general strategyto all kinds of structural ambiguity, not just theone investigated by Khan et al To do so?in anutshell?we enumerate an n-best list of realiza-tions and rerank them if necessary to avoid viciousambiguities, as determined by one or more auto-matic parsers.
A potential obstacle, of course, isthat automatic parsers may not be sufficiently rep-resentative of human readers, insofar as errors thata parser makes may not be problematic for humancomprehension; moreover, parsers are rarely suc-cessful in fully recovering the intended interpreta-tion for sentences of moderate length, even withcarefully edited news text.
Consequently, we ex-amine two reranking strategies, one a simple base-line approach and the other using an SVM reranker(Joachims, 2002).Our simple reranking strategy for self-monitoring is to rerank the realizer?s n-best listby parse accuracy, preserving the original order incase of ties.
In this way, if there is a realization inthe n-best list that can be parsed more accuratelythan the top-ranked realization?even if theintended interpretation cannot be recovered with100% accuracy?it will become the preferredoutput of the combined realization-with-self-monitoring system.
With this simple rerankingstrategy and each of three different Treebankparsers, we find that it is possible to improveBLEU scores on Penn Treebank development datawith White & Rajkumar?s (2011; 2012) baselinegenerative model, but not with their averagedperceptron model.
In inspecting the results ofreranking with this strategy, we observe that whileit does sometimes succeed in avoiding egregiouserrors involving vicious ambiguities, commonparsing mistakes such as PP-attachment errorslead to unnecessarily sacrificing conciseness orfluency in order to avoid ambiguities that would beeasily tolerated by human readers.
Therefore, todevelop a more nuanced self-monitoring rerankerthat is more robust to such parsing mistakes, wetrained an SVM using dependency precision andrecall features for all three parses, their n-bestparsing results, and per-label precision and recallfor each type of dependency, together with therealizer?s normalized perceptron model score asa feature.
With the SVM reranker, we obtain asignificant improvement in BLEU scores overWhite & Rajkumar?s averaged perceptron modelon both development and test data.
Additionally,in a targeted manual analysis, we find that in caseswhere the SVM reranker improves the BLEUscore, improvements to fluency and adequacy areroughly balanced, while in cases where the BLEUscore goes down, it is mostly fluency that is madeworse (with reranking yielding an acceptableparaphrase roughly one third of the time in bothcases).The paper is structured as follows.
In Sec-tion 2, we review the realization ranking mod-els that serve as a starting point for the paper.In Section 3, we report on our experiments withthe simple reranking strategy, including a discus-sion of the ways in which this method typicallyfails.
In Section 4, we describe how we trained anSVM reranker and report our results using BLEUscores (Papineni et al, 2002).
In Section 5, wepresent a targeted manual analysis of the devel-opment set sentences with the greatest change inBLEU scores, discussing both successes and er-rors.
In Section 6, we briefly review related workon broad coverage surface realization.
Finally, inSection 7, we sum up and discuss opportunities forfuture work in this direction.2 BackgroundWe use the OpenCCG1surface realizer for the ex-periments reported in this paper.
The OpenCCGrealizer generates surface strings for input seman-tic dependency graphs (or logical forms) using achart-based algorithm (White, 2006) for Combi-natory Categorial Grammar (Steedman, 2000) to-gether with a ?hypertagger?
for probabilisticallyassigning lexical categories to lexical predicatesin the input (Espinosa et al, 2008).
An exam-ple input appears in Figure 1.
In the figure,nodes correspond to discourse referents labeledwith lexical predicates, and dependency relationsbetween nodes encode argument structure (goldstandard CCG lexical categories are also shown);note that semantically empty function words suchas infinitival-to are missing.
The grammar is ex-tracted from a version of the CCGbank (Hocken-maier and Steedman, 2007) enhanced for realiza-tion; the enhancements include: better analyses ofpunctuation (White and Rajkumar, 2008); less er-ror prone handling of named entities (Rajkumar etal., 2009); re-inserting quotes into the CCGbank;1http://openccg.sf.net414aa1heh3he h2<Det><Arg0> <Arg1><TENSE>pres<NUM>sg<Arg0>w1 want.01m1<Arg1><GenRel><Arg1><TENSE>presp1pointh1have.03make.03<Arg0>s[b]\np/npnp/nnpns[dcl]\np/nps[dcl]\np/(s[to]\np)npFigure 1: Example OpenCCG semantic depen-dency input for he has a point he wants to make,with gold standard lexical categories for each nodeand assignment of consistent semantic roles acrossdiathesis alternations (Boxwell and White, 2008),using PropBank (Palmer et al, 2005).To select preferred outputs from the chart, weuse White & Rajkumar?s (2009; 2012) realizationranking model, recently augmented with a large-scale 5-gram model based on the Gigaword cor-pus.
The ranking model makes choices addressingall three interrelated sub-tasks traditionally con-sidered part of the surface realization task in natu-ral language generation research (Reiter and Dale,2000; Reiter, 2010): inflecting lemmas with gram-matical word forms, inserting function words andlinearizing the words in a grammatical and natu-ral order.
The model takes as its starting point twoprobabilistic models of syntax that have been de-veloped for CCG parsing, Hockenmaier & Steed-man?s (2002) generative model and Clark & Cur-ran?s (2007) normal-form model.
Using the aver-aged perceptron algorithm (Collins, 2002), White& Rajkumar (2009) trained a structured predic-tion ranking model to combine these existing syn-tactic models with several n-gram language mod-els.
This model improved upon the state-of-the-artin terms of automatic evaluation scores on held-out test data, but nevertheless an error analysis re-vealed a surprising number of word order, func-tion word and inflection errors.
For each kind oferror, subsequent work investigated the utility ofemploying more linguistically motivated featuresto improve the ranking model.To improve word ordering decisions, White &Rajkumar (2012) demonstrated that incorporat-ing a feature into the ranker inspired by Gib-son?s (2000) dependency locality theory can de-liver statistically significant improvements in au-tomatic evaluation scores, better match the distri-butional characteristics of sentence orderings, andsignificantly reduce the number of serious order-ing errors (some involving vicious ambiguities) asconfirmed by a targeted human evaluation.
Sup-porting Gibson?s theory, comprehension and cor-pus studies have found that the tendency to min-imize dependency length has a strong influenceon constituent ordering choices; see Temperley(2007) and Gildea and Temperley (2010) for anoverview.Table 1 shows examples from White and Rajku-mar (2012) of how the dependency length feature(DEPLEN) affects the OpenCCG realizer?s outputeven in comparison to a model (DEPORD) witha rich set of discriminative syntactic and depen-dency ordering features, but no features directlytargeting relative weight.
In wsj 0015.7, the de-pendency length model produces an exact match,while the DEPORD model fails to shift the shorttemporal adverbial next year next to the verb, leav-ing a confusingly repetitive this year next year atthe end of the sentence.
Note how shifting nextyear from its canonical VP-final position to appearnext to the verb shortens its dependency lengthconsiderably, while barely lengthening the depen-dency to based on; at the same time, it avoidsambiguity in what next year is modifying.
Inwsj 0020.1 we see the reverse case: the depen-dency length model produces a nearly exact matchwith just an equally acceptable inversion of closelywatching, keeping the direct object in its canoni-cal position.
By contrast, the DEPORD model mis-takenly shifts the direct object South Korea, Tai-wan and Saudia Arabia to the end of the sentencewhere it is difficult to understand following twovery long intervening phrases.With function words, Rajkumar and White(2011) showed that they could improve upon theearlier model?s predictions for when to employthat-complementizers using features inspired byJaeger?s (2010) work on using the principle ofuniform information density, which holds thathuman language use tends to keep informationdensity relatively constant in order to optimizecommunicative efficiency.
In news text, com-415wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections madeuntil Dec. 31 of this year .DEPLEN [same]DEPORD the exact amount of the refund will be determined based on actual collections made until Dec.31 of this year next year .wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan andSaudi Arabia from a list of countries it is closely watching for allegedly failing to honor U.S.patents , copyrights and other intellectual-property rights .DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan andSaudi Arabia from a list of countries it is watching closely for allegedly failing to honor U.S.patents , copyrights and other intellectual-property rights .DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.patents , copyrights and other intellectual-property rights , claiming some success in its tradediplomacy , South Korea , Taiwan and Saudi Arabia .Table 1: Examples of realized output for full models with and without the dependency length feature(White and Rajkumar, 2012)plementizers are left out two times out of three,but in some cases the presence of that is cru-cial to the interpretation.
Generally, inserting acomplementizer makes the onset of a complementclause more predictable, and thus less informa-tion dense, thereby avoiding a potential spike ininformation density that is associated with com-prehension difficulty.
Rajkumar & White?s exper-iments confirmed the efficacy of the features basedon Jaeger?s work, including information density?based features, in a local classification model.2Their experiments also showed that the improve-ments in prediction accuracy apply to cases inwhich the presence of a that-complementizer ar-guably makes a substantial difference to fluencyor intelligiblity.
For example, in (1), the pres-ence of that avoids a local ambiguity, helping thereader to understand that for the second month ina row modifies the reporting of the shortage; with-out that, it is very easy to mis-parse the sentenceas having for the second month in a row modifyingthe saying event.
(1) He said that/??
for the second month in a row,food processors reported a shortage of nonfatdry milk.
(PTB WSJ0036.61)Finally, to reduce the number of subject-verbagreement errors, Rajkumar and White (2010) ex-tended the earlier model with features enabling itto make correct verb form choices in sentencesinvolving complex coordinate constructions and2Note that the features from the local classification modelfor that-complementizer choice have not yet been incorpo-rated into OpenCCG?s global realization ranking model, andthus do not inform the baseline realization choices in thiswork.with expressions such as a lot of where the correctchoice is not determined solely by the head noun.They also improved animacy agreement with rela-tivizers, reducing the number of errors where thator which was chosen to modify an animate nounrather than who or whom (and vice-versa), whilealso allowing both choices where corpus evidencewas mixed.3 Simple Reranking3.1 MethodsWe ran two OpenCCG surface realization modelson the CCGbank dev set (derived from Section 00of the Penn Treebank) and obtained n-best (n =10) realizations.
The first one is the baseline gen-erative model (hereafter, generative model) usedin training the averaged perceptron model.
Thismodel ranks realizations using the product of theHockenmaier syntax model, n-gram models overwords, POS tags and supertags in the training sec-tions of the CCGbank, and the large-scale 5-grammodel from Gigaword.
The second one is theaveraged perceptron model (hereafter, perceptronmodel), which uses all the features reviewed inSection 2.
In order to experiment with multipleparsers, we used the Stanford dependencies (deMarneffe et al, 2006), obtaining gold dependen-cies from the gold-standard PTB parses and auto-matic dependencies from the automatic parses ofeach realization.
Using dependencies allowed usto measure parse accuracy independently of wordorder.
We chose the Berkeley parser (Petrov etal., 2006), Brown parser (Charniak and Johnson,2005) and Stanford parser (Klein and Manning,2003) to parse the realizations generated by the416Berkeley Brown StanfordNo reranking 87.93 87.93 87.93Labeled 87.77 87.87 87.12Unlabeled 87.90 87.97 86.97Table 2: Devset BLEU scores for simple rankingon top of n-best perceptron model realizationsThat?s Not What I Meant!Using Parsers to Avoid Structural Ambiguities in Generated TextManjuan Duan and Michael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210, USA{duan,mwhite}@ling.osu.edu.
.
.
is propelling the region toward economic integrationauxdobjpreppobj(a) gold dependency.
.
.
is propelling toward economic integration the regionauxpobjprepdobj(b) simple ranker.
.
.
is propelling the region toward economic integrationauxdobjpreppobj(c) perceptron bestFigure 1: Example parsing mistake in PP-attachment (wsj 0043.1)AbstractWe investigate .
.
.Figure 2: xa ple parsing istake in PP-attach ent ( sj 0043.1)two realization models and calculated precision,recall and F1of the dependencies for each realiza-tion by comparing them with the gold dependen-cies.
We then ranked the realizations by their F1score of parse accuracy, keeping the original rank-ing in case of ties.
We also tried using unlabeled(and unordered) dependencies, in order to possi-bly make better use of parses that were close tobeing correct.
In this setting, as long as the rightpair of tokens occur in a dependency relation, itwas counted as a correctly recovered dependency.3.2 ResultsSimple ranking with the Berkeley parser of thegenerative model?s n-best realizations raised theBLEU score from 85.55 to 86.07, well belowthe averaged perceptron model?s BLEU score of87.93.
However, as shown in Table 2, none of theparsers yielded significant improvements on thetop of the perceptron model.Inspecting the results of simple ranking re-vealed that while simple ranking did success-fully avoid vicious ambiguities in some cases,parser mistakes with PP-attachments, noun-nouncompounds and coordinate structures too oftenblocked the gold realization from emerging on top.To illustrate, Figure 2 shows an example with aPP-attachment mistake.
In the figure, the key golddependencies of the reference sentence are shownin (a), the dependencies of the realization selectedby the simple ranker are shown in (b), and the de-pendencies of the realization selected by the per-ceptron ranker (same as gold) appear in (c), withthe parsing mistake indicated by the dashed line.The simple ranker ends up choosing (b) as the bestrealization because it has the most accurate parsecompared to the reference sentence, given the mis-take with (c).Other common parse errors are illustrated inFigure 3.
Here, (b) ends up getting chosen by thesimple ranker as the realization with the most ac-curate parse given the failures in (c), where the ad-ditional technology, personnel training is mistak-enly analyzed as one noun phrase, a reading un-likely to be considered by human readers.In sum, although simple ranking helps to avoidvicious ambiguity in some cases, the overall re-sults of simple ranking are no better than the per-ceptron model (according to BLEU, at least), asparse failures that are not reflective of human in-tepretive tendencies too often lead the ranker tochoose dispreferred realizations.
As such, we turnnow to a more nuanced model for combining theresults of multiple parsers in a way that is less sen-sitive to such parsing mistakes, while also lettingthe perceptron model have a say in the final rank-ing.4 Reranking with SVMs4.1 MethodsSince different parsers make different errors, weconjectured that dependencies in the intersectionof the output of multiple parsers may be more re-liable and thus may more reliably reflect humancomprehension preferences.
Similarly, we conjec-tured that large differences in the realizer?s percep-tron model score may more reliably reflect humanfluency preferences than small ones, and thus wecombined this score with features for parser accu-racy in an SVM ranker.
Additionally, given thatparsers may more reliably recover some kinds ofdependencies than others, we included features foreach dependency type, so that the SVM rankermight learn how to weight them appropriately.Finally, since the differences among the n-bestparses reflect the least certain parsing decisions,417the additional technology, personnel training and promotional effortsdetamodnnconjccconjamod(a) gold dependencythe additional technology, training personnel and promotional effortsdetamodnnconjccconjamod(b) simple rankerthe additional technology, personnel training and promotional effortsdetamodnndepccconjamod(c) perceptron bestFigure 2: Example parsing mistakes in a noun-noun compound and a coordinate structure (wsj 0085.45)Figure 3: xa ple parsing istakes in a noun-noun co pound and a coordinate structure ( sj 0085.45)and thus ones that may require more commonsense inference that is easy for humans but notmachines, we conjectured that including featuresfrom the n-best parses may help to better matchhuman performance.
In more detail, we made useof the following feature classes for each candidaterealization:perceptron model score the score from the real-izer?s model, normalized to [0,1] for the real-izations in the n-best listprecision and recall labeled and unlabeled preci-sion and recall for each parser?s best parseper-label precision and recall (dep) precisionand recall for each type of dependencyobtained from each parser?s best parse (usingzero if not defined for lack of predicted orgold dependencies with a given label)n-best precision and recall (nbest) labeled andunlabeled precision and recall for eachparser?s top five parses, along with the samefeatures for the most accurate of these parsesIn training, we used the BLEU scores of eachrealization compared with its reference sentenceto establish a preference order over pairs of candi-date realizations, assuming that the original corpussentences are generally better than related alterna-tives, and that BLEU can somewhat reliably pre-dict human preference judgments.We trained the SVM ranker (Joachims, 2002)with a linear kernel and chose the hyper-parameterc, which tunes the trade-off between training errorand margin, with 6-fold cross-validation on the de-vset.
We trained different models to investigate thecontribution made by different parsers and differ-ent types of features, with the perceptron modelscore included as a feature in all models.
For eachparser, we trained a model with its overall preci-sion and recall features, as shown at the top of Ta-ble 3.
Then we combined these three models to geta new model (Bkl+Brw+St in the table) .
Next,to this combined model we separately added (i)the per-label precision and recall features from allthe parsers (BBS+dep), and (ii) the n-best featuresfrom the parsers (BBS+nbest).
The full model(BBS+dep+nbest) includes all the features listedabove.
Finally, since the Berkeley parser yieldedthe best results on its own, we also tested mod-els using all the feature classes but only using thisparser by itself.4.2 ResultsTable 3 shows the results of different SVM rank-ing models on the devset.
We calculated signifi-cance using paired bootstrap resampling (Koehn,2004).3Both the per-label precision & recall fea-3Kudos to Kevin Gimpel for making his implementa-tion available: http://www.ark.cs.cmu.edu/MT/paired_bootstrap_v13a.tar.gz418BLEU sig.perceptron baseline 87.93 ?Berkeley 88.45 *Brown 88.34Stanford 88.18Bkl+Brw+St 88.44 *BBS+dep 88.63 **BBS+nbest 88.60 **BBS+dep+nbest 88.73 **Bkl+dep 88.63 **Bkl+nbest 88.48 *Bkl +dep+nbest 88.68 **Table 3: Devset results of SVM ranking on topof perceptron model.
Significance codes: ??
forp < 0.05, ?
for p < 0.1.BLEU sig.perceptron baseline 86.94 ?BBS+dep+nbest 87.64 **Table 4: Final test results of SVM ranking on topof perceptron model.
Significance codes: ??
forp < 0.05, ?
for p < 0.1.tures and the n-best parse features contributed toachieving a significant improvement compared tothe perceptron model.
Somewhat surprisingly, theBerkeley parser did as well as all three parsers us-ing just the overall precision and recall features,but not quite as well using all features.
The com-plete model, BBS+dep+nbest, achieved a BLEUscore of 88.73, significantly improving upon theperceptron model (p < 0.02).
We then confirmedthis result on the final test set, Section 23 of theCCGbank, as shown in Table 4 (p < 0.02 as well).5 Analysis and Discussion5.1 Targeted Manual AnalysisIn order to gain a better understanding of the suc-cesses and failures of our SVM ranker, we presenthere a targeted manual analysis of the develop-ment set sentences with the greatest change inBLEU scores, carried out by the second author(a native speaker).
In this analysis, we considerwhether the reranked realization improves uponor detracts from realization quality?in terms ofadequacy, fluency, both or neither?along witha linguistic categorization of the differences be-tween the reranked realization and the originaltop-ranked realization according to the averagedperceptron model.
Unlike the broad-based and ob-jective evaluation in terms of BLEU scores pre-sented above, this analysis is narrowly targetedand subjective, though the interested reader is in-vited to review the complete set of analyzed ex-amples that accompany the paper as a supplement.We leave a more broad-based human evaluation bynaive subjects for future work.Table 5 shows the results of the analysis, bothoverall and for the most frequent categories ofchanges.
Of the 50 sentences where the BLEUscore went up the most, 15 showed an improve-ment in adequacy (i.e., in conveying the intendedmeaning), 22 showed an improvement in fluency(with 3 cases also improving adequacy), and 16yielded no discernible change in fluency or ade-quacy.
By contrast, with the 50 sentences wherethe BLEU score went down the most, adequacywas only affected 4 times, though fluency was af-fected 32 times, and 15 remained essentially un-changed.4The table also shows that differencesin the order of VP constituents usually led to achange in adequacy or fluency, as did orderingchanges within NPs, with noun-noun compoundsand named entities as the most frequent subcate-gories of NP-ordering changes.
Of the cases whereadequacy and fluency were not affected, contrac-tions and subject-verb inversions were the mostfrequent differences.Examples of the changes yielded by the SVMranker appear in Table 6.
With wsj 0036.54,the averaged perceptron model selects a realiza-tion that regrettably (though amusingly) swapspurchasing and more than 250?yielding a sen-tence that suggests that the executives have beenpurchased!
?while the SVM ranker succeeds inranking the original sentence above all competingrealizations.
With wsj 0088.25, self-monitoringwith the SVM ranker yields a realization nearlyidentical to the original except for an extra comma,where it is clear that in public modifies do this;by contrast, in the perceptron-best realization, inpublic mistakenly appears to modify be disclosed.With wsj 0041.18, the SVM ranker unfortunatelyprefers a realization where presumably seems tomodify shows rather than of two politicians as4The difference in the distribution of adequacy change,fluency change and no change counts between the two condi-tions is highly significant statistically (?2= 9.3, df = 2, p <0.01).
In this comparison, items where both fluency and ade-quacy were affected were counted as adequacy cases.419?adq ?flu =eq ?vpord ?npord ?nn ?ne =vpord =sbjinv =cntrcBLEU wins 15 22 16 10 9 7 3 4 - 11BLEU losses 4 32 15 8 13 5 5 4 7 -Table 5: Manual analysis of devset sentences where the SVM ranker achieved the greatest in-crease/decrease in BLEU scores (50 each of wins/losses) compared to the averaged perceptron baselinemodel in terms of positive or negative changes in adequacy (?adq), fluency (?flu) or neither (=eq);changes in VP ordering (?vpord), NP ordering (?npord), noun-noun compound ordering (?nn) andnamed entities (?ne); and neither positive nor negative changes in VP ordering (=vpord), subject-inversion (=sbjinv) and contractions (=cntrc).
In all but one case (counted as =eq here), the BLEUwins saw positive changes and the BLEU losses saw negative changes.wsj 0036.54 the purchasing managers ?
report is based on data provided by more than 250 purchasing executives .SVM RANKER [same]PERCEP BEST the purchasing managers ?
report is based on data provided by purchasing more than 250 executives .wsj 0088.25 Markey said we could have done this in public because so little sensitive information was disclosed ,the aide said .SVM RANKER Markey said , we could have done this in public because so little sensitive information was disclosed ,the aide said .PERCEP BEST Markey said , we could have done this because so little sensitive information was disclosed in public ,the aide said .wsj 0041.18 the screen shows two distorted , unrecognizable photos , presumably of two politicians .SVM RANKER the screen shows two distorted , unrecognizable photos presumably , of two politicians .PERCEP BEST [same as original]wsj 0044.111 ?
I was dumbfounded ?
, Mrs. Ward recalls .SVM RANKER ?
I was dumbfounded ?
, recalls Mrs. Ward .PERCEP BEST [same as original]Table 6: Examples of devset sentences where the SVM ranker improved adequacy (top), made it worse(middle) or left it the same (bottom)in the original, which the averaged perceptronmodel prefers.
Finally, wsj 0044.111 is an exam-ple where a subject-inversion makes no differenceto adequacy or fluency.5.2 DiscussionThe BLEU evaluation and targeted manual analy-sis together show that the SVM ranker increasesthe similarity to the original corpus of realizationsproduced with self-monitoring, often in ways thatare crucial for the intended meaning to be apparentto human readers.A limitation of the experiments reported in thispaper is that OpenCCG?s input semantic depen-dency graphs are not the same as the Stanford de-pendencies used with the Treebank parsers, andthus we have had to rely on the gold parses inthe PTB to derive gold dependencies for measur-ing accuracy of parser dependency recovery.
In arealistic application scenario, however, we wouldneed to measure parser accuracy relative to the re-alizer?s input.
We initially tried using OpenCCG?sparser in a simple ranking approach, but found thatit did not improve upon the averaged perceptronmodel, like the three parsers used subsequently.Given that with the more refined SVM ranker, theBerkeley parser worked nearly as well as all threeparsers together using the complete feature set,the prospects for future work on a more realisticscenario using the OpenCCG parser in an SVMranker for self-monitoring now appear much morepromising, either using OpenCCG?s reimplemen-tation of Hockenmaier & Steedman?s generativeCCG model, or using the Berkeley parser trainedon OpenCCG?s enhanced version of the CCG-bank, along the lines of Fowler and Penn (2010).6 Related WorkApproaches to surface realization have been de-veloped for LFG, HPSG, and TAG, in additionto CCG, and recently statistical dependency-basedapproaches have been developed as well; see thereport from the first surface realization shared420task (Belz et al, 2010; Belz et al, 2011) for anoverview.
To our knowledge, however, a com-prehensive investigation of avoiding vicious struc-tural ambiguities with broad coverage statisticalparsers has not been previously explored.
Asour SVM ranking model does not make use ofCCG-specific features, we would expect our self-monitoring method to be equally applicable to re-alizers using other frameworks.7 ConclusionIn this paper, we have shown that while usingparse accuracy in a simple reranking strategy forself-monitoring fails to improve BLEU scoresover a state-of-the-art averaged perceptron realiza-tion ranking model, it is possible to significantlyincrease BLEU scores using an SVM ranker thatcombines the realizer?s model score together withfeatures from multiple parsers, including ones de-signed to make the ranker more robust to parsingmistakes that human readers would be unlikely tomake.
Additionally, via a targeted manual analy-sis, we showed that the SVM reranker frequentlymanages to avoid egregious errors involving ?vi-cious?
ambiguities, of the kind that would misleadhuman readers as to the intended meaning.As noted in Reiter?s (2010) survey, many NLGsystems use surface realizers as off-the-shelf com-ponents.
In this paper, we have focused onbroad coverage surface realization using widely-available PTB data?where there are many sen-tences of varying complexity with gold-standardannotations?following the common assumptionthat experiments with broad coverage realizationare (or eventually will be) relevant for NLG ap-plications.
Of course, the kinds of ambiguity thatcan be problematic in news text may or may not bethe same as the ones encountered in particular ap-plications.
Moreover, for certain applications (e.g.ones with medical or legal implications), it may bebetter to err on the side of ambiguity avoidance,even at some expense to fluency, thereby requir-ing training data reflecting the desired trade-off toadapt the methods described here.
We leave theseapplication-centered issues for investigation in fu-ture work.The current approach is primarily suitable foroffline use, for example in report generation wherethere are no real-time interaction demands.
In fu-ture work, we also plan to investigate ways thatself-monitoring might be implemented more effi-ciently as a combined process, rather than runningindependent parsers as a post-process followingrealization.AcknowledgmentsWe thank Mark Johnson, Micha Elsner, the OSUClippers Group and the anonymous reviewers forhelpful comments and discussion.
This work wassupported in part by NSF grants IIS-1143635 andIIS-1319318.ReferencesS.
Abney.
1996.
Statistical methods and linguistics.
InJudith Klavans and Philip Resnik, editors, The bal-ancing act: Combining symbolic and statistical ap-proaches to language, pages 1?26.
MIT Press, Cam-bridge, MA.Anja Belz, Mike White, Josef van Genabith, DeirdreHogan, and Amanda Stent.
2010.
Finding commonground: Towards a surface realisation shared task.In Proceedings of INLG-10, Generation Challenges,pages 267?272.Anja Belz, Michael White, Dominic Espinosa, EricKow, Deirdre Hogan, and Amanda Stent.
2011.
Thefirst surface realisation shared task: Overview andevaluation results.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 217?226,Nancy, France, September.
Association for Compu-tational Linguistics.Stephen Boxwell and Michael White.
2008.
ProjectingPropbank roles onto the CCGbank.
In Proc.
LREC-08.F.
Chantree, B. Nuseibeh, A.
De Roeck, and A. Willis.2006.
Identifying nocuous ambiguities in naturallanguage requirements.
In Requirements Engineer-ing, 14th IEEE International Conference, pages 59?68.
IEEE.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL, pages 173?180,Ann Arbor, Michigan.
Association for Computa-tional Linguistics.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and ex-periments with perceptron algorithms.
In Proc.EMNLP-02.421Marie-Catherine de Marneffe, Bill MacCartney, andChristopher Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
InProceedings of LREC.Dominic Espinosa, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proceedings of ACL-08: HLT,pages 183?191, Columbus, Ohio, June.
Associationfor Computational Linguistics.Timothy A. D. Fowler and Gerald Penn.
2010.
Ac-curate context-free parsing with Combinatory Cat-egorial Grammar.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 335?344, Uppsala, Sweden, July.Association for Computational Linguistics.Edward Gibson.
2000.
Dependency locality theory:A distance-based theory of linguistic complexity.In Alec Marantz, Yasushi Miyashita, and WayneO?Neil, editors, Image, Language, brain: Papersfrom the First Mind Articulation Project Symposium.MIT Press, Cambridge, MA.Daniel Gildea and David Temperley.
2010.
Do gram-mars minimize dependency length?
Cognitive Sci-ence, 34(2):286?310.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with Combina-tory Categorial Grammar.
In Proc.
ACL-02.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.T.
Florian Jaeger.
2010.
Redundancy and reduction:Speakers manage information density.
CognitivePsychology, 61(1):23?62, August.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proc.
KDD.I.H.
Khan, K. Van Deemter, and G. Ritchie.
2008.Generation of referring expressions: Managingstructural ambiguities.
In Proceedings of the 22ndInternational Conference on Computational Lin-guistics, pages 433?440.
Association for Computa-tional Linguistics.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Lin-guistics, pages 423?430.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Willem J. M. Levelt.
1989.
Speaking: From Intentionto Articulation.
MIT Press.G?unter Neumann and Gertjan van Noord.
1992.
Self-monitoring with reversible grammars.
In Proceed-ings of the 14th conference on Computational lin-guistics - Volume 2, COLING ?92, pages 700?706,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
ACL-02.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofCOLING-ACL.Rajakrishnan Rajkumar and Michael White.
2010.
De-signing agreement features for realization ranking.In Proc.
Coling 2010: Posters, pages 1032?1040,Beijing, China, August.Rajakrishnan Rajkumar and Michael White.
2011.Linguistically motivated complementizer choice insurface realization.
In Proceedings of the UC-NLG+Eval: Language Generation and EvaluationWorkshop, pages 39?44, Edinburgh, Scotland, July.Association for Computational Linguistics.Rajakrishnan Rajkumar, Michael White, and DominicEspinosa.
2009.
Exploiting named entity classes inCCG surface realization.
In Proc.
NAACL HLT 2009Short Papers.Ehud Reiter and Robert Dale.
2000.
Building natu-ral generation systems.
Studies in Natural LanguageProcessing.
Cambridge University Press.Ehud Reiter.
2010.
Natural language generation.
InAlexander Clark, Chris Fox, and Shalom Lappin,editors, The Handbook of Computational Linguisticsand Natural Language Processing (Blackwell Hand-books in Linguistics), Blackwell Handbooks in Lin-guistics, chapter 20.
Wiley-Blackwell, 1 edition.Mark Steedman.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.David Temperley.
2007.
Minimization of dependencylength in written English.
Cognition, 105(2):300?333.K.
Van Deemter.
2004.
Towards a probabilistic versionof bidirectional OT syntax and semantics.
Journal ofSemantics, 21(3):251?280.Michael White and Rajakrishnan Rajkumar.
2008.A more precise analysis of punctuation for broad-coverage surface realization with CCG.
In Coling2008: Proceedings of the workshop on GrammarEngineering Across Frameworks, pages 17?24.422Michael White and Rajakrishnan Rajkumar.
2009.Perceptron reranking for CCG realization.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 410?419, Singapore, August.
Association for Computa-tional Linguistics.Michael White and Rajakrishnan Rajkumar.
2012.Minimal dependency length in realization ranking.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 244?255, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Michael White.
2006.
Efficient Realization ofCoordinate Structures in Combinatory CategorialGrammar.
Research on Language & Computation,4(1):39?75.423
