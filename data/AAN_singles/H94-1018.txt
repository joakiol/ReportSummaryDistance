TOWARDS BETTER NLP  SYSTEM EVALUATIONKaren Sparck JonesComputer  Laboratory ,  Un ivers i ty  of  Cambr idgeNew Museums Site, Pembroke  Street ,  Cambr idge  CB2 3QGsparck jones@cl .cam.ac .ukABSTRACTThis paper considers key elements of evaluation methodol-ogy, indicating the many points involved and advocating anunpacking approach in specifying an evaluation remit and de-sign.
Recognising the importance of both environment vari-ables and system parameters leads to a grid organisation fortests.
The paper illustrates the application of these notionsthrough two examples.1.
I n t roduct ionThere is a manifest need for a good evaluation method-ology for (S&)NLP systems.
This paper presents uch amethodology, indicating its key elements under the head-ings of performance factors, evaluation gauges, test data,and assessment s rategy, and illustrating its application.The paper is based on Galliers and Sparck Jones (1993),which offers a comprehensive analysis of what is involvedand needed in NLP evaluation, supplemented by an ex-tensive review of methodologies that have already beenproposed or applied, and also refers to evaluation ex-perience in the neighbouring information retrieval (IR)field.2.
BackgroundThere is generally growing interest and activity in NLPsystem evaluation, stimulated partly by the fact thatNLP technology is now solid enough to be of interestfor real applications and partly by the (D)ARPA ini-tiatives (cf Thompson, 1992; HLT1; MUC; S&NLW;TREC).
But evaluation practice is still uneven, andsound methodologies need to be developed, both for lab-oratory experiments and working system investigations.There are many difficult matters to be considered in NLPevaluation, and some significant points seem not to besufficiently recognised.
Thus there are dangers in seekingto apply 'closed problem' approaches, exemplified by theuse of 'word error rate', outside their legitimate bounds;in attaching prime importance to internal objects likeparse trees; or in reifying important but ultimately arbi-trary processing devices, for instance specific sets of caselabels.
The IR community's hard-won experience showsthere are no easy ways of evaluating systems, no magicnumbers encapsulating performance, no 'core' functionsthat can be pursued far in isolation, no fixed meaning-representation devices any system must have.
Further,the lesson of IR is that discourse processing (in this caseindexing) is only as good as the use made of it, whichis necessarily on individual occasions, so it is only pos-sible to infer, over many such occasions, what averageperformance will or can be designed to be.
The moti-vation for a better evaluation methodology is thus morereliable inference, implying both a diligent attempt olay out the premises but also, given the many factorsinvolved in NLP systems and their resistance to precisespecification, that the inferences drawn may still not bevery reliable.The essential problem of NLP evaluation is that theevaluator has to advance between Scylla and Charyb-dis.
Evaluation may be end-to-end for systems devotedto NLP, end-to-end for hybrid ones with both NLPand non-NLP subsystems, or non end-to-end, focusedon some component of an NL processor; and there isa tradeoff, from the NLP evaluation point of view, be-tween least problematic and also least analytic, and mostanalytic but also most problematic.
End-to-end NLPsystem evaluation clearly demonstrates NLP function.Hybrid system evaluation is ambiguous in this respect.NL component evaluation is just this, at best within anartificial boundary but at worst imposing inappropriatestandards.
However with end-to-end NLP systems thesituation of use becomes important, showing that in factin all cases the context surrounding the NLP subjectbeing evaluated matters.In the rest of this paper I shall consider the four clus-ters of concepts that need to be recognised and appliedin evaluation, namely evaluation subjects and perfor-mance factors, kinds and levels of definition for per-formance assessment, sorts of test and evaluation data,and strategies for evaluation design and conduct.
It isclear that there is no single correct way to evaluate anNLP system, and that very different evaluations are re-quired in different circumstances, as I show in the sub-sequent illustration for two different cases involving thesame NLP system.
The important point is that eval-uation has to be approached comprehensively and sys-102tematically, so the presumptions on which is based arelaid bare.
The methodology I outline, summarising Gal-liers and Spaxck Jones (1993), is aimed at achieving this,leading to soundly-based tests that take account of theproperties of both evaluation subjects and their contextswithin a regular comparative framework.
The principlesinvolved in this are not novel; but they are not suffi-ciently applied in NLP evaluation, as noted in my con-cluding comments on the ARPA evaluations, and deservereiteration.3.
Per formance factorsIn NLP the subject of assessment may be a language pro-cessing component, e.g.
a syntax analyser; a languagesubsystem of a larger computational system, e.g.
an in-terface to a fault diagnosis package; a complete compu-tational system which may be wholly devoted to NLP,as in translation, or only partially an NLP system, asin the previous case; or a 'setup', i.e.
a computationalentity plus some body of people involved with it in someway.
As this suggests, the onion metaphor applies, withthe subject of the evaluation complemented by its envi-rpnment.
It is convenient to refer to any of the kinds ofevaluation subject mentioned in the abstract as a sys-tem: I shall therefore use computational system, or 'c-system', specifically for the third case.
In evaluation itis essential to take both system and environment intoaccount as supplying the factors affecting performance.Thus attributing meaning to, or explaining, system per-formance depends on determining both system parame-ters and their potential settings, e.g.
grammar or lexiconused, and environment variables and their values, e.g.text type or post-editor level of skill, in as thorough away as possible.It is easy to pay too much attention to the system andnot enough to its environment as influencing evaluation,so relevant environment variables are not taken into ac-count.
Characterisation of both variables and parame-ters is needed even when an evaluation is an investiga-tion of some given system to establish its performancelevel, but even more so when experiments are conductedto compare system performance when parameter settingsand/or environment variables are changed.
Thus with anautomatic summarising system as subject, for instance,different sets of semantic ategories, or discourse rela-tions, might be considered as parameter settings, anddifferent document ypes, or lengths, as variable values.There can he very many performance factors that arehard to capture so alternatives can be compared and as-sessment of their influence made.
Environment variablesinclude both input and output ones, referring to the datafor processing and the requirements on outputs imposedby their uses, e.g.
for summaries that their intendedusers are able to read them quickly.
With environmentvariables, it may be hard to determine the appropriatelevel of granularity, e.g.
of length differences for input oroutput texts in summarising, or input genres or readerclasses; and it may be hard to keep system parametersseparate.The relation between a system and its environment thushas many implications for assessment practice, notablywith respect o the view taken of system ends in evalu-ation, and in relation to the assessment of generic NLPsystems.
It is necessary to establish whether assessmentof a system's functional performance r fers to its abilityto meet its internal design objectives or its external func-tion requirements, e.g.
for a translation c-system, eitherproviding complete parses or delivering texts that don'tconfuse their readers.
While the contrast between glassbox and black box evaluation is commonplace, it is usefulto interpret his in relation to the system-environmentdistinction and to a specific view of system function.With generic systems (such as SRI Cambridge's CoreLanguage Engine), the issue is that establishing theirmerits by intrinsic evaluations i of limited value, so ex-tensive extrinsic evaluation in a variety of environmentsis required.
This also applies where a generic process,without any explicit generic data resources, is used, asin statistically-based text passage xtraction.4.
Eva luat ion  gaugesApart from the distinctions between kinds of evaluationjust mentioned, it is helpful to categorise performanceevaluation according to whether it is concerned with ef-fectiveness, e~iciency, or acceptability (to humans).
Suchdistinctions help to guide an appropriate choice of per-formance criteria.
For instance a text categorisation sys-tem might be evaluated for effectiveness by its accuracyin classing (say intrinsically against independently as-signed categories for test material, or extrinsically withreference to output dumping by its recipients), or foracceptability in ordering and grouping presented ma-terial.
It is clearly necessary both to contextualise anotion like effectiveness for a given evaluation, and tomotivate performance criteria via their evaluative role.This implies further refinement, for instance of the hu-man class for acceptability.
However it is also essential,for sound evaluation, to recognise that as performancecriteria are general, e.g.
quality for translation output,a criterion requires further definition first by a partic-ular performance measure, e.g.
proportion of sentencesedited, and second, for any measure, by a specific appli-cation method, e.g.
for choosing actual texts.All three levels of characterisation are required for as-sessment, even for non-numerical pproaches to evalua-103tion.
A given criterion can typically be given a rangeof specific interpretations, quantitative or qualitative, asindividual performance measures: for instance transla-tion quality by the proportion of sentences requiring postediting.
Any measure in turn has to be operationalised,say for the precise way in which a random data sampleis gathered, or how a measure is normalised.
Individualchoices of significance test would ordinarily be made atthis level.
Significance testing is required but is hard inNLP because normally only weak, non-parametric testsare applicable.
They also only show the least requiredreal difference: this has to be interpreted in practicalterms in relation to operational system behaviour, andalso needs to be supplemented by some view of whatlarger differences may actually be attainable.
Again, IRexperience has emphasised the relevance of all these lay-ers, for example in their application to recall/precisionfigures.
Thus while recall and precision may be used ascriteria, thay can be more specifically defined in variousways, and also computed ifferently in averaging, withnon-trivial consequences for what performance 'looks'like.
Equally, statistical performance differences maydifficult to translate into statements about getting no-ticeably more good documents in initial search output,while what level of performance is attainable in a givenenvironment is usually unclear.In many cases there are either existing or natural com-parative reference points for evaluation, which may betaken as defining baseline or benchmark performance.The former provides a rockbottom view of performance,e.g.
as in word-for-word translation or simple wordindexing, the latter the current best state of the art,e.g.
sentence-by-sentence literal translation.
Both allowgrounded performance comparisons either for alternativesystem philosophies or for payoffs from extra effort.5.
Data  sor tsThe characterisation f the test data used provides theother support for evaluation.
Evaluation data is not justworking data in the ordinary sense of material to whicha system is applied, e.g.
a body of texts to be translated,but also includes answer data, e.g.
correct ranslationsfor key terms supplied by humans.
1 The particularcharacter of the evaluation data naturally follows fromthe goal and manner of the evaluation, as well as fromthe nature of the system and its environment.
Thuswhile in some cases concrete answers may be availablee.g.
replies to database queries, in others this is notfeasible: there are no correct outputs for a summarisingsystem, only utility judgements.
But it is useful to recog-nise, for NLP evaluation, some important properties of1Training vs test data is a different contrast.test and evaluation data and major sorts of such data: itis all too easy to assume that any sufficiently large andmiscellanous assemblage of material can be used simplybecause it is large and miscellanous.Thus it is necessary to consider equirements for dataas realistic, representative, and legitimate.
For an eval-uation of a system for dealing with newspaper material,for instance, a set of newspaper stories is realistic, but itshould also be a representative sample of the larger uni-verse of stories with which the system may deal.
Rep-resentative data should, further, be so in distribution,e.g.
reflecting the relative frequency of newspaper topicdomains in the larger universe.
The need for legitimacy,even where data is already realistic and representative,seems superfluous but matters because valuation datamay be so expensive to acquire it is natural to seek toexploit it for other tests than those for which it was origi-nally acquired, for example test paragraphs retrieved forspecific queries regardless of other user constraints.The foregoing defines the most strongly constrained eval-uation data.
Other forms are proper and useful, but theirlimitations need to be accepted.
Thus natural corpustest data may be selected for coverage of linguistic phe-nomena rather than to reflect statistical frequency; andartificially constructed bodies of test data or test suites,for example read speech material, may be provided with-out explicit accompanying answers (though these may beassumed as 'obvious', as in the speech case).
Howeverthe most important distinction is between atural andartificial evaluation data, i.e.
between test collectionsand checking collections.
For example, in the IR casea test collection is properly only constituted from therelevance assessments made by the user submitting thequery, while the agreed assessments of a set of librariansconstitutes a checking collection.
There are clear prob-lems on the one hand where the same user cannot be'reused', for instance offered alternative xplanations bydifferent systems for the same original, uninformed ques-tion, and on the other where surrogate users offer theiranswers, especially normative ones.
Different test andevaluation data sorts serve quite distinct functions andneed to be correctly related to an evaluation's goal toavoid illegitimate inferences from the assessment results.Further, the task and situation complexity characterisicof many NLP applications, for instance translating repairinstructions or summarising reports for 'non-reusable'users, implies large data sets.6.
Assessment  s t ra tegyIt is evident that serious evaluation requires a well-understood ecomposition of the whole, in terms of theevaluation aims, or remit, and design, and the precise104definition of the evaluation subject.
This applies evenin the case where all that is wanted is some snapshotof the performance of an operational system.
It appliesmuch more where comparisons, which to be useful mustbe systematic, are wanted, whether in the large graine.g.
for two whole NLP c-systems in some environmentor in the small grain, e.g.
seeking the reasons for differ-ent performance for the two systems.
The former nat-urally leads to a greater emphasis, in testing, on thedata variable values to see whether different systems areimpervious to changes in these; the latter to changesin the system parameter settings, to see if these reallymatter.
However well-founded tests require changes onboth dimensions, implying a grid design with individualruns representing specific ombinations ofvariable valuesand parameter settings.
Clearly when (as in the AI:EPAcases) whole systems with very different properties areinvolved, changes on the system dimension have to berelativised, or taken at several levels.
The implication isnevertheless that for reliable conclusions about compar-ative performance, very many runs are required (as hasbeen found in IR, where long term projects have carriedout literally thousands of them).Overall, therefore, the methodology required for an NLPevaluation is an unpacking one, designed to addressthe very many distinctions involved and make properly-related choices on each.
The outcome of this combi-nation of interdependent choices will be an individualevaluation which may differ substantially, for the samesystem, from others with different motivations.
Thereis no one way to evaluate NLP c-systems, primarily be-cause these are not autonomous entities: assuming thatthere is is a version of the naturalistic fallacy which sup-poses that NLP aspires towards human LP capabilitieswithout allowing for the fact that humans have differ-ent capabilities that are differently deployed in differentcircumstances.The decompositional pproach is naturally followed byanswering a series of questions, gradually working intothe fine detail of the evaluation in one or more cycles ofspecification, to emerge with the scenario for the wholeevaluation.
Thus the questions on the evaluation remitestablish the motivation for the evaluation, and its spe-cific goal, along with the perspective from which it is un-dertaken (e.g.
task-specific, financial), what interest, in-dividual or group, is prompting the evaluation and whatconsumers are envisaged for its findings.
Then giventhe stated goal, what orientation, intrinsic or extrinsic;what kind of test, i.e.
investigation or experiment; whattype of evaluation, black box or glass box; what /orm ofyardstick (e.g.
baseline, benchmark); what style of eval-uation (e.g.
indicative or exhaustive); and what mode(e.g.
quantitative or qualitative)?The answers to this first set of questions on the evalua-tion remit lead to those for the second set, on the evalu-ation design.
Initial questions here define the evaluationsubject at the necessary level of detail, by addresssingits ends, i.e.
objective or function, the subject's context,and its constitution.
The definition of ends validates theremit and also evaluation gauges; with the context andconstitution stated, the first specific subset of questionsabout performance factors, i.e.
environment variablesand system parameters, can be answered.
The secondsubset addresses the choices of criteria, measures andmethods, and the third the choice of evaluation data, itssort e.g.
test suite and status, i.e.
representativeness tc.The fourth subset determines the evaluation procedure.7.
ExampleTo illustrate all the notions outlined, consider two dif-ferent evaluations concerned with the same (notional)c-system, PlanS, a system used for tearching achitec-ture students about house design problems, with whichthey interact by an NL interface and a graphical one.The essential features of PlanS are that teachers give thestudents the requirements specification for a house e.g.four bedrooms, tile roof, cost below 100K dollars, etc.
;the students plan the house in a graphics window, withform filling for features uch as roof material.
During de-sign they may query the system in NL, e.g.
for the pricesof types of bricks.
When the design is finished the sys-tem assesses the student's plan and feature specification,makes comments e.g.
the plan has only three bedrooms,and offers its own version(s).
The student may use theNL interface to query the system's comments or plan(s),after which a new design cycle may begin.The two evaluations for PlanS, showing how the method-ology I have described is applied, are for 'Case U', thesetup involving Plans and its student user body, and'Case L', where the subject is just the NL interface.These examples can only be sketched here: they aremore fully described, with other cases, in Galliers andSparck Jones (1993); the aim here is to show answeringthe questions increasingly constrains the evaluation.7 .1 .
Case  UThe motivation here is educational effectiveness inter-preted for the evaluation goal as finding whether Plansdoes, as claimed, enable students to grasp planning con-cepts rapidly.
The perspective is therefore task oriented.The interest behind the evaluation is the departmentchair, its consumers all the teaching staff.
We have anintrinsic orientation, concerned with PlanS's own func-tion of superior training.
Since various factors e.g.
prob-105lem hardness or problem sequence can affect learningspeed, an experimental kind of evaluation seems calledfor, though this can be a black box study, not chang-ing the setup itself, only its inputs.
The appropriateform of yardstick appears to be performance for similarstudents without PlanS, but this is (we suppose) imprac-tical, so a notion of attainable speed has to be defined.The evident challenge of evaluating U suggests an initialindicative rather than exhaustive valuation style, butas one dealing with learning times, with a quantitativemode.Now working out the evaluation design given this remit,we validate the motivation against the setup's internalpurpose, which is promoting fast learning; characteris-ing U's context includes the contribution of the teach-ers both in assigning design problems and in markingeventual exam results, and the other inputs in the stu-dents' course specifically bearing on the development ofdesign skills.
The context also includes properties ofthe students, e.g.
they are first year ones.
The con-stitution of U includes the students' various activitieswithin U relating to PlanS, e.g.
how often they use it,and all aspects of Plans itself, non-linguistic and lin-guistic.
The environment variables for U include levelsof exposure to planning concepts, and general experi-ence with computing etc; the system parameters includeboth e.g.
'workover' habits of the students exploring dif-ferent designs, and lengths of session, convenience of thegraphics, helpfulness of PlanS's critiques.
Seeking appro-priate performance gauges for U's effectiveness uggestsboth absolute speed to problem solution and increase ofspeed, for some attained esign level, as criteria: so e.g.a measure could be cycle time for acceptable solutions,defined as ones not breaching planning regulations, nothaving obvious faults like rooms with no doors.
Themethod averages times for date sets on plans determinedacceptable for both benchmark reference and new resultsets, with significance tests on comparisons.
The testdata would ideally be session log files, or if not availableselect records of input problems and output critiques,along with times.
The evaluation requires reference datafor good times, which could be obtained by taking lastyear's best students, getting their times for new prob-lems, and using these as benchmarks.
The procedureinvolves assembling a set of problems (of similar diffi-culty), getting the benchmark times, and administeringtwo random subsets to the students at two date intervals.For Case U, the evaluation is rather basic, directed to-wards obtaining a first view of PlanS's performance,probably leading towards further studies.
The test gridhas only one proper run, since the second, benchmark,one is poorly controlled for differences of user set.
Fromthe NLP point of view the evaluation is one of a systemoffering NLP in use, but does not throw any specific lighton the particular contribution made by the NL interface.7 .2.
Case  LThis evaluation is designed specifically to focus on thecontribution made by the NL interface.
The motiva-tion is to establish whether NL interaction is best forPlanS, so the goal is to establish whether NL is bet-ter than the obvious competitor, a menu interface, Theperspective here again is task oriented, with (we say)teachers both as interested and consuming parties; how-ever the orientation is extrinsic, since the emphasis is onthe interface's stimulus to student hinking, with menuprompting deemed to force a more comprehensive iewof design.
An experimental kind of evaluation is againappropriate, comparing NL and menu interfaces, againof black box type but, here, in exhaustive style and withdirect comparison since there is no clear need for or ob-vious yardstick; however hybrid mode, with qualitativeas well as quantitive criteria, seems called for.Now filling out the design, the evaluation is concernedwith the L-system's external function, contributing tothe support of training; its context, however, is not justthe rest of the c-system as a whole, but also the bodyof student users; the subject's constitution consists ofthe L-system's (or analogously the competing menu in-terface's) various components, e.g.
parser, lexicon.
Theenvironment variables for this evaluation are thereforeboth the properties of design problems and students,as before, and those of the rest of the system, for in-stance the knowledge base and critiquing capabilities ofthe design sub-system.
The system parameters are theinterfaces' external communication elements: e.g.'
de-gree of freedom through sentences versus headed boxes,and linguistic resources in vocabulary and sentence typesfor NL or, for the menu, words or phrases used as slotlabels or allowed as fillers.
These are the relevant param-eters, which we assume are supported by common otherparameters like the same information transfer capacitywith respect o the underlying design system.The evaluation criteria, addressing effectiveness, are onthe one hand plan quality and on the other interfaceutility to the user.
The measure for the plans is quan-titative, via ratings on a scale of three by competentjudges, with the method averaging over all completedplans (perhaps upplemented by distribution data).
Thequalitative user view is obtained by questionnaire rat-ing interfaces positive, neutral or negative for ease ofexpression for inputs, of clarity for outputs, and generalappropriateness.
The measures are relative distributionsof the ratings for the features, and the method here is106by self-completed questionnaire at session end.
Howeverthe data gathering is complex.
The evaluation presup-poses a menu interface, which has be strictly comparablein concept coverage with the NL one and will thereforehave to be specially provided, as part of the test enter-prise as a whole.
But it is evident hat while given designproblems may be the same, this is the only common ele-ment, and there is no concrete answer data in the form oftarget plans.
The only strategy, given (we assume) thereal students cannot be disrupted, is to train some otherstudents up to the relevant architecture l vel, and thensupply them with the real students' problems and oneinterface or the other, used over a period of time.
Theevaluation procedure is thus an elaborate and expensiveone which depends on obtaining the menu interface, andthen training the substitute students, giving them prob-lems and assessing the resulting plans, and organisingand processing the questionnaires.The aim of this evaluation makes it very different fromthat for U and, with its controlled comparisons, veryelaborate and costly.
The test grid even so has only tworuns: it needs filling out with checks on environmentvariables like student quality and training, problems et,plan criteria or system critiquing capabilities; and, be-cause variables and parameters interact, parameter set-tings for e.g.
inquiry flexibility or critique phrasing needtesting.
The parameter testing would move towards glassbox interface valuation.
The evaluation described oesno more than assess the NL interface as a whole: it doesnot analyse its behaviour in detail, or attribute this tounderlying properties of its data resources or processors.That would require other, different evaluation with manyruns, though it would still be very hard to link individualdesign characteristics of the interface with its perceivedmerits and demerits as a communication device.
Otherevaluation gauges also need consideration, e.g.
designclosure measured by number of dialogue turns.These brief illustrations also show both how the detaileddesign choices in an evaluation, e.g.
of data or mea-sure, have to be clearly motivated from outside, andhow the performance results for an evaluation subjecthave always to be interpreted in relation to the subject'senvironment.
Even when environment variables are notexplicitly manipulated, they need to be specified in Orderto flag their possible implications, for instance problemand student characteristics for PlanS.8.
Conc lus ionAs noted, while there have been individual evaluationsof interest, and attacks on methodology (cf Thomp-son, 1992), the (D)ARPA/NIST evaluation initiativesare major efforts in terms of their goals, scale, and hardlabour.
Relating these to my analysis and methodologytheir major feature is that they are laboratory experi-ments, and as such are naturally distanced from detailedoperational influences, Moreover the desire for control, tobe achieved not only by blind testing but more materiallyvia highly elaborated and polished answer data, furtheremphasises their detachment.
This is particularly notice-able in the MUC and SLS (ATIS) cases, where the assess-ment data is not realistic or representative in any strong,or at any rate demonstrated, sense.
There is an assump-tion that the nature of the evaluation data reflects realneeds, and that the relative scores obtained correctlypredict relative operational utility.
While SLS assess-ment via logfiles refers more to systems in use, currentMUC evaluation concerns with 'internal' NLP productse.g.
predicate-argument structures, reflect researchers'interests in fine-grained explanatory evaluation concen-trating on system parameters, properly viewed heuris-tically, not absolutely.
These are legitimate interests,but there is not enough of the necessary complementaryconcern, even for the laboratory approach, with envi-ronment variables and their interaction with parametersand impact on performance.
TREC has the advantageof more realistic (and also more simply specified) evalu-ation, but there are still concerns here about legitimacyand generalisation; these instructively include what op-erationally relevant inferences can be drawn from testresults even when these use such intuitively plausibleand universal measures as recall and precision.One of the main requirements for future NLP evalua-tions is thus to approach these in a comprehensive aswell as systematic way, so that the specific tests doneare properly situated, especially in relation to the endsthe evaluation subject is intended to serve, and the prop-erties of the context in which it does this.9.
Re ferencesGalliers, J.R. and Sparck Jones, K. Evaluating natural an-guage processing systems, TR 291, Computer Laboratory,University of Cambridge, 1993, (187pp).
A gzipped copyof TR 291 (needing binary transfer) is available from theanonymous FTP server ftp.cl.cam.ac.uk, as the file TR291-ksj-jrg-evaiuating-nl-systems.ps.gz in thedirectory reports.HLTI: Proceedings off the ARPA Workshop on Human Lan-guage Technology, 1993, Morgan Kaufmann, San Mateo, CA.MUC: Proceedings off the Message Understanding Confer-ences, 1991; 1992; 1993, Morgan Kaufmann, San Mateo, CA.S&NLW: Proceedings off the Speech and Natural LanguageWorkshops, 1991; 1992, Morgan Kaufmann, San Mateo, CA.Thompson, H. (ed) The strategic role off evaluation in naturallanguage processing and speech technology; Workshop record,HCRC, University of Edinburgh, 1992.TREC: Proceedings off the Text Retrieval Conferences, 1992;1993, NIST, Gaithersburg, MD.107
