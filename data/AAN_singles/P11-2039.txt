Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223?229,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsQuery Snowball: A Co-occurrence-based Approach to Multi-documentSummarization for Question AnsweringHajime Morita1 2 and Tetsuya Sakai1 and Manabu Okumura31Microsoft Research Asia, Beijing, China2Tokyo Institute of Technology, Tokyo, Japan3Precision and Intelligence Laboratory, Tokyo Institute of Technology, Tokyo, Japanmorita@lr.pi.titech.ac.jp, tetsuyasakai@acm.org,oku@pi.titech.ac.jpAbstractWe propose a new method for query-orientedextractive multi-document summarization.
Toenrich the information need representation ofa given query, we build a co-occurrence graphto obtain words that augment the originalquery terms.
We then formulate the sum-marization problem as a Maximum CoverageProblem with Knapsack Constraints based onword pairs rather than single words.
Ourexperiments with the NTCIR ACLIA ques-tion answering test collections show that ourmethod achieves a pyramid F3-score of up to0.313, a 36% improvement over a baseline us-ing Maximal Marginal Relevance.1 IntroductionAutomatic text summarization aims at reducing theamount of text the user has to read while preserv-ing important contents, and has many applicationsin this age of digital information overload (Mani,2001).
In particular, query-oriented multi-documentsummarization is useful for helping the user satisfyhis information need efficiently by gathering impor-tant pieces of information from multiple documents.In this study, we focus on extractive summariza-tion (Liu and Liu, 2009), in particular, on sentenceselection from a given set of source documents thatcontain relevant sentences.
One well-known chal-lenge in selecting sentences relevant to the informa-tion need is the vocabulary mismatch between thequery (i.e.
information need representation) and thecandidate sentences.
Hence, to enrich the informa-tion need representation, we build a co-occurrencegraph to obtain words that augment the originalquery terms.
We call this method Query Snowball.Another challenge in sentence selection forquery-oriented multi-document summarization ishow to avoid redundancy so that diverse pieces ofinformation (i.e.
nuggets (Voorhees, 2003)) can becovered.
For penalizing redundancy across sen-tences, using single words as the basic unit may notalways be appropriate, because different nuggets fora given information need often have many wordsin common.
Figure 1 shows an example of thisword overlap problem from the NTCIR-8 ACLIA2Japanese question answering test collection.
Here,two gold-standard nuggets for the question ?Sen toChihiro no Kamikakushi (Spirited Away) is a full-length animated movie from Japan.
The user wantsto know how it was received overseas.?
(in Englishtranslation) is shown.
Each nugget represents a par-ticular award that the movie received, and the twoJapanese nugget strings have as many as three wordsin common: ???
(review/critic)?, ????
(ani-mation)?
and ??
(award).?
Thus, if we use singlewords as the basis for penalising redundancy in sen-tence selection, it would be difficult to cover both ofthese nuggets in the summary because of the wordoverlaps.We therefore use word pairs as the basic unit forcomputing sentence scores, and then formulate thesummarization problem as a Maximum Cover Prob-lem with Knapsack Constraints (MCKP) (Filatovaand Hatzivassiloglou, 2004; Takamura and Oku-mura, 2009a).
This problem is an optimization prob-lem that maximizes the total score of words coveredby a summary under a summary length limit.223?
QuestionSen to Chihiro no Kamikakushi (Spirited Away) is a full-lengthanimated movie from Japan.
The user wants to know how itwas received overseas.?
Nugget example 1????????????
?National Board of Review of Motion Pictures Best AnimatedFeature?
Nugget example 2????????????????
?Los Angeles Film Critics Association Award for Best Ani-mated FilmFigure 1: Question and gold-standard nuggets example inNTCIR-8 ACLIA2 datasetWe evaluate our proposed method using Japanesecomplex question answering test collections fromNTCIR ACLIA?Advanced Cross-lingual Informa-tion Access task (Mitamura et al, 2008; Mitamuraet al, 2010).
However, our method can easily beextended for handling other languages.2 Related WorkMuch work has been done for generic multi-document summarization (Takamura and Okumura,2009a; Takamura and Okumura, 2009b; Celiky-ilmaz and Hakkani-Tur, 2010; Lin et al, 2010a;Lin and Bilmes, 2010).
Carbonell and Goldstein(1998) proposed the Maximal Marginal Relevance(MMR) criteria for non-redundant sentence selec-tion, which consist of document similarity and re-dundancy penalty.
McDonald (2007) presentedan approximate dynamic programming approach tomaximize the MMR criteria.
Yih et al (2007)formulated the document summarization problemas an MCKP, and proposed a supervised method.Whereas, our method is unsupervised.
Filatovaand Hatzivassiloglou (2004) also formulated sum-marization as an MCKP, and they used two typesof concepts in documents: single words and events(named entity pairs with a verb or a noun).
Whiletheir work was for generic summarization, ourmethod is designed specifically for query-orientedsummarization.MMR-based methods are also popular for query-oriented summarization (Jagarlamudi et al, 2005;Li et al, 2008; Hasegawa et al, 2010; Lin et al,2010b).
Moreover, graph-based methods for sum-marization and sentence retrieval are popular (Otter-bacher et al, 2005; Varadarajan and Hristidis, 2006;Bosma, 2009).
Unlike existing graph-based meth-ods, our method explicitly computes indirect rela-tionships between the query and words in the docu-ments to enrich the information need representation.To this end, our method utilizes within-sentence co-occurrences of words.The approach taken by Jagarlamudi et al (2005)is similar to our proposed method in that it uses wordco-occurrence and dependencies within sentences inorder to measure relevance of words to the query.However, while their approach measures the genericrelevance of each word based on Hyperspace Ana-logue to Language (Lund and Burgess, 1996) usingan external corpus, our method measures the rele-vance of each word within the document contexts,and the query relevance scores are propagated recur-sively.3 Proposed MethodSection 3.1 introduces the Query Snowball (QSB)method which computes the query relevance scorefor each word.
Then, Section 3.2 describes howwe formulate the summarization problem based onword pairs.3.1 Query Snowball method (QSB)The basic idea behind QSB is to close the gapbetween the query (i.e.
information need rep-resentation) and relevant sentences by enrichingthe information need representation based on co-occurrences.
To this end, QSB computes a queryrelevance score for each word in the source docu-ments as described below.Figure 2 shows the concept of QSB.
Here, Q isthe set of query terms (each represented by q), R1is the set of words (r1) that co-occur with a queryterm in the same sentence, andR2 is the set of words(r2) that co-occur with a word from R1, excludingthose that are already in R1.
The imaginary rootnode at the center represents the information need,and we assume that the need is propagated throughthis graph, where edges represent within-sentenceco-occurrences.
Thus, to compute sentence scores,we use not only the query terms but also the wordsin R1 and R2.Our first clue for computing a word score isthe query-independent importance of the word.224qqqr1r1r1r1 r1r1r1r2r2r2r2r2r2r2r2r2r2R1R2Qrootr2r2 r2r2r2Figure 2: Co-occurrence Graph (Query Snowball)We represent this base word score by sb(w) =log(N/ctf (w)) or sb(w) = log(N/n(w)), wherectf (w) is the total number of occurrences of wwithin the corpus and n(w) is the document fre-quency of w, and N is the total number of docu-ments in the corpus.
We will refer to these two ver-sions as itf and idf, respectively.
Our second clueis the weight propagated from the center of the co-occurence graph shown in Figure 1.
Below, we de-scribe how to compute the word scores for words inR1 and then those for words in R2.As Figure 2 suggests, the query relevance scorefor r1 ?
R1 is computed based not only on its baseword score but also on the relationship between r1and q ?
Q.
To be more specific, let freq(w,w?
)denote the within-sentence co-occurrence frequencyfor words w and w?, and let distance(w,w?)
denotethe minimum dependency distance between w andw?
: A dependency distance is the path length be-tween nodes w and w?
within a dependency parsetree; the minimum dependency distance is the short-est path length among all dependency parse trees ofsource-document sentences in which w and w?
co-occur.
Then, the query relevance score for r1 can becomputed as:sr(r1) =?q?Qsb(r1)( sb(q)sumQ)( freq(q, r1)distance(q, r1) + 1.0)(1)where sumQ =?q?Q sb(q).
It can be observed thatthe query relevance score sr(r1) reflects the baseword scores of both q and r1, as well as the co-occurrence frequency freq(q, r1).
Moreover, sr(r1)depends on distance(q, r1), the minimum depen-dency distance between q and r1, which reflectsthe strength of relationship between q and r1.
Thisquantity is used in one of its denominators in Eq.1as small values of distance(q, r1) imply a strong re-lationship between q and r1.
The 1.0 in the denom-inator avoids division by zero.Similarly, the query relevance score for r2 ?
R2is computed based on the base word score of r2 andthe relationship between r2 and r1 ?
R1:sr(r2) =?r1?R1sb(r2)( sr(r1)sumR1)( freq(r1, r2)distance(r1, r2) + 1.0)(2)where sumR1 =?r1?R1sr(r1).3.2 Score Maximization Using Word PairsHaving determined the query relevance score, thenext step is to define the summary score.
To this end,we use word pairs rather than individual words as thebasic unit.
This is because word pairs are more in-formative for discriminating across different piecesof information than single common words.
(Re-call the example mentioned in Section 1) Thus, theword pair score is simply defined as: sp(w1, w2) =sr(w1)sr(w2) and the summary score is computedas:fQSBP (S) =?
{w1,w2|w1 6=w2 and w1,w2?u and u?S}sp(w1, w2) (3)where u is a textual unit, which in our case is asentence.
Our problem then is to select S to maxi-mize fQSBP (S).
The above function based on wordpairs is still submodular, and therefore we can applya greedy approximate algorithm with performanceguarantee as proposed in previous work (Khulleret al, 1999; Takamura and Okumura, 2009a).
Letl(u) denote the length of u.
Given a set of sourcedocuments D and a length limit L for a sum-mary,Require: D,L1: W = D,S = ?2: while W 6= ?
do3: u = argmaxu?Wf(S?
{u})?f(S)l(u)4: if l(u) +?uS?S l(uS) ?
L then5: S = S ?
{u}6: end if7: W = W/{u}8: end while9: umax = argmaxu?D f(u)10: if f(umax) > f(S) then11: return umax12: else return S13: end ifwhere f(?)
is some score function such as fQSBP .We call our proposed method QSBP: Query Snow-ball with Word Pairs.2254 Experiments4.1 Experimental EnvironmentACLIA1 ACLIA2Development Test Test#of questions 101 100 80*#of avg.
nuggets 5.8 12.8 11.2*Question types DEFINITION, BIOGRAPHY,RELATIONSHIP, EVENT +WHYArticles years 1998-2001 2002-2005Documents Mainichi Newspaper*After removing the factoid questions.Table 1: ACLIA dataset statisticsWe evaluate our method using Japanese QA testcollections from NTCIR-7 ACLIA1 and NTCIR-8 ACLIA2 (Mitamura et al, 2008; Mitamura etal., 2010).
The collections contain complex ques-tions and their answer nuggets with weights.
Ta-ble 1 shows some statistics of the data.
We use theACLIA1 development data for tuning a parameterfor our baseline as shown in Section 4.2 (whereasour proposed method is parameter-free), and theACLIA1 and ACLIA2 test data for evaluating dif-ferent methods The results for the ACLIA1 test dataare omitted due to lack of space.
As our aim isto answer complex questions by means of multi-document summarization, we removed factoid ques-tions from the ACLIA2 test data.Although the ACLIA test collections were origi-nally designed for Japanese QA evaluation, we treatthem as query-oriented summarization test collec-tions.
We use all the candidate documents fromwhich nuggets were extracted as input to the multi-document summarizers.
That is, in our problem set-ting, the relevant documents are already given, al-though the given document sets also occasionallycontain documents that were eventually never usedfor nugget extraction (Mitamura et al, 2008; Mita-mura et al, 2010).We preprocessed the Japanese documents basi-cally by automatically detecting sentence bound-aries based on Japanese punctuation marks, but wealso used regular-expression-based heuristics to de-tect glossary of terms in articles.
As the descrip-tions of these glossaries are usually very useful foranswering BIOGRAPHY and DEFINITION ques-tions, we treated each term description (generallymultiple sentences) as a single sentence.We used Mecab (Kudo et al, 2004) for morpho-logical analysis, and calculated base word scoressb(w) using Mainichi articles from 1991 to 2005.We also used Mecab to convert each word to its baseform and to filter using POS tags to extract contentwords.
As for dependency parsing for distance com-putation, we used Cabocha (Kudo and Matsumoto,2000).
We did not use a stop word list or any otherexternal knowledge.Following the NTCIR-9 one click access tasksetting1, we aimed at generating summaries ofJapanese 500 characters or less.
To evaluate thesummaries, we followed the practices at the TACsummarization tasks (Dang, 2008) and NTCIRACLIA tasks, and computed pyramid-based preci-sion with an allowance parameter of C, recall, F?
(where ?
is 1 or 3) scores.
The value of C wasdetermined based on the average nugget length foreach question type of the ACLIA2 collection (Mita-mura et al, 2010).
Precision and recall are computedbased on the nuggets that the summary covered aswell as their weights.
The first author of this papermanually evaluated whether each nugget matches asummary.
The evaluation metrics are formally de-fined as follows:precision = min(C ?
(] of matched nuggets)summary length , 1),recall = sum of weights over matched nuggetssum of weights over all nuggets ,F?
= (1 + ?2) ?
precision ?
recall?2 ?
recision + recall .4.2 BaselineMMR is a popular approach in query-oriented sum-marization.
For example, at the TAC 2008 opin-ion summarization track, a top performer in termsof pyramid F score used an MMR-based method.Our own implementation of an MMR-based base-line uses an existing algorithm to maximize the fol-lowing summary set score function (Lin and Bilmes,2010):fMMR(S) = ?
(?u?SSim(u, vD) +?u?SSim(u, vQ))?
(1 ?
?)?
{(ui,uj)|i 6=j and ui,uj?S}Sim(ui, uj) (4)where vD is the vector representing the source docu-ments, vQ is the vector representing the query terms,Sim is the cosine similarity, and ?
is a parameter.1http://research.microsoft.com/en-us/people/tesakai/1click.aspx226Thus, the first term of this function reflects how thesentences reflect the entire documents; the secondterm reflects the relevance of the sentences to thequery; and finally the function penalizes redundantsentences.
We set ?
to 0.8 and the scaling factorused in the algorithm to 0.3 based on a preliminaryexperiment with a part of the ACLIA1 developmentdata.
We also tried incorporating sentence positioninformation (Radev, 2001) to our MMR baseline butthis actually hurt performance in our preliminary ex-periments.4.3 Variants of the Proposed MethodTo clarify the contributions of each components, theminimum dependency distance, QSB and the wordpair, we also evaluated the following simplified ver-sions of QSBP.
(We use the itf version by default,and will refer to the idf version as QSBP(idf). )
Toexamine the contribution of using minimum depen-dency distance, We remove distance(w,w?)
fromEq.1 and Eq.2.
We call the method QSBP(nodist).To examine the contribution of using word pairs forscore maximization (see Section 3.2) on the perfor-mance of QSBP, we replaced Eq.3 with:fQSB(S) =?
{w|w?ui and ui?S}sr(w) .
(5)To examine the contribution of the QSB relevancescoring (see Section 3.1) on the performance ofQSBP, we replaced Eq.3 with:fWP (S) =?
{w1,w2|w1 6=w2 and w1,w2?ui and ui?S}sb(w1)sb(w2) .
(6)We will refer to this as WP.
Note that this relies onlyon base word scores and is query-independent.4.4 ResultsTables 2 and 3 summarize our results.
We usedthe two-tailed sign test for testing statistical signif-icance.
Significant improvements over the MMRbaseline are marked with a ?
(?=0.05) or a ?
(?=0.01); those over QSBP(nodist) are marked witha ] (?=0.05) or a ]] (?=0.01); and those over QSBare marked with a ?
(?=0.05) or a ??
(?=0.01); andthose over WP are marked with a ?
(?=0.05) or a??
(?=0.01).
From Table 2, it can be observed thatboth QSBP and QSBP(idf) significantly outperformsQSBP(nodist), QSB, WP and the baseline in termsof all evaluation metrics.
Thus, the minimum depen-dency distance, Query Snowball and the use of wordpairs all contribute significantly to the performanceof QSBP.
Note that we are using the ACLIA data assummarization test collections and that the officialQA results of ACLIA should not be compared withours.QSBP and QSBP(idf) achieve 0.312 and 0.313 inF3 score, and the differences between the two arenot statistically significant.
Table 3 shows the F3scores for each question type.
It can be observedthat QSBP is the top performer for BIO, DEF andREL questions on average, while QSBP(idf) is thetop performer for EVENT and WHY questions onaverage.
It is possible that different word scoringmethods work well for different question types.Method Precision Recall F1 score F3 scoreBaseline 0.076??
0.370??
0.116??
0.231?
?QSBP 0.107????? ]]
0.482????? ]]
0.161????? ]]
0.312?????
]]QSBP(idf) 0.106????? ]]
0.485????? ]]
0.161????? ]]
0.313?????
]]QSBP(nodist) 0.083???
0.396??
0.125??
0.248?
?QSB 0.086???
0.400??
0.129???
0.253??
?WP 0.053 0.222 0.080 0.152Table 2: ACLIA2 test data resultsType BIO DEF REL EVENT WHYBaseline 0.207?
0.251??
0.270 0.212 0.213QSBP 0.315??
0.329???
0.401?
0.258??? ]]
0.275?
]QSBP(idf) 0.304??]
0.328???
0.397?
0.268???
0.280?
?QSBP(nodist) 0.255 0.281??
0.329 0.196 0.212?
?QSB 0.245??
0.273??
0.324 0.217 0.215WP 0.109 0.037 0.235 0.141 0.161Table 3: F3-scores for each question type (ACLIA2 test)5 Conclusions and Future workWe proposed the Query Snowball (QSB) method forquery-oriented multi-document summarization.
Toenrich the information need representation of a givenquery, QSB obtains words that augment the originalquery terms from a co-occurrence graph.
We thenformulated the summarization problem as an MCKPbased on word pairs rather than single words.
Ourmethod, QSBP, achieves a pyramid F3-score of upto 0.313 with the ACLIA2 Japanese test collection,a 36% improvement over a baseline using MaximalMarginal Relevance.Moreover, as the principles of QSBP are basicallylanguage independent, we will investigate the effec-tiveness of QSBP in other languages.
Also, we planto extend our approach to abstractive summariza-tion.227ReferencesWauter Bosma.
2009.
Contextual salience in query-based summarization.
In Proceedings of the Interna-tional Conference RANLP-2009, pages 39?44.
Asso-ciation for Computational Linguistics.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofthe 21st annual international ACM SIGIR conferenceon Research and development in information retrieval,SIGIR ?98, pages 335?336.
Association for Comput-ing Machinery.Asli Celikyilmaz and Dilek Hakkani-Tur.
2010.
A hy-brid hierarchical model for multi-document summa-rization.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, ACL?10, pages 815?824.
Association for ComputationalLinguistics.Hoa Trang Dang.
2008.
Overview of the tac 2008 opin-ion question answering and summarization tasks.
InProceedings of Text Analysis Conference.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04.
Association for Computational Linguis-tics.Takaaki Hasegawa, Hitoshi Nishikawa, Kenji Imamura,Genichiro Kikui, and Manabu Okumura.
2010.
AWeb Page Summarization for Mobile Phones.
Trans-actions of the Japanese Society for Artificial Intelli-gence, 25:133?143.Jagadeesh Jagarlamudi, Prasad Pingali, and VasudevaVarma.
2005.
A relevance-based language modelingapproach to duc 2005.
In Proceedings of DocumentUnderstanding Conferences (along with HLT-EMNLP2005).Samir Khuller, Anna Moss, and Joseph S. Naor.
1999.The budgeted maximum coverage problem.
Informa-tion Processing Letters, 70(1):39?45.Taku Kudo and Yuji Matsumoto.
2000.
Japanese de-pendency structure analysis based on support vectormachines.
In Proceedings of the 2000 Joint SIGDATconference on Empirical methods in natural languageprocessing and very large corpora: held in conjunc-tion with the 38th Annual Meeting of the Associationfor Computational Linguistics, volume 13, pages 18?25.
Association for Computational Linguistics.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields to Japanesemorphological analysis.
In Proceedings of the Confer-ence on Emprical Methods in Natural Language Pro-cessing (EMNLP 2004), volume 2004, pages 230?237.Wenjie Li, You Ouyang, Yi Hu, and Furu Wei.
2008.PolyU at TAC 2008.
In Proceedings of Text AnalysisConference.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submodularfunctions.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,HLT ?10, pages 912?920.
Association for Computa-tional Linguistics.Hui Lin, Jeff Bilmes, and Shasha Xie.
2010a.
Graph-based submodular selection for extractive summariza-tion.
In Automatic Speech Recognition & Understand-ing, 2009.
ASRU 2009.
IEEEWorkshop on, pages 381?386.
IEEE.Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr.
2010b.Putting the user in the loop: interactive maximalmarginal relevance for query-focused summarization.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 305?308.
Association for Computational Lin-guistics.Fei Liu and Yang Liu.
2009.
From extractive to abstrac-tive meeting summaries: can it be done by sentencecompression?
In Proceedings of the ACL-IJCNLP2009 Conference Short Papers, ACLShort ?09, pages261?264.
Association for Computational Linguistics.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.Inderjeet Mani.
2001.
Automatic summarization.
JohnBenjamins Publishing Co.RyanMcDonald.
2007.
A study of global inference algo-rithms in multi-document summarization.
In Proceed-ings of the 29th European conference on IR research,ECIR?07, pages 557?564.
Springer-Verlag.Teruko Mitamura, Eric Nyberg, Hideki Shima, TsuneakiKato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song,Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, andNoriko Kando.
2008.
Overview of the NTCIR-7ACLIA tasks: Advanced cross-lingual information ac-cess.
In Proceedings of the 7th NTCIR Workshop.Teruko Mitamura, Hideki Shima, Tetsuya Sakai, NorikoKando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin,Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee.2010.
Overview of the NTCIR-8 ACLIA tasks: Ad-vanced cross-lingual information access.
In Proceed-ings of the 8th NTCIR Workshop.Jahna Otterbacher, Gu?nes?
Erkan, and Dragomir R. Radev.2005.
Using random walks for question-focused sen-tence retrieval.
In Proceedings of the conference onHuman Language Technology and Empirical Methods228in Natural Language Processing, HLT ?05, pages 915?922.
Association for Computational Linguistics.Dragomir R. Radev.
2001.
Experiments in single andmultidocument summarization using mead.
In FirstDocument Understanding Conference.Hiroya Takamura and Manabu Okumura.
2009a.
Textsummarization model based on maximum coverageproblem and its variant.
In Proceedings of the 12thConference of the European Chapter of the ACL(EACL 2009), pages 781?789.
Association for Com-putational Linguistics.Hiroya Takamura and Manabu Okumura.
2009b.
Textsummarization model based on the budgeted medianproblem.
In Proceeding of the 18th ACM conferenceon Information and knowledge management, CIKM?09, pages 1589?1592.
Association for ComputingMachinery.Ramakrishna Varadarajan and Vagelis Hristidis.
2006.A system for query-specific document summarization.In Proceedings of the 15th ACM international con-ference on Information and knowledge management,CIKM ?06, pages 622?631.
ACM.Ellen M. Voorhees.
2003.
Overview of the TREC2003 Question Answering Track.
In Proceedings ofthe Twelfth Text REtrieval Conference (TREC 2003),pages 54?68.Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, andHisami Suzuki.
2007.
Multi-document summariza-tion by maximizing informative content-words.
InProceedings of the 20th international joint conferenceon Artifical intelligence, pages 1776?1782.
MorganKaufmann Publishers Inc.229
