Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 384?389, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUT-DB: An Experimental Study on Sentiment Analysis in TwitterZhemin Zhu Djoerd Hiemstra Peter Apers Andreas WombacherCTIT Database Group, University of TwenteDrienerlolaan 5, 7500 AE, Enschede, The Netherlands{z.zhu, d.hiemstra, p.m.g.apers, A.Wombacher}@utwente.nlAbstractThis paper describes our system for participat-ing SemEval2013 Task2-B (Kozareva et al2013): Sentiment Analysis in Twitter.
Givena message, our system classifies whether themessage is positive, negative or neutral senti-ment.
It uses a co-occurrence rate model.
Thetraining data are constrained to the data pro-vided by the task organizers (No other tweetdata are used).
We consider 9 types of fea-tures and use a subset of them in our submittedsystem.
To see the contribution of each type offeatures, we do experimental study on featuresby leaving one type of features out each time.Results suggest that unigrams are the most im-portant features, bigrams and POS tags seemnot helpful, and stopwords should be retainedto achieve the best results.
The overall resultsof our system are promising regarding the con-strained features and data we use.1 IntroductionThe past years have witnessed the emergence andpopularity of short messages such as tweets andSMS messages.
Comparing with the traditional gen-res such as newswire data, tweets are very short anduse informal grammar and expressions.
The short-ness and informality make them a new genre andbring new challenges to sentiment analysis (Pang etal., 2002) as well as other NLP applications suchnamed entity recognition (Habib et al 2013).Recently a wide range of methods and featureshave been applied to sentimental analysis overtweets.
Go et al(2009) train sentiment classi-fiers using machine learning methods, such as NaiveBayes, Maximum Entropy and SVMs, with differentcombinations of features such as unigrams, bigramsand Part-of-Speech (POS) tags.
Microblogging fea-tures such as hashtags, emoticons, abbreviations, all-caps and character repetitions are also found help-ful (Kouloumpis et al 2011).
Saif et al(2012)train Naive Bayes models with semantic features.Also the lexicon prior polarities have been provedvery useful (Agarwal et al 2011).
Davidov et al(2010) utilize hashtags and smileys to build a large-scale annotated tweet dataset automatically.
Thisavoids the need for labour intensive manual anno-tation.
Due to the fact that tweets are generated con-stantly, sentiment analysis over tweets has some in-teresting applications, such as predicting stock mar-ket movement (Bollen et al 2011) and predictingelection results (Tumasjan et al 2010; O?Connor etal., 2010).But there are still some unclear parts in the lit-erature.
For example, it is unclear whether usingPOS tags improves the sentiment analysis perfor-mance or not.
Conflicting results are reported (Pakand Paroubek, 2010; Go et al 2009).
It is alsoa little surprising that not removing stopwords in-creases performance (Saif et al 2012).
In this pa-per, we build a system based on the concept of co-occurrence rate.
9 different types of features are con-sidered.
We find that using a subset of these featuresachieves the best results in our system, so we usethis subset of features rather than all the 9 types offeatures in our submitted system.
To see the contri-bution of each type of features, we perform experi-ments by leaving one type of features out each time.Results show that unigrams are the most important384features, bigrams and POS tags seem not helpful,and retaining stopwords makes the results better.The overall results of our system are also promis-ing regarding the constrained features and data weuse.2 System Description2.1 MethodWe use a supervised method which is similar to theNaive Bayes classifier.
The score of a tweet, denotedby t, and a sentiment category, denoted by c, is cal-culated according to the following formula:Score(t, c) = [n?i=1logCR(fi, c)] + logP (c),where fi is a feature extracted from t. The sentimentcategory c can be positive, negative or neutral.
AndCR(fi, c) is Co-occurrence Rate (CR) of fi and cwhich can be obtained as follows:CR(f, c) =P (fi, c)P (fi)P (c)?#(fi, c)#(fi)#(c),where #(?)
is the number of times that the pattern?
appears in the training dataset.
Then the categoryof the highest score arg maxc Score(t, c) is the pre-diction.This method assumes all the features are inde-pendent which is also the assumption of the NaiveBayes model.
But our model excludes P (fi) be-cause they are observations.
Hence comparing withNaive Bayes, our model saves the effort to modelfeature distributions P (fi).
Also this method canbe trained efficiently because it only depends on theempirical distributions.2.2 FeaturesTo make our system general, we constrain to the textfeatures.
That is we do not use the features outsidethe tweet texts such as features related to the userprofiles, discourse information or links.
The follow-ing 9 types of features are considered:1.
Unigrams.
We use lemmas as the form of un-igrams.
The lemmas are obtained by the Stan-ford CoreNLP1 (Toutanova et al 2003).
Hash-1http://nlp.stanford.edu/software/corenlp.shtmltags and emoticons are also considered as un-igrams.
Some of the unigrams are stopwordswhich will be discussed in the next section.2.
Bigrams.
We consider two adjacent lemmas asbigrams.3.
Named entities.
We use the CMU Twitter Tag-ger (Gimpel et al 2011; Owoputi et al 2013)2to recognize named entities.
The tokens cov-ered by a named entity are not considered asunigrams any more.
Instead a named entity asa whole is treated as a single feature.4.
Dependency relations.
Dependency relationsare helpful to the sentiment prediction.
Here wegive an example to explain this type of features.In the tweet ?I may not be able to vote fromBritain but I COMPLETLEY support you!!!!?
,the dependency relation between the word ?not?and ?able?
is ?NEG?
which stands for nega-tion, and the dependency relation between theword ?COMPLETELY?
and ?support?
is ?ADV-MOD?
which means adverb modifier.
For thisexample, we add ?NEG able?
and ?completelysupport?
as dependency features to our system.We use Stanford CoreNLP (Klein and Man-ning, 2003a; Klein and Manning, 2003b) to ob-tain dependencies.
And we only consider twotypes of dependencies ?NEG?
and ?ADVMOD?.Other dependency relations are not helpful.5.
Lexicon prior polarity.
The prior polarity oflexicons have been proved very useful to sen-timent analysis.
Many lexicon resources havebeen developed.
But for a single lexicon re-source, the coverage is limited.
To achievebetter coverage, we merge three lexicon re-sources.
The first one is SentiStrength3 (Ku-cuktunc et al 2012).
SentiStrength provides afine-granularity system for grading lexicon po-larity which ranges from ?5 (most negative) to+5 (most positive).
Our grading system con-sists of three categories: negative, neutral andpositive.
So we map the words ranging from?5 to ?1 in SentiStrength to negative in ourgrading system, and the words ranging from2http://www.ark.cs.cmu.edu/TweetNLP/3http://sentistrength.wlv.ac.uk/385+1 to +5 to positive.
The rest are mappedto neutral.
We do the same for the other twolexicon resources: OpinionFinder4 (Wiebe etal., 2005) and SentiWordNet5 (Esuli and Sebas-tiani, 2006; Baccianella and Sebastiani, 2010).6.
Intensifiers.
The tweets containing intensifiersare more likely to be non-neutral.
In the sub-mitted system, we merge the boosters in Sen-tiStrength and the intensifiers in OpinionFinderto form a list of intensifiers.
Some of these in-tensifiers strengthen emotion (e.g.
?definitely?
),but others weaken emotion (e.g.
?slightly?
).They are distinguished and assigned with dif-ferent labels {intensifier strengthen,intensifier weaken}.7.
All-caps and repeat characters.
All-caps6 andrepeat characters are common expressions intweets to make emphasis on the applied tokens.They can be considered as implicit intensifiers.In our system, we first normalize the repeatcharacters.
For example, happyyyy is nor-malized to happy as there are ?
3 consequenty.
Then they are treated in the same way asintensifier features discussed above.8.
Interrogative sentence.
Interrogative sentencesare more likely to be neutral.
So we add if atweet includes interrogative sentences as a fea-ture to our system.
The sentences ending witha question mark ???
are considered as inter-rogative sentences.
We first use the StanfordCoreNLP to find the sentence boundaries in atweet, then check the ending mark of each sen-tence.9.
Imperative sentence.
Intuitively, imperativesentences are more likely to be negative.
Soif a tweet contains imperative sentences can bea feature.
We consider the sentences start witha verb as imperative sentences.
The verbs areidentified by the CMU Twitter Tagger.We further filter out the low-frequency featureswhich have been observed less than 3 times in the4https://code.google.com/p/opinionfinder/5http://sentiwordnet.isti.cnr.it/6All characters of a token are in upper case.training data.
Because these features are not stableindicators of sentiment.
Our experiments show thatremoving these low-frequency features increases theaccuracy.2.3 Pre-processingThe pre-processing of our system includes two steps.In the first step, we replace the abbreviations as de-scribed in Section 2.3.1.
In the second step, we usethe CMU Twitter Tagger to extract the features ofemoticons (e.g.
:)), hashtags (e.g.
#Friday), re-ciepts (e.g.
@Peter) and URLs, and remove thesesymbols from tweet texts for further processing.2.3.1 Replacing AbbreviationsAbbreviations are replaced by their original ex-pressions.
We use the Internet Lingo Dictionary(Wasden, 2010) to obtain the original expressionsof abbreviations.
This dictionary originally contains748 acronyms.
But we do not use the acronyms inwhich all characters are digits.
Because we find theyare more likely to be numbers than acronyms.
Thisresults in 735 acronyms.3 ExperimentsOur system is implemented in Java and organizedas a pipeline consisting of a sequence of annotatorsand extractors.
This architecture is very similar tothe framework of UIMA (Ferrucci and Lally, 2004).With such an architecture, we can easily vary theconfigurations of our system.3.1 DatasetsWe use the standard dataset provided by Se-mEval2013 Task2-B (Kozareva et al 2013) fortraining and testing.
The training and develop-ment data provided are merged together to train ourmodel.
Originally, the training and developmentdata contain 9,684 and 1,654 instances, respectively.But due to the policy of Twitter, only the tweet IDscan be released publicly.
So we need to fetch the ac-tual tweets by their IDs.
Some of the tweets are nolonger existing after they were downloaded for an-notation.
So the number of tweets used for trainingis less than the original tweets provided by the orga-nizers.
In our case, we obtained 10,370 tweets fortraining our model.386Class Precision Recall F-ScorePositive 74.86 60.05 66.64Negative 47.80 59.73 53.11Neutral 67.02 73.60 70.15Avg (Pos & Neg) 61.33 59.89 59.87Table 1: Submitted System on Twitter DataClass Precision Recall F-ScorePositive 54.81 57.93 56.32Negative 37.87 67.77 48.59Neutral 80.78 58.11 67.60Avg (Pos & Neg) 46.34 62.85 52.46Table 2: Submitted System on SMS DataThere are two test datasets: Twitter and SMS.
Thefirst dataset consists of 3,813 twitter messages andthe second dataset contains 2,094 SMS messages.The purpose of having a separate test set of SMSmessages is to see how well systems trained on twit-ter data will generalize to other types of data.3.2 Results of Our Submitted SystemWe use a subset of features described in Section 2.2in our submitted system: unigrams, named entities,dependency relations, lexicon prior polarity, inten-sifiers, all-caps and repeat characters, interrogativeand imperative sentences.
The official results on thetwo datasets are given in Table (1, 2).
Our system isranked as #14/51 on the Twitter dataset and #18/44on the SMS dataset.3.3 Feature Contribution AnalysisTo see the contribution of each type of features, wevary the configuration of our system by leaving onetype of features out each time.
The results are listedin Table 3.In Table 3, ?Y(T)?
means the corresponding fea-ture is used and the test dataset is the Twitter Data,and ?N(sms)?
means the corresponding feature is leftout and the test dataset is SMS Data.From Table 3, we can see that unigrams are themost important features.
Leaving out unigramsleads to a radical decrease of F-scores.
On the Twit-ter dataset, the F-score drops from 59.87 to 41.44,and on the SMS dataset, the F-score drops from52.64 to 35.09.
And also filtering out the low-Feature Y(T) N(T) Y(sms) N(sms)Stopword 59.87 58.19 52.64 51.00POS Tag 58.68 59.87 51.87 52.64Bigram 58.47 59.87 51.94 52.64Unigram 59.87 41.22 52.64 35.093 ?
59.87 57.66 52.64 51.20Intensifier 59.87 59.47 52.64 52.39Lexicon 59.87 58.33 52.64 51.26Named Ent.
59.87 59.71 52.64 51.80Interrogative 59.87 59.67 52.64 52.93Imperative 59.87 59.54 52.64 52.14Dependence 59.87 59.37 52.64 52.08Table 3: Avg (Pos & Neg) of Leave-one-out Experimentsfrequency features which happens less than 3 timesincreases the F-scores on Twitter data from 57.66 to59.87, and on SMS data from 51.20 to 52.64.
Re-moving stopwords decreases the scores by 1.66 per-cent.
This result is consistent with that reported bySaif et al(2012).
By taking a close look at thestopwords we use, we find that some of the stop-words are highly related to the sentiment polarity,such as ?can?, ?no?, ?very?
and ?want?, but othersare not, such as ?the?, ?him?
and ?on?.
Removingthe stopwords which are related to the sentimentis obviously harmful.
This means the stopwordswhich originally developed for the purpose of in-formation retrieval are not suitable for sentimentalanalysis.
Dependency relations are also helpful fea-tures which increase F-scores by about 0.5 percent.The POS tags and bigrams seem not helpful in ourexperiments, which is consistent with the results re-ported by (Kouloumpis et al 2011).4 ConclusionsWe described the method and features used in oursystem.
We also did analysis on feautre contribu-tion.
Experiment results suggest that unigrams arethe most important features, POS tags and bigramsseem not helpful, filtering out the low-frequency fea-tures is helpful and retaining stopwords makes theresults better.AcknowledgementsThis work has been supported by the Dutch nationalprogram COMMIT.387ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof twitter data.
In Proceedings of the Workshop onLanguages in Social Media, LSM ?11, pages 30?38,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Andrea Esuli Stefano Baccianella and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European LanguageResources Association (ELRA).J.
Bollen, H. Mao, and X. Zeng.
2011.
Twitter moodpredicts the stock market.
Journal of ComputationalScience.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 241?249, Stroudsburg, PA, USA.Association for Computational Linguistics.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinionmining.
In In Proceedings of the 5th Conference onLanguage Resources and Evaluation (LREC06, pages417?422.David Ferrucci and Adam Lally.
2004.
Uima: an archi-tectural approach to unstructured information process-ing in the corporate research environment.
Nat.
Lang.Eng., 10(3-4):327?348, September.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging fortwitter: annotation, features, and experiments.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 42?47, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Technical report, Stanford University.M.
B. Habib, M. van Keulen, and Z. Zhu.
2013.
Con-cept extraction challenge: University of twente at#msm2013.
In Proceedings of the 3rd workshop on?Making Sense of Microposts?
(#MSM2013), Rio deJaneiro, Brazil, Brazil, May.
CEUR.Dan Klein and Christopher D. Manning.
2003a.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Dan Klein and Christopher D. Manning.
2003b.
Fastexact inference with a factored model for natural lan-guage parsing.
In Advances in Neural InformationProcessing Systems, volume 15.
MIT Press.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In Lada A. Adamic, Ricardo A.Baeza-Yates, and Scott Counts, editors, ICWSM.
TheAAAI Press.Zornitsa Kozareva, Preslav Nakov, Alan Ritter, SaraRosenthal, Veselin Stoyonov, and Theresa Wilson.2013.
Sentiment analysis in twitter.
In Proceedingsof the 7th International Workshop on Semantic Evalu-ation.
Association for Computation Linguistics.Onur Kucuktunc, B. Barla Cambazoglu, Ingmar Weber,and Hakan Ferhatosmanoglu.
2012.
A large-scalesentiment analysis for yahoo!
answers.
In Proceed-ings of the fifth ACM international conference on Websearch and data mining, WSDM ?12, pages 633?642,New York, NY, USA.
ACM.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment to publicopinion time series.
In Proceedings of the FourthInternational Conference on Weblogs and SocialMedia, ICWSM 2010, Washington, DC, USA, May23-26, 2010.Olutobi Owoputi, Brendan OConnor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Hello, who is calling?
: Can words reveal the socialnature of conversations?
In Proceedings of the 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies.
Association for ComputationalLinguistics, June.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing - Volume 10, EMNLP ?02, pages 79?38886, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Hassan Saif, Yulan He, and Harith Alani.
2012.
Seman-tic sentiment analysis of twitter.
In Proceedings of the11th international conference on The Semantic Web -Volume Part I, ISWC?12, pages 508?524, Berlin, Hei-delberg.
Springer-Verlag.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.A.
Tumasjan, T.O.
Sprenger, P.G.
Sandner, and I.M.Welpe.
2010.
Predicting elections with twitter: What140 characters reveal about political sentiment.
InProceedings of the Fourth International AAAI Confer-ence on Weblogs and Social Media, pages 178?185.Lawrence Wasden.
2010.
Internet lingo dictionary: Aparents guide to codes used in chat rooms, instant mes-saging, text messaging, and blogs.
Technical report,Attorney General.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotionsin language.
Language Resources and Evaluation,1(2):0.389
