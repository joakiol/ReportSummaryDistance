Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 31?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPronunciation Modeling in Spelling Correctionfor Writers of English as a Foreign LanguageAdriane BoydDepartment of LinguisticsThe Ohio State University1712 Neil AvenueColumbus, Ohio 43210, USAadriane@ling.osu.eduAbstractWe propose a method for modeling pronunci-ation variation in the context of spell checkingfor non-native writers of English.
Spell check-ers, typically developed for native speakers,fail to address many of the types of spellingerrors peculiar to non-native speakers, espe-cially those errors influenced by differences inphonology.
Our model of pronunciation varia-tion is used to extend a pronouncing dictionaryfor use in the spelling correction algorithmdeveloped by Toutanova and Moore (2002),which includes models for both orthographyand pronunciation.
The pronunciation vari-ation modeling is shown to improve perfor-mance for misspellings produced by Japanesewriters of English.1 IntroductionSpell checkers identify misspellings, select appro-priate words as suggested corrections, and rank thesuggested corrections so that the likely intendedword is high in the list.
Since traditional spellcheckers have been developed with competent na-tive speakers as the target users, they do not appro-priately address many types of errors made by non-native writers and they often fail to suggest the ap-propriate corrections.
Non-native writers of Englishstruggle with many of the same idiosyncrasies of En-glish spelling that cause difficulty for native speak-ers, but differences between English phonology andthe phonology of their native language lead to typesof spelling errors not anticipated by traditional spellcheckers (Okada, 2004; Mitton and Okada, 2007).Okada (2004) and Mitton and Okada (2007) in-vestigate spelling errors made by Japanese writersof English as a foreign language (JWEFL).
Okada(2004) identifies two main sources of errors forJWEFL: differences between English and Japanesephonology and differences between the English al-phabet and the Japanese romazi writing system,which uses a subset of English letters.
Phonolog-ical differences result in number of distinctions inEnglish that are not present in Japanese and romazicauses difficulties for JWEFL because the Latin let-ters correspond to very different sounds in Japanese.We propose a method for creating a model ofpronunciation variation from a phonetically untran-scribed corpus of read speech recorded by non-native speakers.
The pronunciation variation modelis used to generate multiple pronunciations for eachcanonical pronunciation in a pronouncing dictionaryand these variations are used in the spelling correc-tion approach developed by Toutanova and Moore(2002), which uses statistical models of spelling er-rors that consider both orthography and pronuncia-tion.
Several conventions are used throughout thispaper: a word is a sequence of characters from thegiven alphabet found in the word list.
A word listis a list of words.
A misspelling, marked with *, isa sequence of characters not found in the word list.A candidate correction is a word from the word listproposed as a potential correction.2 BackgroundResearch in spell checking (see Kukich, 1992, fora survey of spell checking research) has focusedon three main problems: non-word error detec-tion, isolated-word error correction, and context-dependent word correction.
We focus on the firsttwo tasks.
A non-word is a sequence of letters that31is not a possible word in the language in any con-text, e.g., English *thier.
Once a sequence of let-ters has been determined to be a non-word, isolated-word error correction is the process of determiningthe appropriate word to substitute for the non-word.Given a sequence of letters, there are thus twomain subtasks: 1) determine whether this is a non-word, 2) if so, select and rank candidate words aspotential corrections to present to the writer.
Thefirst subtask can be accomplished by searching forthe sequence of letters in a word list.
The secondsubtask can be stated as follows (Brill and Moore,2000): Given an alphabet ?, a word list D of strings?
?
?, and a string r /?
D and ?
?
?, find w ?
Dsuch that w is the most likely correction.
Minimumedit distance is used to select the most likely candi-date corrections.
The general idea is that a minimumnumber of edit operations such as insertion and sub-stitution are needed to convert the misspelling into aword.
Words requiring the smallest numbers of editoperations are selected as the candidate corrections.2.1 Edit Operations and Edit WeightsIn recent spelling correction approaches, edit op-erations have been extended beyond single charac-ter edits and the methods for calculating edit opera-tion weights have become more sophisticated.
Thespelling error model proposed by Brill and Moore(2000) allows generic string edit operations up to acertain length.
Each edit operation also has an asso-ciated probability that improves the ranking of can-didate corrections by modeling how likely particu-lar edits are.
Brill and Moore (2000) estimate theprobability of each edit from a corpus of spelling er-rors.
Toutanova and Moore (2002) extend Brill andMoore (2000) to consider edits over both letter se-quences and sequences of phones in the pronuncia-tions of the word and misspelling.
They show thatincluding pronunciation information improves per-formance as compared to Brill and Moore (2000).2.2 Noisy Channel Spelling CorrectionThe spelling correction models from Brill andMoore (2000) and Toutanova and Moore (2002) usethe noisy channel model approach to determine thetypes and weights of edit operations.
The idea be-hind this approach is that a writer starts out with theintended word w in mind, but as it is being writ-ten the word passes through a noisy channel result-ing in the observed non-word r. In order to de-termine how likely a candidate correction is, thespelling correction model determines the probabil-ity that the word w was the intended word given themisspelling r: P (w|r).
To find the best correction,the wordw is found for whichP (w|r) is maximized:argmaxw P (w|r).
Applying Bayes?
Rule and dis-carding the normalizing constant P (r) gives the cor-rection model:argmaxw P (w|r) = argmaxw P (w)P (r|w)P (w), how probable the word w is overall, andP (r|w), how probable it is for a writer intending towrite w to output r, can be estimated from corporacontaining misspellings.
In the following experi-ments, P (w) is assumed be equal for all words to fo-cus this work on estimating the error model P (r|w)for JWEFL.1Brill and Moore (2000) allow all edit operations?
?
?
where ?
is the alphabet and ?, ?
?
?
?, witha constraint on the length of ?
and ?.
In order toconsider all ways that a word w may generate r withthe possibility that any, possibly empty, substring ?of w becomes any, possibly empty, substring ?
ofr, it is necessary to consider all ways that w and rmay be partitioned into substrings.
This error modelover letters, called PL, is approximated by Brill andMoore (2000) as shown in Figure 1 by consideringonly the pair of partitions of w and r with the max-imum product of the probabilities of individual sub-stitutions.
Part(w) is all possible partitions of w,|R| is number of segments in a particular partition,and Ri is the ith segment of the partition.The parameters for PL(r|w) are estimated froma corpus of pairs of misspellings and target words.The method, which is described in detail in Brill andMoore (2000), involves aligning the letters in pairsof words and misspellings, expanding each align-ment with up to N neighboring alignments, and cal-culating the probability of each ?
?
?
alignment.Since we will be using a training corpus that con-sists solely of pairs of misspellings and words (seesection 3), we would have lower probabilities for1Of course, P (w) is not equal for all words, but it is notpossible to estimate it from the available training corpus, theAtsuo-Henry Corpus (Okada, 2004), because it contains onlypairs of words and misspellings for around 1,000 target words.32PL(r|w) ?
maxR?Part(r),T?Part(w)|R|?i=1P (Ri ?
Ti)PPHL(r|w) ?
?pronw1|pronw|maxpronrPPH(pronw|pronr)P (pronr|r)Figure 1: Approximations of PL from Brill and Moore (2000) and PPHL from Toutanova and Moore (2002)?
?
?
than would be found in a corpus with mis-spellings observed in context with correct words.
Tocompensate, we approximate P (?
?
?)
by assign-ing it a minimum probability m:P (?
?
?)
={m+ (1?m) count(???)count(?)
if ?
= ?
(1?m) count(???)count(?)
if ?
6= ?2.2.1 Extending to PronunciationToutanova and Moore (2002) describe an extensionto Brill and Moore (2000) where the same noisychannel error model is used to model phone se-quences instead of letter sequences.
Instead of theword w and the non-word r, the error model con-siders the pronunciation of the non-word r, pronr,and the pronunciation of the word w, pronw.
Theerror model over phone sequences, called PPH , isjust like PL shown in Figure 1 except that r and ware replaced with their pronunciations.
The model istrained like PL using alignments between phones.Since a spelling correction model needs to rankcandidate words rather than candidate pronuncia-tions, Toutanova and Moore (2002) derive an er-ror model that determines the probability that aword w was spelled as the non-word r based ontheir pronunciations.
Their approximation of thismodel, called PPHL, is also shown in Figure 1.PPH(pronw|pronr) is the phone error model de-scribed above and P (pronr|r) is provided by theletter-to-phone model described below.2.3 Letter-To-Phone ModelA letter-to-phone (LTP) model is needed to predictthe pronunciation of misspellings for PPHL, sincethey are not found in a pronouncing dictionary.
LikeToutanova and Moore (2002), we use the n-gramLTP model from Fisher (1999) to predict these pro-nunciations.
The n-gram LTP model predicts thepronunciation of each letter in a word consideringup to four letters of context to the left and right.
Themost specific context found for each letter and itscontext in the training data is used to predict the pro-nunciation of a word.
We extended the predictionstep to consider the most probable phone for the topM most specific contexts.We implemented the LTP algorithm and trainedand evaluated it using pronunciations from CMU-DICT.
A training corpus was created by pairing thewords from the size 70 CMUDICT-filtered SCOWLword list (see section 3) with their pronunciations.This list of approximately 62,000 words was splitinto a training set with 80% of entries and a test setwith the remaining 20%.
We found that the best per-formance is seen when M = 3, giving 95.5% phoneaccuracy and 74.9% word accuracy.2.4 Calculating Final ScoresFor a misspelling r and a candidate correction w,the letter model PL gives the probability that w waswritten as r due to the noisy channel taking into ac-count only the orthography.
PPH does the same forthe pronunciations of r and w, giving the probabilitythat pronw was output was pronr.
The pronuncia-tion model PPHL relates the pronunciations mod-eled by PPH to the orthography in order to give theprobability that r was written as w based on pronun-ciation.
PL and PPHL are then combined as followsto calculate a score for each candidate correction.SCMB(r|w) = logPL(r|w) + ?logPPHL(r|w)3 Resources and Data PreparationOur spelling correction approach, which includeserror models for both orthography and pronuncia-tion (see section 2.2) and which considers pronun-ciation variation for JWEFL requires a number ofresources: 1) spoken corpora of American English(TIMIT, TIMIT 1991) and Japanese English (ERJ,see below) are used to model pronunciation vari-ation, 2) a pronunciation dictionary (CMUDICT,CMUDICT 1998) provides American English pro-nunciations for the target words, 3) a corpus of33spelling errors made by JWEFL (Atsuo-Henry Cor-pus, see below) is used to train spelling error mod-els and test the spell checker?s performance, and 4)Spell Checker Oriented Word Lists (SCOWL, seebelow) are adapted for our use.The English Read by Japanese Corpus (Mine-matsu et al, 2002) consists of 70,000 prompts con-taining phonemic and prosodic cues recorded by 200native Japanese speakers with varying English com-petence.
See Minematsu et al (2002) for details onthe construction of the corpus.The Atsuo-Henry Corpus (Okada, 2004) in-cludes a corpus of spelling errors made by JWEFLthat consists of a collection of spelling errors frommultiple corpora.2 For use with our spell checker,the corpus has been cleaned up and modified to fitour task, resulting in 4,769 unique misspellings of1,046 target words.
The data is divided into training(80%), development (10%), and test (10%) sets.For our word lists, we use adapted versions of theSpell Checker Oriented Word Lists.3 The size 50word lists are used in order to create a general pur-pose word list that covers all the target words fromthe Atsuo-Henry Corpus.
Since the target pronun-ciation of each item is needed for the pronunciationmodel, the word list was filtered to remove wordswhose pronunciation is not in CMUDICT.
After fil-tering, the word list contains 54,001 words.4 MethodThis section presents our method for modeling pro-nunciation variation from a phonetically untran-scribed corpus of read speech.
The pronunciation-based spelling correction approach developed inToutanova and Moore (2002) requires a list of pos-sible pronunciations in order to compare the pro-nunciation of the misspelling to the pronunciationof correct words.
To account for target pronuncia-tions specific to Japanese speakers, we observe thepronunciation variation in the ERJ and generate ad-ditional pronunciations for each word in the wordlist.
Since the ERJ is not transcribed, we beginby adapting a recognizer trained on native English2Some of the spelling errors come from an elicitation task,so the distribution of target words is not representative of typi-cal JWEFL productions, e.g., the corpus contains 102 differentmisspellings of albatross.3SCOWL is available at http://wordlist.sourceforge.net.speech.
First, the ERJ is recognized using a mono-phone recognizer trained on TIMIT.
Next, the mostfrequent variations between the canonical and rec-ognized pronunciations are used to adapt the recog-nizer.
The adapted recognizer is then used to rec-ognize the ERJ in forced alignment with the canon-ical pronunciations.
Finally, the variations from theprevious step are used to create models of pronun-ciation variation for each phone, which are used togenerate multiple pronunciations for each word.4.1 Initial RecognizerA monophone speech recognizer was trained on allTIMIT data using the HiddenMarkovModel Toolkit(HTK).4 This recognizer is used to generate a phonestring for each utterance in the ERJ.
Each recog-nized phone string is then aligned with the canon-ical pronunciation provided to the speakers.
Correctalignments and substitutions are considered with nocontext and insertions are conditioned on the previ-ous phone.
Due to restrictions in HTK, deletions arecurrently ignored.The frequency of phone alignments for all utter-ances in the ERJ are calculated.
Because of the lowphone accuracy of monophone recognizers, espe-cially on non-native speech, alignments are observedbetween nearly all pairs of phones.
In order to focuson the most frequent alignments common to multi-ple speakers and utterances, any alignment observedless than 20% as often as the most frequent align-ment for that canonical phone is discarded, which re-sults in an average of three variants of each phone.54.2 Adapting the RecognizerNow that we have probability distributions over ob-served phones, the HMMs trained on TIMIT aremodified as follows to allow the observed varia-tion.
To allow, for instance, variation between pand th, the states for th from the original recog-nizer are inserted into the model for p as a separatepath.
The resulting phone model is shown in Fig-ure 2.
The transition probabilities into the first states4HTK is available at http://htk.eng.cam.ac.uk.5There are 119 variants of 39 phones.
The cutoff of 20%was chosen to allow a few variations for most phones.
A smallnumber of phones have no variants (e.g., iy, w) while a fewhave over nine variants (e.g., ah, l).
It is not surprising thatphones that are well-known to be difficult for Japanese speakers(cf.
Minematsu et al, 2002) are the ones with the most variation.34.
6.
4 th-1 th-2 th-3p-1 p-2 p-3Figure 2: Adapted phone model for p accounting for vari-ation between p and thof the phones come from the probability distributionobserved in the initial recognition step.
The transi-tion probabilities between the three states for eachvariant phone remain unchanged.
All HMMs areadapted in this manner using the probability distri-butions from the initial recognition step.The adapted HMMs are used to recognize the ERJCorpus for a second time, this time in forced align-ment with the canonical pronunciations.
The statetransitions indicate which variant of each phone wasrecognized and the correspondences between thecanonical phones and recognized phones are usedto generate a new probability distribution over ob-served phones for each canonical phone.
These areused to find the most probable pronunciation varia-tions for a native-speaker pronouncing dictionary.4.3 Generating PronunciationsThe observed phone variation is used to generatemultiple pronunciations for each pronunciation inthe word list.
The OpenFst Library6 is used to findthe most probable pronunciations in each case.
First,FSTs are created for each phone using the proba-bility distributions from the previous section.
Next,an FST is created for the entire word by concate-nating the FSTs for the pronunciation from CMU-DICT.
The pronunciations corresponding to the bestn paths through the FST and the original canon-ical pronunciation become possible pronunciationsin the extended pronouncing dictionary.
The size 50word list contains 54,001 words and when expandedto contain the top five variations of each pronuncia-tion, there are 255,827 unique pronunciations.5 ResultsIn order to evaluate the effect of pronunciation vari-ation in Toutanova and Moore (2002)?s spelling cor-rection approach, we compare the performance ofthe pronunciation model and the combined model6OpenFst is available at http://www.openfst.org/.with and without pronunciation variation.We implemented the letter and pronunciationspelling correction models as described in sec-tion 2.2.
The letter error model PL and the phoneerror model PPH are trained on the training set.The development set is used to tune the parametersintroduced in previous sections.7 In order to rankthe words as candidate corrections for a misspellingr, PL(r|w) and PPHL(r|w) are calculated for eachword in the word list using the algorithm describedin Brill and Moore (2000).
Finally, PL and PPHLare combined using SCMB to rank each word.5.1 BaselineThe open source spell checker GNU Aspell8 is usedto determine the baseline performance of a tradi-tional spell checker using the same word list.
AnAspell dictionary was created with the word list de-scribed in section 3.
Aspell?s performance is shownin Table 1.
The 1-Best performance is the percent-age of test items for which the target word was thefirst candidate correction, 2-Best is the percentagefor which the target was in the top two, etc.5.2 Evaluation of Pronunciation VariationThe effect of introducing pronunciation variation us-ing the method described in section 4 can be eval-uated by examining the performance on the test setfor PPHL with and without the additional variations.The results in Table 1 show that the addition of pro-nunciation variations does indeed improve the per-formance of PPHL across the board.
The 1-Best,3-Best, and 4-Best cases for PPHL with variationshow significant improvement (p<0.05) over PPHLwithout variation.5.3 Evaluation of the Combined ModelWe evaluated the effect of including pronunciationvariation in the combined model by comparing theperformance of the combined model with and with-out pronunciation variation, see results in Table 1.Despite the improvements seen in PPHL with pro-nunciation variation, there are no significant differ-ences between the results for the combined modelwith and without variation.
The combined model7The values are: N = 3 for the letter model, N = 4 for thephone model, m = 80%, and ?
= 0.15 in SCMB .8GNU Aspell is available at http://aspell.net.35Model 1-Best 2-Best 3-Best 4-Best 5-Best 6-BestAspell 44.1 54.0 64.1 68.3 70.0 72.5Letter (L) 64.7 74.6 79.6 83.2 84.0 85.3Pronunciation (PHL) without Pron.
Var.
47.9 60.7 67.9 70.8 75.0 77.3Pronunciation (PHL) with Pron.
Var.
50.6 62.2 70.4 73.1 76.7 78.2Combined (CMB) without Pron.
Var.
64.9 75.2 78.6 81.1 82.6 83.2Combined (CMB) with Pron.
Var.
65.5 75.0 78.4 80.7 82.6 84.0Table 1: Percentage of Correct Suggestions on the Atsuo-Henry Corpus Test Set for All ModelsRank Aspell L PHL CMB1 enemy enemy any enemy2 envy envy Emmy envy3 energy money Ne any4 eye emery gunny deny5 teeny deny ebony money6 Ne any anything emery7 deny nay senna nay8 any ivy journey ivyTable 2: Misspelling *eney, Intended Word anywith variation is also not significantly different fromthe letter model PL except for the drop in the 4-Bestcase.To illustrate the performance of each model, theranked lists in Table 2 give an example of the can-didate corrections for the misspelling of any as*eney.
Aspell preserves the initial letter of the mis-spelling and vowels in many of its candidates.
PL?stop candidates also overlap a great deal in orthogra-phy, but there is more initial letter and vowel varia-tion.
As we would predict, PPHL ranks any as thetop correction, but some of the lower-ranked candi-dates for PPHL differ greatly in length.5.4 Summary of ResultsThe noisy channel spelling correction approach de-veloped by Brill and Moore (2000) and Toutanovaand Moore (2002) appears well-suited for writersof English as a foreign language.
The letter andcombined models outperform the traditional spellchecker Aspell by a wide margin.
Although in-cluding pronunciation variation does not improvethe combined model, it leads to significant improve-ments in the pronunciation-based model PPHL.6 ConclusionWe have presented a method for modeling pronun-ciation variation from a phonetically untranscribedcorpus of read non-native speech by adapting amonophone recognizer initially trained on nativespeech.
This model allows a native pronouncingdictionary to be extended to include non-native pro-nunciation variations.
We incorporated a pronounc-ing dictionary extended for Japanese writers of En-glish into the spelling correction model developedby Toutanova and Moore (2002), which combinesorthography-based and pronunciation-based mod-els.
Although the extended pronunciation dictio-nary does not lead to improvement in the combinedmodel, it does leads to significant improvement inthe pronunciation-based model.AcknowledgmentsI would like to thank Eric Fosler-Lussier, the OhioState computational linguistics discussion group,and anonymous reviewers for their helpful feedback.ReferencesBrill, Eric and Robert C. Moore (2000).
An ImprovedError Model for Noisy Channel Spelling Correction.In Proceedings of ACL 2000.CMUDICT (1998).
CMU Pronouncing Dictionaryversion 0.6. http://www.speech.cs.cmu.edu/cgi-bin/cmudict.Fisher, Willam (1999).
A statistical text-to-phone func-tion using ngrams and rules.
In Proceedings of ICASSP1999.Kukich, Karen (1992).
Technique for automatically cor-recting words in text.
ACM Computing Surveys 24(4).Minematsu, N., Y. Tomiyama, K. Yoshimoto,K.
Shimizu, S. Nakagawa, M. Dantsuji, and S. Makino(2002).
English Speech Database Read by JapaneseLearners for CALL System Development.
InProceedings of LREC 2002.Mitton, Roger and Takeshi Okada (2007).
The adapta-tion of an English spellchecker for Japanese writers.In Symposium on Second Language Writing.Okada, Takeshi (2004).
A Corpus Analysis of SpellingErrors Made by Japanese EFL Writers.
Yamagata En-glish Studies 9.TIMIT (1991).
TIMIT Acoustic-Phonetic ContinuousSpeech Corpus.
NIST Speech Disc CD1-1.1.Toutanova, Kristina and Robert Moore (2002).
Pronunci-ation Modeling for Improved Spelling Correction.
InProceedings of ACL 2002.36
