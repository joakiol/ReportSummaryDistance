Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsHead Finalization Reordering for Chinese-to-JapaneseMachine TranslationHan Dan+ Katsuhito Sudoh?
Xianchao Wu?
?Kevin Duh??
Hajime Tsukada?
Masaaki Nagata?+The Graduate University For Advanced Studies, Tokyo, Japan?NTT Communication Science Laboratories, NTT Corporation+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp?
{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jpAbstractIn Statistical Machine Translation, reorder-ing rules have proved useful in extractingbilingual phrases and in decoding duringtranslation between languages that are struc-turally different.
Linguistically motivatedrules have been incorporated into Chinese-to-English (Wang et al, 2007) and English-to-Japanese (Isozaki et al, 2010b) transla-tion with significant gains to the statisticaltranslation system.
Here, we carry out a lin-guistic analysis of the Chinese-to-Japanesetranslation problem and propose one of thefirst reordering rules for this language pair.Experimental results show substantially im-provements (from 20.70 to 23.17 BLEU)when head-finalization rules based on HPSGparses are used, and further gains (to 24.14BLEU) were obtained using more refinedrules.1 IntroductionIn state-of-the-art Statistical Machine Translation(SMT) systems, bilingual phrases are the mainbuilding blocks for constructing a translation givena sentence from a source language.
To extractthose bilingual phrases from a parallel corpus,the first step is to discover the implicit word-to-word correspondences between bilingual sen-tences (Brown et al, 1993).
Then, a symmetriza-tion matrix is built (Och and Ney, 2004) by us-ing word-to-word alignments, and a wide variety?Now at Baidu Japan Inc.?
Now at Nara Institute of Science and Technology(NAIST)of heuristics can be used to extract the bilingualphrases (Zens et al, 2002; Koehn et al, 2003).This method performs relatively well when thesource and the target languages have similar wordorder, as in the case of French, Spanish, and En-glish.
However, when translating between lan-guages with very different structures, as in the caseof English and Japanese, or Japanese and Chinese,the quality of extracted bilingual phrases and theoverall translation quality diminishes.In the latter scenario, a simple but effective strat-egy to cope with this problem is to reorder thewords of sentences in one language so that it re-sembles the word order of another language (Wuet al, 2011; Isozaki et al, 2010b).
The advan-tages of this strategy are two fold.
The first ad-vantage is at the decoding stage, since it enablesthe translation to be constructed almost monoton-ically.
The second advantage is at the trainingstage, since automatically estimated word-to-wordalignments are likely to be more accurate and sym-metrization matrices reveal more evident bilingualphrases, leading to the extraction of better qualitybilingual phrases and cleaner phrase tables.In this work, we focus on Chinese-to-Japanesetranslation, motivated by the increasing interactionbetween these two countries and the need to im-prove direct machine translation without using apivot language.
Despite the countries?
close cul-tural relationship, their languages significantly dif-fer in terms of syntax, which poses a severe diffi-culty in statistical machine translation.
The syntac-tic relationship of this language pair has not beencarefully studied before in the machine translation57field, and our work aims to contribute in this direc-tion as follows:?
We present a detailed syntactic analysis ofseveral reordering issues in Chinese-Japanesetranslation using the information provided byan HPSG-based deep parser.?
We introduce novel reordering rules based onhead-finalization and linguistically inspiredrefinements to make words in Chinese sen-tences resemble Japanese word order.
We em-pirically show its effectiveness (e.g.
20.70 to24.23 BLEU improvement).The paper is structured as follows.
Section 2 in-troduces the background and gives an overview ofsimilar techniques related to this work.
Section 3describes the proposed method in detail.
Exper-imental evaluation of the performance of the pro-posed method is described in section 4.
There is anerror analysis on the obtained results in section 5.Conclusions and a short description on future workderived from this research are given in the finalsection.2 Background2.1 Head FinalizationThe structure of languages can be characterizedby phrase structures.
The head of a phrase is theword that determines the syntactic category of thephrase, and its modifiers (also called dependents)are the rest of the words within the phrase.
In En-glish, the head of a phrase can be usually foundbefore its modifiers.
For that reason, English iscalled a head-initial language (Cook and Newson,1988).
Japanese, on the other hand, is head-finallanguage (Fukui, 1992), since the head of a phrasealways appears after its modifiers.In certain applications, as in the case of ma-chine translation, word reordering can be a promis-ing strategy to ease the task when working withlanguages with different phrase structures like En-glish and Japanese.
Head Finalization is a success-ful syntax-based reordering method designed to re-order sentences from a head-initial language to re-semble the word order in sentences from a head-final language (Isozaki et al, 2010b).
The essenceof this rule is to move the syntactic heads to theend of its dependency by swapping child nodes ina phrase structure tree when the head child appearsbefore the dependent child.Isozaki et al (2010b) proposed a simple methodof Head Finalization, by using an HPSG-baseddeep parser for English (Miyao and Tsujii, 2008)to obtain phrase structures and head information.The score results from several mainstream evalua-tion methods indicated that the translation qualityhad been improved; the scores of Word Error Rate(WER) and Translation Edit Rate (TER) (Snoveret al, 2006) had especially been greatly reduced.2.2 Chinese Deep ParsingSyntax-based reordering methods need parsed sen-tences as input.
Isozaki et al (2010b) used Enju,an HPSG-based deep parser for English, but theyalso discussed using other types of parsers, suchas word dependency parsers and Penn Treebank-style parsers.
However, to use word dependencyparsers, they needed an additional heuristic rule torecover phrase structures, and Penn Treebank-styleparsers are problematic because they output flatphrase structures (i.e.
a phrase may have multipledependents, which causes a problem of reorder-ing within a phrase).
Consequently, compared todifferent types of parsers, Head-Final English per-forms the best on the basis of English Enju?s pars-ing result.In this paper, we follow their observation, anduse the HPSG-based parser for Chinese (ChineseEnju) (Yu et al, 2011) for Chinese syntactic pars-ing.
Since Chinese Enju is based on the same pars-ing model as English Enju, it provides rich syn-tactic information including phrase structures andsyntactic/semantic heads.Figure 1 shows an example of an XML outputfrom Chinese Enju for the sentence ?wo (I) qu (goto) dongjing (Tokyo) he (and) jingdu (Ky-oto).?
The label <cons> and <tok> representthe non-terminal nodes and terminal nodes, respec-tively.
Each node is identified by a unique ?id?and has several attributes.
The attribute ?head?indicates which child node is the syntactic head.In this figure, <head=?c4?
id=?c3?> means thatthe node that has id=?c4?
is the syntactic head ofthe node that has id=?c3?.58Figure 1: An XML output for a Chinese sentence fromChinese Enju.
For clarity, we only draw informationrelated to the phrase structure and the heads.2.3 Related WorkReordering is a popular strategy for improvingmachine translation quality when source and tar-get languages are structurally very different.
Re-searchers have approached the reordering problemin multiple ways.
The most basic idea is pre-ordering (Xia and McCord, 2004; Collins et al,2005), that is, to do reordering during preprocess-ing time, where the source side of the training anddevelopment data and sentences from a source lan-guage that have to be translated are first reorderedto ease the training and the translation, respec-tively.
In (Xu et al, 2009), authors used a depen-dency parser to introduce manually created pre-ordering rules to reorder English sentences whentranslating into five different SOV(Subject-Object-Verb) languages.
Other authors (Genzel, 2010; Wuet al, 2011) use automatically generated rules in-duced from parallel data.
Tillmann (2004) used alexical reordering model, and Galley et al (2004)followed a syntactic-based model.In this work, however, we are centered in thedesign of manual rules inspired by the Head Final-ization (HF) reordering (Isozaki et al, 2010b).
HFreordering is one of the simplest methods for pre-ordering that significantly improves word align-ments and leads to a better translation quality.
Al-though the method is limited to translation wherethe target language is head-final, it requires neithertraining data nor fine-tuning.
To our knowledge,HF is the best method to reorder languages whentranslating into head-final languages like Japanese.The implementation of HF method for English-to-Japanese translation appears to work well.
Areasonable explanation for this is the close matchbetween the concept of ?head?
in this languagepair.
However, for Chinese-to-Japanese, there aredifferences in the definitions of numbers of impor-tant syntactic concepts, including the definition ofthe syntactic head.
We concluded that the diffi-culties we encountered in using HF to Chinese-to-Japanese translation were the result of these differ-ences in the definition of ?head?.
As we believethat such differences are also likely to be observedin other language pairs, the present work is gener-ally important for head-initial to head-final trans-lation as it shows a systematic linguistic analysisthat consistently improves the effectivity of the HFmethod.3 Syntax-based Reordering RulesThis section describes our method for syntax-based reordering for Chinese-to-Japanese transla-tion.
We start by introducing Head Finalizationfor Chinese (HFC), which is a simple adaptationof Isozaki et al (2010b)?s method for English-to-Japanese translation.
However, we found that thissimple method has problems when applied to Chi-nese, due to peculiarities in Chinese syntax.
InSection 3.2, we analyze several distinctive cases ofthe problem in detail.
And following this analysis,Section 3.3 proposes a refinement of the originalHFC, with a couple of exception rules for reorder-ing.3.1 Head Finalization for Chinese (HFC)Since Chinese and English are both known to behead-initial languages1, the reordering rule intro-duced in (Isozaki et al, 2010b) ideally would re-order Chinese sentences to follow the word order1As Gao (2008) summarized, whether Chinese is a head-initial or a head-final language is open for debate.
Neverthe-less, we take the view that most Chinese sentence structuresare head-initial since the written form of Chinese mainly be-haves as an head-initial language.59Figure 2: Simple example for Head-Final Chinese.
The left figure shows the parsing tree of the original sentenceand its English translation.
The right figure shows the reordered sentence along with its Japanese translation.
( ?*?
indicate the syntactic head).of their Japanese counterparts.Figure 2 shows an example of a head finalizedChinese sentence based on the output from Chi-nese Enju shown in Figure 1.
Notice that thecoordination exception rule described in (Isozakiet al, 2010b) also applies to Chinese reordering.This exception rule says that child nodes are notswapped if the node is a coordination2.
Anotherexception rule is for punctuation symbols, whichare also preserved in their original order.
In thiscase, as can be seen in the example in Figure 2, thenodes of c3, c6, and c8 had not been swapped withtheir dependency.
In this account, only the verb?qu?
had been moved to the end of the sentence,following the same word order as its Japanesetranslation.3.2 Discrepancies in Head DefinitionHead Finalization relies on the idea that head-dependent relations are largely consistent amongdifferent languages while word orders are differ-ent.
However, in Chinese, there has been muchdebate on the definition of head3, possibly becauseChinese has fewer surface syntactic features thanother languages like English and Japanese.
Thiscauses some discrepancies between the definitions2Coordination is easily detected in the output ofEnju; it is marked by the attributes xcat="COOD" orschema="coord-left/right" as shown in Figure 1.3In this paper, we only consider the syntactic head.of the head in Chinese and Japanese, which leadsto undesirable reordering of Chinese sentences.Specifically, in preliminary experiments we ob-served unexpected reorderings that are caused bythe differences in the head definitions, which wedescribe below.3.2.1 Aspect ParticleAlthough Chinese has no syntactic tense marker,three aspect particles following verbs can be usedto identify the tense semantically.
They are ?le0?
(did), ?zhe0?
(doing), and ?guo4?
(done), andtheir counterparts in Japanese are ?ta?, ?teiru?,and ?ta?, respectively.
Both the first word andthird word can represent the past tense, but thethird one is more often used in the past perfect.The Chinese parser4 treated aspect particles asdependents of verbs, whereas their Japanese coun-terparts are identified as the head.
For exam-ple in Table 15, ?qu?
(go) and ?guo?
(done)aligned with ?i?
and ?tta?, respectively.
How-ever, since ?guo?
is treated as a dependent of?qu?, by directly implementing the Head FinalChinese (HFC), the sentence will be reordered like4The discussions in this section presuppose the syntacticanalysis done by Chinese Enju, but most of the analysis isconsistent with the common explanation for Chinese syntax.5English translation (En); Chinese original sentence(Ch); reordered Chinese by Head-Final Chinese (HFC); re-ordered Chinese by Refined Head-Final Chinese (R-HFC)and Japanese translation (Ja).60HFC in Table 1, which does not follow the wordorder of the Japanese (Ja) translation.
In contrast,the reordered sentence from refined-HFC (R-HFC)can be translated monotonically.En I have been to Tokyo.Ch wo qu guo dongjing.HFC wo dongjing guo qu.R-HFC wo dongjing qu guo.Ja watashi (wa) Tokyo (ni) i tta.Table 1: An example for Aspect Particle.
Best wordalignment Ja-Ch (En): ?watashi?
?
?wo?
(I); ?Tokyo?
??dongjing?
(Tokyo); ?i?
?
?qu?
(been); ?tta?
?
?guo?
(have).3.2.2 Adverbial Modifier ?bu4?Both in Chinese and Japanese, verb phrase mod-ifiers typically occur in pre-verbal positions, espe-cially when the modifiers are adverbs.
Since ad-verbial modifiers are dependents in both Chineseand Japanese, head finalization works perfectly forthem.
However, there is an exceptional adverb,?bu4?, which means negation and is usually trans-lated into ?nai?, which is always at the end of thesentence in Japanese and thus is the head.
For ex-ample in Table 2, the word ?kan?
(watch) will beidentified as the head and the word ?bu?
is its de-pendent; on the contrary, in the Japanese transla-tion (Ja), the word ?nai?, which is aligned with?bu?, will be identified as the head.
Therefore,the Head Final Chinese is not in the same order,but the reordered sentence by R-HFC obtained thesame order with the Japanese translation.En I do not watch TV.Ch wo bu kan dianshi.HFC wo dianshi bu kan.R-HFC wo dianshi kan bu.Ja watashi (wa) terebi (wo) mi nai.Table 2: An example for Adverbial Modifier bu4.Best word alignment Ja-Ch (En): ?watashi?
?
?wo?
(I);?terebi?
?
?dianshi?
(TV); ?mi?
?
?kan?
(watch); ?nai??
?bu?
(do not).3.2.3 Sentence-final ParticleSentence-final particles often appear at the endof a sentence to express a speaker?s attitude:e.g.
?ba0, a0?
in Chinese, and ?naa, nee?
inJapanese.
Although they appear in the same posi-tion in both Chinese and Japanese, in accordancewith the differences of head definition, they areidentified as the dependent in Chinese while theyare the head in Japanese.
For example in Table 3,since ?a0?
was identified as the dependent, it hadbeen reordered to the beginning of the sentencewhile its Japanese translation ?nee?
is at the endof the sentence as the head.
Likewise, by refiningthe HFC, we can improve the word alignment.En It is good weather.Ch tianqi zhenhao a.HFC a tianqi zhenhao.R-HFC tianqi zhenhao a.Ja ii tennki desu nee.Table 3: An example for Sentence-final Particle.Best word alignment Ja-Ch (En): ?tennki?
?
?tianqi?
(weather); ?ii?
?
?zhenhao?
(good); ?nee?
?
?a?
(None).3.2.4 Et ceteraIn Chinese, there are two expressions for rep-resenting the meaning of ?and other things?
withone Chinese character: ?deng3?
and ?deng3deng3?, which are both identified as dependentof a noun.
In contrast, in Japanese, ?nado?
is al-ways the head because it appears as the right-mostword in a noun phrase.
Table 4 shows an example.En Fruits include apples, etc.Ch shuiguo baokuo pingguo deng.HFC shuiguo deng pingguo baokuo.R-HFC shuiguo pingguo deng baokuo.Ja kudamono (wa) ringo nado (wo)fukunde iru.Table 4: An example for Et cetera.
Best word alignmentJa-Ch (En): ?kudamono?
?
?shuiguo?
(Fruits); ?ringo??
?pingguo?
(apples); ?nado?
?
?deng?
(etc.
); ?fukundeiru?
?
?baokuo?
(include).61AS Aspect particleSP Sentence-final particleETC et cetera (i.e.
deng3 and deng3 deng3)IJ InterjectionPU PunctuationCC Coordinating conjunctionTable 5: The list of POSs for exception reordering rules3.3 Refinement of HFCIn the preceding sections, we have discussed syn-tactic constructions that cause wrong applicationof Head Finalization to Chinese sentences.
Fol-lowing the observations, we propose a method toimprove the original Head Finalization reorderingrule to obtain better alignment with Japanese.The idea is simple: we define a list of POSs,and when we find one of them as a dependentchild of the node, we do not apply reordering.
Ta-ble 5 shows the list of POSs we define in the cur-rent implementation6.
While interjections are notdiscussed in detail, we should obviously not re-order to interjections because they are position-independent.
The rules for PU and CC are ba-sically equivalent to the exception rules proposedby (Isozaki et al, 2010b).4 ExperimentsThe corpus we used as training data comesfrom the China Workshop on Machine Transla-tion (CWMT) (Zhao et al, 2011).
This is aJapanese-Chinese parallel corpus in the news do-main, containing 281, 322 sentence pairs.
We alsocollected another Japanese-Chinese parallel cor-pus from news containing 529, 769 sentences andmerged it with the CWMT corpus to create an ex-tended version of the CWMT corpus.
We will re-fer to this corpus as ?CWMT ext.?
We split an in-verted multi-reference set into a development and atest set containing 1, 000 sentences each.
In thesetwo sets, the Chinese input was different, but theJapanese reference was identical.
We think thatthis split does not pose any severe problem to thecomparison fairness of the experiment, since nonew phrases are added during tuning and the ex-perimental conditions remain equal for all tested6The POSs are from Penn Chinese Treebank.Ch JaCWMTSentences 282KRun.
words 2.5M 3.2MAvg.
sent.
leng.
8.8 11.5Vocabulary 102K 42KCWMT ext.Sentences 811KRun.
words 14.7M 17MAvg.
sent.
leng.
18.1 20.9Vocabulary 249K 95KDev.Sentences 1000Run.
words 29.9K 35.7KAvg.
sent.
leng.
29.9 35.7OoV w.r.t.
CWMT 485 106OoV w.r.t.
CWMT ext.
244 53TestSentences 1000Run.
words 25.8K 35.7KAvg.
sent.
leng.
25.8 35.7OoV w.r.t.
CWMT 456 106OoV w.r.t.
CWMT ext.
228 53Table 6: Characteristics of CWMT and extendedCWMT Chinese-Japanese corpus.
Dev.
stands for De-velopment, OoV for ?Out of Vocabulary?
words, K forthousands of elements, and M for millions of elements.Data statistics were collected after tokenizing.methods.
Detailed Corpus statistics can be foundin Table 6.To parse Chinese sentences, we used ChineseEnju (Yu et al, 2010), an HPSG-based parsertrained with the Chinese HPSG treebank convertedfrom Penn Chinese Treebank.
Chinese Enju re-quires segmented and POS-tagged sentences todo parsing.
We used the Stanford Chinese seg-menter (Chang et al, 2008) and Stanford POS-tagger (Toutanova et al, 2003) to obtain the seg-mentation and POS-tagging of the Chinese side ofthe training, development, and test sets.The baseline system was trained followingthe instructions of recent SMT evaluation cam-paigns (Callison-Burch et al, 2010) by using theMT toolkit Moses (Koehn et al, 2007) in its de-fault configuration.
Phrase pairs were extractedfrom symmetrized word alignments and distor-tions generated by GIZA++ (Och and Ney, 2003)using the combination of heuristics ?grow-diag-final-and?
and ?msd-bidirectional-fe?.
The lan-guage model was a 5-gram language model es-timated on the target side of the parallel cor-pora by using the modified Kneser-Ney smooth-ing (Chen and Goodman, 1999) implemented in62the SRILM (Stolcke, 2002) toolkit.
The weightsof the log-linear combination of feature functionswere estimated by using MERT (Och, 2003) on thedevelopment set described in Table 6.The effectiveness of the reorderings proposedin Section 3.3 was assessed by using two preci-sion metrics and two error metrics on translationquality.
The first evaluation metric is BLEU (Pap-ineni et al, 2002), a very common accuracy metricin SMT that measures N -gram precision, with apenalty for too short sentences.
The second eval-uation metric was RIBES (Isozaki et al, 2010a), arecent precision metric used to evaluate translationquality between structurally different languages.
Ituses notions on rank correlation coefficients andprecision measures.
The third evaluation metric isTER (Snover et al, 2006), another error metric thatcomputes the minimum number of edits requiredto convert translated sentences into its correspond-ing references.
Possible edits include insertion,deletion, substitution of single words, and shifts ofword sequences.
The fourth evaluation metric isWER, an error metric inspired in the Levenshteindistance at word level.
BLEU, WER, and TERwere used to provide a sense of comparison butthey do not significantly penalize long-range wordorder errors.
For this reason, RIBES was used toaccount for this aspect of translation quality.The baseline system was trained and tuned us-ing the same configuration setup described in thissection, but no reordering rule was implemented atthe preprocessing stage.Three systems have been run to translate the testset for comparison when the systems were trainedusing the two training data sets.
They are thebaseline system, the system consisting in the na?
?veimplementation of HF reordering, and the systemwith refined HFC reordering rules.
Assessment oftranslation quality can be found in Table 7.As can be observed in Table 7, the translationquality, as measured by precision and error met-rics, was consistently and significantly increasedwhen the HFC reordering rule was used and wassignificantly improved further when the refinementproposed in this work was used.
Specifically, theBLEU score increased from 19.94 to 20.79 whenthe CWMT corpus was used, and from 23.17 to24.14 when the extended CWMT corpus was used.AS SP ETC IJ PU COOD3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%Table 8: Weighted recall of each exception rule duringreordering on CWMT ext.
training data, dev data, andtest data.
(* actual value 0.0016%.
)Table 8 shows the recall of each exception rulelisted in Section 3, and was computed by countingthe times an exception rule was triggered dividedby the number of times the head finalization ruleapplied.
Data was collected for CWMT ext.
train-ing, dev and test sets.
Although the exception rulesrelated to aspect particles, Et cetera, sentence-finalparticles and interjections have a comparativelylower frequency of application than punctuationor coordination exception rules, the improvementsthey led to are significant.5 Error AnalysisIn Section 3 we have analyzed syntactic differ-ences between Chinese and Japanese that led tothe design of an effective refinement.
A manualerror analysis of the results of our refined reorder-ing rules showed that some more reordering issuesremain and, although they are not side effects ofour proposed rule, they are worth mentioning inthis separate section.5.1 Serial Verb ConstructionSerial verb construction is a phenomenon occur-ring in Chinese, where several verbs are put to-gether as one unit without any conjunction be-tween them.
The relationship between theseverbs can be progressive or parallel.
Apparently,Japanese has a largely corresponding construc-tion, which indicates that no reordering shouldbe applied.
An example to illustrate this fact inChinese is ?weishi (maintain) shenhua (deepen)zhongriguanxi (Japan-China relations) de(of) gaishan (improvement) jidiao (basictone).
?7 The two verbs ?weishi?
(in Japanese,iji) and ?shenhua?
(in Japanese, shinka) areused together, and they follow the same order asin Japanese: ?nicchukankei (Japan-China re-7English translation: Maintain and deepen the improvedbasic tone of Japan-China relations.63CWMT CWMT ext.BLEU RIBES TER WER BLEU RIBES TER WERbaseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used fortraining.
Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposedrefinement of head finalization reordering rules.lations) no (of) kaizan (improvement) kityo(basic tone) wo iji (maintain) shinka (deepen)suru (do).
?5.2 ComplementizerA ?complementizer?
is a particle used to intro-duce a complement.
In English, a very commoncomplementizer is the word ?that?
when making aclausal complement, while in Chinese it can de-note other types of word, such as verbs, adjec-tives or quantifiers.
The complementizer is iden-tified as the dependent of the verb that it modi-fies.
For instance, a Chinese sentence: ?wo (I)mang wan le (have finished the work).?
Thiscan be translated into Japanese: ?watashi (I) washigoto (work) wo owa tta (have finished).?
InChinese, the verb ?mang?
is the head while ?wan?is the complementizer, and its Japanese counter-part ?owa tta?
has the same word order.However, during the reordering, ?mang?
will beplaced at the end of the sentence and ?wan?
in thebeginning, leading to an inconsistency with respectto the Japanese translation where the complemen-tizer ?tta?
is the head.5.3 Verbal Nominalization and NounalVerbalizationAs discussed by Guo (2009), compared to Englishand Japanese, Chinese has little inflectional mor-phology, that is, no inflection to denote tense, case,etc.
Thus, words are extremely flexible, makingverb nominalization and noun verbalization appearfrequently and commonly without any conjugationor declension.
As a result, it is difficult to do dis-ambiguation during POS tagging and parsing.
Forexample, the Chinese word ?kaifa?
may havetwo syntactic functions: verb (develop) and noun(development).
Thus, it is difficult to reliably tagwithout considering the context.
In contrast, inJapanese, ?suru?
can be used to identify verbs.For example, ?kaihatu suru?
(develop) is averb and ?kaihatu?
(development) is a noun.This ambiguity is prone to not only POS taggingerror but also parsing error, and thus affects theidentification of heads, which may lead to incor-rect reordering.5.4 Adverbial ModifierUnlike the adverb ?bu4?
we discussed in Sec-tion 3.2, the ordinary adverbial modifier comesdirectly before the verb it modifies both in Chi-nese and Japanese, but not in English.
Nev-ertheless, in accordance with the principle ofidentifying the head for Chinese, the adverbwill be treated as the dependent and it willnot be reordered following the verb it modi-fied.
As a result, the alignment between adverbsand verbs is non-monotonic.
This can be ob-served in the Chinese sentence ?guojia (coun-try) yanli (severely) chufa (penalize) jiage(price) weifa (violation) xingwei (behavior)?8,and its Japanese translation: ?kuni (country) wakakaku (price) no ihou (violation) koui (be-havior) wo kibisiku (severely) syobatu (penal-ize).?
Both in Chinese and Japanese, the adverbialmodifier ?yanli?
and ?kibisiku?
are directlyin front of the verb ?chufa?
and ?syobatu?, re-spectively.
However, the verb in Chinese is identi-fied as the head and will be reordered to the end ofthe sentence without the adverb.8English translation: The country severely penalizes vio-lations of price restrictions.645.5 POS tagging and Parsing ErrorsThere were word reordering issues not causedsolely by differences in syntactic structures.
Herewe summarize two that are difficult to remedy dur-ing reordering and that are hard to avoid since re-ordering rules are highly dependent on the taggerand parser.?
POS tagging errorsIn Chinese, for example, the word ?Iran?was tagged as ?VV?
or ?JJ?
instead of ?NR?.This led to identifying ?Iran?
as a head inaccordance with the head definition in Chi-nese, and it was reordered undesirably.?
Parsing errorsFor example, in the Chinese verb phrase?touzi (invest) 20 yi (200 million)meiyuan (dollars)?, ?20?
and ?yi?
wereidentified as dependent of ?touzi?
and?meiyuan?, respectively, which led to anunsuitable reordering for posterior wordalignment.6 Conclusion and Future WorkIn the present work, we have proposed novelChinese-to-Japanese reordering rules inspiredin (Isozaki et al, 2010b) based on linguistic analy-sis on Chinese HPSG and differences among Chi-nese and Japanese.
Although a simple implemen-tation of HF to reorder Chinese sentences per-forms well, translation quality was substantiallyimproved further by including linguistic knowl-edge into the refinement of the reordering rules.In Section 5, we found more patterns on reorder-ing issues when reordering Chinese sentences toresemble Japanese word order.
The extraction ofthose patterns and their effective implementationmay lead to further improvements in translationquality, so we are planning to explore this possi-bility.In this work, syntactic information from a deepparser has been used to reorder words better.
Webelieve that using semantic information can fur-ther increase the expressive power of reorderingrules.
With that objective, Chinese Enju can beused since it provides the semantic head of nodesand can interpret sentences by using their semanticdependency.AcknowledgmentsThis work was mainly developed during an intern-ship at NTT Communication Science Laborato-ries.
We would like to thank Prof. Yusuke Miyaofor his invaluable support on this work.ReferencesP.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, andR.L.
Mercer.
1993.
The mathematics of ma-chine translation.
In Computational Linguistics, vol-ume 19, pages 263?311, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, and Omar Zaidan, editors.
2010.
Pro-ceedings of the joint 5th workshop on Statistical Ma-chine Translation and MetricsMATR.
Associationfor Computational Linguistics, July.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word seg-mentation for machine translation performance.
InProceedings of the 3rd Workshop on SMT, pages224?232, Columbus, Ohio.
Association for Compu-tational Linguistics.Stanley F. Chen and Joshua Goodman.
1999.
Anempirical study of smoothing techniques for lan-guage modeling.
Computer Speech and Language,4(13):359?393.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 531?540, Stroudsburg, PA, USA.Association for Computational Linguistics.Vivian James Cook and Mark Newson.
1988.
Chom-sky?s Universal Grammar: An introduction.
Oxford:Basil Blackwell.Naoki Fukui.
1992.
Theory of Projection in Syntax.CSLI Publisher and Kuroshio Publisher.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
Whats in a translation rule?In Proceedings of HLT-NAACL.Qian Gao.
2008.
Word order in mandarin: Reading andspeaking.
In Proceedings of the 20th North Ameri-can Conference on Chinese Linguistics (NACCL-20),volume 2, pages 611?626.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, COLING ?10,65pages 376?384, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yuqing Guo.
2009.
Treebank-based acquisition ofChinese LFG resources for parsing and generation.Ph.D.
thesis, Dublin City University.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010a.
Automaticevaluation of translation quality for distant languagepairs.
In Proceedings of Empirical Methods on Nat-ural Language Processing (EMNLP).Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010b.
Head finalization: A simple re-ordering rule for sov languages.
In Proceedings ofWMTMetricsMATR, pages 244?251.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Sta-tistical phrase-based translation.
In ProceedingsHLT/NAACL?03, pages 48?54.Philipp Koehn et al 2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedingsof the ACL Demo and Poster Sessions, 2007, pages177?180, June 25?27.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic hpsg parsing.
Computa-tional Linguistics, 34:35?80, March.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics.Franz J. Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41st annual conference of the Association for Com-putational Linguistics, 2003, pages 160?167, July 7?12.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In Proceedings of the40th annual conference of the Association for Com-putational Linguistics, 2002, pages 311?318, July 6?12.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas, pages 223?231.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the 7thinternational conference on Spoken Language Pro-cessing, 2002, pages 901?904, September 16?20.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of HLT-NAACL 2004: Short Papers, HLT-NAACL-Short ?04, pages 101?104, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings OF HLT-NAACL, pages 252?259.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 737?745, Prague, Czech Republic, June.
Association forComputational Linguistics.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Extractingpre-ordering rules from predicate-argument struc-tures.
In Proceedings of 5th International Joint Con-ference on Natural Language Processing, pages 29?37, Chiang Mai, Thailand, November.
Asian Feder-ation of Natural Language Processing.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of the 20th internationalconference on Computational Linguistics, COLING?04, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improvesmt for subject-object-verb languages.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 245?253, Stroudsburg, PA, USA.Association for Computational Linguistics.Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-suzaki, and Jun ichi Tsujii.
2010.
Semi-automatically developing chinese hpsg grammarfrom the penn chinese treebank for deep parsing.
InCOLING (Posters)?10, pages 1417?1425.Kun Yu, Yusuke Miyao, Takuya Matsuzaki, XiangliWang, and Junichi Tsujii.
2011.
Analysis of the dif-ficulties in chinese deep parsing.
In Proceedings ofthe 12th International Conference on Parsing Tech-nologies, pages 48?57.R.
Zens, F.J. Och, and H. Ney.
2002.
Phrase-basedstatistical machine translation.
In Proceedings ofKI?02, pages 18?32.Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, YunHuang, and Qun Liu.
2011.
Evaluation reportfor the 7th china workshop on machine translation(cwmt2011).
The 7th China Workshop on MachineTranslation (CWMT2011).66
