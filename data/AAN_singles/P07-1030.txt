Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232?239,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsFully Unsupervised Discovery of Concept-Specific Relationshipsby Web MiningDmitry DavidovICNCThe Hebrew UniversityJerusalem 91904, Israeldmitry@alice.nc.huji.ac.ilAri RappoportInstitute of Computer ScienceThe Hebrew UniversityJerusalem 91904, Israelwww.cs.huji.ac.il/?arirMoshe KoppelDept.
of Computer ScienceBar-Ilan UniversityRamat-Gan 52900, Israelkoppel@cs.biu.ac.ilAbstractWe present a web mining method for discov-ering and enhancing relationships in which aspecified concept (word class) participates.We discover a whole range of relationshipsfocused on the given concept, rather thangeneric known relationships as in most pre-vious work.
Our method is based on cluster-ing patterns that contain concept words andother words related to them.
We evaluate themethod on three different rich concepts andfind that in each case the method generates abroad variety of relationships with good pre-cision.1 IntroductionThe huge amount of information available on theweb has led to a flurry of research on methods forautomatic creation of structured information fromlarge unstructured text corpora.
The challenge is tocreate as much information as possible while pro-viding as little input as possible.A lot of this research is based on the initial insight(Hearst, 1992) that certain lexical patterns (?X is acountry?)
can be exploited to automatically gener-ate hyponyms of a specified word.
Subsequent work(to be discussed in detail below) extended this initialidea along two dimensions.One objective was to require as small a user-provided initial seed as possible.
Thus, it was ob-served that given one or more such lexical patterns,a corpus could be used to generate examples of hy-ponyms that could then, in turn, be exploited to gen-erate more lexical patterns.
The larger and more reli-able sets of patterns thus generated resulted in largerand more precise sets of hyponyms and vice versa.The initial step of the resulting alternating bootstrapprocess ?
the user-provided input ?
could just as wellconsist of examples of hyponyms as of lexical pat-terns.A second objective was to extend the informationthat could be learned from the process beyond hy-ponyms of a given word.
Thus, the approach wasextended to finding lexical patterns that could pro-duce synonyms and other standard lexical relations.These relations comprise all those words that standin some known binary relation with a specified word.In this paper, we introduce a novel extension ofthis problem: given a particular concept (initiallyrepresented by two seed words), discover relationsin which it participates, without specifying theirtypes in advance.
We will generate a concept classand a variety of natural binary relations involvingthat class.An advantage of our method is that it is particu-larly suitable for web mining, even given the restric-tions on query amounts that exist in some of today?sleading search engines.The outline of the paper is as follows.
In the nextsection we will define more precisely the problemwe intend to solve.
In section 3, we will consider re-lated work.
In section 4 we will provide an overviewof our solution and in section 5 we will consider thedetails of the method.
In section 6 we will illustrateand evaluate the results obtained by our method.
Fi-nally, in section 7 we will offer some conclusionsand considerations for further work.2322 Problem DefinitionIn several studies (e.g., Widdows and Dorow, 2002;Pantel et al 2004; Davidov and Rappoport, 2006)it has been shown that relatively unsupervised andlanguage-independent methods could be used togenerate many thousands of sets of words whosesemantics is similar in some sense.
Although ex-amination of any such set invariably makes it clearwhy these words have been grouped together intoa single concept, it is important to emphasize thatthe method itself provides no explicit concept defi-nition; in some sense, the implied class is in the eyeof the beholder.
Nevertheless, both human judgmentand comparison with standard lists indicate that thegenerated sets correspond to concepts with high pre-cision.We wish now to build on that result in the fol-lowing way.
Given a large corpus (such as the web)and two or more examples of some concept X , au-tomatically generate examples of one or more rela-tions R ?
X ?
Y , where Y is some concept and Ris some binary relationship between elements of Xand elements of Y .We can think of the relations we wish to gener-ate as bipartite graphs.
Unlike most earlier work,the bipartite graphs we wish to generate might beone-to-one (for example, countries and their capi-tals), many-to-one (for example, countries and theregions they are in) or many-to-many (for example,countries and the products they manufacture).
For agiven class X , we would like to generate not one butpossibly many different such relations.The only input we require, aside from a corpus,is a small set of examples of some class.
However,since such sets can be generated in entirely unsuper-vised fashion, our challenge is effectively to gener-ate relations directly from a corpus given no addi-tional information of any kind.
The key point is thatwe do not in any manner specify in advance whattypes of relations we wish to find.3 Related WorkAs far as we know, no previous work has directlyaddressed the discovery of generic binary relationsin an unrestricted domain without (at least implic-itly) pre-specifying relationship types.
Most relatedwork deals with discovery of hypernymy (Hearst,1992; Pantel et al 2004), synonymy (Roark andCharniak, 1998; Widdows and Dorow, 2002; Davi-dov and Rappoport, 2006) and meronymy (Berlandand Charniak, 1999).In addition to these basic types, several stud-ies deal with the discovery and labeling of morespecific relation sub-types, including inter-verb re-lations (Chklovski and Pantel, 2004) and noun-compound relationships (Moldovan et al 2004).Studying relationships between tagged named en-tities, (Hasegawa et al 2004; Hassan et al 2006)proposed unsupervised clustering methods that as-sign given (or semi-automatically extracted) sets ofpairs into several clusters, where each cluster corre-sponds to one of a known relationship type.
Thesestudies, however, focused on the classification ofpairs that were either given or extracted using somesupervision, rather than on discovery and definitionof which relationships are actually in the corpus.Several papers report on methods for using theweb to discover instances of binary relations.
How-ever, each of these assumes that the relations them-selves are known in advance (implicitly or explic-itly) so that the method can be provided with seedpatterns (Agichtein and Gravano, 2000; Pantel et al2004), pattern-based rules (Etzioni et al 2004), rela-tion keywords (Sekine, 2006), or word pairs exem-plifying relation instances (Pasca et al 2006; Alfon-seca et al 2006; Rosenfeld and Feldman, 2006).In some recent work (Strube and Ponzetto, 2006),it has been shown that related pairs can be gener-ated without pre-specifying the nature of the rela-tion sought.
However, this work does not focus ondifferentiating among different relations, so that thegenerated relations might conflate a number of dis-tinct ones.It should be noted that some of these papers utilizelanguage and domain-dependent preprocessing in-cluding syntactic parsing (Suchanek et al 2006) andnamed entity tagging (Hasegawa et al 2004), whileothers take advantage of handcrafted databases suchas WordNet (Moldovan et al 2004; Costello et al2006) and Wikipedia (Strube and Ponzetto, 2006).Finally, (Turney, 2006) provided a pattern dis-tance measure which allows a fully unsupervisedmeasurement of relational similarity between twopairs of words; however, relationship types were notdiscovered explicitly.2334 Outline of the MethodWe will use two concept words contained in a con-cept class C to generate a collection of distinct re-lations in which C participates.
In this section weoffer a brief overview of our method.Step 1: Use a seed consisting of two (or more) ex-ample words to automatically obtain other examplesthat belong to the same class.
Call these conceptwords.
(For instance, if our example words wereFrance and Angola, we would generate more coun-try names.
)Step 2: For each concept word, collect instancesof contexts in which the word appears together withone other content word.
Call this other word a tar-get word for that concept word.
(For example, forFrance we might find ?Paris is the capital of France?.Paris would be a target word for France.
)Step 3: For each concept word, group the contextsin which it appears according to the target word thatappears in the context.
(Thus ?X is the capital of Y ?would likely be grouped with ?Y ?s capital is X?.
)Step 4: Identify similar context groups that ap-pear across many different concept words.
Mergethese into a single concept-word-independent clus-ter.
(The group including the two contexts abovewould appear, with some variation, for other coun-tries as well, and all these would be merged intoa single cluster representing the relation capital-of(X,Y).
)Step 5: For each cluster, output the relation con-sisting of all <concept word, target word> pairs thatappear together in a context included in the cluster.
(The cluster considered above would result in a setof pairs consisting of a country and its capital.
Otherclusters generated by the same seed might includecountries and their languages, countries and the re-gions in which they are located, and so forth.
)5 Details of the MethodIn this section we consider the details of each ofthe above-enumerated steps.
It should be notedthat each step can be performed using standard websearches; no special pre-processed corpus is re-quired.5.1 Generalizing the seedThe first step is to take the seed, which might con-sist of as few as two concept words, and generatemany (ideally, all, when the concept is a closed setof words) members of the class to which they be-long.
We do this as follows, essentially implement-ing a simplified version of the method of Davidovand Rappoport (2006).
For any pair of seed wordsSi and Sj , search the corpus for word patterns of theform SiHSj , where H is a high-frequency word inthe corpus (we used the 100 most frequent wordsin the corpus).
Of these, we keep all those pat-terns, which we call symmetric patterns, for whichSjHSi is also found in the corpus.
Repeat this pro-cess to find symmetric patterns with any of the struc-tures HSHS, SHSH or SHHS.
It was shown in(Davidov and Rappoport, 2006) that pairs of wordsthat often appear together in such symmetric pat-terns tend to belong to the same class (that is, theyshare some notable aspect of their semantics).
Otherwords in the class can thus be generated by search-ing a sub-corpus of documents including at least twoconcept words for those words X that appear in asufficient number of instances of both the patternsSiHX and XHSi, where Si is a word in the class.The same can be done for the other three patternstructures.
The process can be bootstrapped as morewords are added to the class.Note that our method differs from that of Davidovand Rappoport (2006) in that here we provide an ini-tial seed pair, representing our target concept, whilethere the goal is grouping of as many words as pos-sible into concept classes.
The focus of our paper ison relations involving a specific concept.5.2 Collecting contextsFor each concept word S, we search the corpus fordistinct contexts in which S appears.
(For our pur-poses, a context is a window with exactly five wordsor punctuation marks before or after the conceptword; we choose 10,000 of these, if available.)
Wecall the aggregate text found in all these context win-dows the S-corpus.From among these contexts, we choose all pat-terns of the form H1SH2XH3 or H1XH2SH3,where:234?
X is a word that appears with frequency belowf1 in the S-corpus and that has sufficiently highpointwise mutual information with S. We usethese two criteria to ensure that X is a contentword and that it is related to S. The lower thethreshold f1, the less noise we allow in, thoughpossibly at the expense of recall.
We used f1 =1, 000 occurrences per million words.?
H2 is a string of words each of which occurswith frequency above f2 in the S-corpus.
Wewant H2 to consist mainly of words commonin the context of S in order to restrict patternsto those that are somewhat generic.
Thus, inthe context of countries we would like to retainwords like capital while eliminating more spe-cific words that are unlikely to express genericpatterns.
We used f2 = 100 occurrences permillion words (there is room here for automaticoptimization, of course).?
H1 and H3 are either punctuation or words thatoccur with frequency above f3 in the S-corpus.This is mainly to ensure that X and S aren?tfragments of multi-word expressions.
We usedf3 = 100 occurrences per million words.?
We call these patterns, S-patterns and we callX the target of the S-pattern.
The idea is that Sand X very likely stand in some fixed relationto each other where that relation is captured bythe S-pattern.5.3 Grouping S-patternsIf S is in fact related to X in some way, there mightbe a number of S-patterns that capture this relation-ship.
For each X , we group all the S-patterns thathave X as a target.
(Note that two S-patterns withtwo different targets might be otherwise identical,so that essentially the same pattern might appear intwo different groups.)
We now merge groups withlarge (more than 2/3) overlap.
We call the resultinggroups, S-groups.5.4 Identifying pattern clustersIf the S-patterns in a given S-group actually capturesome relationship between S and the target, thenone would expect that similar groups would appearfor a multiplicity of concept words S. Suppose thatwe have S-groups for three different concept wordsS such that the pairwise overlap among the threegroups is more than 2/3 (where for this purpose twopatterns are deemed identical if they differ only at Sand X).
Then the set of patterns that appear in two orthree of these S-groups is called a cluster core.
Wenow group all patterns in other S-groups that have anoverlap of more than 2/3 with the cluster core into acandidate pattern pool P .
The set of all patterns inP that appear in at least two S-groups (among thosethat formed P ) pattern cluster.
A pattern cluster thathas patterns instantiated by at least half of the con-cept words is said to represent a relation.5.5 Refining relationsA relation consists of pairs (S, X) where S is a con-cept word and X is the target of some S-pattern in agiven pattern cluster.
Note that for a given S, theremight be one or many values of X satisfying the re-lation.
As a final refinement, for each given S, werank all such X according to pointwise mutual in-formation with S and retain only the highest 2/3.
Ifmost values of S have only a single corresponding Xsatisfying the relation and the rest have none, we tryto automatically fill in the missing values by search-ing the corpus for relevant S-patterns for the missingvalues of S. (In our case the corpus is the web, sowe perform additional clarifying queries.
)Finally, we delete all relations in which all con-cept words are related to most target words and allrelations in which the concept words and the targetwords are identical.
Such relations can certainly beof interest (see Section 7), but are not our focus inthis paper.5.6 Notes on required Web resourcesIn our implementation we use the Google searchengine.
Google restricts individual users to 1,000queries per day and 1,000 pages per query.
In eachstage we conducted queries iteratively, each timedownloading all 1,000 documents for the query.In the first stage our goal was to discover sym-metric relationships from the web and consequentlydiscover additional concept words.
For queries inthis stage of our algorithm we invoked two require-ments.First, the query should contain at least two con-cept words.
This proved very effective in reduc-235ing ambiguity.
Thus of 1,000 documents for thequery bass, 760 deal with music, while if we add tothe query a second word from the intended concept(e.g., barracuda), then none of the 1,000 documentsdeal with music and the vast majority deal with fish,as intended.Second, we avoid doing overlapping queries.
Todo this we used Google?s ability to exclude fromsearch results those pages containing a given term(in our case, one of the concept words).We performed up to 300 different queries for in-dividual concepts in the first stage of our algorithm.In the second stage, we used web queries to as-semble S-corpora.
On average, about 1/3 of the con-cept words initially lacked sufficient data and weperformed up to twenty additional queries for eachrare concept word to fill its corpus.In the last stage, when clusters are constructed,we used web queries for filling missing pairs of one-to-one or several-to-several relationships.
The to-tal number of filling queries for a specific conceptwas below 1,000, and we needed only the first re-sults of these queries.
Empirically, it took between0.5 to 6 day limits (i.e., 500?6,000 queries) to ex-tract relationships for a concept, depending on itssize (the number of documents used for each querywas at most 100).
Obviously this strategy can beimproved by focused crawling from primary Googlehits, which can drastically reduce the required num-ber of queries.6 EvaluationIn this section we wish to consider the variety of re-lations that can be generated by our method from agiven seed and to measure the quality of these rela-tions in terms of their precision and recall.With regard to precision, two claims are beingmade.
One is that the generated relations correspondto identifiable relations.
The other claim is that tothe extent that a generated relation can be reason-ably identified, the generated pairs do indeed belongto the identified relation.
(There is a small degree ofcircularity in this characterization but this is proba-bly the best we can hope for.
)As a practical matter, it is extremely difficult tomeasure precision and recall for relations that havenot been pre-determined in any way.
For each gen-erated relation, authoritative resources must be mar-shaled as a gold standard.
For purposes of evalu-ation, we ran our algorithm on three representativedomains ?
countries, fish species and star constel-lations ?
and tracked down gold standard resources(encyclopedias, academic texts, informative web-sites, etc) for the bulk of the relations generated ineach domain.This choice of domains allowed us to exploredifferent aspects of algorithmic behavior.
Countryand constellation domains are both well defined andclosed domains.
However they are substantially dif-ferent.Country names is a relatively large domain whichhas very low lexical ambiguity, and a large numberof potentially useful relations.
The main challengein this domain was to capture it well.Constellation names, in contrast, are a relativelysmall but highly ambiguous domain.
They are usedin proper names, mythology, names of entertainmentfacilities etc.
Our evaluation examined how well thealgorithm can deal with such ambiguity.The fish domain contains a very high number ofmembers.
Unlike countries, it is a semi-open non-homogenous domain with a very large number ofsubclasses and groups.
Also, unlike countries, itdoes not contain many proper nouns, which are em-pirically generally easier to identify in patterns.
Sothe main challenge in this domain is to extract un-blurred relationships and not to diverge from the do-main during the concept acquisition phase.We do not show here all-to-all relationships suchas fish parts (common to all or almost all fish), be-cause we focus on relationships that separate be-tween members of the concept class, which areharder to acquire and evaluate.6.1 CountriesOur seed consisted of two country names.
The in-tended result for the first stage of the algorithmwas a list of countries.
There are 193 countries inthe world (www.countrywatch.com) some of whichhave multiple names so that the total number ofcommonly used country names is 243.
Of these,223 names (comprising 180 countries) are charac-ter strings with no white space.
Since we consideronly single word names, these 223 are the names wehope to capture in this stage.236Using the seed words France and Angola, weobtained 202 country names (comprising 167 dis-tinct countries) as well as 32 other names (consistingmostly of names of other geopolitical entities).
Us-ing the list of 223 single word countries as our goldstandard, this gives precision of 0.90 and recall of0.86.
(Ten other seed pairs gave results ranging inprecision: 0.86-0.93 and recall: 0.79-0.90.
)The second part of the algorithm generated a setof 31 binary relations.
Of these, 25 were clearlyidentifiable relations many of which are shown inTable 1.
Note that for three of these there are stan-dard exhaustive lists against which we could mea-sure both precision and recall; for the others shown,sources were available for measuring precision butno exhaustive list was available from which to mea-sure recall, so we measured coverage (the numberof countries for which at least one target concept isfound as related).Another eleven meaningful relations were gener-ated for which we did not compute precision num-bers.
These include celebrity-from, animal-of, lake-in, borders-on and enemy-of.
(The set of relationsgenerated by other seed pairs differed only slightlyfrom those shown here for France and Angola.
)6.2 Fish speciesIn our second experiment, our seed consisted of twofish species, barracuda and bluefish.
There are 770species listed in WordNet of which 447 names arecharacter strings with no white space.
The first stageof the algorithm returned 305 of the species listedin Wordnet, another 37 species not listed in Word-net, as well as 48 other names (consisting mostlyof other sea creatures).
The second part of the al-gorithm generated a set of 15 binary relations all ofwhich are meaningful.
Those for which we couldfind some gold standard are listed in Table 2.Other relations generated include served-with,bait-for, food-type, spot-type, and gill-type.6.3 ConstellationsOur seed consisted of two constellation names,Orion and Cassiopeia.
There are 88 standardconstellations (www.astro.wisc.edu) some of whichhave multiple names so that the total number of com-monly used constellations is 98.
Of these, 87 names(77 constellations) are strings with no white space.Relationship Prec.
Rec/CovSample pattern(Sample pair)capital-of 0.92 R=0.79in (x), capital of (y),(Luanda, Angola)language-spoken-in 0.92 R=0.60to (x) or other (y) speaking(Spain, Spanish)in-region 0.73 R=0.71throughout (x), from (y) to(America, Canada)city-in 0.82 C=0.95west (x) ?
forecast for (y).
(England, London)river-in 0.92 C=0.68central (x), on the (y) river(China, Haine)mountain-range-in 0.77 C=0.69the (x) mountains in (y) ,(Chella, Angola)sub-region-of 0.81 C=0.81the (y) region of (x),(Veneto, Italy)industry-of 0.70 C=0.90the (x) industry in (y) ,(Oil, Russia)island-in 0.98 C=0.55, (x) island , (y) ,(Bathurst, Canada)president-of 0.86 C=0.51president (x) of (y) has(Bush, USA)political-position-in 0.81 C=0.75former (x) of (y) face(President, Ecuador)political-party-of 0.91 C=0.53the (x) party of (y) ,(Labour, England)festival-of 0.90 C=0.78the (x) festival, (y) ,(Tanabata, Japan)religious-denomination-of 0.80 C=0.62the (x) church in (y) ,(Christian, Rome)Table 1: Results on seed { France, Angola }.237Relationship Prec.
CovSample pattern(Sample pair)region-found-in 0.83 0.80best (x) fishing in (y) .
(Walleye, Canada)sea-found-in 0.82 0.64of (x) catches in the (y) sea(Shark, Adriatic)lake-found-in 0.79 0.51lake (y) is famous for (x) ,(Marion, Catfish)habitat-of 0.78 0.92, (x) and other (y) fish(Menhaden, Saltwater)also-called 0.91 0.58.
(y) , also called (x) ,(Lemonfish, Ling)eats 0.90 0.85the (x) eats the (y) and(Perch, Minnow)color-of 0.95 0.85the (x) was (y) color(Shark, Gray)used-for-food 0.80 0.53catch (x) ?
best for (y) or(Bluefish, Sashimi)in-family 0.95 0.60the (x) family , includes (y) ,(Salmonid, Trout)Table 2: Results on seed { barracud, bluefish }.The first stage of the algorithm returned 81 constel-lation names (77 distinct constellations) as well as38 other names (consisting mostly of names of indi-vidual stars).
Using the list of 87 single word con-stellation names as our gold standard, this gives pre-cision of 0.68 and recall of 0.93.The second part of the algorithm generated a setof ten binary relations.
Of these, one concernedtravel and entertainment (constellations are quitepopular as names of hotels and lounges) and anotherthree were not interesting.
Apparently, the require-ment that half the constellations appear in a relationlimited the number of viable relations since manyconstellations are quite obscure.
The six interestingrelations are shown in Table 3 along with precisionand coverage.7 DiscussionIn this paper we have addressed a novel type of prob-lem: given a specific concept, discover in fully un-supervised fashion, a range of relations in which itparticipates.
This can be extremely useful for study-ing and researching a particular concept or field ofstudy.As others have shown as well, two concept wordscan be sufficient to generate almost the entire classto which the words belong when the class is well-defined.
With the method presented in this paper,using no further user-provided information, we can,for a given concept, automatically generate a diversecollection of binary relations on this concept.
Theserelations need not be pre-specified in any way.
Re-sults on the three domains we considered indicatethat, taken as an aggregate, the relations that are gen-erated for a given domain paint a rather clear pictureof the range of information pertinent to that domain.Moreover, all this was done using standard searchengine methods on the web.
No language-dependenttools were used (not even stemming); in fact, we re-produced many of our results using Google in Rus-sian.The method depends on a number of numericalparameters that control the subtle tradeoff betweenquantity and quality of generated relations.
There iscertainly much room for tuning of these parameters.The concept and target words used in this paperare single words.
Extending this to multiple-wordexpressions would substantially contribute to the ap-plicability of our results.In this research we effectively disregard many re-lationships of an all-to-all nature.
However, suchrelationships can often be very useful for ontologyconstruction, since in many cases they introducestrong connections between two different concepts.Thus, for fish we discovered that one of the all-to-all relationships captures a precise set of fish bodyparts, and another captures swimming verbs.
Suchrelations introduce strong and distinct connectionsbetween the concept of fish and the concepts of fish-body-parts and swimming.
Such connections maybe extremely useful for ontology construction.238Relationship Prec.
CovSample pattern(Sample pair)nearby-constellation 0.87 0.70constellation (x), near (y),(Auriga, Taurus)star-in 0.82 0.76star (x) in (y) is(Antares , Scorpius)shape-of 0.90 0.55, (x) is depicted as (y).
(Lacerta, Lizard)abbreviated-as 0.93 0.90.
(x) abbr (y),(Hidra, Hya)cluster-types-in 0.92 1.00famous (x) cluster in (y),(Praesepe, Cancer)location 0.82 0.70, (x) is a (y) constellation(Draco, Circumpolar)Table 3: Results on seed { Orion, Cassiopeia }.ReferencesAgichtein, E., Gravano, L., 2000.
Snowball: Extractingrelations from large plain-text collections.
Proceedingsof the 5th ACM International Conference on DigitalLibraries.Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,P., 2006.
Towards large-scale non-taxonomic relationextraction: estimating the precision of rote extractors.Workshop on Ontology Learning and Population atCOLING-ACL ?06.Berland, M., Charniak, E., 1999.
Finding parts in verylarge corpora.
ACL ?99.Chklovski T., Pantel P., 2004.
VerbOcean: mining theweb for fine-grained semantic verb relations.
EMNLP?04.Costello, F., Veale, T., Dunne, S., 2006.
Using Word-Net to automatically deduce relations between wordsin noun-noun compounds, COLING-ACL ?06.Davidov, D., Rappoport, A., 2006.
Efficient unsuperviseddiscovery of word categories using symmetric patternsand high frequency words.
COLING-ACL ?06.Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,Shaked, T., Soderland, S., Weld, D., Yates, A., 2004.Methods for domain-independent information extrac-tion from the web: an experimental comparison.
AAAI?04.Hasegawa, T., Sekine, S., Grishman, R., 2004.
Discover-ing relations among named entities from large corpora.ACL ?04.Hassan, H., Hassan, A., Emam, O., 2006. unsupervisedinformation extraction approach using graph mutualreinforcement.
EMNLP ?06.Hearst, M., 1992.
Automatic acquisition of hyponymsfrom large text corpora.
COLING ?92.Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,Girju, R., 2004.
Models for the semantic classifica-tion of noun phrases.
Workshop on Comput.
LexicalSemantics at HLT-NAACL ?04.Pantel, P., Ravichandran, D., Hovy, E., 2004.
Towardsterascale knowledge acquisition.
COLING ?04.Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A., 2006.Names and similarities on the web: fact extraction inthe fast lane.
COLING-ACL ?06.Roark, B., Charniak, E., 1998.
Noun-phrase co-occurrence statistics for semi-automatic semantic lex-icon construction.
ACL ?98.Rosenfeld B., Feldman, R.: URES : an unsupervisedweb relation extraction system.
Proceedings, ACL ?06Poster Sessions.Sekine, S., 2006 On-demand information extraction.COLING-ACL ?06.Strube, M., Ponzetto, S., 2006.
WikiRelate!
computingsemantic relatedness using Wikipedia.
AAAI ?06.Suchanek F. M., G. Ifrim, G. Weikum.
2006.
LEILA:learning to extract information by linguistic analysis.Workshop on Ontology Learning and Population atCOLING-ACL ?06.Turney, P., 2006.
Expressing implicit semantic relationswithout supervision.
COLING-ACL ?06.Widdows, D., Dorow, B., 2002.
A graph model for unsu-pervised Lexical acquisition.
COLING ?02.239
