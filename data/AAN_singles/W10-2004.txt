Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27?35,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsHHMM Parsing with Limited ParallelismTim MillerDepartment of Computer Scienceand EngineeringUniversity of Minnesota, Twin Citiestmill@cs.umn.eduWilliam SchulerUniversity of Minnesota, Twin Citiesand The Ohio State Universityschuler@ling.ohio-state.eduAbstractHierarchical Hidden Markov Model(HHMM) parsers have been proposed aspsycholinguistic models due to their broadcoverage within human-like workingmemory limits (Schuler et al, 2008) andability to model human reading timebehavior according to various complexitymetrics (Wu et al, 2010).
But HHMMshave been evaluated previously only withvery wide beams of several thousandparallel hypotheses, weakening claims tothe model?s efficiency and psychologicalrelevance.
This paper examines the effectsof varying beam width on parsing accu-racy and speed in this model, showing thatparsing accuracy degrades gracefully asbeam width decreases dramatically (to 2%of the width used to achieve previous topresults), without sacrificing gains over abaseline CKY parser.1 IntroductionProbabilistic parsers have been successful at ac-curately estimating syntactic structure from freetext.
Typically, these systems work by consider-ing entire sentences (or utterances) at once, usingdynamic programming to obtain globally optimalsolutions from locally optimal sub-parses.However, these methods usually do not attemptto conform to human-like processing constraints,e.g.
leading to center embedding and garden patheffects (Chomsky and Miller, 1963; Bever, 1970).For systems prioritizing accurate parsing perfor-mance, there is little need to produce human-likeerrors.
But from a human modeling perspective,the success of globally optimized whole-utterancemodels raises the question of how humans can ac-curately parse linguistic input without access tothis same global optimization.
This question cre-ates a niche in computational research for modelsthat are able to parse accurately while adhering asclosely as possible to human-like psycholinguisticconstraints.Recent work on incremental parsers includeswork on Hierarchical Hidden Markov Model(HHMM) parsers that operate in linear time bymaintaining a bounded store of incomplete con-stituents (Schuler et al, 2008).
Despite this seem-ing limitation, corpus studies have shown thatthrough the use of grammar transforms, this parseris able to cover nearly all sentences contained inthe Penn Treebank (Marcus et al, 1993) using asmall number of unconnected memory elements.But this bounded-memory parsing comes at aprice.
The HHMM parser obtains good coveragewithin human-like memory bounds only by pur-suing an ?optionally arc-eager?
parsing strategy,nondeterministically guessing which constituentscan be kept open for attachment (occupying an ac-tive memory element), or closed for attachment(freeing a memory element for subsequent con-stituents).
Although empirically determining thenumber of parallel competing hypotheses used inhuman sentence processing is difficult, previousresults in computational models have shown thathuman-like behavior can be elicited at very lowlevels of parallelism (Boston et al, 2008b; Brantsand Crocker, 2000), suggesting that large num-bers of active hypotheses are not needed.
Previ-ously, the HHMM parser has only been evaluatedon large beam widths, leaving this aspect of itspsycholinguistic plausibility untested.In this paper, the performance of an HHMMparser will be evaluated in two experiments that27vary the amount of parallelism allowed duringparsing, measuring the degree to which this de-grades the system?s accuracy.
In addition, theevaluation will compare the HHMM parser to anoff-the-shelf probabilistic CKY parser to evaluatethe actual run time performance at various beamwidths.
This serves two purposes, evaluating oneaspect of the plausibility of this parsing frame-work as a psycholinguistic model, and evaluatingits potential utility as a tool for operating on un-segmented text or speech.2 Related WorkThere are several criteria a parser must meetin order to be plausible as a psycholinguisticmodel of the human sentence-processing mecha-nism (HSPM).Incremental operation is perhaps the most obvi-ous.
The HSPM is able to process sentences in-crementally, meaning that at each point in time ofprocessing input, it has some hypothesis of the in-terpretation of that input, and each subsequent unitof input serves to update that hypothesis.The next criterion for psycholinguistic plausi-bility is processing efficiency.
The HSPM notonly operates incrementally, but in standard op-eration it does not lag behind a speaker, even if,for example, the speaker continues speaking at ex-tended length without pause.
Standard machineapproaches, such as chart parsers based on theCKY algorithm, operate in worst-case cubic runtime on the length of input.
Without knowingwhere an utterance or sentence might end, such analgorithm will take more time with each succes-sive word and will eventually fall behind.The third criterion is a reasonable limiting ofmemory resources.
This constraint means that theHSPM, while possibly considering multiple hy-potheses in parallel, is not limitlessly so, as evi-denced by the existence of garden path sentences(Bever, 1970; Lewis, 2000).
If this were not thecase, garden-path sentences would not cause prob-lems, as reaching the disambiguating word wouldsimply result in a change in the favored hypothe-sis.
In fact, garden path sentences typically cannotbe understood on a first pass and must be reread,indicating that the correct analysis is attainableand yet not present in the set of parallel hypothesesof the first pass.While parsers meeting these three criteria canclaim to not violate any psycholinguistic con-straints, there has been much recent work intesting psycholinguistically-motivated parsers tomake forward predictions about human sentenceprocessing, in order to provide positive evidencefor certain probabilistic parsing models as validpsycholinguistic models of sentence processing.This work has largely focused on correlating mea-sures of parsing difficulty in computational modelswith delays in reading time in human subjects.Hale (2001) introduced the surprisal metric forprobabilistic parsers, which measures the log ra-tio of the total probability mass at word t ?
1and word t. In other words, it measures howmuch probability was lost in incorporating thenext word into the current hypotheses.
Boston etal.
(2008a) show that surprisal is a significant pre-dictor of reading time (as measured in self-pacedreading experiments) using a probabilistic depen-dency parser.
Roark et al (2009) dissected parsingdifficulty metrics (including surprisal and entropy)to separate out the effects of syntactic and lexicaldifficulties, and showed that these new metrics arestrong predictors of reading difficulty.Wu et al (2010) evaluate the same HierarchicalHidden Markov Model parser used in this work interms of its ability to reproduce human-like resultsfor various complexity metrics, including some ofthose mentioned above, and introduce a new met-ric called embedding difference.
This metric isbased on the idea of embedding depth, which isthe number of elements in the memory store re-quired to hold a given hypothesis.
Using morememory elements corresponds to center embed-ding in phrase structure trees, and presumably cor-relates to some degree with complexity.
Averageembedding for a time step is computed by com-puting the weighted average number of requiredmemory elements (weighted by probability) for allhypotheses on the beam.
Embedding difference issimply the change in this value when the next wordis encountered.Outside of Wu et al, the most similar workfrom a modeling perspective is an incremen-tal parser implemented using Cascaded HiddenMarkov Models (CHMMs) (Crocker and Brants,2000).
This model is superficially similar to theHierarchical Hidden Markov Models describedbelow in that it relies on multiple levels of interde-pendent HMMs to account for hierarchical struc-ture in an incremental model.
Crocker and Brantsuse the system to parse ambiguous sentences (such28as the athlete realized his goals were out of reach)and examine the relative probabilities of two plau-sible analyses at each time step.
They then showthat the shifting of these two probabilities is con-sistent with empirical evidence about how humansperceive these sentences word by word.However, as will be described below, theHHMM has advantages over the CHMM froma psycholinguistic modeling perspective.
TheHHMM uses a limited memory store contain-ing only four elements which is consistent withmany estimates of human short term memory lim-its (Cowan, 2001; Miller, 1956).
In addition tomodeling memory limits, the limited store acts asa fixed-depth stack that ensures linear asymptoticparsing time, and a grammar transform allows forwide coverage of speech and newspaper corporawithin that limited memory store (Schuler et al,2010).3 Hierarchical Hidden Markov ModelParserHidden Markov Models (HMMs) have long beenused to successfully model sequence data in whichthere is a latent (hidden) variable at each time stepthat generates the observed evidence at that timestep.
These models are used for such applicationsas part-of-speech tagging, and speech recognition.Hierarchical Hidden Markov Models (HH-MMs) are an extension of HMMs which can rep-resent sequential data containing hierarchical rela-tions.
In HHMMs, complex hidden variables mayoutput evidence for several time steps in sequence.This process may recurse, though a finite depthis required to make any guarantees about perfor-mance.
Murphy and Paskin (2001) showed thatthis model could be framed as a Dynamic BayesNetwork (DBN), so that inference is linear on thelength of the input sequence.In the HHMM parser used here, the complexhidden variables are syntactic states that gener-ate sub-sequences of other syntactic states, even-tually generating pre-terminals and words.
Thissection will describe how the trees must be trans-formed, and then mapped to HHMM states.
Thissection will then continue with a formal definitionof an HHMM, followed by a description of howthis model can parse natural language, and finallya discussion of what different aspects of the modelrepresent in terms of psycholinguistic modeling.3.1 Right-Corner TransformIn order to parse with an HHMM, phrase struc-ture trees need to be mapped to a hierarchical se-quence of states of nested HMMs.
Since Mur-phy and Paskin showed that the run time complex-ity of the HHMM is exponential on the depth ofthe nested HMMs, it is important to minimize thedepth of the model for optimal performance.
Inorder to do this, a tree transformation known asa right-corner transform is applied to the phrasestructure trees comprising the training data, totransform right-expanding sequences of completeconstituents into left-expanding sequences of in-complete constituents A?/A?, consisting of an in-stance of an active constituent A?
lacking an in-stance of an awaited constituent A?
yet to be rec-ognized.
This transform can be defined as a syn-chronous grammar that maps every context-freerule expansion in a source tree (in Chomsky Nor-mal Form) to a corresponding expansion in a right-corner transformed tree:1?
Beginning case: the top of a right-expandingsequence in an ordinary phrase structure treeis mapped to the bottom of a left-expandingsequence in a right-corner transformed tree:A?A??0?A??1??A?A?/A??1A??0??(1)?
Middle case: each subsequent branch ina right-expanding sequence of an ordinaryphrase structure tree is mapped to a branch ina left-expanding sequence of the transformedtree:A??
A???A????0?A????1??A?A?/A????1A?/A????A????0??(2)?
Ending case: the bottom of a right-expandingsequence in an ordinary phrase structure tree1Here, ?
and ?
are tree node addresses, consisting of se-quences of zeros, representing left branches, and ones, repre-senting right branches, on a path from the root of the tree toany given node.29a) A?A?0a ?00 A?01A?010a ?0100a ?0101A?011a ?0110a ?0111A?1A?10A?100a ?1000a ?1001A?101a ?1010a ?1011a ?11b) A?A?/A?11A?/A?1A?0A?0/A?0111A?0/A?011A?0/A?01a ?00A?010A?010/A?0101a ?0100 a ?0101a ?0110 a ?0111A?10A?10/A?1011A?10/A?101A?100A?100/A?1001a ?1000 a ?1001a ?1010 a ?1011a ?11Figure 1: Sample right-corner transform ofschematized tree before (a) and after (b) applica-tion of transform.is mapped to the top of a left-expanding se-quence in a right-corner transformed tree:A??
A???a????A?A?/A????A???a???
(3)The application of this transform is exemplified inFigure 1.3.2 Hierarchical Hidden Markov ModelsRight-corner transformed trees are mapped to ran-dom variables in a Hierarchical Hidden MarkovModel (Murphy and Paskin, 2001).A Hierarchical Hidden Markov Model(HHMM) is essentially a factored version ofa Hidden Markov Model (HMM), configured torecognize bounded recursive structures (i.e.
trees).Like HMMs, HHMMs use Viterbi decoding toobtain sequences of hidden states s?1..T givensequences of observations o1..T (words or audiofeatures), through independence assumptionsin a transition model ?A and an observationmodel ?B (Baker, 1975; Jelinek et al, 1975):s?1..Tdef= argmaxs1..TT?t=1P?A(st | st?1) ?
P?B(ot | st)(4)HHMMs then factor the hidden state transition?Ainto a reduce and shift phase (Equation 5), theninto a bounded set of depth-specific operations(Equation 6):P?A(st|st?1) =?rtP?R(rt|st?1)?P?S(st|rt st?1)(5)def=?r1..DtD?d=1P?R,d(rdt | rd+1t sdt?1sd?1t?1 )?P?S,d(sdt |rd+1t rdt sdt?1sd?1t )(6)which allow depth-specific variables to reduce(through ?R-Rdn,d), transition (?S-Trn,d), and ex-pand (?S-Exp,d) like tape symbols in a pushdownautomaton with a bounded memory store, depend-ing on whether the variable below has reduced(rdt ?RG) or not (rdt 6?RG):2P?R,d(rdt | rd+1t sdt?1sd?1t?1 )def={if rd+1t 6?RG : Jrdt =r?Kif rd+1t ?RG : P?R-Rdn,d(rdt | rd+1t sdt?1 sd?1t?1 )(7)P?S,d(sdt | rd+1t rdt sdt?1sd?1t )def=??
?if rd+1t 6?RG, rdt 6?RG : Jsdt =sdt?1Kif rd+1t ?RG, rdt 6?RG : P?S-Trn,d(sdt | rd+1t rdt sdt?1sd?1t )if rd+1t ?RG, rdt ?RG : P?S-Exp,d(sdt | sd?1t )(8)where s0t = s?
and rD+1t = r?
for constants s?
(an incomplete root constituent), r?
(a completelexical constituent) and r?
(a null state resultingfrom reduction failure) s.t.
r?
?RG and r?
6?RG.Right-corner transformed trees, as exemplifiedin Figure 1(b), can then be aligned to HHMMstates as shown in Figure 2, and used to train anHHMM as a parser.Parsing with an HHMM simply involves pro-cessing the input sequence, and estimating a mostlikely hidden state sequence given this observedinput.
Since the output is to be the best possibleparse, the Viterbi algorithm is used, which keepstrack of the highest probability state at each timestep, where the state is the store of incomplete syn-tactic constituents being processed.
State transi-tions are computed using the models above, andeach state at each time step keeps a back pointer tothe state it most probably came from.
Extractingthe highest probability parse requires extracting2Here, J?K is an indicator function: J?K = 1 if ?
is true, 0otherwise.30d=1d=2d=3wordt=1 t=2 t=3 t=4 t=5 t=6 t=7a?0101a?0110a?0111a?1000a?1001a?1010?
?
?
?
?
??
?
?
?A?100 /A?1001A?10 /A?101A?10 /A?1011?A?0 /A?011A?0 /A?0111A?
/A?1A?
/A?1A?
/A?1A?
/A?1Figure 2: Mapping of schematized right-cornertree into HHMM memory elements.the most likely sequence, deterministically map-ping that sequence back to a right-corner tree, andreversing the right-corner transform to produce anordinary phrase structure tree.Unfortunately exact inference is not tractablewith this model and dataset.
The state space istoo large to manage for both space and time rea-sons, and thus approximate inference is carriedout, through the use of a beam search.
At eachtime step, only the top N most probable hypoth-esized states are maintained.
Experiments de-scribed in (Schuler, 2009) suggest that there doesnot seem to be much lost in going from exact in-ference using the CKY algorithm to a beam searchwith a relatively large width.
However, the op-posite experiment, examining the effect of goingfrom a relatively wide beam to a very narrow beamhas not been thoroughly studied in this parsing ar-chitecture.4 Optionally Arc-eager ParsingThe right-corner transform described in Sec-tion 3.1 saves memory because it transforms anyright-expanding sequence with left-child subtreesinto a left-expanding sequence of incomplete con-stituents, with the same sequence of subtrees asright children.
The left-branching sequences ofsiblings resulting from this transform can then becomposed bottom-up through time by replacingeach left child category with the category of theresulting parent, within the same memory element(or depth level).
For example, in Figure 3(a) aleft-child category NP/NP at time t=4 is composedwith a noun new of category NP/NNP (a nounphrase lacking a proper noun yet to come), result-ing in a new parent category NP/NNP at time t=5replacing the left child category NP/NP in the top-most d=1 memory element.This in-element composition preserves ele-ments of the bounded memory store for use in pro-cessing descendants of this composed constituent,yielding the human-like memory demands re-ported in (Schuler et al, 2008).
But wheneveran in-element composition like this is hypothe-sized, it isolates an intermediate constituent (inthis example, the noun phrase ?new york city?
)from subsequent composition.
Allowing accessto this intermediate constituent ?
for example,to allow ?new york city?
to become a modifierof ?bonds?, which itself becomes an argument of?for?
?
requires an analysis in which the interme-diate constituent is stored in a separate memoryelement, shown in Figure 3(b).
This creates a lo-cal ambiguity in the parser (in this case, from timestep t=4) that may have to be propagated acrossseveral words before it can be resolved (in thiscase, at time step t=7).
This is essentially an am-biguity between arc-eager (in-element) and arc-standard (cross-element) composition strategies,as described by Abney and Johnson (1991).
Incontrast, an ordinary (purely arc-standard) parserwith an unbounded stack would only hypothesizeanalysis (b), avoiding this ambiguity.3The right-corner HHMM approach describedin this paper relies on a learned statistical modelto predict when in-element (arc-eager) compo-sitions will occur, in addition to hypothesizingparse trees.
The model encodes a mixed strategy:with some probability arc-eager or arc-standardfor each possible expansion.
Accuracy results ona right-corner HHMM model trained on the PennWall Street Journal Treebank suggest that this kindof optionally arc-eager strategy can be reliably sta-tistically learned.By placing firm limits on the number of openincomplete constituents in working memory, theHierarchical HMM parser maintains parallel hy-potheses on the beam which predict whether eachconstituent will host a subsequent attachment ornot.
Empirical results described in the next section3It is important to note that neither the right-corner norleft-corner parsing strategy by itself creates this ambiguity.The ambiguity arises from the decision to use this option-ally arc-eager strategy to reduce memory store allocation ina bounded memory parser.
Implementations of left-cornerparsers such as that of Henderson (2004) adopt a arc-standardstrategy, essentially always choosing analysis (b) above, andthus do not introduce this kind of local ambiguity.
But inadopting this strategy, such parsers must maintain a stackmemory of unbounded size, and thus are not attractive asmodels of human parsing in short-term memory (Resnik,1992).31a)d=1d=2d=3wordt=1 t=2 t=3 t=4 t=5 t=6 t=7strongdemandfornewyorkcity?
?
?
?
?
??
?
?
?
?
??NP/NNNP/PPNP/NPNP/NNPNP/NNPNP(dem.
)b)d=1d=2d=3wordt=1 t=2 t=3 t=4 t=5 t=6 t=7strongdemandfornewyorkcity?
?
?
?
?
??
?
?
?NNP/NNPNNP/NNPNP(city)?NP/NNNP/PPNP/NPNP/NPNP/NPNP(dem.)/NP(?
)Figure 3: Alternative analyses of ?strong demand for new york city ...?
: a) using in-element composition,compatible with ?strong demand for new york city is ...?
(in which the demand is for the city); and b)using cross-element (or delayed) composition, compatible with either ?strong demand for new york cityis ...?
(in which the demand is for the city) or ?strong demand for new york city bonds is ...?
(in which aforthcoming referent ?
in this case, bonds ?
is associated with the city, and is in demand).
In-elementcomposition (a) saves memory but closes off access to the noun phrase headed by ?city?, and so is notincompatible with the ?...bonds?
completion.
Cross-element composition (b) requires more memory,but allows access to the noun phrase headed by ?city?, so is compatible with either completion.
Thisambiguity is introduced at t=4 and propagated until at least t=7.
An ordinary, non-right-corner stackmachine would exclusively use analysis (b), avoiding ambiguity.show that this added demand on parallelism doesnot substantially degrade parsing accuracy, even atvery narrow beam widths.5 Experimental EvaluationThe parsing model described in Section 3 haspreviously been evaluated on the standard taskof parsing the Wall Street Journal section of thePenn Treebank.
This evaluation was optimizedfor accuracy results, and reported a relatively widebeam width of 2000 to achieve its best results.However, most psycholinguistic models of the hu-man sentence processing mechanism suggest thatif the HSPM does work in parallel, it does so witha much lower number of concurrent hypotheses(Boston et al, 2008b).
Viewing the HHMM pars-ing framework as a psycholinguistic model, a nec-essary (though not sufficient) condition for it beinga valid model is that it be able to maintain rela-tively accurate parsing capabilities even at muchlower beam widths.Thus, the first experiments in this paper evalu-ate the degradation of parsing accuracy dependingon beam width of the HHMM parser.
Experimentswere conducted again on the WSJ Penn Treebank,using sections 02-21 to train, and section 23 as thetest set.
Punctuation was included in both train-ing and testing.
A set of varied beam widths wereconsidered, from a high of 2000 to a low of 15.This range was meant to roughly correspond tothe range of parallelism used in other similar ex-periments, using 2000 as a high end due to its us-age in previous parsing experiments.
However, itshould be noted that in fact the highest value of2000 is already an approximate search ?
prelim-inary experiments showed that exhaustive searchwith the HHMM would require more than 100000elements per time step (exact values may be muchhigher but could not be collected because they ex-hausted system memory).The HHMM parser was compared to a custombuilt (though standard) probabilistic CKY parserimplementation trained on the CNF trees used asinput to the right-corner transform, so that theCKY parser was able to compete on a fair foot-ing.
The accuracy results of these experiments areshown in Figure 4.These results show fairly graceful decline inparsing accuracy with a beam width starting at2000 elements down to about 50 beam elements.This beam width is much less than 1% of the ex-haustive search, though it is around 1% of whatmight be considered the highest reasonable beamwidth for efficient parsing.
The lowest beamwidths attempted, 15, 20, and 25, result in ac-curacy below that of the CKY parser.
The low-est beam width attempted, 15, shows the sharpestdecline in accuracy, putting the HHMM systemnearly 8 points below the CKY parser in terms ofaccuracy.This compares reasonably well to results by3270727476788082840  100  200  300  400  500LabeledF-ScoreBeam WidthFigure 4: Plot of parsing accuracy (labeled F-score) vs. beam widths for an HHMM parser(curved line).
Top line is HHMM accuracy withbeam width of 2000 (upper bound).
The bottomline is CKY parser results.
Points correspond tobeam widths of 15, 20, 25, 50, 100, 250, and 500.Brants and Crocker (2000) showing that an in-cremental chart-parsing algorithm can parse accu-rately with pruning down to 1% of normal memoryusage.
While that parsing algorithm is difficult tocompare directly to this HHMM parser, the reduc-tion in beam width in this system to 50 beam el-ements from an already approximated 2000 beamelements shows similar robustness to approxima-tion.
Accuracy comparisons should be taken witha grain of salt due to additional annotations per-formed to the Treebank before training, but theHHMM parser with a beam width of 50 obtainsapproximately the same accuracy as the Brantsand Crocker incremental CKY parser pruning to3% of chart size.
At 1% pruning, Brants andCrocker achieved around 75% accuracy, whichfalls between the HHMM parser at beam widthsof 20 and 25.Results by Boston et al (2008b) are also dif-ficult to compare directly due to a difference inparsing algorithm and different research priority(that paper was attempting to correlate parsing dif-ficulty with reading difficulty).
However, that pa-per showed that a dependency parser using lessthan ten beam elements (and as few as one) wasjust as capable of predicting reading difficulty asthe parser using 100 beam elements.A second experiment was conducted to eval-uate the HHMM for its time efficiency in pars-ing.
This experiment is intended to address twoquestions: Whether this framework is efficient0246810121410  20  30  40  50  60  70Secondsper sentenceSentence LengthCKYHHMMFigure 5: Plot of parsing time vs. sentence lengthfor HHMM and CKY parsers.enough to be considered a viable psycholinguis-tic model, and whether its parsing time and accu-racy remain competitive with more standard cu-bic time parsing technologies at low beam widths.To evaluate this aspect, the HHMM parser wasrun at low beam widths on sentences of varyinglengths.
The baseline was the widely-used Stan-ford parser (Klein and Manning, 2003), run in?vanilla PCFG?
mode.
This parser was used ratherthan the custom-built CKY parser from the pre-vious experiment, to avoid the possibility that itsimplementation was not efficient enough to pro-vide a realistic test.
The HHMMparser was imple-mented as described in the previous section.
Theseexperiments were run on a machine with a single2.40 GHz Celeron CPU, with 512 MB of RAM.
Inboth implementations the parser timing includesonly time spent actually parsing sentences, ignor-ing the overhead incurred by reading in model filesor training.Figure 5 shows a plot of parsing time versussentence length for the HHMM parser for a beamwidth of 20.
Sentences shorter than 10 words werenot included for visual clarity (both parsers are ex-tremely fast at that length).
At this beam width,the performance of the HHMM parser (labeled F-score) was 74.03%, compared to 71% for a plainCKY parser.
As expected, the HHMM parsingtime increases linearly with sentence length, whilethe CKY parsing time increases super-linearly.
(However, due to high constants in the run timecomplexity of the HHMM, it was not a priori clearthat the HHMM would be faster for any sentenceof reasonable length.
)33The results of this experiment show that theHHMM parser is indeed competitive with a proba-bilistic CKY parser, in terms of parsing efficiency,even while parsing with higher accuracy.
At sen-tences longer that 26 words (including punctua-tion), the HHMM parser is faster than the CKYparser.
This advantage is clear for segmented textsuch as the Wall Street Journal corpus.
However,this advantage is compounded when consideringunsegmented or ambiguously segmented text suchas transcribed speech or less formal written text, asthe HHMM parser can also make decisions aboutwhere to put sentence breaks, and do so in lineartime.46 Conclusion and Future WorkThis paper furthers the case for the HHMM as aviable psycholinguistic model of the human pars-ing mechanism by showing that performance de-grades gracefully as parallelism decreases, provid-ing reasonably accurate parsing even at very lowbeam widths.
In addition, this work shows thatan HHMM parser run at low beam widths is com-petitive in speed with parsers that don?t work in-crementally, because of its asymptotically linearruntime.This is especially surprising given that theHHMM uses parallel hypotheses on the beam topredict whether constituents will remain open forattachment or not.
Success at low beam widthssuggests that this optionally arc-eager predictionis something that is indeed relatively predictableduring parsing, lending credence to claims of psy-cholinguistic relevance of HHMM parsing.Future work should explore further directionsin improving parsing performance at low beamwidths.
The lowest beam value experimentspresented here generally parsed fairly accuratelywhen they completed, but were already encounter-ing problems with unparseable sentences that neg-atively affected parser accuracy.
The large accu-racy decrease between beam sizes of 20 and 15 islikely to be mostly due to the lack of any correctanalysis on the beam when the sentence is com-pleted.It should be noted, however, that no adjustmentswere made to the parser?s syntactic model withthese beam variations.
This syntactic model wasoptimized for accuracy at the standard beam width4It does this probabilistically as a side effect of the pars-ing, by choosing an analysis in which r0t ?
RG (for any t).of 2000, and thus contains some state splittingsthat are beneficial at wide beam widths, but atlow beam widths are redundant and prevent oth-erwise valid hypotheses from being maintained onthe beam.
For applications in which speed is apriority, future research can evaluate tradeoffs inaccuracy that occur at different beam widths witha coarser-grained syntactic representation that al-lows for more variation of hypotheses even onvery small beams.AcknowledgmentsThis research was supported by National ScienceFoundation CAREER/PECASE award 0447685.The views expressed are not necessarily endorsedby the sponsors.ReferencesSteven P. Abney and Mark Johnson.
1991.
Memoryrequirements and local ambiguities of parsing strate-gies.
J. Psycholinguistic Research, 20(3):233?250.James Baker.
1975.
The Dragon system: an overivew.IEEE Transactions on Acoustics, Speech and SignalProcessing, 23(1):24?29.Thomas G. Bever.
1970.
The cognitive basis for lin-guistic structure.
In J.
?R.
Hayes, editor, Cognitionand the Development of Language, pages 279?362.Wiley, New York.Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,Umesh Patil, and Shravan Vasishth.
2008a.
Parsingcosts as predictors of reading difficulty: An evalua-tion using the Potsdam Sentence Corpus.
Journal ofEye Movement Research, 2(1):1?12.Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,and Shravan Vasishth.
2008b.
Surprising parser ac-tions and reading difficulty.
In Proceedings of ACL-08: HLT, Short Papers, pages 5?8, Columbus, Ohio,June.
Association for Computational Linguistics.Thorsten Brants and Matthew Crocker.
2000.
Prob-abilistic parsing and psychological plausibility.
InProceedings of COLING ?00, pages 111?118.Noam Chomsky and George A. Miller.
1963.
Intro-duction to the formal analysis of natural languages.In Handbook of Mathematical Psychology, pages269?321.
Wiley.Nelson Cowan.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storagecapacity.
Behavioral and Brain Sciences, 24:87?185.Matthew Crocker and Thorsten Brants.
2000.
Wide-coverage probabilistic sentence processing.
Journalof Psycholinguistic Research, 29(6):647?669.34John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the Sec-ond Meeting of the North American Chapter of theAssociation for Computational Linguistics, pages159?166, Pittsburgh, PA.James Henderson.
2004.
Lookahead in determinis-tic left-corner parsing.
In Proc.
Workshop on Incre-mental Parsing: Bringing Engineering and Cogni-tion Together, pages 26?33, Barcelona, Spain.Frederick Jelinek, Lalit R. Bahl, and Robert L. Mercer.1975.
Design of a linguistic statistical decoder forthe recognition of continuous speech.
IEEE Trans-actions on Information Theory, 21:250?256.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 423?430, Sapporo,Japan.Richard L. Lewis.
2000.
Falsifying serial and paral-lel parsing models: Empirical conundrums and anoverlooked paradigm.
Journal of PsycholinguisticResearch, 29:241?248.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.George A. Miller.
1956.
The magical number seven,plus or minus two: Some limits on our capacityfor processing information.
Psychological Review,63:81?97.Kevin P. Murphy and Mark A. Paskin.
2001.
Lin-ear time inference in hierarchical HMMs.
In Proc.NIPS, pages 833?840, Vancouver, BC, Canada.Philip Resnik.
1992.
Left-corner parsing and psy-chological plausibility.
In Proceedings of COLING,pages 191?197, Nantes, France.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical andsyntactic expectation-based measures for psycholin-guistic modeling via incremental top-down parsing.Proceedings of the 2009 Conference on EmpiricalMethods in Natural Langauge Processing, pages324?333.William Schuler, Samir AbdelRahman, TimMiller, and Lane Schwartz.
2008.
Toward apsycholinguistically-motivated model of language.In Proceedings of COLING, pages 785?792,Manchester, UK, August.William Schuler, Samir AbdelRahman, TimMiller, andLane Schwartz.
2010.
Broad-coverage incremen-tal parsing using human-like memory constraints.Computational Linguistics, 36(1).William Schuler.
2009.
Parsing with a boundedstack using a model-based right-corner transform.In Proceedings of the North American Associationfor Computational Linguistics (NAACL ?09), pages344?352, Boulder, Colorado.Stephen Wu, Asaf Bachrach, Carlos Cardenas, andWilliam Schuler.
2010.
Complexity metrics in anincremental right-corner parser.
In Proceedings ofthe 49th Annual Conference of the Association forComputational Linguistics.35
