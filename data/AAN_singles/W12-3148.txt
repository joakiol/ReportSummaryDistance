Proceedings of the 7th Workshop on Statistical Machine Translation, pages 374?381,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSelecting Data for English-to-Czech Machine Translation ?Ales?
Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos?
Stanojevic?, Ondr?ej BojarCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?m.
25, Praha 1, CZ-118 00, Czech Republic{tamchyna,galuscakova,kamran,bojar}@ufal.mff.cuni.cz,milosh.stanojevic@gmail.comAbstractWe provide a few insights on data selection formachine translation.
We evaluate the qualityof the new CzEng 1.0, a parallel data sourceused in WMT12.
We describe a simple tech-nique for reducing out-of-vocabulary rate af-ter phrase extraction.
We discuss the bene-fits of tuning towards multiple reference trans-lations for English-Czech language pair.
Weintroduce a novel approach to data selectionby full-text indexing and search: we selectsentences similar to the test set from a largemonolingual corpus and explore several op-tions of incorporating them in a machine trans-lation system.
We show that this method canimprove translation quality.
Finally, we de-scribe our submitted system CU-TAMCH-BOJ.1 IntroductionSelecting suitable data is important in all stages ofcreating an SMT system.
For training, the data sizeplays an essential role, but the data should also be asclean as possible.
The new CzEng 1.0 was preparedwith the emphasis on data quality and we evaluateit against the previous version to show whether theeffect for MT is positive.Out-of-vocabulary rate is another problem relatedto data selection.
We present a simple technique toreduce it by including words that became spuriousOOVs during phrase extraction.?
This work was supported by the project EuroMatrixPlus(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 ofthe Czech Republic) and the Czech Science Foundation grantsP406/11/1499 and P406/10/P259.Another topic we explore is to use multiple refer-ences for tuning to make the procedure more robustas suggested by Dyer et al (2011).
We evaluate thisapproach for translating from English into Czech.The main focus of our paper however lies in pre-senting a method for data selection using full-textsearch.
We index a large monolingual corpus andthen extract sentences from it that are similar to theinput sentences.
We use these sentences in severalways: to create a new language model, a new phrasetable and a tuning set.
The method can be seen asa kind of domain adaptation.
We show that it con-tributes positively to translation quality and we pro-vide a thorough evaluation.2 Data and Tools2.1 Comparison of CzEng 1.0 and 0.9As this year?s WMT is the first to include the newversion of CzEng (Bojar et al, 2012b), we carriedout a few experiments to compare its suitability forMT with its predecessor, CzEng 0.9.
Apart fromsize (which has almost doubled), there are impor-tant differences between the two versions.
In CzEng0.9, the largest portion by far came from movie sub-titles (a data source of varying quality), followed byEU legislation and technical manuals.
On the otherhand, CzEng 1.0 has over 4 million sentence pairsfrom fiction and nearly the same amount of datafrom EU legislation.
Roughly 3 million sentencepairs come from movie subtitles.
This proportionof domains suggests a higher quality of data.
More-over, sentences in CzEng 1.0 were automatically fil-tered using a maximum entropy classifier that uti-374Vocab.
[k]Corpus and Domain Sents BLEU En CsCzEng 0.9all 1M14.77?0.12 187 360CzEng 1.0 15.23?0.18 221 396CzEng 0.9news 100k14.34?0.05 53 125CzEng 1.0 14.01?0.13 47 113Table 1: Comparison of CzEng 0.9 and 1.0.lized a variety of features.We trained contrastive phrase-based Moses SMTsystems?the first one on 1 million randomly se-lected sentence pairs from CzEng 0.9, the other onthe same amount of data from CzEng 1.0.
Anothercontrastive pair of MT systems was based on smallin-domain data only: 100k sentences from the newssections of CzEng 0.9 and 1.0.
For each experiment,the random selection was done 5 times.
In bothexperiments, identical data were used for the LM(News Crawl corpus from 2011), tuning (WMT10test set) and evaluation (WMT11 test set).Table 1 shows the results.
The ?
sign in this casedenotes the standard deviation over the 5 experi-ments (each with a different random sample of train-ing data).
The results indicate that overall, CzEng1.0 is a more suitable source of parallel data?mostlikely thanks to the more favorable distribution ofdomains.
However in the small in-domain setting,using CzEng 0.9 data resulted in significantly higherBLEU scores.The vocabulary size of the news section seems tohave dropped since 0.9.
We attribute this to the filter-ing: sentences with obscure words are hard to alignso they are likely to be filtered out (the word align-ment score as output by Giza++ received a largeweight in the classifier training).
These unusualwords then do not appear in the vocabulary.2.2 LuceneApache Lucene1 is a high performance open-sourcesearch engine library written in Java.
We use Luceneto take advantage of the information retrieval (IR)technique for domain adaptation.
Each sentence ofa large corpus is indexed as a separate document; adocument is the unit of indexing and searching inLucene.
The sentences (documents) can then be re-1http://lucene.apache.orgtrieved based on Lucene similarity formula2, givena ?query corpus?.
Lucene uses Boolean model forinitial filtering of documents.
Vector Space Modelwith a refined version of Tf-idf statistic is then usedto score the remaining candidates.In the normal IR scenario, the query is usuallysmall.
However, for domain adaptation a query canbe a whole corpus.
Lucene does not allow suchbig queries.
This problem is resolved by takingthe query corpus sentence by sentence and search-ing many times.
The final score of a sentence in theindex is calculated as the average of the scores fromthe sentence-level queries.
Methods that make useof this functionality are discussed in Section 5.3 Reducing OOV by Relaxing AlignmentsOut-of-vocabulary (OOV) rate has been shown toincrease during phrase extraction (Bojar and Kos,2010).
This is due to unfortunate alignment of somewords?no consistent phrase pair that includes themcan be extracted.
This issue can be partially over-come by adding translations of these ?lost?
words(according to Giza++ word alignment) to the ex-tracted phrase table.
This is not our original tech-nique, it was suggested by Mermer and Saraclar(2011), though it is not included in the published ab-stract.The extraction of phrases in the (hierarchical) de-coder Jane (Stein et al, 2011) offers a range of sim-ilar heuristics.
Tinsley et al (2009) also observesgains when extending the set of phrases consistentwith the word alignment by phrases consistent withaligned parses.We evaluated this technique on two sets of train-ing data?the news section of CzEng 1.0 and thewhole CzEng 1.0.
The OOV rate of the phrase tablewas reduced nearly to the corpus OOV rate in bothcases, however the improvement was negligible?only a handful of the newly added words occurredin the test set.
Table 2 shows the results.
Trans-lation performance using the improved phrase tablewas identical to the baseline.2http://tiny.cc/ca2ccw375Test Set OOV % NewCzEng Sections Baseline Reduced Phrasesnews (197k sents) 3.69 3.66 12034all (14.8M sents) 1.09 1.09 154204Table 2: Source-side phrase table OOV.Sections 1 reference 3 referencesnews 11.37?0.47 11.62?0.50all 16.07?0.55 15.90?0.57Table 3: BLEU scores on WMT12 test set when tuningon WMT11 test set towards one or more references.4 Tuning to Multiple ReferenceTranslationsTuning towards multiple reference translations hasbeen shown to help translation quality, see Dyer etal.
(2011) and the cited works.
Thanks to the otherreferences, more possible translations of each wordare considered correct, as well as various orderingsof words.We tried two approaches: tuning to one true refer-ence and one pseudo-reference, and tuning to multi-ple human-translated references.For the first method, which resembles computer-generated references via paraphrasing as used in(Dyer et al, 2011), we created the pseudo-referenceby translating the development set using TectoMT,a deep syntactic MT with rich linguistic processingimplemented in the Treex platform3.
We hoped thatthe very different output of this decoder would bebeneficial for tuning, however we achieved no im-provement at all.For the second experiment we used 3 translationsof WMT11 test set.
One is the true reference dis-tributed for the shared task and two were translatedmanually from the German version of the data intoCzech.
We achieved a small improvement in finalBLEU score when training on a small data set.
Onthe complete constrained training data for WMT12,there was no improvement?in fact, the BLEU scoreas evaluated on the WMT12 test set was negligiblylower.
Table 3 summarizes our results.
The ?
signdenotes the confidence bounds estimated via boot-strap resampling (Koehn, 2004).3http://ufal.ms.mff.cuni.cz/treex/Used Selected Sel.
Sents AvgModels per Trans.
Total BLEU?stdNone ?
0 12.39?0.06LM ?
16k ?
rand.
sel.
12.18?0.06LM 3 16k 12.73?0.04LM 100 502k 14.21?0.11LM 1000 3.8M 15.12?0.08LM All Sents 18.3M 15.55?0.11Table 4: Results of experiments with Lucene, languagemodel adapted.5 Experiments with Domain AdaptationDomain adaptation is widely recognized as a tech-nique which can significantly improve translationquality (Wu et al, 2008; Bertoldi and Federico,2009; Daume?
and Jagarlamudi, 2011).
In our ex-periments we tried to select sentences close to thesource side of the test set and use them to improvethe final translation.The parallel data used in this section are onlysmall: the news section of CzEng 1.0 (197k sentencepairs, 4.2M Czech words, 4.8M English words).
Wetuned the models on WMT09 test set and evaluatedon WMT11 test set.
The techniques examined hererely on a large monolingual corpus to select datafrom.
We used all the monolingual data provided bythe organizers of WMT11 (18.3M sentences, 316Mwords).5.1 Tailoring the Language ModelOur first attempt was to tailor the language modelto the test set.
Our approach is similar to Zhao etal.
(2004).
In Moore and Lewis (2010), the authorscompare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and ap-ply them to MT.Naturally, we only used the source side of the testset.
First we translated the test set using a baselinetranslation system.
Lucene indexer was then usedto select sentences similar to the translated ones inthe large target-side monolingual corpus.
Finally, anew language model was created from the selectedsentences.The weight of the new LM has to reflect the im-portance of the language model during both MERTtuning as well as final application on (a different)test set.
If the new LM were based only on the final376test set, MERT would underestimate its value andvice versa.
Therefore, we actually translated bothour development (WMT09) as well as final test set(WMT11) using the baseline model and created aLM relevant to their union.The results of performed experiments with do-main adaptation are in Table 4.
To compensate forlow stability of MERT, we ran the optimization fivetimes and report the average BLEU achieved.
The?
value indicates the standard deviation of the fiveruns.The first row provides the scores for the baselineexperiment with no tailored language model.
Wehave run the experiment for three values of selectedsentences per one sentence of the test corpus: 3,100 and 1000 closest-matching sentences were ex-tracted.
With more and more data in the LM, thescores increase.
The second line in Table 4 confirmsthe usefulness of the sentence selection.
Picking thesame amount of 16k sentences randomly performsworse.
As the last row indicates, taking all availabledata leads to the best score.Note that when selecting the sentences, we usedlemmas instead of word forms to reduce data sparse-ness.
So Lucene was actually indexing the lemma-tized version of the monolingual data and the base-line translation translated English lemmas to Czechlemmas when creating the ?query corpus?.
The finalmodels were created from the original sentences, nottheir lemmatized versions.5.2 Tailoring the Translation ModelReverse self-training is a trick that allows to improvethe translation model using (target-side) monolin-gual data and can lead to a performance improve-ment (Bojar and Tamchyna, 2011; Lambert et al,2011).In our scenario, we translated the selected sen-tences (in the opposite direction, i.e.
from the targetinto the source language).
Then we created a newtranslation model (in the original direction) based onthe alignment of selected sentences and their reversetranslation.
This new model is finally combined withthe baseline model and weighted by MERT.
Thewhole scenario is shown in Figure 1.The results of our experiments are in Table 5.
Weran the experiment with translation model adaptationfor 100 most similar sentences selected by Lucene.Each experiment was again performed five times.Due to the low stability of tuning, we also tried in-creasing the size of n-best lists used by MERT.Experiments with tailored translation model aresignificantly better than the baseline but the im-provement against the experiment with only the lan-guage model adapted (with the corresponding 100sentences selected) is very small.5.3 Discussion of Domain AdaptationExperimentsAccording to the results, using Lucene improvestranslation performance already in the case whenonly three sentences are selected for each translatedsentence.
Our results are further supported by thecontrastive setup that used a language model cre-ated from a random selection of the same number ofsentences?the translation quality even slightly de-graded.On the other hand, adding more sentences to lan-guage model further improves results and the bestresult is achieved when the language model is cre-ated using the whole monolingual corpus.
Thiscould have two reasons:Too good domain match.
The domain of thewhole monolingual corpus is too close to the testcorpus.
Adding the whole monolingual corpus isthus the best option.
For more diverse monolingualdata, some domain-aware subsampling like our ap-proach is likely to actually help.Our style of retrieval.
Our queries to Lucenerepresent sentences as simple bags of words.
Luceneprefers less frequent words and the structure of thesentence is therefore often ignored.
For example itprefers to retrieve sentences with the same propername rather than sentences with similar phrases orlonger expressions.
This may not be the best optionfor language modelling.Our method can thus be useful mainly in the casewhen the data available are too large to be processedas a whole.
It can also highly reduce the compu-tation power and time necessary to achieve goodtranslation quality: the result achieved using the lan-guage model created via Lucene for 1000 selectedsentences is not significantly worse than the resultachieved using the whole monolingual corpus butthe required data are 5 times smaller.377Test Set [EN]Translated TS [CS]SentencesSimilartoTranslated TS [CS]ReverseTranslated SentencesSimilartoTranslated TS [EN]LuceneBaselineTranslation[EN->CS]Domain AdaptedLMReverseTranslation TMReverseTranslation [CS->EN]Original LMOriginal TMTest Set [EN]Translated TestSet [CS]Final Translation[EN->CS]Figure 1: Scenario of reverse self-training.Used N-Best Sel.
Sents Sel.
Sents AvgModels per Trans.
Sent.
Total BLEU?stdNone 100 ?
0 12.39?0.06None 200 ?
0 12.4?0.03LM + TM 100 100 502k 14.32?0.13LM + TM 200 100 502k 14.36?0.07Table 5: Results of experiments with Lucene, translation model applied.5.4 Tuning Towards Selected DataDomain adaptation can also be done by selecting asuitable development corpus (Zheng et al, 2010; Liet al, 2004).
The final model parameters depend onthe domain of the development corpus.
By choos-ing a development corpus that is close to our testset we might tune in the right direction.
We imple-mented this adaptation by querying the source sideof our large parallel corpus using the source side ofthe test corpus.
After that, the development corpusis constructed from the selected sentences and theircorresponding reference translations.This experiment uses a fixed model based on thenews section of CzEng 1.0.
We only use differenttuning sets and run the MERT optimization.
All theresulting systems are tested on the WMT11 test set:Baseline system is tuned on 2489 sentence pairsselected randomly from whole CzEng 1.0 parallelcorpus.
Lucene system uses 2489 sentence pairs se-lected from CzEng 1.0 using Lucene.
The selectionis done by choosing the most similar sentences to thesource side of the final test set.
WMT10 system isSystem avg BLEU?stdBaseline 11.41?0.25Lucene 12.31?0.01WMT10 12.37?0.02Perfect selection 12.64?0.02Bad selection 6.37?0.64Table 6: Results of tuning with different corporatuned on 2489 sentence pairs of WMT10 test set.
Toidentify an upper bound, we also include a Perfectselection system which is tuned on the final WMT11test set.
Naturally, this is not a fair competitor.In order to make the results more reliable, it isnecessary to repeat the experiment several times(Clark et al, 2011).
Lucene and the WMT10 systemwere tuned 3 times while baseline system was tuned9 times because of randomness in selection of tun-ing corpora (3 different tuning corpora each tuned 3times).
The results are shown in Table 6.Even though the variance of the baseline systemis high (because we randomly selected corpora 3378times), the difference in scores between baselineand Lucene system is high enough to conclude thattuning on Lucene-selected corpus helps translationquality.
Still it does not give better BLEU scorethan system tuned on WMT10 corpus.
One possi-ble reason is that the whole CzEng 1.0 is of some-what lower quality than the news section.
Given thatour final test set (WMT11) is also from the newsdomain, tuning towards WMT10 corpus probablyleads to a better domain adaptation that tuning to-wards all the domains in CzEng.The tuning set must not overlap with the trainingset.
To illustrate the problem, we did a small exper-iment with the same settings as above and randomlyselected 2489 sentences from training corpora.
Weagain ran the random selection 3 times and tuned 3times with each of the extracted tuning sets, see the?Bad selection?
in Table 6.In all the experiments with badly selected sen-tences, the distortion and language model get anextremely low weight compared to the weights oftranslation model.
This is because they are not use-ful in translation of tuning data which was alreadyseen during training.
Instead of reordering two shortphrases A and B, system already knows the transla-tion of the phrase A B so no distortion is needed.
Onunseen sentences, such weights lead to poor results.This amplifies a drawback of our approach:source texts have to be known prior to system tuningor even before phrase extraction.There are methods available that could tackle thisproblem.
Wuebker et al (2010) store phrase paircounts per sentence when extracting phrases andthus they can reestimate the probabilities when asentence has to be excluded from the phrase tables.For large parallel corpora, suffix arrays (Callison-Burch et al, 2005) have been used.
Suffix arraysallow for a quick retrieval of relevant sentence pairs,the phrase extraction is postponed and performed onthe fly for each input sentence.
It is trivial to fil-ter out sentences belonging to the tuning set duringthis delayed extraction.
With dynamic suffix arrays(Levenberg et al, 2010), one could even simply re-move the tuning sentences from the suffix array.6 Submitted SystemsThis paper covers the submissions CU-TAMCH-BOJ.We translated from English into Czech.
Our setupwas very similar to CU-BOJAR (Bojar et al, 2012a),but our primary submission is tuned on multiple ref-erence translations as described in Section 4.Apart from the additional references, this is a con-strained setup.
CzEng 1.0 were the only parallel dataused in training.
We used a factored model to trans-late the combination of English surface form andpart-of-speech tag into Czech form+POS.
We usedseparate 6-gram language models trained on CzEng1.0 (interpolated by domain) and all News Crawlcorpora (18.3M setences, interpolated by years).Additionaly, we created an 8-gram language modelon target POS tags.
For reordering, we employed alexicalized model trained on CzEng 1.0.Table 7 summarizes the official result of the pri-mary submission and a contrastive baseline (tuned tojust one reference translation).
There is a slight de-crease in BLEU, but the translation error rate (TER)is slightly better when more references were used.The differences are however very small, suggestingthat tuning to more references did not have any sig-nificant effect.System BLEU TERmultiple references 14.5 0.765contrastive baseline 14.6 0.774Table 7: Scores of the submitted systems.7 ConclusionWe showed that CzEng 1.0 is of better overall qual-ity than its predecessor.
We described a techniquefor reducing phrase-table OOV rate, but achieved noimprovement for WMT12.
Similarly, tuning to mul-tiple references did not prove very beneficial.We introduced a couple of techniques that exploitfull-text search in large corpora.
We showed thatadding selected sentences as an additional LM im-proves translations.
Adding a new phrase table ac-quired via reverse self-training resulted only in smallgains.
Tuning to selected sentences resulted in abetter system than tuning to a random set.
How-ever the Lucene-selected corpus fails to outperformgood-quality in-domain tuning data.379ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.Domain adaptation via pseudo in-domain data selec-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 355?362, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, StatMT?09, pages 182?189, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Ondr?ej Bojar and Kamil Kos.
2010.
2010 Failures inEnglish-Czech Phrase-Based MT.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 60?66, Uppsala, Swe-den, July.
Association for Computational Linguistics.Ondr?ej Bojar and Ales?
Tamchyna.
2011.
Forms Wanted:Training SMT on Monolingual Data.
Abstract atMachine Translation and Morphologically-Rich Lan-guages.
Research Workshop of the Israel ScienceFoundation University of Haifa, Israel, January.Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a.Probes in a Taxonomy of Factored Phrase-Based Mod-els.
In Proceedings of the Seventh Workshop on Sta-tistical Machine Translation, Montreal, Canada, June.Association for Computational Linguistics.
Submit-ted.Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???Mars??
?k, Michal Nova?k, Martin Popel, and Ales?
Tam-chyna.
2012b.
The Joy of Parallelism with CzEng1.0.
In Proceedings of LREC2012, Istanbul, Turkey,May.
ELRA, European Language Resources Associa-tion.
In print.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proceedings of the 43rd Annual Meetingof the ACL, pages 255?262.Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.2011.
Better Hypothesis Testing for Statistical Ma-chine Translation: Controlling for Optimizer Instabil-ity.
In Proceedings of the Association for Computa-tional Lingustics.
Association for Computational Lin-guistics.Hal Daume?, III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by mining un-seen words.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies: short papers - Vol-ume 2, HLT ?11, pages 407?412, Stroudsburg, PA,USA.
Association for Computational Linguistics.Chris Dyer, Kevin Gimpel, Jonathan H. Clark, andNoah A. Smith.
2011.
The CMU-ARK German-English Translation System.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 337?343, Edinburgh, Scotland, July.
Associa-tion for Computational Linguistics.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofEMNLP 2004, Barcelona, Spain.Patrik Lambert, Holger Schwenk, Christophe Servan, andSadaf Abdul-Rauf.
2011.
Investigations on trans-lation model adaptation using monolingual data.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 284?293, Edinburgh, Scot-land, July.
Association for Computational Linguistics.Abby Levenberg, Chris Callison-Burch, and Miles Os-borne.
2010.
Stream-based translation models forstatistical machine translation.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the ACL, pages 394?402.Mu Li, Yinggong Zhao, Dongdong Zhang, and MingZhou.
2004.
Adaptive development data selection forlog-linear model in statistical machine translation.
InIn Proceedings of COLING 2004.Coskun Mermer and Murat Saraclar.
2011.
Un-supervised Turkish Morphological Segmentation forStatistical Machine Translation.
Abstract at Ma-chine Translation and Morphologically-Rich Lan-guages.
Research Workshop of the Israel ScienceFoundation University of Haifa, Israel, January.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,USA.
Association for Computational Linguistics.Daniel Stein, David Vilar, Stephan Peitz, Markus Freitag,Matthias Huck, and Hermann Ney.
2011.
A Guide toJane, an Open Source Hierarchical Translation Toolkit.Prague Bulletin of Mathematical Linguistics, 95:5?18,March.John Tinsley, Mary Hearne, and Andy Way.
2009.
Ex-ploiting parallel treebanks to improve phrase-basedstatistical machine translation.
In Alexander F. Gel-bukh, editor, CICLing, volume 5449 of Lecture Notesin Computer Science, pages 318?331.
Springer.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
InProceedings of the 22nd International Conference onComputational Linguistics - Volume 1, COLING ?08,pages 993?1000, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.380Joern Wuebker, Arne Mauser, and Hermann Ney.
2010.Training phrase translation models with leaving-one-out.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages475?484.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Proceed-ings of the 20th international conference on Compu-tational Linguistics, COLING ?04, Stroudsburg, PA,USA.
Association for Computational Linguistics.Zhongguang Zheng, Zhongjun He, Yao Meng, and HaoYu.
2010.
Domain adaptation for statistical machinetranslation in development corpus selection.
In Uni-versal Communication Symposium (IUCS), 2010 4thInternational, pages 2 ?7, oct.381
