Proceedings of the Third Workshop on Statistical Machine Translation, pages 224?232,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsOptimizing Chinese Word Segmentation for Machine TranslationPerformancePi-Chuan Chang, Michel Galley, and Christopher D. ManningComputer Science Department, Stanford UniversityStanford, CA 94305pichuan,galley,manning@cs.stanford.eduAbstractPrevious work has shown that Chinese word seg-mentation is useful for machine translation to En-glish, yet the way different segmentation strategiesaffect MT is still poorly understood.
In this pa-per, we demonstrate that optimizing segmentationfor an existing segmentation standard does not al-ways yield better MT performance.
We find thatother factors such as segmentation consistency andgranularity of Chinese ?words?
can be more impor-tant for machine translation.
Based on these find-ings, we implement methods inside a conditionalrandom field segmenter that directly optimize seg-mentation granularity with respect to the MT task,providing an improvement of 0.73 BLEU.
We alsoshow that improving segmentation consistency us-ing external lexicon and proper noun features yieldsa 0.32 BLEU increase.1 IntroductionWord segmentation is considered an important firststep for Chinese natural language processing tasks,because Chinese words can be composed of multi-ple characters but with no space appearing betweenwords.
Almost all tasks could be expected to ben-efit by treating the character sequence ?Us?
to-gether, with the meaning smallpox, rather than deal-ing with the individual characters ?U?
(sky) and?s?
(flower).
Without a standardized notion of aword, traditionally, the task of Chinese word seg-mentation starts from designing a segmentation stan-dard based on linguistic and task intuitions, and thenaiming to building segmenters that output words thatconform to the standard.
One widely used standardis the Penn Chinese Treebank (CTB) SegmentationStandard (Xue et al, 2005).It has been recognized that different NLP ap-plications have different needs for segmentation.Chinese information retrieval (IR) systems benefitfrom a segmentation that breaks compound wordsinto shorter ?words?
(Peng et al, 2002), parallel-ing the IR gains from compound splitting in lan-guages like German (Hollink et al, 2004), whereasautomatic speech recognition (ASR) systems preferhaving longer words in the speech lexicon (Gao etal., 2005).
However, despite a decade of very in-tense work on Chinese to English machine transla-tion (MT), the way in which Chinese word segmen-tation affects MT performance is very poorly under-stood.
With current statistical phrase-based MT sys-tems, one might hypothesize that segmenting intosmall chunks, including perhaps even working withindividual characters would be optimal.
This is be-cause the role of a phrase table is to build domainand application appropriate larger chunks that aresemantically coherent in the translation process.
Forexample, even if the word for smallpox is treated astwo one-character words, they can still appear in aphrase like ?U s?smallpox?, so that smallpoxwill still be a candidate translation when the systemtranslates ?U?
?s?.
Nevertheless, Xu et al (2004)show that an MT system with a word segmenter out-performs a system working with individual charac-ters in an alignment template approach.
On differ-ent language pairs, (Koehn and Knight, 2003) and(Habash and Sadat, 2006) showed that data-drivenmethods for splitting and preprocessing can improveArabic-English and German-English MT.Beyond this, there has been no finer-grained anal-ysis of what style and size of word segmentation isoptimal for MT.
Moreover, most discussion of seg-mentation for other tasks relates to the size units toidentify in the segmentation standard: whether tojoin or split noun compounds, for instance.
People224generally assume that improvements in a system?sword segmentation accuracy will be monotonicallyreflected in overall system performance.
This is theassumption that justifies the concerted recent workon the independent task of Chinese word segmenta-tion evaluation at SIGHAN and other venues.
How-ever, we show that this assumption is false: aspectsof segmenters other than error rate are more criti-cal to their performance when embedded in an MTsystem.
Unless these issues are attended to, sim-ple baseline segmenters can be more effective insidean MT system than more complex machine learningbased models, with much lower word segmentationerror rate.In this paper, we show that even having a ba-sic word segmenter helps MT performance, and weanalyze why building an MT system over individ-ual characters doesn?t function as well.
Based onan analysis of baseline MT results, we pin downfour issues of word segmentation that can be im-proved to get better MT performance.
(i) While afeature-based segmenter, like a support vector ma-chine or conditional random field (CRF) model, mayhave very good aggregate performance, inconsistentcontext-specific segmentation decisions can be quiteharmful to MT system performance.
(ii)A perceivedstrength of feature-based systems is that they cangenerate out-of-vocabulary (OOV) words, but thesecan hurt MT performance, when they could havebeen split into subparts from which the meaning ofthe whole can be roughly compositionally derived.
(iii) Conversely, splitting OOV words into non-compositional subparts can be very harmful to anMT system: it is better to produce such OOV itemsthan to split them into unrelated character sequencesthat are known to the system.
One big source of suchOOV words is named entities.
(iv) Since the opti-mal granularity of words for phrase-based MT is un-known, we can benefit from a model which providesa knob for adjusting average word size.We build several different models to address theseissues and to improve segmentation for the benefit ofMT.
First, we emphasize lexicon-based features ina feature-based sequence classifier to deal with seg-mentation inconsistency and over-generating OOVwords.
Having lexicon-based features reduced theMT training lexicon by 29.5%, reduced the MT testdata OOV rate by 34.1%, and led to a 0.38 BLEUpoint gain on the test data (MT05).
Second, we ex-tend the CRF label set of our CRF segmenter to iden-tify proper nouns.
This gives 3.3% relative improve-ment on the OOV recall rate, and a 0.32 improve-ment in BLEU.
Finally, we tune the CRF model togenerate shorter or longer words to directly optimizethe performance of MT.
For MT, we found that itis preferred to have words slightly shorter than theCTB standard.The paper is organized as follows: we describethe experimental settings for the segmentation taskand the task in Section 2.
In Section 3.1 we demon-strate that it is helpful to have word segmenters forMT, but that segmentation performance does not di-rectly correlate with MT performance.
We analyzewhat characteristics of word segmenters most affectMT performance in Section 3.2.
In Section 4 and5 we describe how we tune a CRF model to fit the?word?
granularity and also how we incorporate ex-ternal lexicon and information about named entitiesfor better MT performance.2 Experimental Setting2.1 Chinese Word SegmentationFor directly evaluating segmentation performance,we train each segmenter with the SIGHAN Bake-off 2006 training data (the UPUC data set) and thenevaluate on the test data.
The training data contains509K words, and the test data has 155K words.
Thepercentage of words in the test data that are unseenin the training data is 8.8%.
Detail of the Bakeoffdata sets is in (Levow, 2006).
To understand howeach segmenter learns about OOV words, we willreport the F measure, the in-vocabulary (IV) recallrate as well as OOV recall rate of each segmenter.2.2 Phrase-based Chinese-to-English MTThe MT system used in this paper is Moses, a state-of-the-art phrase-based system (Koehn et al, 2003).We build phrase translations by first acquiring bidi-rectional GIZA++ (Och and Ney, 2003) alignments,and using Moses?
grow-diag alignment symmetriza-tion heuristic.1 We set the maximum phrase lengthto a large value (10), because some segmentersdescribed later in this paper will result in shorter1In our experiments, this heuristic consistently performedbetter than the default, grow-diag-final.225words, therefore it is more comparable if we in-crease the maximum phrase length.
During decod-ing, we incorporate the standard eight feature func-tions of Moses as well as the lexicalized reorderingmodel.
We tuned the parameters of these featureswith Minimum Error Rate Training (MERT) (Och,2003) on the NIST MT03 Evaluation data set (919sentences), and then test the MT performance onNIST MT03 and MT05 Evaluation data (878 and1082 sentences, respectively).
We report the MTperformance using the original BLEU metric (Pap-ineni et al, 2001).
All BLEU scores in this paper areuncased.The MT training data was subsampled fromGALE Year 2 training data using a collectionof character 5-grams and smaller n-grams drawnfrom all segmentations of the test data.
Sincethe MT training data is subsampled with charac-ter n-grams, it is not biased towards any particularword segmentation.
The MT training data contains1,140,693 sentence pairs; on the Chinese side thereare 60,573,223 non-whitespace characters, and theEnglish sentences have 40,629,997 words.Our main source for training our five-gram lan-guage model was the English Gigaword corpus, andwe also included close to one million English sen-tences taken from LDC parallel texts: GALE Year 1training data (excluding FOUO data), Sinorama,AsiaNet, and Hong Kong news.
We restricted theGigaword corpus to a subsample of 25 million sen-tences, because of memory constraints.3 Understanding Chinese WordSegmentation for Phrase-based MTIn this section, we experiment with three typesof segmenters ?
character-based, lexicon-based andfeature-based ?
to explore what kind of characteris-tics are useful for segmentation for MT.3.1 Character-based, Lexicon-based andFeature-based SegmentersThe training data for the segmenter is two orders ofmagnitude smaller than for the MT system, it is notterribly well matched to it in terms of genre andvariety, and the information an MT system learnsabout alignment of Chinese to English might be thebasis for a task appropriate segmentation style forChinese-English MT.
A phrase-based MT systemSegmentation PerformanceSegmenter F measure OOV Recall IV RecallCharBased 0.334 0.012 0.485MaxMatch 0.828 0.012 0.951MT PerformanceSegmenter MT03 (dev) MT05 (test)CharBased 30.81 29.36MaxMatch 31.95 30.73Table 1: CharBased vs. MaxMatchlike Moses can extract ?phrases?
(sequences of to-kens) from a word alignment and the system canconstruct the words that are useful.
These observa-tions suggest the first hypothesis.Hypothesis 1.
A phrase table should capture wordsegmentation.
Character-based segmentation forMT should not underperform a lexicon-based seg-mentation, and might outperform it.Observation In the experiments we conducted,we found that the phrase table cannot capture every-thing a Chinese word segmenter can do, and there-fore having word segmentation helps phrase-basedMT systems.
2To show that having word segmentation helpsMT, we compare a lexicon-based maximum-matching segmenter with character-based segmen-tation (treating each Chinese character as a word).The lexicon-based segmenter finds words by greed-ily matching the longest words in the lexicon in aleft-to-right fashion.
We will later refer to this seg-menter as MaxMatch.
The MaxMatch segmenter is asimple and common baseline for the Chinese wordsegmentation task.The segmentation performance of MaxMatch isnot very satisfying because it cannot generalize tocapture words it has never seen before.
How-ever, having a basic segmenter like MaxMatch stillgives the phrase-based MT system a win over thecharacter-based segmentation (treating each Chinesecharacter as a word).
We will refer to the character-based segmentation as CharBased.In Table 1, we can see that on the Chinese wordsegmentation task, having MaxMatch is obviouslybetter than not trying to identify Chinese words atall (CharBased).
As for MT performance, in Ta-ble 1 we see that having a segmenter, even as sim-2Different phrase extraction heuristics might affect the re-sults.
In our experiments, grow-diag outperforms both one-to-many and many-to-one for both MaxMatch and CharBased.
Wereport the results only on grow-diag.226ple as MaxMatch, can help phrase-based MT systemby about 1.37 BLEU points on all 1082 sentencesof the test data (MT05).
Also, we tested the per-formance on 828 sentences of MT05 where all el-ements are in vocabulary3 for both MaxMatch andCharBased.
MaxMatch achieved 32.09 BLEU andCharBased achieved 30.28 BLEU, which shows thaton the sentences where all elements are in vocabu-lary, there MaxMatch is still significantly better thanCharBased.
Therefore, Hypothesis 1 is refuted.Analysis We hypothesized in Hypothesis 1 thatthe phrase table in a phrase-basedMT system shouldbe able to capture the meaning by building ?phrases?on top of character sequences.
Based on the experi-mental result in Table 1, we see that using character-based segmentation (CharBased) actually performsreasonably well, which indicates that the phrase ta-ble does capture the meaning of character sequencesto a certain extent.
However, the results also showthat there is still some benefit in having word seg-mentation for MT.
We analyzed the decoded out-put of both systems (CharBased and MaxMatch) onthe development set (MT03).
We found that the ad-vantage of MaxMatch over CharBased is two-fold,(i) lexical: it enhances the ability to disambiguatethe case when a character has very different meaningin different contexts, and (ii) reordering: it is easierto move one unit around than having to move twoconsecutive units at the same time.
Having words asthe basic units helps the reordering model.For the first advantage, one example is the char-acter ??
?, which can both mean ?intelligence?, oran abbreviation for Chile (?|).
The comparisonbetween CharBased and MaxMatch is listed in Ta-ble 2.
The word?
?w (dementia) is unknown forboth segmenters.
However, MaxMatch gave a bettertranslation of the character?.
The issue here is notthat the ?????intelligence?
entry never appears inthe phrase table of CharBased.
The real issue is,when ?
means Chile, it is usually followed by thecharacter |.
So by grouping them together, Max-Match avoided falsely increasing the probability oftranslating the stand-alone ?
into Chile.
Based onour analysis, this ambiguity occurs the most whenthe character-based system is dealing with a rare orunseen character sequence in the training data, andalso occurs more often when dealing with translit-3Except for dates and numbers.Reference translation:scientists complete sequencing of the chromosome linked toearly dementiaCharBased segmented input:?
?
[ ?
M ' ?
?
?
?
w ff / ?
N  ?
?
SMaxMatch segmented input:??
[ ?
M' ??
?
?
w ff /?
N ?
?
STranslation with CharBased segmentation:scientists at the beginning of the stake of chile lost the genomesequence completedTranslation with MaxMatch segmentation:scientists at stake for the early loss of intellectual syndromechromosome completed sequencingTable 2: An example showing that character-based segmenta-tion provides weaker ability to distinguish character with mul-tiple unrelated meanings.erations.
The reason is that characters composinga transliterated foreign named entity usually doesn?tpreserve their meanings; they are just used to com-pose a Chinese word that sounds similar to the orig-inal word ?
much more like using a character seg-mentation of English words.
Another example ofthis kind is ?C_?%?w?
(Alzheimer?s dis-ease).
The MT system using CharBased segmenta-tion tends to translate some characters individuallyand drop others; while the system using MaxMatchsegmentation is more likely to translate it right.The second advantage of having a segmenter likethe lexicon-based MaxMatch is that it helps the re-ordering model.
Results in Table 1 are with thelinear distortion limit defaulted to 6.
Since wordsin CharBased are inherently shorter than MaxMatch,having the same distortion limit means CharBasedis limited to a smaller context than MaxMatch.
Tomake a fairer comparison, we set the linear distor-tion limit in Moses to unlimited, removed the lexi-calized reordering model, and retested both systems.With this setting, MaxMatch is 0.46 BLEU point bet-ter than CharBased (29.62 to 29.16) on MT03.
Thisresult suggests that having word segmentation doesaffect how the reordering model works in a phrase-based system.Hypothesis 2.
Better Segmentation PerformanceShould Lead to Better MT PerformanceObservation We have shown in Hypothesis 1 thatit is helpful to segment Chinese texts into wordsfirst.
In order to decide a segmenter to use, themost intuitive thing to do is to find one that giveshigher F measure on segmentation.
Our experimentsshow that higher F measure does not necessarily227lead to higher BLEU score.
In order to contrastwith the simple maximum matching lexicon-basedmodel (MaxMatch), we built another segmenter witha CRF model.
CRF is a statistical sequence model-ing framework introduced by Lafferty et al (2001),and was first used for the Chinese word segmenta-tion task by Peng et al (2004), who treated wordsegmentation as a binary decision task.
We opti-mized the parameters with a quasi-Newton method,and used Gaussian priors to prevent overfitting.The probability assigned to a label sequence for aparticular sequence of characters by a CRF is givenby the equation:p?
(y|x) =1Z(x)expT?t=1K?k=1?k fk(x,yt?1,yt , t) (1)x is a sequence of T unsegmented characters, Z(x) isthe partition function that ensures that Equation 1 isa probability distribution, { fk}Kk=1 is a set of featurefunctions, and y is the sequence of binary predic-tions for the sentence, where the prediction yt = +1indicates the t-th character of the sequence is pre-ceded by a space, and where yt =?1 indicates thereis none.
We trained a CRF model with a set of ba-sic features: character identity features of the currentcharacter, previous character and next character, andthe conjunction of previous and current characters inthe zero-order templates.
We will refer to this seg-menter as CRF-basic.Table 3 shows that the feature-based segmenterCRF-basic outperforms the lexicon-based MaxMatchby 5.9% relative F measure.
Comparing the OOV re-call rate and the IV recall rate, the reason is that CRF-basic wins a lot on the OOV recall rate.
We see thata feature-based segmenter like CRF-basic clearly hasstronger ability to recognize unseen words.
OnMT performance, however, CRF-basic is 0.38 BLEUpoints worse than MaxMatch on the test set.
In Sec-tion 3.2, we will look at how theMT training and testdata are segmented by each segmenter, and providestatistics and analysis for why certain segmenters arebetter than others.3.2 Consistency Analysis of DifferentSegmentersIn Section 3.1 we have refuted two hypotheses.
Nowwe know that: (i) phrase table construction does notfully capture what a word segmenter can do.
Thus itSegmentation PerformanceSegmenter F measure OOV Recall IV RecallCRF-basic 0.877 0.502 0.926MaxMatch 0.828 0.012 0.951CRF-Lex 0.940 0.729 0.970MT PerformanceSegmenter MT03 (dev) MT05 (test)CRF-basic 33.01 30.35MaxMatch 31.95 30.73CRF-Lex 32.70 30.95Table 3: CRF-basic vs MaxMatchSegmenter #MT Training Lexicon Size #MT Test Lexicon SizeCRF-basic 583147 5443MaxMatch 39040 5083CRF-Lex 411406 5164MT Test Lexicon OOV rate Conditional EntropyCRF-basic 7.40% 0.2306MaxMatch 0.49% 0.1788CRF-Lex 4.88% 0.1010Table 4: MT Lexicon Statistics and Conditional Entropy of Seg-mentation Variations of three segmetnersis useful to have word segmentation for MT.
(ii) ahigher F measure segmenter does not necessarilyoutperforms on the MT task.To understand what factors other than segmen-tation F measure can affect MT performance, weintroduce another CRF segmenter CRF-Lex that in-cludes lexicon-based features by using external lex-icons.
More details of CRF-Lex will be describedin Section 5.1.
From Table 3, we see that the seg-mentation F measure is that CRF-Lex > CRF-basic >MaxMatch.
And now we know that the better seg-mentation F measure does not always lead to betterMT BLEU score, because of in terms of MT perfor-mance, CRF-Lex > MaxMatch > CRF-basic.In Table 4, we list some statistics of each seg-menter to explain this phenomenon.
First we lookat the lexicon size of the MT training and test data.While segmenting the MT data, CRF-basic gener-ates an MT training lexicon size of 583K uniqueword tokens, and MaxMatch has a much smaller lex-icon size of 39K.
CRF-Lex performs best on MT,but the MT training lexicon size and test lexiconOOV rate is still pretty high compared to MaxMatch.Only examining the MT training and test lexiconsize still doesn?t fully explain why CRF-Lex outper-forms MaxMatch.
MaxMatch generates a smaller MTlexicon and lower OOV rate, but for MT it wasn?tbetter than CRF-Lex, which has a bigger lexicon andhigher OOV rate.
In order to understand why Max-Match performs worse on MT than CRF-Lex but bet-228ter than CRF-basic, we use conditional entropy ofsegmentation variations to measure consistency.We use the gold segmentation of the SIGHANtest data as a guideline.
For every work type wi,we collect all the different pattern variations vi j inthe segmentation we want to examine.
For exam-ple, for a word ?ABC?
in the gold segmentation, welook at how it is segmented with a segmenter.
Thereare many possibilities.
If we use cx and cy to indi-cate other Chinese characters and to indicate whitespaces, ?cx ABC cy?
is the correct segmentation,because the three characters are properly segmentedfrom both sides, and they are concatenated with eachother.
It can also be segmented as ?cx A BC cy?,which means although the boundary is correct, thefirst character is separated from the other two.
Or,it can be segmented as ?cxA BCcy?, which meansthe first character was actually part of the previousword, while BC are the beginning of the next word.Every time a particular word type wi appears in thetext, we consider a segmenter more consistent if itcan segment wi in the same way every time, but itdoesn?t necessarily have to be the same as the goldstandard segmentation.
For example, if ?ABC?
is aChinese person name which appears 100 times in thegold standard data, and one segmenter segment it ascx A BC cy 100 times, then this segmenter is stillconsidered to be very consistent, even if it doesn?texactly match the gold standard segmentation.
Us-ing this intuition, the conditional entropy of segmen-tation variations H(V |W ) is defined as follows:H(V |W ) = ?
?wiP(wi)?vi jP(vi j|wi) logP(vi j|wi)= ?
?wi?vi jP(vi j,wi) logP(vi j|wi)Now we can look at the overall conditional en-tropy H(V |W ) to compare the consistency of eachsegmenter.
In Table 4, we can see that even thoughMaxMatch has a much smaller MT lexicon size thanCRF-Lex, when we examine the consistency of howMaxMatch segments in context, we find the condi-tional entropy is much higher than CRF-Lex.
We canalso see that CRF-basic has a higher conditional en-tropy than the other two.
The conditional entropyH(V |W ) shows how consistent each segmenter is,and it correlates with the MT performance in Ta-ble 4.
Note that consistency is only one of the com-peting factors of how good a segmentation is forMT performance.
For example, a character-basedsegmentation will always have the best consistencypossible, since every word ABC will just have onepattern: cx A B C cy.
But from Section 3.1 wesee that CharBased performs worse than both Max-Match and CRF-basic on MT, because having wordsegmentation can help the granularity of the Chineselexicon match that of the English lexicon.In conclusion, for MT performance, it is helpfulto have consistent segmentation, while still having aword segmentation matching the granularity of thesegmented Chinese lexicon and the English lexicon.4 Optimal Average Token Length for MTWe have shown earlier that word-level segmentationvastly outperforms character based segmentation inMT evaluations.
Since the word segmentation stan-dard under consideration (Chinese Treebank (Xueet al, 2005)) was neither specifically designed noroptimized for MT, it seems reasonable to investi-gate whether any segmentation granularity in con-tinuum between character-level and CTB-style seg-mentation is more effective for MT.
In this section,we present a technique for directly optimizing a seg-mentation property?characters per token average?for translation quality, which yields significant im-provements in MT performance.In order to calibrate the average word length pro-duced by our CRF segmenter?i.e., to adjust the rateof word boundary predictions (yt = +1), we applya relatively simple technique (Minkov et al, 2006)originally devised for adjusting the precision/recalltradeoff of any sequential classifier.
Specifically, theweight vector w and feature vector of a trained lin-ear sequence classifier are augmented at test timeto include new class-conditional feature functions tobias the classifier towards particular class labels.
Inour case, since we wish to increase the frequency ofword boundaries, we add a feature function:f0(x,yt?1,yt , t) ={1 if yt = +10 otherwiseIts weight ?0 controls the extent of which the classi-fier will make positive predictions, with very largepositive ?0 values causing only positive predic-tions (i.e., character-based segmentation) and largenegative values effectively disabling segmentationboundaries.
Table 5 displays how changes of the229?0 ?1 0 1 2 4 8 32len 1.64 1.62 1.61 1.59 1.55 1.37 1Table 5: Effect of the bias parameter ?0 on the average numberof character per token on MT data.bias parameter ?0 affect segmentation granularity.4Since we are interested in analyzing the differentregimes of MT performance between CTB segmen-tation and character-based, we performed a gridsearch in the range between ?0 = 0 (maximum-likelihood estimate) and ?0 = 32 (a value that islarge enough to produce only positive predictions).For each ?0 value, we ran an entire MT training andtesting cycle, i.e., we re-segmented the entire train-ing data, ran GIZA++, acquired phrasal translationsthat abide to this new segmentation, and ran MERTand evaluations on segmented data using the same?0 values.3030.53131.53232.533-3 -2 -1  0  1  2  3  4  5  6  7  8biasBLEU[%] scoresMT03(dev)MT02MT050.80.820.840.860.880.90.920.940.96-3 -2 -1  0  1  2  3  4  5  6  7  8biasSegmentation performancePrecisionRecallF measureFigure 1: A bias towards more segment boundaries (?0 > 0)yields better MT performance and worse segmentation results.Segmentation and MT results are displayed inFigure 1.
First, we observe that an adjustment ofthe precision and recall tradeoff by setting nega-4Note that character-per-token averages provided in the ta-ble consider each non-Chinese word (e.g., foreign names, num-bers) as one character, since our segmentation post-processingprevents these tokens from being segmented.tive bias values (?0 = ?2) slightly improves seg-mentation performance.
We also notice that rais-ing ?0 yields relatively consistent improvements inMT performance, yet causes segmentation perfor-mance (F measure) to be increasingly worse.
Whilethe latter finding is not particularly surprising, it fur-ther confirms that segmentation and MT evaluationscan yield rather different outcomes.
We chose the?0 = 2 on another dev set (MT02).
On the test setMT05, ?0 = 2 yields 31.47 BLEU, which representsa quite large improvement compared to the unbiasedsegmenter (30.95 BLEU).
Further reducing the av-erage number of characters per token yields gradualdrops of performance until character-level segmen-tation (?0 ?
32, 29.36 BLEU).Here are some examples of how setting ?0 = 2shortens the words in a way that can help MT.?
separating adjectives and pre-modifying adverbs:??
(very big) ??(very)?(big)?
separating nouns and pre-modifying adjectives:p??
(high blood pressure)?p(high)??
(blood pressure)?
separating compound nouns:S?
(Department of Internal Affairs)?S(Internal Affairs)?
(Department).5 Improving Segmentation Consistency ofa Feature-based Sequence Model forSegmentationIn Section 3.1 we showed that a statistical sequencemodel with rich features can generalize better thanmaximum matching segmenters.
However, it alsoinconsistently over-generates a big MT training lexi-con and OOVwords in MT test data, and thus causesa problem for MT.
To improve a feature-based se-quence model for MT, we propose 4 different ap-proaches to deal with named entities, optimal lengthof word for MT and joint search for segmentationand MT decoding.5.1 Making Use of External LexiconsOne way to improve the consistency of the CRFmodel is to make use of external lexicons (whichare not part of the segmentation training data) toadd lexicon-based features.
All the features we useare listed in Table 6.
Our linguistic features areadopted from (Ng and Low, 2004) and (Tseng etal., 2005).
There are three categories of features:230Lexicon-based Features Linguistic Features(1.1) LBegin(Cn),n ?
[?2,1] (2.1) Cn,n ?
[?2,1](1.2) LMid(Cn),n ?
[?2,1] (2.2) Cn?1Cn,n ?
[?1,1](1.3) LEnd(Cn),n ?
[?2,1] (2.3) Cn?2Cn,n ?
[1,2](1.4) LEnd(C?1)+LEnd(C0) (2.4) Single(Cn),n ?
[?2,1]+LEnd(C1) (2.5) UnknownBigram(C?1C0)(1.5) LEnd(C?2)+LEnd(C?1) (2.6) ProductiveA f f ixes(C?1,C0)+LBegin(C0)+LMid(C0) (2.7) Reduplication(C?1,Cn),n ?
[0,1](1.6) LEnd(C?2)+LEnd(C?1)+LBegin(C?1)+LBegin(C0)+LMid(C0)Table 6: Features for CRF-Lexcharacter identity n-grams, morphological and char-acter reduplication features.
Our lexicon-based fea-tures are adopted from (Shi and Wang, 2007), whereLBegin(C0), LMid(C0) and LEnd(C0) represent themaximum length of words found in a lexicon thatcontain the current character as either the first, mid-dle or last character, and we group any length equalor longer than 6 together.
The linguistic featureshelp capturing words that were unseen to the seg-menter; while the lexicon-based features constrainthe segmenter with external knowledge of what se-quences are likely to be words.We built a CRF segmenter with all the featureslisted in Table 6 (CRF-Lex).
The external lexiconswe used for the lexicon-based features come fromvarious sources including named entities collectedfrom Wikipedia and the Chinese section of the UNwebsite, named entities collected by Harbin Instituteof Technology, the ADSO dictionary, EMM NewsExplorer, Online Chinese Tools, Online Dictionaryfrom Peking University and HowNet.
There are423,224 distinct entries in all the external lexicons.The MT lexicon consistency of CRF-Lex in Table4 shows that the MT training lexicon size has beenreduced by 29.5% and the MT test data OOV rate isreduced by 34.1%.5.2 Joint training of Word Segmentation andProper Noun TaggingNamed entities are an important source for OOVwords, and in particular are ones which it is bad tobreak into pieces (particularly for foreign names).Therefore, we use the proper noun (NR) part-of-speech tag information from CTB to extend the labelsets of our CRF model from 2 to 4 ({beginning of aword, continuation of a word} ?
{NR, not NR}).This is similar to the ?all-at-once, character-based?POS tagging in (Ng and Low, 2004), except thatSegmentation PerformanceSegmenter F measure OOV Recall IV RecallCRF-Lex-NR 0.943 0.753 0.970CRF-Lex 0.940 0.729 0.970MT PerformanceSegmenter MT03 (dev) MT05 (test)CRF-Lex-NR 32.96 31.27CRF-Lex 32.70 30.95Table 7: CRF-Lex-NR vs CRF-Lexwe are only tagging proper nouns.
We call the 4-label extension CRF-Lex-NR.
The segmentation andMT performance of CRF-Lex-NR is listed in Table 7.With the 4-label extension, the OOV recall rate im-proved by 3.29%; while the IV recall rate stays thesame.
Similar to (Ng and Low, 2004), we found theoverall F measure only goes up a tiny bit, but we dofind a significant OOV recall rate improvement.On the MT performance, CRF-Lex-NR has a 0.32BLEU gain on the test set MT05.
In addition to theBLEU improvement, CRF-Lex-NR also provides ex-tra information about proper nouns, which can becombined with postprocessing named entity transla-tion modules to further improve MT performance.6 ConclusionIn this paper, we investigated what segmentationproperties can improve machine translation perfor-mance.
First, we found that neither character-basednor a standard word segmentation standard are opti-mal for MT, and show that an intermediate granular-ity is much more effective.
Using an already com-petitive CRF segmentation model, we directly opti-mize segmentation granularity for translation qual-ity, and obtain an improvement of 0.73 BLEU pointon MT05 over our lexicon-based segmentation base-line.
Second, we augment our CRF model withlexicon and proper noun features in order to im-prove segmentation consistency, which provide a0.32 BLEU point improvement.7 AcknowledgementThe authors would like to thank Menqgiu Wang andHuihsin Tseng for useful discussions.
This paper isbased on work funded in part by the Defense Ad-vanced Research Projects Agency through IBM.231ReferencesJianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.2005.
Chinese word segmentation and named entityrecognition: A pragmatic approach.
ComputationalLinguistics.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InProceedings of the Human Language Technology Con-ference of the NAACL, Companion Volume: Short Pa-pers, pages 49?52, New York City, USA, June.
Asso-ciation for Computational Linguistics.Vera Hollink, Jaap Kamps, Christof Monz, and Maartende Rijke.
2004.
Monolingual document retrieval forEuropean languages.
Information Retrieval, 7(1).Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In EACL ?03: Proceed-ings of the tenth conference on European chapter ofthe Association for Computational Linguistics, pages187?193.
Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL-HLT.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning.Gina-Anne Levow.
2006.
The third international Chi-nese language processing bakeoff: Word segmentationand named entity recognition.
In Proc.
of the FifthSIGHAN Workshop on Chinese Language Processing,July.Einat Minkov, Richard Wang, Anthony Tomasic, andWilliam Cohen.
2006.
NER systems that suit user?spreferences: Adjusting the recall-precision trade-offfor entity extraction.
In Proc.
of NAACL-HLT, Com-panion Volume: Short Papers, New York City, USA,June.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
Word-based or character-based?
In Proc.
of EMNLP.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1).Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic eval-uation of machine translation.
In ACL.Fuchun Peng, Xiangji Huang, Dale Schuurmans, andNick Cercone.
2002.
Investigating the relationshipbetween word segmentation performance and retrievalperformance in Chinese IR.
In Proc.
of the 19th Inter-national Conference on Computational Linguistics.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proc.
of COLING.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layerCRFs based joint decoding method for cascaded seg-mentation and labeling tasks.
In IJCAI.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for Sighan bake-off 2005.
In Proc.
of the Fourth SIGHAN Workshop onChinese Language Processing.Jia Xu, Richard Zens, and Hermann Ney.
2004.
Dowe need Chinese word segmentation for statistical ma-chine translation.
In Proc.
of the Third SIGHAN Work-shop on Chinese Language Learning.Nianwen Xue, Fei Xia, Fu dong Chiou, and MarthaPalmer.
2005.
Building a large annotated Chinesecorpus: the Penn Chinese treebank.
Journal of Nat-ural Language Engineering, 11(2).232
