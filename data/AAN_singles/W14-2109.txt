Proceedings of the First Workshop on Argumentation Mining, pages 64?68,Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational LinguisticsA Benchmark Dataset for Automatic Detection of Claims andEvidence in the Context of Controversial TopicsEhud Aharoni* Anatoly Polnarov* Tamar Lavee?
Daniel HershcovichIBM Haifa ResearchLab, Haifa, IsraelHebrew University,IsraelIBM Haifa ResearchLab, Haifa, IsraelIBM Haifa ResearchLab, Haifa, IsraelRan Levy Ruty Rinott Dan Gutfreund Noam Slonim?IBM Haifa ResearchLab, Haifa, IsraelIBM Haifa ResearchLab, Haifa, IsraelIBM Haifa ResearchLab, Haifa, IsraelIBM Haifa ResearchLab, Haifa, IsraelAbstractWe describe a novel and unique argumenta-tive structure dataset.
This corpus consists ofdata extracted from hundreds of Wikipedia ar-ticles using a meticulously monitored manualannotation process.
The result is 2,683 argu-ment elements, collected in the context of 33controversial topics, organized  under a simpleclaim-evidence structure.
The obtained dataare publicly available for academic research.1 IntroductionOne major obstacle in developing automatic ar-gumentation mining techniques is the scarcity ofrelevant high quality annotated data.
Here, wedescribe a novel and unique benchmark data thatrelies on a simple argument model and elabo-rates on the associated annotation process.
Mostimportantly, the argumentative elements weregathered in the context of pre-defined controver-sial topics, which distinguishes our work fromother previous related corpora.
1  Two recent*  These authors contributed equally to this manuscript.?
Present affiliation: Yahoo!?
Corresponding author, at noams@il.ibm.comworks that are currently under review [Rinott etal, Levy et al] have reported first results overdifferent subsets of this data, which is now pub-lically available for academic research upon re-quest.
We believe that this novel corpus shouldbe of practical importance to many researches,and in particular to the emerging community ofargumentation mining.Unlike the classical Toulmin model (Freeleyand Steinberg 2008), we considered a simple androbust argument structure comprising only twocomponents ?
claim and associated supportingevidence.
The argumentative structures werecarefully annotated under a pre-defined topic,introduced as a debate motion.
As the collecteddata covers a diverse set of 33 motions, we ex-pect it could be used to develop generic tools forautomatic detection and construction of argu-mentative structures in the context of new topics.2 Data ModelWe defined and implemented the following con-cepts:Topic ?
a short, usually controversial statementthat defines the subject of interest.
Context De-1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) andVaccine/Injury Project (V/IP) Corpus (Ashley and Walker2013).64pendent Claim (CDC) ?
a general concisestatement that directly supports or contests theTopic.
Context Dependent Evidence (CDE) ?
atext segment that directly supports a CDC in thecontext of a given Topic.
Examples are given inSection 6.Furthermore, since one can support a claim us-ing different types of evidence (Rieke et al 2012,Seech 2008), we defined and considered threeCDE types: Study: Results of a quantitativeanalysis of data given as numbers or as conclu-sions.
Expert: Testimony by a person / group /committee / organization with some known ex-pertise in or authority on the topic.
Anecdotal: adescription of specific event(s)/instance(s) orconcrete example(s).3 Labeling Challenges and ApproachThe main challenge we faced in collecting theannotated data was the inherently elusive natureof concepts such as "claim" and "evidence."
Toaddress that we formulated two sets of criteriafor CDC and CDE, respectively, and relied on ateam of about 20 carefully trained in-house la-belers whose work was closely monitored.
Tofurther enhance the quality of the collected datawe adopted a two-stage labeling approach.
First,a team of five labelers worked independently onthe same text and prepared the initial set of can-didate CDCs or candidate CDEs.
Next, a team offive labelers?not necessarily the same five?independently crosschecked the joint list of thedetected candidates, each of which was eitherconfirmed or rejected.
Candidates confirmed byat least three labelers were included in the corpus.4 Labeling GuidelinesThe labeling guidelines defined the concepts ofTopic, CDC, CDE, and CDE types, along withrelevant examples.
According to these guidelines,given a Topic, a text fragment should be labeledas a CDC if and only if it complies with all ofthe following five CDC criteria: Strength:Strong content that directly supports or conteststhe provided Topic.
Generality: General contentthat deals with a relatively broad idea.
Phrasing:Is well phrased, or requires at most a single andminor "allowed" change.2  Keeping text spirit:Keeps the spirit of the original text from which itwas extracted.
Topic unity: Deals with one, or atmost two related topics.
Four CDE criteria weredefined in a similar way, given a Topic and aCDC, except for the generality criterion.5 Labeling DetailsThe labeling process was carried out in theGATE environment (https://gate.ac.uk/).
The 33Topics were selected at random from the debatemotions at http://idebate.org/ database.
The la-beling process was divided into five stages:Search: Given a Topic, five labelers wereasked to independently search English Wikipe-dia3 for articles with promising content.Claim Detection: At this stage, five labelersindependently detected candidate CDCs support-ing or contesting the Topic within each articlesuggested by the Search team.Claim Confirmation: At this stage, five la-belers independently cross-examined the candi-date CDCs suggested at the Claim Detectionstage, aiming to confirm a candidate and its sen-timent as to the given Topic, or reject it by refer-ring to one of the five CDC Criteria it fails tomeet.
The candidate CDCs confirmed by at leastthree labelers were forwarded to the next stage.Evidence Detection: At this stage, five la-belers independently detected candidate CDEssupporting a confirmed CDC in the context ofthe given Topic.
The search for CDEs was done2 For example, anaphora resolution.
The enclosed data setcontains the corrected version as well, as proposed by thelabelers.3 We considered the Wikipedia dump as of April 3, 2012.65only in the same article where the correspondingCDC was found.Evidence Confirmation: This stage was car-ried out in a way similar to Claim Confirmation.The only difference was that the labelers wererequired to classify accepted CDE under one ormore CDE types.Labelers training and feedback: Before join-ing actual labeling tasks, novice labelers wereassigned with several completed tasks and wereexpected to show a reasonable degree of agree-ment with a consensus solution prepared in ad-vance by the project administrators.
In addition,the results of each Claim Confirmation task wereexamined by one or two of the authors (AP andNS) to ensure the conformity to the guidelines.In case crude mistakes were spotted, the corre-sponding labeler was requested to revise andresubmit.
Due to the large numbers of CDE can-didates, it was impractical to rely on such a rig-orous monitoring process in Evidence Confirma-tion.
Instead, Evidence Consensus Solutionswere created for selected articles by several ex-perienced labelers, who first solved the tasksindependently and then reached consensus in ajoint meeting.
Afterwards, the tasks were as-signed to the rest of the labelers.
Their results onthese tasks were juxtaposed with the ConsensusSolutions, and on the basis of this comparisonindividual feedback reports were drafted andsent to the team members.
Each labeler receivedsuch a report on an approximately weekly basis.6 Data SummaryFor 33 debate motions, a total of 586 Wikipediaarticles were labeled.
The labeling process re-sulted in 1,392 CDCs distributed across 321 ar-ticles.
In 12 debate motions, for which 350 dis-tinct CDCs were confirmed across 104 articles,we further completed the CDE labeling, endingup with a total of 1,291 confirmed CDEs ?
431of type Study, 516 of type Expert, and 529 oftype Anecdotal.
Note that some CDEs were as-sociated with more than one type (for example,118 CDEs were classified both under the typeStudy and Expert).Presented in Tables 1 and 2 are several exam-ples of CDCs and CDEs gathered under theTopics we worked with, as well as some inac-ceptable candidates illustrating some of the sub-tleties of the performed work.TopicThe sale of violent video games to mi-nors should be banned(Pro)CDCViolent video games can increase chil-dren?s aggression(Pro)CDCVideo game publishers unethically trainchildren in the use of weaponsNote that a valid CDC is not necessarily fa c-tual .
(Con)CDCViolent games affect children positivelyInvalidCDC 1Video game addiction is excessive orcompulsive use of computer and videogames that interferes with daily life.This  statement defines  a concept relevant tothe Topic, not a relevant claim.InvalidCDC 2Violent TV shows just mirror the vio-lence that goes on in the real world.This  claim is not relevant enough to Topic.InvalidCDC 3Violent video games should not be soldto children.This candidate simply repeats the Topic andthus  is not considered a va lid CDC.InvalidCDC 4?Doom?
has been blamed for nationallycovered school shooting.This candidate fails the generali ty cri terion,as it focuses on a speci fic single video game.Note that i t could serve as CDE to a moregeneral CDC.Table 1: Examples of CDCs and invalid CDCs.Topic 1The sale of vio lent video games tominors should be banned(Pro) CDCViolent video games increase youthviolenceCDE(Study)The most  recent large scale meta-analysis?examining 130 studies withover 130,000 subjects worldwide?concluded that exposure to violent66video games causes both short termand long term aggression in playersCDE(Anecdotal)In April 2000, a 16-year-old teenagermurdered his father, mother and sisterproclaiming that he was on an "aveng-ing mission" for the main character ofthe video game Final Fantasy VIIIInvalidCDEWhile most experts reject any linkbetween video games content and re-al-life violence, some media scholarsargue that the connection exists.Invalid, because i t includes informationthat contests the CDC.Topic 2The use of performance enhancingdrugs in sports should be permitted(Con) CDCDrug abuse can be harmful to one?shealth and even deadly.CDE(Expert)According to some nurse practition-ers, stopping substance abuse canreduce the risk of dying early and alsoreduce some health risks like heartdisease, lung disease, and strokesInvalidCDESuicide is very common in adolescentalcohol abusers, with 1 in 4 suicidesin adolescents being related to alcoholabuse.Although the candidate CDE does supportthe CDC, the notion of adolescent alcoholabusers  is irrelevant to the Topic.
There-fore, the candidate is invalid.Table 2: Examples of CDE and invalid CDE.7 Agreement and Recall ResultsTo evaluate the labelers?
agreement we used Co-hen?s kappa coefficient (Landis and Koch 1977).The average measure was calculated over alllabelers' pairs, for each pair taking those articleson which the corresponding labelers worked to-gether and omitting labeler pairs which labeledtogether less than 100 CDCs/CDEs.
This strate-gy was chosen since no two labelers worked onthe exact same tasks, so standard multi-rateragreement measures could not be applied.
Theobtained average kappa was 0.39 and 0.4 in theClaim confirmation and Evidence confirmationstages, respectively, which we consider satisfac-tory given the subtlety of the concepts involvedand the fact that the tasks naturally required acertain extent of subjective decision making.We further employed a simple method to ob-tain a rough estimate of the recall at the detectionstages.
For CDCs (and similarly for CDEs), let nbe the number of CDCs detected and confirmedin a given article, and x be the unknown totalnumber of CDCs in this article.
Assuming the i-th labeler detects a ratio  of x, and taking astrong assumption of independence between thelabelers, we get:.We estimated  from the observed data, andcomputed x for each article.
We were then ableto compute the estimated recall per motion, end-ing up with the estimated average recall of90.6% and 90.0% for CDCs and CDEs, respec-tively.8 Future Work and ConclusionThere are several natural ways to proceed further.First, a considerable increase in the quantity ofgathered CDE data can be achieved by expand-ing the search scope beyond the article in whichthe CDC is found.
Second, the argument modelcan be enhanced ?
for example, to include coun-ter-CDE (i.e., evidence that contest the CDC).Third, one may look into ways to add more la-beling layers on the top of the existing model(for example, distinguishing between factualCDCs, value CDCs, and so forth).
Fourth, newtopics and new sources besides Wikipedia can beconsidered.The data is released and available upon requestfor academic research.
We hope that it will proveuseful for different data mining communities,and particularly for various purposes in the fieldof Argumentation Mining.67ReferencesAustin J. Freeley and David L. Steinberg.
2008.Argumentation and Debate.
Wadsworth,Belmont, California.Chris Reed.
2005.
?Preliminary Results from anArgument Corpus?
in Proceedings of the IXSymposium on Social Communication, San-tiago de Cuba, pp.
576-580.J.
Richard Landis and Gary G. Koch.
1977.
?Themeasurement of observer agreement forcategorical data.?
Biometrics 33:159-174.Kevid D. Ashley and Vern R. Walker.
2013.?Toward Constructing Evidence-Based Le-gal Arguments Using Legal Decision Doc-uments and Machine Learning?
in Proceed-ings of the Fourteenth International Con-ference on Artificial Intelligence and Law(ICAIL ?13), Rome, Italy, pp.
176-180.Marie-Francine Moens, Erik Boiy, Raquel Mo-chales Palau, and Chris Reed.
2007.
?Au-tomatic Detection of Arguments in LegalTexts?
in Proceedings of the InternationalConference on AI & Law (ICAIL-2007),Stanford, CA, pp.
225-230.Richard D. Rieke, Malcolm O. Sillars and TarlaRai Peterson.
2012.
Argumentation andCritical Decision Making (8e).
PrenticeHall, USA.Ran Levy, Yonatan Bilu, Daniel Hershcovich,Ehud Aharoni and Noam Slonim.
?ContextDependent Claim Detection.?
SubmittedRuty Rinott, Lena Dankin, Carlos Alzate, EhudAharoni and Noam Slonim.
"Show MeYour Evidence ?
an Automatic Method forContext Dependent Evidence Detection.
?Submitted.Zachary Seech.
2008.
Writing Philosophy Pa-pers (5th edition).
Wadsworth, CengageLearning, Belmont, California.68
