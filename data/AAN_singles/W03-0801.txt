The Talent System: TEXTRACT Architecture and Data ModelMary S. NeffIBM Thomas J. WatsonResearch CenterP.O.
Box 704Yorktown Heights, NY10598MaryNeff@us.ibm.comRoy J. ByrdIBM Thomas J. WatsonResearch CenterP.O.
Box 704Yorktown Heights, NY 10598byrd@watson.ibm.comBranimir K. BoguraevIBM Thomas J. WatsonResearch CenterP.O.
Box 704Yorktown Heights, NY10598bkb@watson.ibm.comAbstractWe present the architecture and data model forTEXTRACT, a document analysis framework fortext analysis components.
The framework andcomponents have been deployed in researchand industrial environments for text analysisand text mining tasks.1IntroductionIn response to a need for a common infrastructure andbasic services for a number of different, but coordi-nated, text analysis activities with a common set of re-quirements, the Talent (Text Analysis and LanguageENgineering Tools) project at IBM Research developedthe first TEXTRACT system in 1993.
It featured a com-mon C API and a tripartite data model, consisting oflinked list annotations and two hash table extensiblevectors for a lexical cache and a document vocabulary.The experience of productizing this system as part ofIBM?s well-known commercial product IntelligentMiner for Text (IM4T1) in 1997, as well as new researchrequirements, motivated the migration of the analysiscomponents to a C++ framework, a more modular archi-tecture modeled upon IBM?s Software Solutions (SWS)Text Analysis Framework (TAF).The current version of TEXTRACT that we outlinehere is significantly different from the one in IM4T;however, it still retains the tripartite model of the centraldata store.In this paper, we first give an overview of theTEXTRACT architecture.
Section 3 outlines differentoperational environments in which the architecture canbe deployed.
In Section 4, we describe the tripartite21 http://www-3.ibm.com/software/data/iminer/fortext/data model.
In Section 5, we illustrate some fundamen-tals of plugin design, by focusing on Talent?s FiniteState Transducer component and its interaction with thearchitecture and data model.
Section 6 reviews relatedwork.
Finally, we conclude and chart  future directions.The TEXTRACT Architecture: OverviewTEXTRACT is a robust document analysis framework,whose design has been motivated by the requirements ofan operational system capable of efficient processing ofthousands of documents/gigabytes of data.
It has beenengineered for flexible configuration in implementing abroad range of document analysis and linguistic proc-essing tasks.
The common architecture features itshares with TAF include:?
interchangeable document parsers allow the ?in-gestion?
of source documents in more than oneformat (specifically, XML, HTML, ASCII, aswell as a range of proprietary ones);?
a document model provides an abstraction layerbetween the character-based document streamand annotation-based document components,both structurally derived (such as paragraphs andsections) and linguistically discovered (such asnamed entities, terms, or phrases);?
linguistic analysis functionalities are providedvia tightly coupled individual plugin compo-nents; these share the annotation repository, lexi-cal cache, and vocabulary and communicate witheach other by posting results to, and readingprior analyses from, them;?
plugins share a common interface, and are dis-patched by a plugin manager according to de-clared dependencies among plugins; a resourcemanager controls shared resources such as lexi-cons, glossaries, or gazetteers; and at a higherlevel of abstraction, an engine maintains thedocument processing cycle;?
the system and individual plugins are softly con-figurable,  completely from the outside;?
the architecture allows for processing of a streamof documents; furthermore, by means of collec-tion-level plugins and applications, cross-document analysis and statistics can be derivedfor entire document collections.TEXTRACT is industrial strength (IBM, 1997), Unicode-ready, and language-independent (currently, analysisfunctionalities are implemented primarily for English).It is a cross-platform implementation, written in C++.TEXTRACT is ?populated?
by a number of plugins,providing functionalities for:?
tokenization;?
document structure analysis, from tags and whitespace;?
lexicon interface, complete with efficient look-up and full morphology;?
importation of lexical and vocabulary analysesfrom a non-TEXTRACT process via XML markup;?
analysis of out-of-vocabulary words (Park,2002);?
abbreviation finding and expansion (Park andByrd, 2001);?
named entity identification and classification(person names, organizations, places, and soforth) (Ravin and Wacholder, 1997);?
technical term identification, in technical prose(Justeson and Katz, 1995);?
vocabulary determination and glossary extrac-tion, in specialized domains (Park et al, 2002);?
vocabulary aggregation, with reduction to ca-nonical form, within and across documents;?
part-of-speech tagging (with different taggers)for determining syntactic categories in context;?
shallow syntactic parsing, for identifying phrasaland clausal constructs and semantic relations(Boguraev, 2000);?
salience calculations, both of inter- and intra-document salience;?
analysis of topic shifts within a document (Bogu-raev and Neff, 2000a);?
document clustering, cluster organization, andcluster labeling;?
single document summarization, configurable todeploy different algorithmic schemes (sentenceextraction, topical highlights, lexical cohesion)(Boguraev and Neff, 2000a, 2000b);?
multi-document summarization, using iterativeresidual rescaling (Ando et al, 2000);?
pattern matching, deploying finite state technol-ogy specially designed to operate over documentcontent abstractions (as opposed to a characterstream alone).The list above is not exhaustive, but indicative of thekinds of text mining TEXTRACT is being utilized for; weanticipate new technologies being continually added tothe inventory of plugins.
As will become clear later inthe paper, the architecture of this system openly catersfor third-party plugin writers.Figure 1: TEXTRACT ArchitectureSpecific TEXTRACT configurations may deploy cus-tom subsets of available plugin components, in order toeffect certain processing; such configurations typicallyimplement an application for a specific content analysis/ text mining task.
From an application's point of view,TEXTRACT plugins deposit analysis results in the sharedrepository; the application itself ?reads?
these via a welldefined interface.
Document application examples todate include document summarization, a customerclaims analysis system (Nasukawa and Nagano, 2001),and so forth.Collection applications have a document analysiscomponent, which may also write to the shared reposi-tory.
These include named relation extraction (Byrdand Ravin, 1999), custom dictionary building (Park, etal., 2001), indexing for question answering (Prager etal., 2000), cross-document coreference (Ravin and Kazi,1999), and statistical collection analysis for documentsummarization or lexical navigation (Cooper and Byrd,1997).Figure 2: TEXTRACT?s GUIFor packaging in applications, Textract has, in addi-tion to native APIs, a C API layer for exporting the con-tents of the data store to external components in C++ orJava.3 Different Operational EnvironmentsFor the purposes of interactive (re-)configuration ofTEXTRACT?s processing chain, rapid application proto-typing, and incremental plugin functionality develop-ment, the system?s underlying infrastructure capabilitiesare available to a graphical interface.
This allows cont-trol over individual plugins; in particular, it exploits theconfiguration object to dynamically reconfigure speci-fied plugins on demand.
By exposing access to thecommon analysis substrate and the document object,and by exploiting a mechanism for declaring, and inter-preting, dependencies among individual plugins, theinterface further offers functionality similar to that ofGATE (Cunningham, 2002).
Such functionality is facili-tated by suitable annotation repository methods, includ-ing a provision for ?rolling back?
the repository to anearlier state, without a complete system reInit().4 The TEXTRACT Data ModelThe plugins and applications communicate via the anno-tations, vocabulary, and the lexical cache.
The collec-tion object owns the lexical cache; the document objectcontains the other two subsystems: the annotation re-pository, and the document vocabulary.
Shared read-only resources are managed by the resource manager.Annotations:  Annotations contain, minimally, thecharacter locations of the beginning and ending positionof the annotated text within the base document, alongwith the type of the annotation.
Types are organizedinto families: lexical, syntactic, document structure,discourse, and markup.
The markup family providesaccess to the text buffer, generally only used by the to-kenizer.
The annotation repository owns the type sys-tem and pre-populates it at startup time.
Annotationfeatures vary according to the type; for example, posi-tion in a hierarchy of vocabulary categories (e.g.
Person,Org) is a feature of lexical annotations.
New types andfeatures (but not new families) can be added dynami-cally by any system component.
The annotation reposi-tory has a container of annotations ordered on startlocation (ascending), end location (descending), priorityof type family (descending), priority within type family(descending), and type name (ascending).
The generaleffect of the family and type priority order is to reflectnesting level in cases where there are multiple annota-tions at different levels with the same span.
With thispriority, an annotation iterator will always return an NPIn addition, the GUI is configurable as a developmentenvironment for finite state (FS) grammar writing anddebugging, offering native grammar editing and compi-lation, contextualized visualization of FS matching, andin-process inspection of the annotation repository atarbitrary level of granularity.
Figure 2 is broadly in-dicative of some of the functional components exposed:in particular, it exemplifies a working context for agrammar writer, which includes an interface for settingoperational parameters, a grammar editor/compiler, andmultiple viewers for the results of the pattern match,mediated via the annotation repository, and making useof different presentation perspectives (e.g.
a parse treefor structural analysis, concordance for pattern match-ing, and so forth.
)(noun phrase) annotation before a covered word annota-tion, no matter how many words are in the NP.Iterators over annotations can move forward andbackward with  respect to this general order.
Iteratorscan be filtered by set of annotation families, types or aspecified text location.
A particular type of filtered it-erator is the subiterator, an iterator that covers the spanof a given annotation (leaving out the given annotation).Iterators can be specified to be ?ambiguous?
or ?unam-biguous.?
Ambiguous scans return all the annotationsencountered; unambiguous scans return only a singleannotation covering each position in the document, thechoice being made according to the sort order above.Unambiguous scans within family are most useful forretrieving just the highest order of analysis.
All thedifferent kinds of filters can be specified in any combi-nation.Lexical Cache:  One of the features on a word an-notation is a reference to an entry in the lexical cache.The cache contains one entry for each unique token inthe text that contains at least one alphabetic character.Initially designed to improve performance of lexicallookup, the cache has become a central location for au-thority information about tokens, whatever the source:lexicon, stop word list, gazetteer, tagger model etc.
Thedefault lifetime of the lexical cache is the collection;however, performance can be traded for memory by aperiodic cache refresh.The lexical lookup (lexalyzer) plugin populates thelexical cache with tokens, their lemma forms, and mor-pho-syntactic features.
Morpho-syntactic features areencoded in an interchange format which mediatesamong notations of different granularities (of syntacticfeature distinctions or morphological ambiguity), usedby dictionaries (we use the IBM LanguageWare dic-tionaries, available for over 30 languages), tag sets, andfinite state grammar symbols.
In principle, differentplugins running together can use different tag sets bydefining appropriate tagset mapping tables via a con-figuration file.
Similarly, a different grammar morpho-syntactic symbol set can also be externally defined.
Aswith annotations, an arbitrary number of additional fea-tures can be specified, on the fly, for tokens and/orlemma forms.
For example, an indexer for domain ter-minology cross references different spellings, as well asmisspellings, of the same thing.
The API to the lexicalcache also provides an automatic pass-through to thedictionary API, so that any plugin can look up a stringthat is not in the text and have it placed in the cache.Vocabulary: Vocabulary annotations (names, do-main terms, abbreviations) have a reference to an entryin the vocabulary.
The canonical forms, variants, andcategories in the vocabulary can be plugin-discovered(Nominator), or plugin-recovered (matched from anauthority resource, such as a glossary).
Collection sali-ence statistics (e.g.
tfxidf), needed, for example, by thesummarizer application, are populated from a resourcederived from an earlier collection run.
As with the an-notations and lexical entries, a plugin may define newfeatures on the fly.Resource Manager:  The Resource Manager, im-plemented as a C++ singleton object so as to be avail-able to any component anywhere, manages the files andAPI?s of an eclectic collection of shared read-only re-sources: a names authority data base (gazetteer), prefixand suffix lists, stop word lists, the IBM LanguageWaredictionaries with their many functions (lemmatization,morphological lookup, synonyms, spelling verification,and spelling correction), and, for use in the researchenvironment, WordNet (Fellbaum, 1998).
The APIwrappers for the resources are deliberately not uniform,to allow rapid absorption and reuse of components.
Forperformance, the results of lookup in these resources arecached as features in the lexical cache or vocabulary.55.1TEXTRACT PluginsTEXTRACT plugins and applications need only to con-form to the API of the plugin manager, which cyclesthrough the plugin vector with methods for: con-struct(), initialize(), processDocument(),and endDocument().
Collection applications andplugins look nearly the same to the plugin manager;they have, additionally, startCollection() andendCollection() methods.
The complete API alsoincludes the interfaces to the annotation repository, lexi-cal cache, and vocabulary.Plugin Example: TEXTRACT?s Finite StateTransducerNumerous NLP applications today deploy finite state(FS) processing techniques?for, among other things,efficiency of processing, perspicuity of representation,rapid prototyping, and grammar reusability (see, forinstance, Karttunen et al, 1996; Kornai, 1999).
TEX-TRACT's FS transducer plugin (henceforth TFST), en-capsulates FS matching and transduction capabilitiesand makes these available for independent developmentof grammar-based linguistic filters and processors.In a pipelined architecture, and in an environmentdesigned to facilitate and promote reusability, there aresome questions about the underlying data stream overwhich the FS machinery operates, as well as about themechanisms for making the infrastructure compo-nents?in particular the annotation repository andshared resources?available to the grammar writer.Given that the document character buffer logically ?dis-appears?
from a plugin?s point of view, FS operationsnow have to be defined over annotations and their prop-erties.
This necessitates the design of a notation, inwhich grammars can be written with reference toTEXTRACT?s underlying data model, and which stillhave access to the full complement of methods for ma-nipulating annotations.In the extreme, what is required is an environmentfor FS calculus over typed feature structures (see Beckeret al, 2002), with pattern-action rules where patternswould be specified over type configurations, and actionswould manipulate annotation types in the annotationrepository.
Manipulation of annotations from FS speci-fications is also done in other annotation-based textprocessing architectures (see, for instance, the JAPEsystem; Cunningham et al 2000).
However, this istypically achieved, as JAPE does, by allowing for codefragments on the right-hand side of the rules.Both assumptions?that a grammar writer would befamiliar with the complete type system employed by all?upstream?
(and possibly third party) plugins, and that agrammar writer would be knowledgeable enough todeploy raw API's to the annotation repository and re-source manager?go against the grain of TEXTRACT?sdesign philosophy.Consequently, we make use of an abstraction layerbetween an annotation representation (as it is imple-mented) and a set of annotation property specificationswhich define individual plugin capabilities and granu-larity of analysis.
We also have developed a notationfor FS operations, which appeals to the system-wide setof annotation families, with their property attributes, aswell as encapsulates operations over annotations?suchas create new ones, remove existing ones, modify and/oradd properties, and so forth?as primitive operations.Note that the abstraction hides from the grammar writersystem-wide design decisions, which separate the anno-tation repository, the lexicon, and the vocabulary (seeSection 3 above): thus, for instance, access to lexicalresources with morpho-syntactic information, or, in-deed, to external repositories like gazetteers or lexicaldatabases, appears to the grammar writer as querying anannotation with morpho-syntactic properties and attrib-ute values; similarly, a rule can post a new vocabularyitem using notational devices identical to those for post-ing annotations.The freedom to define, and post, new annotationtypes ?on the fly?
places certain requirements on theFST subsystem.
In particular, it is necessary to inferhow new annotations and their attributes fit into an al-ready instantiated data model.
The FST plugin there-fore incorporates logic in its reInit() method whichscans an FST file (itself generated by an FST compilertypically running in the background), and determines?by deferring to a symbol compiler?what new annota-tion types and attribute features need to be dynamicallyconfigured and incrementally added to the model.An annotation-based regime of FS matching needs amechanism for picking a particular path through theinput annotation lattice, over which a rule should beapplied: thus, for instance, some grammars would in-spect raw tokens, others would abstract over vocabularyitems (some of which would cover multiple tokens), yetothers might traffic in constituent phrasal units (with anadditional constrain over phrase type) or/and documentstructure elements (such as section titles, sentences, andso forth).For grammars which examine uniform annotationtypes, it is relatively straightforward to infer, and con-struct (for the run-time FS interpreter), an iterator oversuch a type (in this example, sentences).
However, ex-pressive and powerful FS grammars may be writtenwhich inspect, at different?or even the same?point ofthe analysis annotations of different types.
In this caseit is essential that the appropriate iterators get con-structed, and composed, so that a felicitous annotationstream gets submitted to the run-time for inspection;TEXTRACT deploys a special dual-level iterator designedexpressly for this purpose.Additional features of the TFST subsystem allow forseamless integration of character-based regular expres-sion matching, morpho-syntactic abstraction from theunderlying lexicon representation and part-of-speechtagset, composition of complex attribute specificationfrom simple feature tests, and the ability to constrainrule application within the boundaries of specified anno-tation types only.
This allows for the easy specification,via the grammar rules, of a variety of matching regimeswhich can transparently query upstream annotators ofwhich only the externally published capabilities areknown.A number of applications utilizing TFST include ashallow parser (Boguraev, 2000), a front end to a glos-sary identification tool (Park et al, 2002), a parser fortemporal expressions, a named entity recognition de-vice, and a tool for extracting hypernym relations.6 Related WorkThe Talent system, and TEXTRACT in particular, belongsto a family of language engineering systems which in-cludes GATE (University of Sheffield), Alembic(MITRE Corporation), ATLAS (University of Pennsyl-vania), among others.
Talent is perhaps closest in spiritto GATE.
In Cunningham, et al (1997), GATE is de-scribed as ?a software infrastructure on top of whichheterogeneous NLP processing modules may be evalu-ated and refined individually or may be combined intolarger application systems.?
Thus, both Talent andGATE address the needs of researchers and developers,on the one hand, and of application builders, on theother.The GATE system architecture comprises threecomponents: The GATE Document Manager (GDM),The Collection of Reusable Objects for Language Engi-neering (CREOLE), and the GATE Graphical Interface(GGI).
GDM, which corresponds to TEXTRACT?sdriver, engine, and plugin manager, is responsible formanaging the storage and transmission (via APIs) of theannotations created and manipulated by the NLP proc-essing modules in CREOLE.
In TEXTRACT?s terms, theGDM is responsible for the data model kept in the docu-ment and collection objects.
Second, CREOLE is theGATE component model and corresponds to the set ofTEXTRACT plugins.
Cunningham, et al (1997) em-phasize that CREOLE modules, which can encapsulateboth algorithmic and data resources, are mainly createdby wrapping preexisting code to meet the GDM APIs.In contrast, TEXTRACT plugins are typically written ex-pressly in order that they may directly manipulate theanalyses in the TEXTRACT data model.
According toCunningham, et al (2001), available CREOLE modulesinclude: tokenizer, lemmatizer, gazetteer and namelookup, sentence splitter, POS tagger, and a grammarapplication module, called JAPE, which corresponds toTEXTRACT?s TFST.
Finally, GATE?s third component,GGI, is the graphical tool which supports configurationand invocation of GDM and CREOLE for accomplish-ing analysis tasks.
This component is closest toTEXTRACT?s graphical user interface.
As discussed ear-lier, the GUI is used primarily as a tool for grammardevelopment and AR inspection during grammar writ-ing.
Most application uses of TEXTRACT are accom-plished with the programming APIs and configurationtools, rather than with the graphical tool.Most language engineering systems in theTEXTRACT family have been motivated by a particularset of applications: semi-automated, mixed-initiativeannotation of linguistic material for corpus constructionand interchange, and for NLP system creation andevaluation, particularly in machine-learning contexts.As a result, such systems generally highlight graphicaluser interfaces, for visualizing and manipulating annota-tions, and file formats, for exporting annotations toother systems.
Alembic (MITRE, 1997) and ATLAS(Bird, et al, 2000) belong to this group.
Alembic, builtfor participation in the MUC conferences and adheringto the TIPSTER API (Grishman, 1996), incorporatesautomated annotators (?plugins?)
for word/sentencetokenization, part-of-speech tagging, person/ organiza-tion/ location/ date recognition, and coreference analy-sis.
It also provides a phrase rule interpreter similar toTFST.
Alembic incorporates ATLAS?s ?annotationgraphs?
as its logical representation for annotations.Annotation graphs reside in ?annotation sets,?
which areclosest in spirit to TEXTRACT?s annotation repository,although they don't apparently provide APIs for fine-grained manipulation of, and filtered iterations over, thestored annotations.
Rather, ATLAS exports physicalrepresentations of annotation sets as XML files or rela-tional data bases containing stand-off annotations,which may then be processed by external applications.Other systems in this genre are Anvil (Vintar andKipp (2001), LT-XML (Brew, et al, 2000), MATE(McKelvie, et al, 2000), and Transcriber (Barras, et al,(2001).
Like ATLAS, some of these were originallybuilt for processing speech corpora and have been ex-tended for handling text.
With the exception of GATE,all of these systems are devoted mainly to semi-automated corpus annotation and to evaluation of lan-guage technology, rather than to the construction ofindustrial NLP systems, which is TEXTRACT?s focus.As a result, TEXTRACT uses a homogeneous implemen-tation style for its annotation and application plugins,with a tight coupling to the underlying shared analysisdata model.
This is in contrast to the more loosely-coupled heterogeneous plugin and application modelused by the other systems.7 ConclusionIn this paper, we have described an industrial infra-structure for composing and deploying natural languageprocessing components that has evolved in response toboth research and product requirements.
It has beenwidely used, in research projects and product-level ap-plications.A goal of the Talent project has been to create tech-nology that is well-suited for building robust text analy-sis systems.
With its simple plugin interface (seeSection 5), its rich declarative data model, and the flexi-ble APIs to it (Section 4), TEXTRACT has achieved thatgoal by providing a flexible framework for systembuilders.
The system is habitable (external processescan be ?wrapped?
as plugins, thus becoming available asstages in the processing pipeline), and open (completelynew plugins can be written?by anyone?to a simpleAPI, as long as their interfaces to the annotation reposi-tory, the lexical cache, and the vocabulary (Section 4),follow the published set of specifications.Openness is further enhanced by encouraging theuse of TFST, which directly supports the development,and subsequent deployment, of grammar-based pluginsin a congenial style.
Overall, TEXTRACT?s design char-acteristics prompted the adoption of most of the archi-tecture by a new framework for management andprocessing of unstructured information at IBM Research(see below).Performance is not generally an inherent property ofan architecture, but rather of implementations of thatarchitecture.
Also, the performance of different con-figurations of the system would be dependent on thenumber, type, and algorithmic design and implementa-tion of plugins deployed for any given configuration.Thus it is hard to quantify TEXTRACT?s performance.The most recent implementation of the architecture is inC++ and makes extensive use of algorithms, containerclasses and iterators from the C++ Standard TemplateLibrary for manipulating the data objects in the datamodel; its performance therefore benefits from state-of-the-art implementations of the STL.
As an informalindication of achievable throughput, an earlier productimplementation of the tokenization base services andannotation subsystem, in the context of an informationretrieval indexer, was able to process documents at therate of over 2 gigabytes-per-hour on a mid-range Unixworkstation.Allowing TEXTRACT?s plugins to introduce ?
dy-namically ?
new annotation types and properties is animportant part of an open system.
However, a limita-tion of the current design is the fixed organization ofannotations into families (see Section 4).
This makes ithard to accommodate new plugins which need to appealto information which is either not naturally encodable inthe family space TEXTRACT pre-defines, or requires aricher substrate of (possibly mutually dependent) featuresets.In a move towards a fully declarative representationof linguistic information, where an annotation maxi-mally shares an underlying set of linguistic properties, arational re-design of TEXTRACT (Ferrucci and Lally,2003) is adopting a hierarchical system of feature-basedannotation types; it has been demonstrated that evensystems supporting strict single inheritance only arepowerful enough for a variety of linguistic processingapplications (Shieber, 1986), largely through their well-understood mathematical properties (Carpenter, 1992).Some of this migration is naturally supported by theinitial TEXTRACT data model design.
Other architec-tural components will require re-tooling; in particular,the FST subsystem will need further extensions for thedefinition of FS algebra over true typed feature struc-tures (see, for instance, Brawer, 1998; Wunsch, 2003).We will return to this issue in a following paper.8 AcknowledgementsWe acknowledge the contributions of our colleagues,current and former, in the design and implementation ofthe Talent system and plugins:  Rie Ando, Jim Cooper,Aaron Kershenbaum, Youngja Park, John Prager, YaelRavin, Misook Choi, Herb Chong, and Zunaid Kazi.ReferencesAndo, Rie K., Branimir K. Boguraev, Roy J. Byrd andMary S. Neff.
2000.
Multi-document summarizationby visualizing topical content.
Advanced Summari-zation Workshop, NAACL/ANLP-2000, Seattle,WA, April 2000.Barras, Claude, Edouard Geoffrois, Zhibiao Wu, andMark Liberman.
2001.
Transcriber: development anduse of a tool for assisted speech corpora production.In Speech Communication (33):5-22.Becker, Marcus, Witold Dro?d?y?ski, Hans-UlrichKrieger, Jakub Poskorski, Ulrich Sch?fer, Feiyu Xu.2002.
SProUT?Shallow processing with unificationand typed feature structures.
Proceedings of the In-ternational Conference on Natural Language Proc-essing (ICON 2002), Mumbai, India.Bird, Steven, David Day, John Garofolo, John Hender-son, Christohe Laprun, and Mark Liberman.
2000.ATLAS: A Flexible and extensible architecture forlinguistic annotation.
In Proceedings of the SecondInternational Conference on Language Resourcesand Evaluation: 1699-1706.Brew, Chris, David McKelvie, Richard Tobin, HenryThompson, and Andrei Mikheev.
2000.
The XMLLibrary LT XML version 1.2 ?
User Documentationand Reference Guide," available at http://www.ltg.ed.ac.uk/corpora/xmldoc/release/book1.htm.Boguraev, Branimir K. 2000.
Towards finite-stateanalysis of lexical cohesion", In Proceedings of the3rd International Conference on Finite-State Meth-ods for NLP, INTEX-3, Liege, Belgium.Boguraev, Branimir K. and Mary S. Neff.
2000a.
Dis-course segmentation in aid of document summariza-tion.
In Proceedings of the 33rd Hawaii InternationalConference on System Sciences, Maui, HI, January2000.Boguraev, Branimir K. and Mary S. Neff.
2000b.
Lexi-cal cohesion, discourse segmentation, and documentsummarization.
In RIAO-2000, Paris, April 2000.Brawer, Sascha.
1998.
Patti: Compiling Unification-Based Finite-State Automata into Machine Instruc-tions for a Superscalar Pipelined RISC Processor,MA Thesis, University of the Saarland, Saarbr?cken,Germany.Byrd, Roy and Yael Ravin.
1999.
Identifying and ex-tracting relations in text.
Presented at the NLDB?99Conference, Klagenfurt, Austria.Carpenter, Robert.
1992.
The Logic of Typed FeatureStructures.
Cambridge University Press, Cambridge,England.Cooper, James and Roy J. Byrd.
1997.
Lexical naviga-tion ?
visually prompted query expansion and re-finement.
In DIGILIB 97.Cunningham, Hamish, Diana Maynard and ValentinTablan.
2000.
JAPE: A Java Annotation PatternsEngine.
Research memo CS ?
00 ?
10, Institute forLanguage, Speech and Hearing (ILASH), and De-partment of Computer Science, University of Shef-field, UK.Cunningham, Hamish, Diana Maynard, Valentin Tab-lan, Cristian Ursu, and Kalina Bontcheva.
2001 De-veloping Language Processing Components withGATE.
GATE v2.0 User Guide, University of Shef-field.Cunningham, Hamish, Diana Maynard, KalinaBontcheva, Valentin Tablan.
2002.
GATE: Aframework and graphical development environmentfor robust NLP tools and applications.
Proceedingsof the 40th Anniversary Meeting of the Associationfor Computational Linguistics (ACL'02).
Philadel-phia.Cunningham, Hamish, K. Humphreys, R. Gaizauskas,and Yorick Wilks.
1997.
Software infrastructure fornatural language processing," in Proceedings of theFifth Conference on Applied Natural LanguageProcessing (ANLP-97).Grishman, Ralph.
1996.
TIPSTER Architecture DesignDocument Version 2.2  Technical Report, DARPA.Fellbaum, Christiane.
1998.
WordNet, An ElectronicLexical Database.
MIT Press.Ferrucci, David and Adam Lally.
2003.
Acceleratingcorporate research in the development, application,and deployment of human language technologies.NAACL Workshop on Software Engineering andArchitecture of Language Technology Systems, Ed-monton, Canada.IBM Corp, Intelligent Miner for Text Product Overview,1997.
http://www3.ibm.com/software/data/iminer/fortext/.Justeson, John S. and Slava Katz.
1995.
Technical ter-minology: some linguistic properties and an algo-rithm for identification in text.
Natural LanguageEngineering, 1(1):9-27.Karttunen, Lauri, Jean-Pierre Chanod, Gregory Grefen-stette and Anne Schiller.
1996.
Regular expressionsfor language engineering.
Natural Language Engi-neering, 4(1), pp.305-328.Kornai, Andras.
1999.
Extended Finite State Models ofLanguage, Cambridge University Press, Cambridge,UK.McKelvie, David, Amy Isard, Andreas Mengel, MortenBaun M?ller, Michael Grosse, Marion Klein.
2000.The MATE Workbench - an annotation tool for XMLcoded speech corpora.
In Speech Communication.MITRE Corporation.
1997.
Alembic Workbench UsersGuide.
available at http://www.mitre.org/technology/alembic-workbench/.Nasukawa, Tetsuya and T. Nagano.
2001.
Text analy-sis and knowledge mining system.
In IBM SystemsJournal (40:4): 967-984.Park, Youngja.
2002.
Identification of probable realwords: an entropy-based approach.
In Proceedings ofACL Workshop on Unsupervised Lexical Acquisition:pp 1-8.Park, Youngja and Roy J. Byrd.
2001.
Hybrid textmining for finding terms and their abbreviations, InEMNLP-2001.Park, Youngja, Roy J. Byrd and Branimir K. Boguraev.2002.
Automatic glossary extraction: beyond termi-nology identification.
In Proceedings of the 19th In-ternational Conference on ComputationalLinguistics  (COLING): 772-778.Prager, John, Eric Brown, Anni Coden, and DragomirRadev.
2000.
Question-answering by predictive an-notation.
In Proceedings of SIGIR 2000: 184-191,Athens, Greece.Ravin, Yael and Zunaid Kazi.
1999.
Is Hillary RodhamClinton the president?
Disambiguating names acrossdocuments.
In Proceedings of the ACL ?99 Work-shop on Coreference and its Applications, June 1999.Ravin, Yael and Nina Wacholder.
1997.
Extractingnames from natural-language text.
IBM ResearchReport 20338.Shieber, Stuart.
1986.
An Introduction to Unification-Based Approaches to Grammar, CSLI Lecture Notes,Vol.
4, Stanford University, California.Vintar, Spela and Michael Kipp.
2001.
Multi-track an-notation of terminology using Anvil.Wunsch, Holger.
2003.
Annotation Grammars andTheir Compilation into Annotation Transducers.
MAThesis, University of T?bingen, Germany.
