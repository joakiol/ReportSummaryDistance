Proceedings of the 14th European Workshop on Natural Language Generation, pages 94?97,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsThe KBGen ChallengeEva BanikComputationalLinguistics LtdLondon, UKebanik@comp-ling.comClaire GardentCNRS, LORIANancy, Franceclaire.gardent@loria.frEric Kow?ComputationalLinguistics LtdLondon, UKkowey@comp-ling.com1 IntroductionThe KBGen 2013 natural language generationchallenge1 was intended to survey and comparethe performance of various systems which performtasks in the content realization stage of generation(Banik et al 2012).
Given a set of relations whichform a coherent unit, the task is to generate com-plex sentences which are grammatical and fluentin English.
The relations for this year?s challengewere selected from the AURA knowledge base(KB) (Gunning et al 2010).
In this paper we givean overview of the KB, describe our methodologyfor selecting sets of relations from the KB to pro-vide input-output pairs for the challenge, and givedetails of the development and test data set thatwas provided to participating teams.
Three teamshave submitted system outputs for this year?s chal-lenge.
In this paper we show BLEU and NISTscores for outputs generated by the teams.
The fullresults of our evaluation, including human judge-ments, as well as the development and test data setare available at http://www.kbgen.org.2 The AURA Knowledge BaseThe AURA knowledge base (Gunning et al2010) encodes information from a biology text-book (Reece et al 2010).
It was developed tosupport a question answering system, to help stu-dents understand biological concepts by allowingthem to ask questions about the material whilereading the textbook.
AURA is a frame-basedKB which encodes events, the entities that partic-ipate in events, properties, and roles that the en-tities play in an event.
The relations in the KBinclude relations between these types, includingevent-to-entity, event-to-event, event-to-property,entity-to-property.
The KB is built on top of the?The work reported in this paper was supported by fund-ing from Vulcan, Inc.1http://www.kbgen.orgCLIB generic library of concepts (Barker et al2001).
As part of the encoding process, conceptsin CLIB are specialized and/or combined to en-code biology-specific information.
AURA is or-ganized into a set of concept maps, where eachconcept map corresponds to a biological entity orprocess.
The KB was encoded by biology teach-ers and contains around 5,000 concept maps.
It isavailable for download for academic purposes invarious formats including OWL2.3 The Content Selection Process forKBGen 2012The input provided to the participants consistedof a set of content units extracted from the KB,and a sentence corresponding to each content unit.The content units were semi-automatically se-lected from AURA such that:?
the set of relations in each content unitformed a connected graph?
each content unit can be verbalised by asingle, possibly complex sentence which isgrammatical and meaningful?
the set of content units contain as many dif-ferent relations and concepts of different se-mantic types (events, entities, properties, etc)as possible.To produce these inputs we first asked biologyteachers to provide coherent content units usingthe AURA graphical interface.
The basic assump-tion behind this approach was that, since everycontent unit can be expressed by a coherent sen-tence, each set of relations will exhibit a ?coher-ence pattern?.
We then created a search space ofcandidate content units by extracting patterns fromthe KB which were similar to the patterns givenby the biologists.
Finally, we manually selectedcoherent content units.2http://www.ai.sri.com/halo/halobook2010/exported-kb/biokb.html94Figure 1: ?
A hydrophobic compound attaches to acarrier protein at a region called the binding site.
?3.1 Manual Selection of Content UnitsIn the first step of our process, biology teachersmanually selected parts of concept maps whichrepresented educationally useful information forbiology students by searching for specific con-cepts in AURA?s graph-based user interface.
Foreach content unit they wrote a sentence verbalis-ing the selected relations (Fig.
1).
The biologyteachers who identified these coherent, sentence-sized chunks of information were familiar with theencoding practices in AURA, the underlying biol-ogy textbook, and had experience with the Inquiree-book application (Spaulding et al 2011) whichdisplays educationally useful content from the KB.3.2 From Graphs to QueriesIn the second step, the graphical representationsproduced by the biologists were manually trans-lated to specific knowledge base queries whichwere run in AURA to retrieve the instances sat-isfying the queries.
Queries consist of two parts:a set of triples whose domain and range are vari-ables, and a set of instance-of triples stating typeconstraints on the variables.
The graph shown inFigure 1 was translated to the following query:Type constraints:(?CP instance-of Carrier-Protein)(?A instance-of Attach)(?BS instance-of Binding-Site)(?HP instance-of Hydrophilic-Compound)Relation constraints:(?A object ?HP)(?A base ?CP)(?A site ?BS)(?CP has-region ?BS)3.3 From Queries to Generalized QueryPatternsAfter checking that it returns an answer, eachquery was generalized to a query pattern in or-der to find other queries which involved differentconcepts and relations, but still exhibited the samegeneral coherence pattern.
To derive generalizedquery patterns, specific queries were modified intwo ways: 1) by removing type constraints on con-cepts, and 2) by replacing specific relations withgeneralized relation types.Removing type constraintsManually specified queries were extended by re-moving type constraints on variables.
In the aboveexample, types were generalised to Event or En-tity:(?CP instance-of Entity)(?A instance-of Event)(?BS instance-of Entity)(?HP instance-of Entity)Other generalized types we used from the ontol-ogy were Property-Values and Roles.Generalizing relationsEach query was generalized by defining equiva-lence classes over semantically similar relationsand replacing the specific relation in the querywith its equivalence class.
The basic assumptionbehind this was that if a set of relations is coherent,we should be able to replace a relation with an-other, semantically similar relation in the set, andstill have a coherent content unit.
For example,whether two entities are connected by has-partor has-region is unlikely to make a differenceto the coherence of a content unit.Following this approach we identified groups ofsemantically similar relations within each relationtype (Event-to-Event, Event-to-Entity, etc).
Theequivalence classes over relations were straight-forwardly derived from distinctions made in CLIB(Barker et al 2001), the upper ontology and li-brary of general concepts that AURA is built on,although there was some manual fine-tuning re-quired to exclude relations which were not re-liably encoded in the KB.
For example, we di-vided Entity-to-Entity relations into three cate-gories, based on whether they had a spatial ormeronymic sense, or expressed a specific relationbetween two chemicals:en2en-spatial: abuts is-above is-along is-at is-inside is-opposite is-outside is-over location95is-across is-on is-parallel-to is-perpendicular-to is-under is-between is-facing is-below is-beside is-nearen2en-part: possesses has-part has-regionencloses has-basic-structural-unit has-structural-part has-functional-parten2en-chemical: has-solute has-solvent has-atom has-ion has-oxidized-form has-reduced-form has-isomerHere the distinction between spatial relationsand meronymic relations was given by CLIB.
Re-lations in the third group were specific to our do-main and added during the process of encoding.Event-to-entity relations were divided into?aux-participant?
relations, which express the spa-tial orientation of an event, and ?core-participant?relations which describe ways in which entitiesparticipate in the event.
Here we used the cat-egories of spatial relations and ?participant?
re-lations from CLIB.
Our terminology reflects thefact that entities connected to an event by acore-participant relation are typically expressed asobligatory arguments of the verb in a sentence,whereas aux-participants would be expressed asoptional modifiers:core-participants: agent object donor base in-strument raw-material recipient resultaux-participants: away-from destination originpath site towardWith these definitions, the specific query illus-trated above in section 3.2 was translated to thefollowing query pattern:(?A core-participant ?X)(?A core-participant ?CP)(?A aux-participant ?BS)(?CP en2en-part ?BS)3.4 From Query Results to Content UnitsQuery patterns were expanded by producing allvalid instantiations of the pattern in order to cre-ate a search space of candidate content units, andwe ran each expanded query in AURA.
The laststep was filtering the results returned by satisfi-able queries to obtain content units which can beverbalised in a single sentence.
We used the fol-lowing selection criteria to do this:?
A meaningful and grammatical sentencecould be formed by verbalising all concepts,relations and properties present in the queryresult.
(KBGEN-INPUT :ID "ex03c.99-1":TRIPLES ((|Secretion21994| |object| |Mucus21965|)(|Secretion21994| |base| |Earthworm21974|)(|Secretion21994| |site| |Alimentary-Canal21978|)(|Earthworm21974| |has-region||Alimentary-Canal21978|)):INSTANCE-TYPES ((|Mucus21965| |instance-of| |Mucus|)(|Secretion21994| |instance-of| |Secretion|)(|Earthworm21974| |instance-of| |Earthworm|)(|Alimentary-Canal21978| |instance-of||Alimentary-Canal|)):ROOT-TYPES ((|Secretion21994| |instance-of| |Event|)(|Mucus21965| |instance-of| |Entity|)(|Earthworm21974| |instance-of| |Entity|)(|Alimentary-Canal21978| |instance-of| |Entity|)))Figure 2: Input for the sentence ?Mucus is se-creted in the alimentary canal of earthworms.??
The set of content units should be as variedas possible.
In particular, we did not keepa content unit if another very similar contentunit was present in the selected units.
For in-stance, if two content units contain identicalrelations (modulo concept labels), only oneof these two units would be kept.Given the pattern shown in Fig.
1 for instance,we obtained 27 coherent content units.
Each con-tent unit was verbalized as a sentence to providedevelopment data for the content realization chal-lenge.
The following sentences illustrate the vari-ation in the resulting content units:- Polymers are digested in the lysosomes of eu-karyotic cells.- Mucus is secreted in the alimentary canal ofearthworms.- Lysosomal enzymes digest proteins and poly-mers at the lysosome of a eukaryotic cell.- A chemical is attached to the active site of aprotein enzyme with an ionic bond.- An enzyme substrate complex is formedwhen a chemical attaches to the active site ofa protein enzyme with a hydrogen bond.- Starch is stored in the lateral root of carrots.4 Development Data SetThe development data set provided to participantscontained 207 input-output pairs.
These inputs96were based on 19 different coherence patterns.Fig.
2 shows an input-output pair based on thepattern illustrated above.
We also provided twolexicons: a lexicon for events which gave a map-ping from events to verbs, their inflected forms andnominalizations and a lexicon for entities, whichprovided a noun and its plural form.
The rele-vant entries in these lexicons for the input in Fig.
2were:Secretion,secretes,secrete,secreted,secretionMucus, mucus, mucusEarthworm,earthworm,earthwormsAlimentary-Canal,alimentary canal,alimentary canals5 Test SetOur test data set contained 72 inputs in the sameformat (and corresponding lexical resources asabove), which were divided into three categories:(1) seen patterns, seen relations: inputs that haveexactly the same relations as some of the inputs inthe development data set, but different concepts(2) seen patterns, unseen relations: these in-puts are derived from patterns in the developmentdata set.
They have similar structure, but containslightly different combinations of relations.
(3) unseen patterns: inputs extracted from a pre-viously unused pattern, containing combinationsof relations not seen in the development data set.6 EvaluationParticipants submitted two sets of outputs:(1) outputs generated by their system as is (mod-ulo including the lexicon provided in the test dataset) (2) outputs generated 6 days later, duringwhich time teams had a chance to make improve-ments.Each team was allowed to submit a set of 5 rankedoutputs for each input.
We have evaluated allof the submitted outputs using BLEU and NISTscores and we are currently in the process of col-lecting human judgements for the final system out-puts that were ranked first.
Table 1 shows theoverall results of automatic evaluation on both theinitial and final data sets for our three teams3, aswell as the coverage of the individual systems overthe 72 test inputs.
More detail including the fullresults of our evaluation can be found at http://www.kbgen.org, along with a link to download3IMS: Stuttgart University Institute for ComputationalLanguage Processing, LOR: LORIA, University of Nancy,UDEL: University of Delaware, Computer and InformationScience DepartmentNIST BLEU coverageHUMAN-1 10.0098 1.0000 100%UDEL-final-1 5.9749 0.3577 97%UDEL-initial-1 5.6030 0.3165 100%LOR-final-1 4.8569 0.3053 84%LOR-final-3 4.7238 0.2993 100%LOR-final-2 4.6711 0.2945 100%LOR-final-5 4.5720 0.2812 100%LOR-final-4 4.4889 0.2781 100%IMS-final-2 3.9649 0.1107 100%IMS-final-4 3.8813 0.1140 100%IMS-final-1 3.8670 0.1111 100%IMS-final-3 3.7765 0.1023 100%IMS-initial-2 3.6726 0.1117 100%IMS-initial-3 3.6608 0.1181 100%IMS-initial-1 3.6384 0.1173 100%IMS-initial-4 3.5817 0.1075 100%LOR-initial-1 0.1206 0.0822 30%LOR-initial-3 0.1091 0.0751 100%LOR-initial-4 0.0971 0.0732 100%LOR-initial-2 0.0948 0.0757 100%LOR-initial-5 0.0881 0.0714 100%Table 1: BLEU and NIST scores of initial and finalsystem outputs.
The digit behind the team namesrefer to the output rankthe development and test data set used in the chal-lenge, and more information about AURA and re-lated resources.ReferencesE.
Banik, C. Gardent, D. Scott, N. Dinesh, andF.
Liang.
2012.
Kbgen text generation from knowl-edge bases as a new shared task.
In INLG 2012,Starved Rock State Park, Illinois,USA.K.
Barker, B. Porter, and P. Clark.
2001.
A library ofgeneric concepts for composing knowledgebases.
InProceedings K-CAP 2001, pages 14?21.D.
Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-ing, D. Tecuci, and J. Tien.
2010.
Project halo up-date - progress toward digital aristotle.
AIMagazine,Fall:33?58.Jane B. Reece, Lisa A. Urry, Michael L. Cain,Steven A. Wasserman, Peter V. Minorsky, andRobert B. Jackson.
2010.
Campbell Biology.
Pear-son Publishing.A.
Spaulding, A. Overholtzer, J. Pacheco, J. Tien, V. K.Chaudhri, D. Gunning, and P. Clark.
2011.
Inquirefor iPad: Bringing question-answering AI into theclassroom.
In International Conference on AI in Ed-ucation (AIED).97
