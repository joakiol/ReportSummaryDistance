Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430?439,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPLanguage Models Based on Semantic CompositionJeff Mitchell and Mirella LapataSchool of Informatics, University of EdinburghEdinburgh EH8 9LW, UKjeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we propose a novel statisticallanguage model to capture long-range se-mantic dependencies.
Specifically, we ap-ply the concept of semantic composition tothe problem of constructing predictive his-tory representations for upcoming words.We also examine the influence of the un-derlying semantic space on the composi-tion task by comparing spatial semanticrepresentations against topic-based ones.The composition models yield reductionsin perplexity when combined with a stan-dard n-gram language model over then-gram model alone.
We also obtain per-plexity reductions when integrating ourmodels with a structured language model.1 IntroductionStatistical language modeling plays an importantrole in many areas of natural language process-ing including speech recognition, machine trans-lation, and information retrieval.
The prototypi-cal use of language models is to assign proba-bilities to sequences of words.
By invoking thechain rule, these probabilities are generally es-timated as the product of conditional probabili-ties P(wi|hi) of a word wigiven the history ofpreceding words hi?
wi?11.
In theory, the historycould span any number of words up to wisuch assentences or even a paragraphs.
In practice, how-ever, it has proven challenging to deal with thecombinatorial growth in the number of possiblehistories which in turn impacts reliable parame-ter estimation.
A simple and effective strategy isto truncate the chain rule to include only the n-1preceding words (n is often set within the rangeof 3?5).
The simplification reduces the number offree parameters.
However, low values of n imposean artificially local horizon to the language model,and compromise its ability to capture long-rangedependencies, such as syntactic relationships, se-mantic or thematic constraints.The literature offers many examples of how toovercome this limitation, essentially by allowingthe modulation of probabilities by dependencieswhich extend to words beyond the n-gram horizon.Cache language models (Kuhn and de Mori, 1992)increase the probability of words observed in thehistory, e.g., by some factor which decays expo-nentially with distance.
Trigger models (Rosen-feld, 1996) go a step further by allowing arbi-trary word pairs to be incorporated into the cache.Structured language models (e.g., Roark (2001))go beyond the representation of history as a lin-ear sequence of words to capture the syntactic con-structions in which these words are embedded.It is also possible to build representations ofhistory which are semantic rather than syntactic(Bellegarda (2000; Coccaro and Jurafsky (1998;Gildea and Hofmann (1999)).
In this approach, es-timates for the probabilities of upcoming wordsare derived from a comparison of their semanticcontent with the content of the history so far.
Thesemantic representations, in this case, are vectorsderived from the distributional properties of wordsin a corpus, based on the insight that words whichare semantically similar will be found in similarcontexts (Harris, 1968; Firth, 1957).
Although thethe construction of a semantic representation forthe history is crucial to this approach, the under-lying vector-based models are primarily designedto represent isolated words rather than word se-quences.
Ideally, we would like to compose themeaning of the history out of its constituent parts.This is by no means a new idea.
Much work in lin-guistic theory (Partee, 1995; Montague, 1974) hasbeen devoted to compositionality, the process ofdetermining the meaning of complex expressionsfrom simpler ones.
Previous work either ignoresthis issue (e.g., Bellegarda (2000)) or simply com-430putes the centroid of the vectors representing thehistory (e.g., Coccaro and Jurafsky (1998)).
This ismotivated primarily by mathematical conveniencerather than by empirical evidence.In our earlier work (Mitchell and Lapata, 2008)we formulated composition as a function of twovectors and introduced a variety of models basedon addition and multiplication.
In this paper weapply vector composition to the problem of con-structing predictive history representations for lan-guage modeling.
Besides integrating compositionwith language modeling, a task which is novel toour knowledge, our approach also serves as a valu-able testbed of our earlier framework which weoriginally evaluated on a small scale verb-subjectsimilarity task.
We also investigate how the choiceof the underlying semantic representation inter-acts with the choice of composition function bycomparing a spatial model that represents wordsas vectors in a high-dimensional space against aprobabilistic model that represents words as topicdistributions.Our results show that the proposed composi-tion models yield reductions in perplexity whencombined with a standard n-gram model overthe n-gram model alone.
We also show that withan appropriate composition function spatial mod-els outperform the more sophisticated topic mod-els.
Finally, we obtain further perplexity reduc-tions when our models are integrated with a struc-tured language model, indicating that the two ap-proaches to language modeling are complemen-tary.2 Background2.1 Distributional Models of SemanticsThe insight that words with similar meanings willtend to be distributed in similar contexts has givenrise to a number of approaches that constructsemantic representations from corpora.
Broadlyspeaking, these models come in two flavors.
Se-mantic space models represent the meaning ofwords in terms of vectors, with the vector compo-nents being derived from the distributional statis-tics of those words.
Essentially, these models pro-vide a simple procedure for constructing spatialrepresentations of word meaning.
Topic models, incontrast, impose a probabilistic model onto thosedistributional statistics, under the assumption thathidden topic variables drive the process that gener-ates words.
Both approaches represent the mean-ings of words in terms of an n-dimensional seriesof values, but whereas the semantic space modeltreats those values as defining a vector with spatialproperties, the topic model treats them as a proba-bility distribution.A simple and popular (McDonald, 2000; Bul-linaria and Levy, 2007; Lowe, 2000) way to con-struct a semantic space model is to associate eachvector component with a particular context word,and assign it a value based on the strength ofits co-occurrence with the target (i.e., the wordfor which a semantic representation is being con-structed).
For example, in Mitchell and Lapata(2008) we used the 2,000 most frequent contentwords in a corpus as their contexts, and definedco-occurrence in terms of the context word be-ing present in a five word window on either sideof the target word.
We calculated the ratio of theprobability of the context word given the targetword to the overall probability of the context wordand use these values as their vector components.This procedure has the benefits of simplicity andalso of being largely free of any additional the-oretical assumptions over and above the distribu-tional approach to semantics.
This is not to say thatmore sophisticated approaches have not been de-veloped or that they are not useful.
Much work hasbeen devoted to enriching semantic space mod-els with syntactic information (e.g., Grefenstette(1994; Pad?o and Lapata (2007)), selectional pref-erences (Erk and Pad?o, 2008) or with identifyingoptimal ways of defining the vector components(e.g., Bullinaria and Levy (2007)).The semantic space discussed thus far is basedon word co-occurrence statistics.
However, thestatistics of how words are distributed across thedocuments also carry useful semantic informa-tion.
Latent Semantic Analysis (LSA, Landauerand Dumais (1997) utilizes precisely this distribu-tional information to uncover hidden semantic fac-tors by means of dimensionality reduction.
Singu-lar value decomposition (SVD, Berry et al (1994))is applied to a word-document co-occurrence ma-trix which is factored into a product of a numberof other matrices; one of them represents words interms of the semantic factors and another repre-sents documents in terms of the same factors.
Thealgebraic relation between these matrices can beused to show that any document vector is a linearcombination of the vectors representing the wordsit contains.
Thus, within this paradigm it is nat-431ural to treat multi-word structures as a ?pseudo-document?
and represent them via linear combi-nations of word vectors.Due to its generality, LSA has proven a valuableanalysis tool with a wide range of applications.However, the SVD procedure is somewhat ad-hoclacking a sound statistical foundation.
Probabilis-tic Latent Semantic Analysis (pLSA, Hofmann(2001)) casts the relationship between documentsand words in terms of a generative model based ona set of hidden topics.
Documents are representedby distributions over topics and topics are distri-butions over words.
Thus the mixture of topicsin any document determines its vocabulary.
Maxi-mum likelihood estimation of these distributionsover a word-document matrix has a comparableeffect to SVD in LSA: a set of hidden semanticfactors, in this case topics, are extracted and docu-ments and words are represented by these topics.Latent Dirichlet Allocation (Griffiths et al,2007; Blei et al, 2003) enhances further the math-ematical foundation of this approach.
WhereaspLSA treats each document as a separate, inde-pendent mixture of topics, LDA assumes that thetopic distributions of documents are generated bya Dirichlet distribution.
Thus, LDA is a probabilis-tic model of the whole document collection.
In thismodel the process of generating a document canbe described as follows:1. draw a multinomial distribution ?
from aDirichlet distribution parametrized by ?2.
for each word in a document:(a) draw a topic zkfrom the multinomialdistribution characterized by ?
(b) draw a word from a multinomial distri-bution conditioned on the topic zkandword probabilities ?Under this model, constructing a representationfor a multi-word sequence amounts to estimatingthe topic proportions for that sequence.1Struc-ture here arises from the mathematical form of themodel, as opposed to any linguistic assumptions.Without anticipating our results too much, weshould point out that several features of the LDAmodel are likely to affect the representation of1Estimating the posterior distribution P(?,z|w,?,?)
ofthe hidden variables given an observed collection of docu-ments w is intractable in general; however, a variety of ap-proximate inference algorithms have been proposed in theliterature (e.g., Blei et al (2003; Griffiths et al (2007)).multi-word sequences.
Firstly, it is a top-downgenerative model (the topic proportions for a doc-ument are first selected and then this drives thegeneration of words) as opposed to a bottom-upconstructive process (words modulate each otherto produce a complex representation of their com-bination).
Secondly, the top level Dirichlet distri-bution is likely to lead to documents being dom-inated by a small number of topics, producingsparse vectors.
And lastly, the assumption thatwords are generated independently means the in-teraction between them is not modeled.2.2 Language Modeling using SemanticRepresentationsA common approach to embedding semantic rep-resentations within language modeling is to mea-sure the semantic similarity between an upcomingword and its history and use it to modify the prob-abilities from an n-gram model.
In this way, then-gram?s sensitivity to short-range dependenciesis enriched with information about longer-rangesemantic coherence.
Much of previous work hastaken this approach (Bellegarda, 2000; Coccaroand Jurafsky, 1998; Wandmacher and Antoine,2007), whilst relying on LSA to provide seman-tic representations for individual words.
Some au-thors (Coccaro and Jurafsky, 1998; Wandmacherand Antoine, 2007) use the geometric notion ofa vector centroid to construct representations ofhistory, whereas others (Bellegarda, 2000; Dengand Khundanpur, 2003) use the idea of a ?pseudo-document?, which is derived from the algebraicrelation between documents and words assumedwithin LSA.
They all derive P(wi|hi), the probabil-ity of an upcoming word given its history, from thecosine similarity measure which must be somehownormalized in order to yield well-formed probabil-ity estimates.The approach of Gildea and Hofmann (1999)overcomes this difficulty by using representationsconstructed with pLSA, which have a direct prob-abilistic interpretation.
As a result, the probabil-ity of an upcoming word given the history can bederived naturally and directly, avoiding the needfor ad-hoc transformations.
In constructing theirrepresentation of history, Gildea and Hofmann(1999) use an online Expectation Maximizationprocess, which derives from the probabilistic basisof pLSA, to update the history with new words.Extensions on the basic semantic language432models sketched above involve representing thehistory by multiple LSA models of varying granu-larity in an attempt to capture topic, subtopic, andlocal information (Zhang and Rudnicky, 2002); in-corporating syntactic information by building thesemantic space over words and their syntactic an-notations (Kanejiya et al, 2004); and treating theLSA similarity as a feature in a maximum entropylanguage model (Deng and Khundanpur, 2003).3 Composition ModelsThe problem of vector composition has re-ceived relatively little attention within natural lan-guage processing.
Attempts to use tensor products(Smolensky, 1990; Clark et al, 2008; Widdows,2008) as a means of binding one vector to anotherface major computational difficulties as their di-mensionality grows exponentially with the num-ber of constituents being composed.
To overcomethis problem, other techniques (Plate, 1995) havebeen proposed in which the binding of two vectorsresults in a vector which has the same dimension-ality as its components.
Crucially, the success ofthese methods depends on the assumption that thevector components are randomly distributed.
Thisis problematic for modeling language which hasregular structure.Given the above considerations, in Mitchell andLapata (2008) we introduce a general frameworkfor studying vector composition, which we formu-late as a function f of two vectors u and v:h= f (u,v) (1)where h denotes the composition of u and v. Dif-ferent composition models arise, depending onhow f is chosen.
Our earlier work (Mitchell andLapata, 2008) explored two broad classes of mod-els based on additive and multiplicative functions.Additive models are the most common methodof vector combination in the literature.
They havebeen applied to a wide variety of tasks includ-ing document coherence (Foltz et al, 1998), es-say grading (Landauer and Dumais, 1997), mod-eling selectional restrictions (Kintsch, 2001), andnotably language modeling (Coccaro and Jurafsky,1998; Wandmacher and Antoine, 2007):hi= ui+ vi(2)Vector addition (or averaging, which is equivalentunder the cosine similarity measure) is a computa-tionally efficient composition model as it does notincrease the dimensionality of the resulting vector.However, the idea of averaging is somewhat coun-terintuitive from a linguistic perspective.
Compo-sition of simple elements onto more complex onesmust allow the construction of novel meaningswhich go beyond those of the individual elements(Pinker, 1994).In Mitchell and Lapata (2008) we argue thatcomposition models based on multiplication ad-dress this problem:hi= ui?
vi(3)Whereas the addition of vectors ?lumps their con-tent together?, multiplication picks out the contentrelevant to their combination by scaling each com-ponent of one with the strength of the correspond-ing component of the other.
This argument is ap-pealing, especially if one is interested in explain-ing how the meaning of a verb is modulated byits subject.
Here, we also develop a complemen-tary, probabilistic argument for the validity of thismodel.Let us assume that semantic vectors are basedon components defined as the ratio of the condi-tional probability of a context word given the tar-get word to the overall probability of the contextword.vi=p(contexti|target)p(contexti)(4)These vectors represent the distributional proper-ties of a given target word in terms of the strengthof its co-occurrence with a set of context words.Dividing through by the overall probability of eachcontext word prevents the vectors being dominatedby the most frequent context words, which will of-ten also have the highest conditional probabilities.Let us assume vectors u and v represent tar-get words w1and w2.
Now, when we composethese vectors using the multiplicative model andthe components definition in (4), we obtain:hi= vi?ui=p(ci|w1)p(ci)p(ci|w2)p(ci)(5)And by Bayes?
theorem:hi=p(w1|ci)p(w2|ci)p(w1)p(w2)(6)Assuming w1and w2are independent and apply-ing Bayes?
theorem again, hibecomes:hi?p(w1w2|ci)p(w1w2)=p(ci|w1w2)p(ci)(7)433By comparing to (4), we can see that the expres-sion on the right hand side gives us something akinto the vector components we would expect whenour target is the co-occurrence of w1and w2.
Thus,for the multiplicative model, the combined vec-tor hican be thought of as an approximation toa vector representing the distributional propertiesof the phrase w1w2.If multiplication results in a vector which issomething like the representation of w1and w2,then addition produces a vector which is more likethe representation of w1or w2.
Suppose we wereunsure whether a word token x was an instanceof w1or of w2.
It would be reasonable to expressthe probabilities of context words around this to-ken in terms of the probabilities for w1and w2,assuming complete uncertainty between them:p(ci|x) =12p(ci|w1)+12p(ci|w2) (8)Therefore, we could represent x with a vector,based on these probabilities, having the compo-nents:xi=12p(ci|w1)p(ci)+12p(ci|w2)p(ci)(9)Which is exactly the vector averaging approach tosemantic composition.
As more vectors are com-bined, vector addition will lead to greater general-ity rather than greater specificity.
The multiplica-tive approach, on the other hand, picks out thecomponents of the constituents that are relevantto the combination, and represents more faithfullythe properties of their conjunction.As an aside, we should point out that our earlierwork (Mitchell and Lapata, 2008) introduced sev-eral other models, additive and multiplicative, be-sides the ones discussed here.
We selected the ad-ditive model as a baseline and also due to its over-whelming popularity in the language modeling lit-erature.
The multiplicative model presented aboveperformed best in our evaluation study (i.e., pre-dicting verb-subject similarity).4 Language ModelingEstimating Probabilities In language modelingour aim is to derive probabilities, p(w|h), giventhe semantic representations of word, w, and itshistory, h, based on the assumption that probablewords should be semantically coherent with thehistory.
Semantic coherence is commonly mea-sured via the cosine of the angle between two vec-tors:sim(w,h) =w ?h|w||h|(10)w ?h=?iwihi(11)where w ?
h is the dot product of w and h. Coc-caro and Jurafsky (1998) utilize this measure intheir approach to language modeling.
Unfortu-nately, they find it necessary to resort to a numberof ad-hoc mechanisms to turn the cosine similari-ties into useful probabilities.
The primary problemwith the cosine measure is that, although its valueslie between 0 and 1, they do not sum to 1, as prob-abilities must.
Thus, some form of normalizationis required.
A further problem concerns the factthat such a measure takes no account of the under-lying frequency of w, which is crucial for a proba-bilistic model.
For example, encephalon and brainare roughly synonymous, and may be equally sim-ilar to some context, but brain may nonetheless bemuch more likely, as it is generally more common.An ideal measure would take account of the un-derlying probabilities of the elements involved andproduce values that sum to 1.
Our approach is tomodify the dot product (equation (11)) on whichthe cosine measure is based.
Assuming that ourvector components are given by equation (4), thedot product becomes:w ?h=?ip(ci|w)p(ci)p(ci|h)p(ci)(12)which we modify to derive probabilities as fol-lows:p(w|h) = p(w)?ip(ci|w)p(ci)p(ci|h)p(ci)p(ci) (13)This expression now weights the sum with the in-dependent probabilities of the context words andthe word to be predicted.
That this is indeed a validprobability can be seen by the fact it is equiva-lent to?ip(w|ci)p(ci|h).
However, in constructinga representation of the history h, it is more conve-nient to work with equation (13) as it is based onvector components and can be readily used withthe composition models presented in Mitchell andLapata (2008).Equation (13) allows us to derive probabilitiesfrom vectors representing a word and its prior his-tory.
We must also construct a representation of434the history up to the nth word of a sentence.
To dothis, we combine, via some (additive or multiplica-tive) function f , the vector representing that wordwith the vector representing the history up to n?1words:hn= f (wn,hn?1) (14)h1= w1(15)One issue that must be resolved in implement-ing equation (14) is that the history vector shouldremain correctly normalized.
In other words, theproducts hi?
p(ci) must themselves be a valid dis-tribution over context words.
So, after each vec-tor composition the history vector is normalizedas follows:hi=?hi?j?hj?
p(ci)(16)Equations (13)?
(16) define a language modelthat incorporates vector composition.
To generateprobability estimates, it requires a set of word vec-tors whose components are based on the ratio ofprobabilities described by equation (4).Our discussion thus far has assumed a spatialsemantic space model similar to that employed inMitchell and Lapata (2008).
However, there is noreason why the vectors should not be constructedby some other means.
As mentioned earlier, in theLDA topic model, words are represented as dis-tributions over topics.
These distributions are es-sentially components of a vector v correspondingto the target word for which we wish to constructa semantic representation.
Analogously to equa-tion (4), we convert these probabilities to ratios ofprobabilities:vi=p(topici|target)p(topici)(17)Integrating with Other LanguageModels Themodels defined above are based on little more thansemantic coherence.
As such they will be onlyweakly predictive, since they largely ignore wordorder, which n-grammodels primarily exploit.
Thesimplest means to integrate semantic informationwith a standard language model involves combin-ing two probability estimates as a weighted sum:p(w|h) = ?1p1(w|h)+(1??
)p2(w|h) (18)Linear interpolation is guaranteed to producevalid probabilities, and has been used, for exam-ple, to integrate structured language models withn-gram models (Roark, 2001).
However, it willwork best when the models being combined areroughly equally predictive and have complemen-tary strengths and weaknesses.
If one model ismuch weaker than the other, linear interpolationwill typically produce a model of intermediatestrength (i.e., worse than the better model), withthe weaker model contributing a form of smooth-ing at best.Therefore, based on equation (13), we expressour semantic probabilities as the product of theunigram probability, p(w), and a semantic com-ponent, ?, which determines the factor by whichthis probability should be scaled up or down giventhe context in which it occurs.p(w|h) = p(w) ??
(w,h) (19)?
(w,h) =?ip(ci|w)p(ci)p(ci|h)p(ci)p(ci) (20)Thus, it seems reasonable to integrate the n-grammodel by replacing the unigram probabilities withthe n-gram versions.2p?
(wn) = p(wn|wn?1n?2) ??
(wn,h) (21)To obtain a true probability estimate we normalizep?
(wn) by dividing through the sum of all wordprobabilities:p(wn|wn?1n?2,h) =p?(wn)?wp?
(w)(22)In integrating our semantic model with an n-grammodel, we allow the latter to handle short rangedependencies and have the former handle thelonger dependencies outside the n-gram window.For this reason, the history h used by the semanticmodel in the prediction of wnonly includes wordsup to wn?3(i.e., only words outside the n-gram).We also integrate our models with a structuredlanguage model (Roark, 2001).
However, in thiscase we use linear interpolation (equation (18))because the models are roughly equally predic-tive and also because linear interpolation is widelyused when structured language models are com-bined with n-grams and other information sources.This approach also has the benefit of allowing the2Equation (21) can also be expressed as p(wn|wn?1n?2,h) ?p(wn|wn?1n?2)p(wn|h)p(wn), Which is equivalent to assuming that h isconditionally independent of wn?1n?2(Gildea and Hofmann,1999).435models to be combined without out the need torenormalize the probabilities.
In the case of thestructured language model, normalizing across thewhole vocabulary would be prohibitive.5 Experimental SetupIn this section we discuss our experimental designfor assessing the performance of the models pre-sented above.
We give details on our training pro-cedure and parameter estimation, and present themethods used for comparison with our approach.Method Following previous work (e.g., Belle-garda (2000)) we integrated our compositionallanguage models with a standard n-gram model(see equation (21)).
We experimented with addi-tive and multiplicative composition functions, andtwo semantic representations (LDA and the sim-pler semantic space model), resulting in four com-positional models.
In addition, we compared ourmodels against a state of the art structured lan-guage model in order to assess the extent to whichthe information provided by the semantic repre-sentation is complementary to syntactic structure.Our experiments used Roark?s (2001) grammar-based language model.
Similarly to standard lan-guage models, it computes the probability of thenext word based upon the previous words of thesentence.
This is done by computing a subset of allpossible grammatical relations for the prior wordsand then estimating the probability of the nextgrammatical structure and the probability of see-ing the next word given each of the prior gram-matical relations.
When estimating the probabilityof the next word, the model conditions on the twoprior heads of constituents, thereby using informa-tion about word triples (like a trigram model).All our models were evaluated by computingperplexity on the test set.
Roughly, this quanti-fies the degree of unpredictability in a probabil-ity distribution, such that a fair k-sided dice wouldhave a perplexity of k. More precisely, perplexityis the reciprocal of the geometric average of theword probabilities and a lower score indicates bet-ter predictions.Parameter Estimation The compositional lan-guage models were trained on the BLLIP corpus,a collection of texts from the Wall Street Journal(years 1987?89).
The training corpus consisted of38,521,346 words.
We used a development corpusof 50,006 words and a test corpus of similar size.All words were converted to lowercase and num-bers were replaced with the symbol ?num?.
A vo-cabulary of 20,000 words was chosen and the re-maining tokens were replaced with ?unk?.Following Mitchell and Lapata (2008), we con-structed a simple semantic space based on co-occurrence statistics from the BLLIP training set.We used the 2,000 most frequent word types ascontexts and a symmetric five word window.
Vec-tor components were defined as in equation (4).Contrary to our earlier work, we did not lemma-tize the corpus before constructing the vectors asin the context of language modeling this was notappropriate.
We also trained the LDA model onBLLIP, using Blei et al?s (2003) implementation.3We experimented with different numbers of topicson the development set (from 10 to 200) and re-port results on the test set with 100 topics.
In ourexperiments, the hyperparameter ?
was initializedto 0.5, and the ?
word probabilities were initial-ized randomly.We integrated our compositional models with atrigram model which we also trained on BLLIP.The model was built using the SRILM toolkit(Stolcke, 2002) with backoff and Good-Turingsmoothing.
Ideally, we would have liked to trainRoark?s (2001) parser on the same data as thatused for the semantic models.
However, this wouldrequire a gold standard treebank several timeslarger than those currently available.
Followingprevious work on structured language modeling(Roark, 2001; Charniak, 2001; Chelba and Jelinek,1998), we therefore trained the parser on sections2?21 of the Penn Treebank containing 936,017words.
Note that Roark?s (2001) parser producesprefix probabilities for each word of a sentencewhich we converted to conditional probabilities bydividing each current probability by the previousone.6 ResultsTable 1 shows perplexity results when the com-positional models are combined with an n-grammodel.
With regard to the simple semantic spacemodel (SSM) we observe that both additive andmultiplicative approaches to constructing historyare successful in reducing perplexity over then-gram baseline, with the multiplicative modeloutperforming the additive one.
This confirms the3Available from http://www.cs.princeton.edu/?blei/lda-c/index.html.436Model Perplexityn-gram 78.72n-gram+AddSSM76.65n-gram + MultiplySSM75.01n-gram+AddLDA76.60n-gram+MultiplyLDA123.93parser 173.35n-gram + parser 75.22n-gram + parser + AddSSM73.45n-gram + parser + MultiplySSM71.32n-gram + parser + AddLDA71.58n-gram + parser + MultiplyLDA87.93Table 1: Perplexities for n-gram, composition andstructured language models, and their combina-tions; subscriptsSSMandLSArefer to the semanticspace and LDA models, respectively.hypothesis that for this type of semantic space themultiplicative vector combination function pro-duces representations which have a sounder prob-abilistic basis.The results for the LDA model are also reportedin the table.
This model reduces perplexity with anadditive composition function, but performs worsethan the n-gram with a multiplicative function.
Forcomparison, Figure 1 plots the perplexity of thecombined LDA and n-gram models against thenumber of topics.
Increasing the number of top-ics produces higher dimensional representationswhich ought to be richer, more detailed and there-fore more predictive.
While this is true for theadditive model, a greater number of topics actu-ally increases the perplexity of the multiplicativemodel, indicating it has become less predictive.We compared these perplexity reductionsagainst those obtained with a structured lan-guage model.
Following Roark (2001), we com-bined the structured language model with atrigram model using linear interpolation (theweights were optimized on the developmentset).
This model (n-gram + parser) performscomparably to our best compositional model(n-gram + MultiplySSM).
While both models in-corporate long range dependencies, the parser istrained on a hand annotated treebank, whereas thecompositional model uses raw text, albeit froma larger corpus.
Interestingly, when interpolatingthe trigram with the parser and the compositionalmodels, we obtain additional perplexity reduc-tions.
This suggests that the semantic models areFigure 1: Perplexity versus Number of Topics forthe LDA models using additive and multiplicativecomposition functions.encoding useful predictive information about longrange dependencies, which is distinct from and po-tentially complementary to the parser?s syntacticinformation about such dependencies.
Note thatthe semantic space multiplicative model yields thehighest perplexity reduction in this suite of exper-iments followed by the LDA additive model.7 ConclusionsIn this paper we advocated the use of vectorcomposition models for language modeling.
Us-ing semantic representations of words outside then-gram window, we enhanced a trigram modelwith longer range dependencies.
We comparedcomposition models based on addition and multi-plication and examined the influence of the under-lying semantic space on the composition task.
Ourresults indicate that the multiplicative compositionfunction produced the most predictive representa-tions with a simple semantic space.
Interestingly,its effect in the LDA setting was detrimental.
In-creasing the representational power of the LDAmodel, by using a greater number of topics, ren-dered the multiplicative model less predictive.These results, together with the basic mathe-matical structure of the LDA model, suggest thatit may not be well suited to forming represen-tations for word sequences.
In particular, the as-sumption that words are generated independentlywithin documents prevents the interactions be-tween words being modeled.
This assumption,along with the Dirichlet prior on document distri-butions tends to lead to highly sparse word vec-437tors, with a typical word being strongly associatedwith only one or two topics.
Multiplication of anumber of these vectors generally produces a vec-tor in which most of these associations have beenobliterated by the sparse components, resulting ina representation with little predictive power.These shortcomings arise from the mathemati-cal formulation of LDA, which is not directed atmodeling the semantic interaction between words.An interesting future direction would be to opti-mize the vector components of the probabilisticmodel over a suitable training corpus, in order toderive a vector model of semantics adapted specif-ically to the task of composition.
We also plan toinvestigate more sophisticated composition mod-els that take syntactic structure into account.
Ourresults on interpolating the compositional mod-els with a parser indicate that there is substantialmileage to be gained by combining syntactic andsemantic dependencies.Acknowledgements We are grateful to BrianRoark for making his parser available to us.Thanks to Frank Keller and Victor Lavrenkofor insightful comments and suggestions.
Thiswork was supported by the Economic and So-cial Research Council [grant number PTA-030-2006-00341] and the Engineering and Physi-cal Sciences Research Council [grant numberGR/T04540/01].ReferencesJerome R. Bellegarda.
2000.
Exploiting latent se-mantic information in statistical language modeling.Proceedings of the IEEE, 88(8):1279?1296.Michael W. Berry, Susan T. Dumais, and Gavin W.O?Brien.
1994.
Using linear algebra for intelligentinformation retrieval.
SIAM Review, 37(4):573?595.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.J.A.
Bullinaria and J.P. Levy.
2007.
Extracting seman-tic representations from word co-occurrence statis-tics: A computational study.
Behavior ResearchMethods, 39:510?526.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of 35th AnnualMeeting of the Association for Computational Lin-guistics and 8th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 116?123, Toulouse, France.Ciprian Chelba and Frederick Jelinek.
1998.
Exploit-ing syntactic structure for language modeling.
InProceedings of the 17th International Conference onComputational Linguistics and 36th Annual Meet-ing of the Association for Computational Linguis-tics, pages 225?231, Montr?eal, Canada.Stephen Clark, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositional distribu-tional model of meaning.
In Proceedings of the2nd Symposium on Quantum Interaction, pages133?140, Oxford, UK.
College Publications.Noah Coccaro and Daniel Jurafsky.
1998.
Towardsbetter integration of semantic predictors in satisticallanguage modeling.
In Proceedings of the 5th Inter-national Conference on Spoken Language Process-ing, pages 2403?2406, Sydney, Australia.Yonggang Deng and Sanjeev Khundanpur.
2003.
La-tent semantic information in maximum entropy lan-guage models for conversational speech recognition.In Proceedings of the 2003 Human Language Tech-nology Conference of the North American Chapterof the Association for Computational Linguistics,pages 56?63, Edmonton, AL.Katrin Erk and Sebastian Pad?o.
2008.
A structuredvector space model for word meaning in context.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages897?906, Honolulu, Hawaii.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930?1955.
In Studies in Linguistic Analysis, pages 1?32.Philological Society, Oxford.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherencewith latent semantic analysis.
Discourse Process,15:285?307.Daniel Gildea and Thomas Hofmann.
1999.
Topic-based language models using EM.
In Proceedings ofthe 6th European Conference on Speech Communi-ation and Technology, pages 2167?2170, Budapest,Hungary.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer Academic Pub-lishers, Norwell, MA, USA.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological Review, 114(2):211?244.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.Thomas Hofmann.
2001.
Unsupervised learningby probabilistic latent semantic analysis.
MachineLearning, 41(2):177?196.Dharmendra Kanejiya, Arun Kumar, and SurendraPrasad.
2004.
Statistical language modeling withperformance benchmarks using various levels of438syntactic-semantic information.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics, pages 1161?1167, Geneva, Switzerland.Walter Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.Roland Kuhn and Renato de Mori.
1992.
A cachebased natural language model for speech recogni-tion.
IEEE Transactions on Pattern Analysis andMachine Intelligence, (14):570?583.T.
K. Landauer and S. T. Dumais.
1997.
A solutionto Plato?s problem: the latent semantic analysis the-ory of acquisition, induction and representation ofknowledge.
Psychological Review, 104(2):211?240.Will Lowe.
2000.
Topographic Maps of SemanticSpace.
Ph.D. thesis, University of Edinburgh.Scott McDonald.
2000.
Environmental Determinantsof Lexical Processing Effort.
Ph.D. thesis, Univer-sity of Edinburgh.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, OH.R.
Montague.
1974.
English as a formal language.
InR.
Montague, editor, Formal Philosophy.
Yale Uni-versity Press, New Haven, CT.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.B.
Partee.
1995.
Lexical semantics and compositional-ity.
In Lila Gleitman and Mark Liberman, editors,Invitation to Cognitive Science Part I: Language,pages 311?360.
MIT Press, Cambridge, MA.S.
Pinker.
1994.
The Language Instinct: How the MindCreates Language.
HarperCollins, New York.Tony A.
Plate.
1995.
Holographic reduced represen-tations.
IEEE Transactions on Neural Networks,6(3):623?641.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Roni Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech and Language, 10:187?228.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structuresin connectionist systems.
Artificial Intelligence,46:159?216.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing, pages 901?904, Denver, CO.Tonio Wandmacher and Jean-Yves Antoine.
2007.Methods to integrate a language model with seman-tic information for a word prediction component.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 506?513, Prague, CzechRepublic.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Proceedings of the2nd Symposium on Quantum Interaction, Oxford,UK.
College Publications.Rong Zhang and Alexander I. Rudnicky.
2002.
Im-prove latent semantic analysis based language modelby integrating multiple level knowldege.
In Pro-ceedings of the 7th International Conference on Spo-ken Language Processing, pages 893?897, Denver,CO.439
