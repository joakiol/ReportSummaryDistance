Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 873?883,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSemantic Frames to Predict Stock Price MovementBoyi Xie, Rebecca J. Passonneau, Leon WuCenter for Computational Learning SystemsColumbia UniversityNew York, NY USA(bx2109|becky|leon.wu)@columbia.eduGerma?n G. CreamerHowe School of Technology ManagementStevens Institute of TechnologyHoboken, NJ USAgcreamer@stevens.eduAbstractSemantic frames are a rich linguistic re-source.
There has been much workon semantic frame parsers, but less thatapplies them to general NLP problems.We address a task to predict change instock price from financial news.
Seman-tic frames help to generalize from spe-cific sentences to scenarios, and to de-tect the (positive or negative) roles of spe-cific companies.
We introduce a novel treerepresentation, and use it to train predic-tive models with tree kernels using sup-port vector machines.
Our experimentstest multiple text representations on twobinary classification tasks, change of priceand polarity.
Experiments show that fea-tures derived from semantic frame pars-ing have significantly better performanceacross years on the polarity task.1 IntroductionA growing literature evaluates the financial effectsof media on the market (Tetlock, 2007; Engel-berg and Parsons, 2011).
Recent work has appliedNLP techniques to various financial media (con-ventional news, tweets) to detect sentiment in con-ventional news (Devitt and Ahmad, 2007; Haiderand Mehrotra, 2011) or message boards (Chuaet al, 2009), or discriminate expert from non-expert investors in financial tweets (Bar-Haim etal., 2011).
With the exception of Bar-Haim et al(2011), these NLP studies have relied on smallcorpora of hand-labeled data for training or evalu-ation, and the connection to market events is doneindirectly through sentiment detection.
We hy-pothesize that conventional news can be used topredict changes in the stock price of specific com-panies, and that the semantic features that bestrepresent relevant aspects of the news vary acrossOn Wednesday, April 11th, 2012, Google Inc announcedits first quarterly earnings report, a week before the April20 options contracts expiration in contrast to its historyof reporting a day before monthly options expirations.The stock price of Google surged 3.85% from April10th?s $626.86 to 12th?s $651.01.
On Friday, April 13th,news reported Oracle Corp would sue Google Inc ,claiming Google?s Android operating system tramples its intellectual property rights .
Jury selection was set forthe next Monday.
Google?s stock price tumbled 4.06% onFriday, and continued to drop in the following week.Figure 1: Summary of financial news items per-taining to Google in April, 2012.market sectors.
To test this hypothesis, we useprice information to label data from six years offinancial news.
Our experiments test several doc-ument representations for two binary classificationtasks, change of price and polarity.
Our main con-tribution is a novel tree representation based onsemantic frame parses that performs significantlybetter than enriched bag-of-words vectors.Figure 1 shows a constructed example basedon extracts from financial news about Google inApril, 2012.
It illustrates how a series of eventsreported in the news precedes and potentiallypredicts a large change in Google?s stock price.Google?s early announcement of quarterly earn-ings possibly presages trouble, and its stock pricefalls soon after reports of a legal action againstGoogle by Oracle.
To produce a coherent story,the original sentences were edited for Figure 1,but they are in the style of actual sentences fromour dataset.
Accurate detection of events and re-lations that might have an impact on stock priceshould benefit from document representation thatcaptures sentiment in lexical items (e.g., aggres-sive) combined with the conceptual relations cap-tured by FrameNet (Ruppenhofer and Rehbein,2012).
A frame is a lexical semantic representa-873tion of the conceptual roles played by parts of aclause, and relates different lexical items (e.g., re-port, announce) to the same situation type.
In thefigure, some of the words that evoke frames havebeen underlined, and role fillers are outlined byboxes or ovals.
Sentiment words are in italics.To the best of our knowledge, this paper isthe first to apply semantic frames in this do-main.
On the polarity task, the semantic frame fea-tures encoded as trees perform significantly betteracross years and sectors than bag-of-words vectors(BOW), and outperform BOW vectors enhancedwith semantic frame features, and a supervisedtopic modeling approach.
The results on the pricechange task show the same trend, but are not sta-tistically significant, possibly due to the volatilityof the market in 2007 and the following severalyears.
Yet even modest predictive performanceon both tasks could have an impact, as discussedbelow, if incorporated into financial models suchas Rydberg and Shephard (2003).
We first dis-cuss the motivation and related work.
Section 4presents vector-based and tree-based features fromsemantic frame parses, and section 5 describes ourdataset.
The experimental design and results ap-pear in the following section, followed by discus-sion and conclusions.2 MotivationFinancial news is a rich vein for NLP applica-tions to mine.
Many news organizations that fea-ture financial news, such as Reuters, the WallStreet Journal and Bloomberg, devote significantresources to the analysis of corporate news.Much of the data that would support studies ofa link between the news media and the market arepublicly available.
As pointed out by Tetlock etal.
(2008), linguistic communication is a poten-tially important source of information about firms?fundamental values.
Because very few stock mar-ket investors directly observe firms?
production ac-tivities, they get most of their information sec-ondhand.
Their three main sources are analysts?forecasts, quantifiable publicly disclosed account-ing variables, and descriptions of firms?
currentand future profit-generating activities.
If analystand accounting variables are incomplete or biasedmeasures of firms?
fundamental values, linguis-tic variables may have incremental explanatorypower for firms?
future earnings and returns.Consider the following sentences:Oracle sued Google in August 2010, sayingGoogle?s Android mobile operating system in-fringes its copyrights and patents for the Java pro-gramming language.
(a)Oracle has accused Google of violating its in-tellectual property rights to the Java programminglanguage.
(b)Oracle has blamed Google and alleged that thelatter has committed copyright infringement re-lated to Java programming language held by Ora-cle.
(c)Oracle?s Ellison says couldn?t sway Google onJava.
(d)Sentences a, b and c are semantically similar,but lexically rather distinct: the shared words arethe company names and Java (programming lan-guage).
Bag-of-Words (BOW) document repre-sentation is difficult to surpass for many documentclassification tasks, but cannot capture the de-gree of semantic similarity among these sentences.Methods that have proven successful for para-phrase detection (Deerwester et al, 1990; Dolanet al, 2004), as in the main clauses of b andc, include latent variable models that simultane-ously capture the semantics of words and sen-tences, such as latent semantic analysis (LSA) orlatent Dirichlet alocation (LDA).
However, ourtask goes beyond paraphrase detection.
The firstthree sentences all indicate an adversarial relationof Oracle to Google involving a negative judge-ment.
It would be useful to capture the similaritiesamong all three of these sentences, and to distin-guish the role of each company (who is suing andwho is being sued).
Further, these three sentencespotentially have a greater impact on market per-ception of Google in contrast to a sentence like d,that refers to the same conflict more indirectly, andwhose main clause verb is say.
We hypothesizethat semantic frames can address these issues.Most of the NLP literature on semantic framesaddresses how to build robust semantic frameparsers, with intrinsic evaluation against gold stan-dard parses.
There have been few applications ofsemantic frame parsing for extrinsic tasks.
To testfor measurable benefits of semantic frame parsing,this paper poses the following questions:1.
Are semantic frames useful for documentrepresentation of financial news?2.
What aspects of frames are most useful?3.
What is the relative performance of documentrepresentation that relies on frames?8744.
What improvements could be made to bestexploit semantic frames?Our work is not aimed at investment profit.Rather, we investigate whether computational lin-guistic methodologies can improve our under-standing of a company?s fundamental marketvalue, and whether linguistic information derivedfrom news produces a consistent enough result tobenefit more comprehensive financial models.3 Related WorkNLP has recently been applied to financial textfor market analysis, primarily using bag-of-words (BOW) document representation.
Lussand d?Aspremont (2008) use text classification tomodel price movements of financial assets on aper-day basis.
They try to predict the directionof return, and abnormal returns, defined as an ab-solute return greater than a predefined threshold.Kogan et al (2009) address a text regression prob-lem to predict the financial risk of investment incompanies.
They analyze 10-K reports to predictstock return volatility.
They also predict whethera company will be delisted following its 10-K re-port.
Ruiz et al (2012) correlate text with finan-cial time series volume and price data.
They findthat graph centrality measures like page rank anddegree are more strongly correlated to both priceand traded volume for an aggregation of similarcompanies, while individual stocks are less corre-lated.
Lavrenko et al (2000) present an approachto identify news stories that influence the behaviorof financial markets, and predict trends in stockprices based on the content of news stories thatprecede the trends.
Luss and d?Aspremont (2008)and Lavrenko et al (2000) both point out the de-sire for document feature engineering as future re-search directions.
We explore a rich feature spacethat relies on frame semantic parsing.Sentiment analysis figures strongly in NLPwork on news.
General Inquirer (GI), a contentanalysis program, is used to quantify pessimism ofnews in Tetlock (2007) and Tetlock et al (2008).Other resources for sentiment detection includethe Dictionary of Affect in Language (DAL) toscore the prior polarity of words, as in Agarwalet al (2011) on social media data.
Our study in-corporates DAL scores along with other features.FrameNet is a rich lexical resource (Fillmore etal., 2003), based on the theory of frame seman-tics (Fillmore, 1976).
There is active researchCategory Features Value typeFrame F, FT, FE Nattributes wF, wFT, wFE R?0BOW UniG, BiG, TriG NwUniG, wBiG, wTriG R?0pDAL all-Pls, all-Act, all-Img R?
?=0,std=1VB-Pls, VB-Act, VB-Img R?
?=0,std=1JJ-Pls, JJ-Act, JJ-Img R?
?=0,std=1RB-Pls, RB-Act, RB-Img R?
?=0,std=1Table 1: FWD features (Frame, bag-of-Words,part-of-speech DAL score) and their value types.to build more accurate parsers (Das and Smith,2011; Das and Smith, 2012).
Semantic role label-ing using FrameNet has been used to identify anopinion with its holder and topic (Kim and Hovy,2006).
For deep representation of sentiment anal-ysis, Ruppenhofer and Rehbein (2012) proposeSentiFrameNet.Our work addresses classification tasks thathave potential relevance to an influential financialmodel (Rydberg and Shephard, 2003).
This modeldecomposes stock price analysis of financial datainto a three-part ADS model - activity (a binaryprocess modeling the price move or not), direction(another binary process modeling the direction ofthe moves) and size (a number quantifying the sizeof the moves).
Our two binary classification tasksfor news, price change and polarity, are analogousto their activity and direction.
In contrast to theADS model, our approach does not calculate theconditional probability of each factor.
At present,our goal is limited to the determination of whetherNLP features can uncover information from newsthat could help predict stock price movement orsupport analysts?
investigations.4 MethodsWe propose two approaches for the use of seman-tic frames.
The first is a rich vector space basedon semantic frames, word forms and DAL affectscores.
The second is a tree representation thatencodes semantic frame features, and depends ontree kernel measures for support vector machineclassification.
The semantic parses of both meth-ods are derived from SEMAFOR1 (Das and Smith,2012; Das and Smith, 2011), which solves the se-mantic parsing problem by rule-based target iden-tification, log-linear model based frame identifica-tion and frame element filling.1http://www.ark.cs.cmu.edu/SEMAFOR.875Frame (F) Judgment comm.
Commerce buyaccuse buyTarget (FT) sue purchasecharge bidFrame COMMUNICATOR BUYERElement EVALUEE SELLER(FE) REASON GOODSTable 2: Sample frames.4.1 Semantic Frame based FWD FeaturesTable 1 lists 24 types of features, including seman-tic Frame attributes, bag-of-Words, and scores forwords in the Dictionary of Affect in Language bypart of speech (pDAL).
We refer to these featuresas FWD features throughout the paper.
FWD fea-tures are used alone and in combinations.FrameNet defines hundreds of frames, each ofwhich represents a scenario associated with se-mantic roles, or frame elements, that serve asparticipants in the scenario the frame signifies.Table 2 shows two frames.
The frame Judg-ment communication (JC or Judgment comm.
inthe rest of the paper) represents a scenario inwhich a COMMUNICATOR communicates a judg-ment of an EVALUEE for some REASON.
It isevoked by (target) words such as accuse or sue.Here we use F for the frame name, FT for thetarget words, and FE for frame elements.
We useboth frequency and weighted scores.
For exam-ple, we define idf -adjusted weighted frame fea-tures, such as wF for attribute F in document d aswFF,d = f(F, d) ?
log |D||d?D:F?d| , where f(F, d)is the frequency of frame F in d, D is the wholedocument set and |?| is the cardinality operator.Bag-of-Words features include term frequencyand tfidf of unigrams, bigrams, and trigrams.DAL (Dictionary of Affect in Language) is apsycholinguistic resource to measure the emo-tional meaning of words and texts (Whissel,1989).
It includes 8,742 words that were anno-tated for three dimensions: Pleasantness (Pls), Ac-tivation (Act), and Imagery (Img).
Agarwal etal.
(2009) introduced part-of-speech specific DALfeatures for sentiment analysis.
We follow theirapproach by averaging the scores for all words,verb only, adjective only, and adverb only words.Feature values are normalized to mean of zero andstandard deviation of one.4.2 SemTree Feature Space and KernelsWe propose SemTree as another feature space toencode semantic information in trees.
SemTreecan distinguish the roles of each company of in-terest, or designated object (e.g.
who is suing andwho is being sued).4.2.1 Construction of Tree RepresentationThe semantic frame parse of a sentence is a forestof trees, each of which corresponds to a semanticframe.
SemTree encodes the original frame struc-ture and its leaf words and phrases, and highlightsa designated object at a particular node as follows.For each lexical item (target) that evokes a frame, abackbone is found by extracting the path from theroot to the role filler mentioning a designated ob-ject; the backbone is then reversed to promote thedesignated object.
If multiple frames have beenassigned to the same designated object, their back-bones are merged.
Lastly, the frame elements andframe targets are inserted at the frame root.The top of Figure 2 shows the semantic parsefor sentence a from section 2; we use it to illus-trate tree construction for designated object Ora-cle.
The parse has two frames (Figure 2-(1)&(2)),one corresponding to the main clause (verb sue),and the other for the tenseless adjunct (verb say).The reversed paths extracted from each frame rootto the designated object Oracle become the back-bones (Figures 2-(3)&(4)).
After merging the twobackbones we get the resulting SemTree, as shownin Figure 2-(5).
By the same steps, this sentencewould also yield a SemTree with Google at theroot, in the role of EVALUEE.4.2.2 Kernels and Tree SubstructuresThe tree kernel (Moschitti, 2006; Collins andDuffy, 2002) is a function of tree similarity, basedon common substructures (tree fragments).
Thereare two types of substructures.
A subtree (ST) isdefined as any node of a tree along with all its de-scendants.
A subset tree (SST) is defined as anynode along with its immediate children and, op-tionally, part or all of the children?s descendants.Each tree is represented by a d dimensional vec-tor where the i?th component counts the numberof occurrences of the i?th tree fragment.Define the function hi(T ) as the number ofoccurrences of the i?th tree fragment in treeT , so that T is now represented as h(T ) =(h1(T ), h2(T ), ..., hd(T )).
We define the set ofnodes in tree T1 and T2 as NT1 and NT2 respec-tively.
We define the indicator function Ii(n) to be1 if subtree i is seen rooted at node n, and 0 oth-erwise.
It follows that hi(T1) = ?n1?NT1 Ii(n1)876Designated object: Oracle (ORCL)Sentence: Oracle sued Google in August 2010, saying Google?s Android mobile operating system infringes its copyrights and patents for the Java pro-gramming language.SRL: [OracleJC.FE.Communicator,Stmt.FE.Speaker] [suedJC.Target] [GoogleJC.FE.Evaluee] in August 2010, [sayingStmt.Target][Googles?
Android mobile operating system infringes its copyrights and patents for the Java programming languageStmt.FE.Message].
(1) Judgment comm.FE.EvalueeGOOGFE.CommunicatorORCLJudgment comm.Targetsue(2) StatementFE.MessageGOOG?s Android ... languageFE.SpeakerORCLStatement.Targetsay(3) ORCLFE.CommunicatorJudgment comm.
(4) ORCLFE.SpeakerStatement(5) ORCLSpeakerStatementFE.MessageFE.SpeakerStatement.TargetsayCommunicatorJudgment comm.FE.EvalueeFE.CommunicatorJudgment comm.TargetsueFigure 2: Constructing a tree representation for the designated object Oracle in sentence shown.and hi(T2) = ?n2?NT2 Ii(n2).
Their similaritycan be efficiently computed by the inner product,K(T1, T2) = h(T1) ?
h(T2)=?i hi(T1)hi(T2)=?i(?n1?NT1Ii(n1))(?n2?NT2Ii(n2))=?n1?NT1?n2?NT2?i Ii(ni)Ii(n2)=?n1?NT1?n2?NT2?
(n1, n2)where ?
(n1, n2) is the number of common frag-ments rooted in the nodes n1 and n2.
If the pro-ductions of these two nodes (themselves and theirimmediate children) differ, ?
(n1, n2) = 0; other-wise iterate their children recursively to evaluate?
(n1, n2) =?|children|j (?+?
(cjn1 , cjn2)) , where?
= 0 for ST kernel and ?
= 1 for SST kernel.The kernel computational complexity isO(|NT1 | ?
|NT2 |), where all pairwise compar-isons are carried out between T1 and T2.
However,there are fast algorithms for kernel computationthat run in linear time on average, either bydynamic programming (Collins and Duffy, 2002),or pre-sorting production rules before training(Moschitti, 2006).
We use the latter.5 DatasetWe use publicly available financial news fromReuters from January 2007 through August 2012.This time frame includes a severe economic down-turn in 2007-2010 followed by a modest recoveryin 2011-2012.An information extraction pipeline is used topre-process the data.
News full text is extractedfrom HTML.
The timestamp of the news is ex-tracted for a later alignment with stock price infor-mation, which will be discussed in section 6.
Thecompany mentioned is identified by a rule-basedmatching of a finite list of companies.There are a total of 10 sectors in the Global In-dustry Classification Standard (GICS), an industrytaxonomy used by the S&P 500.2 To explore ourapproach for this domain, we select three sectorsfor our experiment: Telecommunication Services(TS, the sector with the smallest number of com-panies), Information Technology (IT), and Con-sumer Staples (CS), due to our familiarity with thecompanies in these sectors and an expectation ofdifferent characteristics they may exhibit.
In theexpectation there would be semantic differencesassociated with these sectors, experiments are per-formed independently for each sector.
There arealso differences in the number of companies in thesector, and the amount of news.We bin news articles by sector.
We remove ar-ticles that only list stock prices or only show ta-bles of accounting reports.
The first preprocess-ing step is to extract sentences that mention the2Standard & Poor?s 500 is an equity market index thatincludes 500 U.S. leading companies in leading industries.877CS (N=40) IT (N=69) TS (N=8)avg # news 5,702?749 13446?1,272 2,177?188avg # sentences 16,090?2,316 48,929?5,927 6,970?1,383avg # com./sent.
1.07?0.01 1.06?0.20 1.14?0.03avg # total 17,131?2,339 51,306?8,637 7,947?1,576Table 3: Data statistics of mean and standard devi-ation by year from January 2007 to August 2012,for three sectors, with the number of companies.relevant companies.
Each data instance is a sen-tence and one of the target companies it mentions.Table 3 summarizes the data statistics.
For exam-ple, the consumer staples sector has 40 companies.It has an average of 5,702 news articles (16,090sentences) per year.
Each sentence that mentionsa consumer staple company mentions 1.07 com-panies on average.
On average, this sector has17,131 instances per year.6 ExperimentsOur current experiments are carried out for eachyear, training on one year and testing on the next.The choice to use a coarse time interval with nooverlap was an expedience to permit more numer-ous exploratory experiments, given the computa-tional resources these experiments require.
We testthe influence of news to predict (1) a change instock price (change task), and (2) the polarity ofchange (increase vs. decrease; polarity task).
Ex-periments evaluate the FWD and SemTree featurespaces compared to two baselines: bag-of-words(BOW) and supervised latent Dirichlet alocation(sLDA) (Blei and McAuliffe, 2007).
BOW in-cludes features of unigram, bigram and trigram.sLDA is a statistical model to classify documentsbased on LDA topic models, using labeled data.
Ithas been applied to and shown good performancein topical text classification, collaborative filter-ing, and web page popularity prediction problems.6.1 Labels, Evaluation Metrics, and SettingsWe align publicly available daily stock price datafrom Yahoo Finance with the Reuters news us-ing a method to avoid back-casting.
In particular,we use the daily adjusted closing price - the pricequoted at the end of a trading day (4PM US East-ern Time), then adjusted by dividends, stock split,and other corporate actions.
We create two typesof labels for news documents using the price data,to label the existence of a change and the direc-tion of change.
Both tasks are treated as binaryclassification problems.
Based on the finding ofa one-day delay of the price response to the in-formation embedded in the news by Tetlock et al(2008), we use ?t = 1 in our experiment.
Toconstrain the number of parameters, we also use athreshold value (r) of a 2% change, based on thedistribution of price changes across our data.
Infuture work, this could be tuned to sector or time.change={+1 if |pt(0)+?t?pt(?1)|pt(?1) > r?1 otherwisepolarity={+1 if pt(0)+?t > pt(?1) and change = +1?1 if pt(0)+?t < pt(?1) and change = +1pt(?1) is the adjusted closing price at the end ofthe last trading day, and pt(0)+?t is the price ofthe end of the trading day after the ?t day delay.Only the instances with changes are included inthe polarity task.There is high variance across years in the pro-portion of positive labels, and often highly skewedclasses in one direction or the other.
The averageratios of +/- classes for change and polarity overthe six years?
data are 0.73 (std=0.35) and 1.12(std=0.25), respectively.
Because the time framefor our experiments includes an economic crisisfollowed by a recovery period, we note that theratio between increase and decrease of price flipsbetween 2007, where it is 1.40, and 2008, where itis 0.71.
Accuracy is very sensitive to skew: when aclass has low frequency, accuracy can be high us-ing a baseline that makes prediction on the major-ity class.
Given the high data skew, and the largechanges from year to year in positive versus nega-tive skew, we use a more robust evaluation metric.Our evaluation relies on the Matthews corre-lation coefficient (MCC, also known as the ?-coefficient) (Matthews, 1975) to avoid the bias ofaccuracy due to data skew, and to produce a ro-bust summary score independent of whether thepositive class is skewed to the majority or minor-ity.
In contrast to f-measure, which is a class-specific weighted average of precision and recall,and whose weighted version depends on a choiceof whether the class-specific weights should comefrom the training or testing data, MCC is a sin-gle summary value that incorporates all 4 cells ofa 2 ?
2 confusion matrix (TP, FP, TN and FN forTrue or False Positive or Negative).
We have alsoobserved that MCC has a lower relative standarddeviation than f-measure.For a 2 ?
2 contingency table, MCC corre-sponds to the square root of the average ?2 statis-tic ?
?2/n, with values in [-1,1].
It has been sug-878Changetest years BOW sLDA FWD SemTreeFWDConsumer Staples2008-2010 0.1015 0.0774 0.1079 0.14262011-2012 0.1663 0.1203 0.1664 0.17365 years 0.1274 0.0945 0.1313 0.1550Information Technology2008-2010 0.0580 0.0585 0.0701 0.08462011-2012 0.0894 0.0681 0.1076 0.12735 years 0.0705 0.0623 0.0851 0.1017Telecommunication Services2008-2010 0.1501 0.1615 0.1497 0.24092011-2012 0.2256 0.2084 0.2191 0.40095 years 0.1803 0.1803 0.1774 0.3049PolarityConsumer Staples2008-2010 0.0359 0.0383 0.0956 0.10542011-2012 0.0938 0.0270 0.1131 0.12855 years 0.0590 0.0338 0.1026 0.1147p-value >>0.1000 0.0918 0.0489Information Technology2008-2010 0.0551 0.0332 0.0697 0.07632011-2012 0.0591 0.0516 0.0764 0.08575 years 0.0567 0.0405 0.0723 0.0801p-value 0.0626 0.0948 0.0103Telecommunication Services2008-2010 0.0402 0.0464 0.0821 0.07452011-2012 0.0366 0.0781 0.0611 0.08095 years 0.0388 0.0591 0.0737 0.0770p-value >>0.1000 0.0950 0.0222Table 4: Average MCC for the change and polaritytasks by feature representation, for 2008-2010; for2011-2012; for all 5 years and associated p-valuesof ANOVAs for comparison to BOW.gested as one of the best methods to summarizeinto a single value the confusion matrix of a binaryclassification task (Jurman and Furlanello, 2010;Baldi et al, 2000).
Given the confusion matrix(TP FNFP TN) :MCC = TP ?TN?FP ?FN?
(TP+FP )(TP+FN)(TN+FP )(TN+FN).All sentences with at least one company men-tion are used for the experiment.
We removestop words and use Stanford CoreNLP for part-of-speech tagging and named entity recognition.Models are constructed using linear kernel sup-port vector machines for both classification tasks.SVM-light with tree kernels3 (Joachims, 2006;Moschitti, 2006) is used for both the FWD andSemTree feature spaces.6.2 ResultsTable 4 shows the mean MCC values for each task,for each sector.
Separate means are shown forthe test years of financial crisis (2008-2010) andeconomic recovery (2011-2012) to highlight thedifferences in performance that might result frommarket volatility.3SVM-light: http://svmlight.joachims.org and TreeKernels in SVM-light: http://disi.unitn.it/moschitti/Tree-Kernel.htm.pos.
1 dow, investors, index, retail, datapos.
2 costs, food, price, prices, named entity 4neu.
1 q3, q1, nov, q2, aprneu.
2 cents, million, share, year, quarterneg.
1 cut, sales, prices, hurt, disappointingneg.
2 percent, call, company, fell, named entity 7Table 5: Sample sLDA topics for consumer staplesfor test year 2010 (train on 2009), polarity task.SemTree combined with FWD (SemTreeFWD)generally gives the best performance in bothchange and polarity tasks.
SemTree results hereare based on the subset tree (SST) kernel, be-cause of its greater precision in computing com-mon frame structures and consistently better per-formance over the subtree (ST) kernel.
SemTreealso provides interpretable features for manualanalysis as discussed in the next section.Analysis of Variance (ANOVA) tests were per-formed on the full 5 years for each sector, to com-pare each feature representation as a predictor ofMCC score with the baseline BOW.
The ANOVAsyield the p-values shown in Table 4.
There were nosignificant differences from BOW on the changetask.
For polarity detection, SemTreeFWD wassignificantly better than BOW for each sector (seeboldface p-values).
No other method was sig-nificantly better than BOW, although FWD ap-proaches significance on all sectors, and sLDA ap-proaches significance on IT.sLDA has promising MCC scores for thetelecommunication sector, which has only 8 com-panies, thus many fewer data instances.
Table 5displays a sample of sLDA topics with good per-formance on polarity for the consumer staples sec-tor for training year 2009.
The positive topics arerelated to stock index details and retail data.
Thenegative topics contain many words with negativesentiment (e.g., hurt, disappointing).7 Discussion7.1 Semantic Parse QualityIn general, SEMAFOR parses capture most ofthe important frames for our purposes.
There is,however, significant room for improvement.
Ona small, randomly selected sample of sentencesfrom all three sectors, two of the authors workingindependently evaluated the semantic parses, withapproximately 80% agreement.
Some of the in-accuracies in frame parses result from errors priorto the SEMAFOR parse, such as tokenization or879+ (Target(jump))+ (RECIPIENT(Receiving))+ (VICTIM(Defend))+ (PERCEIVER AGENTIVE(Perception active(Target)(PERCEIVER AGENTIVE)(PHENOMENON)))+ (DONOR(Giving(Target)(THEME)(DONOR)))+ (Target(beats))...- (PHENOMENON(Perception active(Target)(PERCEIVERAGENTIVE)(PHENOMENON)))- (TRIGGER(Response))- (Target(cuts))- (VICTIM(Cause harm(Target(hurt))(VICTIM)))Figure 3: Best performing SemTree fragments forincrease (+) and decrease (-) of price for consumerstaples sector across training years.dependency parsing errors.
The average sentencelength for the sample was 33.3 words, with an av-erage of 14 frames per sentence, 3 of them with aGICS company as a role filler.
Because SemTreeencodes only the frames containing a designatedobject (company), these are the frames we eval-uated.
On average, about half the frames witha designated object were correct, and two thirdsof those frames we judged to be important.
Be-sides errors due to incorrect tokenization or depen-dency parsing, we observed that about 8% to 10%of frames were incorrectly assigned to due wordsense ambiguity.7.2 Feature AnalysisThe experimental results show the SemTree spaceto be the one representation tested here that is sig-nificantly better than BOW, but only for the po-larity task.
Post hoc analysis indicates this maybe due to the aptness of semantic frame parsingfor polarity.
Limitations in our treatment of timepoint to directions for improvement regarding thechange task.Some strengths of our approach are the separatetreatment of different sectors, and the benefits ofSemTree features.
To analyze which were the bestperforming features within sectors, we extractedthe best performing frame fragments for the po-larity task using a tree kernel feature engineeringmethod presented in Pighin and Moschitti (2009).The algorithm selects the most relevant features inaccordance with the weights estimated by SVM,and uses these features to build an explicit repre-sentation of the kernel space.
Figure 3 shows thebest performing SemTree fragments of the polar-ity task for the consumer staples sector.Recall that we hypothesized differences insemantic frame features across sectors.
Thisshows up as large differences in the strengthof features across sectors.
More strikingly, thesame feature can differ in polarity across sec-tors.
For example, in consumer staples, (EVAL-UEE(Judgment communication)) has positive po-larity, compared with negative polarity in informa-tion technology sector.
The examples we see indi-cate that the positive cases pertain to aggressive re-tail practices that lead to lawsuits with only smallfines, but whose larger impact benefits the bottomline.
A typical case is the sentence, The plaintiffsaccused Wal-Mart of discriminating against dis-abled customers by mounting ?point-of-sale?
ter-minals in many stores at elevated heights that can-not be reached.
Lawsuits in the IT sector, on theother hand, are often about technology patent dis-putes, and are more negative, as illustrated by ourexample sentence in Figure 2.SemTree features capture the differences be-tween semantic roles for the same frame, and be-tween the same semantic role in different frames.For example, the PERCEIVER AGENTIVE role ofthe Perception active frame contributes to predic-tion of an increase in price, as in R.J. Reynoldsis watching this situation closely and will respondas appropriate.
Conversely, a company that fillsthe PHENOMENON role of the same frame con-tributes to prediction of a price decrease, as in In-vestors will get a clearer look at how the marketvalues the Philip Morris tobacco businesses whenAltria Group Inc. ?when-issued?
shares begintrading on Tuesday.
When a company fills theVICTIM role in the Cause harm frame, this canpredict a decrease in price, as in Hershey hasbeen hurt by soaring prices for cocoa, energy andother commodities, whereas filling the VICTIMrole in the Defend frame is associated with an in-crease in price, as in At Berkshire?s annual share-holder meeting earlier this month, Warren Buffettdefended Wal-Mart , saying the scandal did notchange his opinion of the company.One weakness of our approach that we dis-cussed above is that there is a strong effect oftime that we do not address.
The same SemTreefeature can be predictive for one time period andnot for another.
(GOODS(Commerce sell)) is re-lated to a decrease in price for 2008 and 2009 butto an increase in price for 2010-2012.
There isclearly an influence of the overall economic con-text that we do not take into account.
For example,880the practices of acquiring or selling a business aredifferent in downturning versus recovering mar-kets.
An important observation of the MCC val-ues, especially in the case of SemTreeFWD is thatMCC increases during the years 2011-2012.
Weattribute this change to the difficulty of predictingstock price trends when there is the high volatil-ity typical of a financial crisis.
The effect of newson volatility, however, can be explored indepen-dently.
For example, Creamer et al (2012) detecta strong association.Another weakness of our approach is that wetake sentences out of context, which can leadto prediction errors.
For example, the sentenceLongs?
real estate assets alone are worth some$2.9 billion, or $71.50 per share, Ackman wrote,meaning that CVS would essentially be payingfor real estate, but gaining Longs?
pharmacy ben-efit management business and retail operations forfree is treated as predicting a positive polarity forCVS.
This would be accurate if CVS was actuallygoing to acquire Longs?
business.
Later in thesame news item, however, there is a sentence indi-cating that the sale will not go through, which pre-dicts negative polarity for CVS: Pershing SquareCapital Management said on Thursday it won?tsupport a tender offer from CVS Caremark Corpfor rival Longs Drug Stores Corp because the of-fer price ?materially understates the fair value ofthe company,?
according to a filing.8 ConclusionWe have presented a model for predicting stockprice movement from news.
We proposed FWD(Frames, BOW, and part-of-speech specific DAL)features and SemTree data representations.
Oursemantic frame-based model benefits from treekernel learning using support vector machines.The experimental results for our feature represen-tation perform significantly better than BOW onthe polarity task, and show promise on the changetask.
It also facilitates human interpretable analy-sis to understand the relation between a company?smarket value and its business activities.
The sig-nals generated by this algorithm could improve theprediction of a financial time series model, such asADS (Rydberg and Shephard, 2003).Our future work will consider the contextual in-formation for sentence selection, and an aggrega-tion of weighted news content based on the decayeffect over time for individual companies.
We planto use a moving window for training and testing.We will also explore different labeling methods,such as a threshold for price change tuned by sec-tors and background economics.9 AcknowledgementsThe authors thank the anonymous reviewers fortheir insightful comments.ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.2009.
Contextual phrase-level polarity analysis us-ing lexical affect scoring and syntactic N-grams.
InProceedings of the 12th Conference of the Euro-pean Chapter of the ACL (EACL 2009), pages 24?32, Athens, Greece, March.
Association for Compu-tational Linguistics.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of twitter data.
In Proceedings of the Work-shop on Languages in Social Media, LSM ?11, pages30?38.
Association for Computational Linguistics.Pierre Baldi, S?ren Brunak, Yves Chauvin, Claus A. F.Andersen, and Henrik Nielsen.
2000.
Assessing theaccuracy of prediction algorithms for classification:an overview.
Bioinformatics, 16:412 ?
424.Roy Bar-Haim, Elad Dinur, Ronen Feldman, MosheFresko, and Guy Goldstein.
2011.
Identifyingand following expert investors in stock microblogs.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1310?1319, Edinburgh, Scotland, UK., July.
Asso-ciation for Computational Linguistics.David M. Blei and Jon D. McAuliffe.
2007.
Super-vised topic models.
In Advances in Neural Informa-tion Processing Systems, Proceedings of the Twenty-First Annual Conference on Neural InformationProcessing Systems, Vancouver, British Columbia,Canada, December 3-6.Christopher Chua, Maria Milosavljevic, and James R.Curran.
2009.
A sentiment detection engine forinternet stock message boards.
In Proceedings ofthe Australasian Language Technology AssociationWorkshop 2009, pages 89?93, Sydney, Australia,December.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, ACL ?02, pages 263?270, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.881Germa?n G. Creamer, Yong Ren, and Jeffrey V. Nicker-son.
2012.
A Longitudinal Analysis of Asset Re-turn, Volatility and Corporate News Network.
InBusiness Intelligence Congress 3 Proceedings.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised frame-semantic parsing for unknownpredicates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1,HLT ?11, pages 1435?1444, Stroudsburg, PA, USA.Association for Computational Linguistics.Dipanjan Das and Noah A. Smith.
2012.
Graph-basedlexicon expansion with sparsity-inducing penalties.In HLT-NAACL, pages 677?687.
The Associationfor Computational Linguistics.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science.Ann Devitt and Khurshid Ahmad.
2007.
Sentimentpolarity identification in financial news: A cohesion-based approach.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 984?991, Prague, Czech Republic,June.
Association for Computational Linguistics.William Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.Proceedings of the 20th International Conference onComputational Linguistics.Joseph Engelberg and Christopher A. Parsons.
2011.The causal impact of media in financial markets.Journal of Finance, 66(1):67?97.Charles J. Fillmore, Christopher R. Johnson, andMiriam R. L. Petruck.
2003.
Background toFramenet.
International Journal of Lexicography,16(3):235?250, September.Charles J. Fillmore.
1976.
Frame semantics andthe nature of language.
Annals of the New YorkAcademy of Sciences, 280(1):20?32.Syed Aqueel Haider and Rishabh Mehrotra.
2011.Corporate news classification and valence predic-tion: A supervised approach.
In Proceedings ofthe 2nd Workshop on Computational Approaches toSubjectivity and Sentiment Analysis (WASSA 2.011),pages 175?181, Portland, Oregon, June.
Associationfor Computational Linguistics.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, KDD ?06, pages 217?226, NewYork, NY, USA.
ACM.Giuseppe Jurman and Cesare Furlanello.
2010.
A uni-fying view for performance measures in multi-classprediction.
ArXiv e-prints.Soo-Min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text.
In Proceedings of the Work-shop on Sentiment and Subjectivity in Text, SST ?06,pages 1?8, Stroudsburg, PA, USA.
Association forComputational Linguistics.Shimon Kogan, Dimitry Levin, Bryan R. Routledge,Jacob S. Sagi, and Noah A. Smith.
2009.
Pre-dicting risk from financial reports with regression.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, NAACL ?09, pages 272?280, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Victor Lavrenko, Matt Schmill, Dawn Lawrie, PaulOgilvie, David Jensen, and James Allan.
2000.Mining of concurrent text and time series.
In In pro-ceedings of the 6th ACM SIGKDD Int?l Conferenceon Knowledge Discovery and Data Mining Work-shop on Text Mining, pages 37?44.Ronny Luss and Alexandre d?Aspremont.
2008.
Pre-dicting abnormal returns from news using text clas-sification.
CoRR, abs/0809.2792.Brian W. Matthews.
1975.
Comparison of the pre-dicted and observed secondary structure of t4 phagelysozyme.
Biochimica et Biophysica Acta (BBA) -Protein Structure, 405(2):442 ?
451.Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In In Proceed-ings of the 11th Conference of the European Chapterof the Association for Computational Linguistics.Daniele Pighin and Alessandro Moschitti.
2009.
Re-verse engineering of tree kernel feature spaces.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2009, 6-7 August 2009, Singapore, pages 111?120.Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo,Aristides Gionis, and Alejandro Jaimes.
2012.
Cor-relating financial time series with micro-bloggingactivity.
In Proceedings of the fifth ACM interna-tional conference on Web search and data mining,WSDM ?12, pages 513?522, New York, NY, USA.ACM.Josef Ruppenhofer and Ines Rehbein.
2012.
Se-mantic frames as an anchor representation for sen-timent analysis.
In Proceedings of the 3rd Work-shop in Computational Approaches to Subjectivityand Sentiment Analysis, WASSA ?12, pages 104?109, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Tina H. Rydberg and Neil Shephard.
2003.
Dynam-ics of Trade-by-Trade Price Movements: Decompo-sition and Models.
Journal of Financial Economet-rics, 1(1):2?25.882Paul C. Tetlock, Maytal Saar-Tsechansky, and SofusMacskassy.
2008.
More than Words: QuantifyingLanguage to Measure Firms?
Fundamentals.
TheJournal of Finance.Paul C. Tetlock.
2007.
Giving Content to Investor Sen-timent: The Role of Media in the Stock Market.
TheJournal of Finance.Cynthia M. Whissel.
1989.
The dictionary of affect inlanguage.
Emotion: Theory, Research, and Experi-ence, 39(4):113?131.883
