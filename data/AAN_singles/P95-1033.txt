An Algorithm for Simultaneously Bracketing Parallel Textsby Aligning WordsDekai  WuHKUSTDepartment of  Computer ScienceUniversity of  Science & TechnologyClear Water Bay, Hong Kongdekai@cs, ust.
hkAbstractWe describe agrammarless method for simul-taneously bracketing both halves of a paral-lel text and giving word alignments, assum-ing only a translation lexicon for the languagepair.
We introduce inversion-invariant trans-duction grammars which serve as generativemodels for parallel bilingual sentences withweak order constraints.
Focusing on Wans-duction grammars for bracketing, we formu-late a normal form, and a stochastic versionamenable to a maximum-likelihood bracketingalgorithm.
Several extensions and experimentsare discussed.1 In t roduct ionParallel corpora have been shown to provide an extremelyrich source of constraints for statistical analysis (e.g.,Brown et al 1990; Gale & Church 1991; Gale et al 1992;Church 1993; Brown et al 1993; Dagan et al 1993;Dagan & Church 1994; Fung & Church 1994; Wu &Xia 1994; Fung & McKeown 1994).
Our thesis in thispaper is that he lexical information actually gives suffi-cient information toextract not merely word alignments,but also bracketing constraints for both parallel texts.Aside from purely linguistic interest, bracket structurehas been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (Pereira & ~ 1992;Black et al 1993).
Previous algorithms for automaticbracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied totagged text (Magerumn & Marcus 1990).Algorithms for word alignment attempt to find thematching words between parallel sentences.
1 Althoughword alignments are of little use by themselves, theyprovide potential anchor points for other applications,or for subsequent learning stages to acquire more inter-esting structures.
Our technique views word alignment1 Wordmatching is a more accurate rm than word alignmentsince the matchings may cross, but we follow the literature.and bracket annotation for both parallel texts as an inte-grated problem.
Although the examples and experimentsherein are on Chinese and English, we believe the modelis equally applicable to other language pairs, especiallythose within the same family (say Indo-European).Our bracketing method is based on a new formalismcalled an inversion.invariant transduction grammar.
Bytheir nature inversion-invariant transduction grammarsovergenerate, because they permit oo much constituent-ordering freedom.
Nonetheless, they turn out to be veryuseful for recognition when the true grammar isnot fullyknown.
Their purpose is not to flag ungrammatical in-pots; instead they assume that he inputs are grammatical,the aim being to extract structure from the input data, inkindred spirit with robust parsing.2 Inversion-Invariant TransductionGrammarsA Wansduction grammar is a bilingual model that gen-erates two output streams, one for each language.
Theusual view of transducers a  having one input stream andone output stream is more appropriate for restricted ordeterministic f nite-state machines.
Although finite-statetransducers have been well studied, they are insufficientlypowerful for bilingual models.
The models we considerhere are non-deterministic models where the two lan-guages' role is symmetric.We begin by generalizing transduction to context-freeform.
In a context-free transduction grammar, terminalsymbols come in pairs that~ are emitted to separate outputstreams.
It follows that each rewrite rule emits not onebut two streams, and that every non-terminal stands fora class of derivable substring pairs.
For example, in therewrite ruleA ~ B x /y  C z/ethe terminal symbols z and z are symbols of the languageLx and are emitted on stream 1, while the terminal symboly is a symbol of the language L2 and is emitted on stream2.
This rule implies that z /y  must be a valid entry inthe translation lexicon.
A matched terminal symbol pairsuch as z /y  is called a couple.
As a spe,Aal case, thenull symbol e in either language means that no output244SPPNPNNVPWProDetClassPrepNVNP VPPrep NPPro I Det Class NNModN \[ NNPPVV \[ VV NN I VP PPV \] Adv VI/~ I you/f$~-* for/~~.
book/nFigure 1: Example IITG.token is generated.
We call a symbol pair such as x/e anLl-singleton, and ely an L2-singleton.We can employ context-free transduction grammars insimple attempts at generative models for bilingual sen-tence pairs.
For example, pretend for the moment thatthe simple ttansduetion grammar shown in Figure 1 is acontext-free transduction grammar, ignoring the ~ sym-bols that are in place of the usual ~ symbols.
This gram-mar generates the following example pair of English andChinese sentences in translation:(1) a.
\[I \[\[took \[a book\]so \]vp \[for yon\]~ \]vp \]sb.
\[~i \ [ \ [~T \[--*W\]so \]w \ [~\ ]~ \]vt, \]sEach instance of a non-terminal here actually derivestwo subsltings, one in each of the sentences; these twosubstrings are translation counterparts.
This suggestswriting the parse trees together:(2) ~ \[\[took/~Y \[a/~ d~: book/1\[\]so \]vp \[for/~\[~you/~\]pp \]vv \]sThe problem with context-free transduction granunarsis that, just as with finite-state ransducers, both sentencesin a translation pair must share exactly the same gram-matic~d structure (except for optional words that can behandled with lexical singletons).
For example, the fol-lowing sentence pair with a perfectly valid, alternativeChinese translation cannot be generated:(3) a.
\[I \[\[took \[a book\]so \]vp \[for you\]v~ \]vP \]sb.
\[~ \[\[~?~\]~ \ [~T  \ [ - -~\ ]so  \]vt, \]vP \]sWe introduce the device of an inversion-invafiant trans-duction grammar (IITG) to get around the inflexibility ofcontext-free txansduction grammars.
Productions are in-terpreted as rewrite rules just as with context-free trans-duction grammars, with one additional proviso: whengenerating output for stream 2, the constituents on arule's right-hand side may be emitted either left-to-right(as usual) or right-to-left (in inverted order).
We useinstead of --~ to indicate this.
Note that inversion ispermitted at any level of rule expansion.With this simple proviso, the transduction grammar ofFigure 1 straightforwardly generates sentence-pair (3).However, the IITG's weakened ordering constraints nowalso permit the following sentence pairs, where someconstituents have been reversed:(4) & *\[I \[\[for youlpp \[\[a bookl~p tooklvp \]vp \]sb.
\ [~ \[\[~?~\]1~ \[~tT \[--:*:It\]so \]w \]vp \]s(5) a.
*\[\[\[yon for\]re \[\[a book\]so took\]w \]vp I\]sb.
* \ [~ \ [ \ [~\ ] rp  \[\[tl\[:~--\]so ~T\ ]vP  \]VP \]SAs a bilingual generative linguistic theory, therefore,IITGs are not well-motivated (at least for most naturallanguage pairs), since the majority of constructs do nothave freely revexsable constituents.We refer to the direction of a production's L2 con-stituent ordering as an orientation.
It is sometimes usefulto explicitly designate one of the two possible orienta-tions when writing productions.
We do this by dis-tinguishing two varieties of concatenation perators onstring-pairs, depending on tim odeatation.
Tim operator\[\] performs the "usual" paitwise concatenation so that\[ A B\] yields the string-pair ( Cx , C2 ) where Cx = A1Bxand (52 = A2B2.
But the operator 0 concatema~ con-stituents on output stream 1 while reversing them onstream 2, so that Ci = AxBx but C2 = B2A2.
Forexample, the NP .-.
Det Class NN rule in the transduc-tion grammar above actually expands to two standardrewrite rules:- .
\[Bet NN\](DetClass NN)Before turning to bracketing, we take note of threelemmas for IITGs (proofs omitted):Lemma l For any inversion-invariant ransductiongrammar G, there exists an equivalent inversion-invariant transduction grammar G' where T(G) =T( G'), such that:1. l fe E LI(G) and e E L2(G), then G' contains asingle production of the form S' --~ e / c, where S' isthe start symbol of G' and does not appear on theright-hand side of any production of G' ;2. otherwise G' contains no productions of the formA ~ e/e.Lemma2 For any inversion-invariant ransductiongrammar G, there exists an equivalent inversion-invariant transduction gratrm~r G' where T(G) =T(G'), T(G) = T(G'), such that the right-hand sideof any production of G' contains either a single terminal-pair or a list of nonterminals.Lemma3 For any inversion-invariant ransductiongrammar G, there exists an equivalent inversion trans-duction grammar G' where T( G) = T( G'), such that G'does not contain any productions of the form A --, B.3 Bracketing Transduction GrammarsFor the remainder of this paper, we focus our attentionon pure bracketing.
We confine ourselves to bracketing245transduction grammars (BTGs), which are IITGs whereconstituent categories ate not differentiated.
Aside fromthe start symbol S, BTGs contain only one non-terminalsymbol, A, which rewrites either ecursively as a stringof A's or as a single terminal-pair.
In the former case, theproductions has the form A ~-, A !
where we use A !
to ab-breviate A .
.
.
A, where thefanout f denotes the numberof A's.
Each A corresponds toa level of bracketing andcan be thought of as demarcating some unspecified kindof syntactic category.
(This same "repetitive expansion"restriction used with standard context-free grammars andtransduetion grammars yields bracketing grammars with-out orientation i variauce.
)A full bracketing transduction grammar of degree fcontains A productions of every fanout between 2 andf ,  thus allowing constituents of any length up to f .
Inprinciple, a full BTG of high degree is preferable, hav-ing the greatest flexibility to acx~mmdate arbitrarily longmatching sequences.
However, the following theoremsimplifies our algorithms by allowing us to get away withdegree-2 BTGs.
I ~t~ we will see how postprocessingrestores the fanout flexibility (Section 5.2).Theorem 1 For any full bracketing transduction gram-mar T, there exists an equivalent bracketing transductiongrammar T' in normal form where every production takesone of the followingforms:S ~ e /eS ~ AA ~ AAA ~ z /yA ~ ~:/eA ~ elyProof By Lemmas 1, 2, and 3, we may assume Tcontains only productions of the form S ~-* e/e, Az /y ,  A ~ z/e, A ~-* e/y, and A ,--* AA .
.
.
A.
For proofby induction, we need only show that any full BTG T ofdegree f > 2 is equivalent to a full BTG T' of degreef -  1.
It suffices to show that he production A ~-, A !
callbe removed without any loss to the generated language,i.e., tha!
the remaining productions in T' can still deriveany string-pair derivable by T (removing a productioncannot increase the set of derivable string-pairs).
Let(E, C) be any siring-pair derivable from A ~ A 1, whereE is output on stream 1 and C on stream 2.
DefineE i as the substring of E derived from the ith A of theproduction, and similarly define C i.
There are two casesdepending on the concatenation rientation, but (E, C)is derivable by T' in either case.In the first case, if the derivation used was A ..-, \[A!\],thenE = E 1 .
.
.E  l andC = C1.
.
.C  1.
Let (E ' ,C ' )  =(E 1 .
.
.
E !-x,  C1 .
.
.
C1-1).
Then (E', C') is derivablefrom A --~ \[A!-I\], and thus (E, C) = (E~E 1, C~C !
)is derivable from A ~ \[A A\]: In the second case, thederivation used was A ---.
{A !
), and we still have E =E 1 .
.
.
E !
but now C -- CY.
.
.
C 1.
Now let (E', C") =A ~ accountable/~tJ\[A ,---+ anthor i ty /~t~A ~ finauciaYl\[#l~A .-* secretary/~A ~ to/~A ~-, wf l l \ ]~A ~ JoA ,-, beJeA ~ theleFigure 2: Some relevant lexical productions.. .
E 1 -1  , C 1 -1  .
.
.
C1) .
~ (E', C") is der ivable(~A --* (A!- I) ,  and thus (E, e )  - (E 'E  !, C !C  ")is derivable from A ---, (A A).
\[74 Stochastic Bracketing TransductionGrammarsIn a stochastic BTG (SBTG), each rewrite rule has a prob-ability.
Let a!
denote the probability of the A-productionwith fanout degree f .
For the remaining (lexical) pro-dnctions, we use b(z, y) to denote P\[A ~ z/vlA\].
Theprobabiliti~ obey the constraint thatEa!
+ Eb(z'Y)= 1l ~?,YFor our experiments we employed a normal form trans-duction grammar, so a!
= 0 for all f # 2.
The A-productions used were:A ~-* AAA b(&~) z/vA b~O x/eA ~%~) e/Vfor all z, y lexical translationsfor all z English vocabularyfor all y Chinese vocabularyThe b(z, y) distribution actually encodes the English-Chinese translation lexicon.
As discussed below, thelexicon we employed was automatically learned from aparallel corpus, giving us the b(z, y) probabilities di-rectly.
The latter two singleton forms permit any wordin either sentence to be unmatched.
A small e-constantis chosen for the probabilities b(z, e) and b(e, y), so thatthe optimal bracketing resorts to these productions onlywhen it is otherwise impossible to match words.With BTGs, to parse means to build matched bracket-ings for senmnce-pairs rather than sentences.
Tiffs meansthat the adjacency constraints given by the nested levelsmust be obeyed in the bracketings ofboth languages.
Theresult of the parse gives bracketings for both input sen-tences, as well as a bracket algnment indicating the cor-responding brackets between the sentences.
The bracketalignment includes aword alignment as a byproduct.Consider the following sentence pair from our corpus:246Jowill/~\[#~The/c Author i ty /~t~belt accountab l~ theJ~Financh~tt~Figure 3: Bracketing tree.Secretary/--~(6) a.
The Authority will be accountable tothe Finan-cial Secretary.b.
I f t~ l~t '~ l~t~t~oAssume we have the productions inFigure 2, which isa fragment excerpted from our actual BTG.
Ignoring cap-italization, an example of a valid parse that is consistentwith our linguistic ideas is:(7) \[\[\[ The/e Author i ty /~t~ \] \[ w i l l /~  (\[ be&accountable/~t~ \] \[ to/~ \[ the/?
\[\[ Financial/~l~Secretary/~ \]\]\]\])\]\] J.
\]Figure 3 shows a graphic representation f the samebrac&eting, where the 0 level of lrac, keting is markedby the horizontal line.
The English is read in the usualdepth-first left-to-right order, but for the Chinese, ahori-zontal ine means the right subtree is traversed before theleft.The () notation concisely displays the common struc-ture of the two sentences.
However, the bracketing isclearer if we view the sentences monolingually, whichallows us to invert he Chinese constituents within the 0so that only \[\] brackets need to appear..(8) a.
\[\[\[ The Authority \]\[ will \[\[ be accountable \] \[ to\[ the \[\[ Financial Secretary \]\]\]\]\]\]1.
\]k \[\[\[\[ "~, '~  \] \[ ~t '  \[\[ I~ \[\[ ~ ~\] \]\]\]\] \[ ~ .
l\]\]\]\] o \]In the monolingual view, extra brackets appear in one lan-guage whenever there is a singleton i  the other language.If the goal is just to obtain ~ for monolingual sen-tences, the extra brackets can be discarded aft~ parsing:(9) \[\[\[ ~ ,~ \] \[ ~R \[ ~ \[ Igil~ ~ \]\] \[ ~t t t  \]\]\] o \]The basis of the bracketing strategy can be seen aschoosing the bracketing that maximizes the (probabilis-tically weighted) number of words matched, subject othe BTG representational constraint, which has the ef-fect of limiting the possible crossing patterns in the wordalignment.
A simpler, related idea of penalizing dis-tortion from some ideal matching pattern can be foundin the statistical translation (Brown et al 1990; Brownet al 1993) and word alignment (Dagan et al 1993;Dagan & Church 1994) models.
Unlike these mod-els, however, the BTG aims m model constituent s ruc-ture when determining distortion penalties.
In particu-lar, crossings that are consistent with the constituent treestructure are not penalized.
The implicit assumption isthat core arguments of frames remain similar across lan-guages, and tha!
core arguments of the same frame willsurface adjacently.
The accuracy of the method on aparticular language pair will therefore depend upon theextent to which this language universals hypothesis holds.However, the approach is robust because if the assump-tion is violated, damage will be limited to dropping thefewest possible crossed word matchings.We now describe how a dynzmic-programming parsercan compute an optimal bxackcting given a sentence-pairand a stochastic BTG.
In bilingual parsing, just as with or-dinary monolingual parsing, probabilizing the grammar247permits ambiguities tobe resolved by choosing the max-imum likelihood parse.
Our algorithm is similar in spiritto the recognition algorithm for HMMs (Viterbi 1967).Denote the input English sentence by el, ?
?
.
,  er andthe corresponding input Chinese sentence by e l , .
.
.
,  cv.As an abbreviation we write co.., for the sequence ofwords eo+l,e,+2,.
.
.
,e~, and similarly for c~..~.
Let6.tu~ = maxP\[e,..t/e~..~\] be the maximum probabilityof any derivation from A that__ successfully parses bothsubstrings es..t and ?u..v. The best parse of the sentencepair is that with probability 60,T,0y.The algorithm computes 6o,T,0,V following the recur-fences below.
2 The time complexity of this algorithmis O(TaV a) where T and V are the lengths of the twosen~.1.
Initialization6t-- l ,t ,v-- l ,v "-2.
Recursion6t tu  v "--Ottu  u --"wherel<t<Tb(e , /~  ), 1 < v < Vmaxr/~\[\] 60 1 t s tuv~ s tuv J. ,6\[  \] 611 s~ s tuv  ~ s tuv6\[\]uv = max a2 6,suu 6stuvs<S<~ u<V<va\[l stuv "- axg s max 6sSut.r 6$tUv s<S<tu<U<vv \[\] -- arg U max 6,suu6stuv sgut~ s<S<tu<U<v6J~uv -- max a 2 6sSU~ 6StuU s<$<tu<U<v*r!~uv = arg s max 6,SV~ 6Stuffs<S<tu<U<vV~uv = arg U max 6,su~ 6S,uVs<S<t  u<V<v3.
Reconstrm:tion Using 4-tuples to name each nodeof the parse tree, initially set qx = (0, T, 0, V) to be theroot.
The remaining descendants in the optimal parse treeare then given recursively for any q = (s, t, u, v) by:LEFT' " "s ~r\[\] u v \[\] ~ /~q) = ( ' \[~ '"~' '\[\] ''"~) f i f0 , t~ = \[\] mGHT(q) = t,LEFr' " "s o "0 v 0 v"RIGHT(q) = (a!~uv,t,u,v~u~) ) ifO, tuv = 0Several additional extensions on this algorithm werefound to be useful, and are briefly described below.
De-tails are given in Wu (1995).2We are gene~!izing argmax as to allow arg to specify theindex of interest.4.1 Simultaneous segmentationWe often find the same concept realized using differentnumbers of words in the two languages, creating potentialdifficulties for word alignment; what is a single word inEnglish may be realized as a compound inChinese.
SinceChinese text is not orthographically separated into words,the standard methodology is to first preproce~ input extsthrough a segmentation module (Chiang et al 1992;L inet al 1992; Chang & Chert 1993; L inet al 1993;Wu & Tseng 1993; Sproat et al 1994).
However, this se-rionsly degrades our algorithm's performance, since thethe segmenter may encounter ambiguities that are un-resolvable monolingually and thereby introduce rrors.Even if the Chinese segmentation is acceptable moaolin-gually, it may not agree with the division of words presentin the English sentence.
Moreover, conventional com-pounds are frequently and unlmxlictably missing fromtranslation lexicons, and this can furllu~ degrade perfor-Inane.To avoid such problems we have extended the algo-rithm to optimize the segmentation f the Chinese sen-tence in parallel with the ~t ing  lm~:ess.
Note thatthis treatment of segmentation does not attempt to ad-dress the open linguistic question of what constitutes aChinese "word".
Our definition of a correct "segmenta-tion" is purely task-driven: longer segments are desirableif and only ff no compositional translation ispossible.4.2 Pre/post-positional biasesMany of the bracketing errors are caused by singletons.With singletons, there is no cross-lingual discriminationto increase the certainty between alternative brackeaings.A heuristic to deal with this is to specify for each of thetwo languages whether prepositions or postpositionsmore common, where "preposition" here is meant notin the usual part-of-speech sense, but rather in a broadsense of the tendency of function words to attach leftor right.
This simple swategcm is effective because themajority of unmatched singletons are function words thatcounterparts in the other language.
This observationholds assuming that the translation lexicon's coverageis reasonably good.
For both English and Chinese, wespecify a prepositional bias, which means that singletonsare attached to the right whenever possible.4.3 Punctuation constraintsCertain punctuation characters give strong constituencyindications with high reliability.
"Perfect separators",which include colons and Chinese full stops, and "pet-feet delimiters", which include parentheses and quota-tion marks, can be used as bracketing constraints.
Wehave extended the algorithm to precluded hypothesesthat are inconsistent with such constraints, by initializ-ing those entries in the DP table corresponding to illegalsub-hypotheses with zero probabilities, These entries areblocked from recomputation during the DP phase.
Astheir probabilities always remain zero, the illegal brack-etings can never participate inany optimal bracketing.2485 Postprocessing5.1 A Singleton-Rebalancing AlgorithmWe now introduce an algorithm for further improving thebracketing accuracy in cases of singletons.
Consider thefollowing bracketing produced by the algorithm of theprevious ection:(10) \[tThe/~ \[\[Authority/~f~ \[wilg~ad (\[be/~accountable/~t~\] \[to the/~ \[~/~ \[Financial/~i~Seaetary/-nl \]\]\])\]ll\] Jo \]The prepositional bias has already correctly restricted thesingleton "Tbe/d' to attach to the right, but of course"The" does not belong outside the rest of the sentence,but rather with "Authority".
The problem is that single-tons have no discriminative power between alternativebracket matchings--they only contribute to the ambigu-ity.
However, we can minimize the impact by movingsingletons as deep as possible, closer to the individualword they precede or succeed, by widening the scopeof the brackets immediately following the singleton.
Ingeneral this improves precision since wide-scope brack-ets are less constraining.The algorithm employs a rebalancing strategy rem-niscent of balanced-tree structures using left and rightrotations.
A left rotation changes a (A(BC)) structure toa ((AB)C) structure, and vice versa for a right rotation.The task is complicated by the presence of both \[\] and0 brackets with both LI- and L2-singletons, ince eachcombination presents different interactions.
To be legal,a rotation must preserve symbol order on both outputstreams.
However, the following lemma shows that anysubtree can always be rebalanced at its root if either of itschildren is a singleton of either language.Lenuna 4 Let x be a L1 singleton, y be a L2 singleton,and A, B, C be arbitrary constituent subtrees.
Then thefollowing properties hold for the \[\] and 0 operators:(Associativity)\[A\[BC\]\] = \[\[AB\]C\](A(BC)) = ((AB)C)(L, -singleton bidirectionality)lax\] ~-- (A~)\[,A\] : (xA)(L2-singleton flipping commutativity)\[Av\] = (vA)\[uA\] = (Av)(L 1-singleton rotation properties)\[z(AB)\] ~- (x(AB)) ~-- ((zA)B) ~- (\[xA\]B)(x\[aB\]) ~--- \[x\[AB\]\] ~--- \[\[zA\]B\] .~ \[(xA)B\]\[(AB)x\] = ((AB)~) = (A(B~)) = (A\[B~\])(lAB\]x) ~- \[\[AB\]x\] = \[A\[Bx\]\] ~--- \[A(Bx)\](L~-singleton rotation properties)\[v(AB)\] = ((AB)v) = (A(Bv)) = (AtvB\])(y\[AB\]) ~-- \[\[AB\]y\] ~ \[A\[By\]\] ~-- \[A(yB)\]\[(AB)v\] ,~ (y(AB)) ~ ((vA)B) ~- (My\]B)(\[AB\]v) ~ \[v\[AB\]\] = ttvA\]B\] = \[(Av)B\]The method of Figure 4 modifies the input ree to attachsingletons as closely as possible to couples, but remain-ing consistent with the input ree in the following sense:singletons cannot "escape" their inmmdiately surround-ing brackets.
The key is that for any given subtree, ifthe outermost bracket involves a singleton that shouldbe rotated into a subtree, then exactly one of the single-ton rotation properties will apply.
The method proceedsdepth-first, sinking each singleton as deeply as possible.For example, after ebalm~cing, sentence (10) is bracketedas follows:(11) \[\[\[\[The/e Author i ty /~\ ]  \[witV~1t' (\[be/eaccountable/~tft\] \[ o the/~ \[dFBJ \[Fhumciai/ll~'i~Secretary/--~ 111)111 Jo \]5.2 Flattening the BracketingBecause the BTG is in normal form, each bracket canonly hold two constituents.
This improves parsing ef-ficiency, but requires overcommiUnent since the algo-rithm is always forced to choose between (A(BC)) and((AB)C) statures even when no choice is clearly bet-ter.
In the worst case, both senteau:~ might have perfectlyaligned words, lending no discriminative l verage what-soever to the bfac~ter.
This leaves a very large numberof choices: if both sentences are of length i = m, thenthel~ ~ (21) 1 possible lracJw~ngs with fanout 2,none of which is better justitied than any other.
Thus toimprove accuracy, we should reduce the specificity of thebracketing's commitment i  such cases.We implement this with another postprocessing stage.The algorithm proceeds bottom-up, elimiDming as malaybrackets as possible, by making use of the associafiv-ity equivalences \[ABel = \[A\[BC\]\] = \[lAB\]C\] andSINK-SINGLETON(node)1 ffnode is not aleaf2 if a rotation property applies at node3 apply the rotation to node4 ch//d ~-- the child into which the singleton5 was rotated6 SINK-SINGLETON(chi/d)RE~AL~CE-aXEE(node)1 if node is not a leaf2 REBALANCE-TREE(left-child\[node\])3 REeALANCE-TREE(right-child\[node\])4 S ~K-SXNGI.,E'ro~(node)Figure 4: The singleton rebalancing schema.249\ [These /~ arrangements/~ will/e ef~ enhance/~q~ our /~ (\[d~J ability/~;0\] [tok dEt ~ maintain/~t~monetary/~t s ab i l i ty /~ in the years to come/e\]) do \]\[The/e Author i ty /~\]~ w i l l /~  (\[be/e accountable/gt~\] \[to the/e elm Financial/l~i~ Secretary/~\]) Jo \]\[They/~t!l~J ( are/e right/iE~ d-l-Jff tok do/~ e /~ so/e ) io \]\[(\[ Evenk more~ important/l~ \] \[Je however/~_ \]) \[Je e/~, is/~ to make the very best of our/e e /~f f l~  own/~$~ e/~J talent/X~ \]J .
\]hope/e e/o!~l employers/{l\[~l~ will/~ make full/e dg~rj'~ use/~ \[offe those/\]Jl~a~__\] ((\[dJfJ-V who/&\] \[haveaequired/e e /$~ new/~i skills/tS~l~ \]) \[through/L~i~t th is J~ l  programme/~l'|~\]) J.
\]have/~ o at/e length/~l ( on/e how/~g~ we/~ e/~ll~) \[canFaJJ)~ boostk d~ilt our/~:~ e/~ prosperity/$~\]Jo\]Figure 5: Bracketing/alignment ou put examples.
(~  = unrecognized input token.
)(ABC) = (A(BC)) = ((AB)C).
Tim singletonbidi-rectionality and flipping eommutativity equivalences (seeLemma 4) are also applied, whenever they render the as-sociativity equivalences applicable.The final result after flattening sentence (11) is as fol-lows:(12) \[ The/e Author i ty /~\ ]~ wi l l /g~' (\[ be/eaccountable/J~tJ!\[ \] to tl~/e elm Financial/ l~Secretary/--~ 1) j o \]6 ExperimentsEvaluation methodology for bracketing is controversialbecause of varying perspectives on what the "gold stan-dard" should be.
We identify two prototypical positions,and give results for both.
One position uses a linguisticevaluation criterion, where accuracy is measured againstsome theoretic notion of constituent s ructure.
The otherposition uses a functional evaluation criterion, where the"correctness" ofa bracketing depends on its utility withrespect to the application task at hand.
For example, herewe consider a bracket-pair functionally useful if it cor-rectly identifies phrasal translations---especially wherethe phrases in the two languages are not compositionallyderivable solely from obvious word translations.
Noticethat in contrast, the linguistic evaluation criterion is in-sensitive to whether the bracketings of the two sentencesmatch each other in any semantic way, as long as themonolingual bracketings in each sentence are correct.
Ineither case, the bracket precision gives the proportionof found br~&ets that agree with the chosen correctnesscriterion.All experiments reported in this paper were performedon sentence-pairs f om the HKUST English-Chinese Par-allel Bilingual Corpus, which consists of governmentaltranscripts (Wu 1994).
The translation lexicon was au-tomatically learned from the same corpus via statisti-cal sentence alignment (Wu 1994) and statistical Chi-nese word and collocation extraction (Fung & Wu 1994;Wu & Fung 1994), followed by an EM word-translationlearning procedure (Wu & Xia 1994).
The translationlexicon contains an English vocabulary of approximately6,500 words and a Chinese vocabulary of approximately5,500 words.
The mapping is many-to-many, with anaverage of 2.25 Chinese translations per English word.The translation accuracy is imperfect (about 86% percentweighted precision), which turns out to cause many ofthe bracketing errors.Approximately 2,000 sentence-pairs with both Englishand Chinese lengths of 30 words or less were extractedfrom our corpus and bracketed using the algorithm de-scribed.
Several additional criteria were used to filterout unsuitable sentence-pairs.
If the lengths of the pairof sentences differed by more thml a 2:1 ratio, the pairwas rejected; such a difference usually arises as the re-sult of an earlier error in automatic sentence alignment.Sentences containing more than one word absent fromthe translation lexicon were also rejected; the bracketingmethod is not intended to be robust against lexicon inade-quacies.
We also rejected sentence pairs with fewer thantwo matching words, since this gives the bracketing al-gorithm no diso'iminative l verage; such pairs ~c~ountedfor less than 2% of the input data.
A random sampleof the b~keted sentence pairs was then drawn, and thebracket precision was computed under each criterion forcorrectness.
Additional examples are shown in Figure 5.Under the linguistic riterion, the monolingual bracketprecision was 80.4% for the English sentences, and 78.4%for the Chinese sentences.
Of course, monolinguaigrammar-based bracketing methods can achieve higherprecision, but such tools assume grammar resources thatmay not be available, such as good Chinese granuna~.Moreover, if a good monolingual bracketer is available,its output can easily be incorporated in much the sameway as punctn~ion constraints, thereby combining thebest of both worlds.
Under the functional criterion, theparallel bracket precision was 72.5%, lower than themonolingual precision since brackets can be correct inone language but not the other.
Grammar-based bracket-ing methods cannot directly produce results of a compa-rable nature.2507 Conc lus ionWe have proposed a new tool for the corpus linguist'sarsenal: a method for simultaneously bracketing bothhalves of a parallel bilingual corpus, using only a wordtranslation lexicon.
The method can also be seen as aword alignment algorithm that employs a realistic dis-tortion model and aligns consituents as well as words.The basis of  the approach is a new inversion-invarianttransduction grammar formalism.Various extension strategies for simultaneous segmen-tation, positional biases, punctuation constraints, ingle-ton rebalancing, and bracket flattening have been intro-duced.
Parallel bracketing exploits a relatively untappedsource of  constraints, in that parallel bilingual sentencesare used to mutually analyze each other.
The modelnonetheless retains a high degree of  compatibility withmore conventional monolingual formalisms and methods.The bracketing and alignment of  parallel corpora canbe fully automatized with zero initial knowledge re-sources, with the aid of  automatic procedures for learningword translation lexicons.
This is particularly valuablefor work on languages for which online knowledge re-sources are relatively scarce compared with English.AcknowledgementI would like to thank Xuanyin Xia, Eva Wai-Man Foug,Pascale Fung, and Derick Wood.Re ferencesBLACK, EZRA, ROGER GARSIDE, & GEoF~EY I ~  (eds.).1993.
Statistically-driven computer grammars of En-glish: The IB~aster  approach.
Amsterdam: Edi-tions Rodopi.BROWN, Pt~reR F., JOHN COCKE, STEPHEN A. D~1APt~rgA,VINCENT J.
~t~r t tA ,  FR~ERICK J~LnqWK, JOHN D.~ ,  ROBERT L. MERCER, & PAUL S. RoossiN.1990.
A statistical approach to machine translation.
Com-putational Linguistics, 16(2):29-85.BROWN, PETER E, STEPHEN A. DIKLAPmTxA, VINCENT J. DEL-LAPteTgA, & ROBERT L. M~CER.
1993.
The mathematicsof statistical machine translation: Parameter estimation.Computational Linguistics, 19(2):263-311.CHANG, CHAO-HUANG & CHE~G-DER CHEN.
1993.
HMM-based part-of-speech tagging for Chinese corpora.
In Pro-ceedings of the Workshop on Very Large Corpora, 40-47,Columbus, Ohio.CHIANG, TUNG-HUI, JING-SHIN CHANG, MING-YU LIN, & KEH-YIH Su.
1992.
Statistical models for word segmentationand unknown resolution.
In Proceedings of ROCLING-92,121-146 .CHURCH, ~ W. 1993.
Char-align: A program for align-ing parallel texts at the character level.
In Proceedings ofthe 31st Annual Conference of the Association for Com-putational Linguistics, 1-8, Columbus, OH.DAGAN, IDO & KENNETH W. CHURCH.
1994.
Termight: Iden-tifying and translating technical terminology.
InProceed-ings of the Fourth Conference on Applied Natural Lan-guage Processing, 34-40, Stuttgart.DAGAN, IDO, KENNETH W. CHURCH, & W\[\]\[J J~ A. GAL~.1993.
Robust bilingual word alignment for machine aidedtranslation.
In Proceedings of the Wor~hop on Very LargeCorpora, 1-8, Columbus, OH.FUNO, PASCALE & KENNETH W. CHURCH.
1994.
K-vec: A newapproach for aligning parallel texts.
In Proceedings ofthe Fifteenth International Conference on ComputationalLinguistics, 1096-1102, Kyoto.FUNG, PASCALE & KATI~J~ McKEoWN.
1994.
Aligningnoisy parallel corpora cross language groups: Word pairfeature matching by dynamic time warping.
In AMTA-94, Association for Machine Translation i  the Americas,81-88, Columbia, Maryland.FUNO, PASCALE & DEKAI Wu.
1994.
Statistical augmentationof  a Chinese machine-readable dictionary.
In Proceedingsof the Second Annual Workshop on Very Large Corpora,69-85, Kyoto.GALE, WnH~M A.
& ~ W. CHURCH.
1991.
Aprogramfor aligning sentences in bilingual corpora.
In Proceed-ings of the 29th Annual Conference of the Association forComputational Linguistics, 177-184, Berkeley.GALE, WnHAM A., KENNETH W. CHURCH, & DAVIDYAROWSKY.
1992.
Using bilingual materials to developword sense disambiguatlon methods.
In Fourth Inter-national Conference on Theoretical nd MethodologicalIssues in Machine Translation, 101-112, Montreal.I~ ,  M~o-Yu, Tt~o-Hta ~o,  & K~-Ym Su.
1993.A preliminary study on unknown word problem in Chi-nese word segmentation.
I  Proceedings ofROCLING-93,119-141.LIN, YI-CHUNG, TUNG-HUI CHIANG, & KEH-Ym SU.
1992.Discrimination oriented pmbabilistic tagging.
In Proceed-ings of ROCLING-92, 85-96.MAGERMAN, DAVID M. & ~ L p. MARCUS.
1990.
Parsinga natural language using mutual information statistics.
InProceedings of AAAI-90, Eighth National Conference onArtificial Intelligence, 984--989.PEREIRA, FEXNANDO & YVES SCHABES.
1992.
Inside-outsidere, estimation from partially bracketed corpora.
In Proceed-ings of the 30th Annual Conference of the Association forComputational Linguistic:, 128-135, Newark, DE.SPROAT, RICHARD, CHn JN SHItl, Wn I JAM GALE, & N. CHANG.1994.
A stochastic word segmentation algorithm for aMandarin text-to-speech system.
In Proceedings of the32nd Annual Conference of the Association for Computa-tional Linguistics, Lag Cruces, New Mexico.
To appear.VITERBI, ANDREW J.
1967.
Error bounds for convolutionalcodes and an asymptotically optimal decoding algorithm.IEEE Transactions on Information Theory, 13:260-269.WU, DEKAL 1994.
Aligning a parallel English-Chinese corpusstatistically with lexical criteria.
In Proceedings of the32ndAnnual Conference of the Association for Computa-tional Linguistics, 80-87, \[,as Cruces, New Mexico.WU, DEKAI, 1995.
Stochastic nversion transduction grammarsand bilingual parsing of parallel corpora.
In preparation.WU, DEKAI & PASCALE FUNG.
1994.
Improving Chinese tok-enization with linguistic filters on statistical lexical acqui-sition.
In Proceedings of the Fourth Conference on@pliedNatural Language Processing, 180-181, Stuttgart.Wu, D~,AI & XUANTIN XIA.
1994.
Learning an English-Chinese lexicon from a parallel corpus.
In AMTA-94, As-sociation for Machine Translation i  the Americas, 206-213, Columbia, Maryland.Wu, ZIMIN & GWYI~TH TSI~G.
1993.
Chinese text seg-mentation for text retrieval: Achievements and problems.Journal of The American Society for Information Science,44(9):532-542.251
