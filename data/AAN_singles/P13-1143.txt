Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1456?1465,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGrammatical Error Correction Using Integer Linear ProgrammingYuanbin WuDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417wuyb@comp.nus.edu.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nght@comp.nus.edu.sgAbstractWe propose a joint inference algorithm forgrammatical error correction.
Differentfrom most previous work where differenterror types are corrected independently,our proposed inference process considersall possible errors in a unied framework.We use integer linear programming (ILP)to model the inference process, which caneasily incorporate both the power of exist-ing error classiers and prior knowledgeon grammatical error correction.
Exper-imental results on the Helping Our Ownshared task show that our method is com-petitive with state-of-the-art systems.1 IntroductionGrammatical error correction is an important taskof natural language processing (NLP).
It has manypotential applications and may help millions ofpeople who learn English as a second language(ESL).
As a research eld, it faces the challenge ofprocessing ungrammatical language, which is dif-ferent from other NLP tasks.
The task has receivedmuch attention in recent years, and was the focusof two shared tasks on grammatical error correc-tion in 2011 and 2012 (Dale and Kilgarriff, 2011;Dale et al, 2012).To detect and correct grammatical errors, twodifferent approaches are typically used ?
knowl-edge engineering or machine learning.
The rstrelies on handcrafting a set of rules.
For exam-ple, the superlative adjective best is preceded bythe article the.
In contrast, the machine learn-ing approach formulates the task as a classicationproblem based on learning from training data.
Forexample, an article classier takes a noun phrase(NP) as input and predicts its article using classlabels a/an, the, or ?
(no article).Both approaches have their advantages and dis-advantages.
One can readily handcraft a set ofrules to incorporate various prior knowledge fromgrammar books and dictionaries, but rules oftenhave exceptions and it is difcult to build rulesfor all grammatical errors.
On the other hand, themachine learning approach can learn from textswritten by ESL learners where grammatical errorshave been annotated.
However, training data maybe noisy and classiers may need prior knowledgeto guide their predictions.Another consideration in grammatical error cor-rection is how to deal with multiple errors in aninput sentence.
Most previous work deals witherrors individually: different classiers (or rules)are developed for different types of errors (articleclassier, preposition classier, etc).
Classiersare then deployed independently.
An example isa pipeline system, where each classier takes theoutput of the previous classier as its input andproposes corrections of one error type.One problem of this pipeline approach is thatthe relations between errors are ignored.
For ex-ample, assume that an input sentence contains acats.
An article classier may propose to deletea, while a noun number classier may proposeto change cats to cat.
A pipeline approach willchoose one of the two corrections based purelyon which error classier is applied rst.
Anotherproblem is that when applying a classier, the sur-rounding words in the context are assumed to becorrect, which is not true if grammatical errors ap-pear close to each other in a sentence.In this paper, we formulate grammatical er-ror correction as a task suited for joint inference.Given an input sentence, different types of errorsare jointly corrected as follows.
For every possi-ble error correction, we assign a score which mea-sures how grammatical the resulting sentence is ifthe correction is accepted.
We then choose a setof corrections which will result in a corrected sen-tence that is judged to be the most grammatical.The inference problem is solved by integer lin-1456ear programming (ILP).
Variables of ILP are indi-cators of possible grammatical error corrections,the objective function aims to select the best set ofcorrections, and the constraints help to enforce avalid and grammatical output.
Furthermore, ILPnot only provides a method to solve the inferenceproblem, but also allows for a natural integrationof grammatical constraints into a machine learn-ing approach.
We will show that ILP fully utilizesindividual error classiers, while prior knowledgeon grammatical error correction can be easily ex-pressed using linear constraints.
We evaluate ourproposed ILP approach on the test data from theHelping Our Own (HOO) 2011 shared task (Daleand Kilgarriff, 2011).
Experimental results showthat the ILP formulation is competitive with state-of-the-art grammatical error correction systems.The remainder of this paper is organized as fol-lows.
Section 2 gives the related work.
Section3 introduces a basic ILP formulation.
Sections4 and 5 improve the basic ILP formulation withmore constraints and second order variables, re-spectively.
Section 6 presents the experimental re-sults.
Section 7 concludes the paper.2 Related WorkThe knowledge engineering approach has beenused in early grammatical error correction systems(Murata and Nagao, 1993; Bond et al, 1995; Bondand Ikehara, 1996; Heine, 1998).
However, asnoted by (Han et al, 2006), rules usually have ex-ceptions, and it is hard to utilize corpus statisticsin handcrafted rules.
As such, the machine learn-ing approach has become the dominant approachin grammatical error correction.Previous work in the machine learning approachtypically formulates the task as a classicationproblem.
Article and preposition errors are the twomain research topics (Knight and Chander, 1994;Han et al, 2006; Tetreault and Chodorow, 2008;Dahlmeier and Ng, 2011).
Features used in classi-cation include surrounding words, part-of-speechtags, language model scores (Gamon, 2010), andparse tree structures (Tetreault et al, 2010).
Learn-ing algorithms used include maximum entropy(Han et al, 2006; Tetreault and Chodorow, 2008),averaged perceptron, na?ve Bayes (Rozovskayaand Roth, 2011), etc.
Besides article and prepo-sition errors, verb form errors also attract someattention recently (Liu et al, 2010; Tajiri et al,2012).Several research efforts have started to deal withcorrecting different errors in an integrated manner(Gamon, 2011; Park and Levy, 2011; Dahlmeierand Ng, 2012a).
Gamon (2011) uses a high-ordersequential labeling model to detect various errors.Park and Levy (2011) models grammatical errorcorrection using a noisy channel model, where apredened generative model produces correct sen-tences and errors are added through a noise model.The work of (Dahlmeier and Ng, 2012a) is proba-bly the closest to our current work.
It uses a beam-search decoder, which iteratively corrects an in-put sentence to arrive at the best corrected output.The difference between their work and our ILPapproach is that the beam-search decoder returnsan approximate solution to the original inferenceproblem, while ILP returns an exact solution to anapproximate inference problem.Integer linear programming has been success-fully applied to many NLP tasks, such as depen-dency parsing (Riedel and Clarke, 2006; Martinset al, 2009), semantic role labeling (Punyakanoket al, 2005), and event extraction (Riedel and Mc-Callum, 2011).3 Inference with First Order VariablesThe inference problem for grammatical error cor-rection can be stated as follows: ?Given an inputsentence, choose a set of corrections which resultsin the best output sentence.?
In this paper, thisproblem will be expressed and solved by integerlinear programming (ILP).To express an NLP task in the framework of ILPrequires the following steps:1.
Encode the output space of the NLP task us-ing integer variables;2.
Express the inference objective as a linearobjective function; and3.
Introduce problem-specic constraints to re-ne the feasible output space.In the following sections, we follow the aboveformulation.
For the grammatical error correc-tion task, the variables in ILP are indicators of thecorrections that a word needs, the objective func-tion measures how grammatical the whole sen-tence is if some corrections are accepted, and theconstraints guarantee that the corrections do notconict with each other.14573.1 First Order VariablesGiven an input sentence, the main question that agrammatical error correction system needs to an-swer is: What corrections at which positions?
Forexample, is it reasonable to change the word catsto cat in the sentence A cats sat on the mat?
Giventhe corrections at various positions in a sentence,the system can readily come up with the correctedsentence.
Thus, a natural way to encode the outputspace of grammatical error correction requires in-formation about sentence position, error type (e.g.,noun number error), and correction (e.g., cat).Suppose s is an input sentence, and |s| is itslength (i.e., the number of words in s).
Dene rstorder variables:Zkl,p ?
{0, 1}, (1)wherep?
{1, 2, .
.
.
, |s|} is a position in a sentence,l?
L is an error type,k?
{1, 2, .
.
.
, C(l)} is a correction of type l.L: the set of error types,C(l): the number of corrections for error type l.If Zkl,p = 1, the word at position p should be cor-rected to k that is of error type l. Otherwise, theword at position p is not applicable for this correc-tion.
Deletion of a word is represented as k = ?.For example, ZaArt,1 = 1 means that the article(Art) at position 1 of the sentence should be a. IfZaArt,1 = 0, then the article should not be a. Ta-ble 1 contains the error types handled in this work,their possible corrections and applicable positionsin a sentence.3.2 The Objective FunctionThe objective of the inference problem is to ndthe best output sentence.
However, there are expo-nentially many different combinations of correc-tions, and it is not possible to consider all com-binations.
Therefore, instead of solving the orig-inal inference problem, we will solve an approx-imate inference problem by introducing the fol-lowing decomposable assumption: Measuring theoutput quality of multiple corrections can be de-composed into measuring the quality of the indi-vidual corrections.Let s?
be the resulting sentence if the correctionZkl,p is accepted for s, or for simplicity denotingit as sZkl,p???
s?.
Let wl,p,k ?
R, measure howgrammatical s?
is.
Dene the objective function asmax?l,p,kwl,p,kZkl,p.This linear objective function aims to select a setof Zkl,p, such that the sum of their weights is thelargest among all possible candidate corrections,which in turn gives the most grammatical sentenceunder the decomposable assumption.Although the decomposable assumption is astrong assumption, it performs well in practice,and one can relax the assumption by using higherorder variables (see Section 5).For an individual correction Zkl,p, we measurethe quality of s?
based on three factors:1.
The language model score h(s?,LM) of s?based on a large web corpus;2.
The condence scores f(s?, t) of classiers,where t ?
E andE is the set of classiers.
For ex-ample, an article classier trained on well-writtendocuments will score every article in s?, and mea-sure the quality of s?
from the perspective of anarticle ?expert?.3.
The disagreement scores g(s?, t) of classi-ers, where t ?
E. A disagreement score mea-sures how ungrammatical s?
is from the perspec-tive of a classier.
Take the article classier as anexample.
For each article instance in s?, the clas-sier computes the difference between the maxi-mum condence score among all possible choicesof articles, and the condence score of the ob-served article.
This difference represents the dis-agreement on the observed article by the articleclassier or ?expert?.
Dene the maximum differ-ence over all article instances in s?
to be the articleclassier disagreement score of s?.
In general, thisscore is large if the sentence s?
is more ungram-matical.The weight wl,p,k is a combination of thesescores:wl,p,k = ?LMh(s?,LM) +?t?E?tf(s?, t)+?t?E?tg(s?, t), (2)where ?LM, ?t, and ?t are the coefcients.3.3 ConstraintsAn observation on the objective function is thatit is possible, for example, to set ZaArt,p = 1 and1458Type l Correction k C(l) Applicable Variablesarticle a, the, ?
3 article or NP ZaArt,p, Z theArt,p,Z?Art,ppreposition on, at, in, .
.
.
|confusion set| preposition ZonPrep,p, ZatPrep,p, Z inPrep,p, .
.
.noun number singular, plural 2 noun ZsingularNoun,p , ZpluralNoun,ppunctuation punctuation symbols |candidates| determined by rules ZoriginalPunct,p , Zcand1Punct,p, Zcand2Punct,p,.
.
.spelling correctly spelled |candidates| determined by a ZoriginalSpell,p , Zcand1Spell,p, Zcand2Spell,p,.
.
.words spell checkerTable 1: Error types and corrections.
The Applicable column indicates which parts of a sentence areapplicable to an error type.
In the rst row, ?
means deleting an article.Z theArt,p = 1, which means there are two correctionsa and the for the same sentence position p, but ob-viously only one article is allowed.A simple constraint to avoid these conicts is?kZkl,p = 1, ?
applicable l, pIt reads as follows: for each error type l, only oneoutput k is allowed at any applicable position p(note that Zkl,p is a Boolean variable).Putting the variables, objective function, andconstraints together, the ILP problem with respectto rst order variables is as follows:max?l,p,kwl,p,kZkl,p (3)s.t.
?kZkl,p = 1, ?
applicable l, p (4)Zkl,p ?
{0, 1} (5)The ILP problem is solved using lp solve1, aninteger linear programming solver based on the re-vised simplex method and the branch-and-boundmethod for integers.3.4 An Illustrating ExampleTo illustrate the ILP formulation, consider an ex-ample input sentence s:A cats sat on the mat .
(6)First, the constraint (4) at position 1 is:ZaArt,1 + Z theArt,1 + Z?Art,1 = 1,which means only one article in {a, the, ?}
is se-lected.1http://lpsolve.sourceforge.net/Next, to compute wl,p,k, we collect languagemodel score and condence scores from the arti-cle (ART), preposition (PREP), and noun number(NOUN) classier, i.e., E = {ART,PREP,NOUN}.The weight for ZsingularNoun,2 is:wNoun,2,singular = ?LMh(s?,LM)+?ARTf(s?,ART) + ?PREPf(s?,PREP) + ?NOUNf(s?,NOUN)+?ARTg(s?,ART) + ?PREPg(s?,PREP) + ?NOUNg(s?,NOUN).where sZsingularNoun,2?????
s?
= A cat sat on the mat .The condence score f(s?, t) of classier t isthe average of the condence scores of t on theapplicable instances in s?.For example, there are two article instances ins?, located at position 1 and 5 respectively, hence,f(s?,ART)= 12 f(s?
[1], 1,ART) + f(s?
[5], 5,ART)= 12 f(a, 1,ART) + f(the, 5,ART).Here, the symbol ft(s?
[p], p,ART) refers to thecondence score of the article classier at positionp, and s?
[p] is the word at position p of s?.Similarly, the disagreement score g(s?,ART) ofthe article classier isg(s?,ART) = max(g1, g2)g1= argmaxkf(k, 1,ART)?
f(a, 1,ART)g2= argmaxkf(k, 5,ART)?
f(the, 5,ART)Putting them together, the weight forZsingularNoun,2 is:wNoun,2,singular = ?LMh(s?,LM)+ ?ART2 f(a, 1,ART) + f(the, 5,ART)+ ?PREPf(on, 4,PREP)+ ?NOUN2 f(cat, 2,NOUN) + f(mat, 6,NOUN)+ ?ARTg(s?,ART)+ ?PREPg(s?,PREP)+ ?NOUNg(s?,NOUN)1459Input A cats sat on the matCorrections The, ?
cat at, in a, ?
matsZaArt,1 ZsingularNoun,2 ZonPrep,4 ZaArt,5 ZsingularNoun,6Variables Z theArt,1 ZpluralNoun,2 ZatPrep,4 Z theArt,5 ZpluralNoun,6Z?Art,1 Z inPrep,4 Z?Art,5Table 2: The possible corrections on example (6).3.5 ComplexityThe time complexity of ILP is determined bythe number of variables and constraints.
Assumethat for each sentence position, at most K classi-ers are applicable2.
The number of variables isO(K|s|C(l?
)), where l?
= argmaxl?LC(l).
Thenumber of constraints is O(K|s|).4 Constraints for Prior Knowledge4.1 Modication Count ConstraintsIn practice, we usually have some rough gaugeof the quality of an input sentence.
If an inputsentence is mostly grammatical, the system is ex-pected to make few corrections.
This require-ment can be easily satised by adding modica-tion count constraints.In this work, we constrain the number of modi-cations according to error types.
For the error typel, a parameter Nl controls the number of modi-cations allowed for type l. For example, the mod-ication count constraint for article corrections is?p,kZkArt,p ?
NArt, where k 6= s[p].
(7)The condition ensures that the correction k is dif-ferent from the original word in the input sentence.Hence, the summation only counts real modica-tions.
There are similar constraints for preposi-tion, noun number, and spelling corrections:?p,kZkPrep,p?
NPrep, where k 6= s[p], (8)?p,kZkNoun,p?
NNoun, where k 6= s[p], (9)?p,kZkSpell,p?
NSpell, where k 6= s[p].
(10)2In most cases, K = 1.
An example of K > 1 is a nounthat requires changing the word form (between singular andplural) and inserting an article, for whichK = 2.4.2 Article-Noun Agreement ConstraintsAn advantage of the ILP formulation is that itis relatively easy to incorporate prior linguisticknowledge.
We now take article-noun agreementas an example to illustrate how to encode suchprior knowledge using linear constraints.A noun in plural form cannot have a (or an)as its article.
That two Boolean variables Z1 andZ2 are mutually exclusive can be handled using asimple inequality Z1 + Z2 ?
1.
Thus, the fol-lowing inequality correctly enforces article-nounagreement:ZaArt,p1 + ZpluralNoun,p2 ?
1, (11)where the article at p1 modies the noun at p2.4.3 Dependency Relation ConstraintsAnother set of constraints involves dependencyrelations, including subject-verb relation anddeterminer-noun relation.
Specically, for a nounn at position p, we check the word w related to nvia a child-parent or parent-child relation.
Ifw be-longs to a set of verbs or determiners (are, were,these, all) that takes a plural noun, then the nounn is required to be in plural form by adding thefollowing constraint:ZpluralNoun,p = 1.
(12)Similarly, if a noun n at position p is required tobe in singular form due to subject-verb relationor determiner-noun relation, we add the followingconstraint:ZsingularNoun,p = 1.
(13)5 Inference with Second Order Variables5.1 Motivation and DenitionTo relax the decomposable assumption in Section3.2, instead of treating each correction separately,one can combine multiple corrections into a singlecorrection by introducing higher order variables.1460Consider the sentence A cat sat on the mat.When measuring the gain due to ZpluralNoun,2 = 1(change cat to cats), the weight wNoun,2,plural islikely to be small since A cats will get a low lan-guage model score, a low article classier con-dence score, and a low noun number classiercondence score.
Similarly, the weight wArt,1,?
ofZ?Art,1 (delete article A) is also likely to be smallbecause of the missing article.
Thus, if one con-siders the two corrections separately, they are bothunlikely to appear in the nal corrected output.However, the correction from A cat sat on themat.
toCats sat on the mat.
should be a reasonablecandidate, especially if the context indicates thatthere are many cats (more than one) on the mat.Due to treating corrections separately, it is difcultto deal with multiple interacting corrections withonly rst order variables.In order to include the correction ?
Cats, onecan use a new set of variables, second order vari-ables.
To keep symbols clear, let Z = {Zu|Zu =Zkl,p,?l, p, k} be the set of rst order variables, andwu = wl,p,k be the weight of Zu = Zkl,p.
Dene asecond order variableXu,v:Xu,v = Zu ?
Zv, (14)where Zu and Zv are rst order variables:Zu , Zk1l1,p1 , Zv , Zk2l2,p2 .
(15)The denition ofXu,v states that a second ordervariable is set to 1 if and only if its two compo-nent rst order variables are both set to 1.
Thus, itcombines two corrections into a single correction.In the above example, a second order variable isintroduced:Xu,v = Z?Art,1 ?
ZpluralNoun,2,s Xu,v????
s?
= Cats sat on the mat .Similar to rst order variables, let wu,v be theweight of Xu,v.
Note that denition (2) only de-pends on the output sentence s?, and the weight ofthe second order variable wu,v can be dened inthe same way:wu,v = ?LMh(s?,LM) +?t?E?tf(s?, t)+?t?E?tg(s?, t).
(16)5.2 ILP with Second Order VariablesA set of new constraints is needed to enforce con-sistency between the rst and second order vari-ables.
These constraints are the linearization ofdenition (14) ofXu,v:Xu,v = Zu ?
Zv ?Xu,v ?
ZuXu,v ?
ZvXu,v ?
Zu + Zv ?
1(17)A new objective function combines the weightsfrom both rst and second order variables:max?l,p,kwl,p,kZkl,p +?u,vwu,vXu,v.
(18)In our experiments, due to noisy data, someweights of second order variables are small, evenif both of its rst order variables have largeweights and satisfy all prior knowledge con-straints.
They will affect ILP proposing good cor-rections.
We nd that the performance will be bet-ter if we change the weights of second order vari-ables to w?u,v, wherew?u,v , max{wu,v, wu, wv}.
(19)Putting them together, (20)-(25) is an ILP for-mulation using second order variables, whereX isthe set of all second order variables which will beexplained in the next subsection.max?l,p,kwl,p,kZkl,p +?u,vw?u,vXu,v (20)s.t.
?kZkl,p = 1, ?
applicable l, p (21)Xu,v ?
Zu, (22)Xu,v ?
Zv, (23)Xu,v ?
Zu + Zv ?
1,?Xu,v ?
X (24)Xu,v, Zkl,p ?
{0, 1} (25)5.3 Complexity and Variable SelectionUsing the notation in section 3.5, the num-ber of second order variables is O(|Z|2) =O(K2|s|2C(l?
)2) and the number of constraints isO(K2|s|2C(l?)2).
More generally, for variableswith higher order h ?
2, the number of variables(and constraints) is O(Kh|s|hC(l?
)h).Note that both the number of variables and thenumber of constraints increase exponentially withincreasing variable order.
In practice, a smallsubset of second order variables is sufcient to1461Data set Sentences Words EditsDev set 939 22,808 1,264Test set 722 18,790 1,057Table 3: Overview of the HOO 2011 data sets.Corrections are called edits in the HOO 2011shared task.achieve good performance.
For example, nounnumber corrections are only coupled with nearbyarticle corrections, and have no connection withdistant or other types of corrections.In this work, we only introduce second or-der variables that combine article corrections andnoun number corrections.
Furthermore, we re-quire that the article and the noun be in the samenoun phrase.
The set X of second order variablesin Equation (24) is dened as follows:X ={Xu,v = Zu ?
Zv|l1 = Art, l2 = Noun,s[p1], s[p2] are in the same noun phrase},where l1, l2, p1, p2 are taken from Equation (15).6 ExperimentsOur experiments mainly focus on two aspects:how our ILP approach performs compared to othergrammatical error correction systems; and howthe different constraints and the second order vari-ables affect the ILP performance.6.1 Evaluation Corpus and MetricWe follow the evaluation setup in the HOO 2011shared task on grammatical error correction (Daleand Kilgarriff, 2011).
The development set andtest set in the shared task consist of conference andworkshop papers taken from the Association forComputational Linguistics (ACL).
Table 3 givesan overview of the data sets.System performance is measured by precision,recall, and F measure:P = # true edits# system edits, R = # true edits# gold edits, F = 2PRP + R.(26)The difculty lies in how to generate the systemedits from the system output.
In the HOO 2011shared task, participants can submit system editsdirectly or the corrected plain-text system output.In the latter case, the ofcial HOO scorer will ex-tract system edits based on the original (ungram-matical) input text and the corrected system outputtext, using GNU Wdiff3.Consider an input sentence The data is simi-lar with test set.
taken from (Dahlmeier and Ng,2012a).
The gold-standard edits are with?
to and?
?
the.
That is, the grammatically correct sen-tence should be The data is similar to the test set.Suppose the corrected output of a system to beevaluated is exactly this perfectly corrected sen-tence The data is similar to the test set.
However,the ofcial HOO scorer using GNUWdiff will au-tomatically extract only one system edit with?
tothe for this system output.
Since this single systemedit does not match any of the two gold-standardedits, the HOO scorer returns an F measure of 0,even though the system output is perfectly correct.In order to overcome this problem, the Max-Match (M2) scorer was proposed in (Dahlmeierand Ng, 2012b).
Given a set of gold-standard ed-its, the original (ungrammatical) input text, andthe corrected system output text, the M2 scorersearches for the system edits that have the largestoverlap with the gold-standard edits.
For the aboveexample, the system edits automatically deter-mined by theM2 scorer are identical to the gold-standard edits, resulting in an F measure of 1 as wewould expect.
We will use the M2 scorer in thispaper to determine the best system edits.
Once thesystem edits are found, P , R, and F are computedusing the standard denition (26).6.2 ILP Conguration6.2.1 VariablesThe rst order variables are given in Table 1.
Ifthe indenite article correction a is chosen, thenthe nal choice between a and an is decided by arule-based post-processing step.
For each prepo-sition error variable ZkPrep,p, the correction k is re-stricted to a pre-dened confusion set of prepo-sitions which depends on the observed preposi-tion at position p. For example, the confusionset of on is { at, for, in, of }.
The list of prepo-sitions corrected by our system is about, among,at, by, for, in, into, of, on, over, to, under, with,and within.
Only selected positions in a sentence(determined by rules) undergo punctuation correc-tion.
The spelling correction candidates are givenby a spell checker.
We used GNU Aspell4in ourwork.3http://www.gnu.org/software/wdiff/4http://aspell.net14626.2.2 WeightsAs described in Section 3.2, the weight of eachvariable is a linear combination of the languagemodel score, three classier condence scores,and three classier disagreement scores.
We usethe Web 1T 5-gram corpus (Brants and Franz,2006) to compute the language model score fora sentence.
Each of the three classiers (article,preposition, and noun number) is trained with themulti-class condence weighted algorithm (Cram-mer et al, 2009).
The training data consists of allnon-OCR papers in the ACL Anthology5, minusthe documents that overlap with the HOO 2011data set.
The features used for the classiers fol-low those in (Dahlmeier and Ng, 2012a), whichinclude lexical and part-of-speech n-grams, lexi-cal head words, web-scale n-gram counts, depen-dency heads and children, etc.
Over 5 milliontraining examples are extracted from the ACL An-thology for use as training data for the article andnoun number classiers, and over 1 million train-ing examples for the preposition classier.Finally, the language model score, classiercondence scores, and classier disagreementscores are normalized to take values in [0, 1],based on the HOO 2011 development data.
We usethe following values for the coefcients: ?LM= 1(language model); ?t = 1 (classier condence);and ?t = ?1 (classier disagreement).6.2.3 ConstraintsIn Section 4, three sets of constraints are in-troduced: modication count (MC), article-nounagreement (ANA), and dependency relation (DR)constraints.
The values for the modication countparameters are set as follows: NArt= 3, NPrep=2, NNoun= 2, and NSpell= 1.6.3 Experimental ResultsWe compare our ILP approach with two other sys-tems: the beam search decoder of (Dahlmeier andNg, 2012a) which achieves the best published per-formance to date on the HOO 2011 data set, andUI Run1 (Rozovskaya et al, 2011) which achievesthe best performance among all participating sys-tems at the HOO 2011 shared task.
The results aregiven in Table 4.The HOO 2011 shared task provides two sets ofgold-standard edits: the original gold-standard ed-its produced by the annotator, and the ofcial gold-5http://aclweb.org/anthology-new/System Original OfcialP R F P R FUI Run1 40.86 11.21 17.59 54.61 14.57 23.00Beam search 30.28 19.17 23.48 33.59 20.53 25.48ILP 20.54 27.93 23.67 21.99 29.04 25.03Table 4: Comparison of three grammatical errorcorrection systems.standard edits which incorporated corrections pro-posed by the HOO 2011 shared task participants.All three systems listed in Table 4 use the M2scorer to extract system edits.
The results of thebeam search decoder and UI Run1 are taken fromTable 2 of (Dahlmeier and Ng, 2012a).Overall, ILP inference outperforms UI Run1 onboth the original and ofcial gold-standard edits,and the improvements are statistically signicantat the level of signicance 0.01.
The performanceof ILP inference is also competitive with the beamsearch decoder.
The results indicate that a gram-matical error correction system benets from cor-rections made at a whole sentence level, and thatjoint correction of multiple error types achievesstate-of-the-art performance.Table 5 provides the comparison of the beamsearch decoder and ILP inference in detail.
Themain difference between the two is that, except forspelling errors, ILP inference gives higher recallthan the beam search decoder, while its precisionis lower.
This indicates that ILP inference is moreaggressive in proposing corrections.Next, we evaluate ILP inference in differentcongurations.
We only focus on article and nounnumber error types.
Table 6 shows the perfor-mance of ILP in different congurations.
Fromthe results, MC and DR constraints improve pre-cision, indicating that the two constraints can helpto restrict the number of erroneous corrections.
In-cluding second order variables gives the best Fmeasure, which supports our motivation for intro-ducing higher order variables.Adding article-noun agreement constraints(ANA) slightly decreases performance.
By exam-ining the output, we nd that although the overallperformance worsens slightly, the agreement re-quirement is satised.
For example, for the inputWe utilize search engine to .
.
.
, the output withoutANA isWe utilize a search engines to .
.
.
but withANA is We utilize the search engines to .
.
.
, whilethe only gold edit inserts a.1463Original OfcialError type Beam search ILP Beam search ILPP R F P R F P R F P R FSpelling 36.84 0.69 1.35 60.00 0.59 1.17 36.84 0.66 1.30 60.00 0.57 1.12+ Article 19.84 12.59 15.40 18.54 14.75 16.43 22.45 13.72 17.03 20.37 15.61 17.68+ Preposition 22.62 14.26 17.49 17.61 18.58 18.09 24.84 15.14 18.81 19.24 19.68 19.46+ Punctuation 24.27 18.09 20.73 20.52 23.50 21.91 27.13 19.58 22.75 22.49 24.98 23.67+ Noun number 30.28 19.17 23.48 20.54 27.93 23.67 33.59 20.53 25.48 21.99 29.04 25.03Table 5: Comparison of the beam search decoder and ILP inference.
ILP is equipped with all constraints(MC, ANA, DR) and default parameters.
Second order variables related to article and noun number errortypes are also used in the last row.Setting Original OfcialP R F P R FArt+Nn, 1st ord.
17.19 19.37 18.22 18.59 20.44 19.47+ MC 17.87 18.49 18.17 19.23 19.39 19.31+ ANA 17.78 18.39 18.08 19.04 19.11 19.07+ DR 17.95 18.58 18.26 19.23 19.30 19.26+ 2nd ord.
18.75 18.88 18.81 20.04 19.58 19.81Table 6: The effects of different constraints and second order variables.7 ConclusionIn this paper, we model grammatical error correc-tion as a joint inference problem.
The inferenceproblem is solved using integer linear program-ming.
We provide three sets of constraints to in-corporate additional linguistic knowledge, and in-troduce a further extension with second order vari-ables.
Experiments on the HOO 2011 shared taskshow that ILP inference achieves state-of-the-artperformance on grammatical error correction.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Ofce.ReferencesFrancis Bond and Satoru Ikehara.
1996.
When andhow to disambiguate?
countability in machine trans-lation.
In Proceedings of the International Seminaron Multimodal Interactive Disambiguation.Francis Bond, Kentaro Ogura, and Tsukasa Kawaoka.1995.
Noun phrase reference in Japanese-to-Englishmachine translation.
In Proceedings of the 6th In-ternational Conference on Theoretical and Method-ological Issues in Machine Translation.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram corpus version 1.1.
Technical report, GoogleResearch.Koby Crammer, Mark Dredze, and Alex Kulesza.2009.
Multi-class condence weighted algorithms.In Proceedings of EMNLP.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Grammat-ical error correction with alternating structure opti-mization.
In Proceedings of ACL.Daniel Dahlmeier and Hwee Tou Ng.
2012a.
A beam-search decoder for grammatical error correction.
InProceedings of EMNLP.Daniel Dahlmeier and Hwee Tou Ng.
2012b.
Betterevaluation for grammatical error correction.
In Pro-ceedings of NAACL.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th EuropeanWorkshop on Natural Lan-guage Generation.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the Seventh Workshop on Innovative Use ofNLP for Building Educational Applications, pages54?62.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners' writing.
In Proceedings ofNAACL.1464Michael Gamon.
2011.
High-order sequence model-ing for language learner error detection.
In Proceed-ings of the Sixth Workshop on Innovative Use of NLPfor Building Educational Applications.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(2).Julia Heine.
1998.
Deniteness predictions forJapanese noun phrases.
In Proceedings of ACL-COLING.Kevin Knight and Ishwar Chander.
1994.
Automatedpostediting of documents.
In Proceedings of AAAI.Xiaohua Liu, Bo Han, Kuan Li, Stephan HyeonjunStiller, and Ming Zhou.
2010.
SRL-based verb se-lection for ESL.
In Proceedings of EMNLP.Andre Martins, Noah Smith, and Eric Xing.
2009.Concise integer linear programming formulationsfor dependency parsing.
In Proceedings of ACL-IJCNLP.Masaki Murata and Makoto Nagao.
1993.
Determina-tion of referential property and number of nouns inJapanese sentences for machine translation into En-glish.
In Proceedings of the 5th International Con-ference on Theoretical and Methodological Issues inMachine Translation.Y.
Albert Park and Roger Levy.
2011.
Automatedwhole sentence grammar correction using a noisychannel model.
In Proceedings of ACL.Vasin Punyakanok, Dan Roth, Wen tau Yih, and DavZimak.
2005.
Learning and inference over con-strained output.
In Proceedings of IJCAI.Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective de-pendency parsing.
In Proceedings of EMNLP.Sebastian Riedel and Andrew McCallum.
2011.
Fastand robust joint models for biomedical event extrac-tion.
In Proceedings of EMNLP.Alla Rozovskaya and Dan Roth.
2011.
Algorithmselection and model adaptation for ESL correctiontasks.
In Proceedings of ACL.Alla Rozovskaya, Mark Sammons, Joshua Gioja, andDan Roth.
2011.
University of Illinois system inHOO text correction shared task.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration.Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-sumoto.
2012.
Tense and aspect error correction forESL learners using global context.
In Proceedingsof ACL.Joel R. Tetreault and Martin Chodorow.
2008.
Theups and downs of preposition error detection in ESLwriting.
In Proceedings of COLING.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In Proceedings of ACL.1465
