Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1405?1414,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsParsing the Internal Structure of Words:A New Paradigm for Chinese Word SegmentationZhongguo LiState Key Laboratory on Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and TechnologyTsinghua University, Beijing 100084, Chinaeemath@gmail.comAbstractLots of Chinese characters are very produc-tive in that they can form many structuredwords either as prefixes or as suffixes.
Pre-vious research in Chinese word segmentationmainly focused on identifying only the wordboundaries without considering the rich inter-nal structures of many words.
In this paper weargue that this is unsatisfying in many ways,both practically and theoretically.
Instead, wepropose that word structures should be recov-ered in morphological analysis.
An elegantapproach for doing this is given and the resultis shown to be promising enough for encour-aging further effort in this direction.
Our prob-ability model is trained with the Penn ChineseTreebank and actually is able to parse bothword and phrase structures in a unified way.1 Why Parse Word Structures?Research in Chinese word segmentation has pro-gressed tremendously in recent years, with state ofthe art performing at around 97% in precision andrecall (Xue, 2003; Gao et al, 2005; Zhang andClark, 2007; Li and Sun, 2009).
However, virtuallyall these systems focus exclusively on recognizingthe word boundaries, giving no consideration to theinternal structures of many words.
Though it hasbeen the standard practice for many years, we arguethat this paradigm is inadequate both in theory andin practice, for at least the following four reasons.The first reason is that if we confine our defi-nition of word segmentation to the identification ofword boundaries, then people tend to have divergentopinions as to whether a linguistic unit is a word ornot (Sproat et al, 1996).
This has led to many dif-ferent annotation standards for Chinese word seg-mentation.
Even worse, this could cause inconsis-tency in the same corpus.
For instance, ???
?vice president?
is considered to be one word in thePenn Chinese Treebank (Xue et al, 2005), but issplit into two words by the Peking University cor-pus in the SIGHAN Bakeoffs (Sproat and Emerson,2003).
Meanwhile, ???
?vice director?
and ???
?deputy manager?
are both segmented into twowords in the same Penn Chinese Treebank.
In fact,all these words are composed of the prefix?
?vice?and a root word.
Thus the structure of???
?vicepresident?
can be represented with the tree in Fig-ure 1.
Without a doubt, there is complete agree-NNll,,JJf?NNf?
?Figure 1: Example of a word with internal structure.ment on the correctness of this structure among na-tive Chinese speakers.
So if instead of annotatingonly word boundaries, we annotate the structures ofevery word, 1 then the annotation tends to be more1Here it is necessary to add a note on terminology used inthis paper.
Since there is no universally accepted definitionof the ?word?
concept in linguistics and especially in Chinese,whenever we use the term ?word?
we might mean a linguisticunit such as ???
?vice president?
whose structure is shownas the tree in Figure 1, or we might mean a smaller unit such as??
?president?
which is a substructure of that tree.
Hopefully,1405consistent and there could be less duplication of ef-forts in developing the expensive annotated corpus.The second reason is applications have differentrequirements for granularity of words.
Take the per-sonal name ???
?Zhou Shuren?
as an example.It?s considered to be one word in the Penn ChineseTreebank, but is segmented into a surname and agiven name in the Peking University corpus.
Forsome applications such as information extraction,the former segmentation is adequate, while for oth-ers like machine translation, the later finer-grainedoutput is more preferable.
If the analyzer can pro-duce a structure as shown in Figure 4(a), then ev-ery application can extract what it needs from thistree.
A solution with tree output like this is more el-egant than approaches which try to meet the needsof different applications in post-processing (Gao etal., 2004).The third reason is that traditional word segmen-tation has problems in handling many phenomenain Chinese.
For example, the telescopic compound????
?universities, middle schools and primaryschools?
is in fact composed of three coordinating el-ements??
?university?,??
?middle school?
and??
?primary school?.
Regarding it as one flat wordloses this important information.
Another exampleis separable words like ??
?swim?.
With a lin-ear segmentation, the meaning of ?swimming?
as in???
?after swimming?
cannot be properly rep-resented, since ??
?swim?
will be segmented intodiscontinuous units.
These language usages lie at theboundary between syntax and morphology, and arenot uncommon in Chinese.
They can be adequatelyrepresented with trees (Figure 2).
(a) NNHHHJJHHHJJf?JJf?JJf?NNf?
(b) VVHHHVVZZVVf?VVf?NNf?Figure 2: Example of telescopic compound (a) and sepa-rable word (b).The last reason why we should care about wordthe context will always make it clear what is being referred towith the term ?word?.structures is related to head driven statistical parsers(Collins, 2003).
To illustrate this, note that in thePenn Chinese Treebank, the word ????
?En-glish People?
does not occur at all.
Hence con-stituents headed by such words could cause somedifficulty for head driven models in which out-of-vocabulary words need to be treated specially bothwhen they are generated and when they are condi-tioned upon.
But this word is in turn headed by itssuffix ?
?people?, and there are 2,233 such wordsin Penn Chinese Treebank.
If we annotate the struc-ture of every compound containing this suffix (e.g.Figure 3), such data sparsity simply goes away.NNbb""NRf??
?NNf?Figure 3: Structure of the out-of-vocabulary word ????
?English People?.Had there been only a few words with inter-nal structures, current Chinese word segmentationparadigm would be sufficient.
We could simply re-cover word structures in post-processing.
But this isfar from the truth.
In Chinese there is a large numberof such words.
We just name a few classes of thesewords and give one example for each class (a dot isused to separate roots from affixes):personal name: ????
?Nagao Makoto?location name: ????
?New York State?noun with a suffix: ????
?classifier?noun with a prefix: ????
?mother-to-be?verb with a suffix: ????
?automatize?verb with a prefix: ???
?waterproof?adjective with a suffix: ????
?composite?adjective with a prefix: ????
?informal?pronoun with a prefix: ???
?everybody?time expression: ??????
?the year 1995?ordinal number: ????
?eleventh?retroflex suffixation: ????
?flower?This list is not meant to be complete, but we can geta feel of how extensive the words with non-trivialstructures can be.
With so many productive suf-fixes and prefixes, analyzing word structures in post-processing is difficult, because a character may ormay not act as an affix depending on the context.1406For example, the character ?
?people?
in ???
?the one who plants?
is a suffix, but in the personalname???
?Zhou Shuren?
it isn?t.
The structuresof these two words are shown in Figure 4.
(a) NRZZNFf?NGf??
(b) NNZZVVf?
?NNf?Figure 4: Two words that differ only in one character,but have different internal structures.
The character ??people?
is part of a personal name in tree (a), but is asuffix in (b).A second reason why generally we cannot re-cover word structures in post-processing is that somewords have very complex structures.
For example,the tree of ??????
?anarchist?
is shown inFigure 5.
Parsing this structure correctly without aprincipled method is difficult and messy, if not im-possible.NNaaa!!!NNHHHVVZZVVf?NNf??NNf?
?NNf?Figure 5: An example word which has very complexstructures.Finally, it must be mentioned that we cannot storeall word structures in a dictionary, as the word for-mation process is very dynamic and productive innature.
Take?
?hall?
as an example.
Standard Chi-nese dictionaries usually contain ???
?library?,but not many other words such as ???
?aquar-ium?
generated by this same character.
This is un-derstandable since the character ?
?hall?
is so pro-ductive that it is impossible for a dictionary to listevery word with this character as a suffix.
The samething happens for natural language processing sys-tems.
Thus it is necessary to have a dynamic mech-anism for parsing word structures.In this paper, we propose a new paradigm forChinese word segmentation in which not only wordboundaries are identified but the internal structuresof words are recovered (Section 3).
To achieve this,we design a joint morphological and syntactic pars-ing model of Chinese (Section 4).
Our generativestory describes the complete process from sentenceand word structures to the surface string of char-acters in a top-down fashion.
With this probabil-ity model, we give an algorithm to find the parsetree of a raw sentence with the highest probabil-ity (Section 5).
The output of our parser incorpo-rates word structures naturally.
Evaluation showsthat the model can learn much of the regularity ofword structures, and also achieves reasonable ac-curacy in parsing higher level constituent structures(Section 6).2 Related WorkThe necessity of parsing word structures has beennoticed by Zhao (2009), who presented a character-level dependency scheme as an alternative to the lin-ear representation of words.
Although our work isbased on the same notion, there are two key dif-ferences.
The first one is that part-of-speech tagsand constituent labels are fundamental for our pars-ing model, while Zhao focused on unlabeled depen-dencies between characters in a word, and part-of-speech information was not utilized.
Secondly, wedistinguish explicitly the generation of flat wordssuch as ???
?Washington?
and words with inter-nal structures.
Our parsing algorithm also has to beadapted accordingly.
Such distinction was not madein Zhao?s parsing model and algorithm.Many researchers have also noticed the awkward-ness and insufficiency of current boundary-only Chi-nese word segmentation paradigm, so they tried tocustomize the output to meet the requirements ofvarious applications (Wu, 2003; Gao et al, 2004).In a related research, Jiang et al (2009) presented astrategy to transfer annotated corpora between dif-ferent segmentation standards in the hope of savingsome expensive human labor.
We believe the bestsolution to the problem of divergent standards andrequirements is to annotate and analyze word struc-tures.
Then applications can make use of these struc-tures according to their own convenience.1407Since the distinction between morphology andsyntax in Chinese is somewhat blurred, our modelfor word structure parsing is integrated with con-stituent parsing.
There has been many efforts to in-tegrate Chinese word segmentation, part-of-speechtagging and parsing (Wu and Zixin, 1998; Zhou andSu, 2003; Luo, 2003; Fung et al, 2004).
However,in these research all words were considered to beflat, and thus word structures were not parsed.
Thisis a crucial difference with our work.
Specifically,consider the word ???
?olive oil?.
Our parseroutput tree Figure 6(a), while Luo (2003) output tree(b), giving no hint to the structure of this word sincethe result is the same with a real flat word ???
?Los Angeles?(c).
(a) NNZZNNf??NNf?
(b) NNNNf???
(c) NRNRf??
?Figure 6: Difference between our output (a) of parsingthe word ???
?olive oil?
and the output (b) of Luo(2003).
In (c) we have a true flat word, namely the loca-tion name???
?Los Angeles?.The benefits of joint modeling has been noticedby many.
For example, Li et al (2010) reported thata joint syntactic and semantic model improved theaccuracy of both tasks, while Ng and Low (2004)showed it?s beneficial to integrate word segmenta-tion and part-of-speech tagging into one model.
Thelater result is confirmed by many others (Zhang andClark, 2008; Jiang et al, 2008; Kruengkrai et al,2009).
Goldberg and Tsarfaty (2008) showed thata single model for morphological segmentation andsyntactic parsing of Hebrew yielded an error reduc-tion of 12% over the best pipelined models.
This isbecause an integrated approach can effectively takeinto account more information from different levelsof analysis.Parsing of Chinese word structures can be re-duced to the usual constituent parsing, for whichthere has been great progress in the past severalyears.
Our generative model for unified word andphrase structure parsing is a direct adaptation of themodel presented by Collins (2003).
Many other ap-proaches of constituent parsing also use this kindof head-driven generative models (Charniak, 1997;Bikel and Chiang, 2000) .3 The New ParadigmGiven a raw Chinese sentence like ???????
?, a traditional word segmentation systemwould output some result like ???
?
????
(?Lin Zhihao?, ?is?, ?chief engineer?).
In our newparadigm, the output should at least be a linear se-quence of trees representing the structures of eachword as in Figure 7.NRQQNFf?NGf??VVVVf?NNHHHJJJJf?NNZZNNf?
?NNf?Figure 7: Proposed output for the new Chinese word seg-mentation paradigm.Note that in the proposed output, all words are an-notated with their part-of-speech tags.
This is nec-essary since part-of-speech plays an important rolein the generation of compound words.
For example,?
?person?
usually combines with a verb to form acompound noun such as???
?designer?.In this paper, we will actually design an integratedmorphological and syntactical parser trained witha treebank.
Therefore, the real output of our sys-tem looks like Figure 8.
It?s clear that besides allSPPPPNPNRZZNFf?NGf??VPaaa!!!VVVVf?NNHHJJJJf?NNZZNNf?
?NNf?Figure 8: The actual output of our parser trained with afully annotated treebank.the information of the proposed output for the new1408paradigm, our model?s output also includes higher-level syntactic parsing results.3.1 Training DataWe employ a statistical model to parse phrase andword structures as illustrated in Figure 8.
The cur-rently available treebank for us is the Penn ChineseTreebank (CTB) 5.0 (Xue et al, 2005).
Because ourmodel belongs to the family of head-driven statisti-cal parsing models (Collins, 2003), we use the head-finding rules described by Sun and Jurafsky (2004).Unfortunately, this treebank or any other tree-banks for that matter, does not contain annotationsof word structures.
Therefore, we must annotatethese structures by ourselves.
The good news is thatthe annotation is not too complicated.
First, we ex-tract all words in the treebank and check each ofthem manually.
Words with non-trivial structuresare thus annotated.
Finally, we install these smalltrees of words into the original treebank.
Whether aword has structures or not is mostly context indepen-dent, so we only have to annotate each word once.There are two noteworthy issues in this process.Firstly, as we?ll see in Section 4, flat words andnon-flat words will be modeled differently, thus it?simportant to adapt the part-of-speech tags to facili-tate this modeling strategy.
For example, the tag fornouns is NN as in ???
?Iraq?
and ???
?for-mer president?.
After annotation, the former is flat,but the later has a structure (Figure 9).
So we changethe POS tag for flat nouns to NNf, then during bot-tom up parsing, whenever a new constituent endingwith ?f?
is found, we can assign it a probability in away different from a structured word or phrase.Secondly, we should record the head position ofeach word tree in accordance with the requirementsof head driven parsing models.
As an example, theright tree in Figure 9 has the context free rule ?NN?
JJf NNf?, the head of which should be the right-most NNf.
Therefore, in ???
?former president?the head is??
?president?.In passing, the readers should note the fact thatin Figure 9, we have to add a parent labeled NN tothe flat word ???
?Iraq?
so as not to change thecontext-free rules contained inherently in the origi-nal treebank.
(a) NNNNf???
(b) NNll,,JJf?NNf?
?Figure 9: Example word structure annotation.
We add an?f?
to the POS tags of words with no further structures.4 The ModelGiven an observed raw sentences S, our generativemodel tells a story about how this surface sequenceof Chinese characters is generated with a linguisti-cally plausible morphological and syntactical pro-cess, thereby defining a joint probability Pr(T, S)where T is a parse tree carrying word structures aswell as phrase structures.
With this model, the pars-ing problem is to search for the tree T ?
such thatT ?
= argmaxTPr(T, S) (1)The generation of S is defined in a top down fash-ion, which can be roughly summarized as follows.First, the lexicalized constituent structures are gen-erated, then the lexicalized structure of each wordis generated.
Finally, flat words with no structuresare generated.
As soon as this is done, we get a treewhose leaves are Chinese characters and can be con-catenated to get the surface character sequence S.4.1 Generation of Constituent StructuresEach node in the constituent tree corresponds to alexicalized context free ruleP ?
Ln Ln?1 ?
?
?L1HR1R2 ?
?
?Rm (2)where P , Li, Ri and H are lexicalized nonterminalsand H is the head.
To generate this constituent, firstP is generated, then the head child H is generatedconditioned on P , and finally each Li and Rj aregenerated conditioned on P and H and a distancemetric.
This breakdown of lexicalized PCFG rulesis essentially the Model 2 defined by Collins (1999).We refer the readers to Collins?
thesis for further de-tails.14094.2 Generation of Words with InternalStructuresWords with rich internal structures can be describedusing a context-free grammar formalism asword ?
root (3)word ?
word suffix (4)word ?
prefix word (5)Here the root is any word without interesting internalstructures, and the prefixes and suffixes are not lim-ited to single characters.
For example,??
?ism?
asin????
?modernism?
is a well known and veryproductive suffix.
Also, we can see that rules (4) and(5) are recursive and hence can handle words withvery complex structures.By (3)?
(5), the generation of word structures isexactly the same as that of ordinary phrase struc-tures.
Hence the probabilities of these words can bedefined in the same way as higher level constituentsin (2).
Note that in our case, each word with struc-tures is naturally lexicalized, since in the annotationprocess we have been careful to record the head po-sition of each complex word.As an example, consider a word w = R(r)S(s)where R is the root part-of-speech headed by theword r, and S is the suffix part-of-speech headedby s. If the head of this word is its suffix, then wecan define the probability of w byPr(w) = Pr(S, s) ?
Pr(R, r|S, s) (6)This is equivalent to saying that to generate w, wefirst generate its head S(s), then conditioned on thishead, other components of this word are generated.In actual parsing, because a word always occurs insome contexts, the above probability should also beconditioned on these contexts, such as its parent andthe parent?s head word.4.3 Generation of Flat WordsWe say a word is flat if it contains only one mor-pheme such as???
?Iraq?, or if it is a compoundlike ??
?develop?
which does not have a produc-tive component we are currently interested in.
De-pending on whether a flat word is known or not,their generative probabilities are computed also dif-ferently.
Generation of flat words seen in training istrivial and deterministic since every phrase and wordstructure rules are lexicalized.However, the generation of unknown flat wordsis a different story.
During training, words that oc-cur less than 6 times are substituted with the symbolUNKNOWN.
In testing, unknown words are gener-ated after the generation of symbol UNKNOWN, andwe define their probability by a first-order Markovmodel.
That is, given a flat word w = c1c2 ?
?
?
cnnot seen in training, we define its probability condi-tioned with the part-of-speech p asPr(w|p) =n+1?i=1Pr(ci|ci?1, p) (7)where c0 is taken to be a START symbol indicatingthe left boundary of a word and cn+1 is the STOPsymbol to indicate the right boundary.
Note that thegeneration of w is only conditioned on its part-of-speech p, ignoring the larger constituent or word inwhich w occurs.We use a back-off strategy to smooth the proba-bilities in (7):P?r(ci|ci?1, p) = ?1 ?
P?r(ci|ci?1, p)+ ?2 ?
P?r(ci|ci?1)+?3 ?
P?r(ci) (8)where ?1 + ?2 + ?3 = 1 to ensure the conditionalprobability is well formed.
These ?s will be esti-mated with held-out data.
The probabilities on theright side of (8) can be estimated with simple counts:P?r(ci|ci?1, p) =COUNT(ci?1ci, p)COUNT(ci?1, p)(9)The other probabilities can be estimated in the sameway.4.4 Summary of the Generative StoryWe make a brief summary of our generative story forthe integrated morphological and syntactic parsingmodel.
For a sentence S and its parse tree T , if wedenote the set of lexicalized phrase structures in Tby C, the set of lexicalized word structures by W ,and the set of unknown flat words by F , then thejoint probability Pr(T, S) according to our model isPr(T, S) =?c?CPr(c)?w?WPr(w)?f?FPr(f) (10)1410In practice, the logarithm of this probability can becalculated instead to avoid numerical difficulties.5 The Parsing AlgorithmTo find the parse tree with highest probability weuse a chart parser adapted from Collins (1999).
Twokey changes must be made to the search process,though.
Firstly, because we are proposing a newparadigm for Chinese word segmentation, the inputto the parser must be raw sentences by definition.Hence to use the bottom-up parser, we need a lex-icon of all characters together with what roles theycan play in a flat word.
We can get this lexicon fromthe treebank.
For example, from the word?
?/NNf?center?, we can extract a role bNNf for character??middle?
and a role eNNf for character ?
?center?.The role bNNf means the beginning of the flat la-bel NNf, while eNNf stands for the end of the labelNNf.
This scheme was first proposed by Luo (2003)in his character-based Chinese parser, and we find itquite adequate for our purpose here.Secondly, in the bottom-up parser for head drivenmodels, whenever a new edge is found, we must as-sign it a probability and a head word.
If the newlydiscovered constituent is a flat word (its label endswith ?f?
), then we set its head word to be the con-catenation of all its child characters, i.e.
the worditself.
If it is an unknown word, we use (7) to assignthe probability, otherwise its probability is set to be1.
On the other hand, if the new edge is a phrase orword with internal structures, the probability is setaccording to (2), while the head word is found withthe appropriate head rules.
In this bottom-up way,the probability for a complete parse tree is knownas soon as it is completed.
This probability includesboth word generation probabilities and constituentprobabilities.6 EvaluationFor several reasons, it is a little tricky to evaluate theaccuracy of our model for integrated morphologicaland syntactic parsing.
First and foremost, we cur-rently know of no other same effort in parsing thestructures of Chinese words, and we have to anno-tate word structures by ourselves.
Hence there is nobaseline performance to compare with.
Secondly,simply reporting the accuracy of labeled precisionand recall is not very informative because our parsertakes raw sentences as input, and its output includesa lot of easy cases like word segmentation and part-of-speech tagging results.Despite these difficulties, we note that higher-level constituent parsing results are still somewhatcomparable with previous performance in parsingPenn Chinese Treebank, because constituent parsingdoes not involve word structures directly.
Havingsaid that, it must be pointed out that the comparisonis meaningful only in a limited sense, as in previousliteratures on Chinese parsing, the input is alwaysword segmented or even part-of-speech tagged.
Thatis, the bracketing in our case is around charactersinstead of words.
Another observation is we canstill evaluate Chinese word segmentation and part-of-speech tagging accuracy, by reading off the cor-responding result from parse trees.
Again becausewe split the words with internal structures into theircomponents, comparison with other systems shouldbe viewed with that in mind.Based on these discussions, we divide the labelsof all constituents into three categories:Phrase labels are the labels in Peen Chinese Tree-bank for nonterminal phrase structures, includ-ing NP, VP, PP, etc.POS labels represent part-of-speech tags such asNN, VV, DEG, etc.Flat labels are generated in our annotation forwords with no interesting structures.
Recallthat they always end with an ?f?
such as NNf,VVf and DEGf, etc.With this classification, we report our parser?s ac-curacy for phrase labels, which is approximatelythe accuracy of constituent parsing of Penn ChineseTreebank.
We report our parser?s word segmenta-tion accuracy based on the flat labels.
This accu-racy is in fact the joint accuracy of segmentationand part-of-speech tagging.
Most importantly, wecan report our parser?s accuracy in recovering wordstructures based on POS labels and flat labels, sinceword structures may contain only these two kinds oflabels.With the standard split of CTB 5.0 data into train-ing, development and test sets (Zhang and Clark,14112009), the result are summarized in Table 1.
For alllabel categories, the PARSEEVAL measures (Abneyet al, 1991) are used in computing the labeled pre-cision and recall.Types LP LR F1Phrase 79.3 80.1 79.7Flat 93.2 93.8 93.5Flat* 97.1 97.6 97.3POS & Flat 92.7 93.2 92.9Table 1: Labeled precision and recall for the three typesof labels.
The line labeled ?Flat*?
is for unlabeled met-rics of flat words, which is effectively the ordinary wordsegmentation accuracy.Though not directly comparable, we can makesome remarks to the accuracy of our model.
Forconstituent parsing, the best result on CTB 5.0 isreported to be 78% F1 measure for unlimited sen-tences with automatically assigned POS tags (Zhangand Clark, 2009).
Our result for phrase labels isclose to this accuracy.
Besides, the result for flatlabels compares favorably with the state of the artaccuracy of about 93% F1 for joint word segmen-tation and part-of-speech tagging (Jiang et al, 2008;Kruengkrai et al, 2009).
For ordinary word segmen-tation, the best result is reported to be around 97%F1 on CTB 5.0 (Kruengkrai et al, 2009), while ourparser performs at 97.3%, though we should remem-ber that the result concerns flat words only.
Finally,we see the performance of word structure recoveryis almost as good as the recognition of flat words.This means that parsing word structures accuratelyis possible with a generative model.It is interesting to see how well the parser doesin recognizing the structure of words that were notseen during training.
For this, we sampled 100such words including those with prefixes or suffixesand personal names.
We found that for 82 of thesewords, our parser can correctly recognize their struc-tures.
This means our model has learnt somethingthat generalizes well to unseen words.In error analysis, we found that the parser tendsto over generalize for prefix and suffix characters.For example,???
?great writer?
is a noun phraseconsisting of an adjective?
?great?
and a noun??
?writer?, as shown in Figure 10(a), but our parser in-correctly analyzed it into a root ??
?masterpiece?and a suffix ?
?expert?, as in Figure 10(b).
This(a) NPll,,JJJJf?NNNNf??
(b) NNZZNNf?
?NNf?Figure 10: Example of parser error.
Tree (a) is correct,and (b) is the wrong result by our parser.is because the character ?
?expert?
is a very pro-ductive suffix, as in ???
?chemist?
and ????diplomat?.
This observation is illuminating becausemost errors of our parser follow this pattern.
Cur-rently we don?t have any non-ad hoc way of prevent-ing such kind of over generalization.7 Conclusion and DiscussionIn this paper we proposed a new paradigm for Chi-nese word segmentation in which not only flat wordswere identified but words with structures were alsoparsed.
We gave good reasons why this should bedone, and we presented an effective method show-ing how this could be done.
With the progress instatistical parsing technology and the developmentof large scale treebanks, the time has now come forthis paradigm shift to happen.
We believe such anew paradigm for word segmentation is linguisti-cally justified and pragmatically beneficial to realworld applications.
We showed that word struc-tures can be recovered with high precision, thoughthere?s still much room for improvement, especiallyfor higher level constituent parsing.Our model is generative, but discriminative mod-els such as maximum entropy technique (Bergeret al, 1996) can be used in parsing word struc-tures too.
Many parsers using these techniqueshave been proved to be quite successful (Luo, 2003;Fung et al, 2004; Wang et al, 2006).
Anotherpossible direction is to combine generative modelswith discriminative reranking to enhance the accu-racy (Collins and Koo, 2005; Charniak and Johnson,2005).Finally, we must note that the use of flat labelssuch as ?NNf?
is less than ideal.
The most impor-1412tant reason these labels are used is we want to com-pare the performance of our parser with previous re-sults in constituent parsing, part-of-speech taggingand word segmentation, as we did in Section 6.
Theproblem with this approach is that word structuresand phrase structures are then not treated in a trulyunified way, and besides the 33 part-of-speech tagsoriginally contained in Penn Chinese Treebank, an-other 33 tags ending with ?f?
are introduced.
Weleave this problem open for now and plan to addressit in future work.AcknowledgmentsI would like to thank Professor Maosong Sun formany helpful discussions on topics of Chinese mor-phological and syntactic analysis.
The author is sup-ported by NSFC under Grant No.
60873174.
Heart-felt thanks also go to the reviewers for many per-tinent comments which have greatly improved thepresentation of this paper.ReferencesS.
Abney, S. Flickenger, C. Gdaniec, C. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, M. Marcus, S. Roukos, B. San-torini, and T. Strzalkowski.
1991.
Procedure for quan-titatively comparing the syntactic coverage of Englishgrammars.
In E. Black, editor, Proceedings of theworkshop on Speech and Natural Language, HLT ?91,pages 306?311, Morristown, NJ, USA.
Association forComputational Linguistics.Adam L. Berger, Vincent J. Della Pietra, and Stephen A.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71.Daniel M. Bikel and David Chiang.
2000.
Two statis-tical parsing models applied to the Chinese treebank.In Second Chinese Language Processing Workshop,pages 1?6, Hong Kong, China, October.
Associationfor Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 173?180, Morristown, NJ, USA.
Association forComputational Linguistics.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of the fourteenth national conference on artificialintelligence and ninth conference on Innovative ap-plications of artificial intelligence, AAAI?97/IAAI?97,pages 598?603.
AAAI Press.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31:25?70, March.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-feng Chen.
2004.
A maximum-entropy Chineseparser augmented by transformation-based learning.ACM Transactions on Asian Language InformationProcessing, 3:159?168, June.Jianfeng Gao, Andi Wu, Cheng-Ning Huang, Hong qiaoLi, Xinsong Xia, and Hauwei Qin.
2004.
AdaptiveChinese word segmentation.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 462?469,Barcelona, Spain, July.Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.2005.
Chinese word segmentation and named entityrecognition: A pragmatic approach.
ComputationalLinguistics, 31(4):531?574.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gener-ative model for joint morphological segmentation andsyntactic parsing.
In Proceedings of ACL-08: HLT,pages 371?379, Columbus, Ohio, June.
Associationfor Computational Linguistics.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.2008.
A cascaded linear model for joint Chinese wordsegmentation and part-of-speech tagging.
In Proceed-ings of ACL-08: HLT, pages 897?904, Columbus,Ohio, June.
Association for Computational Linguis-tics.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and POS tagging ?
a case study.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 522?530, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint Chinese word segmentation and POStagging.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-1413ing of the AFNLP, pages 513?521, Suntec, Singapore,August.
Association for Computational Linguistics.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for Chinese word segmentation.Computational Linguistics, 35:505?512, December.Junhui Li, Guodong Zhou, and Hwee Tou Ng.
2010.Joint syntactic and semantic parsing of Chinese.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 1108?1117, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.Xiaoqiang Luo.
2003.
A maximum entropy Chinesecharacter-based parser.
In Michael Collins and MarkSteedman, editors, Proceedings of the 2003 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 192?199.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages 277?284, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Richard Sproat and Thomas Emerson.
2003.
The firstinternational Chinese word segmentation bakeoff.
InProceedings of the Second SIGHAN Workshop on Chi-nese Language Processing, pages 133?143, Sapporo,Japan, July.
Association for Computational Linguis-tics.Richard Sproat, William Gale, Chilin Shih, and NancyChang.
1996.
A stochastic finite-state word-segmentation algorithm for Chinese.
ComputationalLinguistics, 22(3):377?404.Honglin Sun and Daniel Jurafsky.
2004.
Shallow se-mantc parsing of Chinese.
In Daniel Marcu Su-san Dumais and Salim Roukos, editors, HLT-NAACL2004: Main Proceedings, pages 249?256, Boston,Massachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.2006.
A fast, accurate deterministic parser for chinese.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 425?432, Sydney, Australia, July.
Associationfor Computational Linguistics.Andi Wu and Jiang Zixin.
1998.
Word segmentation insentence analysis.
In Proceedings of the 1998 Interna-tional Conference on Chinese information processing,Beijing, China.Andi Wu.
2003.
Customizable segmentation of morpho-logically derived words in Chinese.
ComputationalLinguistics and Chinese language processing, 8(1):1?28.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Yue Zhang and Stephen Clark.
2007.
Chinese segmenta-tion with a word-based perceptron algorithm.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 840?847, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Yue Zhang and Stephen Clark.
2008.
Joint word segmen-tation and POS tagging using a single perceptron.
InProceedings of ACL-08: HLT, pages 888?896, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the Chinese treebank using a global dis-criminative model.
In Proceedings of the 11th Inter-national Conference on Parsing Technologies, IWPT?09, pages 162?171, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Hai Zhao.
2009.
Character-level dependencies in Chi-nese: Usefulness and learning.
In Proceedings of the12th Conference of the European Chapter of the ACL(EACL 2009), pages 879?887, Athens, Greece, March.Association for Computational Linguistics.Guodong Zhou and Jian Su.
2003.
A Chinese effi-cient analyser integrating word segmentation, part-of-speech tagging, partial parsing and full parsing.
InProceedings of the Second SIGHAN Workshop on Chi-nese Language Processing, pages 78?83, Sapporo,Japan, July.
Association for Computational Linguis-tics.1414
