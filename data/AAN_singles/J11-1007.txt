Analyzing and IntegratingDependency ParsersRyan McDonald?Google Inc.Joakim Nivre?
?Uppsala UniversityThere has been a rapid increase in the volume of research on data-driven dependency parsers inthe past five years.
This increase has been driven by the availability of treebanks in a wide varietyof languages?due in large part to the CoNLL shared tasks?as well as the straightforwardmechanisms by which dependency theories of syntax can encode complex phenomena in free wordorder languages.
In this article, our aim is to take a step back and analyze the progress that hasbeen made through an analysis of the two predominant paradigms for data-driven dependencyparsing, which are often called graph-based and transition-based dependency parsing.
Ouranalysis covers both theoretical and empirical aspects and sheds light on the kinds of errors eachtype of parser makes and how they relate to theoretical expectations.
Using these observations,we present an integrated system based on a stacking learning framework and show that such asystem can learn to overcome the shortcomings of each non-integrated system.1.
IntroductionSyntactic dependency representations have a long history in descriptive and theoreticallinguistics and many formal models have been advanced, most notably Word Gram-mar (Hudson 1984), Meaning-Text Theory (Mel?c?uk 1988), Functional Generative De-scription (Sgall, Hajic?ova?, and Panevova?
1986), and Constraint Dependency Grammar(Maruyama 1990).
Common to all theories is the notion of directed syntactic depen-dencies between the words of a sentence, an example of which is given in Figure 1 forthe sentence A hearing is scheduled on the issue today, which has been extracted from thePenn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
A dependency graph ofa sentence represents each word and its syntactic modifiers through labeled directedarcs, where each arc label comes from some finite set representing possible syntacticroles.
Returning to our example in Figure 1, we can see multiple instances of labeleddependency relations such as the one from the finite verb is to hearing labeled SBJindicating that hearing is the head of the syntactic subject of the finite verb.
An artificialword has been inserted at the beginning of the sentence that will always serve as thesingle root of the graph and is primarily a means to simplify computation.?
76 Ninth Ave., New York, NY 10011.
E-mail: ryanmcd@google.com.??
Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden.E-mail: joakim.nivre@lingfil.uu.se.Submission received: 25 August 2009; revised submission received: 20 August 2010; accepted for publication:7 October 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 1Figure 1Dependency graph for an English sentence.Syntactic dependency graphs have recently gained a wide interest in the computa-tional linguistics community and have been successfully employed for many problemsranging from machine translation (Ding and Palmer 2004) to ontology construction(Snow, Jurafsky, and Ng 2005).
A primary advantage of dependency representationsis that they have a natural mechanism for representing discontinuous constructions,which arise due to long-distance dependencies or in languages where grammaticalrelations are often signaled by morphology instead of word order.
This is undoubt-edly one of the reasons for the emergence of dependency parsers for a wide range oflanguages (Buchholz and Marsi 2006; Nivre et al 2007).
Thus, the example in Figure 1contains an instance of a discontinuous construction through the subgraph rooted atthe word hearing.
Specifically, the dependency arc from hearing to on spans the wordsis and scheduled, which are not nodes in this subgraph.
An arc of this kind is said to benon-projective.In this article we focus on a common paradigm called data-driven dependencyparsing, which encompasses parsing systems that learn to produce dependency graphsfor sentences from a corpus of sentences annotated with dependency graphs.
Theadvantage of such models is that they are easily ported to any domain or languagein which annotated resources exist.
Many data-driven parsing systems are grammar-less, in that they do not assume the existence of a grammar that defines permissiblesentences of the language.
Instead, the goal of most data-driven parsing systems is todiscriminate good parses from bad for a given sentence, regardless of its grammaticality.Alternatively, one can view such systems as parsers for a grammar that induces thelanguage of all strings.The rise of statistical methods in natural language processing coupled with theavailability of dependency annotated corpora for multiple languages?most notablyfrom the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi 2006; Nivre et al2007)?has led to a boom in research on data-driven dependency parsing.
Making senseof this work is a challenging problem, but an important one if the field is to continue tomake advances.
Of the many important questions to be asked, three are perhaps mostcrucial at this stage in the development of parsers:1.
How can we formally categorize the different approaches to data-drivendependency parsing?2.
Can we characterize the kinds of errors each category of parser makesthrough an empirical analysis?3.
Can we benefit from such an error analysis and build improved parsers?The organizers of the CoNLL-X shared task on dependency parsing (Buchholz andMarsi 2006) point out that there are currently two dominant approaches for data-driven198McDonald and Nivre Analyzing and Integrating Dependency Parsersdependency parsing.
The first category parameterizes models over dependency sub-graphs and learns these parameters to globally score correct graphs above incorrectones.
Inference is also global, in that systems attempt to find the highest scoring graphamong the set of all graphs.
We call such systems graph-based parsing models toreflect the fact that parameterization is over the graph.
Graph-based models are mainlyassociated with the pioneering work of Eisner (Eisner 1996), as well as McDonald andcolleagues (McDonald, Crammer, and Pereira 2005; McDonald et al 2005; McDonaldand Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C?ak?c?, andMeza-Ruiz 2006; Carreras 2007; Koo et al 2007; Nakagawa 2007; Smith and Smith 2007).The second category of parsing systems parameterizes models over transitions from onestate to another in an abstract state-machine.
Parameters in these models are typicallylearned using standard classification techniques that learn to predict one transition froma set of permissible transitions given a state history.
Inference is local, in that systemsstart in a fixed initial state and greedily construct the graph by taking the highestscoring transitions at each state entered until a termination condition is met.
We callsuch systems transition-based parsing models to reflect the fact that parameterizationis over possible state transitions.
Transition-based models have been promoted bythe groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003;Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre andNilsson 2005; Nivre et al 2006), and others (Attardi 2006; Attardi and Ciaramita 2007;Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a,2007b).It is important to note that there is no a priori reason why a graph-based pa-rameterization should require global learning and inference, and a transition-basedparameterization would necessitate local learning and greedy inference.
Nevertheless,as observed by Buchholz and Marsi (2006), it is striking that recent work on data-drivendependency parsing has been dominated by global, exhaustive, graph-based models, onthe one hand, and local, greedy, transition-based models, on the other.
Therefore, a carefulcomparative analysis of these model types appears highly relevant, and this is what wewill try to provide in this article.
For convenience, we will use the shorthand terms?graph-based?
and ?transition-based?
for these models, although both graph-basedand transition-based parameterizations can be (and have been) combined with differenttypes of learning and inference.
For example, the system described by Zhang and Clark(2008) could be characterized as a transition-based model with global learning, and theensemble system of Zeman and Z?abokrtsky` (2005) as a graph-based model with greedyinference.Perhaps the most interesting reason to study the canonical graph-based andtransition-based models is that even though they appear to be quite different theoret-ically (see Section 2), recent empirical studies show that both obtain similar parsingaccuracies on a variety of languages.
For example, Table 1 shows the results of the twotop performing systems in the CoNLL-X shared task, those of McDonald, Lerman, andPereira (2006) (graph-based) and Nivre et al (2006) (transition-based), which exhibitno statistically significant difference in accuracy when averaged across all languages.This naturally leads us to our Question 2, that is, can we empirically characterize theerrors of these systems to understand whether, in practice, these errors are the sameor distinct?
Towards this end, Section 2 describes in detail the theoretical propertiesand expectations of these two parsing systems and Section 4 provides a fine-grainederror analysis of each system on the CoNLL-X shared task data sets (Buchholz andMarsi 2006).
The result of this analysis strongly suggests that (1) the two systems domake different, yet complementary, errors, which lends support to the categorization of199Computational Linguistics Volume 37, Number 1Table 1Labeled parsing accuracy for top-scoring systems at CoNLL-X (Buchholz and Marsi 2006).Language Graph-based Transition-based(McDonald, Lerman, and Pereira 2006) (Nivre et al 2006)Arabic 66.91 66.71Bulgarian 87.57 87.41Chinese 85.90 86.92Czech 80.18 78.42Danish 84.79 84.77Dutch 79.19 78.59German 87.34 85.82Japanese 90.71 91.65Portuguese 86.82 87.60Slovene 73.44 70.30Spanish 82.25 81.29Swedish 82.55 84.58Turkish 63.19 65.68Average 80.83 80.75parsers as graph-based and transition-based, and (2) the errors made by each systemare directly correlated with our expectations, based on their theoretical underpinnings.This leads to our Question 3: Can we use these insights to integrate parsers andachieve improved accuracies?
In Section 5 we consider a simple way of integratinggraph-based and transition-based models in order to exploit their complementarystrengths and thereby improve parsing accuracy beyond what is possible by eithermodel in isolation.
The method integrates the two models by allowing the output of onemodel to define features for the other, which is commonly called ?classifier stacking.
?This method is simple?requiring only the definition of new features?and robust byallowing a model to learn relative to the predictions of the other.
More importantly, wererun the error analysis and show that the integrated models do indeed take advantageof the complementary strengths of both the graph-based and transition-based parsingsystems.Combining the strengths of different machine learning systems, and even parsingsystems, is by no means new as there are a number of previous studies that have lookedat combining phrase-structure parsers (Henderson and Brill 1999), dependency parsers(Zeman and Z?abokrtsky` 2005), or both (McDonald 2006).
Of particular note is past workon combining graph-based and transition-based dependency parsers.
Sagae and Lavie(2006) present a system that combines multiple transition-based parsers with a singlegraph-based parser by weighting each potential dependency relation by the number ofparsers that predicted it.
A final dependency graph is predicted by using spanning treeinference algorithms from the graph-based parsing literature (McDonald et al 2005).Sagae and Lavie report improvements of up to 1.7 percentage points over the best singleparser when combining three transition-based models and one graph-based model forunlabeled dependency parsing, evaluated on data from the Penn Treebank.
The sametechnique was used by Hall et al (2007) to combine six transition-based parsers in thebest performing system in the CoNLL 2007 shared task.Zhang and Clark (2008) propose a parsing system that uses global learning coupledwith beam search over a transition-based backbone incorporating both graph-based200McDonald and Nivre Analyzing and Integrating Dependency Parsersand transition-based features, that is, features over both sub-graphs and transitions.Huang and Sagae (2010) go even further and show how transition-based parsing can betabularized to allow for dynamic programming, which in turn permits an exponentiallylarger search space.
Martins et al (2008) present a method for integrating graph-basedand transition-based parsers based on stacking, which is similar to the approach takenin this work.
Other studies have tried to overcome the weaknesses of parsing modelsby changing the underlying model structure directly.
For example, Hall (2007), Riedel,C?ak?c?, and Meza-Ruiz (2006), Nakagawa (2007), Smith and Eisner (2008), and Martins,Smith, and Xing (2009) attempt to overcome local restrictions in feature scope for graph-based parsers through both approximations and exact solutions with integer linearprogramming.Our work differs from past studies in that we attempt to quantify exactly thetypes of errors these parsers make, tie them to their theoretical expectations, and showthat integrating graph-based and transition-based parsers not only increases overallaccuracy, but does so directly exploiting the strengths of each system.
Thus, this is thefirst large-scale error analysis of modern data-driven dependency parsers.1 The restof the article is structured as follows: Section 2 describes canonical graph-based andtransition-based parsing systems and discusses their theoretical benefits and limitationswith respect to one another; Section 3 introduces the experimental setup based on theCoNLL-X shared task data sets that incorporate dependency treebanks from 13 diverselanguages; Section 4 gives a fine-grained error analysis for the two parsers in this setup;Section 5 describes a stacking-based dependency parser combination framework;Section 6 evaluates the stacking-based parsers in comparison to the original systemswith a detailed error analysis; we conclude in Section 7.2.
Two Models for Dependency ParsingIn this section we introduce central notation and define canonical graph-based andtransition-based dependency parsing at an abstract level.
We further compare andcontrast their theoretical underpinnings with an eye to understanding the kinds oferrors each system is likely to make in practice.2.1 PreliminariesLet L = {l1, .
.
.
, l|L|} be a set of permissible arc labels.
Let x = w0, w1, .
.
.
, wn be an inputsentence where w0 = ROOT.
Formally, a dependency graph for an input sentence x is alabeled directed graph G = (V, A) consisting of a set of nodes V and a set of labeleddirected arcs A ?
V ?
V ?
L; that is, if (i, j, l) ?
A for i, j ?
V and l ?
L, then there isan arc from node i to node j with label l in the graph.
In terms of standard linguisticdependency theory nomenclature, we say that (i, j, l) ?
A if there is a dependency withhead wi, dependent wj, and syntactic role l.A dependency graph G for sentence x must satisfy the following properties:1.
V = {0, 1, .
.
.
, n}.2.
If (i, j, l) ?
A, then j = 0.1 This work has previously been published partially in McDonald and Nivre (2007) and Nivre andMcDonald (2008).201Computational Linguistics Volume 37, Number 13.
If (i, j, l) ?
A, then for all arcs (i?, j, l?)
?
A, i = i?
and l = l?.4.
For all j ?
V ?
{0}, either (0, j, l) for some l ?
L or there is a non-emptysequence of nodes i1, .
.
.
, im ?
V and labels l1, .
.
.
, lm+1 ?
L such that(0, i1, l1),(i1, i2, l2), .
.
.
, (im, j, lm+1)?A.The first constraint states that the dependency graph spans the entire input.
The sec-ond constraint states that the node 0 is a root.
The third constraint states that eachnode has at most one incoming arc in the graph.
The final constraint states that thegraph is connected through directed paths from the node 0 to every other node inthe graph.
It is not difficult to show that a dependency graph satisfying these con-straints is in fact a directed tree originating out of the root node 0 and we will usethe term dependency tree to refer to any valid dependency graph.
The characterizationof syntactic dependency graphs as trees is consistent with most formal theories (e.g.,Sgall, Hajic?ova?, and Panevova?
1986; Mel?c?uk 1988).
Exceptions include Word Grammar(Hudson 1984), which allows a word to modify multiple other words in the sentence,which results in directed acyclic graphs with nodes possibly having multiple incomingarcs.We define an arc (i, j, l) connecting words wi and wj as non-projective if at leastone word occurring between wi and wj in the input sentence is not a descendant ofwi (where ?descendant?
is the transitive closure of the arc relation).
Alternatively, wecan view non-projectivity in trees as breaking the nested property, which can be seenthrough the arcs that cross in the example in Figure 1.
Non-projective dependenciesare typically difficult to represent or parse in phrase-based models of syntax.
This caneither be due to nested restrictions arising in context-free formalisms or computationallyexpensive operations in mildly context-sensitive formalisms (e.g., adjunction in TAGframeworks).2.2 Global, Exhaustive, Graph-Based ParsingFor an input sentence, x = w0, w1, .
.
.
, wn consider the dense graph Gx = (Vx, Ax) de-fined as:1.
Vx = {0, 1, .
.
.
, n}.2.
Ax = {(i, j, l) | i, j ?
Vx and l ?
L}.Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for thesentence x, that is, dependency trees.
Because Gx contains all possible labeled arcs, theset D(Gx) must necessarily contain all dependency trees for x.Assume that there exists a dependency arc scoring function, s : V ?
V ?
L ?
R.Furthermore, define the score of a graph as the sum of its arc scores,s(G = (V, A)) =?
(i,j,l)?As(i, j, l)The score of an arc, s(i, j, l) represents the likelihood of creating a dependency fromhead wi to modifier wj with the label l in a dependency tree.
This score is commonlydefined to be the product of a high dimensional feature representation of the arc and a202McDonald and Nivre Analyzing and Integrating Dependency Parserslearned parameter vector, s(i, j, l) = w ?
f(i, j, l).
If the arc score function is known, thenthe parsing problem can be stated asG?
= arg maxG?D(Gx )s(G) = arg maxG?D(Gx )?
(i,j,l)?As(i, j, l) (1)An example graph Gx and the dependency tree maximizing the scoring function aregiven in Figure 2 for the sentence John saw Mary.
We omit arcs into the root node forsimplicity.McDonald et al (2005) showed that this problem is equivalent to finding the highestscoring directed spanning tree for the graph Gx originating out of the root node 0.
It isnot difficult to see this, because both dependency trees and spanning trees must containall nodes of the graph and must have a tree structure with root 0.
The directed spanningtree problem (also known as the r-arborescence problem) can be solved for both thelabeled and unlabeled case using the Chu-Liu-Edmonds algorithm (Chu and Liu 1965;Edmonds 1967), a variant of which can be shown to have an O(n2) runtime (Tarjan1977).
Non-projective arcs are produced naturally through the inference algorithm thatsearches over all possible directed trees, whether projective or not.Graph-based parsers are typically trained using structured learning algorithms(McDonald, Crammer, and Pereira 2005; Koo et al 2007; Smith and Smith 2007), whichoptimize the parameters of the model to maximize the difference in score/probabilitybetween the correct dependency graph and all incorrect dependency graphs for everysentence in a training set.
Such a learning procedure is global because model parametersare set relative to the classification of the entire dependency graph, and not just oversingle arc attachment decisions.
Although a learning procedure that only optimizes thescore of individual arcs is conceivable, it would not be likely to produce competitiveresults.Going beyond arc-factored models, McDonald and Pereira (2006) presented asystem where scores are increased in scope to include pairs of adjacent arcs in thedependency graph.
In the case of projective dependency trees, polynomial timeparsing algorithms were shown to exist, but non-projective trees required approximateinference that used an exhaustive projective algorithm followed by transformationsto the graph that incrementally introduce non-projectivity.
In general, inference andlearning for graph-based dependency parsing is NP-hard when the score is factoredFigure 2A graph-based parsing example.
A dense graph Gx is shown on the left (arcs into the root areomitted) with corresponding arc scores.
On the right is the predicted dependency tree based onEquation (1).203Computational Linguistics Volume 37, Number 1over anything larger than arcs (McDonald and Satta 2007).
Thus, graph-based parsingsystems cannot easily condition on any extended scope of the dependency graphbeyond a single arc, which is their primary shortcoming relative to transition-basedsystems.
McDonald, Crammer, and Pereira (2005) show that a rich feature set overthe input space, including lexical and surface syntactic features of neighboring words,can partially alleviate this problem, and both Carreras (2007) and Koo et al (2010)explore higher-order models for projective trees.
Additionally, work has been done onapproximate non-factored parsing systems (McDonald and Pereira 2006; Hall 2007;Nakagawa 2007; Smith and Eisner 2008) as well as exact solutions through integer linearprogramming (Riedel, C?ak?c?, and Meza-Ruiz 2006; Martins, Smith, and Xing 2009).The specific graph-based system studied in this work is that presented byMcDonald, Lerman, and Pereira (2006), which uses pairwise arc scoring and approx-imate exhaustive search for unlabeled parsing.
A separate arc label classifier is thenused to label each arc.
This two-stage process was adopted primarily for computationalreasons and often does not affect performance significantly (see McDonald [2006] formore).
Throughout the rest of this study we will refer to this system as MSTParser (orMST for short), which is also the name of the freely available implementation.22.3 Local, Greedy, Transition-Based ParsingA transition system for dependency parsing defines1.
a set C of parser configurations, each of which defines a (partially built)dependency graph G,2.
a set T of transitions, each of which is a partial function t : C ?
C,3.
for every sentence x = w0, w1, .
.
.
, wn,(a) a unique initial configuration cx,(b) a set Cx of terminal configurations.A transition sequence C0,m = (c0, c1, .
.
.
, cm) for a sentence x = w0, w1, .
.
.
, wn is a se-quence of configurations such that c0 = cx, cm ?
Cx, and, for every ci (1 ?
i ?
m), thereis a transition t ?
T such that ci = t(ci?1).
The dependency graph assigned to a sentencex = w0, w1, .
.
.
, wn by a sequence C0,m = (c0, c1, .
.
.
, cm) is the graph Gm defined by theterminal configuration cm.Assume that there exists a transition scoring function, s : C ?
T ?
R. The score of atransition t in a configuration c, s(c, t), represents the likelihood of taking transition t outof configuration c in a transition sequence leading to the optimal dependency graph forthe given sentence.
This score is usually defined by a classifier g taking as input a highdimensional feature representation of the configuration, s(c, t) = g(f(c), t).Given a transition scoring function, the parsing problem consists in finding a ter-minal configuration cm ?
Cx, starting from the initial configuration cx and taking theoptimal transition t?
out of every configuration c:t?
= arg maxt?Ts(c, t)2 http://mstparser.sourceforge.net.204McDonald and Nivre Analyzing and Integrating Dependency ParsersThis can be seen as a greedy search for the optimal dependency graph, based on asequence of locally optimal decisions in terms of the transition system.By way of example, we consider the transition system first presented in Nivre(2003), where a parser configuration is a triple c = (?,?, A), consisting of a stack ?of partially processed nodes, a buffer ?
of remaining input nodes, and a set A oflabeled dependency arcs.
The initial configuration for a sentence x = w0, w1, .
.
.
, wn iscx = ([0], [1, .
.
.
, n], ?)
and the set of terminal configurations Cx contains all configu-rations of the form c = (?, [ ], A) (that is, all configurations with an empty buffer andwith arbitrary ?
and A).
The set T of transitions for this system is specified in Figure 3.The transitions LEFT-ARCl and RIGHT-ARCl extend the arc set A with an arc (labeled l)connecting the top node i on the stack and the first node j of the buffer.
In the case ofLEFT-ARCl, the node i becomes the dependent and is also popped from the stack; inthe case of RIGHT-ARCl, the node j becomes the dependent and is also pushed onto thestack.
The REDUCE transition pops the stack (and presupposes that the top node hasalready been attached to its head in a previous RIGHT-ARCl transition), and the SHIFTtransition extracts the first node of the buffer and pushes it onto the stack.This system can derive any projective dependency tree G for an input sentencex and in doing so always adds arcs as early as possible.
For this reason, the systemis often referred to as arc-eager.
When coupled with the greedy deterministic parsingstrategy, the system guarantees termination after at most 2n transitions (for a sentenceof length n), which means that the time complexity is O(n) given that transitions can beperformed in constant time.
The dependency graph given at termination is guaranteedto be acyclic and projective and to satisfy dependency graph conditions 1?3, whichmeans that it can always be turned into a well-formed dependency graph by addingarcs (0, i, lr) for every node i = 0 that is a root in the output graph (where lr is a spe-cial label for root modifiers).
Whereas the initial formulation in Nivre (2003) was lim-ited to unlabeled dependency graphs, the system was extended to labeled graphs inNivre, Hall, and Nilsson (2004), and Nivre and Nilsson (2005) showed how the restric-tion to projective dependency graphs could be lifted by using graph transformationFigure 3Transitions for dependency parsing (with preconditions).205Computational Linguistics Volume 37, Number 1techniques to pre-process training data and post-process parser output, a techniquecalled pseudo-projective parsing.
Transition systems that derive non-projective treesdirectly have been explored by Attardi (2006) and Nivre (2007, 2009), among others.To learn a scoring function, transition-based parsers use discriminative learningmethods, such as memory-based learning or support vector machines.
The trainingdata are obtained by constructing transition sequences corresponding to gold standardparses from a treebank.
The typical learning procedure is local because only singletransitions are scored?not entire transition sequences?but more global optimizationmethods have also been proposed.
The primary advantage of these models is that thefeature representation is not restricted to a limited number of graph arcs but can takeinto account the entire dependency graph built so far, including previously assignedlabels, and still support efficient inference and learning.
The main disadvantage is thatthe greedy parsing strategy may lead to error propagation as false early predictions caneliminate valid trees due to structural constraints and are also used to create featureswhen making future predictions.
Using beam search instead of strictly deterministicparsing can to some extent alleviate this problem but does not eliminate it.The specific transition-based system studied in this work is that presented by Nivreet al (2006), which uses the projective, arc-eager system described here in combina-tion with pseudo-projective parsing, which uses support vector machines to learn thescoring function for transitions and which uses greedy, deterministic one-best search atparsing time.
We will refer to this system as MaltParser (or Malt for short), which is alsothe name of the freely available implementation.32.4 ComparisonIn the previous two sections we have outlined the theoretical characteristics of canonicalgraph-based and transition-based dependency parsing systems.
From now on, ourexperiments will rely on two standard implementations: MSTParser, a graph-basedsystem, and MaltParser, a transition-based system.
Here we contrast the two parsingsystems with respect to how they are trained, how they produce dependency trees fornew sentences, and what kinds of features they use.Training Algorithms.
Both systems use large-margin learning for linear classifiers.
MST-Parser uses on-line algorithms (McDonald, Crammer, and Pereira 2005; Crammer et al2006) and MaltParser uses support vector machines (Cortes and Vapnik 1995).
Theprimary difference is that MaltParser trains the model to make a single classificationdecision (create arc, shift, reduce, etc.
), whereas MSTParser trains the model to maxi-mize the global score of correct graphs relative to incorrect graphs.
It has been arguedthat locally trained algorithms can suffer from label bias issues (Lafferty, McCallum,and Pereira 2001).
However, it is expensive to train global models since the complexityof learning is typically proportional to inference.
In addition, MaltParser makes use ofkernel functions, which eliminates the need for explicit conjunctions of features.Inference.
MaltParser uses a transition-based inference algorithm that greedily choosesthe best parsing decision based on a trained classifier and current parser history.
MST-Parser instead uses exhaustive search over a dense graphical representation of the3 http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.206McDonald and Nivre Analyzing and Integrating Dependency Parserssentence to find the dependency graph that maximizes the score.
On the one hand,the greedy algorithm is far quicker computationally (O(n) vs. O(n2) for the Chu-Liu-Edmonds algorithm and O(n3) for Eisner?s algorithm).
On the other hand, it may beprone to error propagation when early incorrect decisions negatively influence theparser at later stages.
In particular, MaltParser uses the projective arc-eager transitionsystem first described in Nivre (2003), which has consequences for the form of errorpropagation we may expect to see because the system determines the order in whicharcs must be added to the graph.
On the one hand, if an arc (i, m, l) covers anotherarc ( j, k, l?)
(i.e., i ?
j and k ?
m), then the smaller arc ( j, k, l?)
has to be added to thegraph first (because of projectivity).
On the other hand, if two arcs (i, j, l) and (k, m, l?
)do not overlap, then the leftmost arc has to be added first (because of arc-eagerness).Therefore, we can expect error propagation from shorter to longer overlapping arcs andfrom preceding to succeeding arcs.Feature Representation.
Due to the nature of their inference and training algorithms, thefeature representations of the two systems differ substantially.
MaltParser can introducea rich feature space based on the history of previous parser decisions.
This is becausethe greedy nature of the algorithm allows it to fix the structure of the graph anduse this structure to help improve future parsing decisions.
By contrast, MSTParser isforced to restrict the scope of features to a single or pair of nearby parsing decisionsin order to make exhaustive inference tractable.
As a result, the feature representationavailable to the locally trained greedy models is much richer than the globally trainedexhaustive models.
Concisely, we can characterize MSTParser as using global trainingand inference with local features and MaltParser as using local training and inferencewith global features.
(For more information about the features used in the two systems,see Sections 3.2 and 3.3.
)These differences highlight an inherent trade-off between exhaustive inference algo-rithms plus global learning and expressiveness of feature representations.
MSTParserfavors the former at the expense of the latter and MaltParser the opposite.
Whenanalyzing, and ultimately explaining, the empirical difference between the systems,understanding this trade-off will be of central importance.3.
Experimental SetupThe experiments presented in this article are all based on data from the CoNLL-Xshared task (Buchholz and Marsi 2006).
In this section we first describe the task and theresources created there and then describe how MSTParser and MaltParser were trainedfor the task, including feature representations and learning algorithms.3.1 The CoNLL-X Shared TaskThe CoNLL-X shared task (Buchholz and Marsi 2006) was a large-scale evaluationof data-driven dependency parsers, with data from 13 different languages and 19 par-ticipating systems.
The data sets were quite heterogeneous, both with respect to sizeand with respect to linguistic annotation principles, and the best reported parsingaccuracy varied from 65.7% for Turkish to 91.7% for Japanese.
The official evaluationmetric was the labeled attachment score (LAS), defined as the percentage of tokens,207Computational Linguistics Volume 37, Number 1Table 2Data sets.
Tok = number of tokens (?1000); Sen = number of sentences (?1000); T/S = tokens persentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speechtags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of morphosyntacticfeatures (split into atoms); Dep = number of dependency types; NPT = proportion ofnon-projective dependencies/tokens (%); NPS = proportion of non-projective dependencygraphs/sentences (%).Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPSArabic 54 1.5 37.2 yes 14 19 19 27 0.4 11.2Bulgarian 190 14.4 14.8 no 11 53 50 18 0.4 5.4Chinese 337 57.0 5.9 no 22 303 0 82 0.0 0.0Czech 1,249 72.7 17.2 yes 12 63 61 78 1.9 23.2Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4German 700 39.2 17.8 no 52 52 0 46 2.3 27.8Japanese 151 17.0 8.9 no 20 77 0 7 1.1 5.3Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2Spanish 89 3.3 27.0 yes 15 38 33 21 0.1 1.7Swedish 191 11.0 17.3 no 37 37 0 56 1.0 9.8Turkish 58 5.0 11.5 yes 14 30 82 25 1.5 11.6excluding punctuation, that are assigned both the correct head and the correct depen-dency label.4The outputs of all systems that participated in the shared task are available fordownload and constitute a rich resource for comparative error analysis.
In Section 4,we will use the outputs of MSTParser and MaltParser for all 13 languages, togetherwith the corresponding gold standard graphs used in the evaluation, as the basis for anin-depth error analysis designed to answer Question 2 from Section 1.
In Section 6, wewill then evaluate our stacking-based parsers on the same data sets and repeat the erroranalysis.
This will allow us to compare the error profiles of the new and old systems ata much finer level of detail than in standard evaluation campaigns.Table 2 gives an overview of the training sets available for the 13 languages.
Firstof all, we see that training set size varies from over 1.2 million words and close to73,000 sentences for Czech to only 29,000 words and 1,500 sentences for Slovene.
Wealso see that the average sentence length varies from close to 40 words for Arabic, usingslightly different principles for sentence segmentation than the other languages, to lessthan 10 words for Japanese, where the data consist of transcribed spoken dialogues.Differences such as these can be expected to have a large impact on the parsing accuracyobtained for different languages, and it is probably significant that Japanese has thehighest top score of all languages, whereas Arabic has the second lowest.
We also seethat the amount of information available in the input, in the form of lemmas (Lem),coarse and fine part-of-speech tags (CPoS, PoS), and morphosyntactic features (MSF)varies considerably, as does the granularity of the dependency label sets (Dep).
Fi-nally, the proportion of non-projective structures, whether measured on the token level(NPT) or on the sentence level (NPS), is another important source of variation.4 In addition, results were reported for unlabeled attachment score (UAS) (tokens with the correct head)and label accuracy (LA) (tokens with the correct label).208McDonald and Nivre Analyzing and Integrating Dependency ParsersThe final test set for each language has been standardized in size to about 5,000words, which makes it possible to evaluate the performance of a system over all lan-guages by simply concatenating the system?s output for all test sets and comparing thisto the concatenation of the gold standard test sets.
Thus, most of the statistics used in thesubsequent error analysis are based on the concatenation of all test sets.
Because some ofthe phenomena under study are relatively rare, this allows us to get more reliable esti-mates, even though these estimates inevitably hide important inter-language variation.Analyzing individual languages in more detail would be an interesting complementarystudy but is beyond the scope of this article.3.2 Training MSTParserMSTParser operates primarily over arc-scores, s(i, j, l), which are parameterized by alinear combination of a parameter vector, w, and a corresponding feature vector for thearc, f(i, j, l).
We use a two-stage approach to training.
The first-stage learns a model topredict unlabeled dependency trees for a sentence.
Thus, arc scores do not conditionon possible labels and are parameterized by features only over the head-modifier pair,s(i, j) = w ?
f(i, j).
As a result, for the first stage of training the parser, we must definethe feature representation f(i, j), which is outlined in Table 3(a) and Table 3(b) for apotential unlabeled arc (i, j).
These features represent both information about the headand modifier in the dependency relation as well as the context of the dependency vialocal part-of-speech information.
We include context part-of-speech features for boththe fine-grained and coarse-grained tags (when available).As mentioned in Section 2.2, the implementation of MSTParser used in our exper-iments also contains features over adjacent arcs, (i, j) and (i, k), which we will denotecompactly as (i, j  k).
Scores for adjacent arcs are also defined as a linear combinationbetween weights and a feature vector s(i, j  k) = w ?
f(i, j  k), thus requiring us to de-fine the feature representation f(i, j  k), which is outlined in Table 3(c).
These featuresTable 3Features for MSTParser.
?
indicates a conjunction of features.
?
indicates that all back-offversions of a conjunction feature are included as well.
A back-off version of a conjunction featureis one where one or more base features are disregarded.
?
indicates that all back-off versions areincluded where a single base feature is disregarded.Base features for sentence: x = w0, w1, .
.
.
, wnLexical features: Identity of wi, wi ?
xAffix features: 3-gram lexical prefix/suffix identity of Pref(wi)/Suff(wi), wi ?
xPart-of-speech features: Identity of PoS(wi), wi ?
xMorphosyntactic features: For all morphosyntactic features MSFk for a word wi, identity of MSFk(wi), wi ?
xLabel features: Identity of l in some labeled arc (i, j, l)(a) Head-modifier features for unlabeled arc (i, j)wi ?
PoS(wi ) ?
wj ?
PoS(wj ) ?Pref(wi ) ?
PoS(wi ) ?
Pref(wj ) ?
PoS(wj ) ?Suff(wi ) ?
PoS(wi ) ?
Suff(wj ) ?
PoS(wj ) ?
?k, k?
: MSFk(wi ) ?
PoS(wi ) ?
MSFk?
(wj ) ?
PoS(wj ) ?
(b) PoS-context features for unlabeled arc (i, j)?k, i < k < j : PoS(wi ) ?
PoS(wk ) ?
PoS(wj )PoS(wi?1) ?
PoS(wi ) ?
PoS(wj?1) ?
PoS(wj ) ?PoS(wi?1) ?
PoS(wi ) ?
PoS(wj ) ?
PoS(wj+1 ) ?PoS(wi ) ?
PoS(wi+1 ) ?
PoS(wj?1) ?
PoS(wj ) ?PoS(wi ) ?
PoS(wi+1 ) ?
PoS(wj ) ?
PoS(wj+1) ?
(c) Head-modifier features for unlabeled arc pair (i, jk)wj ?
wkwj ?
PoS(wk )PoS(wj ) ?
wkPoS(wj ) ?
PoS(wk )PoS(wi ) ?
PoS(wj ) ?
PoS(wk )(d) Arc-label features for labeled arc (i, j, l)wi ?
PoS(wi ) ?
wj ?
PoS(wj ) ?
l ?
?k, i < k < j : PoS(wi ) ?
PoS(wk ) ?
PoS(wj ) ?
lPoS(wj?1) ?
PoS(wj ) ?
PoS(wj+1 ) ?
l ?PoS(wi?1) ?
PoS(wi ) ?
PoS(wi+1 ) ?
l ?209Computational Linguistics Volume 37, Number 1attempt to capture likely properties about adjacent arcs in the tree via their lexical andpart-of-speech information.
Finally, all features in Table 3(a)?
(c) contain two versions.The first is the standard feature outlined in the table, and the second is the featureconjoined with both the direction of dependency attachment (left or right) as well as thebucketed distance between the head and modifier in buckets of 0 (adjacent), 1, 2, 3, 4,5?9, and 10+.For the second stage label classifier we use a log-linear classifier, which is againparameterized by a vector of weights and a corresponding feature vector s(l|i, j) =w ?
f(i, j, l), where the score of a label l is now conditioned on a fixed dependency arc(i, j) produced from the first-stage unlabeled parser.
The feature representation f(i, j, l) isdefined in Table 3(d).
These features provide the lexical and part-of-speech context fordetermining whether a given arc label is suitable for a given head and modifier.
Eachfeature in Table 3(d) again has two versions, except this time the second version is onlyconjoined with attachment direction.These feature representations were used to train an on-line large-margin unlabeleddependency parser (McDonald, Crammer, and Pereira 2005; McDonald and Pereira2006) and a log-linear arc-labeler (Berger, Pietra, and Pietra 1996) regularized with azero mean Gaussian prior with the variance hyper-parameter set to 1.0.
The unlabeleddependency parser was trained for 10 iterations and the log-linear arc-labeler wastrained for 100 iterations.
The feature sets and model hyper-parameters were fixed forall languages.
The only exception is that features containing coarse part-of-speech ormorphosyntactic information were ignored if this information was not available in acorresponding treebank.3.3 Training MaltParserTraining MaltParser amounts to estimating a function for scoring configuration-transition pairs (c, t), represented by a feature vector f(c, t) ?
Rk.
Features are definedin terms of arbitrary properties of the configuration c, including the state of the stack?c, the input buffer ?c, and the partially built dependency graph Gc (represented in theconfiguration by the arc set Ac).
In particular, many features involve properties of thetwo target tokens, the token on top of the stack ?c (denoted ?0c ) and the first token inthe input buffer ?c (denoted ?0c ), which are the two tokens that may become connectedby a dependency arc through the transition out of c. The basic feature representationused for all languages in the CoNLL-X shared task included three groups of features:5 Part-of-speech features: Identity of PoS(w), w ?
{?0c ,?1c ,?0c ,?1c ,?2c ,?3c}. Lexical features: Identity of w, w ?
{?0c ,?0c ,?1c} or (w,?0c , l) ?
Gc. Arc features: Identity of l, (w, w?, l) ?
Gc and w ?
{?0c ,?0c} or w?
?
{?0c}.Note in particular that features can be defined with respect to the partially built de-pendency graph Gc.
This is most obvious for the arc features, which extract the labelsof particular arcs in the graph, but it is also true of the last lexical feature, which picksout the word form of the syntactic head of the word on top of the stack.
This is pre-cisely what gives transition-based parsers a richer feature space than their graph-based5 We use the notation ?ic and ?ic to denote the ith element from the top/head of the stack/buffer (withindex 0 for the first element).210McDonald and Nivre Analyzing and Integrating Dependency Parserscounterparts, even though graph-defined features are usually limited to a fairly smallregion of the graph around ?0c and ?0c , such as their leftmost and rightmost dependentsand their syntactic head (if available).Over and above the basic features described here, additional features were addedfor some languages depending on availability in the training data.
This included thefollowing: Coarse part-of-speech features: Identity of CPoS(w), w ?
{?0c ,?0c ,?1c}. Lemma features: Identity of Lem(w), w ?
{?0c ,?0c ,?1c}. Morphosyntactic features: Identity of MSF(w), w ?
{?0c ,?0c ,?1c}.Additional feature selection experiments were carried out for each language to theextent that time permitted.
Complete information about feature representations can befound in Nivre et al (2006) and on the companion web site.6The feature representations described here were used to train support vector ma-chines as implemented in the LIBSVM library (Chang and Lin 2001), with a quadratickernel K(xi, xj) = (?xTi xj + r)2 and LIBSVM?s built-in one-versus-one strategy for multi-class classification, converting symbolic features to numerical ones using the standardtechnique of binarization.7 One thing to note is that the quadratic kernel implicitly addsfeatures corresponding to pairs of explicit features, thus obviating the need for explicitfeature conjunctions as seen in the feature representations of MSTParser.4.
Error AnalysisA primary goal of this study is to characterize the errors made by standard data-drivendependency parsing models.
To that end, this section presents a number of experimentsthat relate parsing errors to a set of linguistic and structural properties of the input andpredicted/gold standard dependency trees.
We argue throughout that the results of thisanalysis can be correlated to specific theoretical aspects of each model?in particular thetrade-off previously highlighted in Section 2.4.For simplicity, all experiments report labeled parsing metrics (either accuracy, preci-sion, or recall).
Identical experiments using unlabeled parsing accuracies did not revealany additional information.
Statistical significance was measured?for each metric ateach point along the operating curve?by employing randomized stratified shuffling atthe instance level using 10,000 iterations.8 Furthermore, all experiments report aggre-gate statistics over the data from all 13 languages together, as explained in Section 3.Finally, in all figures and tables, MSTParser and MaltParser are referred to as MST andMalt, respectively, for short.4.1 Length FactorsIt is well known that parsing systems tend to have lower accuracies for longer sentences.This is primarily due to the increased presence of complex syntactic constructions6 http://maltparser.org/conll/conllx.7 For details about parameter settings, we again refer to Nivre et al (2006) and the companion web sitehttp://maltparser.org/conll/conllx/.8 This is the method used by the CoNLL-X shared task on dependency parsing.211Computational Linguistics Volume 37, Number 1Figure 4Accuracy relative to sentence length.
Differences statistically significant (p < 0.05) at nopositions.involving prepositions, conjunctions, and multi-clause sentences.
Figure 4 shows theaccuracy of both parsing models relative to sentence length (in bins of size 10: 1?10,11?20, etc.).
System performance is almost indistinguishable, but MaltParser tends toperform better on shorter sentences, which require the greedy inference algorithm tomake fewer parsing decisions.
As a result, the chance of error propagation is reducedsignificantly when parsing these sentences.
However, if this was the only differencebetween the two systems, we would expect them to have equal accuracy for shortersentences.
The fact that MaltParser actually has higher accuracy when the likelihoodof error propagation is reduced is probably due to its richer feature space relative toMSTParser.Another interesting property is accuracy relative to dependency length as opposedto sentence length.
We define the length of a dependency from word wi to word wjas equal to |i ?
j|.
Longer dependencies typically represent modifiers of the root orthe main verb in a sentence.
Shorter dependencies are often modifiers of nouns suchas determiners or adjectives or pronouns modifying their direct neighbors.
Figure 5measures the precision and recall for each system relative to dependency lengths in thepredicted and gold standard dependency graphs.
Precision represents the percentageof predicted arcs of length d that were correct.
Recall measures the percentage of goldstandard arcs of length d that were predicted.Here we begin to see separation between the two systems.
MSTParser is far moreprecise for longer dependency arcs, whereas MaltParser does better for shorter depen-dency arcs.
This behavior can be explained using the same reasoning as above: Shorterdependency arcs are usually created first in the greedy parsing procedure of MaltParserand are less prone to error propagation.
In contrast, longer dependencies are typicallyconstructed at the later stages of the parsing algorithm and are affected more by errorpropagation.
Theoretically, MSTParser should not perform better or worse for arcs ofany length.
However, due to the fact that longer dependencies are typically harderto parse, there is still a degradation in performance for MSTParser?up to 20% in the212McDonald and Nivre Analyzing and Integrating Dependency ParsersFigure 5Dependency arc precision/recall relative to predicted/gold dependency length.
Precisionstatistically significant (p < 0.05) at 1, 2, 4, 7, 8, 10 through >14.
Recall statistically significant(p < 0.05) at >14.extreme.
However, the precision curve for MSTParser is much flatter than MaltParser,which sees a drop of up to 40% in the extreme.
Note that even though the area under thecurve is much larger for MSTParser, the number of dependency arcs with a length >10is much smaller than the number with length <10, which is why the overall accuracy ofthe two systems is nearly identical.4.2 Graph FactorsThe structure of the predicted and gold standard dependency graphs can also provideinsight into the differences between each model.
For example, measuring accuracy forarcs relative to their distance to the artificial root node will detail errors at differentlevels of the dependency graph.
For a given arc, we define this distance as the numberof arcs in the reverse path from the modifier of the arc to the root.
For example, thedependency arc from ROOT to is in Figure 1 would have a distance of 1 and the arc fromhearing to A a distance of 3.
Figure 6 plots the precision and recall of each system for arcsof varying distance to the root.
Precision is equal to the percentage of dependency arcsFigure 6Dependency arc precision/recall relative to the predicted/gold distance to root.
Precisionstatistically significant (p < 0.05) at 1, 2, 4 through >6.
Recall statistically significant (p < 0.05) at1, 2, 3.213Computational Linguistics Volume 37, Number 1in the predicted graph that are at a distance of d and are correct.
Recall is the percentageof dependency arcs in the gold standard graph that are at a distance of d and werepredicted.Figure 6 clearly shows that for arcs close to the root, MSTParser is much more pre-cise than MaltParser, and vice versa for arcs further away from the root.
This is probablythe most compelling graph given in this study because it reveals a clear distinction:MSTParser?s precision degrades as the distance to the root increases whereas Malt-Parser?s precision increases.
The plots essentially run in opposite directions crossingnear the middle.
Dependency arcs further away from the root are usually constructedearly in the parsing algorithm of MaltParser.
Again a reduced likelihood of error propa-gation coupled with a rich feature representation benefits that parser substantially.
Fur-thermore, MaltParser tends to over-predict root modifiers, which comes at the expenseof its precision.
This is because all words that the parser fails to attach as modifiers areautomatically connected to the root, as explained in Section 2.3.
Hence, low precisionfor root modifiers (without a corresponding drop in recall) is an indication that thetransition-based parser produces fragmented parses.The behavior of MSTParser is a little trickier to explain.
One would expect thatits errors should be distributed evenly over the graph.
For the most part this is true,with the exception of spikes at the ends of the plot.
The high performance for rootmodification (distance of 1) can be explained through the fact that this is typically alow-entropy decision?usually the parsing algorithm has to determine the main verbfrom a small set of possibilities.
On the other end of the plot there is a slight downwardtrend for arcs of distance greater than 3 from the root.
An examination of dependencylength for predicted arcs shows that MSTParser predicts many more arcs of length 1than MaltParser, which naturally leads to over-predicting more arcs at larger distancesfrom the root due to the presence of chains, which in turn will lower precision for thesearcs.
In ambiguous situations, it is not surprising that MSTParser predicts many length-1 dependencies, as this is the most common dependency length across treebanks.
Thus,whereas MaltParser pushes difficult parsing decisions higher in the graph, MSTParserappears to push these decisions lower.The final graph property we will examine aims to quantify the local neighborhoodof an arc within a dependency graph.
Two dependency arcs, (i, j, l) and (i?, j?, l?
), are clas-sified as siblings if they represent syntactic modifications of the same word (i.e., i = i?
).In Figure 1 the arcs from the word is to the words hearing, scheduled, and the period areall considered siblings under this definition.
Figure 7 measures the precision and recallof each system relative to the number of predicted and gold standard siblings of eacharc.
There is not much to distinguish between the parsers on this metric.
MSTParser isslightly more precise for arcs that are predicted with more siblings, whereas MaltParserhas slightly higher recall on arcs that have more siblings in the gold standard tree.
Arcscloser to the root tend to have more siblings, which ties this result to the previous ones.4.3 Linguistic FactorsIt is important to relate each system?s accuracy to a set of linguistic categories, suchas parts of speech and dependency types.
However, given the important typologicaldifferences that exist between languages, as well as the diversity of annotation schemesused in different treebanks, it is far from straightforward to compare these categoriesacross languages.
Nevertheless, we have made an attempt to distinguish a few broadcategories that are cross-linguistically identifiable, based on the available documenta-tion of the treebanks used in the shared task.214McDonald and Nivre Analyzing and Integrating Dependency ParsersFigure 7Dependency arc precision/recall relative to the number of predicted/gold siblings.
Precisionstatistically significant (p < 0.05) at 0, 4, 6, 7, 9.
Recall statistically significant (p < 0.05) at 5, >9.For parts of speech, we distinguish verbs (including both main verbs and auxil-iaries), nouns (including proper names), pronouns (sometimes also including deter-miners), adjectives, adverbs, adpositions (prepositions, postpositions), and conjunctions(both coordinating and subordinating).
For dependency types, we have only managedto distinguish a general root category (for labels used on arcs from the artificial root,including either a generic label or the label assigned to predicates of main clauses, whichare normally verbs), a subject category, and an object category (including both directand indirect objects).
Unfortunately, we had to exclude many interesting types thatcould not be identified with high enough precision across languages, such as adverbials,which cannot be clearly distinguished in annotation schemes that subsume them undera general modifier category, and coordinate structures, which are sometimes annotatedwith special dependency types, sometimes with ordinary dependency types found alsoin non-coordinated structures.Table 4(a) shows the accuracy of the two parsers for different parts of speech.This figure measures labeled dependency accuracy relative to the part of speech ofthe modifier word in a dependency relation.
We see that MaltParser has slightly betteraccuracy for nouns and pronouns, and MSTParser does better on all other categories, inparticular conjunctions.
This pattern is consistent with previous results insofar as verbsTable 4(a) Accuracy relative to dependent part of speech.
(b) Precision/recall for different dependencytypes.Part of Speech MST MaltVerb 82.6 81.9Noun 80.0 80.7Pronoun 88.4 89.2Adjective 89.1 87.9Adverb 78.3 77.4Adposition 69.9 68.8Conjunction 73.1 69.8(a)Dependency MST MaltType Precision/Recall Precision/RecallRoot 89.9 / 88.7 84.7 / 87.5Subject 79.9 / 78.9 80.3 / 80.7Object 76.5 / 77.7 77.2 / 77.6(b)215Computational Linguistics Volume 37, Number 1and conjunctions are often involved in dependencies closer to the root that span longerdistances, whereas nouns and pronouns are typically attached to verbs and thereforeoccur lower in the graph and with shorter distances.
Thus, the average distance to theroot is 3.1 for verbs and 3.8 for conjunctions, but 4.7 for nouns and 4.9 for pronouns;the average dependency length is 4.2 for verbs, 4.8 for conjunctions, 2.3 for nouns,and 1.6 for pronouns.
Adverbs resemble verbs and conjunctions with respect to rootdistance (3.7) but group with nouns and pronouns for dependency length (2.3), so itappears that the former is more important here.
Furthermore, adverb modifiers have2.4 siblings on average, which is greater than the sibling average for conjunctions (2.1),adpositions (1.9), pronouns (1.7), verbs (1.3), nouns (1.3), and adjectives (1.2).
Thiswould be consistent with the graph in Figure 7.Adpositions and especially adjectives constitute a puzzle.
With a root distance of 4.4and 5.2, respectively, a dependency length of 2.5/1.5 and a sibling average of 1.9/1.2, wewould expect MaltParser to do better than MSTParser for these categories.
Adpositionsdo tend to have a high number of siblings on average, which could explain MSTParser?sperformance on that category.
However, adjectives on average occur the furthest awayfrom the root, have the shortest dependency length, and the fewest siblings.
At present,we do not have an explanation for this behavior.Finally, in Table 4(b), we consider precision and recall for dependents of the rootnode (mostly verbal predicates), and for subjects and objects.
As already noted, MST-Parser has considerably better precision (and slightly better recall) for the root category,but MaltParser has an advantage for the nominal categories, especially subjects.
A pos-sible explanation for the latter result, in addition to the length-based and graph-basedfactors invoked before, is that MaltParser integrates labeling into the parsing process,which means that previously assigned dependency labels can be used as features.This may sometimes be important to disambiguate subjects and objects, especially infree-word order languages where a dependent?s position relative to the verb does notdetermine its syntactic role.4.4 DiscussionThe experiments in this section highlight the fundamental trade-off between globaltraining and exhaustive inference on the one hand and expressive feature representa-tions on the other.
Error propagation is an issue for MaltParser, which typically performsworse on long sentences, long dependency arcs, and arcs higher in the graphs.
But thisis offset by the rich feature representation available to these models that result in betterdecisions for frequently occurring classes of arcs like short dependencies or subjectand object modifiers.
The errors for MSTParser are spread a little more evenly.
Thisis expected, as the inference algorithm and feature representation should not prefer onetype of arc over another.What has been learned?
It was already known that the two systems make differenterrors through the work of Sagae and Lavie (2006).
However, in that work an arc-basedmajority voting scheme was used that took only limited account of the properties ofthe words connected by a dependency arc (more precisely, the overall accuracy of eachparser for the part of speech of the dependent).
The analysis in this work not only showsthat the errors made by each system are different, but that they are different in a waythat can be predicted and quantified.
This is an important step in parser development.By understanding the strengths and weaknesses of each model we have gained insightstowards new and better models for dependency parsing.216McDonald and Nivre Analyzing and Integrating Dependency ParsersTo get some upper bounds on the improvement that can be obtained by combiningthe strengths of each model, we can perform two oracle experiments.
Given the outputof the two systems, we can envision an oracle that can optimally choose which singleparse or combination of sub-parses to predict as a final parse.
For the first experimentthe oracle is provided with the single best parse from each system, say G = (V, A) andG?
= (V?, A?).
The oracle chooses a parse that has the highest number of correctly pre-dicted labeled dependency attachments.
In this situation, the oracle labeled attachmentscore is 84.5%.
In the second experiment the oracle chooses the tree that maximizes thenumber of correctly predicted dependency attachments, subject to the restriction thatthe tree must only contain arcs from A ?
A?.
This can be computed by setting the weightof an arc to 1 if it is in the correct parse and in the set A ?
A?.
All other arc weights areset to negative infinity.
One can then simply find the tree that has maximal sum of arcweights using directed spanning tree algorithms.
This technique is similar to the parservoting methods used by Sagae and Lavie (2006).
In this situation, the oracle accuracyis 86.9%.In both cases we see a clear increase in accuracy: 86.9% and 84.5% relative to 81%for the individual systems.
This indicates that there is still potential for improvement,just by combining the two existing models.
More interestingly, however, we can usethe analysis from this section to generate ideas for new models.
Below we sketch somepossible new directions:1.
Ensemble systems: The error analysis presented in this article could be usedas inspiration for more refined weighting schemes for ensemble systems ofthe kind proposed by Sagae and Lavie (2006), making the weights dependon a range of linguistic and graph-based factors.2.
Integrated/Hybrid systems: Rather than using an ensemble of severalindependent parsers, we may construct systems that trust different parsersin different situations, possibly based on the characteristics of the inputand predicted dependency trees.
The oracle results reported here showthat such an approach could potentially result in substantialimprovements.3.
Novel approaches: The theoretical analysis presented in this article revealsthat the two dominant approaches are each based on a particularcombination of training and inference methods, which raises the questionof which other combinations can fruitfully be explored.
For example, canwe construct globally trained, greedy, transition-based parsers?
Orgraph-based parsers with global features?
To some extent the formercharacterization fits the approach of Zhang and Clark (2008) and Huangand Sagae (2010), and the latter that of Riedel and Clarke (2006),Nakagawa (2007), and others.
The analysis presented in this sectionexplains the relative success of such approaches.In the next two sections we explore a model that falls into category 2.
The system wepropose uses a two-stage stacking framework, where a second-stage parser conditionson the predictions of a first-stage parser during inference.
The second-stage parser isalso learned with access to the first-stage parser?s decisions and thus learns when totrust the first-stage parser?s predictions and when to trust its own.
The method is not atraditional ensemble, because the parsers are not learned independently of one another.217Computational Linguistics Volume 37, Number 15.
Integrated ModelsAs just discussed, there are many conceivable ways of combining the two parsers,including more or less complex ensemble systems and voting schemes, which onlyperform the integration at parsing time.
However, given that we are dealing with data-driven models, it should be possible to integrate at learning time, so that the twocomplementary models can learn from one another.
In this article, we propose to do thisby letting one model generate features for the other in a stacked learning framework.Feature-based integration in this sense has previously been exploited for depen-dency parsing by McDonald (2006), who trained an instance of MSTParser using fea-tures generated by the parsers of Collins (1999) and Charniak (2000), which improvedunlabeled accuracy by 1.7 percentage points on data from the Penn Treebank.
In otherNLP domains, feature-based integration has been used by Taskar, Lacoste-Julien, andKlein (2005), who trained a discriminative word alignment model using features de-rived from the IBM models, by Florian et al (2004), who trained classifiers on auxiliarydata to guide named entity classifiers, and by others.Feature-based integration also has points in common with co-training, which hasbeen applied to syntactic parsing by Sarkar (2001) and Steedman et al (2003), amongothers.
The difference, of course, is that standard co-training is a weakly supervisedmethod, where the first-stage parser?s predictions replace, rather than complement, thegold standard annotation during training.
Feature-based integration is also similar toparse reranking (Collins 2000), where one parser produces a set of candidate parsesand a second-stage classifier chooses the most likely one.
However, feature-based in-tegration is not explicitly constrained to any parse decisions that the first-stage parsermight make.
Furthermore, as only the single most likely parse is used from the first-stage model, it is significantly more efficient than reranking, which requires both com-putationally and conceptually more complex parsing algorithms (Huang and Chiang2005).5.1 Parser Stacking with Rich FeaturesAs explained in Section 2, both models essentially learn a scoring function s : X ?
R,where the domain X is different for the two models.
For the graph-based model, X isthe set of possible dependency arcs (i, j, l); for the transition-based model, X is the set ofpossible configuration-transition pairs (c, t).
But in both cases, the input is representedby a k-dimensional feature vector f : X ?
Rk.
In a stacked parsing system we simplyextend the feature vector for one model, called the base model, with a certain numberof features generated by the other model, which we call the guide model in this context.The additional features will be referred to as guide features, and the version of the basemodel trained with the extended feature vector will be called the guided model.
Theidea is that the guided model should be able to learn in which situations to trust theguide features, in order to exploit the complementary strength of the guide model, sothat performance can be improved with respect to the base model.The exact form of the guide features depends on properties of the base model andwill be discussed in Sections 5.2?5.3, but the overall scheme for the stacked parsingmodel can be described as follows.
Assume as input a training set T = {(xt, Gxt )}|T|t=1 ofinput sentences xt and corresponding gold standard dependency trees Gxt .
In order totrain the guide model we use a cross-validation scheme and divide T into n differentdisjoint subsets Ti (i.e., T =?ni=1 Ti).
Let M[T] be the result of training the model M on T218McDonald and Nivre Analyzing and Integrating Dependency Parsersand let M[T](x) be the result of parsing a new input sentence x with M[T].
Now, considera guide model C, base model B, and guided model BC.
For each x in T, defineGCx = C[T ?
Ti](x) if x ?
TiGCx is the prediction of model C on training input x when C is trained on all the subsetsof T, except the one containing x.
The reason for using this cross-validation scheme isthat if C had been trained on all of T, then GCx would not be representative of the types oferrors that C might make when parsing sentence x.
Using cross-validation in this way issimilar to how it is used in parse reranking (Collins 2000).
Now, define a new training setof the form T?
= {(?xt, GCxt?, Gxt )}|T|t=1.
That is, T?
is identical to T, except that each traininginput x is augmented with the cross-validation prediction of model C. Finally, letBC = B[T?
]This means that, for every sentence x ?
T, BC has access at training time to both thegold standard dependency graph Gx and the graph GCx predicted by C. Thus, BC is ableto define guide features over GCx , which can prove beneficial if features over GCx can beused to discern when parsing model C outperforms or underperforms parsing modelB.
When parsing a new sentence x with BC, x is first parsed with model C[T] (this timetrained on the entire training set) to derive an input ?x, GCx ?, so that the guide featurescan be extracted also at parsing time.
This input is then passed through model BC.5.2 The Guided Graph-Based ModelThe graph-based model, MSTParser, learns a scoring function s(i, j, l) ?
R over labeleddependencies.
As described in Section 3.2, dependency arcs (or pairs of arcs) are repre-sented by a high dimensional feature vector f(i, j, l) ?
Rk, where f is typically a binaryfeature vector over properties of the arc as well as the surrounding input (McDonald,Crammer, and Pereira 2005; McDonald, Lerman, and Pereira 2006).
For the guidedgraph-based model, which we call MSTMalt, this feature representation is modified toinclude an additional argument GMaltx , which is the dependency graph predicted byMaltParser on the input sentence x.
Thus, the new feature representation will map an arcand the entire predicted MaltParser graph to a high dimensional feature representation,f(i, j, l, GMaltx ) ?
Rk+m.
These m additional features account for the guide features overthe MaltParser output.
The specific features used by MSTMalt are given in Table 5.All features are conjoined with the part-of-speech tags of the words involved in theTable 5Guide features for MSTMalt and MaltMST.MSTMalt ?
defined over (i, j, l) MaltMST ?
defined over (c, t)(?
= any label/node) (?
= any label/node)Is (i, j, ?)
in GMaltx ?
Is (?0c ,?0c , ?)
in GMSTx ?Is (i, j, l) in GMaltx ?
Is (?0c ,?0c , ?)
in GMSTx ?Is (i, j, ?)
not in GMaltx ?
Head direction for ?0c in GMSTx (left/right/ROOT)Is (i, j, l) not in GMaltx ?
Head direction for ?0c in GMSTx (left/right/ROOT)Identity of l?
such that (?, j, l?)
is in GMaltx ?
Identity of l such that (?,?0c , l) is in GMSTx ?Identity of l?
such that (i, j, l?)
is in GMaltx ?
Identity of l such that (?,?0c , l) is in GMSTx ?219Computational Linguistics Volume 37, Number 1dependency to allow the guided model to learn weights relative to different surfacesyntactic environments.
Features that include the arc label l are only included in thesecond-stage arc-labeler.
Though MSTParser is capable of defining features over pairsof arcs, we restrict the guide features to single arcs as this resulted in higher accuraciesduring preliminary experiments.5.3 The Guided Transition-Based ModelThe transition-based model, MaltParser, learns a scoring function s(c, t) ?
R over con-figurations and transitions.
The set of training instances for this learning problem is theset of pairs (c, t) such that t is the correct transition out of c in the transition sequencethat derives the correct dependency graph Gx for some sentence x in the training setT.
As described in Section 3.3, each training instance (c, t) is represented by a featurevector f(c, t) ?
Rk, where features are defined in terms of arbitrary properties of theconfiguration c.For the guided transition-based model, which we call MaltMST, training instancesare extended to triples (c, t, GMSTx ), where GMSTx is the dependency graph predicted bythe graph-based MSTParser for the sentence x to which the configuration c belongs.We define m additional guide features, based on properties of GMSTx , and extend thefeature vector accordingly to f(c, t, GMSTx ) ?
Rk+m.
The specific features used by MaltMSTare given in Table 5.
Unlike MSTParser, features are not explicitly defined to conjoinguide features with part-of-speech features.
These features are implicitly added throughthe polynomial kernel used to train the SVM.6.
Integrated Parsing ExperimentsIn this section, we present an experimental evaluation of the two guided models fol-lowed by a comparative error analysis including both the base models and the guidedmodels.
The data sets used in these experiments are identical to those used in Section 4.The guided models were trained according to the scheme explained in Section 5, withtwo-fold cross-validation when parsing the training data with the guide parsers.
Pre-liminary experiments suggested that cross-validation with more folds had a negligibleimpact on the results.
Models are evaluated by their labeled attachment score on the testset using the evaluation software from the CoNLL-X shared task with default settings.9Statistical significance was assessed using Dan Bikel?s randomized parsing evaluationcomparator with the default setting of 10,000 iterations.106.1 ResultsTable 6 shows the results, for each language and on average, for the two base models(MST, Malt) and for the two guided models (MSTMalt, MaltMST).
We also give oraclecombination scores based on both by taking the best graph or the best set of arcsrelative to the gold standard, as discussed in Section 4.4.
First of all, we see that bothguided models show a consistent increase in accuracy compared to their base model,even though the extent of the improvement varies across languages from about half apercentage point (MaltMST on Chinese) up to almost four percentage points (MaltMST on9 http://nextens.uvt.nl/?conll/software.html.10 http://www.cis.upenn.edu/?dbikel/software.html.220McDonald and Nivre Analyzing and Integrating Dependency ParsersTable 6Labeled attachment scores for base parsers and guided parsers (improvement in percentagepoints).OracleLanguage MST MSTMalt Malt MaltMST graph arcArabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09) 70.3 75.8Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18) 90.7 92.4Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52) 90.8 91.5Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76) 84.2 86.6Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66) 87.9 89.6Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32) 83.5 86.4German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84) 89.9 92.0Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55) 93.2 94.1Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04) 90.0 91.6Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94) 77.2 80.7Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12) 85.4 88.2Swedish 82.55 84.66 (+2.11) 84.58 84.31 (?0.27) 86.8 88.8Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70) 69.3 72.6Average 80.83 82.53 (+1.70) 80.75 82.01 (+1.27) 84.5 86.9Slovene).11 It is thus quite clear that both models have the capacity to learn from featuresgenerated by the other model.
However, it is also clear that the graph-based MST modelshows a somewhat larger improvement, both on average and for all languages exceptCzech, German, Portuguese, and Slovene.
Finally, given that the two base models hadthe best performance for these data sets at the CoNLL-X shared task, the guided modelsachieve a substantial improvement of the state of the art.12 Although there is no statis-tically significant difference between the two base models, they are both outperformedby MaltMST (p < 0.0001), which in turn has significantly lower accuracy than MSTMalt(p < 0.0005).An extension to the models described so far would be to iteratively integrate the twoparsers in the spirit of pipeline iteration (Hollingshead and Roark 2007).
For example,one could start with a Malt model, use it to train a guided MSTMalt model, then use thatas the guide to train a MaltMSTMalt model, and so forth.
We ran such experiments, butfound that accuracy did not increase significantly and in some cases decreased slightly.This was true regardless of which parser began the iterative process.
In retrospect, thisresult is not surprising.
Because the initial integration effectively incorporates knowl-edge from both parsing systems, there is little to be gained by adding additional parsersin the chain.6.2 Error AnalysisThe experimental results presented so far show that feature-based integration (stacking)is a viable approach for improving the accuracy of both graph-based and transition-based models for dependency parsing, but they say very little about how the integration11 The only exception to this pattern is the result for MaltMST on Swedish, where we see an unexpected dropin accuracy compared to the base model.12 Martins et al (2008) and Martins, Smith, and Xing (2009) report additional improvements.221Computational Linguistics Volume 37, Number 1benefits the two models and what aspects of the parsing process are improved as aresult.
In order to get a better understanding of these matters, we replicate parts ofthe error analysis presented in Section 4, but include both integrated models into theanalysis.
As in Section 4, for each of the four models evaluated, we aggregate errorstatistics for labeled attachment over all 13 languages together.Figure 8 shows accuracy in relation to sentence length, binned into 10-wordintervals (1?10, 11-20, etc.).
As mentioned earlier, Malt and MST have very similaraccuracy for short sentences but Malt degrades more rapidly with increasing sentencelength because of error propagation.
The guided models, MaltMST and MSTMalt, behavein a very similar fashion with respect to each other but both outperform their baseparser over the entire range of sentence lengths.
However, except for the two extremedata points (0?10 and 51?60) there is also a slight tendency for MaltMST to improve morefor longer sentences (relative to its base model) and for MSTMalt to improve more forshort sentences (relative to its base model).
Thus, whereas most of the improvement forthe guided parsers seems to come from a higher accuracy in predicting arcs in general,there is also some evidence that the feature-based integration allows one parser toexploit the strength of the other.Figure 9 plots precision (left) and recall (right) for dependency arcs of differentlengths (predicted arcs for precision, gold standard arcs for recall).
With respect to recall,the guided models appear to have a slight advantage over the base models for short andmedium distance arcs.
With respect to precision, however, there are two clear patterns.First, the graph-based models have better precision than the transition-based modelswhen predicting long arcs, as discussed earlier.
Secondly, both the guided models havebetter precision than their base model and, for the most part, also their guide model.In particular MSTMalt outperforms MST for all dependency lengths and is comparableto Malt for short arcs.
More interestingly, MaltMST outperforms both Malt and MST forarcs up to length 9, which provides evidence that MaltMST has learned specifically toFigure 8Accuracy relative to sentence length.
Differences between MST+Malt and MST statisticallysignificant (p < 0.05) at all positions.
Differences between Malt+MST and Malt statisticallysignificant (p < 0.05) at all positions.
Differences between MST+Malt and Malt+MSTstatistically significant (p < 0.05) at 11?20, 21?30, and 31?40.222McDonald and Nivre Analyzing and Integrating Dependency ParsersFigure 9Dependency arc precision/recall relative to predicted/gold for dependency length.
Precisionbetween MST+Malt and MST statistically significant (p < 0.05) at 1?7, 9?12, 14, and >14.Recall between MST+Malt and MST statistically significant (p < 0.05) at 1?7, 9, 14, and >14.Precision between Malt+MST and Malt statistically significant (p < 0.05) at 1?8, 10?13,and > 14.
Recall between Malt+MST and Malt statistically significant (p < 0.05) at 1?12, 14,and >14.
Precision between MST+Malt and Malt+MST statistically significant (p < 0.05) at 1 and9?>14.
Recall between MST+Malt and Malt+MST statistically significant (p < 0.05) at 1, 2, 3, 14,and >14.trust the guide features from MST for longer dependencies (those greater than length 4)and its own base features for shorter dependencies (those less than or equal to length 4).However, for dependencies of length greater than 9, the performance of MaltMST beginsto degrade.
Because the absolute number of dependencies of length greater than 9 inthe training sets is relatively small, it might be difficult for MaltMST to learn from theguide parser in these situations.
Interestingly, both models seem to improve most inthe medium range (roughly 8?12 words), although this pattern is clearer for MSTParserthan for MaltParser.Figure 10 shows precision (left) and recall (right) for dependency arcs at differentdistances from the root (predicted arcs for precision, gold standard arcs for recall).Figure 10Dependency arc precision/recall relative to predicted/gold for distance to root.
Precisionbetween MST+Malt and MST statistically significant (p < 0.05) at 1?6.
Recall between MST+Maltand MST statistically significant (p < 0.05) at all positions.
Precision between Malt+MST andMalt statistically significant (p < 0.05) at 1?4.
Recall between Malt+MST and Malt statisticallysignificant (p < 0.05) at all positions.
Precision between MST+Malt and Malt+MST statisticallysignificant (p < 0.05) at 1, 2, 3, 6, and >6.
Recall between MST+Malt and Malt+MSTstatistically significant (p < 0.05) at 4 and >6.223Computational Linguistics Volume 37, Number 1Again, we find the clearest patterns in the graphs for precision, where Malt has verylow precision near the root but improves with increasing depth, whereas MST showsthe opposite trend, as observed earlier.
Considering the guided models, it is clear thatMaltMST improves in the direction of its guide model, with a five-point increase inprecision for dependents of the root and smaller improvements for longer distances(where its base model is most accurate).
Similarly, MSTMalt improves precision thelargest in the range where its base model is inferior to Malt (roughly distances of 2?6) and is always superior to its base model.
This again indicates that the guided modelsare learning from their guide models as they improve the most in situations where thebase model has inferior accuracy.Table 7 gives the accuracy for arcs relative to dependent part of speech.
As observedearlier, we see that MST does better than Malt for all categories except nouns andpronouns.
But we also see that the guided models in all cases improve over their basemodel and, in most cases, also over their guide model.
The general trend is that MSTimproves more than Malt, except for adjectives and conjunctions, where Malt has agreater disadvantage from the start and therefore benefits more from the guide features.The general trend is that the parser with worse performance for a particular part-of-speech tag improves the most in terms of absolute accuracy (5 out of 7 cases), againsuggesting that the guided models are learning when to trust their guide features.
Theexception here is verbs and adverbs, where MST has superior performance to Malt, butMSTMalt has a larger increase in accuracy than MaltMST.Considering the results for parts of speech, as well as those for dependency lengthand root distance, it is interesting to note that the guided models often improve evenin situations where their base models are more accurate than their guide models.
Thissuggests that the improvement is not a simple function of the raw accuracy of the guidemodel but depends on the fact that labeled dependency decisions interact in inferencealgorithms for both graph-based and transition-based parsing systems.
Thus, if a parsercan improve its accuracy on one class of dependencies (for example, longer ones), thenwe can expect to see improvements on all types of dependencies?as we do.6.3 DiscussionIn summation, it is clear that both guided models benefit from a higher accuracy inpredicting arcs in general, which results in better performance regardless of sentencelength, dependency length, or dependency depth.
However, there is strong evidencethat MSTMalt improves in the direction of Malt, with a slightly larger improvementcompared to its base model for short sentences and short dependencies (but not for deepTable 7Accuracy relative to dependent part of speech (improvement in percentage points).Part of Speech MST MSTMalt Malt MaltMSTVerb 82.6 85.1 (2.5) 81.9 84.3 (2.4)Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)224McDonald and Nivre Analyzing and Integrating Dependency Parsersdependencies).
Conversely, MaltMST improves in the direction of MST, with a largerimprovement for long sentences and for dependents of the root.The question remains why MST generally benefits more from the feature-basedintegration.
The likely explanation is the previously mentioned interaction betweendifferent dependency decisions at inference time.
Because inference in MST is exact(or nearly exact), an improvement in one type of dependency has a good chance ofinfluencing the accuracy of other dependencies, whereas in the transition-based model,where inference is greedy, some of these additional benefits will be lost because of errorpropagation.
This is reflected in the error analysis in the following recurrent pattern:Where Malt does well, MaltMST does only slightly better.
But where MST is good,MSTMalt is often significantly better.
Furthermore, this observation easily explains thelimited increases in accuracy of words with verb and adverb modifiers that is observedin MaltMST relative to MSTMalt (Table 7) as these dependencies occur close to the rootand have increased likelihood of being affected by error propagation.Another part of the explanation may have to do with the learning algorithms usedby the systems.
Although both Malt and MST use discriminative algorithms, Malt usesa batch learning algorithm (SVM) and MST uses an on-line learning algorithm (MIRA).If the original rich feature representation of Malt is sufficient to separate the trainingdata, regularization may force the weights of the guided features to be small (as theyare not needed at training time).
On the other hand, an on-line learning algorithm willrecognize the guided features as strong indicators early in training and give them a highweight as a result.
Frequent features with high weight early in training tend to have themost impact on the final classifier due to both weight regularization and averaging.
Thisis in fact observed when inspecting the weights of MSTMalt.Finally, comparing the results of the guided models to the oracle results discussedin Section 4.4, we see that there should be room for further improvement, as the bestguided parser (MSTMalt) does not quite reach the level of the graph selection oracle,let alne that of the arc selection oracle.
Further exploration of the space of possiblesystems, as outlined in Section 6.3, will undoubtedly be necessary to close this gap.As already noted, there are several recent developments in data-driven dependencyparsing, which can be seen as targeting the specific weaknesses of traditional graph-based and transition-based models, respectively.
For graph-based parsers, McDonaldand Pereira (2006), Hall (2007), Nakagawa (2007), and Smith and Eisner (2008) attemptto overcome the limited feature scope of graph-based models by adding global featuresin conjunction with approximate inference.
Additionally, Riedel and Clarke (2006) andMartins, Smith, and Xing (2009) integrate global features and maintain exact inferencethrough integer linear programming solutions.
For transition-based models, the trendis to alleviate error propagation by abandoning greedy, deterministic inference in fa-vor of beam search with globally normalized models for scoring transition sequences,either generative (Titov and Henderson 2007a, 2007b) or conditional (Duan, Zhao,and Xu 2007; Johansson and Nugues 2007).
In addition, Zhang and Clark (2008) hasproposed a learning method for transition-based parsers based on global optimizationsimilar to that traditionally used for graph-based parsers, albeit only with approxi-mate inference through beam search, and Huang and Sagae (2010) has shown how asubclass of transition-based parsers can be tabularized to permit the use of dynamicprogramming.One question that can be asked, given the correlation provided here between ob-served errors and algorithmic expectations, is whether it is possible to characterize theerrors of a new parsing system simply by analyzing its theoretical properties.
This is adifficult question to answer.
Consider a parsing system that uses greedy inference.
One225Computational Linguistics Volume 37, Number 1can speculate that it will result in error propagation and, as a result, a large numberof parsing errors on long dependencies as well as those close to the root.
However,if the algorithm is run on data that contains only deterministic local decisions andcomplex global decisions, such a system might not suffer from error propagation.
Thisis because the early local decisions are made correctly.
Furthermore, saying somethingabout specific linguistic constructions is also difficult, due to the wide spectrum ofdifficulty when parsing certain phenomena across languages.
Ultimately, this is anempirical question.
What we have shown here is that, on a number of data sets, ouralgorithmic expectations about two widely used dependency parsing paradigms areconfirmed.7.
ConclusionIn this article, we have shown that the two dominant approaches to data-driven depen-dency parsing?global, exhaustive, graph-based models and local, greedy, transition-based models?have distinctive error distributions despite often having very similarparsing accuracy overall.
We have demonstrated that these error distributions can beexplained by theoretical properties of the two models, in particular related to the funda-mental tradeoff between global learning and inference, traditionally favored by graph-based parsers, and a rich feature space, typically found in transition-based parsers.Based on this analysis, we have proposed new directions of research on data-drivendependency parsing, some of which are already beginning to be explored.We have also demonstrated how graph-based and transition-based models can beintegrated by letting one model learn from features generated by the other, using thetechnique known as stacking in the machine learning community.
Our experimentalresults show that both models consistently improve their accuracy when given accessto features generated by the other model, which leads to a significant advancementof the state of the art in data-driven dependency parsing.
Moreover, a comparativeerror analysis reveals that the improvements are predictable from the same theoreticalproperties identified in the initial error analysis, such as the tradeoff between globallearning and inference, on the one hand, and rich feature representations, on the other.On a more general note, we believe that this shows the importance of careful erroranalysis, informed by theoretical predictions, for the further advancement of data-driven methods in natural language processing.AcknowledgmentsWe want to thank our collaborators for greatsupport in developing the parsingtechnology, the organizers of the CoNLL-Xshared task for creating the data, and threeanonymous reviewers for their feedback thatsubstantially improved the article.ReferencesAttardi, Giuseppe.
2006.
Experiments with amultilanguage non-projective dependencyparser.
In Proceedings of the 10th Conferenceon Computational Natural LanguageLearning (CoNLL), pages 166?170,New York, NY.Attardi, Giuseppe and MassimilianoCiaramita.
2007.
Tree revision learning fordependency parsing.
In Proceedings ofHuman Language Technologies: The AnnualConference of the North American Chapter ofthe Association for Computational Linguistics(NAACL HLT), pages 388?395,Rochester, NY.Berger, Adam L., Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to naturallanguage processing.
ComputationalLinguistics, 22:39?71.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings of the226McDonald and Nivre Analyzing and Integrating Dependency Parsers10th Conference on Computational NaturalLanguage Learning (CoNLL), pages 149?164,New York, NY.Carreras, Xavier.
2007.
Experiments with ahigher-order projective dependencyparser.
In Proceedings of the CoNLL SharedTask of EMNLP-CoNLL 2007,pages 957?961, Prague.Chang, Chih-Chung and Chih-Jen Lin.
2001.LIBSVM: A Library for Support VectorMachines.
Software available at www.csie.ntu.edu.tw/?cjlin/libsvm.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofthe First Meeting of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL), pages 132?139,Seattle, WA.Cheng, Yuchang, Masayuki Asahara, andYuji Matsumoto.
2006.
Multi-lingualdependency parsing at NAIST.
InProceedings of the 10th Conference onComputational Natural LanguageLearning (CoNLL), pages 191?195,New York, NY.Chu, Yoeng-Jin and Tseng-Hong Liu.
1965.On the shortest arborescence of adirected graph.
Scientia Sinica,14:1396?1400.Collins, Michael.
1999.
Head-DrivenStatistical Models for Natural LanguageParsing.
Ph.D. thesis, University ofPennsylvania.Collins, Michael.
2000.
Discriminativereranking for natural language parsing.
InProceedings of the International Conference onMachine Learning (ICML), pages 175?182,Stanford, CA.Cortes, Corinna and Vladimir Vapnik.
1995.Support-vector networks.
MachineLearning, 20(3):273?297.Crammer, Koby, Ofer Dekel, Joseph Keshet,Shai Shalev-Shwartz, and Yoram Singer.2006.
Online passive-aggressivealgorithms.
The Journal of Machine LearningResearch, 7:551?585.Ding, Yuan and Martha Palmer.
2004.Synchronous dependency insertiongrammars: A grammar formalism forsyntax based statistical MT.
In Workshopon Recent Advances in DependencyGrammars (COLING), pages 90?97,Geneva.Duan, Xiangyu, Jun Zhao, and Bo Xu.
2007.Probabilistic parsing action models formulti-lingual dependency parsing.
InProceedings of the CoNLL Shared Taskof EMNLP-CoNLL 2007, pages 940?946,Prague.Edmonds, Jack.
1967.
Optimum branchings.Journal of Research of the National Bureau ofStandards, 71B:233?240.Eisner, Jason M. 1996.
Three newprobabilistic models for dependencyparsing: An exploration.
In Proceedings ofthe 16th International Conference onComputational Linguistics (COLING),pages 340?345, Copenhagen.Florian, Radu, Hany Hassan, AbrahamIttycheriah, Hongyan Jing, NandaKambhatla, Xiaoqiang Luo, NicolasNicolov, and Salim Roukos.
2004.
Astatistical model for multilingual entitydetection and tracking.
In Proceedings ofHuman Language Technology and theConference of the North American Chapterof the Association for ComputationalLinguistics (HLT-NAACL), pages 1?8,Boston, MA.Hall, Keith.
2007.
K-best spanning treeparsing.
In Proceedings of the 45thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 392?399, Prague.Hall, Johan, Jens Nilsson, Joakim Nivre,Gu?lsen Eryig?it, Bea?ta Megyesi, MattiasNilsson, and Markus Saers.
2007.
Singlemalt or blended?
A study in multilingualparser optimization.
In Proceedings of theCoNLL Shared Task of EMNLP-CoNLL 2007,pages 933?939, Prague.Henderson, John C. and Eric Brill.
1999.Exploiting diversity in natural languageprocessing: Combining parsers.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 188?194, CollegePark, MD.Hollingshead, Kristy and Brian Roark.
2007.Pipeline iteration.
In Proceedings of the45th Annual Meeting of the Association forComputational Linguistics (ACL),pages 952?959, Prague.Huang, Liang and David Chiang.
2005.Better k-best parsing.
In Proceedings of the9th International Workshop on ParsingTechnologies (IWPT), pages 53?64,Vancouver.Huang, Liang and Kenjie Sagae.
2010.Dynamic programming for linear-timeincremental parsing.
In Proceedings of the48th Annual Meeting of the Association forComputational Linguistics (ACL),pages 1077?1086, Uppsala.Hudson, Richard A.
1984.
Word Grammar.Blackwell, Oxford.Johansson, Richard and Pierre Nugues.
2007.Incremental dependency parsing using227Computational Linguistics Volume 37, Number 1online learning.
In Proceedings of theCoNLL Shared Task of EMNLP-CoNLL 2007,pages 1134?1138, Prague.Koo, Terry, Amir Globerson, XavierCarreras, and Michael Collins.
2010.Efficient third-order dependencyparsers.
In Proceedings of the 48thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 1?11, Uppsala.Koo, Terry, Amir Globerson, Xavier Carreras,and Michael Collins.
2007.
Structuredprediction models via the matrix-treetheorem.
In Proceedings of the 2007 JointConference on Empirical Methods in NaturalLanguage Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages 141?150, Prague.Kudo, Taku and Yuji Matsumoto.
2002.Japanese dependency analysis usingcascaded chunking.
In Proceedings of theSixth Workshop on Computational LanguageLearning (CoNLL), pages 63?69, Edmonton.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.
InProceedings of the International Conference onMachine Learning (ICML), pages 282?289,Williamstown, MA.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.Martins, Andre F. T., Dipanjan Das,Noah A. Smith, and Eric P. Xing.
2008.Stacking dependency parsers.
InProceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 157?166,Honolulu, HI.Martins, Andre F. T., Noah A. Smith, andEric P. Xing.
2009.
Concise integer linearprogramming formulations fordependency parsing.
In Proceedings of theJoint Conference of the 47th Annual Meetingof the ACL and the 4th International JointConference on Natural Language Processing ofthe AFNLP (ACL-IJCNLP), pages 342?350,Singapore.Maruyama, Hiroshi.
1990.
Structuraldisambiguation with constraintpropagation.
In Proceedings of the 28thMeeting of the Association for ComputationalLinguistics (ACL), pages 31?38,Pittsburgh, PA.McDonald, Ryan.
2006.
DiscriminativeLearning and Spanning Tree Algorithms forDependency Parsing.
Ph.D. thesis,University of Pennsylvania.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Onlinelarge-margin training of dependencyparsers.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 91?98,Ann Arbor, MI.McDonald, Ryan, Kevin Lerman, andFernando Pereira.
2006.
Multilingualdependency analysis with a two-stagediscriminative parser.
In Proceedings of the10th Conference on Computational NaturalLanguage Learning (CoNLL), pages 216?220,New York, NY.McDonald, Ryan and Joakim Nivre.
2007.Characterizing the errors of data-drivendependency parsing models.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning (EMNLP-CoNLL), pages 122?131,Prague.McDonald, Ryan and Fernando Pereira.
2006.Online learning of approximatedependency parsing algorithms.
InProceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL),pages 81?88, Trento.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic?.
2005.Non-projective dependency parsingusing spanning tree algorithms.
InProceedings of the Human LanguageTechnology Conference and the Conference onEmpirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 523?530,Vancouver.McDonald, Ryan and Giorgio Satta.
2007.On the complexity of non-projectivedata-driven dependency parsing.
InProceedings of the 10th InternationalConference on Parsing Technologies (IWPT),pages 122?131, Prague.Mel?c?uk, Igor.
1988.
Dependency Syntax:Theory and Practice.
State University ofNew York Press.Nakagawa, Tetsuji.
2007.
Multilingualdependency parsing using global features.In Proceedings of the CoNLL Shared Task ofEMNLP-CoNLL 2007, pages 952?956,Prague.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.
InProceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT),pages 149?160, Nancy.228McDonald and Nivre Analyzing and Integrating Dependency ParsersNivre, Joakim.
2007.
Incrementalnon-projective dependency parsing.In Proceedings of Human LanguageTechnologies: The Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics(NAACL HLT), pages 396?403,Rochester, NY.Nivre, Joakim.
2009.
Non-projectivedependency parsing in expected lineartime.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL andthe 4th International Joint Conference onNatural Language Processing of theAFNLP (ACL-IJCNLP), pages 351?359,Singapore.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.In Proceedings of the CoNLL Shared Task ofEMNLP-CoNLL 2007, pages 915?932,Prague.Nivre, Joakim, Johan Hall, and Jens Nilsson.2004.
Memory-based dependency parsing.In Proceedings of the 8th Conference onComputational Natural Language Learning,pages 49?56, Boston, MA.Nivre, Joakim, Johan Hall, Jens Nilsson,Gu?lsen Eryig?it, and Svetoslav Marinov.2006.
Labeled pseudo-projectivedependency parsing with support vectormachines.
In Proceedings of the 10thConference on Computational NaturalLanguage Learning (CoNLL), pages 221?225,New York, NY.Nivre, Joakim and Ryan McDonald.2008.
Integrating graph-based andtransition-based dependency parsers.In Proceedings of the 46th Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 950?958,Columbus, OH.Nivre, Joakim and Jens Nilsson.
2005.Pseudo-projective dependency parsing.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 99?106, Ann Arbor, MI.Riedel, Sebastian and James Clarke.
2006.Incremental integer linear programmingfor non-projective dependency parsing.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 129?137, Sydney.Riedel, Sebastian, Ruket C?ak?c?, and IvanMeza-Ruiz.
2006.
Multi-lingualdependency parsing with incrementalinteger linear programming.
In Proceedingsof the 10th Conference on ComputationalNatural Language Learning (CoNLL),pages 226?230, New York, NY.Sagae, Kenji and Alon Lavie.
2006.
Parsercombination by reparsing.
In Proceedingsof NAACL: Short Papers, pages 129?132,New York, NY.Sarkar, Anoop.
2001.
Applying co-trainingmethods to statistical parsing.
InProceedings of the Second Meeting of theNorth American Chapter of the Association forComputational Linguistics (NAACL),pages 175?182, Pittsburgh, PA.Sgall, Petr, Eva Hajic?ova?, and JarmilaPanevova?.
1986.
The Meaning of theSentence in Its Pragmatic Aspects.
Reidel,Dordrecht.Smith, David A. and Jason Eisner.
2008.Dependency parsing by beliefpropagation.
Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP), pages 145?156,Honolulu, HI.Smith, David A. and Noah A. Smith.
2007.Probabilistic models of nonprojectivedependency trees.
In Proceedings of the 2007Joint Conference on Empirical Methods inNatural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 132?140, Prague.Snow, Rion, Dan Jurafsky, and Andrew Y.Ng.
2005.
Learning syntactic patterns forautomatic hypernym discovery.
InAdvances in Neural Information ProcessingSystems (NIPS), pages 1297?1304,Vancouver.Steedman, Mark, Rebecca Hwa, MilesOsborne, and Anoop Sarkar.
2003.Corrected co-training for statisticalparsers.
In Proceedings of the InternationalConference on Machine Learning (ICML),pages 95?102, Washington, DC.Tarjan, Robert E. 1977.
Finding optimumbranchings.
Networks, 7:25?35.Taskar, Ben, Simon Lacoste-Julien, and DanKlein.
2005.
A discriminative matchingapproach to word alignment.
InProceedings of the Human LanguageTechnology Conference and the Conference onEmpirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 73?80,Vancouver.Titov, Ivan and James Henderson.
2007a.Fast and robust multilingual dependencyparsing with a generative latent variablemodel.
In Proceedings of the CoNLLShared Task of EMNLP-CoNLL 2007,pages 947?951, Prague.Titov, Ivan and James Henderson.
2007b.
Alatent variable model for generative229Computational Linguistics Volume 37, Number 1dependency parsing.
In Proceedings of the10th International Conference on ParsingTechnologies (IWPT), pages 144?155,Prague.Yamada, Hiroyasu and Yuji Matsumoto.2003.
Statistical dependency analysis withsupport vector machines.
In Proceedings ofthe 8th International Workshop on ParsingTechnologies (IWPT), pages 195?206,Nancy.Zeman, Daniel and Zdene?k Z?abokrtsky`.2005.
Improving parsing accuracy bycombining diverse dependency parsers.Proceedings of the International Workshopon Parsing Technologies, pages 171?178,Vancouver.Zhang, Yue and Stephen Clark.
2008.A tale of two parsers: Investigatingand combining graph-based andtransition-based dependency parsing.Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 562?571,Honolulu, HI.230
