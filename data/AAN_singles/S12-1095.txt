First Joint Conference on Lexical and Computational Semantics (*SEM), pages 643?647,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDERI&UPM: Pushing Corpus Based Relatedness to Similarity: SharedTask System DescriptionNitish Aggarwal?
Kartik Asooja?
Paul Buitelaar?
?Unit for Natural Language ProcessingDigital Enterprise Research InstituteNational University of Ireland, Galway, Irelandfirstname.lastname@deri.org?Ontology Engineering GroupUniversidad Politecnica de MadridMadrid, Spainasooja@gmail.comAbstractIn this paper, we describe our system submit-ted for the semantic textual similarity (STS)task at SemEval 2012.
We implemented twoapproaches to calculate the degree of simi-larity between two sentences.
First approachcombines corpus-based semantic relatednessmeasure over the whole sentence with theknowledge-based semantic similarity scoresobtained for the words falling under the samesyntactic roles in both the sentences.
We fedall these scores as features to machine learn-ing models to obtain a single score giving thedegree of similarity of the sentences.
Lin-ear Regression and Bagging models were usedfor this purpose.
We used Explicit SemanticAnalysis (ESA) as the corpus-based seman-tic relatedness measure.
For the knowledge-based semantic similarity between words, amodified WordNet based Lin measure wasused.
Second approach uses a bipartite basedmethod over the WordNet based Lin measure,without any modification.
This paper showsa significant improvement in calculating thesemantic similarity between sentences by thefusion of the knowledge-based similarity mea-sure and the corpus-based relatedness measureagainst corpus based measure taken alone.1 IntroductionSimilarity between sentences is a central conceptof text analysis, however previous studies aboutsemantic similarities have mainly focused eitheron single word similarity or complete documentsimilarity.
Sentence similarity can be defined by thedegree of semantic equivalence of two given sen-tences, where sentences are typically 10-20 wordslong.
The role of sentence semantic similarity mea-sures in text-related research is increasing due topotential number of applications such as documentsummarization, question answering, informationextraction & retrieval and machine translation.One plausible limitation of existing methods forsentence similarity is their adaptation from long text(e.g.
documents) similarity methods, where wordco-occurrence plays a significant role.
However,sentences are too short, thats why taking syntac-tic role of each word with its narrow semanticmeaning into account, can be highly relevant toreflect the semantic equivalence of two sentences.These narrow semantics can be reflected from anyexisting large lexicons [(Wu and Palmer, 1994)and (Lin, 1998)]; nevertheless, these lexicons cannot provide the semantics of words which are outof lexicon (e.g.
guy) or multiword expressions.These semantics can be represented by a largedistributed semantic space such as Wikipedia andsimilarity can be reflected by relatedness of theseextracted semantics.
However, relatedness coversbroader space than similarity, which forced us totune the Wikipedia based relatedness with lexicalstructure (e.g.
WordNet) based similarities drivenby linguistic syntactic structure, in reflecting moresophisticated similarity of two given sentences.In this work, we present a sentence similarity usingESA and syntactic similarities.
The rest of thispaper is organized as follows.
Section 2 explores therelated work.
Section 3 describes our approaches643in detail.
Section 4 explains our three differentsubmitted runs for STS task.
Section 5 shows theresults and finally we conclude in section 6.2 Related WorkIn recent years, there have been a variety of effortsin improving semantic similarity measures, howevermost of these approaches address this problem fromthe viewpoint of large document similarity based onword co-occurrence using string pattern or corpusstatistics.
Corpus based approaches such as LatentSemantic Analysis (LSA) [(Landauer et.
al, 1998)and (Foltz et.
al, 1998)] and ESA (Gabrilovich andMarkovitch, 2007) use corpus statistics informationabout all words and reflect their semantics indistributional high semantic space.
However, theseapproaches perform quite well for long texts as theyuse word co-occurrence and relying on the principlethat words which are used in the same contextstend to have related meanings.
In case of short textsimilarities, syntactic role of each word with itsmeaning plays an important role.There are several linguistic measures [( Achananu-parp et.
al, 2008) and (Islam and Inkpen, 2008)],which can account for pseudo-syntactic informationby analyzing their word order using n-gram.
To dothis, Islam and Inkpen defined a syntactic measure,which considers the word order between twostrings by computing the maximal ordered wordoverlapping.
(Oliva et.
al, 2011) present a similaritymeasure for sentences and short text that takessyntactic information, such as morphology andparsing tree, into account and calculate similaritiesbetween words with same syntactic role, by usingWordNet.Our work takes inspiration from existing approachesthat exploit a combination of Wikipedia based re-latedness with lexical structure based similaritiesdriven by linguistic syntactic structure.3 MethodologyWe implemented two approaches for the STStask [(Agirre et.
al, 2012)].
First approach is afusion of corpus-based semantic relatedness andknowledge-based semantic similarity measures.The core of this combination is the corpus-basedmeasure because the combination includes thecorpus-based semantic relatedness score over thewhole sentences and the knowledge-based semanticsimilarity scores for the words falling under thesame syntactic roles in both the sentences.
Machinelearning models are trained by taking all thesescores as different features.
For the submission,we used Linear regression and Bagging models.Also, the equation obtained after training the linearregression model shows more weightage to the scoreobtained by the corpus-based relatedness measureas this is the only score (feature), which reflects thesemantic relatedness/similarity score over the fullsentences, out of all the considered features for themodel.
We used ESA as the corpus based semanticrelatedness measure and modified WordNet-basedLin measure as the knowledge-based similarity.The WordNet-based Lin relatedness measure wasmodified to reflect better the similarity betweenthe words.
For the knowledge-based similarity,currently we considered only the words lying in thethree major syntactic role categories i.e.
subjects,actions and the objects.
We see the first approachas the corpus-based measure ESA tuned with theknowledge-based measure.
Thus, it is referred asTunedESA later in the paper.Our second approach is based on the bipartitemethod over the WordNet based semantic relat-edness measures.
WordNet-based Lin measure(without any modification) was used for calcu-lating the relatedness scores for all the possiblecorresponding pair of words appearing in both thesentences.
Then, the similarity/relatedness scorefor the sentences is calculated by perceiving theproblem as the computation of a maximum totalmatching weight of a bipartite graph having thewords as nodes and the relatedness scores as theweight of the edges between the nodes.
To solvethis, we used Hungarian method.
Later, we referthis method as WordNet-Bipartite.3.1 TunedESAIn this approach, the ESA based relatedness scorefor the full sentences is combined with the modifiedWordNet-based Lin similarity scores calculated forthe words falling under the corresponding syntacticrole category in both the sentences.644ALL Rank-ALL ALLnrm RankNrm Mean RankMeanBaseline 0.3110 87 0.6732 85 0.4356 70Run1 0.5777 52 0.8158 20 0.5466 52Run2 0.5833 51 0.8183 17 0.5683 42Run3 0.4911 67 0.7696 57 0.5377 53Table 1: Overall Rank and Pearson Correlation of all runsMSRpar MSRvid SMTeuro OnWN SMTnewsBaseline 0.4334 0.2996 0.4542 0.5864 0.3908ESA?
0.2778 0.8178 0.3914 0.6541 0.4366Run1 0.3675 0.8427 0.3534 0.6030 0.4430Run2 0.3720 0.8330 0.4238 0.6513 0.4489Run3 0.5320 0.6874 0.4514 0.5827 0.2818Table 2: Pearson Correlation of all runs with all five STS test datasetsTunedESA could be summarized as these fourbasic steps:?
Calculate the ESA relatedness score betweenthe sentences.?
Find the words corresponding to the linguisticsyntactical categories like subject, action andobject of both the sentences.?
Calculate the semantic similarity between thewords falling in the corresponding subjects, ac-tions and objects in both the sentences usingmodified WordNet-based measure Lin.?
Combine these four scores for ESA, Subject,Action and Object to get the final similarityscore on the basis of an already learned ma-chine learning model with the training data.ESA is a promising technique to find the relatednessbetween documents.
The texts which need to becompared are represented as high dimensional vec-tors containing the TF-IDF weight between the termand the Wikipedia article.
The semantic relatednessmeasure is calculated by taking the cosine measurebetween these vectors.
In this implementation ofESA 1, the score was calculated by considering the1ESA?
considering full sentence at a time to make the vectori.e.
different from standard ESAfull sentence at a time for making the Wikipediaarticle vector while in the standard ESA, vectorsare made for each word of the text followed by theaddition of all these vectors to represent the finalvector for the text/sentence.
It was done just toreduce the time complexity.To calculate the lexical similarity between thewords, we implemented WordNet-based semanticrelatedness measure Lin.
This score was modified toreflect a better similarity between the words.
In thecurrent system, basic linguistic syntactic categoriesi.e.
subjects, actions and objects were used.
Forinstance, below is a sentences pair from the trainingMSRvid dataset with the gold standard score andthe syntactic roles.Sentence 1: A man is playing a guitar.Subject: Man, Action: play, Object: guitarSentence 2: A man is playing a flute.Subject: Man, Action: play, Object: fluteGold Standard Score (0-5): 2.2As the modification, the scores given by Linmeasure were used only for the cases where sub-sumption relation or hypernymy/hyponymy exists645between the words.
This modification was doneonly for the words falling under the category ofsubjects and objects.3.2 WordNet BipartiteWordNet-based semantic relatedness measure wasused for the second approach.Following steps are performed :?
Each sentence is tokenized to obtain the words.?
Semantic relatedness between every possiblepair of words in both the sentences is calculatedusing WordNet-based measure e.g.
Lin.?
Using the scores obtained in the second step,the semantic similarity/relatedness between thesentences is calculated by transforming theproblem as that of computing the maximum to-tal matching weight of a bipartite graph, whichcan be done by using Hungarian method.4 System DescriptionWe submitted three runs in the semantic textualsimilarity task.
The first two runs are based on thefirst approach i.e.
TunedESA and they differ only inthe machine learning algorithm used for obtainingthe final similarity score based on all the consideredscores/features.ESA was implemented on the current Wikipediadump.
WordNet based relatedness measure Linwas modified to give a better semantic similaritydegree.
Stanford Core-NLP library was used forobtaining the words with their syntactic roles.All the required scores/feature i.e.
ESA basedrelatedness for the complete sentences and mod-ified WordNet-based Lin similarity scores werecalculated for the corresponding words lying inthe same syntactic categories.
Bagging and LinearRegression models were built using the training datafor the first and second runs respectively.
Based onthe category of the test dataset, model was trainedon the corresponding training dataset.For the surprise test datasets, we trained ourmodel with the training dataset of the MSRvid databased on the fact that we obtained good results withthis category.
Then the built models were used forcalculating the similarity scores for the test data.For the third run, WordNet Bipartite methodwas used to calculate the similarity scores.
It didn?trequire any training.5 Results and DiscussionAll above described runs are evaluated on STStest dataset.
Table 1 shows the overall results2 ofour three runs against the baseline system whichfollows the bag of words approach.
Table 2 showsthe Pearson correlation on different test datasets forall the three runs.
It provides a comparison betweencorpus based relatedness measure ESA and oursystem TunedESA (Run 1 & Run 2).The results show significant improvement againstESA.
Although, it can be seen that the baselineresults are even better than of the ESA in the casesof MSRpar and SMTeuro.
It may be because thisimplementation of ESA is not the standard one.6 ConclusionWe presented a method to calculate the degree ofsentence similarity based on tuning the corpus basedrelatedness measure with the knowledge-based sim-ilarity measure over the syntactic roles.
The resultsshow a definite improvement by the fusion.
Asfuture work, we plan to improve the syntactic rolehandling and considering more syntactical cate-gories.
Also, experimentation3 with standard ESAand other semantic similarity/relatedness measuresneeds to be performed.AcknowledgmentsThis work is supported in part by the EuropeanUnion under Grant No.
248458 for the Monnetproject as well as by the Science Foundation Irelandunder Grant No.
SFI/08/CE/I1380 (Lion-2).2results can also be found at http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=results-update with the name nitish aggarwal3We plan to provide the further results and information athttp://www.itssimilar.com/646ReferencesAchananuparp Palakorn and Xiaohua Hu and XiajiongShen 2008 The Evaluation of Sentence SimilarityMeasures, In: DaWaK.
pp.
305-316Agirre Eneko , Cer Daniel, Diab Mona and Gonzalez-Agirre Aitor 2012 SemEval-2012 Task 6: A Pilot onSemantic Textual Similarity.
In: Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012).Foltz P. W., Kintsch W. and Landauer T. K. 1998.
In:journal of the Discourse Processes.
pp.
285-307, Themeasurement of textual Coherence with Latent Se-mantic Analysis,Gabrilovich Evgeniy and Markovitch Shaul 2007 Com-puting Semantic Relatedness using Wikipedia-basedExplicit Semantic Analysis, In: Proceedings of TheTwentieth International Joint Conference for ArtificialIntelligence.
pp.
1606?1611,Islam, Aminul and Inkpen, Diana 2008 Semantictext similarity using corpus-based word similarity andstring similarity, In: journal of ACM Trans.
Knowl.Discov.
Data.
pp.
10:1?10:25Landauer Thomas K. ,Foltz Peter W. and Laham Darrell1998.
An Introduction to Latent Semantic Analysis,In: Journal of the Discourse Processes.
pp.
259-284,Lin Dekang 1998 Proceeding of the 15th InternationalConference on Machine Learning.
pp.
296?304 Aninformation-theoretic definition of similarityOliva, Jesu?s and Serrano, Jose?
Ignacio and del Castillo,Mar?
?a Dolores and Iglesias, A?ngel April, 2011SyMSS: A syntax-based measure for short-text seman-tic similarity In: journal of Data Knowledge Engineer-ing.
pp.
390?405Wu, Zhibiao and Palmer, Martha 1994 Verbs seman-tics and lexical selection, In: Proceedings of the 32ndannual meeting on Association for Computational Lin-guistics,647
