Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1224?1234,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsRBPB: Regularization-Based Pattern Balancing Method for EventExtractionLei Sha1,2, Jing Liu2, Chin-Yew Lin2, Sujian Li1, Baobao Chang1, Zhifang Sui11Key Laboratory of Computational Linguistics, Ministry of EducationSchool of Electronics Engineering and Computer Science, Peking University2Knowledge Computing Group, Microsoft Research1{shalei, lisujian, chbb, szf}@pku.edu.cn2{liudani,cyl}@microsoft.comAbstractEvent extraction is a particularly chal-lenging information extraction task,which intends to identify and classifyevent triggers and arguments from rawtext.
In recent works, when determiningevent types (trigger classification), mostof the works are either pattern-onlyor feature-only.
However, althoughpatterns cannot cover all representationsof an event, it is still a very importantfeature.
In addition, when identifyingand classifying arguments, previousworks consider each candidate argumentseparately while ignoring the relationshipbetween arguments.
This paper proposesa Regularization-Based Pattern BalancingMethod (RBPB).
Inspired by the progressin representation learning, we use triggerembedding, sentence-level embedding andpattern features together as our featuresfor trigger classification so that the effectof patterns and other useful features canbe balanced.
In addition, RBPB uses aregularization method to take advantageof the relationship between arguments.Experiments show that we achieve resultsbetter than current state-of-art equivalents.1 IntroductionEvent extraction has become a popular researchtopic in the area of information extraction.
ACE2005 defines event extraction task1as threesub-tasks: identifying the trigger of an event,identifying the arguments of the event, anddistinguishing their corresponding roles.
As anexample in Figure 1, there is an ?Attack?
event1http://www.itl.nist.gov/iad/mig/tests/ace/2005/triggered by ?tear through?
with three arguments.Each argument has one role.In the trigger classification stage, someprevious approaches (Grishman et al, 2005; Jiand Grishman, 2008; Liao and Grishman, 2010;Huang and Riloff, 2012) use patterns to decide thetypes of event triggers.
However, pattern-basedapproaches suffer from low recall since realworld events usually have a large variety ofrepresentations.
Some other approaches (Honget al, 2011; Li et al, 2013; Lu and Roth, 2012)identify and classify event triggers using a largeset of features without using patterns.
Althoughthese features can be very helpful, patterns arestill indispensable in many cases because they canidentify a trigger with the correct event type withmore than 96% accuracy according to our dataanalysis on ACE 2005 data sets.In argument identification and classification,most approaches identify each candidate argumentseparately without considering the relationbetween arguments.
We define two kinds ofargument relations here: (1) Positive correlation:if one candidate argument belongs to one event,then the other is more likely to belong to thesame event.
For example, in Figure 1, the entity?a waiting shed?
shares a common dependencyhead ?tore?
with ?a powerful bomb?, so whenthe latter entity is identified as an argument, theformer is more likely to be identified.
(2) Negativecorrelation: if one candidate argument belongs toone event, then the other is less likely to belongto the same event.
For example, in Figure 1,?bus?
is irrelevant to other arguments, so if otherentities are identified as arguments ?bus?
is lesslikely to be identified.
Note that although all theabove relation examples have something to dowith dependency analysis, the positive/negativerelationship depends not only on dependencyparsing, but many other aspects as well.1224A powerful bomb tore through a waiting shed at the Davao airport while another explosion hit a busTriggerEvent type:AttackArg1Role: InstrumentArg2Role: TargetArg3Role: PlaceFigure 1: Event example: This is an event trigger by ?tear through?
with three argumentsIn this paper, we propose using both patternsand elaborately designed features simultaneouslyto identify and classify event triggers.
Inaddition, we propose using a regularizationmethod to model the relationship betweencandidate arguments to improve the performanceof argument identification.
Our method is calledRegularization-Based Pattern Balancing Methodmethod.The contributions of this paper are as follows:?
Inspired by the progress of representationlearning, we use trigger embedding,sentence-level embedding, and patternfeatures together as the our features forbalancing.?
We proposed a regularization-based methodin order to make use of the relationship be-tween candidate arguments.
Our experimentson the ACE 2005 data set show that the reg-ularization method does improve the perfor-mance of argument identification.2 Related WorkThere is a large body of previous work devoted toevent extraction.
Many traditional works focus onusing pattern based methods for identifying eventtype (Kim and Moldovan, 1993; Riloff and others,1993; Soderland et al, 1995; Huffman, 1996;Freitag, 1998b; Ciravegna and others, 2001; Califfand Mooney, 2003; Riloff, 1996; Riloff et al,1999; Yangarber et al, 2000; Sudo et al, 2003;Stevenson and Greenwood, 2005; Grishman et al,2005; Ji and Grishman, 2008; Liao and Grishman,2010; Huang and Riloff, 2012).
(Shinyama andSekine, 2006; Sekine, 2006) are unsupervisedmethods of extracting patterns from open domaintexts.
Pattern is not always enough, althoughsome methods (Huang and Riloff, 2012; Liu andStrzalkowski, 2012) use bootstrapping to get morepatterns.There are also feature-based classificationmethods (Freitag, 1998a; Chieu and Ng, 2002;Finn and Kushmerick, 2004; Li et al, 2005; Yuet al, 2005).
Apart from the above methods,weakly supervised training (pattern-based andrule-based) of event extraction systems havealso been explored (Riloff, 1996; Riloff et al,1999; Yangarber et al, 2000; Sudo et al, 2003;Stevenson and Greenwood, 2005; Patwardhanand Riloff, 2007; Chambers and Jurafsky, 2011).In some of these systems, human work is neededto delete some nonsense patterns or rules.
Othermethods (Gu and Cercone, 2006; Patwardhanand Riloff, 2009) consider broader context whendeciding on role fillers.
Other systems takethe whole discourse feature into consideration,such as (Maslennikov and Chua, 2007; Liaoand Grishman, 2010; Hong et al, 2011; Huangand Riloff, 2011).
Ji and Grishman (2008) evenconsider topic-related documents, proposing across-document method.
(Liao and Grishman,2010; Hong et al, 2011) use a series of globalfeatures (for example, the occurrence of oneevent type lead to the occurrence of another) toimprove role assignment and event classificationperformance.
Joint models (Li et al, 2013; Luand Roth, 2012) are also considered an effectivesolution.
(Li et al, 2013) make full use of thelexical and contextual features to get better results.The semi-CRF based method (Lu and Roth, 2012)trains separate models for each event type, whichrequires a lot of training data.The dynamic multi-pooling convolutional neu-ral network (DMCNN) (Chen et al, 2015) is cur-rently the only widely used deep neural networkbased approach.
DMCNN is mainly used to modelcontextual features.
However, DMCNN still doesnot consider argument-argument interactions.In summary, most of the above works areeither pattern-only or features-only.
Moreover,all of these methods consider arguments sepa-1225rately while ignoring the relationship betweenarguments, which is also important for argumentidentification.
Even the joint method (Li etal., 2013) does not model argument relationsdirectly.
We use trigger embedding, sentence-level embedding, and pattern features together asour features for trigger classification and designa regularization-based method to solve the twoproblems.3 ACE Event Extraction TaskAutomatic Content Extraction (ACE) is an eventextraction task.
It annotates 8 types and 33 sub-types of events.
ACE defines the following termi-nologies:?
Entity: an object or a set of objects in one ofthe semantic categories of interest?
Entity mention: a reference to an entity, usu-ally a noun phrase (NP)?
Event trigger: the main word which mostclearly expresses an event occurrence?
Event arguments: the entity mentions that areinvolved in an event?
Argument roles: the relation of arguments tothe event where they participate, with 35 totalpossible roles?
Event mention: a phrase or sentence withinwhich an event is described, including triggerand argumentsGiven an English document, an event extractionsystem should identify event triggers with theirsubtypes and arguments from each sentence.An example is shown in Figure 1.
There is an?Attack?
event triggered by ?tear through?
withthree arguments.
Each argument has a role typesuch as ?Instrument?, ?Target?, etc.For evaluation, we follow previous works (Jiand Grishman, 2008; Liao and Grishman, 2010;Li et al, 2013) to use the following criteria todetermine the correctness of the predicted eventmentions.?
A trigger is considered to be correct if andonly if its event type and offsets (position inthe sentence) can match the reference trigger;?
An argument is correctly identified if and on-ly if its event type and offsets can match anyreference arguments;?
An argument is correctly identified and clas-sified if and only if its event type, offsets, androle match any of the reference arguments.4 Baseline: JET Extractor for EventsMany previous works take JET as their baselinesystem, including (Ji and Grishman, 2008), (Liaoand Grishman, 2010), (Li et al, 2013).
JETextracts events independently for each sentence.This system uses pattern matching to predicttrigger and event types, then uses statisticalmodeling to identify and classify arguments.For each event mention in the training corpusof ACE, the patterns are constructed based onthe sequences of constituent heads separatingthe trigger and arguments.
After that, threeMaximum Entropy classifiers are trained usinglexical features.?
Argument Classifier: to distinguish argu-ments from non-arguments?
Role Classifier: to label arguments with anargument role?
Reportable-Event Classifier: to determinewhether there is a reportable event mentioned(worth being taken as an event mention)according to the trigger, event type, and a setof argumentsFigure 2(a) shows the whole test procedure.
Inthe test procedure, each sentence is scanned fornouns, verbs and adjectives as trigger candidates.When a trigger candidate is found, the systemtries to match the context of the trigger against theset of patterns associated with that trigger.
If thispattern matching process is successful, the bestpattern will assign some of the entity mentionsin the sentence as arguments of a potential eventmention.
Then JET uses the argument classifier tojudge if the remaining entity mentions should alsobe identified.
If yes, JET uses the role classifierto assign it a role.
Finally, the reportable-eventclassifier is applied to decide whether this eventmention should be reported.5 Regularization-Based PatternBalancing MethodDifferent with JET, as illustrated in Figure 2(b),our work introduces two major improvements: (1)balance the effect of patterns and other features (2)1226...
In Baghdad, a cameraman died when ...n, v, adj: trigger candidatetrigger = died find best pattern?get arguments & rolesyesMaxEnt for argument?
MaxEnt for role?MaxEnt for reportable event?
(a) The flow chart of JET...
In Baghdad, a cameraman died when ...n, v, adj: trigger candidatetrigger = diedSVM for Event type?MaxEnt for argument?
MaxEnt for role?RegularizationMaxEnt for reportable event?
(b) The flow chart of our approachFigure 2: The left is the flow chart for JET.
The right is the flow chart for our approach.
The thick lineblock is our contributionuse a regularization-based method to make full useof the relation between candidate arguments.The thick-edge blocks in Figure 2(b) representour improvements.
Since JET only uses pattern-s when predicting the event type, we use a SVMclassifier to decide each candidate trigger?s even-t type (classify the trigger).
This classifier usestrigger embedding, sentence-level embedding andpattern features together for balancing.
After theoutputs of argument and role classifier are calcu-lated, we make use of the argument relationship toregularize for a better result.5.1 Balancing the Pattern effectsDeciding the event type is the same as classifyingan event trigger.
JET only uses patterns in thisstep: for a candidate trigger, we find that thebest matched pattern and the correspondingevent type are assigned to this trigger.
Wepropose using feature-based methods while notignoring the effect of patterns.
Inspired byprogress in representation learning, we use triggerembedding, sentence-level embedding and patternembedding together as our features.A pattern example is as follows:(weapon) tore [through] (building) at(place)?
Attack{Roles...}where each pair of round brackets represents anentity and the word inside is one of the 18 entitytypes defined by UIUC NER Tool2.
The word inthe square brackets can choose to exist or not.
Af-ter the right arrow there is an event schema, whichcan tell us what kind of event this is and whichroles each entity should take.Each pattern has a corresponding event type.
Acandidate trigger maymatch more than one patternso that it has an event type distribution.
Assumethat there are NTevent types in total, we denotethe pattern feature vector (namely, the event type?sprobability distribution calculated by the trigger?spattern set) as PE?
RNT, which is calculated byEq 1.PE(i) =#(matched patterns of event type i)#(all matched patterns)(1)Trigger embeddings are obtained using WORD2VEC3with the default ?text8?
training text data withlength 200.Since all of the NPs are potential roles in theevent, they must contain the main information ofthe event.
We extract all the NPs in the sentenceand take the average word embedding of theseNPs?
head word as the sentence-level embedding.For example, in Figure 1, these NPs?
head wordsare bomb, shed, and airport.Pattern feature vectors, as distributions of eventtypes over patterns, are also composed using2http://cogcomp.cs.illinois.edu/page/software view/NETagger3http://code.google.com/p/word2vec/1227continuous real values, which allows them to beviewed as a kind of pattern embedding and treatedsimilarly to trigger and sentence embedding.5.2 Capturing the Relationship BetweenArgumentsWe find that there are two typical relationsbetween candidate arguments: (1) positivecorrelation: if one candidate argument belongs toone event, then the other is more likely to belongto the same event; (2) negative correlation: if onecandidate argument belongs to one event, then theother is less likely to belong to the same event.We calculate a score for all the candidatearguments in a sentence to judge the quality ofthe argument identification and classification.
Forcapturing the two kinds of relations, we intendto make that (1) the more positive relations thechosen arguments have, the higher the score is; (2)the more negative relations the chosen argumentshave, the lower the score is.For a trigger, if there are n candidate arguments,we set a n ?
n matrix C to represent the relation-ship between arguments.
If Ci,j= 1, then argu-ment i and argument j should belong to the sameevent.
If Ci,j= ?1, then argument i and argu-ment j cannot belong to the same event.
We willillustrate how to get matrix C in the next section.We use a n-dim vector X to represent the iden-tification result of arguments.
Each entry ofX is 0or 1.
0 represents ?noArg?, 1 represents ?arg?.
Xcan be assigned by maximizing E(X) as definedby Eq 2.X = argmaxXE(X)E(X) = ?1XTCX + ?2Pargsum+ (1?
?1?
?2)Prolesum(2)Here,XTCX means adding up all the relationshipvalues if the two arguments are identified.
Hence,the more the identified arguments are related, thelarger the value XTCX is.
Pargsumis the sum ofall chosen arguments?
probabilities.
The proba-bility here is the output of the arguments?
max-imum entropy classifier.
Prolesumis the sum of allthe classified roles?
probabilities.
The probabilityhere is the output of the roles?
maximum entropyclassifier.Eq 2 shows that while we should identify andclassify the candidate arguments with a largerprobability, the argument relationship evaluationshould also be as large as possible.
The argumentsshould also follow the following constraints.These constraints together with Eq 2 can makethe argument identification and classification helpeach other for a better result.?
Each entity can only take one role?
Each role can belong to one or more entities?
The role assignment must follow the eventschema of the corresponding type, whichmeans that only the roles in the event schemacan occur in the event mentionWe use the Beam Search method to search for theoptimal assignment X as is shown in Algorithm 1.The hyperparameters ?1and ?2can be chosen ac-cording to development set.Input: Argument relationship matrix: Cthe argument probabilities required byPargsumthe role probabilities required by ProlesumData: K: Beam sizen: Number of candidate argumentsOutput: The best assignment XSet beam B ?
[?]
;for i?
1 ?
?
?n dobuf?
{z??
l|z??
B, l ?
{0, 1}};B ?
[?]
;while j ?
1 ?
?
?K doxbest= argmaxx?bufE(x);B ?
B ?
{xbest};buf?buf?
{xbest};endendSort B descendingly according to E(X);return B[0];Algorithm 1: Beam Search decoding algorith-m for event extraction.
?means to concatenatean element to the end of a vector.5.2.1 Training the Argument RelationshipStructureThe argument relationship matrix C is very im-portant in the regularization process.
We train amaximum entropy classifier to predict the connec-tion between two entities.
We intend to classify theentity pairs into three classes: positive correlation,negative correlation, and unclear correlation.
Theentity pairs in the ground truth events (in training1228data) are used for our training data.
We choose thefollowing features:?
TRIGGER: the trigger of the event.
Thewhole model is a pipelined model, so whenclassifying the argument relationship, thetrigger has been identified and classified.
Sothe ?trigger?
is a feature of the argumentrelation.?
ENTITY DISTANCE: the distance betweenthe two candidate arguments in the sentence,namely the number of intervening words?
Whether the two candidate arguments occuron the same side of the trigger?
PARENT DEPENDENCY DISTANCE: the dis-tance between the two candidate arguments?parents in the dependency parse tree, namely,the path length.?
PARENT POS: if the two candidate ar-guments share the same parent, take thecommon parent?s POS tag as a feature?
Whether the two candidate arguments occuron the same side of the common parent if thetwo candidate arguments share the same par-entFor an entity pair, if both of the entities belong tothe same event?s arguments, we take it as positiveexample.
For each positive example, we randomlyexchange one of the entities with an irrelevant en-tity (an irrelevant entity is in the same sentence asthe event, but it is not the event?s argument) to geta negative example.
In the testing procedure, wepredict the relationship between entity i and entityj using the maximum entropy classifier.When the output of the maximum entropyclassifier is around 0.5, it is not easy to figureout whether it is the first relation or the second.We call this kind of information ?uncertaininformation?
(unclear correlation).
For better per-formance, we strengthen the certain informationand weaken the uncertain information.
We set twothresholds, if the output of the maximum entropyclassifier is larger than 0.8, we set Ci,j= 1(positive correlation), if the output is lower than0.2, we set Ci,j= ?1 (negative correlation),otherwise, we set Ci,j= 0 (unclear correlation).The strengthen mapping is similar to the hardtanh in neural network.
If we do not do this,according to the experiment, the performancecannot beat most of the baselines since theuncertain information has very bad noise.6 Experiments6.1 DataWe utilize ACE 2005 data sets as our testbed.
As isconsistent with previous work, we randomly select10 newswire texts from ACE 2005 training corpo-ra as our development set, and then conduct blindtest on a separate set of 40 ACE 2005 newswiretexts.
The remaining 529 documents in ACE train-ing corpus are used as the training data.The training dataset of the argument relation-ship matrix contains 5826 cases (2904 positive and2922 negative) which are randomly generated ac-cording to the ground truth in the 529 training doc-uments.6.2 Systems to CompareWe compare our system against the following sys-tems:?
JET is the baseline of (Grishman et al,2005), we report the paper values of thismethod;?
Cross-Document is the method proposedby Ji and Grishman (2008), which usestopic-related documents to help extractevents in the current document;?
Cross-Event is the method proposed by Liaoand Grishman (2010), which uses document-level information to improve the performanceof ACE event extraction.?
Cross-Entity is the method proposed byHong et al (2011), which extracts eventsusing cross-entity inference.?
Joint is the method proposed by Li et al(2013), which extracts events based onstructure prediction.
It is the best-reportedstructure-based system.?
DMCNN is the method proposed by Chenet al (2015), which uses a dynamic multi-pooling convolutional neural network toextract events.
It is the only neural networkbased method.The Cross-Document, Cross-Event and Cross-Entity are all extensions of JET.
Among these1229MethodTrigger Argument ArgumentClassification Identification RoleP R F1P R F1P R F1JET 67.6 53.5 59.7 46.5 37.2 41.3 41.0 32.8 36.5Cross-Event 68.7 68.9 68.8 50.9 49.7 50.3 45.1 44.1 44.6Cross-Entity 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3Joint 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7DMCNN 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5RBPB(JET) 62.3 59.9 61.1 50.4 45.8 48.0 41.9 36.5 39.0+ ET 66.7 65.9 66.3 60.6 56.7 58.6 49.2 48.3 48.7+ Regu 67.2 61.7 64.3 62.8 57.5 60.0 52.6 48.4 50.4+ ET + Regu 70.3 67.5 68.9 63.2 59.4 61.2 54.1 53.5 53.8Table 1: Overall performance with gold-standard entities, timex, and values, the candidate argumentsare annotated in ACE 2005.
?ET?
means the pattern balancing event type classifier, ?Regu?
means theregularization methodmethods, Cross-Event, Cross-Entity, and DM-CNN make use of the gold-standard entities,timex, and values annotated in the corpus as theargument candidates.
Cross-Document uses theJET system to extract candidate arguments.
Liet al (2013) report the performance with bothgold-standard argument candidates and predictedargument candidates.
Therefore, we compareour results with methods based on gold argumentcandidates in Table 1 and methods based onpredicted argument candidates in Table 2.We have done a series of ablation experiments:?
RBPB(JET): Our own implementation ofJET?
RBPB(JET) + ET: Add pattern balancedevent type classifier to RBPB(JET)?
RBPB(JET) + Regu: Add regularizationmechanism to RBPB(JET)?
RBPB(JET) + ET + Regu: Add both patternbalanced event type classifier and regulariza-tion mechanism to RBPB(JET)6.2.1 The Selection of Hyper-parametersWe tune the coefficients ?1and ?2of Eq 2 on thedevelopment set, and finally we set ?1= 0.10 and?2= 0.45.
Figure 3 shows the variation of ar-gument identification?s F1measure and argumentclassification?s F1measure when we fix one pa-rameter and change another.
Note that the thirdcoefficient 1?
?1?
?2must be positive, which isthe reason why the curve decreases sharply when?2is fixed and ?1> 0.65.
Therefore, Figure 3illustrates that the robustness of our method is verygood, which means if the hyperparameters ?1, ?2are larger or smaller, it will not affect the resultvery much.6.3 Experiment ResultsWe conduct experiments to answer the followingquestions.
(1) Can pattern balancing lead toa higher performance in trigger classification,argument identification, and classification whileretaining the precision value?
(2) Can theregularization step improve the performance ofargument identification and classification?Table 1 shows the overall performance on theblind test set.
We compare our results with theJET baseline as well as the Cross-Event, Cross-Entity, and joint methods.
When adding the eventtype classifier, in the line titled ?+ ET?, we see asignificant increase in the three measures over theJET baseline in recall.
Although our trigger?s pre-cision is lower than RBPB(JET), it gains 5.2% im-provement on the trigger?s F1measure, 10.6% im-provement on argument identification?s F1mea-sure and 9.7% improvement on argument classifi-cation?s F1measure.
We also test the performancewith argument candidates automatically extractedby JET in Table 2, our approach ?+ ET?
again sig-nificantly outperforms the JET baseline.
Remark-ably, our result is comparable with the Joint modelalthough we only use lexical features.The line titled ?+ Regu?
in Table 1 and Table 2represents the performance when we only use theregularization method.
In Table 1, Compared tothe four baseline systems, the argument identifi-1230Method Trigger F1Arg id F1Arg id+cl F1JET 59.7 42.5 36.6Cross-Document 67.3 46.2 42.6Joint 65.6 - 41.8RBPB(JET) 60.4 44.3 37.1+ ET 66.0 47.8 39.7+ Regu 64.8 54.6 42.0+ ET + Regu 67.8 55.4 43.8Table 2: Overall performance with predicted entities, timex, and values, the candidate arguments areextracted by JET.
?ET?
is the pattern balancing event type classifier, ?Regu?
is the regularization method0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.580.5850.590.5950.60.6050.610.6150.62Another coefficientArgument IdentifyF1?2=0.45, ?1?
(0,1]?1=0.10, ?2?
(0,1](a) Arg Identify0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.470.480.490.50.510.520.53Another coefficientArgument Classifier F1?2=0.45, ?1?
(0,1]?1=0.10, ?2?
(0,1](b) Arg ClassifyFigure 3: The trend graph when fix one coefficient and change anothercation?s F1measure of ?+ Regu?
is significantlyhigher.
In Table 2, the ?+ Regu?
again gains ahigher F1measure than the JET, Cross-Document,joint model baseline and ?+ ET?.The complete approach is denoted as ?RBPB?in Table 1 and Table 2.
Remarkably, our approachperformances comparable in trigger classificationwith the state-of art methods: Cross-Document,Cross-Event, Cross-Entity, Joint model, DMCNNand significantly higher than them in argumentidentification as well as classification althoughwe did not use the cross-document, cross-eventinformation or any global feature.
Therefore,the relationship between argument candidatescan indeed contribute to argument identificationperformance.
The event type classifier alsocontributes a lot in trigger identification &classification.
We do the Wilcoxon SignedRank Test on trigger classification, argumentidentification and argument classification, all thethree have p < 0.01.A more detailed study of the pattern feature?seffect is shown in Table 3.
We can see that RBPBwith both plain feature and pattern feature can gainMethod (RBPB) Trigger Arg id Arg id+cl+ Plain feature 66.0 60.5 50.4+ Pattern feature 65.8 60.1 49.2+ Both 68.9 61.2 53.8Table 3: The effect (F1value) of pattern featuremuch better performance than with two kinds offeatures alone.However, our approach is just a pipelineapproach which suffers from error propagationand the argument performance may not affect thetrigger too much.
We can see from Table 1 thatalthough we use gold argument candidates, thetrigger performance is still lower than DMCNN.Another limitation is that our regularizationmethod does not improve the argument classifi-cation too much since it only uses constraints toaffect roles.
Future work may be done to solvethese two limitations.6.4 Analysis of Argument RelationshipsThe accuracy of the argument relationship max-ent classifier is 82.4%.
Fig 4 shows an exampleof the argument relationship matrix, which works1231PowerfulbombAwaitingshedDavaoairportBusPowerful bombA waiting shedDavao airportBusPowerfulbombAwaitingshedDavaoairportBusPowerful bombA waiting shedDavao airportBusFigure 4: The Argument Relationship Matrix.
Left is the origin matrix.
Right is the strengthened matrixfor the sentence in Fig 1.
In the left part of Fig 4,we can see the argument relationship we capturedirectly (the darker blue means stronger connec-tion, lighter blue means weaker connection).
Afterstrengthening, on the right, the entities with strongconnections are classified as positive correlations(the black squares), weak connections are classi-fied as negative correlations (the white squares).Others (the grey squares) are unclear correlation-s. We can see that positive correlation is between?Powerful bomb?
and ?A waiting shed?
as well as?A waiting shed?
and ?Davao airport?.
Therefore,these entities tend to be extracted at the same time.However, ?Powerful bomb?
and ?Bus?
has a neg-ative correlation, so they tend not to be extractedat the same time.
In practice, the argument prob-ability of ?Powerful bomb?
and ?A waiting shed?are much higher than the other two.
Therefore,?Powerful bomb?, ?A waiting shed?
and ?Davaoairport?
are the final extraction results.7 ConclusionIn this paper, we propose two improvements basedon the event extraction baseline JET.
We find thatJET depends too much on event patterns for eventtype priori and JET considers each candidateargument separately.
However, patterns cannotcover all events and the relationship betweencandidate arguments may help when identifyingarguments.
For a trigger, if no pattern can bematched, the event type cannot be assigned andthe arguments cannot be correctly identified andclassified.
Therefore, we develop an event typeclassifier to assign the event type, using bothpattern matching information and other features,which gives our system the capability to deal withfailed match cases when using patterns alone.On the other hand, we train a maximum entropyclassifier to predict the relationship between can-didate arguments.
Then we propose a regulariza-tion method to make full use of the argument rela-tionship.
Our experiment results show that the reg-ularization method is a significant improvement inargument identification over previous works.In summary, by using the event type classifierand the regularization method, we have achieveda good performance in which the triggerclassification is comparable to state-of-the-art methods, and the argument identification& classification performance is significantlybetter than state-of-the-art methods.
However,we only use sentence-level features and ourmethod is a pipelined approach.
Also, theargument classification seems not to be affectedtoo much by the regularization.
Future workmay be done to integrate our method into a jointapproach, use some global feature, which mayimprove our performance.
The code is availableat https://github.com/shalei120/RBPB/tree/master/RBET_releaseAcknowledgementsWe would like to thank our three anonymousreviewers for their helpful advice on variousaspects of this work.
This research was sup-ported by the National Key Basic ResearchProgram of China (No.2014CB340504) and theNational Natural Science Foundation of China(No.61375074,61273318).
The contact author forthis paper is Zhifang Sui.1232ReferencesMary Elaine Califf and Raymond J Mooney.
2003.Bottom-up relational learning of pattern matchingrules for information extraction.
The Journal ofMachine Learning Research, 4:177?210.Nathanael Chambers and Dan Jurafsky.
2011.Template-based information extraction without thetemplates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistic-s: Human Language Technologies-Volume 1, pages976?986.
Association for Computational Linguistic-s.Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,and Jun Zhao.
2015.
Event extraction via dy-namic multi-pooling convolutional neural networks.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages167?176, Beijing, China, July.
Association for Com-putational Linguistics.Hai Leong Chieu and Hwee Tou Ng.
2002.
A max-imum entropy approach to information extractionfrom semi-structured and free text.
AAAI/IAAI,2002:786?791.Fabio Ciravegna et al 2001.
Adaptive informationextraction from text by rule induction and general-isation.
In International Joint Conference on Ar-tificial Intelligence, volume 17, pages 1251?1256.LAWRENCE ERLBAUM ASSOCIATES LTD.Aidan Finn and Nicholas Kushmerick.
2004.
Multi-level boundary classification for information extrac-tion.
Springer.Dayne Freitag.
1998a.
Multistrategy learning forinformation extraction.
In ICML, pages 161?169.Dayne Freitag.
1998b.
Toward general-purpose learn-ing for information extraction.
In Proceedings of the36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Confer-ence on Computational Linguistics-Volume 1, pages404?408.
Association for Computational Linguistic-s.Ralph Grishman, DavidWestbrook, and AdamMeyers.2005.
Nyus english ace 2005 system description.ACE, 5.Zhenmei Gu and Nick Cercone.
2006.
Segment-basedhidden markov models for information extraction.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 481?488.
Association for Computa-tional Linguistics.Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,Guodong Zhou, and Qiaoming Zhu.
2011.
Us-ing cross-entity inference to improve event extrac-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 1127?1136.
Association for Computational Linguistics.Ruihong Huang and Ellen Riloff.
2011.
Peeling backthe layers: detecting event role fillers in secondarycontexts.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistic-s: Human Language Technologies-Volume 1, pages1137?1147.
Association for Computational Linguis-tics.Ruihong Huang and Ellen Riloff.
2012.
Bootstrappedtraining of event extraction classifiers.
In Proceed-ings of the 13th Conference of the European Chapterof the Association for Computational Linguistics,pages 286?295.
Association for Computational Lin-guistics.Scott B Huffman.
1996.
Learning information ex-traction patterns from examples.
In Connectionist,Statistical and Symbolic Approaches to Learningfor Natural Language Processing, pages 246?260.Springer.Heng Ji and Ralph Grishman.
2008.
Refining eventextraction through cross-document inference.
InProceedings of the 46st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 254?262.Jun-Tae Kim and Dan I Moldovan.
1993.
Acquisitionof semantic patterns for information extraction fromcorpora.
In Artificial Intelligence for Application-s, 1993.
Proceedings., Ninth Conference on, pages171?176.
IEEE.Yaoyong Li, Kalina Bontcheva, and Hamish Cunning-ham.
2005.
Using uneven margins svm and per-ceptron for information extraction.
In Proceedingsof the Ninth Conference on Computational NaturalLanguage Learning, pages 72?79.
Association forComputational Linguistics.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 73?82, Sofia, Bulgaria,August.
Association for Computational Linguistics.Shasha Liao and Ralph Grishman.
2010.
Using doc-ument level cross-event inference to improve eventextraction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 789?797.
Association for ComputationalLinguistics.Ting Liu and Tomek Strzalkowski.
2012.
Bootstrap-ping events and relations from text.
In Proceedingsof the 13th Conference of the European Chapterof the Association for Computational Linguistics,pages 296?305.
Association for Computational Lin-guistics.1233Wei Lu and Dan Roth.
2012.
Automatic event extrac-tion with structured preference modeling.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics: Long Papers-Volume 1, pages 835?844.
Association for Compu-tational Linguistics.Mstislav Maslennikov and Tat-Seng Chua.
2007.A multi-resolution framework for informationextraction from free text.
In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONALLINGUISTICS, volume 45, page 592.
Citeseer.Siddharth Patwardhan and Ellen Riloff.
2007.
Ef-fective information extraction with semantic affinitypatterns and relevant regions.
In EMNLP-CoNLL,volume 7, pages 717?727.
Citeseer.Siddharth Patwardhan and Ellen Riloff.
2009.
Aunified model of phrasal and sentential evidence forinformation extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 1-Volume 1, pages 151?160.
Association for Computational Linguistics.Ellen Riloff et al 1993.
Automatically constructing adictionary for information extraction tasks.
In AAAI,pages 811?816.Ellen Riloff, Rosie Jones, et al 1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In AAAI/IAAI, pages 474?479.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedingsof the national conference on artificial intelligence,pages 1044?1049.Satoshi Sekine.
2006.
On-demand information ex-traction.
In Proceedings of the COLING/ACL onMain conference poster sessions, pages 731?738.Association for Computational Linguistics.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of the main confer-ence on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, pages 304?311.
Associ-ation for Computational Linguistics.Stephen Soderland, David Fisher, Jonathan Aseltine,and Wendy Lehnert.
1995.
Crystal: Inducing a con-ceptual dictionary.
arXiv preprint cmp-lg/9505020.Mark Stevenson and Mark A Greenwood.
2005.
Asemantic approach to ie pattern induction.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 379?386.
As-sociation for Computational Linguistics.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representa-tion model for automatic ie pattern acquisition.
InProceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics-Volume 1, pages224?231.
Association for Computational Linguistic-s.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisitionof domain knowledge for information extraction.
InProceedings of the 18th conference on Computation-al linguistics-Volume 2, pages 940?946.
Associationfor Computational Linguistics.Kun Yu, Gang Guan, and Ming Zhou.
2005.
Resumeinformation extraction with cascaded hybrid model.In Proceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, pages 499?506.
Association for Computational Linguistics.1234
