Knowledge-Free Induction of Inflectional MorphologiesPatrick SCHONE                                 Daniel JURAFSKYUniversity of Colorado at Boulder           University of Colorado at BoulderBoulder, Colorado 80309                        Boulder, Colorado 80309schone@cs.colorado.edu                        jurafsky@cs.colorado.eduAbstractWe propose an algorithm  to automatically inducethe morphology of inflectional languages using onlytext corpora and no human input.
Our algorithmcombines cues from orthography, semantics, andsyntactic distributions to induce morphologicalrelationships in German, Dutch, and English.
UsingCELEX as a gold standard for evaluation, we showour algorithm to be an improvement over anyknowledge-free algorithm yet proposed.1       IntroductionMany NLP tasks, such as building machine-readabledictionaries, are dependent on the results ofmorphological analysis.
While morphologicalanalyzers have existed since the early 1960s, currentalgorithms require human labor to build rules formorphological structure.
In an attempt to avoid thislabor-intensive process, recent work has focused onmachine-learning approaches to inducemorphological structure using large corpora.In this paper, we propose a knowledge-freealgorithm to automatically induce the morphologystructures of a language.
Our algorithm takes asinput a large corpus and  produces as output a set ofconflation sets indicating the various inflected andderived forms for each word in the language.
As anexample, the conflation set of the word ?abuse?would contain ?abuse?,  ?abused?, ?abuses?,?abusive?, ?abusively?, and so forth.
Our algorithmextends earlier approaches to morphology inductionby combining various induced information sources:the semantic relatedness of the affixed forms usinga Latent Semantic Analysis approach to corpus-based semantics (Schone and Jurafsky, 2000), affixfrequency, syntactic context, and transitive closure.Using the hand-labeled CELEX lexicon  (Baayen, etal., 1993) as our gold standard, the current versionof our algorithm achieves an F-score of 88.1% onthe task of identifying conflation sets in English,outperforming earlier algorithms.
Our algorithm isalso applied to German and Dutch and evaluated onits ability to find  prefixes, suffixes, and circumfixesin these languages.
To our knowledge, this servesas the first evaluation of complete regularmorphological induction of German or Dutch(although researchers such as Nakisa and Hahn(1996) have evaluated induction algorithms onmorphological sub-problems in German).2 Previous ApproachesPrevious morphology induction approaches havefallen into three categories.
These categories differdepending on whether human input is provided andon whether the goal is to obtain affixes or completemorphological analysis.
We here briefly describework in each category.2.1 Using a Knowledge Source to BootstrapSome researchers begin with some initial human-labeled source from which they induce othermorphological components.
In particular, Xu andCroft (1998) use word context derived from  acorpus to refine Porter stemmer output.
Gaussier(1999) induces derivational morphology using aninflectional lexicon which includes part of speechinformation.
Grabar and Zweigenbaum (1999) usethe SNOMED corpus of semantically-arrangedmedical terms to find semantically-motivatedmorphological relationships.
Also, Yarowsky andWicentowski (2000) obtained outstanding results atinducing English past tense after beginning with alist of the open class roots in the language, a table ofa language?s inflectional parts of speech, and thecanonical suffixes for each part of speech.2.2 Affix InventoriesA second, knowledge-free category of research hasfocused on obtaining affix inventories.
Brent, et al(1995) used minimum description length (MDL) tofind the most data-compressing suffixes.
Kazakov(1997) does something akin to this using MDL as afitness metric for evolutionary computing.
D?Jean(1998) uses a strategy similar to that of Harris(1951).
He declares that a stem has ended when thenumber of characters following it exceed somegiven threshold and identifies any residual following semantic relations, we identified those word pairsthe stems as suffixes.
that have strong semantic correlations as being2.3 Complete morphological analysisDue to the existence of morphological ambiguity(such as with the word ?caring?
whose stem is?care?
rather than ?car?
), finding affixes alone doesnot constitute a complete morphological analysis.Hence, the last category of research is alsoknowledge-free but attempts to induce, for eachword of a corpus, a complete analysis.
Since our Most of the existing algorithms described focus onapproach falls into this category (expanding upon suffixing in inflectional languages (thoughour earlier approach (Schone and Jurafsky, 2000)), Jacquemin and D?Jean describe work on prefixes).we describe work in this area in more detail.
None of these algorithms consider the general2.3.1 Jacquemin?s multiword approachJacquemin (1997) deems pairs of word n-grams asmorphologically related if two words in the first n-gram have the same first few letters (or stem) as twowords in the second n-gram and if there is a suffixfor each stem whose length is less than k. He alsoclusters groups of words having the same kinds ofword endings, which gives an added performanceboost.
He applies his algorithm to a French term listand scores based on sampled, by-hand evaluation.2.3.2.
Goldsmith: EM and MDLsGoldsmith (1997/2000) tries to automatically severeach word in exactly one place in order to establisha potential set of stems and suffixes.
He uses theexpectation-maximization algorithm (EM) and MDLas well as some triage procedures to help eliminateinappropriate parses for every word in a corpus.
Hecollects the possible suffixes for each stem and callsthese signatures which give clues about wordclasses.
With the exceptions of capitalizationremoval and some word segmentation, Goldsmith'salgorithm is otherwise knowledge-free.
Hisalgorithm, Linguistica, is freely available on theInternet.
Goldsmith applies his algorithm to variouslanguages but evaluates in English and French.2.3.3  Schone and Jurafsky: induced semanticsIn our earlier work, we (Schone and Jurafsky(2000)) generated a list of N candidate suffixes andused this list to identify word pairs which share thesame stem but conclude with distinct candidatesuffixes.
We then applied  Latent SemanticAnalysis (Deerwester, et al, 1990) as a method ofautomatically determining semantic relatednessbetween word pairs.
Using statistics from themorphological variants of each other.
With theexception of word segmentation, we provided  nohuman information to our system.
We applied oursystem to an English corpus and evaluated bycomparing each word?s conflation set as producedby our algorithm to those derivable from CELEX.2.4 Problems with earlier approachesconditions of circumfixing or infixing, nor are theyapplicable to other language types such asagglutinative languages (Sproat, 1992).Additionally, most approaches have centeredaround statistics of orthographic properties.
We hadnoted previously (Schone and Jurafsky, 2000),however, that errors can arise from strictlyorthographic systems.
We had observed in othersystems such errors as inappropriate removal ofvalid affixes (?ally?<?all?
), failure to resolvemorphological ambiguities (?hated?<?hat?
), andpruning of semi-productive affixes (?dirty?h?dirt?
).Yet we illustrated that induced semantics can helpovercome some of these errors.However, we have since observed that inducedsemantics can give rise to different kinds ofproblems.
For instance, morphological variants maybe semantically opaque such that the meaning ofone variant cannot be readily determined by theother (?reusability?h?use?).
Additionally,  high-frequency function words may be conflated due tohaving weak semantic information (?as?<?a?
).Coupling  semantic and orthographic statistics, aswell as introducing induced syntactic informationand relational transitivity can help in overcomingthese problems.
Therefore, we begin with anapproach similar to our previous algorithm.
Yet webuild upon this algorithm in several ways in that we:[1] consider circumfixes, [2] automatically identifycapitalizations by treating them similar to prefixes[3] incorporate frequency information, [4] usedistributional information to help identify syntacticproperties, and [5] use transitive closure to help findvariants that may not have been found to besemantically related but which are related to mutualvariants.
We then apply these strategies to English,Figure 1: Strategy and evaluationFigure 2: Inserting the residual lexicon into a trieGerman, and Dutch.
We evaluate our algorithm Figure 2).
Yet using this approach, there may beagainst the human-labeled CELEX lexicon in all circumfixes whose endings will be overlooked inthree languages and compare our results to those the search for suffixes unless we first remove allthat the Goldsmith and Schone/Jurafsky algorithms candidate prefixes.
Therefore, we build a lexiconwould have obtained on our same data.
We show consisting of all words in our corpus and identify allhow each of our additions result in progressively word beginnings with frequencies in excess of somebetter overall solutions.
threshold (T ).
We call these pseudo-prefixes.
We3  Current Approach3.1 Finding Candidate Circumfix PairingsAs in our earlier approach (Schone and Jurafsky,2000), we begin by generating, from an untaggedcorpus, a list of word pairs that might bemorphological variants.
Our algorithm has changedsomewhat, though, since we previously sought wordpairs that vary only by a prefix or a suffix, yet wenow wish to generalize to those with circumfixingdifferences.
We use ?circumfix?
to mean truecircumfixes like the German ge-/-t as well ascombinations of prefixes and suffixes.
It should bementioned also that we assume the existence oflanguages having valid circumfixes that are notcomposed merely of a prefix  and a suffix thatappear independently elsewhere.To find potential morphological variants, our firstgoal is to find word endings which could serve assuffixes.
We had shown in our earlier work how onemight do this using a character tree, or  trie  (as in1strip all pseudo-prefixes from each word in ourlexicon and add the word residuals back into thelexicon as if they were also words.
Using this finallexicon, we can now seek for suffixes in a mannerequivalent to what we had done before (Schone andJurafsky, 2000).To demonstrate how this is done, suppose ourinitial  lexicon / contained the words ?align,??real,?
?aligns,?
?realign?, ?realigned?,  ?react?,?reacts,?
and ?reacted.?
Due to the high frequencyoccurrence of ?re-?
suppose it is identified as apseudo-prefix.
If we strip off ?re-?
from all words,and add all residuals to a trie, the branch of the trieof words beginning with ?a?
is depicted in Figure 2.In our earlier work, we showed that a majority ofthe regular suffixes in the corpus can be found byidentifying trie branches that appear repetitively.By ?branch?
we mean those places in the trie wheresome splitting occurs.
In the case of Figure 2, forexample, the branches  NULL (empty circle), ?-s?and ?-ed?
each appear twice.
We assemble a list ofall trie branches that occur some minimum numberof times (T ) and refer to such as potential suffixes.2Given this list, we can now  find potential prefixesusing a similar strategy.
Using our original lexicon,we can now strip off all potential suffixes from eachword and form a new augmented lexicon.
Then, (aswe had proposed before) if we reverse the orderingon the words and insert them into a trie, thebranches that are formed will be potential prefixes(in reverse order).Before describing the last steps of this procedure,it is beneficial to define a few terms (some of whichappeared in our previous work):[a] potential circumfix: A pair B/E where B and Eoccur respectively in potential prefix and suffix lists[b] pseudo-stem: the residue of a word after itspotential circumfix is removed[c] candidate circumfix: a potential circumfix whichappears affixed to at least T  pseudo-stems that are3shared by other potential circumfixes[d] rule: a pair of candidate circumfixes sharing atleast T  pseudo-stems4[e] pair of potential morphological variants(PPMV): two words sharing the same rule butdistinct candidate circumfixes[f] ruleset: the set of all PPMVs for a common ruleOur final goal in this first stage of induction is tofind all of the possible rules and their correspondingrulesets.
We therefore re-evaluate each word in theoriginal lexicon to identify all potential circumfixesthat could have been valid for the word.
Forexample, suppose that the lists of potential suffixesand prefixes contained ?-ed?
and  ?re-?
respectively.Note also that NULL exists by default in both listsas well.
If we consider the word ?realigned?
fromour lexicon /, we would find that its potentialcircumfixes would be NULL/ed, re/NULL, andre/ed and the corresponding pseudo-stems would be?realign,?
?aligned,?
and ?align,?
respectively,From /, we also note that circumfixes re/ed andNULL/ing share the pseudo-stems ?us,?
?align,?
and?view?
so a rule could be created: re/ed<NULL/ing.This means that word pairs such as ?reused/using?and ?realigned/aligning?
would be deemed PPMVs.Although the choices in T  through T  is1 4somewhat arbitrary, we chose  T =T =T =10 and1 2 3T =3.
In English, for example, this yielded 305354possible rules.
Table 1 gives a sampling of thesepotential rules in each of the three languages interms of frequency-sorted rank.
Notice that several?rules?
are quite valid, such as the indication of anEnglish suffix -s. There are also valid circumfixeslike the ge-/-t circumfix of German.
Capitalizationalso appears (as a ?prefix?
), such as C< c in English,D<d in German, and V<v in Dutch.
Likewise,thereare also some rules that may only be true in certaincircumstances, such as -d<-r in English (such asworked/worker, but certainly not for steed/steer.
)However, there are some rules that areTable 1: Outputs of the trie stage: potential rulesRank ENGLISH GERMAN DUTCH1 -s< L -n< L -en< L2 -ed< -ing -en< L -e< L4 -ing< L -s< L -n< L8 -ly< L -en< -t de-< L12 C-< c- -en< -te -er< L16 re-< L 1-< L -r< L20 -ers< -ing er-< L V-< v-24 1-< L 1-< 2- -ingen < -e28 -d< -r ge-/-t < -en ge-< -e32 s-< L D-< d- -n< -rswrong: the potential ?s-?
prefix of English  is nevervalid although word combinations like stick/tickspark/park, and slap/lap happen frequently inEnglish.
Incorporating semantics can help determinethe validity of each rule.3.2 Computing SemanticsDeerwester, et al (1990) introduced an algorithmcalled Latent Semantic Analysis (LSA) whichshowed that valid semantic relationships betweenwords and documents in a corpus can be inducedwith virtually no human intervention.
To do this,one typically begins by applying singular valuedecomposition (SVD) to a matrix, M, whose entriesM(i,j) contains the frequency of word i as seen indocument j of the corpus.
The SVD decomposes Minto the product of three matrices, U, D, and V  suchTthat U and V  are orthogonal matrices and D is aTdiagonal matrix whose entries are the singularvalues of M.  The LSA approach then zeros out allbut the top k singular values of the SVD, which hasthe effect of projecting vectors into an optimal k-dimensional subspace.
This methodology iswell-described in the literature (Landauer, et al,1998; Manning and Sch?tze, 1999).In order to obtain semantic representations of eachword, we apply our previous strategy (Schone andJurafsky (2000)).
Rather than using a term-document matrix, we had followed an approach akinto that of Sch?tze (1993), who performed SVD ona Nx2N  term-term matrix.
The N here representsthe N-1 most-frequent words as well as a globposition to account for all other words not in the topN-1.
The matrix  is structured such that for a givenword w?s row, the first N columns denote words that-NCS (?,1)PNCSexp[	((x	?
)/1)2]dxNCS(w1,w2 )mink(1,2)cos(w1 ,w2)	?k1k(1)Pr(NCS)nT-NCS(?T,1T)(nR	nT)-NCS(0,1)  nT-NCS(?T,1T).precede w by up to 50 words, and the second Ncolumns represent those words that follow by up to50 words.
Since SVDs are more designed to work then, if there were n  items in the ruleset, thewith  normally-distributed data (Manning and probability that a NCS is non-random isSch?tze, 1999, p. 565), we fill each entry with anormalized count (or Z-score) rather than straightfrequency.
We then compute the SVD and keep thetop 300 singular values to form semantic vectors for We define Pr (w <w )=Pr(NCS(w ,w )).
Weeach word.
Word w would be assigned the semantic choose to accept as valid relationships only thosevectorU D , where U  represents the row ofW= w k wU corresponding to w and D  indicates that only thektop k diagonal entries of D have been preserved.As a last comment, one would like to be able toobtain a separate semantic vector for every word(not just those in the top N).
SVD computations canbe expensive and impractical for large values of N.Yet due to the fact that U and V  are orthogonalTmatrices, we can start with a matrix of reasonable-sized N and ?fold in?
the remaining terms, which isthe approach we have followed.
For details aboutfolding in terms, the reader is referred to Manningand Sch?tze (1999, p. 563).3.3 Correlating Semantic VectorsTo correlate these semantic vectors, we usenormalized cosine scores (NCSs) as we hadillustrated before (Schone and Jurafsky (2000)).The normalized cosine score between two words w1and w  is determined by first computing cosine2values between each word?s semantic vector and200 other randomly selected semantic vectors.
Thisprovides a mean (?)
and variance (1 ) of correlation2for each word.
The NCS is given to beWe had previously illustrated NCS values onvarious PPMVs and showed that this type of scoreseems to be appropriately identifying semanticrelationships.
(For example, the PPMVs of car/carsand ally/allies had NCS values of 5.6 and 6.5respectively, whereas car/cares and ally/all hadscored only -0.14 and -1.3.)
Further, we showedthat by performing this normalizing process, one canestimate the probability that an NCS is random ornot.
We expect that random NCSs will beapproximately normally distributed according toN(0,1).
We can also estimate the distributionN(?
,1 ) of true correlations and number of  termsT T2in that distribution (n ).
If we define  a functionTRsem 1 2 1 2PPMVs with Pr T , where T  is an acceptancesem 5 5threshold.
We showed in our earlier work thatT =85% affords high overall precision while still5identifying most valid morphological relationships.3.4 Augmenting with Affix FrequenciesThe first major change to our previous algorithm isan attempt to overcome some of the weaknesses ofpurely semantic-based morphology induction byincorporating information about affix frequencies.As validated by Kazakov (1997), high frequencyword endings and beginnings in inflectionallanguages are very likely to be legitimate affixes.
InEnglish, for example, the highest frequency rule is-s<L.
CELEX suggests that 99.7% of our PPMVsfor this rule would be true.
However, since thepurely semantic-based approach tends to select onlyrelationships with contextually similar meanings,only 92% of the PPMVs are retained.
This suggeststhat one might improve the analysis bysupplementing semantic probabilities withorthographic-based probabilities (Pr ).
orthOur approach to obtaining Pr   is motivated byorthan appeal to minimum edit distance (MED).
MEDhas been applied to the morphology inductionproblem by other researchers (such as Yarowskyand Wicentowski, 2000).
MED determines theminimum-weighted set of insertions, substitutions,and deletions required to transform one word intoanother.
For example, only a single deletion isrequired to transform ?rates?
into ?rate?
whereastwo substitutions and an insertion are required totransform it into ?rating.?
Effectively, if Cost(&) istransforming cost, Cost(rates<rate) = Cost(s<L)whereas Cost(rates<rating)=Cost(es<ing).
Moregenerally, suppose word X has circumfix C =B /E1 1 1and pseudo-stem -S-, and word Y has circumfixC =B /E  also with pseudo-stem -S-.
Then,2 2 2Cost(X<Y)=Cost(B SE <B SE )=Cost(C <C ).1 1 2 2 1 2Since we are free to choose whatever cost functionwe desire, we can equally choose one whose rangeCost(C1<C2)12 .
f (C1<C2 )max f (C1<Z) ~Zmax f (W<C2)~Wlies in the interval of [0,1].
Hence, we can assign Consider Table 2 which is a sample of PPMVsPr (X<Y) = 1-Cost(X<Y).
This calculation implies from the ruleset for ?-s<L?
along with theirorththat the orthographic probability that X and Y are probabilities of validity.
A validity threshold (T ) ofmorphological variants is directly derivable from the 85% would mean that the four bottom PPMVscost of transforming C  into C .
would be deemed invalid.
Yet if we find that the1 2The only question remaining is how to determine local contexts of these low-scoring word pairsCost(C <C ).
This cost should depend on a number match the contexts of other PPMVs having high1 2of factors: the frequency of the rule f(C <C ),  the scores (i.e., those whose scores exceed T ), then1 2reliability of the metric in comparison to that of their probabilities of validity should increase.
If wesemantics (., where .
 [0,1]), and the frequencies could compute a syntax-based probability for theseof other rules involving C  and C .
We define the words, namely Pr , then assuming independence1 2orthographic probability of validity as we would have:Figure 3 describes the pseudo-code for anWe suppose that orthographic information is less (L) and right-hand (R) sides of each valid PPMV ofreliable than semantic information, so we arbitrarily a given ruleset, try to find a collection of wordsset .=0.5.
Now since Pr (X<Y)=1-Cost(C <C ), from the corpus that are collocated with L and R butorth 1 2we can readily combine it with Pr  if we assume which occur statistically too many or too few timessemindependence using the ?noisy or?
formulation: in these collocations.
Such word sets formPr (valid) = Pr  +Pr  - (Pr  Pr ).
(2) signatures.
Then, determine similar signatures fors-o sem orth sem orthBy using this formula, we obtain 3% (absolute)more of the correct PPMVs than semantics alonehad provided for the -s<L rule and, as will beshown later, gives reasonable improvements overall.3.5 Local Syntactic ContextSince a primary role of morphology ?
inflectionalmorphology in particular ?
is to convey syntacticinformation, there is no guarantee that two wordsthat are morphological variants need to share similarsemantic properties.
This suggests that performancecould improve if the induction process tookadvantage of  local, syntactic contexts around wordsin addition to the more global, large-windowcontexts used in semantic processing.Table 2: Sample probabilities for ?-s<L?Word+s Word Pr Word+s Word Pragendas agenda .968 legends legend .981ideas idea .974 militias militia 1.00pleas plea 1.00 guerrillas guerrilla 1.00seas sea 1.00 formulas formula 1.00areas area 1.00 railroads railroad 1.00Areas Area .721 pads pad .731Vegas Vega .641 feeds feed .54355syntaxPr (valid) = Pr  +Pr  - (Pr  Pr )s-o syntax s-o syntaxalgorithm to compute Pr .
Essentially, thesyntaxalgorithm has two major components.
First, for lefta randomly-chosen set of words from the corpus aswell as for each of the PPMVs of the ruleset that arenot yet validated.
Lastly, compute the NCS andtheir corresponding probabilities (see equation 1)between the ruleset?s signatures and those of the to-be-validated PPMVs to see if they can be validated.Table 3 gives an example of the kinds ofcontextual words one might expect for the ?-s<L?rule.
In fact, the syntactic signature for ?-s<L?
doesindeed include such words as are, other, these, two,were, and have as indicators of words that occur onthe left-hand side of the ruleset, and a, an, this, is,has, and A as indicators of the right-hand side.These terms help distinguish plurals from singulars.Table 3: Examples of ?-s<L?
contextsContext for L Context for Ragendas are seas were a legend this formulatwo red pads pleas have militia is an areathese ideas other areas railroad has A guerrillaThere is an added benefit from following thisapproach: it can also be used to find rules that,though different, seem to convey similarinformation .
Table 4 illustrates a number of suchagreements.
We have yet to take advantage of thisfeature, but it clearly could be of use for part-of-speech induction.procedure SyntaxProb(ruleset,corpus)leftSig  =GetSignature(ruleset,corpus,left)rightSig=GetSignature(ruleset,corpus,right)=Concatenate(leftSig, rightSig)ruleset(?
,1 )=ComparetoRandom()ruleset ruleset rulesetforeach PPMV in rulesetif   (Pr (PPMV)  T  )   continueS-O 5wLSig=GetSignature(PPMV,corpus,left)wRSig=GetSignature(PPMV,corpus,right)=Concatenate(wLSig, wRSig)PPMV(?
,1 )=ComparetoRandom()PPMV PPMV PPMVprob[PPMV]=Pr(NCS(PPMV,ruleset))end procedurefunction GetSignature(ruleset,corpus,side)foreach PPMV in rulesetif   (Pr (PPMV) < T  )   continueS-O 5if  (side=left) X = LeftWordOf(PPMV)else  X = RightWordOf(PPMV)CountNeighbors(corpus,colloc,X)colloc  =SortWordsByFreq(colloc)for i = 1 to 100 signature[i]=colloc[i]return signatureend functionprocedure CountNeighbors(corpus,colloc,X)foreach W in Corpuspush(lexicon,W)if (PositionalDistanceBetween(X,W)2)count[W] = count[W]+1foreach W in lexiconif ( Zscore(count[W]) 3.0   orZscore(count[W]) -3.0)colloc[W]=colloc[W]+1end procedureFigure 3: Pseudo-code to find Probability  syntax Figure 4: Semantic strengthsTable 4: Relations amongst rulesRule Relative Cos Rule Relative Cos-s<L -ies<y 83.8 -ed<L -d<L 95.5-s<L -es<L 79.5 -ing<L -e<L 94.3-ed<L -ied<y 81.9 -ing<L -ting<L 70.73.6 Branching Transitive ClosureDespite the semantic, orthographic, and syntacticcomponents of the algorithm, there are still validPPMVs, (X<Y), that may seem unrelated due tocorpus choice or weak distributional properties.However, X and Y may appear as members of othervalid PPMVs such as (X<Z) and (Z<Y) containingvariants (Z, in this case) which are eithersemantically or syntactically related to both of theother words.
Figure 4 demonstrates this property ingreater detail.
The words conveyed in Figure 4 areall words from the corpus that have potentialrelationships between variants of the word ?abuse.
?Links between two words, such as ?abuse?
and?Abuse,?
are labeled with a weight which is thesemantic correlation derived by LSA.
Solid linesrepresent valid relationships with Pr 0.85 andsemdashed lines indicate relationships with lower-than-threshold scores.
The absence of a link suggests thateither the potential relationship was never identifiedor discarded at an earlier stage.
Self loops areassumed for each node since clearly each wordshould be related morphologically to itself.
Sincethere are seven words that are valid morphologicalrelationships of ?abuse,?
we would like to see acomplete graph containing 21 solid edges.
Yet, onlyeight connections can be found by semantics alone(Abuse<abuse, abusers<abusing, etc.
).However, note that there is a path that can befollowed along solid edges from every correct wordto every other correct variant.
This suggests thattaking into consideration link transitivity (i.e., ifX<Y , Y <Y , Y <Y ,... and Y<Z, then X<Z)1 1 2 2 3 tmay drastically reduce the number of deletions.There are two caveats that need to be consideredfor transitivity to be properly pursued.
The firstcaveat: if no rule exists that would transform X intoZ, we will assume that despite the fact that theremay be a probabilistic path between the two, wePr?
itNtj0 pj.function BranchProbBetween(X,Z)prob=0foreach independent path ?jprob = prob+Pr (X<Z) - (prob*Pr (X<Z) )?j ?jreturn probFigure 5: Pseudocode for Branching ProbabilityFigure 6: Morphologic relations of ?conduct?will disregard such a path.
The second caveat is that  the algorithms we test against.
Furthermore, sincewe will say that paths can only consist of solid CELEX has limited coverage, many of these lower-edges, namely each Pr(Y<Y ) on every path must frequency words could not be scored anyway.
Thisi i+1exceed  the specified  threshold.
cut-off also helps each of the algorithms to obtainGiven these constraints, suppose now there is a stronger statistical information on the words they dotransitive relation from X to Z by way of some process which means that any observed failuresintermediate path ?={Y Y Y }.
That is, assume cannot be attributed to weak statistics.i 1, 2,.. tthere is a path X<Y  Y <Y ,...,Y<Z.
Suppose Morphological relationships can be represented as1, 1 2 talso that the probabilities of these relationships are directed graphs.
Figure 6, for instance, illustratesrespectively p , p , p ,...,p .
If  is a decay factor in the directed graph, according to CELEX, of words0 1 2 tthe unit interval accounting for the number of link associated with ?conduct.?
We will call the wordsseparations, then we will say that the Pr(X<Z) of such a directed graph the conflation set for any ofalong path ?
has probability                       .
We the words in the graph.
Due to the difficulty inicombine the probabilities of all independent paths developing a scoring algorithm to compare directedbetween X and Z according to Figure 5: graphs, we will follow our earlier approach and onlyIf the returned probability exceeds T , we declare X5and Z to be morphological variants of each other.4 EvaluationWe compare this improved algorithm to our formeralgorithm (Schone and Jurafsky (2000)) as well asto Goldsmith's Linguistica (2000).
We use as inputto our system 6.7 million words of Englishnewswire, 2.3 million of German, and 6.7 million ofDutch.
Our gold standards are the hand-taggedmorphologically-analyzed CELEX lexicon in eachof these languages (Baayen, et al, 1993).
We applythe algorithms only to those words of our corporawith frequencies of 10 or more.
Obviously this cut-off slightly limits the generality of our results, butit also greatly decreases processing time for all ofcompare induced conflation sets to those ofCELEX.
To evaluate, we compute the number ofcorrect (&), inserted (,), and deleted (') words eachalgorithm predicts for each hypothesized conflationset.
If X  represents word w's conflation setwaccording to an algorithm, and if Y   represents itswCELEX-based conflation set, then,& = ~w(|X Y |/|Y |), w w w' = ~w(|Y -(X Y )|/|Y |), andw w w w,  = ~w (|X -(X Y )|/|Y |),w w w wIn making these computations, we disregard anyCELEX words absent from our data set and viceversa.
Most capital words are not in CELEX so thisprocess also discards them.
Hence, we also make anaugmented CELEX to incorporate capitalized forms.Table 5 uses the above scoring mechanism tocompare the F-Scores (product of precision andrecall divided by average of the two ) of our systemat a cutoff threshold of 85% to those of our earlieralgorithm (?S/J2000?)
at the same threshold;Goldsmith; and a baseline system which performsno analysis (claiming that for any word, itsconflation set only consists of itself).
The ?S?
and?C?
columns respectively indicate performance ofsystems when scoring for suffixing andcircumfixing (using the unaugmented CELEX).
The?A?
column shows circumfixing performance usingthe augmented CELEX.
Space limitations requiredthat we illustrate ?A?
scores for one language only,but performance in the other two language issimilarly degraded.
Boxes are shaded out foralgorithms not designed to produce circumfixes.Note that each of our additions resulted in anoverall improvement which held true across each ofthe three languages.
Furthermore, using ten-foldcross validation on the English data, we find that F-score differences of the S column are eachstatistically significant at least at the 95% level.Table 5: Computation of F-ScoresAlgorithms English German DutchS C A S C S CNone 62.8 59.9 51.7 75.8 63.0 74.2 70.0Goldsmith 81.8 84.0 75.8S/J2000 85.2 88.3 82.2+orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9+ syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4+ transitive 84.5 79.7 78.9 79.688.1 92.3 85.85 ConclusionsWe have illustrated three extensions to our earliermorphology induction work (Schone and Jurafsky(2000)).
In addition to induced semantics, weincorporated induced orthographic, syntactic, andtransitive information resulting in almost a 20%relative reduction in overall  induction error.
Wehave also extended the work by illustratingperformance in German and Dutch where, to ourknowledge, complete morphology inductionperformance measures have not previously beenobtained.
Lastly, we showed a mechanism wherebycircumfixes as well as combinations of prefixingand suffixing can be induced in lieu of the suffix-only strategies prevailing in most previous research.For the future, we expect improvements could bederived by coupling this work, which focusesprimarily on inducing regular morphology, with thatof Yarowsky and Wicentowski (2000), who assumesome information about regular morphology in orderto induce irregular morphology.
We also believethat some findings of this work can benefit otherareas of linguistic induction, such as part of speech.AcknowledgmentsThe authors wish to thank the anonymous reviewersfor their thorough review and insightful comments.ReferencesBaayen, R.H., R. Piepenbrock, and H. van Rijn.
(1993)The CELEX lexical database (CD-ROM), LDC, Univ.of Pennsylvania, Philadelphia, PA.Brent, M., S. K. Murthy, A. Lundberg.
(1995).Discovering morphemic suffixes: A case study inMDL induction.
Proc.
Of 5  Int?l Workshop onthArtificial Intelligence and StatisticsD?Jean, H. (1998) Morphemes as necessary concepts forstructures: Discovery from untagged corpora.Workshop on paradigms and Grounding in NaturalLanguage Learning, pp.
295-299.Adelaide, AustraliaDeerwester, S., S.T.
Dumais, G.W.
Furnas, T.K.Landauer, and R. Harshman.
(1990) Indexing byLatent Semantic Analysis.
Journal of the AmericanSociety of Information Science, Vol.
41, pp.391-407.Gaussier, ?.
(1999) Unsupervised learning of derivationalmorphology from inflectional lexicons.
ACL '99Workshop: Unsupervised Learning in NaturalLanguage Processing, Univ.
of Maryland.Goldsmith, J.
(1997/2000) Unsupervised learning of themorphology of a natural language.
Univ.
of Chicago.http://humanities.uchicago.edu/faculty/goldsmith.Grabar, N. and P. Zweigenbaum.
(1999) Acquisitionautomatique de connaissances morphologiques sur levocabulaire  m?dical, TALN, Carg?se, France.Harris, Z.
(1951) Structural Linguistics.
University ofChicago Press.Jacquemin, C. (1997) Guessing morphology from termsand corpora.
SIGIR'97, pp.
156-167, Philadelphia, PA.Kazakov, D. (1997) Unsupervised learning of na?vemorphology with genetic algorithms.
In W.Daelemans, et al, eds., ECML/Mlnet Workshop onEmpirical Learning of Natural Language ProcessingTasks, Prague, pp.
105-111.Landauer, T.K., P.W.
Foltz, and D. Laham.
(1998)Introduction to Latent Semantic Analysis.
DiscourseProcesses.
Vol.
25, pp.
259-284.Manning, C.D.
and H. Sch?tze.
(1999) Foundations ofStatistical Natural Language Processing, MIT Press,Cambridge, MA.Nakisa, R.C., U.Hahn.
(1996) Where defaults don't help:the case of the German plural system.
Proc.
of the18th Conference of the Cognitive Science Society.Schone, P. and D. Jurafsky.
(2000) Knowledge-freeinduction of morphology using latent semanticanalysis.
Proc.
of the Computational NaturalLanguage Learning Conference, Lisbon, pp.
67-72.Sch?tze, H. (1993) Distributed syntactic representationswith an application to part-of-speech tagging.Proceedings of the IEEE International Conference onNeural Networks, pp.
1504-1509.Sproat, R. (1992) Morphology and Computation.
MITPress, Cambridge, MA.Xu, J., B.W.
Croft.
(1998) Corpus-based stemming usingco-occurrence of word variants.
ACM Transactions onInformation Systems, 16 (1), pp.
61-81.Yarowsky, D. and R. Wicentowski.
(2000) Minimallysupervised morphological analysis by multimodalalignment.
Proc.
of the ACL 2000, Hong Kong.
