Generating a grammar for statistical trainb~gR.A.
Sharman, IBH(UK) Sc ience  Cent re ,Winchester  SHARMAN at VENTAY .
Je l inek ,  T.J.WaCson Research,Yorktown He?ghCs JELINEK at YKTVMVR.Hercer, T.J.Wagson Research,Yorktown Heights MERCER at YKTVHVThe problem of parsing Natural LanguageParsing sentences of a Natural LanguaF(NL) is an essential requiRrnent for a variety of NL applications, and has been extensively studied.
In particular, the sort of tasks which it would be de-sirable to do, include the ability to tag each word with its part-of-sin:h; to delineate with brackets,and label with a category name, each syntactic phrase; and to be able to adapt to different ypesof source material.
Despite some 30 years of active research Performing these tasks with a highdegree of accuracy on unrestricted text is still an unsolved problem.The conventional pproach is a grammar, usually created manually by the encoding of some lin-guistic intuitions in some notation.
Many grammars have a substantial context.freegrammar~CFG) component, or ate equivalent in computational power to CFG's.
A standardparsing algorithm can then be used to obtain analyses of any given sentence.
A discussion of therelevant concepts of parsing CFG'S is given in lloperott and Ullman, 1979, where the CKY algo-rithm, the fast of the ~art  parsing t~hnlques, is described.
A recent example of a grammar, ofCFG power, is the C~neralised Phrase Structure Grammar (GPSG) given by Gazdar, Klein,PuUum and Sag, 1985.
Simple context-free grammars, and systems derived from them, have theconsequence that practical grammars of full Natural Languages tend to be very large; algorithms fi~rparsing rarru'nars are computationally expensive; and hand-written grammars arc often incomplete,and are usually highly ambiguous.
Consequently this method alone is unlikely to solve the problemof parsing NL's.An extension to context-free grammars which considers each rule to be associated with a probabifityis called a probabiflstic ontext-free grammar, or P-CFG (see Wetheriil, 1980 for a full discussion).Conceptually, the probability of a rule for a given non-terminal symbol is the likelihood with whichthe rule is applied, as opposed to other rules for rewriting the same non-terminal label.
With theaddition of this extra piece of information it is possible to choose the parse which is the most likelyfrom among all the ambiguous parses.
The "most likely" parse is considered to correspond to the"correct" parse.
A method exists for training such grammars with respect o some corpus, theInside-Outside Algorithm (Baker, 1979).
It is described in Jelinek, 1985(b), and has been used by,for example~ Fujisaki 1987.
Adapting a model with reference to a training cnrlnzv should enable themodel to be used with greater success on an unseen te.~t corpu.v.
The problems of large grammars,expensive algorithms, incomplete and ambiguous grammars, arc essentially the same as for simplecontext-free grammars.
Additionally, estimates must be made of the probabilities of the rules in thegrammar.
Since the true values for a natural anguage, such as I':,nglish, arc not known, the qualityof the estimates made is rather important.
Any arbitrary initial guess may be a Io,g way from thetrue value, and will need a great deal of training to achieve a good estimate, if the process oftraining is slow, then it may not be possible to do enough training to significantly improve the es-timates.
If the grammar being trained is not a good grammar, then it may not be possible to get agood solution to the parsing problem despite a very iF'eat deal of training.It is assumed here that no restriction on vocabulary or sentence construction can be made.
Thereare a number of significant applications, uch as speech recognition and speech synthesis, for whichthis is the only reasonable assumption (see Jelinek, 1985(a)).
The search for a method to performfast, accurate parsing of unrestricted natural language sentences may require other models.
Oneway forward is to attempt o use a formulation of grammatical knowledge which uses h'Lformationin a more compact way.
A popular method is the use of so-called unification grammars (derivedin turn from GPSG).
One attempt o design a computable form for such a grammar is described267by Sharman, 1989.
An alternative step is to attempt to consider a development of the ID/LP no-tation introduced in GPSG.Probabilistic ID/LP grammarsThe idea of separating simple context-free rules into two, orthogunal rule sets, immediatedominance(ID) rules, and linear precedence(LP) rules, gives a notation for writing grammars calledID/LP.
This technique is used in GPSG with a number of other techniques to represent linguisticfacts in a compact and perspicuous formalism.
Some of the other aspects of GPSG, such as theuse of features to represent information in non-terminal categories, the use of feature co-occurrencerestrictions, the use of feature specification defaults, and the definition of feature passing con-ventions, are not considered here.It is assumed in GPSG that dominance and precedence r lations are independent.
The independ-ence of separate dominance rules does not seem problematical, but whether or not precedence rulesare uniform across all phrase types does seem more contentious.
As a result, ID/LP grammars tendto have a rather rich collection of ID rules, and a rather small collection of LP rules.
This raisesthe interesting question of the possibility that ID and LP rules are not independent, but this pos-sibility is not pursued here.The notion of a grammatical relation, such as precedence or dominance, can be generallsed to meanthe propensity of a symbol to relate to another symbol.
For example, a noun phrase has a pro-pensity to contain a noun.
Since it is clear that at least some noun-phrases do not contain nouns,this propensity will not be a certainty.
However, the propensity of noun phrases to contain a nounwill, presumably, be rather greater than, for example, the propensity of noun-phrases to (directly)contain a verb, or of other phrases to contain a noun.
In other words, for any given phrase thereis a probability distribution over the objects which can be its immediate constituents, which we cancall the dominance, probability.
By a similar argument, here is also a probability distribution overthe ordering of items in a phrase which we can call the precedence probability.
Thus an ID/LPgrammar which uses probabilities i a probabilistic ID/LP grammar, or P-ID/LP.Computing the probability of a sentenceIn order to use a P- ID/LP grammar analogously to a I ' -CFG we need to establish a way of com-puting the probability of a sentence, given the probability of I13 and l,P rules.
The likelihood ofthe derivation of a sentence, W, from the topmost distinguished symbol, S, for an ID/LP grammar,can be determined from the independent likelihoods of each step in the derivation.
Each step is theresult of replacing some non-terminal symbol by some other terminal and non-terminal symbols,(the dominance relation), and the result of rearranging those derived symbols in the desired order(the precedence relation).Thus, in the derivation of  the sentence S~ W there is a .sequence of individual steps, which mayresult in a derivation .,erich asS~...~xAy~xBCy.~...~wlw2...w " 1.
WThe immediate dominance rule A --* B, C has been used to generate xBCy from xAy at the inter-mediate step indicated.
"rhat is, the A was replaced by a B and a C without reference to their order.This rule is considered to have the conditional probability Po(B, C\] A) , or the dominance proba-bility of B3 and C given ,4.
Since this would have allowed both the rewriting B C, and CB, theprecedence rule B<C must also have been used.
"rhis rule is considered to have the probabilityPe(B'< C) , or the precedence probability of B before C. If the probability of the string xAy isP(xAy), then the probability of the string xBCy can be considered to be268P(xBCy)- l~xAy).
Po(B, CIA).
P~B,< C)Thus, the derivation is treated as a stochastic process involving independent probability distrib-utions governing the symbols which are produced in each step, and the order of the symbols.
Theresult is the simple stochastic proofs conventionally modelled as production rule probabilities.The probability of a single derivation of W from 5 is denoted by P(WIS, a~), where ~ representsthe i-th derivation.
It is calculated from the product of all the individual steps in the derivation,and is clearly independent of the order of appfication of the individual steps in the derivation.
Thetotal probability of the word string W, or P(W\]~, is the sum of all the separate derivations from3 to W, or ~P(WI 3, ?~).
The more useful quantity in parsing is the derivation for which theprobability is'the greatest, or the max P(W\] 3, a~) .
This derivation is the most likely derivation,Iand thus defines the parse tree to be selected for the given sentence.The general idea can be extended, in a straightforward way, not described here, for the case of un-restricted rules, where there are an unlimited number of symbols on the fight hand side of acontext-free rule.
In this case the longer rules are modelled as successive appfications of shorterrules.
The advantage of this decomposition is that the size of the rule sets are to some extentgoverned by the size of the symbol set chosen, and do not involve the huge tail of low frequencyrules typical of P-CFG grammars.
For example, if there are n non-terminal symbols in the gram-mar, there can only be tO precedence r lations between them, and an estimate of all of them canbe made, avoiding the problem of unknown rules.
Similarly the number of dominance rules is alsorestricted over the equivalent number of conventional phrase structure rules.Determining P-ID/LP grammatical relationsThe values of the probability of each dominance and precedence rule must be known so thatparsing can take place according to the scheme described above.
These values can be determinedeither by observation, or by training.In order to determine the values by observation, a large corpus of pre-analysed sentences can beinspected, and the dominance and precedence frequencies can be determined by counting.
Fromthese frequencies it is simple to compute the dominance and precedence probabilities.Alternatively, values of the dominance and precedence probabilities can be determined by assumingsome arbitrary initial estimate, and training on a corpus, in a way similar to that used in theInside-Outside algorithm.
This involves computing the likelihood of the topmost label, 3 produc-ing each observed sentence, W, and collecting counts of observed ominance and precedence r -lations, which are then used to re-estimate he probabilities of dominance and precedence.
Thisre-estimation can be done many times, until satisfactory estimates of the true probabilities are ob-tained.Since a suitable corpus of pre-parsed sentences was available the first method was used for sim-plicity.
This is called adapting the grammar to the corpus, to distinguish it from training which isthe technical term used for estimates derived from iterative re-estimation algorithms.Creating a P-ID/LP parserThe probability of a sentence can be calculated in an entirely analogous way to that done forP-CFG's, by using a modified form of the CKY algorithm.
The CKY table holds a place for theprobability of every possible substring which the grammar produces for a given sentence.
This iscalculated from the bottom up, re-using already completed calculations where necessary.
Thecomputational complexity of this task is known to be related to the cube of the sentence l ngth (see\[lopcrcfft, 1979), which is at least a polynomial calculation, rather than an exponential one.269T~aniques for thresholding the computation can he used to speed up the parser.
Because therecan be fewer relations to compute than there might be rules in a P-CFG the parser may be faster.Using a P-ID/LP grammar to parse EnglishThis section describes an experiment to determine if P-ID/LP parsing, in the manner describedabove, is a useful technique.
The necessary grammatical relations were obtained from a coUectionof unrestricted sentences of English taken from the Associated Press (AP) newswhe material, con-sisting of about one million words of text, with a vocabelary of about 50,000 words, The sentenceswere available in a pre-parsed form, called a treebank, with about 45,000 different phrase typesmarked by the manual parsers.
A simple CFG could require as many rules as phrase types to rep-resent the complexity shown in this data.
The trcebank was used to determine the values ofdominance and precedence probabilities, and to test the output of the parser.
For these purposesthe original treebank was divided into two equal parts, so that a grammar derived from one partcould be tested on the other.
A modified CKY parser was used to parse the sentences with theP-ID/LP grammar, and to select he most likely parses.
The resulting system has the ~pability toparse any sentence of English, although it is adapted specifically to the AP corpus.The foUowing steps were taken:1.
A restricted set of 16 non-terminal symbols were derived from the 64 actually used in thetreebank.2.
A restricted set of 100 terminal symbols were derived from the 264 actually used in thetrcebank.3.
The trcebank was divided into two parts, one for adapting the grammar, and one for testing.4.
The word list of the adapting data was extracted, with the unigram frequency of each word,and the tag attached to the word.
This word llst is used to create:a.
A lexicon of word-to-tag correspondences, with the unigrarn probability of each entry.This is used to generate the probability of tag assignments o words when those words arefound in a sentence to be parsed.b.
A probability distribution over terminal tags.
This is used to predict he tag of a word ina sentence, when that word does not appear in the lexicon.5.
Initial estimates of the dominance relations for non-terminal symbols were derived from thetreebank.6.
Initial estimates of the precedence r lations for non-terminal and terminal symbols were de-rived from the trcebank.A variant of the CKY parser for unrestricted rules was used to compute the probability of eachsentence.
The completed parse includes the most likely tab assigned to each word, and the mostlikely constituents hypothesised over the substrings of the sentence.
The best parse can be displayedas a conventional parse tree.The resultsA test set of 42 sentences was chosen at random, subject only to the restriction that the sentencelength was less than 30 words.
The average sentence length was 20 (as opposed to 23 for the APas a whole).
These sentencas were parsed, and the results compared with manual analyses of thesame sentences.
For the purposes of comparison, parses were divided into the following classes toshow the accuracy of the parse, relative to the manual parse: exact match; close approximation;incorrect parse; failed parse.
Some examples of these categories are given in the Appendix, to showthe type of assessment which has been made.270The sentences were parsed in four ways: using a grammar which was unadapted(Gl) oradapted(G2), and using a lexicon which was unadapted(Ll) or adapted(L2), the adaptation beingto the sample from which the test sentences were drawn.
It would be expected that the systemshould do well on data which it was adapted to, and less well on data it was no adapted to.
Theresulting parses were manually inspected for errors.The results for tagging are as follows:+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+I TAGGING \] G I , L1  l G1 ,L2  J G2 ,L1  \] G2 ,L2  \]+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+Jcorrect tags I 660 J 789 I 658 I 779 IJavg.
cor rec t  tags /sent i  15.7l 18.8\[ 15.6\[ 18.5J\[~ tags  cor rec t  J 80 J 96 \] 80 J 95 J+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+The results for parsing are as follows:+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+J PARSING J GI ,L1  J G1 ,L2  I G2 ,L1  J G2 ,L2  J+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+\ ] cor rec t  parses  J 8 I 10 I 12 J 18 JJs~ilar parses  J 17 J 22 J 20 J 19 J\ ]wrong parses  \[ 13 J 10 J 6 J 5 J\ ] fa i led  parses  J 6 J 0 J & J 0 J+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
?
.
.
.
.
.
.
.
+The worst performance is the unadapted grammar and lexicon, and the best performance is theadapted grammar and lexicon.
The adapted grammar with an unadapted lexicon, and the una-dapted grammar with an adapted lexicon, are in between, and about as good as each other.
On thebasis of this data there is no reason to distinguish between these latter two cases.Conc lus ionsInitial results indicate that tagging accuracy is quite good, but that parsing accuracy is less good.This should of course be compared to the performance of other parsers and grammars on materialof similar complexity.The performance of this system as a tagging tool is similar to a word tagging system using a I liddenMarker Model.
Such a system would expect an error rate of no more than 5%, and perhaps as lowas one or two percent.
An exact comparison would have to take into account a consideration ofthe actual tags appfied, the type of text used.The performance of the system in parsing is not quite as successful as one would like, but there issome satisfaction to be obtained from the fact that correct, or nearly correct, parses account for60% of the total even for the unadapted grammar, rising to 76% for the intermediate cases, and88% for the fully adapted system.
A large number of the causes of error can be seen to be relatedto semantic and pragmatic issues which the model does not, by definition, address.
This holds outhope that the method may be capable of improvement by terming the precision of the relationsmodelled.Some specific causes of imperfect parses were due to the choice of non-terminal symbols.
Thetreebank symbols, while being perfectly adequate for manual parsing, are insufficiently precise for271an automatic system to distinguish between different categories of phrase.
Either a better set ofnon-terminal symbols hould be used, or features hould be used to make existing categories ca-pable of carrying freer distinctions.Also, the precedence and dominance relations chosen are insufficiently precise to give good results.For example, the pn_.cedence r lation is determined over all non-terminal symbols.
It is possiblethat the order of symbols in some group of phrases is different than the order in another group, andthat different precedence r lations could express this.It is planned to relax these restrictions in future work.ReferencesBaker, J.K. Trainable Grammars for Speech Recognition, in D.H.Klatt and JJ.Woff (eds),Speech communication papers for the 971h meeting of the Acouxtical Society of America, 1979Fujisaki, T A ProbabiiLftic Modelling Methods for Language Parsing, IBM research ReportRC 13296, Yorktown 1987Gazdar, G., E.Klein, G.K.Pullum and l.A.Sag Generalised P/wg~e Structure Grammar,Blackwell 1985Hopcroft, J.E and J.D.Ullman Introduction to Automata, Theory, Languages, andcomputation, Addison-Wesley 1979Jelinek, F. The development of a Large Vocabulary Speech recognition system, Proc.
IEEE vol.73, No.
!1, 1985(a)JeIinek, F. Markov Source Modelling of Text Generation Report of the Continuous SpeechRecognition Group, IBM TJ.Watson Research Center, NY.
1985(b)Sharman, R.A. A Categoriai Plwaze Structure Grammar Development system, IBM UKSCTechnical Report 198, 1989(a)Wetherill, C.S., Probabilistic Languages: A Review and some Open Questions, ComputingSurveys, vol.
12, No.
4, 1980272Appendix - Examples of parse classificationThe ofional sentence is shown (h~dicated by the prefix $:), with the manual parse(indicated by theprefix M:), and the automatic parse (indicated by the prefix A:).
Part of speech tags have been re-moved for clarity.
Each phrase is marked by a bracket pair which is labelled according to the typeof phrase delimited.
The labclfing on the brackets hould be serf-explanatory, with N referring toa noun-pht~e, V a verb-pht~e, P a prepositional-phmze, andso on.Exact matchThe parse produced is an exact match of the hand parsed sentence, xcept for certain totally pre-dictable typographical conventions.S: But cas ino pro f i t s  pl-n~,eted .M: But iN cas ino pro f i t s  N\] iV plummeted V\]A: iS But iN cas ino pro f i t s  N\] iV plummeted V\] .
_ .
S\]Equivalent parseThe parse produced is a reasonable parse of the sentence, and could have equally well beenproduced by the manual analysis.S: The c i ty  has about 900 f i re f ightersM: \[N The c i ty  N\] iV has iN about 900 f i re f ighters  N\] V\] :_.A: \ [S \ [N  The city N\] \[V has \[P about IN 900 firefighters N\] P\] V\] ._.
s\]\]Prepositional phrase attachment problemThe parse is essentially correct, but a prepositional phrase has been attached in the wrong place,or has the wrong scope.S: The pancreas , in addition to making digestive enzymes , isthe body organ that produces insulin , a hormone thatcontrols the level of sugar in the blood .M: iN The pancreas N\] , \[P in iN add i t ion  \[P to \[Tg making iNd igest ive  enzymes N\] Tg\] P\] N\] P\] , iV is  iN the bodyorgan \[Fr that  iV produces iN insu l in  , iN a hormone \[Frthat .
iV  cont ro l s  iN the leve l  \[P of  iN sugar S \ ]P \ ]  {P in iNthe blood N\] P \ ]N \ ]  V\] Fr\] N\] N\] V\] Fr\] N\] V\]A: iS iN The pancreas N\] , \[P in iN add i t ion  \[P to iV makingiN d igest ive  enzymes N\] V\] P\] N\] P\] , iV is iN the bodyorgan \[Fr that  iV produces iN insu l in  N\] V\] Fr\] N\] V\] , iN ahormone \[Yr that  iV cont ro l s  \[N the leve l  \[P of  iN sugar \[Pin iN the blood N\] P\] N\] P \ ]N \ ]  V\] Fr\] N\] S\]Coordination problemsThe parse is essentially correct, but a coordinated phrase has coordinated item of the wrong scope.273S: The bacteria can be transmitted in water and from bodies ofdead b i rds  ,M: \[N The bacteria N\] \[V can be transmitted \[P \[P& in \[N waterN\] P&\] and \[P+ from \[N bod ies  \[P of \[N dead birds N\] P\] N\]P?\] p\] v\]A: \[S \[N The bacteria N\] \[V can be transmitted \[P in \[N water\[ and \[P from \[N bodies \[P of \[N dead birds N\] P\] N\] P\] \]N\] P\] v\] s\]Problems with commasThe parse is essentially correct, but a wrong decision has been made over the interpretation of oneor more commas.S: At garen 's  Co l lec tab les  , owner Karen Walker had a presentfo r  Kennedy , a s te r l ing  s i l ver  t ie - tack  in the  image ofJohn Kennedy .lq: \[P At \[N \[G Karen 's  G\] Collectables N \ ]P \ ]  r\[N owner INKaren Walker N\] N\] IV had \[N a present \ [P fo  \[N Kennedy N\]P\] , \[N a sterling silver tie-tack \[P in \[N the image \[P of\[N John Kennedy N\ ]P \ ]  N\] P\] N\] N\] V\]A: \ [S \ [P  At \[N \[G Karen 's  G\] Co l lec tab les  N\] P\] , \[N ownerKaran Walker \[ had \[N a present  \[P fo r  \[N Kennedy N\] P\] N\]\] N\] ,_, IN a sterling silver tle-tack \[P in \[N the image\[P of \[N John Kennedy N\] P\] N \ ]P \ ]  N\] S\]Wrong ParsesThe bestp~se ~und isnot the  fight parse.
1~?
manualparscisconsideredtobcthe fig t parse.S: Oil-state senators are trying to block an amendment to the "windfall-proflts " tax that would cost the oll industry$22.5 billion over the next decade .M: \[N O i l - s ta te  senators  N\] IV are  t ry ing  \ [T i  to  b lock  \[N anamendment \[P to  IN the  " w indfa l l -p ro f i t s  " tax  \ [Fr  that  \[Vwould cost \[N the oll industry N\] \[N $22.5 billion \[P overIN the next decade N\] P\] N\] V\] Fr\] N\] P\] N\] Ti\] V\]A: (S \[N O i l - s ta te  senators  N\] \[V a re  t ry ing  \ [T i  to  b lock  \[N anamendment N\] to  \[N the  N\] Tt \ ]  V\] " \ [St  w indfa l l -p ro f i t s  " \[Ntax  \ [Fr  that  \[V would \[ cos t  \[N the  o i l  indust ry  \[P $22.5IN b i l l i on  \[P over  \[N the  next  decade N\] P\] N\] P\] N\] \] V\]Fr\]  N\] Si\]  S\]It is worth noting that even in the case of the so-called wrong parse there is much to commend theattempt.
I lowever, it is the judgement here that the parse is not sufficiently accurate to place furtherjudgements on, and so it has been classiticd as wrong.Failed ParsesNo parse is found.
This usually occurs because the thresholding technique used to speed up thecalculation by eUminating low probability parses has eliminated all remaining parses.
It is often aresult of incomplete l xical and grammatical knowledge.274
