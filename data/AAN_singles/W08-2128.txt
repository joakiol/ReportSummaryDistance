CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 208?212Manchester, August 2008A Combined Memory-Based Semantic Role Labeler of EnglishRoser Morante, Walter Daelemans, Vincent Van AschCNTS - Language Technology GroupUniversity of AntwerpPrinsstraat 13, B-2000 Antwerpen, Belgium{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.beAbstractWe describe the system submitted tothe closed challenge of the CoNLL-2008shared task on joint parsing of syntacticand semantic dependencies.
Syntactic de-pendencies are processed with the Malt-Parser 0.4.
Semantic dependencies areprocessed with a combination of memory-based classifiers.
The system achieves78.43 labeled macro F1 for the completeproblem, 86.07 labeled attachment scorefor syntactic dependencies, and 70.51 la-beled F1 for semantic dependencies.1 IntroductionIn this paper we describe the system submitted tothe closed challenge of the CoNLL-2008 sharedtask on joint parsing of syntactic and semantic de-pendencies (Surdeanu et al, 2008).
Compared tothe previous shared tasks on semantic role label-ing, the innovative feature of this one is that itconsists of extracting both syntactic and seman-tic dependencies.
The semantic dependencies taskcomprises labeling the semantic roles of nouns andverbs and disambiguating the frame of predicates.The system that we present extracts syntacticand semantic dependencies independently.
Syn-tactic dependencies are processed with the Malt-Parser 0.4 (Nivre, 2006; Nivre et al, 2007).
Se-mantic dependencies are processed with a combi-nation of memory-based classifiers.Memory-based language processing (Daele-mans and van den Bosch, 2005) is based on thec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.idea that NLP problems can be solved by stor-ing solved examples of the problem in their literalform in memory, and applying similarity-basedreasoning on these examples in order to solve newones.
Keeping literal forms in memory has beenargued to provide a key advantage over abstractingmethods in NLP that ignore exceptions and sub-regularities (Daelemans et al, 1999).Memory-based algorithms have been previouslyapplied to semantic role labeling.
Van denBosch et al (2004) participated in the CoNLL-2004 shared task with a system that extendedthe basic memory-based learning method withclass n-grams, iterative classifier stacking, andautomatic output post-processing.
Tjong KimSang et al (2005) participated in the CoNLL-2005 shared task with a system that incorporatesspelling error correction techniques.
Morante andBusser (2007) participated in the SemEval-2007competition with a semantic role labeler for Span-ish based on gold standard constituent syntax.These systems use different types of constituentsyntax (shallow parsing, full parsing).
We areaware of two systems that perform semantic rolelabeling based on dependency syntax previous tothe CoNLL-2008 shared task.
Hacioglu (2004)converts the data from the CoNLL-2004 sharedtask into dependency trees and uses support vectormachines.
Morante (2008) describes a memory-based semantic role labeling system for Spanishbased on gold standard dependency syntax.We developed a memory-based system for theCoNLL-2008 shared task in order to evaluate theperformance of this methodology in a completelynew semantic role labeling setting.The paper is organised as follows.
In Section 2the system is described, Section 3 contains an anal-ysis of the results, and Section 4 puts forward some208conclusions.2 System descriptionThe system processes syntactic and semantic de-pendencies independently.
The syntactic depen-dencies are processed with the MaltParser 0.4.
Thesemantic dependencies are processed with a cas-cade of memory-based classifiers.
We use theIB1 classifier as implemented in TiMBL (version6.1.2) (Daelemans et al, 2007), a supervised in-ductive algorithm for learning classification tasksbased on the k-nearest neighbor classification rule(Cover and Hart, 1967).
In IB1, similarity is de-fined by computing (weighted) overlap of the fea-ture values of a test instance and a memorized ex-ample.
The metric combines a per-feature valuedistance metric with global feature weights thataccount for relative differences in discriminativepower of the features.2.1 Syntactic dependenciesThe MaltParser 0.41(Nivre, 2006; Nivre et al,2007) is an inductive dependency parser that usesfour essential components: a deterministic algo-rithm for building labeled projective dependencygraphs; history-based feature models for predict-ing the next parser action; support vector ma-chines for mapping histories to parser actions;and graph transformations for recovering non-projective structures.The learner type used was support vector ma-chines, with the same parameter options re-ported by (Nivre et al, 2006).
The parseralgorithm used was Nivre, with the optionsand model (eng.par) for English as specifiedon http://w3.msi.vxu.se/users/jha/conll07/.
Thetagset.pos, tagset.cpos and tagset.dep were ex-tracted from the training corpus.2.2 Semantic dependenciesThe semantics task consists of finding the predi-cates, assigning a PropBank or a NomBank frameto them and extracting their semantic role depen-dencies.
Because of lack of resources, we did nothave time to develop a word sense disambiguationsystem.
So, predicates were assigned the frame?.01?
by default.The system handles the semantic role labelingtask in three steps: predicate identification, seman-1Web page of MaltParser 0.4:http://w3.msi.vxu.se/?nivre/research/MaltParser.html.tic dependency classification, and combination ofclassifiers.2.2.1 Predicate identificationIn this phase, a classifier predicts if a word is apredicate or not.
The IB1 algorithm was param-eterised by using overlap as the similarity metric,information gain for feature weighting, using 7 k-nearest neighbors, and weighting the class vote ofneighbors as a function of their inverse linear dis-tance.
The instances represent all nouns and verbsin the corpus and they have the following features:?
Word form, lemma, part of speech (POS), the three lastletters of the word, and the lemma and POS of the fiveprevious and five next words.
To obtain the previousword we perform a linear left-to-right search.
This ishow previous has to be interpreted further on when fea-tures are described.The accuracy of the classifier on the develop-ment test is 0.9599 (4240/4417) for verbs and0.8981 (9226/10272) for nouns.2.2.2 Semantic dependency classificationIn this phase, three groups of multi-class clas-sifiers predict in one step if there is a dependencybetween a word and a predicate, and the type ofdependency, i.e.
semantic role.Group 1 (G1) consists of two classifiers: onefor predicates that are nouns and another for pred-icates that are verbs.
The instances represent apredicate-word combination.
The predicates arethose that have been classified as such in the previ-ous phase.
As for the combining words, determin-ers and certain combinations are excluded basedon the fact that they never have a role in the train-ing corpus.The IB1 algorithm was parameterised by usingoverlap as the similarity metric, information gainfor feature weighting, using 11 k-nearest neigh-bors, and weighting the class vote of neighbors asa function of their inverse linear distance.
The fea-tures of the noun classifier are:?
About the predicate: word form.
About the combiningword: word form, POS, dependency type, word formof the two previous and two next words.
Chain of POStypes between the word and the predicate.
Distance be-tween the word and the predicate.
Binary feature indi-cating if the word depends on the predicate.
Six chainsof POS tags between the word and its three previous andthree next predicates in relation to the current predicate.209The features of the verb classifier are:?
The same as for the noun classifier and additionally:POS of the word next to the current combining word,binary feature indicating if the combining word de-pends on the predicate previous to the current predicate,binary feature indicating if the predicate previous to thecombining word is located before or after the currentpredicate.The verb classifier achieves an overall accuracyof 0.9244 (80805/87412), and the noun classifier,0.9173 (69836/76132) in the development set.Group 2 (G2) consists also of two classifiers:one for predicates that are nouns and another forpredicates that are verbs.
The instances representcombinations of word-predicate, but the test cor-pus contains only those instances that G1 has clas-sified as having a role.The IB1 algorithm was parameterised in thesame way as for G1, except that it computes 7 k-nearest neighbors instead of 11.
The two classifiersuse the same features:?
About the predicate: word form, chain of lemmas of thesyntactic siblings, chain of lemmas of the syntactic chil-dren.
About the combining word: word form, POS, de-pendency type, word form of the two previous and thetwo next words, POS+type of dependency and lemmaof the syntactic father, chain of dependency types andchain of lemmas of the syntactic children.
Chain ofPOS types between word and predicate, distance andsyntactic dependency type between word and predicate.The verb classifier achieves an overall accuracyof 0.5656 (4160/7355), and the noun classifier,0.5017 (2234/4452) in the development set.Group 3 (G3) consists of one classifier.
LikeG2, instances represent combinations of word-predicate, but the test corpus contains only thoseinstances that G1 has classified as having a role..The IB1 algorithm was parameterised in the sameway as for G2.
It uses the following features:About the predicate: lemma, POS, POS of the 3 previousand 3 next predicates.
About the combining word: lemma,POS, and dependency type, POS of the 3 previous and 3 nextwords.
Distance between the predicate and the word.
A bi-nary feature indicating if the combining word is located be-fore or after the predicate.The classifier achieves an overall accuracy of0.5527 (6526/11807).2.2.3 Combination of classifiersIn this phase the three groups of classifiers arecombined in a simple way: if G2 and G3 agreein classifying a semantic dependency, their solu-tion is chosen, else the solution of G1 is chosen.This system combination choice is explained bythe fact that G1 has a higher accuracy than G2 andG3 when the three classifiers are applied to the de-velopment set.
G2 and G3 are used to eliminateovergeneration of roles by G1.The performance of the system in the develop-ment corpus with only the G1 classifiers is 66.07labeled F1.
The combined system achieves a10.8% error reduction, with 69.75 labeled F1.3 ResultsThe results of the system are shown in Table 1.We will focus on commenting on the semanticscores.
The system scores 71.88 labeled F1 in thein-domain corpus (WSJ) and 59.23 in the out-of-domain corpus (Brown).
Unlabeled F1 in the WSJcorpus is almost 10% higher than labeled F1.
La-beled precision is 12.40% higher than labeled re-call.WSJ BROWNSYNTACTIC SCORESLabeled attachment score 86.88 79.58Unlabeled attachment score 89.37 84.85Label accuracy score 91.48 86.00SEMANTIC SCORESLabeled precision 78.61 65.25Labeled recall 66.21 54.23Labeled F1 71.88 59.23Unlabeled precision 89.13 83.61Unlabeled recall 75.08 69.48Unlabeled F1 81.50 75.89OVERALL MACRO SCORESLabeled macro precision 82.74 72.41Labeled macro recall 76.54 66.90Labeled macro F1 79.52 69.55Unlabeled macro precision 89.25 84.23Unlabeled macro recall 82.22 77.16Unlabeled macro F1 85.59 80.54Table 1: Results of the system in the WSJ andBROWN corpora expressed in %.3.1 DiscussionThe performance of the semantic role labeler is af-fected considerably by the performance of the firstclassifier for predicate detection.
The system can-not recover from the predicates that are missed inthis phase.
Experiments without the first classifierand with gold standard predicates (detection andclassification) result in 80.89 labeled F1, 9.01 %210higher than the results of the system with predi-cate detection.
We opted for identifying predicatesas a first step in order to reduce the number oftraining instances for the second phase, classifica-tion of semantic dependencies.
For the same rea-son, we opted for selecting only nouns and verbsas instances, aware of the fact that we would missa very low number of predicates with other cate-gories.
The results of predicate identification canbe improved by setting up a combined system, in-stead of a single classifier, and by incorporating asystem for frame disambiguation.Equally important would be to find better fea-tures for the identification of noun predicates,since the features used generalise better for verbsthan for nouns.
Table 2 shows that the system isbetter at identifying verbs than it is at identifyingnouns.Total F1 Pred.
F1 Pred.Id.&Cl.
Id.CC 3 - -CD 1 - -IN 3 - -JJ 16 - -NN 3635 77.57 85.59NNP 10 30.77 38.46NNS 1648 75.47 83.65PDT 2 - -RP 4 - -VB 1278 79.28 98.87VBD 1320 85.44 99.24VBG 742 77.05 94.41VBN 985 76.43 92.08VBP 343 78.60 97.81VBZ 504 80.94 97.36WP 2 - -WRB 2 - -Table 2: Predicate (Pred.)
identification (Id.)
andclassification (Cl.)
in the WSJ corpus expressed in%.A characteristic of the semantic role labeler isthat recall is considerably lower than precision(12.40 %).
This can be further analysed with thedata shown in Table 3.Except for the dependency VB*+AM-NEG,precision is higher than recall for all semantic de-pendencies.
We run the semantic role labeler withgold standard predicates and with gold standardsyntax and predicates.
The difference between pre-cision and recall is around 10 % in both cases,which confirms that low recall is a characteristicof the semantic role labeler, probably caused bythe fact that the features do not generalise goodenough.
The semantic role labeler with gold stan-Dependency Total Recall Prec.
F1NN*+A0 2339 42.41 77.80 54.90NN*+A1 3757 61.17 78.32 68.69NN*+A2 1537 45.48 82.24 58.57NN*+A3 349 50.14 88.38 63.98NN*+AM-ADV 32 9.38 37.50 15.01NN*+AM-EXT 33 18.18 85.71 30.00NN*+AM-LOC 232 30.60 63.96 41.40NN*+AM-MNR 344 34.59 79.87 48.27NN*+AM-NEG 35 2.86 100.00 5.56NN*+AM-TMP 492 54.88 83.33 66.18VB*+A0 3509 68.99 82.63 75.20VB*+A1 4844 74.24 83.28 78.50VB*+A2 1085 55.94 69.21 61.87VB*+A3 169 41.42 79.55 54.48VB*+A4 99 74.75 88.10 80.88VB*+AM-ADV 488 38.93 59.19 46.97VB*+AM-CAU 70 50.00 70.00 58.33VB*+AM-DIR 81 29.63 57.14 39.02VB*+AM-DIS 315 52.70 74.11 61.60VB*+AM-EXT 32 50.00 59.26 54.24VB*+AM-LOC 355 52.11 57.10 54.49VB*+AM-MNR 335 46.57 61.18 52.88VB*+AM-MOD 539 92.21 95.95 94.04VB*+AM-NEG 227 94.71 90.34 92.47VB*+AM-PNC 113 33.63 54.29 41.53VB*+AM-TMP 1068 64.51 80.40 71.58VB*+C-A1 192 65.10 74.85 69.64VB*+R-A0 222 65.77 87.43 75.07VB*+R-A1 155 49.68 73.33 59.23VB*+R-AM-LOC 21 23.81 71.43 35.71VB*+R-AM-TMP 52 46.15 66.67 54.54Table 3: Semantic dependencies identification andclassification in the WSJ corpus for dependencieswith more than 20 occurences expressed in %.dard predicates scores 86.06 % labeled precisionand 76.32 % labeled recall.
The semantic rolelabeler with gold standard predicates and syntaxscores 89.20 % precision and 79.47 % recall.Table 3 also shows that the unbalance betweenprecision and recall is higher for dependencies ofnouns than for dependencies of verbs, and thatboth recall and precision are higher for dependen-cies from verbs.
Thus, the system performs betterfor verbs than for nouns.
This is in part causedby the fact that more noun predicates than verbpredicates are missed in the predicate identifica-tion phase.
The scores of the the semantic rolelabeler with gold standard predicates show lowerdifferences in F1 between verbs and nouns.The fact that the semantic role labeler performs3.16 % labeled F1 better with gold standard syntax(compared to the system with gold standard syntaxand predicates) confirms that gold standard syntaxprovides useful information to the system.Additionally, the difference in performance be-tween the semantic role labeler presented to the211competition and the semantic role labeler withgold standard predicates (9.01 % labeled F1) sug-gests that, although the results of the system areencouraging, there is room for improvement, andimprovement should focus on increasing the recallscores.4 ConclusionsIn this paper we have presented a system submittedto the closed challenge of the CoNLL-2008 sharedtask on joint parsing of syntactic and semantic de-pendencies.
We have focused on describing thepart of the system that extracts semantic dependen-cies, a combination of memory-based classifiers.The system achieves a semantic score of 71,88 la-beled F1.
Results show that the system is con-siderably affected by the first phase of predicateidentification, that the system is better at extract-ing the semantic dependencies of verbs than thoseof nouns, and that recall is substantially lower thanprecision.
These facts suggest that, although theresults are encouraging, there is room for improve-ment.5 AcknowledgementsThis work was made possible through financialsupport from the University of Antwerp (GOAproject BIOGRAPH), and from the Flemish Insti-tute for the Promotion of Innovation by Scienceand Technology Flanders (IWT) (TETRA projectGRAVITAL).
The experiments were carried out inthe CalcUA computing facilities.
We are gratefulto Stefan Becuwe for his support.ReferencesCover, T. M. and P. E. Hart.
1967.
Nearest neigh-bor pattern classification.
Institute of Electrical andElectronics Engineers Transactions on InformationTheory, 13:21?27.Daelemans, W. and A. van den Bosch.
2005.
Memory-based language processing.
Cambridge UniversityPress, Cambridge, UK.Daelemans, W., A.
Van den Bosch, and J. Zavrel.
1999.Forgetting exceptions is harmful in language learn-ing.
Machine Learning, Special issue on NaturalLanguage Learning, 34:11?41.Daelemans, W., J. Zavrel, K. Van der Sloot, and A. Vanden Bosch.
2007.
TiMBL: Tilburg memory basedlearner, version 6.1, reference guide.
Technical Re-port Series 07-07, ILK, Tilburg, The Netherlands.Hacioglu, K. 2004.
Semantic role labeling using de-pendency trees.
In COLING ?04: Proceedings ofthe 20th international conference on ComputationalLinguistics, Morristown, NJ, USA.
ACL.Morante, R. and B. Busser.
2007.
ILK2: Semanticrole labelling for Catalan and Spanish using TiMBL.In Proceedings of the 4th International Workshop onSemantic Evaluations (SemEval-2007), pages 183?186.Morante, R. 2008.
Semantic role labeling tools trainedon the Cast3LB-CoNLL-SemRol corpus.
In Pro-ceedings of the LREC 2008, Marrakech, Morocco.Nivre, J., J.
Hall, J. Nilsson, G. Eryigit, and S. Marinov.2006.
Labeled pseudo?projective dependency pars-ing with support vector machines.
In Proceedingsof the Tenth Conference on Computational NaturalLanguage Learning, CoNLL-X, New York City, NY,June.Nivre, J., J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
K?ubler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: a language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(2):95?135.Nivre, J.
2006.
Inductive Dependency Parsing.Springer.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008).Tjong Kim Sang, E., S. Canisius, A. van den Bosch,and T. Bogers.
2005.
Applying spelling error cor-rection techniques for improving semantic role la-belling.
In Proceedings of the Ninth Conferenceon Natural Language Learning (CoNLL-2005), AnnArbor, MI.van den Bosch, A., S. Canisius, W. Daelemans, I. Hen-drickx, and E. Tjong Kim Sang.
2004.
Memory-based semantic role labeling: Optimizing features,algorithm, and output.
In Ng, H.T.
and E. Riloff, ed-itors, Proceedings of the Eighth Conference on Com-putational Natural Language Learning (CoNLL-2004), Boston, MA, USA.212
