Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 57?64,Sydney, July 2006. c?2006 Association for Computational LinguisticsA hybrid approach for extracting semantic relations from textsLucia Specia and Enrico MottaKnowledge Media Institute & Centre for Research in ComputingThe Open University, Walton Hall, MK7 6AA, Milton Keynes, UK{L.Specia,E.Motta}@open.ac.ukAbstractWe present an approach for extracting re-lations from texts that exploits linguisticand empirical strategies, by means of apipeline method involving a parser, part-of-speech tagger, named entity recogni-tion system, pattern-based classificationand word sense disambiguation models,and resources such as ontology, knowl-edge base and lexical databases.
The rela-tions extracted can be used for varioustasks, including semantic web annotationand ontology learning.
We suggest thatthe use of knowledge intensive strategiesto process the input text and corpus-based techniques to deal with unpredictedcases and ambiguity problems allows toaccurately discover the relevant relationsbetween pairs of entities in that text.1 IntroductionSemantic relations extracted from texts are usefulfor several applications, including question an-swering, information retrieval, semantic web an-notation, and construction and extension of lexi-cal resources and ontologies.
In this paper wepresent an approach for relation extraction de-veloped to semantically annotate relationalknowledge coming from raw text, within aframework aiming to automatically acquire highquality semantic metadata for the Semantic Web.In that framework, applications such as se-mantic web portals (Lei et al, 2006) analyze datafrom texts, databases, domain ontologies, andknowledge bases in order to extract the semanticknowledge in an integrated way.
Known entitiesoccurring in the text, i.e., entities that are in-cluded in the knowledge base, are semanticallyannotated with their properties, also provided bythe knowledge base and by databases.
New enti-ties, as given by a named entity recognition sys-tem according to the possible types of entities inthe ontology, are annotated without any addi-tional information.
In this context, the goal of therelation extraction approach presented here is toextract relational knowledge about entities, i.e.,to identify the semantic relations between pairsof entities in the input texts.
Entities can be bothknown and new, since named entity recognitionis also carried out.
Relations include those al-ready existent in the knowledge base, new rela-tions predicted as possible by the domain ontol-ogy, or completely new (unpredicted) relations.The approach makes use of a domain ontol-ogy, a knowledge base, and lexical databases,along with knowledge-based and empirical re-sources and strategies for linguistic processing.These include a lemmatizer, syntactic parser,part-of-speech tagger, named entity recognitionsystem, and pattern matching and word sensedisambiguation models.
The input data used inthe experiments with our approach consists ofEnglish texts from the Knowledge Media Insti-tute (KMi)1 newsletters.
We believe that by inte-grating corpus and knowledge-based techniquesand using rich linguistic processing strategies ina completely automated fashion, the approachcan achieve effective results, in terms of bothaccuracy and coverage.With relational knowledge, a richer represen-tation of the input data can be produced.
More-over, by identifying new entities, the relationextraction approach can also be applied to ontol-ogy population.
Finally, since it extracts newrelations, it can also be used as a first step forontology learning.In the remaining of this paper we first describesome cognate work on relation extraction, par-ticularly those exploring empirical methods, forvarious applications (Section 2).
We then present1http://kmi.open.ac.uk/57our approach, showing its architecture and de-scribing each of its main components (Section 3).Finally, we present the next steps (Section 4).2 Related WorkSeveral approaches have been proposed for theextraction of relations from unstructured sources.Recently, they have focused on the use of super-vised or unsupervised corpus-based techniques inorder to automate the task.
A very common ap-proach is based on pattern matching, with pat-terns composed by subject-verb-object (SVO)tuples.
Interesting work has been done on theunsupervised automatic detection of relationsfrom a small number of seed patterns.
These areused as a starting point to bootstrap the patternlearning process, by means of semantic similaritymeasures (Yangarber, 2000; Stevenson, 2004).Most of the approaches for relation extractionrely on the mapping of syntactic dependencies,such as SVO, onto semantic relations, using ei-ther pattern matching or other strategies, such asprobabilistic parsing for trees augmented withannotations for entities and relations (Miller et al2000), or clustering of semantically similar syn-tactic dependencies, according to their selec-tional restrictions (Gamallo et al, 2002).In corpus-based approaches, many variationsare found concerning the machine learning tech-niques used to produce classifiers to judge rela-tion as relevant or non-relevant.
(Roth and Yih,2002), e.g., use probabilistic classifiers with con-straints induced between relations and entities,such as selectional restrictions.
Based on in-stances represented by a pair of entities and theirposition in a shallow parse tree, (Zelenko et al,2003) use support vector machines and votedperceptron algorithms with a specialized kernelmodel.
Also using kernel methods and supportvector machines, (Zhao and Grishman, 2005)combine clues from different levels of syntacticinformation and applies composite kernels tointegrate the individual kernels.Similarly to our proposal, the framework pre-sented by (Iria and Ciravegna, 2005) aims at theautomation of semantic annotations according toontologies.
Several supervised algorithms can beused on the training data represented through acanonical graph-based data model.
The frame-work includes a shallow linguistic processingstep, in which corpora are analyzed and a repre-sentation is produced according to the datamodel, and a classification step, where classifiersrun on the datasets produced by the linguisticprocessing step.Several relation extraction approaches havebeen proposed focusing on the task of ontologylearning (Reinberger and Spyns, 2004; Schutzand Buitelaar, 2005; Ciaramita et al, 2005).More comprehensive reviews can be found in(Maedche, 2002) and (Gomez-Perez and Man-zano-Macho, 2003).
These approaches aim tolearn non-taxonomic relations between concepts,instead of lexical items.
However, in essence,they can employ similar techniques to extract therelations.
Additional strategies can be applied todetermine whether the relations can be liftedfrom lexical items to concepts, as well as to de-termine the most appropriate level of abstractionto describe a relation (e.g.
Maedche, 2002).In the next section we describe our relation ex-traction approach, which merges features thathave shown to be effective in several of the pre-vious works, in order to achieve more compre-hensive and accurate results.3 A hybrid approach for relation ex-tractionThe proposed approach for relation extraction isillustrated in Figure 1.
It employs knowledge-based and (supervised and unsupervised) corpus-based techniques.
The core strategy consists ofmapping linguistic components with some syn-tactic relationship (a linguistic triple) into theircorresponding semantic components.
This in-cludes mapping not only the relations, but alsothe terms linked by those relations.
The detectionof the linguistic triples involves a series of lin-guistic processing steps.
The mapping betweenterms and concepts is guided by a domain ontol-ogy and a named entity recognition system.
Theidentification of the relations relies on theknowledge available in the domain ontology andin a lexical database, and on pattern-based classi-fication and sense disambiguation models.The main goal of this approach is to providerich semantic annotations for the Semantic Web.Other potential applications include:1) Ontology population: terms are mappedinto new instances of concepts of an ontology,and relations between them are identified, ac-cording to the possible relations in that ontology.3) Ontology learning: new relations betweenexistent concepts are identified, and can be usedas a first step to extend an existent ontology.
Asubsequent step to lift relations between in-stances to an adequate level of abstraction maybe necessary.58Figure 1.
Architecture of the proposed approach3.1 Context and resourcesThe input to our experiments consists of elec-tronic Newsletter Texts2.
These are short textsdescribing news of several natures related tomembers of a research group: projects, publica-tions, events, awards, etc.
The domain Ontologyused (KMi-basic-portal-ontology) was designedbased on the AKT reference ontology3 to includeconcepts relevant to our domain.
The instantia-tions of concepts in this ontology are stored inthe knowledge base (KB) KMi-basic-portal-kb.The other two resources used in our architectureare the lexical database WordNet (Fellbaum,1998) and a repository of Patterns of relations,described in Section 3.4.3.2 Identifying linguistic triplesGiven a newsletter text, the first step of the rela-tion extraction approach is to process the naturallanguage text in order to identify linguistic tri-ples, that is, sets of three elements with a syntac-tic relationship, which can indicate potentiallyrelevant semantic relations.
In our architecture,2http://news.kmi.open.ac.uk/kmiplanet/3http://kmi.open.ac.uk/projects/akt/ref-onto/this is accomplished by the Linguistic Compo-nent module, and adaptation of the linguisticcomponent designed in Aqualog (Lopez et al,2005), a question answering system.The linguistic component uses the infrastruc-ture and the following resources from GATE(Cunningham et al, 2002): tokenizer, sentencesplitter, part-of-speech tagger, morphologicalanalyzer and VP chunker.
On the top of theseresources, which produce syntactic annotationsfor the input text, the linguistic component uses agrammar to identify linguistic triples.
Thisgrammar was implemented in Jape (Cunninghamet al, 2000), which allows the definition of pat-terns to recognize regular expressions using theannotations provided by GATE.The main type of construction aimed to beidentified by our grammar involves a verbal ex-pression as indicative of a potential relation andtwo noun phrases as terms linked by that rela-tion.
However, our patterns also account forother types of constructions, including, e.g., theuse of comma to implicitly indicate a relation, asin sentence (1).
In this case, when mapping theterms into entities (Section 3.3), having identi-fied that ?KMi?
is an organization and ?EnricoESpotter++ LinguisticComponentyesyesPattern-basedclassificationPOS +LemmatizerWSDmoduleno, n rela-tionsAnnotate &add to patternsPatternsnoyesRSS_2WordNetOntologyyes no, 0 rela-tionsyesnoWordNet Patternsno, case 1with nrelationsnoyesAnnotate &add to patternsPatternsyesnoRSS_1LinguistictripleNewsletter TextsCase (1) Case (2)Case (3)Typesidentified1 relationmatchesOntology+ KBClassificationnoDisambiguatedWordNet59Motta?
is a person, it is possible to guess the re-lation indicated by the comma (e.g., work).
Someexamples triples identified by our patterns for thenewsletter in Figure 2 are given in Figure 3.
(1) ?Enrico Motta, at KMi now, is leading aproject on ?.
?.Figure 2.
Example of newsletterFigure 3.
Examples of linguistic triples for thenewsletter in Figure 2Jape patterns are based on shallow syntactic in-formation only, and therefore they are not able tocapture certain potentially relevant triples.
Toovercome this limitation, we employ a parser asa complementary resource to produce linguistictriples.
We use Minipar (Lin, 1993), which pro-duces functional relations for the components ina sentence, including subject and object relationswith respect to a verb.
This allows capturingsome implicit relations, such as indirect objectsand long distance dependence relations.Minipar?s representation is converted into atriple format and therefore the intermediate rep-resentation provided by both GATE and Miniparconsists of triples of the type: <noun_phrase,verbal_expression, noun_phrase>.3.3 Identifying entities and relationsGiven a linguistic triple, the next step is to verifywhether the verbal expression in that triple con-veys a relevant semantic relationship betweenentities (given by the terms) potentially belong-ing to an ontology.
This is the most importantphase of our approach and is represented by aseries of modules in our architecture in Figure 1.As first step we try to map the linguistic tripleinto an ontology triple, by using an adaptation ofAqualog?s Relation Similarity Service (RSS).RSS tries to make sense of the linguistic tripleby looking at the structure of the domain ontol-ogy and the information stored in the KB.
In or-der to map a linguistic triple into an ontologytriple, besides looking for an exact matching be-tween the components of the two triples, RSSconsiders partial matching by using a set of re-sources in order to account for minor lexical orconceptual discrepancies between these two ele-ments.
These resources include metrics for stringsimilarity matching, synonym relations given byWordNet, and a lexicon of previous mappingsbetween the two types of triples.
Different strate-gies are employed to identify a matching forterms and relations, as we describe below.Since we do not consider any interaction withthe user in order to achieve a fully automatedannotation process, other modules were devel-oped to complete the mapping process even ifthere is no matching (Section 3.4) or if there isambiguity (Section 3.5), according to RSS.Strategies for mapping termsTo map terms into entities, the following at-tempts are accomplished (in the given order):1) Search the KB for an exact matching of theterm with any instance.2) Apply string similarity metrics4 to calculatethe similarity between the given term and eachinstance of the KB.
A hybrid scheme combiningthree metrics is used: jaro-Winkler, jlevelDis-tance a wlevelDistance.
Different combinationsof threshold values for the metrics are consid-ered.
The elements in the linguistic triples arelemmatized in order to avoid problems whichcould be incorrectly handled by the string simi-larity metrics (e.g., past tense).2.1) If there is more that one possible match-ing, check whether any of them is a substringof the term.
For example, the instance namefor ?Enrico Motta?
is a substring of the term?Motta?, and thus it should be preferred.For example, the similarity values returned forthe term ?vanessa?
with instances potentiallyrelevant for the mapping are given in Figure 4.The combination of thresholds is met for the in-stance ?Vanessa Lopez?, and thus the mapping isaccomplished.
If there is still more than one pos-sible mapping, we assume there is not enoughevidence to map that term and discard the triple.Figure 4.
String similarity measures for the term?vanessa?
and the instance ?Vanessa Lopez?4http://sourceforge.net/projects/simmetrics/Nobel Summit on ICT and public servicesPeter Scott attended the Public Services Summit in Stock-holm, during Nobel Week 2005.
The theme this year wasResponsive Citizen Centered Public Services.
The eventwas hosted by the City of Stockholm and Cisco SystemsThursday 8 December - Sunday 11 December 2005.?<peter-scott,attend,public-services-summit><public-services-summit,located,stockholm><theme,is,responsive-citizen-centered-public-services><city-of-stockholm-and-cisco-systems,host,event>jaroDistance for ?vanessa?
and ?vanessa-lopez?
=0.8461538461538461wlevel for ?vanessa?
and ?vanessa-lopez?
= 1.0jWinklerDistance for ?vanessa?
and ?vanessa-lopez?
= 0.907692307692307760Strategies for mapping relationsIn order to map the verbal expression into a con-ceptual relation, we assume that the terms of thetriple have already been mapped either into in-stances of classes in the KB by RSS, or into po-tential new instances, by a named entity recogni-tion system (as we explain in the next section).The following attempts are then made for theverb-relation mapping:1) Search the KB for an exact matching of theverbal expression with any existent relation forthe instances under consideration or any possiblerelation between the classes (and superclasses) ofthe instances under consideration.2) Apply the string similarity metrics to calcu-late the similarity between the given verbal ex-pression and the possible relations between in-stances (or their classes) corresponding to theterms in the linguistic triple.3) Search for similar mappings for thetypes/classes of entities under consideration in alexicon of mappings automatically created ac-cording to users?
choices in the question answer-ing system Aqualog.
This lexicon contains on-tology triples along with the original verbal ex-pression, as illustrated in Table 1.
The use of thislexicon represents a simplified form of patternmatching in which only exact matching is con-sidered.given_relation class_1 conceptual relation class_2works project has-project-member personcite project has-publication publicationTable 1.
Examples of lexicon patterns4) Search for synonyms of the given verbalexpression in WordNet, in order to verify if thereis a synonym that matches (complete or partially,using string similarity metrics) any existent rela-tion for the instances under consideration, or anypossible relation between the classes (or super-classes) of those instances (likewise in step 1).If there is no possible mapping for the term,the pattern-based classification model is trig-gered (Section 3.4).
Conversely, if there is morethan one possible mapping, the disambiguationmodel is called (Section 3.5).The application of these strategies to map thelinguistic triples into existent or new instancesand relations is described in what follows.Applying RSS to map entities and relationsIn our architecture, RSS is represented by mod-ules RSS_1 and RSS_2.
RSS_1 first checks ifthe terms in the linguistic triple are instances of aKB (cf.
strategies for mapping terms).
If theterms can be mapped to instances, it checkswhether the relation given in the triple matchesany already existent relation between for thoseinstances, or, alternatively, if that relationmatches any of the possible relations for theclasses (and superclasses) of the two instances inthe domain ontology (cf.
strategies for mappingrelations).
Three situations may arise from thisattempt to map the linguistic triple into an ontol-ogy triple (Cases (1), (2), and (3) in Fig.
1):Case (1): complete matching with instances ofthe KB and a relation of the KB or ontology,with possibly more than one valid conceptualrelation being identified:<instance1, (conceptual_relation)+, instance2>.Case (2): no matching or partial matchingwith instances of the ontology (the relation is notanalyzed (na) when there is not a matching forinstances):<instance1, na , ?>   or   <?, na, instance2>   or<?, na, ?>Case (3): matching with instances of the KB,but no matching with a relation of the KB or on-tology:<instance1, ?, instance2>If the matching attempt results in Case (1) withonly one conceptual relation, then the triple canbe formalized into a semantic annotation.
Thisyields the annotation of an already existent rela-tion for two instances of the KB, as well as a newrelation for two instances of the KB, althoughthis relation was already predicted in the ontol-ogy as possible between the classes of those in-stances.
The generalization of the produced triplefor classes/types of entities, i.e., <class, concep-tual_relation, class>, is added to the repository ofPatterns.On the other hand, if there is more than onepossible conceptual relation in case (1), the sys-tem tries to find the correct one by means of asense disambiguation model, described in Sec-tion 3.5.
Conversely, if there is no matching forthe relation (Case (3)), the system tries an alter-native strategy: the pattern-based classificationmodel (Section 3.4).
Finally, if there is no com-plete matching of the terms with instances of theKB (Case (2)), it means that the entities can benew to the KB.In order to check if the terms in the linguistictriple express new entities, the system first iden-61tifies to what classes of the ontology they belong.This is accomplished by means of ESpotter++,and extension of the named entity recognitionsystem ESpotter (Zhu et al 2005).ESpotter is based on a mixture of lexicon(gazetteers) and patterns.
We extended ESpotterby including new entities (extracted from othergazetteers), a few relevant new types of entities,and a small set of efficient patterns.
All types ofentities correspond to generic classes of our do-main ontology, including: person, organization,event, publication, location, project, research-area, technology, etc.In our architecture, if ESpotter++ is not able toidentify the types of the entities, the process isaborted and no annotation is produced.
This maybe either because the terms do not have any con-ceptual mapping (for example ?it?
), or becausethe conceptual mapping is not part of our domainontology.
Otherwise, if ESpotter++ succeeds,RSS is triggered again (RSS_2) in order to verifywhether the verbal expression encompasses asemantic relation.
Since at least one of the twoentities is recognized by Espotter++, and there-fore at least one entity is new, it is only possibleto check if the relation matches the possible rela-tions between the classes of the recognized enti-ties (cf.
strategies for mapping relations).If the matching attempt results in only oneconceptual relation, then the triple will be for-malized into a semantic annotation.
This repre-sents the annotation of a new (although pre-dicted) relation and two or at least one new en-tity/instance.
The produced triple of the type<class, conceptual_relation, class> is added tothe repository of Patterns.Again, if there are multiple valid conceptualrelations, the system tries to find the correct oneby means of a disambiguation model (Section3.5).
Conversely, if it there is no matching for therelation, the pattern-based classification model istriggered (Section 3.4).3.4 Identifying new relationsThe process described in Section 3.3 for theidentification of relations accounts only for therelations already predicted as possible in the do-main ontology.
However, we are also interestedin the additional information that can be pro-vided by the text, in the form of new types ofrelations for known or new entities.
In order todiscover these relations, we employ a patternmatching strategy to identify relevant relationsbetween types of terms.The pattern matching strategy has proved to bean efficient way to extract semantic relations, butin general has the drawback of requiring the pos-sible relations to be previously defined.
In orderto overcome this limitation, we employ a Pat-tern-based classification model that can identifysimilar patterns based on a very small initialnumber of patterns.We consider patterns of relations betweentypes of entities, instead of the entities them-selves, since we believe that it would be impos-sible to accurately judge the similarity for thekinds of entities we are addressing (names ofpeople, locations, etc).
Thus, our patterns consistof triples of the type <class, conceptual_relation,class>, which are compared against a given tripleusing its classes (already provided by the linguis-tic component or by ESpotter++) in order to clas-sify relations in that triple as relevant or non-relevant.The classification model is based on the ap-proach presented in (Stevenson, 2004).
It is anunsupervised corpus-based module which takesas examples a small set of relevant SVO patterns,called seed patterns, and uses a WordNet-basedsemantic similarity measure to compare the pat-tern to be classified against the relevant ones.Our initial seed patterns (see examples in Table2) mixes patterns extracted from the lexicon gen-erated by Aqualog?s users (cf.
Section 3.3) and asmall number of manually defined relevant pat-terns.
This set of patterns is expected to be en-riched with new patterns as our system annotatesrelevant relations, since the system adds new tri-ples to the initial set of patterns.class_1 conceptual relation class_2project has-project-member personproject has-publication publicationperson develop technologyperson attend eventTable 2.
Examples of seed patternsLikewise (Stevenson, 2004), we use a semanticsimilarity metric based on the information con-tent of the words in WordNet hierarchy, derivedfrom corpus probabilities.
It scores the similaritybetween two patterns by computing the similarityfor each pair of words in those patterns.
Athreshold of 0.90 for this score was used here toclassify two patterns as similar.
In that case, anew annotation is produced for the input tripleand it is added to the set of patterns.It is important to notice that, although Word-Net is also used in the RSS module, in that case62only synonyms are checked, while here the simi-larity metric explores deeper information inWordNet, considering the meaning (senses) ofthe words.
It is also important to distinguish thesemantic similarity metrics employed here fromthe string metrics used in RSS.
String similaritymetrics simply try to capture minor variations onthe strings representing terms/relations, they donot account for the meaning of those strings.3.5 Disambiguating relationsThe ambiguity arising when more than one pos-sible relation exists for a pair of entities is aproblem neglected in most of the current work onrelation extraction.
In our architecture, when theRSS finds more than one possible relation, wechoose one relation by using the word sense dis-ambiguation (WSD) system SenseLearner (Mi-halcea and Csomai, 2005).SenseLearner is supervised WSD system todisambiguate all open class words in any giventext, after being trained on a small data set, ac-cording to global models for word categories.The current distribution includes two defaultmodels for verbs, which were trained on a corpuscontaining 200,000 content words of journalistictexts tagged with their WordNet senses.
SinceSenseLeaner requires a sense tagged corpus inorder to be trained to specific domains and thereis not such a corpus for our domain, we use oneof the default training models.
This is a contex-tual model that relies on the first word before andafter the verb, and its POS tags.
To disambiguatenew cases, it requires only that the words are an-notated with POS tags.
The use of lemmas of thewords instead of the words yields better results,since the models were generated for lemmas.
Inour architecture, these annotations are producedby the component POS + Lemmatizer.Since the WSD module disambiguates amongWordNet senses, it is employed only after theuse of the WordNet subcomponent by RSS.
Thissubcomponent finds all the synonyms for theverb in a linguistic triple and checks which ofthem matches existent or possible relations forthe terms in that triple.
In some cases, however,there is a matching for more than one synonym.Since in WordNet synonyms usually representdifferent uses of the verb, the WSD module canidentify in which sense the verb is being used inthe sentence, allowing the system to choose oneamong all the matching options.For example, given the linguistic triple <en-rico_motta, head, kmi>, RSS is able to identifythat ?enrico_motta?
is a person, and that ?kmi?
isan organization.
However, it cannot find an ex-act or partial matching (using string metrics), oreven a matching (given by the user lexicon) forthe relation ?head?.
After getting all its syno-nyms in WordNet, RSS verifies that two of themmatch possible relations in the ontology betweena person and an organization: ?direct?
and?lead?.
In this case, the WSD module disam-biguates the sense of ?head?
as ?direct?.3.6 Example of extracted relationsAs an example of the relations that can be ex-tracted in our approach, consider the representa-tion of the entity ?Enrico Motta?
and all the rela-tions involving this entity in Figure 5.
The rela-tions were extracted from the text in Figure 6.Figure 5.
Example of newsletterFigure 6.
Semantic annotations produced for thenews in Figure 5In this case, ?Enrico-Motta?
is an instance ofkmi-academic-staff-member, a subclass of personin the domain ontology.
The mapped relation?works-in?
?knowledge-media-institute?
alreadyexisted in the KB.
The new relations pointed outby our approach are the ones referring to theaward received from the ?European Commis-sion?
(an organization, here), for three projects:?NeOn?, ?XMEDIA?, and ?OK?.4 Conclusions and future workWe presented a hybrid approach for the extrac-tion of semantic relations from text.
It was de-KMi awarded ?4M for Semantic Web ResearchProfessor Enrico Motta and Dr John Domingue of theKnowledge Media Institute have received a set of record-breaking awards totalling ?4m from the European Commis-sion's Framework 6 Information Society Technologies (IST)programme.
This is the largest ever combined award ob-tained by KMi associated with a single funding programme.The awards include three Integrated Projects (IPs) andthree Specific Targeted Research Projects (STREPs) andthey consolidate KMi?s position as one of the leading inter-national research centers in semantic technologies.
Specifi-cally Professor Motta has been awarded:a.. ?1.55M for the project NeOn: Lifecycle Support for Net-worked Ontologiesb.. ?565K for XMEDIA: Knowledge Sharing and Reuseacross Media andc.. ?391K for OK: Openknowledge - Open, coordinatedknowledge sharing architecture.
?
(def-instance Enrico-Motta kmi-academic-staff-member((works-in knowledge-media-institute)(award-from european-commission)(award-for NeOn)(award-for XMEDIA)(award-for OK)))63signed mainly to enrich the annotations producedby a semantic web portal, but can be used forother domains and applications, such as ontologypopulation and development.
Currently we areconcluding the integration of the several modulescomposing our architecture.
We will then carryexperiments with our corpus of newsletters inorder to evaluate the approach.
Subsequently, wewill incorporate the architecture to a semanticweb portal and accomplish an extrinsic evalua-tion in the context of that application.
Since theapproach uses deep linguistic processing andcorpus-based strategies not requiring any manualannotation, we expect it will accurately discovermost of the relevant relations in the text.AcknowledgementThis research was supported by the AdvancedKnowledge Technologies (AKT) project.
AKT isan Interdisciplinary Research Collaboration(IRC), which is sponsored by the UK Engineer-ing and Physical Sciences Research Council un-der grant number GR/N15764/01.ReferencesMassimiliano Ciaramita, Aldo Gangemi, EstherRatsch, Jasmim Saric, Isabel Rojas.
2005.
Unsu-pervised learning of semantic relations betweenconcepts of a molecular biology ontology.
19thIJCAI, pp.
659-664Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE: AFramework and Graphical Development Environ-ment for Robust NLP Tools and Applications.
40thACL Meeting, Philadelphia.Hamish Cunningham, Diana Maynard, and ValentinTablan.
2000.
JAPE: a Java Annotation PatternsEngine.
Tech.
Report CS--00--10, University ofSheffield, Department of Computer Science.Christiane D. Fellbaum (ed).
1998.
Wordnet: An Elec-tronic Lexical Database.
The MIT Press.Pablo Gamallo, Marco Gonzalez, Alexandre Agustini,Gabriel Lopes, and Vera S. de Lima.
2002.
Map-ping syntactic dependencies onto semantic rela-tions.
ECAI Workshop on Machine Learning andNatural Language Processing for Ontology Engi-neering, Lyon, France.Asuncion Gomez-Perez and David Manzano-Macho.2003.
A Survey of Ontology Learning Methods andTechniques.
Deliverable 1.5, OntoWeb Project.Jose Iria and Fabio Ciravegna.
2005.
Relation Extrac-tion for Mining the Semantic Web.
Dagstuhl Semi-nar on Machine Learning for the Semantic Web,Dagstuhl, Germany.Yuangui Lei, Marta Sabou, Vanessa Lopez, JianhanZhu, Victoria Uren, and Enrico Motta.
2006.
Aninfrastructure for Acquiring High Quality SemanticMetadata.
To appear in the 3rd ESWC, Budva.Dekang Lin.
1993.
Principle based parsing withoutovergeneration.
31st ACL, Columbus, pp.
112-120.Vanessa Lopez, Michele Pasin, and Enrico Motta.2005.
AquaLog: An Ontology-portable QuestionAnswering System for the Semantic Web.
2ndESWC, Creete, Grece.Alexander D. Maedche.
2002.
Ontology Learning forthe Semantic Web, Kluwer Academic Publishers,Norwell, MA.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statistical pars-ing to extract information from text.
6th ANLP-NAACL, Seattle, pp.
226-233.Rada Mihalcea and Andras Csomai.
2005.
Sense-Learner: Word Sense Disambiguation for AllWords in Unrestricted Text.
43rd ACL Meeting,Ann Arbor.Marie-Laure Reinberger and Peter Spyns.
2004.
Dis-covering knowledge in texts for the learning ofDOGMA inspired ontologies.
ECAI 2004 Work-shop on Ontology Learning and Population, Va-lencia, pp.
19-24.Dan Roth and Wen-tau Yih.
2002.
Probabilistic rea-soning for entity & relation recognition.
19th COL-ING, Taipei, Taiwan, pp.
1-7.Alexander Schutz and Paul Buitelaar.
2005.
RelExt: ATool for Relation Extraction from Text in OntologyExtension.
4th ISWC, pp.
593-606.Mark Stevenson.
2004.
An Unsupervised WordNet-based Algorithm for Relation Extraction.
4th LRECWorkshop Beyond Named Entity: Semantic Label-ing for NLP Tasks, Lisbon.Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-ardella.
2003.
Kernel Methods for Relation Extrac-tion.
Journal of Machine Learning Research,(3):1083-1106.Shubin Zhao and Ralph Grishman.
2005.
ExtractingRelations with Integrated Information Using Ker-nel Methods.
43d ACL Meeting, Ann Arbor.Jianhan Zhu, Victoria Uren, and Enrico Motta.
2005.ESpotter: Adaptive Named Entity Recognition forWeb Browsing.
3rd Conf.
on Professional Knowl-edge Management, Kaiserslautern, pp.
518-529.Roman Yangarber, Ralph Grishman and Pasi Tapana-inen, P. 2000.
Unsupervised Discovery of Sce-nario-Level Patterns for Information Extraction.6th ANLP, pp.
282-289.64
