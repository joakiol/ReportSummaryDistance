Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349?353,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSentence Compression with Semantic Role ConstraintsKatsumasa YoshikawaPrecision and Intelligence Laboratory,Tokyo Institute of Technology, JapanIBM Research-Tokyo, IBM Japan, Ltd.katsumasay@gmail.comRyu IidaDepartment of Computer Science,Tokyo Institute of Technology, Japanryu-i@cl.cs.titech.ac.jpTsutomu HiraoNTT Communication Science Laboratories,NTT Corporation, Japanhirao.tsutomu@lab.ntt.co.jpManabu OkumuraPrecision and Intelligence Laboratory,Tokyo Institute of Technology, Japanoku@lr.pi.titech.ac.jpAbstractFor sentence compression, we propose new se-mantic constraints to directly capture the relationsbetween a predicate and its arguments, whereasthe existing approaches have focused on relativelyshallow linguistic properties, such as lexical andsyntactic information.
These constraints are basedon semantic roles and superior to the constraintsof syntactic dependencies.
Our empirical eval-uation on the Written News Compression Cor-pus (Clarke and Lapata, 2008) demonstrates thatour system achieves results comparable to otherstate-of-the-art techniques.1 IntroductionRecent work in document summarization do notonly extract sentences but also compress sentences.Sentence compression enables summarizers to re-duce the redundancy in sentences and generate in-formative summaries beyond the extractive summa-rization systems (Knight and Marcu, 2002).
Con-ventional approaches to sentence compression ex-ploit various linguistic properties based on lexicalinformation and syntactic dependencies (McDonald,2006; Clarke and Lapata, 2008; Cohn and Lapata,2008; Galanis and Androutsopoulos, 2010).In contrast, our approach utilizes another propertybased on semantic roles (SRs) which improves weak-nesses of syntactic dependencies.
Syntactic depen-dencies are not sufficient to compress some complexsentences with coordination, with passive voice, andwith an auxiliary verb.
Figure 1 shows an examplewith a coordination structure.
11This example is from Written News Compression Cor-pus (http://jamesclarke.net/research/resources).Figure 1: Semantic Role vs.
Dependency RelationIn this example, a SR labeler annotated thatHarariis an A0 argument of left and an A1 argument ofbecame.
Harari is syntactically dependent on left ?SBJ(left-2, Harari-1).
However, Harari is not depen-dent on became and we are hence unable to utilize adependency relation between Harari and became di-rectly.
SRs allow us to model the relations betweena predicate and its arguments in a direct fashion.SR constraints are also advantageous in that wecan compress sentences with semantic information.In Figure 1, became has three arguments, Harari asA1, businessman as A2, and shortly afterward asAM-TMP.
As shown in this example, shortly after-word can be omitted (shaded boxes).
In general,modifier arguments like AM-TMP or AM-LOC aremore likely to be reduced than complement caseslike A0-A4.
We can implement such properties bySR constraints.Liu and Gildea (2010) suggests that SR featurescontribute to generating more readable sentence inmachine translation.
We expect that SR features alsohelp our system to improve readability in sentencecompression and summarization.2 Why are Semantic Roles Useful for Com-pressing Sentences?Before describing our system, we show the statis-tics in terms of predicates, arguments and their rela-349Label In Compression / Total RatioA0 1454 / 1607 0.905A1 1916 / 2208 0.868A2 427 / 490 0.871AM-TMP 261 / 488 0.535AM-LOC 134 / 214 0.626AM-ADV 115 / 213 0.544AM-DIS 8 / 85 0.094Table 1: Statistics of Arguments in Compressiontions in the Written News Compression (WNC) Cor-pus.
It has 82 documents (1,629 sentences).
We di-vided them into three: 55 documents are used fortraining (1106 sentences); 10 for development (184sentences); 17 for testing (339 sentences).Our investigation was held in training data.
Thereare 3137 verbal predicates and 7852 unique argu-ments.
We performed SR labeling by LTH (Johans-son and Nugues, 2008), an SR labeler for CoNLL-2008 shared task.
Based on the SR labels annotatedby LTH, we investigated that, for all predicates incompression, how many their arguments were alsoin.
Table 1 shows the survival ratio of main argu-ments in compression.
Labels A0, A1, and A2 arecomplement case roles and over 85% of them survivewith their predicates.
On the other hand, for modifierarguments (AM-X), survival ratios are down to lowerthan 65%.
Our SR constraints implement the differ-ence of survival ratios by SR labels.
Note that de-pendency labels SBJ and OBJ generally correspondto SR labels A0 and A1, respectively.
But their totalnumbers are 777 / 919 (SBJ) and 918 / 1211 (OBJ)and much fewer than A0 and A1 labels.
Thus, SR la-bels can connect much more arguments to their pred-icates.3 ApproachThis section describes our new approach to sen-tence compression.
In order to introduce rich syn-tactic and semantic constraints to a sentence com-pression model, we employ Markov Logic (Richard-son and Domingos, 2006).
Since Markov Logic sup-ports both soft and hard constraints, we can imple-ment our SR constraints in simple and direct fash-ion.
Moreover, implementations of learning andinference methods are already provided in existingMarkov Logic interpreters such as Alchemy 2 andMarkov thebeast.
3 Thus, we can focus our effort2http://alchemy.cs.washington.edu/3http://code.google.com/p/thebeast/on building a set of formulae called Markov LogicNetwork (MLN).
So, in this section, we describe ourproposed MLN in detail.3.1 Proposed Markov Logic NetworkFirst, let us define our MLN predicates.
We sum-marize the MLN predicates in Table 2.
We have onlyone hidden MLN predicate, inComp(i) which mod-els the decision we need to make: whether a token iis in compression or not.
The other MLN predicatesare called observed which provide features.
With ourMLN predicates defined, we can now go on to in-corporate our intuition about the task using weightedfirst-order logic formulae.
We define SR constraintsand the other formulae in Sections 3.1.1 and 3.1.2,respectively.3.1.1 Semantic Role ConstraintsSemantic role labeling generally includes the threesubtasks: predicate identification; argument role la-beling; sense disambiguation.
Our model exploitsthe results of predicate identification and argumentrole labeling.
4 pred(i) and role(i, j, r) indicate theresults of predicate identification and role labeling,respectively.First, the formula describing a local property of apredicate ispred(i) ?
inComp(i) (1)which denotes that, if token i is a predicate then i isin compression.
A formula with exact one hiddenpredicate is called local formula.A predicate is not always in compression.
The for-mula reducing some predicates ispred(i) ?
height(i,+n) ?
?inComp(i) (2)which implies that a predicate i is not in compressionwith n height in a dependency tree.
Note the + nota-tion indicates that the MLN contains one instance ofthe rule, with a separate weight, for each assignmentof the variables with a plus sign.As mentioned earlier, our SR constraints modelthe difference of the survival rate of role labels incompression.
Such SR constraints are encoded as:role(i, j, +r) ?
inComp(i) ?
inComp( j) (3)role(i, j,+r) ?
?inComp(i) ?
?inComp( j) (4)which represent that, if a predicate i is (not) in com-pression, then its argument j is (not) also in with4Sense information is too sparse because the size of theWNC Corpus is not big enough.350predicate definitioninComp(i) Token i is in compressionpred(i) Token i is a predicaterole(i, j, r) Token i has an argument j with role rword(i, w) Token i has word wpos(i, p) Token i has Pos tag pdep(i, j, d) Token i is dependent on token j withdependency label dpath(i, j, l) Tokens i and j has syntactic path lheight(i, n) Token i has height n in dependency treeTable 2: MLN Predicatesrole r. These formulae are called global formulaebecause they have more than two hidden MLN pred-icates.
With global formulae, our model makes twodecisions at a time.
When considering the examplein Figure 1, Formula (3) will be grounded as:role(9, 1, A0) ?
inComp(9) ?
inComp(1) (5)role(9, 7, AM-TMP) ?
inComp(9) ?
inComp(7).
(6)In fact, Formula (5) gains a higher weight than For-mula (6) by learning on training data.
As a re-sult, our system gives ?1-Harari?
more chance tosurvive in compression.
We also add some exten-sions of Formula (3) combined with dep(i, j, +d) andpath(i, j, +l) which enhance SR constraints.
Note, allour SR constraints are ?predicate-driven?
(only ?not ?
as in Formula (13)).
Because an argument isusually related to multiple predicates, it is difficult tomodel ?argument-driven?
formula.3.1.2 Lexical and Syntactic FeaturesFor lexical and syntactic features, we mainly referto the previous work (McDonald, 2006; Clarke andLapata, 2008).
The first two formulae in this sec-tion capture the relation of the tokens with their lexi-cal and syntactic properties.
The formula describingsuch a local property of a word form isword(i,+w) ?
inComp(i) (7)which implies that a token i is in compression with aweight that depends on the word form.For part-of-speech (POS), we add unigram and bi-gram features with the formulae,pos(i, +p) ?
inComp(i) (8)pos(i, +p1) ?
pos(i + 1,+p2) ?
inComp(i).
(9)POS features are often more reasonable than wordform features to combine with the other properties.The formula,pos(i, +p) ?
height(i, +n) ?
inComp(i).
(10)is a combination of POS features and a height in adependency tree.The next formula combines POS bigram featureswith dependency relations.pos(i,+p1) ?
pos( j, +p2) ?dep(i, j,+d) ?
inComp(i).
(11)Moreover, our model includes the followingglobal formulae,dep(i, j,+d) ?
inComp(i) ?
inComp( j) (12)dep(i, j,+d) ?
inComp(i) ?
inComp( j) (13)which enforce the consistencies between head andmodifier tokens.
Formula (12) represents that ifwe include a head token in compression then itsmodifier must also be included.
Formula (13) en-sures that head and modifier words must be simul-taneously kept in compression or dropped.
ThoughClarke and Lapata (2008) implemented these depen-dency constraints by ILP, we implement them bysoft constraints of MLN.
Note that Formula (12) ex-presses the same properties as Formula (3) replacingdep(i, j, +d) by role(i, j,+r).4 Experiment and Result4.1 Experimental SetupOur experimental setting follows previouswork (Clarke and Lapata, 2008).
As stated inSection 2, we employed the WNC Corpus.
Forpreprocessing, we performed POS tagging bystanford-tagger.
5 and dependency parsing byMST-parser (McDonald et al, 2005).
In addition,LTH 6 was exploited to perform both dependencyparsing and SR labeling.
We implemented ourmodel by Markov Thebeast with Gurobi optimizer.
7Our evaluation consists of two types of automaticevaluations.
The first evaluation is dependency basedevaluation same as Riezler et al (2003).
We per-formed dependency parsing on gold data and systemoutputs by RASP.
8 Then we calculated precision, re-call, and F1 for the set of label(head, modi f ier).In order to demonstrate how well our SR con-straints keep correct predicate-argument structuresin compression, we propose SRL based evalua-tion.
We performed SR labeling on gold data5http://nlp.stanford.edu/software/tagger.shtml6http://nlp.cs.lth.se/software/semantic_parsing:_propbank_nombank_frames7http://www.gurobi.com/8http://www.informatics.susx.ac.uk/research/groups/nlp/rasp/351Original [A0 They] [pred say] [A1 the refugees will enhance productivity and economic growth].MLN with SRL [A0 They] [pred say] [A1 the refugees will enhance growth].Gold Standard [A1?
the refugees will enhance productivity and growth].Original [A0 A ?16.1m dam] [AM?MOD will] [pred hold] back [A1 a 2.6-mile-long artificial lake to beknown as the Roadford Reservoir].MLN with SRL [A0 A dam] will [pred hold] back [A1 a artificial lake to be known as the Roadford Reservoir].Gold Standard [A0 A ?16.1m dam] [AM?MOD will] [pred hold back [A1 a 2.6-mile-long Roadford Reservoir].Table 4: Analysis of ErrorsModel CompR F1-Dep F1-SRLMcDonald 73.6% 38.4% 49.9%MLN w/o SRL 68.3% 51.3% 57.2%MLN with SRL 73.1% 58.9% 64.1%Gold Standard 73.3% ?
?Table 3: Results of Sentence Compressionand system outputs by LTH.
Then we calculatedprecision, recall, and F1 value for the set ofrole(predicate, argument).The training time of our MLN model are approx-imately 8 minutes on all training data, with 3.1GHzIntel Core i3 CPU and 4G memory.
While the pre-diction can be done within 20 seconds on the testdata.4.2 ResultsTable 3 shows the results of our compressionmodels by compression rate (CompR), dependency-based F1 (F1-Dep), and SRL-based F1 (F1-SRL).
Inour experiment, we have three models.
McDonaldis a re-implementation of McDonald (2006).
Clarkeand Lapata (2008) also re-implemented McDonald?smodel with an ILP solver and experimented it on theWNC Corpus.
9 MLN with SRL and MLN w/oSRL are our Markov Logic models with and with-out SR Constraints, respectively.Note our three models have no constraint for thelength of compression.
Therefore, we think the com-pression rate of the better system should get closer tothat of human compression.
In comparison betweenMLNmodels and McDonald, the former models out-perform the latter model on both F1-Dep and F1-SRL.
Because MLN models have global constraintsand can generate syntactically correct sentences.Our concern is how a model with SR constraintsis superior to a model without them.
MLN withSRL outperforms MLN without SRL with a 7.6points margin (F1-Dep).
The compression rate ofMLN with SRL goes up to 73.1% and gets close9Clarke?s re-implementation got 60.1% for CompR and36.0%pt for F1-Depto that of gold standard.
SRL-based evaluation alsoshows that SR constraints actually help extract cor-rect predicate-argument structures.
These results arepromising to improve readability.It is difficult to directly compare our results withthose of state-of-the-art systems (Cohn and Lapata,2009; Clarke and Lapata, 2010; Galanis and An-droutsopoulos, 2010) since they have different test-ing sets and the results with different compressionrates.
However, though our MLN model with SRconstraints utilizes no large-scale data, it is the onlymodel which achieves close on 60% in F1-Dep.4.3 Error AnalysisTable 4 indicates two critical examples which ourSR constraints failed to compress correctly.
For thefirst example, our model leaves an argument with itspredicate because our SR constraints are ?predicate-driven?.
In addition, ?say?
is the main verb in thissentence and hard to be deleted due to the syntacticsignificance.The second example in Table 4 requires to iden-tify a coreference relation between artificial lake andRoadford Reservour.
We consider that discourseconstraints (Clarke and Lapata, 2010) help our modelhandle these cases.
Discourse and coreference infor-mation enable our model to select important argu-ments and their predicates.5 ConclusionIn this paper, we proposed new semantic con-straints for sentence compression.
Our model withglobal constraints of semantic roles selected correctpredicate-argument structures and successfully im-proved performance of sentence compression.As future work, we will compare our model withthe other state-of-the-art systems.
We will also inves-tigate the correlation between readability and SRL-based score by manual evaluations.
Furthermore, wewould like to combine discourse constraints with SRconstraints.352ReferencesJames Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31(1):399?429.James Clarke and Mirella Lapata.
2010.
Discourse con-straints for document compression.
ComputationalLinguistics, 36(3):411?441.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofthe 22nd International Conference on ComputationalLinguistics-Volume 1, pages 137?144.
Association forComputational Linguistics.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of Artificial In-telligence Research, 34:637?674.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,HLT ?10, pages 885?893, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic-semantic analysiswith propbank and nombank.
In Proceedings ofthe Twelfth Conference on Computational NaturalLanguage Learning, pages 183?187.
Association forComputational Linguistics.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelligence,139(1):91?107.Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In Proceedings of the23rd International Conference on Computational Lin-guistics (Coling 2010), pages 716?724, Beijing, China,August.
Coling 2010 Organizing Committee.RyanMcDonald, Fernando Pereira, Kiril Ribarov, and JanHajic?.
2005.
Non-projective dependency parsing us-ing spanning tree algorithms.
In Proceedings of theconference on Human Language Technology and Em-pirical Methods in Natural Language Processing, HLT?05, pages 523?530, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceedingsof EACL, pages 297?304.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62(1-2):107?136.Stefan Riezler, Tracy H. King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensationusing ambiguity packing and stochastic disambigua-tion methods for lexical-functional grammar.
In Pro-ceedings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology-Volume 1, pages118?125.
Association for Computational Linguistics.353
