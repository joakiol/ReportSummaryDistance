Proceedings of the EACL 2009 Student Research Workshop, pages 70?78,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsA Generalized Vector Space Model for Text RetrievalBased on Semantic RelatednessGeorge Tsatsaronis and Vicky PanagiotopoulouDepartment of InformaticsAthens University of Economics and Business,76, Patision Str., Athens, Greecegbt@aueb.gr, vpanagiotopoulou@gmail.comAbstractGeneralized Vector Space Models(GVSM) extend the standard VectorSpace Model (VSM) by embedding addi-tional types of information, besides terms,in the representation of documents.
Aninteresting type of information that canbe used in such models is semantic infor-mation from word thesauri like WordNet.Previous attempts to construct GVSMreported contradicting results.
The mostchallenging problem is to incorporate thesemantic information in a theoreticallysound and rigorous manner and to modifythe standard interpretation of the VSM.In this paper we present a new GVSMmodel that exploits WordNet?s semanticinformation.
The model is based on a newmeasure of semantic relatedness betweenterms.
Experimental study conductedin three TREC collections reveals thatsemantic information can boost textretrieval performance with the use of theproposed GVSM.1 IntroductionThe use of semantic information into text retrievalor text classification has been controversial.
Forexample in Mavroeidis et al (2005) it was shownthat a GVSM using WordNet (Fellbaum, 1998)senses and their hypernyms, improves text clas-sification performance, especially for small train-ing sets.
In contrast, Sanderson (1994) reportedthat even 90% accurate WSD cannot guaranteeretrieval improvement, though their experimentalmethodology was based only on randomly gen-erated pseudowords of varying sizes.
Similarly,Voorhees (1993) reported a drop in retrieval per-formance when the retrieval model was based onWSD information.
On the contrary, the construc-tion of a sense-based retrieval model by Stokoeet al (2003) improved performance, while sev-eral years before, Krovetz and Croft (1992) hadalready pointed out that resolving word senses canimprove searches requiring high levels of recall.In this work, we argue that the incorporationof semantic information into a GVSM retrievalmodel can improve performance by consideringthe semantic relatedness between the query anddocument terms.
The proposed model extendsthe traditional VSM with term to term relatednessmeasured with the use of WordNet.
The success ofthe method lies in three important factors, whichalso constitute the points of our contribution: 1) anew measure for computing semantic relatednessbetween terms which takes into account relationweights, and senses?
depth; 2) a new GVSM re-trieval model, which incorporates the aforemen-tioned semantic relatedness measure; 3) exploita-tion of all the semantic information a thesauruscan offer, including semantic relations crossingparts of speech (POS).
Experimental evaluationin three TREC collections shows that the pro-posed model can improve in certain cases theperformance of the standard TF-IDF VSM.
Therest of the paper is organized as follows: Section2 presents preliminary concepts, regarding VSMand GVSM.
Section 3 presents the term seman-tic relatedness measure and the proposed GVSM.Section 4 analyzes the experimental results, andSection 5 concludes and gives pointers to futurework.2 Background2.1 Vector Space ModelThe VSM has been a standard model of represent-ing documents in information retrieval for almostthree decades (Salton and McGill, 1983; Baeza-Yates and Ribeiro-Neto, 1999).
Let D be a docu-ment collection and Q the set of queries represent-ing users?
information needs.
Let alo ti symbol-70ize term i used to index the documents in the col-lection, with i = 1, .., n. The VSM assumes thatfor each term ti there exists a vector ~ti in the vectorspace that represents it.
It then considers the set ofall term vectors {~ti} to be the generating set of thevector space, thus the space basis.
If each dk,(fork = 1, .., p) denotes a document of the collection,then there exists a linear combination of the termvectors {~ti} which represents each dk in the vectorspace.
Similarly, any query q can be modelled asa vector ~q that is a linear combination of the termvectors.In the standard VSM, the term vectors are con-sidered pairwise orthogonal, meaning that they arelinearly independent.
But this assumption is un-realistic, since it enforces lack of relatedness be-tween any pair of terms, whereas the terms in alanguage often relate to each other.
Provided thatthe orthogonality assumption holds, the similaritybetween a document vector ~dk and a query vec-tor ~q in the VSM can be expressed by the cosinemeasure given in equation 1.cos( ~dk, ~q) =?nj=1 akjqj?
?ni=1 a2ki?nj=1 q2j(1)where akj , qj are real numbers standing for theweights of term j in the document dk and thequery q respectively.
A standard baseline retrievalstrategy is to rank the documents according to theircosine similarity to the query.2.2 Generalized Vector Space ModelWong et al (1987) presented an analysis of theproblems that the pairwise orthogonality assump-tion of the VSM creates.
They were the first toaddress these problems by expanding the VSM.They introduced term to term correlations, whichdeprecated the pairwise orthogonality assumption,but they kept the assumption that the term vectorsare linearly independent1, creating the first GVSMmodel.
More specifically, they considered a newspace, where each term vector ~ti was expressed asa linear combination of 2n vectors ~mr, r = 1..2n.The similarity measure between a document and aquery then became as shown in equation 2, where~ti and ~tj are now term vectors in a 2n dimensionalvector space, ~dk, ~q are the document and the query1It is known from Linear Algebra that if every pair of vec-tors in a set of vectors is orthogonal, then this set of vectorsis linearly independent, but not the inverse.vectors, respectively, as before, a?ki, q?j are the newweights, and n?
the new space dimensions.cos( ~dk, ~q) =?n?j=1?n?i=1 a?kiq?j~ti~tj?
?n?i=1 a?ki2?n?j=1 q?j2(2)From equation 2 it follows that the term vectors~ti and ~tj need not be known, as long as the cor-relations between terms ti and tj are known.
Ifone assumes pairwise orthogonality, the similaritymeasure is reduced to that of equation 1.2.3 Semantic Information and GVSMSince the introduction of the first GVSM model,there are at least two basic directions for em-bedding term to term relatedness, other than ex-act keyword matching, into a retrieval model:(a) compute semantic correlations between terms,or (b) compute frequency co-occurrence statisticsfrom large corpora.
In this paper we focus on thefirst direction.
In the past, the effect of WSD infor-mation in text retrieval was studied (Krovetz andCroft, 1992; Sanderson, 1994), with the results re-vealing that under circumstances, senses informa-tion may improve IR.
More specifically, Krovetzand Croft (1992) performed a series of three exper-iments in two document collections, CACM andTIMES.
The results of their experiments showedthat word senses provide a clear distinction be-tween relevant and nonrelevant documents, reject-ing the null hypothesis that the meaning of a wordis not related to judgments of relevance.
Also, theyreached the conclusion that words being worthof disambiguation are either the words with uni-form distribution of senses, or the words that inthe query have a different sense from the mostpopular one.
Sanderson (1994) studied the in-fluence of disambiguation in IR with the use ofpseudowords and he concluded that sense ambi-guity is problematic for IR only in the cases ofretrieving from short queries.
Furthermore, hisfindings regarding the WSD used were that sucha WSD system would help IR if it could performwith very high accuracy, although his experimentswere conducted in the Reuters collection, wherestandard queries with corresponding relevant doc-uments (qrels) are not provided.Since then, several recent approaches haveincorporated semantic information in VSM.Mavroeidis et al (2005) created a GVSM ker-nel based on the use of noun senses, and theirhypernyms from WordNet.
They experimentally71showed that this can improve text categorization.Stokoe et al (Stokoe et al, 2003) reported an im-provement in retrieval performance using a fullysense-based system.
Our approach differs fromthe aforementioned ones in that it expands theVSM model using the semantic information of aword thesaurus to interpret the orthogonality ofterms and to measure semantic relatedness, in-stead of directly replacing terms with senses, oradding senses to the model.3 A GVSM Model based on SemanticRelatedness of TermsSynonymy (many words per sense) and polysemy(many senses per word) are two fundamental prob-lems in text retrieval.
Synonymy is related withrecall, while polysemy with precision.
One stan-dard method to tackle synonymy is the expansionof the query terms with their synonyms.
This in-creases recall, but it can reduce precision dramat-ically.
Both polysemy and synonymy can be cap-tured on the GVSM model in the computation ofthe inner product between ~ti and ~tj in equation 2,as will be explained below.3.1 Semantic RelatednessIn our model, we measure semantic relatedness us-ing WordNet.
It considers the path length, cap-tured by compactness (SCM), and the path depth,captured by semantic path elaboration (SPE),which are defined in the following.
The two mea-sures are combined to for semantic relatedness(SR) beetween two terms.
SR, presented in defini-tion 3, is the basic module of the proposed GVSMmodel.
The adopted method of building seman-tic networks and measuring semantic relatednessfrom a word thesaurus is explained in the next sub-section.Definition 1 Given a word thesaurus O, a weight-ing scheme for the edges that assigns a weight e ?
(0, 1) for each edge, a pair of senses S = (s1, s2),and a path of length l connecting the two senses,the semantic compactness of S (SCM(S, O)) isdefined as ?li=1 ei, where e1, e2, ..., el are thepath?s edges.
If s1 = s2 SCM(S, O) = 1.
If thereis no path between s1 and s2 SCM(S, O) = 0.Note that compactness considers the path lengthand has values in the set [0, 1].
Higher com-pactness between senses declares higher seman-tic relatedness and larger weight are assigned tostronger edge types.
The intuition behind the as-sumption of edges?
weighting is the fact that someedges provide stronger semantic connections thanothers.
In the next subsection we propose a can-didate method of computing weights.
The com-pactness of two senses s1 and s2, can take differ-ent values for all the different paths that connectthe two senses.
All these paths are examined, asexplained later, and the path with the maximumweight is eventually selected (definition 3).
An-other parameter that affects term relatedness is thedepth of the sense nodes comprising the path.
Astandard means of measuring depth in a word the-saurus is the hypernym/hyponym hierarchical re-lation for the noun and adjective POS and hyper-nym/troponym for the verb POS.
A path with shal-low sense nodes is more general compared to apath with deep nodes.
This parameter of seman-tic relatedness between terms is captured by themeasure of semantic path elaboration introducedin the following definition.Definition 2 Given a word thesaurus O and apair of senses S = (s1, s2), where s1,s2 ?
Oand s1 6= s2, and a path between the two sensesof length l, the semantic path elaboration of thepath (SPE(S,O)) is defined as ?li=1 2didi+1di+di+1 ?
1dmax ,where di is the depth of sense si according to O,and dmax the maximum depth of O.
If s1 = s2,and d = d1 = d2, SPE(S, O) = ddmax .
If there isno path from s1 to s2, SPE(S, O) = 0.Essentially, SPE is the harmonic mean of thetwo depths normalized to the maximum thesaurusdepth.
The harmonic mean offers a lower upperbound than the average of depths and we thinkis a more realistic estimation of the path?s depth.SCM and SPE capture the two most importantparameters of measuring semantic relatedness be-tween terms (Budanitsky and Hirst, 2006), namelypath length and senses depth in the used thesaurus.We combine these two measures naturally towardsdefining the Semantic Relatedness between twoterms.Definition 3 Given a word thesaurus O, a pair ofterms T = (t1, t2), and all pairs of senses S =(s1i, s2j), where s1i, s2j senses of t1,t2 respec-tively.
The semantic relatedness of T (SR(T,S,O))is defined as max{SCM(S, O)?SPE(S, O)}.
SRbetween two terms ti, tj where ti ?
tj ?
t andt /?
O is defined as 1.
If ti ?
O but tj /?
O, orti /?
O but tj ?
O, SR is defined as 0.72...S.i.1= Word NodeIndex:= Sense Node= Semantic LinktitjInitial PhaseS.i.7S.j.1S.j.5...S.i.2S.j.1...Network Expansion Example 1Synonym...Hypernym...AntonymHolonymMeronymS.i.2S.j.2HyponymS.i.2S.j.1...Network Expansion Example 2Synonym...HypernymHyponymMeronymHyponymNetwork Expansion Example 3...S.i.1tiS.i.7S.j.1...S.i.2S.j.2DomainS.j.5tje1e2e3S.i.2.1S.i.2.2Figure 1: Computation of semantic relatedness.3.2 Semantic Networks from Word ThesauriIn order to construct a semantic network for a pairof terms t1 and t2 and a combination of their re-spective senses, i.e., s1 and s2, we adopted thenetwork construction method that we introducedin (Tsatsaronis et al, 2007).
This method was pre-ferred against other related methods, like the oneintroduced in (Mihalcea et al, 2004), since it em-beds all the available semantic information exist-ing in WordNet, even edges that cross POS, thusoffering a richer semantic representation.
Accord-ing to the adopted semantic network constructionmodel, each semantic edge type is given a differentweight.
The intuition behind edge types?
weight-ing is that certain types provide stronger semanticconnections than others.
The frequency of occur-rence of the different edge types in Wordnet 2.0, isused to define the edge types?
weights (e.g.
0.57for hypernym/hyponym edges, 0.14 for nominal-ization edges etc.
).Figure 1 shows the construction of a semanticnetwork for two terms ti and tj .
Let the high-lighted senses S.i.2 and S.j.1 be a pair of sensesof ti and tj respectively.
All the semantic linksof the highlighted senses, as found in WordNet,are added as shown in example 1 of figure 1.
Theprocess is repeated recursively until at least onepath between S.i.2 and S.j.1 is found.
It might bethe case that there is no path from S.i.2 to S.j.1.In that case SR((ti, tj), (S.i.2, S.j.1), O) = 0.Suppose that a path is that of example 2, wheree1, e2, e3 are the respective edge weights, d1 is thedepth of S.i.2, d2 the depth of S.i.2.1, d3 the depthof S.i.2.2 and d4 the depth of S.j.1, and dmax themaximum thesaurus depth.
For reasons of sim-plicity, let e1 = e2 = e3 = 0.5, and d1 = 3.Naturally, d2 = 4, and let d3 = d4 = d2 = 4.
Fi-nally, let dmax = 14, which is the case for Word-Net 2.0.
Then, SR((ti, tj), (S.i.2, S.j.1), O) =0.53 ?
0.4615 ?
0.52 = 0.01442.
Example 3 offigure 2 illustrates another possibility where S.i.7and S.j.5 is another examined pair of senses for tiand tj respectively.
In this case, the two senses co-incide, and SR((ti, tj), (S.i.7, S.j.5), O) = 1 ?
d14 ,where d the depth of the sense.
When two sensescoincide, SCM = 1, as mentioned in definition 1,a secondary criterion must be levied to distinguishthe relatedness of senses that match.
This crite-rion in SR is SPE, which assumes that a senseis more specific as we traverse WordNet graphdownwards.
In the specified example, SCM = 1,but SPE = d14 .
This will give a final value to SRthat will be less than 1.
This constitutes an intrin-sic property of SR, which is expressed by SPE.The rationale behind the computation of SPEstems from the fact that word senses in WordNetare organized into synonym sets, named synsets.Moreover, synsets belong to hierarchies (i.e., nounhierarchies developed by the hypernym/hyponymrelations).
Thus, in case two words map into thesame synset (i.e., their senses belong to the samesynset), the computation of their semantic related-ness must additionally take into account the depthof that synset in WordNet.3.3 Computing Maximum SemanticRelatednessIn the expansion of the VSM model we need toweigh the inner product between any two termvectors with their semantic relatedness.
It is obvi-ous that given a word thesaurus, there can be morethan one semantic paths that link two senses.
Inthese cases, we decide to use the path that max-imizes the semantic relatedness (the product ofSCM and SPE).
This computation can be doneaccording to the following algorithm, which is amodification of Dijkstra?s algorithm for findingthe shortest path between two nodes in a weighteddirected graph.
The proof of the algorithm?s cor-rectness follows with theorem 1.Theorem 1 Given a word thesaurus O, a weight-ing function w : E ?
(0, 1), where a higher valuedeclares a stronger edge, and a pair of sensesS(ss, sf ) declaring source (ss) and destination(sf ) vertices, then the SCM(S, O) ?
SPE(S, O)is maximized for the path returned by Algorithm1, by using the weighting scheme eij = wij ?2?di?djdmax?
(di+dj) , where eij the new weight of the edgeconnecting senses si and sj , and wij the initial73Algorithm 1 MaxSR(G,u,v,w)Require: A directed weighted graph G, twonodes u, v and a weighting scheme w : E ?
(0..1).Ensure: The path from u to v with the maximumproduct of the edges weights.Initialize-Single-Source(G,u)1: for all vertices v ?
V [G] do2: d[v] = ?
?3: ?
[v] = NULL4: end for5: d[u] = 1Relax(u, v, w)6: if d[v] < d[u] ?
w(u, v) then7: d[v] = d[u] ?
w(u, v)8: ?
[v] = u9: end ifMaximum-Relatedness(G,u,v,w)10: Initialize-Single-Source(G,u)11: S = ?12: Q = V [G]13: while v ?
Q do14: s = Extract from Q the vertex with max d15: S = S ?
s16: for all vertices k ?
Adjacency List of s do17: Relax(s,k,w)18: end for19: end while20: return the path following all the ancestors ?
ofv back to uweight assigned by weighting function w.Proof 1 For the proof of this theorem we followthe course of thinking of the proof of theorem25.10 in (Cormen et al, 1990).
We shall showthat for each vertex sf ?
V , d[sf ] is the max-imum product of edges?
weight through the se-lected path, starting from ss, at the time whensf is inserted into S. From now on, the nota-tion ?
(ss, sf ) will represent this product.
Pathp connects a vertex in S, namely ss, to a ver-tex in V ?
S, namely sf .
Consider the first ver-tex sy along p such that sy ?
V ?
S and let sxbe y?s predecessor.
Now, path p can be decom-posed as ss ?
sx ?
sy ?
sf .
We claim thatd[sy] = ?
(ss, sy) when sf is inserted into S. Ob-serve that sx ?
S. Then, because sf is chosen asthe first vertex for which d[sf ] 6= ?
(ss, sf ) when itis inserted into S, we had d[sx] = ?
(ss, sx) whensx was inserted into S.We can now obtain a contradiction to theabove to prove the theorem.
Because sy oc-curs before sf on the path from ss to sf and alledge weights are nonnegative2 and in (0, 1) wehave ?
(ss, sy) ?
?
(ss, sf ), and thus d[sy] =?
(ss, sy) ?
?
(ss, sf ) ?
d[sf ].
But both syand sf were in V ?
S when sf was chosen,so we have d[sf ] ?
d[sy].
Thus, d[sy] =?
(ss, sy) = ?
(ss, sf ) = d[sf ].
Consequently,d[sf ] = ?
(ss, sf ) which contradicts our choice ofsf .
We conclude that at the time each vertex sf isinserted into S, d[sf ] = ?
(ss, sf ).Next, to prove that the returned maximumproduct is the SCM(S, O) ?
SPE(S, O), letthe path between ss and sf with the maximumedge weight product have k edges.
Then, Al-gorithm 1 returns the maximum?ki=1 ei(i+1) =ws2 ?
2?ds?d2dmax?
(ds+d2) ?
w23 ?2?d2?d3dmax?
(d2+d3) ?
... ?
wkf ?2?dk?dfdmax?
(dk+df ) =?ki=1 wi(i+1) ?
?ki=12didi+1di+di+1 ?1dmax = SCM(S, O) ?
SPE(S, O).3.4 Word Sense DisambiguationThe reader will have noticed that our model com-putes the SR between two terms ti,tj , based on thepair of senses si,sj of the two terms respectively,which maximizes the product SCM ?
SPE.
Al-ternatively, a WSD algorithm could have disam-biguated the two terms, given the text fragmentswhere the two terms occurred.
Though interesting,this prospect is neither addressed, nor examined inthis work.
Still, it is in our next plans and part ofour future work to embed in our model some ofthe interesting WSD approaches, like knowledge-based (Sinha and Mihalcea, 2007; Brody et al,2006), corpus-based (Mihalcea and Csomai, 2005;McCarthy et al, 2004), or combinations with veryhigh accuracy (Montoyo et al, 2005).3.5 The GVSM ModelIn equation 2, which captures the document-querysimilarity in the GVSM model, the orthogonalitybetween terms ti and tj is expressed by the innerproduct of the respective term vectors ~ti~tj .
Recallthat ~ti and ~tj are in reality unknown.
We estimatetheir inner product by equation 3, where si andsj are the senses of terms ti and tj respectively,maximizing SCM ?
SPE.~ti~tj = SR((ti, tj), (si, sj), O) (3)Since in our model we assume that each term canbe semantically related with any other term, and2The sign of the algorithm is not considered at this step.74SR((ti, tj), O) = SR((tj , ti), O), the new spaceis of n?
(n?1)2 dimensions.
In this space, each di-mension stands for a distinct pair of terms.
Givena document vector ~dk in the VSM TF-IDF space,we define the value in the (i, j) dimension ofthe new document vector space as dk(ti, tj) =(TF ?
IDF (ti, dk) + TF ?
IDF (tj , dk)) ?
~ti~tj .We add the TF-IDF values because any product-based value results to zero, unless both terms arepresent in the document.
The dimensions q(ti, tj)of the query, are computed similarly.
A GVSMmodel aims at being able to retrieve documentsthat not necessarily contain exact matches of thequery terms, and this is its great advantage.
Thisnew space leads to a new GVSM model, which isa natural extension of the standard VSM.
The co-sine similarity between a document dk and a queryq now becomes:cos( ~dk, ~q) =?
ni=1?
nj=i dk(ti, tj) ?
q(ti, tj)??
ni=1?
nj=i dk(ti, tj)2 ???
ni=1?
nj=i q(ti, tj)2(4)where n is the dimension of the VSM TF-IDFspace.4 Experimental EvaluationThe experimental evaluation in this work is two-fold.
First, we test the performance of the seman-tic relatedness measure (SR) for a pair of wordsin three benchmark data sets, namely the Ruben-stein and Goodenough 65 word pairs (Ruben-stein and Goodenough, 1965)(R&G), the Millerand Charles 30 word pairs (Miller and Charles,1991)(M&C), and the 353 similarity data set(Finkelstein et al, 2002).
Second, we evaluatethe performance of the proposed GVSM in threeTREC collections (TREC 1, 4 and 6).4.1 Evaluation of the Semantic RelatednessMeasureFor the evaluation of the proposed semantic re-latedness measure between two terms we experi-mented in three widely used data sets in which hu-man subjects have provided scores of relatednessfor each pair.
A kind of ?gold standard?
rankingof related word pairs (i.e., from the most relatedwords to the most irrelevant) has thus been cre-ated, against which computer programs can testtheir ability on measuring semantic relatedness be-tween words.
We compared our measure againstten known measures of semantic relatedness: (HS)Hirst and St-Onge (1998), (JC) Jiang and Conrath(1997), (LC) Leacock et al (1998), (L) Lin (1998),(R) Resnik (1995), (JS) Jarmasz and Szpakowicz(2003), (GM) Gabrilovich and Markovitch (2007),(F) Finkelstein et al (2002), (HR) ) and (SP)Strube and Ponzetto (2006).
In Table 1 the resultsof SR and the ten compared measures are shown.The reported numbers are the Spearman correla-tion of the measures?
rankings with the gold stan-dard (human judgements).The correlations for the three data sets show thatSR performs better than any other measure of se-mantic relatedness, besides the case of (HR) in theM&C data set.
It surpasses HR though in the R&Gand the 353-C data set.
The latter contains theword pairs of the M&C data set.
To visualize theperformance of our measure in a more comprehen-sible manner, Figure 2 presents for all pairs in theR&G data set, and with increasing order of relat-edness values based on human judgements, the re-spective values of these pairs that SR produces.
Acloser look on Figure 2 reveals that the values pro-duced by SR (right figure) follow a pattern similarto that of the human ratings (left figure).
Note thatthe x-axis in both charts begins from the least re-lated pair of terms, according to humans, and goesup to the most related pair of terms.
The y-axisin the left chart is the respective humans?
ratingfor each pair of terms.
The right figure shows SRfor each pair.
The reader can consult Budanitskyand Hirst (2006) to confirm that all the other mea-sures of semantic relatedness we compare to, donot follow the same pattern as the human ratings,as closely as our measure of relatedness does (lowy values for small x values and high y values forhigh x).
The same pattern applies in the M&C and353-C data sets.4.2 Evaluation of the GVSMFor the evaluation of the proposed GVSM model,we have experimented with three TREC collec-tions 3, namely TREC 1 (TIPSTER disks 1 and2), TREC 4 (TIPSTER disks 2 and 3) and TREC6 (TIPSTER disks 4 and 5).
We selected thoseTREC collections in order to cover as many dif-ferent thematic subjects as possible.
For example,TREC 1 contains documents from the Wall StreetJournal, Associated Press, Federal Register, andabstracts of U.S. department of energy.
TREC 6differs from TREC 1, since it has documents fromFinancial Times, Los Angeles Times and the For-eign Broadcast Information Service.For each TREC, we executed the standard base-3http://trec.nist.gov/75HS JC LC L R JS GM F HR SP SRR&G 0.745 0.709 0.785 0.77 0.748 0.842 0.816 N/A 0.817 0.56 0.861M&C 0.653 0.805 0.748 0.767 0.737 0.832 0.723 N/A 0.904 0.49 0.855353-C N/A N/A 0.34 N/A 0.35 0.55 0.75 0.56 0.552 0.48 0.61Table 1: Correlations of semantic relatedness measures with human judgements.00.511.522.533.5410 20 30 40 50 60 65HumanRatingPair NumberHUMAN RATINGS AGAINST HUMAN RANKINGScorrelation of human pairs ranking and human ratings0.10.20.30.40.50.60.70.80.910 20 30 40 50 60 65SemanticRelatednessPair NumberSEMANTIC RELATEDNESS AGAINST HUMAN RANKINGScorrelation of human pairs ranking and semantic relatednessFigure 2: Correlation between human ratings and SR in the R&G data set.line TF-IDF VSM model for the first 20 topicsof each collection.
Limited resources prohibitedus from executing experiments in the top 1000documents.
To minimize the execution time, wehave indexed all the pairwise semantic related-ness values according to the SR measure, in adatabase, whose size reached 300GB.
Thus, theexecution of the SR itself is really fast, as all pair-wise SR values between WordNet synsets are in-dexed.
For TREC 1, we used topics 51 ?
70, forTREC 4 topics 201 ?
220 and for TREC 6 topics301 ?
320.
From the results of the VSM model,we kept the top-50 retrieved documents.
In orderto evaluate whether the proposed GVSM can aidthe VSM performance, we executed the GVSMin the same retrieved documents.
The interpo-lated precision-recall values in the 11-standard re-call points for these executions are shown in fig-ure 3 (left graphs), for both VSM and GVSM.
Inthe right graphs of figure 3, the differences in in-terpolated precision for the same recall levels aredepicted.
For reasons of simplicity, we have ex-cluded the recall values in the right graphs, abovewhich, both systems had zero precision.
Thus, forTREC 1 in the y-axis we have depicted the differ-ence in the interpolated precision values (%) of theGVSM from the VSM, for the first 4 recall points.For TRECs 4 and 6 we have done the same for thefirst 9 and 8 recall points respectively.As shown in figure 3, the proposed GVSM mayimprove the performance of the TFIDF VSM up to1.93% in TREC 4, 0.99% in TREC 6 and 0.42%in TREC 1.
This small boost in performanceproves that the proposed GVSM model is promis-ing.
There are many aspects though in the GVSMthat we think require further investigation, like forexample the fact that we have not conducted WSDso as to map each document and query term oc-currence into its correct sense, or the fact that theweighting scheme of the edges used in SR gen-erates from the distribution of each edge type inWordNet, while there might be other more sophis-ticated ways to compute edge weights.
We believethat if these, but also more aspects discussed inthe next section, are tackled, the proposed GVSMmay improve more the retrieval performance.5 Future WorkFrom the experimental evaluation we infer thatSR performs very well, and in fact better than allthe tested related measures.
With regards to theGVSM model, experimental evaluation in threeTREC collections has shown that the model ispromising and may boost retrieval performancemore if several details are further investigated andfurther enhancements are made.
Primarily, thecomputation of the maximum semantic related-ness between two terms includes the selection ofthe semantic path between two senses that maxi-mizes SR.
This can be partially unrealistic sincewe are not sure whether these senses are the cor-rect senses of the terms.
To tackle this issue,WSD techniques may be used.
In addition, therole of phrase detection is yet to be explored and7601020304050607080901000  10  20  30  40PrecisionValues(%)Recall Values (%)Precision-Recall Curves TREC 1VSMGVSM-1-0.7-0.30.00.30.71.00  10  20  30PrecisionDifference(%)Recall Values (%)Differences from Interpolated Precision in TREC 1GVSMTFIDF VSM01020304050607080900  10  20  30  40  50  60  70  80PrecisionValues(%)Recall Values (%)Precision-Recall Curves TREC 4VSMGVSM-2-1.5-100.511.52.00  10  20  30  40  50  60  70  80PrecisionDifference(%)Recall Values (%)Differences from Interpolated Precision in TREC 4GVSMTFIDF VSM0102030405060700  10  20  30  40  50  60  70  80PrecisionValues(%)Recall Values (%)Precision-Recall Curves TREC 6VSMGVSM-2-1.5-100.511.52.00  10  20  30  40  50  60  70PrecisionDifference(%)Recall Values (%)Differences from Interpolated Precision in TREC 6GVSMTFIDF VSMFigure 3: Differences (%) from the baseline in interpolated precision.added into the model.
Since we are using a largeknowledge-base (WordNet), we can add a simplemethod to look-up term occurrences in a specifiedwindow and check whether they form a phrase.This would also decrease the ambiguity of the re-spective text fragment, since in WordNet a phraseis usually monosemous.Moreover, there are additional aspects that de-serve further research.
In previously proposedGVSM, like the one proposed by Voorhees (1993),or by Mavroeidis et al (2005), it is suggestedthat semantic information can create an individualspace, leading to a dual representation of each doc-ument, namely, a vector with document?s termsand another with semantic information.
Ratio-nally, the proposed GVSM could act complemen-tary to the standard VSM representation.
Thus, thesimilarity between a query and a document may becomputed by weighting the similarity in the termsspace and the senses?
space.
Finally, we shouldalso examine the perspective of applying the pro-posed measure of semantic relatedness in a queryexpansion technique, similarly to the work of Fang(2008).6 ConclusionsIn this paper we presented a new measure ofsemantic relatedness and expanded the standardVSM to embed the semantic relatedness betweenpairs of terms into a new GVSM model.
Thesemantic relatedness measure takes into accountall of the semantic links offered by WordNet.
Itconsiders WordNet as a graph, weighs edges de-pending on their type and depth and computesthe maximum relatedness between any two nodes,connected via one or more paths.
The com-parison to well known measures gives promis-ing results.
The application of our measure inthe suggested GVSM demonstrates slightly im-proved performance in information retrieval tasks.It is on our next plans to study the influence ofWSD performance on the proposed model.
Fur-thermore, a comparative analysis between the pro-posed GVSM and other semantic network basedmodels will also shed light towards the condi-tions, under which, embedding semantic informa-tion improves text retrieval.77ReferencesR.
Baeza-Yates and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
Addison Wesley.S.
Brody, R. Navigli, and M. Lapata.
2006.
Ensemblemethods for unsupervised wsd.
In Proc.
of COL-ING/ACL 2006, pages 97?104.A.
Budanitsky and G. Hirst.
2006.
Evaluatingwordnet-based measures of lexical semantic related-ness.
Computational Linguistics, 32(1):13?47.T.H.
Cormen, C.E.
Leiserson, and R.L.
Rivest.
1990.Introduction to Algorithms.
The MIT Press.H.
Fang.
2008.
A re-examination of query expansionusing lexical resources.
In Proc.
of ACL 2008, pages139?147.C.
Fellbaum.
1998.
WordNet ?
an electronic lexicaldatabase.
MIT Press.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
ACMTOIS, 20(1):116?131.E.
Gabrilovich and S. Markovitch.
2007.
Computingsemantic relatedness using wikipedia-based explicitsemantic analysis.
In Proc.
of the 20th IJCAI, pages1606?1611.
Hyderabad, India.G.
Hirst and D. St-Onge.
1998.
Lexical chains as rep-resentations of context for the detection and correc-tion of malapropisms.
In WordNet: An ElectronicLexical Database, chapter 13, pages 305?332, Cam-bridge.
The MIT Press.M.
Jarmasz and S. Szpakowicz.
2003.
Roget?s the-saurus and semantic similarity.
In Proc.
of Confer-ence on Recent Advances in Natural Language Pro-cessing, pages 212?219.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProc.
of ROCLING X, pages 19?33.R.
Krovetz and W.B.
Croft.
1992.
Lexical ambigu-ity and information retrieval.
ACM Transactions onInformation Systems, 10(2):115?141.C.
Leacock, G. Miller, and M. Chodorow.
1998.Using corpus statistics and wordnet relations forsense identification.
Computational Linguistics,24(1):147?165, March.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proc.
of the 15th International Con-ference on Machine Learning, pages 296?304.D.
Mavroeidis, G. Tsatsaronis, M. Vazirgiannis,M.
Theobald, and G. Weikum.
2005.
Word sensedisambiguation for exploiting hierarchical thesauriin text classification.
In Proc.
of the 9th PKDD,pages 181?192.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Finding predominant word senses in untaggedtext.
In Proc, of the 42nd ACL, pages 280?287.Spain.R.
Mihalcea and A. Csomai.
2005.
Senselearner:Word sense disambiguation for all words in unre-stricted text.
In Proc.
of the 43rd ACL, pages 53?56.R.
Mihalcea, P. Tarau, and E. Figa.
2004.
Pagerank onsemantic networks with application to word sensedisambiguation.
In Proc.
of the 20th COLING.G.A.
Miller and W.G.
Charles.
1991.
Contextual cor-relates of semantic similarity.
Language and Cogni-tive Processes, 6(1):1?28.A.
Montoyo, A. Suarez, G. Rigau, and M. Palomar.2005.
Combining knowledge- and corpus-basedword-sense-disambiguation methods.
Journal of Ar-tificial Intelligence Research, 23:299?330, March.P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity.
In Proc.
of the 14th IJCAI,pages 448?453, Canada.H.
Rubenstein and J.B. Goodenough.
1965.
Contex-tual correlates of synonymy.
Communications of theACM, 8(10):627?633.G.
Salton and M.J. McGill.
1983.
Introduction toModern Information Retrieval.
McGraw-Hill.M.
Sanderson.
1994.
Word sense disambiguation andinformation retrieval.
In Proc.
of the 17th SIGIR,pages 142?151, Ireland.
ACM.R.
Sinha and R. Mihalcea.
2007.
Unsupervised graph-based word sense disambiguation using measures ofword semantic similarity.
In Proc.
of the IEEE In-ternational Conference on Semantic Computing.C.
Stokoe, M.P.
Oakes, and J. Tait.
2003.
Word sensedisambiguation in information retrieval revisited.
InProc.
of the 26th SIGIR, pages 159?166.M.
Strube and S.P.
Ponzetto.
2006.
Wikirelate!
com-puting semantic relatedness using wikipedia.
InProc.
of the 21st AAAI.G.
Tsatsaronis, M. Vazirgiannis, and I. Androutsopou-los.
2007.
Word sense disambiguation with spread-ing activation networks generated from thesauri.
InProc.
of the 20th IJCAI, pages 1725?1730.E.
Voorhees.
1993.
Using wordnet to disambiguateword sense for text retrieval.
In Proc.
of the 16thSIGIR, pages 171?180.
ACM.S.K.M.
Wong, W. Ziarko, V.V.
Raghavan, and P.C.N.Wong.
1987.
On modeling of information retrievalconcepts in vector spaces.
ACM Transactions onDatabase Systems, 12(2):299?321.78
