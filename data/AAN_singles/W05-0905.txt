Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 33?40, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsEvaluating Automatic Summaries of Meeting RecordingsGabriel MurrayCentre for Speech Technology ResearchUniversity of EdinburghEdinburgh, United KingdomSteve RenalsCentre for Speech Technology ResearchUniversity of EdinburghEdinburgh, United KingdomJean CarlettaHuman Communication Research CentreUniversity of EdinburghEdinburgh, United KingdomJohanna MooreHuman Communication Research CentreUniversity of EdinburghEdinburgh, United KingdomAbstractThe research below explores schemes forevaluating automatic summaries of busi-ness meetings, using the ICSI MeetingCorpus (Janin et al, 2003).
Both au-tomatic and subjective evaluations werecarried out, with a central interest be-ing whether or not the two types of eval-uations correlate with each other.
Theevaluation metrics were used to compareand contrast differing approaches to au-tomatic summarization, the deteriorationof summary quality on ASR output ver-sus manual transcripts, and to determinewhether manual extracts are rated signifi-cantly higher than automatic extracts.1 IntroductionIn the field of automatic summarization, it is widelyagreed upon that more attention needs to be paidto the development of standardized approaches tosummarization evaluation.
For example, the cur-rent incarnation of the Document UnderstandingConference is putting its main focus on the de-velopment of evaluation schemes, including semi-automatic approaches to evaluation.
One semi-automatic approach to evaluation is ROUGE (Linand Hovy, 2003), which is primarily based on n-gram co-occurrence between automatic and humansummaries.
A key question of the research con-tained herein is how well ROUGE correlates withhuman judgments of summaries within the domainof meeting speech.
If it is determined that the twotypes of evaluations correlate strongly, then ROUGEwill likely be a valuable and robust evaluation tool inthe development stage of a summarization system,when the cost of frequent human evaluations wouldbe prohibitive.Three basic approaches to summarization areevaluated and compared below: Maximal MarginalRelevance, Latent Semantic Analysis, and feature-based classification.
The other major comparisonsin this paper are between summaries on ASR ver-sus manual transcripts, and between manual and au-tomatic extracts.
For example, regarding the for-mer, it might be expected that summaries on ASRtranscripts would be rated lower than summaries onmanual transcripts, due to speech recognition errors.Regarding the comparison of manual and automaticextracts, the manual extracts can be thought of asa gold standard for the extraction task, represent-ing the performance ceiling that the automatic ap-proaches are aiming for.More detailed descriptions of the summarizationapproaches and experimental setup can be found in(Murray et al, 2005).
That work relied solely onROUGE as an evaluation metric, and this paper pro-ceeds to investigate whether ROUGE alone is a reli-able metric for our summarization domain, by com-paring the automatic scores with recently-gatheredhuman evaluations.
Also, it should be noted thatwhile we are at the moment only utilizing intrinsicevaluation methods, our ultimate plan is to evalu-ate these meeting summaries extrinsically within thecontext of a meeting browser (Wellner et al, 2005).332 Description of the SummarizationApproaches2.1 Maximal Marginal Relevance (MMR)MMR (Carbonell and Goldstein, 1998) uses thevector-space model of text retrieval and is particu-larly applicable to query-based and multi-documentsummarization.
The MMR algorithm choosessentences via a weighted combination of query-relevance and redundancy scores, both derived usingcosine similarity.
The MMR score ScMMR(i)for agiven sentence Si in the document is given byScMMR(i) =?
(Sim(Si, D))?
(1?
?
)(Sim(Si, Summ)) ,where D is the average document vector, Summis the average vector from the set of sentences al-ready selected, and ?
trades off between relevanceand redundancy.
Sim is the cosine similarity be-tween two documents.This implementation of MMR uses lambda an-nealing so that relevance is emphasized while thesummary is still short and minimizing redundancy isprioritized more highly as the summary lengthens.2.2 Latent Semantic Analysis (LSA)LSA is a vector-space approach which involves pro-jecting the original term-document matrix to a re-duced dimension representation.
It is based on thesingular value decomposition (SVD) of an m ?
nterm-document matrix A, whose elements Aij rep-resent the weighted term frequency of term i in doc-ument j.
In SVD, the term-document matrix is de-composed as follows:A = USV Twhere U is an m?n matrix of left-singular vectors,S is an n ?
n diagonal matrix of singular values,and V is the n ?
n matrix of right-singular vectors.The rows of V T may be regarded as defining top-ics, with the columns representing sentences fromthe document.
Following Gong and Liu (Gong andLiu, 2001), summarization proceeds by choosing,for each row in V T , the sentence with the highestvalue.
This process continues until the desired sum-mary length is reached.Two drawbacks of this method are that dimen-sionality is tied to summary length and that goodsentence candidates may not be chosen if they donot ?win?
in any dimension (Steinberger and Jez?ek,2004).
The authors in (Steinberger and Jez?ek, 2004)found one solution, by extracting a single LSA-based sentence score, with variable dimensionalityreduction.We address the same concerns, following theGong and Liu approach, but rather than extractingthe best sentence for each topic, the n best sentencesare extracted, with n determined by the correspond-ing singular values from matrix S. The number ofsentences in the summary that will come from thefirst topic is determined by the percentage that thelargest singular value represents out of the sum of allsingular values, and so on for each topic.
Thus, di-mensionality reduction is no longer tied to summarylength and more than one sentence per topic can bechosen.
Using this method, the level of dimension-ality reduction is essentially learned from the data.2.3 Feature-Based ApproachesFeature-based classification approaches have beenwidely used in text and speech summarization, withpositive results (Kupiec et al, 1995).
In this workwe combined textual and prosodic features, usingGaussian mixture models for the extracted and non-extracted classes.
The prosodic features were themean and standard deviation of F0, energy, and du-ration, all estimated and normalized at the word-level, then averaged over the utterance.
The two lex-ical features were both TFIDF-based: the averageand the maximum TFIDF score for the utterance.For our second feature-based approach, we de-rived single LSA-based sentence scores (Steinbergerand Jez?ek, 2004) to complement the six features de-scribed above, to determine whether such an LSAsentence score is beneficial in determining sentenceimportance.
We reduced the original term-documentmatrix to 300 dimensions; however, Steinberger andJez?ek found the greatest success in their work by re-ducing to a single dimension (Steinberger, personalcommunication).
The LSA sentence score was ob-tained using:ScLSAi =???
?n?k=1v(i, k)2 ?
?
(k)2 ,34where v(i, k) is the kth element of the ith sentencevector and ?
(k) is the corresponding singular value.3 Experimental SetupWe used human summaries of the ICSI Meeting cor-pus for evaluation and for training the feature-basedapproaches.
An evaluation set of six meetings wasdefined and multiple human summaries were createdfor these meetings, with each test meeting having ei-ther three or four manual summaries.
The remainingmeetings were regarded as training data and a singlehuman summary was created for these.
Our sum-maries were created as follows.Annotators were given access to a graphical userinterface (GUI) for browsing an individual meetingthat included earlier human annotations: an ortho-graphic transcription time-synchronized with the au-dio, and a topic segmentation based on a shallow hi-erarchical decomposition with keyword-based textlabels describing each topic segment.
The annota-tors were told to construct a textual summary of themeeting aimed at someone who is interested in theresearch being carried out, such as a researcher whodoes similar work elsewhere, using four headings:?
general abstract: ?why are they meeting andwhat do they talk about??;?
decisions made by the group;?
progress and achievements;?
problems describedThe annotators were given a 200 word limit for eachheading, and told that there must be text for the gen-eral abstract, but that the other headings may havenull annotations for some meetings.Immediately after authoring a textual summary,annotators were asked to create an extractive sum-mary, using a different GUI.
This GUI showedboth their textual summary and the orthographictranscription, without topic segmentation but withone line per dialogue act based on the pre-existingMRDA coding (Shriberg et al, 2004) (The dialogueact categories themselves were not displayed, justthe segmentation).
Annotators were told to extractdialogue acts that together would convey the infor-mation in the textual summary, and could be used tosupport the correctness of that summary.
They weregiven no specific instructions about the number orpercentage of acts to extract or about redundant dia-logue act.
For each dialogue act extracted, they werethen required in a second pass to choose the sen-tences from the textual summary supported by thedialogue act, creating a many-to-many mapping be-tween the recording and the textual summary.The MMR and LSA approaches are both unsuper-vised and do not require labelled training data.
Forboth feature-based approaches, the GMM classifierswere trained on a subset of the training data repre-senting approximately 20 hours of meetings.We performed summarization using both the hu-man transcripts and speech recognizer output.
Thespeech recognizer output was created using base-line acoustic models created using a training setconsisting of 300 hours of conversational telephonespeech from the Switchboard and Callhome cor-pora.
The resultant models (cross-word triphonestrained on conversational side based cepstral meannormalised PLP features) were then MAP adaptedto the meeting domain using the ICSI corpus (Hainet al, 2005).
A trigram language model was em-ployed.
Fair recognition output for the whole corpuswas obtained by dividing the corpus into four parts,and employing a leave one out procedure (trainingthe acoustic and language models on three parts ofthe corpus and testing on the fourth, rotating to ob-tain recognition results for the full corpus).
Thisresulted in an average word error rate (WER) of29.5%.
Automatic segmentation into dialogue actsor sentence boundaries was not performed: the dia-logue act boundaries for the manual transcripts weremapped on to the speech recognition output.3.1 Description of the Evaluation SchemesA particular interest in our research is how automaticmeasures of informativeness correlate with humanjudgments on the same criteria.
During the devel-opment stage of a summarization system it is notfeasible to employ many hours of manual evalua-tions, and so a critical issue is whether or not soft-ware packages such as ROUGE are able to measureinformativeness in a way that correlates with subjec-tive summarization evaluations.353.1.1 ROUGEGauging informativeness has been the focusof automatic summarization evaluation research.We used the ROUGE evaluation approach (Linand Hovy, 2003), which is based on n-gram co-occurrence between machine summaries and ?ideal?human summaries.
ROUGE is currently the stan-dard objective evaluation measure for the DocumentUnderstanding Conference 1; ROUGE does not as-sume that there is a single ?gold standard?
summary.Instead it operates by matching the target summaryagainst a set of reference summaries.
ROUGE-1through ROUGE-4 are simple n-gram co-occurrencemeasures, which check whether each n-gram in thereference summary is contained in the machine sum-mary.
ROUGE-L and ROUGE-W are measures ofcommon subsequences shared between two sum-maries, with ROUGE-W favoring contiguous com-mon subsequences.
Lin (Lin and Hovy, 2003) hasfound that ROUGE-1 and ROUGE-2 correlate wellwith human judgments.3.1.2 Human EvalautionsThe subjective evaluation portion of our researchutilized 5 judges who had little or no familiarity withthe content of the ICSI meetings.
Each judge eval-uated 10 summaries per meeting, for a total of sixtysummaries.
In order to familiarize themselves witha given meeting, they were provided with a humanabstract of the meeting and the full transcript of themeeting with links to the audio.
The human judgeswere instructed to read the abstract, and to consultthe full transcript and audio as needed, with the en-tire familiarization stage not to exceed 20 minutes.The judges were presented with 12 questions atthe end of each summary, and were instructed thatupon beginning the questionnaire they should not re-consult the summary itself.
6 of the questions re-garded informativeness and 6 involved readabilityand coherence, though our current research concen-trates on the informativeness evaluations.
The eval-uations used a Likert scale based on agreement ordisagreement with statements, such as the followingInformativeness statements:1.
The important points of the meeting are repre-sented in the summary.1http://duc.nist.gov/2.
The summary avoids redundancy.3.
The summary sentences on average seem rele-vant.4.
The relationship between the importance ofeach topic and the amount of summary spacegiven to that topic seems appropriate.5.
The summary is repetitive.6.
The summary contains unnecessary informa-tion.Statements such as 2 and 5 above are measuringthe same impressions, with the polarity of the state-ments merely reversed, in order to better gauge thereliability of the answers.
The readability/coherenceportion consisted of the following statements:1.
It is generally easy to tell whom or what is be-ing referred to in the summary.2.
The summary has good continuity, i.e.
the sen-tences seem to join smoothly from one to an-other.3.
The individual sentences on average are clearand well-formed.4.
The summary seems disjointed.5.
The summary is incoherent.6.
On average, individual sentences are poorlyconstructed.It was not possible in this paper to gauge howresponses to these readability statements correlatewith automatic metrics, for the reason that auto-matic metrics of readability and coherence have notbeen widely discussed in the field of summariza-tion.
Though subjective evaluations of summariesare often divided into informativeness and readabil-ity questions, only automatic metrics of informative-ness have been investigated in-depth by the summa-rization community.
We believe that the develop-ment of automatic metrics for coherence and read-ability should be a high priority for researchers insummarization evaluation and plan on pursuing thisavenue of research.
For example, work on coher-ence in NLG (Lapata, 2003) could potentially in-form summarization evaluation.
Mani (Mani et al,3600.10.20.30.40.50.60.70.81  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2ROUGE-1-MANROUGE-2-MANROUGE-L-MANROUGE-1-ASRROUGE-2-ASRROUGE-L-ASRFigure 1: ROUGE Scores for the Summarization Ap-proaches1999) is one of the few papers to have discussedmeasuring summary readability automatically.4 ResultsThe results of these experiments can be analyzedin various ways: significant differences of ROUGEresults across summarization approaches, deterio-ration of ROUGE results on ASR versus manualtranscripts, significant differences of human eval-uations across summarization approaches, deterio-ration of human evaluations on ASR versus man-ual transcripts, and finally, the correlation betweenROUGE and human evaluations.4.1 ROUGE results across summarizationapproachesAll of the machine summaries were 10% of the orig-inal document length, in terms of the number of di-alogue acts contained.
Of the four approaches tosummarization used herein, the latent semantic anal-ysis method performed the best on every meetingtested for every ROUGE measure with the excep-tion of ROUGE-3 and ROUGE-4.
This approachwas significantly better than either feature-based ap-proach (p<0.05), but was not a significant improve-ment over MMR.
For ROUGE-3 and ROUGE-4,none of the summarization approaches were signifi-cantly different from each other, owing to data spar-sity.
Figure 1 gives the ROUGE-1, ROUGE-2 andROUGE-L results for each of the summarization ap-proaches, on both manual and ASR transcripts.4.1.1 ASR versus ManualThe results of the four summarization approacheson ASR output were much the same, with LSA andMMR being comparable to each other, and each ofthem outperforming the feature-based approaches.On ASR output, LSA again consistently performedthe best.Interestingly, though the LSA approach scoredhigher when using manual transcripts than whenusing ASR transcripts, the difference was small andinsignificant despite the nearly 30% WER of theASR.
All of the summarization approaches showedminimal deterioration when used on ASR outputas compared to manual transcripts, but the LSAapproach seemed particularly resilient, as evidencedby Figure 1.
One reason for the relatively smallimpact of ASR output on summarization results isthat for each of the 6 meetings, the WER of thesummaries was lower than the WER of the meetingas a whole.
Similarly, Valenza et al(Valenza etal., 1999) and Zechner and Waibel (Zechner andWaibel, 2000) both observed that the WER ofextracted summaries was significantly lower thanthe overall WER in the case of broadcast news.
Thetable below demonstrates the discrepancy betweensummary WER and meeting WER for the sixmeetings used in this research.Meeting Summary WER Meeting WERBed004 27.0 35.7Bed009 28.3 39.8Bed016 39.6 49.8Bmr005 23.9 36.1Bmr019 28.0 36.5Bro018 25.9 35.6WER% for Summaries and MeetingsThere was no improvement in the second feature-based approach (adding an LSA sentence score) ascompared with the first feature-based approach.
Thesentence score used here relied on a reduction to 300dimensions, which may not have been ideal for thisdata.The similarity between the MMR and LSA ap-proaches here mirrors Gong and Liu?s findings, giv-ing credence to the claim that LSA maximizes rele-vance and minimizes redundancy, in a different andmore opaque manner then MMR, but with similar37STATEMENT FB1 LSA MMR FB2IMPORT.
POINTS 5.03 4.53 4.67 4.83NO REDUN.
4.33 2.60 3.00 3.77RELEVANT 4.83 4.07 4.33 4.53TOPIC SPACE 4.43 3.83 3.87 4.30REPETITIVE 3.37 4.70 4.60 3.83UNNEC.
INFO.
4.70 6.00 5.83 5.00Table 1: Human Scores for 4 Approaches on ManualTranscriptsresults.
Regardless of whether or not the singularvectors of V T can rightly be thought of as topics orconcepts (a seemingly strong claim), the LSA ap-proach was as successful as the more popular MMRalgorithm.4.2 Human results across summarizationapproachesTable 1 presents average ratings for the six state-ments across four summarization approaches onmanual transcripts.
Interestingly, the first feature-based approach is given the highest marks on eachcriterion.
For statements 2, 5 and 6 FB1 is signif-icantly better than the other approaches.
It is par-ticularly surprising that FB1 would score well onstatement 2, which concerns redundancy, given thatMMR and LSA explicitly aim to reduce redundancywhile the feature-based approaches are merely clas-sifying utterances as relevant or not.
The secondfeature-based approach was not significantly worsethan the first on this score.Considering the difficult task of evaluating ten ex-tractive summaries per meeting, we are quite satis-fied with the consistency of the human judges.
Forexample, statements that were merely reworded ver-sions of other statements were given consistent rat-ings.
It was also the case that, with the exceptionof evaluating the sixth statement, judges were ableto tell that the manual extracts were superior to theautomatic approaches.4.2.1 ASR versus ManualTable 2 presents average ratings for the six state-ments across four summarization approaches onASR transcripts.
The LSA and MMR approachesperformed better in terms of having less deteri-STATEMENT FB1 LSA MMR FB2IMPORT.
POINTS 3.53 4.13 3.73 3.50NO REDUN.
3.40 2.97 2.63 3.57RELEVANT 3.47 3.57 3.00 3.47TOPIC SPACE 3.27 3.33 3.00 3.20REPETITIVE 4.43 4.73 4.70 4.20UNNEC.
INFO.
5.37 6.00 6.00 5.33Table 2: Human Scores for 4 Approaches on ASRTranscripts012345671  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2HUMAN-1-MANHUMAN-1-ASRFigure 2: INFORMATIVENESS-1 Scores for theSummarization Approachesoration of scores when used on ASR output in-stead of manual transcripts.
LSA-ASR was notsignificantly worse than LSA on any of the 6 rat-ings.
MMR-ASR was significantly worse thanMMR on only 3 of the 6.
In contrast, FB1-ASR was significantly worse than FB1 for 5 ofthe 6 approaches, reinforcing the point that MMRand LSA seem to favor extracting utterances withfewer errors.
Figures 2, 3 and 4 depict thehow the ASR and manual approaches affect theINFORMATIVENESS-1, INFORMATIVENESS-4and INFORMATIVENESS-6 ratings, respectively.Note that for Figure 6, a higher score is a worse rat-ing.4.3 ROUGE and Human correlationsAccording to (Lin and Hovy, 2003), ROUGE-1 correlates particularly well with human judg-ments of informativeness.
In the human eval-uation survey discussed here, the first statement(INFORMATIVENESS-1) would be expected tocorrelate most highly with ROUGE-1, as it is ask-38012345671  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2HUMAN-4-MANHUMAN-4-ASRFigure 3: INFORMATIVENESS-4 Scores for theSummarization Approaches33.544.555.566.571  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2HUMAN-6-MANHUMAN-6-ASRFigure 4: INFORMATIVENESS-6 Scores for theSummarization Approachesing whether the summary contains the importantpoints of the meeting.
As could be guessed from thediscussion above, there is no significant correlationbetween ROUGE-1 and human evaluations whenanalyzing only the 4 summarization approacheson manual transcripts.
However, when lookingat the 4 approaches on ASR output, ROUGE-1and INFORMATIVENESS-1 have a moderate andsignificant positive correlation (Spearman?s rho =0.500, p < 0.05).
This correlation on ASR out-put is strong enough that when ROUGE-1 andINFORMATIVENESS-1 scores are tested for corre-lation across all 8 summarization approaches, thereis a significant positive correlation (Spearman?s rho= 0.388, p < 0.05).The other significant correlations for ROUGE-1 across all 8 summarization approaches are withINFORMATIVENESS-2, INFORMATIVENESS-5and INFORMATIVENESS-6.
However, these arenegative correlations.
For example, with regard toINFORMATIVENESS-2, summaries that are ratedas having a high level of redundancy are given highROUGE-1 scores, and summaries with little redun-dancy are given low ROUGE-1 scores.
Similary,with regard to INFORMATIVENESS-6, summariesthat are said to have a great deal of unnecessary in-formation are given high ROUGE-1 scores.
It isdifficult to interpret some of these negative correla-tions, as ROUGE does not measure redundancy andwould not necessarily be expected to correlate withredundancy evaluations.5 DiscussionIn general, ROUGE did not correlate well with thehuman evaluations for this data.
The MMR andLSA approaches were deemed to be significantlybetter than the feature-based approaches accordingto ROUGE, while these findings were reversed ac-cording to the human evaluations.
An area of agree-ment, however, is that the LSA-ASR and MMR-ASR approaches have a small and insignificant de-cline in scores compared with the decline of scoresfor the feature-based approaches.
One of the mostinteresting findings of this research is that MMR andLSA approaches used on ASR tend to select utter-ances with fewer ASR errors.ROUGE has been shown to correlate well withhuman evaluations in DUC, when used on news cor-pora, but the summarization task here ?
using con-versational speech from meetings ?
is quite differentfrom summarizing news articles.
ROUGE may sim-ply be less applicable to this domain.6 Future WorkIt remains to be determined through further ex-perimentation by researchers using various corporawhether or not ROUGE truly correlates well withhuman judgments.
The results presented above aremixed in nature, but do not present ROUGE as beingsufficient in itself to robustly evaluate a summariza-tion system under development.We are also interested in developing automaticmetrics of coherence and readability.
We now havehuman evaluations of these criteria and are ready to39begin testing for correlations between these subjec-tive judgments and potential automatic metrics.7 AcknowledgementsThanks to Thomas Hain and the AMI-ASR groupfor the speech recognition output.
This work waspartly supported by the European Union 6th FWPIST Integrated Project AMI (Augmented Multi-party Interaction, FP6-506811, publication).ReferencesJ.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proc.
ACM SIGIR,pages 335?336.Y.
Gong and X. Liu.
2001.
Generic text summarizationusing relevance measure and latent semantic analysis.In Proc.
ACM SIGIR, pages 19?25.T.
Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,V.
Wan, R. Ordelman, I.Mc.Cowan, J.Vepa, andS.Renals.
2005.
An investigation into transcription ofconference room meetings.
Submitted to Eurospeech.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The ICSI meeting corpus.In Proc.
IEEE ICASSP.J.
Kupiec, J. Pederson, and F. Chen.
1995.
A trainabledocument summarizer.
In ACM SIGIR ?95, pages 68?73.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In ACL, pages 545?552.C.-Y.
Lin and E. H. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProc.
HLT-NAACL.Inderjeet Mani, Barbara Gates, and Eric Bloedorn.
1999.Improving summaries by revising them.
In Proceed-ings of the 37th conference on Association for Compu-tational Linguistics, pages 558?565, Morristown, NJ,USA.
Association for Computational Linguistics.G.
Murray, S. Renals, and J. Carletta.
2005.
Extractivesummarization of meeting recordings.
Submitted toEurospeech.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-vey.
2004.
The ICSI meeting recorder dialog act(MRDA) corpus.
In Proc.
5th SIGdial Workshop onDiscourse and Dialogue, pages 97?100.J.
Steinberger and K. Jez?ek.
2004.
Using latent semanticanalysis in text summarization and summary evalua-tion.
In Proc.
ISIM ?04, pages 93?100.R.
Valenza, T. Robinson, M. Hickey, and R. Tucker.1999.
Summarization of spoken audio through infor-mation extraction.
In Proc.
ESCA Workshop on Ac-cessing Information in Spoken Audio, pages 111?116.Pierre Wellner, Mike Flynn, Simon Tucker, and SteveWhittaker.
2005.
A meeting browser evaluation test.In CHI ?05: CHI ?05 extended abstracts on Humanfactors in computing systems, pages 2021?2024, NewYork, NY, USA.
ACM Press.K.
Zechner and A. Waibel.
2000.
Minimizing word errorrate in textual summaries of spoken language.
In Proc.NAACL-2000.40
