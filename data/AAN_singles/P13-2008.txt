Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 41?46,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsParaphrasing Adaptation for Web Search RankingChenguang Wang?School of EECSPeking Universitywangchenguang@pku.edu.cnNan DuanMicrosoft Research Asiananduan@microsoft.comMing ZhouMicrosoft Research Asiamingzhou@microsoft.comMing ZhangSchool of EECSPeking Universitymzhang@net.pku.edu.cnAbstractMismatch between queries and documentsis a key issue for the web search task.
Inorder to narrow down such mismatch, inthis paper, we present an in-depth inves-tigation on adapting a paraphrasing tech-nique to web search from three aspect-s: a search-oriented paraphrasing mod-el; an NDCG-based parameter optimiza-tion algorithm; an enhanced ranking mod-el leveraging augmented features comput-ed on paraphrases of original queries.
Ex-periments performed on the large scalequery-document data set show that, thesearch performance can be significantlyimproved, with +3.28% and +1.14% ND-CG gains on dev and test sets respectively.1 IntroductionParaphrasing is an NLP technique that generatesalternative expressions to convey the same mean-ing of the input text in different ways.
Researcher-s have made great efforts to improve paraphrasingfrom different perspectives, such as paraphrase ex-traction (Zhao et al, 2007), paraphrase generation(Quirk et al, 2004), model optimization (Zhao etal., 2009) and etc.
But as far as we know, none ofprevious work has explored the impact of using awell designed paraphrasing engine for web searchranking task specifically.In web search, mismatches between queries andtheir relevant documents are usually caused by ex-pressing the same meaning in different natural lan-guage ways.
E.g., X is the author of Y and Y waswritten by X have identical meaning in most cas-es, but they are quite different in literal sense.
Thecapability of paraphrasing is just right to alleviatesuch issues.
Motivated by this, this paper presents?
This work has been done while the author was visitingMicrosoft Research Asia.an in-depth study on adapting paraphrasing to websearch.
First, we propose a search-oriented para-phrasing model, which includes specifically de-signed features for web queries that can enable aparaphrasing engine to learn preferences on dif-ferent paraphrasing strategies.
Second, we opti-mize the parameters of the paraphrasing model ac-cording to the Normalized Discounted CumulativeGain (NDCG) score, by leveraging the minimumerror rate training (MERT) algorithm (Och, 2003).Third, we propose an enhanced ranking model byusing augmented features computed on paraphras-es of original queries.Many query reformulation approaches havebeen proposed to tackle the query-document mis-match issue, which can be generally summarizedas query expansion and query substitution.
Queryexpansion (Baeza-Yates, 1992; Jing and Croft,1994; Lavrenko and Croft, 2001; Cui et al, 2002;Yu et al, 2003; Zhang and Yu, 2006; Craswell andSzummer, 2007; Elsas et al, 2008; Xu et al, 2009)adds new terms extracted from different sources tothe original query directly; while query substitu-tion (Brill and Moore, 2000; Jones et al, 2006;Guo et al, 2008; Wang and Zhai, 2008; Dangand Croft, 2010) uses probabilistic models, suchas graphical models, to predict the sequence ofrewritten query words to form a new query.
Com-paring to these works, our paraphrasing engine al-ters queries in a similar way to statistical machinetranslation, with systematic tuning and decodingcomponents.
Zhao et al (2009) proposes an uni-fied paraphrasing framework that can be adaptedto different applications using different usabilitymodels.
Our work can be seen as an extension a-long this line of research, by carrying out in-depthstudy on adapting paraphrasing to web search.Experiments performed on the large scale dataset show that, by leveraging additional matchingfeatures computed on query paraphrases, signif-icant NDCG gains can be achieved on both dev41(+3.28%) and test (+1.14%) sets.2 Paraphrasing for Web SearchIn this section, we first summarize our paraphraseextraction approaches, and then describe our para-phrasing engine for the web search task from threeaspects, including: 1) a search-oriented paraphras-ing model; 2) an NDCG-based parameter opti-mization algorithm; 3) an enhanced ranking modelwith augmented features that are computed basedon the extra knowledge provided by the paraphrasecandidates of the original queries.2.1 Paraphrase ExtractionParaphrases can be mined from various resources.Given a bilingual corpus, we use Bannard andCallison-Burch (2005)?s pivot-based approach toextract paraphrases.
Given a monolingual cor-pus, Lin and Pantel (2001)?s method is used to ex-tract paraphrases based on distributional hypoth-esis.
Additionally, human annotated data can al-so be used as high-quality paraphrases.
We useMiller (1995)?s approach to extract paraphrasesfrom the synonym dictionary of WordNet.
Wordalignments within each paraphrase pair are gener-ated using GIZA++ (Och and Ney, 2000).2.2 Search-Oriented Paraphrasing ModelSimilar to statistical machine translation (SMT),given an input query Q, our paraphrasing enginegenerates paraphrase candidates1 based on a linearmodel.Q?
= argmaxQ?
?H(Q)P (Q?|Q)= argmaxQ??H(Q)M?m=1?mhm(Q,Q?
)H(Q) is the hypothesis space containing all para-phrase candidates of Q, hm is the mth featurefunction with weight ?m, Q?
denotes one candi-date.
In order to enable our paraphrasing modelto learn the preferences on different paraphrasingstrategies according to the characteristics of webqueries, we design search-oriented features2 basedon word alignments within Q and Q?, which canbe described as follows:1We apply CYK algorithm (Chappelier and Rajman,1998), which is most commonly used in SMT (Chiang,2005), to generating paraphrase candidates.2Similar features have been demonstrated effective in(Jones et al, 2006).
But we use SMT-like model to gener-ate query reformulations.?
Word Addition feature hWADD(Q,Q?
),which is defined as the number of words inthe paraphrase candidate Q?
without beingaligned to any word in the original query Q.?
Word Deletion feature hWDEL(Q,Q?
),which is defined as the number of words inthe original query Q without being alignedto any word in the paraphrase candidate Q?.?
Word Overlap feature hWO(Q,Q?
), which isdefined as the number of word pairs that alignidentical words between Q and Q?.?
Word Alteration feature hWA(Q,Q?
), whichis defined as the number of word pairs thatalign different words between Q and Q?.?
Word Reorder feature hWR(Q,Q?
), which ismodeled by a relative distortion probabilitydistribution, similar to the distortion model in(Koehn et al, 2003).?
Length Difference feature hLD(Q,Q?
),which is defined as |Q?| ?
|Q|.?
Edit Distance feature hED(Q,Q?
), which isdefined as the character-level edit distancebetween Q and Q?.Besides, a set of traditional SMT features(Koehn et al, 2003) are also used in our paraphras-ing model, including translation probability, lex-ical weight, word count, paraphrase rule count3,and language model feature.2.3 NDCG-based Parameter OptimizationWe utilize minimum error rate training (MERT)(Och, 2003) to optimize feature weights of theparaphrasing model according to NDCG.
We de-fine D as the entire document set.
R is a rank-ing model4 that can rank documents in D basedon each input query.
{Qi,DLabeli }Si=1 is a human-labeled development set.
Qi is the ith query andDLabeli ?
D is a subset of documents, in whichthe relevance between Qi and each document islabeled by human annotators.MERT is used to optimize feature weightsof our linear-formed paraphrasing model.
For3Paraphrase rule count is the number of rules that are usedto generate paraphrase candidates.4The ranking model R (Liu et al, 2007) uses matchingfeatures computed based on original queries and documents.42each query Qi in {Qi}Si=1, we first generate N-best paraphrase candidates {Qji}Nj=1, and com-pute NDCG score for each paraphrase based ondocuments ranked by the ranker R and labeleddocuments DLabeli .
We then optimize the featureweights according to the following criterion:?
?M1 = argmin?M1{S?i=1Err(DLabeli , Q?i;?M1 ,R)}The objective of MERT is to find the optimal fea-ture weight vector ?
?M1 that minimizes the error cri-terionErr according to the NDCG scores of top-1paraphrase candidates.The error function Err is defined as:Err(DLabeli , Q?i;?M1 ,R) = 1?N (DLabeli , Q?i,R)where Q?i is the best paraphrase candidate accord-ing to the paraphrasing model based on the weightvector ?M1 , N (DLabeli , Q?i,R) is the NDCG scoreof Q?i computed on the documents ranked byR ofQ?i and labeled document set DLabeli of Qi.
Therelevance rating labeled by human annotators canbe represented by five levels: ?Perfect?, ?Excel-lent?, ?Good?, ?Fair?, and ?Bad?.
When comput-ing NDCG scores, these five levels are commonlymapped to the numerical scores 31, 15, 7, 3, 0 re-spectively.2.4 Enhanced Ranking ModelIn web search, the key objective of the rankingmodel is to rank the retrieved documents based ontheir relevance to a given query.Given a query Q and its retrieved document setD = {DQ}, for each DQ ?
D, we use the fol-lowing ranking model to compute their relevance,which is formulated as a weighted combination ofmatching features:R(Q,DQ) =K?k=1?kFk(Q,DQ)F = {F1, ..., FK} denotes a set of matching fea-tures that measure the matching degrees betweenQ and DQ, Fk(Q,DQ) ?
F is the kth matchingfeature, ?k is its corresponding feature weight.How to learn the weight vector {?k}Kk=1 is a s-tandard learning-to-rank task.
The goal of learningis to find an optimal weight vector {?
?k}Kk=1, suchthat for any two documentsDiQ ?
D andDjQ ?
D,the following condition holds:R(Q,DiQ) > R(Q,DjQ)?
rDiQ > rDjQwhere rDQ denotes a numerical relevance ratinglabeled by human annotators denoting the rele-vance between Q and DQ.As the ultimate goal of improving paraphrasingis to help the search task, we present a straight-forward but effective method to enhance the rank-ing modelR described above, by leveraging para-phrase candidates of the original query as the extraknowledge to compute matching features.Formally, given a query Q and its N -best para-phrase candidates {Q?1, ..., Q?N}, we enrich the o-riginal feature vector F to {F,F1, ...,FN} for Qand DQ, where all features in Fn have the samemeanings as they are in F, however, their featurevalues are computed based onQ?n andDQ, insteadof Q and DQ.
In this way, the paraphrase candi-dates act as hidden variables and expanded match-ing features between queries and documents, mak-ing our ranking model more tunable and flexiblefor web search.3 Experiment3.1 Data and MetricParaphrase pairs are extracted as we described inSection 2.1.
The bilingual corpus includes 5.1Msentence pairs from the NIST 2008 constrainedtrack of Chinese-to-English machine translationtask.
The monolingual corpus includes 16.7Mqueries from the log of a commercial search en-gine.
Human annotated data contains 0.3M syn-onym pairs from WordNet dictionary.
Word align-ments of each paraphrase pair are trained byGIZA++.
The language model is trained basedon a portion of queries, in which the frequency ofeach query is higher than a predefined threshold,5.
The number of paraphrase pairs is 58M.
Theminimum length of paraphrase rule is 1, while themaximum length of paraphrase rule is 5.We randomly select 2, 838 queries from the logof a commercial search engine, each of which at-tached with a set of documents that are annotat-ed with relevance ratings described in Section 2.3.We use the first 1, 419 queries together with theirannotated documents as the development set totune paraphrasing parameters (as we discussed inSection 2.3), and use the rest as the test set.
Theranking model is trained based on the develop-ment set.
NDCG is used as the evaluation metricof the web search task.433.2 Baseline SystemsThe baselines of the paraphrasing and the rankingmodel are described as follows:The paraphrasing baseline is denoted as BL-Para, which only uses traditional SMT featuresdescribed at the end of Section 2.2.
Weights areoptimized by MERT using BLEU (Papineni et al,2002) as the error criterion.
Development data aregenerated based on the English references of NIST2008 constrained track of Chinese-to-English ma-chine translation task.
We use the first referenceas the source, and the rest as its paraphrases.The ranking model baseline (Liu et al, 2007) isdenoted as BL-Rank, which only uses matchingfeatures computed based on original queries anddifferent meta-streams of web pages, includingURL, page title, page body, meta-keywords, meta-description and anchor texts.
The feature function-s we use include unigram/bigram/trigram BM25and original/normalized Perfect-Match.
The rank-ing model is learned based on SVM rank toolkit(Joachims, 2006) with default parameter setting.3.3 Impacts of Search-Oriented FeaturesWe first evaluate the effectiveness of the search-oriented features.
To do so, we add these featuresinto the paraphrasing model baseline, and denote itas BL-Para+SF, whose weights are optimized inthe same way with BL-Para.
The ranking modelbaseline BL-Rank is used to rank the documents.We then compare the NDCG@1 scores of the bestdocuments retrieved using either original query, orquery paraphrases generated by BL-Para and BL-Para+SF respectively, and list comparison resultsin Table 1, where Cand@1 denotes the best para-phrase candidate generated by each paraphrasingmodel.Test SetBL-Para BL-Para+SFOriginal Query Cand@1 Cand@127.28% 26.44% 26.53%Table 1: Impacts of search-oriented features.From Table 1, we can see, even using the bestquery paraphrase, its corresponding NDCG scoreis still lower than the NDCG score of the originalquery.
This performance dropping makes sense,as changing user queries brings the risks of querydrift.
When adding search-oriented features in-to the baseline, the performance changes little, asthese two models are optimized based on BLEUscore only, without considering characteristics ofmismatches in search.3.4 Impacts of Optimization AlgorithmWe then evaluate the impact of our NDCG-basedoptimization method.
We add the optimization al-gorithm described in Section 2.3 into BL-Para+SF,and get a paraphrasing model BL-Para+SF+Opt.The ranking model baseline BL-Rank is used.Similar to the experiment in Table 1, we comparethe NDCG@1 scores of the best documents re-trieved using query paraphrases generated by BL-Para+SF and BL-Para+SF+Opt respectively, withresults shown in Table 2.Test SetBL-Para+SF BL-Para+SF+OptOriginal Query Cand@1 Cand@127.28% 26.53% 27.06%(+0.53%)Table 2: Impacts of NDCG-based optimization.Table 2 indicates that, by leveraging NDCG asthe error criterion for MERT, search-oriented fea-tures benefit more (+0.53% NDCG) in selectingthe best query paraphrase from the whole para-phrasing search space.
The improvement is statis-tically significant (p < 0.001) by t-test (Smuckeret al, 2007).
The quality of the top-1 paraphrasegenerated by BL-Para+SF+Opt is very close to theoriginal query.3.5 Impacts of Enhanced Ranking ModelWe last evaluate the effectiveness of the en-hanced ranking model.
The ranking model base-line BL-Rank only uses original queries to com-pute matching features between queries and docu-ments; while the enhanced ranking model, denot-ed as BL-Rank+Para, uses not only the originalquery but also its top-1 paraphrase candidate gen-erated by BL-Para+SF+Opt to compute augment-ed matching features described in Section 2.4.Dev SetNDCG@1 NDCG@5BL-Rank 25.31% 33.76%BL-Rank+Para 28.59%(+3.28%) 34.25%(+0.49%)Test SetNDCG@1 NDCG@5BL-Rank 27.28% 34.79%BL-Rank+Para 28.42%(+1.14%) 35.68%(+0.89%)Table 3: Impacts of enhanced ranking model.From Table 3, we can see that NDCG@k (k =1, 5) scores of BL-Rank+Para outperforms BL-Rank on both dev and test sets.
T-test shows that44the improvement is statistically significant (p <0.001).
Such end-to-end NDCG improvementscome from the extra knowledge provided by thehidden paraphrases of original queries.
This nar-rows down the query-document mismatch issue toa certain extent.4 Conclusion and Future WorkIn this paper, we present an in-depth study on us-ing paraphrasing for web search, which pays closeattention to various aspects of the application in-cluding choice of model and optimization tech-nique.
In the future, we will compare and com-bine paraphrasing with other query reformulationtechniques, e.g., pseudo-relevance feedback (Yu etal., 2003) and a conditional random field-based ap-proach (Guo et al, 2008).AcknowledgmentsThis work is supported by the National Natu-ral Science Foundation of China (NSFC GrantNo.
61272343) as well as the Doctoral Programof Higher Education of China (FSSP Grant No.20120001110112).ReferencesRicardo A Baeza-Yates.
1992.
Introduction to datastructures and algorithms related to information re-trieval.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of ACL, pages 597?604.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InProceedings of ACL, pages 286?293.Jean-Ce?dric Chappelier and Martin Rajman.
1998.
Ageneralized cyk algorithm for parsing stochastic cfg.In Workshop on Tabulation in Parsing and Deduc-tion, pages 133?137.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL, pages 263?270.Nick Craswell and Martin Szummer.
2007.
Randomwalks on the click graph.
In Proceedings of SIGIR,SIGIR ?07, pages 239?246.Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-YingMa.
2002.
Probabilistic query expansion usingquery logs.
In Proceedings of WWW, pages 325?332.Van Dang and Bruce W. Croft.
2010.
Query reformu-lation using anchor text.
In Proceedings of WSDM,pages 41?50.Jonathan L. Elsas, Jaime Arguello, Jamie Callan, andJaime G. Carbonell.
2008.
Retrieval and feedbackmodels for blog feed search.
In Proceedings of SI-GIR, pages 347?354.Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng.
2008.A unified and discriminative model for query refine-ment.
In Proceedings of SIGIR, SIGIR ?08, pages379?386.Yufeng Jing and W. Bruce Croft.
1994.
An associationthesaurus for information retrieval.
In In RIAO 94Conference Proceedings, pages 146?160.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In Proceedings of KDD, pages 217?226.Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
InProceedings of WWW, pages 387?396.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL, pages 48?54.Victor Lavrenko and W. Bruce Croft.
2001.
Relevancebased language models.
In Proceedings of SIGIR,pages 120?127.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question-answering.
Natural Lan-guage Engineering, pages 343?360.Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, andHang Li.
2007.
Letor: Benchmark dataset for re-search on learning to rank for information retrieval.In Proceedings of SIGIR workshop, pages 3?10.George A Miller.
1995.
Wordnet: a lexical databasefor english.
Communications of the ACM, pages 39?41.Franz Josef Och and Hermann Ney.
2000.
Improved s-tatistical alignment models.
In Proceedings of ACL,pages 440?447.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic e-valuation of machine translation.
In Proceedings ofACL, pages 311?318.Chris Quirk, Chris Brockett, and William Dolan.2004.
Monolingual machine translation for para-phrase generation.
In Proceedings of EMNLP, pages142?149.Mark D Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance testsfor information retrieval evaluation.
In Proceedingsof CIKM, pages 623?632.45Xuanhui Wang and ChengXiang Zhai.
2008.
Miningterm association patterns from search logs for ef-fective query reformulation.
In Proceedings of the17th ACM conference on Information and knowl-edge management, Proceedings of CIKM, pages479?488.Yang Xu, Gareth J.F.
Jones, and Bin Wang.
2009.Query dependent pseudo-relevance feedback basedon wikipedia.
In Proceedings of SIGIR, pages 59?66.Shipeng Yu, Deng Cai, Ji-Rong Wen, and Wei-YingMa.
2003.
Improving pseudo-relevance feedback inweb information retrieval using web page segmenta-tion.
In Proceedings of WWW, pages 11?18.Wei Zhang and Clement Yu.
2006.
Uic at trec 2006blog track.
In Proceedings of TREC.Shiqi Zhao, Ming Zhou, and Ting Liu.
2007.
Learningquestion paraphrases for qa from encarta logs.
InProceedings of IJCAI, pages 1795?1800.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.In Proceedings of ACL, pages 834?842.46
