Knowledge represen tation and knowledge of words*Richmond H. ThomasonIntel l igent Systems ProgramUnivers i ty of P i t t sburghP i t tsburgh,  PA 15260May 29, 1991Abst rac tThis paper surveys ome opportunities for cooperative research between lin-guists and computer scientists in lexical semantics.
There are exciting possi-bilities and challenging problems.1.
In t roduct ionI will try in this short paper to present some general thoughts on knowledge represen-tation and word meaning for the June, 1991 SmLEX Workshop on Lexical Semantics andKnowledge Representation.
I believe that the topic of this workshop is very timely, andas important and strategic as any in the cognitive sciences.
That is the good news.
Thebad news is that it is very hard to feel confident about this topic, since progress in thisarea will have to overcome fundamental limitations of several of the sciences that are mostclosely involved: artificial intelligence, linguistics, and logic.
The right emotions houldbe a combination of excitement and fear, or at least caution.Difficult problems don't have quick and easy solutions.
I don't promise to say anythingthat will really make a substantive contribution to the research problems.
But I will try toexplain why I believe the problems are hard and to provide some perspectives on the newarea that is emerging here.
This paper was written under time pressure.
I received theabstracts of the papers that were accepted for the conference only a short time ago.
Thishas made it possible (I hope) to make the paper relevant, but has not allowed much timefor scholarship.
I hope to prepare an enlarged version of the paper after the workshop,that will try to provide adequate references to the workshop apers, and to the rest ofthe literature.2.
Goals*The author acknowledges the support of the National Science Foundation under grant IRI-9003165.We need a theory of linguistic meaning that is well grounded in linguistic evidence,that is broad in its coverage of linguistic onstructions and explanatory power, that canbe integrated with appropriate reasoning procedures, and that provides applicable mod-els for technology, such as machine translation, information retrieval, and word-orientedinstructional software.
How are we going to achieve these goals?3.
Background in logicMy own interest in this topic grew in part out of my work some years ago in Montaguegrammar.
This field has developed into a healthy area of linguistics with many well devel-oped research problems.
But fairly early on, it seemed to me that a lot could be learnedby concentrating instead on the limitations of the approach; some of these limitations aredescribed in \[Thomason 1987\].
The shortcomings of a logicist approach to semantics areprobably clearest in connection with word meaning.Knowing such meanings involves access to a broad spectrum of relevant knowledge.Technical terms like 'myelofibrosis', make the point most vividly, but (as Minsky andothers have often pointed out), it also is true of everyday terms like 'birthday party'.A logic-based approach like Montague grammar uses meaning postulates to accountfor inferences like(1) Bill saw someone kiss MargaretSo, someone kissed MargaretIn fact, the underlying logic provides a fairly powerful apparatus for writing these postu-lates.
Lambda bstraction over variables of higher-order types enable the postulate writerto attach conditions to words (in the case of this example, to the word 'see') so that theright intersentential consequences will follow.
(Roughly, 'see' has the property of express-ing a relation such that if anyone is related to a state of affairs by this relation, thenthat state of affairs obtains.
These things look horrible in English, but fine in IntensionalLogic.
)This condition on 'see', though, is far from a characterization f its meaning; it doesn'tdistinguish it from a large class of similar terms, such as 'hear', 'learn', 'remember' and'prove'.
And the underlying logic doesn't deliver the capability of providing such char-acterizations, except in a few cases (like 'and') that are closely connected to the originalpurpose of the logic: explicating mathematical reasoning.Mathematics provides deep chains of exceptionless reasoning, based on relatively fewprimitives.
Thus, concepts can be connected through definitions.
Most common sense do-mains provide relatively shallow patterns of defensible reasoning, based on a large numberof loosely connected concepts.
It is difficult in many cases to separate what is primitivefrom what is derived.
Given enough background knowledge it is possible to characterizethe meanings of terms, but these characterizations seldom take the form of necessary andsufficient conditions.
It is difficult o find reliable methods for articulating the backgroundknowledge and general ways of applying such knowledge in characterizing meanings.We should remember that similar inadequacies were responsible for the failure of at-tempts (most notably, by Rudolph Carnap) to extend Frege's formalization of mathe-matical reasoning to the empirical sciences.
1 Carnap discovered that Frege's method ofderiving definitions failed with color terms, and that terms like 'soluble' could not be givenI See \[Carnap 36-37\].natural and correct definitions in terms of terms like 'dissolve'.
The failure of logic-basedmethods to provide a means of formalizing the relevant background knowledge ven in rel-atively scientific domains provoked a skeptical reaction against he possibility of extendingthese logical methods.
2Montague motivated his addition of possible worlds to the Fregean framework with aproblem in derivational lexical semantics--that of providing a theory of events that wouldallow predicates like 'red' to be related to their nominalizations, like 'redness'.
s Trying toaccount for derivational interconnections between word meanings (rather than providinga framework for making principled distinctions in meaning between arbitrary words) isa more modest goal, and much can be learned by extending a logic-based theory in thisdirection.
But the work in lexical semantics that began in \[Dowty 79\] seems again to belimited in fundamental ways by the underlying logic.
The definition that Dowty providesof agent causality in terms of event causality fails, for logical reasons, in a way that offerslittle hope of repairs.
And, though the idea of normalcy that Dowty found to be neededin accounting for progressive aspect seems intuitively to sanction defeasible inferences,Intensionai Logic provides no good way of accounting for the validity of examples thathave exceptions, like(2) Harry is crossing the street.So Harry will cross the street.There is a natural progression between examples like this, which are focused on inferen-tial properties of telic constructions, to cases that draw more broadly on world knowledge(in this case, knowledge about the normal uses of artifacts), like(3) Alice used the match to light a fire.So Alice struck the match.4.
One relat ion between lexical semantics and knowledge representat ionLinguistic logicist work in the semantics of words, then, is closely related to Iogicistwork in knowledge representation.
Though the relation has not been much exploited yet,it suggests a clear line of research that is likely to benefit both linguistics and AI.I should add that I am thinking of long-term benefits here.
I don't claim that thisextension of the logicist inventory will provide a representation scheme for words thatis nearly adequate.
I do believe that such work is an essential part of any satisfactorysolution to the problem of lexical representation.
There is research in lexical semanticsthat is oriented towards applications but lacks a theoretical basis.
The logical work, onthe other hand, is limited in its applicability to lexical problems but provides an interfacewith sentence meaning; this approach is at its best in showing how meanings of phrasesdepend on meanings of their parts.
Along with this, it provides a specification of correctreasoning that--though it may not be implementablc is general and precise, and can beessential at the design level in knowledge representation applications.Part of the human language capacity is the ability to deal effectively with both wordsand sentences.
Though we may not have a single computational pproach that does both,See \[Quine 60\].3See \[Montague 69\].we can try to stretch partial approaches towards each other in the hope that togetherthey'll cover what needs to be covered.
This is why I am enthusiastic about extensions tothe lexical coverage of the logicist approaches.Logicist work in AI has generally recognized the need for augmenting the Fregeanlogical framework in order to deal with problems of common sense reasoning.
The mostgenerally accepted line of development is the incorporation of nonmonotonicity nto thelogic.
And this feature, it turns out, is precisely what is needed to accommodate manyof the problems that emerged in Montague-style l xical semantics.
It is the defeasiblenature of telicity, for instance, that makes it difficult to deal with (2) adequately withina standard logical framework.
It is no surprise that lexicai semantics i full of defeasiblegeneralizations, and a general technique for expressing such generalizations would greatlyextend the coverage of logicist theories of word meaning.The available approaches to nonmonotonicity could readily be incorporated into theframework of Montague-style s mantics without any changes to the undefeasible part ofthe logic.
4 Thus, the linguistic side has much to gain from the work in AI.Work on common sense reasoning, on the other hand, would also gain much fromcooperative applications to the study of derived word meanings.
For one thing, the projectof accounting for such meanings discloses a limited number of notions that are obviouslyof strategic importance for common sense reasoning.
5Moreover, the linguistic work uses a well developed methodology for marshaling ev-idence and testing theories.
Given the difficulty of delineating common sense reasoningand deciding between competing theories, this methodology could be very useful to theAI community.On the whole, then, this seems like a very natural and promising partnership.5.
Po lysemy and contextIt is encouraging to be able to point to an area of normal research at the interface oflexical semantics and knowledge representation, but at the same time it would be very mis-leading to imagine that all the problems of word meaning can be solved by nonmonotoniclogic, or that the potential areas of partnership are all tidy and unproblematic.In a number of papers published over the last twenty ears, John McCarthy has claimedthat a logical foundation for common sense reasoning should include not only a theory ofnonmonotonic reasoning, but a theory of conte~ct.
6It is easy to see how an account of context is central in approaching reasoning tasksof great or even moderate complexity.
It is essential to avoid being swamped in irrelevantdetail.
But if details are ignored, it is also essential to ignore them intelligently, so thatthe reasoning will retain appropriateness.
Engaged reasoning is located in a local contextwhich makes it focused and feasible, but nevertheless retains its applicability to the largercontext of which the current context is part.4 For an example in which Intensional Logic is combined with Circumscription Theory, see \[Thomason90\].5 At a sympos ium in the recent knowledge representation meeting in Cambridge, Massachusetts,  RayReiter argued that  common sense reasoning might not need to explicate causality; it may be as unimpor-tant in the common sense world as it seems to be in modern physical theories.
The ubiquitous presenceof causal notions in processes of word formation is a strong argument against such a position.SThe need for a theory of context was mentioned in McCarthy's 1971 Turing Award address; see\[McCarthy 87\] for a revised version.
A recent attack on the problem can be found in \[McCarthy 89\].4But there is a hierarchy here.
Contextualization must also be controlled by reasoningprocesses, which themselves may well be located in contexts.
Thus, contexts can havegreater or lesser generality, and some representation f context must also be available toreasoners.Though-- i f  McCarthy is right--we may not yet have a satisfactory theory of context,which could be incorporated into a logicist framework, we do have many applications.Object oriented approaches to programming, in particular, achieve their power throughcatering to the human need for contextual organization of reasoning; they could equallywell be called context oriented approaches?Many of the most difficult problems of involving the meanings of Words have to dowith the variability of interpretation.
In his experiments on the vagueness of terms, for?
.
.
*instance, William Labov noticed that the &stmction between 'cup' and 'bowl' was affectedmore by whether the interpreter was situated in a "coffee" or a "mashed potatoes" contextthan by factors such as the ratio of height to diameter of the artifact.
7To take another example, there is some reason to think that in a context where a bus isleaving for a banquet, 'go' can mean 'go on the bus to the banquet'.
Of course, if someonesays(4) I'm going.in such a context, it means 'I'm going on the bus to the banquet', but this effect couldbe attributed to the speaker meaning of the utterance, without assigning any specialinterpretation to 'go'.
More telling is the fact that in this case it's possible to say(5) No, I'm not going; I'm taking my car.Some of the problems of polysemy that Sowa discusses in his contribution to thisworkshop and in other writings are best regarded, I think, as cases in which the proceduresfor interpreting words are adjusted to context.
Unfortunately, this is an area in whichwe seem to have many alternative ways of accounting for the phenomena: vagueness,ambiguity, strategies of interpreting speaker meaning, and contextual effects.
All theseaccounts are plausible, and each is best equipped to deal with some sorts of examples.But in many cases there is no clear way to pick tile best account.
Perhaps this problemshould be solved not by treating the accounts as competitors and seeking more refinedlinguistic tests, but by providing bridges between one solution and the other; chunking,for instance, provides in many cases a plausible path from conversational implicature toa lexicalized word sense.I have stressed the contextual approach to polysemy because it seems to me to offermore hope for progress than other ways of looking at the problem.
It enables us to draw ona variety of computational pproaches, uch as object oriented programming, and it openspossibilities of collaboration with theoreticians who, influenced by McCarthy, are lookingfor formal ways of modeling contextuality.
The ongoing work of theory developmentbadly needs examples and intuitions; language in general and the lexicon in particular areprobably the most promising source of these.6.
Linguistic workVSee \[Labov 73\].Of course, most of the recent linguistic research on word meaning has been done bynonlogicists.
See \[Levin 85\], for instance, for a useful survey of work in the Government-Binding framework.There is no substitute for the broad empirical work being done by linguists in this area.But as Levin's survey makes clear, it is very difficult to develop a theoretical apparatusthat is well grounded in linguistic evidence in this area.
Despite the efforts of manywell trained linguists to devise good general tests for important notions like agency, theconnection of these concepts to the evidence remains very problematic.Despite difficulties with the high level concepts, the linguistic work has uncoveredmuch taxonomic nformation that is relatively general across languages, and that evidentlyclassifies words not only into categories that pattern similarly, but that share importantsemantic features.This, too, seems to be an area in which cooperation between linguists and the AI com-munity might be fruitful.
The classification schemes that come from linguistics are notonly well motivated, but should be very useful in organizing lexical information on inher-itance principles.
Moreover, it might well be useful for linguists who are grappling withmethodological difficulties to learn to think of their problems along knowledge ngineeringlines rather than syntactic ones.7.
Linguistics and knowledge representat ionRepresentation is crucial in contemporary linguistics, and is found in all the areas wherelinguistic structure is important.
But syntax seems to be the primary source of represen-tational ideas and methods for justifying them.
For over thirty years, syntacticians haveproposed formalisms (which in general are variations on labeled trees, representing phrasestructure), along with rules for systematically generating them.
They have also developedmethods for justifying these formalisms, based mainly on introspective evidence aboutgrammaticality, and an extremely rich battery of techniques for bringing this evidence tobear on hypotheses.Though (except in some cases where natural anguage processing systems are inte-grated with the formalism), these representation systems are tested by introspective evi-dence, and their connection to experiments and to cognitive psychology is in fact tenuousand problematic, many linguists make cognitive claims for their representations.The hope seems to be that eventually the structures that are well supported by theintrospective methods will be eventually be validated by a larger psychological theory ofprocessing that is well supported by experimental evidence.Whether or not such a theory is eventually forthcoming, the current methods usedto support different syntactic theories often seem to leave no way of settling even quitemajor issues.
And when these methods are extended to semantics, they definitely seem toleave theoretical lternatives underconstrained by the available methodology of linguisticargumentation.
Intuitions about meaning are even more problematic than those aboutgrammaticality.
Even though grammaticality is a fairly refined notion, and subject tocontextual factors that are difficult to determine, it seems to be easier to agree aboutgrammaticality judgments than about, for instance, judgments about ambiguity.The criteria that have emerged in knowledge representation seem to me to be wellworth considering in this respect, tlere are some considerations.6.....The criteria are stringent--so stringent, in fact, that, in view of conflict betweendesirable features uch as expressivity and tractability, there really are no general-purpose knowledge representation schemes meeting them all.The criteria of knowledge representation can be added without much violence tothe ones already imposed by linguistic theorists.
In fact, the need for usabil ity--assuming that the users are linguists--would require the use of representations thatmake linguistic sense.
No special cognitive claims need to be made.
The point isthat, though it can be debated whether a generally accepted linguistic formalism isadequate as a representation of human cognition, there is no doubt- - i f  it's gener-ally accepted--that it is a useful way of displaying linguists' insights into linguisticstructure.It often is necessary in linguistics to represent large amounts of information.
As lex-icography becomes computerized, and the need is felt to connect hese computerizedlinguistic knowledge bases to areas of linguistic theory such as syntax, a novel cri-terion emerges--does the theory allow a workable way of organizing large amountsof lexical information?The need to associate knowledge management procedures with representations alsoprovides new constraints, and-- i f  the procedures can be implemented--may lsohelp to automate the testing process.
It is hard to see, for instance, whether asemantic theory can be tested as a mere theory of representation.
Since the mainpurpose of semantic representation is to provide a level at which sound inference cantake place, an explicit specification of the associated inference procedures i neededbefore we can begin to test the theory.There are many similarities of detail that make it easy to build smooth bridgesbetween linguistic formalisms and ones from knowledge representation.8.
Conc lus ionLet's be clear about the problems.The field of knowledge representation began with a strong emphasis on applications innatural anguage understanding, but shifted its emphasis as it developed.
This happenedin part because the opportunities for productive research in the area are concentrated inrelatively small scale, domain specific systems.
It is hard to see how to build larger systemswithout sacrificing a clear understanding of what one is doing, and any hope of reliableperformance.
Thus, in returning to natural language understanding, we are straining thecapabilities of what is known about representing knowledge.
Since there is much interestin larger systems, and some hope of help from existing knowledge sources and from whatlinguists have learned about word meaning, lexieal semantics might be a promising areafor research in scaling up knowledge representation.
But we have to remember that weare trying to extend the field in ways that are pretty fundamental.Linguists have created a successful science by systematically ignoring cases where thereare strong interactions between linguistic knowledge and broadly based world knowledge.They have developed a research methodology that works well for phonology, syntax, mor-phology, and some limited areas of semantics, but that breaks down in other areas ofsemantics and in pragmatics.
They are comfortable with arguments that test represen-tation systems for linguistic correctness, but not with ones that depend on engineeringconsiderations like usability and transportability.
Fairly radical departures from linguisticmethodology are needed, I suspect, in establishing a unified theory of lexical semantics.To try to separate this project from tile task of building large scale knowledge bases isto settle for a partial solution, which may well turn out to be incompatible with sys-tems providing the world knowledge that ultimately needs to be used in natural anguageprocessing applications.To integrate a computational semantics of words with knowledge representation tcch-niques, we need to remember that representations can't be separated from reasoning.
It isall too easy for any representation system to seem adequate until it is put to use in appli-cations such as planning, that call for intensive reasoning.
This requirement is probablygoing to be extremely difficult to observe in practice, but I think that we have to bear itin mind if we are going to have confidence in the representation systems that emerge fromthis work.References\[Carnap 36-37\] Rudolph Carnap.
"Testability and meaning."
Philosophy off Science 3, 1936, pages419-471 and Philosophy of Science 4, 1937, pp.
1-40.\[Dowty 79\] David Dowry.
Word Meaning and Montague Grammar.
D. Reidel, Dordrecht, 1979.\[Labov 73\] William Labor.
"The boundaries of words and their meanings."
In New ways ofanalyzing variation in English.
C.-J.
Bailey and R. Shuy, eds., Georgetown UniversityPress, Washington DC, 1973, pp.
340--373.\[Levin 85\] Beth Levin.
"Lexical semantics in review: an introduction" In Lexical semantics inreview, B. Levin, ed., Lexicon Project Working Papers 1, MIT Center for Cognitive Science,Cambridge MA, 1985, pp.
1-62.\[McCarthy 87\] John McCarthy.
"Generality in artificial intelligence."
Communications of theACM 30 (1987), pp.
1030-1035.\[McCarthy 89\] John McCarthy.
"Artificial intelligence and logic."
In R. Thomason (ed.)
Philo-sophical Logic and Artificial Intelligence, Kluwer Publishing Co., Dordrecht, 1989, pp.
161-190.\[Montague 69\] Richard Montague.
On the nature of certain philosophical entities.
The Monist53 (1969), pages 159-194.\[Quine 60\] Willard Quine.
Word and Object.
MIT Press and John Wiley, Cambridge MA andLondon, 1960.\[Sowa 91\] John Sowa.
"Logical structures ill tile lexicon."
This volume.\[Thomason 1987\] Richmond Thomason.
"Remarks on linguistic semantics."
In Mathematics o\]language, A. Manaster-Ramer, ed., John Benjamins, Amsterdam, 1987, pp.
374-388.\[Thomason 90\] Richmond Thomason.
Propagating epistemic oordination through mutual de-faults I.
In Rohit Parikh (ed.)
Proceedings o/the Third Conference on Theoretical Aspectso\] Reasoning about Knowledge.
Morgan Kaufmann, San Mateo CA, 1990, pages 29-39.
