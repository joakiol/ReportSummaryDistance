Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 377?385,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSentiment Propagation via Implicature ConstraintsLingjia DengIntelligent Systems ProgramUniversity of Pittsburghlid29@pitt.eduJanyce WiebeDepartment of Computer ScienceUniversity of Pittsburghwiebe@cs.pitt.eduAbstractOpinions may be expressed implicitly viainference over explicit sentiments andevents that positively/negatively affect en-tities (goodFor/badFor events).
We in-vestigate how such inferences may beexploited to improve sentiment analysis,given goodFor/badFor event information.We apply Loopy Belief Propagation topropagate sentiments among entities.
Thegraph-based model improves over explicitsentiment classification by 10 points inprecision and, in an evaluation of themodel itself, we find it has an 89% chanceof propagating sentiments correctly.1 IntroductionPrevious research in sentiment analysis andopinion extraction has largely focused on theinterpretation of explicitly stated opinions.
How-ever, many opinions are expressed implicitlyvia opinion implicature (i.e., opinion-orienteddefeasible inference).
Consider the followingsentence:EX(1) The bill would lower health care costs, which wouldbe a tremendous positive change across the entire health-caresystem.The writer is clearly positive toward the idea oflowering health care costs.
But how does s/he feelabout the costs?
If s/he is positive toward the ideaof lowering them, then, presumably, she is nega-tive toward the costs themselves (specifically, howhigh they are).
The only explicit sentiment expres-sion, tremendous positive change, is positive, yetwe can infer a negative attitude toward the objectof the event itself (i.e., health care costs).Going further, since the bill is the agent of anevent toward which the writer is positive, we may(defeasibly) infer that the writer is positive towardthe bill, even though there are no explicit senti-ment expressions describing it.Now, consider The bill would curb skyrocketinghealth care costs.
The writer expresses an explicitnegative sentiment (skyrocketing) toward the ob-ject (health care costs) of the event.
Note thatcurbing costs, like lowering them, is bad for them(the costs are reduced).
We can reason that, be-cause the event is bad for something toward whichthe writer is negative, the writer is positive towardthe event.
We can reason from there, as above,that the writer is positive toward the bill, since itis the agent of the positive event.These examples illustrate how explicit sen-timents toward one entity may be propagatedto other entities via opinion implicature rules.The rules involve events that positively or nega-tively affect entities.
We call such events good-For/badFor (hereafter gfbf )events.This work investigates how gfbf event interac-tions among entities, combined with opinion in-ferences, may be exploited to improve classifica-tion of the writer?s sentiments toward entities men-tioned in the text.
We introduce four rule schemaswhich reveal sentiment constraints among gfbfevents and their agents and objects.
Those con-straints are incorporated into a graph-based model,where a node represents an entity (agent/object),and an edge exists between two nodes if the twoentities participate in one or more gfbf events witheach other.
Scores on the nodes represent the ex-plicit sentiments, if any, expressed by the writertoward the entities.
Scores on the edges are basedon constraints derived from the rules.
Loopy Be-lief Propagation (LBP) (Pearl, 1982) is applied to377accomplish sentiment propagation in the graph.Two evaluations are performed.
The first showsthat the graph-based model improves over an ex-plicit sentiment classification system.
The secondevaluates the graph-based model itself (and hencethe implicature rules), assessing its ability to cor-rectly propagate sentiments to nodes whose polar-ities are unknown.
We find it has an 89% chanceof propagating sentiment values correctly.This is the first paper to address this type ofsentiment propagation to improve sentiment anal-ysis.
To eliminate interference introduced by othercomponents, we use manually annotated gfbf in-formation to build the graph.
Thus, the evaluationsin this paper are able to demonstrate the promiseof the overall framework itself.2 Related WorkMuch work in sentiment analysis has been ondocument-level classification.
Since different sen-timents may be expressed toward different entitiesin a document, fine-grained analysis may be moreinformative for applications.However, fine-grained sentiment analysis re-mains a challenging task for NLP systems.
Forfully-automatic systems evaluated on the MPQAcorpus (Wiebe et al., 2005), for example, a recentpaper (Johansson and Moschitti, 2013) reports re-sults that improve over previous work, yet the F-measures are in the 40s and 50s.Most work in NLP addresses explicit sentiment,but some address implicit sentiment.
For example,(Zhang and Liu, 2011) identify noun product fea-tures that imply opinions, and (Feng et al., 2013)identify objective words that have positive or neg-ative connotations.
However, identifying termsthat imply opinions is a different task than senti-ment propagation between entities.
(Dasigi et al.,2012) search for implicit attitudes shared betweenauthors, while we address inferences within a sin-gle text.Several papers apply compositional semanticsto determine polarity (e.g., (Moilanen and Pul-man, 2007; Choi and Cardie, 2008; Moilanen etal., 2010); see (Liu, 2012) for an overview).
Thegoal of such work is to determine one overall po-larity of an expression or sentence.
In contrast, ourframework commits to a holder having sentimentstoward various events and entities in the sentence,possibly of different polarities.The idea of gfbf events in sentiment analysis isnot entirely new.
For example, two papers men-tioned above (Zhang and Liu, 2011; Choi andCardie, 2008) include linguistic patterns for thetasks that they address that include gfbf events,but they don?t define general implicature rules re-lating sentiments and gfbf events, agents, and ob-jects as we do.
Recently, in linguistics, Anandand Reschke (2010; 2011) identify classes ofgfbf terms, and carry out studies involving artifi-cially constructed gfbf triples and corpus exam-ples matching fixed linguistic templates.
Our workfocuses on gfbf triples in naturally-occurring dataand uses generalized implicature rules.
Goyal etal.
(2012) generate a lexicon of patient polar-ity verbs, which correspond to gfbf events whosespans are verbs.
Riloff et al.
(2013) investigatesarcasm where the writer holds a positive senti-ment toward a negative situation.
However, nei-ther of these works performs sentiment inference.Graph-based models have been used for varioustasks in sentiment analysis.
Some work (Wang etal., 2011; Tan et al., 2011) apply LBP on a graphcapturing the relations between users and tweets inTwitter data .
However, they assume the nodes andthe neighbors of nodes share the same sentiments.In contrast, we don?t assume that neighbors sharethe same sentiment, and the task we address is dif-ferent.3 Opinion ImplicaturesThis section describes the opinion-implicatureframework motivating the design of the graph-based method for sentiment analysis proposed be-low.
The components of the framework are gfbfevents, explicit sentiments, and rules operatingover gfbf events and sentiments.The definition of a gfbf event is from (Deng etal., 2013).
A GOODFOR event is an event thatpositively affects an entity (similarly, for BADFORevents).
(Deng et al., 2013) point out that gfbf ob-jects are not equivalent to benefactive/malefactivesemantic roles.
An example they give is She bakeda cake for me: a cake is the object of GOOD-FOR event baked (creating something is good forit (Anand and Reschke, 2010)), while me is thefiller of its benefactive semantic role (Z?u?niga andKittil?a, 2010).Four implicature rule schemas are relevant forthis paper.1Four individual rules are covered by1Implicatures ?normally accompany the utterances of agiven sentence unless special factors exclude that possibility378each schema.
sent(?)
= ?
means that the writer?ssentiment toward ?
is ?, where ?
is a GOODFORevent, a BADFOR event, or the agent or object ofa gfbf event, and ?
is either positive or negative(pos or neg, for short).
P?
Q is to infer Q from P.Rule1: sent(gfbf event)?
sent(object)1.1 sent(GOODFOR) = pos?
sent(object) = pos1.2 sent(GOODFOR) = neg?
sent(object) = neg1.3 sent(BADFOR) = pos?
sent(object) = neg1.4 sent(BADFOR) = neg?
sent(object) = posRule2: sent(object)?
sent(gfbf event)2.1 sent(object) = pos?
sent(GOODFOR) = pos2.2 sent(object) = neg?
sent(GOODFOR) = neg2.3 sent(object) = pos?
sent(BADFOR) = neg2.4 sent(object) = neg?
sent(BADFOR) = posRule3: sent(gfbf event)?
sent(agent)3.1 sent(GOODFOR) = pos?
sent(agent) = pos3.2 sent(GOODFOR) = neg?
sent(agent) = neg3.3 sent(BADFOR) = pos?
sent(agent) = pos3.4 sent(BADFOR) = neg?
sent(agent) = negRule4: sent(agent)?
sent(gfbf event)4.1 sent(agent) = pos?
sent(GOODFOR) = pos4.2 sent(agent) = neg?
sent(GOODFOR) = neg4.3 sent(agent) = pos?
sent(BADFOR) = pos4.4 sent(agent) = neg?
sent(BADFOR) = negTo explain the rules, we step through an example:EX(2) Why would [President Obama] support [health carereform]?
Because [reform] could lower [skyrocketing healthcare costs], and prohibit [private insurance companies] fromovercharging [patients].Suppose a sentiment analysis system recognizesonly one explicit sentiment expression, skyrock-eting.
According to the annotations, there areseveral gfbf events.
Each is listed below in theform ?agent, gfbf, object?.E1: ?reform, lower, costs?E2: ?reform, prohibit, E3?E3: ?companies, overcharge, patients?E4: ?Obama, support, reform?In E1, from the negative sentiment expressedby skyrocketing (the writer is negative toward the(p.
39).?
(Huddleston and Pullum, 2002)costs because they are too high), and the fact thatcosts is the object of a BADFOR event (lower),Rule2.4 infers a positive attitude toward E1.Now, Rule3.3 applies.
We infer the writer ispositive toward the reform, since it is the agentof E1, toward which the writer is positive.E2illustrates the case where the object is anevent.
Specifically, the object of E2is E3, a BAD-FOR event (overcharging).
As we can see, E2keeps E3from happening.
Events such as E2are REVERSERs, because they reverse the polar-ity of a gfbf event (from BADFOR to GOODFOR,or vice versa).
Note that REVERSERs may be seenas BADFOR events, because they make their ob-jects irrealis (i.e., not happen).
Similarly, a RE-TAINER such as help in ?help Mary save Bill?
canbe viewed as a GOODFOR event.
(We call a RE-VERSER or a RETAINER an INFLUENCER.)
In thispaper, RETAINERS are treated as GOODFOR eventsand REVERSERS are treated as BADFOR events.Above, we inferred that the writer is positive to-ward reform, the agent of E2.
By Rule 4.3, thewriter is positive towardE2; then by Rule 1.3, thewriter is negative toward E3, the object of E2.For E3, using Rule 1.4 we know the writer ispositive toward patients and using Rule 3.4 weknow the writer is negative toward companies.Turning to E4, support health care reform isGOODFOR reform.
We already inferred the writeris positive toward reform.
Rule 2.1 infers that thewriter is positive toward E4.
Rule 3.1 then infersthat the writer is positive toward the agent of E4,Obama.In summary, we infer that the writer is positivetoward E1, health care reform, E2, patients, E4,and Obama, and negative toward E3and privateinsurance companies.4 DataWe use the data described in (Deng et al., 2013),2which consists of 134 documents about a contro-versial topic, ?the Affordable Care Act.?
The doc-uments are editorials and blogs, and are full ofopinions.In the data, gfbf triples are annotated specifyingthe spans of the gfbf event, its agent, and its object,as well as the polarity of the gfbf event (GOODFORor BADFOR), and the writer?s attitude toward theagent and object (positive, negative, or neutral).Influencers are also annotated.
The agents of gfbf2Available at http://mpqa.cs.pitt.edu379and influencer events are noun phrases.
The ob-ject of a gfbf event is a noun phrase, but the objectof an influencer is a gfbf event or another influ-encer.
A triple chain is a chain of zero or moreinfluencers ending in a gfbf event, where the ob-ject of each element of the chain is the followingelement in the chain.
(e.g.
in EX(2), the two eventprohibit and overcharging is a triple chain.
)In total, there are 1,762 annotated gfbf triples,out of which 692 are GOODFOR or RETAINERand 1,070 are BADFOR or REVERSER.
From thewriter?s perspective, 1,495 noun phrases are anno-tated positive, 1,114 are negative and the remain-ing 8 are neutral.
This is not surprising, given thatmost of the sentences in the data are opinionated.5 Graph-based ModelWe propose a graph-based model of entities andthe gfbf relations between them to enable senti-ment propagation between entities.
In this section,we introduce the definition of the graph (in 5.1),the LBP algorithm (in 5.2), and the definition ofits functions for our task (in 5.3 and 5.4).5.1 Definition of the Entity GraphWe define a gfbf entity graph EG = {N,E},in which the node set N consists of nodes, eachrepresenting an annotated noun phrase agent orobject span.
The edge set E consists of edges,each linking two nodes if they co-occur in a triplechain with each other.
Consider the triples ofEX(2) in Section 3 below.E1: ?reform, lower, costs?E2: ?reform, prohibit, E3?E3: ?companies, overcharge, patients?E4: ?Obama, support, reform?The node of reform is linked to nodes of costs viaE1and Obama via E4.3Note that, for E2andE3, the two are linked in a chain: ?reform, pro-hibit, ?companies, overcharge, patients?
?.
Thethree nodes reform, companies and patients partic-ipate in this triple chain; thus, pairwise edges ex-ist among them.
The edge linking companies andpatients is BADFOR (because of overcharging).The edge linking reform and companies is also aBADFOR since we treat a REVERSER as BADFOR.3This assumes that the two instances of ?reform?
co-refer.However, the system does not resolve co-reference ?
themethods that we tried did not improve overall performance.The edge linking reform and patients encodes twoBADFOR events (prohibit-overcharge); computa-tionally we say two BADFORs result in a GOOD-FOR, so the edge linking the two is GOODFOR.4Given a text, we get the spans of gfbf eventsand their agents and objects plus the polarities ofthe events (GOODFOR/BADFOR) from the manualannotations, and then build the graph upon them.However, the manual annotations of the writer?ssentiments toward the agents and objects are usedas the gold standard for evaluation.5.2 Sentiment Inference via LBPinitialize all mi?j(pos) = mi?j(neg) = 1repeatforeach ni?
N doforeach nj?
Neighbor(ni) doforeach y ?
pos, neg docalculate mi?j(y)normalize mi?j(pos) + mi?j(neg) = 1until all mi?jstop changing;for each ni?
N assign its polarity asargmaxy?pos,neg?i(y) ?
?nk?Neighbor(ni)mk?i(y)neutral, in case of a tieTable 1: Loopy Belief PropagationWith graph EG containing cycles and no appar-ent structure, we utilize an approximate collectiveclassification algorithm, loopy belief propagation(LBP) (Pearl, 1982; Yedidia et al., 2005), to clas-sify nodes through belief message passing.
Thealgorithm is shown in Table 1.In LBP, each node has a score, ?i(y), and eachedge has a score, ?ij(yi, yj).
In our case, ?i(y)represents the writer?s explicit sentiment towardni.
?ij(yi, yj) is the score on edge eij, represent-ing the likelihood that node nihas polarity yiandnjhas polarity yj.
The specific definitions of thetwo functions are given in Sections 5.3 and 5.4.LBP is an iterative message passing algorithm.A message from nito njover edge eijhastwo values: mi?j(pos) is how much informationfrom node niindicates node njis positive, andmi?j(neg) is how much information from nodeniindicates node njis negative.
In each iteration,the two are normalized such that mi?j(pos) +mi?j(neg) = 1.
The message from nito its4Also, GOODFOR+BADFOR=BADFOR; GOOD-FOR+GOODFOR=GOODFOR380neighbor njis computed as:mi?j(pos) =?ij(pos, pos)??i(pos)?
?nk?Neighbor(ni)/njmk?i(pos)+?ij(neg, pos)??i(neg)?
?nk?Neighbor(ni)/njmk?i(neg)(1)mi?j(neg) =?ij(neg, neg)??i(neg)?
?nk?Neighbor(ni)/njmk?i(neg)+?ij(pos, neg)??i(pos)?
?nk?Neighbor(ni)/njmk?i(pos)(2)For example, the first part of Equation (1)means that the positive message niconveys tonj(i.e., mi?j(pos)) comes from nibeing pos-itive itself (?i(pos)), the likelihood of edge eijwith its nodes nibeing positive and njbeingpositive (?ij(pos, pos)), and the positive mes-sage ni?s neighbors (besides nj) convey to it(?k?Neighbor(ni)/njmk?i(pos)).After convergence, the polarity of each node isdetermined by its explicit sentiment and the mes-sages its neighbors convey to it, as shown at theend of the algorithm in Table 1.By this method, we take into account both sen-timents and the interactions between entities viagfbf events in order to discover implicit attitudes.Note that the node and edge scores are deter-mined initially and do not change.
Only mi?jchanges from iteration to iteration.5.3 ?ij(yi, yj): GFBF Implicature RelationsThe score ?i,jencodes constraints based on thegfbf relationships that nodes niand njparticipatein, together with the implicature rules given above.Rule schemas 1 and 3 infer sentiments to-ward entities (agent/object) from sentiments to-ward gfbf events.
All cases covered by them areshown in Table 2 (use s(?)
to represent sent(?
)).Rule 3 Rule1s(gfbf) gfbf type ?
s(agent) s(object)pos GOODFOR ?
pos posneg GOODFOR ?
neg negpos BADFOR ?
pos negneg BADFOR ?
neg posTable 2: Rule 1 & Rule 3A table of Rule schemas 2 and 4 would beexactly the same, except that the inference (?
)would be in the opposite direction (?
).From Table 2, we see that, regardless of thewriter?s sentiment toward the event, if the eventis GOODFOR, then the writer?s sentiment towardthe agent and object are the same, while if theevent is BADFOR, the writer?s sentiment towardthe agent and object are opposite.
Thus, the eventtype and the writer?s sentiments toward the agentsand objects give us constraints.
Therefore, we de-fine ?ij(pos, pos) and ?ij(neg, neg) to be 1 if thetwo nodes are linked by a GOODFOR edge; oth-erwise, it is 0; and we define ?ij(neg, pos) and?ij(pos, neg) to be 1 if the two nodes are linkedby a BADFOR edge; otherwise, it is 0.5.4 ?i(y): Explicit Sentiment ClassifierThe score of a node, ?i(y), represents the sen-timent explicitly expressed by the writer towardthat entity in the document.
Since y ranges over(pos, neg), each node has a positive and a nega-tive score; the scores sum to 1.
If it is a positivenode, then its positive value ranges from 0.5 to 1,and its negative value ranges from 0 to 0.5 (sim-ilarly for negative nodes).
For any node withoutexplicit sentiment, both the positive and negativevalues are 0.5, indicating a neutral node.Thus, we build a sentiment classifier that takes anode as input and outputs a positive and a negativescore.
It is built from widely-used, freely availableresources: the OpinionFinder (Wilson et al., 2005)and General Inquirer (Stone et al., 1966) lexiconsand the OpinionFinder system.5We also use a newOpinion Extraction system (Johansson and Mos-chitti, 2013) that shows better performance thanprevious work on fine-grained sentiment analy-sis,6and a new automatically developed connota-tion lexicon (Feng et al., 2013).7We implement a weighted voting methodamong these various sentiment resources.
Afterthat, for nodes that have not yet been assigned po-lar values (positive or negative), we implement asimple local discourse heuristic to try to assignthem polar values.The particular strategies were chosen basedonly on a separate development set, which is not5http://mpqa.cs.pitt.edu andhttp://www.wjh.harvard.edu/ inquirer/6As evaluated on the MPQA corpus.
Note that the authorsran their system for us on the data we use.7http://www.cs.stonybrook.edu/?ychoi/connotation381included in the data used in the experiments.5.4.1 Explicit Sentiment ToolsOpinion Extraction outputs a polarity expressionwith its source, and OpinionFinder outputs a po-larity word.
But neither of the tools extracts thetarget.
To extract the target, for each word in theopinion expression, we select other words in thesentence which are in a mod, obj dependency pars-ing relation with it.We match up the extracted expressions and thegfbf annotations according to their offsets in thetext.
For an opinion expression appearing in thesentence with no gfbf annotation, if the root word(in the dependency parse) of the expression spanis the same as the root word of a gfbf span, or theroot word of an agent span, or the root word of anobject span, we assume they match up.
Then weassign polarity as follows.
If the expression refersonly to the agent or object, then the agent or objectis assigned the polarity of the expression.
If theexpression covers the gfbf event and its object, weassume the sentiment is toward the gfbf event andthen assign sentiment according to Rule schema 1(sent(gfbf event)?
sent(object)).5.4.2 LexiconsTo classify the sentiment expressed within thespan of an agent or object, we check whether thewords in the span appear in one or more of thelexicons.8If a lexicon finds both positive and neg-ative words in the span, we resolve the conflictby choosing the polarity of the root word in thespan.
If the root word does not have a polar value,we choose the majority polarity of the sentimentwords.
If there are an equal number of positiveand negative words, the polarity is neutral.5.4.3 Voting Scheme among ResourcesAll together we have two sentiment systems andthree lexicons.
Before explicit sentiment classi-fying, each node has a positive value of 0.5 anda negative value of 0.5.
We give the five votesequal weight (0.1), and add the number of posi-tive votes multiplied by 0.1 to the positive value,and the number of negative votes multiplied by 0.1to the negative value.
After this addition, both val-ues are in the range 0.5 to 1.
If the positive valueis larger, we maintain the positive value and assign8The comparison is done after lemmatization, using thewordNet lemmatization in NLTK, and with the same POS,according to the Stanford POStagger toolkit.the negative value to be 1-positive value (similarlyif the negative value is larger).5.4.4 DiscourseFor a sentence s, we assume the writer?s senti-ments toward the gfbf events in the clauses of s,the previous sentence, and the next sentence, arethe same.
Consider EX(3):EX(3) ... health-insurance regulations that will prohibit (a)denying coverage for pre-existing conditions, (b) droppingcoverage if the client gets sick, and (c) capping insurancecompany reimbursement...EX(3) has three clauses, (a)-(c).
Suppose the ex-plicit sentiment classifier recognizes that event (a),denying coverage for pre-existing conditions, isnegative and it does not find any other explicit sen-timents in the sentence.
The system assumes thewriter?s sentiments toward (b) and (c) are negativeas well.After assigning all possible polarities to eventswithin a sentence, polarities are propagated to theother still-neutral gfbf events in the previous andnext sentences.Finally, event-level polarities are propagated tostill-neutral objects using Rule schema 1.9If thereis a conflict, we take the majority sentiment; ifthere is a tie, the object remains neutral.However, the confidence of the discourse votingis smaller than the explicit sentiment voting, sincediscourse structure is complex.
If by discourse anobject node is classified as positive, the positivevalue is 0.5 + random(0, 0.1) and the negativevalue is 1-positive value.
Thus, the positive valueof a positive node is larger than its negative value,but not exceeding too much (similarly for negativenodes).6 Experiments and Results6.1 Experiment DataOf the 134 documents in the dataset, 6 were usedas a development set, and 3 do not have any anno-tation.
We use the remaining 125 for experiment.6.2 Evaluation MetricsTo evaluate the performance of classifying thewriter?s sentiments toward agents and objects, we9Note that, in the gfbf entity graph, sentiments can bepropagated from objects to agents, conceptually via Ruleschemas 2 and 3.
Thus, here we only classify objects.382define three metrics to evaluate performance.
Forthe entire dataset, accuracy evaluates the percent-age of nodes that are classified correctly.
Preci-sion and recall are defined to evaluate polar (non-neutral) classification.Accuracy =#node auto=gold#nodes(3)Precision =#node auto=gold & gold != neutral#node auto != neutral(4)Recall =#node auto=gold & gold != neutral#node gold != neutral(5)In the equations, auto is the system?s output andgold is the gold-standard label from annotation.6.3 Overall PerformanceIn this section, we evaluate the performance ofthe overall system.
In 6.5, we evaluate the graphmodel itself.Two baselines are defined.
One is assigningthe majority class label, which is positive, to allagents/objects (Majority(+)).
The second is as-suming that agents/objects in a GOODFOR relationare positive and agents/objects in a BADFOR rela-tion are negative (GFBF ).
In addition, we eval-uate the explicit sentiment classifier introduced inSection 5.4 (Explicit).
The results are shown inTable 3.Accuracy Precision RecallMajority(+) 0.5438 0.5621 0.5443GFBF 0.5437 0.5523 0.5444Explicit 0.3703 0.5698 0.3703Graph-LBP 0.5412 0.6660 0.5419Table 3: Performance of baselines and graph.As can be seen,Majority andGFBF give ap-proximately 56% precision.
Explicit sentimentclassification alone performs hardly better in pre-cision and much lower in recall.
As mentionedin Section 2, fine-grained sentiment analysis isstill very difficult for NLP systems.
However, thegraph model improves greatly over Explicit inboth precision and recall.
While recall of the graphmodel is comparable to the Majority, precisionis much higher.During the experiment, if the LBP does not con-verge until 100 iterations, it is forced to stop.
Theaverage number of iteration is 34.192.6.4 Error AnalysisTable 4 shows the results of an error analysis todetermine what contributes to the graph model?serrors.1 wrong sentiment from voting 0.21322 wrong sentiment from discourse 0.04623 subgraph with wrong polarity 0.31894 subgraph with no polarity 0.41605 other 0.0056Table 4: Errors for graph model.Rows 1-2 are the error sources for nodes as-signed a polar value before graph propagation.Row 1 errors are due to the sentiment-voting sys-tem, Row 2 are due to discourse processing.Rows 3-4 are the error sources for nodes thathave not been assigned a polar value by Explicit.Such a node receives a polar value only via prop-agation from other nodes in its subgraph (i.e., theconnected component of the graph containing thenode).
Row 5 is the percentage of other errors.As shown in Rows 1-2, 25.94% of the errorsare due to Explicit.
These may propagate incor-rect labels to other nodes in the graph.
As shownin Row 3, 31.89% of the errors are due to nodesnot classified polar by Explicit, but given incor-rect values because their subgraph has an incorrectpolarity.
Row 4 shows that 41.60% of the errorsare due to nodes that are not assigned any polarvalue.
Given non-ideal input from sentiment anal-ysis, how does the graph model increase precisionby 10 percentage points?There are two main ways.
For nodes which re-main neutral after Explicit, they might be clas-sified correctly via the graph.
For nodes whichare given incorrect polar labels by Explicit, theymight be fixed by the graph.
Table 5 shows thebest the graph model could do, given the noisy in-put from Explicit.
Over all of the nodes, morepropagated labels are incorrect than correct.
How-ever, if there are no incorrect, or more correct thanincorrect sentiments in the subgraph (connectedcomponent), then many more of the propagated la-bels are correct than incorrect.
In all cases, moreof the changed labels are correct than incorrect.6.5 Consistency and Isolated Performance ofGraph ModelThe implicature rules are defeasible.
In this sec-tion we introduce an experiment to valid the con-383propagated propagated changed changedlabel correct label incorrect correctly incorrectlyall subgraphs399 536 424 274subgraphs having no incorrect sentiment347 41 260 23subgraphs having more correct than incorrect sentiment356 42 288 35Table 5: Effects of graph model given Explicitinputsistency of implicature rule.
Recall that in Section5.3, the definition of ?i,jis based on implicaturerules and sentiment is propagated based on ?i,j.Thus, this is also an evaluation of the performanceof the graph model itself.
We performed an experi-ment to assess the chance of a node being correctlyclassified only via the graph.In each subgraph (connected component), weassign one of the nodes in the subgraph with itsgold-standard polarity.
Then we run LBP on thesubgraph and record whether the other nodes inthe subgraph are classified correctly or not.
Theexperiment is run on the subgraph |S| times, where|S| is the number of nodes in the subgraph, sothat each node is assigned its gold-standard polar-ity exactly once.
Each node is given a propagatedvalue |S| ?
1 times, as each of the other nodes inits subgraph receives its gold-standard polarity.To evaluate the chance of a node given a correctpropagated label, we use Equations (6) and (7).correct(a|b) ={1 a is correct0 otherwise(6)correctness(a) =?b?Sa,b6=acorrect(a|b)|Sa| ?
1(7)where Sais the set of nodes in a?s subgraph.
Givenb being assigned its gold-standard polarity, if a isclassified correctly, then correct(a|b) is 1; other-wise 0.
|Sa| is the number of nodes in a?s sub-graph.
correctness(a) is the percentage of as-signments to a that are correct.
If it is 1, then ais correctly classified given the correct classifica-tion of any single node in its subgraph.For example, suppose there are three nodes ina subgraph, A, B and C. For A we (1) as-sign B its gold label and carry out propagationon the subgraph, (2) assign C its gold label andcarry out propagation again, then (3) calculatecorrectness(A).
Then the same process is re-peated for B and C.Some subgraphs contain only two nodes, theagent and the object.
In this case, graph propa-gation corresponds to single applications of twoimplicature rules.
Other subgraphs contain morenodes.
Two results are shown in Table 6.
One isthe result on the whole experiment data, the otheris the result for all nodes whose subgraphs havemore than two nodes.Dataset # subgraph correctnessall subgraphs 983 0.8874multi-node subgraphs 169 0.9030Table 6: Performance of graph model itself.As we can see, a node has an 89% chance ofbeing correct if there is one correct explicit sub-jectivity node in its subgraph.
If we only considersubgraphs with more than two nodes, the correct-ness chance is higher.
The results indicate that, ifgiven correct sentiments, the graph model will as-sign the unknown nodes with correct labels 90% ofthe time.
Further, the results indicate that the im-plicature rules are consistent for most of the timesacross the corpus.7 ConclusionsWe developed a graph-based model based onimplicature rules to propagate sentiments amongentities.
The model improves over explicitsentiment classification by 10 points in precisionand, in an evaluation of the model itself, we findit has an 89% chance of propagating sentimentscorrectly.
An important question for future workis under what conditions do the implicaturesnot go through in context.
Two cases we havediscovered involve Rule schema 3: the inferencetoward the agent is defeated if the action wasaccidental or if the agent was forced to perform it.We are investigating lexical clues for recognizingsuch cases.Acknowledgments.
This work was supportedin part by DARPA-BAA-12-47 DEFT grant#12475008 and National Science Foundationgrant #IIS-0916046.
We would like to thankRichard Johansson and Alessandro Moschitti forrunning their Opinion Extraction systems on ourdata.384ReferencesPranav Anand and Kevin Reschke.
2010.
Verb classesas evaluativity functor classes.
In InterdisciplinaryWorkshop on Verbs.
The Identification and Repre-sentation of Verb Features.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
In Proceedings ofthe 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 793?801, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Pradeep Dasigi, Weiwei Guo, and Mona Diab.
2012.Genre independent subgroup detection in online dis-cussion threads: A study of implicit attitude us-ing textual latent semantics.
In Proceedings of the50th Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers),pages 65?69, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.2013.
Benefactive and malefactive event and writerattitude annotation.
In 51st Annual Meeting of theAssociation for Computational Linguistics (ACL-2013, short paper).Song Feng, Jun Sak Kang, Polina Kuznetsova, andYejin Choi.
2013.
Connotation lexicon: A dash ofsentiment beneath the surface meaning.
In Proceed-ings of the 51th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), Sofia, Bulgaria, Angust.
Association for Com-putational Linguistics.Amit Goyal, Ellen Riloff, and Hal Daum III.
2012.
Acomputational model for plot units.
ComputationalIntelligence, pages 466?488.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press, April.Richard Johansson and Alessandro Moschitti.
2013.Relational features in fine-grained opinion analysis.Computational Linguistics, 39(3).Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool.Karo Moilanen and Stephen Pulman.
2007.
Senti-ment composition.
In Proceedings of RANLP 2007,Borovets, Bulgaria.Karo Moilanen, Stephen Pulman, and Yue Zhang.2010.
Packed feelings and ordered sentiments:Sentiment parsing with quasi-compositional polar-ity sequencing and compression.
In Proceedings ofthe 1st Workshop on Computational Approaches toSubjectivity and Sentiment Analysis (WASSA 2010),pages 36?43.J.
Pearl.
1982.
Reverend bayes on inference engines:A distributed hierarchical approach.
In Proceedingsof the American Association of Artificial IntelligenceNational Conference on AI, pages 133?136, Pitts-burgh, PA.Kevin Reschke and Pranav Anand.
2011.
Extractingcontextual evaluativity.
In Proceedings of the NinthInternational Conference on Computational Seman-tics, IWCS ?11, pages 370?374, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-dra De Silva, Nathan Gilbert, and Ruihong Huang.2013.
Sarcasm as contrast between a positive sen-timent and negative situation.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 704?714, Seattle,Washington, USA, October.
Association for Com-putational Linguistics.P.J.
Stone, D.C. Dunphy, M.S.
Smith, and D.M.Ogilvie.
1966.
The General Inquirer: A ComputerApproach to Content Analysis.
MIT Press, Cam-bridge.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentimentanalysis incorporating social networks.
In Proceed-ings of the 17th ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 1397?1405.
ACM.Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming zhou,and Ming Zhang.
2011.
Topic sentiment anaylsis intwitter: A graph-based hashtag sentiment classifica-tion appraoch.
In CIKM, pages 1031?1040.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language ann.
Language Resources andEvaluation, 39(2/3):164?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT/EMNLP, pages347?354.Jonathan S Yedidia, William T Freeman, and YairWeiss.
2005.
Constructing free-energy approx-imations and generalized belief propagation algo-rithms.
Information Theory, IEEE Transactions on,51(7):2282?2312.Lei Zhang and Bing Liu.
2011.
Identifying noun prod-uct features that imply opinions.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 575?580, Portland, Oregon, USA, June.Association for Computational Linguistics.F.
Z?u?niga and S. Kittil?a.
2010.
Introduction.
InF.
Z?u?niga and S. Kittil?a, editors, Benefactives andmalefactives, Typological studies in language.
J.Benjamins Publishing Company.385
