Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 1?12,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsMulti-document multilingual summarization corpus preparation, Part 1:Arabic, English, Greek, Chinese, RomanianLei LiBUPT, Chinaleili@bupt.edu.cnCorina ForascuRACAI, RomaniaUAIC, Romaniacorinfor@info.uaic.roMahmoud El-HajLancaster Univ., UKm.el-haj@lancaster.ac.ukGeorge GiannakopoulosNCSR Demokritos, GreeceSciFY NPC, Greeceggianna@iit.demokritos.grAbstractThis document overviews the strategy, ef-fort and aftermath of the MultiLing 2013multilingual summarization data collec-tion.
We describe how the Data Contrib-utors of MultiLing collected and gener-ated a multilingual multi-document sum-marization corpus on 10 different lan-guages: Arabic, Chinese, Czech, English,French, Greek, Hebrew, Hindi, Romanianand Spanish.
We discuss the rationale be-hind the main decisions of the collection,the methodology used to generate the mul-tilingual corpus, as well as challenges andproblems faced per language.
This paperoverviews the work on Arabic, Chinese,English, Greek, and Romanian languages.A second part, covering the remaining lan-guages, is available as a distinct paper inthe MultiLing 2013 proceedings.1 IntroductionSummarization has recently received the focusof media attention (Cahan, 2013; Shih, 2013), dueto a set of corporate buy-outs related to summariza-tion technology companies.
This trend of applyingsummarization is the result of a long research effortrelated to summarization.
Previously, especiallywithin the Text Analysis Conference (TAC) seriesof workshops (Dang, 2005; Dang, 2006; Dang andOwczarzak, 2008), multi-document summariza-tion has covered aspects of summarization suchas update summarization, guided summarizationand cross-lingual summarization.
In TAC 2011the MultiLing Pilot (Giannakopoulos et al 2011)was introduced: a combined community effort topresent and promote multi-document summariza-tion apporaches that are (fully or partly) language-neutral.
To support this effort an organizing com-mittee across more than six countries was assignedto create a multi-lingual corpus on news texts, cov-ering seven different languages: Arabic, Czech,English, French, Greek, Hebrew, Hindi.The Pilot gave birth to an active community ofresearchers, who provided the effort and know-how to realize a continuation of the original ef-fort: MultiLing 2013.
The MultiLing 2013 Work-shop, taking place within ACL 2013, built uponthe existing corpus of MultiLing 2011 to provideadditional languages and challenges for summa-rization systems.
This year 3 new languages wereadded: Chinese, Romanian and Spanish.
Further-more, more texts were added to most existing cor-pus languages (with the exception of French andHindi).In the following paragraphs we first overviewtheMultiLing tasks, for which the corpus was built(Section 2).
We then describe the rationale andstrategy applied for the corpus collection and cre-ation (Section 3).
We continue with special com-ments for the English, Greek, Chinese and Roma-nian languages (Section 4).
Finally, we summarizethe findings at the end of this paper (Section 5).
Wenote that a second paper (Elhadad et al 2013) de-scribes the language-specific notes related to therest of the MultiLing 2013 language contributions(Czech, Hebrew, Spanish).2 The MultiLing tasksThere are two main tasks (and a single-document multilingual summarization pilot de-scribed in a separate paper) in MultiLing 2013:Summarization Task This MultiLing task aimsto evaluate the application of (partially orfully) language-independent summarizationalgorithms on a variety of languages.
Eachsystem participating in the task was calledto provide summaries for a range of differ-ent languages, based on corresponding cor-pora.
In the MultiLing Pilot of 2011 the lan-1guages used were 7, while this year systemswere called to summarize texts in 10 differ-ent languages: Arabic, Chinese, Czech, En-glish, French, Greek, Hebrew, Hindi, Roma-nian, Spanish.
Participating systems were re-quired to apply their methods to a minimumof two languages.The task was aiming at the real problem ofsummarizing news topics, parts of which maybe described or may happen in different mo-ments in time.
We consider, similarly to Mul-tiLing 2011(Giannakopoulos et al 2011) thatnews topics can be seen as event sequences:Definition 1 An event sequence is a set ofatomic (self-sufficient) event descriptions, se-quenced in time, that share main actors, lo-cation of occurence or some other importantfactor.
Event sequences may refer to topicssuch as a natural disaster, a crime investiga-tion, a set of negotiations focused on a singlepolitical issue, a sports event.The summarization task requires to generatea single, fluent, representative summary froma set of documents describing an event se-quence.
The language of the document setwill be within the given range of 10 languagesand all documents in a set share the same lan-guage.
The output summary should be of thesame language as its source documents.
Theoutput summary should be between 240 and250 words.Evaluation Task This task aims to examine howwell automated systems can evaluate sum-maries from different languages.
This tasktakes as input the summaries generated fromautomatic systems and humans in the Sum-marization Task.
The output should be a grad-ing of the summaries.
Ideally, we would wantthe automatic evaluation to maximally corre-late to human judgement.The first task was aiming at the real problem ofsummarizing news topics, parts of which may bedescribed or happen in different moments in time.The implications of including multiple aspects ofthe same event, as well as time relations at a vary-ing level (from consequtive days to years), are stilldifficult to tackle in a summarization context.
Fur-thermore, the requirement for multilingual appli-cability of the methods, further accentuates the dif-ficulty of the task.The second task, summarization evaluation hascome to be a prominent research problem, based onthe difficulty of the summary evaluation process.While commonly used methods build upon a fewhuman summaries to be able to judge automaticsummaries (e.g., (Lin, 2004; Hovy et al 2005)),there also exist works on fully automatic evalua-tion of summaries, without human?model?
sum-maries (Louis and Nenkova, 2012; Saggion et al2010).
The Text Analysis Conference has a sepa-rate track, named AESOP (Dang and Owczarzak,2009) aiming to test and evaluate different auto-matic evaluation methods of summarization sys-tems.Given the tasks, a corpus needed to be gener-ated, that would be able to:?
provide input texts in different languages tosummarization systems.?
provide model summaries in different lan-guages as gold standard summaries, to alsoallow for automatic evaluation using model-dependent methods.?
provide human grades to automatic and hu-man summaries in different languages, tosupport the testing of summary evaluationsystems.In the following section we show how these re-quirements were met in MultiLing 2013.3 Corpus collection and generationThe overall process of creating the corpus ofMultiLing 2013 was, similarly to MultiLing 2011,based on a community effort.
The main processesconsisting of the generation of the corpus are asfollows:?
Selection of a source corpus in a single lan-guage (see Section 3.1).?
Translation of the source corpus to differentlanguages (see Section 3.2).?
Human summarization of corpus topics perlanguage (see Section 3.3).?
Evaluation of human summaries, as well as ofsubmitted system runs (see Section 3.4).2We should note here that the translation is meantto provide a parallel corpus of texts across differ-ent languages.
The main ideas behind this first ap-proach are that:?
the corpus will allow performing secondarystudies, related to the human summarizationeffort in different languages.
Having a paral-lel corpus is such cases can prove critical, inthat it provides a common working base.?
we may be able to study topic-relatedor domain-related summarization difficultyacross languages.?
the parallel corpus highlights language-specific problems (such as ambiguity in wordmeaning, named entity representation acrosslanguages).?
the parallel corpus fixes the setting in whichmethods can show their cross-language ap-plicability.
Examining significantly varyingresults in different languages over a parallelcorpus offers some background on how to im-prove existing methods and may highlight theneed for language-specific resources.On the other hand, the significant organizationaland implementaion effort required for the transla-tion (please see per language notes in the corre-sponding sections) may lead to a comparable (vs.parallel) corpus in future MultiLing endeavours.Given the tasks at hand, the Contributors firstperformed the selection of the texts that would beused for the MultiLing tracks, as described below.3.1 Selecting the corpusTo support the summarization task, we neededa dataset of freely available news texts (to allowreuse), covering news topics that would containevent sequences.
Based on the ?
apparently good?
decisions of the MultiLing 2011 Pilot, we de-termined that each event sequence in the corpusshould contain at least three distinct atomic events,to imply an underlying story.The dataset created was based on the WikiNewssite1, which covers a variety of news topics,while allowing the reuse of the texts based on theCreative Commons Licence.
An example topicwith two sample texts derived from the originalWikiNews documents is provided in Figure 1.
It1See http://www.wikinews.org.can be seen clearly that the event in the examplehas significantly different aspects, since an earth-quake caused a radiation leak, via a series of inter-actions in the real world.
Systems would normallybe expected to express both aspects of the eventwith adequate information.During the selection of the source texts, wefirst gathered an English corpus of 15 topics (10of which were already available from MultiLing2011), each containing 10 texts.
Wemade sure thateach topic contained at least one event sequence.From the original HTML text we only kept unfor-matted content text, without any images, tables orlinks.While choosing topics we made sure that thereexisted topics:?
with varying time granularity.
Some top-ics happen within days (e.g., sports events),while others within years (e.g., Iranian nu-clear policy and international negotiations).?
covering various domains.
There existed top-ics related to international politics, sports,natural disasters, political campaigns andelections.?
with a varying number of apparent actors.Some topics focus on specific individuals(e.g., campaign of Barack Obama) while oth-ers refer to numerous participants (e.g., para-Olympics and participating athletes).?
with numeric aspects, that would change overtime.
Such examples are natural disasters(with the number of estimated victims, orthe estimated magnitude of earthquakes) andsports events (number of medals per country).?
with an important time dimension.
For ex-ample during the Egyptian riots, the order ofevents is non-trivial to determine from text.Determining the order of events is also verychallenging while following multi-day sportsevents.
Ignoring the time dimension in suchtopics is expected to worsen the performanceof summarization systems.Given the English texts, we now needed to pro-vide corresponding texts in all the languages usedin MultiLing.
To this end, we organized a transla-tion process, which is elaborated below.3Fukushima reactor suffers multiple fires, radiation leak confirmedTuesday, March 15, 2011Fires broke out at the Fukushima Daiichi plant's No.
4 reactor in Japan onTuesday, according to the Tokyo Electric Power Company.
The first fire causeda leak of concentrated radioactive material, according to the Japanese primeminister, Naoto Kan.The first fire broke out at 9:40 a.m. local time on Tuesday, and was thoughtto have been put out, but another fire was discovered early on Wednesday,believed to have started because the earlier one had not been fullyextinguished.In a televised statement, the prime minister told residents near the plantthat "I sincerely ask all citizens within the 20 km distance from the reactorto leave this zone."
He went on to say that "[t]he radiation level has risensubstantially.
The risk that radiation will leak from now on has risen.
"Kan warned residents to remain indoors and to shut windows and doors to avoidradiation poisoning.The French Embassy in Japan reports that the radiation will reach Tokyo in 10hours, with current wind speeds.Death toll rises from Japan quakeSunday, March 13, 2011The death toll from the earthquake and subsequent tsunami that hit Japan onFriday has risen to more than a thousand, with many people still missing,according to reports issued over the weekend.While Japan's police says that only 637 are confirmed dead, media reports saythat over a thousand people have been killed, with several hundred bodiesstill being transported.
Thousands more are still unaccounted for; in the townof Minamisanriku, Miyagi Prefecture alone, up to 10,000 people are missing.Four trains that were on the coast have yet to be located.In the aftermath of the disaster, evacuations of around 300,000 people havetaken place; more evacuations are likely in the wake of concerns over adamaged nuclear power plant.
According to Prime Minister Naoto Kan, around3,000 people have been rescued thus far.
50,000 troops from the Japanesemilitary have been deployed to assist in rescue efforts.The tsunami generated by the quake has destroyed communities along Japan'sPacific coast, with up to 90% of the houses in some towns having beendestroyed; at least 3,400 structures have been destroyed in total.
Fires havealso sprung up among the impacted areas.Figure 1: Topic Sample (Japan Earthquake and Nuclear Threat)43.2 Translating the corpusThe English texts selected in the selection stepwere translated using a sentence-by-sentence ap-proach to each of the other languages: Arabic, Chi-nese, Czech, French, Greek, Hebrew, Hindi, Ro-manian, Spanish.
This year there was no supportfor the Hindi and French languages, which stillcontain 10 topics.
Also the Chinese language cov-ers 10 topics.
All the remaining languages cover15 topics.During the translation process, the guidelineswere minimal:Given the source language text A,the translator is requested to translateeach sentence in A, into the target lan-guage.
Each target sentence should keepthe meaning from the source language.Some additional, optional guidelines (providedin the Appendix) were provided by the Romanianlanguage Contributors, proposing ways to react todate formatting, name translations, etc.During the translation process, the translatorswere also asked to keep track of the time spent ondifferent stages of the process: first full reading ofthe source document, translation and verification.The whole set of translated documents togetherwith the original English document set will be re-ferred to as the Source Document Set.
Given thecreation process, the Source Document Set con-tains a total of 1350 texts (vs. 700 from MultiLing2011): 7 languages with 15 topics per language, 10texts per topic for a total of 1050 texts; 3 languageswith 10 topics per language, 10 texts per topic fora total of 300 texts.This Source Document Set was provided to par-ticipating systems as input for their summarizationsystems.
It was also provided to human summa-rizers, so that they would provide human, modelsummaries on each topic and each language.
Thehuman summarization process is described in thefollowing section.3.3 Summarizing topicsIn the summarization step of the corpus creationdifferent summarizers were asked to generate onesummary per topic in each language.
The follow-ing guidelines were provided to help the summa-rizers:The summarizer will read the wholeset of texts at least once.
Then, the sum-marizer should compose a summary,with a minimum size of 240 and a maxi-mum size of 250 words.
The summaryshould be in the same language as thetexts in the set.
The aim is to create asummary that covers all the major pointsof the document set (what is major isleft to summarizer discretion).
The sum-mary should be written using fluent, eas-ily readable language.
No formatting orother markup should be included in thetext.
The output summary should be aself-sufficient, clearly written text, pro-viding no other information than what isincluded in the source documents.After summarization, human evaluation wasperformed.
The evaluation covered human sum-maries, but also summarization system submis-sions.
The details are provided in the followingparagraphs.3.4 Evaluating the summariesThe evaluation of summaries was performedboth automatically and manually.
The manualevaluation was based on the Overall Responsive-ness (Dang and Owczarzak, 2008) of a text, as de-scribed below, and the automatic evaluation usedthe ROUGE (Lin, 2004) and AutoSummENG-MeMoG (Giannakopoulos et al 2008; Gian-nakopoulos and Karkaletsis, 2011) and NPowER(Giannakopoulos and Karkaletsis, 2013) methodsto provide a grading of performance.For the manual evaluation the human evaluatorswere provided the following guidelines:Each summary is to be assigned aninteger grade from 1 to 5, related to theoverall responsiveness of the summary.We consider a text to be worth a 5, ifit appears to cover all the important as-pects of the corresponding document setusing fluent, readable language.
A textshould be assigned a 1, if it is either un-readable, nonsensical, or contains onlytrivial information from the documentset.
We consider the content and thequality of the language to be equally im-portant in the grading.As indicated in the task, the acceptable limits forthe word count of a summary were between 2405and 250 words2 (inclusive).
In the case of Chi-nese there was a problem determining the numberof words.
Based on the model summaries gatheredwe (arbitrarily) set the upper limit of length in bytesof the UTF8-encoded summary files to 750 bytes.4 Language specific notesIn the following paragraphs we providelanguage-specific overviews related to the corpuscontribution effort.
The aim of these overviews isto provide a reusable pool of knowledge for futuresimilar efforts.In this document we elaborate on Arabic, En-glish, Greek, Chinese and Romanian languages.
Asecond document (Elhadad et al 2013) elaborateson the rest of the languages.4.1 Arabic languageThe preparation of the Arabic corpus for the2013 MultiLing Summarization tasks was organ-ised jointly by Lancaster University and the Uni-versity of Essex in the United Kingdom.
20 peopleparticipated in translating the English corpus intoArabic, validating the translation and summarisingthe set of related Arabic articles.
The participantsare studying, or have finished a university degreein an Arabic speaking country.
The participants?age ranged between 21 and 32 years old.The participants translated the English datasetinto Arabic.
For each translated article anothertranslator validated the translation and fixed anyerrors.
For each of the translated articles, threemanual summaries were created by three differentparticipants (human peers).
Amid the summarisa-tion process the participants evaluated the qualityof the generated summary by assigning a score be-tween one (unreadable summary) and five (fluentand readable summary).
No self evaluation wasallowed.The average time for reading the English newsarticles by the Arabic native speaker participantswas 5.58minutes.
The average time it took them totranslate these articles into Arabic was 42.18 min-utes and to validate each of the translated Arabicarticles the participants took 5.25 minutes on aver-age.For the summarisation task the average time forreading the set of related articles (10 articles per2The count of words was provided by thewc -w linux com-mand.each set) was 34.44 minutes.
The average time forsummarising each set was 25.41 minutes.4.1.1 Problems and ChallengesMany difficulties arose during the creation ofthe gold-standard summaries.
Some are language-dependent and relate to the complexity of the Ara-bic language.
This required a special attention tobe paid while creating the summaries.One problem concerns the handling of monthnames in Arabic.
There are twoways of translatingmonth names into Arabic:?
using the Arabic transliteration of theAramic (Syriac) month names (e.g.
?May?,?PAK@?, ?Ayyar?).?
using the Arabic transliteration of theEnglish month names (e.g.
?May?,??
KA?
?, ?Mayo?
).Some of the participants found it difficult totranslate sentences where they believe they containan ambiguous structure.
For example: ?She saidIranian security Chief Saeed Jalili had requested ameeting in a telephone call?.
The translators (whoare Native Arabic speakers) found it a bit hard tochoose between two translations:?
?Saeed Jalili asked to schedule a telephonemeeting??
?Saeed Jalili phoned to request a meeting?.Arabic sentence structure is highly complex andtherefore great attention must be paid when mov-ing forward or pushing back phrases within a sen-tence, as such shifts are likely to change the over-all meaning.
In addition, the use of passive voice,metaphors and idioms in the original English texthas captured the translators attention, as the mean-ing in such cases takes precedence over the literaltranslation.During the summarisation process, a sum-mariser found that ordering a set of related articles(discussing the same topic) in chronological ordersimplifies the summarisation process.Many participants found it difficult to meet the250 summary word-limit as they believe 250 is notenough to cover all the essential information de-rived from a given set of documents.Another problem concerns ?proper nouns?
whentranslating into Arabic.
The Arabic electronic dis-course would sometimes show two variants of one6English proper noun, as in the case with the name?Francois Hollande?.
Mostly in such cases, thevariant used in popular websites such as the Arabicversion Wikipedia was adopted.Finally, there were many questions by the par-ticipants on whether to create abstractive or extrac-tive summaries.4.2 Chinese languageBelow we provide an overview of the organiza-tional effort and comments on a variety of prob-lems related to the preparation of the Chinese cor-pus for MultiLing 2013.4.2.1 OrganizationFirst, the Chinese language team translated twotexts from English to Chinese together in order tomake an original unified example for each trans-lator, including file format, title format, date for-mat, named entity translation, etc.
Second, we as-signed different set of news texts as specific taskfor each translator.
For each news topic, we usu-ally split the ten texts to two different translators atleast, so as to bring more thoughts from differentviewers and prepare enough for later discussion.During the process of each translator, they wereasked to note any problems in a ?problem file?, in-cluding the source English part and the target Chi-nese part.
Third, we summed up a big problemfile from each translator.
After a series of discus-sions, we classified the problems into different cat-egories and solved some of the problems success-fully.
The remaining problems were noted down ina detailed report to the organizer of the MultiLing2013Workshop of ACL 2013, as a knowledge poolfor future efforts.
Fourth, we performed the verifi-cation task.
During the process, we made sure thatfor each text, the verifier was different from thetranslator.
Also each verifier was demanded to logany problems.
Fifth, we did another discussion fornew problems coming from the verification phase.Some problems were solved; others were added tothe detailed report.
Sixth, we generated the neededresult files and made sure that they were in the re-quested format (e.g., UTF8, no-BOM, plain textfiles for summaries).For the process of summarization and humanevaluation, first, we assigned three summarizers,each of which needed to read all the ten topics andwrite a summary for each topic.
Second, we as-signed three evaluators, making sure that for eachsummary, the evaluator was different from thesummarizer.
Third, we made a discussion aboutthe process of summarization and evaluation.
Allagreed that summarization and evaluation weremuch easier than translation.There were mainly two common problems.
Onewas about the summary length.
So we set a uni-fied method for length checking.
The other prob-lem was more complex, which was that therewere many different information in the original tentexts, but the result summary was limited to 250words, so it was very difficult to choose the mostimportant information.
As a result, some infor-mation could be lost in final summaries.
At thesame time, we also found minor problems regard-ing the translation, improved the translation filesand updated the detailed report about the problemswe faced.4.2.2 Problems and proposed solutionsIn fact, related problems mainly came up fromthe task of translation.
Most of them were com-mon questions of the translators and language-dependent problems that needed special care.
Herewe only list the main categories of problems 3.First, there were problems with the translation ofperson names.
There are several sub-problemshere:?
There are some person names which are notso popular, we could not find a result, sowe finally keep the unknown English wordsamong Chinese words.?
There is no specific separator between firstname, middle name and family name in En-glish, only normal space.
But in Chinese, weusually add a separator ??
?
between them.??
There is also some ambiguity in person nameto us, since we may be not quite familiar withsome specific knowledge of news related do-main.
??
There are also some person names whichseem to contain non-English characters.These names are more difficult for us, so wejust keep most of them as the original formatin English news.
??
There are some person names with only onecapitalized character and a dot in the middle3A more detailed report has been submitted to the orga-nizer of the Workshop.7part.
It?s really difficult for us to find a cor-responding Chinese translation for it, so wejust keep it as the original English format inthe Chinese translations and keep the originalEnglish name in the following brackets.Second, the translation for the English name ofsome websites, companies, organizations, etc, cancause problems.
Since the full name may be toolong for news reports, most of them also have oc-curred in corresponding simple format of abbre-viation.
Some of them are famous enough thatwe have a popular Chinese translation for them,while others are not so popular.
So we decidedthat for unknown ones, we just reserve the Englishname, but for those known ones, we add the Chi-nese translation and keep some of the English ab-breviation.Third, the translation of time expressions is non-trivial.
In English, the order usually used is: Week-day, Month Day, Year.
But according to Chinesehabit, we mention time usually in the following or-der: Year Month Day, Weekday.Fourth, translation of locations names may notexist.
There are many location names in thesenews texts.
We tried to find their Chinese transla-tion from many resources, but there are still somedifficult ones left.Fifth, there are someEnglishwords in the sourcetexts which seem to be unrelated to other sentencesin the news text (these may be text captions of pho-tos in the source WikiNews articles).
We just leftthem as they were.Sixth, there are some sentences which are diffi-cult to understand clearly because the context andstructure are ambiguous.
In these cases, we madea Chinese translation which seems best to us.The above problems conclude the Chinese lan-guage contribution language-specific notes.4.3 English and Greek languagesThe effort related to the organization of the En-glish and Greek languages was essentially equiva-lent to the MultiLing 2011 pilot (Giannakopouloset al 2011).
This year 5 new topics were addedto the two languages.
The effort for English wasreduced because no translation was needed.
In thefollowing subsections we elaborate on the organi-zation details and the problems faced during thedifferent subprocesses of the corpus creation.4.3.1 OrganizationA total of 7 people (being either MSc students,or researchers, all with fluency in English andGreek) were recruited for the two languages.
Aninitial meeting was held to provide the basic guide-lines and discuss questions on the translation pro-cess.
Subsequently, e-mail communication andperiodic conferences were used to assign the nexttasks, related to summarization and evaluation.For the purposes of meaningful assignment wecreated and used an automatic assignment script,that allows pre-allocating specific texts to workers(for any of the required tasks), while it automati-cally distributes work according to the availabilityof workers.
The script avoids assigning workers totexts/tasks more than once.In the evaluation process, we made sure(through pre-assignments) that no human wouldjudge their own summary.
It would have increasedefficiency, if we had ascertained that human sum-marization would occur right after the translationof the texts.The average time for reading the English newsarticles by the Greek native speaker participantswas around 8 minutes.
The average time it tookthem to translate these articles into Greek wasaround 48 minutes on average (with a couple ofextreme cases exceeding 100 minutes, due to tech-nical terminology, which was difficult to trans-late).
The summarization time of the new topicsin English was around 24 minutes per topic (plusan average of 8 minutes allocated to reading thesource texts).
For Greek the summary time wasaround 50 minutes per topic (we note that the sum-marizers?
groups for English and Greek were onlyminimally overlapping).
In the Greek case, somedeeper search showed that a single summarizerheavily biased the distribution of times to highervalues.To follow the progress of tasks, a generic projectmanagement tool was used.
However, the toolproved insufficient in the micro-planning of the ef-fort (individual assignments tracking).
It wouldclearly make sense to use an ad-hoc designed sys-tem for planning and implementation of the effort.4.3.2 Problems and proposed solutionsThe main problems identified by contributorsfor Greek and English translation were related towell-known translation problems: named entitytranslation, date formatting, highly technical ordomain specific terminology, ambiguous terms in8the source text.
Additional effort from translatorsprovided solutions to these problems according tocommon practice in the translation domain.The summarization effort indicated a few inter-esting points.
Even though summarizers have theirindividual method for summarizing, some com-mon practices and notes arise:?
A non-thorough glimpse of the source textshelps determine the overall topic.?
Time ordering is important in several cases,thus time ordering of the source texts is ap-plied before the summarization process itself.The process is non-trivial even for humans.?
An initial summary which may be longer thanthe target size is created and several reductivetransformations are applied.
The 250 wordlimit proved critical and challenging, in that itforced summarizers to carefully choose infor-mation, essentially not covering the whole setof information from the source documents.?
Syntactic compression and rewriting is thelast line of summarization, when it is obviousthat more compression is needed.As related to the evaluation process, we notedthat there exists an inherent tendency for evalua-tors to determine whether a human or a machineperformed the summarization.
There were caseswhere evaluators altered their grading, becausethey inferred that not all texts were from humansor not all were from machines.
We had noted thisphenomenon also in MultiLing 2011.
There areseveral cases where the evaluator also tries to de-termine the strategy of the system and, when oneunderstands the underlying strategy, this may biasthe grade.
It would be interesting to evaluate thisbias in the future.Some additional notes are related to problemswith the organization of the effort:?
A distributed work environment that wouldhelp track the progress of individuals andassignment of new tasks without significantcommunication effort, would have been veryhelpful.?
The assignment script was really critical infacilitating the organization of the effort andwe plan to make it publicly available to allowreuse.Overall, the collection and generation of the cor-pus was a very challenging effort, both in termsof organization and individual questions arising.However, next steps can build upon the lessonslearnt, if the effort is well documented and the doc-uments are freely and openly shared.4.4 Romanian languageAtMultiLing 2013, Romanian was addressed asa language for the first time.
Following the Callfor Contributors launched by the MultiLing orga-nizers and based on the experience in the QA @CLEF4 evaluation campaign (Pe?as et al 2012),we started the data collection process workingwitha group of ten MSc students in ComputationalLinguistics from our Faculty, later adding anotherMSc student to the working group.
Below we pro-vide some notes on the translation and generationof human summaries processes:?
The translation, including verification, ofthe 150 WikiNews text documents from En-glish into Romanian, was performed in a dis-tributed context, theoretically based on an ar-chitecture like the one described in (Alboaieet al 2003).
Each student received one topic(10 documents) to be translated, based on aset of guidelines.
We devised guidelines totackle any language-dependent problems thatneed special care, and they were improved af-ter each solution received from the studentsand based also on their questions.
The fullguidelines are provided in the Appendix ofthis document.We started with the following workflow: stu-dent A receives 10 English documents to betranslated and summarized and sends the re-sults to the organizer; another student, B, re-ceives the English documents and the Roma-nian translations (made by student A) and s/heverifies the translations and prepares anothersummary.
Finally, another student, C, re-ceives from the organizer the 10 Romaniandocuments and s/he prepares the third sum-mary of a given topic.Since the task proved to be very time-consuming for the students, all the last fivetopics (the ones introduced this year) weregiven to one student and then the translationswere verified by the organizer.4See http://celct.fbk.eu/ResPubliQA/index.php for more information.9?
The generation of human summaries was per-formed immediately after the translation.
Foreach topic, the aim was to create a summarythat covers all the major points of the topic(what is major was left to summarizer?s dis-cretion), being a self-sufficient, clearly writ-ten text, providing no other information thanwhat is included in the source documents.The students were given no specific recom-mendations regarding the type of summarythey should produce, e.g.
an abstract ver-sus an extract (Mani and Maybury, 1999),but they were specifically instructed to under-stand the main aspects of summarization.5 Conclusions and lessons learntThe corpus generated throughout the MultiLingcorpus preparation provides a benchmark datasetfor multilingual summarization.
It tries to cap-tured interesting, representative events, coveringa variety of well-known news events around theworld.
The recent corporate interest in summa-rization, in conjunction with the ever-present in-crease of information flow from the Web and in-formation redundancy, show that having a scien-tifically plausible set of evaluation tools for sys-tems can help bring useful summarization systemsto a wide audience.
MultiLing functions as a fo-cus point for multilingual summarization researchand this document described the methods used tocreate a commonly accepted multilingual, multi-document summarization corpus.Concerning thoughts on the future work of Mul-tiLing, there are some points that have been raisedby Contributors that we reproduce in the followingsentences:?
In the translation phase, it would be useful tohave translators for different languages dis-cuss directly about some difficult cases, suchas some ambiguous words, phrases and sen-tences, especially when they are expressed insome language-specific way.?
It would be very interesting to exploit the po-tential of comparable corpora, and not onlyof the parallel ones, especially if we considerthe multilingual setting of MultiLing 2013.This means that the data should be collectedstarting from a given topic and each languagecontributor should find 10 documents on thatgiven topic in his/her language.?
Creating a collaborative platform for build-ing and improving summarization corporacould significantly facilitate the corpus build-ing process for future efforts.We remind the reader that a second paper (El-hadad et al 2013) addresses the problems andchallenges faced in the remaining languages ac-tively contributed to in MultiLing 2013 (Czech,Hebrew and Spanish), thus completing the lessonslearnt from theMultiLing 2013 contribution effort.Extended technical reports recapitulating discus-sions and findings from the MultiLing Workshopwill be available after the workshop at the Multi-Ling Community website5, as an addenum to theproceedings.AcknowledgmentsMultiLing is a community effort and this com-munity is what keeps it alive and interesting.
Wewould like to thank Contributors for their organi-zational effort, which made MultiLing possible inso many languages and all volunteers, helpers andresearchers that helped realize individual steps ofthe process.
A more detailed reference of the con-tributor teams can be found in Appendix A.The MultiLing 2013 organization has been par-tially supported by the NOMAD FP7 EU Project(cf.
http://www.nomad-project.eu).References[Alboaie et al003] Lenuta Alboaie, Sabin C Buraga,and S?nica Alboaie.
2003. tuBiG?a layered infras-tructure to provide support for grid functionalities.Omega, 2:3.
[Cahan2013] Adam Cahan.
2013.
Yahoo!
To AcquireSummly http://yodel.yahoo.com/blogs/general/yahoo-acquire-summly-13171.html, March 25th.
[Dang and Owczarzak2008] H. T. Dang andK.
Owczarzak.
2008.
Overview of the TAC2008 update summarization task.
In TAC 2008Workshop - Notebook papers and results, pages10?23, Maryland MD, USA, November.
[Dang and Owczarzak2009] Hoa Trang Dang andK.
Owczarzak.
2009.
Overview of the tac 2009summarization track, Nov.[Dang2005] H. T. Dang.
2005.
Overview of DUC2005.
In Proceedings of the Document Under-standing Conf.
Wksp.
2005 (DUC 2005) at the5See http://multiling.iit.demokritos.gr/pages/view/1256/proceedings-addenum)10Human Language Technology Conf./Conf.
on Em-pirical Methods in Natural Language Processing(HLT/EMNLP 2005).
[Dang2006] H. T. Dang.
2006.
Overview of DUC2006.
In Proceedings of HLT-NAACL 2006.
[Elhadad et al013] Michael Elhadad, SabinoMiranda-Jim?nez, Josef Steinberger, and GeorgeGiannakopoulos.
2013.
Multi-document multi-lingual summarization corpus preparation, part 2:Czech, hebrew and spanish.
In MultiLing 2013Workshop in ACL 2013, Sofia, Bulgaria, August.
[Giannakopoulos and Karkaletsis2011] George Gi-annakopoulos and Vangelis Karkaletsis.
2011.Autosummeng and memog in evaluating guidedsummaries.
In TAC 2011 Workshop, Maryland MD,USA, November.
[Giannakopoulos and Karkaletsis2013] George Gi-annakopoulos and Vangelis Karkaletsis.
2013.Summary evaluation: Together we stand npower-ed.In Computational Linguistics and Intelligent TextProcessing, pages 436?450.
Springer.
[Giannakopoulos et al008] George Giannakopoulos,Vangelis Karkaletsis, George Vouros, and Panagio-tis Stamatopoulos.
2008.
Summarization systemevaluation revisited: N-gram graphs.
ACM Trans.Speech Lang.
Process., 5(3):1?39.
[Giannakopoulos et al011] G. Giannakopoulos,M.
El-Haj, B. Favre, M. Litvak, J. Steinberger,and V. Varma.
2011.
TAC 2011 MultiLing pilotoverview.
In TAC 2011 Workshop, Maryland MD,USA, November.
[Hovy et al005] E. Hovy, C. Y. Lin, L. Zhou, andJ.
Fukumoto.
2005.
Basic elements.
[Lin2004] C. Y. Lin.
2004.
Rouge: A package forautomatic evaluation of summaries.
Proceedings ofthe Workshop on Text Summarization Branches Out(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and AniNenkova.
2012.
Automatically assessing ma-chine summary content without a gold standard.Computational Linguistics, 39(2):267?300, Aug.[Mani and Maybury1999] Inderjeet Mani and Mark TMaybury.
1999.
Advances in automatic text sum-marization.
the MIT Press.
[Pe?as et al012] Anselmo Pe?as, Eduard H. Hovy,Pamela Forner, ?lvaro Rodrigo, Richard F. E. Sut-cliffe, Caroline Sporleder, Corina Forascu, YassineBenajiba, and Petya Osenova.
2012.
Overview ofqa4mre at clef 2012: Question answering for ma-chine reading evaluation.
In CLEF (Online WorkingNotes/Labs/Workshop).
[Saggion et al010] H. Saggion, J. M. Torres-Moreno,I.
Cunha, and E. SanJuan.
2010.
Multilingual sum-marization evaluation without human models.
InProceedings of the 23rd International Conferenceon Computational Linguistics: Posters, page 1059?1067.
[Shih2013] Gerry Shih.
2013.
Sound Famil-iar?
After Yahoo Buys Summly, GoogleBuys News Summarization App Waviihttp://www.huffingtonpost.com/2013/04/24/google-wavii_n_3143116.html, April23rd.
[Tufis et al004] Dan Tufis, DanCristea, and Sofia Sta-mou.
2004.
Balkanet: Aims, methods, results andperspectives.
a general overview.
Romanian Journalof Information science and technology, 7(1-2):9?43.Appendix A: Contributor teamsArabic language teamTeam members Mahmoud El-Haj (LancasterUniversity, UK); Ans Alghamdi, MahaAlthobaiti (Essex University, UK); AhmadAlharthi (King Saud University, SaudiArabia)Contact e-mail m.el-haj@lancaster.ac.ukChinese language teamTeam members Lei Li, Wei Heng, Jia Yu, Yu Liu,Qian LiTeam affiliation Center for Intelligence Scienceand Technology (CIST), School of Com-puter Science,Beijing University of Posts andTelecommunications,Postal Address P.O.Box 310, Beijing Universityof Posts and Telecommunications, XituchengRoad 10, Haidian District, Beijing, ChinaContact e-mail leili@bupt.edu.cnEnglish and Greek languages teamTeam members Zoe Angelou, ArgyroMavridakis, Valentini Mellas, EfrosiniZacharopoulou, George Kiomourtzis,George Petasis, George GiannakopoulosTeam affiliation NCSR?Demokritos?Postal Address Institute of Informatics andTelecommunications, Patriarchou Grigoriouand Neapoleos Str., Aghia Paraskevi Attikis,Athens, GreeceContact e-mail ggianna@iit.demokritos.gr11Romanian language teamTeam members Corina Forascu, Raluca Moi-seanu; Ana Maria Timofciuc, AlexandraCristea, Alexandrina Sbiera, Bogdan Puiu,and Tudor Popoiu; other contributors to thetask were Monica Ancu?a, Romic?
Iarca,Claudiu Popa, and Cosmin Vl?du?uTeam affiliation UAIC, RomaniaContact e-mail corinfor@info.uaic.roAppendix B: Romanian guidelines1.
Translation equivalents belonging to the samepart of speech should be used.
The Romanianwords should be as?closest?as possible totheir English equivalents: If the English wordhas as equivalent a cognate in Romanian, thisone should be used.
The Romanian wordnet6(Tufis et al 2004) should be used for prob-lematic situations.
If the English word doesn?t have a Romanian cognate, then the transla-tor should not try to paraphrase it.
Example:The English ?sporadic?will be translatedinto?sporadic?, even though the translatorwould be tempted to use instead?izolat?or?rar?.
It is not recommended to give trans-lations such as ?mai pu?in?or ?mai rar?.2.
English words should not be omitted andwords which are not in the original Englishtext should not be added because of stylisticreasons.
Example:?The Telegraph?will benot translated when it refers to the newspa-per and, moreover, the translators will not in-troduce an explanation, like?cotidianul TheTelegraph?
[English: The Telegraph newspa-per].3.
The Romanian diacritics have to be used, inUTF-8 encoding.4.
The translators must preserve as much as pos-sible the tenses of the English verbs.
Any dis-agreement from the English tense is allowedfor linguistic reasons only (Romanian spe-cific constructions), and not for stylistic ones.5.
The translators will preserve the format ofdates, times, numbers.
For example, for theissuing date of an article being ?March 25,6See http://www.racai.ro/wnbrowser/.2010?, the Romanian translation will be?25martie 2010?and NOT ?Martie, 25, 2010?OR?25 Martie, 2010?.6.
The format of the numbers should follow theRomanian convention with respect to the dec-imal separator, which is comma (,), and notthe period (.
), like in English-speaking coun-tries.7.
The unclear or unsure situations encounteredby the translators will be separately recordedin a file, indicating the provenance of the doc-ument, the ID used for the problematic sen-tence and the commentaries/suggestions.12
