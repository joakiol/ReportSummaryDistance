Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 307?312,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsSHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for TranslationQuality EstimationDaniel Beck and Kashif Shah and Lucia SpeciaDepartment of Computer ScienceUniversity of SheffieldSheffield, United Kingdom{debeck1,kashif.shah,l.specia}@sheffield.ac.ukAbstractWe describe our systems for the WMT14Shared Task on Quality Estimation (sub-tasks 1.1, 1.2 and 1.3).
Our submissionsuse the framework of Multi-task Gaus-sian Processes, where we combine multi-ple datasets in a multi-task setting.
Due tothe large size of our datasets we also ex-periment with Sparse Gaussian Processes,which aim to speed up training and predic-tion by providing sensible sparse approxi-mations.1 IntroductionThe purpose of machine translation (MT) qualityestimation (QE) is to provide a quality predictionfor new, unseen machine translated texts, with-out relying on reference translations (Blatz et al.,2004; Specia et al., 2009; Bojar et al., 2013).
Acommon use of quality predictions is the decisionbetween post-editing a given machine translatedsentence and translating its source from scratch,based on whether its post-editing effort is esti-mated to be lower than the effort of translating thesource sentence.The WMT 2014 QE shared task defined a groupof tasks related to QE.
In this paper, we de-scribe our submissions for subtasks 1.1, 1.2 and1.3.
Our models are based on Gaussian Pro-cesses (GPs) (Rasmussen and Williams, 2006),a non-parametric kernelised probabilistic frame-work.
We propose to combine multiple datasetsto improve our QE models by applying GPs ina multi-task setting.
Our hypothesis is that us-ing sensible multi-task learning settings gives im-provements over simply pooling all datasets to-gether.Task 1.1 focuses on predicting post-editing ef-fort for four language pairs: English-Spanish(en-es), Spanish-English (es-en), English-German(en-de), and German-English (de-en).
Each con-tains a different number of source sentences andtheir human translations, as well as 2-3 versionsof machine translations: by a statistical (SMT)system, a rule-based system (RBMT) system and,for en-es/de only, a hybrid system.
Source sen-tences were extracted from tests sets of WMT13and WMT12, and the translations were producedby top MT systems of each type and a humantranslator.
Labels range from 1 to 3, with 1 in-dicating a perfect translation and 3, a low qualitytranslation.The purpose of task 1.2 is to predict HTERscores (Human Translation Error Rate) (Snoveret al., 2006) using a dataset composed of 896English-Spanish sentences translated by a MT sys-tem and post-edited by a professional translator.Finally, task 1.3 aims at predicting post-editingtime, using a subset of 650 sentences from theTask 1.2 dataset.For each task, participants can submit two typesof results: scoring and ranking.
For scoring, eval-uation is made in terms of Mean Absolute Error(MAE) and Root Mean Square Error (RMSE).
Forranking, DeltaAvg and Spearman?s rank correla-tion were used as evaluation metrics.2 ModelGaussian Processes are a Bayesian non-parametricmachine learning framework considered the state-of-the-art for regression.
They assume the pres-ence of a latent function f : RF?
R, which mapsa vector x from feature space F to a scalar value.Formally, this function is drawn from a GP prior:f(x) ?
GP(0, k(x,x?
)),which is parameterised by a mean function (here,0) and a covariance kernel function k(x,x?).
Eachresponse value is then generated from the functionevaluated at the corresponding input, yi= f(xi)+?, where ?
?
N (0, ?2n) is added white-noise.307Prediction is formulated as a Bayesian inferenceunder the posterior:p(y?|x?,D) =?fp(y?|x?, f)p(f |D),where x?is a test input, y?is the test responsevalue andD is the training set.
The predictive pos-terior can be solved analitically, resulting in:y??
N (kT?
(K + ?2nI)?1y,k(x?, x?)?
kT?
(K + ?2nI)?1k?
),where k?= [k(x?,x1)k(x?,x2) .
.
.
k(x?,xn)]Tis the vector of kernel evaluations between thetraining set and the test input and K is the kernelmatrix over the training inputs (the Gram matrix).The kernel function encodes the covariance(similarity) between each input pair.
While a vari-ety of kernel functions are available, here we fol-lowed previous work in QE using GP (Cohn andSpecia, 2013; Shah et al., 2013) and employeda squared exponential (SE) kernel with automaticrelevance determination (ARD):k(x,x?)
= ?2fexp(?12F?i=1xi?
x?ili),where F is the number of features, ?2fis the co-variance magnitude and li> 0 are the featurelengthscales.The resulting model hyperparameters (SE vari-ance ?2f, noise variance ?2nand SE lengthscales li)were learned from data by maximising the modellikelihood.
All our models were trained using theGPy1toolkit, an open source implementation ofGPs written in Python.2.1 Multi-task learningThe GP regression framework can be extended tomultiple outputs by assuming f(x) to be a vec-tor valued function.
These models are commonlyreferred as coregionalization models in the GP lit-erature (?Alvarez et al., 2012).
Here we refer tothem as multi-task kernels, to emphasize our ap-plication.In this work, we employ a separable multi-taskkernel, similar to the one used by Bonilla et al.
(2008) and Cohn and Specia (2013).
Consider-ing a set of D tasks, we define the correspondingmulti-task kernel as:k((x, d), (x?, d?))
= kdata(x,x?
)?Bd,d?, (1)1http://sheffieldml.github.io/GPy/where kdatais a kernel on the input points, d andd?are task or metadata information for each inputand B ?
RD?Dis the multi-task matrix, whichencodes task covariances.
For task 1.1, we con-sider each language pair as a different task, whilefor tasks 1.2 and 1.3 we use additional datasetsfor the same language pair (en-es), treating eachdataset as a different task.To perform the learning procedure the multi-task matrix should be parameterised in a sensibleway.
We follow the parameterisations proposedby Cohn and Specia (2013), which we briefly de-scribe here:Independent: B = I.
In this setting each task ismodelled independently.
This is not strictlyequivalent to independent model training be-cause the tasks share the same data kernel(and the same hyperparameters);Pooled: B = 1.
Here the task identity is ignored.This is equivalent to pooling all datasets in asingle task model;Combined: B = 1 + ?I.
This setting lever-ages between independent and pooled mod-els.
Here, ?
> 0 is treated as an hyperparam-eter;Combined+: B = 1 + diag(?).
Same as ?com-bined?, but allowing one different ?
value pertask.2.2 Sparse Gaussian ProcessesThe performance bottleneck for GP models is theGram matrix inversion, which is O(n3) for stan-dard GPs, with n being the number of training in-stances.
For multi-task settings this can be a po-tential issue because these models replicate the in-stances for each task and the resulting Gram ma-trix has dimensionality nd ?
nd, where d is thenumber of tasks.Sparse GPs tackle this problem by approximat-ing the Gram matrix using only a subset of m in-ducing inputs.
Without loss of generalisation, con-sider these m points as the first instances in thetraining data.
We can then expand the Gram ma-trix in the following way:K =[KmmKm(n?m)K(n?m)mK(n?m)(n?m)].Following the notation in (Rasmussen andWilliams, 2006), we refer Km(n?m)as Kmnand308its transpose as Knm.
The block structure of Kforms the basis of the so-called Nystr?om approxi-mation:?K = KnmK?1mmKmn, (2)which results in the following predictive posterior:y??
N (kTm?
?G?1Kmny, (3)k(x?,x?)?
kTm?K?1mmkm?+?2nkTm??G?1km?
),where?G = ?2nKmm+ KmnKnmand km?is thevector of kernel evaluations between test input x?and the m inducing inputs.
The resulting trainingcomplexity is O(m2n).The remaining question is how to choose the in-ducing inputs.
We follow the approach of Snelsonand Ghahramani (2006), which note that these in-ducing inputs do not need to be a subset of thetraining data.
Their method considers each in-put as a hyperparameter, which is then optimisedjointly with the kernel hyperparameters.2.3 FeaturesFor all tasks we used the QuEst framework (Spe-cia et al., 2013) to extract a set of 80 black-boxfeatures as in Shah et al.
(2013), for which we hadall the necessary resources available.
Examples ofthe features extracted include:?
N-gram-based features:?
Number of tokens in source and targetsegments;?
Language model (LM) probability ofsource and target segments;?
Percentage of source 1?3-grams ob-served in different frequency quartiles ofa large corpus of the source language;?
Average number of translations persource word in the segment as given byIBM 1 model from a large parallel cor-pus of the language, with probabilitiesthresholded in different ways.?
POS-based features:?
Ratio of percentage of nouns/verbs/etcin the source and target segments;?
Ratio of punctuation symbols in sourceand target segments;?
Percentage of direct object personal orpossessive pronouns incorrectly trans-lated.For the full set of features we refer readers toQuEst website.2To perform feature selection, we followed theapproach used in Shah et al.
(2013) and rankedthe features according to their learned lengthscales(from the lowest to the highest).
The lengthscaleof a feature can be interpreted as the relevance ofsuch feature for the model.
Therefore, the out-come of a GP model using an ARD kernel can beviewed as a list of features ranked by relevance,and this information can be used for feature selec-tion by discarding the lowest ranked (least useful)ones.3 Preliminary ExperimentsOur submissions are based on multi-task settings.For task 1.1, we consider each language pair as adifferent task, training one model for all pairs.
Fortasks 1.2 and 1.3, we used additional datasets andencoded each one as a different task (totalling 3tasks):WMT13: these are the datasets provided in lastyear?s QE shared task (Bojar et al., 2013).We combined training and test sets, totalling2, 754 sentences for HTER prediction and1, 003 sentences for post-editing time predic-tion, both for English-Spanish.EAMT11: this dataset is provided by Specia(2011) and is composed of 1, 000 English-Spanish sentences annotated in terms ofHTER and post-editing time.For each task we prepared two submissions: onetrained on a standard GP with the full 80 featuresset and another one trained on a sparse GP witha subset of 40 features.
The features were chosenby training a smaller model on a subset of 400 in-stances and following the procedure explained inSection 2.3 for feature selection, with a pre-definecutoff point on the number of features (40), basedon previous experiments.
The sparse models weretrained using 400 inducing inputs.To select an appropriate multi-task setting forour submissions we performed preliminary exper-iments using a 90%/10% split on the correspond-ing training set for each task.
The resulting MAEscores are shown in Tables 1 and 2, for standardand sparse GPs, respectively.
The boldface fig-ures correspond to the settings we choose for the2http://www.quest.dcs.shef.ac.uk/quest_files/features_blackbox309Task 1.1 Task 1.2 Task 1.3en-es es-en en-de de-en en-es en-esIndependent 0.4905 0.5325 0.5962 0.5452 0.2047 0.4486Pooled 0.4957 0.5171 0.6012 0.5612 0.2036 0.8599Combined 0.4939 0.5162 0.6007 0.5550 0.2321 0.7489Combined+ 0.4932 0.5182 0.5990 0.5514 0.2296 0.4472Table 1: MAE results for preliminary experiments on standard GPs.
Post-editing time scores for task 1.3are shown on log time per word.Task 1.1 Task 1.2 Task 1.3en-es es-en en-de de-en en-es en-esIndependent 0.5036 0.5274 0.6002 0.5532 0.3432 0.3906Pooled 0.4890 0.5131 0.5927 0.5532 0.1597 0.6410Combined 0.4872 0.5183 0.5871 0.5451 0.2871 0.6449Combined+ 0.4935 0.5255 0.5864 0.5458 0.1659 0.4040Table 2: MAE results for preliminary experiments on sparse GPs.
Post-editing time scores for task 1.3are shown on log time per word.official submissions, after re-training on the corre-sponding full training sets.To check the speed-ups obtained from usingsparse GPs, we measured wall clock times fortraining and prediction in Task 1.1 using the ?In-dependent?
multi-task setting.
Table 3 shows theresulting times and the corresponding speed-upswhen comparing to the standard GP.
For compar-ison, we also trained a model using 200 inducinginputs, although we did not use the results of thismodel in our submissions.Time (secs) Speed-upStandard GP 12122 ?Sparse GP (m=400) 3376 3.59xSparse GP (m=200) 978 12.39xTable 3: Wall clock times and speed-ups for GPstraining and prediction: full versus sparse GPs.4 Official Results and DiscussionTable 4 shows the results for Task 1.1.
Us-ing standard GPs we obtained improved resultsover the baseline for English-Spanish and English-German only, with particularly substantial im-provements for English-Spanish, which also hap-pens for sparse GPs.
This may be related to thelarger size of this dataset when compared to theothers.
Our results here are mostly inconclusivethough and we plan to investigate this setting morein depth in the future.
Specifically, due to thecoarse behaviour of the labels, ordinal regressionGP models (like the one proposed in (Chu et al.,2005)) could be useful for this task.Results for Task 1.2 are shown in Table 5.
Thestandard GP model performed unusually poorlywhen compared to the baseline or the sparse GPmodel.
To investigate this, we inspected the re-sulting model hyperparameters.
We found out thatthe noise ?2nwas optimised to a very low value,close to zero, which characterises overfitting.
Thesame behaviour was not observed with the sparsemodel, even though it had a much higher numberof hyperparameters to optimise, and was thereforemore prone to overfitting.
We plan to investigatethis issue further but a possible cause could be badstarting values for the hyperparameters.Table 6 shows results for Task 1.3.
In this task,the standard GP model outperformed the base-line, with the sparse GP model following veryclosely.
These figures represent significant im-provements compared to our submission to thesame task in last year?s shared task (Beck et al.,2013), where we were not able to beat the baseline.The main differences between last year?s and thisyear?s models are the use of additional datasetsand a higher number of features (25 vs. 40).
Thecompetitive results for the sparse GP models arevery promising because they show we can com-bine multiple datasets to improve post-editing timeprediction while employing a sparse model to copewith speed issues.310en-es es-en en-de de-en?
?
?
?
?
?
?
?Standard GP 0.21 -0.33 0.11 -0.15 0.26 -0.36 0.24 -0.27Sparse GP 0.17 0.27 0.12 -0.17 0.23 -0.33 0.14 -0.17Baseline 0.14 -0.22 0.12 -0.21 0.23 -0.34 0.21 -0.25en-es es-en en-de de-enMAE RMSE MAE RMSE MAE RMSE MAE RMSEStandard GP 0.49 0.63 0.62 0.77 0.63 0.74 0.65 0.77Sparse GP 0.54 0.69 0.54 0.69 0.64 0.75 0.66 0.79Baseline 0.52 0.66 0.57 0.68 0.64 0.76 0.65 0.78Table 4: Official results for task 1.1.
The top table shows results for the ranking subtask (?
: DeltaAvg;?
: Spearman?s correlation).
The bottom table shows results for the scoring subtask.Ranking Scoring?
?
MAE RMSEStandard GP 0.72 0.09 18.15 23.41Sparse GP 7.69 0.43 15.04 18.38Baseline 5.08 0.31 15.23 19.48Table 5: Official results for task 1.2.Ranking Scoring?
?
MAE RMSEStandard GP 16.08 0.64 17.13 27.33Sparse GP 16.33 0.63 17.42 27.35Baseline 14.71 0.57 21.49 34.28Table 6: Official results for task 1.3.5 ConclusionsWe proposed a new setting for training QE mod-els based on Multi-task Gaussian Processes.
Oursettings combined different datasets in a sensibleway, by considering each dataset as a differenttask and learning task covariances.
We also pro-posed to speed-up training and prediction timesby employing sparse GPs, which becomes crucialin multi-task settings.
The results obtained arespecially promising in the post-editing time task,where we obtained the same results as with stan-dard GPs and improved over our models from thelast evaluation campaign.In the future, we plan to employ our multi-taskmodels in large-scale settings, like datasets an-notated through crowdsourcing platforms.
Thesedatasets are usually labelled by dozens of annota-tors and multi-task GPs have proved an interest-ing framework for learning the annotation noise(Cohn and Specia, 2013).
However, multiple taskscan easily make training and prediction times pro-hibitive, and thus another direction if work is touse recent advances in sparse GPs, like the oneproposed by Hensman et al.
(2013).
We believethat the combination of these approaches couldfurther improve the state-of-the-art performance inthese tasks.AcknowledgmentsThis work was supported by funding fromCNPq/Brazil (No.
237999/2012-9, Daniel Beck)and from European Union?s Seventh FrameworkProgramme for research, technological develop-ment and demonstration under grant agreementno.
296347 (QTLaunchPad).ReferencesMauricio A.?Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2012.
Kernels for Vector-Valued Func-tions: a Review.
Foundations and Trends in Ma-chine Learning, pages 1?37.Daniel Beck, Kashif Shah, Trevor Cohn, and LuciaSpecia.
2013.
SHEF-Lite : When Less is More forTranslation Quality Estimation.
In Proceedings ofWMT13, pages 337?342.John Blatz, Erin Fitzgerald, and George Foster.
2004.Confidence estimation for machine translation.
InProceedings of the 20th Conference on Computa-tional Linguistics, pages 315?321.Ondej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Workshopon Statistical Machine Translation.
In Proceedingsof WMT13, pages 1?44.311Edwin V. Bonilla, Kian Ming A. Chai, and ChristopherK.
I. Williams.
2008.
Multi-task Gaussian ProcessPrediction.
Advances in Neural Information Pro-cessing Systems.Wei Chu, Zoubin Ghahramani, Francesco Falciani, andDavid L Wild.
2005.
Biomarker discovery in mi-croarray gene expression data with Gaussian pro-cesses.
Bioinformatics, 21(16):3385?93, August.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality Es-timation.
In Proceedings of ACL.James Hensman, Nicol`o Fusi, and Neil D. Lawrence.2013.
Gaussian Processes for Big Data.
In Pro-ceedings of UAI.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian processes for machinelearning, volume 1.
MIT Press Cambridge.Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.An Investigation on the Effectiveness of Features forTranslation Quality Estimation.
In Proceedings ofMT Summit XIV.Edward Snelson and Zoubin Ghahramani.
2006.Sparse Gaussian Processes using Pseudo-inputs.
InProceedings of NIPS.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Lucia Specia, Craig Saunders, Marco Turchi, ZhuoranWang, and John Shawe-Taylor.
2009.
Improvingthe confidence of machine translation quality esti-mates.
In Proceedings of MT Summit XII.Lucia Specia, Kashif Shah, Jos?e G. C. De Souza, andTrevor Cohn.
2013.
QuEst - A translation qual-ity estimation framework.
In Proceedings of ACLDemo Session.Lucia Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
In Pro-ceedings of EAMT.312
