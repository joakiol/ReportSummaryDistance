Proceedings of NAACL-HLT 2013, pages 1174?1184,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsGenerating Expressions that Refer to Visible ObjectsMargaret MitchellJohns Hopkins HLTCOEm.mitchell@jhu.eduKees van DeemterUniversity of Aberdeenk.vdeemter@abdn.ac.ukEhud ReiterUniversity of Aberdeene.reiter@abdn.ac.ukAbstractWe introduce a novel algorithm for generat-ing referring expressions, informed by humanand computer vision and designed to refer tovisible objects.
Our method separates abso-lute properties like color from relative proper-ties like size to stochastically generate a di-verse set of outputs.
Expressions generatedusing this method are often overspecified andmay be underspecified, akin to expressionsproduced by people.
We call such expressionsidentifying descriptions.
The algorithm out-performs the well-known Incremental Algo-rithm (Dale and Reiter, 1995) and the Graph-Based Algorithm (Krahmer et al 2003; Vi-ethen et al 2008) across a variety of imagesin two domains.
We additionally motivatean evaluation method for referring expressiongeneration that takes the proposed algorithm?snon-determinism into account.1 IntroductionReferring expression generation (REG) is the taskof generating an expression that can identify a ref-erent to a listener.
These expressions generally takethe form of a definite noun phrase such as ?the largeorange plate?
or ?the furry running dog?.
Researchin REG primarily focuses on the subtask of select-ing a set of properties that may be used to constructthe final surface expression, e.g., ?color:orange,size:large, type:plate?.
This property selection taskis optimized to meet different goals: for example,to be identical to those a person would generate inthe same situation, or to be unique to the intendedreferent and no other item in the discourse.We focus on the task of generating referring ex-pressions for visible objects, specifically with thegoal of generating descriptive, human-like referringexpressions.
We are motivated by the desire to con-nect this algorithm to input from a computer visionsystem, and discuss how this may work through-out the paper.
Computer vision (CV) does not yetreliably provide features for some of the most fre-quent properties that people use in visual descrip-tion (in particular, size-based features), and so weuse a gold-standard visual input, evaluating purelyon REG.
The proposed algorithm, which we callthe Visible Objects Algorithm, is designed to ap-proximate human variation identifying an object ina group of visible, real world objects.Our primary contributions are the following.Background for each issue is provided in Section 2:1.
An approach accounting for overspecification,underspecification, and some of the known ef-fects of vision on reference.2.
A function to approximate the stochastic natureof reference.
This reflects that people will pro-duce different references to the same object.3.
A separation between absolute properties likecolor, which may be detected directly by CV,from relative properties like size and loca-tion, which require reasoning over visual fea-tures to determine an appropriate form (e.g.,height/width and distance features betweenpixels are available from a visual input; sayingan object is ?tall?
requires further reasoning).4.
An evaluation method for non-deterministicREG that aligns generated and observed dataand calculates accuracy over alignments.11742 Motivation & OverviewMost implemented algorithms for referring expres-sion generation focus on unique identification of areferent, determining the set of properties that dis-tinguish a particular target object from the other ob-jects in the scene (the contrast set) (Dale, 1989; Re-iter and Dale, 1992; Dale and Reiter, 1995; Krahmeret al 2003; Areces et al 2008).
This view of refer-ence was first outlined by Olson (1970), ?the spec-ification of an intended referent relative to a set ofalternatives?.
A substantial body of evidence nowshows that contrastive value relative to alternativesis not the only factor motivating speakers?
propertychoices, specifically in visual domains.
The phe-nomena of overspecification and redundancy, wherespeakers select properties that have little or no con-trastive value, was observed in early developmen-tal studies in visual domains (Ford and Olson, 1975;Whitehurst, 1976; Sonnenschein, 1985) as well aslater studies on adult speakers in visual domains(Pechmann, 1989; Engelhardt et al 2006; Koolen etal., 2011).
The related phenomenon of underspecifi-cation, where speakers select a set of properties thatdo not linguistically specify the referent, has also re-ceived some attention, particularly in visual domains(Clark et al 1983; Kelleher et al 2005).These findings make sense in light of visual ev-idence that some properties ?pop out?
in the scene(Treisman and Gelade, 1980), and speakers may be-gin referring before scanning the full set of scene ob-jects (Pechmann, 1989), selecting those propertiesthat are salient for them (Horton and Keysar, 1996;Bard et al 2009) without spending a great amountof cognitive effort considering the perception of ahearer (Keysar and Henly, 2002).We take this evidence to suggest an approach fora visual reference algorithm that generates natural,human-like reference by generating visual proper-ties that are salient for a speaker.1 We can under-stand what is salient visually (what does the visualsystem first respond to, what guides attention?
), lin-guistically (what do people tend to mention in visualscenes?
), and cognitively, which we will not haveroom to discuss in this paper (what is atypical for1We can also add functionality to ensure that a referent isuniquely identified against the contrast set (whether or not thatreflects what a person would do), which we will describe.Figure 1: Relative properties, like size and location, aredifficult to obtain from a two-dimensional image.
We findit easy to perceive the background object as larger thanthe one in the front; but they are technically the same sizein the image (from Murray et al(2006)).this object?
); as well as in terms of broader notionsof salience, e.g., discourse salience (Krahmer andTheune, 2002).This suggests a paradigm shift in the generationtask when referring to visible objects, if the goal isto produce human-like reference.
In particular, thissuggests moving from selecting properties that ruleout other scene objects to selecting properties thatare salient for the speaker (visually, conversation-ally, based on previous experiences, etc.).
This mir-rors related research on the tradeoff between audi-ence design and egocentrism in language production(Clark and Murphy, 1982; Horton and Keysar, 1996;Bard et al 2009; Gann and Barr, 2013).
Under-and overspecification naturally fall out from such anapproach, with no need to specifically model eitherphenomenon.Perhaps unsurprisingly, the set of properties thatare visually salient and the set of properties that arelinguistically salient largely overlap.
Color is thefirst property our visual system processes, followedsoon after by size (Murray et al 2006; Fang et al2008; Schwarzkopf et al 2010); and people tendto use color (Pechmann, 1989; Viethen et al 2012)and size when identifying objects, with size com-mon when there is another object of the same typein the scene (Brown-Schmidt and Tanenhaus, 2006).Following this, our algorithm gives a privilegedposition to these properties, processing them first.Using computer vision techniques to determine anobject?s color works reasonably well (Berg et al2011), and the relevant visual features for this taskmay be useful in future work to return several pos-sible color labels that capture differences in lexicalchoice (cf.
Reiter and Sripada (2002)).Detecting size does not work well (Forsyth,11752011); and when it does, it will likely not take theform supposed in recent generation work.
MostREG algorithms use a predefined single-featuredvalue, such as ?big?
; however, given an image-basedinput, obtaining such a value requires (1) determin-ing how the object is situated in a three-dimensionalspace, difficult to obtain from a two-dimensional im-age (see Figure 1); and (2) determining what thevalue should be: object detectors currently can pro-vide the height and width of the location where anobject is likely to exist (its bounding box), as well asthe x- and y-axis locations of the pixels within theobject detection; but a value from these features like?big?, ?tall?, or ?long?
requires further reasoning.As such, we incorporate the top-performing size al-gorithm introduced in Mitchell et al(2011), whichtakes as input the height and widths of objects in theimage and outputs a size value or NONE, indicatingthat size should not be used to describe the object.In addition to color and size, location and orien-tation begin to be processed early on in the visualsystem (Treisman, 1985; Itti and Koch, 2001), withour first perception of location corresponding to ba-sic cues of where an object is relative to our focusof attention.
For an input image, this simple type oflocation corresponds to surface forms such as, e.g.,?on the right of the image?
or ?at the top of the im-age?.
Along with size, location and orientation makeup the three primary relative properties that we aimto generate language for.After the simple forms for color, size, location,and orientation properties are processed, our visualsystem feeds forward to two parallel pathways, theso-called ?what?
and ?where?
pathways (Ungerlei-der and Mishkin, 1982), which process propertieswith growing complexity.
The ?what?
pathway in-cludes absolute properties like shape and material,which computer vision has had some success de-tecting (Ferrari and Zisserman, 2007; Farhadi et al2009) while the ?where?
pathway corresponds tomore complex spatial orientation and location infor-mation, such as where objects are relative to one an-other and which way they are facing.To begin connecting this process to the genera-tion of human-like descriptions of visible objects,we start with the following simplification: Color andsize have a privileged status, the first properties pro-cessed.
These are followed by the relative propertiesFigure 2: Initial model for generating visual reference.of location and orientation, which may feed forwardto more complex location and orientation propertiesin one pathway; and absolute properties followingcolor, like material and shape, which may be pro-cessed in another pathway.This gives us the basic model for generating ref-erence to visible objects shown in Figure 2.
To gen-erate reference in this model, nodes correspond togeneral visual attributes and may generate forms forvisual properties (attribute:value pairs).
That is, aproperty such as color:red is generated from the at-tribute node color and a property such as size:tall isgenerated from the attribute node size.
We are lim-ited by existing REG corpora in which properties wecan evaluate; in this paper, we examine the effect ofthe independent selection of color and size, followedby location and orientation.2Generating human-like expressions in this settingbegins to be possible by adopting recent propos-als that REG handle speaker variation (Viethen andDale, 2010) and the non-deterministic nature of ref-erence (van Gompel et al 2012; van Deemter etal., 2012b).
We can capture such variation simplyby estimating ?att, the likelihood that an attributeatt generates a corresponding visual property.
Dur-ing generation, the algorithm passes through each at-tribute node, and uses this estimate to stochasticallyadd each property to the output property set.Such a non-deterministic process means that thealgorithm will not return the same output every time,which offers new challenges for evaluation.
If werun the algorithm 1,000 times, we have a distribu-tion over several possible output property sets.
Fromthis we can obtain the majority set and check if itmatches the majority observed set.
Similarly, we can2We have also built an algorithm and corpus with more com-plex properties in order to tease out further details of visual ref-erence, but must leave these details for follow up work; for now,we focus on the properties common to REG corpora.1176run the algorithm for as many instances as we havein our test data, and see how well the property setsit produces aligns to the observed property sets.
Wediscuss evaluation using both methods in Section 6.3 The State of the Art in REG3.1 AlgorithmsIn order to understand how this approach comparesto the state of the art in REG, we evaluate againsttwo of the most well-known algorithms, the Incre-mental Algorithm (Dale and Reiter, 1995) and theGraph-Based Algorithm (Krahmer et.
al, 2003, asimplemented in Viethen et al 2008).
Details onthese algorithms are available in their correspondingpapers.
As a brief summary, both algorithms formal-ize the objects in the discourse as a set of properties(attribute:value pairs).
For example, one object maybe represented as ?type:box, color:red, size:large?.The task is to find the set of properties that uniquelyspecify the referent.
This is known as a content se-lection problem, and the set of properties chosen bythe algorithm is called a distinguishing description.The Incremental Algorithm (IA) proceeds by it-erating through attributes in a predefined order (apreference order), and for each attribute, it checkswhether specifying a value would rule out at leastone item in the contrast set that has not already beenruled out.
If it will, the attribute:value is added tothe distinguishing description.
This process contin-ues until all contrast items (distractors) are ruled outor all available properties have been checked.
Weuse the implementation of the IA available from theNLTK (Bird et al 2009).3In the Graph-Based Algorithm (GB), the objectsin the discourse are represented within a labeled di-rected graph, and content selection is a subgraphconstruction problem.
Each object is represented asa vertex, with properties for an object represented asself-edges on the object vertex, and spatial relationsbetween objects represented as edges between ver-tices.
The algorithm seeks to find the cheapest sub-graph, calculated from the edge costs.
We use theimplementation available from Viethen et al(2008),which adds a preference order to decide betweenedges with the same cost during search.
This has3https://github.com/nltk/nltk contrib/blob/master/nltk contrib/referring.py retrieved 1.Aug.2012.been one of the best-performing systems in recentgeneration challenges (Gatt and Belz, 2008; Gatt etal., 2009).An important commonality between these algo-rithms, and much of the work on REG that theyhave influenced, is the focus on unique identifica-tion and operating deterministically.
Both produceone property set (and only one), and stop once a tar-get item has been uniquely identified (or else fail).Their driving goal is to rule out distractor objects.In the approach introduced here, the algorithmproduces a distribution over several possible out-puts, and the initial driving mechanism is based onlikelihood estimates for each attribute independentof the other objects in the scene, rather than rulingout all distractors.
This offers a way to capture someaspects of human-like reference, including under-and overspecification and speaker variation.
Due tothe fundamentally different objective of this algo-rithm, we will call the kind of expression it generatesan identifying description, following Searle (1969).This is a description that the system finds (1) usefulto describe the referent and (2) true of the referent.4 The AlgorithmThe Visible Objects Algorithm iterates through listsof visible attributes, stochastically adding propertiesto the property set it will generate.
After this initialsearch, the algorithm then scans through the objectsin the scene, roughly corresponding to how peoplescan a scene when referring (Pechmann, 1989).
Thetarget referent type, corresponding to the head nounin the final generated description, is added to theproperty set at the end of the algorithm.We represent the basic components of the algo-rithm graphically in Figure 3.
Full code is availableonline.4 After START, the algorithm proceeds in par-allel through a list of absolute attributes and a listof relative attributes.
The likelihood of generating aproperty is a function of the prior likelihood ?att and?, a penalty on the length of the constructed prop-erty set up to that point.
This ensures that only a fewproperties are generated for a referent, and the ex-pression will not be too complex.
This is also in linewith recent research suggesting that there are rarelymore than three adjectives in a visual noun phrase4https://github.com/itallow/VisibleObjectsAlgorithm.1177(Berg et al 2011).
Once the algorithm hits END,it scans through the objects in the scene.
If it findsan object that is the same type as the referent object,the algorithm checks through the attributes again ina preference order akin to the IA, comparing the ob-ject?s properties against the referent?s and generatingproperties as a function of the length penalty alone.If the algorithm does not find an object that it is thesame type, no further properties are added.4.1 RequirementsThe algorithm requires the following:1.
Prior likelihood estimates on the inclusion ofdifferent attributes.
Represented as ?att.2.
Ordered list of absolute attributes beyond color.Represented as AP.3.
Ordered list of relative attributes beyond size.Represented as RP.4.
Ordered list of all attributes.
Represented as P.5.
Ordered list specifying the order in which toscan through other scene objects.
The currentimplementation uses the order in which the ob-jects are listed in the corpora it is run on.
(1) is similar to the cost functions for GB, butattributes are selected non-deterministically usingprior likelihoods.
(2), (3), and (4) are similar tothe IA?s and GB?s preference order.
For our eval-uation corpora, AP is empty and RP contains loca-tion and orientation.
(5) is novel to this algorithm,defining an order in which to compare the target ob-ject against other objects in the scene.
This is in-spired by the process of incremental speech produc-tion (Pechmann, 1989), where speakers scan objectsduring naming, incrementally producing properties.4.2 The Stochastic ProcessGenerally speaking, we want to penalize longer de-scriptions and encourage the attributes that we knowpeople are likely to use.
We can encourage a likelyattribute by using its prior likelihood as an estimateof whether to include it.
We can penalize longer de-scriptions with a penalty proportional to the lengthof the property set under construction.
In otherwords, given a prior likelihood estimate for includ-ing an attribute att, ?att, and the property set con-structed so farA, we compute whether to add a prop-a.
b.TUNA corpus GRE3D3 corpusFigure 4: Example scenes from corpora.erty for att toA as a function of ?att and the length-based penalty ?
:f(A ?
{x}) = ??attwhere?
={1?|A| if |A| > 01 otherwiseand ?
is an empirically determined weight.
Thealgorithm then chooses a random number n, 0 ?n ?
1.
If n < f(A ?
{x}), it adds the property.4.3 Scanning Through ObjectsAfter the initial pass through the properties, the al-gorithm compares each object in the scene that isthe same type as the target.
If the values for anattribute are different, then the corresponding prop-erty is added to the property set based on the lengthpenalty alone; when the goal is unique identification,the algorithm can use no penalty.
In development,we found that incrementally scanning through ob-jects after initially adding properties resulted in bet-ter performance than an algorithm that did not con-tain this step.4.4 Worked ExampleSuppose the input in Figure 6 (visualized in Figure4a), with the goal of referring to obj1 by producinga property set A.
First, the algorithm scans throughcolor and size in parallel.
For color, it finds the cor-responding value grey; with a computer vision in-put, this would be possible using the object pixelsas features.
There is no length penalty at this point(|A|=0), so it adds the property color:grey to A withlikelihood ?color.
For our evaluation domains, ?coloris around .90 across folds, and so a color property isusually added.For size, the algorithm finds an appropriate valueusing the Size Algorithm from Mitchell et al(2011).The Size Algorithm uses the average height and1178Figure 3: Basic model for generating visual reference.width of all objects that are the same type as the ref-erent object; in this case, obj2, obj3, obj4.
This re-turns a size value large, and so the property size:largeis added toAwith likelihood ?size (around .40 to .70across folds, depending on the domain).The most likely property set at this point is sim-ply ?color:grey?.
The next most likely is ?color:grey,size:large?, then ?size:large?.
There are no fur-ther absolute properties in this example, but thereare values for the relative attributes loc (location)and ori (orientation).
Assuming RP=?location,orientation?, the algorithm first analyzes location,then orientation.
A location property is added to Awith likelihood ?loc multiplied by the length penalty?= 1(?
?1) if A=?color:grey?
; ?=1(?
?2) if A=?color:grey,size:large?, etc.
; and an orientation property is addedto A with likelihood ?ori multiplied by the lengthpenalty ?= 1(?
?1) if the property set is ?color:grey?,etc.
At this point, the likelihood of adding furtherproperties quickly diminishes.Once all properties have been analyzed, the algo-rithm scans through the objects in the scene.
Foreach object obj2.
.
.
objn, if the object is the sametype as the target object obj1, then any differentproperty of the target referent is added to A witha likelihood based on the length penalty alone ?.?type:desk?
is added at the end.For this example scene, the algorithm will gen-erate the property sets ?color:grey, type:desk?,?color:grey, size:large, type:desk?, ?size:large,type:desk?, ?color:grey, ori:front, type:desk?,?color:grey, loc:(3, 1), type:desk?, etc., with dif-ferent frequencies.
Due to the length penalty,generated property sets will almost never have morethan 3 properties.tg color:yellow size:(63,63) type:ball loc:right-handlm color:red size:(345,345) type:cube loc:right-handobj3 color:yellow size:(70,70) type:cube loc:left-handFigure 5: Example input scene: GRE3D3 corpus.
For IAAnd GB, gold-standard size values are provided ratherthan measurements (small, large).obj1 colour:grey size:(454,454) type:desk loc:(3,1) ori:frontobj2 colour:blue size:(454,454) type:desk loc:(2,1) ori:frontobj3 colour:red size:(454,454) type:desk loc:(3,2) ori:backobj4 colour:green size:(254,254) type:desk loc:(4,1) ori:leftobj5 colour:blue size:(454,454) type:fan loc:(1,1) ori:frontobj6 colour:red size:(454,454) type:fan loc:(5,1) ori:backobj7 colour:green size:(254,254) type:fan loc:(2,2) ori:leftFigure 6: Example input scene: TUNA corpus.
For IAAnd GB, gold-standard size values are provided ratherthan measurements (small, large).As such, although ?color:grey, type:desk?
wouldsufficiently distinguish the intended referent, weinstead produce a variety of sets, overspecify-ing in some instances (e.g., ?color:grey, ori:front,type:desk?
), and with a small chance of underspec-ifying in others (e.g., ?size:large, type:desk?
).5 Evaluation Algorithms & Corpora5.1 CorporaWe evaluate on two well-known REG corpora, theGRE3D3 corpus (Viethen and Dale, 2008) and thesingular furniture section of the TUNA corpus (vanDeemter et al 2006).
Both corpora contain expres-sions elicited to computer-generated objects, and soprovide a reasonable starting point for evaluatingreference to visible objects.
For all algorithms, weevaluate on the selection of referent attributes.
Lex-ical choice and word order are not taken into ac-count.
Example images from GRE3D3 and TUNAare shown in Figure 4, and example algorithm input1179from these corpora are shown in Figures 5 and 6.In GRE3D3, we evaluate on the selection of type,color, size, and location, but leave aside proper-ties of relatum objects, which are not currently ad-dressed by this algorithm or the IA.
In TUNA, weevaluate on the selection of type, color, size andorientation.55.2 Algorithms5.2.1 The Incremental AlgorithmThe Incremental Algorithm requires a preferenceorder list (PO) specifying the order to iterate throughscene attributes.
We determine the preference or-der from corpus frequencies using cross-validationto hold out a test scene and list attributes from thetraining scenes in descending order.
We find thatcolor precedes size in the preference orders, in linewith recent research showing that this allows the al-gorithm to perform optimally on the TUNA corpus(van Deemter et al 2012a).
In development, we findthat IA performs best with type as the last attributein the PO, and report on numbers with this approach.5.2.2 The Graph-Based AlgorithmThe version of the Graph-Based Algorithm thatwe use is available from Viethen et al(2008).
Thisalgorithm requires (1) a set of cost functions for eachedge, and (2) a PO for deciding between propertiesin the case of a tie.
For (1), we use the method fromTheune et al(2011) to assign two costs (0, 1) tothe edges.
We first determine the relative frequencywith which each property is mentioned for a targetobject, and then create costs for each property usingk-means clustering (k=2) in the Weka toolkit (Hallet al 2009).
We refer interested readers to the The-une et alpaper for further details.
For (2), we followthe same method as for the Incremental Algorithm.5.2.3 The Visual Objects AlgorithmThe proposed algorithm requires ?att, which weestimate as the relative frequency of each attributeatt in the training data.
The ordered attribute lists forthe algorithm (AP, RP and P) are built in the sameway as the preference order list for the IA and GB,listing attributes from the training data in order of5We remove location from evaluation in this corpus.
Lo-cation is not annotated directly, but split such that only x-dimension or y-dimension may be marked for a reference.descending frequency.
For these corpora, there arenot absolute properties beyond color, so AP is empty.6 EvaluationPrevious evaluation of REG algorithms have usedmeasurements such as Uniqueness, Minimality,Dice (Belz and Gatt, 2008), and Accuracy (Gatt etal., 2009; Reiter and Belz, 2009).
Uniqueness isthe proportion of outputs that identify the referentuniquely, and Minimality is the proportion of out-puts that are both minimal and unique.
As our goalis to mimic human reference, these metrics are notas useful for the evaluations as the others.The Dice metric provides a value for the similar-ity between a generated description and a human-produced description, and therefore serves as a rea-sonable objective measure for how human-like theproduced sets are.
Given the generated property set(DS) and the human-produced property set (DH ),Dice is calculated as:2?
|DS ?DH ||DS |+ |DH |For each input domain, we evaluate over booleanvalues (included or excluded) for the attributes D(see Table 1).
Note that this means the specific val-ues for the attributes are not compared.
In this for-mulation based on boolean values, |DS |=|DH |=|D|and Dice reduces to:|DS ?DH ||D|Calculating Dice over the same number of at-tributes for both the observed and generated datahas the nice mathematical property of making Diceequal to other common metrics for evaluating amodel, including Accuracy, Precision, and Recall.6Since the proposed algorithm is stochastic, this in-troduces a problem in using a metric that comparessingle expressions.
We therefore seek to find thebest alignment between the set of expressions pro-duced by the algorithm and the set of expressionsproduced by people.
We formulate this alignment asan assignment problem weighted by Dice.
For thecorpus of observed property sets H and the corpusof generated property sets S, we find the best align-6A false positive is a false negative, and there are no truenegatives, so all four metrics are equivalent.1180Example Corresponding EvaluatedExpression Property Set Property Setthe red ball ?color:red, type:ball?
type:1 color:1size:0 loc:0Table 1: Example human expression and correspondingboolean-valued property set for evaluation in GRE3D3,with D={type, color, size, and location}.ment x out of all possible alignmentsX between thecorpora:argmaxx?X?
(S,H)?xDice(DS , DH )This may be solved in polynomial time using theHungarian method (Kuhn, 1955; Munkres, 1957).Note that because IA and GB are deterministic, find-ing an optimal alignment is trivial.
We call thismethod ALIGNED DICE.It is an open question whether an alignment-basedevaluation is fair: the proposed algorithm has morethan one chance to match the human descriptions.In the second evaluation method (MAJORITY) weaddress this issue, comparing how often the mostfrequent generated set compares with the most fre-quent observed set.
We run the proposed algorithm1,000 times, and the generated property sets are or-dered by frequency.
The most frequent generatedset is compared against the most frequent human-produced set.
The majority score is the percentageof folds where these two sets match.
For IA and FB,the most frequent generated set is the only gener-ated set.
This is a simple way to fairly compare theoutput of deterministic and non-deterministic algo-rithms.
There are no ties in the generated sets, butin the case of a tie in the observed data, we count amatch if any match the most frequent generated set.6.1 GRE3D3We randomly select two scenes (7, 9) from Set 1and their mirrored counterparts in Set 2 (17, 19) fordevelopment.
We empirically determine ?=5 for thelength-based penalty ?
in the proposed algorithm.We use the eight remaining scenes in each Setfor eight-fold cross-validation, estimating parame-ters for the algorithms on the seven training scenesin each fold, as discussed in Section 5.2.For ALIGNED DICE, we run the proposed algo-rithm five times in each fold and report the averageAlgorithmALIGNED DICE MAJORITYSet 1 Set 2 Set 1 Set 2Proposed Alg.
88.23 90.06 62.50 50.00IA 87.71 85.13 62.50 25.00GB 87.71 88.73 62.50 50.00Table 2: GRE3D3: Results (in %).AlgorithmALIGNED DICE MAJORITY+LOC -LOC +LOC -LOCProposed Alg.
88.75 86.07 40.00 40.00IA 81.79 81.55 0.00 100.00GB 75.36 66.04 20.00 20.00Table 3: TUNA: Results (in %).score.
Results are shown in Table 2.7The proposed Visible Objects Algorithm achieveshigher accuracy than either version of the Incremen-tal Algorithm or the Graph-Based Algorithm usingALIGNED DICE.
In MAJORITY, the Graph-Basedand the Visible Objects Algorithm both predict themajority property set in this evaluation at least 50%of the time.
The algorithm is competitive with thestate of the art on this corpus.6.2 TUNATUNA is split into two conditions: subjects discour-aged to use location (-LOC) or not (+LOC).
We ran-domly hold out two scenes from both conditions (1and 2), and find a value of ?=5 again works well onthe development data.As in the GRE3D3 corpus, we use the TUNAscenes in five-fold cross-validation, estimating pa-rameters on the four training scenes in each fold.
ForALIGNED DICE, we average over five runs of the al-gorithm, and for MAJORITY, we run the proposedalgorithm 1,000 times for each test scene.Results are shown in Table 3.
Again we see thatthe proposed Visible Objects Algorithm is compet-itive with the IA and GB for both ALIGNED DICEand MAJORITY.
GB performs poorly here, and thismay be due to the data sparsity issue that arises whenrequiring the algorithm to train on properties.8 In7We do not report statistical significance; the proposed algo-rithm produces several possible outputs for one input, while theIA and GB produce only one.8The original property-based weighting approach (Theuneet al 2011; Koolen et al 2012, see Section 5.2) trained on ob-ject collections that were identical to their test data in all proper-ties except x- and y-dimension, and so this was less of an issue.We hope to explore whether basing weights on attributes alone1181MAJORITY, the Visible Objects Algorithm is rela-tively stable across conditions, generating the ma-jority property set in 40% of the test scenes.
It doesnot outperform the IA in the -LOC condition, but theIA has a large range across the two conditions (0%and 100%).7 Conclusions and Future WorkWe have introduced a new algorithm for generatingreferring expressions, inspired by human and com-puter vision and aiming to refer in a human-like wayto visible objects.
The algorithm successfully gener-ates the most common attributes that people choosefor different objects, and offers a varied output tocapture speaker variation.
In contrast to most algo-rithms for the generation of referring expressions,which have aimed to produce distinguishing descrip-tions when these exist (Krahmer and van Deemter,2012), the core idea behind this algorithm is to gen-erate what is likely for a speaker in a visual domain.Since the driving mechanism behind the algorithmis not to uniquely identify the object, but rather topipeline the analysis of properties in a way similarto human visual processing, the generated expres-sion may be overspecified or underspecified.We are limited by available REG corpora to re-liably assess methods for generating more com-plex absolute properties like shape and material, butadding such properties would help advance the gen-eration of human-like reference in visual scenes andoffers further points of connection between the gen-eration process and computer vision property detec-tion.
Models for generating more complex spatialrelations are currently available, and are a naturalextension to this framework (e.g., those of Kelleherand Costello (2009)) as object detection becomesmore robust.We may also be able to build more sophisticatedgraphical models as larger corpora become avail-able.
For example, modeling the conditional proba-bility of generating reference for a property vn giventhe previously generated context p(vn|v1 .
.
.
vn?1)may bring us closer to human-like output.There are several additional issues that do notarise in this evaluation, but we expect must be ac-counted for when referring to naturalistic objects inimproves performance.visual domains.
These include:?
The interconnected nature of properties, wheresome properties entail others; for example, awooden object is likely to be called wooden, re-ferring to its material, rather than tan or brown.?
The role of typicality, where properties are se-lected because they are atypical for the object.?
Referring to more complex properties, e.g., ma-terial, texture, etc., and object parts.?
Better methods for determining the lengthpenalty and attribute likelihoods.We hope to discuss extensions to this algorithmcovering these aspects of reference in future work.AcknowledgmentsFunding for this research has been provided bySICSA and ORSAS.
We thank the anonymous re-viewers for useful comments on this paper.ReferencesCarlos Areces, Alexander Koller, and Kristina Striegnitz.2008.
Referring expressions as formulas of descrip-tion logic.
Proceedings of the 5th International Nat-ural Language Generation Conference (INLG 2008),pages 42?29.Ellen Gurman Bard, Robin Hill, Manabu Arai, andMary Ellen Foster.
2009.
Accessibility and attentionin situated dialogue: Roles and regulations.
Proceed-ings of the Workshop on the Production of ReferringExpressions (PRE-CogSci 2009).Anja Belz and Albert Gatt.
2008.
Intrinsic vs. extrin-sic evaluation measures for referring expression gen-eration.
Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics (ACL2008), pages 197?200.Alexander C. Berg, Tamara L. Berg, Hal Daume?
III,Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-sch, Margaret Mitchell, Karl Stratos, and Kota Yam-aguchi.
2011.
An exploration of how to learn fromvisually descriptive text.
JHU-CLSP Summer Work-shop Whitepaper.Steven Bird, Edward Loper, and Ewan Klein.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia Inc., Sebastopol, CA.Sarah Brown-Schmidt and Michael K. Tanenhaus.
2006.Watching the eyes when talking about size: An investi-gation of message formulation and utterance planning.Journal of Memory and Language, 54:592?609.1182Herbert H. Clark and Gregory L. Murphy.
1982.
Audi-ence Design in Meaning and Reference.
In J. F. LeNyand W. Kintsch, editors, Language and Comprehen-sion, volume 9 of Advances in Psychology, pages 287?299.
North-Holland, Amsterdam.Herbert H. Clark, Robert Schreuder, and Samuel But-trick.
1983.
Common ground and the understandingof demonstrative reference.
Journal of Verbal Learn-ing and Verbal Behavior, 22:245?258.Robert Dale and Ehud Reiter.
1995.
Computational in-terpretations of the gricean maxims in the generationof referring expressions.
Cognitive Science, 19:233?263.Robert Dale.
1989.
Cooking up referring expressions.Proceedings of the 27th Annual Meeting of the Associ-ation for Computational Linguistics (ACL 1989).P.
E. Engelhardt, K. Bailey, and F. Ferreira.
2006.
Dospeakers and listeners observe the gricean maxim ofquantity?
Journal of Memory and Language, 54:554?573.Fang Fang, Huseyin Boyaci, Daniel Kersten, and Scott O.Murray.
2008.
Attention-dependent representationof a size illusion in human V1.
Current biology,18(21):1707?1712.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their attributes.Proceedings of IEEE Conference on Computer Visionand Pattern Recognition (CVPR 2009).V.
Ferrari and A. Zisserman.
2007.
Learning visual at-tributes.
Advances in Neural Information ProcessingSystems (NIPS 2007).William Ford and David Olson.
1975.
The elabora-tion of the noun phrase in children?s description of ob-jects.
The Journal of Experimental Child Psychology,19:371?382.David A. Forsyth.
2011.
Personal communica-tion.
Video clip of communication available from:http://vimeo.com/40553150.
At 1:06:46.T.
M. Gann and D. J. Barr.
2013.
Speaking from expe-rience: Audience design as expert performance.
Lan-guage and Cognitive Processes.
In press.Albert Gatt and Anja Belz.
2008.
Attribute selec-tion for referring expression generation: New algo-rithms and evaluation methods.
Proceedings of 5th In-ternational Natural Language Generation Conference(INLG 2008), pages 50?58.Albert Gatt, Anja Belz, and Eric Kow.
2009.
The TUNAREG challenge 2009: Overview and evaluation results.Proceedings of the 12th European Workshop on Natu-ral Language Generation (ENLG 2009), pages 174?182.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11(1).William S. Horton and Boaz Keysar.
1996.
When dospeakers take into account common ground?
Cogni-tion, 59(1):91?117.Laurent Itti and Christof Koch.
2001.
Computationalmodelling of visual attention.
Nature Reviews Neuro-science, 2:194?203.John Kelleher and Fintan Costello.
2009.
Applyingcomputational models of spatial prepositions to vi-sually situated dialog.
Computational Linguistics,35(2):271?306.John Kelleher, Fintan Costello, and Josef van Genabith.2005.
Dynamically structuring, updating and interre-lating representations of visual and linguistic discoursecontext.
Artificial Intelligence, 167:62?102.Boaz Keysar and Anne S. Henly.
2002.
Speakers?
over-estimation of their effectiveness.
Psychological Sci-ence, 13(3):207?212.Ruud Koolen, Martijn Goudbeek, and Emiel Krahmer.2011.
Effects of scene variation on referential over-specification.
Proceedings of the 33rd Annual Meetingof the Cognitive Science Society (CogSci 2011).Ruud Koolen, Emiel Krahmer, and Marie?t Theune.
2012.Learning preferences for referring expression genera-tion: Effects of domain, language and algorithm.
Pro-ceedings of the 7th International Workshop on NaturalLanguage Generation (INLG 2012).Emiel Krahmer and Marie?t Theune.
2002.
Efficientcontext-sensitive generation of referring expressions.Information Sharing: Reference and Presuppositionin Language Generation and Interpretation, 143:223?263.Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A survey.Computational Linguistics, 38:173?218.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.H.
W. Kuhn.
1955.
The hungarian method for the assign-ment problem.
Naval Research Logistics Quarterly,2:83?97.Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2011.
Two approaches for generating size modifiers.Proceedings of the 13th European Workshop on Natu-ral Language Generation (ENLG 2011).James Munkres.
1957.
Algorithms for the assignmentand transportation problems.
Journal of Industrial andApplied Mathematics, 5(1):32?38.Scott O. Murray, Huseyin Boyaci, and Daniel Kersten.2006.
The representation of perceived angular size inhuman primary visual cortex.
Nature Neuroscience,9(3):429?434.1183David R. Olson.
1970.
Language and thought: Aspectsof a cognitive theory of semantics.
Psychological Re-view, 77:257?273.Thomas Pechmann.
1989.
Incremental speech pro-duction and referential overspecification.
Linguistics,27:89?110.Ehud Reiter and Anja Belz.
2009.
An investigation intothe validity of some metrics for automatically evalu-ating natural language generation systems.
Computa-tional Linguistics, 35(4):529?558.Ehud Reiter and Robert Dale.
1992.
A fast algorithmfor the generation of referring expressions.
Proceed-ings of the 14th International Conference on Compu-tational Linguistics (COLING 1992), 1:232?238.Ehud Reiter and Somayajulu Sripada.
2002.
Humanvariation and lexical choice.
Computational Linguis-tics, 28:545?553.D.
Samuel Schwarzkopf, Chen Song, and Geraint Rees.2010.
The surface area of human V1 predicts thesubjective experience of object size.
Nature Neuro-science, 14(1):28?30.J.
R. Searle.
1969.
Speech Acts: An Essay in the Philos-ophy of Language.
Cambridge University Press, Cam-bridge.Susan Sonnenschein.
1985.
The development of referen-tial communication skills: Some situations in whichspeakers give redundant messages.
Journal of Psy-cholinguistic Research, 14:489?508.Marie?t Theune, Ruud Koolen, Emiel Krahmer, andSander Wubben.
2011.
Does size matter ?
how muchdata is required to train a REG algorithm?
Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics (ACL 2011).Anne M. Treisman and Garry Gelade.
1980.
A featureintegration theory of attention.
Cognitive Psychology,12:97?13.Anne Treisman.
1985.
Preattentive processing in vi-sion.
Computer Vision, Graphics, and Image Process-ing, 31:156177.L.
G. Ungerleider and M. Mishkin.
1982.
Two Corti-cal Visual Systems.
In D. J. Ingle, M. Goodale, andR.
J. W. Mansfield, editors, Analysis of Visual Be-haviour, chapter 18, pages 549?586.
The MIT Press.Kees van Deemter, Ielka van der Sluis, and Albert Gatt.2006.
Building a semantically transparent corpus forthe generation of referring expressions.
Proceedingsof the 4th International Conference on Natural Lan-guage Generation (INLG 2006).Kees van Deemter, Albert Gatt, Ielka van der Sluis, andRichard Power.
2012a.
Generation of referring ex-pressions: Assessing the incremental algorithm.
Cog-nitive Science, 36(5):799?836.Kees van Deemter, Emiel Krahmer, Roger van Gompel,and Albert Gatt.
2012b.
Towards a computational psy-cholinguistics of reference production.
TopiCS: Pro-duction of Referring Expressions - Bridging the Gapbetween Computational and Empirical Approaches toReference.Roger P. G. van Gompel, Albert Gatt, Emiel Krahmer,and Kees van Deemter.
2012.
PRO: A computationalmodel of referential overspecification.
Architecturesand Mechanisms for Language Processing (AMLaP2012).Jette Viethen and Robert Dale.
2008.
The use of spatialrelations in referring expression generation.
Proceed-ings of the 5th International Natural Language Gener-ation Conference (INLG 2008), pages 59?67.Jette Viethen and Robert Dale.
2010.
Speaker-dependentvariation in content selection for referring expressiongeneration.
Proceedings of the 8th Australasian Lan-guage Technology Workshop (ALTW 2010), pages 81?89.Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t The-une, and Pascal Touset.
2008.
Controlling redundancyin referring expressions.
Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation (LREC 2008).Jette Viethen, Martijn Goudbeek, and Emiel Krahmer.2012.
The impact of colour difference and colourecodability on reference production.
Proceedings of the34th Annual Meeting of the Cognitive Science Society(CogSci 2012).G.
J. Whitehurst.
1976.
The development of communi-cation: Changes with age and modeling.
Child Devel-opment, 47:473?482.1184
