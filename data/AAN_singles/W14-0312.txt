Workshop on Humans and Computer-assisted Translation, pages 78?83,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsConfidence-based Active Learning Methods for Machine TranslationVarvara LogachevaUniversity of SheffieldSheffield, United Kingdomv.logacheva@sheffield.ac.ukLucia SpeciaUniversity of SheffieldSheffield, United Kingdoml.specia@sheffield.ac.ukAbstractThe paper presents experiments with ac-tive learning methods for the acquisitionof training data in the context of machinetranslation.
We propose a confidence-based method which is superior to thestate-of-the-art method both in terms ofquality and complexity.
Additionally,we discovered that oracle selection tech-niques that use real quality scores lead topoor results, making the effectiveness ofconfidence-driven methods of active learn-ing for machine translation questionable.1 IntroductionActive learning (AL) is a technique for the auto-matic selection of data which is most useful formodel building.
In the context of machine trans-lation (MT), AL is particularly important as theacquisition of data often has a high cost, i.e.
newsource texts need to be translated manually.
Thusit is beneficial to select for manual translation sen-tences which can lead to better translation quality.The majority of AL methods for MT is basedon the (dis)similarity of sentences with respect tothe training data, with particular focus on domainadaptation.
Eck et al.
(2005) suggest a TF-IDFmetric to choose sentences with words absent inthe training corpus.
Ambati et al.
(2010) proposea metric of informativeness relying on unseen n-grams.Bloodgood and Callison-Burch (2010) use n-gram frequency and coverage of the additionaldata as selection criteria.
Their technique solic-its translations for phrases instead of entire sen-tences, which saves user effort and leads to qualityimprovements even if the initial dataset is alreadysizeable.A recent trend is to select source sentencesbased on an estimate of the quality of their trans-lation by a baseline MT system.
It is assumedthat if a sentence has been translated well with theexisting data, it will not contribute to improvingthe translation quality.
If however a sentence hasbeen translated erroneously, it might have wordsor phrases that are absent or incorrectly repre-sented.
Haffari et al.
(2009) train a classifier todefine the sentences to select.
The classifier usesa set of features of the source sentences and theirautomatic translations: n-grams and phrases fre-quency, MT model score, etc.
Ananthakrishnan etal.
(2010) build a pairwise classifier that ranks sen-tences according to the proportion of n-grams theycontain that can cause errors.
For quality estima-tion, Banerjee et al.
(2013) train language modelsof well and badly translated sentences.
The use-fulness of a sentence is measured as the differenceof its perplexities in these two language models.In this research we also explore a quality-basedAL technique.
Compared to its predecessors, ourmethod is based on a more complex and thereforepotentially more reliable quality estimation frame-work.
It uses wider range of features, which gobeyond those used in previous work, covering in-formation from both source and target sentences.Another important novel feature in our work isthe addition of real post-editions to the MT train-ing data, as opposed to simulated post-editions(human reference translations) as in previous workon AL for MT.
As we show in section 3.2, addingpost-editions leads to superior translation qualityimprovements.
Additionally, this is a suitable so-lution for ?human in the loop?
settings, as post-editing automatically translated sentences tends tobe faster and easier than translation from scratch(Koehn and Haddow, 2009).
Also, different fromprevious work, we do not focus on domain adapta-tion: our experiments involve only in-domain data.Compared to previous work on confidence-driven AL, our approach has led to better results,but these proved to be highly dependent on a sen-tence length bias.
However, an oracle-based selec-78tion using true quality scores has not been shownto perform well.
This indicates that the usefulnessof quality scores as AL selection criterion in thecontext of MT needs to be further investigated.2 Active selection strategyOur AL sentence selection strategy relies on qual-ity estimation (QE).
QE is aimed at predicting thequality of a translated text (in this case, a sen-tence) without resorting to reference translations.It considers features of the source and machinetranslated texts, and an often small number (a fewhundreds) of examples of translations labelled forquality by humans to train a machine learning al-gorithm to predict such quality labels for new data.We use the open source QE framework QuEst(Specia et al., 2013).
In our settings it was trainedto predict an HTER score (Snover et al., 2006) foreach sentence, i.e., the edit distance between theautomatic translation and its human post-editedversion.
QuEst can extract a wide range of fea-tures.
In our experiments we use only the 17 so-called baseline features, which have been shownto perform well in evaluation campaigns (Bojaret al., 2013): number of tokens in sentences, av-erage token length, language model probabilitiesfor source and target sentences, average number oftranslations per source word, percentage of higherand lower frequency n-grams in source sentencebased on MT training corpus, number of punctua-tion marks in source and target sentences.Similarly to Ananthakrishnan et al.
(2010), weassume that the most useful sentences are thosethat lead to larger translation errors.
However,instead of looking at the n-grams that caused er-rors ?
a very sparse indicator requiring signifi-cantly larger amounts of training data, we accountfor errors in a more general way: the (QuEst pre-dicted) percentage of edits (HTER) that would benecessary to transform the MT output into a cor-rect sentence.3 Experiments and results3.1 Datasets and MT settingsFor the AL data selection experiment, two datasetsare necessary: parallel sentences to train an ini-tial, baseline MT system, and an additional poolof parallel sentences to select from.
Our goalwas to study potential improvements in the base-line MT system in a realistic ?human in the loop?scenario, where source sentences are translated bythe baseline system and post-edited by humans be-fore they are added to the system.
As it has beenshown in (Potet et al., 2012), post-editions tend tobe closer to source sentences than freely createdtranslations.
One of our research questions was toinvestigate whether they would be more useful toimprove MT quality.We chose the biggest corpus with machinetranslations and post-editions available to date: theLIG French?English post-editions corpus (Potetet al., 2012).
It contains 10,881 quadruples ofthe type: <source sentence, reference transla-tion, automatic translation, post-edited automatictranslation>.
Out of these, we selected 9,000 asthe pool to be added to be baseline MT system,and the remaining 1,881 to train the QE system forthe experiments with AL.
For QE training, we usethe HTER scores between MT and its post-editedversion as computed by the TERp tool.1We use the Moses toolkit with standard set-tings2to build the (baseline) statistical MT sys-tems.
As training data, we use the French?English News Commentary corpus released by theWMT13 shared task (Bojar et al., 2013).
For theAL experiments, the size of the pool of additionaldata (10,000) poses a limitation.
To examine im-provements obtained by adding fractions of up toonly 9,000 sentences, we took a small random sub-set of the WMT13 data for these experiments (Ta-ble 1).
Although these figures may seem small, thesettings are realistic for many language pairs andtext domains where larger data sets are simply notavailable.We should also note that all the data used in ourexperiments belongs to the same domain: the LIGSMT system which produced sentences for thepost-editions corpus was trained on Europarl andNews commentary datasets (Potet et al., 2010), butthe post-edited sentences themselves were takenfrom news test sets released for WMT shared tasksin different years.
Our baseline system is trainedon a fraction of the news commentary corpus.
Fi-nally, we tune and test all our systems on WMTshared task news news datasets (those which donot overlap with the post-editions corpus).1http://www.umiacs.umd.edu/?snover/terp/2http://www.statmt.org/moses/?n=Moses.Baseline79Corpora Size(sentences)Initial data (baseline MT system)Training - subset of 10, 000News Commentary corpusTuning - WMT newstest-2012 3, 000Test - WMT newstest-2013 3, 000Additional data (AL data)Post-editions corpus: 10, 881- Training QE system 1, 881- AL pool 9, 000Table 1: Datasets3.2 Post-editions versus referencesIn order to compare the impact of post-editionsand reference translations on MT quality, weadded these two variants of translations to base-line MT systems of different sizes, including theentire News Commentary corpus.
The figures forBLEU (Papineni et al., 2002) scores in Table 2show that adding post-editions results in signifi-cantly better quality than adding the same numberof reference translations3.
This effect can be seeneven when the additional data corresponds to onlya small fraction of the training data.In addition, it does not seem to matter whichMT system produced the translations which werethen post-edited in the post-edition corpus.
Even ifthe output of a third-party system was used (as inour case), it improves the quality of machine trans-lations for unseen data.
We assume that since post-editions tend to be closer to original sentences thanfree translations (Potet et al., 2012), they gener-ally help produce better source-target alignments,leading to the extraction of good quality phrases.Baseline corpus Results (BLEU)(sentences) Baseline Ref PE150,000 22.41 22.95 23.2150,000 20.22 20.91 22.0110,000 15.09 18.65 20.44Table 2: Influence of post-edited and referencetranslations on MT quality.
Ref: baseline systemwith added free references, PE: baseline systemwith added post-editions.3These systems use the whole post-editions set (10,881sentences) as opposed to 9,000-sentence subset which we usefurther in our AL experiments.
Therefore the figures reportedin this table are higher than those in subsequent sections.3.3 AL settingsThe experimental settings for all methods are asfollows.
First, a baseline MT system is trained.Then a batch of 1,000 sentences is selected fromthe data pool with an AL strategy, and the selecteddata is removed from the pool.
The MT system isrebuilt using a concatenation of the initial trainingdata and the new batch.
The process is repeateduntil the pool is empty, with subsequent steps us-ing the MT system trained on the previous step asa baseline.
The performance of each MT systemis measured in terms of BLEU scores.
We use thefollowing AL strategies:?
QuEst: our method described in section 2.?
Random: random selection of sentences.?
HTER: oracle-based selection based on trueHTER scores of sentences in the pool, insteadof the QuEst estimated HTER scores.?
Ranking: AL strategy described in (Anan-thakrishnan et al., 2010) for comparison.3.4 AL resultsOur initial results in Figure 1 show that our selec-tion strategy (QuEst) consistently outperforms theRandom selection baseline.Figure 1: Performance of MT systems enhancedwith data selected by different AL strategiesIn comparison with previous work, we foundthat the error-based Ranking strategy performsclosely to Random selection, although (Anan-thakrishnan et al., 2010) reports it to be better.80Compared to QuEst, we believe the lower figuresof the Ranking strategy are due to the fact that thelatter considers features of only one type (sourcen-grams), whereas QuEst uses a range of differentfeatures of the source and translation sentences.Interestingly, the Oracle method under-performs our QE-based method, although weexpected the use of real HTER scores to be moreeffective.
In order to understand the reasonsbehind such behaviour, we examined the batchesselected by QuEst and Oracle strategies moreclosely.
We found that the distribution of sentencelengths in batches by the two strategies is verydifferent (see Figure 2).
While in batches selectedby QuEst the average sentence length steadilydecreases as more data is added, in Oraclebatches the average length was almost uniform forall batches, except the first one, which containsshorter sentences.This is explained by HTER formulation: HTERis computed as the number of edits over the sen-tence length, and therefore in shorter sentences ev-ery edit is given more weight.
For example, theHTER score of a 5-word sentence with one erroris 0.2, whereas a sentence of 20 words with thesame single error has a score of 0.05.
However, itis doubtful that the former sentence will be moreuseful for an MT system than the latter.
Regardingthe nature of length bias in the predictions done byQuEst system, sentence length is used there as afeature, and longer sentences tend to be estimatedas having higher HTER scores (i.e., lower transla-tion quality).Therefore, sentences with the highest HTERmay not actually be the most useful, which makesthe Oracle strategy inferior to QuEst.
Moreover,longer sentences chosen by our strategy simplyprovide more data, so their addition might be moreuseful even regardless of the amount of errors.This seems to indicate that the success of ourstrategy might not be related to the quality of thetranslations only, but to their length.
Another pos-sibility is that sentences selected by QuEst mighthave more errors, which means that they can con-tribute more to the MT system.3.5 Additional experimentsIn order to check the two hypotheses put forwardin the previous section, we conduct two other setsof AL experiments: (i) a selection strategy thatchooses longer sentences first (denoted as Length)Figure 2: Number of words in batches selected bydifferent AL strategiesand (ii) a selection strategy that chooses sentenceswith larger numbers of errors first (Errors).Figure 3 shows that a simple length-based strat-egy yields better results than any of the othertested strategies.
Therefore, in cases when thecorpus has sufficient variation in sentence length,length-based selection might perform at least aswell as other more sophisticated criteria.
Theexperiments with confidence-based selection de-scribed in (Ananthakrishnan et al., 2010) were freeof this length bias, as sentences much longer orshorter than average were deliberately filtered out.Interestingly, results for the Errors strategy areslightly worse than those for QuEst, although theformer is guaranteed to choose sentences with thelargest number of errors and has even strongerlength bias than QuEst (see figure 2).
Therefore,the reasons hypothesised to be behind the superi-ority of QuEst over Oracle (longer sentences andlarger number of errors) are actually not the onlyfactors that influence the quality of an AL strategy.3.6 Length-independent resultsDespite the success of the length-based strategy,we do not believe that it is enough for an effectiveAL technique.
First of all, the experiment withthe Errors strategy demonstrated that more datadoes not always lead to better results.
Further-more, our aim is to reduce the translator?s effort incases when the additional data needs to be trans-lated or post-edited manually.
However, longersentences usually take more time to translate oredit, so choosing the longest sentences from a poolof sentences will not reduce translator?s effort.81Figure 3: Comparison of our QuEst-based selec-tion with a length-based selectionTherefore, we would like to study the effec-tiveness of our strategy by isolating the sentencelength bias.
One option is to filter out long sen-tences, as it was done in (Ananthakrishnan et al.,2010).
However, our pool is already too small.Therefore, we plot the performance improvementswith respect to training data size in words, in-stead of sentences.
As it was already noted byBloodgood and Callison-Burch (2010), measuringthe amount of added data in sentences can signifi-cantly contort the real annotation cost (the cost ofacquisition of new translations).
So we switch tolength-independent representation.Figure 4: Active learning quality plotted with re-spect to data size in words: QuEst vs Oraclestrategies.Figure 4 shows that the Oracle strategy inFigure 5: AL quality plotted with respect to datasize in words: QuEst vs Length and Errorsstrategies.length-independent representation can still be seento perform worse than both our strategy and ran-dom selection.
Results of Length and Errorstrategies (plotted separately in figure 5 for read-ability) are very close and both underperform ourQuEst-based strategy and random selection ofdata.Here our experience echoes the results of (Mo-hit and Hwa, 2007), where the authors propose theidea of difficult to translate phrases.
It is assumedthat extending an MT system with phrases that cancause difficulties during translation is more effec-tive than simply adding new data and re-buildingthe system.
Due to the lack of time and humanannotators, the authors extracted difficult phrasesautomatically using a set of features: alignmentfeatures, syntactic features, model score, etc.
Con-versely, we had the human-generated informationon what segments have been translated incorrectly.We assumed that the use of this knowledge as partof our AL strategy would give us an upper boundfor our AL method results.
However, it turned outthat prediction based on multiple features is morereliable than precise information on quality, whichaccounts for only one aspect of data.4 ConclusionsWe presented experiments with an active learningstrategy for machine translation based on qualitypredictions.
This strategy performs well comparedto another quality-driven strategy and a randombaseline.
However, we found that it was success-82ful mostly due to its tendency to rate long sen-tences as having lower quality.
Consequently, theAL application that chooses the longest sentencesis not less successful when selecting from corporawith large variation in sentence length.
A length-independent representation of the results showedthat an oracle selection is less effective than ourquality-based strategy, which we believe to be dueto the nature of corrections and small size of thepost-edition corpus.
In addition to that, anotheroracle selection based on the amount of errors andlength-based selection show poor results when dis-played in length-independent mode.We believe that the quality estimation strategybenefits from other features that reflect the useful-ness of a sentence better than its HTER score andthe amount of user corrections.
In future work wewill examine the influence of individual featuresof the quality estimation model (such as languagemodel scores) as active learning selection strategy.ReferencesVamshi Ambati, Stephan Vogel, and Jaime Carbonell.2010.
Active Learning and Crowd-Sourcing for Ma-chine Translation.
LREC 2010: Proceedings of theseventh international conference on Language Re-sources and Evaluation, 17-23 May 2010, Valletta,Malta, pages 2169?2174.Sankaranarayanan Ananthakrishnan, Rohit Prasad,David Stallard, and Prem Natarajan.
2010.
Dis-criminative Sample Selection for Statistical MachineTranslation.
EMNLP-2010: Proceedings of the2010 Conference on Empirical Methods in Natu-ral Language Processing, October 9-11, 2010, MIT,Massachusetts, USA, (October):626?635.Pratyush Banerjee, Raphael Rubino, Johann Roturier,and Josef van Genabith.
2013.
Quality Estimation-guided Data Selection for Domain Adaptation ofSMT.
MT Summit XIV: proceedings of the four-teenth Machine Translation Summit, September 2-6,2013, Nice, France, pages 101?108.Michael Bloodgood and Chris Callison-Burch.
2010.Bucking the Trend: Large-Scale Cost-Focused Ac-tive Learning for Statistical Machine Translation.ACL 2010: the 48th Annual Meeting of the Associ-ation for Computational Linguistics, Uppsala, Swe-den, July 11-16, 2010, pages 854?864.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Matthias Eck, Stephan Vogel, and Alex Waibel.
2005.Low Cost Portability for Statistical Machine Trans-lation based on N-gram Frequency and TF-IDF.IWSLT 2005: Proceedings of the InternationalWorkshop on Spoken Language Translation.
Octo-ber 24-25, 2005, Pittsburgh, PA.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-basedmachine translation.
Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics on - NAACL ?09.Philipp Koehn and Barry Haddow.
2009.
InteractiveAssistance to Human Translators using StatisticalMachine Translation Methods.
MT Summit XII: pro-ceedings of the twelfth Machine Translation Sum-mit, August 26-30, 2009, Ottawa, Ontario, Canada,pages 73?80.Behrang Mohit and Rebecca Hwa.
2007.
Localiza-tion of Difficult-to-Translate Phrases.
ACL 2007:proceedings of the Second Workshop on StatisticalMachine Translation, June 23, 2007, Prague, CzechRepublic, pages 248?255.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
ACL 2002: 40thAnnual Meeting of the Association for Computa-tional Linguistics, July 2002, Philadelphia, pages311?318.Marion Potet, Laurent Besacier, and Herv?e Blanchon.2010.
The LIG machine translation system forWMT 2010.
ACL 2010: Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 161?166.Marion Potet, Emmanuelle Esperanc?a-Rodier, LaurentBesacier, and Herv?e Blanchon.
2012.
Collectionof a Large Database of French-English SMT OutputCorrections.
LREC 2012: Eighth international con-ference on Language Resources and Evaluation, 21-27 May 2012, Istanbul, Turkey, pages 4043?4048.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
AMTA 2006: Proceedings of the 7th Con-ference of the Association for Machine Translationin the Americas, Visions for the Future of MachineTranslation, August 8-12, 2006, Cambridge, Mas-sachusetts, USA, pages 223?231.Lucia Specia, Kashif Shah, Jose G C de Souza, andTrevor Cohn.
2013.
QuEst - A translation qualityestimation framework.
ACL 2013: Annual Meet-ing of the Association for Computational Linguis-tics, Demo session, August 2013, Sofia, Bulgaria.83
