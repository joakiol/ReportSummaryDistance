Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 28?32,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsTask-Based Evaluation of NLG Systems: Control vs Real-World ContextEhud ReiterDept of Computing ScienceUniversity of Aberdeene.reiter@abdn.ac.ukAbstractCurrently there is little agreement about, oreven discussion of, methodologies for task-based evaluation of NLG systems.
I discussone specific issue in this area, namely the im-portance of control vs the importance of eco-logical validity (real-world context), and sug-gest that perhaps we need to put more empha-sis on ecological validity in NLG evaluations.1 IntroductionTask-based extrinsic evaluation of a Natural Lan-guage Generation (NLG) system involves measuringthe impact of an NLG system on how well subjectsperform a task.
It is usually regarded as the ?goldstandard?
for NLG evaluation, and it is the only typeof evaluation which will be seriously considered bymany external user communities.Despite the importance of task-based evaluations,however, there is surprisingly little discussion (oragreement) in the NLG community about how theseshould be carried out.
In recent years there has beena fair amount of discussion about the appropriateuse of corpus-based metrics, and there seems (defacto) to be some level of agreement about evalua-tions based on opinions of human subjects.
But thereis little discussion and much diversity in task-basedevaluation methodology.In this paper I focus on one one specific method-ological issue, which is the relative importance ofcontrol and ecological validity (real-world context).An ideal task-based evaluation would be controlled,that is the impact of NLG texts would be comparedagainst the impact of controlled or baseline texts ina manner which minimises confounding factors.
Itwould also be ecologically valid, that is the eval-uation would be carried out by representative real-world users in a real-world context while performingreal-world tasks.
Unfortunately, because of prag-matic constraints including time, money, and ethicalapproval, it is not always possible to achieve both ofthese goals.
So which is more important?The methodologies currently used for task-basedevaluation in NLG largely derive from the Human-Computer Interaction community, which in turn arelargely based on methodologies for experiments incognitive psychology.
Now, psychologists placemuch more emphasis on control than on ecologi-cal validity; they regard control as absolutely es-sential, but (with some exceptions) they see littlewrong with conducting experiments on unrepresen-tative subjects (undergraduates) in artificial contexts(psychology labs).
Indeed many psychologists arenow embracing web-based experiments, where theydo not even know who the subjects are and what con-texts they are working in.
For the research goals ofpsychologists, this probably makes sense.
But theresearch goals of the NLG community are differentfrom the research goals of the psychological com-munity; should we place more emphasis on ecologi-cal validity than they do, and less on control?My own opinions on this matter are changing.Five years ago, I would have echoed the feeling thatcontrol is all-important.
Now, though, I am begin-ning to think that in order to achieve both NLG?sscientific goals (understanding language and com-putation) and NLG?s technological goals (developing28useful real-world technology), we need to put moreemphasis on ecological validity in our evaluations.2 Evaluation which is both controlled andin real-world context: STOP and DIAGAn ideal evaluation is one which is both controlledand done in a real-world context.
An example is theevaluation of the STOP system.
which generated tai-lored smoking-cessation advice based on the user?sresponse to a questionnaire (Lennox et al, 2001; Re-iter et al, 2003).
The STOP project was a collabora-tion with medical colleagues, and the STOP evalua-tion (which was designed by the medics) was car-ried out as a randomised controlled clinical trial.
Werecruited 2500 smokers, and sent one-third of themSTOP letters, one-third a non-tailored (canned) let-ter, and one-third a letter which just thanked themfor being in out study.
After 6 months we askedparticipants if they had stopped smoking; we testedsaliva samples from people who said they had quitin order to verify their smoking status.
The resultof this evaluation was that the STOP tailored letterswere no more effective than the control non-tailoredletter.
The STOP evaluation cost about UK?75,000,and took about 20 months to design, organise, andcarry out.The STOP evaluation was carried out in a real-world context; the letters were sent to actual smok-ers, and we measured whether they quit smoking.
Itwas also controlled, since the impact of STOP letterswas compared to the impact of non-tailored letters.However there was a lot of ?noise?
(in the statisticalsense) in the STOP evaluation, because different peo-ple (with different personalities, attitudes towardssmoking, personal circumstances, etc) received thetailored and non-tailored letters, and this impactedsmoking-cessation rates in the the three groups.Another evaluation which was controlled and wasdone at least partially in a real-world context was theevaluation of the DIAG-NLP intelligent tutoring sys-tem (di Eugenio et al, 2005).
In this experiment, 75students (the appropriate subject group for this tu-toring system) were divided into three groups: twogroups interacted with two versions of the DIAG-NLP system, and a third interacted with a controlversion of DIAG which did not include any NLG.
Ef-fectiveness was measured by learning gain (changein knowledge, measured by differences in scores ina pre-test and post-test), which is standard in the tu-toring system domain.
The evaluation showed thatstudents learned more from the second (more ad-vanced) version of the DIAG-NLP system than fromthe non-NLG version of DIAG.The DIAG-NLP evaluation was controlled, and itwas real-world in the sense that it used represen-tative subjects and measured real-world outcome.However, it appears (the paper is not completely ex-plicit about this) that the evaluation assessed learn-ing about a topic (fixing a home heating system)which was not part of the student?s normal curricu-lum; if this is the case, then the evaluation was not100% in a real-world context.3 Evaluation which is controlled but notreal-world: BT-45 and Young (1999)The Babytalk project (Gatt et al, 2009) developedseveral NLG systems which summarised clinical datafrom babies in neonatal intensive care (NICU), fordifferent audiences and purposes; one of these sys-tems, BT45 (Portet et al, 2009), summarised 45minutes of data for doctors and nurses, to supportimmediate decision-making.
Babytalk was a collab-orative project with clinical staff and psychologists,and the psychologists designed the BT45 evaluation(van der Meulen et al, 2010).We picked 24 data sets (scenarios) based on his-torical data from babies who had been in NICU5 years previously, and for each data set cre-ated three presentations: visualisation, computer-generated text, and human-written text.
For eachdata set, we also asked expert consultants what ac-tions should be taken by medical staff.
We thenasked 35 medical staff (doctors and nurses of var-ied expertise levels) to look at the scenarios using amix of presentations, in a Latin Square design; eg,1/3 of the subjects saw the visualisation of scenario1 data, 1/3 saw the computer-generated summaryof scenario 1 data, and 1/3 saw the human-writtensummary of this data.
Also each subject saw thesame number of scenarios in each condition, this re-duced the impact of individual differences betweensubjects.
Subjects were asked to make decisionsabout appropriate medical actions (or say no actionshould be taken), and responses were compared to29the ?gold standard?
recommendations from the con-sultants.
The result was that decision performancewas best with the human-written summaries; therewas no significant difference between overall deci-sion performance with the computer-generated sum-maries and the visualisation (although at the levelof individual scenarios, computer texts were moreeffective in some scenarios, and visualisations wasmore effective in other scenarios).
The BT45 evalua-tion cost about UK?20,000, and took about 6 monthsto design, organise, and carry out.The BT45 evaluation was carefully controlledHowever, it was not done in a real-world context.Doctors and nurses sat in an experiment room (notin the ward) and looked at data from babies theydid not remember (as opposed to babies whom theyknew well because they has been looking after themfor the past few weeks); they also did not visuallyobserve the babies, which is a very important infor-mation source for NICU staff.Many other task-based evaluations of NLG sys-tems have been controlled but not done in a real-world context, including the very first task-basedNLG evaluation I am aware of, by Young (1999).Young developed four algorithms for generating in-structional texts, and tested these by asking 26 stu-dents to follow the instructions generated by the var-ious algorithms on several scenarios, and measurederror rates in carrying out the instructions.
The in-structions involved carrying out actions on campus(going to labs, playing in soccer matches, etc).
Thestudents did not actually carry out these actions, in-stead they interacted with a ?text-based virtual real-ity system?.
Hence the evaluation was controlled butnot carried out in real-world context.4 Evaluation which is real-world but notcontrolled: BT-NurseThe next Babytalk system (after BT45) was BT-NURSE; it generated summaries of 12-hours of clin-ical data, to support nursing shift handover (Hunteret al, 2011).
We initially expected to evaluate BT-NURSE using a similar methodology to the BT45evaluation.
However the medical people involvedin BabyTalk complained that it was unrealistic toevaluate the system in an artificial controlled con-text, where clinical staff were looking at data out ofcontext.
So instead we evaluated BT-NURSE by in-stalling the system in the NICU, so that nurses usedit to get information about babies they were actu-ally caring for.
The primary outcome measure wassubjective ratings by nurses as to the helpfulness ofBT-NURSE texts; and indeed most nurses thought thetexts were helpful.The BT-NURSE evaluation was significantly moreexpensive than the BT45 evaluation, because wehired a full-time software engineer for a year toensure that the software was sufficiently well en-gineered so that it could be deployed and used inthe hospital; we were also required by the medicalethics committee to have a research nurse on-sitewho checked texts for errors before they were shownto the duty nurses, and removed them from the ex-periment if they were factually incorrect and coulddamage patient care (in fact this never happened, theresearch nurse did not regard any of the BT-NURSEtexts as potentially harmful from this perspective).All in all cost was probably about UK?50,000, andthe entire process (including the software engineer-ing) took about 18 months.The BT-NURSE evaluation was not controlled; wedid not compare the computer generated texts toanything else, and indeed did not directly measureany task outcome variable, instead we solicited opin-ions as to utility.
It was however ecologically valid,since it was carried out by asking nurses (real-worldusers) to use BT-NURSE for care planning (real-world task) in a real-world context (on-ward, involv-ing babies the nurses were familiar with and couldvisually observe).5 DiscussionIdeally a task-based evaluation should be both con-trolled and ecologically valid (done in a real-worldcontext).
But if it is not possible to achieve bothof these objectives, which is most important?
Obvi-ously in many cases the desires of collaborators needto be considered; for example psychologists gener-ally place much more emphasis on control than onecological validity, whereas many commercial or-ganisation take the opposite perspective.
But whichis more important from an NLG perspective?From a pragmatic perspective, two important ar-guments for focusing on control are cost and publi-30cations.
The figures given above suggest that doingan evaluation in a real-world context makes it sub-stantially more expensive.
Of course this is basedon very limited data, but I believe this is correct,deploying a system in a real-world context requiresaddressing engineering and ethical issues which areexpensive and time-consuming to resolve.
From apublications perspective; most NLG reviewers aremuch more concerned about control than about eco-logical validity.
Especially in high-prestige venues,reviewers are likely to complain about uncontrolledevaluations, while making little (if any) mention ofconcerns about lack of ecological validity.For what its worth, my own view on this issue haschanged.
If asked five years ago, I would have saidthat control was more important, but now I am veer-ing more towards ecological validity.
The techno-logical goal of NLG is to develop technology whichis used in real-world applications, and from this per-spective if we do not evaluate in real-world contexts,we risk being side-tracked into technology whichlooks good in a controlled environment but is uselessin the real world.
Similarly, if our goal is to developa better scientific understanding of computation andlanguage, I think we have to look at how languageis used in real-world contexts, which (at least in mymind) is quite different from how language is usedin artificial contexts.Plaisant (2004) made some related points in herdiscussion of evaluation of information visualisa-tion.
She pointed out that controlled evaluationsof visualisation systems in artificial contexts mightbe less informative than uncontrolled evaluationsin real-world contexts.
She also pointed out thatcontrolled evaluations could not evaluate some ofthe most important benefits of visualisation systems.For example, sometimes the primary objective of vi-sualisation systems is to support scientific discov-ery, that is to make it easier for scientists who areanalysing data to come up with new insights andhypotheses.
However, testing effectiveness at sup-porting scientific discovery in a controlled fashionis almost impossible.
Perhaps in theory one couldcompare the ?productivity?
of two groups of scien-tists, one with and one without visualisation tools,but the comparison would have to involve a largenumber of scientists over a period of months or evenyears, with scientists in one group not allowed tocommunicate with scientists in the other group.
Itis difficult to imagine that such an experiment couldin fact be carried out (or that it would be approvedby a research ethics committee).
Plaisant argues thatfocusing on controlled experiments means focusingon things that are easily measurable in such experi-ments, which may lead researchers to ignore the out-comes that we really care about.Another important point is that the goal of evalua-tion is not just to assess if something works, but alsoto come up with insights as to how to improve analgorithm, module, or system.
In NLG evaluationssuch insights are often based on free-text commentsmade by subjects, and in my experience better andmore insightful comments are obtained from evalu-ations in real-world contexts.An important potential caveat is that all of the ex-amples cited above were system evaluations, whichattempted to assess how useful a system was from anapplied perspective.
If the goal of an evaluation is totest a scientific theory or model, should we always(as psychologists do) favour control over ecologicalvalidity?
My own belief is that the psychologists aremissing important insights and findings by ignoringecological validity, and the most effective way forthe NLG community to ?add value?
to the enterpriseof understanding language is not to imitate the psy-chologists, but rather to use a different experimentalparadigm, which focuses much more on ecologicalvalidity.
But others will no doubt disagree.6 ConclusionIt is difficult to choose between control and ecolog-ical validity, because clearly both greatly contributeto the usefulness of an evaluation.
But this trade-off must be made in many cases, and it would bepreferable for it to be explicitly discussed.
And ofcourse there are many other desirable factors whichmay need to be involved in a tradeoff; for example,how important is it that subjects be representative ofthe user community, instead of whoever is easiestto recruit (eg, undergraduates).
My hope is that theNLG community can explicitly discuss such issues,and come up with recommended evaluation method-ologies for task-based studies, which are based thescientific and technological objectives of our com-munity.31ReferencesBarbara di Eugenio, Davide Fossati, Dan Yu, SusanHaller, and Michael Glass.
2005.
Aggregation im-proves learning: experiments in natural language gen-eration for intelligent tutoring systems.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, pages 50?57.
Associationfor Computational Linguistics.Albert Gatt, Francois Portet, Ehud Reiter, Jum Hunter,Saad Mahamood, Wendy Moncur, and SomayajuluSripada.
2009.
From data to text in the neonatal in-tensive care unit: Using NLG technology for decisionsupport and information management.
AI Communi-cations, 22(3):153?186.James Hunter, Yvonne Freer, Albert Gatt, Ehud Reiter,Somayajulu Sripada, Cindy Sykes, and Dave Westwa-ter.
2011.
BT-Nurse: Computer generation of natu-ral language shift summaries from complex heteroge-neous medical data.
Journal of the Americal MedicalInformatics Association.
In press.Scott Lennox, Liesl Osman, Ehud Reiter, Roma Robert-son, James Friend, Ian McCann, Diane Skatun, andPeter Donnan.
2001.
The cost-effectiveness ofcomputer-tailored and non-tailored smoking cessationletters in general practice: A randomised controlledstudy.
British Medical Journal, 322:1396?1400.Catherine Plaisant.
2004.
The challenge of informationvisualization evaluation.
In Proceedings of AdvancedVisual Interfaces (AVI) 2004.Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,Somayajulu Sripada, Yvonne Freer, and Cindy Sykes.2009.
Automatic generation of textual summariesfrom neonatal intensive care data.
Artificial Intelli-gence, 173(7-8):789?816.Ehud Reiter, Roma Robertson, and Liesl Osman.
2003.Lessons from a failure: Generating tailored smokingcessation letters.
Artificial Intelligence, 144:41?58.Marianne van der Meulen, Robert Logie, Yvonne Freer,Cindy Sykes, Neil McIntosh, and Jim Hunter.
2010.When a graph is poorer than 100 words: A compari-son of computerised natural language generation, hu-man generated descriptions and graphical displays inneonatal intensive care.
Applied Cognitive Psychol-ogy, 24(1):77?89.Michael Young.
1999.
Using Grice?s maxim of quan-tity to select the content of plan descriptions.
ArtificialIntelligence, 115:215?256.32
