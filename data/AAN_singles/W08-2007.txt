Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 49?52Manchester, August 2008Semantic structure from Correspondence AnalysisBarbara McGillivrayDipartimento di LinguisticaUniversit`a di PisaPisa, Italybarbara.mcgillivray@aksis.uib.noChrister JohanssonDept.
of LinguisticsUniversity of BergenBergen, Norwaychrister.johansson@uib.noDaniel ApollonText Technology Lab.Aksis, UNIFOBBergen, Norwaydaniel.apollon@aksis.uib.noAbstractA common problem for clustering tech-niques is that clusters overlap, whichmakes graphing the statistical structure inthe data difficult.
A related problem isthat we often want to see the distributionof factors (variables) as well as classes(objects).
Correspondence Analysis (CA)offers a solution to both these problems.The structure that CA discovers may be animportant step in representing similarity.We have performed an analysis for Italianverbs and nouns, and confirmed that simi-lar structures are found for English.1 IntroductionOver the past years, distributional methods havebeen used to explore the semantic behaviour ofverbs, looking at their contexts in corpora (Lan-dauer and Laham, 1998; Redington and Finch,1998; Biemann, 2006, inter al.).
We follow a gen-eral approach suggested already by Firth (1957),to associate distributional similarity with semanticsimilarity.One question concerns the syntax-semantics in-terface.
Results using distributions of verbs in con-text had an impact on verb classification (Levin,1993), automatic verb clustering (Schulte imWalde, 2003), and selectional preference acquisi-tion (Resnik, 1993; Li and Abe, 1995; McCarthy,2001; Agirre and Martinez, 2001, inter al.
).In automatic verb clustering, verbs are repre-sented by vectors of a multidimensional spacewhose dimensions (variables) are identified bysome linguistic features, ranging, for example,from subcategorization frames to participation inc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.diathesis alternations and lexical selectional pref-erences.
The verbs cluster on co-occurrence withthe features chosen, and such information providea generalisation over the verbs with respect to thevariables.In the case of selectional preference acquisition,a verb (or a verb class) is associated to a class ofnouns that can be the lexical fillers of a case frameslot for the verb.
This allows us to calculate theassociation strength between the verb and its fillernouns.
The generalisation step is performed for thecase frame instances (observations) and producesmore abstract noun classes that can be applied tounseen cases.
This often utilizes hierarchies of ex-isting thesauri or wordnets.We propose a method that uses CorrespondenceAnalysis (CA) to study the distribution (and asso-ciated semantic behaviour) of a list of verbs withnouns occurring in a particular syntactic relation,for example their subjects.
This is collected froma corpus, and reflects usage in that corpus.
Un-like clustering methods, this technique does notimply an exclusive choice between a) classifyingverbs on the basis of the noun fillers in their syn-tactic frame, or b) associating noun classes to verbs(sometimes mediated by a semantic hierarchy).
In-stead, this approach yields a geometric representa-tion of the relationships between the nouns and theverbs in a common dual space (biplot).
CA aimsto find an overall structure (if any) of the data.
Themethod emphasizes unusual observations, as de-viance from the expected is what creates the axesof the analysis.
CA generalizes over the actual oc-currences of verb-noun pairs in the corpus, and vi-sualizes the shape of the correspondence space.When associating verbs with nouns, CA takes asinput a contingency table (here rows correspond tothe verbs, and columns correspond to their subjectfillers).
Each verb is a row point in the multidimen-sional noun space, and each noun is a column pointin the multidimensional verb space.
The CA goals49are to reduce the dimension of the dual originalspace, and to find an optimal subspace that is theclosest to this cloud of points in the ?2-metric.
Thebest subspace is determined by finding the smallestnumber of orthogonal axes that describe the mostvariance from the original cloud.Finally the coordinates of both row and col-umn points of the ?2contingency table are pro-jected onto this optimal subspace, simultaneouslydisplaying row and column points.
If we considerthose points that are well represented, the closerthey are in this geometric representation, the moresimilar their original distributions are.
In this way,we can detect not only that there is a relationshipbetween the verb (e.g.
explode) and the noun (e.g.bomb), but also how each word relates to eachother word.2 Correspondence AnalysisCA is a data analytic technique developed byBenz?ecri in the 1960s, which has been widely usedin describing large contingency tables and binarydata.
At the heart of CA is Singular Value Decom-position (SVD), from which many other methodswere derived (Biplot, Classical Principal Compo-nent Analysis, PCA and more).Compared to usual clustering methods, CAgives a more fine-grained view of the spread ofthe input points.
Benz?ecri (1973) points out thatCA is more efficient than clustering in terms of de-composition of variance.
Secondly, CA representspossible regions in space with varying density, andproduces a flexible ?compound clustering?
on bothobjects and variables.
Verb-nouns association pro-files may not cluster in distinct space regions, butmay be evenly distributed, follow a gradient-likedistribution, or show overlapping clusters.
In suchdifficult cases for clustering, CA is able to offera representation of the geometry of the input pro-files.
Finally, CA offers the possibility of recon-structing the original space from the output sub-space.Let us consider a data matrix M whose size is(r, c), the (i, j)thentry of M containing the num-ber of occurrences of verb j with noun i as its sub-ject in a corpus.
We calculate the relative frequen-cies by dividing each entry M(i, j) by the sum ofrow i, i. e. the frequency of noun i, to get the ma-trix of row profiles R(i, j).
Therefore, the moresimilar two row profiles i1and i2are, the morethese two nouns can be considered as distributionalsynonyms.The next step implies comparing the row pro-files with the average distribution where each entry(i, j) is the product of the frequency of noun i bythe frequency of verb j divided by the grand totalN of the table.
This comparison is calculated us-ing the ?2-distance (i.e.
a weighted Euclidean dis-tance), which eliminates effects of high frequencyalone.
The next formula shows calculations forrows.
Calculations for columns are analogous.
?2(i1, i2) =c?j=1(R(i1, j) ?R(i2, j))2?ri=1M(i, j)The ?2-distance between a profile point and the av-erage profile (barycentre) is called inertia of theprofile point and the total inertia measures howthe individual profiles piare spread around thebarycentre:Inertia =1Nr?i=1c?j=1M(i, j)?2(pi, p?
)CA then searches for the optimal subspace S thatminimises the distance from the profile points.Once specified its dimension k ?
min(r ?
1, c ?1), S is found by applying the Singular Value De-composition (SVD) to matrix R ?
1p?
, which de-composes it as the product N ?D ?M : where D isa diagonal matrix with positive coefficients ?1??2?
.
.
.
?
?k(singular values) and N and M areorthonormal matrices (NTN = MTM = I).
Therows of M are the orthonormal basis vectors thatdefine S (called principal axes of inertia) and therows of matrix F = N ?
D are the projections ofthe row profiles onto S. For k = 2, this allows usto plot the new coordinates in a two-dimensionalspace and get the correspondence analysis of therow profiles.The total inertia is decomposed into the direc-tion of the principal axes of inertia.
The first axisrepresents the direction where the inertia of thecloud is the maximum; the second axis maximisesthe inertia among all the directions orthogonal tothe first axis, and so on.The geometry of column profiles can be anal-ysed similarly, because the two problems are di-rectly linked and two transition formulae can beused to pass from one coordinate system to theother, explaining the French name ?analyse descorrespondances?.As a result, both analyses decompose the sameinertia into the same system of principal axes.
Thisallows us to merge the two representations in onesingle geometric display showing at the same time50the projections of row and column points in thesubspace.In addition to this dual space representation, CAgives a system of diagnostic measures for each ofthe two dual spaces:?
contributions of the rows (and columns) to theaxes, i. e. the inertia of the points projected ontothe axes, which contributes to the principal inertia;?
contributions of the axes to the row (and col-umn) points;?
quality of representation (cumulative sum ofcontributions of the axes for each point); this high-lights well represented points.3 ExplorationsWe performed a CA using the Matlab AnalyticaToolbox developed by Daniel Apollon.
We testedthis technique first on the Italian newspaper corpusLA REPUBBLICA, which consists of 450 millionword tokens.
This corpus was syntactically parsedusing the MALT dependency parser (Nivre, 2006).A list of 196 verbs was compiled following the listof German verbs contained in (Schulte im Walde,2003) and adapting it to Italian.
Looking at thesyntactic analyses of the corpus where the verbs ofthe list showed a subcategorization frame contain-ing a subject slot, their lexical subject fillers wereautomatically extracted.
The matrix M , whose2553 row entries correspond to the nouns extractedas subject fillers, was then used as input for the CA(|M | = 196?
2553 = 500388).Starting from the quality of representationscores of this analysis, we isolated a set of pointswith increasingly good representation, ending withan extremely faithful and low dimensional rep-resentation.
We called this method ?incrementalpruning?.
Figure 1 shows the dual display of theanalysis for the Italian data in a two dimensionalspace, after filtering out those points showing aquality of representation below a threshold of 30%.We can conceptualize the data set C after a CAas the cumulative effect of three different underly-ing phenomena: K, R and E.K can be seen as a reduction of the latent struc-ture of C; it contains its core structure as it hasbeen underlined by the analysis and left after prun-ing.R refers to the residual variance, not included inthe core analysis.
It contains the most predictablepoints1, which are plotted near to origin (barycen-1In our data: pronouns she, I, he, every-, no-, some-body,Figure 1: Correspondence graph for Italiantre of the data cloud).
These points give a smallcontribution to the inertia of the principal axes.E contains the error in the representation, aswell as badly represented points.Points far from the origin display strong struc-ture; they may correspond to rare words used inspecial contexts.
Figure 1 shows that words relatedto destruction2are aligned in the same direction,whereas the second vector is mainly constituted bynouns and verbs that have to do with the politicaland legal area3.
The first principal axis accountsfor nearly 16% of the total inertia, whereas thesecond axis accounts for 12%.
The first six axesaccounts for over 70% of variation.
Many wordswere not well represented, but contribute to vari-ance.We confirmed our method on English, using theBritish National Corpus4.
A similar structure wasfound.
We restrict ourselves to reproduce the graphfor Italian.who, nouns with partly pronominal qualities husband, wife,friend, sir, son, father, mother, fact, event.2Along the y-axis from top down to the middle, we find thenouns flame, extend, stick of dynamite, excavator, chemother-apy, effusion, blaze, seism, demon, dynamite, fire, aviation,earthquake, artificer, explosive device, insect, gas, landslide,virus, rain, bulldozer, hurricane, wave, speculation, artillery,remorse, bomb, missile, violence, revolution, etc.3Along the x-axis we find, from left to the middle,the nouns order, regulations, norm, code, legislation, rules,treaty, constitution, circular letter, system, directive, law, de-cree, article, judgement, amendment, court, etc.4via sketchengine http://www.sketchengine.co.uk514 ConclusionCA detects a structure for Italian verb-noun cor-respondences in LA REPUBBLICA (?
450 mil-lion words).
A similar structure was confirmed us-ing BNC for English.
Both global and local struc-tures are found, which gives possibilities to rep-resent lexical units with reference to both princi-pal axes and word similarity.
The main dimen-sions of the Italian corpus are topical (crime re-lated vs. natural catastrophes, and laws vs. po-litical institutions).
Semantic relatedness were ob-served in closely mapped words.
Both global andlocal structure is found, and we can speculate thatthis helps representing lexical units in semantic la-beling (Giuglea and Moschitti, 2006) for machinelearning tasks.
We can conceptualize text graphs intwo distinct usages: knowledge re-presenting (e.g.FrameNet) and visualizing relations in a data set.Our method belongs in the second category.AcknowledgementsThe first author is a MULTILINGUA fellow atUni.
Bergen, financially supported by a MarieCurie action (European Commission).
We wish tothank the anonymous reviewers who gave impor-tant leads to future research.ReferencesAgirre, Eneko and David Martinez.
2001.
Learningclass-to-class selectional preferences.
In Proc.
ofthe ACL/EACL Workshop on Computational NaturalLanguage Learning, pages 1?8, Toulouse, France.Benz?ecri, Jean-Paul.
1973.
L?Analyse des Donn?ees,volume 1.
Dunod.Biemann, Chris.
2006.
Chinese Whispers ?
anEfficient Graph Clustering Algorithm and its Ap-plication to Natural Language Processing Prob-lems.
In Proc.
of the HLT-NAACL-06 Workshop onTextgraphs-06, pages 73?80, New York, USA.Firth, John R., 1957.
Studies in Linguistic Analysis,chapter A synopsis of linguistic theory 1930-1955.Philological Society.Giuglea, Ana-Maria and Alessandro Moschitti.
2006.Semantic Role Labeling via FrameNet, VerbNet andPropBank.
In Proc.
of the 21st Int.
Conf.
on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages929?936, Sydney, Australia.Landauer, Thomas K., Peter W. Foltz and Darrell La-ham.
1998.
Introduction to Latent Semantic Analy-sis.
Discourse Processes, 25:259?284.Levin, Beth.
1993.
English Verb Classes and Alterna-tions.
The University of Chicago Press.Li, Hang and Naoki Abe.
1995.
Generalizing caseframes using a thesaurus and the MDL principle.
InProc.
of Recent Advances in Natural Language Tech-nology, pages 239?248.McCarthy, Diana.
2001.
Lexical Acquisition at theSyntax-Semantics Interface: Diathesis Alternations,Subcategorization Frames and Selectional Prefer-ences.
Ph.D. thesis, University of Sussex.Nivre, Joakim.
2006.
Inductive Dependency Parsing.Springer.Redington, Martin, Nick Chater and Steven Finch.1998.
Distributional information: A powerful cuefor acquiring syntactic categories.
Cognitive Sci-ence, 22:425?469.Resnik, Philip.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania.Schulte im Walde, Sabine.
2003.
Experiments onthe Automatic Induction of German Semantic VerbClasses.
Ph.D. thesis, Institut f?ur MaschinelleSprachverarbeitung, Universit?at Stuttgart.52
