Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715?1725,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNeural Machine Translation of Rare Words with Subword UnitsRico Sennrich and Barry Haddow and Alexandra BirchSchool of Informatics, University of Edinburgh{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.ukAbstractNeural machine translation (NMT) mod-els typically operate with a fixed vocabu-lary, but translation is an open-vocabularyproblem.
Previous work addresses thetranslation of out-of-vocabulary words bybacking off to a dictionary.
In this pa-per, we introduce a simpler and more ef-fective approach, making the NMT modelcapable of open-vocabulary translation byencoding rare and unknown words as se-quences of subword units.
This is based onthe intuition that various word classes aretranslatable via smaller units than words,for instance names (via character copyingor transliteration), compounds (via com-positional translation), and cognates andloanwords (via phonological and morpho-logical transformations).
We discuss thesuitability of different word segmentationtechniques, including simple character n-gram models and a segmentation based onthe byte pair encoding compression algo-rithm, and empirically show that subwordmodels improve over a back-off dictionarybaseline for the WMT 15 translation tasksEnglish?German and English?Russianby up to 1.1 and 1.3 BLEU, respectively.1 IntroductionNeural machine translation has recently shownimpressive results (Kalchbrenner and Blunsom,2013; Sutskever et al, 2014; Bahdanau et al,2015).
However, the translation of rare wordsis an open problem.
The vocabulary of neu-ral models is typically limited to 30 000?50 000words, but translation is an open-vocabulary prob-The research presented in this publication was conductedin cooperation with Samsung Electronics Polska sp.
z o.o.
-Samsung R&D Institute Poland.lem, and especially for languages with produc-tive word formation processes such as aggluti-nation and compounding, translation models re-quire mechanisms that go below the word level.As an example, consider compounds such as theGerman Abwasser|behandlungs|anlange ?sewagewater treatment plant?, for which a segmented,variable-length representation is intuitively moreappealing than encoding the word as a fixed-lengthvector.For word-level NMT models, the translationof out-of-vocabulary words has been addressedthrough a back-off to a dictionary look-up (Jean etal., 2015; Luong et al, 2015b).
We note that suchtechniques make assumptions that often do nothold true in practice.
For instance, there is not al-ways a 1-to-1 correspondence between source andtarget words because of variance in the degree ofmorphological synthesis between languages, likein our introductory compounding example.
Also,word-level models are unable to translate or gen-erate unseen words.
Copying unknown words intothe target text, as done by (Jean et al, 2015; Luonget al, 2015b), is a reasonable strategy for names,but morphological changes and transliteration isoften required, especially if alphabets differ.We investigate NMT models that operate on thelevel of subword units.
Our main goal is to modelopen-vocabulary translation in the NMT networkitself, without requiring a back-off model for rarewords.
In addition to making the translation pro-cess simpler, we also find that the subword modelsachieve better accuracy for the translation of rarewords than large-vocabulary models and back-offdictionaries, and are able to productively generatenew words that were not seen at training time.
Ouranalysis shows that the neural networks are able tolearn compounding and transliteration from sub-word representations.This paper has two main contributions:?
We show that open-vocabulary neural ma-1715chine translation is possible by encoding(rare) words via subword units.
We find ourarchitecture simpler and more effective thanusing large vocabularies and back-off dictio-naries (Jean et al, 2015; Luong et al, 2015b).?
We adapt byte pair encoding (BPE) (Gage,1994), a compression algorithm, to the taskof word segmentation.
BPE allows for therepresentation of an open vocabulary througha fixed-size vocabulary of variable-lengthcharacter sequences, making it a very suit-able word segmentation strategy for neuralnetwork models.2 Neural Machine TranslationWe follow the neural machine translation archi-tecture by Bahdanau et al (2015), which we willbriefly summarize here.
However, we note that ourapproach is not specific to this architecture.The neural machine translation system is imple-mented as an encoder-decoder network with recur-rent neural networks.The encoder is a bidirectional neural networkwith gated recurrent units (Cho et al, 2014)that reads an input sequence x = (x1, ..., xm)and calculates a forward sequence of hiddenstates (?
?h1, ...,?
?hm), and a backward sequence(?
?h1, ...,??hm).
The hidden states??hjand?
?hjareconcatenated to obtain the annotation vector hj.The decoder is a recurrent neural network thatpredicts a target sequence y = (y1, ..., yn).
Eachword yiis predicted based on a recurrent hiddenstate si, the previously predicted word yi?1, anda context vector ci.
ciis computed as a weightedsum of the annotations hj.
The weight of eachannotation hjis computed through an alignmentmodel ?ij, which models the probability that yiisaligned to xj.
The alignment model is a single-layer feedforward neural network that is learnedjointly with the rest of the network through back-propagation.A detailed description can be found in (Bah-danau et al, 2015).
Training is performed on aparallel corpus with stochastic gradient descent.For translation, a beam search with small beamsize is employed.3 Subword TranslationThe main motivation behind this paper is thatthe translation of some words is transparent inthat they are translatable by a competent transla-tor even if they are novel to him or her, basedon a translation of known subword units such asmorphemes or phonemes.
Word categories whosetranslation is potentially transparent include:?
named entities.
Between languages that sharean alphabet, names can often be copied fromsource to target text.
Transcription or translit-eration may be required, especially if the al-phabets or syllabaries differ.
Example:Barack Obama (English; German)?????
?????
(Russian)???????
(ba-ra-ku o-ba-ma) (Japanese)?
cognates and loanwords.
Cognates and loan-words with a common origin can differ inregular ways between languages, so thatcharacter-level translation rules are sufficient(Tiedemann, 2012).
Example:claustrophobia (English)Klaustrophobie (German)?????????????
(Klaustrofobi?)
(Russian)?
morphologically complex words.
Words con-taining multiple morphemes, for instanceformed via compounding, affixation, or in-flection, may be translatable by translatingthe morphemes separately.
Example:solar system (English)Sonnensystem (Sonne + System) (German)Naprendszer (Nap + Rendszer) (Hungarian)In an analysis of 100 rare tokens (not amongthe 50 000 most frequent types) in our Germantraining data1, the majority of tokens are poten-tially translatable from English through smallerunits.
We find 56 compounds, 21 names,6 loanwords with a common origin (emanci-pate?emanzipieren), 5 cases of transparent affix-ation (sweetish ?sweet?
+ ?-ish??
s?
?lich ?s???
+?-lich?
), 1 number and 1 computer language iden-tifier.Our hypothesis is that a segmentation of rarewords into appropriate subword units is suffi-cient to allow for the neural translation networkto learn transparent translations, and to general-ize this knowledge to translate and produce unseenwords.2We provide empirical support for this hy-1Primarily parliamentary proceedings and web crawl data.2Not every segmentation we produce is transparent.While we expect no performance benefit from opaque seg-mentations, i.e.
segmentations where the units cannot betranslated independently, our NMT models show robustnesstowards oversplitting.1716pothesis in Sections 4 and 5.
First, we discuss dif-ferent subword representations.3.1 Related WorkFor Statistical Machine Translation (SMT), thetranslation of unknown words has been the subjectof intensive research.A large proportion of unknown words arenames, which can just be copied into the tar-get text if both languages share an alphabet.
Ifalphabets differ, transliteration is required (Dur-rani et al, 2014).
Character-based translation hasalso been investigated with phrase-based models,which proved especially successful for closely re-lated languages (Vilar et al, 2007; Tiedemann,2009; Neubig et al, 2012).The segmentation of morphologically complexwords such as compounds is widely used for SMT,and various algorithms for morpheme segmen-tation have been investigated (Nie?en and Ney,2000; Koehn and Knight, 2003; Virpioja et al,2007; Stallard et al, 2012).
Segmentation al-gorithms commonly used for phrase-based SMTtend to be conservative in their splitting decisions,whereas we aim for an aggressive segmentationthat allows for open-vocabulary translation with acompact network vocabulary, and without havingto resort to back-off dictionaries.The best choice of subword units may be task-specific.
For speech recognition, phone-level lan-guage models have been used (Bazzi and Glass,2000).
Mikolov et al (2012) investigate subwordlanguage models, and propose to use syllables.For multilingual segmentation tasks, multilingualalgorithms have been proposed (Snyder and Barzi-lay, 2008).
We find these intriguing, but inapplica-ble at test time.Various techniques have been proposed to pro-duce fixed-length continuous word vectors basedon characters or morphemes (Luong et al, 2013;Botha and Blunsom, 2014; Ling et al, 2015a; Kimet al, 2015).
An effort to apply such techniquesto NMT, parallel to ours, has found no significantimprovement over word-based approaches (Linget al, 2015b).
One technical difference from ourwork is that the attention mechanism still oper-ates on the level of words in the model by Linget al (2015b), and that the representation of eachword is fixed-length.
We expect that the attentionmechanism benefits from our variable-length rep-resentation: the network can learn to place atten-tion on different subword units at each step.
Re-call our introductory example Abwasserbehand-lungsanlange, for which a subword segmentationavoids the information bottleneck of a fixed-lengthrepresentation.Neural machine translation differs from phrase-based methods in that there are strong incentives tominimize the vocabulary size of neural models toincrease time and space efficiency, and to allow fortranslation without back-off models.
At the sametime, we also want a compact representation of thetext itself, since an increase in text length reducesefficiency and increases the distances over whichneural models need to pass information.A simple method to manipulate the trade-off be-tween vocabulary size and text size is to use short-lists of unsegmented words, using subword unitsonly for rare words.
As an alternative, we pro-pose a segmentation algorithm based on byte pairencoding (BPE), which lets us learn a vocabularythat provides a good compression rate of the text.3.2 Byte Pair Encoding (BPE)Byte Pair Encoding (BPE) (Gage, 1994) is a sim-ple data compression technique that iteratively re-places the most frequent pair of bytes in a se-quence with a single, unused byte.
We adapt thisalgorithm for word segmentation.
Instead of merg-ing frequent pairs of bytes, we merge characters orcharacter sequences.Firstly, we initialize the symbol vocabulary withthe character vocabulary, and represent each wordas a sequence of characters, plus a special end-of-word symbol ??
?, which allows us to restore theoriginal tokenization after translation.
We itera-tively count all symbol pairs and replace each oc-currence of the most frequent pair (?A?, ?B?)
witha new symbol ?AB?.
Each merge operation pro-duces a new symbol which represents a charac-ter n-gram.
Frequent character n-grams (or wholewords) are eventually merged into a single sym-bol, thus BPE requires no shortlist.
The final sym-bol vocabulary size is equal to the size of the initialvocabulary, plus the number of merge operations?
the latter is the only hyperparameter of the algo-rithm.For efficiency, we do not consider pairs thatcross word boundaries.
The algorithm can thus berun on the dictionary extracted from a text, witheach word being weighted by its frequency.
Aminimal Python implementation is shown in Al-1717Algorithm 1 Learn BPE operationsimport re, collectionsdef get_stats(vocab):pairs = collections.defaultdict(int)for word, freq in vocab.items():symbols = word.split()for i in range(len(symbols)-1):pairs[symbols[i],symbols[i+1]] += freqreturn pairsdef merge_vocab(pair, v_in):v_out = {}bigram = re.escape(' '.join(pair))p = re.compile(r'(?<!\S)' + bigram + r'(?
!\S)')for word in v_in:w_out = p.sub(''.join(pair), word)v_out[w_out] = v_in[word]return v_outvocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}num_merges = 10for i in range(num_merges):pairs = get_stats(vocab)best = max(pairs, key=pairs.get)vocab = merge_vocab(best, vocab)print(best)r ?
?
r?l o ?
lolo w ?
lowe r?
?
er?Figure 1: BPE merge operations learned from dic-tionary {?low?, ?lowest?, ?newer?, ?wider?
}.gorithm 1.
In practice, we increase efficiency byindexing all pairs, and updating data structures in-crementally.The main difference to other compression al-gorithms, such as Huffman encoding, which havebeen proposed to produce a variable-length en-coding of words for NMT (Chitnis and DeNero,2015), is that our symbol sequences are still in-terpretable as subword units, and that the networkcan generalize to translate and produce new words(unseen at training time) on the basis of these sub-word units.Figure 1 shows a toy example of learned BPEoperations.
At test time, we first split words intosequences of characters, then apply the learned op-erations to merge the characters into larger, knownsymbols.
This is applicable to any word, andallows for open-vocabulary networks with fixedsymbol vocabularies.3In our example, the OOV?lower?
would be segmented into ?low er?
?.3The only symbols that will be unknown at test time areunknown characters, or symbols of which all occurrencesin the training text have been merged into larger symbols,like ?safeguar?, which has all occurrences in our training textmerged into ?safeguard?.
We observed no such symbols attest time, but the issue could be easily solved by recursivelyreversing specific merges until all symbols are known.We evaluate two methods of applying BPE:learning two independent encodings, one for thesource, one for the target vocabulary, or learningthe encoding on the union of the two vocabular-ies (which we call joint BPE).4The former has theadvantage of being more compact in terms of textand vocabulary size, and having stronger guaran-tees that each subword unit has been seen in thetraining text of the respective language, whereasthe latter improves consistency between the sourceand the target segmentation.
If we apply BPE in-dependently, the same name may be segmenteddifferently in the two languages, which makes itharder for the neural models to learn a mappingbetween the subword units.
To increase the con-sistency between English and Russian segmenta-tion despite the differing alphabets, we transliter-ate the Russian vocabulary into Latin characterswith ISO-9 to learn the joint BPE encoding, thentransliterate the BPE merge operations back intoCyrillic to apply them to the Russian training text.54 EvaluationWe aim to answer the following empirical ques-tions:?
Can we improve the translation of rare andunseen words in neural machine translationby representing them via subword units??
Which segmentation into subword units per-forms best in terms of vocabulary size, textsize, and translation quality?We perform experiments on data from theshared translation task of WMT 2015.
ForEnglish?German, our training set consists of 4.2million sentence pairs, or approximately 100 mil-lion tokens.
For English?Russian, the training setconsists of 2.6 million sentence pairs, or approx-imately 50 million tokens.
We tokenize and true-case the data with the scripts provided in Moses(Koehn et al, 2007).
We use newstest2013 as de-velopment set, and report results on newstest2014and newstest2015.We report results with BLEU (mteval-v13a.pl),and CHRF3 (Popovi?c, 2015), a character n-gramF3score which was found to correlate well with4In practice, we simply concatenate the source and targetside of the training set to learn joint BPE.5Since the Russian training text also contains words thatuse the Latin alphabet, we also apply the Latin BPE opera-tions.1718human judgments, especially for translations outof English (Stanojevi?c et al, 2015).
Since ourmain claim is concerned with the translation ofrare and unseen words, we report separate statis-tics for these.
We measure these through unigramF1, which we calculate as the harmonic mean ofclipped unigram precision and recall.6We perform all experiments with Groundhog7(Bahdanau et al, 2015).
We generally follow set-tings by previous work (Bahdanau et al, 2015;Jean et al, 2015).
All networks have a hiddenlayer size of 1000, and an embedding layer sizeof 620.
Following Jean et al (2015), we only keepa shortlist of ?
= 30000 words in memory.During training, we use Adadelta (Zeiler, 2012),a minibatch size of 80, and reshuffle the train-ing set between epochs.
We train a network forapproximately 7 days, then take the last 4 savedmodels (models being saved every 12 hours), andcontinue training each with a fixed embeddinglayer (as suggested by (Jean et al, 2015)) for 12hours.
We perform two independent training runsfor each models, once with cut-off for gradientclipping (Pascanu et al, 2013) of 5.0, once witha cut-off of 1.0 ?
the latter produced better singlemodels for most settings.
We report results of thesystem that performed best on our development set(newstest2013), and of an ensemble of all 8 mod-els.We use a beam size of 12 for beam search,with probabilities normalized by sentence length.We use a bilingual dictionary based on fast-align(Dyer et al, 2013).
For our baseline, this servesas back-off dictionary for rare words.
We also usethe dictionary to speed up translation for all ex-periments, only performing the softmax over a fil-tered list of candidate translations (like Jean et al(2015), we use K = 30000; K?= 10).4.1 Subword statisticsApart from translation quality, which we will ver-ify empirically, our main objective is to representan open vocabulary through a compact fixed-sizesubword vocabulary, and allow for efficient train-ing and decoding.8Statistics for different segmentations of the Ger-6Clipped unigram precision is essentially 1-gram BLEUwithout brevity penalty.7github.com/sebastien-j/LV_groundhog8The time complexity of encoder-decoder architectures isat least linear to sequence length, and oversplitting harms ef-ficiency.man side of the parallel data are shown in Table1.
A simple baseline is the segmentation of wordsinto character n-grams.9Character n-grams allowfor different trade-offs between sequence length(# tokens) and vocabulary size (# types), depend-ing on the choice of n. The increase in sequencelength is substantial; one way to reduce sequencelength is to leave a shortlist of the k most frequentword types unsegmented.
Only the unigram repre-sentation is truly open-vocabulary.
However, theunigram representation performed poorly in pre-liminary experiments, and we report translation re-sults with a bigram representation, which is empir-ically better, but unable to produce some tokens inthe test set with the training set vocabulary.We report statistics for several word segmenta-tion techniques that have proven useful in previousSMT research, including frequency-based com-pound splitting (Koehn and Knight, 2003), rule-based hyphenation (Liang, 1983), and Morfessor(Creutz and Lagus, 2002).
We find that they onlymoderately reduce vocabulary size, and do notsolve the unknown word problem, and we thus findthem unsuitable for our goal of open-vocabularytranslation without back-off dictionary.BPE meets our goal of being open-vocabulary,and the learned merge operations can be appliedto the test set to obtain a segmentation with nounknown symbols.10Its main difference fromthe character-level model is that the more com-pact representation of BPE allows for shorter se-quences, and that the attention model operateson variable-length units.11Table 1 shows BPEwith 59 500 merge operations, and joint BPE with89 500 operations.In practice, we did not include infrequent sub-word units in the NMT network vocabulary, sincethere is noise in the subword symbol sets, e.g.because of characters from foreign alphabets.Hence, our network vocabularies in Table 2 aretypically slightly smaller than the number of typesin Table 1.9Our character n-grams do not cross word boundaries.
Wemark whether a subword is word-final or not with a specialcharacter, which allows us to restore the original tokenization.10Joint BPE can produce segments that are unknown be-cause they only occur in the English training text, but theseare rare (0.05% of test tokens).11We highlighted the limitations of word-level attention insection 3.1.
At the other end of the spectrum, the characterlevel is suboptimal for alignment (Tiedemann, 2009).1719vocabulary BLEU CHRF3 unigram F1(%)name segmentation shortlist source target single ens-8 single ens-8 all rare OOVsyntax-based (Sennrich and Haddow, 2015) 24.4 - 55.3 - 59.1 46.0 37.7WUnk - - 300 000 500 000 20.6 22.8 47.2 48.9 56.7 20.4 0.0WDict - - 300 000 500 000 22.0 24.2 50.5 52.4 58.1 36.8 36.8C2-50k char-bigram 50 000 60 000 60 000 22.8 25.3 51.9 53.5 58.4 40.5 30.9BPE-60k BPE - 60 000 60 000 21.5 24.5 52.0 53.9 58.4 40.9 29.3BPE-J90k BPE (joint) - 90 000 90 000 22.8 24.7 51.7 54.1 58.5 41.8 33.6Table 2: English?German translation performance (BLEU, CHRF3 and unigram F1) on newstest2015.Ens-8: ensemble of 8 models.
Best NMT system in bold.
Unigram F1(with ensembles) is computed forall words (n = 44085), rare words (not among top 50 000 in training set; n = 2900), and OOVs (not intraining set; n = 1168).segmentation # tokens # types # UNKnone 100 m 1 750 000 1079characters 550 m 3000 0character bigrams 306 m 20 000 34character trigrams 214 m 120 000 59compound splitting4102 m 1 100 000 643morfessor* 109 m 544 000 237hyphenation186 m 404 000 230BPE 112 m 63 000 0BPE (joint) 111 m 82 000 32character bigrams129 m 69 000 34(shortlist: 50 000)Table 1: Corpus statistics for German trainingcorpus with different word segmentation tech-niques.
#UNK: number of unknown tokens innewstest2013.
4: (Koehn and Knight, 2003); *:(Creutz and Lagus, 2002); : (Liang, 1983).4.2 Translation experimentsEnglish?German translation results are shown inTable 2; English?Russian results in Table 3.Our baseline WDict is a word-level model witha back-off dictionary.
It differs from WUnk in thatthe latter uses no back-off dictionary, and just rep-resents out-of-vocabulary words as UNK12.
Theback-off dictionary improves unigram F1for rareand unseen words, although the improvement issmaller for English?Russian, since the back-offdictionary is incapable of transliterating names.All subword systems operate without a back-offdictionary.
We first focus on unigram F1, whereall systems improve over the baseline, especiallyfor rare words (36.8%?41.8% for EN?DE;26.5%?29.7% for EN?RU).
For OOVs, thebaseline strategy of copying unknown wordsworks well for English?German.
However, whenalphabets differ, like in English?Russian, thesubword models do much better.12We use UNK for words that are outside the model vo-cabulary, and OOV for those that do not occur in the trainingtext.Unigram F1scores indicate that learning theBPE symbols on the vocabulary union (BPE-J90k) is more effective than learning them sep-arately (BPE-60k), and more effective than usingcharacter bigrams with a shortlist of 50 000 unseg-mented words (C2-50k), but all reported subwordsegmentations are viable choices and outperformthe back-off dictionary baseline.Our subword representations cause big im-provements in the translation of rare and unseenwords, but these only constitute 9-11% of the testsets.
Since rare words tend to carry central in-formation in a sentence, we suspect that BLEUand CHRF3 underestimate their effect on transla-tion quality.
Still, we also see improvements overthe baseline in total unigram F1, as well as BLEUand CHRF3, and the subword ensembles outper-form the WDict baseline by 0.3?1.3 BLEU and0.6?2 CHRF3.
There is some inconsistency be-tween BLEU and CHRF3, which we attribute to thefact that BLEU has a precision bias, and CHRF3 arecall bias.For English?German, we observe the bestBLEU score of 25.3 with C2-50k, but the bestCHRF3 score of 54.1 with BPE-J90k.
For com-parison to the (to our knowledge) best non-neuralMT system on this data set, we report syntax-based SMT results (Sennrich and Haddow, 2015).We observe that our best systems outperform thesyntax-based system in terms of BLEU, but notin terms of CHRF3.
Regarding other neural sys-tems, Luong et al (2015a) report a BLEU score of25.9 on newstest2015, but we note that they use anensemble of 8 independently trained models, andalso report strong improvements from applyingdropout, which we did not use.
We are confidentthat our improvements to the translation of rarewords are orthogonal to improvements achievablethrough other improvements in the network archi-1720tecture, training algorithm, or better ensembles.For English?Russian, the state of the art isthe phrase-based system by Haddow et al (2015).It outperforms our WDict baseline by 1.5 BLEU.The subword models are a step towards closingthis gap, and BPE-J90k yields an improvement of1.3 BLEU, and 2.0 CHRF3, over WDict.As a further comment on our translation results,we want to emphasize that performance variabil-ity is still an open problem with NMT.
On our de-velopment set, we observe differences of up to 1BLEU between different models.
For single sys-tems, we report the results of the model that per-forms best on dev (out of 8), which has a stabi-lizing effect, but how to control for randomnessdeserves further attention in future research.5 Analysis5.1 Unigram accuracyOur main claims are that the translation of rare andunknown words is poor in word-level NMT mod-els, and that subword models improve the trans-lation of these word types.
To further illustratethe effect of different subword segmentations onthe translation of rare and unseen words, we plottarget-side words sorted by their frequency in thetraining set.13To analyze the effect of vocabularysize, we also include the system C2-3/500k, whichis a system with the same vocabulary size as theWDict baseline, and character bigrams to repre-sent unseen words.Figure 2 shows results for the English?Germanensemble systems on newstest2015.
UnigramF1of all systems tends to decrease for lower-frequency words.
The baseline system has a spikein F1for OOVs, i.e.
words that do not occur inthe training text.
This is because a high propor-tion of OOVs are names, for which a copy fromthe source to the target text is a good strategy forEnglish?German.The systems with a target vocabulary of 500 000words mostly differ in how well they translatewords with rank > 500 000.
A back-off dictionaryis an obvious improvement over producing UNK,but the subword system C2-3/500k achieves betterperformance.
Note that all OOVs that the back-off dictionary produces are words that are copiedfrom the source, usually names, while the subword13We perform binning of words with the same training setfrequency, and apply bezier smoothing to the graph.systems can productively form new words such ascompounds.For the 50 000 most frequent words, the repre-sentation is the same for all neural networks, andall neural networks achieve comparable unigramF1for this category.
For the interval between fre-quency rank 50 000 and 500 000, the comparisonbetween C2-3/500k and C2-50k unveils an inter-esting difference.
The two systems only differ inthe size of the shortlist, with C2-3/500k represent-ing words in this interval as single units, and C2-50k via subword units.
We find that the perfor-mance of C2-3/500k degrades heavily up to fre-quency rank 500 000, at which point the modelswitches to a subword representation and perfor-mance recovers.
The performance of C2-50k re-mains more stable.
We attribute this to the factthat subword units are less sparse than words.
Inour training set, the frequency rank 50 000 corre-sponds to a frequency of 60 in the training data;the frequency rank 500 000 to a frequency of 2.Because subword representations are less sparse,reducing the size of the network vocabulary, andrepresenting more words via subword units, canlead to better performance.The F1numbers hide some qualitative differ-ences between systems.
For English?German,WDict produces few OOVs (26.5% recall), butwith high precision (60.6%) , whereas the subwordsystems achieve higher recall, but lower precision.We note that the character bigram model C2-50kproduces the most OOV words, and achieves rel-atively low precision of 29.1% for this category.However, it outperforms the back-off dictionaryin recall (33.0%).
BPE-60k, which suffers fromtransliteration (or copy) errors due to segmenta-tion inconsistencies, obtains a slightly better pre-cision (32.4%), but a worse recall (26.6%).
In con-trast to BPE-60k, the joint BPE encoding of BPE-J90k improves both precision (38.6%) and recall(29.8%).For English?Russian, unknown names canonly rarely be copied, and usually require translit-eration.
Consequently, the WDict baseline per-forms more poorly for OOVs (9.2% precision;5.2% recall), and the subword models improveboth precision and recall (21.9% precision and15.6% recall for BPE-J90k).
The full unigram F1plot is shown in Figure 3.1721vocabulary BLEU CHRF3 unigram F1(%)name segmentation shortlist source target single ens-8 single ens-8 all rare OOVphrase-based (Haddow et al, 2015) 24.3 - 53.8 - 56.0 31.3 16.5WUnk - - 300 000 500 000 18.8 22.4 46.5 49.9 54.2 25.2 0.0WDict - - 300 000 500 000 19.1 22.8 47.5 51.0 54.8 26.5 6.6C2-50k char-bigram 50 000 60 000 60 000 20.9 24.1 49.0 51.6 55.2 27.8 17.4BPE-60k BPE - 60 000 60 000 20.5 23.6 49.8 52.7 55.3 29.7 15.6BPE-J90k BPE (joint) - 90 000 100 000 20.4 24.1 49.7 53.0 55.8 29.7 18.3Table 3: English?Russian translation performance (BLEU, CHRF3 and unigram F1) on newstest2015.Ens-8: ensemble of 8 models.
Best NMT system in bold.
Unigram F1(with ensembles) is computed forall words (n = 55654), rare words (not among top 50 000 in training set; n = 5442), and OOVs (not intraining set; n = 851).10010110210310410510600.20.40.60.8150 000 500 000training set frequency rankunigramF1BPE-J90kC2-50kC2-300/500kWDictWUnkFigure 2: English?German unigram F1on new-stest2015 plotted by training set frequency rankfor different NMT systems.10010110210310410510600.20.40.60.8150 000 500 000training set frequency rankunigramF1BPE-J90kC2-50kWDictWUnkFigure 3: English?Russian unigram F1on new-stest2015 plotted by training set frequency rankfor different NMT systems.5.2 Manual AnalysisTable 4 shows two translation examples forthe translation direction English?German, Ta-ble 5 for English?Russian.
The baseline sys-tem fails for all of the examples, either by delet-ing content (health), or by copying source wordsthat should be translated or transliterated.
Thesubword translations of health research insti-tutes show that the subword systems are capa-ble of learning translations when oversplitting (re-search?Fo|rs|ch|un|g), or when the segmentationdoes not match morpheme boundaries: the seg-mentation Forschungs|instituten would be linguis-tically more plausible, and simpler to align to theEnglish research institutes, than the segmentationForsch|ungsinstitu|ten in the BPE-60k system, butstill, a correct translation is produced.
If the sys-tems have failed to learn a translation due to datasparseness, like for asinine, which should be trans-lated as dumm, we see translations that are wrong,but could be plausible for (partial) loanwords (asi-nine Situation?Asinin-Situation).The English?Russian examples show thatthe subword systems are capable of translitera-tion.
However, transliteration errors do occur,either due to ambiguous transliterations, or be-cause of non-consistent segmentations betweensource and target text which make it hard forthe system to learn a transliteration mapping.Note that the BPE-60k system encodes Mirza-yeva inconsistently for the two language pairs(Mirz|ayeva????|??|???
Mir|za|eva).
This ex-ample is still translated correctly, but we observespurious insertions and deletions of characters inthe BPE-60k system.
An example is the translit-eration of rakfisk, where a ?
is inserted and a ?is deleted.
We trace this error back to transla-tion pairs in the training data with inconsistentsegmentations, such as (p|rak|ri|ti????|???
?|?1722system sentencesource health research institutesreference GesundheitsforschungsinstituteWDict ForschungsinstituteC2-50k Fo|rs|ch|un|gs|in|st|it|ut|io|ne|nBPE-60k Gesundheits|forsch|ungsinstitu|tenBPE-J90k Gesundheits|forsch|ungsin|stitutesource asinine situationreference dumme SituationWDict asinine situation?
UNK?
asinineC2-50k as|in|in|e situation?
As|in|en|si|tu|at|io|nBPE-60k as|in|ine situation?
A|in|line-|SituationBPE-J90K as|in|ine situation?
As|in|in-|SituationTable 4: English?German translation example.?|?
marks subword boundaries.system sentencesource Mirzayevareference ????????
(Mirzaeva)WDict Mirzayeva ?
UNK?MirzayevaC2-50k Mi|rz|ay|ev|a???|??|??|??
(Mi|rz|ae|va)BPE-60k Mirz|ayeva ????|??|???
(Mir|za|eva)BPE-J90k Mir|za|yeva ????|??|???
(Mir|za|eva)source rakfiskreference ????????
(rakfiska)WDict rakfisk ?
UNK?
rakfiskC2-50k ra|kf|is|k?
??|??|??|?
(ra|kf|is|k)BPE-60k rak|f|isk ?
???|?|???
(pra|f|isk)BPE-J90k rak|f|isk ?
???|?|????
(rak|f|iska)Table 5: English?Russian translation examples.?|?
marks subword boundaries.
(pra|krit|i)), from which the translation (rak????
)is erroneously learned.
The segmentation of thejoint BPE system (BPE-J90k) is more consistent(pra|krit|i????|????|?
(pra|krit|i)).6 ConclusionThe main contribution of this paper is that weshow that neural machine translation systems arecapable of open-vocabulary translation by repre-senting rare and unseen words as a sequence ofsubword units.14This is both simpler and moreeffective than using a back-off translation model.We introduce a variant of byte pair encoding forword segmentation, which is capable of encod-ing open vocabularies with a compact symbol vo-cabulary of variable-length subword units.
Weshow performance gains over the baseline withboth BPE segmentation, and a simple character bi-gram segmentation.Our analysis shows that not only out-of-vocabulary words, but also rare in-vocabularywords are translated poorly by our baseline NMT14The source code of the segmentation algorithmsis available at https://github.com/rsennrich/subword-nmt.system, and that reducing the vocabulary sizeof subword models can actually improve perfor-mance.
In this work, our choice of vocabulary sizeis somewhat arbitrary, and mainly motivated bycomparison to prior work.
One avenue of futureresearch is to learn the optimal vocabulary size fora translation task, which we expect to depend onthe language pair and amount of training data, au-tomatically.
We also believe there is further po-tential in bilingually informed segmentation algo-rithms to create more alignable subword units, al-though the segmentation algorithm cannot rely onthe target text at runtime.While the relative effectiveness will depend onlanguage-specific factors such as vocabulary size,we believe that subword segmentations are suit-able for most language pairs, eliminating the needfor large NMT vocabularies or back-off models.AcknowledgmentsWe thank Maja Popovi?c for her implementa-tion of CHRF, with which we verified our re-implementation.
The research presented in thispublication was conducted in cooperation withSamsung Electronics Polska sp.
z o.o.
- Sam-sung R&D Institute Poland.
This project receivedfunding from the European Union?s Horizon 2020research and innovation programme under grantagreement 645452 (QT21).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural Machine Translation by JointlyLearning to Align and Translate.
In Proceedings ofthe International Conference on Learning Represen-tations (ICLR).Issam Bazzi and James R. Glass.
2000.
Modeling out-of-vocabulary words for robust speech recognition.In Sixth International Conference on Spoken Lan-guage Processing, ICSLP 2000 / INTERSPEECH2000, pages 401?404, Beijing, China.Jan A. Botha and Phil Blunsom.
2014.
CompositionalMorphology for Word Representations and Lan-guage Modelling.
In Proceedings of the 31st Inter-national Conference on Machine Learning (ICML),Beijing, China.Rohan Chitnis and John DeNero.
2015.
Variable-Length Word Encodings for Neural TranslationModels.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing(EMNLP).1723Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio.
2014.
Learn-ing Phrase Representations using RNN Encoder?Decoder for Statistical Machine Translation.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 1724?1734, Doha, Qatar.
Association forComputational Linguistics.Mathias Creutz and Krista Lagus.
2002.
UnsupervisedDiscovery of Morphemes.
In Proceedings of theACL-02 Workshop on Morphological and Phonolog-ical Learning, pages 21?30.
Association for Compu-tational Linguistics.Nadir Durrani, Hassan Sajjad, Hieu Hoang, and PhilippKoehn.
2014.
Integrating an Unsupervised Translit-eration Model into Statistical Machine Translation.In Proceedings of the 14th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, EACL 2014, pages 148?153, Gothen-burg, Sweden.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A Simple, Fast, and Effective Reparame-terization of IBM Model 2.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 644?648, At-lanta, Georgia.
Association for Computational Lin-guistics.Philip Gage.
1994.
A New Algorithm for Data Com-pression.
C Users J., 12(2):23?38, February.Barry Haddow, Matthias Huck, Alexandra Birch, Niko-lay Bogoychev, and Philipp Koehn.
2015.
TheEdinburgh/JHU Phrase-based Machine TranslationSystems for WMT 2015.
In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 126?133, Lisbon, Portugal.
Association forComputational Linguistics.S?bastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2015.
On Using Very LargeTarget Vocabulary for Neural Machine Translation.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages1?10, Beijing, China.
Association for Computa-tional Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013.
RecurrentContinuous Translation Models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, Seattle.
Association forComputational Linguistics.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M. Rush.
2015.
Character-Aware Neural Lan-guage Models.
CoRR, abs/1508.06615.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In EACL ?03:Proceedings of the Tenth Conference on EuropeanChapter of the Association for Computational Lin-guistics, pages 187?193, Budapest, Hungary.
Asso-ciation for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the ACL-2007 Demo and PosterSessions, pages 177?180, Prague, Czech Republic.Association for Computational Linguistics.Franklin M. Liang.
1983.
Word hy-phen-a-tion bycom-put-er.
Ph.D. thesis, Stanford University, De-partment of Linguistics, Stanford, CA.Wang Ling, Chris Dyer, Alan W. Black, Isabel Tran-coso, Ramon Fermandez, Silvio Amir, Luis Marujo,and Tiago Luis.
2015a.
Finding Function in Form:Compositional Character Models for Open Vocab-ulary Word Representation.
In Proceedings of the2015 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 1520?1530, Lisbon, Portugal.
Association for Computa-tional Linguistics.Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.Black.
2015b.
Character-based Neural MachineTranslation.
ArXiv e-prints, November.Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better Word Representationswith Recursive Neural Networks for Morphology.In Proceedings of the Seventeenth Conference onComputational Natural Language Learning, CoNLL2013, Sofia, Bulgaria, August 8-9, 2013, pages 104?113.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015a.
Effective Approaches to Attention-based Neural Machine Translation.
In Proceed-ings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1412?1421, Lisbon, Portugal.
Association for Computa-tional Linguistics.Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,and Wojciech Zaremba.
2015b.
Addressing theRare Word Problem in Neural Machine Translation.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages11?19, Beijing, China.
Association for Computa-tional Linguistics.Tomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernock?.
2012.Subword Language Modeling with Neural Net-works.
Unpublished.1724Graham Neubig, Taro Watanabe, Shinsuke Mori, andTatsuya Kawahara.
2012.
Machine Translationwithout Words through Substring Alignment.
In The50th Annual Meeting of the Association for Compu-tational Linguistics, Proceedings of the Conference,July 8-14, 2012, Jeju Island, Korea - Volume 1: LongPapers, pages 165?174.Sonja Nie?en and Hermann Ney.
2000.
ImprovingSMT quality with morpho-syntactic analysis.
In18th Int.
Conf.
on Computational Linguistics, pages1081?1085.Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-gio.
2013.
On the difficulty of training recurrentneural networks.
In Proceedings of the 30th Inter-national Conference on Machine Learning, ICML2013, pages 1310?1318, Atlanta, USA.Maja Popovi?c.
2015. chrF: character n-gram F-scorefor automatic MT evaluation.
In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 392?395, Lisbon, Portugal.
Association forComputational Linguistics.Rico Sennrich and Barry Haddow.
2015.
A JointDependency Model of Morphological and Syntac-tic Structure for Statistical Machine Translation.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages2081?2087, Lisbon, Portugal.
Association for Com-putational Linguistics.Benjamin Snyder and Regina Barzilay.
2008.
Unsu-pervised Multilingual Learning for MorphologicalSegmentation.
In Proceedings of ACL-08: HLT,pages 737?745, Columbus, Ohio.
Association forComputational Linguistics.David Stallard, Jacob Devlin, Michael Kayser,Yoong Keok Lee, and Regina Barzilay.
2012.
Unsu-pervised Morphology Rivals Supervised Morphol-ogy for Arabic MT.
In The 50th Annual Meeting ofthe Association for Computational Linguistics, Pro-ceedings of the Conference, July 8-14, 2012, JejuIsland, Korea - Volume 2: Short Papers, pages 322?327.Milo?
Stanojevi?c, Amir Kamran, Philipp Koehn, andOnd?rej Bojar.
2015.
Results of the WMT15 Met-rics Shared Task.
In Proceedings of the Tenth Work-shop on Statistical Machine Translation, pages 256?273, Lisbon, Portugal.
Association for Computa-tional Linguistics.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to Sequence Learning with Neural Net-works.
In Advances in Neural Information Process-ing Systems 27: Annual Conference on Neural Infor-mation Processing Systems 2014, pages 3104?3112,Montreal, Quebec, Canada.J?rg Tiedemann.
2009.
Character-based PSMT forClosely Related Languages.
In Proceedings of 13thAnnual Conference of the European Association forMachine Translation (EAMT?09), pages 12?19.J?rg Tiedemann.
2012.
Character-Based Pivot Trans-lation for Under-Resourced Languages and Do-mains.
In Proceedings of the 13th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 141?151, Avignon, France.Association for Computational Linguistics.David Vilar, Jan-Thorsten Peter, and Hermann Ney.2007.
Can We Translate Letters?
In Second Work-shop on Statistical Machine Translation, pages 33?39, Prague, Czech Republic.
Association for Com-putational Linguistics.Sami Virpioja, Jaakko J. V?yrynen, Mathias Creutz,and Markus Sadeniemi.
2007.
Morphology-AwareStatistical Machine Translation Based on MorphsInduced in an Unsupervised Manner.
In Proceed-ings of the Machine Translation Summit XI, pages491?498, Copenhagen, Denmark.Matthew D. Zeiler.
2012.
ADADELTA: An AdaptiveLearning Rate Method.
CoRR, abs/1212.5701.1725
