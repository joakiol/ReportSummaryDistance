Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 166?169,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsWINGNUS: Keyphrase Extraction Utilizing Document Logical StructureThuy Dung NguyenDepartment of Computer ScienceSchool of ComputingNational University of Singaporenguyen14@comp.nus.edu.sgMinh-Thang LuongDepartment of Computer ScienceSchool of ComputingNational University of Singaporeluongmin@comp.nus.edu.sgAbstractWe present a system description of theWINGNUS team work1for the SemEval-2010 task #5 Automatic Keyphrase Ex-traction from Scientific Articles.
A keyfeature of our system is that it utilizes aninferred document logical structure in ourcandidate identification process, to limitthe number of phrases in the candidate list,while maintaining its coverage of impor-tant phrases.
Our top performing systemachieves an F1of 25.22% for the com-bined keyphrases (author and reader as-signed) in the final test data.
We note thatthe method we report here is novel and or-thogonal from other systems, so it can becombined with other techniques to poten-tially achieve higher performance.1 IntroductionKeyphrases are noun phrases (NPs) that capturethe primary topics of a document.
While benefi-cial for applications such as summarization, clus-tering and indexing, only a minority of documentshave manually-assigned keyphrases, as it is a time-consuming process.
Automatic keyphrase genera-tion is thus a focus for many researchers.Most existing keyphrase extraction systemsview this task as a supervised classification task intwo stages: generating a list of candidates ?
can-didate identification; and using answer keyphrasesto distinguish true keyphrases ?
candidate selec-tion.
The selection model uses a set of features thatcapture the saliency of a phrase as a keyphrase.A major challenge of the keyphrase extractiontask lies in the candidate identification process.A narrow candidate list will overlook some true1This work was supported by a National Research Foun-dation grant ?Interactive Media Search?
(grant # R-252-000-325-279).keyphrases (favoring precision), whereas a broadlist will produce more errors and require more pro-cessing in latter selection stage (favoring recall).In our previous system (Nguyen and Kan,2007), we made use of the document logical struc-ture in the proposed features.
The premise of thisearlier work was that keyphrases are distributednon-uniformly in different logical sections of a pa-per, favoring sections such as introduction, andrelated work.
We introduced features indicatingwhich sections a candidate occurrs in.
For ourfielded system in this task (Kim et al, 2010), wefurther leverage the document logical structure forboth candidate identification and selection stages.Our contributions are as follows: 1) We suggestthe use of Google Scholar-based crawler to auto-matically find PDF files to enhance logical struc-ture extraction; 2) We provide a keyphrase distri-bution study with respect to different logical struc-tures; and 3) From the study result, we propose acandidate identification approach that uses logicalstructures to effectively limit the number of candi-dates considered while ensuring good coverage.2 PreprocessingAlthough we have plain text for all test input, weposit that logical structure recovery is much morerobust given the original richly-formatted docu-ment (e.g., PDF), as font and formatting informa-tion can be used for detection.
As a bridge be-tween plain text data provided by the organizerand PDF input required to extract formatting fea-tures, we first describe our Google Scholar-basedcrawler to find PDFs given plain texts.
We thendetail on the logical structure extraction process.Google Scholar-based Paper CrawlerOur crawler2takes inputs as titles to query GoogleScholar (GS) by means of web scraping.
It pro-2http://wing.comp.nus.edu.sg/?lmthang/GS/166cesses GS results and performs approximate ti-tle matching using character-based Longest Com-mon Subsequence similarity.
Once a matching ti-tle with high similarity score (> 0.7 experimen-tally) is found, the crawler retrieves the list ofavailable PDFs, and starts downloading until oneis correctly stored.
We enforce that PDFs acceptedshould have the OCR texts closely match the pro-vided plain texts in terms of lines and tokens.In the keyphrase task, we approximate the titleinputs to our crawler by considering the first twolines of each plain text provided.
For 140 trainand 100 test input documents, the crawler down-loaded 117 and 80 PDFs, of which 116 and 76 filesare correct, respectively.
This yields an accept-able level of performance in terms of (Precision,Recall) of (99.15%, 82.86%) for train and (95%,76%) for test data.Logical Structure ExtractionLogical structure is defined as ?a hierarchy of log-ical components, for example, titles, authors, affil-iations, abstracts, sections, etc.?
in (Mao et al,2003).
Operationalizing this definition, we em-ploy an in-house software, called SectLabel (Lu-ong et al, to appear), to obtain comprehensivelogical structure information for each document.SectLabel classifies each text line in a scholarlydocument with a semantic class (e.g., title, header,bodyText).
Header lines are furthered classifiedinto generic roles (e.g., abstract, intro, method).A prominent feature of SectLabel is that it iscapable of utilizing rich information, such as fontformat and spatial layout, from an optical char-acter recognition (OCR) output if PDF files arepresent3.
In case PDFs are unavailable, SectLa-bel still handles plain text based logical structurediscovery, but with degraded performance.3 Candidate Phrase IdentificationPhrase Distribution StudyWe perform a study of keyphrase distribution onthe training data over different logical structures(LSs) to understand the importance of each sec-tion within documents.
These LSs include: ti-tle, headers, abstract, introduction (intro), relatedwork (rw), conclusion, and body text4(body).3We note that the PDFs have author assigned keyphrasesof the document, but we filtered this information before pass-ing to our keyphrases system to ensure a fair test.4We utilize the comprehensive output of our logical struc-ture system to filter out copyright, email, equation, figure,We make a key observation that within a para-graph, important phrases occur mostly in the firstn sentences.
To validate our hypothesis, we con-sider keyphrase distribution over bodyn, which isthe subset of all of the body LS, limited to the firstn sentences of each paragraph (n = 1, 2, 3 experi-mentally).Ath Rder Com Sent Dentitle 142 175 251 122 2.06headers 158 342 425 1,893 0.22abstract 276 745 897 1,124 0.80intro 335 984 1,166 4,338 0.27rw 160 345 443 1,945 0.23concl 227 488 616 1,869 0.33body 398 1,175 1,411 39,179 0.04full 465 1,720 1,994 50,512 0.04body1333 839 1,035 11,280 0.09body2366 980 1,197 20,024 0.06body3382 1,042 1,269 26,163 0.05fulltext 480 1,773 2,059 166,471 0.01Table 1: Keyphrase distribution over different log-ical structures computed from the 144 trainingdocuments.
The type counts of author-assigned(ath), reader-assigned (rder) and combined (comb)keyphrases are shown.
Sent indicates the numberof sentences in each LS.
The Den column gives thedensity of keyphrases for each LS.Results in Table 1 show that individual LSs(title, headers, abstract, intro, rw, concl) con-tain a high concentration (i.e., density > 0.2)of keyphrases, with title and abstract having thehighest density, and intro being the most dominantLS in terms of keyphrase count.
With all theseLSs and body, we obtain the full setting, covering1994/2059=96.84% of all keyphrases appearing inthe original text, fulltext, while effectively reduc-ing the number of processed sentences by morethan two-thirds.Considering only the first sentence of each para-graph in the body text, body1, yields fair keyphrasecoverage of 1035/1411=73.35% relative to that offulltext.
The number of lines to be processed ismuch smaller, about a third, which validates ouraforementioned hypothesis.Keyphrase ExtractionResults from the keyphrase distribution study mo-tivates us to further explore the use of logicalstructures (LS).
The idea is to limit the searchscope of our candidate identification system whilemaintaining coverage.
We propose a new ap-caption, footnote, and reference lines.167proach, which extracts candidates according to theregular expression rules discussed in (Kim andKan, 2009).
However, instead of using the wholedocument text as input, we abridge the input textat different levels from full to minimal.Input Description Cand Com Recallminimaltitle + headers30,702 1,312 63.72%+ abs + intromediummin + rw44,975 1,414 68.67%+ conclusionfull1med + body173,958 1,580 76.74%full2med + body290,624 1,635 79.41%full3med + body3101,006 1,672 81.20%full med + body 121,378 1,737 84.36%fulltext original text 148,411 1,766 85.77%Table 2: Different levels of abridged inputs com-puted on the training data.
Cand shows thenumber of candidate keyphrases extracted foreach input type; Com gives the number of cor-rect keyphrases appear as candidates; Recall iscomputed with respect to the total number ofkeyphrases in the original texts (2059).Results in Table 2 show that we could gathera recall of 63.72% when considering a signifi-cantly abridged form of the input culled from ti-tle, headers, abstract (abs) and introduction (in-tro) ?
minimal.
Further adding related work (rw)and conclusion ?
medium ?
enhances the recall by4.95%.
When adding only the first line of eachparagraph in the body text, we achieve a good re-call of 76.74% while effectively reducing the num-ber of candidate phrases to be process by a halfwith respect to the fulltext input.
Even thoughfull2, full3, and full show further improvements interms of recall, we opt to use full1in our experi-mental runs, which trades off recall for less com-putational complexity, which may influence down-stream classification.4 Candidate Phrase SelectionFollowing (Nguyen and Kan, 2007), we use theNa?
?ve Bayes model implemented in Weka (Hall etal., 2009) for candidate phrase selection.
As dif-ferent learning models have been discussed muchprevious work, we just list the different featureswith which we experimented with.
Our features5are as follows (where n indicates a numeric fea-ture; b, a boolean one):5Detailed feature definitions are described in (Nguyen andKan, 2007; Kim and Kan, 2009).F1-F3 (n): TF?IDF, term frequency, term fre-quency of substrings.F4-F5 (n): First and last occurrences (word off-set).F6 (n): Length of phrases in words.F7 (b): Typeface attribute (available when PDFis present) ?
Indicates if any part of the candidatephrase has appeared in the document with boldor italic format, a good hint for its relevance asa keyphrase.F8 (b): InTitle ?
shows whether a phrase is alsopart of the document title.F9 (n): TitleOverlap ?
the number of timesa phrase appears in the title of other scholarlydocuments (obtained from a dump of the DBLPdatabase).F10-F14 (b): Header, Abstract, Intro, RW,Concl ?
indicate whether a phrase appears in head-ers, abstract, introduction, related work or conclu-sion sections, respectively.F15-F19 (n): HeaderF, AbstractF, IntroF, RWF,ConclF - indicate the frequency of a phrase inthe headers, abstract, introduction, related work orconclusion sections, respectively.5 Experiments5.1 DatasetsFor this task (Kim et al, 2010), we are given twodatasets: train (144 docs) and test (100 docs) withdetailed answers for train.
To tune our system,we split the train dataset into train and validationsubsets: traint(104 docs) and trainv(40 docs).Once the best setting is derived from traint-trainv,we obtain the final model trained on the full data,and apply it to the test set for the final results.5.2 EvaluationOur evaluation process is accomplished in twostages: we first experiment different feature com-binations by using the input types fulltext and full1.We then fix the best feature set, and vary our dif-ferent abridged inputs to find the optimal one.Feature CombinationTo evaluate the performance of individual features,we define a base feature set, as F1,4, and measurethe performance of each feature added separatelyto the base.
Results in Table 3 have highlightedthe set of positive features, which is F3,5,6,13,16.From the positive set F3,5,6,13,16, we tried dif-ferent combinations for the two input types shown168System F Score System F Scorebase 23.42% + F1123.42%+ F221.13% + F1223.42%+ F324.57% + F1323.75%+ F524.08% + F1422.28%+ F625.06% + F1522.11%+ F723.42% + F1623.59%+ F822.77% + F1722.60%+ F922.28% + F1823.26%+ F1023.42% + F1921.95%Table 3: Performance of individual features (onfulltext) added separately to the base set F1,4.in Table 4.
The results indicate that while fulltextobtains the best performance with F3,6,5added, us-ing full1shows superior performance at 28.18% FScore with F3,6added.
Hence, we have identifiedour best feature set as F1,3,4,6.fulltext full1base (F1,4) 23.42% 22.60%+ F3,625.88% 28.18%+ F3,6,526.21% 26.21%+ F3,6,5,1324.90% 26.21%+ F3,6,5,1624.24% 26.70%+ F3,6,5,13,1623.42% 26.70%Table 4: Performance (F1) over difference featurecombinations for fulltext and full1inputs.Abridged InputsTable 5 gives the performance for the abridgedinputs we tried with the best feature set F1,3,4,6.All full1, full2, full3and full show improved per-formance compared to those on the fulltext.
Weachieve our best performance with full1at 28.18%F Score.
These results validate the effectivenessof our approach in utilizing logical structure forthe candidate identification.
We report our resultssubmitted in Table 6.
These figures are achievedusing the best feature combination F1,3,4,6.6 ConclusionWe have described and evaluated our keyphraseextraction system for the SemEval-2 Task #5.With the use of logical structure in the candidateidentification, our system has demonstrated its su-perior performance over systems that do not usesuch information.
Moreover, we have effectivelyreduced the numbers of text lines and candidateInput @5 @10 @15 Fscoremin 62 110 145 23.75%med 79 130 158 25.88%full184 135 172 28.18%full290 132 164 26.86%full389 134 162 26.54%full 84 130 164 26.86%fulltext 82 127 158 25.88%Table 5: Performance over different abridged in-puts using the best feature set F1,3,4,6.
?@N?
indi-cates the number of top N keyphrase matches.System Description F@5 F@10 F@15WINGNUS1full, F1,3,4,620.65% 24.66% 24.95%WINGNUS2full1, F1,3,4,620.45% 24.73% 25.22%Table 6: Final results on the test data.phrases to be processed in the candidate identifi-cation and selection respectively by about half.Our system takes advantage of the logical struc-ture analysis but not to the extent we had hoped.We had hypothesized that formatting features (F7)such as bold and italics, would help discriminatekey phrases, but in our limited experiments forthis task did not validate this.
Similarly, externalknowledge should help in the keyphrase task, butthe prior knowledge about keyphrase likelihood(F9) in DBLP hurt performance in our tests.
Weplan to further explore these issues for the future.ReferencesMark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Su Nam Kim and Min-Yen Kan. 2009.
Re-examiningautomatic keyphrase extraction approaches in scien-tific articles.
In MWE ?09.Su Nam Kim, Alyona Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010.
Task 5: Automatickeyphrase extraction from scientific articles.
In Se-mEval.Minh-Thang Luong, Thuy Dung Nguyen, and Min-YenKan.
to appear.
Logical structure recovery in schol-arly articles with rich document features.
IJDLS.Forthcoming, accepted for publication.Song Mao, Azriel Rosenfeld, and Tapas Kanungo.2003.
Document structure analysis algorithms: a lit-erature survey.
In Proc.
SPIE Electronic Imaging.Thuy Dung Nguyen and Min-Yen Kan. 2007.Keyphrase extraction in scientific publications.
InICADL.169
