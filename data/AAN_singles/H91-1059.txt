THIRD MESSAGE UNDERSTANDING EVALUATION ANDCONFERENCE (MUC-3): PHASE 1 STATUS REPORTBeth M. SundheimNaval Ocean Systems CenterCode 444San Diego, CA 92152-5000ABSTRACTThe Naval Ocean Systems Center is conductingthe third in a series of evaluations of English textanalysis systems.
The premise on which theevaluations are based is that task-oriented testsenable straightforward comparisons among systemsand provide useful quantitative data on the state ofthe art in text understanding.
Furthermore, the datacan be interpreted in light of information known abouteach system's text analysis techniques in order toyield qualitative insights into the relative validity ofthose techniques as applied to the general problem ofinformation extraction.
A dry-run phase of the thirdevaluation was completed in February, 1991, and theofficial testing will be done in May, 1991, concludingwith the Third Message Understanding Conference(MUC-3).
Twelve sites reported results for the dry-run test at a meeting held in February, 1991.
Allsystems are being evaluated on the basis ofperformance on the information extraction task in ablind test at the end of each phase of the evaluation.BACKGROUNDThe Naval Ocean Systems Center (NOSC) isextending the scope of previous efforts in thearea of evaluat ing Engl ish text analys issystems.
These evaluations are intended toadvance our understanding of the merits ofcurrent text analysis techniques, as applied tothe per formance of a real ist ic informationextraction task.
The current one is alsointended to provide insight into informationretr ieval technology (document retrieval andcategorization) used instead of or in concertwith language understanding technology.
Theinputs to the ana lys i s /ext rac t ion  processconsist of  natural ly-occurring texts that wereobta ined  by NOSC in the form of electronicmessages.
The outputs of the process are a setof templates or semantic frames resembling thecontents of a partially formatted database.The premise on which the evaluations arebased is that task -or iented  tests enablestraightforward comparisons among systems andprovide useful quantitative data on the state ofthe art in text understanding.
Even though thetests are designed to treat the systems underevaluat ion as black boxes,  they are alsodesigned to point up system performance onindividual aspects of the task as well as on thetask overall.
Furthermore, these quantitativedata can be interpreted in light of informationknown about each system's text analysistechniques in order to yield qualitative insightsinto the relative validity of those techniques asapplied to the general problem of informationext ract ion .SCOPEThe third evaluation began in October,1990.
A dry-run phase was completed inFebruary, 1991, and the official testing will becarried out in May, 1991, concluding with theThird Message Understanding Conference (MUC-3).
This evaluation is significantly broader inscope than previous ones in most respects,inc lud ing  text  character i s t i cs ,  taskspecifications, performance measures, and rangeof  text unders tand ing  and in fo rmat ionextraction techniques.
The corpus and task aresufficiently challenging that they are likely tobe used again (with a new test set) in a futureevaluation of the same and/or similar systems.The corpus was formed via a keyword queryto an electronic database containing articles inmessage format from open sources worldwide,compiled, translated (if necessary), edited, andd isseminated  by the Fore ign  BroadcastInformation Service of the U.S. Government.
Atraining set of 1300 texts was identified, and301additional texts were set aside for use as testdata.
The corpus presents realistic challengesin terms of its overall size (over 2.5 mb), thelength of the individual articles (approximatelyhalf-page each on average), the variety of texttypes (newspaper articles, summary reports,speech and interv iew transcr ipts ,  rebelcommuniques, etc.
), the range of linguisticphenomena represented (both well-formed andill-formed), and the open-ended nature of thevocabulary (especially with respect to propernouns).TST1-MUC3-0080BOGOTA, 3 APR 90 (INRAVISION TELEVISIONCADENA 1) -- \[REPORT\] [JORGE ALONSO SIERRAVALENCIA\] \[TEXT\] LIBERAL SENATOR FEDERICOESTRADA VELEZ WAS KIDNAPPED ON 3 APRIL ATTHE CORNER OF 60TH AND 48TH STREETS INWESTERN MEDELLIN, ONLY 100 METERS FROM AMETROPOLITAN POLICE CAI \[IMMEDIATEATTENTION CENTER\].
THE ANTIOQUIADEPARTMENT LIBERAL PARTY LEADER HAD LEFTHIS HOUSE WITHOUT ANY BODYGUARDS ONLYMINUTES EARLIER.
AS HE WAITED FOR THETRAFFIC LIGHT TO CHANGE, THREE HEAVILYARMED MEN FORCED HIM TO GET OUT OF HIS CARAND GET INTO A BLUE RENAULT.HOURS LATER, THROUGH ANONYMOUS TELEPHONECALLS TO THE METROPOLITAN POLICE AND TO THEMEDIA, THE EXTRADITABLES CLAIMEDRESPONSIBILITY FOR THE KIDNAPPING.
IN THECALLS, THEY ANNOUNCED THAT THEY WILLRELEASE THE SENATOR WITH A NEW MESSAGE FORTHE NATIONAL GOVERNMENT.LAST WEEK, FEDERICO ESTRADA VELEZ HADREJECTED TALKS BETWEEN THE GOVERNMENT ANDTHE DRUG TRAFFICKERS.Figure 1.
Sample MUC-3 Terrorist MessageThe task is to extract information onterror ist  inc idents ( inc ident  type, date,locat ion,  perpetrator ,  target,  instrument ,outcome, etc.)
from the relevant messages in ablind test on 100 previously unseen texts in thetest set.
Approximately half of the messageswill be irrelevant to the task as it has beendefined.
The extracted information is to berepresented in the template in one of severalways, according to the information requirementsof each slot.
Some fills are required to becategories from a predefined set of possibilities(e.g., for the various types of terrorist incidentssuch as BOMBING, ATTEMPTED BOMBING, BOMBTHREAT) ;  others are requi red to becanonicalized forms (e.g., for dates) or numbers;still others are to be in the form of strings (e.g.,for person names).
The part ic ipantscollectively created a set of training templates,each site manually filling in templates for 100messages.
A simple text and correspondinganswer-key template are shown in Figures 1 and2.
Note that the text in Figure 1 is all uppercase, that the dateline includes the source of thearticle Clnravision Television Cadena 1") andthat the article is a news report by Jorge AlonsoSierra Valencia.0.
MSG ID TST1-MUC3-00801.
TEMPLATE ID 12.
INCIDENT DATE 03 APR 903.
INCIDENT TYPE KIDNAPPING4.
INCIDENT CATEGORY TERRORIST ACT5.
INDIV PERPETRATORS "THREE HEAVILY ARMEDMEN"6.
ORG PERPETRATORS "THE EXTRADITABLES" /"EXTRADITABLES"7.
PERP CONFIDENCE REPORTED AS FACT:"THREE HEAVILY ARMEDMEN"CLAIMED OR ADMrITED:"THE EXTRADITABLES" /"EXTRADITABLES"8.
PHYS TARGET ID *9.
PHYS TARGET NUM *10.
PHYS TARGET TYPE *11.
HUM TARGET ID "FEDERICO ESTRADAVELEZ" ("LIBERALSENATOR" /"ANTIOQUIADEPARTMENT LIBERALPARTY LEADER" /"SENATOR" / "LIBERALPARTY LEADER" /"PARTY LEADER")12.
HUM TARGET NUM 113.
HUM TARGET TYPE GOVERNMENT OFFICIAL /POLITICAL FIGURE14.
FOREIGN TGT NA'FN15.
INSTRUMENT TYPE *16.
INCIDENT LOCATION COLOMBIA: MEDELLIN(CITY)17.
PHYS TGT EFFECT *18.
HUM TGT EFFECTFigure 2.
Sample Key Template302In Figure 2, the slot labels have beenabbreviated to save space.
The right-handcolumn contains the "correct answers" asdefined by NOSC.
Slashes mark alternativecorrect responses (systems are to generate justone of the possibilities), an asterisk marks slotsthat are inapplicable to the incident type beingreported, and a hyphen marks a slot for whichthe text provides no fill.A call for participation was sent toorganizations in the U.S. that were known to beengaged in system design or development in thearea of text analysis or information retrieval.Twelve of the sites that responded participatedin the dry run and reported results at a meetingheld in February, 1991.
These sites areAdvanced Decision Systems (Mountain View,CA), General Electric (Schenectady, NY), GTE(Mountain View, CA), Intel l igent TextProcessing, Inc. (Santa Moniea, CA), LanguageSystems, Inc. (Woodland Hills, CA), New YorkUniversity (New York City, NY), PlanningResearch Corp. (McLean, VA), SRI International(Menlo Park, CA), TRW (Redondo Beach, CA),Unisys CAIT (Paoli, PA), the University ofMassachusetts (Amherst, MA), and theUniversity of Nebraska (Lincoln, NE) inassociation with the University of SouthwestLouisiana (Lafayette, LA).
The meeting alsoserved as a forum for resolving issues thataffect the test design, scoring, etc.
for theofficial test in May.A wide range of text interpretationtechniques (e.g., stat ist ical ,  key-word,template-driven, pattern-matching, and naturallanguage processing) were represented in thisphase of the evaluation.
One of theparticipating sites, TRW, offered a preliminarybaseline performance measure for a pattern-matching approach to information extractionthat they have already successfully put intooperational use as an interactive system appliedto texts of a somewhat more homogeneous andstraightforward nature than those found in theMUC-3 corpus.
All sites reporting in Februaryare likely to continue development in phase 2and undergo official testing in May.
Inaddition, three sites that did not report resultsfor the dry run are expecting to report resultson the official run.MEASURES OF  PERFORMANCEAll systems are being evaluated on thebasis of performance on the informationextraction task in a blind test at the end of eachphase of the evaluation.
It is expected that thedegree of success achieved by the differenttechniques in May will depend on such factorsas whether the number of possible slot fillers issmall, finite, or open-ended and whether theslot can typical ly be fi l led by fairlystraightforward extraction or not.
Systemcharacteristics such as amount of domaincoverage, degree of robustness, and generalability to make proper use of information foundin novel input will also be major factors.
Thedry-run test results cannot be assumed toprovide a good basis for estimating performanceon the official test in May.An excellent, s~mi-automated scoringprogram has been developed and distributed toall participants to enable the calculation of thevarious measures of performance.
The twoprimary measures are completeness (recall) andaccuracy (precision).
There are two additionalmeasures, one to isolate the amount of spuriousdata generated (overgeneration) and the other todetermine the rate of incorrect generation as afunction of the number of opportunities toincorrectly generate (fallout).
Fallout can becalculated only for those slots whose fillersform a closed set.
Scores for the other threemeasures are calculated for the test set overall,with breakdowns by template slot.
Figure 3presents a somewhat simplif ied set ofdefinitions for the measures.MEASURE 11 DEFIN IT IONRECALLPRECISIONOVER-GENERATIONFALLOUT#correct fills generated#fills in key#correct fills generated#fills 8enerate&#svurious fills generated#fills \[enerated#incorrect+snurious ~en'edv#possible incorrect fillsFigure 3.
MUC-3 Scoring Metrics303The most significant things to note are thatprecision and recall are actually calculated onthe basis of points -- the term "correct"includes system responses that matched the keyexactly (earning 1 point each) and systemresponses that were judged to be a good partialmatch (earning .5 point each).
It should also benoted that overgeneration figures in precisionby contributing to the denominator in additionto being isolated as a measure in its own right.Overgenerat ion also f igures in fal lout bycontributing to the numerator.
This fact willcome up again in the next section in thediscussion of the phase 1 results.In addit ion to the off ic ial  measures,unofficial measures will be obtained in May ofperformance on particular linguistic phenomena(e.g., conjunction), as measured by the databasefills generated by the systems in particular setsof instances.
That is, sentences exemplifying aselected phenomenon will be marked forseparate scoring if successful handling of thephenomenon seems to be required in order tofill one or more template slots correctly for thatsentence.
An experiment involving severalphenomena tests was conducted as part of thedry run.
The tests concerned the interpretationof active versus passive clauses, main versusembedded clauses, conjunction of noun phrases,and negation.
The results for the dry run wereextremely inconclusive, given the lack of basicdomain coverage of the systems and, for severalsites, the exclus ive use of nonl inguist icprocessing components.
In addition, the utilityof this means of judging linguistic coverage waseroded by the fact that most systems hadmultiple points of failure; some may havehandled the linguistic phenomena correctly inthe early stages of analysis, but failed to fillthe slots correctly due to subsequent processingfa i lu re .PHASE 1 RESULTSThe results obtained in the first phase ofthe evaluation are unofficial and will thereforenot be presented in their entirety.
To givereaders an idea of the current top level ofper formance of the part ic ipat ing systems,scores from two systems are presentedanonymously.
Table 1 presents a summary ofthe scores obtained on recall, precision, andovergenerat ion for the system that scoredhighest overall on recall and the system thatscored highest overall on precision (with recallabove a threshold of 10%).
The results for thefallout measure cannot be calculated for the testoverall (because the fillers for some slots do notform closed sets) and are therefore not includedin Table 1.CR ITERION VGS 1: SYSTEMW/HIGHEST 5 2 6 0 2 2RECALL$2: SYSTEMW/HIGHEST 1 4 6 8 1 1PRECISIONTable 1.
Summary of Phase 1 Scores (%) forBest-Performing Systems on Recall andPrec is ionSLOT123456"7-"89lO1112131417ImE I Iml lmmBa iBmmunmnnnammmne mnnnnrnlm mu'mn uu'Ml',  .un mUu snmmuanun imnmmnMnmnnnmn mlmlmlmm, -..mn mlim, mLun u ., IR IlNBIEjn nl mu'mJl,illln lmln u'mil,,mwnn munmnmliHmNl nNne' lnm!nm,mmnnmlmimmm Jine mmE, llt lujgn ne'Mnil, imnmnmnnmmm.,'uI NBNNN\]I ITable 2.
Breakdown of Phase 1 Scores (%) byTemplate Slot for the Best-Performing Systemson Recall and PrecisionSystems will tend to show a performancetrade-off between recall and precision.
$1 has304nearly four times greater recall than $2, and soit is not surprising that its overgeneration scoreis significantly worse than S2's.
In this regard,it should be noted that generating a spurioustemplate incurs a penalty that affects only slot1, the template ID slot.
Thus, although theprecision of S1 is lower than S2's as expected,the difference is not nearly as marked as itwould be if the penalty for generating aspurious template affected all slots rather thanjust the template ID slot.The recall columns in Table 2 suggest towhat extent S1 and $2 have been developed tofill data in the various template slots.
$2 haszero percent recall for several of the slots.
Inthe particular case of $2, a system based onthorough syntactic and semantic analysis, thereason for the zero reall is that systemdevelopment simply has not focused yet onfilling those slots.
Only one (slot 4) requires astring fill; the other three take a set fill.However, in the ease of systems based on textcategorization techniques (not represented inTables 1 and 2), zero recall is more likely toappear consistently in the slots whose fillers donot form a closed set, reflecting an inherentlimitation in the approach.
In order to obtainmeasures that give a fair appraisal of allsystems in terms of their ability to selectproper categories of responses, it has beensuggested that a second set of "overal l"measures be calculated that includes only thoseslots for which the fillers form a closed set.As defined for MUC-3, the numerator forfallout includes both the number of spuriousslot fillers and the number of incorrect slotfillers.
The inclusion of the spurious fillers inthe numerator changes the intended meaning ofthe measure, as seen in the results for slot 4 inTable 2.
That slot can be filled with one of onlytwo possible set fills, either STATE-SPONSOREDVIOLENCE or TERRORIST ACT, or it sometimesis intended to be null (represented as a hyphenin the notation).
All other slots for whichfallout can be computed have significantly moreoptions, i.e., "opportunit ies to incorrect lygenerate."
If the fallout score were computedwithout including spurious fillers, the scoresfor the CATEGORY OF INCIDENT slot should berelatively low compared to the other slot scoresfor fallout.
Instead, the scores for fallout onthat slot are higher than for any of the others,probably showing that the systems frequentlyfilled that slot when it was supposed to be null.ACKNOWLEDGEMENTSThe author is indebted to all theorganizations participating in MUC-3 and tocertain individuals in particular who havecontributed extra time and energy to ensure theevaluation's uccess, among them Laura Balcom,Scan Boisen, Nancy Chinchor, Ralph Grishman,Pete Halverson, Jerry Hobbs, Cheryl Kariya,George Krupka, David Lewis, Lisa Rau, JohnSterling, Charles Wayne, and Carl Weir.305
