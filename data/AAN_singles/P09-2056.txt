Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPCATiB: The Columbia Arabic TreebankNizar Habash and Ryan M. RothCenter for Computational Learning SystemsColumbia University, New York, USA{habash,ryanr}@ccls.columbia.eduAbstractThe Columbia Arabic Treebank (CATiB)is a database of syntactic analyses of Ara-bic sentences.
CATiB contrasts with pre-vious approaches to Arabic treebankingin its emphasis on speed with some con-straints on linguistic richness.
Two ba-sic ideas inspire the CATiB approach: noannotation of redundant information andusing representations and terminology in-spired by traditional Arabic syntax.
Wedescribe CATiB?s representation and an-notation procedure, and report on inter-annotator agreement and speed.1 Introduction and MotivationTreebanks are collections of manually-annotatedsyntactic analyses of sentences.
They are pri-marily intended for building models for statis-tical parsing; however, they are often enrichedfor general natural language processing purposes.For Arabic, two important treebanking efforts ex-ist: the Penn Arabic Treebank (PATB) (Maamouriet al, 2004) and the Prague Arabic DependencyTreebank (PADT) (Smr?
and Haji?c, 2006).
Inaddition to syntactic annotations, both resourcesare annotated with rich morphological and seman-tic information such as full part-of-speech (POS)tags, lemmas, semantic roles, and diacritizations.This allows these treebanks to be used for traininga variety of applications other than parsing, suchas tokenization, diacritization, POS tagging, mor-phological disambiguation, base phrase chunking,and semantic role labeling.In this paper, we describe a new Arabic tree-banking effort: the Columbia Arabic Treebank(CATiB).1CATiB is motivated by the followingthree observations.
First, as far as parsing Arabicresearch, much of the non-syntactic rich annota-tions are not used.
For example, PATB has over400 tags, but they are typically reduced to around36 tags in training and testing parsers (Kulick et1This work was supported by Defense Advanced Re-search Projects Agency Contract No.
HR0011-08-C-0110.al., 2006).
The reduction addresses the fact thatsub-tags indicating case and other similar featuresare essentially determined syntactically and arehard to automatically tag accurately.
Second, un-der time restrictions, the creation of a treebankfaces a tradeoff between linguistic richness andtreebank size.
The richer the annotations, theslower the annotation process, the smaller the re-sulting treebank.
Obviously, bigger treebanks aredesirable for building better parsers.
Third, bothPATB and PADT use complex syntactic represen-tations that come from modern linguistic traditionsthat differ from Arabic?s long history of syntac-tic studies.
The use of these representations putshigher requirements on the kind of annotators tohire and the length of their initial training.CATiB contrasts with PATB and PADT inputting an emphasis on annotation speed for thespecific task of parser training.
Two basic ideasinspire the CATiB approach.
First, CATiB avoidsannotation of redundant linguistic information orinformation not targeted in current parsing re-search.
For example, nominal case markers inArabic have been shown to be automatically de-terminable from syntax and word morphology andneedn?t be manually annotated (Habash et al,2007a).
Also, phrasal co-indexation, empty pro-nouns, and full lemma disambiguation are notcurrently used in parsing research so we do notinclude them in CATiB.
Second, CATiB uses asimple intuitive dependency representation andterminology inspired by Arabic?s long traditionof syntactic studies.
For example, CATiB rela-tion labels include tamyiz (specification) and idafa(possessive construction) in addition to universalpredicate-argument structure labels such as sub-ject, object and modifier.
These representationchoices make it easier to train annotators withoutbeing restricted to hire people who have degreesin linguistics.This paper briefly describes CATiB?s repre-sentation and annotation procedure, and reportson produced data, achieved inter-annotator agree-ment and annotation speeds.2212 CATiB: Columbia Arabic TreebankCATiB uses the same basic tokenization schemeused by PATB and PADT.
However, the CATiBPOS tag set is much smaller than the PATB?s.Whereas PATB uses over 400 tags specifyingevery aspect of Arabic word morphology suchas definiteness, gender, number, person, mood,voice and case, CATiB uses 6 POS tags: NOM(non-proper nominals including nouns, pronouns,adjectives and adverbs), PROP (proper nouns),VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositionsor conjunctions) and PNX (punctuation).2CATiB?s dependency links are labeled with oneof eight relation labels: SBJ (subject of verbor topic of simple nominal sentence), OBJ (ob-ject of verb, preposition, or deverbal noun), TPC(topic in complex nominal sentences containingan explicit pronominal referent), PRD (predicatemarking the complement of the extended cop-ular constructions for kAn3 A?E@?k@??A?
and AnA?E@?k@??
@), IDF (relation between the posses-sor [dependent] to the possessed [head] in theidafa/possesive nominal construction), TMZ (re-lation of the specifier [dependent] to the specified[head] in the tamyiz/specification nominal con-structions), MOD (general modifier of verbs ornouns), and ?
(marking flatness inside construc-tions such as first-last proper name sequences).This relation label set is much smaller than thetwenty or so dashtags used in PATB to mark syn-tactic and semantic functions.
No empty cate-gories and no phrase co-indexation are made ex-plicit.
No semantic relations (such as time andplace) are annotated.Figure 1 presents an example of a tree in CATiBannotation.
In this example, the verb @?P@ 	P zArwA?visited?
heads a subject, an object and a prepo-sitional phrase.
The subject includes a com-plex number construction formed using idafa andtamyiz and headed by the number???
?gxmswn?fifty?, which is the only carrier of the subject?ssyntactic nominative case here.
The preposition ?
?fy heads the prepositional phrase, whose object isa proper noun,P?
??tmwz ?July?
with an adjectivalmodifier, ???A??
@ AlmADy ?last?.
See Habash et al(2009) for a full description of CATiB?s guidelinesand a detailed comparison with PATB and PADT.2We are able to reproduce a parsing-tailored tag set [size36] (Kulick et al, 2006) automatically at 98.5% accuracy us-ing features from the annotated trees.
Details of this resultwill be presented in a future publication.3Arabic transliterations are in the Habash-Soudi-Buckwalter transliteration scheme (Habash et al, 2007b).VRB@?P@ 	P zArwA?visited?SBJNOM????gxmswn?fifty?TMZNOM??
@ Alf?thousand?IDFNOMl'A?
sA?yH?tourist?OBJPROP?AJJ.
?
lbnAn?Lebanon?MODPRT??
fy?in?OBJPROPP???tmwz?July?MODNOM???A??
@ AlmADy?last?Figure 1: CATiB annotation for the sentence???A??
@ 	P???
???AJJ.
?
@?P@P l'A???@???
?gxmswn Alf sA?yH zArwA lbnAn fy tmwz AlmADy?50 thousand tourists visited Lebanon last July.
?3 Annotation ProcedureAlthough CATiB is independent of previous anno-tation projects, it builds on existing resources andlessons learned.
For instance, CATiB?s pipelineuses PATB-trained tools for tokenization, POS-tagging and parsing.
We also use the TrEd anno-tation interface developed in coordination with thePADT.
Similarly, our annotation manual is guidedby the wonderfully detailed manual of the PATBfor coverage (Maamouri et al, 2008).Annotators Our five annotators and their super-visor are all educated native Arabic speakers.
An-notators are hired on a part-time basis and are notrequired to be on-site.
The annotation files are ex-changed electronically.
This arrangement allowsmore annotators to participate, and reduces logis-tical problems.
However, having no full-time an-notators limits the overall weekly annotation rate.Annotator training took about two months (150hrs/annotator on average).
This training time ismuch shorter than the PATB?s six-month trainingperiod.4Below, we describe our pipeline in some detailincluding the different resources we use.Data Preparation The data to annotate is splitinto batches of 3-5 documents each, with eachdocument containing around 15-20 sentences(400-600 tokens).
Each annotator works on onebatch at a time.
This procedure and the sizeof the batches was determined to be optimal forboth the software and the annotators?
productivity.To track the annotation quality, several key doc-uments are selected for inter-annotator agreement(IAA) checks.
The IAA documents are chosen to4Personal communication with Mohamed Maamouri.222cover a range of sources and to be of average doc-ument size.
These documents (collectively about10% of the token volume) are seeded throughoutthe batches.
Every annotator eventually annotateseach one of the IAA documents, but is never toldwhich documents are for IAA.Automatic Tokenization and POS Tagging Weuse the MADA&TOKAN toolkit (Habash andRambow, 2005) for initial tokenization and POStagging.
The tokenization F-score is 99.1% andthe POS tagging accuracy (on the CATiB POS tagset; with gold tokenization) is above 97.7%.Manual Tokenization Correction Tokeniza-tion decisions are manually checked and correctedby the annotation supervisor.
New POS tags areassigned manually only for corrected tokens.
FullPOS tag correction is done as part of the manualannotation step (see below).
The speed of this stepis well over 6K tokens/hour.Automatic Parsing Initial dependency parsingin CATiB is conducted using MaltParser (Nivre etal., 2007).
An initial parsing model was built usingan automatic constituency-to-dependency conver-sion of a section of PATB part 3 (PATB3-Train,339K tokens).
The quality of the automatic con-version step is measured against a hand-annotatedversion of an automatically converted held-outsection of PATB3 (PATB3-Dev, 31K tokens).
Theresults are 87.2%, 93.16% and 83.2% for attach-ment (ATT), label (LAB) and labeled attachment(LABATT) accuracies, respectively.
These num-bers are 95%, 98% and 94% (respectively) of theIAA scores on that set.5At the production mid-point another parsing model was trained by addingall the CATiB annotations generated up to thatpoint (513K tokens total).
An evaluation of theparser against the CATiB version of PATB3-Devshows the ATT, LAB and LABATT accuraciesare 81.7%, 91.1% and 77.4% respectively.6Manual Annotation CATiB uses the TrEd toolas a visual interface for annotation.7The parsedtrees are converted to TrEd format and deliveredto the annotators.
The annotators are asked to onlycorrect the POS, syntactic structure and relationlabels.
Once annotated (i.e.
corrected), the docu-ments are returned to be packaged for release.5Conversion will be discussed in a future publication.6Since CATiB POS tag set is rather small, we extend itautomatically deterministically to a larger tag set for parsingpurposes.
Details will be presented in a future publication.7http://ufal.mff.cuni.cz/?pajas/tredIAA Set Sents POSATTLABLABATTPATB3-Dev All 98.6 91.5 95.3 88.8?
40 98.7 91.7 94.7 88.6PROD All 97.6 89.2 93.0 85.0?
40 97.7 91.5 94.1 87.7Table 1: Average pairwise IAA accuracies for 5annotators.
The Sents column indicates whichsentences were evaluated, based on token length.The sizes of the sets are 2.4K (PATB3-Dev) and3.8K (PROD) tokens.4 ResultsData Sets CATiB annotated data is takenfrom the following LDC-provided resources:8LDC2007E46, LDC2007E87, GALE-DEV07,MT05 test set, MT06 test set, and PATB (part 3).These datasets are 2004-2007 newswire feeds col-lected from different news agencies and news pa-pers, such as Agence France Presse, Xinhua, Al-Hayat, Al-Asharq Al-Awsat, Al-Quds Al-Arabi,An-Nahar, Al-Ahram and As-Sabah.
The CATiB-annotated PATB3 portion is extracted from An-Nahar news articles from 2002.
Headlines, date-lines and bylines are not annotated and some sen-tences are excluded for excessive (>300 tokens)length and formatting problems.
Over 273K to-kens (228K words, 7,121 trees) of data were anno-tated, not counting IAA duplications.
In addition,the PATB part 1, part 2 and part 3 data is automat-ically converted into CATiB representation.
Thisconverted data contributes an additional 735K to-kens (613K words, 24,198 trees).
Collectively, theCATiB version 1.0 release contains over 1M to-kens (841K words, 31,319 trees), including anno-tated and converted data.Annotator Speeds Our POS and syntax annota-tion rate is 540 tokens/hour (with some reachingrates as high as 715 tokens/hour).
However, dueto the current part-time arrangement, annotatorsworked an average of only 6 hours/week, whichmeant that data was annotated at an average rate of15K tokens/week.
These speeds are much higherthan reported speeds for complete (POS+syntax)annotation in PATB (around 250-300 tokens/hour)and PADT (around 75 tokens/hour).9Basic Inter-Annotator Agreement We presentIAA scores for ATT, LAB and LABATT on IAA8http://www.ldc.upenn.edu/9Extrapolated from personal communications, MohamedMaamouri and Otakar Smr?.
In the PATB, the syntactic anno-tation step alone has similar speed to CATiB?s full POS andsyntax annotation.
The POS annotation step is what slowsdown the whole process in PATB.223IAA File Toks/hr POSATTLABLABATTHI 398 97.0 94.7 96.1 91.2HI-S 956 97.0 97.8 97.9 95.7LO 476 98.3 88.8 91.7 82.3LO-S 944 97.7 91.0 93.8 85.8Table 2: Highest and lowest average pairwise IAAaccuracies for 5 annotators achieved on a singledocument ?
before and after serial annotation.
The?-S?
suffix indicates the result after the second an-notation.subsets from two data sets in Table 1: PATB3-Dev is based on an automatically converted PATBset and PROD refers to all the new CATiB data.We compare the IAA scores for all sentences andfor sentences of token length ?
40 tokens.
TheIAA scores in PROD are lower than PATB3-Dev,this is understandable given that the error rate ofthe conversion from a manual annotation (startingpoint of PATB3-Dev) is lower than parsing (start-ing point for PROD).
Length seems to make a bigdifference in performance for PROD, but less sofor PATB3-Dev, which makes sense given theirorigins.
Annotation training did not include verylong sentences.
Excluding long sentences duringproduction was not possible because the data has ahigh proportion of very long sentences: for PRODset, 41% of sentences had >40 tokens and theyconstituted over 61% of all tokens.The best reported IAA number for PATBis 94.3% F-measure after extensive efforts(Maamouri et al, 2008).
This number does not in-clude dashtags, empty categories or indices.
Ournumbers cannot be directly compared to theirnumber because of the different metrics used fordifferent representations.Serial Inter-Annotator Agreement We test thevalue of serial annotation, a procedure in whichthe output of annotation is passed again as input toanother annotator in an attempt to improve it.
TheIAA documents with the highest (HI, 333 tokens)and lowest (LO, 350 tokens) agreement scores inPROD are selected.
The results, shown in Table 2,indicate that serial annotation is very helpful re-ducing LABATT error by 20-50%.
The reductionin LO is not as large as that in HI, unfortunately.The second round of annotation is almost twice asfast as the first round.
The overall reduction inspeed (end-to-end) is around 30%.Disagreement Analysis We conduct an erroranalysis of the basic-annotation disagreements inHI and LO.
The two sets differ in sentence length,source and genre: HI has 28 tokens/sentence andcontains AFP general news, while LO has 58 to-kens/sentence and contains Xinhua financial news.The most common POS disagreement in both setsis NOM/PROP confusion, a common issue in Ara-bic POS tagging in general.
The most commonattachment disagreements in LO are as follows:prepositional phrase (PP) and nominal modifiers(8% of the words had at least one dissenting an-notation), complex constructions (dates, propernouns, numbers and currencies) (6%), subordina-tion/coordination (4%), among others.
The re-spective proportions for HI are 5%, 5% and 1%.Label disagreements are mostly in nominal modi-fication (MOD/TMZ/IDF/?)
(LO 10%, HI 5% ofthe words had at least one dissenting annotation).The error differences between HI and LO seemto primarily correlate with length difference andless with genre and source differences.5 Conclusion and Future WorkWe presented CATiB, a treebank for Arabic pars-ing built with faster annotation speed in mind.
Inthe future, we plan to extend our annotation guide-lines focusing on longer sentences and specificcomplex constructions, introduce serial annotationas a standard part of the annotation pipeline, andenrich the treebank with automatically generatedmorphological information.ReferencesN.
Habash, R. Faraj and R. Roth.
2009.
Syntactic Annota-tion in the Columbia Arabic Treebank.
In Conference onArabic Language Resources and Tools, Cairo, Egypt.N.
Habash and O. Rambow.
2005.
Arabic Tokenization,Part-of-Speech Tagging and Morphological Disambigua-tion in One Fell Swoop.
In ACL?05, Ann Arbor, Michi-gan.N.
Habash, R. Gabbard, O. Rambow, S. Kulick, and M. Mar-cus.
2007a.
Determining case in Arabic: Learning com-plex linguistic behavior requires complex linguistic fea-tures.
In EMNLP?07, Prague, Czech Republic.N.
Habash, A. Soudi, and T. Buckwalter.
2007b.
On Ara-bic Transliteration.
In A. van den Bosch and A. Soudi,editors, Arabic Computational Morphology.
Springer.S.
Kulick, R. Gabbard, and M. Marcus.
2006.
Parsing theArabic Treebank: Analysis and Improvements.
In Tree-banks and Linguistic Theories Conference, Prague, CzechRepublic.M.
Maamouri, A. Bies, and T. Buckwalter.
2004.
The PennArabic Treebank: Building a large-scale annotated Arabiccorpus.
In Conference on Arabic Language Resources andTools, Cairo, Egypt.M.
Maamouri, A. Bies and S. Kulick.
2008.
Enhancing theArabic treebank: a collaborative effort toward new anno-tation guidelines.
In LREC?08, Marrakech, Morocco.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler,S.
Marinov, and E. Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency parsing.Natural Language Engineering, 13(2):95?135.O.
Smr?
and J. Haji?c.
2006.
The Other Arabic Treebank:Prague Dependencies and Functions.
In Ali Farghaly, edi-tor, Arabic Computational Linguistics.
CSLI Publications.224
