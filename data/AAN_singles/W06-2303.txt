Robust Parsing of the Proposition BankGabriele MusilloDepts of Linguistics and Computer ScienceUniversity of Geneva2 Rue de Candolle1211 Geneva 4Switzerlandmusillo4@etu.unige.chPaola MerloDepartment of LinguisticsUniversity of Geneva2 Rue de Candolle1211 Geneva 4Switzerlandmerlo@lettres.unige.chAbstractIn this paper, we extend an existing statis-tical parsing model to produce richer out-put parse trees, annotated with PropBanksemantic role labels.
Our results showthat the model can be robustly extended toproduce more complex output parse treeswithout any loss in performance and sug-gest that joint inference of syntactic andsemantic representations is a viable alter-native to approaches based on a pipelineof local processing steps.1 IntroductionRecent successes in statistical syntactic parsingbased on supervised learning techniques trainedon a large corpus of syntactic trees (Collins, 1999;Charniak, 2000; Henderson, 2003) have broughtforth the hope that the same approaches could beapplied to the more ambitious goal of recover-ing the propositional content and the frame se-mantics of a sentence.
Moving towards a shal-low semantic level of representation is a first ini-tial step towards the distant goal of natural lan-guage understanding and has immediate applica-tions in question-answering and information ex-traction.
For example, an automatic flight reserva-tion system processing the sentence I want to booka flight from Geneva to Trento will need to knowthat from Geneva denotes the origin of the flightand to Trento denotes its destination.
Knowingthat these two phrases are prepositional phrases,the information provided by a syntactic parser, isonly moderately useful.The growing interest in learning deeper infor-mation is to a large extent supported and due tothe recent development of semantically annotateddatabases such as FrameNet (Baker et al, 1998)or the Proposition Bank (Palmer et al, 2005), thatcan be used as training resources for a number ofsupervised learning paradigms.
We focus here onthe Proposition Bank (PropBank).
PropBank en-codes propositional information by adding a layerof argument structure annotation to the syntacticstructures of the Penn Treebank (Marcus et al,1993).
Verbal predicates in the Penn Treebank(PTB) receive a label REL and their argumentsare annotated with abstract semantic role labelsA0-A5 or AA for those complements of the pred-icative verb that are considered arguments whilethose complements of the verb labelled with a se-mantic functional label in the original PTB receivethe composite semantic role label AM-X , whereX stands for labels such as LOC, TMP or ADV,for locative, temporal and adverbial modifiers re-spectively.
A tree structure with PropBank labelsfor a sentence from the PTB (section 00) is shownin Figure 1 below.
PropBank uses two levels ofgranularity in its annotation, at least conceptually.Arguments receiving labels A0-A5 or AA do notexpress consistent semantic roles and are specificto a verb, while arguments receiving an AM-X la-bel are supposed to be adjuncts and the respectiveroles they express are consistent across all verbs.1Recent approaches to learning semantic role la-bels are based on two-stage architectures.
The firststage selects the elements to be labelled, while thesecond determines the labels to be assigned to theselected elements.
While some of these modelsare based on full parse trees (Gildea and Jurafsky,2002; Gildea and Palmer, 2002), other methodshave been proposed that eschew the need for a full1There are thirteen semantic role labels for modifiers.
See(Palmer et al, 2005) for a detailed discussion of PropBanksemantic roles labels.11SHHHHHHHHHHNP-A1PPPPPPPthe government?s borrowing authorityVPHHHHHHHHXXXXXXXXXXXXXXXXXVBD-RELdroppedPP-AM-TMP HHINatNPNNmidnightNP-AM-TMPNNPTuesdayPP-A4 HHTOtoNPQPPPP$ 2.80 trillionPP-A3 HHINfromNPQPPPP$ 2.87 trillionFigure 1: A sample syntactic structure from the PropBank with semantic role annotations.parse (CoNNL, 2004; CoNLL, 2005).
Because ofthe way the problem has been formulated ?
as apipeline of parsing (or chunking) feeding into la-belling ?
specific investigations of integrated ap-proaches that solve both the parsing and the se-mantic role labelling problems at the same timehave not been studied.We present work to test the hypothesis that acurrent statistical parser (Henderson, 2003) canoutput richer information robustly, that is with-out any significant degradation of the parser?s ac-curacy on the original parsing task, by explicitlymodelling semantic role labels as the interface be-tween syntax and semantics.We achieve promising results both on the simpleparsing task, where the accuracy of the parser ismeasured on the standard Parseval measures, andalso on the parsing task where the more complexlabels of PropBank are taken into account.
We willcall the former task Penn Treebank parsing (PTBparsing) and the latter task PropBank parsing be-low.These results have several consequences.
Onthe one hand, we show that it is possible to build asingle integrated robust system successfully.
Thisis a meaningful achievement, as a task combiningsemantic role labelling and parsing is more com-plex than simple syntactic parsing.
While the shal-low semantics of a constituent and its structuralposition are often correlated, they sometimes di-verge.
For example, some nominal temporal mod-ifiers occupy an object position without being ob-jects, like Tuesday in Figure 1 below.
On the otherhand, our results indicate that the proposed mod-els are robust.
To model our task accurately, ad-ditional parameters must be estimated.
However,given the current limited availability of annotatedtreebanks, this more complex task will have to besolved with the same overall amount of data, ag-gravating the difficulty of estimating the model?sparameters due to sparse data.
The limited avail-ability of data is increased further by the high vari-ability of the argumental labels A0-A5 whose se-mantics is specific to a given verb or a given verbsense.
Solving this more complex problem suc-cessfully, then, indicates that the models used arerobust.Finally, we achieve robustness without simpli-fying the parsing architecture.
Specifically, ro-bustness is achieved without resorting to the stip-ulation of strong independence assumptions tocompensate for the limited availability and highvariability of data.
Consequently, such an achieve-ment demonstrates not only that the robustnessof the parsing model, but also its scalability andportability.2 The Basic Parsing ArchitectureTo achieve the complex task of assigning seman-tic role labels while parsing, we use a family ofstatistical parsers, the Simple Synchrony Network(SSN) parsers (Henderson, 2003), which do notmake any explicit independence assumptions, andare therefore likely to adapt without much modi-fication to the current problem.
This architecturehas shown state-of-the-art performance.SSN parsers comprise two components, onewhich estimates the parameters of a stochasticmodel for syntactic trees, and one which searchesfor the most probable syntactic tree given the12parameter estimates.
As with many other sta-tistical parsers (Collins, 1999; Charniak, 2000),SSN parsers use a history-based model of parsing.Events in such a model are derivation moves.
Theset of well-formed sequences of derivation movesin this parser is defined by a Predictive LR push-down automaton (Nederhof, 1994), which imple-ments a form of left-corner parsing strategy.
Thederivation moves include: projecting a constituentwith a specified label, attaching one constituentto another, and shifting a tag-word pair onto thepushdown stack.Unlike standard history-based models, SSNparsers do not state any explicit independence as-sumptions between derivation steps.
They use aneural network architecture, called Simple Syn-chrony Network (Henderson and Lane, 1998), toinduce a finite history representation of an un-bounded sequence of moves.
The history repre-sentation of a parse history d1, .
.
.
, di?1, whichwe denote h(d1, .
.
.
, di?1), is assigned to the con-stituent that is on the top of the stack before the ithmove.The representation h(d1, .
.
.
, di?1) is computedfrom a set f of features of the derivation movedi?1 and from a finite set D of recent history rep-resentations h(d1, .
.
.
, dj), where j < i ?
1.
Be-cause the history representation computed for themove i ?
1 is included in the inputs to the com-putation of the representation for the next movei, virtually any information about the derivationhistory could flow from history representation tohistory representation and be used to estimate theprobability of a derivation move.
However, the re-cency preference exhibited by recursively definedneural networks biases learning towards informa-tion which flows through fewer history represen-tations.
(Henderson, 2003) exploits this bias bydirectly inputting information which is consideredrelevant at a given step to the history representa-tion of the constituent on the top of the stack be-fore that step.
In addition to history representa-tions, the inputs to h(d1, .
.
.
, di?1) include hand-crafted features of the derivation history that aremeant to be relevant to the move to be chosenat step i.
For each of the experiments reportedhere, the set D that is input to the computation ofthe history representation of the derivation movesd1, .
.
.
, di?1 includes the most recent history rep-resentation of the following nodes: topi, the nodeon top of the pushdown stack before the ith move;the left-corner ancestor of topi (that is, the secondtop-most node on the parser?s stack); the leftmostchild of topi; and the most recent child of topi, ifany.
The set of features f includes the last move inthe derivation, the label or tag of topi, the tag-wordpair of the most recently shifted word, and the left-most tag-word pair that topi dominates.
Given thehidden history representation h(d1, ?
?
?
, di?1) of aderivation, a normalized exponential output func-tion is computed by SSNs to estimate a probabil-ity distribution over the possible next derivationmoves di.2The second component of SSN parsers, whichsearches for the best derivation given the pa-rameter estimates, implements a severe pruningstrategy.
Such pruning handles the high compu-tational cost of computing probability estimateswith SSNs, and renders the search tractable.
Thespace of possible derivations is pruned in two dif-ferent ways.
The first pruning occurs immediatelyafter a tag-word pair has been pushed onto thestack: only a fixed beam of the 100 best deriva-tions ending in that tag-word pair are expanded.For training, the width of such beam is set to five.A second reduction of the search space prunesthe space of possible project or attach derivationmoves: a best-first search strategy is applied to thefive best alternative decisions only.The next section describes our model, extendedto produce richer output parse trees annotated withsemantic role labels.3 Learning Semantic Role LabelsPrevious work on learning function labels duringparsing (Merlo and Musillo, 2005; Musillo andMerlo, 2005) assumed that function labels repre-sent the interface between lexical semantics andsyntax.
We extend this hypothesis to the seman-tic role labels assigned in PropBank, as they arean exhaustive extension of function labels, whichhave been reorganised in a coherent inventory oflabels and assigned exhaustively to all sentences inthe PTB.
Because PropBank is built on the PTB, itinherits in part its notion of function labels whichis directly integrated into the AM-X role labels.A0-A5 or AA labels correspond to many of theunlabelled elements in the PTB and also to thoseelements that PTB annotators had classified as re-2The on-line version of Backpropagation is used to trainSSN parsing models.
It performs a gradient descent witha maximum likelihood objective function and weight decayregularization (Bishop, 1995).13SHHHHHHHHHHNP-A1PPPPPPPthe government?s borrowing authorityVPHHHHHHHHXXXXXXXXXXXXXXXXXVBD-RELdroppedPP-AM-TMP HHIN(-AM-TMP)atNPNNmidnightNP-AM-TMPNNP(-AM-TMP)TuesdayPP-A4 HHTOtoNPQPPPP$ 2.80 trillionPP-A3 HHINfromNPQPPPP$ 2.87 trillionFigure 2: A sample syntactic structure with semantic role labels lowered onto the preterminals.ceiving a syntactic functional label such as SBJ(subject) or DTV (dative).Because they are projections of the lexical se-mantics of the elements in the sentence, semanticrole labels are projected bottom-up, they tend toappear low in the tree and they are infrequentlyfound on the higher levels of the parse tree, whereprojections of grammatical, as opposed to lexical,elements usually reside.
Because they are the in-terface level with syntax, semantic labels are alsosubject to distributional constraints that governsyntactic dependencies, such as argument struc-ture or subcategorization.
We attempt to capturesuch constraints by modelling the c-command re-lation.
Recall that the c-command relation relatestwo nodes in a tree, even if they are not close toeach other, provided that the first node dominat-ing one node also dominate the other.
This notionof c-command captures both linear and hierarchi-cal constraints and defines the domain in whichsemantic role labelling applies.While PTB function labels appear to overlap toa large extent with PropBank semantic rolel labels,work by (Ye and Baldwin, 2005) on semantic la-belling prepositional phrases, however, indicatesthat the function labels in the Penn Treebank areassigned more sporadically and heterogeneouslythan in PropBank.
Apparently only the ?easy?cases have been tagged functionally, because as-signing these function tags was not the main goalof the annotation.
PropBank instead was anno-tated exhaustively, taking all cases into account,annotating multiple roles, coreferences and dis-continuous constituents.
It is therefore not voidof interest to test our hypothesis that, like functionlabels, semantic role labels are the interface be-tween syntax and semantics, and they need to berecovered by applying constraints that model bothhigher level nodes and lower level ones.We assume that semantic roles are very oftenprojected by the lexical semantics of the words inthe sentence.
We introduce this bottom-up lexicalinformation by fine-grained modelling of seman-tic role labels.
Extending a technique presented in(Klein and Manning, 2003) and adopted in (Merloand Musillo, 2005; Musillo and Merlo, 2005) forfunction labels, we split some part-of-speech tagsinto tags marked with semantic role labels.
Thesemantic role labels attached to a non-terminal di-rectly projected by a preterminal and belonging toa few selected categories (DIR, EXT, LOC, MNR,PNC, CAUS and TMP) were propagated down tothe pre-terminal part-of-speech tag of its head.
Toaffect only labels that are projections of lexical se-mantics properties, the propagation takes into ac-count the distance of the projection from the lex-ical head to the label, and distances greater thantwo are not included.
Figure 2 illustrates the resultof this operation.In our augmented model, inputs to each historyrepresentation are selected according to a linguis-tically motivated notion of structural locality overwhich dependencies such as argument structure orsubcategorization could be specified.In SSN parsing models, the set D of nodes thatare structurally local to a given node on top of thestack defines the structural distance between thisgiven node and other nodes in the tree.
Such a no-tion of distance determines the number of historyrepresentations through which information passes14?
?
?
??
?
?
??1?2.
.
.
.
.
.. .
.. .
.SVPC-COMMANDFigure 3: Flow of information in original SSN parsers (dashed lines), enhanced by biases specific tosemantic role labels to capture the notion of c-command (solid lines).to flow from the representation of a node i to therepresentation of a node j.
By adding nodes tothe set D, one can shorten the structural distancebetween two nodes and enlarge the locality do-main over which dependencies can be specified.To capture a locality domain appropriate for se-mantic role parsing, we add the most recent childof topi labelled with a semantic role label to the setD.
These additions yield a model that is sensitiveto regularities in structurally defined sequencesof nodes bearing semantic role labels, within andacross constituents.
This modification of the bi-ases is illustrated in Figure 3.This figure displays two constituents, S and VPwith some of their respective child nodes.
The VPnode is assumed to be on the top of the parser?sstack, and the S one is supposed to be its left-corner ancestor.
The directed arcs represent theinformation that flows from one node to another.According to the original SSN model in (Hender-son, 2003), only the information carried over bythe leftmost child and the most recent child of aconstituent directly flows to that constituent.
Inthe figure above, only the information conveyedby the nodes ?
and ?
is directly input to the nodeS.
Similarly, the only bottom-up information di-rectly input to the VP node is conveyed by thechild nodes ?
and ?.
In the original SSN models,nodes bearing a function label such as ?1 and ?2are not directly input to their respective parents.In our extended model, information conveyed by?1 and ?2 directly flows to their respective par-ents.
So the distance between the nodes ?1 and?2, which stand in a c-command relation, is short-ened.
For more information on this technique tocapture domains induced by the c-command rela-tion, see (Musillo and Merlo, 2005).We report the effects of these augmentations onparsing results in the experiments described below.4 ExperimentsOur extended semantic role SSN parser wastrained on sections 2-21 and validated on section24 from the PropBank.
Training, validating andtesting data sets consist of the PTB data anno-tated with PropBank semantic roles labels, as pro-vided in the CoNLL-2005 shared task (Carrerasand Marquez, 2005).Our augmented model has a total 613 of non-terminals to represents both the PTB and Prop-Bank labels of constituents, instead of the 33 ofthe original SSN parser.
The 580 newly introducedlabels consist of a standard PTB label followedby a set of one or more PropBank semantic rolesuch as PP-AM-TMP or NP-A0-A1.
As a resultof lowering the six AM-X semantic role labels,240 new part-of-speech tags were introduced topartition the original tag set which consisted of 45tags.
SSN parsers do not tag their input sentences.To provide the augmented model with tagged in-put sentences, we trained an SVM tagger whosefeatures and parameters are described in detail in(Gimenez andMarquez, 2004).
Trained on section2-21, the tagger reaches a performance of 95.45%on the test set (section 23) using our new tag set.As already mentioned, argumental labels A0-A5are specific to a given verb or a given verb sense,thus their distribution is highly variable.
To re-duce variability, we add some of the tag-verb pairslicensing these argumental labels to the vocabu-15F R PPropBank training and PropBank parsing task 82.3 82.1 82.4PropBank training and PTB parsing task 88.8 88.6 88.9PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9Table 1: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasksand the original SSN parser.lary of our model.
We reach a total of 4970 tag-word pairs.3 This vocabulary comprises the orig-inal 512 pairs of the original SSN model, and ouradded pairs which must occur at least 10 times inthe training data.
Our vocabulary as well as thenew 240 POS tags and the new 580 non-terminallabels are included in the set f of features input tothe history representations as described in section2.We perform two different evaluations on ourmodel trained on PropBank data.
Recall thatwe distinguish between two parsing tasks: thePropBank parsing task and the PTB parsing task.To evaluate the first parsing task, we computethe standard Parseval measures of labelled recalland precision of constituents, taking into accountnot only the 33 original labels but also the 580newly introduced PropBank labels.
This evalua-tion gives us an indication of how accurately andexhaustively we can recover this richer set of non-terminal labels.
The results, computed on the test-ing data set from the PropBank, are shown on thefirst line of Table 1.To evaluate the PTB task, we compute the la-belled recall and precision of constituents, ignor-ing the set of PropBank semantic role labels thatour model assigns to constituents.
This evalua-tion indicates how well we perform on the stan-dard PTB parsing task alone, and its results on thetesting data set from the PTB are shown on thesecond line of Table 1.The third line of Table 1 gives the performanceon the simpler PTB parsing task of the originalSSN parser (Henderson, 2003), that was trainedon the PTB data sets contrary to our SSN modeltrained on the PropBank data sets.5 DiscussionThese results clearly indicate that our model canperform the PTB parsing task at levels of per-3Such pairs consists of a tag and a word token.
No attemptat collecting word types was made.formance comparable to state-of-the-art statisticalparsing, by extensions that take the nature of thericher labels to be recovered into account.
Theyalso suggest that the relationship between syntac-tic PTB parsing and semantic PropBank parsingis strict enough that an integrated approach to theproblem of semantic role labelling is beneficial.In particular, recent models of semantic role la-belling separate input indicators of the correlationbetween the structural position in the tree and thesemantic label, such as path, from those indicatorsthat encode constraints on the sequence, such asthe previously assigned role (Kwon et al, 2004).In this way, they can never encode directly the con-straining power of a certain role in a given struc-tural position onto a following node in its struc-tural position.
In our augmented model, we at-tempt to capture these constraints by directly mod-elling syntactic domains defined by the notion ofc-command.Our results also confirm the findings in (Palmeret al, 2005).
They take a critical look at somecommonly used features in the semantic role la-belling task, such as the path feature.
They sug-gest that the path feature is not very effective be-cause it is sparse.
Its sparseness is due to the oc-currence of intermediate nodes that are not rele-vant for the syntactic relations between an argu-ment and its predicate.
Our model of domains isless noisy, and consequently more robust, becauseit can focus only on c-commanding nodes bearingsemantic role labels, thus abstracting away fromthose nodes that smear the pertinent relations.
(Yi and Palmer, 2005) share the motivation ofour work.
Like the current work, they observethat the distributions of semantic labels could po-tentially interact with the distributions of syntacticlabels and redefine the boundaries of constituents,thus yielding trees that reflect generalisations overboth these sources of information.To our knowledge, no results have yet been pub-lished on parsing the PropBank.
Accordingly, it isnot possible to draw a straigthforward quantitative16F R P(Haghighi et al, 2005) 83.4 83.1 83.7(Pradhan et al, 2005) 83.3 83.0 83.5(Punyakanok et al, 2005) 83.1 82.8 83.3(Marquez et al, 2005) 83.1 82.8 83.3(Surdeanu and Turmo, 2005) 82.7 82.5 83.0PropBank SSN 81.6 81.3 81.9Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and state-of-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBankvalidating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).comparison between our PropBank SSN parserand other PropBank parsers.
However, state-of-the-art semantic role labelling systems (CoNLL,2005) use parse trees output by state-of-the-artparsers (Collins, 1999; Charniak, 2000), both fortraining and testing, and return partial trees anno-tated with semantic role labels.
An indirect wayof comparing our parser with semantic role la-bellers suggests itself.
We merge the partial treesoutput by a semantic role labeller with the outputof a parser it was trained on, and compute Prop-Bank parsing performance measures on the result-ing parse trees.
The first five lines of Table 2 re-port such measures for the five best semantic rolelabelling systems (Haghighi et al, 2005; Pradhanet al, 2005; Punyakanok et al, 2005; Marquezet al, 2005; Surdeanu and Turmo, 2005) accord-ing to (CoNLL, 2005).
The partial trees outputby these systems were merged with the parse treesreturned by (Charniak, 2000)?s parser.
These sys-tems use (Charniak, 2000)?s parse trees both fortraining and testing as well as various other infor-mation sources including sets of n-best parse trees(Punyakanok et al, 2005; Haghighi et al, 2005)or chunks (Marquez et al, 2005; Pradhan et al,2005) and named entities (Surdeanu and Turmo,2005).
While our preliminary results indicated inthe last line of Table 2 are not state-of-the-art, theydo demonstrate the viability of SSN parsers forjoint inference of syntactic and semantic represen-tations.6 ConclusionsIn this paper, we have explored extensions to anexisting state-of-the-art parsing model.
We haveachieved promising results on parsing the Propo-sition Bank, showing that our extensions are suf-ficiently robust to produce parse trees annotatedwith shallow semantic information.
Future workwill lie in extracting semantic role relations fromsuch richly annotated trees, for applications suchas information extraction or question answering.In addition, further research will explore the rele-vance of semantic role features to parse reranking.AcknowledgementsWe thank the Swiss National Science Foundationfor supporting this research under grant number101411-105286/1.
We also thank James Hender-son and Ivan Titov for allowing us to use and mod-ify their SSN software, Xavier Carreras for pro-viding the CoNLL-2005 shared task data sets andthe anonymous reviewers for their valuable com-ments.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the Thirty-Sixth Annual Meeting of the As-sociation for Computational Linguistics and Seven-teenth International Conference on ComputationalLinguistics (ACL-COLING?98), pages 86?90, Mon-treal, Canada.Christopher M. Bishop.
1995.
Neural Networks forPattern Recognition.
Oxford University Press, Ox-ford, UK.Xavier Carreras and Lluis Marquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
In Proceedings of CoNLL-2005, Ann Arbor,MI USA.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Meetingof North American Chapter of Association for Com-putational Linguistics (NAACL?00), pages 132?139,Seattle, Washington.Michael John Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D. the-17sis, Department of Computer Science, University ofPennsylvania.CoNLL.
2005.
Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005).
AnnArbor, MI, USA.CoNNL.
2004.
Eighth Conference on Computa-tional Natural Language Learning (CoNLL-2004).Boston, MA, USA.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
InProceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2002),pages 239?246, Philadelphia, PA.Jesus Gimenez and Lluis Marquez.
2004.
Svmtool:A general POS tagger generator based on SupportVector Machines.
In Proceedings of the 4th Interna-tional Conference on Language Resources and Eval-uation (LREC?04), Lisbon, Portugal.Aria Haghighi, Kristina Toutanova, and ChristopherManning.
2005.
A joint model for semantic rolelabeling.
In Proceedings of CoNLL-2005, Ann Ar-bor, MI USA.James Henderson and Peter Lane.
1998.
A connection-ist architecture for learning to parse.
In Proceedingsof 17th International Conference on ComputationalLinguistics and the 36th Annual Meeting of the As-sociation for Computational Linguistics (COLING-ACL?98), pages 531?537, University of Montreal,Canada.Jamie Henderson.
2003.
Inducing history representa-tions for broad-coverage statistical parsing.
In Pro-ceedings of the Joint Meeting of the North AmericanChapter of the Association for Computational Lin-guistics and the Human Language Technology Con-ference (NAACL-HLT?03), pages 103?110, Edmon-ton, Canada.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the ACL (ACL?03), pages423?430, Sapporo, Japan.Namhee Kwon, Michael Fleischman, and EduardHovy.
2004.
Senseval automatic labeling of se-mantic roles using maximum entropy models.
InSenseval-3, pages 129?132, Barcelona, Spain.Mitch Marcus, Beatrice Santorini, and M.A.Marcinkiewicz.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.Computational Linguistics, 19:313?330.Lluis Marquez, Pere Comas, Jesus Gimenez, and NeusCatala.
2005.
Semantic role labeling as sequentialtagging.
In Proceedings of CoNLL-2005, Ann Ar-bor, MI USA.Paola Merlo and Gabriele Musillo.
2005.
Accuratefunction parsing.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Process-ing, pages 620?627, Vancouver, British Columbia,Canada, October.Gabriele Musillo and Paola Merlo.
2005.
Lexical andstructural biases for function parsing.
In Proceed-ings of the Ninth International Workshop on Pars-ing Technology, pages 83?92, Vancouver, BritishColumbia, October.Mark Jan Nederhof.
1994.
Linguistic Parsing and Pro-gram Transformations.
Ph.D. thesis, Department ofComputer Science, University of Nijmegen.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31:71?105.Sameer Pradhan, Kadri Hacioglu, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic role chunking combining complementarysyntactic views.
In Proceedings of CoNLL-2005,Ann Arbor, MI USA.Vasin Punyakanok, Peter Koomen, Dan Roth, and Wentau Yih.
2005.
Generalized inference with multiplesemantic role labeling systems.
In Proceedings ofCoNLL-2005, Ann Arbor, MI USA.Mihai Surdeanu and Jordi Turmo.
2005.
Semantic rolelabeling using complete syntactic analysis.
In Pro-ceedings of CoNLL-2005, Ann Arbor, MI USA.Patrick Ye and Timothy Baldwin.
2005.
Semantic rolelabelling of prepositional phrases.
In Proceedings ofthe Second International Joint Conference on Nat-ural Language Processing (IJCNLP-05), pages pp.779?791, Jeju, South Korea.Szu-ting Yi and Martha Palmer.
2005.
The integrationof semantic parsing and semantic role labelling.
InProceedings of CoNLL?05, Ann Arbor, Michigan.18
