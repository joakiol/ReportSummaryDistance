Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1097?1104Manchester, August 2008Grammar Comparison Study for Translational EquivalenceModeling and Statistical Machine TranslationMin Zhang1,  Hongfei Jiang2,  Haizhou Li1,  Aiti Aw1  and  Sheng Li21Institute for Infocomm Research, Singapore2Harbin Institute of Technology, China{mzhang, hli, aaiti}@i2r.a-star.edu.sg{hfjiang, lisheng}@mtlab.hit.edu.cnAbstractThis paper presents a general platform,namely synchronous tree sequence sub-stitution grammar (STSSG), for thegrammar comparison study in Transla-tional Equivalence Modeling (TEM) andStatistical Machine Translation (SMT).Under the STSSG platform, we comparethe expressive abilities of various gram-mars through synchronous parsing and areal translation platform on a variety ofChinese-English bilingual corpora.
Ex-perimental results show that the STSSGis able to better explain the data in paral-lel corpora than other grammars.
Ourstudy further finds that the complexity ofstructure divergence is much higher thansuggested in literature, which imposes abig challenge to syntactic transformation-based SMT.1 IntroductionTranslational equivalence is a mathematical rela-tion that holds between linguistic expressionswith the same meaning (Wellington et al, 2006).The common explicit representations of this rela-tion are word alignments, phrase alignments andstructure alignments between bilingual sentences.Translational Equivalence Modeling (TEM) is aprocess to describe and build these alignmentsusing mathematical models.
Thus, the study ofTEM is highly relevant to Statistical MachineTranslation (SMT).Grammar is the most important infrastructurefor TEM and SMT since translation models?
ex-pressive and generative abilities are mainly de-?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.termined by the grammar.
Many grammars, suchas finite-state grammars (FSG), bracket/inversiontransduction grammars (BTG/ITG) (Wu, 1997),context-free grammar (CFG), tree substitutiongrammar (TSG) (Comon et al, 2007) and theirsynchronous versions, have been explored inSMT.
Based on these grammars, a great numberof SMT models have been recently proposed,including string-to-string model (SynchronousFSG) (Brown et al, 1993; Koehn et al, 2003),tree-to-string model (TSG-string) (Huang et al,2006; Liu et al, 2006; Liu et al, 2007), string-to-tree model (string-CFG/TSG) (Yamada andKnight, 2001; Galley et al, 2006; Marcu et al,2006), tree-to-tree model (SynchronousCFG/TSG, Data-Oriented Translation) (Chiang,2005; Cowan et al, 2006; Eisner, 2003; Ding andPalmer, 2005; Zhang et al, 2007; Bod, 2007;Quirk wt al., 2005; Poutsma, 2000; Hearne andWay, 2003) and so on.Although many achievements have been ob-tained by these advances, it is still unclear whichof these important pursuits is able to best explainhuman translation data, as each has its advan-tages and disadvantages.
Therefore, it has greatmeaning in both theory and practice to do com-parison studies among these grammars and SMTmodels to see which of them are capable of betterdescribing parallel translation data.
This is a fun-damental issue worth exploring in multilingualinformation processing.
However, little effort inprevious work has been put in this point.
To ad-dress this issue, in this paper we define a generalplatform, namely synchronous tree sequencesubstitution grammar (STSSG), for the compari-son studies.
The STSSG can be seen as a gener-alization of Synchronous TSG (STSG) by replac-ing elementary tree (a single subtree used inSTSG) with contiguous tree sequence as the ba-sic translation unit.
As a result, most of previousgrammars used in SMT can be interpreted as thereduced versions of the STSSG.
Under theSTSSG platform, we compare the expressive1097abilities of various grammars and translationmodels through linguistically-based synchronousparsing and a real translation platform.
By syn-chronous parsing, we aim to study which gram-mar can well explain translation data (i.e.
transla-tional equivalence alignment) while by the realtranslation platform, we expect to investigatewhich model can achieve better translation per-formance.
In addition, we also measure the im-pact of various factors in this study, including thegenera of corpora (newspaper domain via spokendomain), the accuracy of word alignments andsyntax parsing (automatically vs. manually).We report our experimental settings, experi-mental results and our findings in detail in therest of the paper, which is organized as follows:Section 2 reviews previous work.
Section 3elaborates the general framework while Section 4reports the experimental results.
Finally, we con-clude our work in Section 5.2 Previous WorkThere are only a few of previous work related tothe study of translation grammar comparison.Fox (2002) is the first to look at how well pro-posed translation models fit actual translationdata empirically.
She examined the issue ofphrasal cohesion between English and Frenchand discovered that while there is less cohesionthan one might desire, there is still a largeamount of regularity in the constructions wherebreakdowns occur.
This suggests that reorderingwords by phrasal movement is a reasonable strat-egy (Fox, 2002).
She has also examined the dif-ferences in cohesion between Treebank-styleparse trees, trees with flattened verb phrases, anddependency structures.
Their experimental re-sults indicate that the highest degree of cohesionis present in dependency structures.Motivated by the same problem raised by Fox(2002), Galley et al (2004) study what rule canbetter explain human translation data.
They firstpropose a theory that gives formal semantics toword-level alignments defined over parallel cor-pora, and then use the theory to introduce a linearalgorithm that is used to derive from word-aligned, parallel corpora the minimal set of syn-tactically motivated transformation rules to ex-plain human translation data.
Their basic idea isto create transformation rules that condition onlarger fragments of tree structure.
Their experi-mental results suggest that their proposed rulesprovide a good, realistic indicator of the com-plexities inherent in translation than SCFG.Wellington et al (2006) describes their studyof the patterns of translational equivalence exhib-ited by a variety of bilingual/monolingual bitexts.They empirically measure the lower bounds onalignment failure rates with and without gapsunder the constraints of word alignment alone orwith one or both side parse trees.
Their studyfinds surprisingly many examples of translationalequivalence that could not be analyzed using bi-nary-branching structures without discontinuities.Thus, they claim that the complexity of thesepatterns in every bitext is higher than suggestedin the literature.
In addition, they suggest that thelow coverage rates without gaps under the con-straints of independently generated monolingualparse trees might be the main reason why ?syn-tactic?
constraints have not yet increased the ac-curacy of SMT systems.
However, they find thatsimply allowing a single gap in bilingual phrasesor other types of constituent can improve cover-age dramatically.DeNeefe et al (2007) compares the strengthsand weaknesses of a syntax-based MT modelwith a phrase-based MT model from the view-points of translational equivalence extractionmethods and coverage.
They find that there aresurprising differences in phrasal coverage ?
nei-ther is merely a superset of the other.
They alsoinvestigate the reason why some phrase pairs arenot learned by the syntax-based model.
They fur-ther propose several solutions and evaluate onthe syntax-based extraction techniques in light ofphrase pairs captured and translation accuracy.Finally, significant performance improvement isreported using their solutions.Different from previous work discussed above,this paper mainly focuses on the expressive abil-ity comparison studies among different gram-mars and models through synchronous parsingand a real SMT platform.
Fox (2002), Galley etal (2004) and Wellington et al (2006) examineTEM only.
DeNeefe et al (2007) only comparesthe strengths and weaknesses of a syntax-basedMT model with a phrase-based MT model.3 The General Platform: the STSSGIn this section, we first define the STSSG plat-form in Subsection 3.1, and then explain why itis a general framework that can cover most ofprevious syntax-based translation grammars andmodels in Subsection 3.2.
In Subsection 3.3 and3.4, we discuss the STSSG-based SMT and syn-chronous parsing, which are used to comparedifferent grammars and translation models.10981( )IT e1( )JT fAFigure 1.
A word-aligned parse tree pairs of a Chi-nese sentence and its English translationFigure 2.
Two examples of translation rules3.1 Definition of the STSSGThe STSSG is an extension of the STSG by us-ing tree sequences (rather than elementary trees)as the basic translation unit.
A STSSG is a septet, , , , ,,t t ts s sG N N S S P?
?=< > , where:z s?
and t?
are source and target terminalalphabets (POSs or lexical words), respec-tively, andz sN  and tN are source and target non-terminal alphabets (linguistic phrase tag, i.e.NP/VP?
), respectively, andz s sS N?
and t tS N?
are the source and tar-get start symbols (roots of source and targetparse trees), andz P is a production rule set.A grammar rule ir  in the STSSG is an alignedtree sequence pair, < s?
, t?
, A  >, where s?
andt?
are tree sequences of source side and targetsides, respectively, and A is the alignments be-tween leaf nodes of two tree sequences.
Here, thekey concept of ?tree sequence?
refers to an or-dered subtree sequence covering a consecutivetree fragment in a complete parse tree.
The leafnodes of a subtree in a tree sequence can be ei-ther non-terminal symbols or terminal symbols.Fig.
2 shows two STSSG rules extracted fromthe aligned tree pair shown in Fig.
1, where 1r isalso a STSG rule.In the STSSG, a translational equivalence ismodeled as a tree sequence pair while MT isviewed as a tree sequence substitution process.From the definition of ?tree sequence?, we cansee that a subtree in a tree sequence is a so-calledelementary tree used in TSG.
This suggests thatSCFG and STSG are only a subset of STSSGand SCFG is a subset of STSG.
The next subsec-tion discusses how to configure the STSSG toimplement the other two simplified grammars.This is the reason why we call the STSSG a gen-eral framework for synchronous grammar-basedtranslation modeling.It is worth noting that, from rule rewritingviewpoint, STSSG can be thought of as a re-stricted version of synchronous multi-componentTAGs (Schuler et al, 2000) although TAG ismore powerful than TSG due to the additionaloperation ?adjunctions?.
The synchronous multi-component TAG can also rewrite several non-terminals in one step of derivation.
The differ-ence between them is that the rewriting sites (i.e.the substitution nodes) must be contiguous inSTSSG.
In addition, STSSG is also related totree automata (Comon et al, 2007).
However, thediscussion on the theoretical relation and com-parison between them is out of the scope of thepaper.
In this paper, we focus on the comparisonstudy of SMT grammars using the STSSG plat-form.3.2 Rule Extraction and Grammar Con-figurationAll the STSSG mapping rules are extracted frombi-parsed trees.
Our rule extraction algorithm isan extension of that presented at (Chiang, 2005;Liu et al, 2006; Zhang et al, 2007).
We modifytheir tree-to-tree/string rule extraction algorithmsto extract tree-sequence-to-tree-sequence rules.Our rules2 are extracted in two steps:2  We classify the rules into two categories: initialrules, whose leaf nodes must be terminals, and ab-10991) Extracting initial rules from bi-parsed trees.This is rather straightforward.
We first generateall fully lexicalized source and target tree se-quences (whose leaf nodes must be lexical words)using a DP algorithm and then iterate over allgenerated source and target sequence pairs.
Iftheir word alignments are all within the scope ofthe current tree sequence pair, then the currenttree sequence pair is an initial rule.2) Extracting abstract rules from the extractedinitial rules.
The idea behind is that we generatean abstract rule from a ?big?
initial rule by re-moving one or more ?small?
initial rules fromthe ?big?
one, where the ?small?
ones must be asub-graph of the ?big?
one.
Please refer to(Chiang, 2005; Liu et al, 2006; Zhang et al,2007) for the implementation details.As indicated before (Chiang, 2005; Zhang etal., 2007), the above scheme generates a verylarge number of rules, which not only makes thesystem too complicated but also introduces toomany undesirable ambiguities.
To control theoverall model complexity, we introduce the fol-lowing parameters:1) The maximal numbers of trees in the sourceand target tree sequences: s?
and t?
.2) The maximal tree heights in the source andtarget tree sequences: s?
and t?
.3) The maximal numbers of non-terminal leafnodes in the source and target tree sequences:s?
and t?
.Now let us see how to implement other mod-els in relation to STSSG based the STSSGthrough configuring the above parameters.1) STSG-based tree-to-tree model (Zhang etal., 2007; Bod, 2007) when s?
= t?
=1.2) SCFG-based tree-to-tree model when s?
=t?
=1 and s?
= t?
=2.3) Phrase-based translation model only (no re-ordering model) when s?
= t?
=0 and s?
= t?
=1.4) TSG-CFG-based tree-to-string model (Liuet al, 2006) when s?
= t?
=1, t?
=2 and ignorephrase tags in target side.5) CFG-TSG-based string-to-tree model (Gal-ley et al, 2006) when s?
= t?
=1and s?
=2.6) TSSG-CFG-based tree-sequence-to-stringmodel (Liu et al, 2007) when t?
=2 and ignorephrase tags in target side.stract rule that having at least one non-terminal leafnode.From the above definitions, we can see that allof previous related models/grammars can be canbe interpreted as the reduced versions of theSTSSG.
This is the reason why we use theSTSSG as a general platform for our model andgrammar comparison studies.3.3 Model Training and Decoder for SMTWe use the tree sequence mapping rules to modelthe translation process.
Given the source parsetree 1( )JT f , there are multiple derivations3 thatcould lead to the same target tree 1( )IT e , themapping probability 1 1( ( ) | ( ))I JrP T e T f is ob-tained by summing over the probabilities of allderivations.
The probability of each derivation?is given by the product of the probabilities of allthe rules ( )ip r  used in the derivation (here weassume that a rule is applied independently in aderivation).1 1 1 1( | ) ( ( ) | ( ))= ( )iI J I Jirr rP e f P T e T fp r?
??=??
(1)The model is implemented under log-linearframework.
We use seven basic features that areanalogous to the commonly used features inphrase-based systems (Koehn, 2004): 1) bidirec-tional rule mapping probabilities; 2) bidirectionallexical translation probabilities; 3) the target lan-guage model; 4) the number of rules used and 5)the number of target words.
Besides, we definetwo new features: 1) the number of lexical wordsin a rule to control the model?s preference forlexicalized rules over un-lexicalized rules and 2)the average tree height in a rule to balance theusage of hierarchical rules and more flat rules.The overall training process is similar to theprocess in the phrase-based system (koehn et al,2007): word alignment, rule extraction, featureextraction and probability calculation and featureweight tuning.Given 1( )JT f , the decoder is to find the bestderivation ?
that generates < 1( )JT f , 1( )IT e >.111 1,?
arg max ( ( ) | ( ))arg max ( )IIiI Jeie rre P T e T fp r?
??=?
?
(2)By default, same as other SMT decoder, herewe use Viterbi derivation in Eq (2) instead of the3 A derivation is a sequence of tree sequence rules thatmaps a source parse tree to its target one.1100summing probabilities in Eq (3).
This is to makethe decoder speed not too slow.
The decoder is astandard span-based chart parser together with afunction for mapping the source derivations tothe target ones.
To speed up the decoder, we util-ize several thresholds to limit the search beamsfor each span, such as the number of rules usedand the number of hypotheses generated.3.4 Synchronous ParsingA synchronous parser is an algorithm that caninfer the syntactic structure of each componenttext in a multitext and simultaneously infer thecorrespondence relation between these structures.When a parser?s input can have fewer dimen-sions than the parser?s grammar, we call it atranslator.
When a parser?s grammar can havefewer dimensions than the parser?s input, we callit a synchronizer (Melamed, 2004).
Therefore,synchronous parsing and MT are closed to eachother.
In this paper, we use synchronous parsingto compare the ability of different grammars intranslational equivalence modeling.Given a bilingual sentence pair 1Jf and 1Ie , thesynchronous parser is to find a derivation ?
thatgenerates < 1( )JT f , 1( )IT e >.
Our synchronousparser is similar to the synchronous CKY parserpresented at (Melamed, 2004).
The difference isthat we implement it based on our STSSG de-coder.
Therefore, in nature the parser is a stan-dard synchronous chart parser but constrained bythe rules of the STSSG grammar.
In our imple-mentation, we simply use our decoder to simu-late the bilingual parser: 1) for each sentence pair,we extract one model; 2) we use the model andthe decoder to translate the source sentence ofthe given sentence pair; 3) if the target sentenceis successfully generated by the decoder, then wesay the symphonious parsing is successful.Please note that the synchronous parsing is con-sidered as successful once the last words in thesource and target sentences are covered by thedecoder even if there is no a complete targetparse tree generated (it may be a tree sequence).This is because our study only concerns whetherall translational equivalences are linked togetherby the synchronous parser correctly.4 Experiments4.1 Experimental SettingsSynchronous parsing settings: Our experimentsof synchronous parsing are carried on three Chi-nese-to-English bilingual corpora: the FBIS cor-pus, the IWSLT 2007 training set and the HITCorpus.
The FBIS data is a collection of trans-lated newswire documents published by majornews agencies from three representative loca-tions: Beijing, Taipei and Hongkong.
TheIWSLT data is a multilingual speech corpus ontravel domain while the HIT corpus consists ofexample sentences of a Chinese-English diction-ary.
The first two corpora are sentence-alignedwhile the HIT corpus is a manually bi-parsedcorpus with manually annotated word alignments.We use the three corpora to study whether themodels?
expressive abilities are domain depend-ent and how the performance of word alignmentand parsing affect the ability of translation mod-els.
We selected 2000 sentence pairs from eachindividual corpus for the comparison study oftranslational equivalence modeling.
Table 1gives descriptive statistics of the tree data set.Chinese EnglishFBIS 48,331 59,788IWSLT  17,667 18,427HIT 18,215 20,266Table 1.
# of words of experimental datafor synchronous parsing (there are 2k sen-tence pairs in each individual corpus)In the synchronous parsing experiments, wecompared three synchronous grammars: SCFG,STSG and STSSG using the STSSG platform.We use the same settings except the followingparameters (please refer to Subsection 3.2 fortheir definitions): s?
= t?
=1, s?
= t?
=2 forSCFG ; s?
= t?
=1 and s?
= t?
=6 for STSG;s?
= t?
= 4 and s?
= t?
=6 for STSSG.
We iter-ate over each sentence pair in the three corporawith the following process:1) to used Stanford parser (Klein and Manning,2003) to parse bilingual sentences separately,this means that our study is based on the PennTreebank style grammar.2) to extract SCFG, STSG and STSSG rulesform each sentence pair, respectively;3) to do synchronous parsing using the exactedrules.Finally, we can calculate the successful rate ofthe synchronous parsing on each corpus.SMT evaluation settings: For the SMT ex-periments, we trained the translation model onthe FBIS corpus (7.2M (Chinese)+9.2M(English)words) and trained a 4-gram language model on1101the Xinhua portion of the English Gigaword cor-pus (181M words) using the SRILM Toolkits(Stolcke, 2002) with modified Kneser-Neysmoothing (Chen and Goodman, 1998).
We usedthese sentences with less than 50 characters fromthe NIST MT-2002 test set as our developmentset and the NIST MT-2005 test set as our test set.We used the Stanford parser (Klein and Manning,2003) to parse bilingual sentences on the trainingset and Chinese sentences on the developmentand test sets.
The evaluation metric is case-sensitive BLEU-4 (Papineni et al, 2002).
Weused GIZA++ and the heuristics ?grow-diag-final?
to generate m-to-n word alignments.
Forthe MER training, we modified Koehn?s MERtrainer (Koehn, 2004) for our STSSG-based sys-tem.
For significance test, we used Zhang et alsimplementation (Zhang et al 2004).
We com-pared four SMT systems: Moses (Koehn et al,2007), SCFG-based, STSG-based and STSSG-based tree-to-tree translation models.
For Moses,we used its default settings.
For the others, weimplemented them on the STSSG platform byadopting the same settings as used in the syn-chronous parsing.
We optimized the decodingparameters on the development sets empirically.4.2 Experimental ResultsSCFG STSG STSSGFBIS 7 (0.35%) 143 (7.15%) 388 (19.4%)IWSLT 171 (8.6%) 1179 (58.9%) 1708 (85.4%)HIT 65 (3.23%) 1133 (56.6%) 1532 (76.6%)Table 2.
Successful rates (numbers insidebracket) of synchronous parsing over 2,000sentence pairs, where the integers outsidebracket are the numbers of successfully-parsed sentence pairsTable 2 reports the experimental results of syn-chronous parsing.
It shows that:1) As an extension of STSG/SCFG, STSSGoutperforms STSG and SCFG consistently in thethree data sets.
The significant difference sug-gests that the STSSG is much more effective inmodeling translational equivalences and structuredivergences.
The reason is simply because theSTSSG uses tree sequences as the basic transla-tion unit so that it can model non-syntacticphrase equivalence with structure informationand handle structure reordering in a large span.2) STSG shows much better performance thanSCFG.
It is mainly due to that STSG allow mul-tiple level tree nodes operation and reordering ina larger span than SCFG.
It reconfirms that onlyallowing sibling nodes reordering as done inSCFG may be inadequate for translational equiva-lence modeling (Galley et al, 2004)4.3) All the three models on the FBIS corpusshow much lower performance than that on theother two corpora.
The main reason, as shown inTable 1, is that the sentences in the FBIS corpusare much longer than that in the other corpus, sotheir syntactic structures are significantly morecomplicated than the other two.
In addition, al-though tree sequences are utilized, STSSG showmuch lower performance in the FBIS corpus.This implies that the complexity of structure di-vergence between two languages is higher thansuggested in literature (Fox, 2002; Galley et al,2004).
Therefore, structure divergence is still abig challenge to translational equivalence model-ing when using syntactic structure mapping.4) The HIT corpus does not show better per-formance than the IWSLT corpus although theHIT corpus is manually annotated with parsetrees and word alignments.
In order to studywhether high performance word alignment andparsing results can help synchronous parsing, wedo several cross validations and report the ex-perimental results in Table 3.Gold Word AlignmentAutomaticWord Align-mentGold Parse 3.2/56.6/76.6 2.9/57.7/80.9AutomaticParse 3.2/55.6/76.0 2.9/54.2/78.8Table 3.
Successful rates (SCFG/STSG/STSSG)(%) with regards to different wordalignments and parse trees  on the HIT corpusTable 3 compares the performance of syn-chronous parsing on the HIT corpus when usinggold and automatic parser and word alignment.
Itis surprised that gold word alignments and parsetrees do not help and even decrease the perform-ance slightly.
Our analysis further finds that4 This claim is mainly hold for linguistically-informedSCFG since formal SCFG and BTG already showedmuch better performance in the formally syntax-basedtranslation framework (Chiang, 2005).
This is becausethe formal syntax is learned from phrase translationalequivalences directly without relying on any linguistictheory (Chiang, 2005).
Thus, it may not suffer fromthe issues of non-isomorphic structure alignment andnon-syntactic phrase usage heavily (Wellington et al,2006).1102more than 90% sentence pairs out of all the sen-tence pairs that can be successfully bi-parsed arein common in the four experiments.
This sug-gests that the STSSG/STSG (SCFG achieves toomuch lower performance) and our rule extractionalgorithm are robust in dealing with the errorsintroduced by the word alignment and parsingprograms.
If a parser, for example, makes a sys-tematic error, we expect to learn a rule that cannevertheless be systematically used to model cor-rect translational equivalence.
Our error analysison the three corpora shows that most of the fail-ures of synchronous parsing are due to the struc-ture divergence (i.e.
the nature of non-isomorphic structure mapping) and the long dis-tance dependence in the syntactic structures.SCFG Moses STSG STSSGBLEU(%) 22.72 23.86 24.71 26.07Table 3.
Performance comparison of dif-ferent grammars on FBIS corpusTable 3 compares different grammars in termsof translation performance.
It shows that:1) The same as synchronous parsing, theSTSSG-based model statistically significantlyoutperforms (p < 0.01) previous phrase-based andlinguistically syntax-based methods.
This empiri-cally verifies the effect of the tree-sequence-basedgrammar for statistical machine translation.2) Both STSSG and STSG outperform Mosessignificantly and STSSG clearly outperformsSTSG, which suggest that:z The linguistically motivated structure fea-tures are still useful for SMT, which can be cap-tured by the two syntax-based grammars throughtree node operations.z STSSG is much more effective in utiliz-ing linguistic structures than STSG since it usestree sequence as the basic translation unit.
Thisenables STSSG not only to handle structure reor-derings by tree node operations in a larger span,but also to capture non-syntactic phrases with syn-tactic information, and hence giving the grammarmore expressive power.3) The linguistic-based SCFG shows muchlower performance.
This is largely because SCFGonly allows sibling nodes reordering and fails toutilize both non-syntactic phrases and those syn-tactic phrases that cannot be covered by a singleCFG rule.
It thereby suggests that SCFG is lesseffective in modelling parse tree structure trans-fer.The above two experimental results show thatSTSSG achieves significant improvements overthe other two grammars in terms of synchronousparsing?s successful rate and translation Bleuscore.5 ConclusionsGrammar is the fundamental infrastructure intranslational equivalence modeling and statisticalmachine translation since grammar formalizeswhat kind of rule to be learned from a paralleltext.
In this paper, we first present a general plat-form STSSG and demonstrate that a number ofsynchronous grammars and SMT models can beeasily implemented based on the platform.
Wethen compare the expressive abilities of differentgrammars on the platform using synchronousparsing and statistical machine translation.
Ourexperimental results show that STSSG can betterexplain the data in parallel corpora than the othertwo synchronous grammars.
We further findsthat, although syntactic structure features arehelpful in modeling translational equivalence, thecomplexity of structure divergence is muchhigher than suggested in literature, which im-poses a big challenge to syntactic transformation-based SMT.
This may explain why traditionalsyntactic constraints in SMT do not yield muchperformance improvement over robust phrase-substitution models.The fundamental assumption underlying muchrecent work on syntax-based modeling, which isconsidered to be one of next technology break-throughs in SMT, is that translational equiva-lence can be well modeled by structural trans-formation.
However, as discussed in prior arts(Galley et al, 2004) and this paper, linguisti-cally-informed SCFG is an inadequate model forparallel corpora due to its nature that only allow-ing child-node reorderings.
Although STSGshows much better performance than SCFG, itstwo major limitations are that it only allowsstructure distortion operated on a single sub-treeand cannot model non-syntactic phrases.
STSSGextends STSG by using tree sequence as the ba-sic translation unit.
This gives the grammar muchmore expressive power.There are many open issues in the syntactictransformation-based SMT due to the divergencenature between bilingual structure mappings.
Wefind that structural divergences are more seriousthan suggested in the literature (Fox, 2002; Gal-lery et al, 2004) or what we expected when sen-tences are longer.
We will continue to investigate1103whether and how parallel corpora can be wellmodeled by syntactic structure mappings.ReferencesRens Bod.
2007.
Unsupervised Syntax-Based Ma-chine Translation: The Contribution of Discon-tinuous Phrases.
MT-Summmit-07.
51-56.Peter F. Brown, S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
The mathematics of ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.S.
F. Chen and J. Goodman.
1998.
An empirical studyof smoothing techniques for language modeling.Technical Report TR-10-98, Harvard UniversityCenter for Research in Computing Technology.David Chiang.
2005.
A hierarchical phrase-basedmodel for SMT.
ACL-05.
263-270.H.
Comon, M. Dauchet, R. Gilleron, F. Jacquemard,D.
Lugiez, S. Tison, and M. Tommasi.
2007.
Treeautomata techniques and applications.
Available at:http://tata.gforge.inria.fr/.Brooke Cowan, Ivona Kucerova and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
EMNLP-06.
232-241.S.
DeNeefe, K. Knight, W. Wang and D. Marcu.
2007.What Can Syntax-based MT Learn from Phrase-based MT?
EMNLP-CoNLL-07.
755-763Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
ACL-05.
541-548.Bonnie J. Dorr (1994).
Machine Translation Diver-gences: A formal description and proposed solu-tion.
Computational Linguistics, 20(4): 597-633Jason Eisner.
2003.
Learning non-isomorphic treemappings for MT.
ACL-03 (companion volume).Heidi J.
Fox.
2002.
Phrasal Cohesion and StatisticalMachine Translation.
EMNLP-2002.
304-311Michel Galley, J. Graehl, K. Knight, D. Marcu, S.DeNeefe, W. Wang and I. Thayer.
2006.
ScalableInference and Training of Context-Rich SyntacticTranslation Models.
COLING-ACL-06.
961-968M.
Galley, M. Hopkins, K. Knight and D. Marcu.2004.
What?s in a translation rule?
HLT-NAACL.Liang Huang, Kevin Knight and Aravind Joshi.
2006.Statistical Syntax-Directed Translation with Ex-tended Domain of Locality.
AMTA-06 (poster).Mary Hearne and Andy Way.
2003.
Seeing the woodfor the trees: data-oriented translation.
MT Sum-mit IX, 165-172.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
ACL-03.
423-430.Philipp Koehn, F. J. Och and D. Marcu.
2003.
Statis-tical phrase-based translation.
HLT-NAACL-03.127-133.Philipp Koehn.
2004.
Pharaoh: a beam search de-coder for phrase-based statistical machine transla-tion models.
AMTA-04, 115-124.Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W.Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin and E. Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.ACL-07 (poster) 77-180.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
COLING-ACL-06.
609-616.Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.2007.
Forest-to-String Statistical Translation Rules.ACL-07.
704-711.Daniel Marcu, W. Wang, A. Echihabi and K. Knight.2006.
SPMT: Statistical Machine Translation withSyntactified Target Language Phrases.
EMNLP-06.44-52.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
ACL-04.
653-660.K.
Papineni, Salim Roukos, ToddWard and Wei-JingZhu.
2002.
BLEU: a method for automatic evalua-tion of machine translation.
ACL-02.
311-318.Arjen Poutsma.
2000.
Data-oriented translation.COLING-2000.
635-641Chris Quirk, Arul Menezes and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
ACL-05.
271-279.William Schuler, David Chiang and Mark Dras.
2000.Multi-Component TAG and Notions of FormalPower.
ACL-2000.
448-455Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
ICSLP-02.
901-904.Benjamin Wellington, Sonjia Waxmonsky and I. DanMelamed.
2006.
Empirical Lower Bounds on theComplexity of Translational Equivalence.
COL-ING-ACL-06.
977-984.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377-403.K.
Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
ACL-01.
523-530.M.
Zhang, H. Jiang, A. Aw, J.
Sun, S. Li and C. Tan.2007.
A Tree-to-Tree Alignment-based Model forSMT.
MT-Summit-07.
535-542.Y.
Zhang, S. Vogel and A. Waibel.
2004.
InterpretingBLEU/NIST scores: How much improvement dowe need to have a better system?
LREC-04.1104
