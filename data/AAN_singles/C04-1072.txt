ORANGE: a Method for Evaluating Automatic Evaluation Metricsfor Machine TranslationChin-Yew Lin and Franz Josef OchInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292, USA{cyl,och}@isi.eduAbstractComparisons of automatic evaluation metricsfor machine translation are usually conductedon corpus level using correlation statisticssuch as Pearson?s product moment correlationcoefficient or Spearman?s rank ordercorrelation coefficient between human scoresand automatic scores.
However, suchcomparisons rely on human judgments oftranslation qualities such as adequacy andfluency.
Unfortunately, these judgments areoften inconsistent and very expensive toacquire.
In this paper, we introduce a newevaluation method, ORANGE, for evaluatingautomatic machine translation evaluationmetrics automatically without extra humaninvolvement other than using a set of referencetranslations.
We also show the results ofcomparing several existing automatic metricsand three new automatic metrics usingORANGE.1 IntroductionTo automatically evaluate machine translations,the machine translation community recentlyadopted an n-gram co-occurrence scoringprocedure BLEU (Papineni et al 2001).
A similarmetric, NIST, used by NIST (NIST 2002) in acouple of machine translation evaluations in thepast two years is based on BLEU.
The main idea ofBLEU is to measure the translation closenessbetween a candidate translation and a set ofreference translations with a numerical metric.Although the idea of using objective functions toautomatically evaluate machine translation qualityis not new (Su et al 1992), the success of BLEUprompts a lot of interests in developing betterautomatic evaluation metrics.
For example, Akibaet al (2001) proposed a metric called RED basedon edit distances over a set of multiple references.Nie?en et al (2000) calculated the length-normalized edit distance, called word error rate(WER), between a candidate and multiplereference translations.
Leusch et al (2003)proposed a related measure called position-independent word error rate (PER) that did notconsider word position, i.e.
using bag-of-wordsinstead.
Turian et al (2003) introduced GeneralText Matcher (GTM) based on accuracy measuressuch as recall, precision, and F-measure.With so many different automatic metricsavailable, it is necessary to have a common andobjective way to evaluate these metrics.Comparison of automatic evaluation metrics areusually conducted on corpus level using correlationanalysis between human scores and automaticscores such as BLEU, NIST, WER, and PER.However, the performance of automatic metrics interms of human vs. system correlation analysis isnot stable across different evaluation settings.
Forexample, Table 1 shows the Pearson?s linearcorrelation coefficient analysis of 8 machinetranslation systems from 2003 NIST Chinese-English machine translation evaluation.
ThePearson?
correlation coefficients are computedaccording to different automatic evaluationmethods vs. human assigned adequacy andfluency.
BLEU1, 4, and 12 are BLEU withmaximum n-gram lengths of 1, 4, and 12respectively.
GTM10, 20, and 30 are GTM withexponents of 1.0, 2.0, and 3.0 respectively.
95%confidence intervals are estimated using bootstrapresampling (Davison and Hinkley 1997).
From theBLEU group, we found that shorter BLEU has betteradequacy correlation while longer BLEU has betterfluency correlation.
GTM with smaller exponenthas better adequacy correlation and GTM withlarger exponent has better fluency correlation.NIST is very good in adequacy correlation but notas good as GTM30 in fluency correlation.
Basedon these observations, we are not able to concludewhich metric is the best because it depends on themanual evaluation criteria.
This results alsoindicate that high correlation between human andautomatic scores in both adequacy and fluencycannot always been achieved at the same time.The best performing metrics in fluencyaccording to Table 1 are BLEU12 and GTM30(dark/green cells).
However, many metrics arestatistically equivalent (gray cells) to them whenwe factor in the 95% confidence intervals.
Forexample, even PER is as good as BLEU12 inadequacy.
One reason for this might be due to datasparseness since only 8 systems are available.The other potential problem for correlationanalysis of human vs. automatic framework is thathigh corpus-level correlation might not translate tohigh sentence-level correlation.
However, highsentence-level correlation is often an importantproperty that machine translation researchers lookfor.
For example, candidate translations shorterthan 12 words would have zero BLEU12 score butBLEU12 has the best correlation with humanjudgment in fluency as shown in Table 1.In order to evaluate the ever increasing numberof automatic evaluation metrics for machinetranslation objectively, efficiently, and reliably, weintroduce a new evaluation method: ORANGE.
Wedescribe ORANGE in details in Section 2 andbriefly introduce three new automatic metrics thatwill be used in comparisons in Section 3.
Theresults of comparing several existing automaticmetrics and the three new automatic metrics usingORANGE will be presented in Section 4.
Weconclude this paper and discuss future directions inSection 5.2 ORANGEIntuitively a good evaluation metric should givehigher score to a good translation than a bad one.Therefore, a good translation should be rankedhigher than a bad translation based their scores.One basic assumption of all automatic evaluationmetrics for machine translation is that referencetranslations are good translations and the more amachine translation is similar to its referencetranslations the better.
We adopt this assumptionand add one more assumption that automatictranslations are usually worst than their referencetranslations.
Therefore, reference translationsshould be ranked higher than machine translationson average if a good automatic evaluation metric isused.
Based on these assumptions, we propose anew automatic evaluation method for evaluation ofautomatic machine translation metrics as follows:Given a source sentence, its machinetranslations, and its reference translations, wecompute the average rank of the referencetranslations within the combined machine andreference translation list.
For example, astatistical machine translation system such asISI?s AlTemp SMT system (Och 2003) cangenerate a list of n-best alternative translationsgiven a source sentence.
We compute theautomatic scores for the n-best translationsand their reference translations.
We then rankthese translations, calculate the average rankof the references in the n-best list, andcompute the ratio of the average referencerank to the length of the n-best list.
We callthis ratio ?ORANGE?
(Oracle1 Ranking forGisting Evaluation) and the smaller the ratiois, the better the automatic metric is.There are several advantages of the proposedORANGE evaluation method:?
No extra human involvement ?
ORANGEuses the existing human references but nothuman evaluations.?
Applicable on sentence-level ?
Diagnosticerror analysis on sentence-level is naturallyprovided.
This is a feature that manymachine translation researchers look for.?
Many existing data points ?
Every sentenceis a data point instead of every system(corpus-level).
For example, there are 919sentences vs. 8 systems in the 2003 NISTChinese-English machine translationevaluation.?
Only one objective function to optimize ?Minimize a single ORANGE score instead ofmaximize Pearson?s correlation coefficientsbetween automatic scores and humanjudgments in adequacy, fluency, or otherquality metrics.?
A natural fit to the existing statisticalmachine translation framework ?
A metricthat ranks a good translation high in an n-best list could be easily integrated in aminimal error rate statistical machinetranslation training framework (Och 2003).The overall system performance in terms of1 Oracles refer to the reference translations used inthe evaluation procedure.Method Pearson 95%L 95%U Pearson 95%L 95%UBLEU1 0.86 0.83 0.89 0.81 0.75 0.86BLEU4 0.77 0.72 0.81 0.86 0.81 0.90BLEU12 0.66 0.60 0.72 0.87 0.76 0.93NIST 0.89 0.86 0.92 0.81 0.75 0.87WER 0.47 0.41 0.53 0.69 0.62 0.75PER 0.67 0.62 0.72 0.79 0.74 0.85GTM10 0.82 0.79 0.85 0.73 0.66 0.79GTM20 0.77 0.73 0.81 0.86 0.81 0.90GTM30 0.74 0.70 0.78 0.87 0.81 0.91Adequacy FluencyTable 1.
Pearson's correlation analysis of 8machine translation systems in 2003 NISTChinese-English machine translationevaluation.generating more human like translationsshould also be improved.Before we demonstrate how to use ORANGE toevaluate automatic metrics, we briefly introducethree new metrics in the next section.3 Three New MetricsROUGE-L and ROUGE-S are described in detailsin Lin and Och (2004).
Since these two metrics arerelatively new, we provide short summaries ofthem in Section 3.1 and Section 3.3 respectively.ROUGE-W, an extension of ROUGE-L, is new andis explained in details in Section 3.2.3.1 ROUGE-L: Longest Common Sub-sequenceGiven two sequences X and Y, the longestcommon subsequence (LCS) of X and Y is acommon subsequence with maximum length(Cormen et al 1989).
To apply LCS in machinetranslation evaluation, we view a translation as asequence of words.
The intuition is that the longerthe LCS of two translations is, the more similar thetwo translations are.
We propose using LCS-basedF-measure to estimate the similarity between twotranslations X of length m and Y of length n,assuming X is a reference translation and Y is acandidate translation, as follows:Rlcs mYXLCS ),(=       (1)Plcs nYXLCS ),(=       (2)FlcslcslcslcslcsPRPR22 )1(?
?++=   (3)Where LCS(X,Y) is the length of a longestcommon subsequence of X and Y, and ?
= Plcs/Rlcswhen ?Flcs/?Rlcs_=_?Flcs/?Plcs.
We call the LCS-based F-measure, i.e.
Equation 3, ROUGE-L.Notice that ROUGE-L is 1 when X = Y sinceLCS(X,Y) = m or n; while ROUGE-L is zero whenLCS(X,Y) = 0, i.e.
there is nothing in commonbetween X and Y.One advantage of using LCS is that it does notrequire consecutive matches but in-sequencematches that reflect sentence level word order as n-grams.
The other advantage is that it automaticallyincludes longest in-sequence common n-grams,therefore no predefined n-gram length is necessary.By only awarding credit to in-sequence unigrammatches, ROUGE-L also captures sentence levelstructure in a natural way.
Consider the followingexample:S1.
police killed the gunmanS2.
police kill the gunmanS3.
the gunman kill policeUsing S1 as the reference translation, S2 has aROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE-L score of 2/4 = 0.5, with ?
= 1.
Therefore S2 isbetter than S3 according to ROUGE-L. Thisexample illustrated that ROUGE-L can workreliably at sentence level.
However, LCS suffersone disadvantage: it only counts the main in-sequence words; therefore, other alternative LCSesand shorter sequences are not reflected in the finalscore.
In the next section, we introduce ROUGE-W.3.2 ROUGE-W: Weighted Longest CommonSubsequenceLCS has many nice properties as we havedescribed in the previous sections.
Unfortunately,the basic LCS also has a problem that it does notdifferentiate LCSes of different spatial relationswithin their embedding sequences.
For example,given a reference sequence X and two candidatesequences Y1 and Y2 as follows:X:  [A B C D E F G]Y1: [A B C D H I K]Y2:  [A H B K C I D]Y1 and Y2 have the same ROUGE-L score.However, in this case, Y1 should be the betterchoice than Y2 because Y1 has consecutive matches.To improve the basic LCS method, we can simplyremember the length of consecutive matchesencountered so far to a regular two dimensionaldynamic program table computing LCS.
We callthis weighted LCS (WLCS) and use k to indicatethe length of the current consecutive matchesending at words xi and yj.
Given two sentences Xand Y, the recurrent relations can be written asfollows:(1) If xi = yj Then// the length of consecutive matches at// position i-1 and j-1k = w(i-1,j-1)c(i,j) = c(i-1,j-1) + f(k+1) ?
f(k)// remember the length of consecutive// matches at position i, jw(i,j) = k+1(2) OtherwiseIf c(i-1,j) > c(i,j-1) Thenc(i,j) = c(i-1,j)w(i,j) = 0           // no match at i, jElse c(i,j) = c(i,j-1)w(i,j) = 0           // no match at i, j(3) WLCS(X,Y) = c(m,n)Where c is the dynamic programming table, 0 <=i <= m, 0 <= j <= n, w is the table storing thelength of consecutive matches ended at c tableposition i and j, and f is a function of consecutivematches at the table position, c(i,j).
Notice that byproviding different weighting function f, we canparameterize the WLCS algorithm to assigndifferent credit to consecutive in-sequencematches.The weighting function f must have the propertythat f(x+y) > f(x) + f(y) for any positive integers xand y.
In other words, consecutive matches areawarded more scores than non-consecutivematches.
For example, f(k)-=-?k ?
?
when k >= 0,and ?, ?
> 0.
This function charges a gap penaltyof ??
for each non-consecutive n-gram sequences.Another possible function family is the polynomialfamily of the form k?
where -?
> 1.
However, inorder to normalize the final ROUGE-W score, wealso prefer to have a function that has a close forminverse function.
For example, f(k)-=-k2 has a closeform inverse function f -1(k)-=-k1/2.
F-measurebased on WLCS can be computed as follows,given two sequences X of length m and Y of lengthn:Rwlcs ????????=?
)(),(1mfYXWLCSf       (4)Pwlcs ????????=?
)(),(1nfYXWLCSf       (5)FwlcswlcswlcswlcswlcsPRPR22 )1(?
?++=           (6)f -1 is the inverse function of f. We call theWLCS-based F-measure, i.e.
Equation 6, ROUGE-W.
Using Equation 6 and f(k)-=-k2 as theweighting function, the ROUGE-W scores forsequences Y1 and Y2 are 0.571 and 0.286respectively.
Therefore, Y1 would be rankedhigher than Y2 using WLCS.
We use thepolynomial function of the form k?
in theexperiments described in Section 4 with theweighting factor ?
varying from 1.1 to 2.0 with 0.1increment.
ROUGE-W is the same as ROUGE-Lwhen ?
is set to 1.In the next section, we introduce the skip-bigramco-occurrence statistics.3.3 ROUGE-S: Skip-Bigram Co-OccurrenceStatisticsSkip-bigram is any pair of words in their sentenceorder, allowing for arbitrary gaps.
Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a setof reference translations.
Using the example givenin Section 3.1:S1.
police killed the gunmanS2.
police kill the gunmanS3.
the gunman kill policeS4.
the gunman police killedeach sentence has C(4,2)2 = 6 skip-bigrams.
Forexample, S1 has the following skip-bigrams:(?police killed?, ?police the?, ?police gunman?,?killed the?, ?killed gunman?, ?the gunman?
)Given translations X of length m and Y of lengthn, assuming X is a reference translation and Y is acandidate translation, we compute skip-bigram-based F-measure as follows:Rskip2 )2,(),(2mCYXSKIP=           (7)Pskip2 )2,(),(2nCYXSKIP=           (8)Fskip2222222 )1(skipskipskipskipPRPR?
?++=   (9)Where SKIP2(X,Y) is the number of skip-bigrammatches between X and Y, ?
= Pskip2/Rskip2 when?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and  C is thecombination function.
We call the skip-bigram-based F-measure, i.e.
Equation 9, ROUGE-S. UsingEquation 9 with ?
= 1 and S1 as the reference, S2?sROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333.Therefore, S2 is better than S3 and S4, and S4 isbetter than S3.One advantage of skip-bigram vs. BLEU is that itdoes not require consecutive matches but is stillsensitive to word order.
Comparing skip-bigramwith LCS, skip-bigram counts all in-ordermatching word pairs while LCS only counts onelongest common subsequence.
We can limit themaximum skip distance, between two in-orderwords to control the admission of a skip-bigram.We use skip distances of 1 to 9 with increment of 1(ROUGE-S1 to 9) and without any skip distanceconstraint (ROUGE-S*).In the next section, we present the evaluations ofBLEU, NIST, PER, WER, ROUGE-L, ROUGE-W,and ROUGE-S using the ORANGE evaluationmethod described in Section 2.2 Combinations: C(4,2) = 4!/(2!*2!)
= 6.4 ExperimentsComparing automatic evaluation metrics usingthe ORANGE evaluation method is straightforward.To simulate real world scenario, we use n-best listsfrom ISI?s state-of-the-art statistical machinetranslation system, AlTemp (Och 2003), and the2002 NIST Chinese-English evaluation corpus asthe test corpus.
There are 878 source sentences inChinese and 4 sets of reference translationsprovided by LDC3.
For exploration study, wegenerate 1024-best list using AlTemp for 872source sentences.
AlTemp generates less than 1024alternative translations for 6 out of the 878 source3 Linguistic Data Consortium prepared these manualtranslations as part of the DARPA?s TIDES project.sentences.
These 6 source sentences are excludedfrom the 1024-best set.
In order to compute BLEUat sentence level, we apply the followingsmoothing technique:Add one count to the n-gram hit and total n-gram count for n > 1.
Therefore, for candidatetranslations with less than n words, they canstill get a positive smoothed BLEU score fromshorter n-gram matches; however if nothingmatches then they will get zero scores.We call the smoothed BLEU: BLEUS.
For eachcandidate translation in the 1024-best list and eachreference, we compute the following scores:1.
BLEUS1 to 92.
NIST, PER, and WER3.
ROUGE-L4.
ROUGE-W with weight ranging from 1.1to 2.0 with increment of 0.15.
ROUGE-S with maximum skip distanceranging from 0 to 9 (ROUGE-S0 to S9)and without any skip distance limit(ROUGE-S*)We compute the average score of the referencesand then rank the candidate translations and thereferences according to these automatic scores.The ORANGE score for each metric is calculated asthe average rank of the average reference (oracle)score over the whole corpus (872 sentences)divided by the length of the n-best list plus 1.Assuming the length of the n-best list is N and thesize of the corpus is S (in number of sentences), wecompute Orange as follows:ORANGE =)1()(1+????????
?=NSOracleRankSii(10)Rank(Oraclei) is the average rank of sourcesentence i?s reference translations in n-best list i.Table 2 shows the results for BLEUS1 to 9.
Toassess the reliability of the results, 95% confidenceintervals (95%-CI-L for lower bound and CI-U forupper bound) of average rank of the oracles areMethod ORANGE Avg Rank 95%-CI-L  95%-CI-UBLEUS1 35.39% 363 337 387BLEUS2 25.51% 261 239 283BLEUS3 23.74% 243 221 267BLEUS4 23.13% 237 215 258BLEUS5 23.13% 237 215 260BLEUS6 22.91% 235 211 257BLEUS7 22.98% 236 213 258BLEUS8 23.20% 238 214 261BLEUS9 23.56% 241 218 265Table 2.
ORANGE scores for BLEUS1 to 9.Method Pearson 95%L 95%U Pearson 95%L 95%UBLEUS1 0.87 0.84 0.90 0.83 0.77 0.88BLEUS2 0.84 0.81 0.87 0.85 0.80 0.90BLEUS3 0.80 0.76 0.84 0.87 0.82 0.91BLEUS4 0.76 0.72 0.80 0.88 0.83 0.92BLEUS5 0.73 0.69 0.78 0.88 0.83 0.91BLEUS6 0.70 0.65 0.75 0.87 0.82 0.91BLEUS7 0.65 0.60 0.70 0.85 0.80 0.89BLEUS8 0.58 0.52 0.64 0.82 0.76 0.86BLEUS9 0.50 0.44 0.57 0.76 0.70 0.82Adequacy FluencyTable 3.
Pearson's correlation analysisBLEUS1 to 9 vs. adequacy and fluency of 8machine translation systems in 2003 NISTChinese-English machine translationevaluation.Method ORANGE Avg Rank 95%-CI-L 95%-CI-UROUGE-L 20.56% 211 190 234ROUGE-W-1.1 20.45% 210 189 232ROUGE-W-1.2 20.47% 210 186 230ROUGE-W-1.3 20.69% 212 188 234ROUGE-W-1.4 20.91% 214 191 238ROUGE-W-1.5 21.17% 217 196 241ROUGE-W-1.6 21.47% 220 199 242ROUGE-W-1.7 21.72% 223 200 245ROUGE-W-1.8 21.88% 224 204 246ROUGE-W-1.9 22.04% 226 203 249ROUGE-W-2.0 22.25% 228 206 250Table 4.
ORANGE scores for ROUGE-L andROUGE-W-1.1 to 2.0.Method ORANGE Avg Rank 95%-CI-L 95%-CI-UROUGE-S0 25.15% 258 234 280ROUGE-S1 22.44% 230 209 253ROUGE-S2 20.38% 209 186 231ROUGE-S3 19.81% 203 183 226ROUGE-S4 19.66% 202 177 224ROUGE-S5 19.95% 204 184 226ROUGE-S6 20.32% 208 187 230ROUGE-S7 20.77% 213 191 236ROUGE-S8 21.42% 220 198 242ROUGE-S9 21.92% 225 204 247ROUGE-S* 27.43% 281 259 304Table 5.
ORANGE scores for ROUGE-S1 to 9and ROUGE-S*.estimated using bootstrap resampling (Davison andHinkley).
According to Table 2, BLEUS6(dark/green cell) is the best performer among allBLEUSes, but it is statistically equivalent toBLEUS3, 4, 5, 7, 8, and 9 with 95% of confidence.Table 3 shows Pearson?s correlation coefficientfor BLEUS1 to 9 over 8 participants in 2003 NISTChinese-English machine translation evaluation.According to Table 3, we find that shorter BLEUShas better correlation with adequacy.
However,correlation with fluency increases when longer n-gram is considered but decreases after BLEUS5.There is no consensus winner that achieves bestcorrelation with adequacy and fluency at the sametime.
So which version of BLEUS should we use?A reasonable answer is that if we would like tooptimize for adequacy then choose BLEUS1;however, if we would like to optimize for fluencythen choose BLEUS4 or BLEUS5.
According toTable 2, we know that BLEUS6 on average placesreference translations at rank 235 in a 1024-bestlist machine translations that is significantly betterthan BLEUS1 and BLEUS2.
Therefore, we havebetter chance of finding more human-liketranslations on the top of an n-best list by choosingBLEUS6 instead of BLEUS2.
To design automaticmetrics better than BLEUS6, we can carry out erroranalysis over the machine translations that areranked higher than their references.
Based on theresults of error analysis, promising modificationscan be identified.
This indicates that the ORANGEevaluation method provides a natural automaticevaluation metric development cycle.Table 4 shows the ORANGE scores for ROUGE-Land ROUGE-W-1.1 to 2.0.
ROUGE-W 1.1 does havebetter ORANGE score but it is equivalent to otherROUGE-W variants and ROUGE-L. Table 5 listsperformance of different ROUGE-S variants.ROUGE-S4 is the best performer but is onlysignificantly better than ROUGE-S0 (bigram),ROUGE-S1, ROUGE-S9 and ROUGE-S*.
Therelatively worse performance of ROUGE-S* mightto due to spurious matches such as ?the the?
or?the of?.Table 6 summarizes the performance of 7different metrics.
ROUGE-S4 (dark/green cell) isthe best with an ORANGE score of 19.66% that isstatistically equivalent to ROUGE-L and ROUGE-W-1.1 (gray cells) and is significantly better thanBLEUS6, NIST, PER, and WER.
Among themPER is the worst.To examine the length effect of n-best lists onthe relative performance of automatic metrics, weuse the AlTemp SMT system to generate a 16384-best list and compute ORANGE scores for BLEUS4,PER, WER, ROUGE-L, ROUGE-W-1.2, andROUGE-S4.
Only 474 source sentences that havemore than 16384 alternative translations are usedin this experiment.
Table 7 shows the results.
Itconfirms that when we extend the length of the n-best list to 16 times the size of the 1024-best, therelative performance of each automatic evaluationmetric group stays the same.
ROUGE-S4 is still thebest performer.
Figure 1 shows the trend ofORANGE scores for these metrics over N-best listof N from 1 to 16384 with length increment of 64.It is clear that relative performance of these metricsstay the same over the entire range.5 ConclusionIn this paper we introduce a new automaticevaluation method, ORANGE, to evaluate automaticevaluation metrics for machine translations.
Weshowed that the new method can be easilyimplemented and integrated with existingstatistical machine translation frameworks.ORANGE assumes a good automatic evaluationmetric should assign high scores to goodtranslations and assign low scores to badtranslations.
Using reference translations asexamples of good translations, we measure thequality of an automatic evaluation metric based onthe average rank of the references within a list ofalternative machine translations.
Comparing withtraditional approaches that require humanjudgments on adequacy or fluency, ORANGErequires no extra human involvement other thanthe availability of reference translations.
It alsostreamlines the process of design and error analysisfor developing new automatic metrics.
UsingORANGE, we have only one parameter, i.e.ORANGE itself, to optimize vs. two in correlationanalysis using human assigned adequacy andfluency.
By examining the rank position of theMethod ORANGE Avg Rank 95%-CI-L 95%-CI-UBLEUS6 22.91% 235 211 257NIST 29.70% 304 280 328PER 36.84% 378 350 403WER 23.90% 245 222 268ROUGE-L 20.56% 211 190 234ROUGE-W-1.1 20.45% 210 189 232ROUGE-S4 19.66% 202 177 224Table 6.
Summary of ORANGE scores for 7automatic evaluation metrics.Method ORANGE Avg Rank 95%-CI-L 95%-CI-UBLEUS4 18.27% 2993 2607 3474PER 28.95% 4744 4245 5292WER 19.36% 3172 2748 3639ROUGE-L 16.22% 2657 2259 3072ROUGE-W-1.2 15.87% 2600 2216 2989ROUGE-S4 14.92% 2444 2028 2860Table 7.
Summary of ORANGE scores for 6automatic evaluation metrics (16384-best list).references, we can easily identify the confusion setof the references and propose new features toimprove automatic metrics.One caveat of the ORANGE method is that whatif machine translations are as good as referencetranslations?
To rule out this scenario, we cansample instances where machine translations areranked higher than human translations.
We thencheck the portion of the cases where machinetranslations are as good as the human translations.If the portion is small then the ORANGE methodcan be confidently applied.
We conjecture that thisis the case for the currently available machinetranslation systems.
However, we plan to conductthe sampling procedure to verify this is indeed thecase.ReferencesAkiba, Y., K. Imamura, and E. Sumita.
2001.
UsingMultiple Edit Distances to Automatically RankMachine Translation Output.
In Proceedings of theMT Summit VIII, Santiago de Compostela, Spain.Cormen, T. R., C. E. Leiserson, and R. L. Rivest.
1989.Introduction to Algorithms.
The MIT Press.Davison, A. C. and D. V. Hinkley.
1997.
BootstrapMethods and Their Application.
CambridgeUniversity Press.Leusch, G., N. Ueffing, and H. Ney.
2003.
A NovelString-to-String Distance Measure with Applicationsto Machine Translation Evaluation.
In Proceedings ofMT Summit IX, New Orleans, U.S.A.Lin, C-Y.
and F.J. Och.
2004.
Automatic Evaluation ofMachine Translation Quality Using LongestCommon Subsequence and Skip-Bigram Statistics.Submitted.Nie?en S., F.J. Och, G, Leusch, H. Ney.
2000.
AnEvaluation Tool for Machine Translation: FastEvaluation for MT Research.
In Proceedings of the2nd International Conference on Language Resourcesand Evaluation, Athens, Greece.NIST.
2002.
Automatic Evaluation of MachineTranslation Quality using N-gram Co-OccurrenceStatistics.
AAAAAAAAAAAhttp://www.nist.gov/speech/tests/mt/docFranz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics (ACL-2003), Sapporo,Japan.Papineni, K., S. Roukos, T. Ward, and W.-J.
Zhu.
2001.Bleu: a Method for Automatic Evaluation of MachineTranslation.
IBM Research Report RC22176(W0109-022).Su, K.-Y., M.-W. Wu, and J.-S. Chang.
1992.
A NewQuantitative Quality Measure for MachineTranslation System.
In Proceedings of COLING-92,Nantes, France.Turian, J. P., L. Shen, and I. D. Melamed.
2003.Evaluation of Machine Translation and itsEvaluation.
In Proceedings of MT Summit IX, NewOrleans, U.S.A.2000 4000 6000 8000 10000 12000 14000 160000.10.150.20.250.30.350.4ORANGE at Different NBEST Cutoff Length avgNBEST Cutoff Length = 1 to 16384ORANGEBLEUS4ROUGE?LPERROUGE?S4WERROUGE?W?1?2Figure 1.
ORANGE scores for 6 metrics vs. length of n-best list from 1 to16384 with increment of 64.
