Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1060?1068, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExtending Machine Translation Evaluation Metrics with Lexical CohesionTo Document LevelBilly T. M. Wong and Chunyu KitDepartment of Chinese, Translation and LinguisticsCity University of Hong Kong83 Tat Chee Avenue, Kowloon, Hong Kong SAR, P. R. China{tmwong,ctckit}@cityu.edu.hkAbstractThis paper proposes the utilization of lexicalcohesion to facilitate evaluation of machinetranslation at the document level.
As a linguis-tic means to achieve text coherence, lexicalcohesion ties sentences together into a mean-ingfully interwoven structure through wordswith the same or related meaning.
A compar-ison between machine and human translationis conducted to illustrate one of their criticaldistinctions that human translators tend to usemore cohesion devices than machine.
Variousways to apply this feature to evaluate machine-translated documents are presented, includingone without reliance on reference translation.Experimental results show that incorporatingthis feature into sentence-level evaluation met-rics can enhance their correlation with humanjudgements.1 IntroductionMachine translation (MT) has benefited a lot fromthe advancement of automatic evaluation in the pastdecade.
To a certain degree, its progress is also con-fined to the limitations of evaluation metrics in use.Most efforts devoted to evaluate the quality of MToutput so far have still focused on the sentence levelwithout sufficient attention to how a larger text isstructured.
This is notably reflected in the represen-tative MT evaluation metrics, such as BLEU (Pap-ineni et al 2002), METEOR (Banerjee and Lavie,2005) and TER (Snover et al 2006), that adopt asentence-by-sentence fashion to score MT outputs.The evaluation result for a document by any of themis usually a simple average of its sentence scores.
Adrawback of this kind of sentence-based evaluationis the neglect of document structure.
There is noguarantee for the coherence of a text if it is producedby simply putting together stand-alone sentences, nomatter how well-translated, without adequate inter-sentential connection.
As a consequence, MT sys-tem optimized this way to any of these metrics canonly have a very dim chance of producing translateddocument that reads as natural as human writing.The accuracy of MT output at the document levelis particularly important to MT users, for they careabout the overall meaning of a text in question morethan the grammatical correctness of each sentence(Visser and Fuji, 1996).
Post-editors particularlyneed to ensure the quality of a whole document ofMT output when revising its sentences.
The con-nectivity of sentences is surely a significant factorcontributing to the understandability of a text as awhole.This paper studies the inter-sentential linguisticfeatures of cohesion and coherence and presentsplausible ways to incorporate them into thesentence-based metrics to support MT evaluation atthe document level.
In the Framework for MT Eval-uation in the International Standards of LanguageEngineering (FEMTI) (King et al 2003), coherenceis defined as ?the degree to which the reader can de-scribe the role of each individual sentence (or groupof sentences) with respect to the text as a whole?.The measurement of coherence has to rely on cohe-sion, referring to the ?relations of meaning that existwithin the text?
(Halliday and Hasan, 1976).
Cohe-sion is realized via the interlinkage of grammaticaland lexical elements across sentences.
Grammatical1060cohesion refers to the syntactic links between textitems, while lexical cohesion is achieved through theword choices in a text.
This paper focuses on thelatter.
A quantitative comparison of lexical cohesiondevices between MT output and human translationis first conducted, to examine the weakness of cur-rent MT systems in handling this feature.
Differentways of exploiting lexical cohesion devices for MTevaluation at the document level are then illustrated.2 Related WorksCohesion and coherence are both necessary mono-lingual features in a target text.
They can hardlybe evaluated in isolation and have to be conjoinedwith other quality criteria such as adequacy and flu-ency.
A survey of MT post-editing (Vasconcellos,1989) suggests that cohesion and coherence serveas higher level quality criteria beyond many otherssuch as syntactic well-formedness.
Post-editors tendto correct syntactic errors first before any amend-ment for improving the cohesion and coherence ofan MT output.
Also, as Wilks (1978)1 noted, itis rather unlikely for a sufficiently large sample oftranslations to be coherent and totally wrong at thesame time.
Cohesion and coherence are appropri-ate to serve as criteria for the overall quality of MToutput.Previous researches in MT predominantly focuson specific types of cohesion devices.
For grammat-ical cohesion, a series of works, including Nakaiwaand Ikehara (1992), Nakaiwa et al(1995), andNakaiwa and Shirai (1996), present approaches toresolving Japanese zero pronouns and to integrat-ing them into a Japanese-English transferred-basedMT system.
Peral et al(1999) propose an inter-lingual mechanism for pronominal anaphora gen-eration by exploiting a rich set of lexical, syntac-tic, morphologic and semantic information.
Mu-rata and Nagao (1993) and Murata et al(2001) de-velop a rule base to identify the referential prop-erties of Japanese noun phrases, so as to facilitateanaphora resolution for Japanese and article gen-eration for English during translation.
A recentCOMTIS project (Cartoni et al 2011) begins to ex-ploit inter-sentential information for statistical MT.A phase of its work is to have grammatical devices,1As cited in van Slype (1979).such as verbal tense/aspect/mode, discourse connec-tives and pronouns, manually annotated in multilin-gual corpora, in hopes of laying a foundation for thedevelopment of automatic labelers for them that canbe integrated into an MT model.For lexical cohesion, it has been only partially andindirectly addressed in terms of translation consis-tency in MT output.
Different approaches to main-taining consistency in target word choices are pro-posed (Itagaki et al 2007; Gong et al 2011; Xiaoet al 2011).
Carpuat (2009) also observes a generaltendency in human translation that a given sense isusually lexicalized in a consistent manner through-out the whole translation.Nevertheless there are only a few evaluationmethods explicitly targeting on the quality of a docu-ment.
Miller and Vanni (2001) devise a human eval-uation approach to measure the comprehensibilityof a text as a whole, based on the Rhetorical Struc-ture Theory (Mann and Thompson, 1988), a theoryof text organization specifying coherence relationsin an authentic text.
Snover et al(2006) proposesHTER to assess post-editing effort through humanannotation.
Its automatic versions TER and TERp(Snover et al 2009), however, remain sentence-based metrics.
Comelles et al(2010) present afamily of automatic MT evaluation measures, basedon the Discourse Representation Theory (Kamp andReyle, 1993), that generate semantic trees to put to-gether different text entities for the same referent ac-cording to their contexts and grammatical connec-tions.
Apart from MT evaluation, automated essayscoring programs such as E-rater (Burstein, 2003)also employ a rich set of discourse features for as-sessment.
However, the parsing process needed forthese linguistic-heavy approaches may suffer seri-ously from grammatical errors, which are unavoid-able in MT output.
Hence their accuracy and reli-ability inevitably fluctuate in accord with differentevaluation data.Lexical cohesion has far been neglected in bothMT and MT evaluation, even though it is the singlemost important form of cohesion devices, account-ing for nearly half of the cohesion devices in En-glish (Halliday and Hasan, 1976).
It is also a signif-icant feature contributing to translation equivalenceof texts by preserving their texture (Lotfipour-Saedi,1997).
The lexical cohesion devices in a text can be1061represented as lexical chains conjoining related en-tities.
There are many methods of computing lexicalchains for various purposes, e.g., Morris and Hirst(1991), Barzilay and Elhadad (1997), Chan (2004),Li et al(2007), among many others.
Contrary togrammatical cohesion highly depending on syntac-tic well-formedness of a text, lexical cohesion is lessaffected by grammatical errors.
Its computation hasto rely on a thesaurus, which is usually available foralmost every language.
In this research, a numberof formulations of lexical cohesion, with or withoutreliance on external language resource, will be ex-plored for the purpose of MT evaluation.3 Lexical Cohesion in Machine andHuman TranslationThis section presents a comparative study of MT andhuman translation (HT) in terms of the use of lexi-cal cohesion devices.
It is an intuition that more co-hesion devices are used by humans than machinesin translation, as part of the superior quality of HT.Two different datasets are used to ensure the relia-bility and generality of the comparison.
The resultsconfirm the incapability of MT in handling this fea-ture and the necessity of using lexical cohesion inMT evaluation.3.1 DataThe MetricsMATR 2008 development set (Przy-bocki et al 2009) and the Multiple-Translation Chi-nese (MTC) part 4 (Ma, 2006) are used for thisstudy.
They consist of MT outputs of differentsource languages in company with reference trans-lations.
The data of MetricsMATR is selected fromthe NIST Open MT 2006 evaluation, while MTC4 isfrom the TIDES 2003 MT evaluation.
Both datasetsinclude human assessments of MT output, fromwhich the part of adequacy assessment is selectedfor this study.
Table 1 provides overall statistics ofthe datasets.3.2 Identification of Lexical Cohesion DevicesLexical cohesion is achieved through word choicesof two major types: reiteration and collocation.
Re-iteration can be realized in a continuum or a cline ofspecificity, with repetition of the same lexical item atone end and the use of a general noun to point to theMetricsMATR MTC4Number of systems 8 6Number of documents 25 100Number of segments 249 919Number of references 4 4Source language Arabic ChineseGenre Newswire NewswireTable 1: Information about the datasets in usesame referent at the other.
In between the two endsis to use a synonym (or near-synonym) and superor-dinate.
Collocation refers to those lexical items thatshare the same or similar semantic relations, includ-ing complementarity, antonym, converse, coordinateterm, meronym, troponym, and so on.In this study, lexical cohesion devices are definedas content words (i.e., tokens after stopword havingbeen removed) that reiterate once or more times ina document, including synonym, near-synonym andsuperordinate, besides those repetition and colloca-tion.
Repetition refers to the same words or stemsin a document.
Stems are identified with the aid ofPorter stemmer (1980).To classify the semantic relationships of words,WordNet (Fellbaum, 1998) is used as a lexical re-source, which clusters words of the same sense (i.e.,synonyms) into a semantic group, namely a synset.Synsets are interlinked in WordNet according totheir semantic relationships.
Superordinate and col-location are formed by words in a proximate se-mantic relationship, such as bicycle and vehicle (hy-pernym), bicycle and wheel (meronym), bicycle andcar (coordinate term), and so on.
They are definedas synset pairs with a distance of 1 in WordNet.The measure of semantic distance (Wu and Palmer,1994) is also applied to identify near-synonyms, i.e.,words that are synonyms in a broad sense but notgrouped in the same synset.
It quantifies the seman-tic similarity of word pairs as a real number in be-tween 0 and 1 (the higher the more similar) assim(c1, c2) =2 d(lcs(c1, c2))d(c1) + d(c2)where c1 and c2 are the concepts (synsets) that thetwo words in question belong to, d is the distancein terms of the shortest path from a concept to the1062Word typeMetricsMATR MTC4MT HT Difference (%) MT HT Difference (%)Content word 4428 4636 208 (4.7) 16162 16982 830 (5.1)- Not lexical cohesion device 2403 2381 -22 (-1.0) 8657 8814 157 (1.8)- Lexical cohesion device 2025 2255 230 (11.4) 7505 8168 663 (8.9)- Repetition 1297 1445 148 (11.4) 4888 5509 621 (12.7)- Synonym and near-synonym 318 350 32 (10.1) 1323 1311 -12 (-0.9)- Superordinate and collocation 410 460 50 (12.4) 1294 1348 54 (4.2)Table 2: Statistics of lexical cohesion devices in machine versus human translation (average frequencies per versionof MT/HT)0.200.250.300.350.400.450.500.55MetricsMATR0.200.250.300.350.400.450.500.55MTC4RC (MT)RC (HT)LC (MT)LC (HT)ValuesofRC&LCFigure 1: Use of lexical cohesion devices in machine versus human translationglobal root node in WordNet, and lcs is the leastcommon subsumer (i.e., the most specific ancestorconcept) of c1 and c2.
A threshold is set to 0.96for words to be considered near-synonyms of eachother, based on the empirical observation in a previ-ous study (Wong, 2010).3.3 ResultsThe difference between MT and HT (referencetranslation) in terms of the frequencies of lexical co-hesion devices in MetricsMATR and MTC4 datasetsis presented in Table 2.
The frequencies are aver-aged by the number of MT/HT versions.
A furthercategorization breaks down content words into lex-ical cohesion devices and those that are not.
Thecount of each type of lexical cohesion device is alsoprovided.
In general the two datasets provide highlysimilar statistics.
There are 4.7?5.1% more contentwords in HT than in MT.
The numbers of ordinarycontent words (i.e., not lexical cohesion devices) areclose in MT and HT.
The difference of content wordsin HT and MT is mostly due to that of lexical co-hesion devices, which are mostly repetition.
8.9?11.4% more lexical cohesion devices are found inHT than in MT in the datasets.A further analysis is carried out to investigate intothe use of lexical cohesion devices in each versionof MT and HT in terms of the following two ratios,LC = lexical cohesion devices / content words,RC = repetition / content words.A higher LC or RC ratio means that a greater pro-portion of content words are used as lexical cohesiondevices.Figure 1 illustrates the RC and LC ratios in thetwo datasets.
The ratios of different MT systemsare presented in an ascending order in each graphfrom left to right, according to their human assess-ment results.
The distributions of these values showa strong similarity between the two datasets.
First,most of the RC and LC ratios are within an observ-able range, i.e., 0.25?0.35 for the former and 0.40?0.50 for the latter, except a particularly low LC for1063MT 11 Chine scrambled research on 16 key technical2 These techniques are from within headline everyones boosting science and technology and achiev-ing goals and contend of delivered on time bound through achieving breakthroughs in essentialtechnology and complimentarity resources .
nationalBLEU: 0.224 (1-gram:7, 2-gram:0, 3-gram:2, 4-gram:1)LC: 0.107 (number of lexical cohesion devices: 5)Human assessment: 2.67MT 21 China is accelerating research 16 main technologies2 These technologies are within the important realm to promote sciences and technology andachieve national goals and must be completed in a timely manner through achieving main dis-coveries in technology and integration of resources .BLEU: 0.213 (1-gram:5, 2-gram:3, 3-gram:2, 4-gram:1)LC: 0.231 (number of lexical cohesion devices: 9)Human assessment: 4.33Reference1 China Accelerates Research on 16 Main Technologies2 These technologies represent a significant part in the development of science and technology andthe achievement of national goals.
They must be accomplished within a fixed period of time byrealizing breakthroughs in essential technologies and integration of resources.Table 3: An example of MT outputs of different quality (underlined: matched n-grams; italic: lexical cohesion devices)one MT system.
Second, the ratios in those differ-ent HT versions are very stable in comparison withthose of MT.
Especially, all four HT versions in theMetricsMATR dataset share the sameRC ratio 0.31.This shows a typical level of the use of lexical cohe-sion device.
Third, the ratios in MT are lower than orat most equal to those in HT, suggesting their corre-lation with translation quality: the closer their RCand LC ratios to those in HT, the better the MT.These results verify our assumption that lexical co-hesion can serve as an effective proxy of the level oftranslation quality.4 MT Evaluation at Document LevelAs a feature at the discourse level, lexical cohesionis a good complement to current evaluation met-rics focusing on features at the sentence level.
Ta-ble 3 illustrates an example selected from the Met-ricsMATR dataset, consisting two versions of MToutput for a short document of two segments only.The n-grams matched with the reference are under-lined, while the lexical cohesion devices are itali-cized.
The two MT outputs have a similar num-ber of matched n-grams and hence receive similarBLEU scores.
These scores, however, do not reflecttheir real difference in quality: the second version isbetter, according to human assessment of adequacy.Instead, their LC ratios seem to represent such avariation more accurately.
The theme of the secondoutput is also highlighted through the lexical chains,including main/important, technology/technologiesand achieve/achieving, which create a tight texturebetween the two sentences, a crucial factor of textquality.To perform MT evaluation at the document level,the LC and RC ratios can be used alone or in-tegrated into a sentence-level metric.
The formerway has an advantage that it does not have to relyon any reference translation.
LC mainly requiresa thesaurus for computing semantic relation, whileRC only needs a morphological processor such asstemmer, both of which are available for most lan-1064guages.
Its drawback, however, lies in the risk ofrelying on a single discourse feature.
Although lex-ical cohesion gives a strong indication of text co-herence, it is not indispensable, because a text canbe coherent without any surface cohesive clue.
Fur-thermore, the quality of a document is also reflectedin that of its sentences.
A coherent translation maybe mistranslated, and on the other hand, a text con-taining lots of sentence-level errors would make itdifficult to determine its document-level quality.
Aprevious study comparing MT evaluation at the sen-tence versus document level (Wong et al 2011) re-ports a poor consistency in the evaluation results atthese two levels when the sentence-level scores ofMT output are low.
In regard of these, how to inte-grate these two levels of MT evaluation is particu-larly worth studying.5 ExperimentsWe examine, through experiments, the effectivenessof using LC and RC ratios alone and integratingthem into other evaluation metrics for MT evalua-tion at the document and system levels.
Three evalu-ation metrics, namely BLEU, TER and METEOR,2are selected for testing.
They represent three dis-tinctive types of evaluation metrics: n-gram, edit-distance, and unigram with external language re-sources, respectively.
These metrics are evaluated interms of their correlation with human assessments,using Pearson?s r correlation coefficient.
The Met-ricsMATR and MTC4 datasets and their adequacyassessments are used as evaluation data.
Note thatthe adequacy assessment is in fact an evaluationmethod for the sentence level.
We have to rely onan assumption that this evaluation data may emulatedocument-level quality, since its MT outputs wereassessed sentence by sentence in sequence as in adocument.
All experiments are performed under asetting of multiple reference translations.The integration of the two ratios into an evaluationmetric follows a simple weighted average approach.A hybrid metric H is formulated asH = ?
mdoc + (1?
?)
msegwhere mdoc refers to the document-level feature in2METEOR 1.0 with default parameters optimized over theadequacy assessments.use (i.e., LC or RC), mseg to a sentence-level met-ric, and ?
to a weight controlling their proportion.The MetricsMATR dataset is used as training data tooptimize the values of ?
for different metrics, whilethe MTC4 is used as evaluation data.
Table 4 showsthe optimized weights for the metrics for evaluationat the document level.Metrics RC LCBLEU 0.28 0.29TER 0.40 0.38METEOR 0.19 0.18Table 4: Optimized weights for the integration of dis-course feature into sentence-level metricsTable 5 presents the correlation rates of evalua-tion metrics obtained in our experiments under dif-ferent settings, with their 95% conference intervals(CI) provided.
The LC and RC ratios are found tohave strong correlations with human assessments atthe system level even when used alone, highly com-parable to BLEU and TER.
At the document level,however, they are not as good as the others.
Theyshow their advantages when integrated into othermetrics, especially BLEU and TER.
LC raises thecorrelation of BLEU from 0.447 to 0.472 and from0.861 to 0.905 at the document and system levels,respectively.
It improves TER even more signifi-cantly, in that the correlation rates are boosted upfrom -0.326 to -0.390 at the document level, andeven from -0.601 to -0.763 at the system level.
Sincethere are only six systems in the MTC4 data, such adramatic change may not be as meaningful as thesmooth improvement at the document level.
ME-TEOR is a special case in this experiment.
Its corre-lation cannot be improved by integrating LC orRC,and is even slightly dropped at the document level.The cause for this is yet to be identified.
Neverthe-less, these results confirm the close relationship ofan MT system?s capability to appropriately generatelexical cohesion devices with the quality of its out-put.Table 6 presents the Pearson correlations betweenevaluation results at the document level using dif-ferent evaluation metrics in the MTC4 data.
It il-lustrates the homogeneity/heterogeneity of differentmetrics and helps explain the performance change1065Document SystemMetrics Correlation 95% CI Correlation 95% CIRC 0.243 (0.167, 0.316) 0.873 (0.211, 0.985)LC 0.267 (0.192, 0.339) 0.818 (0.020, 0.979)BLEU 0.447 (0.381, 0.508) 0.861 (0.165, 0.984)BLEU+RC 0.463 (0.398, 0.523) 0.890 (0.283, 0.987)BLEU+LC 0.472 (0.408, 0.531) 0.905 (0.352, 0.989)TER -0.326 (-0.253, -0.395) -0.601 (-0.411, -0.949)TER+RC -0.370 (-0.299, -0.437) -0.740 (-0.179, -0.969)TER+LC -0.390 (-0.320, -0.455) -0.763 (-0.127, -0.972)METEOR 0.557 (0.500, 0.609) 0.961 (0.679, 0.995)METEOR+RC 0.555 (0.498, 0.608) 0.960 (0.672, 0.995)METEOR+LC 0.556 (0.499, 0.609) 0.962 (0.687, 0.995)Table 5: Correlation of different metrics with adequacy assessment in MTC4 dataBLEU 1TER -0.699 1METEOR 0.834 -0.510 1RC 0.287 -0.204 0.405 1LC 0.263 -0.097 0.437 0.736 1BLEU TER METEOR RC LCTable 6: Correlation between the evaluation results of different metricsby combining sentence- and document-level met-rics.
The table shows that the two ratios LC andRC highly correlate with each other, as if they aretwo variants of quantifying lexical cohesion devices.The three sentence-level metrics, BLEU, TER andMETEOR, also show strong correlations with eachother, especially between BLEU and METEOR.
Thecorrelations are generally weaker between sentence-and document-level metrics, for instance, 0.263 be-tween BLEU and LC and only -0.097 between TERand LC, showing that they are quite heterogeneousin nature.
This accounts for the significant perfor-mance gain from their combination: their differenceallows them to complement each other.
It is alsoworth noting that between METEOR and LC thecorrelation of 0.437 is mildly strong, explaining thenegative result of their integration.
On the one hand,lexical cohesion is word choice oriented, which isonly sensitive to the reiteration and semantic relat-edness of words in MT output.
On the other hand,METEOR is strong in unigram matching, with mul-tiple strategies to maximize the matching rate be-tween MT output and reference translation.
In thissense they are homogeneous to a certain extent, ex-plaining the null effect of their combination.6 Discussion and ConclusionIn this study we have attempted to address the prob-lem that most existing MT evaluation metrics dis-regard the connectivity of sentences in a document.By focusing on a typical type of cohesion, i.e., lexi-cal cohesion, we have shown that its use frequency isa significant factor to differentiate HT from MT andMT outputs of different quality from each other.
Thehigh correlation rate of its use with translation ade-quacy also suggests that the more lexical cohesiondevices in use, the better the quality of MT output.Accordingly we have used two ratios, LC and RC,to capture such correlativity.
Our experimental re-sults have confirmed the effectiveness of this featurein accounting for the document-level quality of MToutput.
The performance of two evaluation metrics,BLEU and TER, is highly improved through incor-porating this document-level feature, in terms of the1066change of their correlation with human assessments.This finding is positive and sheds light on a regionof MT research that is still severely under-explored.Our approach to extending the granularity of MTevaluation from sentence to document through lex-ical cohesion is highly applicable to different lan-guages.
It has a relatively weak demand for lan-guage resource in comparison with the processing ofother discourse features like grammatical cohesion.It is also much unaffected by grammatical problemsor errors commonly seen in natural languages and,in particular, MT outputs.Our future work will continue to explore the re-lationship of lexical cohesion to translation quality,so as to identify, apart from its use frequency, othersignificant aspects for MT evaluation at the docu-ment level.
A frequent use of cohesion devices ina text is not necessarily appropriate, because an ex-cess of them may decrease the quality and readabil-ity of a text.
Human writers can strategically changethe ways of expression to achieve appropriate coher-ence and also avoid overuse of the same lexical item.To a certain extent, this is one of the causes for theunnaturalness of MT output: it may contain a largenumber of lexical cohesion devices which are sim-ply direct translation of those in a source text thatdo not fit in the target context.
How to use lexicalcohesion devices appropriately instead of frequentlyis thus an important issue to tackle before we canadopt them in MT and MT evaluation by a suitablemeans.AcknowledgmentsThe research described in this paper was substan-tially supported by the Research Grants Council(RGC) of Hong Kong SAR, P. R. China throughGRF grant 144410.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, Michigan.Regina Barzilay and Michael Elhadad.
1997.
Using lex-ical chains for text summarization.
In Proceedings ofthe ACL Workshop on Intelligent Scalable Text Sum-marization, pages 10?17.Jill Burstein.
2003.
The E-rater scoring engine: Auto-mated essay scoring with natural language processing.In Mark D. Shermis and Jill Burstein, editors, Auto-mated Essay Scoring: A Cross-Disciplinary Perspec-tive, chapter 7, pages 113?122.
Lawrence Erlbaum As-sociates.Marine Carpuat.
2009.
One translation per discourse.In Proceedings of the NAACL HLT Workshop on Se-mantic Evaluations: Recent Achievements and FutureDirections, pages 19?27, Boulder, Colorado.Bruno Cartoni, Andrea Gesmundo, James Henderson,Cristina Grisot, Paola Merlo, Thomas Meyer, JacquesMoeschler, Sandrine Zufferey, and Andrei Popescu-Belis.
2011.
Improving MT coherence through text-level processing of input texts: The COMTIS project.In Tralogy, Paris.Samuel W. K. Chan.
2004.
Extraction of sailent tex-tual patterns: Synergy between lexical cohesion andcontextual coherence.
IEEE Transactions on Systems,Man and Cybernetics, Part A: Systems and Humans,34(2):205?218.Elisabet Comelles, Jesus Gime?nez, Llu?`s Ma?rquez, IreneCastello`n, and Victoria Arranz.
2010.
Document-level automatic MT evaluation based on discourse rep-resentations.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metrics-MATR, pages 333?338, Uppsala.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011.Cache-based document-level statistical machine trans-lation.
In EMNLP 2011, pages 909?919, Edinburgh,Scotland.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
London: Longman.Masaki Itagaki, Takako Aikawa, and Xiaodong He.2007.
Automatic validation of terminology translationconsistency with statistical method.
In MT Summit XI,pages 269?274.Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic: An Introduction to Modeltheoretic Semantics ofNatural Language, Formal Logic and Discourse Rep-resentation Theory.
Dordrecht: Kluwer.Margaret King, Andrei Popescu-Belis, and Eduard Hovy.2003.
FEMTI: Creating and using a framework forMT evaluation.
In MT Summit IX, pages 224?231,New Orleans.Jing Li, Le Sun, Chunyu Kit, and Jonathan Webster.2007.
A query-focused multi-document summarizerbased on lexical chains.
In DUC 2007, Rochester,New York.1067Kazem Lotfipour-Saedi.
1997.
Lexical cohesion andtranslation equivalence.
Meta, 42(1):185?192.Xiaoyi Ma.
2006.
Multiple-Translation Chinese (MTC)part 4.
Linguistic Data Consortium.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Keith J. Miller and Michelle Vanni.
2001.
Scalingthe ISLE taxonomy: Development of metrics for themulti-dimensional characterisation of machine trans-lation quality.
In MT Summit VIII, pages 229?238.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1):21?48.Masaki Murata and Makoto Nagao.
1993.
Determina-tion of referential property and number of nouns inJapanese sentences for machine translation into En-glish.
In TMI 1993, pages 218?225, Kyoto.Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hi-toshi Isahara.
2001.
A machine-learning approach toestimating the referential properties of Japanese nounphrases.
In CICLING 2001, pages 142?153, Mexico-City.Hiromi Nakaiwa and Satoru Ikehara.
1992.
Zero pro-noun resolution in a machine translation system by us-ing Japanese to English verbal semantic attributes.
InANLP 1992, pages 201?208.Hiromi Nakaiwa and Satoshi Shirai.
1996.
Anaphoraresolution of Japanese zero pronouns with deictic ref-erence.
In COLING 1996, pages 812?817, Copen-hagen.Hiromi Nakaiwa, Satoshi Shirai, Satoru Ikehara, andTsukasa Kawaok.
1995.
Extrasentential resolutionof Japanese zero pronouns using semantic and prag-matic constraints.
In Proceedings of the AAAI 1995Spring Symposium Series: Empirical Methods in Dis-course Interpretation and Generation, pages 99?105,Stanford.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In ACL 2002, pages311?318.Jesu?s Peral, Manuel Palomar, and Antonio Ferra`ndez.1999.
Coreference-oriented interlingual slot structureand machine translation.
In Proceedings of the ACLWorkshop on Coreference and its Applications, pages69?76, College Park, MD.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Mark Przybocki, Kay Peterson, and Se?bastien Bronsart.2009.
2008 NIST metrics for machine translation(MetricsMATR08) development data.
Linguistic DataConsortium.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In AMTA 2006, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments with atunable MT metric.
In Proceedings of the 4th Work-shop on Statistical Machine Translation, pages 259?268, Athens.Georges van Slype.
1979.
Critical Study of Methods forEvaluating the Quality of Machine Translation.
Tech-nical report, Bureau Marcel van Dijk / European Com-mission, Brussels.Muriel Vasconcellos.
1989.
Cohesion and coherence inthe presentation of machine translation products.
InJames E. Alatis, editor, Georgetown University RoundTable on Languages and Linguistics 1989: LanguageTeaching, Testing, and Technology: Lessons from thePast with a View Toward the Future, pages 89?105.Georgetown University Press.Eric M. Visser and Masaru Fuji.
1996.
Using sentenceconnectors for evaluating MT output.
In COLING1996, pages 1066?1069.Yorick Wilks.
1978.
The Value of the MonolingualComponent in MT Evaluation and its Role in the Bat-telle.
Report on Systran, Luxembourg CEC Memoran-dum.Billy T. M. Wong, Cecilia F. K. Pun, Chunyu Kit, andJonathan J. Webster.
2011.
Lexical cohesion for eval-uation of machine translation at document level.
InNLP-KE 2011, pages 238?242, Tokushima.Billy Tak-Ming Wong.
2010.
Semantic evaluation of ma-chine translation.
In LREC 2010, pages 2884?2888,Valletta.Zhibiao Wu and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In ACL 1994, pages 133?138,Las Cruces.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level consistency verification in ma-chine translation.
In MT summit XIII, pages 131?138,Xiamen.1068
