Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 740?749,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsHistorical Analysis of Legal Opinionswith a Sparse Mixed-Effects Latent Variable ModelWilliam Yang Wang1 and Elijah Mayfield1 and Suresh Naidu2 and Jeremiah Dittmar31School of Computer Science, Carnegie Mellon University2Department of Economics and SIPA, Columbia University3American University and School of Social Science, Institute for Advanced Study{ww,elijah}@cmu.edu sn2430@columbia.edu dittmar@american.eduAbstractWe propose a latent variable model to enhancehistorical analysis of large corpora.
This workextends prior work in topic modelling by in-corporating metadata, and the interactions be-tween the components in metadata, in a gen-eral way.
To test this, we collect a corpusof slavery-related United States property lawjudgements sampled from the years 1730 to1866.
We study the language use in theselegal cases, with a special focus on shifts inopinions on controversial topics across differ-ent regions.
Because this is a longitudinaldata set, we are also interested in understand-ing how these opinions change over the courseof decades.
We show that the joint learningscheme of our sparse mixed-effects model im-proves on other state-of-the-art generative anddiscriminative models on the region and timeperiod identification tasks.
Experiments showthat our sparse mixed-effects model is moreaccurate quantitatively and qualitatively inter-esting, and that these improvements are robustacross different parameter settings.1 IntroductionMany scientific subjects, such as psychology, learn-ing sciences, and biology, have adopted computa-tional approaches to discover latent patterns in largescale datasets (Chen and Lombardi, 2010; Baker andYacef, 2009).
In contrast, the primary methods forhistorical research still rely on individual judgementand reading primary and secondary sources, whichare time consuming and expensive.
Furthermore,traditional human-based methods might have goodprecision when searching for relevant information,but suffer from low recall.
Even when languagetechnologies have been applied to historical prob-lems, their focus has often been on information re-trieval (Gotscharek et al, 2009), to improve acces-sibility of texts.
Empirical methods for analysis andinterpretation of these texts is therefore a burgeoningnew field.Court opinions form one of the most importantparts of the legal domain, and can serve as an excel-lent resource to understand both legal and politicalhistory (Popkin, 2007).
Historians often use courtopinions as a primary source for constructing in-terpretations of the past.
They not only report theproceedings of a court, but also express a judges?views toward the issues at hand in a case, and reflectthe legal and political environment of the region andperiod.
Since there exists many thousands of earlycourt opinions, however, it is difficult for legal his-torians to manually analyze the documents case bycase.
Instead, historians often restrict themselves todiscussing a relatively small subset of legal opinionsthat are considered decisive.
While this approachhas merit, new technologies should allow extractionof patterns from large samples of opinions.Latent variable models, such as latent Dirichlet allocation (LDA) (Blei et al, 2003) and probabilisticlatent semantic analysis (PLSA) (Hofmann, 1999),have been used in the past to facilitate social scienceresearch.
However, they have numerous drawbacks,as many topics are uninterpretable, overwhelmed byuninformative words, or represent background lan-guage use that is unrelated to the dimensions of anal-ysis that qualitative researchers are interested in.SAGE (Eisenstein et al, 2011a), a recently pro-posed sparse additive generative model of language,addresses many of the drawbacks of LDA.
SAGEassumes a background distribution of language use,and enforces sparsity in individual topics.
Anotheradvantage, from a social science perspective, is thatSAGE can be derived from a standard logit random-utility model of judicial opinion writing, in contrastto LDA.
In this work we extend SAGE to the su-pervised case of joint region and time period pre-diction.
We formulate the resulting sparse mixed-effects (SME) model as being made up of mixedeffects that not only contain random effects fromsparse topics, but also mixed effects from availablemetadata.
To do this we augment SAGE with twosparse latent variables that model the region andtime of a document, as well as a third sparse latent740variable that captures the interactions among the re-gion, time and topic latent variables.
We also intro-duce a multiclass perceptron-style weight estimationmethod to model the contributions from differentsparse latent variables to the word posterior prob-abilities in this predictive task.
Importantly, the re-sulting distributions are still sparse and can thereforebe qualitatively analyzed by experts with relativelylittle noise.In the next two sections, we overview work re-lated to qualitative social science analysis using la-tent variable models, and introduce our slavery-related early United States court opinion data.
Wedescribe our sparse mixed-effects model for jointmodeling of region, time, and topic in section 4.Experiments are presented in section 5, with a ro-bust analysis from qualitative and quantitative stand-points in section 5.2, and we discuss the conclusionsof this work in section 6.2 Related WorkNatural Language Processing (NLP) methods forautomatically understanding and identifying keyinformation in historical data have not yet beenexplored until recently.
Related research effortsinclude using the LDA model for topic model-ing in historical newspapers (Yang et al, 2011),a rule-based approach to extract verbs in histor-ical Swedish texts (Pettersson and Nivre, 2011),a system for semantic tagging of historical Dutcharchives (Cybulska and Vossen, 2011).Despite our historical data domain, our approachis more relevant to text classification and topic mod-elling.
Traditional discriminative methods, such assupport vector machine (SVM) and logistic regres-sion, have been very popular in various text cate-gorization tasks (Joachims, 1998; Wang and McKe-own, 2010) in the past decades.
However, the mainproblem with these methods is that although they areaccurate in classifying documents, they do not aimat helping us to understand the documents.Another problem is lack of expressiveness.
Forexample, SVM does not have latent variables tomodel the subtle differences and interactions of fea-tures from different domains (e.g.
text, links, anddate), but rather treats them as a ?bag-of-features?.Generative methods, by contrast, can show thecauses to effects, have attracted attentions in re-cent years due to the rich expressiveness of themodels and competitive performances in predictivetasks (Wang et al, 2011).
For example, Nguyen etal.
(2010) study the effect of the context of inter-action in blogs using a standard LDA model.
Guoand Diab (2011) show the effectiveness of using se-mantic information in multifaceted topic models fortext categorization.
Eisenstein et al (2010) use alatent variable model to predict geolocation infor-mation of Twitter users, and investigate geographicvariations of language use.
Temporally, topic mod-els have been used to show the shift in language useover time in online communities (Nguyen and Rose?,2011) and the evolution of topics over time (Shub-hankar et al, 2011).When evaluating understandability, however,dense word distributions are a serious issue in manytopic models as well as other predictive tasks.
Suchtopic models are often dominated by function wordsand do not always effectively separate topics.
Re-cent work have shown significant gains in both pre-dictiveness and interpretatibility by enforcing spar-sity, such as in the task of discovering sociolinguisticpatterns of language use (Eisenstein et al, 2011b).Our proposed sparse mixed-effects model bal-ances the pros and cons the above methods, aim-ing at higher classification accuracies using the SMEmodel for joint geographic and temporal aspects pre-diction, as well as richer interaction of componentsfrom metadata to enhance historical analysis in legalopinions.
To the best of our knowledge, this study isthe first of its kind to discover region and time spe-cific topical patterns jointly in historical texts.3 DataWe have collected a corpus of slavery-related UnitedStates supreme court legal opinions from LexisNexis.
The dataset includes 5,240 slavery-relatedstate supreme court cases from 24 states, during theperiod of 1730 - 1866.
Optical character recognition(OCR) software was used by Lexis Nexis to digitizethe original documents.
In our region identificationtask, we wish to identify whether an opinion waswritten in a free state1 (R1) or a slave state (R2)2.In our time identification experiment, we approx-imately divide the legal documents into four timequartiles (Q1, Q2, Q3, and Q4), and predict whichquartile the testing document belongs to.
Q1 con-tains cases from 1837 or earlier, where as Q2 is for1838-1848, Q3 is for 1849-1855, and Q4 is for 1856and later.4 The Sparse Mixed-Effects ModelTo address the over-parameterization, lack of ex-pressiveness and robustness issues in LDA, theSAGE (Eisenstein et al, 2011a) framework draws a1Including border states, this set includes CT, DE, IL, KY,MA, MD, ME, MI, NH, NJ, NY, OH, PA, and RI.2These states include AR, AL, FL, GA, MS, NC, TN, TX,and VA.741Figure 1: Plate diagram representation of the proposedSparse Mixed-Effects model with K topics, Q time peri-ods, and R regions.constant background distribution m, and additivelymodels the sparse deviation ?
from the backgroundin log-frequency space.
It also incorporates latentvariables ?
to model the variance for each sparse de-viation ?.
By enforcing sparsity, the model might beless likely to overfit the training data, and requiresestimation of fewer parameters.This paper further extends SAGE to analyze mul-tiple facets of a document collection, such as theregional and temporal differences.
Figure 1 showsthe graphical model of our proposed sparse mixed-effects (SME) model.
In this SME model, we stillhave the same Dirichlet ?, the latent topic proportion?, and the latent topic variable z as the original LDAmodel.
For each document d, we are able to ob-serve two labels: the region label y(R)d and the timequartile label y(Q)d .
We also have a background dis-tributionm that is drawn from a uninformative prior.The three major sparse deviation latent variables are?
(T )k for topics, ?
(R)j for regions, and ?
(Q)q for timeperiods.
All of the three latent variables are condi-tioned on another three latent variables, which aretheir corresponding variances ?
(T )k , ?
(R)j and ?
(Q)q .In the intersection of the plates for topics, regions,and time quartiles, we include another sparse latentvariable ?
(I)qjk, which is conditioned on a variance?
(I)qjk, to model the interactions among topic, regionand time.
?
(I)qjk is the linear combination of time pe-riod, region and topic sparse latent variables, whichabsorbs the residual variation that is not captured inthe individual effects.In contrast to traditional multinomial distributionof words in LDA models, we approximate the con-ditional word distribution in the document d as theexponentiated sum ?
of all latent sparse deviations?
(T )k , ?
(R)j , ?
(Q)q , and ?
(I)qjk, as well as the backgroundm:P (w(d)n |z(d)n , ?,m, y(R)d , y(Q)d ) ?
?=exp(m+ ?
(T )z(d)n+ ?(R)?
(R)y(r)+ ?(Q)?
(Q)y(q) + ?
(I)y(r),y(q),z(d)n)Despite SME learns in a Bayesian framework, theabove ?
(R) and ?
(Q) are dynamic parameters thatweight the contributions of ?
(R)y(r)and ?
(Q)y(q)to theapproximated word posterior probability.
A zero-mean Laplace prior ?
, which is conditioned on pa-rameter ?, is introduced to induce sparsity, whereits distribution is equivalent to the joint distribution,?N (?
;m, ?)?(?
;?)d?
, and ?(?
;?)d?
is the Expo-nential distribution (Lange and Sinsheimer, 1993).We first describe a generative story for this SMEmodel:?
Draw a background m from corpus mean and ini-tialize ?
(T ), ?
(R), ?
(Q) and ?
(I) sparse deviationsfrom corpus?
For each topic k?
For each word i?
Draw ?
(T )k,i ?
?(?)?
Draw ?
(T )k,i ?
N (0, ?
(T )k,i )?
Set ?k ?
exp(m+?k+?(R)?(R)+?(Q)?(Q)+?(I))?
For each region j?
For each word i?
Draw ?
(R)j,i ?
?(?)?
Draw ?
(R)j,i ?
N (0, ?
(R)j,i )?
Update ?j ?
exp(m + ?
(R)?j + ?
(T ) +?(Q)?
(Q) + ?(I))?
For each time quartile q?
For each word i?
Draw ?
(Q)q,i ?
?(?)?
Draw ?
(Q)q,i ?
N (0, ?
(Q)q,i )?
Update ?q ?
exp(m + ?
(Q)?q + ?
(T ) +?(R)?
(R) + ?(I))?
For each time quartile q, for each region j, for eachtopic k?
For each word i?
Draw ?
(I)q,j,k,i ?
?(?)?
Draw ?
(I)q,j,k,i ?
N (0, ?(I)q,j,k,i)?
Update ?q,j,k ?
exp(m + ?q,j,k + ?
(T ) +?(R)?
(R) + ?(Q)?(Q))742?
For each document d?
Draw the region label y(R)d?
Draw the time quartile label y(Q)d?
For each word n, draw w(d)n ?
?yd4.1 Parameter EstimationWe follow the MAP estimation method that Eisen-stein et al (2011a) used to train all sparse latent vari-ables ?, and perform Bayesian inference on other la-tent variables.
The estimation of all variance vari-ables ?
remains as plugging the compound distri-bution of Normal-Jeffrey?s prior, where the latter isa replacement of the Exponential prior.
When per-forming Expectation-Maximization (EM) algorithmto infer the latent variables in SME, we derive thefollowing likelihood function:L =?d?logP (?d|?
)?+?logP (Z(d)n |?d)?+Nd?n?logP (w(d)n |z(d)n , ?,m, y(R)d , y(Q)d )?+?k?logP (?
(T )k |0, ?
(T )k )?+?k?logP (?
(T )k |?
)?+?j?logP (?
(R)j |0, ?
(R)j )?+?j?logP (?
(R)j |?
)?+?q?logP (?
(Q)q |0, ?Q)q )?+?q?logP (?
(Q)q |?
)?+?q?j?k?logP (?
(I)q,j,k|0, ?
(I)q,j,k)?+?q?j?k?logP (?
(I)q,j,k|?)??
?logQ(?, z, ?
)?The above E step likelihood score can be intuitivelyinterpreted as the sum of topic proportion scores, la-tent topic scores, the word scores, the ?
scores withtheir priors, and minus the joint variance.
In the Mstep, when we use Newton?s method to optimize thesparse deviation ?k parameter, we need to modifythe original likelihood function in SAGE and its cor-responding first and second order derivatives whenderiving the gradient and Hessian matrix.
The like-lihood function for sparse topic deviation ?k is:L(?k) = ?c(T )k ?T?k?
Cd log?q?j?iexp(?
(Q)?qi + ?
(R)?ji+ ?ki + ?qjki +mi)?
?kTdiag(?(?
(T )k )?1?)?
(T )k /2and we can derive the gradient when taking the firstorder partial derivative:?L??
(T )k=?c(T )k ?
??q?j?Cqjk??qjk?
diag(?(?
(T )k )?1?)?
(T )kwhere c(T )k is the true count, and ?qjk is the logword likelihood in the original likelihood function.Cqjk is the expected count from combinations oftime, region and topic.?q?j?Cqjk?
?qjk will thenbe taken the second order derivative to form the Hes-sian matrix, instead of ?Ck?
?k in the previous SAGEsetting.To learn the weight parameters ?
(R) and ?
(Q),we can approximate the weights using a multiclassperceptron-style (Collins, 2002) learning method.
Ifwe say that the notation of?V (R?)
is to marginalizeout all other variables in ?
except ?
(R), and P (y(R)d )is the prior for the region prediction task, we can pre-dict the expected region value y?
(R)d of a document d:y?
(R)d ?
argmaxy?
(R)dexp(?V (R?)
log ?
+ logP (y(R)d ))=argmaxy?
(R)d(exp(?V (R?
)(m+ ?
(T )z(d)n+ ?(R)?
(R)y(R)d+ ?(Q)?
(Q)y(Q)d+ ?
(I)y(R)d ,y(Q)d ,z(d)n))P (y(R)d ))If the symbol ?
is the hyperprior for the learningrate and y?
(R)d is the true label, the update procedurefor the weights becomes:?(R?
)d = ?
(R)d + ?(y?
(R)d ?
y?
(R)d )Similarly, we derive the ?
(Q) parameter using theabove formula.
It is necessary to normalize theweights in each EM loop to preserve the sparsityproperty of latent variables.
The weight update of?
(R) and ?
(Q) is bound by the averaged accuracyof the two classification tasks in the training data,which is similar to the notion of minimizing empiri-cal risk (Bahl et al, 1988).
Our goal is to choose thetwo weight parameters that minimize the empiricalclassification error rate on training data when learn-ing the word posterior probability.5 Prediction ExperimentsWe perform three quantitative experiments to evalu-ate the predictive power of the sparse mixed-effectsmodel.
In these experiments, to predict the regionand time period labels of a given document, we743jointly learn the two labels in the SME model, andchoose the pair which maximizes the probability ofthe document.In the first experiment, we compare the predictionaccuracy of our SME model to a widely used dis-criminative learner in NLP ?
the linear kernel sup-port vector machine (SVM)3.
In the second experi-ment, in addition to the linear kernel SVM, we alsocompare our SME model to a state-of-the-art sparsegenerative model of text (Eisenstein et al, 2011a),and vary the size of input vocabulary W exponen-tially from 29 to the full size of our training vocab-ulary4.
In the third experiment, we examine the ro-bustness of our model by examining how the numberof topics influences the prediction accuracy whenvarying the K from 10 to 50.Our data consists of 4615 training documents and625 held-out documents for testing.
While individ-ual judges wrote multiple opinions in our corpus,no judges overlapped between training and test sets.When measuring by the majority class in the testingcondition, the chance baseline for the region iden-tification task is 57.1% and the time identificationtask is 32.3%.
We use three-fold cross-validation toinfer the learning rate ?
and cost C hyperpriors inthe SME and SVM model respectively.
We use thepaired student t-test to measure the statistical signif-icance.5.1 Quantitative Results5.1.1 Comparing SME to SVMWe show in this section the predictive power ofour sparse mixed-effects model, comparing to a lin-ear kernel SVM learner.
To compare the two mod-els in different settings, we first empirically set thenumber of topics K in our SME model to be 25, asthis setting was shown to yield a promising result ina previous study (Eisenstein et al, 2011a) on sparsetopic models.
In terms of the size of vocabulary Wfor both the SME and SVM learner, we select threevalues to represent dense, medium or sparse featurespaces: W1 = 29, W2 = 212, and the full vocabu-lary size of W3 = 213.8.
Table 1 shows the accuracyof both models, as well as the relative improvement(gain) of SME over SVM.When looking at the experiment results under dif-ferent settings, we see that the SME model alwaysoutperforms the SVM learner.
In the time quar-tile prediction task, the advantage of SME model3In our implementation, we use LibSVM (Chang and Lin,2011).4To select the vocabulary size W , we rank the vocabularyby word frequencies in a descending order, and pick the top-Wwords.Method Time Gain Region GainSVM (W1) 33.2% ?
69.7% ?SME (W1) 36.4% 9.6% 71.4% 2.4%SVM (W2) 35.8% ?
72.3% ?SME (W2) 40.9% 14.2% 74.0% 2.4%SVM (W3) 36.1% ?
73.5% ?SME (W3) 41.9% 16.1% 74.8% 1.8%Table 1: Compare the accuracy of the linear kernel sup-port vector machine to our sparse mixed-effects model inthe region and time identification tasks (K = 25).
Gain:the relative improvement of SME over SVM.is more salient.
For example, with a medium den-sity feature space of 212, SVM obtained an accuracyof 35.8%, but SME achieved an accuracy of 40.9%,which is a 14.2% relative improvement (p < 0.001)over SVM.
When the feature space becomes sparser,the SME obtains an increased relative improvement(p < 0.001) of 16.1%, using full size of vocabu-lary.
The performance of SVM in the binary regionclassification is stronger than in the previous task,but SME is able to outperform SVM in all three set-tings, with tightened advantages (p < 0.05 in W2and p < 0.001 in W3).
We hypothesize that it mightbecause that SVM, as a strong large margin learner,is a more natural approach in a binary classificationsetting, but might not be the best choice in a four-way or multiclass classification task.5.1.2 Comparing SME to SAGEIn this experiment, we compare SME with a state-of-the-art sparse generative model: SAGE (Eisen-stein et al, 2011a).Most studies on topic modelling have not beenable to report results when using different sizes ofvocabulary for training.
Because of the importanceof interpretability for social science research, thechoice of vocabulary size is critical to ensure un-derstandable topics.
Thus we report our results atvarious vocabulary sizes W on SME and SAGE.
Tobetter validate the performance of SME, we also in-clude the performance of SVM in this experiment,and fix the number of topics K = 10 for the SMEand SAGE models, which is a different value for thenumber of topicsK than the empiricalK we used inthe experiment of Section 5.1.1.
Figure 2 and Fig-ure 3 show the experiment results in both time andregion classification task.In Figure 2, we evaluate the impacts of W on ourtime quartile prediction task.
The advantage of theSME model is very obvious throughout the experi-ments.
Interestingly, when we continue to increase744Figure 2: Accuracy on predicting the time quartile vary-ing the vocabulary size W , while K is fixed to 10.Figure 3: Accuracy on predicting the region varying thevocabulary size W , while K is fixed to 10.the vocabulary size W exponentially and make thefeature space more sparse, SME obtains its best re-sult at W = 213, where the relative improvementover SAGE and SVM is 16.8% and 22.9% respec-tively (p < 0.001 under all comparisons).Figure 3 shows the impacts of W on the accu-racy of SAGE and SME in the region identificationtask.
In this experiment, the results of SME modelare in line with SAGE and SVM when the featurespace is dense.
However, when W reaches the fullvocabulary size, we have observed significantly bet-ter results (p < 0.001 in the comparison to SAGEand p < 0.05 with SVM).
We hypothesize that theremight be two reasons: first, the K parameter is setto 10 in this experiment, which is much denser thanthe experiment setting in Section 5.1.1.
Under thiscondition, the sparse topic advantage of SME mightbe less salient.
Secondly, in the two tasks, it is ob-served that the accuracy of the binary region classi-fication task is much higher than the four-way task,thus while the latter benefits significantly from thisjoint learning scheme of the SME model, but the for-mer might not have the equivalent gain5.5We hypothesize that this problem might be eliminated if5.1.3 Influence of the number of topics KFigure 4: Accuracy on predicting the time quartile vary-ing the number of topics K, while W is fixed to 29.Figure 5: Accuracy on predicting the region varying thenumber of topics K, while W is fixed to 29.Unlike hierarchical Dirichlet processes (Teh et al,2006), in parametric Bayesian generative models,the number of topics K is often set manually, andcan influence the model?s accuracy significantly.
Inthis experiment, we fix the input vocabulary W to29, and compare the mixed-effect model with SAGEin both region and time identification tasks.Figure 4 shows how the variations of K can in-fluence the system performance in the time quartileprediction task.
We can see that the sparse mixed-effects model (SME) reaches its best performancewhen the K is 40.
After increasing the number oftopics K, we can see SAGE consistently increaseits accuracy, obtaining its best result when K = 30.When comparing these two models, SME?s best per-formance outperforms SAGE?s with an absolute im-provement of 3%, which equals to a relative im-provement (p < 0.001) of 8.4%.
Figure 5 demon-strates the impacts of K on the predictive power ofSME and SAGE in the region identification task.the two tasks in SME have similar difficulties and accuracies,but this needs to be verified in future work.745Keywords discovered by the SME modelPrior to 1837 (Q1) pauperis, footprints, American Colonization Society, manumissions, 17971838 - 1848 (Q2) indentured, borrowers, orphan?s, 1841, vendee?s, drawer?s, copartners1849 - 1855 (Q3) Frankfort, negrotrader, 1851, Kentucky Assembly, marshaled, classedAfter 1856 (Q4) railroadco, statute, Alabama, steamboats, Waterman?s, mulattoes, man-trapFree Region (R1) apprenticed, overseer?s, Federal Army, manumitting, Illinois constitutionSlave Region (R2) Alabama, Clay?s Digest, oldest, cotton, reinstatement, sanction, plantation?sTopic 1 in Q1 R1 imported, comaker, runs, writ?s, remainderman?s, converters, runawayTopic 1 in Q1 R2 comaker, imported, deceitful, huston, send, bright, remainderman?sTopic 2 in Q1 R1 descendent, younger, administrator?s, documentary, agreeable, emancipatedTopic 2 in Q1 R2 younger, administrator?s, grandmother?s, plaintiffs, emancipated, learnedlyTopic 3 in Q2 R1 heir-at-law, reconsidered, manumissions, birthplace, mon, mother-in-lawTopic 3 in Q2 R2 heir-at-law, reconsideration, mon, confessions, birthplace, father-in-law?sTopic 4 in Q2 R1 indentured, apprenticed, deputy collector, stepfather?s, traded, seizesTopic 4 in Q2 R2 deputy collector, seizes, traded, hiring, stepfather?s, indentured, teachingTopic 5 in Q4 R1 constitutionality, constitutional, unconstitutionally, Federal Army, violatedTopic 5 in Q4 R2 petition, convictions, criminal court, murdered, constitutionality, man-trapTable 2: A partial listing of an example for early United States state supreme court opinion keywords generated fromthe time quartile ?
(Q) , region ?
(R) and topic-region-time ?
(I) interactive variables in the sparse mixed-effects model.Except that the two models tie up when K = 10,SME outperforms SAGE for all subsequent varia-tions ofK.
Similar to the region task, SME achievesthe best result when K is sparser (p < 0.01 whenK = 40 and K = 50).5.2 Qualitative AnalysisIn this section, we qualitatively evaluate the topicsgenerated vis-a-vis the secondary literature on thelegal and political history of slavery in the UnitedStates.
The effectiveness of SME could depend notjust on its predictive power, but also in its abilityto generate topics that will be useful to historiansof the period.
Supreme court opinions on slaveryare of significant interest for American political his-tory.
The conflict over slave property rights was atthe heart of the ?cold war?
(Wright, 2006) betweenNorth and South leading up to the U.S. Civil War.The historical importance of this conflict betweenNorthern and Southern legal institutions is one of themotivations for choosing our data domain.We conduct qualitative analyses on the top-rankedkeywords6 that are associated with different geo-graphical locations and different temporal frames,generated by our SME model.
In our analysis, for6Keywords were ranked by word posterior probabilities.each interaction of topic, region, and time period, alist of the most salient vocabulary words was gener-ated.
These words were then analyzed in the contextof existing historical literature on the shift in atti-tudes and views over time and across regions.
Table2 shows an example of relevant keywords and topics.This difference between Northern and Southernopinion can be seen in some of the topics generatedby the SME.
Topic 1 deals with transfers of humanbeings as slave property.
The keyword ?remainder-man?
designates a person who inherits or is entitledto inherit property upon the termination of an es-tate, typically after the death of a property owner,and appears in Northern and Southern cases.
How-ever, in Topic 1 ?runaway?
appears as a keyword indecisions from free states but not in decisions fromslave states.
The fact that ?runaway?
is not a topword in the same topic in the Southern legal opin-ions is consistent with a spatial (geolocational) di-vision in which the property claims of slave ownersover runaways were not heavily contested in South-ern courts.Topic 3 concerns bequests, as indicated by theterm ?heir-at-law?, but again the term ?manumis-sions?, ceases to show up in the slave states after thefirst time quartile, perhaps reflecting the hostility to746manumissions that southern courts exhibited as theconflict over slavery deepened.Topic 4 concerns indentures and apprentices.
In-terestingly, the terms indentures and apprenticeshipsare more prominent in the non-slave states, reflect-ing the fact that apprenticeships and indentures wereused in many border states as a substitute for slavery,and these were often governed by continued usage ofMaster and Servant law (Orren, 1992).Topic 5 shows the constitutional crisis in thestates.
In particular, the anti-slavery state courts areprone to use the term ?unconstitutional?
much moreoften than the slave states.
The word ?man-trap?, aterm used to refer to states where free blacks couldbe kidnapped purpose of enslaving them.
The fugi-tive slave conflicts of the mid-19th century that ledto the civil war were precisely about this aversionof the northern states to having to return runawayslaves to the Southern states.Besides these subjective observations about thehistorical significance of the SME topics, we alsoconduct a more formal analysis comparing the SMEclassification to that conducted by a legal histo-rian.
Wahl (2002) analyses and classifies by hand10989 slave cases in the US South into 6 categories:?Hires?, ?Sales?, ?Transfers?, ?Common Carrier?,?Black Rights?
and ?Other?.
An example of ?Hires?is Topic 4.
Topics 1, 2, and 3 concern ?Transfers?
ofslave property between inheritors, descendants andheirs-at-law.
Topic 5 would be classified as ?Other?.We take each of our 25 modelled topics and clas-sify them along Wahl?s categories, using ?Other?when a classification could not be obtained.
Theclassifications are quite transparent in virtually allcases, as certain words (such as ?employer?
or ?be-quest?)
clearly designate certain categories (respec-tively, such as ?Hires?
or ?Transfers?).
We then cal-culate the probability of each of Wahl?s categories inRegion 2.
We then compare these to the relative fre-quencies of Wahl?s categorization in the states thatoverlap with our Region 2 in Figure 6 and do a ?2test for goodness of fit, which allows us to reject dif-ference at 0.1% confidence.The SME model thus delivers topics that, at a firstpass, are consistent with the history of the periodas well as previous work by historians, showing thequalitative benefits of the model.
We plan to conductmore vertical and temporal analyses using SME inthe future.6 Conclusion and Future WorkIn this work, we propose a sparse mixed-effectsmodel for historical analysis of text.
This model isbuilt on the state-of-the-art in latent variable mod-Figure 6: Comparison with Wahl (2002) classification.elling and extends that model to a setting wheremetadata is available for analysis.
We jointly modelthose observed labels as well as unsupervised topicmodelling.
In our experiments, we have shown thatthe resulting model jointly predicts the region andthe time of a given court document.
Across vocab-ulary sizes and number of topics, we have achievedbetter system accuracy than state-of-the-art genera-tive and discriminative models of text.
Our quantita-tive analysis shows that early US state supreme courtopinions are predictable, and contains distinct viewstowards slave-related topics, and the shifts amongopinions depending on different periods of time.
Inaddition, our model has been shown to be effectivefor qualitative analysis of historical data, revealingpatterns that are consistent with the history of theperiod.This approach to modelling text is not limitedto the legal domain.
A key aspect of future workwill be to extend the Sparse Mixed-Effects paradigmto other problems within the social sciences wheremetadata is available but qualitative analysis at alarge scale is difficult or impossible.
In additionto historical documents, this can include humani-ties texts, which are often sorely lacking in empir-ical justifications, and analysis of online communi-ties, which are often rife with available metadata butproduce content far faster than it can be analyzed byexperts.AcknowledgmentsWe thank Jacob Eisenstein, Noah Smith, and anony-mous reviewers for valuable suggestions.
WilliamYang Wang is supported by the R. K. Mellon Presi-dential Fellowship.747ReferencesLalit R. Bahl, Peter F.
Brown., Peter V. de Souza, andRobert L. Mercer.
1988.
A new algorithm for theestimation of hidden Markov model parameters.
InIEEE Inernational Conference on Acoustics, Speechand Signal Processing, ICASSP, pages 493?496.Ryan S.J.D.
Baker and Kalina Yacef.
2009.
The state ofeducational data mining in 2009: a review and futurevisions.
In Journal of Educational Data Mining, pages3?17.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
Journal of Machine Learn-ing Research (JMLR), pages 993?1022.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Transac-tions on Intelligent System Technologies, pages 1?27.Jake Chen and Stefano Lombardi.
2010.
Biological datamining.
Chapman and Hall/CRC.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), pages 1?8.Agata Katarzyna Cybulska and Piek Vossen.
2011.
His-torical event extraction from text.
In Proceedings ofthe 5th ACL-HLT Workshop on Language Technologyfor Cultural Heritage, Social Sciences, and Humani-ties, pages 39?43.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable modelfor geographic lexical variation.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP 2010), pages 1277?1287.Jacob Eisenstein, Amr Ahmed, and Eric.
Xing.
2011a.Sparse additive generative models of text.
Proceed-ings of the 28th International Conference on MachineLearning (ICML 2011), pages 1041?1048.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011b.
Discovering sociolinguistic associations withstructured sparsity.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL HLT 2011),pages 1365?1374.Annette Gotscharek, Andreas Neumann, Ulrich Reffle,Christoph Ringlstetter, and Klaus U. Schulz.
2009.Enabling information retrieval on historical documentcollections: the role of matching procedures and spe-cial lexica.
In Proceedings of The Third Workshopon Analytics for Noisy Unstructured Text Data (AND2009), pages 69?76.Weiwei Guo and Mona Diab.
2011.
Semantic topic mod-els: combining word distributional statistics and dic-tionary definitions.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2011), pages 552?561.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proceedings of Uncertainty in ArtificialIntelligence (UAI 1999), pages 289?296.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: learning with many relevant fea-tures.Kenneth Lange and Janet S. Sinsheimer.
1993.
Nor-mal/independent distributions and their applications inrobust regression.Dong Nguyen and Carolyn Penstein Rose?.
2011.
Lan-guage use as a reflection of socialization in onlinecommunities.
In Workshop on Language in Social Me-dia at ACL.Dong Nguyen, Elijah Mayfield, and Carolyn P. Rose?.2010.
An analysis of perspectives in interactive set-tings.
In Proceedings of the First Workshop on SocialMedia Analytics (SOMA 2010), pages 44?52.Karen Orren.
1992.
Belated feudalism: labor, the law,and liberal development in the united states.Eva Pettersson and Joakim Nivre.
2011.
Automatic verbextraction from historical swedish texts.
In Proceed-ings of the 5th ACL-HLT Workshop on Language Tech-nology for Cultural Heritage, Social Sciences, and Hu-manities, pages 87?95.William D. Popkin.
2007.
Evolution of the judicial opin-ion: institutional and individual styles.
NYU Press.Kumar Shubhankar, Aditya Pratap Singh, and VikramPudi.
2011.
An efficient algorithm for topic rankingand modeling topic evolution.
In Proceedings of Inter-national Conference on Database and Expert SystemsApplications.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, pages 1566?1581.Jenny Bourne Wahl.
2002.
The Bondsman?s Burden: AnEconomic Analysis of the Common Law of SouthernSlavery.
Cambridge University Press.William Yang Wang and Kathleen McKeown.
2010.
?gotyou!?
: automatic vandalism detection in wikipediawith web-based shallow syntactic-semantic modeling.In Proceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages1146?1154.William Yang Wang, Kapil Thadani, and Kathleen McK-eown.
2011.
Identifyinge event descriptions using co-training with online news summaries.
In Proceedingsof the 5th International Joint Conference on NaturalLanguage Processing (IJCNLP 2011), pages 281?291.748Gavin Wright.
2006.
Slavery and american economicdevelopment.
Walter Lynwood Fleming Lectures inSouthern History.Tze-I Yang, Andrew Torget, and Rada Mihalcea.
2011.Topic modeling on historical newspapers.
In Proceed-ings of the 5th ACL-HLT Workshop on Language Tech-nology for Cultural Heritage, Social Sciences, and Hu-manities, pages 96?104.749
