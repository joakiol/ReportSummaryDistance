An Efficient A* Search Algorithm for Statistical Machine TranslationFranz Josef Och, Nicola Ueffing, Hermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen - University of TechnologyD-52056 Aachen, Germany{och,ueffing,ney}@informatik.rwth-aachen.deAbstractIn this paper, we describe an efficientA* search algorithm for statistical ma-chine translation.
In contrary to beam-search or greedy approaches it is possi-ble to guarantee the avoidance of searcherrors with A*.
We develop various so-phisticated admissible and almost ad-missible heuristic functions.
Especiallyour newly developped method to per-form a multi-pass A* search with aniteratively improved heuristic functionallows us to translate even long sen-tences.
We compare the A* search al-gorithm with a beam-search approachon the Hansards task.1 IntroductionThe goal of machine translation is the transla-tion of a text given in some source language intoa target language.
We are given a source stringfJ1 = f1...fj ...fJ , which is to be translated into atarget string eI1 = e1...ei...eI .
Among all possibletarget strings, we will choose the string with thehighest probability:e?I1 = argmaxeI1{Pr(eJ1 |f I1 )}= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)}The argmax operation denotes the search prob-lem, i.e.
the generation of the output sentencein the target language.
Pr(eI1) is the languagemodel of the target language, whereas Pr(fJ1 |eI1)denotes the translation model.Many statistical translation models (Brown etal., 1993; Vogel et al, 1996; Och and Ney, 2000b)try to model word-to-word correspondences be-tween source and target words.
These correspon-dences are called an alignment.
The model isoften further restricted in a way such that eachsource word is assigned exactly one target word.The alignment mapping is j ?
i = aj fromsource position j to target position i = aj .
Thealignment aJ1 may contain alignments aj = 0with the ?empty?
word e0 to account for sourcewords that are not aligned to any target word.
In(statistical) alignment models Pr(fJ1 , aJ1 |eI1), thealignment aJ1 is introduced as a hidden variable.Typically, the search is performed using the so-called maximum approximation:e?I1 = argmaxeI1????
?Pr(eI1) ?
?aJ1Pr(fJ1 , aJ1 |eI1)????
?= argmaxeI1{Pr(eI1) ?maxaJ1Pr(fJ1 , aJ1 |eI1)}The search space consists of the set of all possibletarget language strings eI1 and all possible align-ments aJ1 .2 IBM Model 4Various statistical alignment models of the formPr(fJ1 , aJ1 |eI1) have been introduced in (Brownet al, 1993; Vogel et al, 1996; Och and Ney,2000a).
In this paper we use the so-called Model4 from (Brown et al, 1993).In Model 4 the statistical alignment model isdecomposed into five sub-models:?
the lexicon model p(f |e) for the probabilitythat the source word f is a translation of thetarget word e,?
the distortion model p=1(j?j?|C(fj), E) forthe probability that the translations of twoconsecutive target words have the positiondifference j ?
j?
where C(fj) is the wordclass of fj and E is the word class of thefirst of the two consecutive target words,?
the distortion model p>1(j ?
j?|C(fj)) forthe probability that the words aligned to onetarget words have the position difference j?j?,?
the fertility model p(?|e) for the probabilitythat a target language word e is aligned to ?source language words,?
the empty word fertility model p(?0|e0) forthe probability that exactly ?0 words remainunaligned to.The final probability p(fJ1 , aJ1 |eI1) for Model 4 isobtained by multiplying the probabilities of thesub-models for all words.
For a detailed descrip-tion for Model 4 the reader is referred to (Brownet al, 1993).We use Model 4 in this paper for two reasons.First, it has been shown that Model 4 producesa very good alignment quality in comparison tovarious other alignment models (Och and Ney,2000b).
Second, the dependences in the distortionmodel along the target language words make itquite easy to integrate standard n-gram languagemodels in the search process.
This would be moredifficult in the HMM alignment model (Vogel etal., 1996).
Yet, many of the results presented inthe following are also applicable to other align-ment models.3 Search problemThe following tasks have to be performed both us-ing A* and beam search (BS):?
The search space has to be structured intoa search graph.
This search graph typicallyincludes an initial node, intermediary nodes(partial hypotheses), and goal nodes (com-pleted hypotheses).
A node contains the fol-lowing information:?
the predecessor words u, v in the targetlanguage,?
the score of the hypothesis,?
a backpointer to the preceding partialhypothesis,?
the model specific information de-scribed at the end of this subsection.?
A scoring function Q(n) + h(n) has to bedefined which assigns a score to every noden.
For beam search, this is the score Q(n) ofa best path to this node.
In the A* algorithm,an estimation h(n) of the score of a best pathfrom node n to a goal node is added.
(Berger et al, 1996) presented a method to struc-ture the search space.
Our search algorithm forModel 4 uses a similar structuring of the searchspace.
We will shortly review the basic conceptsof this search space structure: Every partial hy-pothesis consists of a prefix of the target sentenceand a corresponding alignment.
A partial hypoth-esis is extended by accounting for exactly one ad-ditional word of the source sentence.
Every exten-sion yields an extension score which is computedby taking into account the lexicon, distortion, andfertility probabilities involved with this extension.A partial hypothesis is called open if more sourcewords are to be aligned to the current target wordin the following extensions.
A hypothesis that isnot open is said to be closed.
Every extension ofan open hypothesis will extend the fertility of thepreviously produced target word and an extensionof a closed hypothesis will produce a new word.Therefore, the language model score is added aswell if a closed hypothesis is extended.It is prohibitive to consider all possible transla-tions of all words.
Instead, we restrict the searchto the most promising candidates by calculating?inverse translations?
(Al-Onaizan et al, 1999).The inverse translation probability p(e | f) of asource word f is calculated asp(e | f) = p (f | e) p (e)?e?p (f | e?)
p (e?)
,where we use a unigram model p (e) to esti-mate the prior probability of a target word be-ing used.
Like (Al-Onaizan et al, 1999), we useonly the top 12 translations of a given source lan-guage word.
In addition, we remove from this listall words whose inverse translation probability islower than 0.01 times the best inverse translationprobability.
This observation pruning is the onlypruning involved in our A* search algorithm.
Ex-periments showed this does not impair translationquality, but the search becomes much more effi-cient.In order to keep the search space as small aspossible it is crucial to perform a recombina-tion of search hypotheses.
Every two hypothe-ses which can be distinguished by neither the lan-guage model state nor the translation model statecan be recombined, only the hypothesis with abetter score of the two needs to be considered inthe subsequent search process.
We use a standardtrigram language model, so the relevant languagemodel state of node n consists of the current wordw(n) and the previous word v(n) (later on we willdescribe an improvement to this).
The translationmodel state depends on the specific model depen-dencies of Model 4:?
a coverage set C(n) containing the alreadytranslated source language positions,?
the position j(n) of the previously translatedsource word,?
a flag indicating whether the hypothesis isopen or closed,?
the number of source language words whichare aligned to the empty word,?
a flag showing whether the hypothesis is acomplete hypothesis or not.Efficient language model recombinationThe recombination procedure which is describedabove can be improved by taking into account thebacking-off structure of the language model.
Thetrigram language model we use has the propertythat if the count of the bigram N(u, v) = 0, thenthe probability P (w|u, v) depends only on v. Inthis case the recombination can be significantlyimproved by recombining all nodes whose lan-guage model state has the property N(u, v) = 0only with respect to v. Obviously, this could begeneralized to other types of language models aswell.Experiments have shown that by using this ef-ficient recombination, the number of needed hy-potheses can be reduced by about a factor of 4.Search algorithmsWe evaluate the following two search algorithms:?
beam search algorithm (BS): (Tillmann,2001; Tillmann and Ney, 2000)In this algorithm the search space is exploredin a breadth-first manner.
The search algo-rithm is based on a dynamic programmingapproach and applies various pruning tech-niques in order to restrict the number of con-sidered hypotheses.
For more details see(Tillmann, 2001).?
A* search algorithm:In A*, all search hypotheses are managed ina priority queue.
The basic A* search (Nils-son, 1971) can be described as follows:1. initialize priority queue with an emptyhypothesis2.
remove the hypothesis with the highestscore from the priority queue3.
if this hypothesis is a goal hypothesis:output this hypothesis and terminate4.
produce all extensions of this hypothe-sis and put the extensions to the queue5.
goto 2The so-called heuristic function estimatesthe probability of a completion of a partialhypothesis.
This function is called admissi-ble if it never underestimates this probabil-ity.
Thus, admissible heuristic functions arealways optimistic.
The A* search algorithmcorresponds to the Dijkstra algorithm if theheuristic function is equal to zero.4 Admissible heuristic functionIn order to perform an efficient search with theA* search algorithm it is crucial to use a goodheuristic function.
We only know of the work by(Wang and Waibel, 1997) dealing with heuristicfunctions for search in statistical machine trans-lation.
They developed a simple heuristic func-tion for Model 2 from (Brown et al, 1993) whichwas non admissible.
In the following we de-velop a guaranteed admissible heuristic functionfor Model 4 taking into account distortion proba-bilities and the coupling of lexicon, fertility, andlanguage model probabilities.The basic idea for developing a heuristic func-tion for the alignment models is the fact that allsource sentence positions which have not beencovered so far still have to be translated in orderto complete the sentence.
Therefore, the value ofthe heuristic function HX(n) for a node n canbe deduced if we have an estimation hX(j) of theoptimal score of translating position j (here X de-notes different possibilities to choose the heuristicfunction):HX(n) =?j 6?C(n)hX(j) ,where C(n) is the coverage set.The simplest realization of a heuristic func-tion, denoted as hT (j), takes into account onlythe translation probability p(f |e):hT (j) = maxe p(fj |e)This heuristic function can be refined by intro-ducing also the fertility probabilities (symbol F)of a target word e:hTF (j) == max{maxe 6=e0,?p(fj |e) ?
?p(?|e), p(f |e0)}Thereby, a coupling between the translation andfertility probabilities is achieved.
We have totake the ?-th root in order to avoid that the fer-tility probability of a target word whose fertilityis higher than one is taken into account for everysource word aligned to it.
For words which aretranslated by the empty word e0, no fertility prob-ability is used.The language model can be incorporated byconsidering that for every target word there existsan optimal language model probability:pL(e) = maxu,v p(e|u, v)Here, we assume a trigram language model.Thus, a heuristic function including a couplingbetween translation, fertility, and language modelprobabilities (TFL) is given by:hTFL(j) == max{maxe,?p(fj |e) ?
?p(?|e)pL(e), p(f |e0)}This value can be precomputed efficiently beforethe search process itself starts.The heuristic function for the distortion proba-bilities depends on the used model.
For Model 4,we obtain:hD(j) = maxj?,Ep(j ?
j?|E,C(fj))Here, E refers to the class of the previouslyaligned target word.The heuristic functions hD(j) involve maxi-mizations over the source positions j?.
The do-main of this variable shrinks during search asmore and more words get translated.
Therefore, itis possible to improve this heuristic function dur-ing search to perform a maximization only overthe free source language positions j?.
For Model 4we compute the following heuristic function withtwo arguments:hD(j?, j) = maxEp(j ?
j?|E,C(fj))Thus, we obtain as an estimation of the distortionprobabilityhD(j) = maxj?
6?C(n)hD(j?, j) .This yields the following heuristic functions tak-ing into account translation, fertility, language,and distortion model probabilities:HTFLD(n) =?j 6?C(n)hTFL(j) ?
hD(j) (1)Using these heuristic functions we have the over-head of performing this rest cost estimation forevery coverage set in search.
The experimentswill show that these additional costs are over-compensated by the gain in reducing the searchspace that has to be expanded during the A*search.To assess the predictive power of the vari-ous components in the heuristic, we compare thevalue of the heuristic function of the empty hy-pothesis with the score of the optimal transla-tion.
A heuristic function is better if the dif-ference between these two values is small.
Ta-ble 1 contains a comparison of various heuristicfunctions.
We compare the average costs (nega-tive logarithm of the probabilities) of the optimaltranslation and the average of the estimated costsof the empty hypothesis.
Typically, the estimatedcosts of TFLD and the real costs differ by factor3.Table 1: Predictive power of admissible and almost admissible heuristic functions.sentence HF for initial node empirical goal nodelength T TF TFL TFLD score score6 5.1 7.2 12.7 13.0 25.9 35.58 5.7 8.2 16.0 16.3 29.8 43.710 8.1 11.6 19.4 19.7 36.5 55.812 9.5 13.7 20.7 21.1 43.9 63.4We will see later in Section 6 that the guar-anteed admissible heuristic functions describedabove result in dramatically more efficient search.5 Empirical heuristic functionsIn this section we describe a new method to ob-tain an almost admissible heuristic function by amulti pass search.
This yields a significantly moreefficient search than using the admissible heuris-tic functions.
Thus, we lose the strict guarantee toavoid search errors, but obtain a significant timegain.The idea of an empirical heuristic functionis to perform a multi-pass search.
In the firstpass a good admissible heuristic function (here:HTFLD) is used.
If this search does not need toomuch memory the search process is finished.
Ifthe search failed, it is restarted using an improvedheuristic function which had been obtained duringthe initial search process.
This heuristic functionis computed such that it has the property that itis admissible with respect to the explored searchspace.
That means, the heuristic function is op-timistic with respect to every node in the searchspace explored in the first pass.Specifically, during the first pass, we maintaina two-dimensional matrix hE(j, j?)
with (J +2) ?
(J + 2) entries which are all initialized with ?.The entry hE(j, j?)
is the best score that was com-puted for translating the source language word inposition j?
if the previously covered source sen-tence position is j.
The matrix entry is updatedfor every extension of a node n ?
n?
:hE(j(n), j(n?))
:== max{hE(j(n), j(n?
)), p(n, n?
)}Here, p(n, n?)
is the probability of the extensionn ?
n?.
hE(0, j) is the empirical score of start-ing a sentence by covering the j-th source sen-tence position first.
Likewise, hE(j, J + 1) is theempirical score of finishing a sentence with j asthe last source sentence position that was covered.This yieldshE(j) = maxj?
6?C(n)?j?=J+1hE(j, j?)
.In this calculation of hE(j), we maximize overthe columns of a matrix.
The translation of thesource sentence can be viewed as a TravelingSalesman Problem where the source sentence po-sitions are the cities that have to be visited.
Thus,the maximization over the columns is equivalentto assuring that the position j will be left afterthe visit.
We design an improved heuristic func-tion using the following principle (Aigner, 1993):Each city has to be both reached and left.
There-fore, in order to take an upper bound of reachinga city into account, we divide each column of thematrix by its maximum and maximize over therows of the matrix (Aigner, 1993):hE+(j) = maxj?
6?C(n)?j?=j(n)hE(j?, j)/hE(j?)
.We obtain the following empirical heuristic func-tions:HE(n) =?j 6?C(n)?j=j(n)hE(j)HE+(n) ==?j 6?C(n)?j=j(n)hE(j) ??j?
6?C(n)?j?=J+1hE+(j?
)If the search fails in the first pass due to the re-striction of the number of hypotheses ?
which was1 million in all experiments ?
the search can bestarted again using HE+(n) as a heuristic.
Toavoid an overestimation of the actual costs, wemultiply the empirical costs by a factor lower thanTable 3: Training corpus statistics (* withoutpunctuation marks).French Englishsentences 49000 49000words 743903 816964words* 664058 730880average sentence length 16.9 14.6vocabulary size 19831 24892Table 4: Test corpora statistics.Corpus # Sentences # WordsF ET6 50 300 329T8 50 400 403T10 50 500 509T12 50 600 601T14 50 700 6441.
We found in our experiments that a factor of0.7 is sufficient.
The search was restarted up to 4times if it failed.
Using this method, it is possi-ble to translate sentences that are longer than 10words with a restriction to 1 million hypotheses.Table 1 shows the value of the empirical heuris-tic function of the empty node compared to thescore of the optimal goal node.
The estimatedcosts and the real costs now differ only by a fac-tor of 1.5 instead of a factor of 3 for the TFLDheuristic function before.6 ResultsWe present results on the HANSARDS task whichconsists of proceedings of the Canadian parlia-ment that are kept both in French and in English.Table 3 shows the details of our training corpus.We used different the test corpora with sentencesof length 6-14 words (Table 4).In all experiments, we use the following twoerror criteria:?
WER (word error rate):The WER is computed as the minimumnumber of substitution, insertion and dele-tion operations that have to be performed toconvert the generated string into the targetstring.?
PER (position independent word error rate):The word order of a French/English sentencepair can be quite different.
As a result, theword order of the automatically generatedtarget sentence can be different from that ofthe given target sentence, but neverthelessacceptable so that the WER measure alonecould be misleading.
In order to overcomethis problem, we introduce the position inde-pendent word error rate (PER) as additionalmeasure.
This measure compares the wordsin the two sentences without taking the wordorder into account.In the following experiments we restricted themaximum number of active search hypotheses inA* search to 1 million.
Every hypothesis has aneffective memory requirement of about 100 Byte.Therefore, we obtain a dynamic memory require-ment of about 100 MByte.In order to speed up the search, we restrictedthe reordering of words in IBM-style (Berger etal., 1996; Tillmann, 2001).
According to this re-striction, up to 3 source sentence positions may beskipped and translated later, i. e. during the searchprocess there may be up to 3 uncovered positionsleft of the rightmost covered position in the sourcesentence.
The word error rate does not increasecompared to a non-restricted reordering, but thesearch becomes much more efficient.Table 5 shows how many sentences with differ-ent sentence lengths can be translated using beamsearch and A* with various heuristic functions.Obviously, the BS approach is able to translateany sentence length, therefore the search successrate is 100%.
Without any heuristic function A*is only able to translate all 8-word sentences (withthe restriction of a maximum number of 1 millionhypotheses).
Using more sophisticated heuristicfunctions we are also able to translate all 10-wordsentences with A*.Table 6 compares the search errors of A* andBS.
During the BS search, translation pruningis carried out.
The different hypotheses are dis-tinguished according to the set of covered posi-tions of the source sentence.
For every set, thebest score of all hypotheses is computed.
Onlythose hypotheses are kept whose score is greaterthan this best score multiplied with a threshold.We chose the threshold to be 2.5, 5.0, 7.5 and 10.0(see Table 6).Table 2: Effect of observation pruning on the translation quality (average over all test sets).# inverse 10 12 14 16 18 20translationsWER 73.81 73.33 75.50 76.23 76.19 76.59PER 68.02 66.93 70.07 71.16 71.24 71.16Table 5: Search Success Rate (1 million hypothe-ses) [%].sentence length 6 8 10 12BS 100 100 100 100A*: no 100 100 86 12T 100 100 88 20TF 100 100 88 22TFL 100 100 92 36TFLD 100 100 92 36E 100 100 100 74E+ 100 100 100 84Table 6: Search errors [%].sentence length 6 8 10 12 14BS 2.5 26 28 38 50 385.0 2 0 2 6 47.5 0 0 0 4 210.0 0 0 0 4 2A* 0 0 0 0 0For A* we never observe any search errors.
Inthe case of the admissible heuristic functions, thisis guaranteed by the approach.
As can be seenfrom Table 6, the BS algorithm with a large beamrarely produces search errors.Table 7 compares the translation efficiency ofthe various search algorithms.
We see that beamsearch even with a very large beam producingonly very few search errors is much more efficientthan the used A* search algorithm.Table 8 contains an assessment of translationquality comparison of A* and BS using the T6,T8, T10, T12-test corpus.
For A*, we use the E+rest cost estimation as this gives optimal results.From the 200 sentences of these test corpora wecan translate 192 sentences using the 1 million hy-potheses constraint.
For the remaining sentenceswe performed a search with 4 million hypothesesTable 7: Average search time [s] per sentence.sentencelength 6 8 10 12BS: 2.5 0.06 0.18 0.60 1.165.0 0.24 0.84 2.90 6.487.5 0.50 2.14 7.06 16.2610.0 0.78 3.30 11.86 26.42A*: E+ 1.58 13.04 100 394(cf.
below) which lead to a success for all the 12-word sentences.The number of hypotheses in A* searchWe restricted the maximal number of hypothesesto 1 million.
This was sufficient for translating10-word sentences, as the search algorithm suc-cess rate in Table 5 shows.
For longer sentencesit is necessary to allow for a larger number of hy-potheses.
For the sentences of lengths 12 and 14,we performed an A* search (E+) with 2, 4 and8 million possible hypotheses.
The search algo-rithm success rate for those searches is containedin Table 9.
We see a significant effect on the num-ber of successful searches.7 ConclusionWe have developed sophisticated admissible andalmost admissible heuristic functions for statis-tical machine translation.
We have focussed onModel 4, but most of the computations couldbe easily extended to other statistical alignmentmodels (like HMM or Model 5).
We especiallyhave observed the following effects:?
The heuristic function has a strong effect onthe efficiency of the A* search.
Without anyheuristic function only 75 % of the test cor-pus sentences can be translated (using the1 million hypotheses constraint).
Using theTable 8: Translation quality.BS (2.5) BS (5.0) BS (7.5) BS (10.0) A* (E+)WER 69.65 68.78 68.68 68.68 68.68PER 62.65 61.62 61.51 61.51 61.45Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].# hypotheses 1 million 2 million 4 million 8 million12 42 80 100 10014 2 20 70 100best admissible heuristic function TFLDwe can translate 82 %.?
Using the empirical heuristic function wecan translate 96 % of the sentences withA* search.
This heuristic function does notguarantee to avoid search errors, but thiscase never occurred in our experiments.From these results we conclude that it is oftenpossible to faster compute acceptable results us-ing a beam search approach.
Therefore, this isthe method of choice in practice.
From a theo-retical viewpoint it is interesting that using A* itis possible to translate guaranteed without searcherrors.
In addition, without having a chance toperform search without search errors it is almostimpossible to assess if errors in translation shouldbe assigned to the model/training or to the searchheuristics.
Therefore, the A* algorithm is espe-cially useful during the development of a statisti-cal machine translation system.AcknowledgmentThis paper is based on work supported partlyby the VERBMOBIL project (contract number01 IV 701 T4) by the German Federal Min-istry of Education, Science, Research and Tech-nology.
In addition, this work was supportedby the National Science Foundation under GrantNo.
IIS-9820687 through the 1999 Workshop onLanguage Engineering, Center for Language andSpeech Processing, Johns Hopkins University.ReferencesM.
Aigner.
1993.
Diskrete Mathematik.
Verlag Vieweg,Braunschweig/Wiesbaden, Germany.Y.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.Smith, and D. Yarowsky.
1999.
Statistical ma-chine translation, final report, JHU workshop.http://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-final-report.ps.A.
L. Berger, S. A. Della Pietra P. F. Brown, V. J. DellaPietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.1996.
Language translation apparatus and method of us-ing context-based translation models.
In United StatesPatent, number 5510981.
April.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.N.
Nilsson.
1971.
Problem-Solving Methods in ArtificialIntelligence.
McGraw-Hill, McGraw-Hill, New York.F.
J. Och and H. Ney.
2000a.
A comparison of alignmentmodels for statistical machine translation.
In COLING?00: The 18th Int.
Conf.
on Computational Linguistics,pages 1086?1090, Saarbru?cken, Germany, August.F.
J. Och and H. Ney.
2000b.
Improved statistical alignmentmodels.
In Proc.
of the 38th Annual Meeting of the As-sociation for Computational Linguistics, pages 440?447,Hongkong, China, October.C.
Tillmann and H. Ney.
2000.
Word re-ordering and DP-based search in statistical machine translation.
In COL-ING ?00: The 18th Int.
Conf.
on Computational Linguis-tics, pages 850?856, Saarbru?cken, Germany, August.C.
Tillmann.
2001.
Word Re-Ordering and Dynamic Pro-gramming based Search Algorithms for Statistical Ma-chine Translation.
Ph.D. thesis, RWTH Aachen, Ger-many, May.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based wordalignment in statistical translation.
In COLING ?96: The16th Int.
Conf.
on Computational Linguistics, pages 836?841, Copenhagen, August.Ye-Yi Wang and Alex Waibel.
1997.
Decoding algorithm instatistical translation.
In Proc.
35th Annual Conf.
of theAssociation for Computational Linguistics, pages 366?372, Madrid, Spain, July.
