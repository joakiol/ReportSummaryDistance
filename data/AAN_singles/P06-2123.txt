Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961?968,Sydney, July 2006. c?2006 Association for Computational LinguisticsSubword-based Tagging for Confidence-dependent Chinese WordSegmentationRuiqiang Zhang1,2 and Genichiro Kikui?
and Eiichiro Sumita1,21National Institute of Information and Communications Technology2ATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan{ruiqiang.zhang,eiichiro.sumita}@atr.jpAbstractWe proposed a subword-based tagging forChinese word segmentation to improvethe existing character-based tagging.
Thesubword-based tagging was implementedusing the maximum entropy (MaxEnt)and the conditional random fields (CRF)methods.
We found that the proposedsubword-based tagging outperformed thecharacter-based tagging in all compara-tive experiments.
In addition, we pro-posed a confidence measure approach tocombine the results of a dictionary-basedand a subword-tagging-based segmenta-tion.
This approach can produce anideal tradeoff between the in-vocaularyrate and out-of-vocabulary rate.
Our tech-niques were evaluated using the test datafrom Sighan Bakeoff 2005.
We achievedhigher F-scores than the best results inthree of the four corpora: PKU(0.951),CITYU(0.950) and MSR(0.971).1 IntroductionMany approaches have been proposed in Chineseword segmentation in the past decades.
Segmen-tation performance has been improved significantly,from the earliest maximal match (dictionary-based)approaches to HMM-based (Zhang et al, 2003) ap-proaches and recent state-of-the-art machine learn-ing approaches such as maximum entropy (Max-Ent) (Xue and Shen, 2003), support vector machine?Now the second author is affiliated with NTT.
(SVM) (Kudo and Matsumoto, 2001), conditionalrandom fields (CRF) (Peng and McCallum, 2004),and minimum error rate training (Gao et al, 2004).By analyzing the top results in the first and secondBakeoffs, (Sproat and Emerson, 2003) and (Emer-son, 2005), we found the top results were producedby direct or indirect use of so-called ?IOB?
tagging,which converts the problem of word segmentationinto one of character tagging so that part-of-speechtagging approaches can be used for word segmen-tation.
This approach was also called ?LMR?
(Xueand Shen, 2003) or ?BIES?
(Asahara et al, 2005)tagging.
Under the scheme, each character of aword is labeled as ?B?
if it is the first character of amultiple-character word, or ?I?
otherwise, and ?O?if the character functioned as an independent word.For example, ??
(whole) ???
(Beijing city)?
islabeled as ??/O?/B?/I?/I?.
Thus, the trainingdata in word sequences are turned into IOB-labeleddata in character sequences, which are then used asthe training data for tagging.
For new test data, wordboundaries are determined based on the results oftagging.While the IOB tagging approach has been widelyused in Chinese word segmentation, we found thatso far all the existing implementations were usingcharacter-based IOB tagging.
In this work we pro-pose a subword-based IOB tagging, which assignstags to a pre-defined lexicon subset consisting of themost frequent multiple-character words in additionto single Chinese characters.
If only Chinese char-acters are used, the subword-based IOB tagging isdowngraded to a character-based one.
Taking thesame example mentioned above, ??????
is la-961beled as ?
?/O ?
?/B ?/I?
in the subword-basedtagging, where ???/B?
is labeled as one unit.
Wewill give a detailed description of this approach inSection 2.There exists a clear weakness with the IOB tag-ging approach: It yields a very low in-vocabularyrate (R-iv) in return for a higher out-of-vocabulary(OOV) rate (R-oov).
In the results of the closedtest in Bakeoff 2005 (Emerson, 2005), the workof (Tseng et al, 2005), using CRFs for the IOB tag-ging, yielded a very high R-oov in all of the fourcorpora used, but the R-iv rates were lower.
WhileOOV recognition is very important in word segmen-tation, a higher IV rate is also desired.
In this workwe propose a confidence measure approach to lessenthis weakness.
By this approach we can change theR-oov and R-iv and find an optimal tradeoff.
Thisapproach will be described in Section 2.3.In addition, we illustrate our word segmentationprocess in Section 2, where the subword-based tag-ging is described by the MaxEnt method.
Section 3presents our experimental results.
The effects usingthe MaxEnts and CRFs are shown in this section.Section 4 describes current state-of-the-art methodswith Chinese word segmentation, with which our re-sults were compared.
Section 5 provides the con-cluding remarks and outlines future goals.2 Chinese word segmentation frameworkOur word segmentation process is illustrated inFig.
1.
It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IVwords, a maximum entropy subword-based taggerfor recognizing OOVs, and a confidence-dependentword disambiguation used for merging the resultsof both the dictionary-based and the IOB-tagging-based.
An example exhibiting each step?s results isalso given in the figure.2.1 Dictionary-based N-gram wordsegmentationThis approach can achieve a very high R-iv, but noOOV detection.
We combined with it the N-gramlanguage model (LM) to solve segmentation ambi-guities.
For a given Chinese character sequence,C = c0c1c2 .
.
.
cN , the problem of word segmenta-tion can be formalized as finding a word sequence,????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\input???????
?+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\Dictionary-based word segmentation?%?,?,?2?2?
?%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,Subword-based IOB tagging?%?,?,?2?2?
?%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,Confidence-based disambiguation???????
?+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\outputFigure 1: Outline of word segmentation processW = wt0wt1wt2 .
.
.wtM , which satisfieswt0 = c0 .
.
.
ct0 , wt1 = ct0+1 .
.
.
ct1wti = cti?1+1 .
.
.
cti , wtM = ctM?1+1 .
.
.
ctMti > ti?1, 0 ?
ti ?
N, 0 ?
i ?
Msuch thatW = arg maxWP(W |C) = arg maxWP(W)P(C|W)= arg maxWP(wt0wt1 .
.
.wtM )?
(c0 .
.
.
ct0 ,wt0)?
(ct0+1 .
.
.
ct1 ,wt1) .
.
.
?
(ctM?1+1 .
.
.
cM,wtM )(1)We applied Bayes?
law in the above derivation.Because the word sequence must keep consistentwith the character sequence, P(C|W) is expandedto be a multiplication of a Kronecker delta functionseries, ?
(u, v), equal to 1 if both arguments are thesame and 0 otherwise.
P(wt0wt1 .
.
.wtM ) is a lan-guage model that can be expanded by the chain rule.If trigram LMs are used, we haveP(w0)P(w1|w0)P(w2|w0w1) ?
?
?
P(wM |wM?2wM?1)where wi is a shorthand for wti .Equation 1 indicates the process of dictionary-based word segmentation.
We looked up the lexiconto find all the IVs, and evaluated the word sequencesby the LMs.
We used a beam search (Jelinek, 1998)instead of a viterbi search to decode the best word962sequence because we found that a beam search canspeed up the decoding.
N-gram LMs were used toscore all the hypotheses, of which the one with thehighest LM scores is the final output.
The exper-imental results are presented in Section 3.1, wherewe show the comparative results as we changed theorder of LMs.2.2 Subword-based IOB taggingThere are several steps to train a subword-based IOBtagger.
First, we extracted a word list from the train-ing data sorted in decreasing order by their countsin the training data.
We chose all the single charac-ters and the top multi-character words as a lexiconsubset for the IOB tagging.
If the subset consists ofChinese characters only, it is a character-based IOBtagger.
We regard the words in the subset as the sub-words for the IOB tagging.Second, we re-segmented the words in the train-ing data into subwords of the subset, and as-signed IOB tags to them.
For the character-based IOB tagger, there is only one possibilityfor re-segmentation.
However, there are multi-ple choices for the subword-based IOB?tagger.For example, ????(Beijing-city)?
can besegmented as ??
?
?(Beijing-city)/O,?
or??
?
(Beijing)/B ?(city)/I,?
or ??(north)/B?
(capital)/I ?(city)/I.?
In this work we used for-ward maximal match (FMM) for disambiguation.Because we carried out FMMs on each words in themanually segmented training data, the accuracy ofFMM was much higher than applying it on wholesentences.
Of course, backward maximal match(BMM) or other approaches are also applicable.
Wedid not conduct comparative experiments due to triv-ial differences in the results of these approaches.In the third step, we used the maximum entropy(MaxEnt) approach (the results of CRF are given inSection 3.4) to train the IOB tagger (Xue and Shen,2003).
The mathematical expression for the MaxEntmodel isP(t|h) = exp???????
?i?i fi(h, t)???????
/Z, Z =?tP(t|h) (2)where t is a tag, ?I,O,B,?
of the current word; h,the context surrounding the current word, includingword and tag sequences; fi, a binary feature equalto 1 if the i-th defined feature is activated and 0 oth-erwise; Z, a normalization coefficient; and ?i, theweight of the i-th feature.Many kinds of features can be defined for improv-ing the tagging accuracy.
However, to conform tothe constraints of closed test in Bakeoff 2005, somefeatures, such as syntactic information and characterencodings for numbers and alphabetical characters,are not allowed.
Therefore, we used the featuresavailable only from the provided training corpus.?
Contextual information:w0, t?1,w0t?1,w0t?1w1, t?1w1, t?1t?2,w0t?1t?2,w0w1,w0w1w2,w?1,w0w?1,w0w?1w1,w?1w1,w?1w?2,w0w?1w?2,w1,w1w2where w stands for word and t, for IOB tag.The subscripts are position indicators, where0 means the current word/tag; ?1,?2, the firstor second word/tag to the left; 1, 2, the first orsecond word/tag to the right.?
Prefixes and suffixes.
These are very useful fea-tures.
Using the same approach as in (Tsenget al, 2005), we extracted the most frequentwords tagged with ?B?, indicating a prefix, andthe last words tagged with ?I?, denoting a suf-fix.
Features containing prefixes and suffixeswere used in the following combinations withother features, where p stands for prefix; s, suf-fix; p0 means the current word is a prefix ands1 denotes that the right first word is a suffix,and so on.p0,w0 p?1,w0 p1, s0,w0s?1,w0s1,p0w?1, p0w1, s0w?1, s0w?2?
Word length.
This is defined as the numberof characters in a word.
The length of a Chi-nese word has discriminative roles for wordcomposition.
For example, single-characterwords are more apt to form new words thanare multiple-character words.
Features usingword length are listed below, where l0 meansthe word length of the current word.
Others canbe inferred similarly.l0,w0l?1,w0l1,w0l?1l1, l0l?1, l0l1As to feature selection, we simply adopted the ab-solute count for each feature in the training data as963the metric, and defined a cutoff value for each fea-ture type.We used IIS to train the maximum entropy model.For details, refer to (Lafferty et al, 2001).The tagging algorithm is based on the beam-search method (Jelinek, 1998).
After the IOB tag-ging, each word is tagged with a B/I/O tag.
Theword segmentation is obtained immediately.
Theexperimental effect of the word-based tagger andits comparison with the character-based tagger aremade in section 3.2.2.3 Confidence-dependent word segmentationIn the last two steps we produced two segmentationresults: the one by the dictionary-based approachand the one by the IOB tagging.
However, nei-ther was perfect.
The dictionary-based segmenta-tion produced a result with a higher R-iv but lowerR-oov while the IOB tagging yielded the contraryresults.
In this section we introduce a confidencemeasure approach to combine the two results.
Wedefine a confidence measure, CM(tiob|w), to measurethe confidence of the results produced by the IOBtagging by using the results from the dictionary-based segmentation.
The confidence measure comesfrom two sources: IOB tagging and dictionary-basedword segmentation.
Its calculation is defined as:CM(tiob|w) = ?CMiob(tiob|w) + (1 ?
?)?
(tw, tiob)ng(3)where tiob is the word w?s IOB tag assigned by theIOB tagging; tw, a prior IOB tag determined by theresults of the dictionary-based segmentation.
Afterthe dictionary-based word segmentation, the wordsare re-segmented into subwords by FMM before be-ing fed to IOB tagging.
Each subword is given aprior IOB tag, tw.
CMiob(t|w), a confidence proba-bility derived in the process of IOB tagging, whichis defined asCMiob(t|w) =?hi P(t|w, hi)?t?hi P(t|w, hi)where hi is a hypothesis in the beam search.?
(tw, tiob)ng denotes the contribution of thedictionary-based segmentation.?
(tw, tiob)ng is a Kronecker delta function definedas?
(tw, tiob)ng = { 1 if tw = tiob0 otherwiseIn Eq.
3, ?
is a weighting between the IOB tag-ging and the dictionary-based word segmentation.We found an empirical value 0.8 for ?.By Eq.
3 the results of IOB tagging were re-evaluated.
A confidence measure threshold, t, wasdefined for making a decision based on the value.If the value was lower than t, the IOB tag was re-jected and the dictionary-based segmentation wasused; otherwise, the IOB tagging segmentation wasused.
A new OOV was thus created.
For the twoextreme cases, t = 0 is the case of the IOB tag-ging while t = 1 is that of the dictionary-based ap-proach.
In Section 3.3 we will present the experi-mental segmentation results of the confidence mea-sure approach.
In a real application, we can actuallychange the confidence threshold to obtain a satisfac-tory balance between R-iv and R-oov.An example is shown in Figure 1.
In the stage ofIOB tagging, a confidence is attached to each word.In the stage of confidence-based, a new confidencewas made after merging with dictionary-based re-sults where all single-character words are labeledas ?O?
by default except ?Beijing-city?
labeled as?Beijing/B?
and ?city/I?.3 ExperimentsWe used the data provided by Sighan Bakeoff 2005to test our approaches described in the previous sec-tions.
The data contain four corpora from differ-ent sources: Academia sinica, City University ofHong Kong, Peking University and Microsoft Re-search (Beijing).
The statistics concerning the cor-pora is listed in Table 3.
The corpora provided bothunicode coding and Big5/GB coding.
We used theBig5 and CP936 encodings.
Since the main purposeof this work is to evaluate the proposed subword-based IOB tagging, we carried out the closed testonly.
Five metrics were used to evaluate the seg-mentation results: recall (R), precision (P), F-score(F), OOV rate (R-oov) and IV rate (R-iv).
For a de-tailed explanation of these metrics, refer to (Sproatand Emerson, 2003).964Corpus Abbrev.
Encodings Training size (words) Test size (words)Academia Sinica AS Big5/Unicode 5.45M 122KBeijing University PKU CP936/Unicode 1.1M 104KCity University of Hong Kong CITYU Big5/Unicode 1.46M 41KMicrosoft Research (Beijing) MSR CP936/Unicode 2.37M 107KTable 1: Corpus statistics in Sighan Bakeoff 20053.1 Effects of N-gram LMsWe obtained a word list from the training data as thevocabulary for dictionary-based segmentation.
N-gram LMs were generated using the SRI LM toolkit.Table 2 shows the performance of N-gram segmen-tation by changing the order of N-grams.We found that bigram LMs can improve segmen-tation over unigram, though we observed no effectfrom the trigram LMs.
For the PKU corpus, therewas a relatively strong improvement due to using bi-grams rather than unigrams, posssibly because thePKU corpus?
training size was smaller than the oth-ers.
For a sufficiently large training corpus, the un-igram LMs may be enough for segmentation.
Thisexperiment revealed that language models above bi-grams do not improve word segmentation.
Sincethere were some single-character words present intest data but not in the training data, the R-oov rateswere not zero in this experiment.
In fact, we did notuse any OOV detection for the dictionary-based ap-proach.3.2 Comparisons of Character-based andSubword-based taggerIn Section 2.2 we described the character-based andsubword-based IOB tagging methods.
The main dif-ference between the two is the lexicon subset usedfor re-segmentation.
For the subword-based IOBtagging, we need to add some multiple-characterwords into the lexicon subset.
Since it is hard todecide the optimal number of words to add, we testthree different lexicon sizes, as shown in Table 3.The first one, s1, consisting of all the characters, isa character-based approach.
The second, s2, added2,500 top words from the training data to the lexi-con of s1.
The third, s3, added another 2,500 topwords to the lexicon of s2.
All the words wereamong the most frequent in the training corpora.
Af-ter choosing the subwords, the training data were re-segmented using the subwords by FMM.
The finalAS CITYU MSR PKUs1 6,087 4,916 5,150 4,685s2 8,332 7,338 7,464 7,014s3 10,876 9,996 9,990 9,053Table 3: Three different vocabulary sizes used in subword-based tagging.
s1 contains all the characters.
s2 and s3 containssome common words.lexicons were collected again, consisting of single-character words and multiple-character words.
Ta-ble 3 shows the sizes of the final lexicons.
There-fore, the minus of the lexicon size of s2 to s1 are not2,500, exactly.The segmentation results of using three lexiconsare shown in Table 4.
The numbers are separatedby a ?/?
in the sequence of ?s1/s2/s3.?
We found al-though the subword-based approach outperformedthe character-based one significantly, there was noobvious difference between the two subword-basedapproaches, s2 and s3, adding respective 2,500 and5,000 subwords to s1.
The experiments show thatwe cannot find an optimal lexicon size from 2,500to 5,000.
However, there might be an optimal pointless than 2,500.
We did not take much effort to findthe optimal point, and regarded 2,500 as an accept-able size for practical usages.The F-scores of IOB tagging shown in Table 4 arebetter than that of N-gram word segmentation in Ta-ble 2, which proves that the IOB tagging is effectivein recognizing OOV.
However, we found there was alarge decrease in the R-ivs, which shows the weak-ness of the IOB tagging approach.
We use the con-fidence measure approach to deal with this problemin next section.3.3 Effects of the confidence measureUp to now we had two segmentation results by usingthe dictionary-based word segmentation and the IOBtagging.
In Section 2.3, we proposed a confidencemeasure approach to re-evaluate the results of IOBtagging by combining the two results.
The effects of965R P F R-oov R-ivAS 0.934/0.942/0.941 0.884/0.881/0.881 0.909/0.910/0.910 0.041/0.040/0.038 0.975/0.983/0.982CITYU 0.924/0.929/0.928 0.851/0.851/0.851 0.886/0.888/0.888 0.162/0.162/0.164 0.984/0.990/0.989PKU 0.938/0.949/0.948 0.909/0.912/0.912 0.924/0.930/0.930 0.407/0.403/0.408 0.971/0.982/0.981MSR 0.965/0.969/0.968 0.927/0.927/0.927 0.946/0.947/0.947 0.036/0.036/0.048 0.991/0.994/0.993Table 2: Segmentation results of dictionary-based segmentation in closed test of Bakeoff 2005.
A ?/?
separates the results ofunigram, bigram and trigram.R P F R-oov R-ivAS 0.922/0.942/0.943 0.914/0.930/0.930 0.918/0.936/0.937 0.641/0.628/0.609 0.935/0.956/0.959CITYU 0.906/0.933/0.934 0.905/0.929/0.927 0.906/0.931/0.930 0.668/0.671/0.671 0.925/0.954/0.955PKU 0.913/0.934/0.936 0.922/0.938/0.940 0.918/0.936/0.938 0.744/0.724/0.713 0.924/0.946/0.949MSR 0.929/0.953/0.953 0.934/0.955/0.952 0.932/0.954/0.952 0.656/0.684/0.665 0.936/0.961/0.961Table 4: Segmentation results by the pure subword-based IOB tagging.
The separator ?/?
divides the results by three lexicon sizesas illustrated in Table 3.
The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3).0.940.950.960.970.980.9910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8R-ivR-oovt=0t=1t=0t=1t=0t=1t=0t=0ASCITYUPKUMSRFigure 2: R-iv and R-oov varing as the confidence threshold, t.the confidence measure are shown in Table 5, wherewe used ?
= 0.8 and confidence threshold t = 0.7.These are empirical numbers.
We obtained the opti-mal values by multiple trials on held-out data.
Thenumbers in the slots of Table 5 are divided by a sep-arator ?/?
and displayed as the sequence ?s1/s2/s3?,just as Table 4.
We found that the results in Table 5were better than those in Table 4 and Table 2, whichproved that using the confidence measure approachyielded the best performance over the N-gram seg-mentation and the IOB tagging approaches.Even with the use of the confidence measure, thesubword-based IOB tagging still outperformed thecharacter-based IOB tagging, proving that the pro-posed subword-based IOB tagging was very effec-tive.
Though the improvement under the confidencemeasure was decreasing, it was still significant.We can change the R-oov and R-iv by changingthe confidence threshold.
The effect of R-oov and R-iv?s varing as the threshold is shown in Fig.
2, whereR-oovs and R-ivs are moving in different directions.When the confidence threshold t = 0, the case forthe IOB tagging, R-oovs are maximal.
When t = 1,representing the dictionary-based segmentation, R-oovs are the minimal.
The R-oovs and R-ivs variedlargely at the start and end point but little around themiddle section.3.4 Subword-based tagging by CRFsOur proposed approaches were presented and eval-uated using the MaxEnt method in the previoussections.
When we turned to CRF-based tagging,we found a same effect as the MaxEnt method.Our subword-based tagging by CRFs was imple-mented by the package ?CRF++?
from the site?http://www.chasen.org/t?aku/software.
?We repeated the previous sections?
experimentsusing the CRF approach except that we did one ofthe two subword-based tagging, the lexicon size s3.The same values of the confidence measure thresh-old and ?
were used.
The results are shown in Ta-ble 6.We found that the results using the CRFs weremuch better than those of the MaxEnts.
How-ever, the emphasis here was not to compare CRFsand MaxEnts but the effect of subword-based IOBtagging.
In Table 6, the results before ?/?
arethe character-based IOB tagging and after ?/?, thesubword-based.
It was clear that the subword-basedapproaches yielded better results than the character-based approach though the improvement was not ashigher as that of the MaxEnt approaches.
There was966R P F R-oov R-ivAS 0.938/0.950/0.953 0.945/0.946/0.951 0.941/0.948/0.948 0.674/0.641/0.606 0.950/0.964/0.969CITYU 0.932/0.949/0.946 0.944/0.933/0.944 0.938/0.941/0.945 0.705/0.597/0.667 0.950/0.977/0.968PKU 0.941/0.948/0.949 0.945/0.947/0.947 0.943/0.948/0.948 0.672/0.662/0.660 0.958/0.966/0.966MSR 0.944/0.959/0.961 0.959/0.964/0.963 0.951/0.961/0.962 0.671/0.674/0.631 0.951/0.967/0.970Table 5: Effects of combination using the confidence measure.
Here we used ?
= 0.8 and confidence threshold t = 0.7.
Theseparator ?/?
divides the results of s1, s2, and s3.no change on F-score for AS corpus, but a better re-call rate was found.
Our results are better than thebest one of Bakeoff 2005 in PKU, CITYU and MSRcorpora.Detailed descriptions about subword tagging byCRF can be found in our paper (Zhang et al, 2006).4 Discussion and Related worksThe IOB tagging approach adopted in this work isnot a new idea.
It was first implemented in Chi-nese word segmentation by (Xue and Shen, 2003)using the maximum entropy methods.
Later, (Pengand McCallum, 2004) implemented the idea us-ing the CRF-based approach, which yielded bet-ter results than the maximum entropy approach be-cause it could solve the label bias problem (Laf-ferty et al, 2001).
However, as we mentioned be-fore, this approach does not take advantage of theprior knowledge of in-vocabulary words; It pro-duced a higher R-oov but a lower R-iv.
This prob-lem has been observed by some participants in theBakeoff 2005 (Asahara et al, 2005), where theyapplied the IOB tagging to recognize OOVs, andadded the OOVs to the lexicon used in the HMM-based or CRF-based approaches.
(Nakagawa, 2004)used hybrid HMM models to integrate word leveland character level information seamlessly.
Weused confidence measure to determine a better bal-ance between R-oov and R-iv.
The idea of us-ing the confidence measure has appeared in (Pengand McCallum, 2004), where it was used to recog-nize the OOVs.
In this work we used it more thanthat.
By way of the confidence measure we com-bined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve theoptimal performance.Our main contribution is to extend the IOB tag-ging approach from being a character-based to asubword-based one.
We proved that the new ap-proach enhanced the word segmentation signifi-cantly in all the experiments, MaxEnts, CRFs andusing confidence measure.
We tested our approachusing the standard Sighan Bakeoff 2005 data set inthe closed test.
In Table 7 we align our results withsome top runners?
in the Bakeoff 2005.Our results were compared with the best perform-ers?
results in the Bakeoff 2005.
Two participants?results were chosen as bases: No.15-b, ranked thefirst in the AS corpus, and No.14, the best per-former in CITYU, MSR and PKU.
.
The No.14used CRF-modeled IOB tagging while No.15-b usedMaxEnt-modeled IOB tagging.
Our results pro-duced by the MaxEnt are denoted as ?ours(ME)?while ?ours(CRF)?
for the CRF approaches.
Weachieved the highest F-scores in three corpora ex-cept the AS corpus.
We think the proposed subword-based approach played the important role for theachieved good results.A second advantage of the subword-based IOBtagging over the character-based is its speed.
Thesubword-based approach is faster because fewerwords than characters needed to be labeled.
We ob-served a speed increase in both training and testing.In the training stage, the subword approach was al-most two times faster than the character-based.5 ConclusionsIn this work, we proposed a subword-based IOB tag-ging method for Chinese word segmentation.
Theapproach outperformed the character-based methodusing both the MaxEnt and CRF approaches.
Wealso successfully employed the confidence measureto make a confidence-dependent word segmentation.By setting the confidence threshold, R-oov and R-ivcan be changed accordingly.
This approach is effec-tive for performing desired segmentation based onusers?
requirements to R-oov and R-iv.967R P F R-oov R-ivAS 0.953/0.956 0.944/0.947 0.948/0.951 0.607/0.649 0.969/0.969CITYU 0.943/0.952 0.948/0.949 0.946/0.951 0.682/0.741 0.964/0.969PKU 0.942/0.947 0.957/0.955 0.949/0.951 0.775/0.748 0.952/0.959MSR 0.960/0.972 0.966/0.969 0.963/0.971 0.674/0.712 0.967/0.976Table 6: Effects of using CRF.
The separator ?/?
divides the results of s1, and s3.Participants R P F R-oov R-ivHong Kong City Universityours(CRF) 0.952 0.949 0.951 0.741 0.969ours(ME) 0.946 0.944 0.945 0.667 0.96814 0.941 0.946 0.943 0.698 0.96115-b 0.937 0.946 0.941 0.736 0.953Academia Sinica15-b 0.952 0.951 0.952 0.696 0.963ours(CRF) 0.956 0.947 0.951 0.649 0.969ours(ME) 0.953 0.943 0.948 0.608 0.96914 0.95 0.943 0.947 0.718 0.960Microsoft Researchours(CRF) 0.972 0.969 0.971 0.712 0.97614 0.962 0.966 0.964 0.717 0.968ours(ME) 0.961 0.963 0.962 0.631 0.97015-b 0.952 0.964 0.958 0.718 0.958Peking Universityours(CRF) 0.947 0.955 0.951 0.748 0.95914 0.946 0.954 0.950 0.787 0.956ours(ME) 0.949 0.947 0.948 0.660 0.96615-b 0.93 0.951 0.941 0.76 0.941Table 7: List of results in Sighan Bakeoff 2005AcknowledgementsThe authors thank the reviewers for the commentsand advice on the paper.
Some related software forthis work will be released very soon.ReferencesMasayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-Ling Goh, Yotaro Watanabe, Yuji Matsumoto, andTakashi Tsuzuki.
2005.
Combination of machinelearning methods for optimum chinese word seg-mentation.
In Forth SIGHAN Workshop on ChineseLanguage Processing, Proceedings of the Workshop,pages 134?137, Jeju, Korea.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, Jeju, Korea.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive chinese word segmentation.
In ACL-2004,Barcelona, July.Frederick Jelinek.
1998.
Statistical methods for speechrecognition.
the MIT Press.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machine.
In Proc.
of NAACL-2001,pages 192?199.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: probabilistic modelsfor segmenting and labeling sequence data.
In Proc.
ofICML-2001, pages 591?598.Tetsuji Nakagawa.
2004.
Chinese and japanese wordsegmentation using word-level and character-level in-formation.
In Proceedings of Coling 2004, pages 466?472, Geneva, August.Fuchun Peng and Andrew McCallum.
2004.
Chinesesegmentation and new word detection using condi-tional random fields.
In Proc.
of Coling-2004, pages562?568, Geneva, Switzerland.Richard Sproat and Tom Emerson.
2003.
The first inter-national chinese word segmentation bakeoff.
In Pro-ceedings of the Second SIGHAN Workshop on ChineseLanguage Processing, Sapporo, Japan, July.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for Sighan bake-off 2005.
In Proceedings of the Fourth SIGHAN Work-shop on Chinese Language Processing, Jeju, Korea.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as LMR tagging.
In Proceedings of theSecond SIGHAN Workshop on Chinese Language Pro-cessing.Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.2003.
HHMM-based Chinese lexical analyzer ICT-CLAS.
In Proceedings of the Second SIGHAN Work-shop on Chinese Language Processing, pages 184?187.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional randomfields for chinese word segmentation.
In Proc.
of HLT-NAACL.968
