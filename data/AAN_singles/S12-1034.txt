First Joint Conference on Lexical and Computational Semantics (*SEM), pages 256?264,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsMonolingual Distributional Similarity for Text-to-Text GenerationJuri Ganitkevitch, Benjamin Van Durme, and Chris Callison-BurchCenter for Language and Speech ProcessingHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21218, USAAbstractPrevious work on paraphrase extraction andapplication has relied on either paralleldatasets, or on distributional similarity met-rics over large text corpora.
Our approachcombines these two orthogonal sources of in-formation and directly integrates them intoour paraphrasing system?s log-linear model.We compare different distributional similar-ity feature-sets and show significant improve-ments in grammaticality and meaning reten-tion on the example text-to-text generationtask of sentence compression, achieving state-of-the-art quality.1 IntroductionA wide variety of applications in natural languageprocessing can be cast in terms of text-to-text gen-eration.
Given input in the form of natural lan-guage, a text-to-text generation system producesnatural language output that is subject to a set ofconstraints.
Compression systems, for instance, pro-duce shorter sentences.
Paraphrases, i.e.
differ-ing textual realizations of the same meaning, are acrucial components of text-to-text generation sys-tems, and have been successfully applied to taskssuch as multi-document summarization (Barzilay etal., 1999; Barzilay, 2003), query expansion (An-ick and Tipirneni, 1999; Riezler et al, 2007), ques-tion answering (McKeown, 1979; Ravichandran andHovy, 2002), sentence compression (Cohn and La-pata, 2008; Zhao et al, 2009), and simplification(Wubben et al, 2012).Paraphrase collections for text-to-text generationhave been extracted from a variety of different cor-pora.
Several approaches rely on bilingual paral-lel data (Bannard and Callison-Burch, 2005; Zhaoet al, 2008; Callison-Burch, 2008; Ganitkevitch etal., 2011), while others leverage distributional meth-ods on monolingual text corpora (Lin and Pantel,2001; Bhagat and Ravichandran, 2008).
So far, how-ever, only preliminary studies have been undertakento combine the information from these two sources(Chan et al, 2011).In this paper, we describe an extension of Gan-itkevitch et al (2011)?s bilingual data-based ap-proach.
We augment the bilingually-sourced para-phrases using features based on monolingual distri-butional similarity.
More specifically:?
We show that using monolingual distributionalsimilarity features improves paraphrase qualitybeyond what we can achieve with features esti-mated from bilingual data.?
We define distributional similarity for para-phrase patterns that contain constituent-levelgaps, e.g.sim(one JJ instance of NP , a JJ case of NP).This generalizes over distributional similarityfor contiguous phrases.?
We compare different types of monolingualdistributional information and show that theycan be used to achieve significant improve-ments in grammaticality.?
Finally, we compare our method to severalstrong baselines on the text-to-text generationtask of sentence compression.
Our methodshows state-of-the-art results, beating a purelybilingually sourced paraphrasing system.256... ihre Pl?ne w?rdentheir plans would...ohne aufzugebenwithoutlangfristigenin the long termlangfristigenlong-term...... ......Pl?neplansseinegiving up his......Figure 1: Pivot-based paraphrase extraction for con-tiguous phrases.
Two phrases translating to the samephrase in the foreign language are assumed to beparaphrases of one another.2 BackgroundApproaches to paraphrase extraction differ based ontheir underlying data source.
In Section 2.1 we out-line pivot-based paraphrase extraction from bilin-gual data, while the contextual features used to de-termine closeness in meaning in monolingual ap-proaches is described in Section 2.2.2.1 Paraphrase Extraction via PivotingFollowing Ganitkevitch et al (2011), we formulateour paraphrases as a syntactically annotated syn-chronous context-free grammar (SCFG) (Aho andUllman, 1972; Chiang, 2005).
An SCFG rule hasthe form:r = C ?
?f, e,?, ~?
?,where the left-hand side of the rule, C, is a nonter-minal and the right-hand sides f and e are stringsof terminal and nonterminal symbols.
There is aone-to-one correspondency between the nontermi-nals in f and e: each nonterminal symbol in f hasto also appear in e. The function ?
captures this bi-jective mapping between the nonterminals.
Drawingon machine translation terminology, we refer to f asthe source and e as the target side of the rule.Each rule is annotated with a feature vector of fea-ture functions ~?
= {?1...?N} that, using a corre-sponding weight vector ~?, are combined in a log-linear model to compute the cost of applying r:cost(r) = ?N?i=1?i log?i.
(1)A wide variety of feature functions can be formu-lated.
We detail the feature-set used in our experi-ments in Section 4.NP NNNPEUNN NPNPintentions'sEUder......?h i'sin the long termin the long termlangfristigen Pl?nethe long-term ofEuropeofthe long-term plansIBM goalsIBM 's's in the long terml?ngerfristige ZieleIBMofthe long-term ambitions..Figure 2: Extraction of syntactic paraphrases via thepivoting approach: We aggregate over different sur-face realizations, matching the lexicalized portionsof the rule and generalizing over the nonterminals.To extract paraphrases we follow the intuition thattwo English strings e1 and e2 that translate to thesame foreign string f can be assumed to have thesame meaning, as illustrated in Figure 1.1First, we use standard machine translation meth-ods to extract a foreign-to-English translation gram-mar from a bilingual parallel corpus (Koehn, 2010).Then, for each pair of translation rules where theleft-hand side C and foreign string f match:r1 = C ?
?f, e1,?1, ~?1?r2 = C ?
?f, e2,?2, ~?2?,we pivot over f to create a paraphrase rule rp:rp = C ?
?e1, e2,?p, ~?p?,with a combined nonterminal correspondency func-tion ?p.
Note that the common source side f im-plies that e1 and e2 share the same set of nonterminalsymbols.The paraphrase feature vector ~?p is computedfrom the translation feature vectors ~?1 and ~?2 byfollowing the pivoting idea.
For instance, we esti-mate the conditional paraphrase probability p(e2|e1)by marginalizing over all shared foreign-languagetranslations f :p(e2|e1) =?fp(e2, f |e1) (2)=?fp(e2|f, e1)p(f |e1) (3)?
?fp(e2|f)p(f |e1).
(4)1See Yao et al (2012) for an analysis of this assumption.257twelve cartoons insulting the prophet mohammadCD NNS JJ DTNNPNPNPVPNPDT+NNP12 the prophet mohammadCD NNS JJDTNNPNPNPVPNPDT+NNPcartoons offensiveof the that are toFigure 3: An example of a synchronous paraphras-tic derivation, here a sentence compression.
Shadedwords are deleted in the indicated rule applications.Figure 2 illustrates syntax-constrained pivoting andfeature aggregation over multiple foreign languagetranslations for a paraphrase pattern.After the SCFG has been extracted, it can be usedwithin standard machine translation machinery, suchas the Joshua decoder (Ganitkevitch et al, 2012).Figure 3 shows an example for a synchronous para-phrastic derivation produced as a result of applyingour paraphrase grammar in the decoding process.The approach outlined relies on aligned bilingualtexts to identify phrases and patterns that are equiva-lent in meaning.
When extracting paraphrases frommonolingual text, we have to rely on an entirely dif-ferent set of semantic cues and features.2.2 Monolingual Distributional SimilarityMethods based on monolingual text corpora mea-sure the similarity of phrases based on contextualfeatures.
To describe a phrase e, we define a set offeatures that capture the context of an occurrence ofe in our corpus.
Writing the context vector for thei-th occurrence of e as ~se,i, we can aggregate overall occurrences of e, resulting in a distributional sig-nature for e, ~se =?i ~se,i.
Following the intuitionthat phrases with similar meanings occur in similarcontexts, we can then quantify the goodness of e?
asa paraphrase of e by computing the cosine similaritybetween their distributional signatures:sim(e, e?)
= ~se ?
~se?|~se||~se?
|.A wide variety of features have been used to de-scribe the distributional context of a phrase.
Rich,linguistically informed feature-sets that rely on de-pendency and constituency parses, part-of-speechtags, or lemmatization have been proposed in widelyknown work such as by Church and Hanks (1991)and Lin and Pantel (2001).
For instance, a phraseis described by the various syntactic relations it haswith lexical items in its context, such as: ?for whatverbs do we see with the phrase as the subject?
?, or?what adjectives modify the phrase?
?.However, when moving to vast text collections orcollapsed representations of large text corpora, lin-guistic annotations can become impractically expen-sive to produce.
A straightforward and widely usedsolution is to fall back onto lexical n-gram features,e.g.
?what words or bigrams have we seen to the leftof this phrase??
A substantial body of work has fo-cussed on using this type of feature-set for a varietyof purposes in NLP (Lapata and Keller, 2005; Bha-gat and Ravichandran, 2008; Lin et al, 2010; VanDurme and Lall, 2010).2.3 Other Related WorkRecently, Chan et al (2011) presented an initial in-vestigation into combining phrasal paraphrases ob-tained through bilingual pivoting with monolingualdistributional information.
Their work investigateda reranking approach and evaluated their method viaa substitution task, showing that the two sources ofinformation are complementary and can yield im-provements in paraphrase quality when combined.3 Incorporating Distributional SimilarityIn order to incorporate distributional similarity in-formation into the paraphrasing system, we needto calculate similarity scores for the paraphrasticSCFG rules in our grammar.
For rules with purelylexical right-hand sides e1 and e2 this is a simpletask, and the similarity score sim(e1, e2) can be di-rectly included in the rule?s feature vector ~?.
How-ever, if e1 and e2 are long, their occurrences becomesparse and their similarity can no longer be reliablyestimated.
In our case, the right-hand sides of ourrules often contain gaps and computing a similarityscore is less straightforward.Figure 4 shows an example of such a discontin-uous rule and illustrates our solution: we decom-pose the discontinuous patterns that make up the258NPthe's NPofNPlonglong-termtermtheinNNNNthe long-termin the long term'sof?+ sim?
?
!sim(r) = 12sim?Figure 4: Scoring a rule by extracting and scoringcontiguous phrases consistent with the alignment.The overall score of the rule is determined by av-eraging across all pairs of contiguous subphrases.right-hand sides of a rule r into pairs of contiguousphrases P(r) = {?e, e??
}, for which we can lookup distributional signatures and compute similarityscores.
This decomposition into phrases is non-trivial, since our sentential paraphrase rules ofteninvolve significant reordering or structural changes.To avoid comparing unrelated phrase pairs, we re-quire P(r) to be consistent with a token alignmenta.
The alignment is defined analogously to wordalignments in machine translation, and computed bytreating the source and target sides of our paraphraserules as a parallel corpus.We define the overall similarity score of the ruleto be the average of the similarity scores of all ex-tracted phrase pairs:sim(r,a) = 1|P(a)|?(e,e?
)?P(a)sim(e, e?
).Since the distributional signatures for long, rarephrases may be computed from only a handful ofoccurrences, we additionally query for the shortersub-phrases that are more likely to have been ob-served often enough to have reliable signatures andthus similarity estimates.Our definition of the similarity of two discon-tinuous phrases substantially differs from others inthe literature.
This difference is due to a differ-ence in motivation.
Lin and Pantel (2001), for in-stance, seek to find new paraphrase pairs by compar-ing their arguments.
In this work, however, we tryto add orthogonal information to existing paraphrasepairs.
Both our definition of pattern similarity andour feature-set (see Section 4.3) are therefore gearedtowards comparing the substitutability and contextsimilarity of a pair of paraphrases.Our two similarity scores are incorporated intothe paraphraser as additional rule features in ~?,simngram and simsyn , respectively.
We estimate thecorresponding weights along with the other ?i as de-tailed in Section 4.4 Experimental Setup4.1 Task: Sentence CompressionTo evaluate our method on a real text-to-text appli-cation, we use the sentence compression task.
Totune the parameters of our paraphrase system forsentence compression, we need an appropriate cor-pus of reference compressions.
Since our model isdesigned to compress by paraphrasing rather thandeletion, the commonly used deletion-based com-pression data sets like the Ziff-Davis corpus are notsuitable.
We thus use the dataset introduced in ourprevious work (Ganitkevitch et al, 2011).Beginning with 9570 tuples of parallel English?English sentences obtained from multiple referencetranslations for machine translation evaluation, weconstruct a parallel compression corpus by select-ing the longest reference in each tuple as the sourcesentence and the shortest reference as the target sen-tence.
We further retain only those sentence pairswhere the compression ratio cr falls in the range0.5 < cr ?
0.8.
From these, we select 936 sen-tences for the development set, as well as 560 sen-tences for a test set that we use to gauge the perfor-mance of our system.We contrast our distributional similarity-informedparaphrase system with a pivoting-only baseline, aswell as an implementation of Clarke and Lapata(2008)?s state-of-the-art compression model whichuses a series of constraints in an integer linear pro-gramming (ILP) solver.4.2 Baseline Paraphrase GrammarWe extract our paraphrase grammar from theFrench?English portion of the Europarl corpus (ver-sion 5) (Koehn, 2005).
The Berkeley aligner (Lianget al, 2006) and the Berkeley parser (Petrov andKlein, 2007) are used to align the bitext and parsethe English side, respectively.
The paraphrase gram-mar is produced using the Hadoop-based Thrax259the long-termachieve25goals 23plans 97investment 10confirmed64revise43Left Rightthe long-termthe long-termthe long-termthe long-termthe long-term....L-achieve = 25L-confirmed= 64L-revise = 43?R-goals= 23R-plans  = 97R-investment= 10?the long-term?=~signgram?Figure 5: An example of the n-gram feature extrac-tion on an n-gram corpus.
Here, ?the long-term?
isseen preceded by ?revise?
(43 times) and followedby ?plans?
(97 times).
The corresponding left- andright-side features are added to the phrase signaturewith the counts of the n-grams that gave rise to them.grammar extractor?s paraphrase mode (Ganitkevitchet al, 2012).
The syntactic nonterminal labels weallowed in the grammar were limited to constituentlabels and CCG-style slashed categories.
Paraphrasegrammars extracted via pivoting tend to grow verylarge.
To keep the grammar size manageable, wepruned away all paraphrase rules whose phrasalparaphrase probabilities p(e1|e2) or p(e2|e1) weresmaller than 0.001.We extend the feature-set used in Ganitkevitch etal.
(2011) with a number of features that aim to bet-ter describe a rule?s compressive power: on top ofthe word count features wcountsrc and wcount tgtand the word count difference feature wcountdiff ,we add character based count and difference featuresccountsrc , ccount tgt , and ccountdiff , as well as log-compression ratio features wordcr = log wcount tgtwcountsrcand the analogously defined charcr = log ccount tgtccountsrc .For model tuning and decoding we used theJoshua machine translation system (Ganitkevitch etal., 2012).
The model weights are estimated using animplementation of the PRO tuning algorithm (Hop-kins and May, 2011), with PRE?CIS as our objectivefunction (Ganitkevitch et al, 2011).
The languagemodel used in our paraphraser and the Clarke andLapata (2008) baseline system is a Kneser-Ney dis-counted 5-gram model estimated on the Gigawordcorpus using the SRILM toolkit (Stolcke, 2002).long-term investment holding on todetamodtheJJ NN VBG IN TO DTNPPPVP?
?the long-term?=~sigsyntax?dep-det-R-investmentpos-L-TOpos-R-NNlex-R-investmentlex-L-todep-amod-R-investmentsyn-gov-NP syn-miss-L-NNlex-L-on-topos-L-IN-TOdep-det-R-NN dep-amod-R-NNFigure 6: An example of the syntactic feature-set.
The phrase ?the long-term?
is annotated withposition-aware lexical and part-of-speech n-gramfeatures (e.g.
?on to?
on the left, and ?investment?and ?NN?
to its right), labeled dependency links(e.g.
amod ?
investment) and features derivedfrom the phrase?s CCG label NP/NN .4.3 Distributional Similarity ModelTo investigate the impact of the feature-set used toconstruct distributional signatures, we contrast twoapproaches: a high-coverage collection of distribu-tional signatures with a relatively simple feature-set,and a much smaller set of signatures with a rich, syn-tactically informed feature-set.4.3.1 n-gram ModelThe high-coverage model (from here on: n-grammodel) is drawn from a web-scale n-gram corpus(Brants and Franz, 2006; Lin et al, 2010).
We ex-tract signatures for phrases up to a length of 4.
Foreach phrase p we look at n-grams of the form wpand pv, where w and v are single words.
We thenextract the corresponding features wleft and vright .The feature count is set to the count of the n-gram,reflecting the frequency with which p was precededor followed, respectively, by w and v in the data then-gram corpus is based on.
Figure 5 illustrates thisfeature extraction approach.
The resulting collectioncomprises distributional signatures for the 200 mil-lion most frequent 1-to-4-grams in the n-gram cor-pus.2604.3.2 Syntactic ModelFor the syntactically informed signature model(from here on: syntax model), we use theconstituency and dependency parses provided inthe Annotated Gigaword corpus (Napoles et al,2012).
We limit ourselves to the Los AngelesTimes/Washington Post portion of the corpus andextract phrases up to a length of 4.
The followingfeature set is used to compute distributional signa-tures for the extracted phrases:?
Position-aware lexical and part-of-speech uni-gram and bigram features, drawn from a three-word window to the right and left of the phrase.?
Features based on dependencies for both linksinto and out of the phrase, labeled with the cor-responding lexical item and POS.
If the phrasecorresponds to a complete subtree in the con-stituency parse we additionally include lexicaland POS features for its head word.?
Syntactic features for any constituents govern-ing the phrase, as well as for CCG-style slashedconstituent labels for the phrase.
The latter aresplit in governing constituent and missing con-stituent (with directionality).Figure 6 illustrates the syntax model?s feature ex-traction for an example phrase occurrence.
Usingthis method we extract distributional signatures forover 12 million 1-to-4-gram phrases.4.3.3 Locality Sensitive HashingCollecting distributional signatures for a largenumber of phrases quickly leads to unmanageablylarge datasets.
Storing the syntax model?s 12 mil-lion signatures in a compressed readable format,for instance, requires over 20GB of disk space.Like Ravichandran et al (2005) and Bhagat andRavichandran (2008), we rely on locality sensitivehashing (LSH) to make the use of these large collec-tions practical.In order to avoid explicitly computing the fea-ture vectors, which can be memory intensive for fre-quent phrases, we chose the online LSH variant de-scribed by Van Durme and Lall (2010), as imple-mented in the Jerboa toolkit (Van Durme, 2012).This method, based on the earlier work of Indyk andMotwani (1998) and Charikar (2002), approximatesthe cosine similarity between two feature vectorsbased on the Hamming distance in a dimensionality-reduced bitwise representation.
Two feature vec-tors u, v each of dimension d are first projectedthrough a d?b random matrix populated with drawsfrom N (0, 1).
We then convert the resulting b-dimensional vectors into bit-vectors by setting eachbit of the signature conditioned on whether the cor-responding projected value is less than 0.
Now,given the bit signatures h(~u) and h(~v), we can ap-proximate the cosine similarity of u and v as:sim ?
(u, v) = cos(D(h(~u), h(~v))b pi),where d(?, ?)
is the Hamming distance.
In our ex-periments we use 256-bit signatures.
This reducesthe memory requirements for the syntax model toaround 600MB.5 Evaluation ResultsTo rate the quality of our output, we solicit humanjudgments of the compressions along two five-pointscales: grammaticality and meaning preservation.Judges are instructed to decide how much the mean-ing from a reference translation is retained in thecompressed sentence, with a score of 5 indicatingthat all of the important information is present, and1 being that the compression does not retain any ofthe original meaning.
Similarly, a grammar scoreof 5 indicates perfect grammaticality, while a scoreof 1 is assigned to sentences that are entirely un-grammatical.
We ran our evaluation on Mechani-cal Turk, where a total of 126 judges provided 3 re-dundant judgments for each system output.
To pro-vide additional quality control, our HITs were aug-mented with both positive and negative control com-pressions.
For the positive control we used the refer-ence compressions from our test set.
Negative con-trol was provided by adding a compression modelbased on random word deletions to the mix.In Table 1 we compare our distributionalsimilarity-augmented systems to the plain pivoting-based baseline and the ILP approach.
The compres-sion ratios of the paraphrasing systems are tuned tomatch the average compression ratio seen on the de-velopment and test set.
The ILP system is config-261ured to loosely match this ratio, as to not overly con-strain its search space.
Our results indicate that theparaphrase approach significantly outperforms ILPon meaning retention.
However, the baseline sys-tem shows notable weaknesses in grammaticality.Adding the n-gram distributional similarity modelto the paraphraser recovers some of the difference ingrammaticality while simultaneously yielding somegain in the compressions?
meaning retention.
Mov-ing to distributional similarity estimated on the syn-tactic feature-set yields additional improvement, de-spite the model?s lower coverage.It is known that human evaluation scores correlatelinearly with the compression ratio produced by asentence compression system (Napoles et al, 2011).Thus, to ensure fairness in our comparisons, we pro-duce a pairwise comparison breakdown that onlytakes into account compressions of almost identicallength.2 Figure 7 shows the results of this analysis,detailing the number of wins and ties in the humanjudgements.We note that the gains in meaning retention overboth the baseline and the ILP system are still presentin the pairwise breakdown.
The gains over theparaphrasing baseline, as well as the improvementin meaning over ILP are statistically significant atp < 0.05 (using the sign test).We can observe that there is substantial overlapbetween the baseline paraphraser and the n-grammodel, while the syntax model appears to yield no-ticeably different output far more often.Table 2 shows two example sentences drawn fromour test set and the compressions produced by thedifferent systems.
It can be seen that both theparaphrase-based and ILP systems produce goodquality results, with the paraphrase system retainingthe meaning of the source sentence more accurately.6 ConclusionWe presented a method to incorporate monolingualdistributional similarity into linguistically informedparaphrases extracted from bilingual parallel data.Having extended the notion of similarity to dis-contiguous pattern with multi-word gaps, we inves-tigated the effect of using feature-sets of varying2We require the compressions to be within ?10% length ofone another.Score 050100150200250300050100150200250300Syntax :: ILP Syntax :: n?gram n?gram :: PPGrammarMeaningFigure 7: A pairwise breakdown of the human judg-ments comparing the systems.
Dark grey regionsshow the number of times the two systems were tied,and light grey shows how many times one systemwas judged to be better than the other.CR Meaning GrammarReference 0.80 4.80 4.54ILP 0.74 3.44 3.41PP 0.78 3.53 2.98PP + n-gram 0.80 3.65 3.16PP + syntax 0.79 3.70 3.26Random Deletions 0.78 2.91 2.53Table 1: Results of the human evaluation on longercompressions: pairwise compression rates (CR),meaning and grammaticality scores.
Bold indicatesa statistically significance difference at p < 0.05.complexity to compute distributional similarity forour paraphrase collection.
We conclude that, com-pared to a simple large-scale model, a rich, syntax-based feature-set, even with significantly lower cov-erage, noticeably improves output quality in a text-to-text generation task.
Our syntactic method sig-nificantly improves grammaticality and meaning re-tention over a strong paraphrastic baseline, and of-fers substantial gains in meaning retention over adeletion-based state-of-the-art system.Acknowledgements This research was supportedin part by the NSF under grant IIS-0713448 andin part by the EuroMatrixPlus project funded bythe European Commission (7th Framework Pro-gramme).
Opinions, interpretations, and conclu-sions are the authors?
alone.262Source should these political developments have an impact on sports ?Reference should these political events affect sports ?Syntax should these events have an impact on sports ?n-gram these political developments impact on sports ?PP should these events impact on sports ?ILP political developments have an impactSource now we have to think and make a decision about our direction and choose only one way .thanks .Reference we should ponder it and decide our path and follow it , thanks .Syntax now we think and decide on our way and choose one way .
thanks .n-gram now we have and decide on our way and choose one way .
thanks .PP now we have and decide on our way and choose one way .
thanks .ILP we have to think and make a decision and choose way thanksTable 2: Example compressions produced by our systems and the baselines Table 1 for three input sentencesfrom our test data.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice Hall.Peter G. Anick and Suresh Tipirneni.
1999.
The para-phrase search assistant: terminological feedback foriterative information seeking.
In Proceedings of SI-GIR.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.Regina Barzilay, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the contextof multi-document summarization.
In Proceedings ofACL.Regina Barzilay.
2003.
Information Fusion for Mutli-document Summarization: Paraphrasing and Genera-tion.
Ph.D. thesis, Columbia University, New York.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL/HLT.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.Tsz Ping Chan, Chris Callison-Burch, and Benjamin VanDurme.
2011.
Reranking bilingually extracted para-phrases using monolingual distributional similarity.
InEMNLP Workshop on GEMS.Moses Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In Proceedings of STOC.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL.Kenneth Church and Patrick Hanks.
1991.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 6(1):22?29.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:273?381.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings of theCOLING.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
In Proceedings of EMNLP.Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,263and Chris Callison-Burch.
2012.
Joshua 4.0: Packing,PRO, and paraphrases.
In Proceedings of WMT12.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of EMNLP.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In Proceedings of STOC.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT summit, volume 5.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Mirella Lapata and Frank Keller.
2005.
Web-based mod-els for natural language processing.
ACM Transac-tions on Speech and Language Processing, 2(1).Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT/NAACL.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules from text.
Natural Language Engineering.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scalen-grams.
In Proceedings of LREC.Kathleen R. McKeown.
1979.
Paraphrasing using givenand new information in a question-answer system.
InProceedings of ACL.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011.
Paraphrasticsentence compression with a character-based metric:Tightening without deletion.
Workshop on Monolin-gual Text-To-Text Generation.Courtney Napoles, Matt Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In Proceedingsof AKBC-WEKEX 2012.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofHLT/NAACL.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsufrace text patterns for a question answering system.In Proceedings of ACL.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized Algorithms and NLP: Using Lo-cality Sensitive Hash Functions for High Speed NounClustering.
In Proceedings of ACL.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of ACL.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceeding of the InternationalConference on Spoken Language Processing.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of ACL, Short Papers.Benjamin Van Durme.
2012.
Jerboa: A toolkit forrandomized and streaming algorithms.
Technical Re-port 7, Human Language Technology Center of Excel-lence, Johns Hopkins University.Sander Wubben, Antal van den Bosch, and Emiel Krah-mer.
2012.
Sentence simplification by monolingualmachine translation.
In Proceedings of ACL.Xuchen Yao, Benjamin Van Durme, and Chris Callison-Burch.
2012.
Expectations of word sense in parallelcorpora.
In Proceedings of HLT/NAACL.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008.
Pivot approach for extracting paraphrase pat-terns from bilingual corpora.
In Proceedings ofACL/HLT.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.In Proceedings of ACL.264
