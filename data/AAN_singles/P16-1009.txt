Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 86?96,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsImproving Neural Machine Translation Models with Monolingual DataRico Sennrich and Barry Haddow and Alexandra BirchSchool of Informatics, University of Edinburgh{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.ukAbstractNeural Machine Translation (NMT) hasobtained state-of-the art performance forseveral language pairs, while only us-ing parallel data for training.
Target-side monolingual data plays an impor-tant role in boosting fluency for phrase-based statistical machine translation, andwe investigate the use of monolingual datafor NMT.
In contrast to previous work,which combines NMT models with sep-arately trained language models, we notethat encoder-decoder NMT architecturesalready have the capacity to learn the sameinformation as a language model, and weexplore strategies to train with monolin-gual data without changing the neural net-work architecture.
By pairing monolin-gual training data with an automatic back-translation, we can treat it as additionalparallel training data, and we obtain sub-stantial improvements on the WMT 15task English?German (+2.8?3.7 BLEU),and for the low-resourced IWSLT 14 taskTurkish?English (+2.1?3.4 BLEU), ob-taining new state-of-the-art results.
Wealso show that fine-tuning on in-domainmonolingual and parallel data gives sub-stantial improvements for the IWSLT 15task English?German.1 IntroductionNeural Machine Translation (NMT) has obtainedstate-of-the art performance for several languagepairs, while only using parallel data for training.Target-side monolingual data plays an importantrole in boosting fluency for phrase-based statisti-The research presented in this publication was conductedin cooperation with Samsung Electronics Polska sp.
z o.o.
-Samsung R&D Institute Poland.cal machine translation, and we investigate the useof monolingual data for NMT.Language models trained on monolingual datahave played a central role in statistical machinetranslation since the first IBM models (Brown etal., 1990).
There are two major reasons for theirimportance.
Firstly, word-based and phrase-basedtranslation models make strong independence as-sumptions, with the probability of translation unitsestimated independently from context, and lan-guage models, by making different independenceassumptions, can model how well these translationunits fit together.
Secondly, the amount of avail-able monolingual data in the target language typi-cally far exceeds the amount of parallel data, andmodels typically improve when trained on moredata, or data more similar to the translation task.In (attentional) encoder-decoder architecturesfor neural machine translation (Sutskever et al,2014; Bahdanau et al, 2015), the decoder is es-sentially an RNN language model that is also con-ditioned on source context, so the first rationale,adding a language model to compensate for the in-dependence assumptions of the translation model,does not apply.
However, the data argument is stillvalid in NMT, and we expect monolingual data tobe especially helpful if parallel data is sparse, ora poor fit for the translation task, for instance be-cause of a domain mismatch.In contrast to previous work, which integratesa separately trained RNN language model into theNMT model (G?l?ehre et al, 2015), we explorestrategies to include monolingual training data inthe training process without changing the neuralnetwork architecture.
This makes our approachapplicable to different NMT architectures.The main contributions of this paper are as fol-lows:?
we show that we can improve the machinetranslation quality of NMT systems by mix-ing monolingual target sentences into the86training set.?
we investigate two different methods to fillthe source side of monolingual training in-stances: using a dummy source sentence, andusing a source sentence obtained via back-translation, which we call synthetic.
We findthat the latter is more effective.?
we successfully adapt NMT models to a newdomain by fine-tuning with either monolin-gual or parallel in-domain data.2 Neural Machine TranslationWe follow the neural machine translation archi-tecture by Bahdanau et al (2015), which we willbriefly summarize here.
However, we note that ourapproach is not specific to this architecture.The neural machine translation system is imple-mented as an encoder-decoder network with recur-rent neural networks.The encoder is a bidirectional neural networkwith gated recurrent units (Cho et al, 2014)that reads an input sequence x = (x1, ..., xm)and calculates a forward sequence of hiddenstates (?
?h1, ...,?
?hm), and a backward sequence(?
?h1, ...,??hm).
The hidden states??hjand?
?hjareconcatenated to obtain the annotation vector hj.The decoder is a recurrent neural network thatpredicts a target sequence y = (y1, ..., yn).
Eachword yiis predicted based on a recurrent hiddenstate si, the previously predicted word yi?1, anda context vector ci.
ciis computed as a weightedsum of the annotations hj.
The weight of eachannotation hjis computed through an alignmentmodel ?ij, which models the probability that yiisaligned to xj.
The alignment model is a single-layer feedforward neural network that is learnedjointly with the rest of the network through back-propagation.A detailed description can be found in (Bah-danau et al, 2015).
Training is performed on aparallel corpus with stochastic gradient descent.For translation, a beam search with small beamsize is employed.3 NMT Training with MonolingualTraining DataIn machine translation, more monolingual data(or monolingual data more similar to the test set)serves to improve the estimate of the prior prob-ability p(T ) of the target sentence T , before tak-ing the source sentence S into account.
In con-trast to (G?l?ehre et al, 2015), who train separatelanguage models on monolingual training data andincorporate them into the neural network throughshallow or deep fusion, we propose techniquesto train the main NMT model with monolingualdata, exploiting the fact that encoder-decoder neu-ral networks already condition the probability dis-tribution of the next target word on the previoustarget words.
We describe two strategies to do this:providing monolingual training examples with anempty (or dummy) source sentence, or providingmonolingual training data with a synthetic sourcesentence that is obtained from automatically trans-lating the target sentence into the source language,which we will refer to as back-translation.3.1 Dummy Source SentencesThe first technique we employ is to treat monolin-gual training examples as parallel examples withempty source side, essentially adding training ex-amples whose context vector ciis uninformative,and for which the network has to fully rely onthe previous target words for its prediction.
Thiscould be conceived as a form of dropout (Hintonet al, 2012), with the difference that the train-ing instances that have the context vector droppedout constitute novel training data.
We can alsoconceive of this setup as multi-task learning, withthe two tasks being translation when the sourceis known, and language modelling when it is un-known.During training, we use both parallel and mono-lingual training examples in the ratio 1-to-1, andrandomly shuffle them.
We define an epoch as oneiteration through the parallel data set, and resam-ple from the monolingual data set for every epoch.We pair monolingual sentences with a single-worddummy source side <null> to allow processing ofboth parallel and monolingual training exampleswith the same network graph.1For monolingualminibatches2, we freeze the network parametersof the encoder and the attention model.One problem with this integration of monolin-1One could force the context vector cito be 0 for monolin-gual training instances, but we found that this does not solvethe main problem with this approach, discussed below.2For efficiency, Bahdanau et al (2015) sort sets of 20minibatches according to length.
This also groups monolin-gual training instances together.87gual data is that we cannot arbitrarily increase theratio of monolingual training instances, or fine-tune a model with only monolingual training data,because different output layer parameters are opti-mal for the two tasks, and the network ?unlearns?its conditioning on the source context if the ratioof monolingual training instances is too high.3.2 Synthetic Source SentencesTo ensure that the output layer remains sensitive tothe source context, and that good parameters arenot unlearned from monolingual data, we proposeto pair monolingual training instances with a syn-thetic source sentence from which a context vec-tor can be approximated.
We obtain these throughback-translation, i.e.
an automatic translation ofthe monolingual target text into the source lan-guage.During training, we mix synthetic parallel textinto the original (human-translated) parallel textand do not distinguish between the two: no net-work parameters are frozen.
Importantly, only thesource side of these additional training examplesis synthetic, and the target side comes from themonolingual corpus.4 EvaluationWe evaluate NMT training on parallel text,and with additional monolingual data, onEnglish?German and Turkish?English,using training and test data from WMT15 for English?German, IWSLT 15 forEnglish?German, and IWSLT 14 forTurkish?English.4.1 Data and MethodsWe use Groundhog3as the implementation of theNMT system for all experiments (Bahdanau et al,2015; Jean et al, 2015a).
We generally follow thesettings and training procedure described by Sen-nrich et al (2016).For English?German, we report case-sensitiveBLEU on detokenized text with mteval-v13a.pl forcomparison to official WMT and IWSLT results.For Turkish?English, we report case-sensitiveBLEU on tokenized text with multi-bleu.perl forcomparison to results by G?l?ehre et al (2015).G?l?ehre et al (2015) determine the networkvocabulary based on the parallel training data,3github.com/sebastien-j/LV_groundhogdataset sentencesWMTparallel4 200 000WITparallel200 000WMTmono_de160 000 000WMTsynth_de3 600 000WMTmono_en118 000 000WMTsynth_en4 200 000Table 1: English?German training data.and replace out-of-vocabulary words with a spe-cial UNK symbol.
They remove monolingual sen-tences with more than 10% UNK symbols.
In con-trast, we represent unseen words as sequences ofsubword units (Sennrich et al, 2016), and can rep-resent any additional training data with the exist-ing network vocabulary that was learned on theparallel data.
In all experiments, the network vo-cabulary remains fixed.4.1.1 English?GermanWe use all parallel training data provided by WMT2015 (Bojar et al, 2015)4.
We use the News Crawlcorpora as additional training data for the exper-iments with monolingual data.
The amount oftraining data is shown in Table 1.Baseline models are trained for a week.
Ensem-bles are sampled from the last 4 saved models oftraining (saved at 12h-intervals).
Each model isfine-tuned with fixed embeddings for 12 hours.For the experiments with synthetic paralleldata, we back-translate a random sample of3 600 000 sentences from the German monolin-gual data set into English.
The German?Englishsystem used for this is the baseline system(parallel).
Translation took about a week onan NVIDIA Titan Black GPU.
For experimentsin German?English, we back-translate 4 200 000monolingual English sentences into German, us-ing the English?German system +synthetic.Note that we always use single models for back-translation, not ensembles.
We leave it to fu-ture work to explore how sensitive NMT trainingwith synthetic data is to the quality of the back-translation.We tokenize and truecase the training data, andrepresent rare words via BPE (Sennrich et al,2016).
Specifically, we follow Sennrich et al(2016) in performing BPE on the joint vocabularywith 89 500 merge operations.
The network vo-4http://www.statmt.org/wmt15/88dataset sentencesWIT 160 000SETimes 160 000Gigawordmono177 000 000Gigawordsynth3 200 000Table 2: Turkish?English training data.cabulary size is 90 000.We also perform experiments on the IWSLT15 test sets to investigate a cross-domain setting.5The test sets consist of TED talk transcripts.
As in-domain training data, IWSLT provides the WIT3parallel corpus (Cettolo et al, 2012), which alsoconsists of TED talks.4.1.2 Turkish?EnglishWe use data provided for the IWSLT 14 machinetranslation track (Cettolo et al, 2014), namely theWIT3parallel corpus (Cettolo et al, 2012), whichconsists of TED talks, and the SETimes corpus(Tyers and Alperen, 2010).6After removal ofsentence pairs which contain empty lines or lineswith a length ratio above 9, we retain 320 000 sen-tence pairs of training data.
For the experimentswith monolingual training data, we use the En-glish LDC Gigaword corpus (Fifth Edition).
Theamount of training data is shown in Table 2.
Withonly 320 000 sentences of parallel data availablefor training, this is a much lower-resourced trans-lation setting than English?German.G?l?ehre et al (2015) segment the Turkish textwith the morphology tool Zemberek, followed bya disambiguation of the morphological analysis(Sak et al, 2007), and removal of non-surface to-kens produced by the analysis.
We use the samepreprocessing7.
For both Turkish and English, werepresent rare words (or morphemes in the case ofTurkish) as character bigram sequences (Sennrichet al, 2016).
The 20 000 most frequent words(morphemes) are left unsegmented.
The networkshave a vocabulary size of 23 000 symbols.To obtain a synthetic parallel trainingset, we back-translate a random sample of3 200 000 sentences from Gigaword.
We use anEnglish?Turkish NMT system trained with thesame settings as the Turkish?English baselinesystem.5http://workshop2015.iwslt.org/6http://workshop2014.iwslt.org/7github.com/orhanf/zemberekMorphTRWe found overfitting to be a bigger problemthan with the larger English?German data set,and follow G?l?ehre et al (2015) in using Gaus-sian noise (stddev 0.01) (Graves, 2011), anddropout on the output layer (p=0.5) (Hinton et al,2012).
We also use early stopping, based on BLEUmeasured every three hours on tst2010, which wetreat as development set.
For Turkish?English,we use gradient clipping with threshold 5, follow-ing G?l?ehre et al (2015), in contrast to the thresh-old 1 that we use for English?German, followingJean et al (2015a).4.2 Results4.2.1 English?German WMT 15Table 3 shows English?German results withWMT training and test data.
We find that mixingparallel training data with monolingual data with adummy source side in a ratio of 1-1 improves qual-ity by 0.4?0.5 BLEU for the single system, 1 BLEUfor the ensemble.
We train the system for twiceas long as the baseline to provide the training al-gorithm with a similar amount of parallel traininginstances.
To ensure that the quality improvementis due to the monolingual training instances, andnot just increased training time, we also continuedtraining our baseline system for another week, butsaw no improvements in BLEU.Including synthetic data during training is veryeffective, and yields an improvement over ourbaseline by 2.8?3.4 BLEU.
Our best ensemblesystem also outperforms a syntax-based baseline(Sennrich and Haddow, 2015) by 1.2?2.1 BLEU.We also substantially outperform NMT results re-ported by Jean et al (2015a) and Luong et al(2015), who previously reported SOTA result.8We note that the difference is particularly largefor single systems, since our ensemble is not asdiverse as that of Luong et al (2015), who used8 independently trained ensemble components,whereas we sampled 4 ensemble components fromthe same training run.4.2.2 English?German IWSLT 15Table 4 shows English?German results onIWSLT test sets.
IWSLT test sets consist of TEDtalks, and are thus very dissimilar from the WMT8Luong et al (2015) report 20.9 BLEU (tokenized) onnewstest2014 with a single model, and 23.0 BLEU with anensemble of 8 models.
Our best single system achieves a to-kenized BLEU (as opposed to untokenized scores reported inTable 3) of 23.8, and our ensemble reaches 25.0 BLEU.89BLEUname training instances newstest2014 newstest2015single ens-4 single ens-4syntax-based (Sennrich and Haddow, 2015) 22.6 - 24.4 -Neural MT (Jean et al, 2015b) - - 22.4 -parallel 37m (parallel) 19.9 20.4 22.8 23.6+monolingual 49m (parallel) / 49m (monolingual) 20.4 21.4 23.2 24.6+synthetic 44m (parallel) / 36m (synthetic) 22.7 23.8 25.7 26.5Table 3: English?German translation performance (BLEU) on WMT training/test sets.
Ens-4: ensembleof 4 models.
Number of training instances varies due to differences in training time and speed.name fine-tuning BLEUdata instances tst2013 tst2014 tst2015NMT (Luong and Manning, 2015) (single model) 29.4 - -NMT (Luong and Manning, 2015) (ensemble of 8) 31.4 27.6 30.11 parallel - - 25.2 22.6 24.02 +synthetic - - 26.5 23.5 25.53 2+WITmono_deWMTparallel/ WITmono200k/200k 26.6 23.6 25.44 2+WITsynth_deWITsynth200k 28.2 24.4 26.75 2+WITparallelWIT 200k 30.4 25.9 28.4Table 4: English?German translation performance (BLEU) on IWSLT test sets (TED talks).
Singlemodels.test sets, which are news texts.
We investigate ifmonolingual training data is especially valuable ifit can be used to adapt a model to a new genre ordomain, specifically adapting a system trained onWMT data to translating TED talks.Systems 1 and 2 correspond to systems in Table3, trained only on WMT data.
System 2, trained onparallel and synthetic WMT data, obtains a BLEUscore of 25.5 on tst2015.
We observe that even asmall amount of fine-tuning9, i.e.
continued train-ing of an existing model, on WIT data can adapta system trained on WMT data to the TED do-main.
By back-translating the monolingual WITcorpus (using a German?English system trainedon WMT data, i.e.
without in-domain knowledge),we obtain the synthetic data set WITsynth.
A sin-gle epoch of fine-tuning on WITsynth(system 4) re-sults in a BLEU score of 26.7 on tst2015, or an im-provement of 1.2 BLEU.
We observed no improve-ment from fine-tuning on WITmono, the monolin-gual TED corpus with dummy input (system 3).These adaptation experiments with monolin-gual data are slightly artificial in that parallel train-ing data is available.
System 5, which is fine-tuned with the original WIT training data, obtainsa BLEU of 28.4 on tst2015, which is an improve-9We leave the word embeddings fixed for fine-tuning.BLEUname 2014 2015PBSMT (Haddow et al, 2015) 28.8 29.3NMT (G?l?ehre et al, 2015) 23.6 -+shallow fusion 23.7 -+deep fusion 24.0 -parallel 25.9 26.7+synthetic 29.5 30.4+synthetic (ensemble of 4) 30.8 31.6Table 5: German?English translation perfor-mance (BLEU) on WMT training/test sets (new-stest2014; newstest2015).ment of 2.9 BLEU.
While it is unsurprising thatin-domain parallel data is most valuable, we findit encouraging that NMT domain adaptation withmonolingual data is also possible, and effective,since there are settings where only monolingualin-domain data is available.The best results published on this dataset areby Luong and Manning (2015), obtained with anensemble of 8 independently trained models.
Ina comparison of single-model results, we outper-form their model on tst2013 by 1 BLEU.904.2.3 German?English WMT 15Results for German?English on the WMT 15data sets are shown in Table 5.
Like for the re-verse translation direction, we see substantial im-provements (3.6?3.7 BLEU) from adding mono-lingual training data with synthetic source sen-tences, which is substantially bigger than the im-provement observed with deep fusion (G?l?ehre etal., 2015); our ensemble outperforms the previousstate of the art on newstest2015 by 2.3 BLEU.4.2.4 Turkish?English IWSLT 14Table 6 shows results for Turkish?English.
Onaverage, we see an improvement of 0.6 BLEU onthe test sets from adding monolingual data with adummy source side in a 1-1 ratio10, although wenote a high variance between different test sets.With synthetic training data (Gigawordsynth), weoutperform the baseline by 2.7 BLEU on average,and also outperform results obtained via shallowor deep fusion by G?l?ehre et al (2015) by 0.5BLEU on average.
To compare to what extent syn-thetic data has a regularization effect, even withoutnovel training data, we also back-translate the tar-get side of the parallel training text to obtain thetraining corpus parallelsynth.
Mixing the originalparallel corpus with parallelsynth(ratio 1-1) givessome improvement over the baseline (1.7 BLEUon average), but the novel monolingual trainingdata (Gigawordmono) gives higher improvements,despite being out-of-domain in relation to the testsets.
We speculate that novel in-domain monolin-gual data would lead to even higher improvements.4.2.5 Back-translation Quality for SyntheticDataOne question that our previous experiments leaveopen is how the quality of the automatic back-translation affects training with synthetic data.
Toinvestigate this question, we back-translate thesame German monolingual corpus with three dif-ferent German?English systems:?
with our baseline system and greedy decod-ing?
with our baseline system and beam search(beam size 12).
This is the same system usedfor the experiments in Table 3.10We also experimented with higher ratios of monolingualdata, but this led to decreased BLEU scores.BLEUDE?EN EN?DEback-translation 2015 2014 2015none - 20.4 23.6parallel (greedy) 22.3 23.2 26.0parallel (beam 12) 25.0 23.8 26.5synthetic (beam 12) 28.3 23.9 26.6ensemble of 3 - 24.2 27.0ensemble of 12 - 24.7 27.6Table 7: English?German translation perfor-mance (BLEU) on WMT training/test sets (new-stest2014; newstest2015).
Systems differ in howthe synthetic training data is obtained.
Ensemblesof 4 models (unless specified otherwise).?
with the German?English system that wasitself trained with synthetic data (beam size12).BLEU scores of the German?English sys-tems, and of the resulting English?German sys-tems that are trained on the different back-translations, are shown in Table 7.
The qualityof the German?English back-translation differssubstantially, with a difference of 6 BLEU on new-stest2015.
Regarding the English?German sys-tems trained on the different synthetic corpora, wefind that the 6 BLEU difference in back-translationquality leads to a 0.6?0.7 BLEU difference intranslation quality.
This is balanced by the factthat we can increase the speed of back-translationby trading off some quality, for instance by reduc-ing beam size, and we leave it to future researchto explore how much the amount of synthetic dataaffects translation quality.We also show results for an ensemble of 3 mod-els (the best single model of each training run),and 12 models (all 4 models of each training run).Thanks to the increased diversity of the ensemblecomponents, these ensembles outperform the en-sembles of 4 models that were all sampled fromthe same training run, and we obtain another im-provement of 0.8?1.0 BLEU.4.3 Contrast to Phrase-based SMTThe back-translation of monolingual target datainto the source language to produce synthetic par-allel text has been previously explored for phrase-based SMT (Bertoldi and Federico, 2009; Lambertet al, 2011).
While our approach is technicallysimilar, synthetic parallel data fulfills novel roles91name training BLEUdata instances tst2011 tst2012 tst2013 tst2014baseline (G?l?ehre et al, 2015) 18.4 18.8 19.9 18.7deep fusion (G?l?ehre et al, 2015) 20.2 20.2 21.3 20.6baseline parallel 7.2m 18.6 18.2 18.4 18.3parallelsynthparallel/parallelsynth6m/6m 19.9 20.4 20.1 20.0Gigawordmonoparallel/Gigawordmono7.6m/7.6m 18.8 19.6 19.4 18.2Gigawordsynthparallel/Gigawordsynth8.4m/8.4m 21.2 21.1 21.8 20.4Table 6: Turkish?English translation performance (tokenized BLEU) on IWSLT test sets (TED talks).Single models.
Number of training instances varies due to early stopping.system BLEUWMT IWSLTparallel 20.1 21.5+synthetic 20.8 21.6PBSMT gain +0.7 +0.1NMT gain +2.9 +1.2Table 8: Phrase-based SMT results(English?German) on WMT test sets (aver-age of newstest201{4,5}), and IWSLT test sets(average of tst201{3,4,5}), and average BLEUgain from adding synthetic data for both PBSMTand NMT.in NMT.To explore the relative effectiveness of back-translated data for phrase-based SMT andNMT, we train two phrase-based SMT systemswith Moses (Koehn et al, 2007), using onlyWMTparallel, or both WMTparalleland WMTsynth_defor training the translation and reordering model.Both systems contain the same language model,a 5-gram Kneser-Ney model trained on all avail-able WMT data.
We use the baseline featuresdescribed by Haddow et al (2015).Results are shown in Table 8.
In phrase-basedSMT, we find that the use of back-translated train-ing data has a moderate positive effect on theWMT test sets (+0.7 BLEU), but not on the IWSLTtest sets.
This is in line with the expectation thatthe main effect of back-translated data for phrase-based SMT is domain adaptation (Bertoldi andFederico, 2009).
Both the WMT test sets and theNews Crawl corpora which we used as monolin-gual data come from the same source, a web crawlof newspaper articles.11In contrast, News Crawlis out-of-domain for the IWSLT test sets.In contrast to phrase-based SMT, which can11The WMT test sets are held-out from News Crawl.0 5 10 15 20 25 302468training time (training instances ?106)cross-entropyparallel (dev)parallel (train)parallelsynth(dev)parallelsynth(train)Gigawordmono(dev)Gigawordmono(train)Gigawordsynth(dev)Gigawordsynth(train)Figure 1: Turkish?English training and develop-ment set (tst2010) cross-entropy as a function oftraining time (number of training instances) fordifferent systems.make use of monolingual data via the languagemodel, NMT has so far not been able to use mono-lingual data to great effect, and without requir-ing architectural changes.
We find that the effectof synthetic parallel data is not limited to domainadaptation, and that even out-of-domain syntheticdata improves NMT quality, as in our evaluationon IWSLT.
The fact that the synthetic data is moreeffective on the WMT test sets (+2.9 BLEU) thanon the IWSLT test sets (+1.2 BLEU) supports thehypothesis that domain adaptation contributes tothe effectiveness of adding synthetic data to NMTtraining.It is an important finding that back-translateddata, which is mainly effective for domain adapta-tion in phrase-based SMT, is more generally use-ful in NMT, and has positive effects that go beyonddomain adaptation.
In the next section, we will in-vestigate further reasons for its effectiveness.920 20 40 60 802468training time (training instances ?106)cross-entropyWMTparallel(dev)WMTparallel(train)WMTsynth(dev)WMTsynth(train)Figure 2: English?German training and develop-ment set (newstest2013) cross-entropy as a func-tion of training time (number of training instances)for different systems.4.4 AnalysisWe previously indicated that overfitting is a con-cern with our baseline system, especially on smalldata sets of several hundred thousand trainingsentences, despite the regularization employed.This overfitting is illustrated in Figure 1, whichplots training and development set cross-entropyby training time for Turkish?English models.For comparability, we measure training set cross-entropy for all models on the same random sam-ple of the parallel training set.
We can seethat the model trained on only parallel train-ing data quickly overfits, while all three mono-lingual data sets (parallelsynth, Gigawordmono, orGigawordsynth) delay overfitting, and give bet-ter perplexity on the development set.
Thebest development set cross-entropy is reached byGigawordsynth.Figure 2 shows cross-entropy forEnglish?German, comparing the system trainedon only parallel data and the system that includessynthetic training data.
Since more training data isavailable for English?German, there is no indi-cation that overfitting happens during the first 40million training instances (or 7 days of training);while both systems obtain comparable trainingset cross-entropies, the system with synthetic datareaches a lower cross-entropy on the developmentset.
One explanation for this is the domain effectdiscussed in the previous section.A central theoretical expectation is that mono-lingual target-side data improves the model?s flu-system produced attested naturalparallel 1078 53.4% 74.9%+mono 994 61.6% 84.6%+synthetic 1217 56.4% 82.5%Table 9: Number of words in system out-put that do not occur in parallel training data(countref= 1168), and proportion that is attestedin data, or natural according to native speaker.English?German; newstest2015; ensemble sys-tems.ency, its ability to produce natural target-languagesentences.
As a proxy to sentence-level flu-ency, we investigate word-level fluency, specif-ically words produced as sequences of subwordunits, and whether NMT systems trained with ad-ditional monolingual data produce more naturalwords.
For instance, the English?German sys-tems translate the English phrase civil rights pro-tections as a single compound, composed of threesubword units: B?rger|rechts|schutzes12, and weanalyze how many of these multi-unit words thatthe translation systems produce are well-formedGerman words.We compare the number of words in the systemoutput for the newstest2015 test set which are pro-duced via subword units, and that do not occur inthe parallel training corpus.
We also count howmany of them are attested in the full monolingualcorpus or the reference translation, which we allconsider ?natural?.
Additionally, the main authors,a native speaker of German, annotated a randomsubset (n = 100) of unattested words of each sys-tem according to their naturalness13, distinguish-ing between natural German words (or names)such as Literatur|klassen ?literature classes?, andnonsensical ones such as *As|best|atten (a miss-spelling of Astbestmatten ?asbestos mats?
).In the results (Table 9), we see that the sys-tems trained with additional monolingual or syn-thetic data have a higher proportion of novel wordsattested in the non-parallel data, and a higherproportion that is deemed natural by our annota-tor.
This supports our expectation that additionalmonolingual data improves the (word-level) flu-ency of the NMT system.12Subword boundaries are marked with ?|?.13For the annotation, the words were blinded regarding thesystem that produced them.935 Related WorkTo our knowledge, the integration of monolingualdata for pure neural machine translation architec-tures was first investigated by (G?l?ehre et al,2015), who train monolingual language models in-dependently, and then integrate them during de-coding through rescoring of the beam (shallow fu-sion), or by adding the recurrent hidden state ofthe language model to the decoder state of theencoder-decoder network, with an additional con-troller mechanism that controls the magnitude ofthe LM signal (deep fusion).
In deep fusion, thecontroller parameters and output parameters aretuned on further parallel training data, but the lan-guage model parameters are fixed during the fine-tuning stage.
Jean et al (2015b) also report onexperiments with reranking of NMT output witha 5-gram language model, but improvements aresmall (between 0.1?0.5 BLEU).The production of synthetic parallel texts bearsresemblance to data augmentation techniques usedin computer vision, where datasets are often aug-mented with rotated, scaled, or otherwise distortedvariants of the (limited) training set (Rowley et al,1996).Another similar avenue of research is self-training (McClosky et al, 2006; Schwenk, 2008).The main difference is that self-training typicallyrefers to scenario where the training set is en-hanced with training instances with artificiallyproduced output labels, whereas we start withhuman-produced output (i.e.
the translation), andartificially produce an input.
We expect that thisis more robust towards noise in the automatictranslation.
Improving NMT with monolingualsource data, following similar work on phrase-based SMT (Schwenk, 2008), remains possible fu-ture work.Domain adaptation of neural networks via con-tinued training has been shown to be effective forneural language models by (Ter-Sarkisov et al,2015), and in work parallel to ours, for neuraltranslation models (Luong and Manning, 2015).We are the first to show that we can effectivelyadapt neural translation models with monolingualdata.6 ConclusionIn this paper, we propose two simple methods touse monolingual training data during training ofNMT systems, with no changes to the networkarchitecture.
Providing training examples withdummy source context was successful to some ex-tent, but we achieve substantial gains in all tasks,and new SOTA results, via back-translation ofmonolingual target data into the source language,and treating this synthetic data as additional train-ing data.
We also show that small amounts of in-domain monolingual data, back-translated into thesource language, can be effectively used for do-main adaptation.
In our analysis, we identified do-main adaptation effects, a reduction of overfitting,and improved fluency as reasons for the effective-ness of using monolingual data for training.While our experiments did make use of mono-lingual training data, we only used a small ran-dom sample of the available data, especially forthe experiments with synthetic parallel data.
It isconceivable that larger synthetic data sets, or datasets obtained via data selection, will provide big-ger performance benefits.Because we do not change the neural net-work architecture to integrate monolingual train-ing data, our approach can be easily applied toother NMT systems.
We expect that the effective-ness of our approach not only varies with the qual-ity of the MT system used for back-translation, butalso depends on the amount (and similarity to thetest set) of available parallel and monolingual data,and the extent of overfitting of the baseline model.Future work will explore the effectiveness of ourapproach in more settings.AcknowledgmentsThe research presented in this publication wasconducted in cooperation with Samsung Elec-tronics Polska sp.
z o.o.
- Samsung R&D In-stitute Poland.
This project received fundingfrom the European Union?s Horizon 2020 researchand innovation programme under grant agreement645452 (QT21).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural Machine Translation by JointlyLearning to Align and Translate.
In Proceedings ofthe International Conference on Learning Represen-tations (ICLR).Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In Proceedings of theFourth Workshop on Statistical Machine Translation94StatMT 09.
Association for Computational Linguis-tics.Ond?rej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp,Philipp Koehn, Varvara Logacheva, Christof Monz,Matteo Negri, Matt Post, Carolina Scarton, LuciaSpecia, and Marco Turchi.
2015.
Findings of the2015 Workshop on Statistical Machine Translation.In Proceedings of the Tenth Workshop on StatisticalMachine Translation, pages 1?46, Lisbon, Portugal.Association for Computational Linguistics.P.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, F. Je-linek, J.D.
Lafferty, R.L.
Mercer, and P.S.
Roossin.1990.
A Statistical Approach to Machine Transla-tion.
Computational Linguistics, 16(2):79?85.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
WIT3: Web Inventory of Transcribedand Translated Talks.
In Proceedings of the 16thConference of the European Association for Ma-chine Translation (EAMT), pages 261?268, Trento,Italy.Mauro Cettolo, Jan Niehues, Sebastian St?ker, LuisaBentivogli, and Marcello Federico.
2014.
Reporton the 11th IWSLT Evaluation Campaign, IWSLT2014.
In Proceedings of the 11th Workshop on Spo-ken Language Translation, pages 2?16, Lake Tahoe,CA, USA.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio.
2014.
Learn-ing Phrase Representations using RNN Encoder?Decoder for Statistical Machine Translation.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 1724?1734, Doha, Qatar.
Association forComputational Linguistics.Alex Graves.
2011.
Practical Variational Inference forNeural Networks.
In J. Shawe-Taylor, R.S.
Zemel,P.L.
Bartlett, F. Pereira, and K.Q.
Weinberger, ed-itors, Advances in Neural Information ProcessingSystems 24, pages 2348?2356.
Curran Associates,Inc.
?aglar G?l?ehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Lo?c Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
OnUsing Monolingual Corpora in Neural MachineTranslation.
CoRR, abs/1503.03535.Barry Haddow, Matthias Huck, Alexandra Birch, Niko-lay Bogoychev, and Philipp Koehn.
2015.
TheEdinburgh/JHU Phrase-based Machine TranslationSystems for WMT 2015.
In Proceedings of theTenth Workshop on Statistical Machine Translation,pages 126?133, Lisbon, Portugal.
Association forComputational Linguistics.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.CoRR, abs/1207.0580.S?bastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2015a.
On Using Very LargeTarget Vocabulary for Neural Machine Translation.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages1?10, Beijing, China.
Association for Computa-tional Linguistics.S?bastien Jean, Orhan Firat, Kyunghyun Cho, RolandMemisevic, and Yoshua Bengio.
2015b.
MontrealNeural Machine Translation Systems for WMT?15 .In Proceedings of the Tenth Workshop on StatisticalMachine Translation, pages 134?140, Lisbon, Por-tugal.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the ACL-2007 Demo and PosterSessions, pages 177?180, Prague, Czech Republic.Association for Computational Linguistics.Patrik Lambert, Holger Schwenk, Christophe Servan,and Sadaf Abdul-Rauf.
2011.
Investigations onTranslation Model Adaptation Using MonolingualData.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation, pages 284?293, Edin-burgh, Scotland.
Association for Computational Lin-guistics.Minh-Thang Luong and Christopher D. Manning.2015.
Stanford Neural Machine Translation Sys-tems for Spoken Language Domains.
In Proceed-ings of the International Workshop on Spoken Lan-guage Translation 2015, Da Nang, Vietnam.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Effective Approaches to Attention-based Neural Machine Translation.
In Proceed-ings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1412?1421, Lisbon, Portugal.
Association for Computa-tional Linguistics.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective Self-training for Parsing.
InProceedings of the Main Conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, HLT-NAACL ?06, pages 152?159, NewYork.
Association for Computational Linguistics.Henry Rowley, Shumeet Baluja, and Takeo Kanade.1996.
Neural Network-Based Face Detection.
InComputer Vision and Pattern Recognition ?96.95Has?im Sak, Tunga G?ng?r, and Murat Sara?lar.
2007.Morphological Disambiguation of Turkish Text withPerceptron Algorithm.
In CICLing 2007, pages107?118.Holger Schwenk.
2008.
Investigations on Large-ScaleLightly-Supervised Training for Statistical MachineTranslation.
In International Workshop on SpokenLanguage Translation, pages 182?189.Rico Sennrich and Barry Haddow.
2015.
A JointDependency Model of Morphological and Syntac-tic Structure for Statistical Machine Translation.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages2081?2087, Lisbon, Portugal.
Association for Com-putational Linguistics.Rico Sennrich, Barry Haddow, and Alexandra Birch.2016.
Neural Machine Translation of Rare Wordswith Subword Units.
In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2016), Berlin, Germany.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to Sequence Learning with Neural Net-works.
In Advances in Neural Information Process-ing Systems 27: Annual Conference on Neural Infor-mation Processing Systems 2014, pages 3104?3112,Montreal, Quebec, Canada.Alex Ter-Sarkisov, Holger Schwenk, Fethi Bougares,and Lo?c Barrault.
2015.
Incremental AdaptationStrategies for Neural Network Language Models.In Proceedings of the 3rd Workshop on Continu-ous Vector Space Models and their Composition-ality, pages 48?56, Beijing, China.
Association forComputational Linguistics.Francis M. Tyers and Murat S. Alperen.
2010.
SE-Times: A parallel corpus of Balkan languages.
InWorkshop on Exploitation of multilingual resourcesand tools for Central and (South) Eastern EuropeanLanguages at the Language Resources and Evalua-tion Conference, pages 1?5.96
