Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733?736,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSoftmax-Margin CRFs: Training Log-Linear Models with Cost FunctionsKevin Gimpel Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractWe describe a method of incorporating task-specific cost functions into standard condi-tional log-likelihood (CLL) training of linearstructured prediction models.
Recently intro-duced in the speech recognition community,we describe the method generally for struc-tured models, highlight connections to CLLand max-margin learning for structured pre-diction (Taskar et al, 2003), and show thatthe method optimizes a bound on risk.
Theapproach is simple, efficient, and easy to im-plement, requiring very little change to anexisting CLL implementation.
We presentexperimental results comparing with severalcommonly-used methods for training struc-tured predictors for named-entity recognition.1 IntroductionConditional random fields (CRFs; Lafferty et al2001) and other conditional log-linear models(Berger et al, 1996) achieve strong performancefor many NLP problems, but the conditional log-likelihood (CLL) criterion optimized when trainingthese models cannot take a task-specific cost func-tion into account.In this paper, we describe a simple approachfor training conditional log-linear models with costfunctions.
We show how the method relates to othermethods and how it provides a bound on risk.
Weapply the method to train a discriminative modelfor named-entity recognition, showing a statisticallysignificant improvement over CLL.2 Structured Log-Linear ModelsLet X denote a structured input space and, for a par-ticular x ?
X, let Y(x) denote a structured outputspace for x.
The size of Y(x) is often exponentialin x, which differentiates structured prediction frommulticlass classification.
For named-entity recogni-tion, for example, x might be a sentence and Y(x)the set of all possible named-entity labelings for thesentence.
Given an x ?
X and a y ?
Y(x), we use aconditional log-linear model for p?(y|x):p?
(y|x) =exp{?>f(x, y)}?y?
?Y(x) exp{?>f(x, y?
)}(1)where f(x, y) is a feature vector representation ofx and y and ?
is a parameter vector containing onecomponent for each feature.2.1 Training CriteriaMany criteria exist for training the weights ?.
Wenext review three choices in detail.
For the follow-ing, we assume a training set consisting of n exam-ples {?x(i), y(i)?}ni=1.
Some criteria will make use ofa task-specific cost function that measures the extentto which a structure y differs from the true structurey(i), denoted by cost(y(i), y).2.1.1 Conditional Log-LikelihoodThe learning problem for maximizing conditionallog-likelihood is shown in Eq.
3 in Fig.
1 (we trans-form it into a minimization problem for easier com-parison).
This criterion is commonly used when aprobabilistic interpretation of the model is desired.2.1.2 Max-MarginAn alternative approach to training structured lin-ear classifiers is based on maximum-margin Markovnetworks (Taskar et al, 2003).
The basic idea isto choose weights such that the linear score of each?x(i), y(i)?
is better than ?x(i), y?
for all alternativesy ?
Y(x(i)) \ {y(i)}, with a larger margin for thosey with higher cost.
The ?margin rescaling?
form ofthis training criterion is shown in Eq.
4.
Note thatthe cost function is incorporated into the criterion.2.1.3 RiskRisk is defined as the expected value of the costwith respect to the conditional distribution p?
(y|x);733on training data:?ni=1?y?Y(x(i)) p?
(y|x(i))cost(y(i), y) (2)With a log-linear model, learning then requires solv-ing the problem shown in Eq.
5.
Unlike the previoustwo criteria, risk is typically non-convex.Risk minimization first appeared in the speechrecognition community (Kaiser et al, 2000; Poveyand Woodland, 2002).
In NLP, Smith and Eis-ner (2006) minimized risk using k-best lists to de-fine the distribution over output structures.
Li andEisner (2009) introduced a novel semiring for min-imizing risk using dynamic programming; Xiong etal.
(2009) minimized risk in a CRF.2.1.4 Other CriteriaMany other criteria have been proposed to at-tempt to tailor training conditions to match task-specific evaluation metrics.
These include the aver-age per-label marginal likelihood for sequence label-ing (Kakade et al, 2002), minimum error-rate train-ing for machine translation (Och, 2003), F1 for lo-gistic regression classifiers (Jansche, 2005), and awide range of possible metrics for sequence label-ing and segmentation tasks (Suzuki et al, 2006).3 Softmax-MarginThe softmax-margin objective is shown as Eq.
6 andis a generalization of that used by Povey et al (2008)and similar to that used by Sha and Saul (2006).The simple intuition is the same as the intuitionin max-margin learning: high-cost outputs for x(i)should be penalized more heavily.
Another viewsays that we replace the probabilistic score insidethe exp function of CLL with the ?cost-augmented?score from max-margin.
A third view says that wereplace the ?hard?
maximum of max-margin withthe ?softmax?
(log?exp) from CLL; hence we usethe name ?softmax-margin.?
Like CLL and max-margin, the objective is convex; a proof is providedin Gimpel and Smith (2010).3.1 Relation to Other ObjectivesWe next show how the softmax-margin criterion(Eq.
6) bounds the risk criterion (Eq.
5).
We firstdefine some additional notation:E(i)[F ] =?y?Y(x(i)) p?
(y | x(i))F (y)for some function F : Y(x(i)) ?
R. First note thatthe softmax-margin objective (Eq.
6) is equal to:(Eq.
3) +?ni=1 logE(i)[exp cost(y(i), ?)]
(7)The first term must be nonnegative.
Taking each partof the second term, and using Jensen?s inequality,logE(i)[ecost(y(i),?)]
?
E(i)[log ecost(y(i),?
)]= E(i)[cost(y(i), ?
)]which is exactly Eq.
5.
Softmax-margin is also anupper bound on the CLL criterion because, assum-ing cost is nonnegative, logE[exp cost] ?
0.
Fur-thermore, softmax-margin is a differentiable upperbound on max-margin, because the softmax functionis a differentiable upper bound on the max function.We note that it may also be interest-ing to consider minimizing the function?ni=1 logE(i)[exp cost(y(i), ?
)], since it is anupper bound on risk but requires less computationfor computing the gradient.1 We call this objec-tive the Jensen risk bound and include it in ourexperimental comparison below.3.2 ImplementationMost methods for training structured models withcost functions require the cost function to decom-pose across the pieces of the structure in the sameway as the features, such as the standard methodsfor maximizing margin and minimizing risk (Taskaret al, 2003; Li and Eisner, 2009).
If the same con-ditions hold, softmax-margin training can be im-plemented atop standard CRF training simply byadding additional ?features?
to encode the localcost components, only when computing the partitionfunction during training.2 The weights of these ?costfeatures?
are not learned.4 ExperimentsWe consider the problem of named-entity recog-nition (NER) and use the English data from theCoNLL 2003 shared task (Tjong Kim Sang and DeMeulder, 2003).
The data consist of news articles1Space does not permit a full discussion; see Gimpel andSmith (2010) for details.2Since cost(y(i), y(i)) = 0 by definition, these ?features?will never fire for the numerator and can be ignored.734CLL: min?n?i=1?
?>f(x(i), y(i)) + log?y?Y(x(i))exp{?>f(x(i), y)} (3)Max-Margin: min?n?i=1?
?>f(x(i), y(i)) + maxy?Y(x(i))(?>f(x(i), y) + cost(y(i), y))(4)Risk: min?n?i=1?y?Y(x(i))cost(y(i), y)exp{?>f(x(i), y)}?y?
?Y(x(i)) exp{?>f(x(i), y?
)}(5)Softmax-Margin: min?n?i=1?
?>f(x(i), y(i)) + log?y?Y(x(i))exp{?>f(x(i), y) + cost(y(i), y)} (6)Figure 1: Objective functions for training linear models.
Regularization terms (e.g., C?dj=1 ?2j ) are not shown here.annotated with four entity types: person, location,organization, and miscellaneous.
Our experimentsfocus on comparing training objectives for struc-tured sequential models for this task.
For all objec-tives, we use the same standard set of feature tem-plates, following Kazama and Torisawa (2007) withadditional token shape like those in Collins (2002b)and simple gazetteer features.
A feature was in-cluded if it occurred at least once in training data(total 1,312,255 features).The task is evaluated using the F1 score, whichis the harmonic mean of precision and recall (com-puted at the level of entire entities).
Since this metricis computed from corpus-level precision and recall,it is not easily decomposable into features used instandard chain CRFs.
For simplicity, we only con-sider Hamming cost in this paper; experiments withother cost functions more targeted to NER are pre-sented in Gimpel and Smith (2010).4.1 BaselinesWe compared softmax-margin to several baselines:the structured perceptron (Collins, 2002a), 1-bestMIRA with cost-augmented inference (Crammer etal., 2006), CLL, max-margin, risk, and our Jensenrisk bound (JRB) introduced above.We used L2 regularization, experimenting withseveral coefficients for each method.
For CLL,softmax-margin, max-margin, and MIRA, we usedregularization coefficients C ?
{0.01, 0.1, 1}.
Riskhas not always been used with regularization, as reg-ularization does not have as clear a probabilistic in-terpretation with risk as it does with CLL; so, forrisk and JRB we only used C ?
{0.0, 0.01}.
Inaddition, since these two objectives are non-convex,we initialized with the output of the best-performingCLL model on dev data (which was the CLL modelwith C = 0.01).3 All methods except CLL and theperceptron make use of a cost function, for whichwe used Hamming cost.
We experimented with dif-ferent fixed multipliers m for the cost function, form ?
{1, 5, 10, 20}.The hyperparameters C and m were tuned on thedevelopment data and the best-performing combina-tion was used to label the test data.
We also tunedthe decision to average parameters across all train-ing iterations; this has generally been found to helpthe perceptron and MIRA, but in our experimentshad mixed results for the other methods.We ran 100 iterations through the training data foreach method.
For CLL, softmax-margin, risk, andJRB, we used stochastic gradient ascent with a fixedstep size of 0.01.
For max-margin, we used stochas-tic subgradient ascent (Ratliff et al, 2006) also witha fixed step size of 0.01.4 For the perceptron andMIRA, we used their built-in step size formulas.4.2 ResultsTable 1 shows our results.
On test data, softmax-margin is statistically indistinguishable from MIRA,risk, and JRB, but performs significantly betterthan CLL, max-margin, and the perceptron (p <0.03, paired bootstrap with 10,000 samples; Koehn,3When using initialization of all ones for risk and JRB, re-sults were several points below the results here, and with allzeroes, learning failed, resulting in 0.0 F-measure on dev data.Thus, risk and JRB appear sensitive to model initialization.4In preliminary experiments, we tried other fixed and de-creasing step sizes for (sub)gradient ascent and found that afixed step of 0.01 consistently performed well across trainingobjectives, so we used it for all settings for simplicity.735Method Dev.
Test (C, m, avg.?
)Perceptron 90.48 83.98 (Y)MIRA 91.13 85.72 (0.01, 20, Y)CLL 90.79 85.46 (0.01, N)Max-Margin 91.17 85.28 (0.01, 1, Y)Risk 91.14 85.59 (0.01, 10, N)JRB 91.05 85.65 (0.01, 1, N)Softmax-Margin 91.30 85.84 (0.01, 5, N)Table 1: Results on development and test sets, along withhyperparameter values chosen using development set.2004).
It may be surprising that an improvementof 0.38 in F1 could be significant, but this indicatesthat the improvements are not limited to certain cate-gories of phenomena in a small number of sentencesbut rather appear throughout the majority of the testset.
The Jensen risk bound performs comparably torisk, and takes roughly half as long to train.5 DiscussionThe softmax-margin approach offers (1) a convexobjective, (2) the ability to incorporate task-specificcost functions, and (3) a probabilistic interpretation(which supports, e.g., hidden-variable learning andcomputation of posteriors).
In contrast, max-margintraining and MIRA do not provide (3); risk andJRB do not provide (1); and CLL does not support(2).
Furthermore, softmax-margin training improvesover standard CLL training of CRFs, is straightfor-ward to implement, and requires the same amount ofcomputation as CLL.We have also presented the Jensen risk bound,which is easier to implement and faster to train thanrisk, yet gives comparable performance.
The pri-mary limitation of all these approaches, includingsoftmax-margin, is that they only support cost func-tions that factor in the same way as the features ofthe model.
Future work might exploit approximateinference for more expressive cost functions.AcknowledgmentsWe thank the reviewers, John Lafferty, and Andre?
Martinsfor helpful comments and feedback on this work.
Thisresearch was supported by NSF grant IIS-0844507.ReferencesA.
Berger, V. J. Della Pietra, and S. A. Della Pietra.
1996.
Amaximum entropy approach to natural language processing.Computational Linguistics, 22(1):39?71.M.
Collins.
2002a.
Discriminative training methods for hiddenMarkov models: Theory and experiments with perceptronalgorithms.
In Proc.
of EMNLP.M.
Collins.
2002b.
Ranking algorithms for named-entity ex-traction: Boosting and the voted perceptron.
In Proc.
ofACL.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, andY.
Singer.
2006.
Online passive-aggressive algorithms.Journal of Machine Learning Research, 7:551?585.K.
Gimpel and N. A. Smith.
2010.
Softmax-margin trainingfor structured log-linear models.
Technical report, CarnegieMellon University.M.
Jansche.
2005.
Maximum expected F -measure training oflogistic regression models.
In Proc.
of HLT-EMNLP.J.
Kaiser, B. Horvat, and Z. Kacic.
2000.
A novel loss functionfor the overall risk criterion based discriminative training ofHMM models.
In Proc.
of ICSLP.S.
Kakade, Y. W. Teh, and S. Roweis.
2002.
An alternate ob-jective function for Markovian fields.
In Proc.
of ICML.J.
Kazama and K. Torisawa.
2007.
A new perceptron algorithmfor sequence labeling with non-local features.
In Proc.
ofEMNLP-CoNLL.P.
Koehn.
2004.
Statistical significance tests for machine trans-lation evaluation.
In Proc.
of EMNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proc.
of ICML.Z.
Li and J. Eisner.
2009.
First- and second-order expecta-tion semirings with applications to minimum-risk training ontranslation forests.
In Proc.
of EMNLP.F.
J. Och.
2003.
Minimum error rate training for statisticalmachine translation.
In Proc.
of ACL.D.
Povey and P. C. Woodland.
2002.
Minimum phone error andI-smoothing for improved discrimative training.
In Proc.
ofICASSP.D.
Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran,G.
Saon, and K. Visweswariah.
2008.
Boosted MMI formodel and feature space discriminative training.
In Proc.
ofICASSP.N.
Ratliff, J.
A. Bagnell, and M. Zinkevich.
2006.
Subgradientmethods for maximum margin structured learning.
In ICMLWorkshop on Learning in Structured Output Spaces.F.
Sha and L. K. Saul.
2006.
Large margin hidden Markovmodels for automatic speech recognition.
In Proc.
of NIPS.D.
A. Smith and J. Eisner.
2006.
Minimum risk annealing fortraining log-linear models.
In Proc.
of COLING-ACL.J.
Suzuki, E. McDermott, and H. Isozaki.
2006.
Training con-ditional random fields with multivariate evaluation measures.In Proc.
of COLING-ACL.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Advances in NIPS 16.E.
F. Tjong Kim Sang and F. DeMeulder.
2003.
Introduction tothe CoNLL-2003 shared task: Language-independent namedentity recognition.
In Proc.
of CoNLL.Y.
Xiong, J. Zhu, H. Huang, and H. Xu.
2009.
Minimum tagerror for discriminative training of conditional random fields.Information Sciences, 179(1-2):169?179.736
