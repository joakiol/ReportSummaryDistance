Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 84?91,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTowards Identifying Unresolved Discussions in Student Online ForumsJihie Kim, Jia Li, and Taehwan KimUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty Was, Marina del Rey, CA, U.S.A{jihie, jiali, taehwan}@isi.eduAbstractAutomatic tools for analyzing student onlinediscussions are highly desirable for providingbetter assistance and promoting discussionparticipation.
This paper presents an approachfor identifying student discussions with unre-solved issues or unanswered questions.
In or-der to handle highly incoherent data, weperform several data processing steps.
Wethen apply a two-phase classification algo-rithm.
First, we classify ?speech acts?
of indi-vidual messages to identify the roles that themessages play, such as question, issue raising,and answers.
We then use the resulting speechacts as features for classifying discussionthreads with unanswered questions or unre-solved issues.
We performed a preliminaryanalysis of the classifiers and the systemshows an average F score of 0.76 in discus-sion thread classification.1 Introduction*Online discussion boards have become a popularand important medium for distance education.Students use discussion forums to collaborate, toexchange information, and to seek answers toproblems from their instructors and classmates.Making use of the dialog to assess student under-standing is an open research problem.
As the classsize increases and online interaction becomesheavier, automatic tools for analyzing student dis-cussions are highly desirable for providing betterassistance and promoting discussion participation.In this paper, we present an approach for automati-cally identifying discussions that have unresolvedissues or unanswered questions.
The resulting dis-*cussions can be reported to instructors for furtherassistance.We present a two-phase machine learning ap-proach where the first phase identifies high leveldialogue features (speech acts such as question,issue raising, answer, and acknowledgement) thatare appropriate for assessing student interactions.The second phase uses speech acts as features increating thread classifiers that identify discussionswith unanswered questions or unresolved issues.We also describe an approach where thread classi-fiers are created directly from the features in dis-cussion messages.
The preliminary results indicatethat although the direct learning approach canidentify threads with unanswered questions well,SA based learning provide a little better results inidentifying threads with issues and threads withunresolved issues.2 Modeling Student DiscussionsOur study takes place in the context of an under-graduate course discussion board that is an integralcomponent of an Operating Systems course in theComputer Science Department at the University ofSouthern California.
We obtain our data from anexisting online discussion board that hosts studenttechnical discussions.
Total 291 discussion threads(219 for training and 72 for test) with 1135 mes-sages (848 for training and 287 for test) from twosemesters?
discussions were used for this study.168 students participated in the discussions.2.1 Discussion ThreadsUnlike prototypical collaborative argumentationwhere a limited number of members take part inthe conversation with a strong focus on solvingspecific problems, student online discussions havemuch looser conversational structure, possibly in-volving multiple anonymous discussants.
Student84discussions are very informal and noisy with re-spect to grammar, syntax and punctuation.
There isa lot of variance in the way that students presentsimilar information.
Messages about programmingassignments include various forms of references toprogramming code.
Figure 1 shows an examplediscussion thread that is relatively technical andformal.
The raw data include humorous messagesand personal announcements as well as technicalquestions and answers.Figure 1.
An example discussion threadThe average number of messages per discussionthread in our undergraduate course is 3.9, andmany discussion threads contain only two or threemessages.
Discussions often start with a questionfrom a student on a project or an assignment.
Insome cases, the discussion ends with an answerthat follows the question.
In some other cases, theoriginal poster may raise additional issues or askquestions about the answer.
The discussion cancontinue with the following answer from anotherstudent as in Figure 1.
However, sometimes thediscussion ends with hanging issues or questionswithout an answer.2.2 Speech Acts in messages: Identifying rolesthat a message playsFor conversation analysis, we adopted the theoryof Speech Acts (SAs) to capture relations betweenmessages (Austin, 1962; Searle, 1969).
Each mes-sage within a discussion thread may play a differ-ent role.
A message could include a question for aparticular problem, or it could contain an answer orsuggestion with respect to a previous question inthe thread.
Messages can include question, answer,acknowledgement, and objection.
Since SAs areuseful in understanding contributions made by stu-dents in discussions, and are natural indicators forunanswered questions or unresolved issues, we useSAs as features for classifying discussion threadsin a two phase learning as described below.Table 1.
Speech Act Categories and Kappa valuesSACategory Description  kappaQUESA question about a problem, in-cluding question about a previousmessage0.94ANSA simple or complex answer to aprevious question.
Suggestion oradvice0.72ISSUEReport misunderstanding, unclearconcepts or issues in solving prob-lems0.88Pos-AckAn acknowledgement, complimentor support in response to a prev.message0.87Neg-AckA correction or objection (or com-plaint) to/on a previous message0.85We divide message roles into several SA cate-gories, extending the approaches presented in (Kimet al, 2006; Kim and Ravi 2007).
We focus on thecategories that are relevant to the problem of iden-tifying discussion threads with unanswered ques-tion or unresolved issues.The message might contain a question about aparticular problem (QUES) or report a misunder-standing, unclear concepts or issues in solving aproblem (ISSUE).
It might propose an answer orsuggestion with respect to a previous question inthe thread (ANS).
Finally, a message might ac-knowledge the previous message with supportMessage1: QUESMessage2: ANSPoster1: I am still confused.
I understand it is in thesame address space as the parent process, where do weallocate the 8 pages of mem for it?
And how do wekeep track of .....?
I am sure it is a simple concept thatI am just missing.Poster2: Have you read the student documentation forthe Fork syscall?
?Poster1: The Professor gave us 2 methods for forkingthreads from the main program.
One was .......  Theother was to .........
When you fork a thread where doesit get created and take its 8 pages from?
Do you have tocalculate ......?
If so how?
Where does it store itsPCReg .......?
Any suggestions would be helpfule.Poster3: If you use the first implementation....,then you'll have a hard limit on the number ofthreads....If you use the second implementation,you need to....Either way, you'll need to implement theAddrSpace::NewStack() function and make surethat there is memory available.
?Message3: ISSUE, QUESMessage4: ANS85(Pos-Ack) or show disagreement or objection(Neg-Ack).
SAs relate a pair of messages that has a?reply-to?
relation.
A pair of messages can be la-beled with multiple SAs, and a message can havemultiple SAs with more than one messages.
Thisallows us to capture various relations among mes-sages.
Table 1 describes the categories we are fo-cusing on and the kappa values from twoannotators.
Figure 1 shows SA relations betweenmessage pairs.During annotation of the corpus, the annotatorsmarked the cues that are relevant to a particular SAcategory as well as the SA categories themselves.Such information provides hints on the kinds offeatures that are useful.
We also interviewed theannotators to capture additional cues or indicatorsthat they used during the annotation.
We iteratedwith several different annotation approaches untilwe reach enough agreement among the annotatorson a new dataset that was not seen by the annota-tors before.Table 2 shows the distribution statistics of eachSA category among the whole training and testcorpus.
Since a message may have more than oneSA, the percentage sum of all SAs doesn?t equal to1.
As we can see, Pos-Ack and Neg-Ack are ex-periencing lacking data problem which is one ofthe challenges we are facing for SA classification.Table 2.
Statistics for each Speech Act CategoryTraining set Test set SACategory # of msgs Percentage # of msgs PercentageQUES 469 55.31% 146 50.87%ANS 508 59.91% 176 61.32%ISSUE 136 16.03% 46 16.03%Pos-Ack 78 9.20% 30 10.45%Neg-Ack 23 2.71% 8 2.79%3 Message Speech Act ClassifiersIn this section, we first describe how raw discus-sion data is processed and show the features gener-ated from the data, and we then present the currentSA classifiers.3.1 Discussion Data Pre-processingBesides typical data preprocessing steps, suchas stemming and filtering, which are taken by mostNLP systems, our system performs additional stepsto reduce noise and variance (Ravi and Kim 2007).We first remove the text from previous mes-sages that is automatically inserted by the discus-sion board system starting with righ angle braket(>) when the user clicks on a ?Reply to?
button.We also apply a simple stemming algorithm thatremoves ?s?
and ?es?
for plurals.
Apostrophes arealso converted to their original forms.
E.g., ?I?m?is converted to ?I am?.
For discussions on pro-gramming assignment, the discussion included pro-gramming code fragments.
Each section ofprogramming code or code fragment is replacedwith a single term called code.
Similar substitutionpatterns were used for a number of categories likefiletype extensions (?.html?, ?.c?, ?.c++?,?.doc?
), URL links and others.
Students also tendto use informal words (e.g.
?ya?, ?yeah?, ?yup?
).We substitute some of such words with one form(?yes?).
For words like ?which?,  ?where?,?when?, ?who?
and ?how?, we used the termcateg_wh.
We do not replace pronouns (?I?, ?we?,?they?,) since they may be useful for identifyingsome SAs.
For example, ?You can?
may be a cuefor ANS but ?I can?
may not.We also apply a simple sentence divider withsimple cues (punctuation and white spaces such asnewline) in order to captures the locations of thefeatures in the message, such as cue words in thefirst sentence vs. cues in the last sentence.3.2 Features for Speech Act ClassificationWe have used six different types of features basedon input from the annotators.F1: cue phases and their positions: In addition toSAs (e.g.
QUES), the human annotators markedthe parts within the message (cue phrases or sen-tences), which helped them identify the SAs in themessage.
In order to overcome data sparseness, wegenerate features from the marked phrases.
That is,from each phrase, we extract all the unigrams, bi-grams, trigrams (sequence of 1/2/3 words) and addthem to the feature set.
We also added two separateunigrams, three separate unigrams and a unigramand a bigram combinations since the annotations inthe training data indicated that they could be a use-ful pattern.
All the cues including separate cuessuch as two unigrams are captured and used for asingle sentence.
Positions of the cues are includedsince in longer messages the cues in the beginning86sentences and the ones in the end sentences canindicate different SAs.
For example, THANK inthe beginning indicates a positive answer butTHANK in latter part of the message usuallymeans politeness (thank in advance).F2: Message Position: Position of current messagewithin the discussion thread (e.g.
the first message,the last message, or middle in the thread).F3: Previous Message Information: SAs in theprevious message that the current message is reply-ing to.F4: Poster Class: Student or Instructor.F5: Poster Change: Was the current messageposted by the same person who posted the messagethat the current message is replying to?F6: Message Length: Values include Short(1-5words), Medium(6-30 words), and Long(>30words).F1 is a required feature since the annotators in-dicated cues as useful feature in most cases.
All theothers are optional.3.3 Speech Act ClassifiersWe applied SVM in creating binary classifiers foreach SA category using Chang and Lin (2001).Also, Transformation-based Learning (TBL) wasapplied as it has been successfully used in spokendialogue act classification (Samuel 2000; Brill1995).
It starts with the unlabeled corpus andlearns the best sequence of admissible ?transforma-tion rules?
that must be applied to the training cor-pus to minimize the error for the task.
Thegenerated rules are easy to understand and usefulfor debugging the features used.
TBL results arealso used in generating dependencies among SAcategories for F3, i.e.
which SAs tend to followwhich other SAs1, as describe below.SA Classification with TBLEach rule iRule is composed of two parts - (1)iRuleLHS  - A combination of features that shouldbe checked for applicability to the current message(2)iRuleTAG  - SA tag to apply, if the feature com-bination is applicable to the current message.1 It is possible to collect related clues from SVM with distribution offeature values and information gain although dependencies can beeasily recognized in TBL rules.iii RuleTAGRuleLHSRule !
"::Where ii XRuleLHS !
)654321(; FFFFFFXXX i #####$%The iRuleLHS  component can be instantiatedfrom all the combination of the features F1, ?,F6.iRuleTAG  is any SA (single SA) chosen from a listof all the SA categories.
An example rule used inSpeech Act learning is shown below:Rule1 :: IF cue-phrase = {?not?, ?work?
}& poster-info = Student& post-length = Long=> ISSUERule1 means if the post contains two unigrams?not?
and ?work?, the poster is a student, and thepost length is long, then the Speech Act for thepost is ISSUE.We apply each rule in the potential rule set onall the posts in the training corpus and transformthe post label if the post is applicable.
The rulewith highest improvement by F score is selectedinto the optimal rule set and moved from the po-tential rule set.
The iteration continues until thereis no significant improvement with any rule.The training corpus was divided into 3 parts for3-fold cross validation.
The rules from 3 rule setsare merged and sorted by weighted Mean Recipro-cal Rank (MRR) (Voorhees, 2001).
For example, ifwe have 5 rules among 3 rule sets as follows,Rule set 1 (0.85 on test): R1 R2 R3Rule set 2 (0.88 on test): R2 R1 R4Rule set 3 (0.79 on test): R1 R4 R5For R1, we calculate the weighted MRR as(0.85*1 + 0.88*(1/2) + 0.79*1) / 3.
After sorting,we get top N rules from the merged rule set.
Table3 shows some of the rules learned.Table 3.
TBL rule examplesIF cue-phrase = {???}
=> QUESIF cue-phrase = {?yes you can?
}& poster-info = Instructor& post-length = Medium  => ANSIF cue-phrase = {?yes?
}& cue-position = CP_BEGIN& prev-SA = QUES=> ANSIF cue-phrase = {?not know?
}87& poster-info = student& poster-change = YES  => ISSUEBased on the rules generated from TBL, weanalyze dependencies among the SA categories forF3 (previous message SAs).
In TBL rules, ANSdepends on ISSUE and QUES, i.e.
some ANSrules have QUES and ISSUE for F3.
Also Pos-Ackand Neg-Ack tend to follow ANS.
Both SVM andTBL classifiers use this information during testing.That is, we apply independent classifiers first andthen use dependent classifiers according to the de-pendency order as following:Currently there is no loop in the selected rulesbut we plan to address potential issues with loopsin SA dependencies.SA Classification with SVMTable 4.
Some of the top selected features by Infor-mation GainSACategory Top featuresQUES??
?POST_POSITION?_category_wh_ ?
?
?PREV_SA_FIRST_NONE?to ?
??ANSPOST_POSITIONPREV_SA_QUESTION??
?POSTER_INFOISSUEPOSTER_INFO?not ?
sure?POST_POSITIONFEATURE_LENGTH?error?Pos-AckPREV_SA_ANSWERPOST_POSITIONPREV_SA_FIRST_NONE?thanks?
& cue-position = CP_BEGIN?ok?
& cue-position = CP_BEGINNeg-Ack?yes,  ?
?, but?POST_POSITION?, but?
?are ?
wrong?Given all the combination of the features F1,?,F6, we use Information Gain (Yang and Pederson1997) for pruning the feature space and selectingfeatures.
For each Speech Act, we sort all the fea-tures (lexical and non-lexical) by Information Gainand use the top N (=200) features.
Table 4 showsthe top features selected by Information Gain.
Theresulting features are used in representing a mes-sage in a vector format.We did 5-fold cross validation in the training.RBF (Radial Basis Function) is used as the kernelfunction.
We performed grid search to get the bestparameter (C and gamma) in training and appliedthem to the test corpus.Table 5.
SA classification resultsSVM TBLSA Cat-egoryPrec.Re-callFscorePrec.Re-callFscoreQUES 0.95 0.90 0.94 0.96 0.91 0.95ANS 0.87 0.80 0.85 0.83 0.64 0.78ISSUE 0.65 0.54 0.62 0.46 0.76 0.50Pos-Ack0.57 0.44 0.54 0.58 0.56 0.57Neg-Ack 0 0 0 0.5 0.38 0.47Table 5 shows the current classification accura-cies with SVM and TBL.
The main reason thatISSUE, Pos-Ack and Neg-Ack show low scores isthat they have relatively small number of examples(see statistics in Table 2).
We plan to add moreexamples as we collect more discussion annota-tions.
For thread classification described below, weuse features with QUES, ANS, ISSUE andPos_Ack only.4 Identifying Discussions with Unan-swered or Unresolved Questions:Thread ClassificationFigure 2 shows typical patterns of interactions inour corpus.
Many threads follow pattern (a) wherethe first message includes a question and the sub-sequent message provides an answer.
In (b), afteran answer, the student presents an additional ques-tion or misunderstanding (ISSUE), which is fol-lowed by another answer.
Often students providepositive acknowledgement when an answer is sat-ISSUEANSQUESPos-AckNeg-Ack88isfying.
Pattern (c) covers cases for when thequestion is unanswered.Figure 2.
Example patterns in student discussionthreadsWe are interested in the following assessmentquestions.
(Q1) Were all questions answered?
(Y/N)(Q2) Were there any issues or confusion?
(Y/N)(Q3) Were those issues or confusions resolved?
(Y/N)There can be multiple questions, and Q1 is falseif there is any question that does not have a corre-sponding answer.
That is, even when some ques-tions were resolved, it couldstill be False (not resolved) if some were not re-solved.
If Q2 is False (i.e.
there is no issue orquestion), then Q3 is also False.These questions are useful for distinguishingdifferent interaction patterns, including threadswith unanswered questions.
In the second phase oflearning, we use SA-based features.
Our initialanalysis of student interactions as above indicatesthat the following simple features can be useful inanswering such questions:(T-F1) Whether there was an [SA] in the thread(T-F2) Whether the last message in the thread in-cluded [SA]We used TBL rules for Pos-Ack and SVM clas-sifiers for other SA categories because of relativelyhigher score of Pos-Ack from TBL and other cate-gories from SVM.
We use 8 (2 x 4) features cre-ated from T-F1 and T-F2.
SVM settings are similarto the ones used in the SA classification.Table 6 shows the thread classification results.We checked SVM classification results with hu-man annotated SAs since they can show how use-ful SA-based features are (T-F1 and T-F2 inparticular) in answering Q1?Q3.
The resultsshown in Table 6-(a) indicate that the features (T-F1 and T-F2) are in fact useful for the questions.When we used the SA classifiers and SVM in apipeline, the system shows precisions (recalls) of83%(84%), 77%(74%) and 68%(69%) for Q1, Q2,and Q3 respectively.Table 6.
Thread Classification ResultsPrecision Recall F scoreQ1 0.93    0.93 0.93Q2 0.93 0.93 0.93Q3 0.89 0.89 0.89(a) Classification results with human annotated SAsPrecision Recall F scoreQ1 0.83 0.84 0.83Q2 0.77 0.74 0.76Q3 0.68 0.69 0.68(b) SVM classification results with system generatedSAsThe results with system generated SAs providean average F score of 0.76.
Although the ISSUEclassifier has F score of 0.62, the score for Q2 is0.76.
Q2 checks one or more occurrences ofISSUE rather than identifying existence of ISSUEin a message, and it may become an easier problemwhen there are multiple occurrences of ISSUEs.5 Direct Thread Classification withoutSAsAs an alternative to the SA-based two-phase learn-ing, we crated thread classifiers directly from thefeatures in discussion messages.
We used SVMwith the following features that we can capturedirectly from a discussion thread.F1?
: cue phases and their positions in thethread:  we use the same cue features in F1 but weuse an optional thread level cue position:Last_message and Dont_Care.
For example, for agiven cue ?ok?, if it appears in the the last messageof the thread, we generate two features,"ok"_Last_message and "ok"_Dont_Care.Given a set of candidate features, we use In-formation Gain to select the top N (=200) features.The resulting features are used in creating vectorsas described inS 3.3.
Similar cross-validation andSVM settings are applied.89Table 7.
Results from Direct Thread ClassificationPrecision Recall F scoreQ1 0.86 0.86 0.86Q2 0.81 0.62 0.70Q3 0.75 0.33 0.46Table 7 shows the classification results.
Al-though the direct learning approach can identifythreads with unanswered questions well, SA basedlearning provides a little better results in identify-ing threads with issues (Q2) and unresolved issues(Q3).
It seems that SA-based features may helpperforming more difficult tasks (e.g.
assessmentfor ISSUEs in discussions) We need further inves-tigation on different types of assessment tasks.6 Related WorkRhetorical Structure Theory (Mann and Thom-son, 1988) based discourse processing has attractedmuch attention with successful applications in sen-tence compression and summarization.
Most of thecurrent work on discourse processing focuses onsentence-level text organization (Soricut andMarcu, 2003) or the intermediate step (Sporlederand Lapata, 2005).
Analyzing and utilizing dis-course information at a higher level, e.g., at theparagraph level, still remains a challenge to thenatural language community.
In our work, we util-ize the discourse information at a message level.There has been prior work on dialogue actanalysis and associated surface cue words (Samuel2000; Hirschberg and Litman 1993).
There havealso been Dialogue Acts modeling approaches forautomatic tagging and recognition of conversa-tional speech (Stolcke et al, 2000) and relatedwork in corpus linguistics where machine learningtechniques have been used to find conversationalpatterns in spoken transcripts of dialogue corpus(Shawar and Atwell, 2005).
Although spoken dia-logue is different from message-based conversa-tion in online discussion boards, they are closelyrelated to our thread analysis work, and we plan toinvestigate potential use of conversation patterns inspoken dialogue in threaded discussions.Carvalho and Cohen (2005) present a depend-ency-network based collective classificationmethod to classify email speech acts.
However,estimated speech act labeling between messages isnot sufficient for assessing contributor roles oridentifying help needed by the participants.
Weincluded other features like participant profiles.Also our corpus consists of less informal studentdiscussions rather than messages among projectparticipants, which tend to be more technicallycoherent.Requests and commitments of email exchangeare analyzed in (Lampert et al, 2008).
As in theiranalysis, we have a higher kappa value for ques-tions than answers, and some sources of ambiguityin human annotations such as different forms ofanswers also appear in our data.
However, studentdiscussions tend to focus on problem solving ratherthan task request and commitment as in projectmanagement applications, and their data show dif-ferent types of ambiguity due to different nature ofparticipant interests.There also has been work on non-traditional,qualitative assessment of instructional discourse(Graesser et al, 2005; McLaren et al, 2007; Boyeret al, 2008).
The assessment results can be used infinding features for student thinking skills or levelof understanding.
Although the existing work hasnot been fully used for discussion thread analysis,we are investigating opportunities for using suchfeatures to cover additional discourse analysis ca-pabilities.
Similar approaches for classifyingspeech acts were investigated (Kim and Ravi2007).
Our work captures more features that arerelevant to analyzing noisy student discussionthreads and support a full automatic analysis ofstudent discussions instead of manual generation ofthread analysis rules.7 Summary and Future WorkWe have presented an approach for automaticallyclassifying student discussions to identify discus-sions that have unanswered questions and needinstructor attention.
We applied a multi-phaselearning approach, where the first phase classifiesindividual messages with SAs and the secondphase classifies discussion threads with SA-basedfeatures.
We also created thread classifiers directlyfrom features in discussion messages.
The prelimi-nary results indicate that SA-based features mayhelp difficult classification tasks.
We plan to per-form more analysis on different types of threadclassification tasks.We found that automatic classification of un-dergraduate student discussions is very challenging90due to incoherence and noise in the data.
Espe-cially messages that contain long sentences, infor-mal statements with uncommon words, answers inform of question, are difficult to classify.
In orderto use other SA categories such as Neg-Ack andanalyze various types of student interactions, weplan to use more annotated discussion data.A deeper assessment of online discussions re-quires a combination with other information suchas discussion topics (Feng et al, 2006).
For exam-ple, classification of discussion topics can be usedin identifying topics that participants have moreconfusion about.
Furthermore, such informationcan also be used in profiling participants such asidentifying mentors or help seekers on a particulartopic as in (Kim and Shaw 2009).
We are investi-gating several extensions in order to generate moreuseful instructional tools.AcknowledgmentsThis work was supported by National ScienceFoundation, CCLI Phase II grant (#0618859).ReferencesAustin, J., How to do things with words.
1962.
Cam-bridge, Massachusetts: Harvard Univ.
Press.Boyer, K., Phillips, R., Wallis M., Vouk M., Lester, J.,Learner Characteristics and Feedback in TutorialDialogue.
2008.
ACL workshop on Innovative Use ofNLP for Building Educational Applications.Brill, E. 1962.
Transformation-based error-driven learn-ing and natural language processing: a case study inpart-of-speech tagging.
Comput.
Linguist., 21(4).Carvalho, V.R.
and Cohen, W.W. 2005.
On the collec-tive classification of email speech acts.
Proceedingsof SIGIR.Chang, C.-C. and Lin, C.-J.
2001.
LIBSVM: a libraryfor support vector machines.Feng, D., Kim, J., Shaw, E., Hovy E., 2006.
TowardsModeling Threaded Discussions through Ontology-based Analysis.
Proceedings of National Confer-ence on Artificial Intelligence.Graesser, A. C., Olney, A., Ventura, M., Jackson, G. T.2005.
AutoTutor's Coverage of Expectations duringTutorial Dialogue.
Proceedings of the FLAIRS Con-ference.Hirschberg, J. and Litman, D. 1993.
Empirical Studieson the Disambiguation of Cue Phrases?, Computa-tional Linguistics, 19 (3).Kim, J., Chern, G., Feng, D., Shaw, E., and Hovy, E.2006.
Mining and Assessing Discussions on theWeb through Speech Act Analysis.
Proceedings ofthe ISWC'06 Workshop on Web Content MiningwithHuman Language Technologies (2006).Kim J. and Shaw E. 2009.
Pedagogical Discourse: Con-necting Students to Past Discussions and Peer Men-tors within an Online Discussion Board, InnovativeApplications of Artificial Intelligence Conference.Lampert, A., Dale, R., and Paris, C. 2008.
The Nature ofRequests and Commitments in Email Messages,AAAI workshop on Enhanced Messaging.Mann, W.C. and Thompson, S.A. 1988.
Rhetoricalstructure theory: towards a functional theory of textorganization.
Text: An Interdisciplinary Journal forthe Study of Text, 8 (3).McLaren, B. et al,2007.
Using Machine LearningTechniques to Analyze and Support Mediation ofStudent E!Discussions, Proc.
of AIED 2007.Ravi, S., Kim, J., 2007.
Profiling Student Interactions inThreaded Discussions with Speech Act Classifiers.Proceedings of AI in Education.Samuel, K. 2000.An Investigation of Dialogue Act Tag-ging using Transformation-Based Learning, PhDThesis, University of Delaware.Searle, J.
1969.
Speech Acts.
Cambridge: CambridgeUniv.
Press.Soricut, R. and Marcu, D. 2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
Proceedings of HLT/NAACL-2003.Sporleder, C. and Lapata, M., 2005.
Discourse chunkingand its application to sentence compression.
InProceedings of Human Language Technology con-ference ?
EMNLP.Stolcke, A. , Coccaro, N. , Bates, R. , Taylor, P. , et al,2000.
Dialogue act modeling for automatic taggingand recognition of conversational speech, Compu-tational Linguistics, v.26 n.3.Shawar, B.
A. and Atwell, E. 2005.
Using corpora inmachine-learning chatbot systems.?
InternationalJournal of Corpus Linguistics, vol.
10.Witten, I. H., and Frank, E. 2005.
Data Mining: Practi-cal machine learning tools and techniques, 2ndEdition, Morgan Kaufmann, San Francisco.Yang, Y. and Pedersen, J.
1997.
A Comparative Studyon Feature Selection in Text Categorization.
Proc.International Conference on Machine Learning.91
