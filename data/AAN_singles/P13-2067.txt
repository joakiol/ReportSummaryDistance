Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375?381,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImproving machine translation by training againstan automatic semantic frame based evaluation metricChi-kiu Lo and Karteek Addanki and Markus Saers and Dekai WuHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo|vskaddanki|masaers|dekai}@cs.ust.hkAbstractWe present the first ever results show-ing that tuning a machine translation sys-tem against a semantic frame based ob-jective function, MEANT, produces morerobustly adequate translations than tun-ing against BLEU or TER as measuredacross commonly used metrics and humansubjective evaluation.
Moreover, for in-formal web forum data, human evalua-tors preferredMEANT-tuned systems overBLEU- or TER-tuned systems by a sig-nificantly wider margin than that for for-mal newswire?even though automatic se-mantic parsing might be expected to fareworse on informal language.
We arguethat by preserving themeaning of the trans-lations as captured by semantic framesright in the training process, an MT sys-tem is constrained to make more accu-rate choices of both lexical and reorder-ing rules.
As a result, MT systems tunedagainst semantic frame based MT evalu-ation metrics produce output that is moreadequate.
Tuning a machine translationsystem against a semantic frame based ob-jective function is independent of the trans-lation model paradigm, so, any transla-tion model can benefit from the semanticknowledge incorporated to improve trans-lation adequacy through our approach.1 IntroductionWe present the first ever results of tuning a statis-tical machine translation (SMT) system against asemantic frame based objective function in orderto produce a more adequate output.
We comparethe performance of our system with that of twobaseline SMT systems tuned against BLEU andTER, the commonly used n-gram and edit distancebased metrics.
Our system performs better thanthe baseline across seven commonly used evalu-ation metrics and subjective human evaluation onadequacy.
Surprisingly, tuning against a seman-tic MT evaluation metric also significantly out-performs the baseline on the domain of informalweb forum data wherein automatic semantic pars-ing might be expected to fare worse.
These resultsstrongly indicate that using a semantic frame basedobjective function for tuning would drive develop-ment of MT towards direction of higher utility.Glaring errors caused by semantic role confu-sion that plague the state-of-the-art MT systemsare a consequence of using fast and cheap lexi-cal n-gram based objective functions like BLEUto drive their development.
Despite enforcing flu-ency it has been established that these metrics donot enforce translation utility adequately and oftenfail to preservemeaning closely (Callison-Burch etal., 2006; Koehn and Monz, 2006).We argue that instead of BLEU, a metric that fo-cuses on getting the meaning right should be usedas an objective function for tuning SMT so as todrive continuing progress towards higher utility.MEANT (Lo et al, 2012), is an automatic seman-tic MT evaluation metric that measures similaritybetween the MT output and the reference transla-tion via semantic frames.
It correlates better withhuman adequacy judgment than other automaticMT evaluation metrics.
Since a high MEANTscore is contingent on correct lexical choices aswell as syntactic and semantic structures, we be-lieve that tuning against MEANT would improveboth translation adequacy and fluency.Incorporating semantic structures into SMT bytuning against a semantic frame based evaluationmetric is independent of the MT paradigm.
There-fore, systems from different MT paradigms (suchas hierarchical, phrase based, transduction gram-mar based) can benefit from the semantic informa-tion incorporated through our approach.3752 Related WorkRelatively little work has been done towards bi-asing the translation decisions of an SMT systemto produce adequate translations that correctly pre-servewho did what to whom, when, where and why(Pradhan et al, 2004).
This is because the devel-opment of SMT systemswas predominantly drivenby tuning against n-gram based evaluation met-rics such as BLEU or edit distance based metricssuch as TER which do not sufficiently bias SMTsystem?s decisions to produce adequate transla-tions.
Although there has been a recent surge ofwork aimed towards incorporating semantics intothe SMT pipeline, none attempt to tune against asemantic objective function.
Below, we describesome of the attempts to incorporate semantic in-formation into the SMT and present a brief surveyon evaluation metrics that focus on rewarding se-mantically valid translations.Utilizing semantics in SMT In the past fewyears, there has been a surge of work aimed at in-corporating semantics into various stages of theSMT.
Wu and Fung (2009) propose a two-passmodel that reorders the MT output to match theSRL of the input, which is too late to affect thetranslation decisions made by the MT system dur-ing decoding.
In contrast, training against a se-mantic objective function attempts to improve thedecoding search strategy by incorporating a biastowards meaningful translations into the model in-stead of postprocessing its results.Komachi et al (2006) and Wu et al (2011) pre-process the input sentence to match the verb framealternations in the output side.
Liu and Gildea(2010) and Aziz et al (2011) use input side SRLto train a tree-to-string SMT system.
Xiong et al(2012) trained a discriminative model to predictthe position of the semantic roles in the output.All these approaches are orthogonal to the presentquestion of whether to train toward a semantic ob-jective function.
Any of the above models couldpotentially benefit from tuning with semantic met-rics.MT evaluation metrics As mentioned previ-ously, tuning against n-gram based metrics suchas BLEU (Papineni et al, 2002), NIST (Dod-dington, 2002), METEOR (Banerjee and Lavie,2005) does not sufficiently drive SMT into mak-ing decisions to produce adequate translationsthat correctly preserve ?who did what to whom,when, where and why?.
In fact, a number oflarge scale meta-evaluations (Callison-Burch etal., 2006; Koehn and Monz, 2006) report caseswhere BLEU strongly disagrees with human judg-ments of translation accuracy.
Tuning against editdistance based metrics such as CDER (Leusch etal., 2006), WER (Nie?en et al, 2000), and TER(Snover et al, 2006) also fails to sufficiently biasSMT systems towards producing translations thatpreserve semantic information.We argue that an SMT system tuned against anadequacy-oriented metric that correlates well withhuman adequacy judgement produces more ade-quate translations.
For this purpose, we chooseMEANT, an automatic semantic MT evaluationmetric that focuses on getting the meaning right bycomparing the semantic structures of the MT out-put and the reference.
We briefly describe someof the alternative semantic metrics below to justifyour choice.ULC (Gim?nez and M?rquez, 2007, 2008) isan aggregated metric that incorporates several se-mantic similarity features and shows improvedcorrelation with human judgement on translationquality (Callison-Burch et al, 2007; Gim?nezand M?rquez, 2007; Callison-Burch et al, 2008;Gim?nez and M?rquez, 2008) but no work hasbeen done towards tuning an MT system againstULC perhaps due to its expensive running time.Lambert et al (2006) did tune on QUEEN, a sim-plified version of ULC that discards the seman-tic features and is based on pure lexical features.Although tuning on QUEEN produced slightlymore preferable translations than solely tuning onBLEU, themetric does not make use of any seman-tic features and thus fails to exploit any potentialgains from tuning to semantic objectives.Although TINE (Rios et al, 2011) is an recall-oriented automatic evaluation metric which aimsto preserve the basic event structure, no work hasbeen done towards tuning an SMT system againstit.
TINE performs comparably to BLEU andworsethan METEOR on correlation with human ade-quacy judgment.In contrast to TINE, MEANT (Lo et al, 2012),which is the weighted f-score over the matched se-mantic role labels of the automatically aligned se-mantic frames and role fillers, outperforms BLEU,NIST, METEOR, WER, CDER and TER.
Thismakes it more suitable for tuning SMT systems toproduce much adequate translations.376newswire BLEU NIST METEOR no_syn METEOR WER CDER TER MEANTBLEU-tuned 29.85 8.84 52.10 55.42 67.88 55.67 58.40 0.1667TER-tuned 25.37 6.56 48.26 51.24 66.18 52.58 56.96 0.1578MEANT-tuned 25.91 7.81 50.15 53.60 67.76 54.56 58.61 0.1676Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire dataforum BLEU NIST METEOR no_syn METEOR WER CDER TER MEANTBLEU-tuned 9.58 4.10 31.77 34.63 80.09 64.54 76.12 0.1711TER-tuned 6.94 2.21 28.55 30.85 76.15 57.96 74.73 0.1539MEANT-tuned 7.92 3.11 30.40 33.08 77.32 61.01 74.64 0.1727Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data3 Tuning SMT against MEANTWe now show that using MEANT as an objec-tive function to drive minimum error rate training(MERT) of state-of-the-art MT systems improvesMT utility not only on formal newswire text, buteven on informal forum text, where automatic se-mantic parsing is difficult.Toward improving translation utility of state-of-the-art MT systems, we chose to use a strong andcompetitive system in the DARPA BOLT programas our baseline.
The baseline system is a Moseshierarchical model trained on a collection of LDCnewswire and a small portion of Chinese-Englishparallel web forum data, together with a 5-gramlanguage model.
For the newswire experiment, weused a collection of NIST 02-06 test sets as our de-velopment set and NIST 08 test set for evaluation.The development and test sets contain 6,331 and1,357 sentences respectively with four references.For the forum data experiment, the developmentand test sets were a held-out subset of the BOLTphase 1 training data.
The development and testsets contain 2,000 sentences and 1,697 sentenceswith one reference.We use ZMERT (Zaidan, 2009) to tune the base-line because it is a widely used, highly competi-tive, robust, and reliable implementation of MERTthat is also fully configurable and extensible withregard to incorporating new evaluation metrics.
Inthis experiment, we use aMEANT implementationalong the lines described in Lo et al (2012).In each experiment, we tune two contrastiveconventional 100-best MERT tuned baseline sys-tems on both newswire and forum data genres; onetuned against BLEU, an n-gram based evaluationmetric and the other using TER, an edit distancebased metric.
As semantic role labeling is expen-sive we only tuned using 10-best list for MEANT-tuned system.
Tuning against BLEU and TER tookaround 1.5 hours and 5 hours per iteration respec-tively whereas tuning against MEANT took about1.6 hours per iteration.4 ResultsOf course, tuning against any metric would maxi-mize the performance of the SMT system on thatparticular metric, but would be overfitting.
Forexample, something would be seriously wrongif tuning against BLEU did not yield the bestBLEU scores.
A far more worthwhile goal wouldbe to bias the SMT system to produce adequatetranslations while achieving the best scores acrossall the metrics.
With this as our objective, wepresent the results of comparing MEANT-tunedsystems against the baselines as evaluated on com-monly used automatic metrics and human ade-quacy judgement.Cross-evaluation using automatic metrics Ta-bles 1 and 2 show that MEANT-tuned systemsachieve the best scores across all other metrics inboth newswire and forum data genres, when avoid-ing comparison of the overfit metrics too similar tothe one the system was tuned on (the cells shadedin grey in the table: NIST and METEOR are n-gram based metrics, similar to BLEU while WERand CDER are edit distance based metrics, similarto TER).
In the newswire domain, however, oursystem achieves marginally lower TER score thanBLEU-tuned system.Figure 1 shows an example where the MEANT-tuned system produced a more adequate transla-tion that accurately preserves the semantic struc-ture of the input sentence than the two baselinesystems.
The MEANT scores for the MT outputfrom the BLEU-, TER- and MEANT-tuned sys-tems are 0.0635, 0.1131 and 0.2426 respectively.Both the MEANT score and the human evaluatorsrank the MT output from the MEANT-tuned sys-377Figure 1: Examples of machine translation output and the corresponding semantic parses from the [B]BLEU-, [T] TER-and [M]MEANT-tuned systems together with [IN] the input sentence and [REF] thereference translation.
Note that the MT output of the BLEU-tuned system has no semantic parse outputby the automatic shallow semantic parser.tem as the most adequate translation.
In this exam-ple, the MEANT-tuned system has translated thetwo predicates ????
and ????
in the input sen-tence into the correct form of the predicates ?at-tack?
and ?adopted?
in theMT output, whereas theBLEU-tuned system has translated both of themincorrectly (translates the predicates into nouns)and the TER-tuned system has correctly translatedonly the first predicate (into ?seized?)
and droppedthe second predicate.
Moreover, for the frame ????
in the input sentence, the MEANT-tuned sys-tem has correctly translated the ARG0 ?????????
into ?Hamas militants?
and the ARG1 ??????
into ?Gaza?.
However, the TER-tunedsystem has dropped the predicate ????
so thatthe corresponding arguments ?The Palestinian Au-thority?
and ?into a state of emergency?
have allbeen incorrectly associated with the predicate ???
/seized?.
This example shows that the transla-tion adequacy of SMT has been improved by tun-ing against MEANT because the MEANT-tunedsystem is more accurately preserving the semanticstructure of the input sentence.Our results show that MEANT-tuned systemmaintains a balance between lexical choices andword order because it performs well on n-grambased metrics that reward lexical matching andedit distance metrics that penalize incorrect wordorder.
This is not surprising as a high MEANTscore relies on a high degree of semantic structurematching, which is contingent upon correct lexi-cal choices as well as syntactic and semantic struc-tures.Human subjective evaluation In line with ouroriginal objective of biasing SMT systems towardsproducing adequate translations, we conduct a hu-man evaluation to judge the translation utility ofthe outputs produced by MEANT-, BLEU- andTER-tuned systems.
Following the manual eval-uation protocol of Lambert et al (2006), we ran-domly draw 150 sentences from the test set in eachdomain to form the manual evaluation set.
Table3 shows the MEANT scores of the two manualevaluation sets.
In both evaluation sets, like in thetest sets, the output from the MEANT-tuned sys-tem score slightly higher inMEANT than that fromthe BLEU-tuned system and significantly higherthan that from the TER-tuned system.
The outputof each tuned MT system along the input sentenceand the reference were presented to human evalu-ators.
Each evaluation set is ranked by two evalu-ators for measuring inter-evaluator agreement.Table 4 indicates that output of the MEANT-tuned system is ranked adequate more frequentlycompared to BLEU- and TER-tuned baselines forboth newswire and web forum genres.
The inter-378newswire forumBLEU-tuned 0.1564 0.1663TER-tuned 0.1203 0.1453MEANT-tuned 0.1633 0.1737Table 3: MEANT scores of each system in the 150-sentence manual evaluation set.newswire forumEval 1 Eval 2 Eval 1 Eval 2BLEU-tuned (B) 37 42 47 42TER-tuned (T) 22 24 28 23MEANT-tuned (M) 55 56 59 68B=T 14 12 0 0M=B 5 4 8 9M=T 4 4 4 4M=B=T 13 9 4 4Table 4: No.
of sentences ranked the most ade-quate by human evaluators for each system.H1 newswire forumMEANT-tuned > BLEU-tuned 80% 95%MEANT-tuned > TER-tuned 99% 99%Table 5: Significance level of accepting the alter-native hypothesis.evaluator agreement is 84% and 70% for newswireand forum data genres respectively.We performed the right-tailed two proportionsignificance test on human evaluation of the SMTsystem outputs for both the genres.
Table 5 showsthat the MEANT-tuned system generates more ad-equate translations than the TER-tuned system atthe 99% significance level for both newswire andweb forum genres.
The MEANT-tuned system isranked more adequate than the BLEU-tuned sys-tem at the 95% significance level on the web fo-rum genre and for the newswire genre the hypoth-esis is accepted at a significance level of 80%.The high inter-evaluator agreement and the signif-icance tests confirm that MEANT-tuned system isbetter at producing adequate translations comparedto BLEU- or TER-tuned systems.Informal vs. formal text The results of table4 and 5 also show that?surprisingly?the humanevaluators preferred MEANT-tuned system out-put over BLEU-tuned and TER-tuned system out-put by a far wider margin on the informal forumtext compared to the formal newswire text.
TheMEANT-tuned system is better than both base-lines at the 80% significance level for the formaltext genre.
For the informal text genre, it per-forms the two baselines at the 95% significancelevel.
Although one might expect an semanticframe dependent metric such as MEANT to per-form poorly on the domain of informal text, sur-prisingly, it nonetheless significantly outperformsthe baselines at the task of generating adequate out-put.
This indicates that the design of the MEANTevaluation metric is robust enough to tune an SMTsystem towards adequate output on informal textdomains despite the shortcomings of automaticshallow semantic parsing.5 ConclusionWe presented the first ever results to demon-strate that tuning an SMT system against MEANTproduces much adequate translation than tuningagainst BLEU or TER, as measured across allother commonly used metrics and human subjec-tive evaluation.
We also observed that tuningagainst MEANT succeeds in producing adequateoutput significantly more frequently even on theinformal text such as web forum data.
By pre-serving the meaning of the translations as capturedby semantic frames right in the training process,an MT system is constrained to make more accu-rate choices of both lexical and reordering rules.The performance of our system as measured acrossall commonly used metrics indicate that tuningagainst a semantic MT evaluation metric does pro-duce output which is adequate and fluent.We believe that tuning onMEANTwould proveequally useful for MT systems based on anyparadigm, especially where the model does notincorporate semantic information to improve theadequacy of the translations produced and usingMEANT as an objective function to tune SMTwould drive sustainable development of MT to-wards the direction of higher utility.AcknowledgmentThis material is based upon work supported inpart by the Defense Advanced Research ProjectsAgency (DARPA) under BOLT contract no.HR0011-12-C-0016, and GALE contract nos.HR0011-06-C-0022 and HR0011-06-C-0023; bythe European Union under the FP7 grant agree-ment no.
287658; and by the Hong KongResearch Grants Council (RGC) research grantsGRF620811, GRF621008, and GRF612806.
Anyopinions, findings and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the views ofDARPA, the EU, or RGC.379ReferencesWilker Aziz, Miguel Rios, and Lucia Specia.
Shal-low semantic trees for SMT.
In Proceedingsof the Sixth Workshop on Statistical MachineTranslation (WMT2011), 2011.Satanjeev Banerjee and Alon Lavie.
METEOR:An automatic metric forMT evaluation with im-proved correlation with human judgments.
InProceedings of the ACL Workshop on Intrinsicand Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 65?72, Ann Arbor, Michigan, June 2005.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in Ma-chine Translation Research.
In Proceedings ofthe 13th Conference of the European Chapterof the Association for Computational Linguis-tics (EACL-06), pages 249?256, 2006.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation.
InProceedings of the 2nd Workshop on StatisticalMachine Translation, pages 136?158, 2007.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.Further Meta-evaluation of Machine Transla-tion.
In Proceedings of the 3rd Workshop onStatistical Machine Translation, pages 70?106,2008.George Doddington.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the2nd International Conference on Human Lan-guage Technology Research, pages 138?145,San Diego, California, 2002.Jes?s Gim?nez and Llu?s M?rquez.
Linguisticfeatures for automatic evaluation of heteroge-nous MT systems.
In Proceedings of the Sec-ond Workshop on Statistical Machine Transla-tion, pages 256?264, Prague, Czech Republic,June 2007.Jes?s Gim?nez and Llu?sM?rquez.
A smorgasbordof features for automaticMT evaluation.
In Pro-ceedings of the Third Workshop on StatisticalMachine Translation, pages 195?198, Colum-bus, Ohio, June 2008.Philipp Koehn and Christof Monz.
Manual andAutomatic Evaluation of Machine Translationbetween European Languages.
In Proceedingsof the Workshop on Statistical Machine Trans-lation (WMT-06), pages 102?121, 2006.Mamoru Komachi, Yuji Matsumoto, and MasaakiNagata.
Phrase reordering for statistical ma-chine translation based on predicate-argumentstructure.
In Proceedings of the 3rd Interna-tional Workshop on Spoken Language Transla-tion (IWSLT 2006), 2006.Patrik Lambert, Jes?s Gim?nez, Marta R Costa-juss?, Enrique Amig?, Rafael E Banchs, Llu?sM?rquez, and JAR Fonollosa.
Machine Transla-tion system development based on human like-ness.
In Spoken Language Technology Work-shop, 2006.
IEEE, pages 246?249.
IEEE, 2006.Gregor Leusch, Nicola Ueffing, and HermannNey.
CDer: Efficient MT Evaluation UsingBlock Movements.
In Proceedings of the 13thConference of the European Chapter of the As-sociation for Computational Linguistics (EACL-06), 2006.Ding Liu and Daniel Gildea.
Semantic role fea-tures for machine translation.
In Proceedings ofthe 23rd international conference on Computa-tional Linguistics (COLING-10), 2010.Chi-kiu Lo, Anand Karthik Tumuluru, and DekaiWu.
Fully Automatic Semantic MT Evaluation.In Proceedings of the Seventh Workshop on Sta-tistical Machine Translation (WMT2012), 2012.Sonja Nie?en, Franz Josef Och, Gregor Leusch,and Hermann Ney.
A Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Re-search.
In Proceedings of the 2nd InternationalConference on Language Resources and Evalu-ation (LREC-2000), 2000.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a method for automaticevaluation of machine translation.
In Proceed-ings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318, Philadelphia, Pennsylvania, July 2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H.Martin, and Dan Jurafsky.
Shallow Se-mantic Parsing Using Support Vector Machines.In Proceedings of the 2004 Conference on Hu-man Language Technology and the North Amer-ican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL-04), 2004.Miguel Rios, Wilker Aziz, and Lucia Specia.
Tine:A metric to assess mt adequacy.
In Proceed-380ings of the Sixth Workshop on Statistical Ma-chine Translation, pages 116?122.
Associationfor Computational Linguistics, 2011.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
A studyof translation edit rate with targeted human an-notation.
In Proceedings of the 7th Conferenceof the Association for Machine Translation inthe Americas (AMTA-06), pages 223?231, Cam-bridge, Massachusetts, August 2006.Dekai Wu and Pascale Fung.
Semantic Roles forSMT: A Hybrid Two-Pass Model.
In Proceed-ings of the 2009 Conference of the North Amer-ican Chapter of the Association for Computa-tional Linguistics - Human Language Technolo-gies (NAACL-HLT-09), pages 13?16, 2009.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Ha-jime Tsukada, and Masaaki Nagata.
Extract-ing preordering rules from predicate-argumentstructures.
In Proceedings of the 5th Interna-tional Joint Conference on Natural LanguageProcessing (IJCNLP-11), 2011.Deyi Xiong, Min Zhang, and Haizhou Li.
Mod-eling the Translation of Predicate-ArgumentStructure for SMT.
In Proceedings of the Jointconference of the 50th AnnualMeeting of the As-sociation for Computational Linguistics (ACL-12), 2012.Omar F. Zaidan.
Z-MERT: A Fully Config-urable Open Source Tool for Minimum ErrorRate Training of Machine Translation Systems.The Prague Bulletin of Mathematical Linguis-tics, 91:79?88, 2009.381
