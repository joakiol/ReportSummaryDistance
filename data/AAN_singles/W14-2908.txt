Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54?58,Baltimore, Maryland, USA, June 22-27, 2014.c?2014 Association for Computational LinguisticsIs the Stanford Dependency Representation Semantic?Rachel Rudinger1and Benjamin Van Durme1,2Center for Language and Speech Processing1Human Language Technology Center of Excellence2Johns Hopkins Universityrudinger@jhu.edu, vandurme@cs.jhu.eduAbstractThe Stanford Dependencies are a deepsyntactic representation that are widelyused for semantic tasks, like RecognizingTextual Entailment.
But do they captureall of the semantic information a meaningrepresentation ought to convey?
This pa-per explores this question by investigatingthe feasibility of mapping Stanford depen-dency parses to Hobbsian Logical Form,a practical, event-theoretic semantic rep-resentation, using only a set of determin-istic rules.
Although we find that such amapping is possible in a large number ofcases, we also find cases for which such amapping seems to require information be-yond what the Stanford Dependencies en-code.
These cases shed light on the kindsof semantic information that are and arenot present in the Stanford Dependencies.1 IntroductionThe Stanford dependency parser (De Marneffe etal., 2006) provides ?deep?
syntactic analysis ofnatural language by layering a set of hand-writtenpost-processing rules on top of Stanford?s sta-tistical constituency parser (Klein and Manning,2003).
Stanford dependency parses are commonlyused as a semantic representation in natural lan-guage understanding and inference systems.1Forexample, they have been used as a basic meaningrepresentation for the Recognizing Textual Entail-ment task proposed by Dagan et al.
(2005), such asby Haghighi et al.
(2005) or MacCartney (2009)and in other inference systems (Chambers et al.,2007; MacCartney, 2009).Because of their popular use as a semantic rep-resentation, it is important to ask whether the Stan-ford Dependencies do, in fact, encode the kind of1Statement presented by Chris Manning at the*SEM 2013 Panel on Language Understandinghttp://nlpers.blogspot.com/2013/07/the-sem-2013-panel-on-language.html.information that ought to be present in a versa-tile semantic form.
This paper explores this ques-tion by attempting to map the Stanford Depen-dencies into Hobbsian Logical Form (henceforth,HLF), a neo-Davidsonian semantic representationdesigned for practical use (Hobbs, 1985).
Our ap-proach is to layer a set of hand-written rules ontop of the Stanford Dependencies to further trans-form the representation into HLFs.
This approachis a natural extension of the Stanford Dependen-cies which are, themselves, derived from manuallyengineered post-processing routines.The aim of this paper is neither to demonstratethe semantic completeness of the Stanford Depen-dencies, nor to exhaustively enumerate their se-mantic deficiencies.
Indeed, to do so would be topresuppose HLF as an entirely complete seman-tic representation, or, a perfect semantic standardagainst which to compare the Stanford Dependen-cies.
We make no such claim.
Rather, our intent isto provide a qualitative discussion of the StanfordDependencies as a semantic resource through thelens of this HLF mapping task.
It is only necessarythat HLF capture some subset of important seman-tic phenomena to make this exercise meaningful.Our results indicate that in a number of cases,it is, in fact, possible to directly derive HLFs fromStanford dependency parses.
At the same time,however, we also find difficult-to-map phenomenathat reveal inherent limitations of the dependen-cies as a meaning representation.2 BackgroundThis section provides a brief overview of the HLFand Stanford dependency formalisms.2.1 Hobbsian Logical FormThe key insight of event-theoretic semantic repre-sentations is the reification of events (Davidson,1967), or, treating events as entities in the world.As a logical, first-order representation, Hobbsian54Logical Form (Hobbs, 1985) employs this ap-proach by allowing for the reification of any pred-icate into an event variable.
Specifically, for anypredicate p(x1, ?
?
?
, xn), there is a correspondingpredicate, p?
(E, x1, ?
?
?
, xn), where E refers tothe predicate (or event) p(x1, ?
?
?
, xn).
The reifiedpredicates are related to their non-reified formswith the following axiom schema:(?x1?
?
?xn)p(x1?
?
?xn) ?
(?e)Exist(e) ?p?
(e, x1?
?
?xn)In HLF, ?A boy runs?
would be represented as:(?e, x)Exist(e) ?
run?
(e, x) ?
boy(x)and the sentence ?A boy wants to build a boatquickly?
(Hobbs, 1985) would be represented as:(?e1, e2, e3, x, y)Exist(e1) ?
want?
(e1, x, e2) ?quick?
(e2, e3)?build?
(e3, x, y)?boy(x)?boat(y)2.2 Stanford DependenciesA Stanford dependency parse is a set of triplesconsisting of two tokens (a governor and a depen-dent), and a labeled syntactic or semantic relationbetween the two tokens.
Parses can be renderedas labeled, directed graphs, as in Figure 1.
Notethat this paper assumes the collapsed version ofthe Stanford Dependencies.2Figure 1: Dependency parse of ?A boy wants tobuild a boat quickly.
?3 Mapping to HLFWe describe in this section our deterministic algo-rithm for mapping Stanford dependency parses toHLF.
The algorithm proceeds in four stages: event2The collapsed version is more convenient for our pur-poses, but using the uncollapsed version would not signifi-cantly affect our results.extraction, argument identification, predicate-argument assignment, and formula construction.We demonstrate these steps on the above examplesentence ?A boy wants to build a boat quickly.
?3The rule-based algorithm operates on the sen-tence level and is purely a function of the depen-dency parse or other trivially extractible informa-tion, such as capitalization.3.1 Event ExtractionThe first step is to identify the set of event predi-cates that will appear in the final HLF and assignan event variable to each.
Most predicates are gen-erated by a single token in the sentence (e.g., themain verb).
For each token t in the sentence, anevent (ei, pt) (where eiis the event variable and ptis the predicate) is added to the set of events if anyof the following conditions are met:1. t is the dependent of the relation root,ccomp, xcomp, advcl, advmod, orpartmod.2.
t is the governor of the relation nsubj, dobj,ccomp, xcomp, xsubj, advcl, nsubjpass,or agent.Furthermore, an event (ei, pr) is added for anytriple (rel, gov, dep) where rel is prefixed with?prep ?
(e.g., prep to, prep from, prep by, etc.
).Applying this step to our example sentence ?Aboy wants to build a boat quickly.?
yields the fol-lowing set:(e1, wants), (e2, quickly), (e3, build)3.2 Argument IdentificationNext, the set of entities that will serve as predicatearguments are identified.
Crucially, this set willinclude some event variables generated in the pre-vious step.
For each token, t, an argument (xi, t)is added to the set of arguments if one of the fol-lowing conditions is met:1. t is the dependent of the relation nsubj,xsubj, dobj, ccomp, xcomp, nsubjpass,agent, or iobj.2.
t is the governor of the relation advcl,advmod, or partmod.3Hobbs (1985) uses the example sentence ?A boy wantedto build a boat quickly.
?55Applying this step to our example sentence, weget the following argument set:(x1, boat), (x2, build), (x3, boy)Notice that the token build has generated bothan event predicate and an argument.
This is be-cause in our final HLF, build will be both an eventpredicate that takes the arguments boy and boat,as well as an argument to the intensional predicatewant.3.3 Predicate-Argument AssignmentIn this stage, arguments are assigned to each pred-icate.
pt.argidenotes the ithargument of pred-icate ptand arg(t) denotes the argument associ-ated with token t. For example, arg(boy) = x2and arg(quickly) = e3.
We also say that if thetoken t1governs t2by some relation, e.g.
nsubj,then t1nsubj-governs t2, or t2nsubj-depends ont1.
Note that argirefers to any slot past arg2.
Ar-guments are assigned as follows.For each predicate pt(corresponding to tokent):1.
If there is a token t?such that t nsubj-,xsubj-, or agent-governs t?, then pt.arg1=arg(t?).2.
If there is a token t?such that t dobj-governst?, then pt.arg2= arg(t?).3.
If there is a token t?such that t nsubjpass-governs t?, then pt.argi= arg(t?).4.
If there is a token t?such that t partmod-depends on t?, then pt.arg2= arg(t?).5.
If there is a token t?such that t iobj-governst?, then pt.argi= arg(t?).6.
If there is a token t?such that t ccomp- orxcomp-governs t?, then pt.argi= arg(t?
)(a) UNLESS there is a token t?
?such thatt?advmod-governs t?
?, in which casept.argi= arg(t??).7.
If there is a token t?such that t advmod- oradvcl-depends on t?, then pt.argi= arg(t?
).And for each prgenerated from relation(rel, gov, dep) (i.e.
all of the ?prep ?
relations):1. pr.arg1= arg(gov)2. pr.argi= arg(dep)After running this stage on our example sen-tence, the predicate-argument assignments are asfollows:wants(x3, e2), build(x3, x1), quickly(e3)Each predicate can be directly replaced with itsreified forms (i.e., p?):wants?
(e1, x3, e2),build?
(e3, x3, x1),quickly?
(e2, e3)Two kinds of non-eventive predicates still needto be formed.
First, every entity (xi, t) that isneither a reified event nor a proper noun, e.g.,(x3, boy), generates a predicate of the form t(xi).Second, we generate Hobbs?s Exist predicate,which identifies which event actually occurs in the?real world.?
This is simply the event generatedby the dependent of the root relation.3.4 Formula ConstructionIn this stage, the final HLF is pieced together.
Wejoin all of the predicates formed above with theand conjunction, and existentially quantify overevery variable found therein.
For our example sen-tence, the resulting HLF is:A boy wants to build a boat quickly.
(?e1, e2, e3, x1, x3)[Exist(e1) ?
boat(x1) ?boy(x3) ?
wants?
(e1, x3, e2) ?
build?
(e3, x3, x1)?
quickly?
(e2, e3)]4 Analysis of ResultsThis section discusses semantic phenomena thatour mapping does and does not capture, providinga lens for assessing the usefulness of the StanfordDependencies as a semantic resource.4.1 SuccessesFormulas 1-7 are correct HLFs that our mappingrules successfully generate.
They illustrate the di-versity of semantic information that is easily re-coverable from Stanford dependency parses.Formulas 1-2 show successful parses in sim-ple transitive sentences with active/passive alter-nations, and Formula 3 demonstrates success inparsing ditransitives.
Also easily recovered fromthe dependency structures are semantic parses ofsentences with adverbs (Formula 4) and reportingverbs (Formula 5).
Lest it appear that these phe-nomena may only be handled in isolation, Equa-tions 6-7 show successful parses for sentences56with arbitrary combinations of the above phenom-ena.A boy builds a boat.
(?e1, x1, x2)[Exist(e1) ?
boy(x2) ?
boat(x1)?
builds?
(e1, x2, x1)](1)A boat was built by a boy.
(?e1, x1, x2)[Exist(e1) ?
boat(x2) ?
boy(x1)?
built?
(e1, x1, x2)](2)John gave Mary a boat.
(?e1, x1)[Exist(e1) ?
boat(x1)?
gave?
(e1, John, x1,Mary)](3)John built a boat quickly.OR John quickly built a boat.
(?e1, e2, x1)[Exist(e1) ?
boat(x1) ?quickly(e2, e1) ?
built?
(e1, John, x1)](4)John told Mary that a boy built a boat.
(?e1, e2, x1, x4)[Exist(e1)?boy(x1)?boat(x4)?built?
(e2, x1, x4) ?
told?
(e1, John,Mary, e2)](5)John told Mary that Sue told Joethat Adam loves Eve.
(?e1, e2, e3)[Exist(e1)?told?
(e2, Sue, Joe, e3)?loves?
(e3, Adam,Eve) ?told?
(e1, John,Mary, e2)](6)John was told by Mary that Sue wantsJoe to build a boat quickly.
(?e1, e2, e3, e4, x7)[Exist(e1) ?
boat(x7) ?build?
(e2, Joe, x7)?told?
(e1,Mary, John, e4)?wants?
(e4, Sue, e3) ?
quickly?
(e3, e2)](7)4.2 LimitationsThough our mapping rules enable us to directly ex-tract deep semantic information directly from theStanford dependency parses in the above cases,there are a number of difficulties with this ap-proach that shed light on inherent limitations ofthe Stanford Dependencies as a semantic resource.A major such limitation arises in cases of eventnominalizations.
Because dependency parses aresyntax-based, their structures do not distinguishbetween eventive noun phrases like ?the bombingof the city?
and non-eventive ones like ?the motherof the child?
; such a distinction, however, wouldbe found in the corresponding HLFs.Certain syntactic alternations also prove prob-lematic.
For example, the dependency structuredoes not recognize that ?window?
takes the samesemantic role in the sentences ?John broke the mir-ror.?
and ?The mirror broke.?
The use of addi-tional semantic resources, like PropBank (Palmeret al., 2005), would be necessary to determine this.Prepositional phrases present another problemfor our mapping task, as the Stanford dependen-cies will typically not distinguish between PPsindicating arguments and adjuncts.
For exam-ple, ?Mary stuffed envelopes with coupons?
and?Mary stuffed envelopes with John?
have identicaldependency structures, yet ?coupons?
and ?John?are (hopefully for John) taking on different seman-tic roles.
This is, in fact, a prime example of howStanford dependency parses may resolve syntacticambiguity without resolving semantic ambiguity.Of course, one might manage more HLF cov-erage by adding more rules to our system, but thelimitations discussed here are fundamental.
If twosentences have different semantic interpretationsbut identical dependency structures, then there canbe no deterministic mapping rule (based on depen-dency structure alone) that yields this distinction.5 ConclusionWe have presented here our attempt to map theStanford Dependencies to HLF via a second layerof hand-written rules.
That our mapping rules,which are purely a function of dependency struc-ture, succeed in producing correct HLFs in somecases is good evidence that the Stanford Depen-dencies do contain some practical level of seman-tic information.
Nevertheless, we were also able toquickly identify aspects of meaning that the Stan-ford Dependencies did not capture.Our argument does not require that HLF be anoptimal representation, only that it capture worth-while aspects of semantics and that it not be read-ily derived from the Stanford representation.
Thisis enough to conclude that the Stanford Dependen-cies are not complete as a meaning representation.While not surprising (as they are intended as asyntactic representation), we hope this short studywill help further discussion on what the commu-nity wants or needs in a meaning representation:what gaps are acceptable, if any, and whether amore ?complete?
representation is needed.AcknowledgmentsThis material is partially based on research spon-sored by the NSF under grant IIS-1249516 andDARPA under agreement number FA8750-13-2-0017 (the DEFT program).57ReferencesNathanael Chambers, Daniel Cer, Trond Grenager,David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh,and Christopher D Manning.
2007.
Learning align-ments and leveraging natural logic.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing, pages 165?170.
Associa-tion for Computational Linguistics.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailmentchallenge.
In Proceedings of the PASCAL Chal-lenges Workshop on Recognising Textual Entail-ment.Donald Davidson.
1967.
The logical form of actionsentences.
In The Logic of Decision and Action,pages 81?120.
Univ.
of Pittsburgh Press.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, volume 6, pages 449?454.Aria D Haghighi, Andrew Y Ng, and Christopher DManning.
2005.
Robust textual inference via graphmatching.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 387?394.Association for Computational Linguistics.Jerry R Hobbs.
1985.
Ontological promiscuity.
InProceedings of the 23rd annual meeting on Associ-ation for Computational Linguistics, pages 60?69.Association for Computational Linguistics.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Bill MacCartney.
2009.
Natural language inference.Ph.D.
thesis, Stanford University.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.58
