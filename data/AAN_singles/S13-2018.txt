Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 103?107, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsIIRG: A Na?
?ve Approach to Evaluating Phrasal SemanticsLorna Byrne, Caroline Fenlon, John DunnionSchool of Computer Science and InformaticsUniversity College DublinIreland{lorna.byrne@ucd.ie,caroline.fenlon@ucdconnect.ie,john.dunnion@ucd.ie}AbstractThis paper describes the IIRG 1 system en-tered in SemEval-2013, the 7th InternationalWorkshop on Semantic Evaluation.
We partic-ipated in Task 5 Evaluating Phrasal Semantics.We have adopted a token-based approach tosolve this task using 1) Na?
?ve Bayes methodsand 2) Word Overlap methods, both of whichrely on the extraction of syntactic features.
Wefound that the word overlap method signifi-cantly out-performs the Na?
?ve Bayes methods,achieving our highest overall score with an ac-curacy of approximately 78%.1 IntroductionThe Phrasal Semantics task consists of two relatedsubtasks.
Task 5A requires systems to evaluatethe semantic similarity of words and compositionalphrases.
Task 5B requires systems to evaluate thecompositionality of phrases in context.
We partici-pated in Task 5B and submitted three runs for eval-uation, two runs using the Na?
?ve Bayes MachineLearning Algorithm and a Word Overlap run usinga simple bag-of-words approach.Identifying non-literal expressions poses a majorchallenge in NLP because they occur frequently andoften exhibit irregular behavior by not adhering togrammatical constraints.
Previous research in thearea of identifying literal/non-literal use of expres-sions includes generating a wide range of differentfeatures for use with a machine learning predictionalgorithm.
(Li and Sporleder, 2010) present a system1Intelligent Information Retrieval Groupinvolving identifying the global and local contexts ofa phrase.
Global context was determined by look-ing for occurrences of semantically related wordsin a given passage, while local context focuses onthe words immediately preceding and following thephrase.
Windows of five words at each side of thetarget were taken as features.
More syntactic fea-tures were also used, including details of nodes fromthe dependency tree of each example.
The systemproduced approximately 90% accuracy when tested,for both idiom-specific and generic models.
It wasfound that the statistical features (global and localcontexts) performed well, even on unseen phrases.
(Katz and Giesbrecht, 2006) found that similaritiesbetween words in the expression and its context indi-cate literal usage.
This is comparable to (Sporlederand Li, 2009), which used cohesion-based classi-fiers based on lexical chains and graphs.
Unsu-pervised approaches to classifying idiomatic use in-clude clustering (Fazly et al 2009), which classifieddata based on semantic analyzability (whether themeaning of the expression is similar to the mean-ings of its parts) and lexical and syntactic flexibility(measurements of how much variation exists withinthe expression).2 Task 5BIn Task 5B, participants were required to make abinary decision as to whether a target phrase is usedfiguratively or literally within a given context.
Thephrase ?drop the ball?
can be used figuratively, forexample in the sentenceWe get paid for completing work, so we?ve designeda detailed workflow process to make sure we don?t103drop the ball.and literally, for example in the sentenceIn the end, the Referee drops the ball with theattacking player nearby.In order to train systems, participants were giventraining data consisting of approximately 1400 textsnippets (one or more sentences) containing 10 tar-get phrases, together with real usage examples sam-pled from the WaCky (Baroni et al 2009) corpora.The number of examples and distribution of figura-tive and literal instances varied for each phrase.Participants were allowed to submit three runs forevaluation purposes.2.1 ApproachThe main assumption for our approach is that tokenspreceding and succeeding the target phrase might in-dicate the usage of the target phrase, i.e.
whether thetarget phrase is being used in a literal or figurativecontext.
Firstly, each text snippet was processed us-ing the Stanford Suite of Core NLP Tools2 to to-kenise the snippet and produce part-of-speech tagsand lemmas for each token.During the training phase, we identified and ex-tracted a target phrase boundary for each of the tar-get phrases.
A target phrase boundary consists of awindow of tokens immediately before and after thetarget phrase.
The phrase boundaries identified forthe first two runs were restricted to windows of one,i.e.
the token immediately before and after the targetphrase were extracted, tokens were also restricted tothe canonical form.For example, the target phrase boundary iden-tified for the snippet: ?The returning team willdrop the ball and give you a chance to recover .?
isas follows:before:willafter:andand the target phrase boundary identified forthe snippet: ?Meanwhile , costs are goingthrough the roof .?
is as follows:before:goafter:.2http://nlp.stanford.edu/softwareIIRG Training RunsRunID Accuracy (%)Run0 85.29Run1 81.84Run2 95.92Table 1: Results of IIRG Training RunsWe then trained multiple Na?
?ve Bayes classifierson these extracted phrase boundaries.
The first clas-sifier was trained on the set of target phrase bound-aries extracted from the entire training set of tar-get phrases and usage examples (Run0); the sec-ond classifier was trained on the set of target phraseboundaries extracted from the entire training set oftarget phrases and usage examples including thephrase itself as a predictor variable (Run1); and aset of target-phrase classifiers, one per target phrase,were trained on the set of target phrase boundariesextracted from each individual target phrase (Run2).The results of the initial training runs can beseen in Table 1.
Although Run0 yielded very highaccuracy scores on the training data, outperformingRun1, in practice this approach performed poorlyon unseen data and was biased towards a figurativeclassification.
We thus opted not to implement thisrun in the testing phase and instead concentrated onRun1 and Run2.For our third submitted run, we adopted a wordoverlap method which implemented a simple bag-of-words approach.
For each target phrase we cre-ated a bag-of-words by selecting the canonical formof all of the noun tokens in each corresponding train-ing usage example.
The frequency of occurrenceof each token within a given context was recordedand each token was labeled as figurative or literaldepending on its frequency of occurrence within agiven context.
The frequency of occurrence of eachtoken was also recorded in order to adjust the thresh-old of token occurrences for subsequent runs.
Forthis run, Run3, the token frequency threshold wasset to 2, so that a given token must occur two or moretimes in a given context to be added to the bag-of-words.1043 ResultsSystem performance is measured in terms of accu-racy.
The results of the submitted runs can be seenin Table 2.Of the submitted runs, the Word Overlap method(Run3) performed best overall.
This approach wasalso consistently good across all phrases, with scoresranging from 70% to 80%, as seen in Table 3.The classifiers trained on the canonical phraseboundaries (Run1 and Run2) performed poorly onunseen data.
They were also biased towards a figura-tive prediction.
For several phrases they incorrectlyclassified all literal expressions as figurative.
Theywere not effective at processing all of the phrases:in Run1, some phrases had very high scores rela-tive to the overall score (e.g.
?break a leg?
), whileothers scored very poorly (e.g.
?through the roof?
).In Run2, a similar effect was found.
Interestingly,even though separate classifiers were trained foreach phrase, the accuracy was lower than that ofRun1 in several cases (e.g.
?through the roof?).
Thismay be a relic of the small, literally-skewed, train-ing data for some of the phrases, or may suggest thatthis approach is not suitable for those expressions.The very high accuracy of the classifiers tested on asubset of the training data may be attributed to over-fitting.
The approach used in Run1 and Run2 is un-likely to yield very accurate results for the classifi-cation of general data, due to the potential for manyunseen canonical forms of word boundaries.3.1 Additional RunsAfter the submission deadline, we completed someadditional runs, the results of which can be seen inTable 4.These runs were similar to Run1 and Run2, wherewe used Na?
?ve Bayes Classifiers to train on ex-tracted target phrase boundaries.
However, for Run4and Run5 we restricted the phrase boundaries to thecanonical form of the nearest verb (Run4) or nearestnoun (Run5) that was present in a bag-of-words.We used the same bag-of-words created for Run3for the noun-based bag-of-words, and this same ap-proach was used to create the (canonical form) verb-based bag-of-words.
If there were no such verbs ornouns present then the label NULL was applied.
Ifa phrase occurred at the start or end of a text snip-pet this information was also captured.
The Na?
?veBayes classifiers were then trained using labels fromthe following set of input labels: FIGURATIVE,LITERAL, START, END or NULL, which indicatethe target phrase boundaries of the target phrases.For example, the target phrase boundaries identi-fied for the snippet: ?Meanwhile , costs are goingthrough the roof .?
for Run4 and Run5, respectively,are as follows:before:FIGURATIVEafter:ENDwhere the FIGURATIVE label is the classificationof the token ?going?
as indicated in the verb-basedbag-of-words, andbefore:FIGURATIVEafter:ENDwhere the FIGURATIVE label is the classificationof the token ?costs?
as indicated in the noun-basedbag-of-words.As in Run1 and Run2, an entire-set classifier andindividual target-phrase classifiers were trained forboth runs.
These additional runs performed well,yielding high accuracy results and significantly out-performing Run1 and Run2.The Run4 classifiers did not perform compara-tively well across all phrases.
In particular, the targetphrase ?break a leg?, had very low accuracy scores,possibly because the training data for the phrase wassmall and contained mostly literal examples.
Theranges of phrase scores for the noun classificationruns (Run5) were similar to those of the Word Over-lap runs.
The results across each phrase were alsoconsistent, with no scores significantly lower thanthe overall accuracy.
Using target phrase boundariesbased on noun classifications may prove to yield rea-sonable results when extended to more phrases, asopposed to the erratic results found when using verbclassifications.In both Run4 and Run5, very similar overall re-sults were produced from both the entire-set andtarget-phrase classifiers.
In most cases, the run per-formed poorly on the same phrases in both instances,indicating that the approach may not be appropriatefor the particular phrase.
For example, the verb clas-sifications runs scored low accuracy for ?drop theball?, while the noun classifications run was approx-imately 80% accurate for the same phrase using both105IIRG Submitted Runs (%)RunID OverallAccuracyPrecision(Figurative)Recall(Figurative)Precision(Literal)Recall(Literal)Run1 53.03 52.03 89.97 60.25 15.65Run2 50.17 50.81 41.81 54.06 58.84Run3 77.95 79.65 75.92 76.62 80.27Table 2: Results of Runs Submitted to Sem-Eval 2013IIRG Submitted Runs - Per Phrase Accuracy (%)RunID At theendof thedayBreadandbutterBreaka legDroptheballIn thebagIn thefastlanePlayballRubit inThroughthe roofUnderthemicro-scopeRun1 68.92 57.89 40.00 40.82 43.42 67.86 52.63 66.67 64.94 33.33Run2 45.95 38.16 83.33 57.14 48.68 75.00 46.05 56.67 29.87 62.82Run3 75.68 82.89 73.33 83.67 72.37 75.00 78.95 60.00 80.52 83.33Table 3: Results of Runs Submitted to Sem-Eval 2013 (per phrase)IIRG Additional Runs - Accuracy (%)RunID Entire-SetClassifierTarget-PhraseClassifierRun4 64.81 65.99Run5 75.25 76.60Table 4: Accuracy of Additional Unsubmitted Runsan entire-set and target-phrase classifier.4 ConclusionThis is the first year we have taken part in the Se-mantic Evaluation Exercises, participating in Task5b, Evaluating Phrasal Semantics.
Task 5B requiressystems to evaluate the compositionality of phrasesin context.
We have adopted a token-based approachto solve this task using 1) Na?
?ve Bayes methodswhereby target phrase boundaries were identifiedand extracted in order to train multiple classifiers;and 2) Word Overlap methods, whereby a simplebag-of-words was created for each target phrase.
Wesubmitted three runs for evaluation purposes, tworuns using Na?
?ve Bayes methods (Run1 and Run2)and one run based on a Word Overlap approach(Run3).
The Word Overlap approach, which limitedeach bag-of-words to using the canonical form of thenouns in the text snippets, yielded the highest accu-racy scores of all submitted runs, at approximately78% accurate.
An additional run (Run5), also us-ing the canonical form of the nouns in the usage ex-amples but implementing a Na?
?ve Bayes approach,yielded similar results, almost 77% accuracy.
Theapproaches which were restricted to using the nounsin the text snippets yielded the highest accuracy re-sults, thus indicating that nouns provide importantcontextual information for distinguishing literal andfigurative usage.In future work, we will explore whether we canimprove the performance of the target phrase bound-aries by experimenting with the local context win-dow sizes.
Another potential improvement mightbe to examine whether implementing more sophisti-cated strategies for selecting tokens for the bags-of-words improves the effectiveness of the Word Over-lap methods.ReferencesM.
Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.2009.
The WaCky Wide Web: A Collection ofVery Large Linguistically Processed Web-CrawledCorpora.
Language Resources and Evaluation 43,3(3):209?226.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised Type and Token Identificationof Idiomatic Expressions.
Computational Linguistics,35(1):61?103, March.106Graham Katz and Eugenie Giesbrecht.
2006.
AutomaticIdentification of Non-Compositional Multi-Word Ex-pressions using Latent Semantic Analysis.
In Pro-ceedings of the ACL/COLING-06 Workshop on Multi-word Expressions: Identifying and Exploiting Under-lying Properties, pages 12?19.Linlin Li and Caroline Sporleder.
2010.
Linguistic Cuesfor Distinguishing Literal and Non-Literal Usages.
InProceedings of the 23rd International Conference onComputational Linguistics (COLING ?10), pages 683?691.Caroline Sporleder and Linlin Li.
2009.
UnsupervisedRecognition of Literal and Non-Literal Use of Id-iomatic expressions.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL (EACL2009).107
