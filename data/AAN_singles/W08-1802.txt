Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 9?16Manchester, UK.
August 2008Exact Phrases in Information Retrieval for Question AnsweringSvetlana Stoyanchev, and Young Chol Song, and William LahtiDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400svetastenchikova, nskystars, william.lahti @gmail.comAbstractQuestion answering (QA) is the task offinding a concise answer to a natural lan-guage question.
The first stage of QA in-volves information retrieval.
Therefore,performance of an information retrievalsubsystem serves as an upper bound for theperformance of a QA system.
In this workwe use phrases automatically identifiedfrom questions as exact match constituentsto search queries.
Our results show an im-provement over baseline on several docu-ment and sentence retrieval measures onthe WEB dataset.
We get a 20% relativeimprovement in MRR for sentence extrac-tion on the WEB dataset when using au-tomatically generated phrases and a fur-ther 9.5% relative improvement when us-ing manually annotated phrases.
Surpris-ingly, a separate experiment on the indexedAQUAINT dataset showed no effect on IRperformance of using exact phrases.1 IntroductionQuestion answering can be viewed as a sophisti-cated information retrieval (IR) task where a sys-tem automatically generates a search query froma natural language question and finds a conciseanswer from a set of documents.
In the open-domain factoid question answering task systemsanswer general questions like Who is the creator ofThe Daily Show?, or When was Mozart born?.
Avariety of approaches to question answering havebeen investigated in TREC competitions in the lastc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.decade from (Vorhees and Harman, 1999) to (Danget al, 2006).
Most existing question answeringsystems add question analysis, sentence retrievaland answer extraction components to an IR sys-tem.Since information retrieval is the first stage ofquestion answering, its performance is an up-per bound on the overall question answering sys-tem?s performance.
IR performance depends onthe quality of document indexing and query con-struction.
Question answering systems create asearch query automatically from a user?s question,through various levels of sophistication.
The sim-plest way of creating a query is to treat the wordsin the question as the terms in the query.
Somequestion answering systems (Srihari and Li, 1999)apply linguistic processing to the question, iden-tifying named entities and other query-relevantphrases.
Others (Hovy et al, 2001b) use ontolo-gies to expand query terms with synonyms and hy-pernyms.IR system recall is very important for questionanswering.
If no correct answers are present in adocument, no further processing will be able tofind an answer.
IR system precision and rank-ing of candidate passages can also affect questionanswering performance.
If a sentence without acorrect answer is ranked highly, answer extrac-tion may extract incorrect answers from these erro-neous candidates.
Collins-Thompson et al (2004)show that there is a consistent relationship betweenthe quality of document retrieval and the overallperformance of question answering systems.In this work we evaluate the use of exact phrasesfrom a question in document and passage retrieval.First, we analyze how different parts of a ques-tion contribute to the performance of the sentenceextraction stage of question answering.
We ana-9lyze the match between linguistic constituents ofdifferent types in questions and sentences contain-ing candidate answers.
For this analysis, we use aset of questions and answers from the TREC 2006competition as a gold standard.Second, we evaluate the performance of doc-ument retrieval in our StoQA question answeringsystem.
We compare the performance of docu-ment retrieval from the Web and from an indexedcollection of documents using different methods ofquery construction, and identify the optimal algo-rithm for query construction in our system as wellas its limitations.Third, we evaluate passage extraction from a setof documents.
We analyze how the specificity of aquery affects sentence extraction.The rest of the paper is organized as follows:In Section 2, we summarize recent approaches toquestion answering.
In Section 3, we describe thedataset used in this experiment.
In Section 5, wedescribe our method and data analysis.
In Sec-tion 4, we outline the architecture of our questionanswering system.
In Section 6, we describe ourexperiments and present our results.
We summa-rize in Section 7.2 Related WorkInformation retrieval (IR) for question answeringconsists of 2 steps: document retrieval and passageretrieval.Approaches to passage retrieval include sim-ple word overlap (Light et al, 2001), density-based passage retrieval (Clarke et al, 2000), re-trieval based on the inverse document frequency(IDF) of matched and mismatched words (Itty-cheriah et al, 2001), cosine similarity between aquestion and a passage (Llopis and Vicedo, 2001),passage/sentence ranking by weighting differentfeatures (Lee and others, 2001), stemming andmorphological query expansion (2004), and vot-ing between different retrieval methods (Tellexet al, 2003).
As in previous approaches, weuse words and phrases from a question for pas-sage extraction and experiment with using exactlymatched phrases in addition to words.
Similarlyto Lee (2001), we assign weights to sentences inretrieved documents according to the number ofmatched constituents.Systems vary in the size of retrieved passages.Some systems identify multi-sentence and variablesize passages (Ittycheriah et al, 2001; Clarke etal., 2000).
An optimal passage size may dependon the method of answer extraction.
We use singlesentence extraction because our system?s semanticrole labeling-based answer extraction functions onindividual sentences.White and Sutcliffe (2004) performed a man-ual analysis of questions and answers for 50 of theTREC questions.
The authors computed frequencyof terms matching exactly, with morphological, orsemantic variation between a question and a an-swer passage.
In this work we perform a similaranalysis automatically.
We compare frequenciesof phrases and words matching between a questionand candidate sentences.Query expansion has been investigated in sys-tems described in (Hovy et al, 2001a; Harabagiuet al, 2006).
They use WordNet (Miller, 1995) forquery expansion, and incorporate semantic roles inthe answer extraction process.
In this experimentwe do not expand query terms.Corpus pre-processing and encoding informa-tion useful for retrieval was shown to improve doc-ument retrieval (Katz and Lin, 2003; Harabagiuet al, 2006; Chu-Carroll et al, 2006).
In ourapproach we evaluate linguistic question process-ing technique which does not require corpus pre-processing.Statistical machine translation model is usedfor information retrieval by (Murdock and Croft,2005).
The model estimates probability of a ques-tion given an answer and is trained on <question,candidate sentence> pairs.
It capturing synonymyand grammar transformations using a statisticalmodel.3 DataIn this work we evaluate our question answeringsystem on two datasets: the AQUAINT corpus, a 3gigabyte collection of news documents used in theTREC 2006 competition; and the Web.We use questions from TREC, a yearly ques-tion answering competition.
We use a subsetof questions with non-empty answers 1 from theTREC 2006 dataset 2.
The dataset provides a listof matching documents from the AQUAINT cor-pus and correct answers for each question.
Thedataset contains 387 questions; the AQUAINT cor-pus contains an average of 3.5 documents per ques-1The questions where an answer was not in the datasetwere not used in this analysis2http://trec.nist.gov/data/qa/t2006 qadata.html10tion that contain the correct answer to that ques-tion.
Using correct answers we find the correctsentences from the matching documents.
We usethis information as a gold standard for the IR task.We index the documents in the AQUAINT cor-pus using the Lucene (Apache, 2004 2008) engineon the document level.
We evaluate document re-trieval using gold standard documents from theAQUAINT corpus.
We evaluate sentence extrac-tion on both AQUAINT and the Web automaticallyusing regular expressions for correct answers pro-vided by TREC.In our experiments we use manually and auto-matically created phrases.
Our automatically cre-ated phrases were obtained by extracting noun,verb and prepositional phrases and named entitiesfrom the question dataset using then NLTK (Birdet al, 2008) and Lingpipe (Carpenter and Bald-win, 2008) tools.
Our manually created phraseswere obtained by hand-correcting these automaticannotations (e.g.
to remove extraneous words andphrases and add missed words and phrases fromthe questions).4 SystemFor the experiments in this paper we use the StoQAsystem.
This system employs a pipeline architec-ture with three main stages as illustrated in Fig-ure 1: question analysis, document and sentenceextraction (IR), and answer extraction.
After theuser poses a question, it is analyzed.
Target namedentities and semantic roles are determined.
Aquery is constructed, tailored to the search tools inuse.
Sentences containing target terms are then ex-tracted from the documents retrieved by the query.The candidate sentences are processed to identifyand extract candidate answers, which are presentedto the user.We use the NLTK toolkit (Bird et al, 2008)for question analysis and can add terms to searchqueries using WordNet (Miller, 1995).
Our systemcan currently retrieve documents from either theWeb (using the Yahoo search API (Yahoo!, 2008)),or the AQUAINT corpus (Graff, 2002) (throughthe Lucene indexer and search engine (Apache,2004 2008)).
When using Lucene, we can assigndifferent weights to different types of search term(e.g.
less weight to terms than to named entitiesadded to a query) (cf.
(Lee and others, 2001)).We currently have two modules for answer ex-traction, which can be used separately or together.Candidate sentences can be tagged with named en-tity information using the Lydia system (Lloyd etal., 2005).
The tagged word/phrase matching thetarget named entity type most frequently found ischosen as the answer.
Our system can also extractanswers through semantic role labeling, using theSRL toolkit from (Punyakanok et al, 2008).
Inthis case, the tagged word/phrase matching the tar-get semantic role most frequently found is chosenas the answer.Figure 1: Architecutre of our question answeringsystem5 Method5.1 MotivationQuestion answering is an engineering-intensivetask.
System performance improves as more so-phisticated techniques are applied to data process-ing.
For example, the IR stage in question an-swering is shown to improve with the help of tech-niques like predictive annotations and relation ex-traction; matching of semantic and syntactic re-11Target United NationsQuestion What was the number of member nations of the U.N. in 2000?Named Entity U.N., United NationsPhrases ?member nations of the U.N.?Converted Q-phrase ?member nations of the U.N. in 2000?Baseline Query was the number of member nations of the U.N. in 2000United NationsLucene Query with phrases was the number of member nations of the U.N. in 2000and NE ?United Nations?, ?member nations of the u.n.?Cascaded web queryquery1 ?member nations of the U.N. in 2000?
AND ( United Nations )query2 ?member nations of the u.n.?
AND ( United Nations )query3 (number of member nations of the U.N. in 2000) AND ( UnitedNations )query4 ( United Nations )Table 1: Question processing example: terms of a querylations in a question and a candidate sentenceare known to improve overall QA system perfor-mance (Prager et al, 2000; Stenchikova et al,2006; Katz and Lin, 2003; Harabagiu et al, 2006;Chu-Carroll et al, 2006).In this work we analyze less resource expensivetechniques, such as chunking and named entity de-tection, for IR in question answering.
Linguisticanalysis in our system is applied to questions andto candidate sentences only.
There is no need forannotation of all documents to be indexed, so ourtechniques can be applied to IR on large datasetssuch as the Web.Intuitively, using phrases in query constructionmay improve retrieval precision.
For example,if we search for In what year did the movie winacademy awards?
using a disjunction of wordsas our query we may match irrelevant documentsabout the military academy or Nobel prize awards.However, if we use the phrase ?academy awards?as one of the query terms, documents with thisterm will receive a higher ranking.
A counterargu-ment for using phrases is that academy and awardsare highly correlated and therefore the documentsthat contain both will be more highly ranked.
Wehypothesize that for phrases where constituents arenot highly correlated, exact phrase extraction willgive more benefit.5.2 Search QueryWe process each TREC question and target 3 toidentify named entities.
Often, the target is a com-plete named entity (NE), however, in some of theTREC questions the target contains a named entity,e.g.
tourists massacred at Luxor in 1997, or 1991eruption of Mount Pinatubo with named entitiesLuxor and Mount Pinatubo.
For the TREC ques-tion What was the number of member nations ofthe U.N. in 2000?, the identified constituents andautomatically constructed query are shown in Ta-ble 1.
Named entities are identified using Ling-pipe (Carpenter and Baldwin, 2008), which iden-tifies named entities of type organization, locationand person.
Phrases are identified automaticallyusing the NLTK toolkit (Bird et al, 2008).
Weextract noun phrases, verb phrases and preposi-tional phrases.
The rules for identifying phrasesare mined from a dataset of manually annotatedparse trees (Judge et al, 2006) 4.
Converted Q-phrases are heuristically created phrases that para-phrase the question in declarative form using asmall set of rules.
The rules match a question to apattern and transform the question using linguisticinformation.
For example, one rule matches Whois|was NOUN|PRONOUN VBD and converts it toNOUN|PRONOUN is|was VBD.
53The TREC dataset alo provides a target topic for eachquestions, and we include it in the query.4The test questions are not in this dataset.5Q-phrase is extracted only for who/when/where ques-tions.
We used a set of 6 transformation patterns in this ex-periment.12Named Entities Phrasesgreat pyramids; frank sinatra; mt.pinatubo; miss america; manchesterunited; clinton administrationcapacity of the ballpark; groath rate; se-curity council; tufts university endow-ment; family members; terrorist organi-zationTable 2: Automatically identified named entities and phrasesA q-phrase represents how a simple answer isexpected to appear, e. g. a q-phrase for the ques-tion When was Mozart born?
is Mozart was born.We expect a low probability of encountering a q-phrase in retrieved documents, but a high prob-ability of co-occurrence of q-phrases phrase withcorrect answers.In our basic system (baseline), words (trivialquery constituents) from question and target formthe query.
In the experimental system, the query iscreated from a combination of words, quoted exactphrases, and quoted named entities.
Table 2 showssome examples of phrases and named entities usedin queries.
The goal of our analysis is to evaluatewhether non-trivial query constituents can improvedocument and sentence extraction.We use a back-off mechanism with both ofour IR subsystems to improve document extrac-tion.
The Lucene API allows the user to cre-ate arbitrarily long queries and assign a weight toeach query constituent.
We experiment with as-signing different weights based on the type of aquery constituent.
Assigning a higher weight tophrase constituents increases the scores for docu-ments matching a phrase, but if no phrase matchesare found documents matching lower-scored con-stituents will be returned.The query construction system for the Web firstproduces a query containing only converted q-phrases which have low recall and high precision(query 1 in table 1).
If this query returns less than20 results, it then constructs a query using phrases(query 2 in table 1), if this returns less than 20 re-sults, queries without exact phrases (queries 3 and4) are used.
Every query contains a conjunctionwith the question target to increase precision forthe cases where the target is excluded from con-verted q-phrase or an exact phrase.For both our IR subsystems we return a maxi-mum of 20 documents.
We chose this relativelylow number of documents because our answer ex-traction algorithm relies on semantic tagging ofcandidate sentences, which is a relatively time-consuming operation.The text from each retrieved documents is splitinto sentences using Lingpipe.
The same sen-tence extraction algorithm is used for the outputfrom both IR subsystems (AQUAINT/Lucene andWeb/Yahoo).
The sentence extraction algorithmassigns a score to each sentence according to thenumber of matched terms it contains.5.3 Analysis of ConstituentsFor our analysis of the impact of different linguis-tic constituent types on document retrieval we usethe TREC 2006 dataset which consists of ques-tions, documents containing answers to each ques-tion, and supporting sentences, sentences fromthese documents that contain the answer to eachquestion.Table 3 shows the number of times each con-stituent type appears in a supporting sentence andthe proportion of supporting sentences containingeach constituent type (sent w/answer column).
The?All Sentences?
column shows the number of con-stituents in all sentences of candidate documents.The precision column displays the chance that agiven sentence is a supporting sentence if a con-stituent of a particular type is present in it.
Con-verted q-phrase has the highest precision, followedby phrases, verbs, and named entities.
Words havethe highest chance of occurrence in a supportingsentence (.907), but they also have a high chanceof occurrence in a document (.745).This analysis supports our hypothesis that usingexact phrases may improve the performance of in-formation retrieval for question answering.6 ExperimentIn these experiments we look at the impact of usingexact phrases on the performance of the documentretrieval and sentence extraction stages of questionanswering.
We use our StoQA question answeringsystem.
Questions are analyzed as described in theprevious section.
For document retrieval we usethe back-off method described in the previous sec-13sent w/ answer all sentences precisionnum proportion num proportionNamed Entity 907 0.320 4873 0.122 .18Phrases 350 0.123 1072 0.027 .34Verbs 396 0.140 1399 0.035 .28Q-Phrases 11 0.004 15 0.00038 .73Words 2573 0.907 29576 0.745 .086Total Sentences 2836 39688Table 3: Query constituents in sentences of correct documentsavg doc avg doc overall avg overall avg corr avg corr avg corrsent sent sent sent sentrecall MRR doc recall MRR recall in top 1 in top 10 in top 50IR with Lucene on AQUAINT datasetbaseline (words disjunction 0.530 0.631 0.756 0.314 0.627 0.223 1.202 3.464from target and question)baseline 0.514 0.617 0.741 0.332 0.653 0.236 1.269 3.759+ auto phraseswords 0.501 0.604 0.736 0.316 0.653 0.220 1.228 3.705+ auto NEs & phrasesbaseline 0.506 0.621 0.738 0.291 0.609 0.199 1.231 3.378+ manual phraseswords 0.510 0.625 0.738 0.294 0.609 0.202 1.244 3.368+ manual NEs & phrasesIR with Yahoo API on WEBbaseline - - - 0.183 0.570 0.101 0.821 2.316words disjunctioncascaded - - - 0.220 0.604 0.140 0.956 2.725using auto phrasescascaded - - - 0.241 0.614 0.155 1.065 3.016using manual phrasesTable 4: Document retrieval evaluation.tion.
We performed the experiments using first au-tomatically generated phrases, and then manuallycorrected phrases.For document retrieval we report: 1) average re-call, 2) average mean reciprocal ranking (MRR),and 3) overall document recall.
Each question hasa document retrieval recall score which is the pro-portion of documents identified from all correctdocuments for this question.
The average recallis the individual recall averaged over all questions.MRR is the inverse index of the first correct doc-ument.
For example, if the first correct documentappears second, the MRR score will be 1/2.
MRRis computed for each question and averaged overall questions.
Overall document recall is the per-centage of questions for which at least one correctdocument was retrieved.
This measure indicatesthe upper bound on the QA system.For sentence retrieval we report 1) average sen-tence MRR, 2) overall sentence recall, 3) averageprecision of the first sentence, 4) number of cor-rect candidate sentences in the top 10 results, and5) number of correct candidate sentences in the top50 results 6.Table 4 shows our experimental results.
First,we evaluate the performance of document retrievalon the indexed AQUAINT dataset.
Average doc-ument recall for our baseline system is 0.53, in-dicating that on average half of the correct doc-uments are retrieved.
Average document MRRis .631, meaning that on average the first correctdocument appears first or second.
Overall docu-ment recall indicates that 75.6% of queries con-tain a correct document among the retrieved docu-ments.
Average sentence recall is lower than docu-ment recall indicating that some proportion of cor-rect answers is not retrieved using our heuristicsentence extraction algorithm.
The average sen-tence MRR is .314 indicating that the first correctsentence is approximately third on the list.
With6Although the number of documents is 20, multiple sen-tences may be extracted from each document.14the AQUAINT dataset, we notice no improvementwith exact phrases.Next, we evaluate sentence retrieval from theWEB.
There is no gold standard for the WEBdataset so we do not report document retrievalscores.
Sentence scores on the WEB dataset arelower than on the AQUAINT dataset 7.Using back-off retrieval with automatically cre-ated phrases and named entities, we see an im-provement over the baseline system performancefor each of the sentence measures on the WEBdataset.
Average sentence MRR increases 20%from .183 in the baseline to .220 in the experimen-tal system.
With manually created phrases MRRimproves a further 9.5% to .241.
This indicatesthat information retrieval on the WEB dataset canbenefit from a better quality of chunker and from aproperly converted question phrase.
It also showsthat the improvement is not due to simply match-ing random substrings from a question, but thatlinguistic information is useful in constructing theexact match phrases.
Precision of automaticallydetected phrases is affected by errors during auto-matic part-of-speech tagging of questions.
An ex-ample of an error due to POS tagging is the iden-tification of a phrase was Rowling born due to afailure to identify that born is a verb.Our results emphasize the difference betweenthe two datasets.
AQUAINT dataset is a collec-tion of a large set of news documents, while WEBis a much larger resource of information from avariety of sources.
It is reasonable to assumethat on average there are much fewer documentswith query words in AQUAINT corpus than on theWEB.
Proportion of correct documents from all re-trieved WEB documents on average is likely to belower than this proportion in documents retrievedfrom AQUAINT.
When using words on a queryto AQUAINT dataset, most of the correct docu-ments are returned in the top matches.
Our resultsindicate that over 50% of correct documents areretrieved in the top 20 results.
Results in table 3indicate that exactly matched phrases from a ques-tion are more precise predictors of presence of ananswer.
Using exact matched phrases in a WEBquery allows a search engine to give higher rank tomore relevant documents and increases likelihoodof these documents in the top 20 matches.Although overall performance on the WEBdataset is lower than on AQUAINT, there is a po-7Our decision to use only 20 documents may be a factor.tential for improvement by using a larger set ofdocuments and improving our sentence extractionheuristics.7 Conclusion and Future WorkIn this paper we present a document retrieval ex-periment on a question answering system.
Weevaluate the use of named entities and of noun,verb, and prepositional phrases as exact matchphrases in a document retrieval query.
Our re-sults indicate that using phrases extracted fromquestions improves IR performance on WEB data.Surprisingly, we find no positive effect of usingphrases on a smaller closed set of data.Our data analysis shows that linguistic phrasesare more accurate indicators for candidate sen-tences than words.
In future work we plan to evalu-ate how phrase type (noun vs. verb vs. preposition)affects IR performance.AcknowledgmentWe would like to thank professor Amanda Stentfor suggestions about experiments and proofread-ing the paper.
We would like to thank the reviewersfor useful comments.ReferencesApache.
2004-2008.
Lucene.http://lucene.apache.org/java/docs/index.html.Bilotti, M., B. Katz, and J. Lin.
2004.
What worksbetter for question answering: Stemming or morpho-logical query expansion?
In Proc.
SIGIR.Bird, S., E. Loper, and E. Klein.
2008.Natural Language ToolKit (NLTK).http://nltk.org/index.php/Main Page.Carpenter, B. and B. Baldwin.
2008.
Lingpipe.http://alias-i.com/lingpipe/index.html.Chu-Carroll, J., J. Prager, K. Czuba, D. Ferrucci, andP.
Duboue.
2006.
Semantic search via XML frag-ments: a high-precision approach to IR.
In Proc.SIGIR.Clarke, C., G. Cormack, D. Kisman, and T. Lynam.2000.
Question answering by passage selection(multitext experiments for TREC-9).
In Proc.
TREC.Collins-Thompson, K., J. Callan, E. Terra, and C. L.A.Clarke.
2004.
The effect of document retrieval qual-ity on factoid question answering performance.
InProc.
SIGIR.Dang, H., J. Lin, and D. Kelly.
2006.
Overview ofthe TREC 2006 question answering track.
In Proc.TREC.15Graff, D. 2002.
The AQUAINT corpus of Englishnews text.
Technical report, Linguistic Data Con-sortium, Philadelphia, PA, USA.Harabagiu, S., A. Hickl, J. Williams, J. Bensley,K.
Roberts, Y. Shi, and B. Rink.
2006.
Questionanswering with LCC?s CHAUCER at TREC 2006.In Proc.
TREC.Hovy, E., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.Lin.
2001a.
Question answering in Webclopedia.
InProc.
TREC.Hovy, E., U. Hermjakob, and C.-Y.
Lin.
2001b.
Theuse of external knowledge in factoid QA.
In Proc.TREC.Ittycheriah, A., M. Franz, and S. Roukos.
2001.
IBM?sstatistical question answering system ?
TREC-10.
InProc.
TREC.Judge, J., A. Cahill, and J. van Genabith.
2006.QuestionBank: Creating a corpus of parse-annotatedquestions.
In Proc.
ACL.Katz, B. and J. Lin.
2003.
Selectively using relations toimprove precision in question answering.
In Proc.
ofthe EACL Workshop on Natural Language Process-ing for Question Answering.Lee, G. G. et al 2001.
SiteQ: Engineering high per-formance QA system using lexico-semantic patternmatching and shallow NLP.
In Proc.
TREC.Light, M., G. S. Mann, E. Riloff, and E. Breck.
2001.Analyses for elucidating current question answeringtechnology.
Journal of Natural Language Engineer-ing, 7(4).Llopis, F. and J. L. Vicedo.
2001.
IR-n: A passage re-trieval system at CLEF-2001.
In Proc.
of the SecondWorkshop of the Cross-Language Evaluation Forum(CLEF 2001).Lloyd, L., D. Kechagias, and S. Skiena.
2005.
Ly-dia: A system for large-scale news analysis.
In Proc.SPIRE, pages 161?166.Miller, George A.
1995.
WordNet: a lexical databasefor english.
Communications of the ACM, 38(11).Murdock, V. and W. B. Croft.
2005.
Simple transla-tion models for sentence retrieval in factoid questionanswering.
In Proc.
SIGIR.Prager, J., E. Brown, and A. Coden.
2000.
Question-answering by predictive annotation.
In ACM SIGIR.QA -to site.Punyakanok, V., D. Roth, and W. Yih.
2008.
The im-portance of syntactic parsing and inference in seman-tic role labeling.
Computational Linguistics, 34(2).Srihari, R. and W. Li.
1999.
Information extractionsupported question answering.
In Proc.
TREC.Stenchikova, S., D. Hakkani-Tur, and G. Tur.
2006.QASR: Question answering using semantic roles forspeech interface.
In Proc.
ICSLP-Interspeech 2006.Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.2003.
Quantitative evaluation of passage retrieval al-gorithms for question answering.
In Proc.
SIGIR.Vorhees, V. and D. Harman.
1999.
Overview of theeighth Text REtrieval Conference (TREC-8).
In?Proc.
TREC?.White, K. and R. Sutcliffe.
2004.
Seeking an upperbound to sentence level retrieval in question answer-ing.
In Proc.
SIGIR.Yahoo!, Inc. 2008.
Yahoo!
search API.http://developer.yahoo.com/search/.16
