Coling 2010: Poster Volume, pages 36?44,Beijing, August 2010Robust Sentiment Detection on Twitter from Biased and Noisy DataLuciano BarbosaAT&T Labs - Researchlbarbosa@research.att.comJunlan FengAT&T Labs - Researchjunlan@research.att.comAbstractIn this paper, we propose an approach toautomatically detect sentiments on Twit-ter messages (tweets) that explores somecharacteristics of how tweets are writtenand meta-information of the words thatcompose these messages.
Moreover, weleverage sources of noisy labels as ourtraining data.
These noisy labels wereprovided by a few sentiment detectionwebsites over twitter data.
In our experi-ments, we show that since our features areable to capture a more abstract represen-tation of tweets, our solution is more ef-fective than previous ones and also morerobust regarding biased and noisy data,which is the kind of data provided by thesesources.1 IntroductionTwitter is one of the most popular social networkwebsites and has been growing at a very fast pace.The number of Twitter users reached an estimated75 million by the end of 2009, up from approx-imately 5 million in the previous year.
Throughthe twitter platform, users share either informationor opinions about personalities, politicians, prod-ucts, companies, events (Prentice and Huffman,2008) etc.
This has been attracting the attentionof different communities interested in analyzingits content.Sentiment detection of tweets is one of the basicanalysis utility functions needed by various appli-cations over twitter data.
Many systems and ap-proaches have been implemented to automaticallydetect sentiment on texts (e.g., news articles, Webreviews and Web blogs) (Pang et al, 2002; Pangand Lee, 2004; Wiebe and Riloff, 2005; Glanceet al, 2005; Wilson et al, 2005).
Most of theseapproaches use the raw word representation (n-grams) as features to build a model for sentimentdetection and perform this task over large piecesof texts.
However, the main limitation of usingthese techniques for the Twitter context is mes-sages posted on Twitter, so-called tweets, are veryshort.
The maximum size of a tweet is 140 char-acters.In this paper, we propose a 2-step sentimentanalysis classification method for Twitter, whichfirst classifies messages as subjective and ob-jective, and further distinguishes the subjectivetweets as positive or negative.
To reduce the la-beling effort in creating these classifiers, insteadof using manually annotated data to compose thetraining data, as regular supervised learning ap-proaches, we leverage sources of noisy labels asour training data.
These noisy labels were pro-vided by a few sentiment detection websites overtwitter data.
To better utilize these sources, weverify the potential value of using and combiningthem, providing an analysis of the provided labels,examine different strategies of combining thesesources in order to obtain the best outcome; and,propose a more robust feature set that captures amore abstract representation of tweets, composedby meta-information associated to words and spe-cific characteristics of how tweets are written.
Byusing it, we aim to handle better: the problemof lack of information on tweets, helping on thegeneralization process of the classification algo-rithms; and the noisy and biased labels providedby those websites.The remainder of this paper is organized as fol-lows.
In Section 2, we provide some context aboutmessages on Twitter and about the websites usedas label sources.
We introduce the features usedin the sentiment detection and also provide a deepanalysis of the labels generated by those sourcesin Section 3.
We examine different strategies of36combining these sources and present an extensiveexperimental evaluation in Section 4.
Finally, wediscuss previous works related to ours in Section 5and conclude in Section 6, where we outline direc-tions and future work.2 PreliminariesIn this section, we give some context about Twittermessages and the sources used for our data-drivenapproach.Tweets.
The Twitter messages are called tweets.There are some particular features that can be usedto compose a tweet (Figure 1 illustrates an ex-ample): ?RT?
is an acronym for retweet, whichmeans the tweet was forwarded from a previouspost; ?@twUser?
represents that this message is areply to the user ?twUser?
; ?#obama?
is a tag pro-vided by the user for this message, so-called hash-tag; and ?http://bit.ly/9K4n9p?
is a link to someexternal source.
Tweets are limited to 140 charac-ters.
Due to this lack of information in terms ofwords present in a tweet, we explore some of thetweet features listed above to boost the sentimentdetection, as we will show in detail in Section 3.Data Sources.
We collected data from 3 differ-ent websites that provide almost real-time senti-ment detection for tweets: Twendz, Twitter Sen-timent and TweetFeel.
To collect data, we issueda query containing a common stopword ?of?, aswe are interested in collecting generic data, andretrieved tweets from these sites for three weeks,archiving the returned tweets along with their sen-timent labels.
Table 1 shows more details aboutthese sources.
Two of the websites provide 3-class detection: positive, negative and neutral andone of them just 2-class detection.
One thing tonote is our crawling process obtained a very dif-ferent number of tweets from each website.
Thismight be a result of differences among their sam-pling processes of Twitter stream or some kind offiltering process to output.
For instance, a sitemay only present the tweets it has more confi-dence about their sentiment.
In Section 3, wepresent a deep analysis of the data provided bythese sources, showing if they are useful to builda sentiment classification.RT @twUser: Obama is the first U.S. president not tohave seen a new state added in his lifetime.http://bit.ly/9K4n9p #obamaFigure 1: Example of a tweet.3 Twitter Sentiment DetectionOur goal is to categorize a tweet into one of thethree sentiment categories: positive, neutral ornegative.
Similar to (Pang and Lee, 2004; Wil-son et al, 2005), we implement a 2-step sentimentdetection framework.
The first step targets on dis-tinguishing subjective tweets from non-subjectivetweets (subjectivity detection).
The second onefurther classifies the subjective tweets into posi-tive and negative, namely, the polarity detection.Both classifiers perform prediction using an ab-stract representation of the sentences as features,as we show later in this section.3.1 FeaturesA variety of features have been exploited on theproblem of sentiment detection (Pang and Lee,2004; Pang et al, 2002; Wiebe et al, 1999; Wiebeand Riloff, 2005; Riloff et al, 2006) including un-igrams, bigrams, part-of-speech tags etc.
A natu-ral choice would be to use the raw word represen-tation (n-grams) as features, since they obtainedgood results in previous works (Pang and Lee,2004; Pang et al, 2002) that deal with large texts.However, as we want to perform sentiment detec-tion on very short messages (tweets), this strat-egy might not be effective, as shown in our ex-periments.
In this context, we are motivated todevelop an abstract representation of tweets.
Wepropose the use of two sets of features: meta-information about the words on tweets and char-acteristics of how tweets are written.Meta-features.
Given a word in a tweet, we mapit to its part-of-speech using a part-of-speech dic-tionary1.
Previous approaches (Wiebe and Riloff,2005; Riloff et al, 2003) have shown that the ef-fectiveness of using POS tags for this task.
Theintuition is certain POS tags are good indica-tors for sentiment tagging.
For example, opin-ion messages are more likely containing adjec-1The pos dictionary we used in this paper is available at:http://wordlist.sourceforge.net/pos-readme.37Data sources URL # Tweets SentimentsTwendz http://twendz.waggeneredstrom.com/ 254081 pos/neg/neutralTwitter Sentiment http://twittersentiment.appspot.com/ 79696 pos/neg/neutralTweetFeel http://www.tweetfeel.com/ 13122 pos/negTable 1: Information about the 3 data sources.tives or interjections.
In addition to POS tags,we map the word to its prior subjectivity (weakand strong subjectivity), also used by (Wiebe andRiloff, 2005), and polarity (positive, negative andneutral).
The prior polarity is switched from pos-itive to negative or vice-versa when a negativeexpression (as, e.g., ?don?t?, ?never?)
precedesthe word.
We obtained the prior subjectivity andpolarity information from subjectivity lexicon ofabout 8,000 words used in (Riloff and Wiebe,2003)2.
Although this is a very comprehensivelist, slang and specific Web vocabulary are notpresent on it, e.g., words as ?yummy?
or ?ftw?.For this reason, we collected popular words usedon online discussions from many online sourcesand added them to this list.Tweet Syntax Features.
We exploited the syn-tax of the tweets to compose our features.
Theyare: retweet; hashtag; reply; link, if the tweet con-tains a link; punctuation (exclamation and ques-tions marks); emoticons (textual expression rep-resenting facial expressions); and upper cases (thenumber of words that starts with upper case in thetweet).The frequency of each feature in a tweet is di-vided by the number of the words in the tweet.3.2 Subjectivity ClassifierAs we mentioned before, the first step in our tweetsentiment detection is to predict the subjectivity ofa given tweet.
We decided to create a single clas-sifier by combining the objectivity sentences fromTwendz and Twitter Sentiment (objectivity class)and the subjectivity sentences from all 3 sources.As we do not know the quality of the labels pro-vided by these sources, we perform a cleaningprocess over this data to assure some reasonablequality.
These are the steps:1.
Disagreement removal: we remove the2The subjectivity lexicon is available athttp://www.cs.pitt.edu/mpqa/tweets that are disagreed between the datasources in terms of subjectivity;2.
Same user?s messages: we observed that theusers with the highest number of messagesin our dataset are usually those ones that postsome objective messages, for example, ad-vertising some product or posting some jobrecruiting information.
For this reason, weallowed in the training data only one messagefrom the same user.
As we show later, thisboosts the classification performance, mainlybecause it removes tweets labeled as subjec-tive by the data sources but are in fact objec-tive;3.
Top opinion words: to clean the objectivetraining set, we remove from this set tweetsthat contain the top-n opinion words in thesubjectivity training set, e.g., words as cool,suck, awesome etc.As we show in Section 4, this process is in factable to remove certain noisy in the training data,leading to a better performing subjectivity classi-fier.To illustrate which of the proposed features aremore effective for this task, the top-5 features interms of information gain, based on our trainingdata, are: positive polarity, link, strong subjec-tive, upper case and verbs.
Three of them aremeta-information (positive polarity, strong sub-jective and verbs) and the other two are tweetsyntax features (link and upper case).
Here isa typical example of a objective tweet in whichthe user pointed an external link and used manyupper case words: ?Starbucks Expands Pay-By-IPhone Pilot to 1,000 Stores?Starbucks cus-tomers with Apple iPhones or iPod touches can.. http://oohja.com/x9UbC?.383.3 Polarity ClassifierThe second step of our sentiment detection ap-proach is polarity classification, i.e., predict-ing positive or negative sentiment on subjectivetweets.
In this section, first we analyze the qual-ity of the polarity labels provided by the threesources, and whether their combination has thepotential to bring improvement.
Second, wepresent some modifications in the proposed fea-tures that are more suitable for this task.3.3.1 Analysis of the Data SourcesThe 3 data sources used in this work providesome kind of polarity labels (see Table 1).
Twoquestions we investigate regarding these sourcesare: (1) how useful are these polarity labels?
and(2) does combining them bring improvement inaccuracy?We take the following aspects into considera-tion:?
Labeler quality: if the labelers have low qual-ity, combine them might not bring much im-provement (Sheng et al, 2008).
In our case,each source is treated as a labeler;?
Number of labels provided by the labelers:if the labels are informative, i.e., the prob-ability of them being correct is higher than0.5, the more the number of labels, the higheris the performance of a classifier built fromthem (Sheng et al, 2008);?
Labeler bias: the labeled data provided bythe labelers might be only a subset of thereal data distribution.
For instance, labelersmight be interested in only providing labelsthat they are more confident about;?
Different labeler bias: if labelers make simi-lar mistakes, the combination of them mightnot bring much improvement.We provide an empirical analysis of thesedatasets to address these points.
First, we measurethe polarity detection quality of a source by calcu-lating the probability p of a label from this sourcebeing correct.
We use the data manually labeledfor assessing the classifiers?
performance (testingdata, see Section 4) to obtain the correct labels ofData sources Quality EntropyTwendz 0.77 8.3TwitterSentiment 0.82 7.9TweetFeel 0.89 7.5Table 2: Quality of the labels and entropy of thetweets provided by each data source for the polar-ity detection.a data sample.
Table 2 shows their values.
We canconclude from these numbers that the 3 sourcesprovide a reasonable quality data.
This means thatcombining them might bring some improvementto the polarity detection instead of, for instance,using one of them in isolation.
An aspect that isoverlooked by quality is the bias of the data.
Forinstance, by examining the data from TwitterFeel,we found out that only 4 positive words (?awe-some?,?rock?,?love?
and ?beat?)
cover 95% oftheir positive examples and only 6 negative words(?hate?,?suck?,?wtf?,?piss?,?stupid?
and ?fail?
)cover 96% of their negative set.
Clearly, the dataprovided by this source is biased towards thesewords.
This is probably the reason why this web-site outputs such fewer number of tweets com-pared to the other websites (see Table 1) as wellas why its data has the smallest entropy amongthe sources (see Table 2).The quality of the data and its individual biashave certainly impact in the combination of labels.However, there is other important aspect that oneneeds to consider: different bias between the la-belers.
For instance, if labelers a and b make sim-ilar decisions, we expect that combining their la-bels would not bring much improvement.
There-fore, the diversity of labelers is a key element incombining them (Polikar, 2006).
One way to mea-sure this is by calculating the agreement betweenthe labels produced by the labelers.
We use thekappa coefficient (Cohen, 1960) to measure thedegree of agreement between two sources.
Ta-ble 3 presents the coefficients for each par of datasource.
All the coefficients are between 0.4 and0.6, which represents a moderate agreement be-tween the labelers (Landis and Koch, 1977).
Thismeans that in fact the sources provide differentbias regarding polarity detection.39Data sources KappaTwendz/TwitterSentiment 0.58TwitterSentiment/TweetFeel 0.58Twendz/TweetFeel 0.44Table 3: Kappa coefficient between pairs ofsources.From this analysis we can conclude that com-bining the labels provided by the 3 sources canimprove the performance of the polarity detec-tion instead of using one of them in isolation be-cause they provide diverse labels (moderate kappaagreement) of reasonable quality, although thereis some issues related to bias of the labels pro-vided by them.
In our experimental evaluation inSection 4, we present results obtained by differentstrategies of combining these sources that confirmthese findings.3.3.2 Polarity FeaturesThe features used in the polarity detection arethe same ones used in the subjectivity detection.However, as one would expect the set of the mostdiscriminative features is different between thetwo tasks.
For subjectivity detection, the top-5features in terms of information gain, based onthe training data, are: negative polarity, positivepolarity, verbs, good emoticons and upper case.For this task, the meta-information of the words(negative polarity, positive polarity and verbs) ismore important than specific features from Twitter(good emoticons and upper case), whereas for thesubjectivity detection, tweet syntax features havea higher relevance.This analysis show that prior polarity is veryimportant for this task.
However, one limitationof using it from a generic list is its values mightnot hold for some specific scenario.
For instance,the polarity of the word ?spot?
is positive accord-ing to this list.
However, looking at our trainingdata almost half of the occurrences of this wordappears in the positive set and the other half inthe negative set.
Thus, it is not correct to as-sume that prior polarity of ?spot?
is 1 for thisparticular data.
This example illustrates our strat-egy to weight the prior polarities: for each wordw with prior polarity defined by the list, we cal-culate the prior polarity of w, pol(w), based onthe distribution of w in the positive and negativesets.
Thus, polpos(w) = count(w, pos)/count(w)and polneg(w) = 1?
polpos(w).
We assume thepolarity of a word is associated with the polar-ity of the sentence, which seems to be reasonablesince we are dealing with very short messages.Although simple, this strategy is able to improvethe polarity detection, as we show in Section 4.4 ExperimentsWe have performed an extensive performanceevaluation of our solution for twitter sentimentdetection.
Besides analyzing its overall perfor-mance, our goals included: examining differentstrategies to combine the labels provided by thesources; comparing our approach to previous onesin this area; and evaluating how robust our solu-tion is to the noisy and biased data described inSection 3.4.1 Experimental SetupData Sets.
For the subjectivity detection, afterthe cleansing processing (see Section 3), the train-ing data contains about 200,000 tweets (roughly100,000 tweets were labeled by the sources assubjective ones and 100,000 objective ones), andfor polarity detection, 71046 positive and 79628negative tweets.
For test data, we manually la-beled 1,000 tweets as positive, negative and neu-tral.
We also built a development set (1,000tweets) to tune the parameters of the classificationalgorithms.Approaches.
For both tasks, subjectivity and po-larity detection, we compared our approach withprevious ones reported in the literature.
Detailedexplanation about them are as follows:?
ReviewSA: this is the approach proposedby Pang and Lee (Pang and Lee, 2004)for sentiment analysis in regular online re-views.
It performs the subjectivity detec-tion on a sentence-level relying on the prox-imity between sentences to detect subjectiv-ity.
The set of sentences predicted as subjec-tive is then classified as negative or positivein terms of polarity using the unigrams that40compose the sentences.
We used the imple-mentation provided by LingPipe (LingPipe,2008);?
Unigrams: Pang et al (Pang et al, 2002)showed unigrams are effective for sentimentdetection in regular reviews.
Based on that,we built unigram-based classifiers for thesubjectivity and polarity detections over thetraining data.
Another approach that uses un-igrams is the one used by TwitterSentimentwebsite.
For polarity detection, they selectthe positive examples for the training datafrom the tweets containing good emoticonsand negative examples from tweets contain-ing bad emoticons.
(Go et al, 2009).
Webuilt a polarity classifier using this approach(Unigrams-TS).?
TwitterSA: TwitterSA exploits the featuresdescribed in Section 3 in this paper.
Forthe subjectivity detection, we trained a clas-sifier from the two available sources, us-ing the cleaning process described in Sec-tion 3 to remove noise in the training data,TwitterSA(cleaning), and other classifiertrained from the original data, TwitterSA(no-cleaning).
For the polarity detection task,we built a few classifiers to compare theirperformances: TwitterSA(single) and Twit-terSA(weights) are two classifiers we trainedusing combined data from the 3 sources.The only difference is TwitterSA(weights)uses the modification of weighting the priorpolarity of the words based on the train-ing data.
TwitterSA(voting) and Twit-terSA(maxconf) combine classification out-puts from 3 classifiers respectively trainedfrom each source.
TwitterSA(voting) usesmajority voting to combine them and Twit-terSA(maxconf) picks the one with maxi-mum confidence score.We use Weka (Witten and Frank, 2005) to cre-ate the classifiers.
We tried different learning al-gorithms available on Weka and SVM obtainedthe best results for Unigrams and TwitterSA.
Ex-perimental results reported in this section are ob-tained using SVM.4.2 Subjectivity Detection EvaluationTable 4 shows the error rates obtained by the dif-ferent subjectivity detection approaches.
Twit-terSA achieved lower error rate than both Uni-grams and ReviewSA.
As a result, these num-bers confirm that features inferred from meta-information of words and specific syntax featuresfrom tweets are better indicators of the subjectiv-ity than unigrams.
Another advantage of our ap-proach is since it uses only 20 features, the train-ing and test times are much faster than using thou-sands of features like Unigrams.
One of the rea-sons why TwitterSA obtained such a good perfor-mance was the process of data cleansing (see Sec-tion 3).
The label quality provided by the sourcesfor this task was very poor: 0.66 for Twendz and0.68 for TwitterSentiment.
By cleaning the data,the error decreased from 19.9, TwitterSA(no-cleaning), to 18.1, TwitterSA(cleaning).
Regard-ing ReviewSA, its lower performance is expectedsince tweets are composed by single sentencesand ReviewSA relies on the proximity betweensentences to perform subjectivity detection.We also investigated the influence of the size oftraining data on classification performance.
Fig-ure 2 plots the error rates obtained by TwitterSAand Unigrams versus the number of training ex-amples.
The curve corresponding to TwitterSAshowed that it achieved good performances evenwith a small training data set, and kept almost con-stant as more examples were added to the train-ing data, whereas for Unigrams the error rate de-creased.
For instance, with only 2,000 tweets astraining data, TwitterSA obtained 20% of errorrate whereas Unigrams 34.5%.
These numbersshow that our generic representation of tweetsproduces models that are able to generalize evenwith a few examples.4.3 Polarity Detection EvaluationWe provide the results for polarity detectionin Table 5.
The best performance was ob-tained by TwitterSA(maxconf), which combinesresults of the 3 classifiers, respectively trainedfrom each source, by taking the output by themost confident classifier, as the final predic-tion.
TwitterSA(maxconf) was followed by Twit-terSA(weights) and TwitterSA(single), both cre-41ated from a single training data.
This result showsthat computing the prior polarity of the wordsbased on the training data TwitterSA(weights)brings some improvement for this task.
Twit-terSA(voting) obtained the highest error rateamong the TwitterSA approaches.
This impliesthat, in our scenario, the best way of combiningthe merits of the individual classifiers is by usinga confidence score approach.Unigrams also achieved comparable perfor-mances.
However, when reducing the size of thetraining data, the performance gap between Twit-terSA and Unigrams is much wider.
Figure 3shows the error rate of both approaches3 in func-tion of the training size.
Similar to subjectivity de-tection, the training size does not have much influ-ence in the error rate for TwitterSA.
However forUnigrams, it decreased significantly as the train-ing size increased.
For instance, for a trainingsize with 2,000 tweets, the error rate for Unigramswas 46% versus 23.8% for our approach.
As forsubjectivity detection, this occurs because our fea-tures are in fact able to capture a more general rep-resentation of the tweets.Another advantage of TwitterSA over Uni-grams is that it produces more robust models.
Toillustrate this, we present the error rates of Uni-grams and TwitterSA where the training data iscomposed by data from each source in isolation.For the TweetFeel website, where data is very bi-ased (see Section 3), Unigrams obtained an errorrate of 44.5% whereas over a sample of the samesize of the combined training data (Figure 3), itobtained an error rate of around 30%.
Our ap-proach also performed worse over this data thanthe general one, but still had a reasonable er-ror rate, 25.1%.
Regarding the Twendz website,which is the noisiest one (Section 3), Unigramsalso obtained a poor performance comparing itagainst its performance over a sample of the gen-eral data with a same size (see Table 5 and Fig-ure 3).
Our approach, on the other hand, wasnot much influenced by the noise (22.9% on noisydata and around 20% on the sample of same sizeof the general data).
Finally, since the data qual-ity provided by TwitterSentiment is better than the3For this experiment, we used the TwitterSA(single) con-figuration.Approach Error rateTwitterSA(cleaning) 18.1TwitterSA(no-cleaning) 19.9Unigrams 27.6ReviewSA 32Table 4: Results for subjectivity detection.Approach Error rateTwitterSA(maxconf) 18.7TwitterSA(weights) 19.4TwitterSA(single) 20TwitterSA(voting) 22.6Unigrams 20.9ReviewSA 21.7Unigrams-TS 24.3Table 5: Results for polarity detection.Site Training Size TwitterSA UnigramsTweetFeel 13120 25.1 44.5Twendz 78025 22.9 32.3TwitterSentiment 59578 22 23.4Table 6: Training data size for each source anderror rates obtained by classifiers built from them.0 5 10 15 20 25 3035 400  20000  40000  60000  80000  100000  120000  140000  160000  180000  200000Error Rate Training Size UnigramsTwitterSAFigure 2: Influence of the training data size in theerror rate of subjectivity detection using Unigramsand TwitterSA.previous sources (Table 2), there was not muchimpact over both classifiers created from it.From this analysis over real data, we can con-clude that our approach produces (1) an effectivepolarity classifier even when only a small numberof training data is available; (2) a robust model tobias and noise in the training data; and (3) com-bining data sources with such distinct characteris-tics, as our data analysis in Section 3 pointed out,is effective.420 10 20 3040 500  20000  40000  60000  80000  100000  120000  140000  160000Error Rate Training Size UnigramsTwitterSAFigure 3: Influence of the training data size inthe error rate of polarity detection using Unigramsand TwitterSA.5 Related WorkThere is a rich literature in the area of sentimentdetection (see e.g., (Pang et al, 2002; Pang andLee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005).
Most of these ap-proaches try to perform this task on large texts, ase.g., newspaper articles and movie reviews.
An-other common characteristic of some of them isthe use of n-grams as features to create their mod-els.
For instance, Pang and Lee (Pang and Lee,2004) explores the fact that sentences close in atext might share the same subjectivity to create abetter subjectivity detector and, similar to (Pang etal., 2002), uses unigrams as features for the polar-ity detection.
However, these approaches do notobtain a good performance on detecting sentimenton tweets, as we showed in Section 4, mainly be-cause tweets are very short messages.
In additionto that, since they use a raw word representation,they are more sensible to bias and noise, and needa much higher number of examples in the train-ing data than our approach to obtain a reasonableperformance.The Web sources used in this paper and someother websites provide sentiment detection fortweets.
A great limitation to evaluate them is theydo not make available how their classification wasbuilt.
One exception is TwitterSentiment (Go etal., 2009), for instance, which considers tweetswith good emoticons as positive examples andtweets with bad emoticons as negative examplesfor the training data, and builds a classifier usingunigrams and bigrams as features.
We showedin Section 4 that our approach works better thantheirs for this problem, obtaining lower error rates.6 Conclusions and Future WorkWe have presented an effective and robust sen-timent detection approach for Twitter messages,which uses biased and noisy labels as input tobuild its models.
This performance is due to thefact that: (1) our approach creates a more abstractrepresentation of these messages, instead of usinga raw word representation of them as some pre-vious approaches; and (2) although noisy and bi-ased, the data sources provide labels of reasonablequality and, since they have different bias, com-bining them also brought some benefits.The main limitation of our approach is the casesof sentences that contain antagonistic sentiments.As future work, we want to perform a more finegrained analysis of sentences in order to identifyits main focus and then based the sentiment clas-sification on it.ReferencesCohen, J.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and psychological measure-ment, 20(1):37.Glance, N., M. Hurst, K. Nigam, M. Siegler, R. Stock-ton, and T. Tomokiyo.
2005.
Deriving marketingintelligence from online discussion.
In Proceed-ings of the eleventh ACM SIGKDD, pages 419?428.ACM.Go, A., R. Bhayani, and L. Huang.
2009.
Twit-ter sentiment classification using distant supervi-sion.
Technical report, Stanford Digital LibraryTechnologies Project.Landis, J.R. and G.G.
Koch.
1977.
The measurementof observer agreement for categorical data.
Biomet-rics, pages 159?174.LingPipe.
2008.
LingPipe 3.9.1.http://alias-i.com/lingpipe.Pang, B. and L. Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe ACL, volume 2004.43Pang, B., L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learn-ing techniques.
In Proceedings of the ACL, pages79?86.
Association for Computational Linguistics.Polikar, R. 2006.
Ensemble based systems in deci-sion making.
IEEE Circuits and systems magazine,6(3):21?45.Prentice, S. and E. Huffman.
2008.
Social MediasNew Role In Emergency Management.
Idaho Na-tional Laboratory, pages 1?5.Riloff, E. and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proceedings ofthe 2003 conference on Empirical methods in natu-ral language processing, pages 105?112.Riloff, E., J. Wiebe, and T. Wilson.
2003.
Learningsubjective nouns using extraction pattern bootstrap-ping.
In Proceedings of the 7th Conference on Nat-ural Language Learning, pages 25?32.Riloff, E., S. Patwardhan, and J. Wiebe.
2006.
Featuresubsumption for opinion analysis.
In Proceedingsof the 2006 Conference on Empirical Methods inNatural Language Processing, pages 440?448.
As-sociation for Computational Linguistics.Sheng, V.S., F. Provost, and P.G.
Ipeirotis.
2008.
Getanother label?
Improving data quality and data min-ing using multiple, noisy labelers.
In Proceeding ofthe 14th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 614?622.
ACM.Wiebe, J. and E. Riloff.
2005.
Creating subjectiveand objective sentence classifiers from unannotatedtexts.
Computational Linguistics and IntelligentText Processing, pages 486?497.Wiebe, J.M., RF Brace, and T.P.
O?Hara.
1999.
Devel-opment and use of a gold-standard data set for sub-jectivity classifications.
In Proceedings of the ACL,pages 246?253.
Association for Computational Lin-guistics.Wilson, T., J. Wiebe, and P. Hoffmann.
2005.
Rec-ognizing contextual polarity in phrase-level senti-ment analysis.
In EMNLP, page 354.
Associationfor Computational Linguistics.Witten, Ian H. and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.Morgan Kaufmann.44
