Proceedings of the ACL Student Research Workshop, pages 158?164,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsRobust Multilingual Statistical Morphological Generation ModelsOndr?ej Du?ek and Filip Jurc?
?c?ekCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostransk?
n?me?st?
25, CZ-11800 Praha, Czech Republic{odusek,jurcicek}@ufal.mff.cuni.czAbstractWe present a novel method of statisti-cal morphological generation, i.e.
the pre-diction of inflected word forms givenlemma, part-of-speech and morphologicalfeatures, aimed at robustness to unseen in-puts.
Our system uses a trainable classifierto predict ?edit scripts?
that are then usedto transform lemmas into inflected wordforms.
Suffixes of lemmas are included asfeatures to achieve robustness.
We evalu-ate our system on 6 languages with a vary-ing degree of morphological richness.
Theresults show that the system is able to learnmost morphological phenomena and gen-eralize to unseen inputs, producing sig-nificantly better results than a dictionary-based baseline.1 IntroductionSurface realization is an integral part of all natu-ral language generation (NLG) systems, albeit of-ten implemented in a very simple manner, suchas filling words into ready hand-written templa-tes.
More sophisticated methods use hand-writtengrammars (Gatt and Reiter, 2009), possibly incombination with a statistical reranker (Langkildeand Knight, 1998).
Existing NLG systems arevery often applied to languages with little mor-phology, such as English, where a small set ofhand-written rules or the direct use of word formsin the symbolic representation or templates is usu-ally sufficient, and so the main focus of these sys-tems lies on syntax and word order.However, this approach poses a problem in lan-guages with a complex morphology.
Avoidinginflection, i.e.
ensuring that a word lemma willkeep its base form at all times, often leads tovery unnatural results (see Figure 1).
Some gen-erators use a hand-made morphological dictionaryToto se l?b?
u?ivateli Jana Nov?kov?.----------- -- --?
?This is liked by user (name) femininenominativemasculinedativeword inserted to avoid inflecting the namename left uninflected(correct form: vocative)D?kujeme, Jan Nov?k , va?e hlasov?n?
Thank you, (name) your poll has been createdbylo vytvo?eno.nominativee uFigure 1: Unnatural language resulting from tem-plates with no inflection.The sentences are taken from the Czech translations of Face-book and Doodle, which use simple templates to generatepersonalized texts.
Corrections to make the text fluent areshown in red.for inflection (Pt?c?ek and ?abokrtsk?, 2006) or adictionary learned from automatically tagged data(Toutanova et al 2008).
That gives good results,but reaching sufficient coverage with a hand-madedictionary is a very demanding task and even usingextreme amounts of automatically annotated datawill not generalize beyond the word forms alreadyencountered in the corpus.
Hand-written rules canbecome overly complex and are not easily adapt-able for a different language.Therefore, the presented method relies on a sta-tistical approach that learns to predict morpholog-ical inflection from annotated data.
As a result,such approach is more robust, i.e.
capable of gen-eralizing to unseen inputs, and easily portable todifferent languages.An attempt to implement statistical morpholog-ical generation has already been made by Bohnetet al(2010).
However, their morphology genera-tion was only a component of a complex genera-tion system.
Therefore, no deep analysis of the ca-pabilities of the methods has been performed.
Inaddition, their method did not attempt to general-ize beyond seen inputs.
In this paper, we propose158several improvements and provide a detailed eval-uation of a statistical morphological inflection sys-tem, including more languages into the evaluationand focusing on robustness to unseen inputs.The paper is structured as follows: first, weexplain the problem of morphological generation(Section 2), then give an account of our system(Section 3).
Section 4 provides a detailed evalua-tion of the performance of our system in differentlanguages.
We then compare our system to relatedworks in Section 5.
Section 6 concludes the paper.2 The Problem of MorphologicalRealizationThe problem of morphological surface realizationis inverse to part-of-speech tagging and lemma-tization (or stemming): given a lemma/stem ofa word and its part-of-speech and morphologicalproperties, the system should output the correctlyinflected form of the word.
An example is givenin Figure 2.
This does not include generating aux-iliary words (such as be?
will be), which are as-sumed to be already generated.word NNS words+Wort NN W?rtern+be VBZ is+ser V gen=c,num=s,person=3,mood=indicative,tense=present es+Neut,Pl,DatFigure 2: The task of morphological generation(examples for English, German, and Spanish).While this problem can be solved by a set ofrules to a great extent for languages with little mor-phology such as English (Minnen et al 2001),it becomes much more complicated in languageswith a complex nominal case system or multiplesynthetic verbal inflection patterns, such as Ger-man or Czech.
Figure 3 shows an example of am-biguity in these languages.This research aims to create a system that iseasy to train from morphologically annotated data,yet able to infer and apply more general rules andgenerate forms unseen in the training corpus.3 Our Morphological Generation SetupSimilarly to Bohnet et al(2010), our system isbased on the prediction of edit scripts (diffs) be-tween the lemma and the target word form (seeSection 3.1), which are then used to derive the tar-get word form from the lemma.
This allows thewordN wordNemSs+Wt?enwWb?VBZWiVThis lkedgdwo gdwoemSs+W=?enwWc?VBZW,VTbyked um+gp3 gon?um+aSs+Wa?enwWv?VBZWiV Ss+Wa?enwWv?VBZWiVT yuredTrludPsllsr PsllsrsSs+?iD?anw VVT(niaed!Zn !ZnsVdZ?,l?=sZVVTmuik)hid "srr "srrenVdZ?,l?anw VVTrludan++ a?++erVdZ?,l?anw VVThikdFigure 3: Morphological ambiguity in Germanand Czech.The same inflection pattern is used to express multiple mor-phological properties (left) and multiple patterns may expressthe same property (right).system to operate even on previously unseen lem-mas.
The employed classifier and features are de-scribed in Sections 3.2 and 3.3.
Section 3.4 thengives an overview of the whole morphological in-flection process.3.1 Lemma-Form Edit ScriptsOur system uses lemma-form edit scripts basedon the Levenshtein string distance metric (Lev-enshtein, 1966): the dynamic programming algo-rithm used to compute the distance can be adaptedto produce diffs on characters, i.e.
a mapping fromthe source string (lemma) to the target string (wordform) that indicates which characters were added,replaced or removed.We use the distance from the end of the word toindicate the position of a particular change, sameas Bohnet et al(2010).
We have added severalenhancements to this general scenario:?
Our system treats separately changes at thebeginning of the word, since they are usuallyindependent of the word length and alwaysoccur at the beginning, such as the prefix ge-for past participles in German or ne- for nega-tion in Czech.?
Adjacent changes in the string are joined to-gether to produce a total lower number ofmore complex changes.?
If the Levenshtein edit script indicates a re-moval of letters from the beginning of theword, we treat the target word form as irreg-ular, i.e.
as if the whole word changed.?
In our setup, the edit scripts need not betreated as atomic, which allows to train sep-arate classification models for word changesthat are orthogonal (cf.
Section 3.4).159An example of the edit scripts generated by oursystem is shown in Figure 4.worr wdrrNS >0-er,3:1-?s+ s+Wrt >0-ing?N We *isenoSNr tNenoSb >2-t,<geVVNtoS VVNtB >2-?Zisib rNZg=N >4-?me,<necNr, cN, >2:1-=orbNrWS =orbWrsSor >0-an,2:1-d,4:1-iFigure 4: Example edit scripts generated by oursystem.The changes are separated by commas.
?>?
denotes a changeat the end of the word, ?N :?
denotes a change at the N -thcharacter from the end.
The number of deleted charactersand their replacement follows in both cases.
?<?
marks ad-ditions to the beginning of a word (regardless of its length).?*?
marks irregular forms where the whole word is replaced.Our diffs are case-insensitive since we believethat letter-casing and morphology are distinct phe-nomena and should be treated separately.
Case-insensitivity, along with merging adjacent changesand the possibility to split models, causes a de-crease in the number of different edit scripts, thussimplifying the task for the classifier.In our preliminary experiments on Czech, wealso explored the possibility of using different dis-tance metrics for the edit scripts, such as vari-ous settings of the Needleman-Wunsch algorithm(Needleman and Wunsch, 1970) or the longestcommon subsequence1 post-edited with regularexpressions to lower the total number of changes.However, this did not have any noticeable impacton the performance of the models.3.2 Used Statistical ModelsWe use the multi-class logistic regression classi-fier from the LibLINEAR package2 (Fan et al2008) for the prediction of edit scripts.
We useL1-regularization since it yields models that aresmaller in size and the resulting trained weightsindicate the important features in a straightforwardway.
This direct influence on features (similar tokeyword spotting) allows for a simple interpreta-tion of the learned models.
We examined varioussettings of the regularization cost and the termina-tion criterion (See Section 4.1).We have also experimented with support vec-tor machines from the LibSVM package (Chang1We used the Perl implementation of this algorithm fromhttps://metacpan.org/module/String::Diff.2We use it via the Python wrapper in the Scikit-Learn li-brary (http://scikit-learn.org).and Lin, 2011), but the logistic regression clas-sifier proved to be better suited to this task, pro-viding a higher edit script accuracy on the devel-opment set for German and Czech (when featureconcatenation is used, cf.
Section 3.3), while alsorequiring less CPU time and RAM to train.3.3 FeaturesWhile the complete set of features varies acrosslanguages given their specifics, most of the fea-tures are common to all languages:?
lemma of the word in question,?
coarse and fine-grained part-of-speech tag,?
morphological features (e.g.
case, gender,tense etc., tagset-dependent), and?
suffixes of the lemma of up to 4 characters.Since morphological changes usually occur nearthe end of the word, they mostly depend just onthat part of the word and not on e.g.
prefixes orprevious parts of a compound.
Therefore, usingsuffixes allows the classifier to generalize to un-known words.In addition, as we use a linear classifier, we havefound the concatenation of various morphologi-cal features, such as number, gender, and case innouns or tense and person in verbs, to be very ben-eficial.
We created new features by concatenatingall possible subsets of morphological features, aslong as all their values were non-empty (to preventfrom creating duplicate values).
To avoid com-binatorial explosion, we resorted to concatenatingonly case, number, and gender for Czech and ex-cluding the postype feature from concatenationfor Spanish and Catalan.We also employ the properties of adjacentwords in the sentence as features in our modelsfor the individual languages (see Section 4).
Theseare used mainly to model congruency (is vs. are inEnglish, different adjectival declension after defi-nite and indefinite article in German) or article vo-calization (l?
vs. el in Catalan).
The congruencyinformation could be obtained more reliably fromelsewhere in a complete NLG system (e.g.
featuresfrom the syntactic realizer), which would probablyresult in a performance gain, but lies beyond thescope of this paper.No feature pruning was needed in our setup asour classifier was able to handle the large amountof features (100,000s, language-dependent).1603.4 Overall Schema of the PredictorAfter an examination of the training data, we de-cided to use a separate model for the changes thatoccur at the beginning of the word since they tendto be much simpler than and not very dependent onthe changes towards the end of the word (e.g.
theusages of the Czech negation prefix ne- or the Ger-man infinitive prefix zu- are quite self-containedphenomena).The final word inflection prediction schemalooks as follows:1.
Using the statistical model described in Sec-tion 3.2, predict an edit script (cf.
Section 3.1)for changes at the end or in the middle of theword.32.
Predict an edit script for the possible additionof a prefix using a separate model.3.
Apply the edit scripts predicted by the pre-vious steps as rules to generate the final in-flected word form.4 Experimental EvaluationWe evaluate our morphological generation setupon all of the languages included in the CoNLL2009 Shared Task data sets except Chinese (which,as an isolating language, lacks morphology almostaltogether): English, German, Spanish, Catalan,Japanese, and Czech.
We use the CoNLL 2009data sets (Hajic?
et al 2009) with gold-standardmorphology annotation for all our experiments(see Table 1 for a detailed overview).We give a discussion of the overall performanceof our system in all the languages in Section 4.1.We focus on Czech in the detailed analysis of thegeneralization power of our system in Section 4.2since Czech has the most complicated morphologyof all these languages.
In addition, the morpho-logical annotation provided in the CoNLL 2009Czech data set is more detailed than in the otherlanguages, which eliminates the need for addi-tional syntactic features (cf.
Section 3.3).
We alsoprovide a detailed performance overview on En-glish for comparison.4.1 Overall PerformanceThe performance of our system in the best set-tings for the individual languages measured on the3Completely irregular forms (see Section 3.1) are alsopredicted by this step.CoNLL 2009 evaluation test sets is shown in Ta-ble 2.
We used the classifier and features describedin Sections 3.2 and 3.3 (additional features for theindividual languages are listed in the table).
Weused two models as described in Section 3.4 forall languages but English, where no changes at thebeginning of the word were found in the trainingdata set and a single model was sufficient.
We per-formed a grid search for the best parameters of thefirst model4 and used the same parameters for bothmodels.5One can see from the results in Table 2 thatthe system is able to predict the majority of wordforms correctly and performs well even on dataunseen in the training set.When manually inspecting the errors producedby the system, we observed that in some cases thesystem in fact assigned a form synonymous to theone actually occurring in the test set, such as notinstead of n?t in English or tak?
instead of taky(both meaning also) in Czech.
However, most er-rors are caused by the selection of a more frequentrule, even if incorrect given the actual morpholog-ical features.
We believe that this could possiblybe mitigated by using features combining lemmasuffixes and morphological categories, or featuresfrom the syntactic context.The lower score for German is caused partly bythe lack of syntactic features for the highly am-biguous adjective inflection and partly by a some-what problematic lemmatization of punctuation(all punctuation has the lemma ?_?
and the part-of-speech tag only distinguishes terminal, comma-like and other characters).4.2 Generalization PowerTo measure the ability of our system to generalizeto previously unseen inputs, we compare it againsta baseline that uses a dictionary collected from thesame data and leaves unseen forms intact.
The per-formance of our system on unseen forms is shownin Table 2 for all languages.
A comparison withthe dictionary baseline for varying training datasizes in English and Czech is given in Table 3.It is visible from Table 3 that our approach4We always used L1-norm and primal form and modi-fied the termination criterion tol and regularization strengthC.
The best values found on the development data sets for theindividual languages are listed in Table 2.5As the changes at the beginning of words are much sim-pler, changing parameters did not have a significant influenceon the performance of the second model.161Language Data set sizes In Eval (%)Train Dev Eval -Punct InflF UnkFEnglish 958,167 33,368 57,676 85.93 15.14 1.80German 648,677 32,033 31,622 87.24 45.12 8.69Spanish 427,442 50,368 50,630 85.42 29.96 6.16Catalan 390,302 53,015 53,355 86.75 31.89 6.28Japanese 112,555 6,589 13,615 87.34 10.73 6.43Czech 652,544 87,988 92,663 85.50 42.98 7.68Table 1: The CoNLL 2009 data sets: Sizes and propertiesThe data set sizes give the number of words (tokens) in the individual sets.
The right column shows the percentage of data inthe evaluation set: -Punct = excluding punctuation tokens, InflF = only forms that differ from the lemma (i.e.
have a non-emptyedit script), UnkF = forms unseen in the training set.Language Additional features Best parameters Rule (%) Form accuracy (%)accuracy Total -Punc InflF UnkFEnglish W-1/LT C=10, tol=1e-3 99.56 99.56 99.49 97.76 98.26German W-1/LT, MC C=10, tol=1e-3 96.66 / 99.91 96.46 98.01 92.64 89.63Spanish MC C=100, tol=1e-3 99.05 / 99.98 99.01 98.86 97.10 91.11Catalan W+1/C1, MC C=10, tol=1e-3 98.91 / 99.86 98.72 98.53 96.49 94.24Japanese MC C=100, tol=1e-3 99.94 / 100.0 99.94 99.93 99.59 99.54Czech MC C=100, tol=1e-3 99.45 / 99.99 99.45 99.35 98.81 95.93Table 2: The overall performance of our system in different languages.The additional features include: MC = concatenation of morphological features (see Section 3.3), W-1/LT = lemma and part-of-speech tag of the previous word, W+1/C1 = first character of the following word.Rule (edit script) accuracy is given for the prediction of changes at the end or in the middle and at the beginning of the word,respectively.The form accuracy field shows the percentage of correctly predicted (lowercased) target word forms: Total = on the wholeevaluation set; -Punct, InflF, UnkF = on subsets as defined in Table 1.maintains a significantly6 higher accuracy whencompared to the baseline for all training datasizes.
It is capable of reaching high performanceeven with relatively small amounts of training in-stances.
The overall performance difference be-comes smaller as the training data grow; how-ever, performance on unseen inputs and relativeerror reduction show a different trend: the im-provement stays stable.
The relative error reduc-tion decreases slightly for English where unknownword forms are more likely to be base forms ofunknown lemmas, but keeps increasing for Czechwhere unknown word forms are more likely to re-quire inflection (the accuracy reached by the base-line method on unknown forms equals the percent-age of base forms among the unknown forms).Though the number of unseen word forms is de-clining with increasing amounts of training data,which plays in favor of the dictionary method, un-seen inputs will still occur and may become veryfrequent for out-of-domain data.
Our system istherefore beneficial ?
at least as a back-off for un-seen forms ?
even if a large-coverage morpholog-6Significance at the 99% level has been assessed usingpaired bootstrap resampling (Koehn, 2004).ical dictionary is available.We observed upon manual inspection that thesuffix features were among the most prominentfor the prediction of many edit scripts, which indi-cates their usefulness; e.g.
LemmaSuffix1=e isa strong feature (along with POS_Tag=VBD) forthe edit script >0d in English.5 Related WorkStatistical morphological realizers are very raresince most NLG systems are either fully basedon hand-written grammars, including morpholog-ical rules (Bateman et al 2005; Gatt and Reiter,2009; Lavoie and Rambow, 1997), or employ sta-tistical methods only as a post-processing step toselect the best one of several variants generatedby a rule-based system (Langkilde and Knight,1998; Langkilde-Geary, 2002) or to guide the de-cision among the rules during the generation pro-cess (Belz, 2008).
While there are fully statisticalsurface realizers (Angeli et al 2010; Mairesse etal., 2010), they operate in a phrase-based fashionon word forms with no treatment of morphology.Morphological generation in machine translationtends to use dictionaries ?
hand-written (?abokrt-162Train Czech Englishdata Unseen Dict.
acc.
Our sys.
acc.
Error Unseen Dict acc.
Our sys.
acc.
Errorpart forms Total UnkF Total UnkF reduct.
forms Total UnkF Total UnkF reduct.0.1 63.94 62.00 41.54 76.92 64.43 39.27 27.77 89.18 78.73 95.02 93.14 53.910.5 51.38 66.78 38.65 88.73 78.83 66.08 19.96 91.34 76.33 97.89 95.56 75.641 45.36 69.43 36.97 92.23 83.60 74.60 14.69 92.76 73.95 98.28 95.27 76.195 31.11 77.29 35.56 96.63 90.36 85.17 6.82 96.21 75.73 99.05 97.13 74.9610 24.72 80.97 33.88 97.83 92.45 88.61 4.66 97.31 77.13 99.34 97.76 75.4420 17.35 85.69 32.47 98.72 94.28 91.02 3.10 98.09 78.52 99.46 97.57 71.6530 14.17 87.92 31.85 98.95 94.56 91.34 2.46 98.40 79.79 99.48 97.63 67.7550 11.06 90.34 31.62 99.20 95.25 91.69 1.76 98.69 80.53 99.54 98.04 64.8175 9.01 91.91 31.54 99.34 95.60 91.89 1.35 98.86 82.23 99.55 98.17 60.61100 7.68 92.88 30.38 99.45 95.93 92.21 1.12 98.94 82.53 99.56 98.26 58.85Table 3: Comparison of our system with a dictionary baseline on different training data sizes.All numbers are percentages.
The accuracy of both methods is given for the whole evaluation set (Total) and for word formsunseen in the training set (UnkF).
Error reduct.
shows the relative error reduction of our method in comparison to the baselineon the whole evaluation set.sk?
et al 2008), learnt from data (Toutanova etal., 2008), or a combination thereof (Popel and?abokrtsk?, 2009).The only statistical morphological generatorknown to us is that of Bohnet et al(2010), em-ployed as a part of a support-vector-machines-based surface realizer from semantic structures.They apply their system to a subset of CoNLL2009 data sets and their results (morphological ac-curacy of 97.8% for English, 97.49% for Germanand 98.48% for Spanish) seem to indicate that oursystem performs better for English, slightly bet-ter for Spanish and slightly worse for German, butthe numbers may not be directly comparable to ourresults as it is unclear whether the authors use theoriginal data set or the output of the previous stepsof their system for evaluation and whether they in-clude punctuation and/or capitalization.Since the morphological generator of Bohnet etal.
(2010) is only a part of a larger system, theydo not provide a thorough analysis of the results.While their system also predicts edit scripts de-rived from Levenshtein distance, their edit scriptrepresentation seems less efficient than ours.
Theyreport using about 1500 and 2500 different scriptsfor English and German, respectively, disregard-ing scripts occurring only once in the training data.However, our representation only yields 154 En-glish and 1785 German7 edit scripts with no prun-ing.
Along with the independent models for thebeginning of the word, this simplifies the taskfor the classifier.
In addition to features used by7We get this number when counting the edit scripts asatomic; they divide into 1735 changes at the end or in themiddle of the words and 18 changes at the beginning.Bohnet et al(2010), our system includes the suf-fix features to generalize to unseen inputs.6 Conclusions and Further WorkWe have presented a fully trainable morphologi-cal generation system aimed at robustness to pre-viously unseen inputs, based on logistic regressionand Levenshtein distance edit scripts between thelemma and the target word form.
The results fromthe evaluation on six different languages from theCoNLL 2009 data sets indicate that the system isable to learn most morphological rules correctlyand is able to cope with previously unseen input,performing significantly better than a dictionarylearned from the same amount of data.
The sys-tem is freely available for download at:http://ufal.mff.cuni.cz/~odusek/flectIn future, we plan to integrate our generatorinto a semantic NLG scenario, as well as a sim-pler template-based system, and evaluate it onfurther languages.
We also consider employ-ing transformation-based learning (Brill, 1995) forprediction to make better use of the possibility ofsplitting the edit scripts and applying the morpho-logical changes one-by-one.AcknowledgmentsThis research was partly funded by the Ministry ofEducation, Youth and Sports of the Czech Repub-lic under the grant agreement LK11221 and coreresearch funding of Charles University in Prague.The authors would like to thank Mate?j Korvas andMartin Popel for helpful comments on the draftand David Marek, Ondr?ej Pl?tek and Luk??
?ilkafor discussions.163ReferencesG.
Angeli, P. Liang, and D. Klein.
2010.
A simpledomain-independent probabilistic approach to gen-eration.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, page 502?512.J.
A. Bateman, I.
Kruijff-Korbayov?, and G.-J.
Krui-jff.
2005.
Multilingual resource sharing acrossboth related and unrelated languages: An imple-mented, open-source framework for practical natu-ral language generation.
Research on Language andComputation, 3(2-3):191?219.A.
Belz.
2008.
Automatic generation of weatherforecast texts using comprehensive probabilisticgeneration-space models.
Natural Language Engi-neering, 14(4):431?455.B.
Bohnet, L. Wanner, S. Mille, and A. Burga.
2010.Broad coverage multilingual deep sentence genera-tion with a stochastic multi-level realizer.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, page 98?106.E.
Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational lin-guistics, 21(4):543?565.C.
C. Chang and C. J. Lin.
2011.
LIBSVM: a libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology (TIST), 2(3):27.R.
E Fan, K. W Chang, C. J Hsieh, X. R Wang, andC.
J Lin.
2008.
LIBLINEAR: a library for large lin-ear classification.
The Journal of Machine LearningResearch, 9:1871?1874.A.
Gatt and E. Reiter.
2009.
SimpleNLG: a realisationengine for practical applications.
In Proceedings ofthe 12th European Workshop on Natural LanguageGeneration, page 90?93.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,M.
A Mart?, L. M?rquez, A. Meyers, J. Nivre,S.
Pad?, J.
?te?p?nek, et al2009.
The CoNLL-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, page 1?18.P.
Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, volume 4, page 388?395.I.
Langkilde and K. Knight.
1998.
Generation thatexploits corpus-based statistical knowledge.
In Pro-ceedings of the 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics-Volume 1, page 704?710.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.
In Proceedings of the 12th Inter-national Natural Language Generation Workshop,page 17?24.B.
Lavoie and O. Rambow.
1997.
A fast and portablerealizer for text generation systems.
In Proceedingsof the fifth conference on Applied natural languageprocessing, page 265?268.V.
I. Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions and reversals.
SovietPhysics Doklady, 10(8):707.F.
Mairesse, M.
Ga?ic?, F.
Jurc?
?c?ek, S. Keizer, B. Thom-son, K. Yu, and S. Young.
2010.
Phrase-based sta-tistical language generation using graphical modelsand active learning.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, page 1552?1561.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.S.
B. Needleman and C. D. Wunsch.
1970.
A generalmethod applicable to the search for similarities inthe amino acid sequence of two proteins.
Journal ofmolecular biology, 48(3):443?453.M.
Popel and Z.
?abokrtsk?.
2009.
Improv-ing English-Czech tectogrammatical MT.
ThePrague Bulletin of Mathematical Linguistics, 92(-1):115?134.J.
Pt?c?ek and Z.
?abokrtsk?.
2006.
Synthesis ofCzech sentences from tectogrammatical trees.
InText, Speech and Dialogue.K.
Toutanova, H. Suzuki, and A. Ruopp.
2008.
Ap-plying morphology generation models to machinetranslation.
In Proc.
of ACL, volume 8.Z.
?abokrtsk?, J. Pt?c?ek, and P. Pajas.
2008.
Tec-toMT: highly modular MT system with tectogram-matics used as transfer layer.
In Proceedings of theThird Workshop on Statistical Machine Translation,page 167?170.
Association for Computational Lin-guistics.164
