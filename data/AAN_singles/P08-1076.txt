Proceedings of ACL-08: HLT, pages 665?673,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSemi-Supervised Sequential Labeling and Segmentationusing Giga-word Scale Unlabeled DataJun Suzuki and Hideki IsozakiNTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan{jun, isozaki}@cslab.kecl.ntt.co.jpAbstractThis paper provides evidence that the use ofmore unlabeled data in semi-supervised learn-ing can improve the performance of Natu-ral Language Processing (NLP) tasks, suchas part-of-speech tagging, syntactic chunking,and named entity recognition.
We first pro-pose a simple yet powerful semi-superviseddiscriminative model appropriate for handlinglarge scale unlabeled data.
Then, we describeexperiments performed on widely used testcollections, namely, PTB III data, CoNLL?00and ?03 shared task data for the above threeNLP tasks, respectively.
We incorporate upto 1G-words (one billion tokens) of unlabeleddata, which is the largest amount of unlabeleddata ever used for these tasks, to investigatethe performance improvement.
In addition,our results are superior to the best reported re-sults for all of the above test collections.1 IntroductionToday, we can easily find a large amount of un-labeled data for many supervised learning applica-tions in Natural Language Processing (NLP).
There-fore, to improve performance, the development ofan effective framework for semi-supervised learning(SSL) that uses both labeled and unlabeled data is at-tractive for both the machine learning and NLP com-munities.
We expect that such SSL will replace mostsupervised learning in real world applications.In this paper, we focus on traditional and impor-tant NLP tasks, namely part-of-speech (POS) tag-ging, syntactic chunking, and named entity recog-nition (NER).
These are also typical supervisedlearning applications in NLP, and are referred toas sequential labeling and segmentation problems.In some cases, these tasks have relatively largeamounts of labeled training data.
In this situation,supervised learning can provide competitive results,and it is difficult to improve them any further byusing SSL.
In fact, few papers have succeeded inshowing significantly better results than state-of-the-art supervised learning.
Ando and Zhang (2005) re-ported a substantial performance improvement com-pared with state-of-the-art supervised learning re-sults for syntactic chunking with the CoNLL?00shared task data (Tjong Kim Sang and Buchholz,2000) and NER with the CoNLL?03 shared taskdata (Tjong Kim Sang and Meulder, 2003).One remaining question is the behavior of SSLwhen using as much labeled and unlabeled dataas possible.
This paper investigates this question,namely, the use of a large amount of unlabeled datain the presence of (fixed) large labeled data.To achieve this, it is paramount to make the SSLmethod scalable with regard to the size of unlabeleddata.
We first propose a scalable model for SSL.Then, we apply our model to widely used test collec-tions, namely Penn Treebank (PTB) III data (Mar-cus et al, 1994) for POS tagging, CoNLL?00 sharedtask data for syntactic chunking, and CoNLL?03shared task data for NER.
We used up to 1G-words(one billion tokens) of unlabeled data to explore theperformance improvement with respect to the unla-beled data size.
In addition, we investigate the per-formance improvement for ?unseen data?
from theviewpoint of unlabeled data coverage.
Finally, wecompare our results with those provided by the bestcurrent systems.The contributions of this paper are threefold.First, we present a simple, scalable, but power-ful task-independent model for semi-supervised se-quential labeling and segmentation.
Second, we re-port the best current results for the widely used test665collections described above.
Third, we confirm thatthe use of more unlabeled data in SSL can really leadto further improvements.2 Conditional Model for SSLWe design our model for SSL as a natural semi-supervised extension of conventional supervisedconditional random fields (CRFs) (Lafferty et al,2001).
As our approach for incorporating unla-beled data, we basically follow the idea proposed in(Suzuki et al, 2007).2.1 Conventional Supervised CRFsLet x?X and y?Y be an input and output, whereX and Y represent the set of possible inputs and out-puts, respectively.
C stands for the set of cliques inan undirected graphical model G(x,y), which indi-cates the interdependency of a given x and y. ycdenotes the output from the corresponding clique c.Each clique c?C has a potential function ?c.
Then,the CRFs define the conditional probability p(y|x)as a product of ?cs.
In addition, let f =(f1, .
.
., fI)be a feature vector, and ?
= (?1, .
.
., ?I) be a pa-rameter vector, whose lengths are I .
p(y|x;?)
on aCRF is defined as follows:p(y|x;?)
= 1Z(x)?c?c(yc,x;?
), (1)where Z(x) =?y?Y?c?C ?c(yc,x;?)
is the par-tition function.
We generally assume that the po-tential function is a non-negative real value func-tion.
Therefore, the exponentiated weighted sumover the features of a clique is widely used, so that,?c(yc,x;?)=exp(?
?
f c(yc,x)) where f c(yc,x)is a feature vector obtained from the correspondingclique c in G(x,y).2.2 Semi-supervised Extension for CRFsSuppose we have J kinds of probability mod-els (PMs).
The j-th joint PM is represented bypj(xj ,y;?j) where ?j is a model parameter.
xj =Tj(x) is simply an input x transformed by a pre-defined function Tj .
We assume xj has the samegraph structure as x.
This means pj(xj ,y) canbe factorized by the cliques c in G(x,y).
That is,pj(xj ,y;?j)=?c pj(xjc,yc;?j).
Thus, we can in-corporate generative models such as Bayesian net-works including (1D and 2D) hidden Markov mod-els (HMMs) as these joint PMs.
Actually, there isa difference in that generative models are directedgraphical models while our conditional PM is anundirected.
However, this difference causes no vi-olations when we construct our approach.Let us introduce ?
?=(?1, .
.
., ?I, ?I+1, .
.
., ?I+J),and h = (f1, .
.
., fI, log p1, .
.
., log pJ), which isthe concatenation of feature vector f and the log-likelihood of J-joint PMs.
Then, we can define anew potential function by embedding the joint PMs;??c(yc,x;??,?
)= exp(?
?
f c(yc,x)) ?
?jpj(xjc,yc;?j)?I+j= exp(??
?
hc(yc,x)).where ?= {?j}Jj=1, and hc(yc,x) is h obtainedfrom the corresponding clique c in G(x,y).
Sinceeach pj(xjc,yc) has range [0, 1], which is non-negative, ?
?c can also be used as a potential func-tion.
Thus, the conditional model for our SSL canbe written as:P (y|x;??,?)
= 1Z ?(x)?c??c(yc,x;??,?
), (2)where Z ?
(x)=?y?Y?c?C ??c(yc,x;??,?).
Here-after in this paper, we refer to this conditional modelas a ?Joint probability model Embedding style Semi-Supervised Conditional Model?, or JESS-CM forshort.Given labeled data, Dl={(xn,yn)}Nn=1, the MAPestimation of ??
under a fixed ?
can be written as:L1(??|?)
=?nlogP (yn|xn;??,?)
+ log p(??
),where p(??)
is a prior probability distribution of ?
?.Clearly, JESS-CM shown in Equation 2 has exactlythe same form as Equation 1.
With a fixed ?, thelog-likelihood, log pj , can be seen simply as the fea-ture functions of JESS-CM as with fi.
Therefore,embedded joint PMs do not violate the global con-vergence conditions.
As a result, as with super-vised CRFs, it is guaranteed that ??
has a value thatachieves the global maximum of L1(??|?).
More-over, we can obtain the same form of gradient as thatof supervised CRFs (Sha and Pereira, 2003), that is,?L1(??|?)
=EP?
(Y,X ;??,?
)[h(Y,X )]?
?nEP (Y|xn;??,?)[h(Y,xn)]+?
log p(??
).Thus, we can easily optimize L1 by using theforward-backward algorithm since this paper solely666focuses on a sequence model and a gradient-basedoptimization algorithm in the same manner as thoseused in supervised CRF parameter estimation.We cannot naturally incorporate unlabeled datainto standard discriminative learning methods sincethe correct outputs y for unlabeled data are un-known.
On the other hand with a generative ap-proach, a well-known way to achieve this incorpora-tion is to use maximum marginal likelihood (MML)parameter estimation, i.e., (Nigam et al, 2000).Given unlabeled data Du = {xm}Mm=1, MML esti-mation in our setting maximizes the marginal distri-bution of a joint PM over a missing (hidden) variabley, namely, it maximizes?m log?y?Y p(xm,y; ?
).Following this idea, there have been introduceda parameter estimation approach for non-generativeapproaches that can effectively incorporate unla-beled data (Suzuki et al, 2007).
Here, we refer to itas ?Maximum Discriminant Functions sum?
(MDF)parameter estimation.
MDF estimation substitutesp(x,y) with discriminant functions g(x,y).
There-fore, to estimate the parameter ?
of JESS-CM byusing MDF estimation, the following objective func-tion is maximized with a fixed ??:L2(?|??)
=?mlog?y?Yg(xm,y;??,?)
+ log p(?
),where p(?)
is a prior probability distribution of?.
Since the normalization factor does not af-fect the determination of y, the discriminant func-tion of JESS-CM shown in Equation 2 is definedas g(x,y;??,?)
=?c?C ??c(yc,x;??,?).
Witha fixed ?
?, the local maximum of L2(?|??)
aroundthe initialized value of?
can be estimated by an iter-ative computation such as the EM algorithm (Demp-ster et al, 1977).2.3 Scalability: Efficient Training AlgorithmA parameter estimation algorithm of ??
and ?
canbe obtained by maximizing the objective functionsL1(??|?)
and L2(?|??)
iteratively and alternately.Figure 1 summarizes an algorithm for estimating ?
?and ?
for JESS-CM.This paper considers a situation where there aremany more unlabeled data M than labeled data N ,that is, N << M .
This means that the calculationcost for unlabeled data is dominant.
Thus, in orderto make the overall parameter estimation procedureInput: training data D = {Dl,Du}where labeled data Dl = {(xn,yn)}Nn=1,and unlabeled data Du = {xm}Mm=1Initialize: ?
(0) ?
uniform distribution, t ?
0do1.
t ?
t + 12.
(Re)estimate ??
:maximize L1(??|?)
with fixed ???
(t?1) using Dl.3.
Estimate ?
(t): (Initial values = ?
(t?1))update one step toward maximizing L2(?|??
)with fixed ??
using Du.do until |?(t)??(t?1)||?
(t?1)| < ?.Reestimate ??
: perform the same procedure as 1.Output: a JESS-CM, P (y|x,??,?
(t)).Figure 1: Parameter estimation algorithm for JESS-CM.scalable for handling large scale unlabeled data, weonly perform one step of MDF estimation for each tas explained on 3. in Figure 1.
In addition, the cal-culation cost for estimating parameters of embeddedjoint PMs (HMMs) is independent of the number ofHMMs, J , that we used (Suzuki et al, 2007).
As aresult, the cost for calculating the JESS-CM param-eters, ??
and ?, is essentially the same as execut-ing T iterations of the MML estimation for a singleHMM using the EM algorithm plus T +1 time opti-mizations of the MAP estimation for a conventionalsupervised CRF if it converged when t = T .
Inaddition, our parameter estimation algorithm can beeasily performed in parallel computation.2.4 Comparison with Hybrid ModelSSL based on a hybrid generative/discriminative ap-proach proposed in (Suzuki et al, 2007) has beendefined as a log-linear model that discriminativelycombines several discriminative models, pDi , andgenerative models, pGj , such that:R(y|x;?,?,?
)=?i pDi (y|x;?i)?i?j pGj (xj ,y;?j)?j?y?i pDi (y|x;?i)?i?j pGj (xj ,y;?j)?j,where ?={?i}Ii=1, and ?={{?i}Ii=1, {?j}I+Jj=I+1}.With the hybrid model, if we use the same labeledtraining data to estimate both ?
and ?, ?js will be-come negligible (zero or nearly zero) since pDi is al-ready fitted to the labeled training data while pGj aretrained by using unlabeled data.
As a solution, agiven amount of labeled training data is divided intotwo distinct sets, i.e., 4/5 for estimating ?, and the667remaining 1/5 for estimating ?
(Suzuki et al, 2007).Moreover, it is necessary to split features into sev-eral sets, and then train several corresponding dis-criminative models separately and preliminarily.
Incontrast, JESS-CM is free from this kind of addi-tional process, and the entire parameter estimationprocedure can be performed in a single pass.
Sur-prisingly, although JESS-CM is a simpler version ofthe hybrid model in terms of model structure andparameter estimation procedure, JESS-CM providesF -scores of 94.45 and 88.03 for CoNLL?00 and ?03data, respectively, which are 0.15 and 0.83 pointshigher than those reported in (Suzuki et al, 2007)for the same configurations.
This performance im-provement is basically derived from the full bene-fit of using labeled training data for estimating theparameter of the conditional model while the com-bination weights, ?, of the hybrid model are esti-mated solely by using 1/5 of the labeled trainingdata.
These facts indicate that JESS-CM has sev-eral advantageous characteristics compared with thehybrid model.3 ExperimentsIn our experiments, we report POS tagging, syntac-tic chunking and NER performance incorporating upto 1G-words of unlabeled data.3.1 Data SetTo compare the performance with that of previ-ous studies, we selected widely used test collec-tions.
For our POS tagging experiments, we usedthe Wall Street Journal in PTB III (Marcus et al,1994) with the same data split as used in (Shen etal., 2007).
For our syntactic chunking and NER ex-periments, we used exactly the same training, devel-opment and test data as those provided for the sharedtasks of CoNLL?00 (Tjong Kim Sang and Buchholz,2000) and CoNLL?03 (Tjong Kim Sang and Meul-der, 2003), respectively.
The training, developmentand test data are detailed in Table 11 .The unlabeled data for our experiments wastaken from the Reuters corpus, TIPSTER corpus(LDC93T3C) and the English Gigaword corpus,third edition (LDC2007T07).
As regards the TIP-1The second-order encoding used in our NER experimentsis the same as that described in (Sha and Pereira, 2003) exceptremoving IOB-tag of previous position label.
(a) POS-tagging: (WSJ in PTB III)# of labels 45Data set (WSJ sec.
IDs) # of sent.
# of wordsTraining 0?18 38,219 912,344Development 19?21 5,527 131,768Test 22?24 5,462 129,654(b) Chunking: (WSJ in PTB III: CoNLL?00 shared task data)# of labels 23 (w/ IOB-tagging)Data set (WSJ sec.
IDs) # of sent.
# of wordsTraining 15?18 8,936 211,727Development N/A N/A N/ATest 20 2,012 47,377(c) NER: (Reuters Corpus: CoNLL?03 shared task data)# of labels 29 (w/ IOB-tagging+2nd-order encoding)Data set (time period) # of sent.
# of wordsTraining 22?30/08/96 14,987 203,621Development 30?31/08/96 3,466 51,362Test 06?07/12/96 3,684 46,435Table 1: Details of training, development, and test data(labeled data set) used in our experimentsdata abbr.
(time period) # of sent.
# of wordsTipster wsj 04/90?03/92 1,624,744 36,725,301Reuters reu 09/96?08/97* 13,747,227 215,510,564Corpus *(excluding 06?07/12/96)English afp 05/94?12/96 5,510,730 135,041,450Gigaword apw 11/94?12/96 7,207,790 154,024,679ltw 04/94?12/96 3,094,290 72,928,537nyt 07/94?12/96 15,977,991 357,952,297xin 01/95?12/96 1,740,832 40,078,312total all 48,903,604 1,012,261,140Table 2: Unlabeled data used in our experimentsSTER corpus, we extracted all the Wall Street Jour-nal articles published between 1990 and 1992.
Withthe English Gigaword corpus, we extracted articlesfrom five news sources published between 1994 and1996.
The unlabeled data used in this paper is de-tailed in Table 2.
Note that the total size of the unla-beled data reaches 1G-words (one billion tokens).3.2 Design of JESS-CMWe used the same graph structure as the linear chainCRF for JESS-CM.
As regards the design of the fea-ture functions fi, Table 3 shows the feature tem-plates used in our experiments.
In the table, s indi-cates a focused token position.
Xs?1:s represents thebi-gram of feature X obtained from s?
1 and s po-sitions.
{Xu}Bu=A indicates that u ranges from A toB.
For example, {Xu}s+2u=s?2 is equal to five featuretemplates, {Xs?2, Xs?1, Xs, Xs+1, Xs+2}.
?wordtype?
or wtp represents features of a word such ascapitalization, the existence of digits, and punctua-tion as shown in (Sutton et al, 2006) without regularexpressions.
Although it is common to use external668(a) POS tagging:(total 47 templates)[ys], [ys?1:s], {[ys, pf-Ns], [ys, sf-Ns]}9N=1,{[ys,wdu], [ys,wtpu], [ys?1:s,wtpu]}s+2u=s?2,{[ys,wdu?1:u], [ys,wtpu?1:u], [ys?1:s,wtpu?1:u]}s+2u=s?1(b) Syntactic chunking: (total 39 templates)[ys], [ys?1:s], {[ys,wdu], [ys, posu], [ys,wdu, posu],[ys?1:s,wdu], [ys?1:s, posu]}s+2u=s?2, {[ys,wdu?1:u],[ys, posu?1:u], {[ys?1:s, posu?1:u]}s+2u=s?1,(c) NER: (total 79 templates)[ys], [ys?1:s], {[ys,wdu], [ys, lwdu], [ys, posu], [ys,wtpu],[ys?1:s, lwdu], [ys?1:s, posu], [ys?1:s,wtpu]}s+2u=s?2,{[ys, lwdu?1:u], [ys, posu?1:u], [ys,wtpu?1:u],[ys?1:s, posu?1:u], [ys?1:s,wtpu?1:u]}s+2u=s?1,[ys, poss?1:s:s+1], [ys,wtps?1:s:s+1], [ys?1:s, poss?1:s:s+1],[ys?1:s,wtps?1:s:s+1], [ys,wd4ls], [ys,wd4rs],{[ys, pf-Ns], [ys, sf-Ns], [ys?1:s, pf-Ns], [ys?1:s, sf-Ns]}4N=1wd: word, pos: part-of-speech lwd : lowercase of word,wtp: ?word type?, wd4{l,r}: words within the left or right 4 tokens{pf,sf}-N: N character prefix or suffix of wordTable 3: Feature templates used in our experiments    	                                							                           	(a) Influence of ?
(b) Changes in performancein Dirichlet prior and convergence propertyFigure 2: Typical behavior of tunable parametersresources such as gazetteers for NER, we used none.All our features can be automatically extracted fromthe given training data.3.3 Design of Joint PMs (HMMs)We used first order HMMs for embedded joint PMssince we assume that they have the same graph struc-ture as JESS-CM as described in Section 2.2.To reduce the required human effort, we simplyused the feature templates shown in Table 3 to gener-ate the features of the HMMs.
With our design, onefeature template corresponded to one HMM.
Thisdesign preserves the feature whereby each HMMemits a single symbol from a single state (or transi-tion).
We can easily ignore overlapping features thatappear in a single HMM.
As a result, 47, 39 and 79distinct HMMs are embedded in the potential func-tions of JESS-CM for POS tagging, chunking andNER experiments, respectively.3.4 Tunable ParametersIn our experiments, we selected Gaussian andDirichlet priors as the prior distributions in L1 andL2, respectively.
This means that JESS-CM has twotunable parameters, ?2 and ?, in the Gaussian andDirichlet priors, respectively.
The values of thesetunable parameters are chosen by employing a bi-nary line search.
We used the value for the best per-formance with the development set2.
However, itmay be computationally unrealistic to retrain the en-tire procedure several times using 1G-words of unla-beled data.
Therefore, these tunable parameter val-ues are selected using a relatively small amount ofunlabeled data (17M-words), and we used the se-lected values in all our experiments.
The left graphin Figure 2 shows typical ?
behavior.
The left endis equivalent to optimizing L2 without a prior, andthe right end is almost equivalent to consideringpj(xj ,y) for all j to be a uniform distribution.
Thisis why it appears to be bounded by the performanceobtained from supervised CRF.
We omitted the in-fluence of ?2 because of space constraints, but its be-havior is nearly the same as that of supervised CRF.Unfortunately, L2(?|??)
may have two or morelocal maxima.
Our parameter estimation proceduredoes not guarantee to provide either the global opti-mum or a convergence solution in ?
and ??
space.An example of non-convergence is the oscillation ofthe estimated ?.
That is, ?
traverses two or morelocal maxima.
Therefore, we examined its con-vergence property experimentally.
The right graphin Figure 2 shows a typical convergence property.Fortunately, in all our experiments, JESS-CM con-verged in a small number of iterations.
No oscilla-tion is observed here.4 Results and Discussion4.1 Impact of Unlabeled Data SizeTable 4 shows the performance of JESS-CM us-ing 1G-words of unlabeled data and the perfor-mance gain compared with supervised CRF, whichis trained under the same conditions as JESS-CM ex-cept that joint PMs are not incorporated.
We empha-size that our model achieved these large improve-ments solely using unlabeled data as additional re-sources, without introducing a sophisticated model,deep feature engineering, handling external hand-2Since CoNLL?00 shared task data has no development set,we divided the labeled training data into two distinct sets, 4/5for training and the remainder for the development set, and de-termined the tunable parameters in preliminary experiments.669(a) POS tagging (b) Chunking (c) NERmeasures label accuracy entire sent.
acc.
F?=1 sent.
acc.
F?=1 entire sent.
acc.eval.
data dev.
test dev.
test test test dev.
test dev.
testJESS-CM (CRF/HMM) 97.35 97.40 56.34 57.01 95.15 65.06 94.48 89.92 91.17 85.12(gain from supervised CRF) (+0.17) (+0.19) (+1.90) (+1.63) (+1.27) (+4.92) (+2.74) (+3.57) (+3.46) (+3.96)Table 4: Results for POS tagging (PTB III data), syntactic chunking (CoNLL?00 data), and NER (CoNLL?03 data)incorporated with 1G-words of unlabeled data, and the performance gain from supervised CRF    	  	          	                                                                        	        	   	                                                       	 	            	                                     	   	                    	   	        (a) POS tagging (b) Syntactic chunking (c) NERFigure 3: Performance changes with respect to unlabeled data size in JESS-CMcrafted resources, or task dependent human knowl-edge (except for the feature design).
Our method cangreatly reduce the human effort needed to obtain ahigh performance tagger or chunker.Figure 3 shows the learning curves of JESS-CMwith respect to the size of the unlabeled data, wherethe x-axis is on the logarithmic scale of the unla-beled data size (Mega-word).
The scale at the topof the graph shows the ratio of the unlabeled datasize to the labeled data size.
We observe that a smallamount of unlabeled data hardly improved the per-formance since the supervised CRF results are com-petitive.
It seems that we require at least dozensof times more unlabeled data than labeled trainingdata to provide a significant performance improve-ment.
The most important and interesting behav-ior is that the performance improvements against theunlabeled data size are almost linear on a logarith-mic scale within the size of the unlabeled data usedin our experiments.
Moreover, there is a possibil-ity that the performance is still unsaturated at the1G-word unlabeled data point.
This suggests thatincreasing the unlabeled data in JESS-CM may fur-ther improve the performance.Suppose J=1, the discriminant function of JESS-CM is g(x,y) = A(x,y)p1(x1,y;?1)?I+1 whereA(x,y) = exp(?
?
?c f c(yc,x)).
Note that bothA(x,y) and ?I+j are given and fixed during theMDF estimation of joint PM parameters ?.
There-fore, the MDF estimation in JESS-CM can be re-garded as a variant of the MML estimation (see Sec-tion 2.2), namely, it is MML estimation with a bias,A(x,y), and smooth factors, ?I+j .
MML estima-tion can be seen as modeling p(x) since it is equiv-alent to maximizing?m log p(xm) with marginal-ized hidden variables y, where?y?Y p(x,y) =p(x).
Generally, more data will lead to a more ac-curate model of p(x).
With our method, as withmodeling p(x) in MML estimation, more unlabeleddata is preferable since it may provide more accuratemodeling.
This also means that it provides better?clusters?
over the output space since Y is used ashidden states in HMMs.
These are intuitive expla-nations as to why more unlabeled data in JESS-CMproduces better performance.4.2 Expected Performance for Unseen DataWe try to investigate the impact of unlabeled dataon the performance of unseen data.
We divide thetest set (or the development set) into two disjointsets: L.app and L.neg app.
L.app is a set of sen-tences constructed by words that all appeared in theLabeled training data.
L.?app is a set of sentencesthat have at least one word that does not appear inthe Labeled training data.Table 5 shows the performance with these twosets obtained from both supervised CRF and JESS-CM with 1G-word unlabeled data.
As the super-vised CRF results, the performance of the L.?appsets is consistently much lower than that of the cor-670(a) POS tagging (b) Chunking (c) NEReval.
data development test test development testL.
?app L.app L.?app L.app L.?app L.app L.?app L.app L.?app L.apprates of sentences (46.1%) (53.9%) (40.4%) (59.6%) (70.7%) (29.3%) (54.3%) (45.7%) (64.3%) (35.7%)supervised CRF (baseline) 46.78 60.99 48.57 60.01 56.92 67.91 79.60 97.35 75.69 91.03JESS-CM (CRF/HMM) 49.02 62.60 50.79 61.24 62.47 71.30 85.87 97.47 80.84 92.85(gain from supervised CRF) (+2.24) (+1.61) (+2.22) (+1.23) (+5.55) (+3.40) (+6.27) (+0.12) (+5.15) (+1.82)U.app 83.7% 96.3% 84.3% 95.8% 89.5% 99.2% 95.3% 99.8% 94.9% 100.0%Table 5: Comparison with L.?app and L.app sets obtained from both supervised CRF and JESS-CM with 1G-wordunlabeled data evaluated by the entire sentence accuracies, and the ratio of U.app.unlab.
data dev (Aug. 30-31) test (Dec. 06-07)(period) #sent.
#wds F?=1 U.app F?=1 U.appreu(Sep.) 1.0M 17M 93.50 82.0% 88.27 69.7%reu(Oct.) 1.3M 20M 93.04 71.0% 88.82 72.0%reu(Nov.) 1.2M 18M 92.94 68.7% 89.08 74.3%reu(Dec.)* 9M 15M 92.91 67.0% 89.29 84.4%Table 6: Influence of U.app in NER experiments: *(ex-cluding Dec. 06-07)responding L.app sets.
Moreover, we can observethat the ratios of L.?app are not so small; nearly half(46.1% and 40.4%) in the PTB III data, and morethan half (70.7%, 54.3% and 64.3%) in CoNLL?00and ?03 data, respectively.
This indicates that wordsnot appearing in the labeled training data are reallyharmful for supervised learning.
Although the per-formance with L.?app sets is still poorer than withL.app sets, the JESS-CM results indicate that the in-troduction of unlabeled data effectively improves theperformance of L.?app sets, even more than that ofL.app sets.
These improvements are essentially veryimportant; when a tagger and chunker are actuallyused, input data can be obtained from anywhere andthis may mostly include words that do not appearin the given labeled training data since the labeledtraining data is limited and difficult to increase.
Thismeans that the improved performance of L.?app canlink directly to actual use.Table 5 also shows the ratios of sentences thatare constructed from words that all appeared in the1G-word Unlabeled data used in our experiments(U.app) in the L.?app and L.app.
This indicates thatmost of the words in the development or test sets arecovered by the 1G-word unlabeled data.
This maybe the main reason for JESS-CM providing largeperformance gains for both the overall and L.?appset performance of all three tasks.Table 6 shows the relation between JESS-CM per-formance and U.app in the NER experiments.
Thedevelopment data and test data were obtained fromsystem dev.
test additional resourcesJESS-CM (CRF/HMM) 97.35 97.40 1G-word unlabeled data(Shen et al, 2007) 97.28 97.33 ?
(Toutanova et al, 2003) 97.15 97.24 crude company name detector[sup.
CRF (baseline)] 97.18 97.21 ?Table 7: POS tagging results of the previous top systemsfor PTB III data evaluated by label accuracysystem test additional resourcesJESS-CM (CRF/HMM) 95.15 1G-word unlabeled data94.67 15M-word unlabeled data(Ando and Zhang, 2005) 94.39 15M-word unlabeled data(Suzuki et al, 2007) 94.36 17M-word unlabeled data(Zhang et al, 2002) 94.17 full parser output(Kudo and Matsumoto, 2001) 93.91 ?
[supervised CRF (baseline)] 93.88 ?Table 8: Syntactic chunking results of the previous topsystems for CoNLL?00 shared task data (F?=1 score)30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters newsarticles, respectively.
We find that temporal proxim-ity leads to better performance.
This aspect can alsobe explained as U.app.
Basically, the U.app increaseleads to improved performance.The evidence provided by the above experimentsimplies that increasing the coverage of unlabeleddata offers the strong possibility of increasing theexpected performance of unseen data.
Thus, itstrongly encourages us to use an SSL approach thatincludes JESS-CM to construct a general tagger andchunker for actual use.5 Comparison with Previous Top Systemsand Related WorkIn POS tagging, the previous best performance wasreported by (Shen et al, 2007) as summarized inTable 7.
Their method uses a novel sophisticatedmodel that learns both decoding order and labeling,while our model uses a standard first order Markovmodel.
Despite using such a simple model, ourmethod can provide a better result with the help ofunlabeled data.671system dev.
test additional resourcesJESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data93.66 89.36 37M-word unlabeled data(Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data(Florian et al, 2003) 93.87 88.76 own large gazetteers,2M-word labeled data(Suzuki et al, 2007) N/A 88.41 27M-word unlabeled data[sup.
CRF (baseline)] 91.74 86.35 ?Table 9: NER results of the previous top systems forCoNLL?03 shared task data evaluated by F?=1 scoreAs shown in Tables 8 and 9, the previous bestperformance for syntactic chunking and NER wasreported by (Ando and Zhang, 2005), and is re-ferred to as ?ASO-semi?.
ASO-semi also incorpo-rates unlabeled data solely as additional informa-tion in the same way as JESS-CM.
ASO-semi usesunlabeled data for constructing auxiliary problemsthat are expected to capture a good feature repre-sentation of the target problem.
As regards syntac-tic chunking, JESS-CM significantly outperformedASO-semi for the same 15M-word unlabeled datasize obtained from the Wall Street Journal in 1991as described in (Ando and Zhang, 2005).
Unfor-tunately with NER, JESS-CM is slightly inferior toASO-semi for the same 27M-word unlabeled datasize extracted from the Reuters corpus.
In fact,JESS-CM using 37M-words of unlabeled data pro-vided a comparable result.
We observed that ASO-semi prefers ?nugget extraction?
tasks to ?field seg-mentation?
tasks (Grenager et al, 2005).
We can-not provide details here owing to the space limi-tation.
Intuitively, their word prediction auxiliaryproblems can capture only a limited number of char-acteristic behaviors because the auxiliary problemsare constructed by a limited number of ?binary?
clas-sifiers.
Moreover, we should remember that ASO-semi used the human knowledge that ?named en-tities mostly consist of nouns or adjectives?
duringthe auxiliary problem construction in their NER ex-periments.
In contrast, our results require no suchadditional knowledge or limitation.
In addition, thedesign and training of auxiliary problems as well ascalculating SVD are too costly when the size of theunlabeled data increases.
These facts imply that ourSSL framework is rather appropriate for handlinglarge scale unlabeled data.On the other hand, ASO-semi and JESS-CM havean important common feature.
That is, both meth-ods discriminatively combine models trained by us-ing unlabeled data in order to create informative fea-ture representation for discriminative learning.
Un-like self/co-training approaches (Blum and Mitchell,1998), which use estimated labels as ?correct la-bels?, this approach automatically judges the relia-bility of additional features obtained from unlabeleddata in terms of discriminative training.
Ando andZhang (2007) have also pointed out that this method-ology seems to be one key to achieving higher per-formance in NLP applications.There is an approach that combines individuallyand independently trained joint PMs into a discrimi-native model (Li and McCallum, 2005).
There is anessential difference between this method and JESS-CM.
We categorize their approach as an ?indirectapproach?
since the outputs of the target task, y,are not considered during the unlabeled data incor-poration.
Note that ASO-semi is also an ?indirectapproach?.
On the other hand, our approach is a?direct approach?
because the distribution of y ob-tained from JESS-CM is used as ?seeds?
of hiddenstates during MDF estimation for join PM param-eters (see Section 4.1).
In addition, MDF estima-tion over unlabeled data can effectively incorporatethe ?labeled?
training data information via a ?bias?since ?
included in A(x,y) is estimated from la-beled training data.6 ConclusionWe proposed a simple yet powerful semi-supervisedconditional model, which we call JESS-CM.
It isapplicable to large amounts of unlabeled data, forexample, at the giga-word level.
Experimental re-sults obtained by using JESS-CM incorporating 1G-words of unlabeled data have provided the currentbest performance as regards POS tagging, syntacticchunking, and NER for widely used large test col-lections such as PTB III, CoNLL?00 and ?03 sharedtask data, respectively.
We also provided evidencethat the use of more unlabeled data in SSL can leadto further improvements.
Moreover, our experimen-tal analysis revealed that it may also induce an im-provement in the expected performance for unseendata in terms of the unlabeled data coverage.
Our re-sults may encourage the adoption of the SSL methodfor many other real world applications.672ReferencesR.
Ando and T. Zhang.
2005.
A High-PerformanceSemi-Supervised Learning Method for Text Chunking.In Proc.
of ACL-2005, pages 1?9.R.
Ando and T. Zhang.
2007.
Two-view Feature Genera-tion Model for Semi-supervised Learning.
In Proc.
ofICML-2007, pages 25?32.A.
Blum and T. Mitchell.
1998.
Combining Labeled andUnlabeled Data with Co-Training.
In Conference onComputational Learning Theory 11.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm.
Journal of the Royal Statistical Soci-ety, Series B, 39:1?38.R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang.
2003.Named Entity Recognition through Classifier Combi-nation.
In Proc.
of CoNLL-2003, pages 168?171.T.
Grenager, D. Klein, and C. Manning.
2005.
Unsu-pervised Learning of Field Segmentation Models forInformation Extraction.
In Proc.
of ACL-2005, pages371?378.T.
Kudo and Y. Matsumoto.
2001.
Chunking with Sup-port Vector Machines.
In Proc.
of NAACL 2001, pages192?199.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
ofICML-2001, pages 282?289.W.
Li and A. McCallum.
2005.
Semi-Supervised Se-quence Modeling with Syntactic Topic Models.
InProc.
of AAAI-2005, pages 813?818.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.2000.
Text Classification from Labeled and UnlabeledDocuments using EM.
Machine Learning, 39:103?134.F.
Sha and F. Pereira.
2003.
Shallow Parsing with Condi-tional Random Fields.
In Proc.
of HLT/NAACL-2003,pages 213?220.L.
Shen, G. Satta, and A. Joshi.
2007.
Guided Learningfor Bidirectional Sequence Classification.
In Proc.
ofACL-2007, pages 760?767.C.
Sutton, M. Sindelar, and A. McCallum.
2006.
Reduc-ing Weight Undertraining in Structured DiscriminativeLearning.
In Proc.
of HTL-NAACL 2006, pages 89?95.J Suzuki, A Fujino, and H Isozaki.
2007.
Semi-Supervised Structured Output Learning Based on aHybrid Generative and Discriminative Approach.
InProc.
of EMNLP-CoNLL, pages 791?800.E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 Shared Task: Chunking.
InProc.
of CoNLL-2000 and LLL-2000, pages 127?132.E.
T. Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Proc.
ofCoNLL-2003, pages 142?147.K.
Toutanova, D. Klein, C.D.
Manning, andY.
Yoram Singer.
2003.
Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network.In Proc.
of HLT-NAACL-2003, pages 252?259.T.
Zhang, F. Damerau, and D. Johnson.
2002.
TextChunking based on a Generalization of Winnow.
Ma-chine Learning Research, 2:615?637.673
