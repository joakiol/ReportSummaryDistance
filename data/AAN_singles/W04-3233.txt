NP Bracketing by Maximum Entropy Tagging and SVM RerankingHal Daume?
III and Daniel MarcuUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292{hdaume,marcu}@isi.eduAbstractWe perform Noun Phrase Bracketing by using a lo-cal, maximum entropy-based tagging model, whichproduces bracketing hypotheses.
These hypothe-ses are subsequently fed into a reranking frame-work based on support vector machines.
We solvethe problem of hierarchical structure in our tag-ging model by modeling underspecified tags, whichare fully determined only at decoding time.
Thetagging model performs comparably to competingapproaches and the subsequent reranking increasesour system?s performance from an f-score of 81.7 to86.1, surpassing the best reported results to date of83.8.1 Introduction and Prior WorkNoun Phrase Bracketing (NP Bracketing) is the taskof identifying any and all noun phrases in a sen-tence.
It is a strictly more difficult problem thanNP Chunking (Ramshaw and Marcus, 1995), inwhich only non-recursive (or ?base?)
noun phrasesare identified.
It is simultaneously strictly more sim-ple than either full parsing (Collins, 2003; Charniak,2000) or supertagging (Bangalore and Joshi, 1999).NP Bracketing is both a useful first step toward fullparsing and also a meaningful task in its own right;for instance as an initial step toward co-referenceresolution and noun-phrase translation.While existing NP Bracketers (including the onedescribed in this paper) tend to achieve worse over-all F-measures than a full statistical parser (eg.,(Collins, 2003; Charniak, 2000)), they can be sig-nificantly more computationally efficient.
Statisti-cal parsers tend to scale exponentially in sentencelength, unless a narrow beam is employed, whichleads to globally poorer parses.
In contrast, thebracketer described in this paper scales linearly in[[Confidence] in [the pound]] is widely expectedto take [another sharp dive] if [[[trade figures] for[September]] , due for [release] [tomorrow] ,] .
.
.Figure 1: Sample sentence with NPs bracketed.the length of the sentence to find the globally op-timal solution.
This trade-off is depicted graphi-cally in Figure 2.
This figure shows the amount oftime (excluding any startup overhead) spent pars-ing or bracketing using this system (the two lowestlines) versus the parsers of Collins (2003) and Char-niak (2000) run with default settings.NP Bracketing was the shared task of the Com-putational Natural Language Learning workshop in1999 (CoNLL-99).
In this competition, NP Brack-eting systems were trained on sections 15-18 of theWall Street Journal corpus, while section 20 wasused for testing.
The bracketing information wasextracted directly from the Penn Treebank, essen-tially disregarding all non-NP brackets.
An examplebracketed sentence is in Figure 1.There have been several successful approachesreported in the literature to solve this task.
TjongKim Sang (1999) first used repeated chunking to at-tain an f-score of 82.98 during the CoNLL compe-tition and subsequently (Sang, 2002) an f-score of83.79 using a combination of two different systems.Krymolowski and Dagan (2000) have obtained sim-ilar results using more training data and lexicaliza-tion.
Brandts (1999) has used cascaded HMMs tosolve the NP Bracketing problem; however, he eval-uated his system only on German NPs, so his resultscannot be directly compared.Obviously, the difficulty that arises in NP Brack-eting that differentiates it from NP Chunking is theissue of embedded NPs, thus requiring output in the5 10 15 20 25 30 35 40 45 50 55051015202530Sentence LengthSecondstoParse(normalized)CharniakCollinsBracketer+SVMBracketerFigure 2: Speed of different systemsform of a tree structure.
Most solutions to prob-lems involving building trees from sequences buildin to the model a concept of depth (in parsing, thisis typically in the form of a chart; in bracketing andshallow parsing, this is typically in the form of em-bedded finite-state automata).
We elect to take acompletely different approach.
The model we useis agnostic to any sort of depth: it hypothesizes un-derspecified tags and allows the matching bracketconstraint to select a solution.Specifically, we approach the NP Bracketingproblem as a tagging and reranking problem.
Weuse an efficient maximum entropy-based tagger tohypothesize possible bracketings (see Section 2)and then rerank these hypotheses using a supportvector reranking system (see Section 3).
Using onlythe tagger (without reranking), we achieve compa-rable results to those referenced above and, with theaddition of the reranking system, achieve, to ourknowledge, the best reported results to date.2 Bracketing as a Tagging ProblemIn any tagging problem, the task is to associate eachword in the input with a single tag.
There aremany competing approaches to tagging problemsincluding Hidden Markov Models (HMMs), Maxi-mum Entropy Markov Models (MEMMs) and Con-ditional Random Fields (CRFs).
We adopt a slightvariant of the MEMM framework.2.1 Maximum Entropy Tagging ModelIn the formulation of the maximum entropy taggingmodel, we assume that the probability distributionof tags takes the form of an exponential distribution,parameterized by a sequence of feature weights,?m1 , where there are m-many features.
Thus, weobtain a distribution for Pr?m1 (ti ti?1, w?)
of theform:1Zti?1,w?exp?
?m?j=1?jfj(ti, ti?1, w?)??
(1)where Zti?1,w?
is a normalizing factor.Like other maximum entropy approaches, thisdistribution is unimodal and optimal values for the?s can be found through various algorithms; weuse GIS.
A good introduction to maximum entropymodels can be found in (Berger et al, 1996).In our approach, we use a tag set of exactly fivetags: {open, close, in, out, sing}.
An open tag isassigned to all words that open a bracketing (regard-less of the number of brackets opened) and do notalso close a bracketing.
A close tag is assigned to allwords that close a bracketing and do not also openone.
An in tag is assigned to all words enclosed inan NP, but which neither open nor close one.
An outtag is assigned to all words which are not enclosedin an NP.
A sing(leton) tag is assigned to all wordsthat both open and close a bracketing (regardless ofwhether they open or close more than just their ownbracketing).Note that such a tagging does not uniquely deter-mine a bracketing.
For instance, the tag sequence?sing sing?
could correspond either to [[w1] [w2]]or to [w1] [w2].
Nevertheless, due to the constraintsinvolved in the tagging process (namely that a closetag cannot appear unless one is already within anNP and that one cannot have two close tags whenthe corresponding open tags appear at the same lo-cation1), we hope that our system will be able to dis-ambiguate sufficiently.
In other words, although ourtaggings are under-specified, we hope that the ad-ditional constraints that we subsequently associatewith these tags will yield high quality bracketings.2.2 Feature FunctionsThe probability distribution shown in Equation 1 isbased on m-many real-valued feature functions, fj .We use two classes of features, closed features andopen features (these roughly correspond to whetherthey look at closed class elements or open class ele-ments).
The open features for position i are appliedat positions i, i ?
1 and i + 1.
The closed featuresare applied at i, i ?
1, i ?
2, i ?
3 and i + 1, i + 2and i + 3.1For instance, the bracketing [[wi .
.
.
wj ]] is disallowed;this bracketing must appear simply as [wi .
.
.
wj ].Closed features include: part of speech tag (ac-cording to Brill?s (1995) tagger); two character suf-fix of word; first character of part of speech; initialcharacter capitalized; word fully capitalized; lastcharacter is period; word position in sentence; andtwo features for when the word is either the first orlast word in the sentence.
Open features include: theword itself; the word lower-cased; the lower-casedstem (Porter, 1980); the lower-cased stem plus thepart of speech; and 3 features that are each truewhen there is a CC in the next 2 through 5 words.In addition, we include a feature for tag ti?1.2.3 Maximum Entropy TrainingWe used generalized iterative scaling to train themaximum entropy model2 on 929, 921 features and211, 728 training instances from sections 15-18 ofthe Penn Treebank (20% of which was set aside asa validation set).
Training was run for ten thousanditerations and, at convergence, achieved a taggingerror rate of 2.1% on the training data and 6.9% onthe validation data.2.4 Decoding AlgorithmWe use a Viterbi-like dynamic programming de-coding algorithm, where transition probabilitiesare governed by the discriminative tagging model.However, the tags generated by our decoder are notthe same as those predicted by the maximum en-tropy model.
Our decoder does not search in theoriginal space of tags (sing, in, out, .
.
. )
but ratherin a new space that yields only well-formed brack-etings.
In the secondary search space, the algorithmis guaranteed to find the most likely well-formedbracketing, even though this might not correspondto the most likely tag sequence.
While it would bepossible to simply tag using the original tag set andallow the reranker (see Section 3) to select a well-formed bracketing, it is unlikely that this will leadto improved performance: the complexity of the de-coders will be the same, yet the bracketer wouldhave to wade through significantly more bad tag-gings to find a good solution.Our decoding tags take one of five forms, capi-talized to distinguish them from the maximum en-tropy tags: On, Cn , N , OnC , OCn where n ?
1for all but OCn where n ?
2.
The meaning of thetags is: On means n simultaneous open brackets:Cn means n simultaneous close brackets.
N meansthat no brackets appear at this position.
OnC corre-2Using the YASMET maximum entropy training package:http://www.isi.edu/?och/YASMET/.0 20 40 60 80 100 120 140 160 180 200828486889092949698PrecisionRecallF?ScoreFigure 3: Plot of n versus maximal f-score (and as-sociated precision and recall) for test data.sponds to n open brackets and one close bracket,while OCn corresponds to one open bracket andn ?
2 close brackets.
These tags are enough todecode any well-formed bracketing.Our decoder assumes a maximum depth of tagsd has been prespecified and then solves a dynamicprogramming problem on an n ?
d ?
t array A,where n is the sentence length and t denotes an inte-ger corresponding to the highest possible decodingtag in an enumeration.
The value Ai,d,t stores theprobability of being at position i and depth d af-ter applying tag t at that position.
It is always thecase that t ?
4d.
The time and space complexityof this decoding problem is thus O(d2n).
The dy-namic programming problem is:A1,d,t = Pr??
(t?0) (2)Ap,d,t = maxt?
Ap?1,d??t,t?
?
Pr??
(t?d t?)
(3)wheret?d =??????????
?out t = N ?
d = 0in t = N ?
d > 0sing t ?
{OnC,OCn}begin t = Onend t = Cn(4)?t =??????????
?n t = Onn ?
1 t = OnC?n t = Cn?n + 1 t = OCn0 t = N(5)The intuition for calculating the value of Ap,d,tfor p > 1 (see Equation 3) is that we first choosethe optimal previous tag, t?.
Furthermore, based ont and d, we can calculate the depth (d ?
?t, seeEquation 5) we must have been at previously.
Thus,we must take the value of Ap?1,d??t,t?
which is theprobability of having arrived at position p ?
1 atdepth d ?
?t with tag t?.
We then multiply thisby the probability of getting from that position tothe current position, which is given by Pr??
(t?d t?
)(note that the normalization occurs over the newspace of tags).
The optimal tagging is given byback-tracing through A, beginning at An,0,t for anytag t. Even for long sentences, this algorithm re-quires very little time and memory.2.5 Model DeficienciesWhile the bracketing model described above al-ready performs comparably to competing ap-proaches (see Section 4), it is still subject to mak-ing categorical mistakes.
Most of its errors are dueto the locality of the decisions made.
Because ofthe coarseness of the tags used in the maximum en-tropy tagging framework, the model is unable to dis-criminate between some bad bracketings and somegood ones.
For instance, it must assign preciselythe same probability to both of the following brack-etings, since the maximum entropy tags (shown be-neath) are identical:[[John,] [president] of [the company] ,][[John,] [[president] of [the company]]] ,]sing sing in open close closeThis limitation causes the model to make con-sistent mistakes distinguishing between, for exam-ple, lists and appositional phrases.
To solve theseproblems in the tagging model would be nearly im-possible, without giving up on efficiency.
However,our decoder is able to produce n-best lists using ex-act A?
search that very frequently contain globallysuperior taggings, even though the simple taggingmodel cannot recognize them as such.In Figure 3, we show the maximal f-score (andcorresponding precision and recall) for the bestbracketing chosen out of the n-best, as we let nrange from 1 to 400 for both the validation dataand the test data.
As we can see from these graphs,we have the possibility of improving our system?s f-score performance by about ten points ?
from 82%to 93%, simply by being able to choose the correcthypothesis from the n-best list; also working with100-best lists is likely sufficient.3 Hypothesis RerankingIn the previous section, we described a taggingmodel for NP Bracketing that can produce n-bestlists.
In this section, we describe a machine learn-ing method for reranking these lists in an attempt tochoose a hypothesis which is superior to the first-best output of the decoder.
Reranking of n-best listshas recently become popular in several natural lan-guage problems, including parsing (Collins, 2003),machine translation (Och and Ney, 2002) and websearch (Joachims, 2002).
Each of these researcherstakes a different approach to reranking.
Collins(2003) uses both Markov Random Fields and boost-ing, Och and Ney (2002) use a maximum entropyranking scheme, and Joachims (2002) uses a sup-port vector approach.
As SVMs tend to exhibit lessproblems with over-fitting than other competing ap-proaches in noisy scenarios, we also adopt the sup-port vector approach.3.1 Support Vector RerankingA support vector classifier is a binary classifier witha linear decision boundary.
The selected decisionboundary is a hyperplane that is chosen in such away that the distance between it and the nearest datapoints is maximized.
Slack variables are commonlyintroduced when the problem is not linearly separa-ble, leading to soft margins.For reranking, we assume that instead of havingbinary classes for the yis, we have real values whichspecify the relative ordering (higher values comefirst).
For this task, we get the following optimiza-tion problem (Joachims, 2002):minimize 12 ||w?||2 + CN?i=1?i,j (6)subject to w?
?
x?i ?
w?
?
x?j + 1 ?
?i,j (7)?i,j ?
0 (8)Where the i, js are drawn from comparable datapoints and yi ?
yj and C is a regularization param-eter that specifies how great the cost of mis-orderingis.
As noticed by Joachims, the condition in Equa-tion 7 can be reduced to the standard SVM modelby subtracting w?
?
x?j from both sides.3.2 Reranking Feature FunctionsSince our problem is closely related to that ofCollins?
(2003), we use many of the same featurefunctions he does, though we do introduce many ofour own (those which are copied from Collins aremarked with an asterisk).
We view the hypothesizedbracketing as a tree in a context free grammar andinclude features based on each rule used to gener-ate the given tree.
For concreteness, we will use theCFG rule NP ?
DT JJ NP (where the NP is selectedas the head) as an example.Rules*: the full CFG rule; in this case, the activerule would be NP ?
DT JJ NP.Markov 2 Rules: CFG rules where 2-levelMarkovization has been applied.
That is, we lookat the rule for generating the first two tags, thenthe next two (given the previous one), then the nexttwo (given the previous one), and so on.
A startof branch tag ([S]) and end of branch tag ([/S]) areadded to the beginning and end of the children lists.In this case, the rules that fire are: NP!
?
[S] DT,NP!
[S] ?
DT JJ, NP!DT ?
JJ NP and NP!JJ ?
NP[/S].
The notation is X!Y ?
A B, where X is the trueparent, Y was the previous child in the Markoviza-tion, and A B are the two children.Lex-Rules*: full CFG rules, where terminal POStags are replaced with lexical items.Markov 2 Lex-Rules: Markov 2-style rules, ter-minal POS tags are replaced with lexical items.Bigrams*: pairs of adjacent tags in the CFGrule; in our example, the active pairs are ([S],DT),(DT,JJ), (JJ,NP) and (NP,[/S]).Lex-Bigrams*: same as BIGRAMS, but with lex-ical heads instead of POS tags.Head Pairs*: pairs of internal node tags with thehead type; in the example, (DT, NP), (JJ, NP) and(NP, NP).Sizes: the child count, conditioned on the internaltag; eg., NP ?
3.Word Count: pair of the SIZES and total numberof words under this constituent.Boundary Heads: pairs of the first and last headin the constituent.POS-Counts: a scheme of features that countthe number of children whose part of speech tagmatches a given predicate.
There are six of these:(1) children whose tag begins with N, (2) childrenwhose tag begins with N but is not NP, (3) childrenwhich are DTs, (4) children whose tag begin with V,(5) children which are commas, (6) children whosetag is CC.
In this case, we get a count of 1 for rules(2) and (3), and 2 for rule (1).Lex-Tag/Head Pairs: same as HEAD PAIRS, butwhere lexical items are used instead of POS tags.Special Tag Pairs: count of the lexical heads tothe left and right of leaves tagged with each of POS,CC, IN and TO.Tag-Counts: another schema of features thatreplicates some of the features used in the maxi-mum entropy tagger.
This schema includes all theoriginal maximum entropy tags, as well as a featurefor each maximum entropy tag at position i, pairedwith (a) the part of speech tag at position i, i?1 andi + 1, (b) the word at position i, i ?
1 and i + 1, (c)the part of speech + word pair at those positions, (d)the maximum entropy tag at that position.3.3 SVM TrainingWe develop three reranking systems, differentiatedby the amount of training data used.
The first,RR1, is trained on the validation part of the train-ing set (20% of sections 15-18).
The second, RR2.is trained on the entire training set through cross-validation (all of sections 15-18).
The final, RR3 istrained on the entire Penn Treebank corpus, exceptsection 20.Training the reranking system only on the valida-tion data (RR1) results in only a marginal gain ofoverall f-score, due primarily to the fact that mostof the features use lexical information to prefer onebracketing over another.
The validation data fromsections 15-18 gives rise to 2, 012 training instancesand 362, 415 features.
In order to train the rerankingsystem on all of the training data (RR2), we builtfive decoders, each with a different 20% of the train-ing data held out.
Each decoder is then used to tagthe held-out 20% (this is done so that the tagger doesnot do ?too well?
on its training data).
This leads to8, 935 sentences for training, with a total of 1.1 mil-lion features.
Training on all the WSJ data exceptsection 20 (RR3) gives rise to 39, 953 training in-stances and a total of just over 2.1 million features.These examples give 1, 462, 568 rank constraints.4 ResultsWe compare our system against those reported inthe literature.
In all, the evaluation is over 2, 012sentences of test data.
In Table 1, we display the re-sults of state-of-the-art systems, and the system de-scribed in this paper (both with and without rerank-ing).
The upper part of the table displays resultsfrom systems which are trained only on sections 15-18 of the WSJ.
The lower part displays results basedon systems trained on more data.System BR BP BF CBTKS99 76.1 91.3 82.8 0.14TKS02 78.4 90.0 83.8 -TAG 81.0 86.0 83.4 0.26RR1 82.1 88.8 85.3 0.18RR2 82.7 89.8 86.1 0.14COL03NP 68.6 68.9 68.7 0.91COL03Full 88.2 87.7 87.9 0.31CHUNK 73.0 100.0 84.4 -COL03All 88.0 89.8 88.9 0.18KD00 79.3 88.5 83.7 -RR3 84.3 90.8 87.4 0.12Table 1: Results on test data.
The systems in thelower half are not directly comparable, since theywere either trained or tested on different data.In the table, TKS99 and TKS02 are the sys-tems of Tjong Kim Sang (1999; 2002).
KD00 isthe system of (Krymolowski and Dagan, 2000).
Allthe COL03 systems are results obtained using therestriction of the output of Collins (2003) parser.In particular, the two comparable numbers comingfrom Collins?
parser are COL03NP and COL03Full .The difference between these two systems is that theNP system is trained on parse trees, with all non-NPnodes removed.
The FULL system is trained on fullparse trees, and then the output is reduced to just in-clude NPs.
COL03All is trained on sections 2-21 ofWSJ and tested on section 23, and is thus an upperbound, since these numbers are testing on trainingdata.3 Our RR3 system had the reranking compo-nent (but not the tagging component) trained on allof the WSJ except for section 20.The CHUNK row in the results table is the per-formance of an optimally performing NP chunker.That is, this is the performance attainable given achunker that identifies base NPs perfectly (at 100%precision).
However, since this hypothetical sys-tem only chunks base NPs, it misses all non-baseNPs and thus achieves a recall of only 73.0, yield-ing an overall F-score below our system?s perfor-mance.
Note also that no chunker will perform thiswell.
Current systems attain approximately 94%precision and recall on the chunking task (Sha andPereira, 2002; Kudo and Matsumoto, 2001), so the3Collins independently reports a recall of 91.2 and preci-sion of 90.3 for NPs (Collins, 2003); however, these numbersare based on training on all the data and testing on section 0.Moreover, it is possible that his evaluation of NP bracketing isnot identical to our own.
The results in row COL03Full aretherefore perhaps more relevant.actual performance for a real system would be sub-stantially lower.The four criteria these systems are evaluatedon are bracketing recall (BR), bracketing precision(BP), bracketing f-score (BF) and average crossingbrackets (CB).
Some systems do not report theircrossing bracket rate.
All of these metrics are cal-culated only on NP* and WHNP* brackets.5 Comparison of PerformanceThe results depicted in Table 1 show that, whencomparing our system directly to Collins?
parser,his system tends to achieve significantly higher lev-els of recall, while maintaining a slight advantagein terms of precision.
This table, however, does nottell the full story.
As is typically observed in thesesort of applications, it is not the case that Collins?parser is ?winning?
by a little on all the data, butrather that Collins?
parser wins on some of the dataand our bracketer wins on some of the data.
In thissection, we analyze the differences.Overall, there are 2, 012 sentences in the testdata.
In 558 cases, both the bracketing system andCollins?
parser achieve perfect precision.
In 505cases, both achieve perfect recall.
For the remainderof the discussion in this section, when discussingprecision, we will only consider the cases in whichnot both achieved perfect scores, and similarly forrecall.In Figure 4, we depict (excluding the mutuallyperfect sentences) the percentage of sentences onwhich each system is better than the other by a dis-tance of at least .
Along the X-axes, the value of ranges from 0 to 20.
At a given value of , the seg-mentation along the Y-axes depict (a) along the top(in yellow where available), the proportion of sen-tences for which the bracketer?s precision (for theleft hand image) was at least  of that of Collins?
;(b) in the middle (in red), the proportion of sen-tences for which Collins?
was at least  better; and(c) along the bottom (in blue), the proportion of sen-tences where the two systems performed within  ofeach other.As should be expected, as  increases, the?Equal?
region also increases.
However, it is worthnoticing that even at an  of 20 precision points,there are still roughly 11% of the sentences forwhich one system?s performance is noticeably dif-ferent from the other?s (and furthermore, that theseare about even).
As can be immediately seen fromthe right-hand graph, Collins?
parser consistentlyoutperforms the bracketer in terms of recall.
How-Figure 4: Proportion of sentences for which one system outperforms the other with difference at least .Precision RecallTag RR2 COL03 RR2 COL03NP 21.4 19.8 20.5 21.3VP 7.49 8.52 8.31 7.57NN 8.22 7.62 7.43 7.83IN 6.01 5.89 5.31 6.15PP 5.90 5.63 5.16 6.03S 4.96 5.82 5.44 5.15NNP 6.15 4.79 6.29 5.82Table 2: Percentage of tags on superior system.ever, in contrast to the Precision graph, for thefirst 10 or so values of , these proportions remainroughly the same (in fact, for a short period, Collins?actually looses ground).
This suggests that there area relatively large proportion of sentences for whichour system is performing abominably (with > 10recall points difference) in comparison to Collins?.However, once a critical mass of  > 10 is reached,the relative differences become less strong.Since neither system is winning in all cases, in aneffort to better understand the conditions in whichone system will outperform the other, we inspectthe sentences for which there was a difference inperformance of at least 10 (for precision and recallseparately).
To perform this investigation, we lookat the distribution of tags in the true, full parse treesfor those sentences.
These percentages, for the 7most common tags, are summarized in Table 2 (forexample, the relative frequency of the NP tag in sen-tences where the RR2 system achieved higher pre-cision was 21.4, while for the sentences for whichCOL03 achieved higher precision was 19.8).The first thing worth noticing in this table is thatin general, when one system achieves higher preci-sion, the other system achieves higher recall, whichis not surprising.
However, in the last row, corre-sponding to proper nouns, the RR2 system outper-forms the COL03 (this is the ?Full?
implementa-tion) in both precision and recall, suggesting thatour system is better able to capture the phrasingof proper nouns.
We attribute this to the fact thatour model is specialized to identify noun phrases,of which proper nouns comprise a large part.
Simi-larly, the largest gains in recall for COL03 over RR2are in sentences with many PPs.
This coincides withour intuition about the syntactic parser being betterable to capture long, embedded noun phrases.6 ConclusionWe have presented a method for performing nounphrase bracketing, which outperforms competingmethods both in terms of f-score and recall.
Thesystem is based on two separate components: amaximum entropy-based tagging system and a sup-port vector machine reranking system.
The keycomponent of the tagging system is that it producesunderspecified tags that are determined only at de-coding time by bracketing constraints.
The taggingsystem operates very quickly and can tag and rerankat a rate of approximately two sentences per second.The tagger alone achieves an f-score of 83.4.
Thisscore is only 0.4% lower (absolute) than the best re-ported result to date of 83.8.After tagging, we have fed 100 best lists into asupport vector reranking system, which performsglobal optimization to choose a good bracketing.Our reranking system is able to increase the f-scoreof our bracketing approach from 83.4 to 86.1, im-proving our performance beyond the best reportedsystem to date.As we can see from Table 1, by comparing theoutput of our system to that of COL00Full , there ismuch in the way of recall to be gained by using afull syntactic parser.
However, this gain comes attwo expenses.
First, full syntactic parsers are com-putationally more expensive to run.
Moreover, per-formance of Collins?
parser degrades significantly(from 87.9 to 68.7 in f-score) when it cannot takeadvantage of other constituent information.
Thishas a strong influence when one is faced with thetask of moving to a new domain.
On the one hand,our system (as well as the other bracketing systemscited) requires data to only be annotated at the NPlevel in order to achieve high performance.
Con-versely, without full parses, using a parser for learn-ing NPs is inadequate.Despite these successes, there is still much thatcan be improved upon.
While the reranking isvery efficient in the classification phase, traininga support vector reranking system is computation-ally very expensive.
Other well grounded statisticallearning systems might allow us to train this com-ponent on more data and using more features.
Wealso hope to be able to improve our system?s perfor-mance from its current rate of 86.1 (on official data)and 87.4 (on all data) closer to the n-best optimal,depicted in Figure 3.7 AcknowledgmentsThis work was partially supported by DARPA-ITOgrant N66001-00-1-9814, NSF grant IIS-0097846,and a USC Dean Fellowship to Hal Daume?
III.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.Supertagging: An approach to alsmost parsing.Computational Linguistics, 25(2):237?265.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Com-putational Linguistics, 22(1):39?71.Thorsten Brandts.
1999.
Cascaded markov models.In Proceedings of EACL 1999.Eric Brill.
1995.
Transformation-based error-driven learning and natural language processing:a case study in part of speech tagging.
Computa-tional Linguistics, December.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the First An-nual Meeting of the North American Chapterof the Association for Computational LinguisticsNAACL?2000, pages 132?139, Seattle, Washing-ton, April 29 ?
May 3.Michael Collins.
2003.
Head-driven statisticalmodels for natural language parsing.
Computa-tional Linguistics, 29(4), December.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings ofthe ACM Conference on Knowledge Discoveryand Data Mining (KDD).
ACM.Yuval Krymolowski and Ido Dagan.
2000.
Incorpo-rating compositional evidence in memory-basedpartial parsing.
In Proceedings of ACL 2000,Hong Kong.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In NAACL.Franz Josef Och and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
In ACL 02, pages295?302, Philadelphia, PA, July.M.F.
Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14:130?137.Lance A. Ramshaw and Michell P. Marcus.
1995.Text chunking using transformation-based learn-ing.
In Proceedings of the Third ACL Workshopon Very Large Corpora.
Association for Compu-tational Linguistics.Erik F. Tjong Kim Sang.
1999.
Noun phrase detec-tion by repeated chunking.
In CoNLL-99 Work-shop, Bergen, Norway.Erik F. Tjong Kim Sang.
2002.
Memory-basedshallow parsing.
Journal of Machine LearningResearch, 2:559 ?
594, March.Fei Sha and Fernando Pereira.
2002.
Shallow pars-ing with conditional random fields.
In HLT-NAACL.
