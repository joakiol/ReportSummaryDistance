Comparing Knowledge Sources for NominalAnaphora ResolutionKatja Markert?University of LeedsMalvina Nissim?University of EdinburghWe compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphoraand definite noun phrase coreference.
Specifically, we compare an algorithm that relies on linksencoded in the manually created lexical hierarchy WordNet and an algorithm that mines corporaby means of shallow lexico-semantic patterns.
As corpora we use the British National Corpus(BNC), as well as the Web, which has not been previously used for this task.
Our resultsshow that (a) the knowledge encoded in WordNet is often insufficient, especially for anaphor?antecedent relations that exploit subjective or context-dependent knowledge; (b) for other-anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite NPcoreference, the Web-based method yields results comparable to those obtained using WordNetover the whole data set and outperforms the WordNet-based method on subsets of the dataset; (d) in both case studies, the BNC-based method is worse than the other methods becauseof data sparseness.
Thus, in our studies, the Web-based method alleviated the lexical knowledgegap often encountered in anaphora resolution and handled examples with context-dependentrelations between anaphor and antecedent.
Because it is inexpensive and needs no hand-modelingof lexical knowledge, it is a promising knowledge source to integrate into anaphora resolutionsystems.1.
IntroductionMost work on anaphora resolution has focused on pronominal anaphora, oftenachieving good accuracy.
Kennedy and Boguraev (1996), Mitkov (1998), and Strube,Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and anF-measure of 82.8% for personal pronouns, respectively.
Less attention has been paidto nominal anaphors with full lexical heads, which cover a variety of phenomena, suchas coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparativeanaphora (Examples (3?4)).1?
School of Computing, University of Leeds, Woodhouse Lane, LS2 9JT Leeds, UK.
E-mail:markert@comp.leeds.ac.uk.?
School of Informatics, University of Edinburgh, 2 Buccleuch Place, EH8 9LW Edinburgh, UK.
E-mail:mnissim@inf.ed.ac.uk.1 In all examples presented in this article, the anaphor is typed in boldface and the correct antecedent initalics.
The abbreviation in parentheses at the end of each example specifies the corpus from which theexample is taken: WSJ stands for the Wall Street Journal, Penn Treebank, release 2; BNC stands for BritishNational Corpus (Burnard 1995), and MUC-6 for the combined training/test set for the coreference taskof the Sixth Message Understanding Conference (Hirschman and Chinchor 1997).Submission received: 15 December 2003; revised submission received: 21 November 2004; accepted forpublication: 19 March 2005.?
2005 Association for Computational LinguisticsComputational Linguistics Volume 31, Number 3(1) The death of Maxwell, the British publishing magnate whose empirecollapsed in ruins of fraud, and who was the magazine?s publisher, gave theperiodical a brief international fame.
(BNC)(2) [.
.
. ]
you don?t have to undo the jacket to get to the map?particularlyimportant when it?s blowing a hooley.
There are elasticated adjustabledrawcords on the hem, waist and on the hood.
(BNC)(3) In addition to increasing costs as a result of greater financial exposure formembers, these measures could have other, far-reaching repercussions.
(WSJ)(4) The ordinance, in Moon Township, prohibits locating a group home for thehandicapped within a mile of another such facility.
(WSJ)In Example (1), the definite noun phrase (NP) the periodical corefers with themagazine.2 In Example (2), the definite NP the hood can be felicitously used becausea related entity has already been introduced by the NP the jacket, and a part-ofrelation between the two entities can be established.
Examples (3)?
(4) are instancesof other-anaphora.
Other-anaphora are a subclass of comparative anaphora (Hallidayand Hasan 1976; Webber et al 2003) in which the anaphoric NP is introduced bya lexical modifier (such as other, such, and comparative adjectives) that specifiesthe relationship (such as set-complement, similarity and comparison) between theentities invoked by anaphor and antecedent.
For other-anaphora, the modifiersother or another provide a set-complement to an entity already evoked in thediscourse model.
In Example (3), the NP other, far-reaching repercussions refers toa set of repercussions excluding increasing costs and can be paraphrased as other(far-reaching) repercussions than (increasing) costs.
Similarly, in Example (4), the NPanother such facility refers to a group home which is not identical to the specific (planned)group home mentioned before.A large and diverse amount of lexical or world knowledge is usually necessaryto understand anaphors with full lexical heads.
For the examples above, we needthe knowledge that magazines are periodicals, that hoods are parts of jackets,that costs can be or can be viewed as repercussions of an event, and that institutionalhomes are facilities.
Therefore, many resolution systems that handle these phenomena(Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely onhand-crafted resources of lexico-semantic knowledge, such as the WordNet lexicalhierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that hasgiven strong indications that such resources are insufficient for the entire range offull NP anaphora.
Additionally, we discuss some serious methodological problemsthat arise when fixed ontologies are used that have been encountered by previousresearchers and/or us: the costs of building, maintaining and mining ontolo-gies; domain-specific and context-dependent knowledge; different ways of encodinginformation; and sense ambiguity.2 In this article, we restrict the notion of definite NPs to NPs modified by the article ?the.
?3 These systems also use surface-level features (such as string matching), recency, and grammaticalconstraints.
In this article, we concentrate on the lexical and semantic knowledge employed.368Markert and Nissim Knowledge Sources for Anaphora ResolutionIn Section 3, we discuss an alternative to the manual construction of knowledgebases, which we call the corpus-based approach.
A number of researchers (Hearst1992; Berland and Charniak 1999, among others) have suggested that knowledgebases be enhanced via (semi)automatic knowledge extraction from corpora, and suchenhanced knowledge bases have also been used for anaphora resolution, specificallyfor bridging (Poesio et al 2002; Meyer and Dale 2002).
Building on our previous work(Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in twoways.
First, we suggest using the Web for anaphora resolution instead of the smaller-size, but less noisy and more balanced, corpora used previously, making availablea huge additional source of knowledge.4 Second, we do not induce a fixed lexicalknowledge base from the Web but use shallow lexicosyntactic patterns and their Webfrequencies for anaphora resolution on the fly.
This allows us to circumvent some of theabove-mentioned methodological problems that occur with any fixed ontology, whetherconstructed manually or automatically.The core of this article consists of an empirical comparison of these differentsources of lexical knowledge for the task of antecedent selection or antecedent rankingin anaphora resolution.
We focus on two types of full NP anaphora: other-anaphora(Section 4) and definite NP coreference (Section 5).5 In both case studies, we comparean algorithm that relies mainly on the frequencies of lexico-syntactic patterns in corpora(both the Web and the BNC) with an algorithm that relies mainly on a fixed ontology(WordNet 1.7.1).
We specifically address the following questions:1.
Can the shortcomings of using a fixed ontology that have been stipulatedby previous research on definite NPs be confirmed in our coreferencestudy?
Do they also hold for other-anaphora, a phenomenon less studiedso far?2.
How does corpus-based knowledge acquisition compare to usingmanually constructed lexical hierarchies in antecedent selection?
And isthe use of the Web an improvement over using smaller, but manuallycontrolled, corpora?3.
To what extent is the answer to the previous question dependent on theanaphoric phenomenon addressed?In Section 6 we discuss several aspects of our findings that still need elaborationin future work.
Specifically, our work is purely comparative and regards the differentlexical knowledge sources in isolation.
It remains to be seen how the results carryforward when the knowledge sources interact with other features (for example,grammatical preferences).
A similar issue concerns the integration of the methodsinto anaphoricity determination in addition to antecedent selection.
Additionally,future work should explore the contribution of different knowledge sources for yetother anaphora types.4 There is a growing body of research that uses the Web for NLP.
As we concentrate on anaphora resolutionin this article, we refer the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as theDecember 2003 special issue of Computational Linguistics, for an overview of the use of the Web for otherNLP tasks.5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to theentity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor andantecedent are identical.369Computational Linguistics Volume 31, Number 32.
The Knowledge Gap and Other Problems for Lexico-semantic ResourcesA number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002;Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexicaland world knowledge for the resolution of full NP anaphora and the lack of suchknowledge in existing ontologies (Section 2.1).
In addition to this knowledge gap, wesummarize other, methodological problems with the use of ontologies in anaphoraresolution (Section 2.2).2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical HeadsIn the following, we discuss previous studies on the automatic resolution of coreference,bridging and comparative anaphora, concentrating on work that yields insights into theuse of lexical and semantic knowledge.2.1.1 Coreference.
The prevailing current approaches to coreference resolution areevaluated on MUC-style (Hirschman and Chinchor 1997) annotated text and treatpronominal and full NP anaphora, named-entity coreference, and non-anaphoriccoreferential links that can be stipulated by appositions and copula.
The performance ofthese approaches on definite NPs is often substantially worse than on pronouns and/ornamed entities (Connolly, Burger, and Day 1997; Strube, Rapp, and Mueller 2002; Ngand Cardie 2002b; Yang et al 2003).
For example, for a coreference resolution algorithmon German texts, Strube, Rapp, and Mueller (2002) report an F-measure of 33.9% fordefinite NPs that contrasts with 82.8% for personal pronouns.Several reasons for this performance difference have been established.
First,whereas pronouns are mostly anaphoric in written text, definite NPs do not haveto be so, inducing the problem of whether a definite NP is anaphoric in addition todetermining an antecedent from among a set of potential antecedents (Fraurud 1990;Vieira and Poesio 2000).6 Second, the antecedents of definite NP anaphora can occur atconsiderable distance from the anaphor, whereas antecedents to pronominal anaphoratend to be relatively close (Preiss, Gasperin, and Briscoe 2004; McCoy and Strube 1999).An automatic system can therefore more easily restrict its antecedent set for pronominalanaphora.Third, it is in general believed that pronouns are used to refer to entities in focus,whereas entities that are not in focus are referred to by definite descriptions (Hawkins1978; Ariel 1990; Gundel, Hedberg, and Zacharski 1993), because the head nouns ofanaphoric definite NPs provide the reader with lexico-semantic knowledge.
Antecedentaccessibility is therefore additionally restricted via semantic compatibility and does notneed to rely on notions of focus or salience to the same extent as for pronouns.
Given thislexical richness of common noun anaphors, many resolution algorithms for coreferencehave incorporated manually controlled lexical hierarchies, such as WordNet.
They use,for example, a relatively coarse-grained notion of semantic compatibility between a fewhigh-level concepts in WordNet (Soon, Ng, and Lim 2001), or more detailed hyponymyand synonymy links between anaphor and antecedent head nouns (Vieira and Poesio6 A two-stage process in which the first stage identifies anaphoricity of the NP and the second theantecedent for anaphoric NPs (Uryupina 2003; Ng 2004) can alleviate this problem.
In this article, wefocus on the second stage, namely, antecedent selection.370Markert and Nissim Knowledge Sources for Anaphora Resolution2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b, among others).However, several researchers have pointed out that the incorporated information isstill insufficient.
Harabagiu, Bunescu, and Maiorano (2001) (see also Kameyama 1997)report that evaluation of previous systems has shown that ?more than 30% of themissed coreference links are due to the lack of semantic consistency informationbetween the anaphoric noun and its antecedent noun?
(page 59).
Vieira and Poesio(2000) report results on anaphoric definite NPs in the WSJ that stand in a synonymyor hyponymy relation to their antecedents (as in Example (1)).
Using WordNet linksto retrieve the appropriate knowledge proved insufficient, as only 35.0% of synonymyrelations and 56.0% of hyponymy relations needed were encoded in WordNet as director inherited links.7 The semantic knowledge used might also not necessarily improveon string matching: Soon, Ng, and Lim (2001) final, automatically derived decision treedoes not incorporate their semantic-compatibility feature and instead relies heavily onstring matching and aliasing, thereby leaving open how much information in a lexicalhierarchy can improve over string matching.In this article, we concentrate on this last of the three problems (insufficient lexicalknowledge).
We investigate whether the knowledge gap for definite NP coreferencecan be overcome by using corpora as knowledge sources as well as whether theincorporation of lexical knowledge sources improves over simple head noun matching.2.1.2 Comparative Anaphora.
Modjeska (2002)?one of the few computational studieson comparative anaphora?shows that lexico-semantic knowledge plays a larger rolethan grammatical salience for other-anaphora.
In this article, we show that the semanticknowledge provided via synonymy and hyponymy links in WordNet is insufficientfor the resolution of other-anaphora, although the head of the antecedent is normallya synonym or hyponym of the head of the anaphor in other-anaphora (Section 4.4).82.1.3 Bridging.
Vieira and Poesio (2000) report that 62.0% of meronymy relations (seeExample (2)) needed for bridging resolution in their corpus were not encoded inWordNet.
Gardent, Manuelian, and Kow (2003) identified bridging descriptions in aFrench corpus, of which 187 (52%) exploited meronymic relations.
Almost 80% of thesewere not found in WordNet.
Hahn, Strube, and Markert (1996) report experiments on109 bridging cases from German information technology reports, using a hand-crafted,domain-specific knowledge base of 449 concepts and 334 relations.
They state that 42(38.5%) links between anaphor and antecedents were missing in their knowledge base,a high proportion given the domain-specific task.
In this article, we will not addressbridging, although we will discuss the extension of our work to bridging in Section 6.2.2 Methodological Problems for the Use of Ontologies in Anaphora ResolutionOver the years, several major problems have been identified with the use of ontologiesfor anaphora resolution.
In the following we provide a summary of the different issuesraised, using the examples in the Introduction.7 Whenever we refer to ?hyponymy/meronymy (relations/links)?
in WordNet, we include both direct andinherited links.8 From this point on, we will often use the terms anaphor and antecedent instead of head of anaphor and headof antecedent if the context is non-ambiguous.371Computational Linguistics Volume 31, Number 32.2.1 Problem 1: Knowledge Gap.
As discussed above, even in large ontologies thelack of knowledge can be severe, and this problem increases for non-hyponymy rela-tions.
None of the examples in Section 1 are covered by synonymy, hyponymy, ormeronymy links in WordNet; for example, hoods are not encoded as parts of jackets,and homes are not encoded as a hyponym of facilities.
In addition, building, extending,and maintaining ontologies by hand is expensive.2.2.2 Problem 2: Context-Dependent Relations.
Whereas the knowledge gap mightbe reduced as (semi)automatic efforts to enrich ontologies become available (Hearst1992; Berland and Charniak 1999; Poesio et al 2002), the second problem is intrinsic tofixed context-independent ontologies: How much and which knowledge should theyinclude?
Thus, Hearst (1992) raises the issue of whether underspecified, context- orpoint-of-view-dependent hyponymy relations (like the context-dependent link betweencosts and repercussions in Example (3)) should be included in a fixed ontology, inaddition to universally true hyponymy relations.
Some other hyponymy relations thatwe encountered in our studies whose inclusion in ontologies is debatable are age:(risk)factor, coffee:export, pilots:union, country:member.2.2.3 Problem 3: Information Encoding.
Knowledge might be encoded in manydifferent ways in a lexical hierarchy, and this can pose a problem for anaphora resolution(Humphreys et al 1997; Poesio, Vieira, and Teufel 1997).
For example, althoughmagazine and periodical are not linked in WordNet via synonymy/hyponymy, the glossrecords magazine as a periodic publication.
Thus, the desired link might be derivedthrough the analysis of the gloss together with derivation of periodical from periodic.However, such extensive mining of the ontology (as performed, e.g., by Harabagiu,Bunescu, and Maiorano [2001]) can be costly.
In addition, different information sourcesmust be weighed (e.g., is a hyponymy link preferred over a gloss inclusion?)
andcombined (should hyponyms/hyperonyms/sisters of gloss expressions be consideredrecursively?).
Extensive combinations also increase the risk of false positives.92.2.4 Problem 4: Sense Proliferation.
Using all senses of anaphor and potential an-tecedents in the search for relations might yield a link between an incorrect antecedentcandidate and the anaphor due to an inappropriate sense selection.
On the other hand,considering only the most frequent sense for anaphor and antecedent (as is done inSoon, Ng, and Lim [2001]) might lead to wrong antecedent assignment if a minoritysense is intended in the text.
So, for example, the most frequent sense of hood inWordNet is criminal, whereas the sense used in Example (2) is headdress.
The alterna-tives are either weighing senses according to different domains or a more costly sensedisambiguation procedure before anaphora resolution (Preiss 2002).3.
The Alternative: Corpus-Based Knowledge ExtractionThere have been a considerable number of efforts to extract lexical relations fromcorpora in order to build new knowledge sources and enrich existing ones without time-9 Even without extensive mining, this risk can be high: Vieira and Poesio (2000) report a high number offalse positives for one of their data sets, although they use only WordNet-encoded links.372Markert and Nissim Knowledge Sources for Anaphora Resolutionconsuming hand-modeling.
This includes the extraction of hyponymy and synonymyrelations (Hearst 1992; Caraballo 1999, among others) as well as meronymy (Berlandand Charniak 1999; Meyer 2001).10 One approach to the extraction of instances of aparticular lexical relation is the use of patterns that express lexical relations structurallyexplicitly in a corpus (Hearst 1992; Berland and Charniak 1999; Caraballo 1999; Meyer2001), and this is the approach we focus on here.
As an example, the pattern NP1 andother NP2 usually expresses a hyponymy/similarity relation between the hyponymNP1 and its hypernym NP2 (Hearst 1992), and it can therefore be postulated that twonoun phrases that occur in such a pattern in a corpus should be linked in an ontology viaa hyponymy link.
Applications of the extracted relations to anaphora resolution are lessfrequent.
However, Poesio et al (2002) and Meyer and Dale (2002) have used patternsfor the corpus-based acquisition of meronymy relations: these patterns are subsequentlyexploited for bridging resolution.Although automatic acquisition can help bridge the knowledge gap (see Prob-lem 1 in Section 2.2.1), the incorporation of the acquired knowledge into a fixedontology yields other problems.
Most notably, it has to be decided which knowl-edge should be included in ontologies, because pattern-based acquisition willalso find spurious, subjective and context-dependent knowledge (see Problem 2 inSection 2.2.2).
There is also the problem of pattern ambiguity, since patterns donot necessarily have a one-to-one correspondence to lexical relations (Meyer 2001).Following our work in Markert, Nissim, and Modjeska (2003), we argue that for thetask of antecedent ranking, these problems can be circumvented by not constructinga fixed ontology at all.
Instead, we use the pattern-based approach to find lexicalrelationships holding between anaphor and antecedent in corpora on the fly.
Forinstance, in Example (3), we do not need to know whether costs are always repercus-sions (and should therefore be linked via hyponymy in an ontology) but only thatthey are more likely to be viewed as repercussions than the other antecedent candidates.We therefore adapt the pattern-based approach in the following way for antecedentselection.Step 1: Relation Identification.
We determine which lexical relation usu-ally holds between anaphor and antecedent head nouns for a partic-ular anaphoric phenomenon.
For example, in other-anaphora, a hyponymy/similarity relation between anaphor and antecedent is exploited (homes arefacilities) or stipulated by the context (costs are viewed as repercussions).Step 2: Pattern Selection.
We select patterns that express this lexical relationstructurally explicitly.
For example, the pattern NP1 and other NP2 usuallyexpresses hyponymy/similarity relations between the hyponym NP1 and itshypernym NP2 (see above).Step 3: Pattern Instantiation.
If the lexical relation between anaphor andantecedent head nouns is strong, then it is likely that the anaphor andantecedent also frequently co-occur in the selected explicit patterns.
Weextract all potential antecedents for each anaphor and instantiate the explicit10 There is also a long history in the extraction of other lexical knowledge, which is also potentially usefulfor anaphora resolution, for example, of selectional restrictions/preferences.
In this article we focus onthe lexical relations that can hold between antecedent and anaphor head nouns.373Computational Linguistics Volume 31, Number 3for all anaphor/antecedent pairs.
In Example (4) the pattern NP1 andother NP2 can be instantiated with ordinances and other facilities, MoonTownship and other facilities, homes and other facilities, handicappedand other facilities, and miles and other facilities.11Step 4: Antecedent Assignment.
The instantiation of a pattern can be searchedin any corpus to determine its frequency.
We follow the rationale that themost frequent of these instantiated patterns determines the most likelyantecedent.
Therefore, should the head noun of an antecedent candidateand the anaphor co-occur in a pattern although they do not stand in thelexical relationship considered (because of pattern ambiguity, noise in thecorpus, or spurious occurrences), this need not prove a problem as longas the correct antecedent candidate co-occurs more frequently with theanaphor.As the patterns can be elaborate, most manually controlled and linguistically processedcorpora are too small to determine the pattern frequencies reliably.
Therefore, the sizeof the corpora used in some previous approaches leads to data sparseness (Berland andCharniak 1999), and the extraction procedure can therefore require extensive smoothing.Thus as a further extension, we suggest using the largest corpus available, the Web,in the above procedure.
The instantiation for the correct antecedent homes and otherfacilities in Example (4), for instance, does not occur at all in the BNC but yields over1,500 hits on the Web.12 The competing instantiations (listed in Step 3) yield 0 hits in theBNC and fewer than 20 hits on the Web.In the remainder of this article, we present two comparative case studies oncoreference and other-anaphora that evaluate the ontology- and corpus-based ap-proaches in general and our extensions in particular.4.
Case Study I: Other-AnaphoraWe now describe our first case study for antecedent selection in other-anaphora.4.1 Corpus Description and AnnotationWe use Modjeska?s (2003) annotated corpus of other-anaphors from the WSJ.
Allexamples in this section are from this corpus.
Modjeska restricts the notion of other-anaphora to anaphoric NPs with full lexical heads modified by other or another(Examples (3)?
(4)), thereby excluding idiomatic non-referential uses (e.g., on the otherhand), reciprocals such as each other, ellipsis, and one-anaphora.
The excluded caseseither are non-anaphoric or do not have a full lexical head and would therefore re-quire a mostly non-lexical approach to resolution.
Modjeska?s corpus also excludes11 These simplified instantiations serve as an example; for final instantiations, see Section 4.5.1.12 This search and all searches for the Web experiments in Section 4 were executed on August 29, 2003.
AllWeb searches for Section 5 were executed August 27, 2004.374Markert and Nissim Knowledge Sources for Anaphora Resolutionother-anaphors with structurally available antecedents: In list contexts such asExample (5), the antecedent is normally given as the left conjunct of the list:(5) [.
.
.]
AZT can relieve dementia and other symptoms in children [.
.
.
]A similar case is the construction Xs other than Ys.
For a computational treatment ofother-NPs with structural antecedents, see Bierner (2001).The original corpus collected and annotated by Modjeska (2003) contains 500instances of other-anaphors with NP antecedents in a five-sentence window.
In thisstudy we use the 408 (81.6%) other-anaphors in the corpus that have NP antecedentswithin a two-sentence window (the current or previous sentence).13 An antecedentcandidate is manually annotated as correct if it is the latest mention of the entity towhich the anaphor provides the set complement.
The tag lenient was used to annotateprevious mentions of the same entity.
In Example (6), all other bidders refers to all biddersexcluding United Illuminating Co., whose latest mention is it.
In this article, lenientantecedents are underlined.
All other potential antecedents (e.g., offer in Example (6)),are called distractors.
(6) United Illuminating Co. raised its proposed offer to one it valued at $2.29billion from $2.19 billion, apparently topping all other bidders.The antecedent can be a set of separately mentioned entities, like May and July inExample (7).
For such split antecedents (Modjeska 2003), the latest mention of each setmember is annotated as correct, so that there can be more than one correct antecedent toan anaphor.14(7) The May contract, which also is without restraints, ended with a gain of0.45 cent to 14.26 cents.
The July delivery rose its daily permissible limit of0.50 cent a pound to 14.00 cent, while other contract months showednear-limit advances.4.2 Antecedent Extraction and PreprocessingFor each anaphor, all previously occurring NPs in the two-sentence window wereautomatically extracted exploiting the WSJ parse trees.
NPs containing a possessive NPmodifier (e.g., Spain?s economy) were split into a possessor phrase (Spain) and a possessedentity (Spain?s economy).15 Modjeska (2003) identifies several syntactic positions thatcannot serve as antecedents of other-anaphors.
We automatically exclude only NPspreceding an appositive other-anaphor from the candidate antecedent set.
In ?MaryElizabeth Ariail, another social-studies teacher,?
the NP Mary Elizabeth Ariail cannot13 We concentrate on this majority of cases to focus on the comparison of different sources of lexicalknowledge without involving discourse segmentation or focus tracking.
In Section 5 we expand thewindow size to allow equally high coverage for definite NP coreference.14 The occurrence of split antecedents also motivated the distinction between correct and lenientantecedents in the annotation.
Anaphors with split antecedents have several antecedent candidatesannotated as correct.
All other anaphors have only one antecedent candidate annotated as correct, withprevious mentions of the same entity marked as lenient.15 We thank Natalia Modjeska for the extraction and for making the resulting sets of candidate antecedentsavailable to us.375Computational Linguistics Volume 31, Number 3be the antecedent of another social-studies teacher as the two phrases are coreferential andcannot provide a set complement to each other.The resulting set of potential NP antecedents for an anaphor ana (with a uniqueidentifier anaid) is called Aanaid.16 The final number of extracted antecedents for thewhole data set is 4,272, with an average of 10.5 antecedent candidates per anaphor.After extraction, all modification was eliminated, and only the rightmost noun ofcompounds was retained, as modification results in data sparseness for the corpus-based methods, and compounds are often not recorded in WordNet.For the same reasons we automatically resolved named entities (NEs).
Theywere classified into the ENAMEX MUC-7 categories (Chinchor 1997) PERSON,ORGANIZATION and LOCATION, using the software ANNIE (GATE2; http://gate.ac.uk).
We then automatically obtained more-fine-grained distinctions for the NE cate-gories LOCATION and ORGANIZATION, whenever possible.
We classified LOCATIONSinto COUNTRY, (US) STATE, CITY, RIVER, LAKE, and OCEAN in the following way.
First,small gazetteers for these subcategories were extracted from the Web.
Second, if anentity marked as LOCATION by ANNIE occurred in exactly one of these gazetteers(e.g., Texas in the (US) STATE gazetteer) it received the corresponding specific label;if it occurred in none or in several of the gazetteers (e.g., Mississippi occurred inboth the state and the river gazetteer), then the label was left at the LOCATION level.We further classified an ORGANIZATION entity by using its internal makeup as follows.We extracted all single-word hyponyms of the noun organization from WordNet andused the members of this set, OrgSet, as the target categories for the fine-graineddistinctions.
If an entity was classified by ANNIE as ORGANIZATION and it hadan element <ORG> of OrgSet as its final lemmatized word (e.g., Deutsche Bank) orcontained the pattern <ORG> of (for example, Bank of America), it was subclassifiedas <ORG> (here, BANK).
In cases of ambiguity, again, no subclassification was carriedout.
No further distinctions were developed for the category PERSON.
We used regularexpression matching to classify numeric and time entities into DAY, MONTH, and YEARas well as DOLLAR or simply NUMBER.
This subclassification of the standard cate-gories provides us with additional lexical information for antecedent selection.
Thus, inExample (8), for instance, a finer-grained classification of South Carolina into STATEprovides more useful information than resolving both South Carolina and GreenvilleCounty as LOCATION only:(8) Use of Scoring High is widespread in South Carolina and common inGreenville County.
.
.
.
Experts say there isn?t another state in the countrywhere .
.
.Finally, all antecedent candidates and anaphors were lemmatized.
The procedure ofextraction and preprocessing results in the following antecedent sets and anaphorsfor Examples (3) and (4): A3 = {..., addition, cost, result, exposure, member, measure} andana = repercussion and A4 = {..., ordinance, Moon Township [= location], home, handicapped,mile} and ana = facility.Table 1 shows the distribution of antecedent NP types in the other-anaphora dataset.17 NE resolution is clearly important as 205 of 468 (43.8%) of correct antecedentsare NEs.16 In this article the anaphor ID corresponds to the example numbers.17 Note that there are more correct antecedents than anaphors because the data include split antecedents.376Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 1Distribution of antecedent NP types in the other-anaphora data set.Correct Lenient Distractors AllPronouns 49 19 329 397Named entities 205 56 806 1,067Common nouns 214 104 2,490 2,808Total 468 179 3,625 4,2724.3 Evaluation Measures and BaselinesFor each anaphor, each algorithm selects at most one antecedent as the correct one.
Ifthis antecedent provides the appropriate set complement to the anaphor (i.e., is markedin the gold standard as correct or lenient), the assignment is evaluated as correct.18Otherwise, it is evaluated as wrong.
We use the following evaluation measures: Precisionis the number of correct assignments divided by the number of assignments, recall is thenumber of correct assignments divided by the number of anaphors, and F-measure isbased on equal weighting of precision and recall.
In addition, we also give the coverageof each algorithm as the number of assignments divided by the number of anaphors.This last measure is included to indicate how often the algorithm has any knowledge togo on, whether correct or false.
For algorithms in which the coverage is 100%, precision,recall, and F-measure all coincide.We developed two simple rule-based baseline algorithms.
The first, a recency-basedbaseline (baselineREC), always selects the antecedent candidate closest to the anaphor.The second (baselineSTR) takes into account that the lemmatized head of an other-anaphor is sometimes the same as that of its antecedent, as in the pilot?s claim .
.
.
otherbankruptcy claims.
For each anaphor, baselineSTR string-compares its last (lemmatized)word with the last (lemmatized) word of each of its potential antecedents.
If the stringsmatch, the corresponding antecedent is chosen as the correct one.
If several antecedentsproduce a match, the baseline chooses the most recent one among them.
If no antecedentproduces a match, no antecedent is assigned.We tested two variations of this baseline.19 The algorithm baselineSTRv1 usesonly the original antecedents for string matching, disregarding named-entity res-olution.
If string-comparison returns no match, a back-off version (baselineSTR?v1)chooses the antecedent closest to the anaphor among all antecedent candidates, therebyyielding a 100% coverage.
The second variation (baselineSTRv2) uses the replacementsfor named entities for string matching; again a back-off version (baselineSTR?v2) usesa recency back-off.
This baseline performs slightly better, as now cases such as that inExample (8) (South Carolina .
.
.
another state, in which South Carolina is resolved to STATE)can also be resolved.
The results of all baselines are summarized in Table 2.
Results ofthe 100% coverage backoff algorithms are indicated by Precision?
in all tables.
The setsof anaphors covered by the string-matching baselines baselineSTRv1 and baselineSTRv218 This does not hold for anaphors with split antecedents, for which all antecedents marked as correct needto be found in order to provide the complete set complement.
Therefore, all our algorithms?
assignmentsin these cases are evaluated as wrong, as they select at most one antecedent.19 Different versions of the same prototype algorithm are indicated via an index of v1, v2, .
.
.
.
The generalprototype algorithm is referred to without indices.377Computational Linguistics Volume 31, Number 3Table 2Overview of the results for all baselines for other-anaphora.Algorithm Coverage Precision Recall F-measure Precision?baselineREC 1.000 0.178 0.178 0.178 0.178baselineSTRv1 0.282 0.686 0.194 0.304 0.333baselineSTRv2 0.309 0.698 0.216 0.329 0.350will be called StrSetv1 and StrSetv2, respectively.
These sets do not include the casesassigned by the recency back-off in baselineSTR?v1 and baselineSTR?v2.For our WordNet and corpus-based algorithms we additionally deleted pronounsfrom the antecedent sets, since they are lexically not very informative and are alsonot encoded in WordNet.
This removes 49 (10.5%) of the 468 correct antecedents(see Table 1); however, we can still resolve some of the anaphors with pronounantecedents if they also have a lenient non-pronominal antecedent, as in Example (6).After pronoun deletion, the total number of antecedents in our data set is 3,875for 408 anaphors, of which 419 are correct antecedents, 160 are lenient, and 3,296are distractors.4.4 Wordnet as a Knowledge Source for Other-Anaphora Resolution4.4.1 Descriptive Statistics.
As most antecedents are hyponyms or synonyms of theiranaphors in other-anaphora, for each anaphor ana, we look up which elements of itsantecedent set Aanaid are hyponyms/synonyms of ana in WordNet, considering allsenses of anaphor and candidate antecedent.
In Example (4), for example, we lookup whether ordinance, Moon Township, home, handicapped, and mile are hyponyms orsynonyms of facility in WordNet.
Similarly, in Example (9), we look up whether WillQuinlan [= PERSON], gene, and risk are hyponyms/synonyms of child.
(9) Will Quinlan had not inherited a damaged retinoblastoma supressor geneand, therefore, faced no more risk than other children .
.
.As proper nouns (e.g., Will Quinlan) are often not included in WordNet, we alsolook up whether the NE category of an NE antecedent is a hyponym/synonym ofthe anaphor (e.g., whether person is a synonym/hyponym of child) and vice versa(e.g., whether child is a synonym/hyponym of person).
This last inverted look-upis necessary, as the NE category of the antecedent is often too general to preservethe normal hyponymy relationship to the anaphor.
Indeed, in Example (9), it is theinverted look-up that captures the correct hyponymy relation between person andchild.
If the single look-up for common nouns or any of the three look-ups forproper nouns is successful, we say that a hyp/syn relation between candidateantecedent and anaphor holds in WordNet.
Note that each noun in WordNetstands in a hyp/syn relation to itself.
Table 3 summarizes how many correct/lenient antecedents and distractors stand in a hyp/syn relation to their anaphor inWordNet.Correct/lenient antecedents stand in a hyp/syn relation to their anaphor sig-nificantly more often than distractors do (p < 0.001, t-test).
The use of WordNethyponymy/synonymy relations to distinguish between correct/lenient antecedentsand distractors is therefore plausible.
However, Table 3 also shows two limitations378Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 3Descriptive statistics for WordNet hyp/syn relations for other-anaphora.Hyp/syn relation No hyp/syn relation Totalto anaphor to anaphorCorrect antecedents 180 (43.0%) 239 (57.0%) 419 (100%)Lenient antecedents 68 (42.5%) 92 (57.5%) 160 (100%)Distractors 296 (9.0%) 3,000 (91.0%) 3,296 (100%)All antecedents 544 (14.0%) 3,331 (86.0%) 3,875 (100%)of relying on WordNet in resolution algorithms.
First, 57% of correct and lenientantecedents are not linked via a hyp/syn relation to their anaphor in WordNet.This will affect coverage and recall (see also Section 2.2.1).
Examples from our dataset that are not covered are home:facility, cost:repercussion, age:(risk) factor, pension:benefit,coffee:export, and pilot(s):union, including both missing universal hyponymy links andcontext-stipulated ones.
Second, the raw frequency (296) of distractors that standin a hyp/syn relation to their anaphor is higher than the combined raw frequencyfor correct/lenient antecedents (248) that do so, which can affect precision.
This isdue to both sense proliferation (Section 2.2.4) and anaphors that require more than justlexical knowledge about antecedent and anaphor heads to select a correct antecedentover a distractor.
In Example (10), the distractor product stands in a hyp/syn relation-ship to the anaphor commodity and?disregarding other factors?is a good antecedentcandidate.20(10) .
.
.
the move is designed to more accurately reflect the value of productsand to put steel on a more equal footing with other commodities.4.4.2 The WordNet-Based Algorithm.
The WordNet-based algorithm resolves eachanaphor ana to a hyponym or synonym in Aanaid, if possible.
If several antecedentcandidates are hyponyms or synonyms of ana, it uses a tiebreaker based on stringmatch and recency.
When no candidate antecedent is a hyponym or synonym ofana, string match and recency can be used as a possible back-off.21 String compari-son for tiebreaker and back-off can again use the original or the replaced anteced-ents, yielding two versions, algoWNv1 (original antecedents) and algoWNv2 (replacedantecedents).The exact procedure for the version algoWNv1 given an anaphor ana is as follows:22(i) for each antecedent a in Aanaid, look up whether a hyp/syn relationbetween a and ana holds in WordNet; if this is the case, push a into a setAhyp/synanaid ;20 This problem is not WordNet-specific but affects all algorithms that rely on lexical knowledge only.21 Because each noun is a synonym of itself, anaphors in StrSetv1/StrSetv2 that do have a string-matchingantecedent candidate will already be covered by the WordNet look-up prior to back-off in almost allcases: Back-off string matching will take effect only if the anaphor/antecedent head noun is not inWordNet at all.
Therefore, the described back-off will most of the time just amount to a recency back-off.22 The algorithm algoWNv2 follows the same procedure apart from the variation in string matching.379Computational Linguistics Volume 31, Number 3(ii) if Ahyp/synanaid contains exactly one element, choose this element and stop;(iii) otherwise, if Ahyp/synanaid contains more than one element, string-compareeach antecedent in Ahyp/synanaid with ana (using original antecedents only).If exactly one element of Ahyp/synanaid matches ana, select this one and stop;if several match ana, select the closest to ana within these matchingantecedents and stop; if none match, select the closest to ana withinAhyp/synanaid and stop;(iv) otherwise, if Ahyp/synanaid is empty, make no assignment and stop.The back-off algorithm algoWN?v1 uses baselineSTR?v1 as a back-off (iv?)
if no antecedentcan be assigned:(iv?)
otherwise, if Ahyp/synanaid is empty, use baselineSTR?v1 to assign anantecedent to ana and stop;Both algoWNv1 and algoWNv2 achieved the same results, namely, a coverage of65.2%, precision of 56.8%, and recall of 37.0%, yielding an F-measure of 44.8%.The low coverage and recall confirm our predictions in Section 4.4.1.
Using backoffalgoWN?v1/algoWN?v2 achieves a coverage of 100% and a precision/recall/F-measureof 44.4%.4.5 Corpora as Knowledge Sources for Other-Anaphora ResolutionIn Section 3 we suggested the use of shallow lexico-semantic patterns for obtaininganaphor?antecedent relations from corpora.
In our first experiment we use the Web,which with its approximately 8,058M pages23 is the largest corpus available to theNLP community.
In our second experiment we use the same technique on the BNC,a smaller (100 million words) but virtually noise-free and balanced corpus of contem-porary English.4.5.1 Pattern Selection and Instantiation.
The list-context Xs and other Ys explicitlyexpresses a hyponymy/synonymy relationship with X being hyponyms/synonymsof Y (see also Example (5) and [Hearst 1992]).
This is only one of the possiblestructures that express hyponymy/synonymy.
Others involve such, including, andespecially (Hearst 1992) or appositions and coordination.
We derive our patterns from thelist-context because it corresponds relatively unambigously to hyponymy/synonymyrelations (in contrast to coordination, which often links sister concepts instead of ahyponym and its hyperonym, as in tigers and lions, or even completely unrelatedconcepts).
In addition, it is quite frequent (for example, and other occurs morefrequently on the Web than such as and other than).
Future work has to explorewhich patterns have the highest precision and/or recall and how different patternscan be combined effectively without increasing the risk of false positives (see alsoSection 2.2.3).23 Google (http://www.google.com), estimate from November 2004.380Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 4Patterns and instantiations for other-anaphora.Common-noun patterns Common-noun instantiationsW1: (N1{sg} OR N1{pl}) and other N2{pl} WIc1: (home OR homes) and other facilities.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.B1: (...) and D* other A* N2{pl} BIc1: (home OR homes) and D* other A* facilitiesProper-noun patterns Proper-noun instantiationsW1: (N1{sg} OR N1{pl}) and other N2{pl} WIp1: (person OR persons) and other childrenWIp2: (child OR children) and other personsW2: N1 and other N2{pl} WIp3: Will Quinlan and other children.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.B1: (...) and D* other A* N2{pl} BIp1: (person OR persons) and D* other A* childrenBIp2: (child OR children) and D* other A* personsB2: N1 and D* other A* N2{pl} BIp3: Will Quinlan and D* other A* childrenWeb.
For the Web algorithm (algoWeb), we use the following pattern:24(W1) (N1{sg} OR N1{pl}) and other N2{pl}Given an anaphor ana and a common-noun antecedent candidate a in Aanaid, weinstantiate (W1) by substituting a for N1 and ana for N2.
An instantiated pattern forExample (4) is (home OR homes) and other facilities (WIc1 in Table 4).25 This patterninstantiation is parallel to the WordNet hyp/syn relation look-up for common nouns.For NE antecedents we instantiate (W1) by substituting the NE category of theantecedent for N1, and ana for N2.
An instantiated pattern for Example (9) is (personOR persons) and other children (WIp1 in Table 4).
In this instantiation, N1 (person) is nota hyponym of N2 (child); instead N2 is a hyponym of N1 (see the discussion on invertedqueries in Section 4.4.1).
Therefore, we also instantiate (W1) by substituting ana for N1and the NE type of the antecedent for N2 (WIp2 in Table 4).
Finally, for NE antecedents,we use an additional pattern:(W2) N1 and other N2{pl}which we instantiate by substituting the original NE antecedent for N1 andana for N2 (WIp3 in Table 4).
The three instantiations for NEs are parallel to the threehyp/syn relation look-ups in the WordNet experiment in Section 4.4.1.
We submitthese instantiations as queries to the Google search engine, making use of the GoogleAPI technology.BNC.
For BNC patterns and instantiations, we exploit the BNC?s part-of-speech tagging.On the one hand, we restrict the instantiation of N1 and N2 to nouns to avoid noise,and on the other hand, we allow occurrence of modification to improve coverage.
We24 In all patterns and instantiations in this article, OR is the boolean operator, N1 and N2 are variables, andand and other are constants.25 All common-noun instantiations are marked by a superscript c and all proper-noun instantiations by asuperscript p.381Computational Linguistics Volume 31, Number 3therefore extend (W1) and (W2) to the patterns (B1) and (B2).26 An instantiation for(B1), for example, also matches ?homes and the other four facilities.?
Otherwise theinstantiations are produced parallel to the Web (see Table 4).
We search the instantiationsin the BNC using the IMS Corpus Query Workbench (Christ 1995).
(B1) (N1{sg} OR N1{pl}) and D* other A* N2{pl}(B2) N1 and D* other A* N2{pl}For both algoWeb and algoBNC, each antecedent candidate a in Aanaid is assigned ascore.
The procedure, using the notation for the Web, is as follows.
We obtain the rawfrequencies of all instantiations in which a occurs (WIc1 for common nouns, or WIp1, WIp2,and WIp3 for proper names) from the Web, yielding freq(WIc1), or freq(WIp1 ), freq(WIp2 ),and freq(WIp3 ).
The maximum WMa over these frequencies is the score associated witheach antecedent (given an anaphor ana), which we will also simply refer to as theantecedent?s Web score.
For the BNC, we call the corresponding maximum score BMaand refer to it as the antecedent?s BNC score.
This simple maximum score is biasedtoward antecedent candidates whose head nouns occur more frequently overall.
In aprevious experiment we used mutual information to normalize Web scores (Markert,Nissim, and Modjeska 2003).
However, the results achieved with normalized andnon-normalized scores showed no significant difference.
Other normalization methodsmight yield significant improvements over simple maximum scoring and can beexplored in future work.4.5.2 Descriptive Statistics.
Table 5 gives descriptive statistics for the Web and BNCscore distributions for correct/lenient antecedents and distractors, including the mini-mum and maximum score, mean score and standard deviation, median, and number ofzero scores, scores of one, and scores greater than one.Web scores resulting from simple pattern-based search produce on average signif-icantly higher scores for correct/lenient antecedents (mean: 2,416.68/807.63; median:68/68.5) than for distractors (mean: 290.97; median: 1).
Moreover, the method producessignificantly fewer zero scores for correct/lenient antecedents (19.6%/22.5%) than fordistractors (42.3%).27 Therefore the pattern-based Web method is a good candidate fordistinguishing correct/lenient antecedents and distractors in anaphora resolution.
Inaddition, the median for correct/lenient antecedents is relatively high (68/68.5), whichensures a relatively large amount of data upon which to base decisions.
Only 19.6%of correct antecedents have scores of zero, which indicates that the method mighthave high coverage (compared to the missing 57% of hyp/syn relations for correctantecedents in WordNet; Section 4.4).Although the means of the BNC score distributions of correct/lenient antecedentsare significantly higher than that of the distractors, this is due to a few outliers; moreinterestingly, the median for the BNC score distributions is zero for all antecedentgroups.
This will affect precision for a BNC-based algorithm because of the smallamount of data decisions are based on.
In addition, although the number of zero scores26 The star operator indicates zero or more occurrences of a variable.
The variable D can be instantiated byany determiner; the variable A can be instantiated by any adjective or cardinal number.27 Difference in means was calculated via a t-test; for medians we used chi-square, and for zero counts at-test for proportions.
The significance level used was 5%.382Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 5Descriptive statistics for Web scores and BNC scores for other-anaphora.Min?Max Mean SD Med 0 scores 1 scores scores > 1All possible antecedents (Total: 3,875)BNC 0?22 0.07 0.60 0 3,714 (95.8%) 109 (2.8%) 52 (1.4%)Web 0?283,000 542.15 8,352.46 2 1,513 (39.0%) 270 (7.0%) 2,092 (54.0%)Correct antecedents (Total: 419)BNC 0?22 0.32 1.62 0 360 (85.9%) 39 (9.3%) 20 (4.8%)Web 0?283,000 2,416.68 15,947.93 68 82 (19.6%) 11 (2.6%) 326 (77.8%)Lenient antecedents (Total: 160)BNC 0?4 0.21 0.62 0 139 (86.9%) 13 (8.1%) 8 (5.0%)Web 0?8,840 807.63 1,718.13 68.5 36 (22.5%) 3 (1.9%) 121 (75.6%)Distractors (Total: 3,296)BNC 0?6 0.03 0.25 0 3,215 (97.5%) 57 (1.7%) 24 (0.8%)Web 0?283,000 290.97 7,010.07 1 1,395 (42.3%) 256 (7.8%) 1,645 (49.9%)for correct/lenient antecedents (85.9%/86.9%) is significantly lower than for distractors(97.5%), the number of zero scores is well above 80% for all antecedent groups.
Thus,the coverage and recall of a BNC-based algorithm will be very low.
Although theBNC scores are in general much lower than Web scores and although the Web scoresdistinguish better between correct/lenient antecedents and distractors, we observe thatWeb and BNC scores still correlate significantly, with correlation coefficients between0.20 and 0.35, depending on antecedent group.28To summarize, the pattern-based method yields correlated results on differentcorpora, but it is expected to depend on large corpora to be really successful.4.5.3 The Corpus-Based Algorithms.
The prototype Web-based algorithm resolves eachanaphor ana to the antecedent candidate in Aanaid with the highest Web score above zero.If several potential antecedents achieve the same Web score, it uses a tiebreaker based onstring match and recency.
If no antecedent candidate achieves a Web score above zero,string match and recency can be used as a back-off.
String comparison for tiebreaker andback-off can again use the original or the replaced antecedents, yielding two versions,algoWebv1 (original antecedents) and algoWebv2 (replaced antecedents).The exact procedure for the version algoWebv1 for an anaphor ana is as follows:29(i) for each antecedent a in Aanaid, compute its Web score WMa.
Computethe maximum WM of all Web scores over all antecedents in Aanaid.
If WMais equal to WM and bigger than zero, push a into a set AWManaid;28 Correlation significance was measured by both a t-test for the correlation coefficient and also by thenonparametric paired Kendall rank correlation test, both yielding significance at the 1% level.29 The algorithm algoWebv2 follows the same basic procedure apart from the variation regardingoriginal/replaced antecedents in string matching.383Computational Linguistics Volume 31, Number 3(ii) if AWManaid contains exactly one element, select this element and stop;(iii) otherwise, if AWManaid contains more than one element, string-compareeach antecedent in AWManaid with ana (using original antecedents).
If exactlyone element of AWManaid matches ana, select this one and stop; if several matchana, select the closest to ana within these matching antecedents and stop; ifnone match, select the closest to ana within AWManaid and stop;(iv) otherwise, if AWManaid is empty, make no assigment and stop.The back-off algorithm algoWeb?v1 uses baselineSTR?v1 as a back-off (iv?)
if no antecedentcan be assigned (parallel to the back-off in algoWN?v1):(iv?)
otherwise, if AWManaid is empty, use baselineSTR?v1 to assign an antecedentto ana and stop;algoWebv1 and algoWebv2 can overrule string matching for anaphors in StrSetv1/StrSetv2.This happens when the Web score of an antecedent candidate that does not matchthe anaphor is higher than the Web scores of matching antecedent candidates.
Inparticular, there is no guarantee that matching antecedent candidates are includedin AWManaid.
In that respect, algoWebv1 and algoWebv2 differ from the correspondingWordNet alorithms: Matching antecedent candidates are always synonyms of theanaphor (as each noun is a synonym of itself) and therefore always included in Ahyp/synanaid .Therefore the WordNet alorithms can be seen as a direct extension of baselineSTR;that is, they achieve the same results as the string-matching baseline on the setsStrSetv1/StrSetv2.Given the high precision of baselineSTR, we might want to exclude the possibilitythat the Web algorithms overrule string matching.
Instead we can use string matchingprior to Web scoring, use the Web scores only when there are no matching antecedentcandidates, and use recency as the final back-off.
This variation then achieves the sameresults on the sets StrSetv1/StrSetv2 as the WordNet alorithms and the string-matchingbaselines.
In combination with the possibility of using original or replaced antecedentsfor string matching this yields four algorithm variations overall (see Table 6).
Theresults (see Table 7) do not show any significant differences according to the variationexplored.The BNC-based algorithms follow the same procedures as the Web-based algo-rithms, using the BNC scores instead of Web scores.
The results (see Table 8) aredisappointing because of data sparseness (see above).
No variation yields considerableimprovement over baselineSTRv2 in the final precision?
; in fact, in most cases the varia-Table 6Properties of the variations for the corpus-based algorithms for other-anaphora.Replaced/original antecedent Overrule string matching?v1 original yesv2 replaced yesv3 original nov4 replaced no384Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 7Web results for other-anaphora.Algorithm Coverage Precision Recall F-measure Precision?algoWebv1 0.950 0.520 0.495 0.507 0.512algoWebv2 0.950 0.518 0.493 0.505 0.509algoWebv3 0.958 0.534 0.512 0.523 0.519algoWebv4 0.961 0.538 0.517 0.527 0.524Table 8BNC results for other-anaphora.Algorithm Coverage Precision Recall F-measure Precision?algoBNCv1 0.210 0.488 0.103 0.170 0.355algoBNCv2 0.210 0.488 0.103 0.170 0.360algoBNCv3 0.417 0.618 0.257 0.363 0.370algoBNCv4 0.419 0.626 0.262 0.369 0.375tions just apply a string-matching baseline, either as a back-off or prior to checking BNCscores, depending on the variation used.4.6 Discussion and Error AnalysisThe performances of the best versions of all algorithms for other-anaphora are summa-rized in Table 9.4.6.1 Algorithm Comparison.
Algorithms are compared on their final precision?
usingtwo tests throughout this article.
We used a t-test to measure the difference between twoalgorithms in the proportion of correctly resolved anaphors.
However, there are manyexamples which are easy (for example, string-matching examples) and that thereforemost or all algorithms will resolve correctly, as well as many that are too hard for allalgorithms.
Therefore, we also compare two algorithms using McNemar?s test, whichonly relies on the part of the data set in which the algorithms do not give the sameanswer.30 If not otherwise stated, all significance claims hold at the 5% level for both thet-test and McNemar?s test.The algorithm baselineSTR significantly outperforms baselineREC in precision?,showing that the ?same predicate match?
is quite accurate even though not veryfrequent (coverage is only 30.9%).
The WordNet-based and Web-based algorithmsachieve a final precision that is significantly better than the baselines?
as well asalgoBNC?s.
Most interestingly, the Web-based algorithms significantly outperform theWordNet-based algorithms, confirming our predictions based on the descriptive statis-tics.
The Web approach, for example, resolves Examples (3), (4), (6), and (11) (whichWordNet could not resolve) in addition to Examples (8) and (9), which both the Weband WordNet alorithms could resolve.30 We thank an anonymous reviewer for suggesting the use of McNemar?s test for this article.385Computational Linguistics Volume 31, Number 3Table 9Overview of the results for the best algorithms for other-anaphora.Algorithm Coverage Precision Recall F-measure Precision?baselineREC 1.000 0.178 0.178 0.178 0.178baselineSTRv2 0.309 0.698 0.216 0.329 0.350algoBNCv4 0.419 0.626 0.262 0.369 0.375algoWNv2 0.652 0.568 0.370 0.448 0.444algoWebv4 0.961 0.538 0.517 0.527 0.524As expected, the WordNet-based algorithms suffer from the problems discussedin Section 2.2.
In particular, Problem 1 proved to be quite severe, as algoWN achieveda coverage of only 65.2%.
Missing links in WordNet alo affect precision if a gooddistractor has a link to the anaphor in WordNet, whereas the correct antecedent doesnot (Example (10)).
Missing links are both universal relations that should be includedin an ontology (such as home:facility) and context-dependent links (e.g., age:(risk) factor,costs:repercussions; see Problem 2 in Section 2.2.2).
Further mining of WordNet beyondfollowing hyponymy/synonymy links might alleviate Problem 1 but is more costlyand might lead to false positives (Problem 3).
To a lesser degree, the WordNet alo-rithms also suffer from sense proliferation (Problem 4), as all senses of both anaphorand antecedent candidates were considered.
Therefore, some hyp/syn relations basedon a sense not intended in the text were found, leading to wrong-antecedent selectionand lowering precision.
In Example (11), for instance, there is no hyponymy link be-tween the head noun of the correct antecedent (question) and the head noun of theanaphor (issue), whereas there is a hyponymy link between issue and person = [Mr.Dallara] (using the sense of issue as offspring) as well as a synonymy link between numberand issue.
While in this case considering the most frequent sense of the anaphor issue asindicated in WordNet would help, this would backfire in other cases in our data set inwhich issue is mostly used in the minority sense of stock, share.
Obviously, prior wordsense disambiguation would be the most principled but also a more costly solution.
(11) While Mr. Dallara and Japanese officials say the question of investorsaccess to the U.S. and Japanese markets may get a disproportionateshare of the public?s attention, a number of other important economicissues [.
.
.
]The Web-based method does not suffer as much from these problems.
The linguisticallymotivated patterns we use reduce long-distance dependencies between anaphor andantecedent to local dependencies.
By looking up these patterns on the Web we makeuse of a large amount of data that is very likely to encode strong semantic linksvia these local dependencies and to do so frequently.
This holds both for universalhyponymy relations (addressing Problem 1) and relations that are not necessarily tobe included in an ontology (addressing Problem 2).
The problem of whether to includesubjective and context-dependent relations in an ontology (Problem 2) is circumventedby using Web scores only in comparison to Web scores of other antecedent candidates.In addition, the Web-based algorithm needs no hand-processing or hand-modelingwhatsoever, thereby avoiding the manual effort of building ontologies.
Moreover, thelocal dependencies we use reduce the need for prior word sense disambiguation (Prob-lem 4), as the anaphor and the antecedent constrain each other?s sense within the386Markert and Nissim Knowledge Sources for Anaphora ResolutionFigure 1Decision tree for error classification.context of the pattern.
Furthermore, the Web scores are based on frequency, whichbiases the Web-based algorithms toward frequent senses as well as sense pairs thatoccur together frequently.
Thus, the Web algorithm has no problem resolving issueto question in Example (11) because of the high frequency of the query question ORquestions and other issues.
Problem 3 is still not addressed, however, as any corpus canencode the same semantic relations via different patterns.
Combining patterns mighttherefore yield problems similar to those presented by combining information sourcesin an ontology.Our pattern-based method, though, seems to work on very large corpora only.Unlike the Web-based algorithms, the BNC-based ones make use of POS taggingand observe sentence boundaries, thus reducing the noise intrinsic to an unprocessedcorpus like the Web.
Moreover, the instantiations used in algoBNC allow for modifi-cation to occur (see Table 4), thus increasing chances of a match.
Nevertheless, theBNC-based algorithms performed much worse than the Web-based ones: Only 4.2% ofall pattern instantiations were found in the BNC, yielding very low coverage and recall(see Table 5).4.6.2 Error Analysis.
Although the Web algorithms perform best, algoWEBv4 still incurs194 errors (47.6% of 408).
Because in several cases there is more than one reason for awrong assignment, we use the decision tree in Figure 1 for error classification.
By usingthis decision tree, we can, for example, exclude from further analysis those cases thatnone of the algorithms could resolve because of their intrinsic design.As can be seen in Table 10, quite a large number of errors result from deletingpronouns as well as not dealing with split antecedents (44 cases, or 22.7% of all mis-takes).31 Out of these 44, 30 involve split antecedents.
In 19 of these 30 cases, oneof the several correct antecedents has indeed been chosen by our algorithm, but allthe correct antecedents need to be found to allow for the resolution to be counted ascorrect.Given the high number of NE antecedents in our corpus (43.8% of correct, 25%of all antecedents; see Table 1), NE resolution is crucial.
In 11.3% of the cases, thealgorithm selects a distractor instead of the correct antecedent because the NER module31 Percentages of errors are rounded to the first decimal; rounding errors account for the coverage of 99.9%of errors instead of 100%.387Computational Linguistics Volume 31, Number 3Table 10Occurrences of error types for the best other-anaphora algorithm algoWebv4.Error type Number of cases Percentage of casesDesign 44 22.7Named entity 22 11.3String matching 19 9.8Zero score 48 24.7Tiebreaker 13 6.7Other 48 24.7Total 194 99.9either leaves the correct antecedent unresolved (which could then lead to very few orzero hits in Google) or resolves the named entity to the wrong NE category.
Stringmatching is a minor cause of errors (under 10%).
This is because, apart from its beinggenerally reliable, there is also a possible string match only in just about 30% of the cases(see Table 2).Many mistakes, instead, occur because other-anaphora can express heavily context-dependent and very unconventional relations, such as the description of dolls as winnersin Example (12).
(12) Coleco bounced back with the introduction of the Cabbage Patchdolls.
[.
.
. ]
But as the craze died, Coleco failed to come up with anotherwinner.
[.
.
.
]In such cases, the relation between the anaphor and antecedent head nouns is notfrequent enough to be found in a corpus even as large as the Web.32 This is mir-rored in the high percentage of zero-score errors (24.7% of all mistakes).
Although theWeb algorithm suffers from a knowledge gap to a smaller degree than WordNet,there is still a substantial number of cases in which we cannot find the right lexicalrelation.Errors of type other are normally due to good distractors that achieve higher Webscores than the correct antecedent.
A common reason is that the wished-for relationis attested but rare and therefore other candidates yield higher scores.
This is simi-lar to zero-score errors.
Furthermore, the elimination of modification, although useful toreduce data sparseness, can sometimes lead to the elimination of information thatcould help disambiguate among several candidate antecedents.
Lastly, lexical informa-tion, albeit crucial and probably more important than syntactic information (Modjeska2002), is not sufficient for the resolution of other-anaphora.
The integration of otherfeatures, such as grammatical function, NP form, and discourse structure, could prob-ably help when very good distractors cannot be ruled out by purely lexical methods(Example (10)).
The integration of the Web feature in a machine-learning algorithmusing several other features has yielded good results (Modjeska, Markert, and Nissim2003).32 Using different or simply more patterns might yield some hits for anaphor?antecedent pairs that return azero score when instantiated in the pattern we use in this article.388Markert and Nissim Knowledge Sources for Anaphora Resolution5.
Case Study II: Definite NP CoreferenceThe Web-based method we have described outperforms WordNet as a knowledgesource for antecedent selection in other-anaphora resolution.
However, it is not clearhow far the method and the achieved comparative results generalize to other kinds offull NP anaphora.
In particular, we are interested in the following questions: Is the knowledge gap encountered in WordNet for other-anaphora equallysevere for other kinds of full NP anaphora?
A partial (mostly affirmative)answer to this is given by previous researchers, who put the knowledgegap for coreference at 30?50% and for bridging at 38?80%, depending onlanguage, domain, and corpus (see Section 2). Do the Web-based method and the specific search patterns we usegeneralize to other kinds of anaphora? Do different anaphoric phenomena require different lexical knowledgesources?As a contribution, we investigate the performance of the knowledge sources discussedfor other-anaphora in the resolution of coreferential NPs with full lexical heads,concentrating on definite NPs (see Example (1)).
The automatic resolution of suchanaphors has been the subject of quite significant interest in the past years, but resultsare much less satisfactory than those obtained for the resolution of pronouns (seeSection 2).The relation between the head nouns of coreferential definite NPs and theirantecedents is again, in general, one of hyponymy or synonymy, making an extensionof our approach feasible.
However, other-anaphors are especially apt at conveyingcontext-specific or subjective information by forcing the reader via the other-expressionto accommodate specific viewpoints.
This might not hold for definite NPs.335.1 Corpus CollectionWe extracted definite NP anaphors and their candidate antecedents from the MUC-6coreference corpus, including both the original training and test material, for a totalof 60 documents.
The documents were automatically preprocessed in the followingway: All meta-information about each document indicated in XML (such as WSJ cat-egory and date) was discarded, and the headline was included and counted as onesentence.
Whenever headlines contained three dashes, everything after the dashes wasdiscarded.We then converted the MUC coreference chains into an anaphor?antecedent anno-tation concentrating on anaphoric definite NPs.
All definite NPs which are in, but notat the beginning of, a coreference chain are potential anaphors.
We excluded definiteNPs with proper noun heads (such as the United States) from this set, since these donot depend on an antecedent for interpretation and are therefore not truly anaphoric.34We also excluded appositives, which provide coreference structurally and are therefore33 We thank an anonymous reviewer for pointing out that this role for coreference is more likely to beprovided by demonstratives than definite NPs.34 Proper-noun heads are approximated by capitalization in the exclusion procedure.389Computational Linguistics Volume 31, Number 3not anaphoric.
Otherwise, we strictly followed the MUC annotation for coreference inour extraction, although it is not entirely consistent and not necessarily comprehensive(van Deemter and Kibble 2000).
This extraction method yielded a set of 565 anaphoricdefinite NPs.For each extracted anaphor in a coreference chain C we regard the NP in C that isclosest to the anaphor as the correct antecedent, whereas all other previous mentionsin C are regarded as lenient.
NPs that occur before the anaphor but are not marked asbeing in the same coreference chain are distractors.
Since anaphors with split antecedentsare not annotated in MUC, anaphors cannot have more than one correct antecedent.
InExample (13), the NPs with the head nouns Pact, contract, and settlement are markedas coreferent in MUC: In our annotation, the settlement is an anaphor with a correctantecedent headed by contract and a lenient antecedent Pact.
Other NPs prior to theanaphor (e.g., Canada or the IWA-Canada union) are distractors.35(13) Forest Products Firms Tentatively Agree On Pact in Canada.
A group oflarge British Columbia forest products companies has reached a tentative,three-year labor contract with about 18,000 members of the IWA-Canada union,.
.
.The settlement involves .
.
.With respect to other-anaphora, we expanded our window size from two to five sen-tences (the current and the four previous sentences) and excluded all anaphors withno correct or lenient antecedent within this window size, thus yielding a final set of 477anaphors (84.4% of 565).
This larger window size is motivated by the fact that a windowsize of two would cover only 62.3% of all anaphors (352 out 565).5.2 Antecedent Extraction, Preprocessing, and BaselinesAll NPs prior to the anaphor within the five-sentence window were extractedas antecedent candidates.36 We further processed anaphors and antecedents as inCase Study I (see Section 4.2): Modification was stripped and all NPs were lemmatized.In this experiment, named entities were resolved using Curran and Clark?s (2003) NEtagger rather than GATE.37 The identified named entities were further subclassified intofiner-grained entities, as described for Case Study I.The final number of extracted antecedents for the whole data set of 477 anaphors is14,233, with an average of 29.84 antecedent candidates per anaphor.
This figure is muchhigher than the average number of antecedent candidates for other-anaphors (10.5)because of the larger window size used.
The data set includes 473 correct antecedents,803 lenient antecedents, and 12,957 distractors.
Table 11 shows the distribution of NPtypes for correct and lenient antecedents and for distractors.There are fewer correct antecedents (473) than anaphors (477) because the MUCannotation also includes anaphors whose antecedent is not an NP but, for exam-ple, a nominal modifier in a compound.
Thus, in Example (14), the bankruptcy codeis annotated in MUC as coreferential to bankruptcy-law, a modifier in bankruptcy-lawprotection.35 All examples in the coreference study are from the MUC-6 corpus.36 This extraction was conducted manually, to put this study on an equal footing with Case Study I. Itpresupposes perfect NP chunking.
A further discussion of this issue can be found in Section 6.37 Curran and Clark?s (2003) tagger was not available to us during the first case study.
Both NE taggers arestate-of-the-art taggers trained on newspaper text.390Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 11Distribution of antecedent NP types for definite NP anaphora.Correct Lenient Distractors AllPronouns 70 145 1,078 1,293Named entities 123 316 3,108 3,547Common nouns 280 342 8,771 9,133Total 473 803 12,957 14,233(14) All legal proceedings against Eastern, a unit of Texas Air Corp., were puton hold when Eastern filed for bankruptcy-law protection March 9. .
.
.
If itdoesn?t go quickly enough, the judge said he may invoke a provision ofthe bankruptcy code [.
.
.
]In our scheme we extract the bankruptcy code as anaphoric but our method of extract-ing candidate antecedents does not include bankruptcy-law.
Therefore, there are fouranaphors in our data set with no correct/lenient antecedent extracted.
These cannot beresolved by any of the suggested approaches.We use the same evaluation measures as for other-anaphora as well as the samesignificance tests for precision?.
We also use the same baseline variations baselineREC,baselineSTRv1, and baselineSTRv2 (see Table 12 and cf.
Table 2).
The recency baseline per-forms worse than for other-anaphora.
String matching improves dramatically on simplerecency.
It also seems to be more relevant than for our other-anaphora data set, achievinghigher coverage, precision, and recall.
This confirms the high value of string matchingthat has been assigned to coreference resolution by previous researchers (Soon, Ng, andLim 2001; Strube, Rapp, and Mueller 2002, among others).As the MUC data set does not include split antecedents, an anaphor ana usuallyagrees in number with its antecedent.
Therefore, we also explored variations of allalgorithms that as a first step delete from Aanaid all candidate antecedents that do notagree in number with ana.38 The algorithms then proceed as usual.
Algorithms thatuse number checking are marked with an additional n in the subscript.
Using numberchecking leads to small but consistent gains for all baselines.As in Case Study I, we deleted pronouns for the WordNet- and corpus-based meth-ods, thereby removing 70 of 473 (14.8%) of correct antecedents (see Table 11).
Afterpronoun deletion, the total number of antecedents in our data set is 12,940 for 477anaphors, of which 403 are correct antecedents, 658 are lenient antecedents, and 11,879are distractors.38 The number feature can have the values singular, plural, or unknown.
All NE antecedent candidatesreceived the value singular, as this was by far the most common occurrence in the data set.
Informationabout the grammatical number of anaphors and common-noun antecedent candidates was calculatedand retained as additional information during the lemmatization process.
If lemmatization to both aplural and a singular noun (as determined by WordNet and CELEX) was possible (for example, the wordtalks could be lemmatized to talk or talks), the value unknown was used.
An anaphor and an antecedentcandidate were said to agree in number if they had the same value or if at least one of the two values wasunknown.391Computational Linguistics Volume 31, Number 3Table 12Overview of the results for all baselines for coreference.Algorithm Coverage Precision Recall F-measure Precision?baselineREC 1.000 0.031 0.031 0.031 0.031baselineSTRv1 0.637 0.803 0.511 0.625 0.532baselineSTRv2 0.717 0.775 0.555 0.647 0.570With number checkingbaselineRECn 1.000 0.086 0.086 0.086 0.086baselineSTRv1n 0.614 0.833 0.511 0.634 0.549baselineSTRv2n 0.694 0.809 0.562 0.664 0.5915.3 WordNet for Antecedent Selection in Definite NP CoreferenceWe hypothesize that again most antecedents are hyponyms or synonyms of theiranaphors in definite NP coreference (see Examples (1) and (13)).
Therefore we use thesame look-up for hyp/syn relations that was used for other-anaphora (see Section 4.4),including the specifications for common noun and proper name look-ups.
Parallel toTable 3, Table 13 summarizes how many correct and lenient antecedents and distractorsstand in a hyp/syn relation to their anaphor in WordNet.As already observed for other-anaphora, correct and lenient antecedents stand in ahyp/syn relation to their anaphor significantly more often than distractors do (t-test,p < 0.001).
Hyp/syn relations in WordNet might be better at capturing the relationbetween antecedent and anaphors for definite NP coreference than for other-anaphora:39A higher percentage of correct and lenient antecedents of definite NP coreference(71.96%/67.78%) stand in a hyp/syn relation to their anaphors than is the case forother-anaphora (43.0%/42.5%).
At the same time, though, there is no difference in thepercentage of distractors that stand in a hyp/syn relation to their anaphors (9% for other-anaphora, 8.80% for definite NP coreference).
For our WordNet alorithms, this is likelyto translate directly into higher coverage and recall and potentially into higher precisionthan in Case Study I.
Still, about 30% of correct antecedents are not in a hyp/synrelation to their anaphor in the current case study, confirming results by Harabagiu,Bunescu, and Maiorano (2001), who also look at MUC-style corpora.40 This gap, though,is alleviated by a quite high number of lenient antecedents, whose resolution can makeup for a missing link between anaphor and correct antecedent.41The WordNet-based algorithms are defined exactly as in Section 4.4, with theadditional two algorithms that include number checking.
Results are summarized inTable 14.All variations of the WordNet alorithms perform significantly better than thecorresponding versions of the string-matching baseline (i.e., algoWNv1 is better thanbaselineSTRv1, .
.
.
, algoWNv2n is better than baselineSTRv2n), showing that they add39 Some of this difference might be due to the corpus used instead of the phenomenon as such.40 Harabagiu, Bunescu, and Maiorano (2001) include all common-noun coreference links in their countings,whereas we concentrate on definite NPs only, so that the results are not exactly the same.41 The possibility of resolving to lenient antecedents follows a similar approach as that of Ng and Cardie(2002b), who suggest a ?best-first?
coreference resolution approach instead of a ?most recent first?approach.392Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 13Descriptive statistics for WordNet hyp/syn relations on the coreference data set.Hyp/syn relation to anaphor No hyp/syn relation TotalCorrect antecedents 290 (71.96%) 113 (28.04%) 403 (100%)Lenient antecedents 446 (67.78%) 212 (32.22%) 658 (100%)Distractors 1,046 (8.80%) 10,833 (91.20%) 11,879 (100%)All antecedents 1,782 (13.77%) 11,158 (86.23%) 12,940 (100%)additional lexical knowledge to string matching.
As expected from the descriptivestatistics discussed above, the results are better than those obtained by the WordNetalgorithms for other-anaphora, even if we disregard the additional morphosyntacticnumber constraint.5.4 The Corpus-Based Approach for Definite NP CoreferenceFollowing the assumption that most antecedents are hyponyms or synonyms oftheir anaphors in definite NP coreference, we use the same list-context pattern andinstantiations that were used for other-anaphora, allowing us to evaluate whether theyare transferrable.
The corpora we use are again the Web and the BNC.As with other-anaphora, the Web scores do well in distinguishing between cor-rect/lenient antecedents and distractors, with significantly higher means/medians forcorrect/lenient antecedents (median 472/617 vs. 2 for distractors), as well as signifi-cantly fewer zero scores (8% for correct/lenient vs. 41% for distractors).
This indicatestransferability of the Web-based approach to coreference.
Compared to other-anaphora,the number of zero-scores is lower for correct/lenient antecedent types, so that weexpect better overall results, similar to our expectations for the WordNet alorithm.The BNC scores can also distinguish between correct/lenient antecedents anddistractors, since the number of zero scores for correct/lenient antecedents (68.98%/58.05%) is significantly lower than for distractors (96.97%).
Although more than50% of correct/lenient antecedents receive a zero score, there are fewer zero scoresthan for other-anaphora (for which more than 80% of correct/lenient antecedents re-ceived zero scores).
However, BNC scores are again in general much lower than Webscores, as measured by means, medians, and zero scores.
Nevertheless, Web scoresand BNC scores correlate significantly, with the correlations reaching higher coeffi-Table 14Overview of the results for all WordNet alorithms for coreference.Algorithm Coverage Precision Recall F-measure Precision?algoWNv1 0.874 0.715 0.625 0.666 0.631algoWNv2 0.874 0.724 0.633 0.676 0.639With number checkingalgoWNv1n 0.866 0.734 0.635 0.681 0.648algoWNv2n 0.866 0.751 0.649 0.697 0.662393Computational Linguistics Volume 31, Number 3cients (0.53 to 0.65, depending on antecedent group) than they did in the case study forother-anaphora.The corpus-based algorithms for coreference resolution are parallel to thosedescribed for other-anaphora and are marked by the same subscripts.
The variations thatinclude number checking are again marked by a subscript n. Tables 15 and 16 report theresults for all the Web and BNC algorithms, respectively.5.5 Discussion and Error Analysis5.5.1 Algorithm Comparison.
Using the original or the replaced antecedent for stringmatching (versions v1 vs. v2, v1n vs. v2n, v3 vs. v4, and v3n vs. v4n) never resultsin interesting differences in any of the approaches discussed.
Also, number matchingprovides consistent improvements.
Therefore, from this point on, our discussion willdisregard those variations, that use original antecedents only (v1, v1n, v3, and v3n) aswell as algorithms that do not use number matching (v2, v4).
We will also concentrateon the final precision?
of the full-coverage algorithms.
The set of anaphors that are cov-ered by the best string-matching baseline, prior to recency back-off, will again be denotedby StrSetv2n.
Again, both a t-test and McNemar?s test will be used, when statementsabout significance are made.The results for the string-matching baselines and for the lexical methods are higherfor definite coreferential NPs than for other-anaphora.
This is largely a result of thehigher number of string-matching antecedent/anaphor pairs in coreference, the higherprecision of string matching, and to a lesser degree, the lower number of unusualredescriptions.Similar to the results for other-anaphora, the WordNet-based algorithms beat thecorresponding baselines.
The first striking result is that the Web algorithm variationalgoWebv2n, which relies only on the highest Web scores and is therefore allowedto overrule string matching, does not outperform the corresponding string-matchingbaseline baselineSTRv2n and performs significantly worse than the correspondingWordNet alorithm algoWNv2n.
This contrasts with the results for other-anaphora.
Whenthe results were examined in detail, it emerged that for a considerable number ofanaphors in StrSetv2n, the highest Web score was indeed achieved by a distractor witha high-frequency head noun when the correct or lenient antecedent could be insteadfound by a simple string match to the anaphor.
This problem is much more severe thanTable 15Overview of the results for all Web algorithms for coreference.Algorithm Coverage Precision Recall F-measure Precision?algoWebv1 0.994 0.561 0.558 0.559 0.562algoWebv2 0.994 0.553 0.549 0.550 0.554algoWebv3 0.998 0.674 0.673 0.673 0.673algoWebv4 0.998 0.679 0.677 0.678 0.677With number checkingalgoWebv1n 0.992 0.613 0.608 0.610 0.612algoWebv2n 0.992 0.607 0.602 0.604 0.606algoWebv3n 0.996 0.705 0.702 0.703 0.703algoWebv4n 0.996 0.716 0.713 0.714 0.713394Markert and Nissim Knowledge Sources for Anaphora ResolutionTable 16Overview of the results for all BNC algorithms for coreference.Algorithm Coverage Precision Recall F-measure Precision?algoBNCv1 0.438 0.559 0.245 0.341 0.524algoBNCv2 0.438 0.559 0.245 0.341 0.526algoBNCv3 0.769 0.749 0.576 0.651 0.589algoBNCv4 0.777 0.757 0.589 0.663 0.599With number checkingalgoBNCv1n 0.411 0.612 0.251 0.356 0.562algoBNCv2n 0.411 0.622 0.256 0.369 0.570algoBNCv3n 0.753 0.769 0.579 0.661 0.610algoBNCv4n 0.761 0.785 0.597 0.678 0.627for other-anaphora because of (1) the larger window size that includes more distractorsand (2) the higher a priori precision of the string-matching baseline, which means thatoverruling string matching leads to wrong results more frequently.
Typical examplesinvolve named-entity recognition and inverted queries.
Thus, in Example (15), theanaphor the union is coreferent with the first occurrence of the union, a case easilyresolved by string matching.
However, the distractor organization [= Chrysler Canada]achieves a higher Web score, because of the score of the inverted query union OR unionsand other organizations.42(15) [.
.
. ]
The union struck Chrysler Canada Tuesday after rejecting a companyoffer on pension adjustments.
The union said the size of the adjustmentswas inadequate.Several potential solutions exist to this problem, such as normalization of Web scoresor penalizing of inverted queries.
The solution we have adopted in algoWebv4n is touse Web scores only after string matching, thereby making the Web-based approachmore comparable to the WordNet approach.
Therefore, baselineSTRv2n, algoWebv4n, andalgoWNv2n (as well as algoBNCv4n) all coincide in their decisions for anaphors in StrSetv2nand only differ in the decisions made for anaphors that do not have a matchingantecedent candidate.
Indeed, algoWebv4n performs significantly better than the base-lines at the 1% level, and results rise from a precision?
of 60.6% for algoWebv2n to 71.3%for algoWebv4n.
It also significantly outperforms the best BNC results, thus showing thatovercoming data sparseness is more important than working with a controlled, tagged,and representative corpus.
Furthermore, shows better performance than WordNet in thefinal algorithm variation (71.3% vs. 66.2%).43 According to results of a t-test, however,this last difference is not significant.
McNemar?s test, concentrating on the part of thedata in which the methods differ, shows instead significance at the 1% level.Indeed, one of the problems in comparing algorithm results for coreference is thatsuch a large number of anaphors are covered by simple string matching, leaving only42 Remember that this problem does not affect the WordNet-based algorithm, which always achieves thesame results as the string-matching baseline on StrSetv2n.
Both the correct antecedent and the organization[= Chrysler Canada] distractor stand in a hyp/syn relation to the anaphor, and then string matching isused as a tiebreaker.43 In general, the WordNet methods achieve higher precision, with the Web method achieving higher recall.395Computational Linguistics Volume 31, Number 3a small data set on which the lexical methods can differ.
Thus, StrSetv2n contains 331 of477 cases (268 of which are assigned correctly by baselineStrv2n), so that improvements bythe other methods are confined to the set of the remaining 146 anaphors.
Of these 146,baselineStr?v2n assigns the correct antecedent to 13 (8.9%) anaphors by using a recencyback-off, the best WordNet method to 55 anaphors (37.67%), and the best Web methodto 72 anaphors (49.31%).
Therefore the Web-based method is a better complement tostring matching than WordNet, which is reflected in the results of McNemar?s test.Anaphor?antecedent relations that were not covered in WordNet but that did not provea problem for the Web algorithm were again both general hyponymy relations, such asretailer:organization, bill:legislation and month:time, and more subjective relations like (wage)cuts:concessions and legislation:attack.5.5.2 Error Analysis.
The best-performing Web-based algorithm, algoWebv4n, still selectsthe wrong antecedent for a given anaphor in 137 of 477 cases (28.7%).
Again, we usethe decision tree in Figure 1 to classify errors.
Design errors now do not include splitantecedents but do include errors that occur because the condition of number agreementwas violated, pronoun deletion errors, and the four cases in which the antecedent is anon-NP antecedent and therefore not extracted in the first place (see Section 5.1 andExample (14)).
Table 17 reports the frequency of each error type.Differently from other-anaphora, the design and NE errors together account forunder 15% of the mistakes.
Also rare are zero-score errors (only 8%).
When comparedto the number of zero-score errors in other anaphora (24.7%), this low figure suggeststhat other-anaphora is more prone to exploit rare, unusual, and context-dependentredescriptions than full NP coreference.
Nevertheless, it is yet possible to find non-standard redescriptions in coreference as well which yield zero scores, such as the useof transaction to refer to move in Example (16).
(16) Conseco Inc., in a move to generate about $200 million in tax deductions,said it induced five of its top executives to exercise stock options topurchase about 3.6 million common shares of the financial-servicesconcern.
As a result of the transaction, .
.
.Much more substantial is the weight of errors due to string matching, tiebreaker deci-sions, and the presence of good distractors (the main reason for errors of type other),which together account for over three-quarters of all mistakes.String matching is quite successful for coreference (baselineSTRv2n covers nearly70% of the cases with a precision of 80.9%).
However, because algoWebv4n never over-Table 17Occurrences of error types for the best coreference algorithm algoWebv4n.Error type Number of cases Percentage of casesDesign 12 8.7Named entity 7 5.1String matching 33 24.1Zero scores 11 8.0Tiebreaker 34 24.8Other 40 29.2Total 137 99.9396Markert and Nissim Knowledge Sources for Anaphora Resolutionrules string matching, the errors of baselineSTRv2n are preserved here and account for24.1% of all mistakes.44 Tiebreaker errors are quite frequent too (24.8%), as our far-from-sophisticated tiebreaker was needed in nearly half of the cases (224 times; 47.0%).The remaining errors (29.2%) are due to the presence of good distractors that scorehigher than the correct/lenient antecedent.
In Example (17), for instance, a distractorwith a higher Web score (comment) prevents the algorithm from selecting the correctantecedent (investigation) for the anaphor the inquiry.
(17) Mr. Adams couldn?t be reached for comment.
Though the investigation hasbarely begun, persons close to the board said Messrs. Lavin and Youngwill get a ?hard look?
as to whether they were involved, and are bothconsidered a ?natural focus?
of the inquiry.Example (18) shows how stripping modification might have eliminated informationcrucial to identifying the correct antecedent: Only the head process was retained of theanaphor arbitration process, so that the surface link between anaphor and antecedent(arbitration) was lost and the distractor securities industry, reduced to industry, wasinstead selected.
(18) The securities industry has favored arbitration because it keeps brokers anddealers out of court.
But consumer advocates say customers sometimesunwittingly sign away their right to sue.
?We don?t necessarily have a beefwith the arbitration process,?
says Martin Meehan, [.
.
.
]6.
Open Issues6.1 Preprocessing and Prior AssumptionsOur algorithms build on two main preprocessing assumptions.
First, we assume perfectbase-NP chunking and expect results to be lower with automatic chunking.
Neverthe-less, since automatic chunking will affect all algorithms in the same way, we do expectcomparative results to stand.
We are not, however, dependent on full parsing, as noparsing-dependent grammatical features are used by the algorithms.Second, the anaphoricity of the definite NPs in Case Study II has de facto beenmanually determined, as we restrict our study to antecedent selection for the NPsthat are marked in the MUC corpus as coreferent.
One of the reasons why pronoun res-olution has been more successful than definite NP resolution is that whereas pronounsare mostly anaphoric, definite NPs do not have to be so (see Section 2).
In fact, it has beenargued by several researchers that an anaphora resolution algorithm should proceed toantecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a two-stage process which we also follow in this article.
Although recent work on automaticanaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), ouralgorithms will perform worse when building on non-manually determined anaphors.Future work will explore the extent of such a decrease in performance.44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors.397Computational Linguistics Volume 31, Number 36.2 Directions for ImprovementAll algorithms we have described can be considered blueprints for more complexversions.
Specifically, the WordNet-based algorithms could be improved by exploitinginformation encoded in WordNet beyond explicitly encoded links (glosses could bemined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]).
The Web-based algorithms could similarly benefit from the exploration of different patterns andtheir combination, as well as from using non-pattern-based approaches for hyponymydetection (Shinzato and Torisawa 2004).
In addition, we have evaluated the contributionof lexical resources in isolation rather than within a more sophisticated system thatintegrates additional non-lexical features.
It is unclear whether integrating such knowl-edge sources in a full-resolution system might even out the differences between theWeb-based and the WordNet-based algorithms or exacerbate them.
Modjeska, Markert,and Nissim (2003) included a feature based on Web scores in a naive Bayes modelfor other-anaphora resolution that also used grammatical features and showed thatthe addition of the Web feature yielded an 11.4-percentage-point improvement overusing a WordNet-based feature.
This gives some indication that additional grammaticalfeatures might not be able to compensate fully for the knowledge gap encountered inWordNet.6.3 Extension to Yet Other Anaphora TypesUsing the Web for antecedent selection in anaphora resolution is novel and needsfurther study for other types of full NP anaphora than the ones studied in this article.If an anaphora type exploits hyponymy/synonymy relationships between anaphor andantecedent head nouns, it can in principle be treated with the exact same pattern weused in this article.
This holds, for example, for demonstratives and such-anaphors.
Thelatter, in particular, are similar to other-anaphora in that they establish a comparisonbetween the entity they invoke and that invoked by the antecedent and are also easilyused to accommodate subjective viewpoints.
They should therefore benefit especiallyfrom not relying wholly on standard taxonomic links.Different patterns can be developed for anaphora types that build on non-hyponymy relations.
For example, bridging exploits meronymy and/or causal rela-tions (among others).
Therefore, patterns that express ?part-of?
links, for example,such as X of Y and genitives, would be appropriate.
Indeed, these patterns have beenrecently used in Web search for antecedent selection for bridging anaphora by Poesioet al (2004).
They compare accuracy in antecedent selection for a method that inte-grates Web hits and focusing techniques with a method that uses WordNet and fo-cusing, achieving comparable results for both methods.
This strenghtens our hypothesisthat antecedent selection for full NP anaphora without hand-modeled lexical knowl-edge has become feasible.7.
ConclusionsWe have explored two different ways of exploiting lexical knowledge for antecedentselection in other-anaphora and definite NP coreference.
Specifically, we have compareda hand-crafted and -structured source of information such as WordNet and a simpleand inexpensive pattern-based method operating on corpora.
As corpora we have usedthe BNC and also suggested the Web as the biggest corpus available.398Markert and Nissim Knowledge Sources for Anaphora ResolutionWe confirmed results by other researchers that show that a substantial number oflexical links often exploited in coreference are not included in WordNet.
We have alsoshown the presence of an even more severe knowledge gap for other-anaphora (see alsoQuestion 1 in Section 1).
Largely because of this knowledge gap, the novel Web-basedmethod that we proposed proved better than WordNet at resolving other-anaphora.Although the gains for coreference are not as high, the Web-based method improvesmore substantially on string-matching techniques for coreference than WordNet does(see the success rate beyond StrSetv2n for coreference, Section 5.5).
In both studies, theWeb-based method clearly outperformed the BNC-based one.
This shows that, for ourtasks, overcoming data sparseness was more important than working with a manuallycontrolled, virtually noise-free, but relatively small corpus, which addresses Question 2in Section 1: Corpus-induced knowledge can indeed rival and even outperform theknowledge obtained via lexical hierarchies, as long as the corpus is large enough.Corpus-based methods can therefore be a very useful complement to resolution al-gorithms for languages for which hand-crafted taxonomies have not yet been createdbut for which large corpora do exist.
In answer to Question 3 in Section 1, our resultssuggest that different anaphoric phenomena suffer in varying degrees from missingknowledge and that the Web-based method performs best when used to deal withphenomena that standard taxonomy links do not capture that easily or that frequentlyexploit subjective and context-dependent knowledge.In addition, the Web-based method that we propose does not suffer from someof the intrinsic limitations of ontologies, specifically, the problem of what knowledgeshould be included (see Section 2.2).
It is also inexpensive and does not need anypostprocessing of the Web pages returned or any hand-modeling of lexical knowledge.To summarize, antecedent selection for other-anaphora and definite NP coreferencewithout hand-crafted lexical knowledge is feasible.
This might also be the case for yetother full NP anaphora types with similar properties?an issue that we will explore infuture work.AcknowledgmentsWe especially thank Natalia Modjeska forproviding us with her annotated corpus ofother-anaphors as well as with the extractedand partially preprocessed sets of candidateantecedents for Case Study I.
She alsocollaborated on previous related work onother-anaphora (Markert, Nissim, andModjeska 2003; Modjeska, Markert, andNissim 2003) on which this article builds.
Wewould also like to thank Johan Bos, JamesCurran, Bonnie Webber, and fouranonymous reviewers for helpful comments,which allowed us to greatly improve thisarticle.
Malvina Nissim was partiallysupported by Scottish EnterpriseStanford-Link Grants R36766 (ParaphraseGeneration) and R36759 (SEER).ReferencesAriel, Mira.
1990.
Accessing Noun PhraseAntecedents.
Routledge, London andNew York.Berland, Matthew and Eugene Charniak.1999.
Finding parts in very large corpora.In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics,pages 57?64, Providence, RI.Bierner, Gann.
2001.
Alternative phrasesand natural language informationretrieval.
In Proceedings of the 39thAnnual Meeting of the Association forComputational Linguistics, pages 58?65,Toulouse, France.Burnard, Lou, 1995.
Users?
Reference Guide,British National Corpus.
British NationalCorpus Consortium, Oxford.Caraballo, Sharon.
1999.
Automaticacquisition of a hypernym-labelled nounhierarchy from text.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics, pages 120?126,Providence, RI.Chinchor, Nancy.
1997.
MUC-7 named entitytask definition.
In Proceedings of the SeventhConference on Message Understanding,Washington, DC.399Computational Linguistics Volume 31, Number 3Christ, Oliver, 1995.
The XKWIC User Manual.Institute for Computational Linguistics,University of Stuttgart.Clark, Herbert H. 1975.
Bridging.
InProceedings of the Conference on TheoreticalIssues in Natural Language Processing,pages 169?174, Cambridge, MA.Connolly, Dennis, John D. Burger, andDavid S. Day.
1997.
A machine learningapproach to anaphoric reference.
InDaniel Jones and Harold Somers, editors,New Methods in Language Processing.University College London Press, London,pages 133?144.Curran, James and Stephen Clark.
2003.Language independent NER usinga maximum entropy tagger.
InProceedings of the Seventh Conference onNatural Language Learning (CoNLLO3),pages 164?167, Edmonton, Alberta,Canada.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Fraurud, Kari.
1990.
Definiteness and theprocessing of NPs in natural discourse.Journal of Semantics, 7:395?433.Gardent, Claire, Helene Manuelian, andEric Kow.
2003.
Which bridges forbridging definite descriptions?
InProceedings of the EACL 2003 Workshopon Linguistically Interpreted Corpora,pages 69?76, Budapest.Grefenstette, Gregory.
1999.
The WWW as aresource for example-based MT tasks.
InProceedings of ASLIB?99: Translating and theComputer 21, London.Gundel, Jeanette, Nancy Hedberg, and RonZacharski.
1993.
Cognitive status and theform of referring expressions in discourse.Language, 69(2):274?307.Hahn, Udo, Michael Strube, and KatjaMarkert.
1996.
Bridging textual ellipses.
InProceedings of the 16th InternationalConference on Computational Linguistics,pages 496?501, Copenhagen.Halliday, Michael A. K. and Ruqaiya Hasan.1976.
Cohesion in English.
Longman,London.Harabagiu, Sanda.
1997.
WordNet-BasedInference of Textual Context, Cohesion andCoherence.
Ph.D. thesis, University ofSouthern California.Harabagiu, Sanda, Razvan Bunescu, andSteven J. Maiorano.
2001.
Text andknowledge mining for coreferenceresolution.
In Proceedings of the SecondConference of the North American Chapter ofthe ACL, pages 55?62, Pittsburgh.Hawkins, John A.
1978.
Definiteness andIndefiniteness.
Croom Helm, London.Hearst, Marti.
1992.
Automatic acquisition ofhyponyms from large text corpora.
InProceedings of the 14th InternationalConference on Computational Linguistics,Nantes, France.Hirschman, Lynette and Nancy Chinchor.1997.
MUC-7 coreference task definition.In Proceedings of the Seventh Conference onMessage Understanding, Washington, DC.Humphreys, Kevin, Robert Gaizauskas,Saliha Azzam, Chris Huyck, BrianMitchell, and Hamish Cunningham.
1997.University of Sheffield: Description of theLaSie-II system as used for MUC-7.
InProceedings of the Seventh MessageUnderstanding Conference (MUC-7),Washington, DC.Kameyama, Megumi.
1997.
Recognizingreferential links: An information extractionperspective.
In Proceedings of the ACL-1997Workshop on Operational Factors in Practical,Robust Anaphora Resolution for UnrestrictedTexts, pages 46?53, Madrid.Keller, Frank and Maria Lapata.
2003.
Usingthe Web to obtain frequencies for unseenbigrams.
Computational Linguistics,29(3):459?484.Kennedy, Christopher and BranimirBoguraev.
1996.
Anaphora for everyone:Pronominal anaphora resolution without aparser.
In Proceedings of the 16thInternational Conference on ComputationalLinguistics, pages 113?118, Copenhagen.Markert, Katja, Malvina Nissim, andNatalia N. Modjeska.
2003.
Using theWeb for nominal anaphora resolution.
InRobert Dale, Kees van Deemter, andRuslan Mitkov, editors, Proceedings of theEACL Workshop on the ComputationalTreatment of Anaphora, pages 39?46,Budapest.McCoy, Kathleen and Michael Strube.
1999.Generating anaphoric expressions:Pronoun or definite description?
In ACL-99Workshop on the Relation ofDiscourse/Dialogue Structure and Reference,pages 63?71, College Park, MD.Meyer, Ingrid.
2001.
Extractingknowledge-rich contexts forterminography.
In Didier Bourigault,Christian Jacquemin, and Marie-ClaudeL?Homme, editors, Recent Advances inComputational Terminology.
John Benjamins,Amsterdam, pages 279?301.Meyer, Josef and Robert Dale.
2002.Mining a corpus to support associativeanaphora resolution.
In Proceedings of400Markert and Nissim Knowledge Sources for Anaphora Resolutionthe Fourth International Conference onDiscourse Anaphora and Anaphor Resolution,Lisbon.Mitkov, Ruslan.
1998.
Robust pronounresolution with limited knowledge.
InProceedings of the 17th InternationalConference on Computational Linguistics and36th Annual Meeting of the Association forComputational Linguistics, pages 869?879,Montreal.Modjeska, Natalia N. 2002.
Lexical andgrammatical role constraints in resolvingother-anaphora.
In Proceedings of DAARC2002, pages 129?134, Lisbon.Modjeska, Natalia N. 2003.
Resolvingother-anaphora.
Ph.D. thesis, School ofInformatics, University of Edinburgh.Modjeska, Natalia N., Katja Markert, andMalvina Nissim.
2003.
Using the Web inmachine learning for other-anaphoraresolution.
In Proceedings of the 2003Conference on Empirical Methods in NaturalLanguage Processing, pages 176?183,Sapporo, Japan.Ng, Vincent.
2004.
Learning noun phraseanaphoricity to improve coreferenceresolution: Issues in representation andoptimization.
In Proceedings of the 42ndAnnual Meeting of the Association forComputational Linguistics, pages 151?158,Barcelona.Ng, Vincent and Claire Cardie.
2002a.Identifying anaphoric and non-anaphoricnoun phrases to improve coreferenceresolution.
In Proceedings of the 19thInternational Conference on ComputationalLinguistics, pages 730?736, Taipei.Ng, Vincent and Claire Cardie.
2002b.Improving machine learning approachesto coreference resolution.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, pages 104?111,Philadelphia.Poesio, Massimo, Tomonori Ishikawa, SabineSchulte im Walde, and Renata Vieira.
2002.Acquiring lexical knowledge for anaphoraresolution.
In Proceedings of the ThirdInternational Conference on LanguageResources and Evaluation, pages 1220?1224,Las Palmas, Canary Islands.Poesio, Massimo, Rahul Mehta, AxelMaroudas, and Janet Hitzeman.
2004.Learning to resolve bridging references.
InProceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics,pages 143?150, Barcelona.Poesio, Massimo, Renata Vieira, and SimoneTeufel.
1997.
Resolving bridging referencesin unrestricted text.
In Ruslan Mitkov,editor, Proceedings of the ACL Workshop onOperational Factors in Robust AnaphoraResolution, pages 1?6, Madrid.Preiss, Judita.
2002.
Anaphora resolutionwith word sense disambiguation.
InProceedings of SENSEVAL-2, pages 143?146,Philadelphia.Preiss, Judita, Caroline Gasperin, and TedBriscoe.
2004.
Can anaphoric definitedescriptions be replaced by pronouns?
InProceedings of the Fourth InternationalConference on Language Resources andEvaluation, pages 1499?1502, Lisbon.Shinzato, Keiji and Kentaro Torisawa.
2004.Acquiring hyponymy relations from Webdocuments.
In Proceedings of the Conferenceof the North American Chapter of the ACL,pages 73?80, Boston.Soon, Wee Meng, Hwee Tou Ng Ng, andDaniel Chung Yung Lim.
2001.
A machinelearning approach to coreferenceresolution of noun phrases.
ComputationalLinguistics, 27(4):521?544.Strube, Michael, Stefan Rapp, andChristoph Mueller.
2002.
The influenceof minimum edit distance on referenceresolution.
In Proceedings of the 2002Conference on Empirical Methods in NaturalLanguage Processing, pages 312?319,Philadelphia.Uryupina, Olga.
2003.
High-precisionidentification of discourse new and uniquenoun phrases.
In Proceedings of the ACL2003 Student Workshop, pages 80?86,Sapporo, Japan.van Deemter, Kees and Rodger Kibble.
2000.On coreferring: Coreference in MUC andrelated annotation schemes.
ComputationalLinguistics, 26(4):615?662.Vieira, Renata and Massimo Poesio.
2000.
Anempirically-based system for processingdefinite descriptions.
ComputationalLinguistics, 26(4):539?593.Webber, Bonnie, Matthew Stone, AravindJoshi, and Alistair Knott.
2003.
Anaphoraand discourse structure.
ComputationalLinguistics, 29(4):545?587.Yang, Xiaofeng, Guodong Zhou, Jian Su,and Chew Lim Tan.
2003.
Coreferenceresolution using competition learningapproach.
In Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics, pages 176?183, Sapporo, Japan.401
