A Word-Order Database for TestingComputational Models of Language AcquisitionWilliam Gregory SakasDepartment of Computer SciencePhD Programs in Linguistics and Computer ScienceHunter College and The Graduate CenterCity University of New Yorksakas@hunter.cuny.eduAbstractAn investment of effort over the last twoyears has begun to produce a wealth ofdata concerning computational psycholin-guistic models of syntax acquisition.
Thedata is generated by running simulationson a recently completed database of wordorder patterns from over 3,000 abstractlanguages.
This article presents the designof the database which contains sentencepatterns, grammars and derivations thatcan be used to test acquisition models fromwidely divergent paradigms.
The domainis generated from grammars that are lin-guistically motivated by current syntactictheory and the sentence patterns have beenvalidated as psychologically/developmen-tally plausible by checking their frequencyof occurrence in corpora of child-directedspeech.
A small case-study simulation isalso presented.1 IntroductionThe exact process by which a child acquires thegrammar of his or her native language is one of themost beguiling open problems of cognitivescience.
There has been recent interest in computersimulation of the acquisition process and theinterrelationship between such models and linguis-tic and psycholinguistic theory.
The hope is thatthrough computational study, certain bounds canbe established which may be brought to bear onpivotal issues in developmental psycholinguistics.Simulation research is a significant departurefrom standard learnability models that provideresults through formal proof (e.g., Bertolo, 2001;Gold, 1967; Jain et al, 1999; Niyogi, 1998; Niyogi& Berwick, 1996; Pinker, 1979; Wexler & Culi-cover, 1980, among many others).
Althoughresearch in learnability theory is valuable andongoing, there are several disadvantages to formalmodeling of language acquisition:?
Certain proofs may involve impractically manysteps for large language domains (e.g.
thoseinvolving Markov methods).?
Certain paradigms are too complex to readilylend themselves to deductive study (e.g.
con-nectionist models).1?
Simulations provide data on intermediate stageswhereas formal proofs typically prove whethera domain is (or more often is not) learnable apriori to specific trials.?
Proofs generally require simplifying assump-tions which are often distant from natural lan-guage.However, simulation studies are not withoutdisadvantages and limitations.
Most notableperhaps, is that out of practicality, simulations aretypically carried out on small, severely circum-scribed domains ?
usually just large enough toallow the researcher to hone in on how a particularmodel (e.g.
a connectionist network or a principles& parameters learner) handles a few grammaticalfeatures (e.g.
long-distance agreement and/ortopicalization) often, though not always, in a singlelanguage.
So although there have been manysuccessful studies that demonstrate how onealgorithm or another is able to acquire some aspectof grammatical structure, there is little doubt thatthe question of what mechanism children actuallyemploy during the acquisition process is still open.This paper reports the development of a large,multilingual database of sentence patterns, gram-1 Although see Niyogi, 1998 for some insight.mars and derivations that may be used to testcomputational models of syntax acquisition fromwidely divergent paradigms.
The domain isgenerated from grammars that are linguisticallymotivated by current syntactic theory and thesentence patterns have been validated as psycho-logically/developmentally plausible by checkingtheir frequency of occurrence in corpora of child-directed speech.
We report here the structure of thedomain, its interface and a case-study that demon-strates how the domain has been used to test thefeasibility of several different acquisition strate-gies.The domain is currently publicly available onthe web via http://146.95.2.133 and it is our hopethat it will prove to be a valuable resource forinvestigators interested in computational models ofnatural language acquisition.2 The Language Domain DatabaseThe focus of the language domain database,(hereafter LDD), is to make readily available thedifferent word order patterns that children aretypically exposed to, together with all possiblesyntactic derivations of each pattern.
The patternsand their derivations are generated from a largebattery of grammars that incorporate many featuresfrom the domain of natural language.At this point the multilingual language domaincontains sentence patterns and their derivationsgenerated from 3,072 abstract grammars.
Thepatterns encode sentences in terms of tokensdenoting the grammatical roles of words andcomplex phrases, e.g., subject (S), direct object(O1), indirect object (O2), main verb (V), auxiliaryverb (Aux), adverb (Adv), preposition (P), etc.
Anexample pattern is S Aux V O1 which correspondsto the English sentence: The little girl can make apaper airplane.
There are also tokens for topic andquestion markers for use when a grammar specifiesovert topicalization or question marking.Declarative sentences, imperative sentences,negations and questions are represented within theLDD, as is prepositional movement/stranding(pied-piping), null subjects, null topics, topicaliza-tion and several types of movement.Although more work needs to be done, a firstround study of actual child-directed sentences fromthe CHILDES corpus (MacWhinney, 1995)indicates that our patterns capture many sententialword orders that children typically encounter in theperiod from 1-1/2 to 2-1/2 years; the periodgenerally accepted by psycholinguists to be whenchildren establish the correct word order of theirnative language.
For example, although the LDD iscurrently limited to degree-0 (i.e.
no embedding)and does not contain DP-internal structure, afterexamining by hand, several thousand sentencesfrom corpora in the CHILDES database in fivelanguages (English, German, Italian, Japanese andRussian), we found that approximately 85% aredegree-0 and an approximate 10 out of 11 have nointernal DP structure.Adopting the principles and parameters (P&P)hypothesis (Chomsky, 1981) as the underlyingframework, we implemented an application thatgenerated patterns and derivations given thefollowing points of variation between languages:1.
Affix Hopping  2.
Comp Initial/Final3.
I to C Movement  4.
Null Subject5.
Null Topic  6.
Obligatory Topic7.
Object Final/Initial  8.
Pied Piping9.
Question Inversion   10.
Subject Initial/Final11.
Topic Marking   12.
V to I Movement13.
Obligatory Wh movementThe patterns have fully specified X-bar struc-ture, and movement is implemented as HPSG localdependencies.
Pattern production is generated top-down via rules applied at each subtree level.Subtree levels include: CP, C', IP, I', NegP, Neg',VP, V' and PP.
After the rules are applied, thesubtrees are fully specified in terms of nodecategories, syntactic feature values and constituentorder.
The subtrees are then combined by a simpleunification process and syntactic features arepercolated down.
In particular, movement chainsare represented as traditional ?slash?
featureswhich are passed (locally) from parent to daughter;when unification is complete, there is a trace at thebottom of each slash-feature path.
Other featuresinclude +/-NULL for non-audible tokens (e.g.S[+NULL] represents a null subject pro), +TOPICto represent a topicalized token, +WH to represent?who?, ?what?, etc.
(or ?qui?, ?que?
if one pre-fers), +/-FIN to mark if a verb is tensed or not andthe illocutionary (ILLOC) features Q, DEC, IMPfor questions, declaratives and imperatives respec-tively.Although further detail is beyond the scope ofthis paper, those interested may refer to Fodor etal.
(2003) which resides on the LDD website.It is important to note that the domain is suit-able for many paradigms beyond the P&P frame-work.
For example the context-free rules (withlocal dependencies) could be easily extracted andused to test probabilistic CFG learning in amultilingual domain.
Likewise the patterns,without their derivations, could be used as input tostatistical/connectionist models which eschewtraditional (generative) structure altogether andsearch for regularity in the left-to-right strings oftokens that makeup the learner's input stream.
Or,the patterns could help bootstrap the creation of adomain that might be used to test particular typesof lexical learning by using the patterns as tem-plates where tokens may be instantiated with actualwords from a lexicon of interest to the investigator.The point is that although a particular grammarformalism was used to generate the patterns, thepatterns are valid independently of the formalismthat was in play during generation.2To be sure, similar domains have been con-structed.
The relationship between the LDD andother artificial domains is summarized in Table 1.In designing the LDD, we chose to includesyntactic phenomena which:i) occur in a relatively high proportion of theknown natural languages;2 If this is the case, one might ask: Why bother with agrammar formalism at all; why not use actual child-directedspeech as input instead of artificially generated patterns?Although this approach has proved workable for several typesof non-generative acquisition models, a generative (or hybrid)learner is faced with the task of selecting the rules orparameter values that generate the linguistic environmentbeing encountered by the learner.
In order to simulate this,there must be some grammatical structure incorporated intothe experimental design that serves as the target the learnermust acquire.
Constructing a viable grammar and a parser withcoverage over a multilingual domain of real child-directedspeech is a daunting proposition.
Even building a parser toparse a single language of child-directed speech turns out to beextremely difficult.
See, for example, Sagae, Lavie, &MacWhinney (2001), which discusses an impressive numberof practical difficulties encountered while attempting to builda parser that could cope with the EVE corpus; one the cleanesttranscriptions in the CHILDES database.
By abstracting awayfrom actual child-directed speech, we were able to build apattern generator and include the pattern derivations in thedatabase for retrieval during simulation runs, effectivelysidestepping the need to build an online multilingual parser.ii) are frequently exemplified in speech di-rected to 2-year-olds;iii) pose potential learning problems (e.g.
cross-language ambiguity) for which theoreticalsolutions are needed;iv) have been a focus of linguistic and/or psy-cholinguistic research;v) have a syntactic analysis that is broadlyagreed on.As a result the following have been included:?
By criteria (i) and (ii): negation, non-declarative sentences (questions, impera-tives).?
By criterion (iv): null subject parameter(Hyams 1986 and since).?
By criterion (iv): affix-hopping (though notwidespread in natural languages).?
By criterion (v): no scrambling yet.There are several phenomena that the LDDdoes not yet include:?
No verb subcategorization.?
No interface with LF (cf.
Briscoe 2000;Villavicencio 2000).?
No discourse contexts to license sentencefragments (e.g., DP or PP fragments).?
No XP-internal structure yet (except PP = P+ O3, with piping or stranding).?
No Linear Correspondence Axiom (Kayne1994).?
No feature checking as implementation ofmovement parameters (Chomsky 1995).Table 1: A history of abstract domains for word-order acquisition modeling.#parameters#lan-guagesTreestruc-ture?LanguagepropertiesGibson &Wexler(1994)3 8 Not fully specified Word order, V2Bertolo et.al (1997b) 764distinct YesG&W + V-raising toAgr, T; deg-2Kohl (1999)based onBertolo12 2,304 PartialBertolo et al(1997b) +scramblingSakas &Nishimoto(2002)4 16 Yes G&W + null subject/topicLDD 13 3,072 YesS&N + wh-movt +imperatives +auxinversion,  etc.The LDD on the web: The two primary purposesof the web-interface are to allow the user tointeractively peruse the patterns and the derivationsthat the LDD contains and to download raw datafor the user to work with locally.Users are asked to register before using theLDD online.
The user ID is typically an emailaddress, although no validity checking is carriedout.
The benefit of entering a valid email address issimply to have the ability to recover a forgottenpassword, otherwise a user can have full accessanonymously.The interface has three primary areas: Gram-mar Selection, Sentence Selection and DataDownload.
First a user has to specify, on theGrammar Selection page, which settings of the 13parameters are of interest and save those settings asan available grammar.
A user may specify multiplegrammars.
Then in the sentence selection page auser may peruse sentences and their derivations.On this page a user may annotate the patterns andderivations however he or she wishes.
All grammarsettings and annotations are saved and availablethe next time the user logs on.
Finally on the DataDownload page, users may download data so thatthey can use the patterns and derivations offline.The derivations are stored as bracketed stringsrepresenting tree structure.
These are practicallyindecipherable by human users.
E.g.
:(CP[ILLOC Q][+FIN][+WH] "Adv[+TOPIC]" (Cbar[ILLOCQ] [+FIN][+WH][SLASH Adv](C[ILLOC Q][+FIN] "KA" )(IP[ILLOC Q][+FIN][+WH][SLASH Adv]"S" (Ibar[ILLOCQ][+FIN][+WH][SLASH Adv](I[ILLOCQ][+FIN]"Aux[+FIN]")(NegP[+WH] [SLASHAdv](NegBar[+WH][SLASH Adv](Neg "NOT")(VP[+WH][SLASH Adv](Vbar[+WH][SLASHAdv](V"Verb")"O1" "O2" (PP[+WH] "P" "O3[+WH]")"Adv[+NULL][SLASH Adv]"))))))))To be readable, the derivations are displayedgraphically as tree structures.
Towards this end wehave utilized a set of publicly available LaTexmacros: QTree (Siskind & Dimitriadis, [online]).
Aserver-side script parses the bracketed structuresinto the proper QTree/LaTex format from which apdf file is generated and subsequently sent to theuser's client application.Even with the graphical display, a simple sen-tence-by-sentence presentation is untenable giventhe large amount of linguistic data contained in thedatabase.
The Sentence Selection area allows usersto access the data filtered by sentence type and/orby grammar features (e.g.
all sentences that haveobligatory-wh movement and contain a preposi-tional phrase), as well as by the user?s definedgrammar(s) (all sentences that are "Italian-like").On the Data Download page, users may filtersentences as on the Sentence Selection page anddownload sentences in a tab-delimited format.
Theentire LDD may also be downloaded ?
approxi-mately 17 MB compressed, 600 MB as a raw asciifile.3 A Case Study: Evaluating the efficiencyof parameter-setting acquisition models.We have recently run experiments of sevenparameter-setting (P&P) models of acquisition onthe domain.
What follows is a brief discussion ofthe algorithms and the results of the experiments.We note in particular where results stemming fromwork with the LDD lead to conclusions that differfrom those previously reported.
We stress that thisis not intended as a comprehensive study ofparameter-setting algorithms or acquisitionalgorithms in general.
There is a large number ofmodels that are omitted; some of which are targetsof current investigation.
Rather, we present thestudy as an example of how the LDD could beeffectively utilized.In the discussion that follows we will use theterms ?pattern?, ?sentence?
and ?input?
inter-changeably to mean a left-to-right string of tokensdrawn from the LDD without its derivation.3.1 A Measure of FeasibilityAs a simple example of a learning strategy andof our simulation approach, consider a domain of 4binary parameters and a memoryless learner 3which blindly guesses how all 4 parameters shouldbe set upon encountering an input sentence.
Sincethere are 4 parameters, there are 16 possiblecombinations of parameter settings.
i.e., 16different grammars.
Assuming that each of the 16grammars is equally likely to be guessed, thelearner will consume, on average, 16 sentencesbefore achieving the target grammar.
This is onemeasure of a model?s efficiency or feasibility.3 By ?memoryless?
we mean that the learner processes inputsone at a time without keeping a history of encountered inputsor past learning events.However, when modeling natural languageacquisition, since practically all human learnersattain the target grammar, the average number ofexpected inputs is a less informative statistic thanthe expected number of inputs required for, say,99% of all simulation trials to succeed.
For ourblind-guess learner, this number is 72.4 We willuse this 99-percentile feasibility measure for mostdiscussion that follows, but also include theaverage number of inputs for completeness.3.2 The SimulationsIn all experiments:?
The learners are memoryless.?
The language input sample presented to thelearner consists of only grammatical sentencesgenerated by the target grammar.?
For each learner, 1000 trials were run for eachof the 3,072 target languages in the LDD.?
At any point during the acquisition process,each sentence of the target grammar is equallylikely to be presented to the learner.Subset Avoidance and Other Local Maxima:Depending on the algorithm, it may be the casethat a learner will never be motivated to change itscurrent hypothesis (Gcurr), and hence be unable toultimately achieve the target grammar (Gtarg).
Forexample, most error-driven learners will be trappedif Gcurr generates a language that is a superset ofthe language generated by Gtarg.
There is a wealthof learnability literature that addresses localmaxima and their ramifications.5 However, sinceour study?s focus is on feasibility (rather than onwhether a domain is learnable given a particularalgorithm), we posit a built-in avoidance mecha-nism, such as the subset principle and/or defaultvalues that preclude local maxima; hence, we setaside trials where a local maximum ensues.4 The average and 99-percentile figures (16 and 72) in thissection are easily derived from the fact that input consumptionfollows a hypergeometric distribution.5 Discussion of the problem of subset relationships amonglanguages starts with Gold?s (1967) seminal paper and isdiscussed in Berwick (1985) and Wexler & Manzini (1987).Detailed accounts of the types of local maxima that the learnermight encounter in a domain similar to the one we employ aregiven in Frank & Kapur (1996), Gibson & Wexler (1994), andNiyogi & Berwick (1996).3.3 The Learners' strategiesIn all cases the learner is error-driven: if Gcurr canparse the current input pattern, retain it.6The following refers to what the learner doeswhen Gcurr fails on the current input.?
Error-driven, blind-guess (EDBG): adopt anygrammar from the domain chosen at random ?not psychologically plausible, it serves as ourbaseline.?
TLA (Gibson & Wexler, 1994): change any oneparameter value of those that make up Gcurr.Call this new grammar Gnew.
If Gnew can parsethe current input, adopt it.
Otherwise, retainGcurr.?
Non-Greedy TLA (Niyogi & Berwick, 1996):change any one parameter value of those thatmake up Gcurr.
Adopt it.
(I.e.
there is no testingof the new grammar against the current input).?
Non-SVC TLA (Niyogi & Berwick, 1996): tryany grammar in the domain.
Adopt it only in theevent that it can parse the current input.?
Guessing STL (Fodor, 1998a): Perform astructural parse of the current input.
If a choicepoint is encountered, chose an alternative basedon one of the following and then set parametervalues based on the final parse tree:?
STL Random Choice (RC) ?
randomly pick aparsing alternative.?
Minimal Chain (MC) ?
pick the choice thatobeys the Minimal Chain Principle (De Vin-cenzi, 1991), i.e., avoid positing movementtransformations if possible.?
Local Attachment/Late Closure (LAC) ?pickthe choice that attaches the new word to thecurrent constituent (Frazier, 1978).The EDBG learner is our first learner of inter-est.
It is easy to show that the average and 99%scores increase exponentially in the number ofparameters and syntactic research has proposedmore than 100 (e.g.
Cinque, 1999).
Clearly, humanlearners do not employ a strategy that performs aspoorly as this.
Results will serve as a baseline tocompare against other models.6 We intend for a ?can-parse/can?t-parse outcome?
to beequivalent to the result from a language membership test.
Ifthe current input sentence is one of the set of sentencesgenerated by Gcurr, can-parse is engendered; if not, can?t-parse.99% AverageEDBG 16,663 3,589Table 2: EDBG, # of sentences consumedThe TLA: The TLA incorporates two searchheuristics: the Single Value Constraint (SVC) andGreediness.
In the event that Gcurr cannot parse thecurrent input sentence s, the TLA attempts asecond parse with a randomly chosen new gram-mar, Gnew, that differs from Gcurr by exactly oneparameter value (SVC).
If Gnew can parse s, Gnewbecomes the new Gcurr otherwise Gnew is rejected asa hypothesis (Greediness).
Following Berwick andNiyogi (1996), we also ran simulations on twovariants of the TLA ?
one with the Greedinessheuristic but without the SVC (TLA minus SVC,TLA?SVC) and one with the SVC but withoutGreediness (TLA minus Greediness, TLA?Greed).The TLA has become a seminal model and hasbeen extensively studied (cf.
Bertolo, 2001 andreferences therein; Berwick & Niyogi, 1996; Frank& Kapur, 1996; Sakas, 2000; among others).
Theresults from the TLA variants operating in theLDD are presented in Table 3.99% AverageTLA-SVC 67,896 11,273TLA-Greed 19,181 4,110TLA 16,990 961Table 3: TLA variants, # of sentences consumedParticularly interesting is that contrary to resultsreported by Niyogi & Berwick (1996) and Sakas &Nishimoto (2002), the SVC and Greedinessconstraints do help the learner achieve the target inthe LDD.
The previous research was based onsimulations run on much smaller 9 and 16 lan-guage domains (see Table 1).
It would seem thatthe local hill-climbing search strategies employedby the TLA do improve learning efficiency in theLDD.
However, even at best, the TLA performsless well than the blind guess learner.
We conjec-ture that this fact probably rules out the TLA as aviable model of human language acquisition.The STL: Fodor?s Structural Triggers Learner(STL) makes greater use of the parser than theTLA.
A key feature of the model is that parametervalues are not simply the standardly presumed 0 or1, but rather bits of tree structure or treelets.
Thus,a grammar, in the STL sense, is a collection oftreelets rather than a collection of 1's and 0's.
TheSTL is error-driven.
If Gcurr cannot license s, newtreelets will be utilized to achieve a successfulparse.7 Treelets are applied in the same way as any?normal?
grammar rule, so no unusual parsingactivity is necessary.
The STL hypothesizesgrammars by adding parameter value treelets toGcurr when they contribute to a successful parse.The basic algorithm for all STL variants is:1.
If Gcurr can parse the current input sentence,retain the treelets that make up Gcurr.2.
Otherwise, parse the sentence making use ofany or all parametric treelets available andadopt those treelets that contribute to a suc-cessful parse.
We call this parametric de-coding.Because the STL can decode inputs into theirparametric signatures, it stands apart from otheracquisition models in that it can detect when aninput sentence is parametrically ambiguous.During a parse of s, if more than one treelet couldbe used by the parser (i.e., a choice point isencountered), then s is parametrically ambiguous.The TLA variants do not have this capacitybecause they rely only on a can-parse/can?t-parseoutcome and do not have access to the on-lineoperations of the parser.
Originally, the ability todetect ambiguity was employed in two variationsof the STL: the strong STL (SSTL) and the weakSTL.The SSTL executes a full parallel parse of eachinput sentence and adopts only those treelets(parameter values) that are present in all thegenerated parse trees.
This would seem to makethe SSTL an extremely powerful, albeit psycho-logically implausible, learner.8 However, this is notnecessarily the case.
The SSTL needs someunambiguity to be present in the structures derivedfrom the sentences of the target language.
Forexample, there may not be a single input generatedby Gtarg that when parsed yields an unambiguoustreelet for a particular parameter.7 In addition to the treelets, UG principles are also availablefor parsing, as they are in the other models discussed above.8 It is important to note that Fodor (1998a) does not put forththe strong STL as a psychologically plausible model.
Rather, itis intended to demonstrate the potential effectiveness ofparametric decoding.Unlike the SSTL, the weak STL executes apsychologically plausible left-to-right serial(deterministic) parse.
One variant of the weakSTL, the waiting STL (WSTL), deals with ambigu-ous inputs abiding by the heuristic: Don?t learnfrom sentences that contain a choice point.
Thesesentences are simply discarded for the purposes oflearning.
This is not to imply that children do notparse ambiguous sentences they hear, but only thatthey set no parameters if the current evidence isambiguous.As with the TLA, these STL variants have beenstudied from a mathematical perspective (Bertoloet al, 1997a; Sakas, 2000).
Mathematical analysespoint to the fact that the strong and weak STL areextremely efficient learners in conducive domainswith some unambiguous inputs but may becomeparalyzed in domains with high degrees of ambigu-ity.
These mathematical analyses among otherconsiderations spurred a new class of weak STLvariants which we informally call the guessing STLfamily.The basic idea behind the guessing STL modelsis that there is some information available even insentences that are ambiguous, and some strategythat can exploit that information.
We incorporatethree different heuristics into the original STLparadigm, the RC, MC and LAC heuristicsdescribed above.Although the MC and LAC heuristics are notstochastic, we regard them as ?guessing?
heuristicsbecause, unlike the WSTL, a learner cannot becertain that the parametric treelets obtained from aparse guided by MC and LAC are correct for thetarget.
These heuristics are based on well-established human parsing strategies.
Interestingly,the difference in performance between the threevariants is slight.
Although we have just begun tolook at this data in detail, one reason may be thatthe typical types of problems these parsingstrategies address are not included in the LDD (e.g.relative clause attachment ambiguity).
Still, theSTL variants perform the most efficiently of thestrategies presented in this small study (approxi-mately a 100-fold improvement over the TLA).Certainly this is due to the STL's ability to performparametric decoding.
See Fodor (1998b) and Sakas& Fodor (2001) for detailed discussion about thepower of decoding when applied to the acquisitionprocess.GuessingSTL 99% AverageRC 1,486 166MC 1,412 160LAC 1,923 197Table 4: guessing STL family, # of sen-tences consumed4 Conclusion and future workThe thrust of our current research is directed atcollecting data for a comprehensive, comparativestudy of psycho-computational models of syntaxacquisition.
To support this endeavor, we havedeveloped the Language Domain Database ?
apublicly available test-bed for studying acquisitionmodels from diverse paradigms.Mathematical analysis has shown that learnersare extremely sensitive to various distributions inthe input stream (Niyogi & Berwick, 1996; Sakas,2000, 2003).
Approaches that thrive in one domainmay dramatically flounder in others.
So, whether aparticular computational model is successful as amodel of natural language acquisition is ultimatelyan empirical issue and depends on the exactconditions under which the model performs welland the extent to which those favorable conditionsare in line with the facts of human language.
TheLDD is a useful tool that can be used within suchan empirical research program.Future work: Though the LDD has been vali-dated against CHILDES data in certain respects,we intend to extend this work by adding distribu-tions to the LDD that correspond to actual distribu-tions of child-directed speech.
For example, whatpercentage of utterances, in child-directed Japa-nese, contain pro-drop?
object-drop?
How often inEnglish does the pattern: S[+WH] aux Verb O1occur and at what periods of a child's develop-ment?
We believe that these distributions will shedsome light on many of the complex subtletiesinvolved in ambiguity disambiguation and the roleof nondeterminism and statistics in the languageacquisition process.
This is proving to be aformidable, yet surmountable task; one that we arejust beginning to tackle.AcknowledgementsThis paper reports work done in part with othermembers of CUNY-CoLAG (CUNY's Computa-tional Language Acquisition Group) includingJanet Dean Fodor, Virginia Teller, Eiji Nishimoto,Aaron Harnley, Yana Melnikova, Erika Troseth,Carrie Crowther, Atsu Inoue, Yukiko Koizumi,Lisa Resig-Ferrazzano, and Tanya Viger.
Alsothanks to Charles Yang for much useful discussion,and valuable comments from the anonymousreviewers.
This research was funded by PSC-CUNY Grant #63387-00-32 and CUNY Collabora-tive Grant #92902-00-07.ReferencesBertolo, S.
(Ed.)
(2001).
Language Acquisition andLearnability.
Cambridge, UK: Cambridge UniversityPress.Bertolo, S., Broihier, K., Gibson, E., & Wexler, K.(1997a).
Characterizing learnability conditions forcue-based learners in parametric language systems.Proceedings of the Fifth Meeting on Mathematics ofLanguage.Bertolo, S., Broihier, K., Gibson, E., and Wexler, K.(1997b) Cue-based learners in parametric languagesystems: Application of general results to a recentlyproposed learning algorithm based on unambiguous'superparsing'.
In M. G. Shafto and P. Langley (eds.
)the Cognitive Science Society, Mahwah NJ: Law-rence Erlbaum Associates.Berwick, R. C., & Niyogi, P. (1996).
Learning fromtriggers.
Linguistic Inquiry, 27 (4), 605-622.Briscoe, T. (2000).
Grammatical acquisition: Inductivebias and coevolution of language and the languageacquisition device.
Language, 76 (2), 245-296.Chomsky, N. (1981) Lectures on Government andBinding, Dordrecht: Foris Publications.Chomsky, N. (1995) The Minimalist Program.
Cam-bridge MA: MIT Press.Cinque, G. (1999) Adverbs and Functional Heads.Oxford Oxford, UK:University Press, Oxford, UK.Fodor, J. D. (1998a)  Unambiguous triggers, LinguisticInquiry 29.1, 1-36.Fodor, J. D. (1998b) Parsing to learn.
Journal ofPsycholinguistic Research 27.3, 339-374.Fodor, J.D., Melnikova, Y.
& Troseth, E. (2002) Astructurally defined language domain for testingsyntax acquisition models.
Technical Report.
CUNYGraduate Center.Gibson, E. and Wexler, K. (1994) Triggers.
LinguisticInquiry 25, 407-454.Gold, E. M. (1967) Language identification in the limit.Information and Control 10, 447-474.Hyams, N. (1986) Language Acquisition and the Theoryof Parameters.
Dordrecht: Reidel.Jain, S., E. Martin, D. Osherson, J. Royer, and A.Sharma.
(1991) Systems That Learn.
2nd ed.
Cam-bridge, MA: MIT Press.Kayne, R. S. (1994) The Antisymmetry of Syntax.Cambridge MA: MIT Press.Kohl, K.T.
(1999)  An Analysis of Finite ParameterLearning in Linguistic Spaces.
Master?s Thesis, MIT.MacWhinney, B.
(1995) The CHILDES Project: Toolsfor Analyzing Talk.
(2nd ed.)
Hillsdale, NJ: LawrenceErlbaum Associates.Niyogi, P (1998) The Informational Complexity ofLearning: Perspectives on Neural Networks andGenerative Grammar Dordrecht: Kluwer Academic.Pinker, S. (1979) Formal models of language learning,Cognition 7, 217-283.Sagae, K., Lavie, A., MacWhinney, B.
(2001) Parsingthe CHILDES database: Methodology and lessonslearned.
In Proceedings of the Seventh InternationalWorkshop in Parsing Technologies.
Beijing, China.Sakas, W.G.
(in prep) Grammar/Language smoothnessand the need (or not) of syntactic parameters.
HunterCollege and The Graduate Center, City University ofNew York.Sakas, W.G.
(2000) Ambiguity and the ComputationalFeasibility of Syntax Acquisition, Doctoral Disserta-tion, City University of New York.Sakas, W.G.
and Fodor, J.D.
(2001).
The StructuralTriggers Learner.
In S. Bertolo (ed.)
Language Ac-quisition and Learnability.
Cambridge, UK: Cam-bridge University Press.Sakas, W.G.
and Nishimoto, E. (2002) Search, Structureor Statistics?
A Comparative Study of MemorylessHeuristics for Syntax Acquisition, Proceedings of the24th Annual  Conference  of the Cognitive ScienceSociety.
Hillsdale NJ: Lawrence  Erlbaum Associ-ates,Siskind, J.M & Dimitriadis, A., [Online 5/20/2003]Documentation for qtree, a LaTex tree packagehttp://www.ling.upenn.edu/advice/latex/qtree/Villavicencio, A.
(2000) The use of default unificationin a system of lexical types.
Paper presented at theWorkshop on Linguistic Theory and Grammar Im-plementation, Birmingham,UK.Wexler, K. and Culicover, P. (1980) Formal Principlesof Language Acquisition.
Cambridge MA: MITPress.
