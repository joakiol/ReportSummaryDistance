Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 668?672,Dublin, Ireland, August 23-24, 2014.tucSage: Grammar Rule Induction for Spoken Dialogue Systems viaProbabilistic Candidate SelectionArodami Chorianopoulou?, Georgia Athanasopoulou?, Elias Iosif?
?,Ioannis Klasinas?, Alexandros Potamianos?
?School of ECE, Technical University of Crete, Chania 73100, Greece?School of ECE, National Technical University of Athens, Zografou 15780, Greece??Athena?
Research Center, Marousi 15125, Greece{achorianopoulou,gathanasopoulou,iklasinas}@isc.tuc.griosife@telecom.tuc.gr, apotam@gmail.comAbstractWe describe the grammar induction sys-tem for Spoken Dialogue Systems (SDS)submitted to SemEval?14: Task 2.
A sta-tistical model is trained with a rich fea-ture set and used for the selection of can-didate rule fragments.
Posterior probabil-ities produced by the fragment selectionmodel are fused with estimates of phrase-level similarity based on lexical and con-textual information.
Domain and languageportability are among the advantages ofthe proposed system that was experimen-tally validated for three thematically dif-ferent domains in two languages.1 IntroductionA critical task for Spoken Dialogue Systems(SDS) is the understanding of the transcribed userinput, that utilizes an underlying domain grammar.An obstacle to the rapid deployment of SDS tonew domains and languages is the time-consumingdevelopment of grammars that require human ex-pertise.
Machine-assisted grammar induction hasbeen an open research area for decades (K. Lariand S. Young, 1990; S. F. Chen, 1995) aimingto lower this barrier.
Induction algorithms canbe broadly distinguished into resource-based, e.g.,(A.
Ranta, 2004), and data-driven, e.g., (H. Mengand K.-C. Siu, 2002).
The main drawback ofthe resource-based paradigm is the requirement ofpre-existing knowledge bases.
This is addressedby the data-driven paradigm that relies (mostly)on plain corpora.
SDS grammars are built by uti-lizing low- and high-level rules.
Low-level rulesThis work is licenced under a Creative Commons Attri-bution 4.0 International License.
Page numbers and pro-ceedings footer are added by the organizers.
License de-tails: http://creativecommons.org/licenses/by/4.0/are similar to gazetteers consisting of terminal en-tries, e.g., list of city names.
High-level rules canbe lexicalized as textual fragments (or chunks),which are semantically defined on top of low-level rules, e.g., ?depart from <City>?.The data-driven induction of low-level rules is awell-researched area enabled by various technolo-gies including web harvesting for corpora creation(Klasinas et al., 2013), term extraction (K. Frantziand S. Ananiadou, 1997), word-level similaritycomputation (Pargellis et al., 2004) and cluster-ing (E. Iosif and A. Potamianos, 2007).
High-levelrule induction is a less researched area that posestwo main challenges: 1) the extraction and selec-tion of salient candidate fragments from a corpusthat convey semantics relevant to the domain of in-terests and 2) the organization of such fragments(e.g., via clustering) according to their semanticsimilarity.
Despite the recent interest on phrase (J.Mitchell and M. Lapata, 2010) and sentence simi-larity, each respective problem remains open.Next, our submission1for the Se-mEval?14: Task2 is briefly described, whichconstitutes a data-driven approach for inducinghigh-level SDS grammar rules.
At the system?score lies a statistical model for the selection oftextual fragments based on a rich set of features.This set includes various lexical features, aug-mented with statistics from n-gram languagemodels, as well as with heuristic features.
Thecandidate selection model posterior is fusedwith a phrase-level semantic similarity metric.Two different approaches are used for similaritycomputation relying on the overlap of characterbigrams or context-based similarity accordingto the distributional hypothesis of meaning.The domain and language portability of theproposed system is demonstrated by its successfulapplication across three different domains and1Please note that the last three authors of this submissionare among the organizers of this task.668two languages.
All the four subtasks defined bythe organizers were completed with very goodperformance that exceeds the baseline.2 System DescriptionThe basic functionality of the proposed systemis the mapping (assignment) of unknown textualfragments into known high-level grammar rules.Let E be the set of unknown fragments, while theset of known rules is denoted byR.
Each unknownfragment f ?E is allowed to be mapped to a sin-gle high-level rule rs?R, where 1?
s?
m andm is the total number of rules in the grammar.Figure 1: Overview of system architecture.The system consists of three major components asshown at the system architecture diagram in Fig.1, specifically: 1) candidate selection: a set ofclassifiers is built, one for each rsto select whetherf ?
E is a candidate member of the specific rule2,2) similarity computation between f and rs, and3) mapping f to a high-level rule rs(denoted asf 7?
rs) according to the following model:argmaxs{p(rs|f)wS(f, rs)} : f 7?
rs(1)where p(rs|f) stands for the probability of fbelonging to rule rsand it is estimated via therespective classifier.
The similarity betweenf and rsis denoted by S(f |rs), while w isa fixed weight taking values in the interval[0 ?).
The fusion weight w controls the rela-tive importance of the candidate selection andsemantic similarity modules, e.g., for w = 0only the similarity metric S(f, rs) is used in thedecision.
For example, consider the fragment f?leaving <City>?.
Also, assume two high-level rules, namely, <ArrCity>={?arrive2The requirement for building a classifier for each gram-mar rule is realistic for the case of SDS, especially for the typ-ical iterative human-in-the-loop grammar development sce-nario.at <City>?,...}
and <DepCity>={?depart <City>?,...}.
According to (1)f is mapped to the <DepCity> rule.2.1 Candidate SelectionIn this section, the features used for building thecandidate selection module for each rs?
R arebriefly described.
Given a pair (f ,rs) a two-classstatistical classification model that corresponds torsis used for estimating p(rs|f) in (1).Definitions.
A high-level rule rscan be con-sidered as a set of fragments, e.g.,?depart<City>?, ?leaving <City>?.
For eachfragment there are two types of constituents,namely, lexical (e.g., ?depart?,?leaving?
)and low-level rules (e.g., ?<City>?).
The fol-lowing features are extracted for rsconsidering itsrespective fragments, as well as for f .Shallow features.
1) the number of constituents(i.e., tokens), 2) the count of lexical constituentsto the number of tokens, 3) the count of low-levelrules to the number of tokens, 4) the count of lex-ical constituents that follow the right-most low-level rule of the fragment, and 5) the count of low-level rules that appear twice in a fragment.Perplexity-based features.
A fragment?f canbe represented as a sequence of tokens asw1w2... wz.
The perplexity of?f is defined asPP (?f)=2H(?f), where H(?f)=1zlog(p(?f)).
p(?f)stands for the probability of?f estimated using ann-gram language model.
Two PP values wereused as features computed for n=2, 3.Features of lexical similarity.
Four scores of lex-ical similarity computed between f and rswereused as features.
Let Nsdenote the set of frag-ments that are included in the training set of eachrule rs.
The following metrics were employedfor computing the similarity between the unknownfragment f and a fragment fs?
Ns: 1) the nor-malized longest common subsequence (Stoilos etal., 2005) denoted as SC, 2) the normalized over-lap in character bigrams that is denoted as SBandit is defined in (2), 3) a proposed variation of theLevenshtein distance, SL, defined as SL(f, fs) =l1?L(f,fs)l1+d, where l1and l2are the lengths (in char-acters) of the lengthiest and the shortest fragmentbetween f and fs, respectively, while d= l1?
l2.L(.)
stands for the Levenshtein distance (V. I. Lev-enshtein, 1966; R. A. Wagner and M. J. Fischer,1974).
4) if f and fsdiffer by one token exactlySLis applied, otherwise their similarity is set to0.
Regarding SCand SB, the similarity between669f and rswas estimated as the maximum similarityyielded when computing the similarities betweenf and each fs?Ns.
For the rest metrics, the sim-ilarity between f and rswas estimated by averag-ing the |Ns| similarities computed between f andeach fs?Ns.Heuristic features.
Considering an unknownfragment f and the set of training fragments Nscorresponding to rule rs, in total nine featureswere used: 1) the difference between the aver-age length (in tokens) of fragments in Nsand thelength of f , 2) the difference between the averagenumber of low-level rules in Nsand the numberof low-level rules in f , 3) as 2) but consideringthe lexical constituents instead of low-level rules,4) the number of low-level rules shared betweenNsand f , 5) as 4) but considering the lexical con-stituents instead of low-level rules, 6) a booleanfunction that equals 1 if f is a substring of at leastone fs?
Ns, 7) a boolean function that equals 1 iff shares the same lexical constituents at least onefs?
Ns, 8) a boolean function that equals 1 if fis shorter by one token compared to any fs?
Ns,9) a boolean function that equals 1 if f is lengthierby one token compared to any fs?
Ns.Selection.
The aforementioned features are usedfor building a binary classifier for each rs?
R,where 1 ?
s ?
m, for deciding whether f canbe regarded as a candidate member of rsor not.Given an unknown fragment f these classifiers areemployed for estimating in total m probabilitiesp(rs|f).2.2 Similarity MetricsHere, two types of similarity metrics are defined,which are used for estimating S(f, rs) in (1).String-based similarity.
Consider two fragmentsfiand fjwhose sets of character bigrams are de-noted as Miand Mj, respectively.
Also, Mmin=min(|Mi|, |Mj|) and Mmax= max(|Mi|, |Mj|).
The similarity between fiand fjis based onthe overlap of their respective character bigramsdefined as (Jimenez et al., 2012):SB(fi, fj) =|Mi?Mj|?Mmax+ (1?
?
)Mmin, (2)where 0???
1, while, here we use ?=0.5.
Thesimilarity between a fragment f and a rule rsiscomputed by averaging the similarities computedbetween f and each fs?Ns.Context-based similarity.
This is a corpus-basedmetric relying on the distributional hypothesis ofmeaning suggesting that similarity of context im-plies similarity of meaning (Z. Harris, 1954).
Acontextual window of size 2K+1 words is cen-tered on the fragment of interest fiand lexicalfeatures are extracted.
For every instance of fiinthe corpus the K words left and right of fifor-mulate a feature vector vi.
For a given value of Kthe context-based semantic similarity between twofragments, fiand fj, is computed as the cosine oftheir feature vectors: SK(fi, fj) =vi.vj||vi|| ||vj||.
Theelements of feature vectors can be weighted ac-cording various schemes (E. Iosif and A. Potami-anos, 2010), while, here we use a binary scheme.The similarity between a fragment f and a rulersis computed by averaging the similarities com-puted between f and each fs?Ns.2.3 Mapping of Unknown FragmentsThe output of the described system is the mappingof a fragment f to a single (i.e., one-to-one assign-ment) high-level rule rs?
R, where 1 ?
s ?
m.This is achieved by applying (1).
The p(rs|f)probabilities were estimated as described in Sec-tion 2.1.
The S(f, rs) similarities were estimatedusing either SKor SBdefined in Section 2.2.3 Datasets and ExperimentsDatasets.
The data was organized with respect tothree different domains: 1) air travel (flight book-ing, car rental etc.
), 2) tourism (information forcity guide), and 3) finance (currency exchange).
Intotal, there are four separate datasets: two datasetsfor the air travel domain in English (EN) andGreek (GR), one dataset for the tourism domainin English, and one dataset for the finance domainin English.The number of high-level rules for each datasetDomain #rules #train frag.
#test frag.Travel:EN 32 982 284Travel:GR 35 956 324Tourism:EN 24 1004 285Finance:EN 9 136 37Table 1: Number of rules and train/test fragments.are shown in Table 1, along with the numberof fragments included in training and test data.Experiments.
Regarding the computation ofperplexity-based features (defined in Section 2.1)the SRILM toolkit (A. Stolcke, 2002) was used.The n-gram probabilities were estimated over acorpus that was created by aggregating all the670valid fragments included in the training data.For the computation of the context-based similar-ity metric SK(defined in Section 2.2) a corpusof web-harvested data was created for each do-main/language.
The context window size K wasDomain # sentencesTravel:EN 5721Travel:GR 6359Tourism:EN 829516Finance:EN 168380Table 2: Size of corpora used in SKmetric.set to 1.
The size of the used corpora are presentedTable 2, while the process of corpus creation isdetailed in (Klasinas et al., 2013).
The classifiersused for the candidate selection module, describedin Section 2.1 were random forests with 50 trees(L. Breiman, 2001).4 Evaluation Metrics and ResultsThe proposed model defined by (1) was evaluatedin terms of weighted F-measure, (FM ).
Initially,we run our system using the training and develop-ment set provided by the task organizers, in orderto tune the w and K parameters.
The tuning wasconducted on the Travel English domain, while therespective evaluation results are shown in Table 3in terms of FM .
We observe that the best re-Weight w 0 1 50 500FM 0.68 0.72 0.70 0.72Table 3: Results for the tuning of w.sults are achieved for w = 1 and w = 500.
Inthe case where w = 0 the rule mapping relies onlyon the similarity metric.
In addition, we exper-imented with various values the context windowsize K of the context-based similarity metric SK:K = 1, 3, 7.
For all values of K similar perfor-mance was obtained (0.70).
Given the aforemen-Domains Baseline Run 1 Run 2 Run 3Travel:EN 0.51 0.66 0.65 0.68Travel:GR 0.26 0.52 0.49 0.49Tourism:EN 0.87 0.86 0.85 0.86Finance:EN 0.60 0.70 0.63 0.58UA 0.56 0.69 0.66 0.65WA 0.52 0.66 0.64 0.65Table 4: Official results.tioned tuning the following values were selectedfor the official runs: w = 1, w = 500 and K = 1.In total, three system runs were submitted:Run 1.
The character bigram similarity metric wasused, while w was set to 1.Run 2.
The context-based similarity metrics wasused with K = 1, while w was set to 1.Run 3.
The character bigram similarity metric wasused, while w was set to 500.The results for the aforementioned runs, alongwith the baseline performance are shown in Ta-ble 4.
An overview of the participating systemssuggests that our submission achieved the high-est performance for almost all domains and lan-guages.
The weighted (WA) and unweighted (UA)average across the 4 datasets are also presented,where the weight depends on the number of rulesin the dataset.
Using these measures, our mainrun (Run 1) obtained the best results.
We ob-serve that the performance is consistently worsefor Runs 2 and 3, with the exception of the TravelEnglish dataset.
Comparing the performance ofRuns 1 and 2, we observe that the character bigrammetric consistently outperforms the context-basedone.
For individual datasets, our system underper-forms for the Finance (in Run 3) and the Tourismdomain (in all Runs).
For the case of the Financedomain this may be attributed to the relatively lim-ited training data.5 ConclusionsWe proposed a supervised grammar induction sys-tem using the fusion of a grammar fragment se-lection and similarity estimation modules.
Thebest configuration of our system was Run 1 whichachieved the highest performance compared toother submissions, in almost all domains.
To sum-marize, 1) the selection module boost the sys-tem?s performance significanlty, 2) the high per-formance in different domains is a promising indi-cator for domain and language portability.
Futurework should involve the implementation of morecomplex features for the candidate selection algo-rithm and further investigation of phrase level sim-ilarity metrics.AcknowledgementsThis work has been partially funded by theprojects: 1) SpeDial, and 2) PortDial, supportedby the EU Seventh Framework Programme (FP7),with grant number 611396 and 296170, respec-tively.671ReferencesElias Iosif and Alexandros Potamianos.
2010.
Un-supervised semantic similarity computation betweenterms using web documents.
IEEE Transactions onKnowledge and Data Engineering, 22(11), pp.
1637-1647.Sergio Jimenez, Claudia Becerra and Alexander Gel-bukh.
2012.
Soft Cardinality: A parameterized sim-ilarity function for text comparison.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics (*SEM), pp.
449-453Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,Spyros Georgiladakis and Gianluka Mameli.
2013.Web data harvesting for speech understandinggrammar induction.
in Proceedings of the Inter-speech.Helen M. Meng and Kai-Chung Siu 2002.
Semi-automatic acquisition of semantic structures forunderstanding domain-specific natural languagequeries.
IEEE Transactions on Knowledge and DataEngineering, 14(1), pp.
172-181.PortDial Project free data deliverable D3.1.https://sites.google.com/site/portdial2/deliverables-publicationAndreas Stolcke 2002 Srilm-an extensible languagemodeling toolkit in Proceedings of the Interspeech2002Karim Lari and Steve J.
Young 2002.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language,4(1), pp.
35-56.Stanley F. Chen 1995.
Bayesian grammar inductionfor language modeling.
in Proceedings of the 33rdannual meeting of ACLZellig Harris 1954.
Distributional structure.
Word,10(23), pp.
146-162.Rebecca Hwa 1999.
Supervised grammar inductionusing training data with limited constituent informa-tion.
in Proceedings of the 37th annual meeting ofACLMatthew Lease, Eugene Charniak, and Mark Johnson2005.
Parsing and its applications for conversa-tional speech.
in Proceedings of Acoustics, Speech,and Signal Processing (ICASSP)Vladimir I. Levenshtein 1966.
Binary codes capableof correcting deletions, insertions and reversals.
inSoviet physics doklady, 10(8), pp.
707-710.Leo Breiman 2001.
Random forests.
in MachineLearning, 45(1), pp.
5-32.Dan Jurafsky and James H. Martin 2009.
Speechand language processing an introduction to naturallanguage processing, computational linguistics, andspeech.
Pearson Education IncGiorgos Stoilos, Giorgos Stamou, and Stefanos Kollias2005.
A string metric for ontology alignment.
inThe Semantic WebISWC, pp.
624637Robert A. Wagner and Michael J. Fisher 1974.
Thestring-to-string correction problem.
Journal of theACM (JACM), 21(1), pp.
168-173Katerina Frantzi and Sophia Ananiadou 1997.
Au-tomatic term recognition using contextual cues.
inProceedings of International Joint Conferences onArtificial IntelligenceElias Iosif and Alexandros Potamianos 2007.
A soft-clustering algorithm for automatic induction of se-mantic classes.
in Proceedings of InterspeechJeffrey Mitchell and Mirela Lapata 2010.
Composi-tion in distributional models of semantics.
CognitiveScience, 34(8):1388-1429.Ye-Yi Wang and Alex Acero 2006.
Rapid develop-ment of spoken language understanding grammars.Speech Communication, 48(3), pp.
360-416.Eric Brill 1992.
A simple rule-based part of speechtagger.
in Proceedings of the workshop on Speechand Natural LanguageAlexander Clark 2001.
Unsupervised inductionof stochastic context-free grammars using distribu-tional clustering.
in Proceedings of the 2001 work-shop on Computational Natural Language LearningBenjamin Snyder, Tahira Naseem, and Regina Barzilay2009.
Unsupervised multilingual grammar induc-tion.
in Proceedings of the Joint Conference of the47th Annual Meeting of the ACLAarne Ranta 2009.
Grammatical framework: A type-theoretical grammar formalism.
Journal of Func-tional Programming: 14(2), pp.
145-189Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee,Alexandros Potamianos and Augustine Tsai 2009.Auto-induced Semantic Classes.
Speech Communi-cation: 43(3), pp.
183-203Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre 2012.
SemEval-2012 Task 6: APilot on Semantic Textual Similarity.
in Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics (*Sem), pp.
385-393672
