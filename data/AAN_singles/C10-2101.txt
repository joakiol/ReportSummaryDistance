Coling 2010: Poster Volume, pages 876?884,Beijing, August 2010Semantic Classification of Automatically Acquired Nounsusing Lexico-Syntactic CluesYugo MurawakiGraduate School of InformaticsKyoto Universitymurawaki@nlp.kuee.kyoto-u.ac.jpSadao KurohashiGraduate School of InformaticsKyoto Universitykuro@i.kyoto-u.ac.jpAbstractIn this paper, we present a two-stage ap-proach to acquire Japanese unknown mor-phemes from text with full POS tags as-signed to them.
We first acquire unknownmorphemes only making a morphology-level distinction, and then apply semanticclassification to acquired nouns.
One ad-vantage of this approach is that, at the sec-ond stage, we can exploit syntactic cluesin addition to morphological ones becauseas a result of the first stage acquisition, wecan rely on automatic parsing.
Japanesesemantic classification poses an interest-ing challenge: proper nouns need to bedistinguished from common nouns.
Itis because Japanese has no orthographicdistinction between common and propernouns and no apparent morphosyntacticdistinction between them.
We explorelexico-syntactic clues that are extractedfrom automatically parsed text and inves-tigate their effects.1 IntroductionA dictionary plays an important role in Japanesemorphological analysis, or the joint task ofsegmentation and part-of-speech (POS) tag-ging (Kurohashi et al, 1994; Asahara and Mat-sumoto, 2000; Kudo et al, 2004).
Like Chi-nese and Thai, Japanese does not delimit wordsby white-space.
This makes the first step of nat-ural language processing more ambiguous thansimple POS tagging.
Accordingly, morphemes ina pre-defined dictionary compactly represent ourknowledge about both segmentation and POS.One obvious problem with the dictionary-basedapproach is caused by unknown morphemes,or morphemes not defined in the dictionary.Even though, historically, extensive human re-sources were used to build high-coverage dictio-naries (Yokoi, 1995), texts other than newspa-per articles, in particular web pages, contain alarge number of unknown morphemes.
These un-known morphemes often cause segmentation er-rors.
For example, morphological analyzer JU-MAN 6.01 wrongly segments the phrase ???????
(saQporo eki, ?Sapporo Station?
), where ??????
(saQporo) is an unknown morpheme,as follows:???
(sa, noun-common, ?difference?),???
(Q, UNK), ???
(po, UNK),???
(ro, noun-common, ?sumac?)
and???
(eki, noun-common, ?station?
),where UNK refers to unknown morphemes auto-matically identified by the analyzer.
Such an er-roneous sequence has disastrous effects on appli-cations of morphological analysis.
For example, itcan hardly be identified as a LOCATION in namedentity recognition.One solution to the unknown morpheme prob-lem is unknown morpheme acquisition (Mori andNagao, 1996; Murawaki and Kurohashi, 2008).
Itis the task of automatically augmenting the dictio-nary by acquiring unknown morphemes from text.In the above example, the goal is to acquire themorpheme ??????
(saQporo) with the POStag ?noun-location name.?
However, unknownmorpheme acquisition usually adopts a coarserPOS tagset that only represents the morphologylevel distinction among noun, verb and adjective.This means that ??????
(saQporo) is acquiredas just a noun and that the semantic label ?loca-tion name?
remains to be assigned.
The reasononly the morphology level distinction is made is1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html876that the semantic level distinction cannot easilybe captured with morphological clues that are ex-ploited in unknown morpheme acquisition.In this paper, we investigate the remainingproblem and introduce the new task of seman-tic classification that is to be applied to automat-ically acquired nouns.
In this task, we can ex-ploit syntactic clues in addition to morphologi-cal ones because, as a result of acquisition, wecan now rely on automatic parsing.
For exam-ple, since text containing ??????
(saQporo,noun-unclassified) is correctly segmented, we canextract not only the phrase ?saQporo station,?
butthe tree fragment ??
go to saQporo,?
and we candetermine its semantic label.Japanese semantic classification poses an inter-esting challenge: proper nouns need to be distin-guished from common nouns.
Like Chinese andThai, Japanese has no orthographic distinction be-tween common and proper nouns as there is nosuch thing as capitalization.
In addition, thereseems no morphosyntactic (i.e.
grammatical) dis-tinction between them.In this paper, we explore lexico-syntactic cluesthat can be extracted from automatically parsedtext.
We train a classification model on manuallyregistered nouns and apply it to automatically ac-quired nouns.
We then investigate the effects oflexico-syntactic clues.2 Semantic Classification Task2.1 Two-Stage Approach to UnknownMorpheme AcquisitionOur goal is to identify unknownmorphemes in un-segmented text and assign POS tags to them.
Inthis section, we omit the details of boundary iden-tification (segmentation) and review the JapanesePOS tagset to see why we propose a two-stage ap-proach to assign full POS tags.The Japanese POS tagset derives from tradi-tional grammar.
It is a mixture of several linguis-tic levels: morphology, syntax and semantics.
Inother words, information encoded in a POS tagis more than how the morpheme behaves in a se-quence of morphemes.
In fact, POS tags given topre-defined morphemes are useful for applicationsof morphological analysis, such as dependencyparsing (Kudo and Matsumoto, 2002), named en-tity recognition (Asahara and Matsumoto, 2003;Sasano and Kurohashi, 2008) and anaphora res-olution (Iida et al, 2009; Sasano and Kurohashi,2009).
In these applications, POS tags are incor-porated as features for models.On the other hand, the mixed nature of the POStagset poses a challenge to unknown morphemeacquisition.
Previous approaches (Mori and Na-gao, 1996; Murawaki and Kurohashi, 2008) di-rectly or indirectly reply on morphology, or ourknowledge on how a morpheme behaves in a se-quence of morphemes.
This means that semanticlevel distinction is difficult to make in these ap-proaches, and in fact, is left unresolved.
To bespecific, nouns are only distinguished from verbsand adjectives but they have subcategories in theoriginal tagset.
These are what we try to classifyacquired nouns into in this paper.2.2 Semantic LabelsThe Japanese noun subcategories may require anexplanation since they are different from the En-glish ones (Marcus et al, 1993) in many re-spects.
Singular and mass nouns are not distin-guished from plural nouns because Japanese hasno grammatical distinction between them.
Moreimportantly for this paper, proper nouns have sub-categories such as person name, location nameand organization name in addition to the distinc-tion from common nouns.
These subcategoriesprovide important information to named entityrecognition among other applications.
For propernouns, we adopt these subcategories as semanticlabels in our task.In contrast to proper nouns, common nounshave only one subcategory ?common.?
How-ever, we consider that subcategories of commonnouns similar to those of proper nouns are use-ful for, for example, anaphora resolution (Sasanoand Kurohashi, 2009).
We adopt the ?categories?of morphological analyzer JUMAN, with whichcommon nouns in its dictionary are annotated.There are 22 ?categories?
including PERSON,ORGANIZATION and CONCEPT.
We collapsethese ?categories?
into coarser semantic labelsthat roughly correspond to those for proper nouns.To sum up, we define 9 semantic labels as shown877Table 1: List of semantic labels.labels P/C sources1 manually registered nouns automatically acquired nounsPSN-PpropersubPOS:person name ??
(matsui, a surname) ???
(sayuri, a given name)????
(jo?ji, ?George?)
???
(kyoN, a nickname)LOC-P subPOS:place name ??
(kyouto, ?Kyoto?)
???
(akiba, ?Akihabara?)???
(doitsu, ?Germany?)
????
(waikiki, ?Waikiki?
)ORG-P subPOS:organization name ??
(nichigin, a bank) ???
(matsuda, ?Mazda?
)NHK (a broadcaster) ???
(yahu?, ?Yahoo?
)OTH-P subPOS:proper noun ??
(heisei, an era name) ????
(jipush?
?, ?Gypsy?)???
(surabu, ?Slav?
)PSN-Ccommoncategory:PERSON ??
(seNsei, ?teacher?)
???
(merutomo, ?keypal?)????
(sutaQfu, ?staff?)
???
(n?
?to, ?NEET?
)LOC-C category:PLACE-?2 ??
(shokuba, ?office?)
???
(irori, ?hearth?)???
(kafe, ?cafe?)
??
(hojou, ?farm field?
)ORG-C category:ORGANIZATION ??
(seifu, ?government?)
???
(me?ka, ?manufacturer?)???
(ch?
?mu, ?team?)
??
(heisho, ?our office?
)ANI-C category:ANIMAL and ?
(inu, ?dog?)
???
(chiwawa, ?Chihuahua?
)category:ANIMAL-PART ?
(kao, ?face?)
???
(maNta, ?manta?
)OTH-C other categories ??
(shuchou, ?argument?)
??
(jiNbei, a kind of clothing)?
(makura, ?pillow?)
???
(chakumero, ?ringtone?
)1 A subPOS refers to a subcategory of noun.
For example, PSN-P corresponds to the POS tag ?noun-person name?.2 category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others.in Table 1.2.3 Related TasksA line of research is dedicated to identify un-known morphemes with varying degrees of identi-fication.
Asahara and Matsumoto (2004) only fo-cus on boundary identification (segmentation) ofunknown morphemes.
Mori and Nagao (1996),Nagata (1999) and Murawaki and Kurohashi(2008) assign POS tags at the morphology level.Uchimoto et al (2001) assign full POS tags butunsurprisingly the accuracy is low.
Nakagawaand Matsumoto (2006) also assign full POS tags.They address the fact that local information usedin previous studies is inherently insufficient andpresent a method that uses global information,in other words, takes into consideration all oc-currences of each unknown word in a document.They report an improvement in tagging propernouns in Japanese.A related task is named entity recognition(NER).
It can handle a named entity longer thana single morpheme and is usually formalized as achunking problem.
Since Japanese does not de-limit words by white-space, the unit of chunk-ing can be a character (Asahara and Matsumoto,2003; Kazama and Torisawa, 2008) or a mor-pheme (Sasano and Kurohashi, 2008).
In eithercase, NER models encode the output of morpho-logical analysis and therefore are affected by itserrors.
In fact, Saito et al (2007) report that a ma-jority of unknown named entities (those never ap-pear in a training corpus) contain unknown mor-phemes as their constituents and that NER modelsperform poorly on them.
A straightforward solu-tion to this problem would be to acquire unknownmorphemes and to assign semantic labels to them.Another related task is supersense tagging (Cia-ramita and Johnson, 2003; Curran, 2005; Cia-ramita and Altun, 2006).
A supersense corre-sponds to one of the 26 broad categories definedby WordNet (Fellbaum, 1998).
Each noun synsetis associated with a supersense.
For example,?chair?
has supersenses PERSON, ARTIFACTand ACT because it belongs to several synsets.Since supersense tagging is studied in English,it differs from our task in several respects.
In En-glish, the distinction between common and propernouns is clear.
In fact, the tagging models can usePOS features even for unknown nouns.
In addi-tion, the syntactic behavior of English nouns isdifferent from that of Japanese nouns (Gil, 1987).Definiteness is not marked in Japanese as it lacksdeterminers (e.g.
?the?
and ?a?
), and Japanese hasno obligatory plural marking.
On the other hand,Japanese obligatorily uses numeral classifiers toindicate the count of nouns, as in(1) saNthreesatsuCLnoGENhoNbookthree volumes of books, or three books,878where ?satsu?
is a numeral classifier for books.
Anumber together with its numeral classifier formsa numeral quantifier.
Numeral quantifiers wouldbe informative about the semantic categories ofnouns.
Note that Japanese shares the above fea-tures with Chinese and Thai.
Our findings in thispaper may hold for these languages.3 Proposed Method3.1 Lexico-Syntactic CluesIn the task of semantic classification, we can ex-ploit syntactic clues in addition to morpholog-ical ones.
As a result of unknown morphemeacquisition, text containing acquired morphemes,or former unknown morphemes, is correctly seg-mented.
Now we can treat automatic parsing as(at least partly) reliable with regard to acquiredmorphemes.For noun X , we use the following sets of fea-tures for classification.call: noun phrase Y that appears in a pat-tern like ?Y called X?
and ?Y such as X ,?
e.g.?call:kuni?
fromXXtoQTiucallkunicountrya country called X .cf: predicate with a case marker with which ittakes X as an argument, e.g.
?cf:tooru:wo?
fromXXwoACCtoorupass?
pass through X .demo: demonstrative that modifies X , e.g.?demo:kono?
from ?kono X?
(this X) and?demo:doNna?
from ?doNna X?
(what kind ofX).ncf1: noun phrase which X modifies with thegenitive case marker ?no,?
e.g.
?ncf1:heya?
fromXXnoGENheyaroomX?s room.ncf2: noun phrase that modifies X with thegenitive case marker ?no,?
e.g.
?ncf2:subete?fromsubeteallnoGENXXall X .suf: suffix or suffix-like noun that follows X ,e.g.
?suf:saN?
from ?X saN?
(Mr./Ms.
X) and?suf:eki?
from ?X eki?
(X station).Using automatically parsed text to extract syn-tactic features has an advantage.
Since no manualannotation is necessary, we can utilize a huge rawcorpus.
On the other hand, parsing errors are in-evitable.
However, we can circumvent this prob-lem by using the constraints of Japanese depen-dency structures: head-final and projective.
Thesimplest example is the second last element of asentence, which always depends on the last ele-ment.
With these constraints, we can focus onsyntactically unambiguous dependency pairs andextract syntactic features accurately.
We followKawahara and Kurohashi (2001) to extract a pairof an argument noun and a predicate (cf), andSasano et al (2004) to extract a pair of nouns con-nected with the genitive case marker ?no?
(ncf1and ncf2).Noun X can be part of a compound noun.
Weleave it for named entity recognition.
Except forsuf, we extract features only when X alone formsa word.
Similarly, we extract suf features onlywhen X and a suffix alone form a noun phrase.For call, ncf1, and ncf2, we generalizenumerals within noun phrases.
For ?hoN?
(book) in example 1, we extract the feature?ncf2:<NUM>satsu.
?3.2 Instances for ClassificationNow that features are extracted for each noun, thequestion is how to combine them together to makean instance for classification.
One factor we needto consider is polysemy: a noun can be a personname in one context and a location name in an-other.
If we combine features extracted from thewhole corpus, they may represent several seman-tic labels.Modeling a mixture of semantic labels mightbe a solution, but we do not take this approach onthe grounds that each occurrence of a noun corre-sponds to a single semantic label.In our strategy, we perform classification mul-tiple times for each noun and aggregate the resultsat the end.
The features for each classification areextracted from a relatively small subset of a cor-pus where the noun is supposedly consistent in879terms of semantic labels.
In the field of namedentity recognition, it is known that label consis-tency holds strongly at the level of a documentand less strongly across different documents (Kr-ishnan and Manning, 2006).
Thus we start with adocument and gradually cluster related documentsuntil a sufficient number of features are obtained.For the specific procedures we took in the experi-ments, see Section 4.1.3.3 Training DataFollowing unknown morpheme acquisition (Mu-rawaki and Kurohashi, 2008), we create trainingdata using manually registered nouns, for whichwe can obtain correct semantic labels.
We per-form the same procedure as above to make in-stances of registered nouns.Some registered nouns are tagged with morethan one semantic label, which we call ?explicitpolysemy.?
We drop them from the training data.The remaining problem is ?implicit polysemy.
?Nouns are sometimes used with an uncoveredsense.
In preliminary experiments, we found thata typical case of implicit polysemy was that aproper noun derived from a basic noun.
To al-leviate this problem, we use an NE tagger for fil-tering.
We run an NE tagger over a small portionof the corpus and extract common nouns that arefrequently tagged as named entities.
Then we re-move these nouns from the training data.We also drop nouns that appear extremely fre-quently such as ???
(hito, ?person?
), ???
(koto,?thing?)
and ???
(watashi, ?I?2).
Since acquirednouns to be classified are typically low frequencymorphemes, they would not behave similarly tothese basic nouns.3.4 ClassifierTo assign a semantic label to each instance, we usea multiclass discriminative classifier.
The input ittakes is an instance that is represented by a featurevector x ?
Rd.
The output is one semantic labely ?
Y , where Y is the set of semantic labels.We use a linear classifier.
It has a weight vectorwy ?
Rd for each y and outputs y that maximizes2Japanese personal pronouns are treated as commonnouns because they show no special morphosyntactic behav-ior.the inner product of wy and x.y = argmaxy?wy, x?.Several methods have been proposed to esti-mate weight vector wy from training data.
We useonline algorithms because they are easy to imple-ment and scale to huge instances.
We try the Per-ceptron family of algorithms.4 Experiments4.1 SettingsWe used JUMAN for morphological analysis andKNP3 for dependency parsing.
The dictionaryof JUMAN was augmented with automaticallyacquired morphemes (Murawaki and Kurohashi,2008).
The number of manually registered mor-phemes was 120 thousands while there were13,071 acquired morphemes, of which 12,615morphemes were nouns.We used a web corpus that was compiledthrough the procedures proposed by Kawaharaand Kurohashi (2006).
It consisted of 100 millionpages.We first extracted features from the web cor-pus.
To keep the model size manageable, weused 447,082 features that appeared more than100 times in the corpus.We constructed training data from manuallyregistered nouns and test data from automaticallyacquired nouns.
For each noun, we combined texttogether until the number of features grew to morethan 100.
We started with a single web page, thenmerge pages that share a domain name and fi-nally clustered texts across different domains.
Wesplit the web corpus into 40 subcorpora and ap-plied this procedure in parallel.
We used Bayon4for clustering domain texts.
We sequentially readtexts and applied the repeated bisections cluster-ing every time some 5,000 pages were appended.The vectors for clustering were nouns, both regis-tered and acquired, with their tf-idf scores.
We ob-tained 4,843,085 instances for 10,613 registerednouns and 196,098 instances for 2,556 acquirednouns.3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html4http://code.google.com/p/bayon/880Table 2: Results of semantic classification.learning algorithms acquired nouns registered nounsAveraged Perceptron 86.40% (432 / 500) 88.59% (123,113 / 138,971)Passive-Aggressive 87.00% (435 / 500) 91.68% (127,407 / 138,971)Confidence-Weighted 85.20% (426 / 500) 89.66% (124,604 / 138,971)baseline1 69.60% (348 / 500) 79.14% (109,980 / 138,971)1 assign OTH-C to all instances.Table 3: Examples of aggregated instances.acquired nouns instances labels???
(hikaru, a person name) 84 PSN-P:58.33%, PSN-C:41.67%???
(chiwawa, ?Chihuahua?)
128 ANI-C:54.69%, OTH-C:45.31%????
(kamisaN, colloq.
?wife?)
131 PSN-C:100%?????
(rasubegasu, ?Las Vegas?)
136 LOC-P:97.06%, LOC-C:2.94%????
(aQpuru, ?Apple/apple?)
187 ORG-P:63.10%, PSN-C:34.76%, OTH-C:2.14%????
(merumaga, abbr.
of ?mail magazine?)
1,622 OTH-C:99.32%, LOC-C:0.55%, PSN-C:0.06%In order to handle polysemy, we evaluated se-mantic classification on an instance-by-instancebasis.
We randomly selected 500 instances fromthe test data and manually assigned the correct la-bels to them.
For comparison purposes, we alsoclassified registered nouns.
We split the trainingdata: 829 nouns or 138,971 instances for testingand the rest for training.We trained the model with three online learn-ing algorithms, (1) the averaged version (Collins,2002) of Perceptron (Crammer and Singer, 2003),(2) the Passive-Aggressive algorithm (Crammeret al, 2006), and (3) the Confidence-Weightedalgorithm (Crammer et al, 2009).
For Passive-Aggressive algorithm, we used PA-I and set pa-rameter C to 1.
For Confidence-Weighted, weused the single-constraint updates.
All algorithmsiterated five times through the training data.4.2 ResultsTable 2 shows the results of semantic classifica-tion.
All algorithms significantly improved overthe baseline.
As suggested by the gap in accu-racy between acquired and registered nouns in thebaseline method, the label distribution of the train-ing data differed from that of the test data, but thedecrease in accuracy was smaller than expected.The Passive-Aggressive algorithm performedbest on both acquired and registered nouns.
Forthe rest of this paper, we report the results of thePassive-Aggressive algorithm.Table 3 shows aggregated instances of some ac-quired nouns.
Although classification sometimesfailed, correct labels took the majority.
How-ever, it is noticeable that PSN-P was frequentlymisidentified as PSN-C while PSN-C was cor-rectly identified.
This phenomenon is clearly seenin the confusion matrix (Table 4).
Half of PSN-Pinstances were misidentified as PSN-C but thepercentage of errors in the opposite direction wasjust above 9%.
We will investigate this in the nextsection.4.3 DiscussionOur interest is in determining what kinds of fea-tures are effective in semantic classification.
Wefirst performed standard ablation experiments.
Wetrained a series of models on the training data af-ter removing each feature set.
The training andtest data were the same with those in Section 4.1.Table 5 shows the results of ablation experi-ments.
Significant decreases in accuracy are ob-served in the cf dataset.
This is easily explained bythe fact that more than half of features belongedto cf.
The ratio of ncf1 was much the same withthat of ncf2, but the removal of ncf1 resulted in aworse performance in classifying registered nounsthan that of ncf2.
This means that a modifiee of anoun explains more about the noun than its modi-fier.The ablation experiments cannot capture inter-esting properties of features because each featureset has a great diversity within it.
Next, we di-rectly examine features instead.
Since we use asimple linear classifier, a feature has |Y | corre-sponding weights, each of which represents howlikely a noun belongs to label y.
For example,features whose weights for PSN-C are the largest881Table 4: Confusion matrix of acquired nouns.ActualPSN-P LOC-P ORG-P OTH-P PSN-C LOC-C ORG-C ANI-C OTH-CPredictedPSN-P 16 1 4 1LOC-P 1ORG-P 4OTH-PPSN-C 16 39 1 2LOC-C 2 2 1 10 4ORG-C 2ANI-C 28OTH-C 3 1 1 1 13 9 338Table 5: Results of ablation experiments.feature set ratio1 acquired nouns registered nouns-call 0.23% 87.60% (438 / 500) 91.58% (127,276 / 138,971)-cf 54.84% 84.80% (424 / 500) 88.96% (123,630 / 138,971)-demo 2.40% 88.00% (440 / 500) 91.38% (126,996 / 138,971)-ncf1 19.03% 87.20% (436 / 500) 89.23% (124,008 / 138,971)-ncf2 18.40% 85.60% (428 / 500) 91.54% (127,220 / 138,971)-suf 5.10% 87.40% (437 / 500) 91.30% (126,889 / 138,971)all 87.00% (435 / 500) 91.68% (127,407 / 138,971)1 The proportion of each feature set that appears in the instances of the testdata.include:?
cf:nakusu:wo (??
lose X to the disease?),?
cf:oshieru:ni (?
?1 teach X ?2?),?
ncf2:ooku (?many/much X?
), and?
ncf2:<NUM>niN (X is modified by<NUM> plus a numeral classifier forpersons).As briefly mentioned in Section 2.3, Japanesenumeral quantifiers received scholarly attentionin the fields of linguistic philosophy and lin-guistics in relation to the count/mass distinc-tion (Quine, 1969; Gil, 1987).
In our featuresets, numeral quantifiers typically appear as ncf2,e.g.
?ncf2:<NUM>niN.?
The weights given tothem demonstrate their effectiveness in semanticclassification.
They discriminate common nounsfrom proper nouns as the weights given to com-mon nouns are larger with wide margins.
It is notsurprising because, say, the phrase ?two Johns?
issemantically acceptable but extremely rare in re-ality.
They are also informative about the distinc-tion among PSN, LOC and others.
For example,the classifier ?niN?
for persons suggest the noun inquestion is a person while ?keN?
for houses wouldmodify a location-like noun.
However, we foundquite a few ?noises?
about these features in data.The modifiee of a numeral expression is not al-ways the noun to be counted, as demonstrated bythe following example:(2) saNthreeniNCLnoGENmoNdaiproblemmatters among the three persons.From the above, the feature ?ncf2:<NUM>niN?is extracted although ?moNdai?
is OTH-C. Theis?noise?
is attributed to the genitive case marker?no?
because it can denote a wide range of rela-tions between two nouns.
We might be able toavoid this problem if we focus on ?floating?
nu-meral quantifiers.
A floating numeral quantifierhas no direct dependency relation to the noun tobe counted, as in(3) seitostudentgaNOMsaNthreeniNCLkeQsekiabsenceshitadothree students were absent,where the numeral quantifier modifies the verbphrase instead of the noun.
Further work isneeded to anchor floating numeral quantifierssince they bring a different kind of ambiguitythemselves (Bond et al, 1998).Closely related to numeral quantifiers are quan-tificational nouns that appear as ?ncf2:ooku?(?many/much?
), ?ncf2:subete?
(?all?)
and oth-ers.
They distinguish common nouns from proper882nouns but does not make a further classifica-tion.
The same is true of other numeral expres-sions such as ?cf:hueru:ga?
(?X increase in num-ber?)
and ?cf:nai:ga?
(?there is no X?
or ?Xdo not exist?).
We found that, other than nu-meral expressions, some features distinguishedcommon nouns from proper nouns because theyindicated the noun denoted an attribute.
Such fea-tures include ?cf:naru:ni?
(??
become X?)
and?cf:kaneru:wo?
(??
double as X?
).We expected that demonstratives (demo)served similar functions to quantificational ex-pressions, but it turned out to be more com-plex.
The distal demonstrative ?ano?
(?that?)
of-ten modifies proper nouns to give emphasis.
Infact, the model gave larger weights to propernouns.
On the other hand, interrogative demon-stratives such as ?dono?
(?which?)
and ?doNna?
(?what kind of?)
are rarely used with proper nounsalthough semantically acceptable.As seen above, there is an abundant varietyof features that distinguish common nouns fromproper nouns.
Also, it is not difficult to make adistinction among PSN, LOC and others althoughthe far largest cluster OTH-C sometimes absorbsother instances.
The remaining question is how todistinguish proper nouns from common nouns, orspecifically PSN-P from PSN-C. We examinedfeatures that gave larger weights to PSN-P thanto PSN-C.
They generally had smaller marginsin weights than those which distinguish PSN-Cfrom PSN-P.
Among them, features such as?cf:utau:ga?
(?X sing?)
and ?cf:hanasu:ni?
(?
?talk to X?)
have no problem with being used forcommon nouns in terms of both semantics andpragmatics.
They seem to have resulted fromover-training.
There were seemingly appropriatefeatures such as ?suf:saNchi?
(?X?s house?)
and?suf:seNshu?
(honorific suffix for players), butthey were not ubiquitous in the corpus.
PSN-P in-stances suffered from lack of distinctive features.One solution to this problem is to combine ad-ditional knowledge about person names.
For ex-ample, a Japanese family name is followed by agiven name, and most Chinese names consist ofthree Chinese characters.
However, quite a fewperson names in the web corpus do not followthe usual patterns of person names because theyare handles (or nicknames) and names for fic-tional characters.
Thus it would be desirable to beable to classify person names without additionalknowledge.5 ConclusionIn this paper, we presented the new task of seman-tic classification of Japanese nouns and applied itto nouns automatically acquired from text.
Unlikein unknown morpheme identification in previousstudies, we can exploit automatically parsed text.We explored lexico-syntactic clues and investi-gated their effects.
We found plenty of featuresthat distinguished common nouns from propernouns, but few features worked in the opposite di-rection.
Further work is needed to overcome thisbias.ReferencesAsahara, Masayuki and Yuji Matsumoto.
2000.
Ex-tended models and tools for high-performance part-of-speech tagger.
In Proc.
of COLING 2000, pages21?27.Asahara, Masayuki and Yuji Matsumoto.
2003.Japanese named entity extraction with redundantmorphological analysis.
In Proc.
of HLT/NAACL2003, pages 8?15.Asahara, Masayuki and Yuji Matsumoto.
2004.Japanese unknown word identification by character-based chunking.
In Proc.
COLING 2004, pages459?465.Bond, Francis, Daniela Kurz, and Satoshi Shirai.1998.
Anchoring floating quantifiers in Japanese-to-English machine translation.
In Proc.
of COL-ING 1998, pages 152?159.Ciaramita, Massimiliano and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.In Proc.
of EMNLP 2006, pages 594?602.Ciaramita, Massimiliano and Mark Johnson.
2003.Supersense tagging of unknown nouns in WordNet.In Proc.
of EMNLP 2003, pages 168?175.Collins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: Theory and ex-periments with perceptron algorithms.
In Proc.
ofEMNLP 2002, pages 1?8.883Crammer, Koby and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Crammer, Koby, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Crammer, Koby, Mark Dredze, and Alex Kulesza.2009.
Multi-class confidence weighted algorithms.In Proc.
of EMNLP 2009, pages 496?504.Curran, James R. 2005.
Supersense tagging of un-known nouns using semantic similarity.
In Proc.
ofACL 2005, pages 26?33.Fellbaum, Christiane, editor.
1998.
WordNet: AnElectronic Lexical Database.
The MIT Press, Cam-bridge, MA.Gil, David.
1987.
Definiteness, NP configurationalityand the count-mass distinction.
In Reuland, Eric J.and Alice G. B. ter Meulen, editors, The Representa-tion of (In)definiteness, pages 254?269.
MIT Press.Iida, Ryu, Kentaro Inui, and Yuji Matsumoto.
2009.Capturing salience with a trainable cache model forzero-anaphora resolution.
In Proc.
of ACL/IJCNLP2009, pages 647?655.Kawahara, Daisuke and Sadao Kurohashi.
2001.Japanese case frame construction by coupling theverb and its closest case component.
In Proc.
ofHLT 2001, pages 204?210.Kawahara, Daisuke and Sadao Kurohashi.
2006.Case frame compilation from the web using high-performance computing.
In Proc.
of LREC-06,pages 1344?1347.Kazama, Jun?ichi and Kentaro Torisawa.
2008.
Induc-ing gazetteers for named entity recognition by large-scale clustering of dependency relations.
In Proc.
ofACL 2008, pages 407?415, June.Krishnan, Vijay and Christopher D. Manning.
2006.An effective two-stage model for exploiting non-local dependencies in named entity recognition.
InProc.
of COLING-ACL 2006, pages 1121?1128.Kudo, Taku and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc.
of CONLL 2002, pages 1?7.Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proc.
ofEMNLP 2004, pages 230?237.Kurohashi, Sadao, Toshihisa Nakamura, Yuji Mat-sumoto, and Makoto Nagao.
1994.
Improvementsof Japanese morphological analyzer JUMAN.
InProc.
of The International Workshop on SharableNatural Language Resources, pages 22?38.Marcus, Mitchell P., Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn treebank.
Com-putational Linguistics, 19(2):313?330.Mori, Shinsuke and Makoto Nagao.
1996.
Word ex-traction from corpora and its part-of-speech estima-tion using distributional analysis.
In Proc.
of COL-ING 1996, volume 2, pages 1119?1122.Murawaki, Yugo and Sadao Kurohashi.
2008.
Onlineacquisition of Japanese unknown morphemes us-ing morphological constraints.
In Proc.
of EMNLP2008, pages 429?437.Nagata, Masaaki.
1999.
A part of speech estimationmethod for Japanese unknown words using a statis-tical model of morphology and context.
In Proc.
ofACL 1999, pages 277?284.Nakagawa, Tetsuji and Yuji Matsumoto.
2006.
Guess-ing parts-of-speech of unknown words using globalinformation.
In Proc.
of COLING-ACL 2006, pages705?712.Quine, Willard Van.
1969.
Ontological Relativity andOther Essays.
Columbia University Press.Saito, Kuniko, Jun Suzuki, and Kenji Imamura.
2007.Extraction of named entities from blogs using CRF.In Proc.
of The 13th Annual Meeting of The Associ-ation for Natural Language Processing, pages 107?110.
(in Japanese).Sasano, Ryohei and Sadao Kurohashi.
2008.
Japanesenamed entity recognition using structural naturallanguage processing.
In Proc.
of IJCNLP 2008,pages 607?612.Sasano, Ryohei and Sadao Kurohashi.
2009.
A prob-abilistic model for associative anaphora resolution.In Proc.
of EMNLP 2009, pages 1455?1464.Sasano, Ryohei, Daisuke Kawahara, and Sadao Kuro-hashi.
2004.
Automatic construction of nominalcase frames and its application to indirect anaphoraresolution.
In Proc.
of COLING 2004, pages 1201?1207.Uchimoto, Kiyotaka, Satoshi Sekine, and Hitoshi Isa-hara.
2001.
The unknown word problem: a mor-phological analysis of Japanese using maximum en-tropy aided by a dictionary.
In Proc.
of EMNLP2001, pages 91?99.Yokoi, Toshio.
1995.
The EDR electronic dictionary.Communications of the ACM, 38(11):42?44.884
