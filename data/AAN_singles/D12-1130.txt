Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1423?1433, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsGrounded Models of Semantic RepresentationCarina Silberer and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABc.silberer@ed.ac.uk, mlap@inf.ed.ac.ukAbstractA popular tradition of studying semantic rep-resentation has been driven by the assump-tion that word meaning can be learned fromthe linguistic environment, despite ample ev-idence suggesting that language is groundedin perception and action.
In this paper wepresent a comparative study of models thatrepresent word meaning based on linguisticand perceptual data.
Linguistic information isapproximated by naturally occurring corporaand sensorimotor experience by feature norms(i.e., attributes native speakers consider impor-tant in describing the meaning of a word).
Themodels differ in terms of the mechanisms bywhich they integrate the two modalities.
Ex-perimental results show that a closer corre-spondence to human data can be obtained byuncovering latent information shared amongthe textual and perceptual modalities ratherthan arriving at semantic knowledge by con-catenating the two.1 IntroductionDistributional models of lexical semantics have seenconsiderable success at accounting for a wide rangeof behavioral data in tasks involving semantic cog-nition (Landauer and Dumais, 1997; Griffiths etal., 2007).
These models have also enjoyed last-ing popularity in natural language processing.
Ex-amples involve information retrieval (Salton et al1975), word sense discrimination (Schu?tze, 1998),text segmentation (Choi et al 2001), and numerousstudies of lexicon acquisition (Grefenstette, 1994;Lin, 1998).
Despite their widespread use, distribu-tional models have been criticized as ?disembodied?in that they learn exclusively from linguistic infor-mation but are not grounded in perception and ac-tion (Perfetti, 1998; Barsalou, 1999; Glenberg andKaschak, 2002).This lack of grounding contrasts with many ex-perimental studies suggesting that word meaning isacquired not only from exposure to the linguisticenvironment but also from our interaction with thephysical world (Landau et al 1998; Bornstein et al2004).
Beyond language acquisition, there is consid-erable evidence across both behavioral experimentsand neuroimaging studies that the perceptual asso-ciates of words play an important role in languageprocessing (for a review see Barsalou (2008)).It is thus no surprise that recent years have wit-nessed the emergence of perceptually grounded dis-tributional models.
An important question in the for-mulation of such models concerns the provenanceof perceptual information.
A few models use fea-ture norms as a proxy for sensorimotor experience(Howell et al 2005; Andrews et al 2009; Steyvers,2010; Johns and Jones, 2012).
These are obtainedby asking native speakers to write down attributesthey consider important in describing the meaningof a word.
The attributes represent perceived phys-ical and functional properties associated with thereferents of words.
For example, apples are typi-cally green or red, round, shiny, smooth, crunchy,tasty, and so on; dogs have four legs and bark,whereas chairs are used for sitting.
Other models fo-cus solely on the visual modality under the assump-tion that it represents a major source of data from1423which humans can learn semantic representationsof both linguistic and non-linguistic communicativeactions (Regier, 1996).
For example, Feng and Lap-ata (2010) learn semantic representations from cor-pora of texts paired with naturally co-occurring im-ages (e.g., news articles and their associated pic-tures), whereas Bruni et al(2011) learn textual andvisual representations independently from distinctdata sources.Aside from the type of data used to capture per-ceptual information, another important issue con-cerns how the two modalities (perceptual and tex-tual) are integrated.
A simple solution would beto learn both modalities independently (Bruni et al2011) or to infer one modality by means of the other(Johns and Jones, 2012) and to arrive at a groundedrepresentation simply by concatenating the two.
Analternative is to learn from both modalities jointly(Andrews et al 2009; Feng and Lapata, 2010;Steyvers, 2010).
According to this view, seman-tic knowledge is gained by simultaneously learningfrom the statistical structure within each modalityassuming both data sources have been generated bya shared set of meanings or topics.In this paper we undertake the first comparativestudy of perceptually grounded distributional mod-els.
We examine three models with different as-sumptions regarding the integration of perceptualand linguistic data.
The first model, originally pro-posed by Andrews et al(2009), is an extension oflatent Dirichlet alcation (LDA, Blei et al(2003)).It simultaneously considers the distribution of wordsacross contexts in a text corpus and the distribu-tion of words across perceptual features and extractsjoint information from both data sources.
Our sec-ond model is based on Johns and Jones (2012) whorepresent the meaning of a word as the concatena-tion of its textual and its perceptual vector.
Interest-ingly, their model allows to infer a perceptual vectorfor words without feature norms, simply by takinginto account similar words for which perceptual in-formation is available.Finally, we propose Canonical Correlation Anal-ysis (Hotelling, 1936; Hardoon et al 2004) as ourthird model.
CCA is a data analysis and dimen-sionality reduction method similar to PCA.
WhilePCA deals with only one data space, CCA is a tech-nique for joint dimensionality reduction across twoFeatures table dog applehas 4 legs .28 .60 0used for eating .50 0 0a pet 0 .40 0is brown 0 0 0is crunchy 0 0 .58is round .22 0 .42has fangs 0 0 0Table 1: Feature norms for the nouns table, dog, andapple shown as a distribution.
(or more) spaces that provide heterogeneous repre-sentations of the same objects.
The assumption isthat the representations in these two spaces containsome joint information that is reflected in correla-tions between them.In all three models we use feature norms as aproxy for perceptual information.
Despite theirshortcomings (e.g., they often cover a small frac-tion of the vocabulary of an adult speaker due tothe effort involved in eliciting them), feature normsprovide detailed knowledge about meaning repre-sentations and are a useful starting point for study-ing the integration of perceptual and textual infor-mation without being susceptible to the effects ofnoise, e.g., coming from image processing.
In otherwords, feature norms can serve as an upper boundof what can be achieved when integrating detailedperceptual information with vanilla text-based dis-tributional models.Our experimental results demonstrate that jointmodels give a better fit to human word similarity andassociation data than a model that considers onlyone data source, or the simple concatenation of thetwo sources.2 Perceptually Grounded ModelsIn this study we examine semantic representationmodels that rely on linguistic and perceptual data.The linguistic environment is approximated by cor-pora such as the British National Corpus (BNC).As mentioned earlier, we resort to feature normsas proxy for perceptual information.
In our exper-iments, we relied on the norming study of McRae etal.
(2005), in which a large number of human par-ticipants were presented with a series of words and1424asked to list relevant features of the words?
refer-ents.
Table 1 presents examples of features partici-pants listed for the nouns apple, dog, and table.
Thenumber of participants listing a certain feature for aword can be used to compute a probability distribu-tion over features given the word:P( fk|w) =f ( fk,w)F?m=1f ( fm,w)(1)where f ( fk,w) is the number of participants wholisted feature fk for word w and F is the total numberof features.In the remainder of this section we will describeour models and how they arrive at an integrated per-ceptual and linguistic representation.2.1 Feature-topic ModelAndrews et al(2009) present an extension of LDA(Blei et al 2003) where words in documents as wellas their associated features are treated as observedvariables that are explained by a generative process.The underlying training data consists of a corpus Dwhere each document is represented by words andtheir frequency of occurrence within the document.In addition, those words of a document that are alsoincluded in the feature norms are paired with one oftheir features, where a feature is sampled accordingto the feature distribution given that word.
For ex-ample, suppose a document d j consists of the sen-tence Mix in the apple, celery, raisins, and applejuice.
Suppose further that all content words ex-cept of mix and juice are included in the featurenorms.
Then, a representation for d j is mix:1, ap-ple;is red:2, celery;has leaves:1, raisin;is edible:1,juice:1.The plate diagram in Figure 1 illustrates thegraphical model in detail.
Each document d jin D is generated by a mixture of components{x1, ...,xc, ...,xC} ?
C ; a component xc comprises alatent discourse topic coupled with a feature clus-ter originating from the feature norms.
A dis-course topic belonging to xc, in turn, is a distribu-tion ?c ?
?= {?1, ...,?C} over words, and a featurecluster is a distribution ?c ?
?= {?1, ...,?C} overfeatures.In order to create document d j, a distribution pi jover components is sampled from a Dirichlet distri-pix?
w, f ???
?
?xc ?
C ?i ?
{1, ...,n j}?
j ?
{1, ...,D}?xc ?
CFigure 1: Feature-topic model.
The components x ji of adocument d j are sampled from pi j.
For each xc = x ji, aword w ji is drawn from distribution ?c and a feature f ji isdrawn from distribution ?c.bution parametrized by ?.
To generate each wordw ji ?
{w j1, ...,w jn j}, a component xc = x ji is drawnfrom pi j; w ji is then drawn from the correspondingdistribution ?c.
If w ji is in the feature norms, it iscoupled with a feature f ji which is correspondinglydrawn from ?c.
A symmetric Dirichlet prior withhyperparameters ?
and ?
is placed on ?
and ?, re-spectively.
The probability of the corpus D is de-fined as:P((w?
f )1:D|?,?,?)
=D?j=1?dpi jn j?i=1P(pi j|?
)C?c=1P(w ji|x ji = xc,?
)P( f ji|x ji = xc,?
)P(x ji = xc|pi j)(2)where D is the number of documents and C thepredefined number of components.
Computing theposterior distribution P(?,?,?,?,?|(w ?
f )1:D) ofthe hidden variables given the data is generally in-tractable:P(?,?,?,?,?|(w?
f )1:D) ?
P((w?
f )1:D|?,?,?)P(?|?)P(?|?)P(?)P(?)P(?
)(3)Equation (3) may be approximated using the Gibbs1425[ x1 x2 x12 ... x28 x75 x107 x119 x125 x148 x182 ... x266 x326 x349 x350apple 3e-5 3e-5 0 .
.
.
5e-4 9e-4 .09 .002 7.6e-5 2e-4 .003 .
.
.
0 0 3e-6 0]Figure 2: Example of the representation of the meaning of apple with the model of (Andrews et al 2009) .
[ ... d16 ... d322 ... d2469 d2470 ... dDapple .
.
.
1 .
.
.
1 .
.
.
0 1 .
.
.
0][ a f ruit has f angs is crunchy ... is yellow is red is green is round0 0 0 .
.
.
0 0 0 0][ ... d16 ... d322 ... d2469 d2470 ... dDapple .
.
.
1 .
.
.
1 .
.
.
0 1 .
.
.
0][ a f ruit has f angs is crunchy ... is yellow is red is green is round.006 1.8e-5 8e-4 .
.
.
.004 .004 .006 .02]Figure 3: Example representation for apple before (first row) and after (second row) applying the perceptual inferencemethod of Johns and Jones (2012).sampling procedure described in Andrews et al(2009).Inducing feature-topic components from a docu-ment collection D with the extended LDA modeljust described gives two sets of parameters: wordprobabilities given components PW (wi|X = xc) forwi, i = 1, ...,N, and feature probabilities given com-ponents PF( fk|X = xc) for fk, k= 1, ...,F .
For exam-ple, most of the probability mass of component x107would be reserved for the words apple, fruit, lemon,orange, tree and the features is red, tastes sweet,is round and so on.Word meaning in this model is represented by thedistribution PX |W over the learned components (seeFigure 2 for an example).
Assuming a uniform dis-tribution over components xc in D , PX |W can be ap-proximated as:PX=xc|W=wi =P(wi|xc)P(xc)P(wi)?P(wi|xc)C?l=1P(wi|xl)(4)where C is the total number of components.
Themodel can be also used to infer features for wordsthat were not originally included in the featurenorms.
The probability distribution PF |W over fea-tures given a word wi is simply inferred by summingover all components xc for each feature fk:PF( fk|W = wi) =C?c=1P( fk|xc)P(xc|wi) (5)2.2 Global Similarity ModelJohns and Jones (2012) propose an approach forgenerating perceptual representations for words bymeans of global lexical similarity.
Their model doesnot place so much emphasis on the integration ofperceptual and linguistic information, rather its mainfocus is on inducing perceptual representations forwords with no perceptual correlates.
Their idea is toassume that lexically similar words also share per-ceptual features and hence it should be possible totransfer perceptual information onto words that havenone from their linguistically similar neighbors.Let T ?
{1,0}N?D denote a binary term-document matrix, where each cell records the pres-ence or absence of a term in a document.
LetP ?
[0,1]N?F denote a perceptual matrix, represent-ing a probability distribution over features for eachword (see Table 1).
A word?s meaning is repre-sented by the concatenation of its textual and per-ceptual vectors (see Figure 3).
If a word has notbeen normed, its perceptual vector will be all zeros.Johns and Jones (2012) propose a two-step estima-tion process for words without perceptual vectors.Initially, a perceptual vector is constructed based onthe word?s weighted similarity to other words thathave non-zero perceptual vectors:pin f =N?i=1ti ?
sim(ti,p)?
(6)where p is the representation of a word with a tex-tual vector but an empty perceptual vector, ts arecomposite representations consisting of textual andperceptual vectors, sim is a measure of distributionalsimilarity such as cosine, ?
a weighting parameter,and pin f the resulting inferred representation of theword.
The process is repeated a second time, soas to incorporate the inferred perceptual vector inthe computation of the inferred vectors of all otherwords.
An example of this inference procedure isillustrated in Figure 3.1426[ ... d16 ... d322 ... d2470 ... dDapple .
.
.
.006 .
.
.
.003 .
.
.
.1e-6 .
.
.
0][ a f ruit has f angs is crunchy ... is yellow is red is green is round.13 0 .06 .
.
.
.04 .14 .09 .04][ k1 k2 k3 ... k409 k410apple ?.003 ?.01 .002 .
.
.
?.002 ?.01][ k1 k2 k3 ... k409 k410.008 ?.03 ?.008 .
.
.
?.02 ?.07]Figure 4: Example representation for apple before (first row) and after (second row) applying CCA.2.3 Canonical Correlation AnalysisOur third model uses Canonical Correlation Analy-sis (CCA, Hardoon et al(2004)) to learn a joint se-mantic representation from the textual and percep-tual views.
Given two random variables x and y(or two sets of vectors), CCA can be seen as de-termining two sets of basis vectors in such a way,that the correlation between the projections of thevariables onto these bases is mutually maximized(Borga, 2001).
In effect, the representation-specificdetails pertaining to the two views of the same phe-nomenon are discarded and the underlying hiddenfactors responsible for the correlation are revealed.In our case the linguistic view is represented by aterm-document matrix, T ?
RN?D, containing infor-mation about the occurrence of each word in eachdocument.
The perceptual view is captured by aperceptual matrix, P ?
[0,1]N?F , representing wordsas a probability distribution over normed features.CCA is concerned with describing linear dependen-cies between two sets of variables of relatively lowdimensionality.
Since the correlation between thelinguistic and perceptual views may exist in somenonlinear relationship, we used a kernelized versionof CCA (Hardoon et al 2004) which first projectsthe data into a higher-dimensional feature space andthen performs CCA in this new feature space.
Thetwo kernel matrices are KT = TT ?
and KP = PP?.After applying CCA we obtain two matrices pro-jected onto L basis vectors, Ct ?
RN?L, resultingfrom the projection of the textual matrix T onto thenew basis and Cp ?RN?L, resulting from the projec-tion of the corresponding perceptual feature matrix.The meaning of a word can thus be represented byits projected textual vector in CT , its projected per-ceptual vector in CP or their concatenation.
Figure 4shows an example of the textual and perceptual vec-tors for the word apple which were used as input forCCA (first row) and their new representation afterthe projection onto new basis vectors (second row).The CCA model as sketched above will only ob-tain full representations for words with perceptualfeatures available.
One solution would be to applythe method from Johns and Jones (2012) to infer theperceptual vectors and then perform CCA on the in-ferred vectors.
Another approach which we assessexperimentally (see Section 4) is to create a percep-tual vector for a word that has none from its k-most(textually) similar neighbors, simply by taking theaverage of their perceptual vectors.
This inferenceprocedure can be applied to the original vectors orthe projected vectors in CT and CP, respectively,once CCA has taken place.2.4 DiscussionJohns and Jones (2012) primarily present a model ofperceptual inference, where textual data is used toinfer perceptual information for words not includedin feature norms.
There is no means in this model toobtain a joint representation resulting from the mu-tual influence of the perceptual and textual views.As shown in the example in Figure 3 the textualvector on the left-hand side does not undergo anytransformation whatsoever.
The generative modelput forward by Andrews et al(2009) learns meaningrepresentations by simultaneously considering doc-uments and features.
Rather than simply adding per-ceptual information to textual data it integrates bothmodalities jointly in a single representation whichis desirable, at least from a cognitive perspective.It is unlikely that we have separate representationsfor different aspects of word meaning (Rogers et al2004).
Similarly to Johns and Jones (2012), An-drews et al (2009) feature-topic model can alsoinfer perceptual representations for words that havenone.
The inference is performed automatically inan implicit manner during component induction.In CCA, textual and perceptual data represent twodifferent views of the same objects and the modeloperates on these views directly without combiningor manipulating any of them a priori.
Instead, thecombination of the two modalities is realized via1427correlating the linear relationships between them.
Adrawback of the model lies in the need of additionalmethods for inferring perceptual representations forwords not available in feature norms.3 Experimental SetupData All our experiments used a lemmatized ver-sion of the British National Corpus (BNC) as asource of textual information.
The feature norms ofMcRae et al(2005) were used as a proxy for percep-tual information.
The BNC comprises 4,049 textstotalling approximately 100 million words.
McRaeet als feature norms consist of 541 words and 2,526features; 824 of these features occur with at least twodifferent words.Evaluation Tasks Our evaluation experimentscompared the models discussed above on threetasks.
Two of them have been previously usedto evaluate semantic representation models, namelyword association and word similarity.
In orderto simulate word association, we used the humannorms collected by (Nelson et al 1998).1 Thesewere established by presenting a large number ofparticipants with a cue word (e.g., rice) and ask-ing them to name an associate word in response(e.g., Chinese, wedding, food, white).
For each cueword, the norms provide a set of associates and thefrequencies with which they were named.
We canthus compute the probability distribution over asso-ciates for each cue.
Analogously, we can estimatethe degree of similarity between a cue and its as-sociates using our models (see the following sec-tion for details on the similarity measures we em-ployed).
The norms contain 63,619 unique normedcue-associate pairs in total.
Of these, 25,968 pairswere covered by all models and 520 appeared inMcRae et als (2005) norms.
Using correlation anal-ysis, we examined the degree of linear relationshipbetween the human cue-associate probabilities andthe automatically derived similarity values.Our word similarity experiments used theWordSimilarity-353 test collection (Finkelstein etal., 2002)2 which consists of relatedness judgments1Available at http://www.usf.edu/Freeassociation.2Available at http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/.for word pairs.
For each pair, a similarity judg-ment (on a scale of 0 to 10) was elicited from 13 or16 human subjects (e.g., tiger-cat are very similar,whereas delay?racism are not).
The average ratingfor each pair represents an estimate of the perceivedsimilarity of the two words.
The task varies slightlyfrom word association.
Here, participants are askedto rate perceived similarity rather than to generatethe first word that came to mind in response to a cueword.
The collection contains similarity ratings for353 word pairs.
Of these, 76 pairs appeared in ourcorpus and 3 in McRae et als (2005) norms.
Again,we evaluated how well model produced similaritiescorrelate with human ratings.
Throughout this paperwe report correlation coefficients using Pearson?s r.Our third task assessed the models?
ability to in-fer perceptual vectors for words that have none.
Todo this, we conducted 10-fold cross-validation onMcRae et als (2005) norms.
We treated the per-ceptual vectors in each test fold as unseen, and usedthe data in the corresponding training fold togetherwith the models presented in Section 2 to infer them.Then, for each word, we examined how close the in-ferred vector was to the actual one, via correlationanalysis.Model Parameters The feature-topic model has afew parameters that must be instantiated.
These in-clude, C, the number of predefined components andthe priors ?, ?, and ?.
Following Andrews et al(2009), the components C were set to 350.3 A vagueinverse gamma prior was placed on ?, ?, and ?.4 Tomeasure word similarity within this model, we adoptGriffiths et als (2007) definition.
The underlyingidea is that word association can be expressed as aconditional distribution.
If we have seen word w1,then we can determine the probability that w2 willbe also generated by computing P(w2|w1).
Assum-ing that both w1 and w2 came from a single compo-nent, P(w2|w1) can be estimated as:P(w2|w1) =C?c=1P(w2|xc)P(xc|w1)P(xc|w1) ?
P(w1|xc)P(xc)(7)3As we explain in Section 4 the feature-topic model wascompared to a vanilla LDA model trained on the BNC only.For that model, C was set to 250.4That is P(?)
= exp(?
1?
)?
?2.1428where P(xc) is uniform, a single component xc issampled from the distribution P(xc|w1), and an over-all estimate is obtained by averaging over all C com-ponents.Johns and Jones?
(2012) model uses binary tex-tual vectors to represent word meaning.
If the wordis present in a given document, that vector elementis coded as one; if it is absent, it is coded as zero.We built a binary term-document matrix from theBNC over 14,000 lemmas.
The value of the similar-ity weighting parameter ?
was set to the same valuesreported by Johns and Jones (?1=3 for Step 1 and?2 = 13 for Step 2).For the CCA model, we represented the textualview with a term-document co-occurrence matrix.Matrix cells were set to their tf-idf values.5 The tex-tual and perceptual matrices were projected onto 410vectors.
As mentioned in Section 2.3, CCA does notnaturally lend itself to inferring perceptual vectors,yet a perceptual vector for a word can be createdfrom its k-nearest neighbors.
We inferred a percep-tual vector by averaging over the perceptual vectorsof the word?s k most similar words; textual similaritybetween two words was measured using the cosineof the angle of the two vectors representing them.To find the optimal value for k, we used one third ofNelson?s (1998) cues as development set.
The high-est correlation was achieved with k = 2 when theperceptual vectors were created prior to CCA andk = 8 when they were inferred on the projected tex-tual and perceptual matrices.4 ResultsOur experiments were designed to answer threequestions: (1) Does the integration of perceptual andtextual information yield a better fit with behavioraldata compared to a model that considers only onedata source?
(2) What is the best way to integratethe two modalities, e.g., via simple concatenation orjointly?
(3) How accurately can we approximate theperceptual information when the latter is absent?To answer the first question, we assessed the mod-els?
performance when textual and perceptual infor-mation are both available.
The results in Table 2are thus computed on the subset of Nelson?s (1998)5Experiments with a binarized version of the term-documentmatrix consistently performed worse.Models Modality Pearson?s rFeature-topic +t +p .35Feature-topic +t ?p .12Feature-topic ?t +p .22Global similarity +t +p .23Global similarity +t ?p .11Global similarity ?t +p .22CCA +t +p .32CCA +t ?p .14CCA ?t +p .29Upper Bound ?
.91Table 2: Performance of feature-topic, global similarity,and CCA models on a subset of the Nelson et al(1998)norms when taking into account the textual and percep-tual modalities on their own (+t?p and ?t+p) and incombination (+t+p).
All correlation coefficients are sta-tistically significant (p < 0.01).norms (520 cue-associate pairs) that also appeared inMcRae et al(2005) and for which a perceptual vec-tor was present.
The table shows different instanti-ations of the three models depending on the type ofmodality taken into account: textual, perceptual orboth.As can be seen, Andrews et als (2009) feature-topic model provides a better fit with the associationdata when both modalities are taken into account(+t+p).
A vanilla LDA model constructed solelyon the BNC (+t?p) or McRae et als (2005) fea-ture norms (?t+p) yields substantially lower corre-lations.
We observe a similar pattern with Johns andJones?
(2012) global similarity model.
Concatena-tion of perceptual and textual vectors yields the bestfit with the norming data, relying on perceptual in-formation alone (?t+p) comes close, whereas tex-tual information on its own seems to have a weakereffect (+t?p).6 The CCA model takes perceptualand textual information as input in order to find aprojection onto basis vectors that are maximally cor-related.
Although by definition the CCA model mustoperate on the two views, we can nevertheless iso-late the contribution of each modality by consideringthe vectors resulting from the projection of the tex-6In this evaluation setting, the model does not infer any per-ceptual representations; perceptual vectors are taken directlyfrom McRae et al(2005).1429tual matrix (+t?p), the perceptual matrix (?t+p) ortheir concatenation (+t+p).
We obtain best resultswith the latter representation; again we observe thatthe perceptual information is more dominant.Overall we find that the feature-topic model andCCA perform best.
In fact the correlations achievedby the two models do not differ significantly, us-ing a t-test (Cohen and Cohen, 1983).
The per-formance of the global similarity model is signifi-cantly worse than the feature-topic model and CCA(p < 0.01).
Recall that the feature-topic model(+t+p) represents words as distributions over com-ponents, whereas the global similarity model sim-ply concatenates the textual and perceptual vectors.The same input is also given to CCA which in turnattempts to interpret the data by inferring commonrelationships between the two views.
In sum, wecan conclude that the higher correlation with humanjudgments indicates that integrating textual and per-ceptual modalities jointly is preferable to concatena-tion.However, note that all models in Table 2 fallshort of the human upper bound which we mea-sured by calculating the reliability of Nelson et als(1998) norms.
Reliability estimates the likelihoodof a similarly-composed group of participants pre-sented with the same task under the same circum-stances producing identical results.
We split the col-lected cue-associate pairs randomly into two halvesand computed the correlation between them; thiscorrelation was averaged across 200 random splits.These correlations were adjusted by applying theSpearman-Brown prediction formula (Voorspoels etal., 2008).The results in Table 2 are computed on a smallfraction of Nelson et als (1998) norms.
One mighteven argue that the comparison is slightly unfair asthe global similarity model is more geared towardsinferring perceptual vectors rather than integratingthe two modalities in the best possible way.
To gaina better understanding of the models?
behavior andto allow comparisons on a larger dataset and moreequal footing, we also report results on the entiredataset (20,556 cue-associate pairs).7 This entailsthat the models will infer perceptual vectors for the7This excludes the data used as development set for tuningthe k-nearest neighbors for CCA.Models Pearson?s rFeature-topic .15Global similarity .03Global similarity CCA .12k-NN CCA .11CCA k-NN .12Upper Bound .96Table 3: Performance of the feature-topic, global simi-larityand CCA models on the Nelson et al(1998) norms(entire dataset).
All correlation coefficients are statisti-cally significant (p < 0.01).words that are not attested in McRae et als norms.Recall from Section 2.3 that CCA does not havea dedicated inference mechanism.
We thus experi-mented with three options (a) interfacing the infer-ence method of Johns and Jones (2012) with CCA(global similarity  CCA) (b) creating a percep-tual vector from the words?
k-nearest neighbors be-fore (k-NN  CCA) or (c) after CCA takes place(CCA k-NN).Our results are summarized in Table 3.
The up-per bound was estimated in the same fashion as forthe smaller dataset.
Despite being statistically sig-nificant (p < 0.01), the correlation coefficients arelower.
This is hardly surprising as perceptual infor-mation is approximate and in several cases likely tobe wrong.
Interestingly, we observe similar mod-eling trends, irrespective of whether the models areperforming perceptual inference or not.
The feature-topic model achieves the best fit with the data, fol-lowed by CCA.
The inference method here does notseem to have much of an impact: CCA  k-NNdoes as well as global similarity  CCA.
This isperhaps expected as the inference procedure adoptedby Johns and Jones (2012) is a generalization of ourk-nearest neighbor approach.
The global similaritymodel performs worst; we conjecture that this is dueto the way semantic information is integrated ratherthan the inference method itself.
CCA works withsimilar input, yet achieves better correlations withthe human data, due to its ability to represent thecommonalities shared by the two modalities.
Takentogether the results in Tables 2 and 3 provide an an-swer to our second question.
Models that capture la-tent information shared between the two modalities1430Models Pearson?s rFeature-topic .17Global similarity .25Global similarity CCA .21k-NN CCA .19CCA k-NN .13Table 4: Mean correlation coefficients between origi-nal and inferred feature vectors in McRae et als (2005)norms.create more accurate semantic representations com-pared to simply treating the two as independent datasources.In order to isolate the influence of the inferencemethod from the resulting semantic representationwe evaluated the inferred perceptual vectors on theirown by computing their correlation with the originalfeature distributions in McRae et als (2005) norms.The correlation coefficients are reported in Table 4and were computed by averaging the coefficients ob-tained for individual words.
Here, the global simi-larity model achieves the highest correlation, and fora good reason.
It is the only model with an empha-sis on inference, the other two models do not havesuch a dedicated mechanism.
CCA has in fact none,whereas in the feature-topic model the inference ofmissing perceptual information is a by-product ofthe generative process.
The results in Table 4 indi-cate that the perceptual vectors are not reconstructedvery accurately (the highest correlation coefficientis .25) and that better inference mechanisms are re-quired for perceptual information to have a positiveimpact on semantic representation.In Table 5 we examine the models?
performanceon semantic similarity rather than association usingthe WordSimilarity-353 dataset (Finkelstein et al2002).
The models were evaluated on 76 word pairsthat appeared in the BNC.
We inferred the percep-tual vectors for 51 words.
We computed the upperbound using the reliability method described ear-lier.
Again, the joint models achieve better resultsthan the simple concatenation model.
The feature-topic and CCA models perform comparably, withthe global similarity model lagging substantially be-hind.
In sum, our results indicate that the issueof how to best integrate the two modalities has aModels Pearson?s rFeature-topic .35Global similarity .08Global similarity CCA .38k-NN CCA .39CCA k-NN .28Upper Bound .98Table 5: Model performance on predicting word similar-ity.
All correlation coefficients are statistically significant(p < 0.01), except for the global similarity model.greater impact on the resulting semantic representa-tions compared to the mechanism by which missingperceptual information is inferred.5 ConclusionsIn this paper, we have presented a comparative studyof semantic representation models which computeword meaning on the basis of linguistic and per-ceptual information.
The models differ in termsof the mechanisms by which they integrate the twomodalities.
In the feature-topic model (Andrews etal., 2009), the textual and perceptual views are in-tegrated via a set of latent components that are in-ferred from the joint distribution of textual wordsand perceptual features.
The model based on Canon-ical Correlation Analysis (Hardoon et al 2004) in-tegrates the two views by deriving a consensus rep-resentation based on the correlation between the lin-guistic and perceptual modalities.
Johns and Jones?
(2012) similarity-based model simply concatentatesthe two representations.
In addition, it uses the lin-guistic representations of words to infer perceptualinformation when the latter is absent.Experiments on word association and similarityshow that all models benefit from the integration ofperceptual data.
We also find that joint models aresuperior as they obtain a closer fit with human judg-ments compared with an approach that simply con-catenates the two views.
We have also examinedhow these models perform on the perceptual infer-ence task which has implications for the wider appli-cability of grounded semantic representation mod-els.
Johns and Jones?
(2012) inference mechanismgoes some way towards reconstructing the informa-tion contained in the feature norms, however, further1431work is needed to achieve representations accurateenough to be useful in semantic tasks.In this paper we have used McRae et als (2005)norms without any extensive feature engineeringother than applying a frequency cut-off.
In the fu-ture we plan to experiment with feature selectionmethods in an attempt to represent perceptual in-formation more succinctly.
For example, it maybe that different features are appropriate for differ-ent word classes (e.g., color versus event denotingnouns).
Although feature norms are a useful first ap-proximation of perceptual data, the effort involved ineliciting them limits the scope of any computationalmodel based on normed data.
A natural avenue forfuture work would be to develop semantic represen-tation models that exploit perceptual data that is bothnaturally occurring and easily accessible (e.g., im-ages, physical simulations).Acknowledgments We are grateful to BrendanJohns for his help with the re-implementation of hismodel.
Thanks to Frank Keller and Michael Roth fortheir input on earlier versions of this work, IoannisKonstas for his help with the final version, and mem-bers of the ILCC at the School of Informatics forvaluable discussions and comments.
We acknowl-edge the support of EPSRC through project grantEP/I032916/1.ReferencesM.
Andrews, G. Vigliocco, and D. Vinson.
2009.
Inte-grating Experiential and Distributional Data to LearnSemantic Representations.
Psychological Review,116(3):463?498.Lawrence Barsalou.
1999.
Perceptual Symbol Systems.Behavioral and Brain Sciences, 22:577?609.Lawrence W. Barsalou.
2008.
Grounded Cognition.
An-nual Review of Psychology, 59:617?845.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022, March.Magnus Borga.
2001.
Canonical Correlation - a Tutorial,January.M.
H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.Park, and L. Pascual.
2004.
Cross-linguistic Analy-sis of Vocabulary in Young Children: Spanish, Dutch,French, Hebrew, Italian, Korean, and American En-glish.
Child Development, 75(4):1115?1139.Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional Semantics from Text and Images.
InProceedings of the GEMS 2011 Workshop on GEomet-rical Models of Natural Language Semantics, pages22?32, Edinburgh, UK, July.
Association for Compu-tational Linguistics.Freddy Choi, Peter Wiemer-Hastings, and JohannaMoore.
2001.
Latent Semantic Analysis for Text Seg-mentation.
In Proceedings of the 6th EMNLP, pages109?117, Seattle, WA.J Cohen and P Cohen.
1983.
Applied Multiple Regres-sion/Correlation Analysis for the Behavioral Sciences.Hillsdale, NJ: Erlbaum.Yansong Feng and Mirella Lapata.
2010.
Visual Infor-mation in Semantic Representation.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 91?99, Los Ange-les, California, June.
Association for ComputationalLinguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing Search in Context: The Con-cept Revisited.
ACMTransactions on Information Sys-tems, 20(1):116?131, January.Arthur M. Glenberg and Michael P. Kaschak.
2002.Grounding Language in Action.
Psychonomic Bulletinand Review, 9(3):558?565.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers.T.
L. Griffiths, M. Steyvers, and J.
B. Tenenbaum.
2007.Topics in Semantic Representation.
Psychological Re-view, 114(2):211?244.David R. Hardoon, Sandor R. Szedmak, and John R.Shawe-Taylor.
2004.
Canonical Correlation Analysis:An Overview with Application to Learning Methods.Neural Computation, 16(12):2639?2664.H Hotelling.
1936.
Relations between Two Sets of Vari-ates.
Biometrika, 28:312?377.Steve R. Howell, Damian Jankowicz, and SuzannaBecker.
2005.
A Model of Grounded Language Ac-quisition: Sensorimotor Features Improve Lexical andGrammatical Learning.
Journal of Memory and Lan-guage, 53(2), 258-276, 53(2):258?276.Brendan T. Johns and Michael N. Jones.
2012.
Percep-tual Inference through Global Lexical Similarity.
Top-ics in Cognitive Science, 4(1):103?120.B.
Landau, L. Smith, and S. Jones.
1998.
Object Percep-tion and Object Naming in Early Development.
Trendsin Cognitive Science, 27:19?24.T.
Landauer and S. T. Dumais.
1997.
A Solution toPlato?s Problem: the Latent Semantic Analysis The-ory of Acquisition, Induction, and Representation ofKnowledge.
Psychological Review, 104(2):211?240.1432Dekang Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of the joint AnnualMeeting of the Association for Computational Linguis-tics and International Conference on ComputationalLinguistics, pages 768?774, Montre?al, Canada.K.
McRae, G. S. Cree, M. S. Seidenberg, and C. McNor-gan.
2005.
Semantic Feature Production Norms for aLarge Set of Living and Nonliving Things.
BehaviorResearch Methods, 37(4):547?559, November.D.
L. Nelson, C. L. McEvoy, and T. A. Schreiber.
1998.The University of South Florida Word Association,Rhyme, and Word Fragment Norms.C.
Perfetti.
1998.
The Limits of Co-occurrence: Toolsand Theories in Language Research.
Discourse Pro-cesses, (25):363?377.Terry Regier.
1996.
The Human Semantic Potential.MIT Press, Cambridge, MA.T.
T. Rogers, M. A. Lambon Ralph, P. Garrard, S. Bozeat,J.
L. McClelland, J. R. Hodges, and K. Patterson.2004.
Structure and Deterioration of Semantic Mem-ory: A Neuropsychological and Computational Inves-tigation.
Psychological Review, 111(1):205?235.G Salton, A Wang, and C Yang.
1975.
A Vector-spaceModel for Information Retrieval.
Journal of the Amer-ican Society for Information Science, 18:613?620.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrim-ination.
Computational Linguistics, 24(1):97?124.Mark Steyvers.
2010.
Combining Feature Norms andText Data with Topic Models.
Acta Psychologica,133(3):234?342.Wouter Voorspoels, Wolf Vanpaemel, and Gert Storms.2008.
Exemplars and Prototypes in Natural LanguageConcepts: A Typicality-based Evaluation.
Psycho-nomic Bulletin & Review, 15:630?637.1433
