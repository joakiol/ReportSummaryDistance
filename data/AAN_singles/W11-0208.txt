Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 65?73,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsAutomatic Acquisition of Huge Training Datafor Bio-Medical Named Entity RecognitionYu Usami?
?
Han-Cheol Cho?
Naoaki Okazaki?
and Jun?ichi Tsujii?
?Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan?
Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan?
Inui Laboratory, Department of System Information Sciences, Tohoku University, Sendai, Japan?
Microsoft Research Asia, Beijing, China{yusmi, hccho}@is.s.u-tokyo.ac.jpokazaki@ecei.tohoku.ac.jpjtsujii@microsoft.comAbstractNamed Entity Recognition (NER) is an im-portant first step for BioNLP tasks, e.g., genenormalization and event extraction.
Employ-ing supervised machine learning techniquesfor achieving high performance recent NERsystems require a manually annotated corpusin which every mention of the desired seman-tic types in a text is annotated.
However, greatamounts of human effort is necessary to buildand maintain an annotated corpus.
This studyexplores a method to build a high-performanceNER without a manually annotated corpus,but using a comprehensible lexical databasethat stores numerous expressions of seman-tic types and with huge amount of unanno-tated texts.
We underscore the effectiveness ofour approach by comparing the performanceof NERs trained on an automatically acquiredtraining data and on a manually annotated cor-pus.1 IntroductionNamed Entity Recognition (NER) is the task widelyused to detect various semantic classes such asgenes (Yeh et al, 2005), proteins (Tanabe andWilbur, 2002), and diseases in the biomedical field.A na?
?ve approach to NER handles the task as adictionary-matching problem: Prepare a dictionary(gazetteer) containing textual expressions of namedentities of specific semantic types.
Scan an inputtext, and recognize a text span as a named entity ifthe dictionary includes the expression of the span.Although this approach seemingly works well, itpresents some critical issues.
First, the dictionarymust be comprehensive so that every NE mentioncan be found in the dictionary.
This requirementfor dictionaries is stringent because new terminol-ogy is being produced continuously, especially inthe biomedical field.
Second, this approach mightsuffer from an ambiguity problem in which a dic-tionary includes an expression as entries for multi-ple semantic types.
For this reason, we must usethe context information of an expression to makesure that the expression stands for the target seman-tic type.Nadeau and Sekine (2007) reported that a strongtrend exists recently in applying machine learning(ML) techniques such as Support Vector Machine(SVM) (Kazama et al, 2002; Isozaki and Kazawa,2002) and Conditional Random Field (CRF) (Set-tles, 2004) to NER, which can address these issues.In this approach, NER is formalized as a classifi-cation problem in which a given expression is clas-sified into a semantic class or other (non-NE) ex-pressions.
Because the classification problem is usu-ally modeled using supervised learning methods, weneed a manually annotated corpus for training NERclassifier.
However, preparing manually annotatedcorpus for a target domain of text and semantic typesis cost-intensive and time-consuming because hu-man experts are needed to reliably annotate NEs intext.
For this reason, manually annotated corporafor NER are often limited to a specific domain andcovers a small amount of text.In this paper we propose a novel method for au-tomatically acquiring training data for NER from acomprehensible lexical database and huge amountsof unlabeled text.
This paper presents four contribu-65Gene or Protein nameOfficial nameAliasesReferencesFigure 1: Example of an Entrez Gene record.tions:1.
We show the ineffectiveness of a na?
?vedictionary-matching for acquiring a trainingdata automatically and the significance of thequality of training data for supervised NERs2.
We explore the use of reference informationthat bridges the lexical database and unlabeledtext for acquiring high-precision and low-recalltraining data3.
We develop two strategies for expanding NEannotations, which improves the recall of thetraining data4.
The proposed method acquires a large amountof high-quality training data rapidly, decreasingthe necessity of human efforts2 Proposed methodThe proposed method requires two resources to ac-quire training data automatically: a comprehen-sive lexical database and unlabeled texts for a tar-get domain.
We chose Entrez Gene (National Li-brary of Medicine, 2005) as the lexical database be-cause it provides rich information for lexical entriesand because genes and proteins constitute an im-portant semantic classes for Bio NLP.
Entrez Geneconsists of more than six million gene or proteinrecords, each of which has various information suchas the official gene (protein) name, synonyms, or-ganism, description, and human created references.Figure 1 presents an example of an Entrez Generecord.
We created a dictionary by collecting offi-cial gene (protein) names and their synonyms fromthe Entrez Gene records.
For unlabeled text, we usethe all 2009 release MEDLINE (National Libraryof Medicine, 2009) data.
MEDLINE consists ofabout ten million abstracts covering various fields ofbiomedicine and health.
In our study, we focused onrecognizing gene and protein names within biomed-ical text.Our process to construct a NER classifier is as fol-lows: We apply the GENIA tagger (Tsuruoka et al,2005) to split the training data into tokens and to at-tach part of speech (POS) tags and chunk tags.
Inthis work, tokenization is performed by an externalprogram that separates tokens by a space, hyphen,comma, period, semicolon, or colon character.
Partof speech tags present grammatical roles of tokens,e.g.
verbs, nouns, and prepositions.
Chunk tagscompose tokens into syntactically correlated seg-ments, e.g.
verb phrases, noun phrases, and preposi-tional phrases.
We use the IOBES notation (Ratinovand Roth, 2009) to represent NE mentions with labelsequences, thereby NER is formalized as a multi-class classification problem in which a given tokenis classified into IOBES labels.
To classify labels oftokens, we use a linear kernel SVM which appliesthe one-vs.-the-rest method (Weston and Watkins,1999) to extend binary classification to multi-classclassification.
Given the t-th token xt in a sentence,we predict the label yt,yt = argmaxys(y|xt, yt?1).In this equation, s(y|xt, yt?1) presents the score(sum of feature weights) when the token xt is la-beled y.
We use yt?1 (the label of the previous to-ken) to predict yt, expecting that this feature behavesas a label bigram feature (also called translation fea-ture) in CRF.
If the sentence consists of x1 to xT , werepeat prediction of labels sequentially from the be-ginning (y1) to the end (yT ) of a sentence.
We usedLIBLINEAR (Fan et al, 2008) as an SVM imple-mentation.Table 1 lists the features used in the classifiermodeled by SVM.
For each token (?Human?
in theexample of Table 1), we created several features in-cluding: token itself (w), lowercase token (wl), partof speech (pos), chunk tag (chk), character pattern of66Name Description Example Valuew token Humanwl token in small letters humanpos part of speech NNPchk chunk tag B-NPshape entity pattern ULLLLshaped entity pattern 2 ULtype token type InitCappn(n = 1...4) prefix n characters (H,Hu,Hum,Huma)sn(n = 1...4) suffix n characters (n,an,man,uman)Table 1: Example of features used in machine learningprocess.token (shape), character pattern designated (shaped),token type (type), prefixes of length n (pn), and suf-fixes of length n (sn).
More precisely, the characterpattern of token (shape) replaces each character inthe token with either an uppercase letter (U), a low-ercase letter (L), or a digit (D).
The character pat-tern designated (shaped) is similar to a shape feature,but the consecutive character types are reduced toone symbol, for example, ?ULLLL?
(shape) is rep-resented with ?UL?
(shaped) in the example of Ta-ble 1).
The token type (type) represents whether thetoken satisfies some conditions such as ?begins witha capital letter?, ?written in all capitals?, ?writtenonly with digits?, or ?contains symbols?.
We createdunigram features and bigram features (excluding wl,pn, sn) from the prior 2 to the subsequent 2 tokensof the current position.2.1 Preliminary ExperimentAs a preliminary experiment, we acquired trainingdata using a na?
?ve dictionary-matching approach.We obtained the training data from all 2009 MED-LINE abstracts with an all gene and protein dictio-nary in Entrez Gene.
The training data consisted ofnine hundred million tokens.
We constructed a NERclassifier using only four million tokens of the train-ing data because of memory limitations.
For evalua-tion, we used the Epigenetics and Post-translationalModification (EPI) corpus BioNLP 2011 SharedTask (SIGBioMed, 2011).
Only development dataand training data are released as the EPI corpus atpresent, we used both of the data sets for evalua-tion in this experiment.
Named entities in the corpusare annotated exhaustively and belong to a single se-mantic class, Gene or Gene Product (GGP) (Ohtaet al, 2009).
We evaluated the performance of theMethod A P R F1dictionary matching 92.09 39.03 42.69 40.78trained on acquired data 85.76 10.18 23.83 14.27Table 2: Results of the preliminary experiment.
(a) It is clear that in culture media of AM,cystatin C and cathepsin B are present asproteinase?antiproteinase complexes.
(b) Temperature in the puerperium is higherin AM, and lower in PM.Figure 2: Dictionary-based gene name annotating exam-ple (annotated words are shown in italic typeface).NER on four measures: Accuracy (a), Precision (P),Recall (R), and F1-measure (F1).
We used the strictmatching criterion that a predicted named entity iscorrect if and only if the left and the right bound-aries are both correct.Table 2 presents the evaluation results of this ex-periment.
The first model ?dictionary matching?performs exact dictionary-matching on the test cor-pus.
It achieves a 40.78 F1-score.
The second model?trained on acquired data?
uses the training dataacquired automatically for constructing NER clas-sifier.
It scores very low-performance (14.27 F1-score), even compared with the simple dictionary-matching NER.
Exploring the annotated trainingdata, we investigate why this machine learning ap-proach shows extremely low performance.Figure 2 presents an example of the acquiredtraining data.
The word ?AM?
in the example (a)is correct because it is gene name, although ?AM?in the example (b) is incorrect because ?AM?
in (b)is the abbreviation of ante meridiem, which meansbefore noon.
This is a very common problem, espe-cially with abbreviations and acronyms.
If we usethis noisy training data for learning, then the resultof NER might be low because of such ambiguity.
Itis very difficult to resolve errors in the training dataeven with the help of machine learning methods.2.2 Using Reference InformationTo obtain high-precision data, we used reference in-formation included with each record in Entrez Gene.Figure 3 portrays a simple example of reference in-formation.
It shows the reference information of the67PMID 1984484:It is clear that in culture media of AM,cystatin C and cathepsin B are present asproteinase-antiproteinase complexes.Gene: AMEntrez Gene RecordsMEDLINE AbstractsPMID 23456:Temperature in puerperium is higher in AM,lower in PM.ReferenceFigure 3: Reference to MEDLINE abstract example.Entrez Gene record which describes that the gene?AM?.
The reference information indicates PMIDsin which the gene or protein is described.We applied the rule whereby we annotated adictionary-matching in each MEDLINE abstractonly if they were referred by the Entrez Generecords.
Figure 3 shows that the gene ?AM?
hasreference to the MEDLINE abstract #1984484 only.Using this reference information between the En-trez Gene record ?AM?
and the MEDLINE abstract#1984484, we can annotate the expansion ?AM?
inMEDLINE abstract #1984484 only.
In this way, wecan avoid incorrect annotation such as example b inFigure 2.We acquired training data automatically using ref-erence information, as follows:1.
Construct a gene and protein dictionary includ-ing official names, synonyms and reference in-formation in Entrez Gene2.
Apply a dictionary-matching on the all MED-LINE abstracts with the dictionary3.
Annotate the MEDLINE abstract only if it wasreferred by the Entrez Gene records which de-scribe the matched expressionsWe obtained about 48,000,000 tokens of trainingdata automatically by using this process using all the2009 MEDLINE data.
This training data includesabout 3,000,000 gene mentions.?
... in the following order: tna, gltC, gltS,pyrE; gltR is located near ...?
The three genes concerned (designatedentA, entB and entC) ...?
Within the hypoglossal nucleus largeamounts of acetylcholinesterase (AChE)activity are ...Figure 4: False negative examples.2.3 Training Data ExpansionIn the previous section, we were able to obtain train-ing data with high-precision by exploiting referenceinformation in the Entrez Gene.
However, the result-ing data include many false negatives (low-recall),meaning that correct gene names in the data areunannotated.
Figure 4 presents an example of miss-ing annotation.
In this figure, all gene mentionsare shown in italic typeface.
The underlined en-tities were annotated by using the method in Sec-tion 2.2, because they were in the Entrez Gene dic-tionary and this MEDLINE abstract was referred bythese entities.
However, the entities in italic type-face with no underline were not annotated, becausethese gene names in Entrez Gene have no link tothis MEDLINE abstract.
Those expressions becamefalse negatives and became noise for learning.
Thislow-recall problem occurred because no guaranteeexists of exhaustiveness in Entrez Gene reference in-formation.To improve the low-recall while maintaininghigh-precision, we focused on coordination struc-tures.
We assumed that coordinated noun phrasesbelong to the same semantic class.
Figure 5 portraysthe algorithm for the annotation expansion basedon coordination analysis.
We expanded trainingdata annotation using this coordination analysis al-gorithm to improve annotation recall.
This algo-rithm analyzes whether the words are reachable ornot through coordinate tokens such as ?,?, ?.
?, or?and?
from initially annotated entities.
If the wordsare reachable and their entities are in the EntrezGene records (ignoring reference information), thenthey are annotated.68Input: Sequence of sentence tokens S, Set ofsymbols and conjunctions C, Dictionary with-out reference D, Set of annotated tokens AOutput: Set of Annotated tokens Abeginfor i = 1 to |S| doif S[i] ?
A thenj ?
i?
2while 1 ?
j ?
|S| ?
S[j] ?
D ?
S[j] /?A ?
S[j + 1] ?
C doA?
A ?
{S[j]}j ?
j ?
2end whilej ?
i + 2while 1 ?
j ?
|S| ?
S[j] ?
D ?
S[j] /?A ?
S[j ?
1] ?
C doA?
A ?
{S[j]}j ?
j + 2end whileend ifend forOutput AendFigure 5: Coordination analysis algorithm.2.4 Self-trainingThe method described in Section 2.3 reduces falsenegatives based on coordination structures.
How-ever, the training data contain numerous false neg-atives that cannot be solved through coordinationanalysis.
Therefore, we used a self-training algo-rithm to automatically correct the training data.
Ingeneral, a self-training algorithm obtains trainingdata with a small amount of annotated data (seed)and a vast amount of unlabeled text, iterating thisprocess (Zadeh Kaljahi, 2010):1.
Construct a classification model from a seed,then apply the model on the unlabeled text.2.
Annotate recognized expressions as NEs.3.
Add the sentences which contain newly anno-tated expressions to the seed.In this way, a self-training algorithm obtains a hugeamount of training data.Input: Labeled training data D, Machinelearning algorithm A, Iteration times n,Threshold ?Output: Training data TnbeginT0 ?
A seed data from Di?
0D ?
D\T0while i 6= n doMi ?
Construct model with TiU ?
Sample some amount of data from DL?
Annotate U with model MiUnew ?Merge U with L if their confidencevalues are larger than ?Ti+1 ?
Ti ?
UnewD ?
D\Ui?
i + 1end whileOutput TnendFigure 6: Self-training algorithm.In contrast, our case is that we have a largeamount of training data with numerous false neg-atives.
Therefore, we adapt a self-training algo-rithm to revise the training data obtained using themethod described in Section 2.3.
Figure 6 showsthe algorithm.
We split the data set (D) obtained inSection 2.3 into a seed set (T0) and remaining set(D\T0).
Then, we iterate the cycle (0 ?
i ?
n):1.
Construct a classification model (Mi) trainedon the training data (Ti).2.
Sample some amount of data (U ) from the re-maining set (D).3.
Apply the model (Mi) on the sampled data (U ).4.
Annotate entities (L) recognized by this model.5.
Merge newly annotated expressions (L) withexpressions annotated in Section 2.3 (U ) iftheir confidence values are larger than a thresh-old (?).6.
Add the merged data (Unew) to the training data(Ti).69In this study, we prepared seed data of 683,000 to-kens (T0 in Figure 6).
In each step, 227,000 tokenswere sampled from the remaining set (U ).Because the remaining set U has high precisionand low recall, we need not revise NEs that wereannotated in Section 2.3.
It might lower the qual-ity of the training data to merge annotated entities,thus we used confidence values (Huang and Riloff,2010) to revise annotations.
Therefore, we retain theNE annotations of the remaining setU and overwritea span of a non-NE annotation only if the currentmodel predicts the span as an NE with high confi-dence.
We compute the confidence of the prediction(f(x)) which a token x is predicted as label y as,f(x) = s(x, y)?max(?z 6=ys(x, z)).Here, s(x, y) denotes the score (the sum of featureweights) computed using the SVM model describedin the beginning of Section 2.
A confidence scorepresents the difference of scores between the pre-dicted (the best) label and the second-best label.
Theconfidence value is computed for each token labelprediction.
If the confidence value is greater thana threshold (?)
and predicted as an NE of length 1token (label S in IOBES notation), then we revisethe NE annotation.
When a new NE with multipletokens (label B, I, or E in IOBES notation) is pre-dicted, we revise the NE annotation if the averageof confidence values is larger than a threshold (?
).If a prediction suggests a new entity with multipletokens xi, ..., xj , then we calculate the average ofconfidence values asf(xi, ..., xj) =1j ?
i + 1j?k=if(xk).The feature set presented in the beginning of Sec-tion 2 uses information of the tokens themselves.These features might overfit the noisy seed set, evenif we use regularization in training.
Therefore, whenwe use the algorithm of Figure 6, we do not gen-erate token (w) features from tokens themselves butonly from tokens surrounding the current token.
Inother words, we hide information from the tokens ofan entity, and learn models using information fromsurrounding words.Method A P R F1dictionary matching 92.09 39.03 42.69 40.78svm 85.76 10.18 23.83 14.27+ reference 93.74 69.25 39.12 50.00+ coordination 93.97 66.79 47.44 55.47+ self-training 93.98 63.72 51.18 56.77Table 3: Evaluation results.3 ExperimentThe training data automatically generated using theproposed method have about 48,000,000 tokens and3,000,000 gene mentions.
However, we used onlyabout 10% of this data because of the computationalcost.
For evaluation, we chose to use the BioNLP2011 Shared Task EPI corpus and evaluation mea-sures described in Section 2.1.3.1 Evaluation of Proposed MethodsIn the previous section, we proposed three methodsfor automatic training data acquisition.
We first in-vestigate the effect of these methods on the perfor-mance of NER.
Table 3 presents evaluation results.The first method ?dictionary matching?
simplyperforms exact string matching with the Entrez Genedictionary on the evaluation corpus.
It achieves a40.78 F1-measure; this F1-measure will be used asthe baseline performance.
The second method, asdescribed in Section 2.1, ?svm?
uses training datagenerated automatically from the Entrez Gene andunlabeled texts without reference information of theEntrez Gene.
The third method, ?+ reference?
ex-ploits the reference information of the Entrez Gene.This method drastically improves the performance.As shown in Table 3, this model achieves the highestprecision (69.25%) with comparable recall (39.12%)to the baseline model with a 50.00 F1-measure.
Thefourth method, ?+ coordination?, uses coordinationanalysis results to expand the initial automatic an-notation.
Compared to the ?+ reference?
model, theannotation expansion based on coordination analy-sis greatly improves the recall (+8.32%) with onlya slight decrease of the precision (-2.46%).
Thelast method ?+ self-training?
applies a self-trainingtechnique to improve the performance further.
Thismodel achieves the highest recall (51.18%) amongall models with a reasonable cost in the precision.70Figure 7: Results of self-training.To analyze the effect of self-training, we evalu-ated the performance of this model for each itera-tion.
Figure 7 shows the F1-measure of the modelas iterations increase.
The performance improvedgradually.
It did not converge even for the last iter-ation.
The size of the training data at the 17th itera-tion was used in Table 3 experiment.
It is the sameto the size of the training data for other methods.3.2 Comparison with a Manually AnnotatedCorpusNER systems achieving state-of-the-art performanceare based mostly on supervised machine learn-ing trained on manually annotated corpus.
Inthis section, we present a comparison of our best-performing NER model with a NER model trainedon manually annotated corpus.
In addition to theperformance comparison, we investigate how muchmanually annotated data is necessary to outperformour best-performing system.
In this experiment, weused only the development data for evaluation be-cause the training data are used for training the NERmodel.We split the training data of EPI corpus randomlyinto 20 pieces and evaluated the performance ofthe conventional NER system as the size of manu-ally annotated corpus increases.
Figure 8 presentsthe evaluation results.
The performance of our ourbest-performing NER is a 62.66 F1-measure; thisis shown as horizontal line in Figure 8.
The NERmodel trained on the all training data of EPI cor-Figure 8: Manual annotation vs. our method.pus achieves a 67.89 F1-measure.
The result showsthat our best-performing models achieve compara-ble performance to that of the NER model when us-ing about 40% (60,000 tokens, 2,000 sentences) ofthe manually annotated corpus.3.3 DiscussionAlthough the proposed methods help us to obtaintraining data automatically with reasonably highquality, we found some shortcomings in these meth-ods.
For example, the annotation expansion methodbased on coordination analysis might find new enti-ties in the training data precisely.
However, it wasinsufficient in the following case.tna loci, in the following order: tna, gltC,gltS, pyrE; gltR is located near ...In this example, all gene mentions are shown initalic typeface.
The words with underline were ini-tial annotation with reference information.
The sur-rounding words represented in italic typeface are an-notated by annotation expansion with coordinationanalysis.
Here, the first word ?tna?
shown in italictypeface in this example is not annotated, althoughits second mention is annotated at the annotation ex-pansion step.
We might apply the one sense per dis-course (Gale et al, 1992) heuristic to label this case.Second, the improvement of self-training tech-niques elicited less than a 1.0 F1-measure.
To as-certain the reason for this small improvement, weanalyzed the distribution of entity length both origi-71OriginalAdded0% 25% 50% 75% 100%Length 1 Length 2 Length 3 More than 4Figure 9: Distribution of entity length.nally included entities and newly added entities dur-ing self-training, as shown in Figure 9.
They repre-sent the ratio of entity length to the number of totalentities.
Figure 9 shows the added distribution ofentity length (Added) differs from the original one(Original).
Results of this analysis show that self-training mainly annotates entities of the length oneand barely recognizes entities of the length two ormore.
It might be necessary to devise a means to fol-low the corpus statistics of the ratio among the num-ber of entities of different length as the self-trainingiteration proceeds.4 Related WorkOur study focuses mainly on achieving high per-formance NER without manual annotation.
Severalprevious studies aimed at reducing the cost of man-ual annotations.Vlachos and Gasperin (2006) obtained noisytraining data from FlyBase1 with few manually an-notated abstracts from FlyBase.
This study sug-gested the possibility of acquiring high-quality train-ing data from noisy training data.
It used a boot-strapping method and a highly context-based classi-fiers to increase the number of NE mentions in thetraining data.
Even though the method achieved ahigh-performance NER in the biomedical domain, itrequires curated seed data.Whitelaw et al (2008) attempted to create ex-tremely huge training data from the Web using aseed set of entities and relations.
In generating train-ing data automatically, this study used context-basedtagging.
They reported that quite a few good re-sources (e.g., Wikipedia2) listed entities for obtain-ing training data automatically.1http://flybase.org/2http://www.wikipedia.org/Muramoto et al (2010) attempted to create train-ing data from Wikipedia as a lexical database andblogs as unlabeled text.
It collected about one mil-lion entities from these sources, but they did not re-port the performance of the NER in their paper.5 ConclusionsThis paper described an approach to the acquisi-tion of huge amounts of training data for high-performance Bio NER automatically from a lexicaldatabase and unlabeled text.
The results demon-strated that the proposed method outperformeddictionary-based NER.
Utilization of reference in-formation greatly improved its precision.
Using co-ordination analysis to expand annotation increasedrecall with slightly decreased precision.
Moreover,self-training techniques raised recall.
All strategiespresented in the paper contributed greatly to theNER performance.We showed that the self-training algorithmskewed the length distribution of NEs.
We planto improve the criteria for adding NEs during self-training.
Although we obtained a huge amount oftraining data by using the proposed method, wecould not utilize all of acquired training data be-cause they did not fit into the main memory.
A fu-ture direction for avoiding this limitation is to em-ploy an online learning algorithm (Tong and Koller,2002; Langford et al, 2009), where updates of fea-ture weights are done for each training instance.
Thenecessity of coordination handling and self-trainingoriginates from the insufficiency of reference infor-mation in the lexical database, which was not de-signed to be comprehensive.
Therefore, establish-ing missing reference information from a lexicaldatabase to unlabeled texts may provide another so-lution for improving the recall of the training data.ReferencesRong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A Li-brary for Large Linear Classification.
Journal of Ma-chine Learning Research, 9:1871?1874.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In Pro-ceedings of the workshop on Speech and Natural Lan-guage, pages 233?237.72Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages275?285.Hideki Isozaki and Hideto Kazawa.
2002.
Efficient sup-port vector classifiers for named entity recognition.
InProceedings of the 19th international conference onComputational linguistics - Volume 1, pages 1?7.Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, andJun?ichi Tsujii.
2002.
Tuning support vector ma-chines for biomedical named entity recognition.
InProceedings of the ACL-02 workshop on Natural lan-guage processing in the biomedical domain - Volume3, pages 1?8.John Langford, Lihong Li, and Tong Zhang.
2009.Sparse online learning via truncated gradient.
J. Mach.Learn.
Res., 10:777?801.Hideki Muramoto, Nobuhiro Kaji, Naoki Suenaga, andMasaru Kitsuregawa.
2010.
Learning semantic cat-egory tagger from unlabeled data.
In The Fifth NLPSymposium for Yung Researchers.
(in Japanese).David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Lingvisti-cae Investigationes, 30(1):3?26.National Library of Medicine.
2005.
Entrez Gene.
avail-able at http://www.ncbi.nlm.nih.gov/gene.National Library of Medicine.
2009.
MEDLINE.
avail-able at http://www.ncbi.nlm.nih.gov/.Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, YueWang, and Jun?ichi Tsujii.
2009.
Incorporatinggenetag-style annotation to genia corpus.
In Proceed-ings of the Workshop on Current Trends in BiomedicalNatural Language Processing, pages 106?107.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning, pages 147?155.Burr Settles.
2004.
Biomedical named entity recognitionusing conditional random fields and rich feature sets.In Proceedings of the International Joint Workshop onNatural Language Processing in Biomedicine and itsApplications, pages 104?107.SIGBioMed.
2011.
BioNLP 2011 Shared Task.http://sites.google.com/site/bionlpst/.Lorraine K. Tanabe and W. John Wilbur.
2002.
Tagginggene and protein names in biomedical text.
Bioin-formatics/computer Applications in The Biosciences,18:1124?1132.Simon Tong and Daphne Koller.
2002.
Support vectormachine active learning with applications to text clas-sification.
J. Mach.
Learn.
Res., 2:45?66.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun ?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advances inInformatics, volume 3746, pages 382?392.Andreas Vlachos and Caroline Gasperin.
2006.
Boot-strapping and evaluating named entity recognition inthe biomedical domain.
In Proceedings of the HLT-NAACL BioNLP Workshop on Linking Natural Lan-guage and Biology, pages 138?145.Jason Weston and Chris Watkins.
1999.
Support vec-tor machines for multi-class pattern recognition.
InESANN?99, pages 219?224.Casey Whitelaw, Alex Kehlenbeck, Nemanja Petrovic,and Lyle Ungar.
2008.
Web-scale named entity recog-nition.
In Proceeding of the 17th ACM conference onInformation and knowledge management, pages 123?132.Alexander Yeh, Alexander Morgan, Marc Colosimo, andLynette Hirschman.
2005.
Biocreative task 1a: genemention finding evaluation.
BMC Bioinformatics,6(1):S2.Rasoul Samad Zadeh Kaljahi.
2010.
Adapting self-training for semantic role labeling.
In Proceedings ofthe ACL 2010 Student Research Workshop, pages 91?96.73
