Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 398?408,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLatent Domain Word Alignment for Heterogeneous CorporaHoang Cuong and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamScience Park 107, 1098 XG Amsterdam, The Netherlands{c.hoang,k.simaan}@uva.nlAbstractThis work focuses on the insensitivity of ex-isting word alignment models to domain dif-ferences, which often yields suboptimal re-sults on large heterogeneous data.
A novellatent domain word alignment model is pro-posed, which induces domain-conditionedlexical and alignment statistics.
We proposeto train the model on a heterogeneous corpusunder partial supervision, using a small num-ber of seed samples from different domains.The seed samples allow estimating sharper,domain-conditioned word alignment statisticsfor sentence pairs.
Our experiments showthat the derived domain-conditioned statistics,once combined together, produce notable im-provements both in word alignment accuracyand in translation accuracy of their resultingSMT systems.1 IntroductionWord alignment currently constitutes the basis forphrase extraction and reordering in phrase-basedsystems, and its statistics provide lexical parame-ters used for smoothing the phrase pair estimates.For over two decades since IBM models (Brownet al, 1993) and the HMM alignment model (Vo-gel et al, 1996), word alignment remains an activeresearch line, e.g., see recent work (Simion et al,2013; Tamura et al, 2014; Chang et al, 2014).During the past years we witnessed an increas-ing need to collect and use large heterogeneous par-allel corpora from different domains and sources,e.g., News, Wikipedia, Parliament Proceedings.
Itis tacitly assumed that assembling a larger corpusshould improve a phrase-based system coverage andperformance.
Recent work (Sennrich et al, 2013;Carpuat et al, 2014; Cuong and Sima?an, 2014b;Kirchhoff and Bilmes, 2014; Cuong and Sima?an,2014a) shows that this is not necessarily true asphrase translations as well as (bi- and monolingual)word co-occurrence statistics could differ across do-mains.
This suggests that the word alignment qual-ity obtained from IBM and HMM alignment modelsmight also be affected in heterogeneous corpora.Intuitively, in heterogeneous data certain wordsare present across many domains, whereas oth-ers are more specific to few domains.
This sug-gests that the translation probabilities for words willbe as fractioned as the diversity of its translationsacross the domains.
Furthermore, because the IBMand HMM alignment models use context-insensitiveconditional probabilities, in heterogeneous corporathe estimates of these probabilities will be aggre-gated over different domains.
Both issues could leadto suboptimal word alignment quality.Surprisingly, the insensitivity of the existing IBMand HMM alignment models to domain differenceshas not received much attention thus far (see thestudy of Bach et al (2008) and Gao et al (2011) forreference in the literature).
We conjecture that this isbecause it is not fully clear how to define what con-stitutes a (sub)-domain.
In this paper we propose toexploit the contrast between the alignment statisticsin a handful of seed samples from different domainsin order to induce domain-conditioned probabilitiesfor each sentence pair in the heterogeneous corpus.Crucially, some sentence pairs will be more similarto a seed domain than others, whereas some sentence398pairs might be dissimilar to all seed domains.
Thenumber and choice of seed domains depends largelyon the available resources but intuitively these seeddomains are chosen to be relevant to parts of the het-erogeneous corpus.
A small number of such seedscan be expected to notably improve word alignmentaccuracy.
In fact, a single seed sample already al-lows us to exploit the contrast between two parts inthe corpus: similar or dissimilar to the seed data.Considering the small seed samples as partial su-pervision, in this paper we explore the question:how to obtain better word alignment in a heteroge-neous, mix-of-domains corpus?
We present a novellatent domain HMM alignment model, which aimsto tighten the probability estimates of the genera-tive alignment process of a sentence pair, and ofthe probability estimates of the sentence pair itselffor a specific domain.
We also present an accompa-nying training regime guided by partial supervisionusing the seed samples, exploiting the contrast be-tween the domain-conditioned alignment statisticsin these samples.
This way we aim for an align-ment model that is more domain-sensitive than theoriginal HMM alignment model.
Once the domain-conditioned statistics are induced, we discuss how tocombine them together to express the probability ofa sentence pair as a mixture over specific domains.Finally, we report experimental results over het-erogeneous corpora of 1M, 2M and 4M sentencepairs, where we are provided domain informationfor different samples of 10%, 5% and 2.5% of theheterogeneous data respectively.
A large number ofexperiments are reported, showing that the latent do-main HMM model produces notable improvementsin word alignment accuracy over the original HMMalignment model.
Furthermore, the translation ac-curacy of the resulting SMT systems is significantlyimproved across four different translation tasks.2 HMM Alignment ModelIn this section, we briefly review the HMM align-ment model (Vogel et al, 1996).
The generativestory of the model is shown in Figure 1.
The latentstates take values from the target language wordsand generate source language words.Formally, we use e = (e1, .
.
.
, eI) to denote thetarget sentence with length I and f = (f1, .
.
.
, fJ)fj?1fjfj+1aj?1ajaj+1Observed layer(source words)Latent alignmentlayer (target words)Figure 1: HMM alignment model with observed and la-tent alignment layers.to denote the source sentence with length J .
For analignment a = (a1, .
.
.
, aJ) of a sentence pair ?e, f?,the model factors P (f, a| e) into the word transla-tion and transition probabilities:P (f, a| e) =?Jj=1P (fj| eaj)P (aj| aj?1).
(1)Here, P (fj| eaj) represents the word translationprobabilities and P (aj| aj?1)1represents the tran-sition probabilities between positions.
Note thatP (aj| aj?1) depends only on the distance (aj?aj?1).
Note also that the first-order dependencymodel is an extension of the uniform dependencymodel and zero-order dependency model of IBMmodels 1 and 2, respectively.In this work, we model explicitly distances in therange ?5.
Note that null-links are also explicitlyadded in our implementation, following Och andNey (2003) and Graca et al (2010).Once the HMM alignment model is trained, themost probable alignment,?a for each sentence paircan be computed by:?a = argmaxaP (f, a| e).Here, the search problem can be solved by theViterbi algorithm.3 Latent Domain HMM Alignment ModelBecause the heterogeneous data contains a mix ofdiverse domains, the induced statistics derived fromword alignment models reflect translation prefer-ences aggregated over these domains.
In this sense,they can be considered domain-confused statistics(Cuong and Sima?an, 2014a).
This work thus fo-cuses on more representative statistics: the domain-conditioned word alignment statistics, i.e., the statis-tics with respect to each of the diverse domains.By introducing a latent variable D represent-ing domains of the heterogeneous data, we aim1The ?full?
formula for transition probabilities would beP (aj| aj?1, I).
For convenience, we ignore I in our presen-tation.399fj?1fjfj+1aj?1ajaj+1DObserved layer(source words)Latent alignmentlayer (target words)Latent domain layerFigure 2: Latent domain HMM alignment model.
An ad-ditional latent layer representing domains has been con-ditioned on by both the rest two layers.to learn the D-conditioned word alignment modelP (f, a| e, D).2Relying on the HMM alignmentmodel, our latent domain HMM alignment modelfactors P (f, a| e, D) into the domain-conditionedword translation and transition probabilities:P (f,a|e, D) =?Jj=1P (fj|eaj, D)P (aj|aj?1, D).
(2)The generative story of the model is shown in Fig-ure 2.
Note how domain-conditioned alignmentstatistics, P (?| ?, D) contain their former domain-confused alignment statistics, P (?| ?)
as special caseP (fj|eaj, D) =P (fj|eaj)P (D|fj, eaj)?fP (fj|eaj)P (D|fj, eaj),(3)P (aj|aj?1, D) =P (aj|aj?1)P (D|aj, aj?1)?ajP (aj|aj?1)P (D|aj, aj?1).
(4)With an additional latent domain layer, it becomescrucial to train the model in an efficient way.
Assuggested by Eq.
3 and 4, we could simplify train-ing by breaking up the estimation process into twosteps.
That is, we train alignment parameters, P (?| ?
)or domain parameters, P (D| ?, ?)
first, hold themfixed before training the other kind of the parame-ters.3Instead, in this work we design an algorithmthat trains both of them simultaneously via trainingdomain-conditioned parameters P (?| , ?, D) directly.2Note that P (f, a| e, D) contains their former P (f, a| e) asspecial case, i.e., P (f, a| e, D) =P (f, a| e)P (D| f, a, e)?f?aP (f, a| e)P (D| f, a, e).3This training scheme is in fact applied in the work of Cuongand Sima?an (2014a), however, for a different purpose.3.1 TrainingBasically, our model can be viewed as having aset, ?
of N subsets of domain-conditioned pa-rameters, ?Dfor N different domains, i.e., ?
={?D1, .
.
.
, ?DN}.
In this work, to simplify thelearning problem we assume that the domains arevery different from each other.
If this assumptiondoes not hold, the learning problem would shift fromsingle-label learning to multiple-label learning.
Weleave this extension for future work.Our training procedure seeks the parameters ?that maximize the log-likelihood, L of the data:L =?
?f, e?log?D?aP?D(f, e, D, a).
There,however, does not exist a closed-form solution formaximizing L, and EM comes as an alternative so-lution to fit the model.
EM maximizes L via block-coordinate ascent on a ?free energy?
lower boundF(q, ?)
(Neal and Hinton, 1999), using an aux-iliary distribution q over both the latent variables:F(q, ?)
=?
?f, e?
?D?aq logP?D(a, D, f, e)q.In the E-step of the EM algorithm, we fix ?and aim to find the distribution q?that maximizesF(q,?)
over the heterogeneous data.
Simple math-ematics lead to F(q, ?)
=?
?f, e?logP?
(f, e) ?KL[q || P?D(a, D| f, e)], where KL[?
|| ?]
is theKullback-Leiber divergence between two distribu-tions.
The distribution q?can be thus derived asq?= argmaxqF(q, ?
)= argminqKL[q || P?D(a, D| f, e)]=P?D(f, a| e, D)?aP?D(f, a| e, D)P?D(D| f, e).Here, P?D(D| f, e) aims to exploit the contrastbetween the domain-sensitive alignment statistics.Assigning higher probability to one domain forceslower probability assignment to other domains.Note that P?D(f, a| e, D) is given in Eq.
2and?aP?D(f, a| e, D) can be computed effi-ciently using dynamic programming.4Meanwhile,P?D(D| f, e) can be derived by Bayes?
rule, i.e.,P?D(D| f, e) ?
P?D(f, e| D)P?D(D).Here, the estimation of the domain prior parametersis easy, P?D(D) ??
?f, e?P?D(D| f, e).
The esti-mation of P?D(f, e| D) raises a task of defining a4Its time complexity is O(J ?
I2) for each sentence pair?f, e?
with their length J and I respectively.400E-step ?D ?
{D1, .
.
.
, DN} doc(D; f, e) = P(c)(D| f, e)c(f | e; f, e, D) = P(c)(D| f, e)?aP(c)(a| f, e, D)?Jj=1?
(f, fj)?Ii=0?
(e, ei)c(i| i?
; f, e, D) = P(c)(D| f, e)?aP(c)(a| f, e, D)?Jj=1?
(aj, i)?
(aj?1, i?
)M-step ?D ?
{D1, .
.
.
, DN} doP(+)(f |e,D) =?
?f,e?c(f |e; f, e, D)?f?
?f,e?c(f |e; f, e, D)P(+)(i|i?, D) =??f,e?c(i|i?
; f, e, D)?i??f,e?c(i|i?
; f, e, D)P(+)(D) =?
?f,e?c(D; f, e)?D?
?f,e?c(D; f, e)Figure 3: Pseudocode for the training algorithm for the latent domain HMM alignment model.
Note that notation P(c)denotes current iteration estimates, and P(+)denotes the re-estimates.generative process for every sentence pair in the het-erogeneous data with respect to a specific domain.Following (Cuong and Sima?an, 2014b), we factorit into two kinds of models in a symmetrized strat-egy: P?D(f, e| D) ?
(P?D(e| D)P?D(f| e, D) +P?D(f| D)P?D(e| f, D)).Basically, P?D(?| ?, D) can be thought of as thedomain-conditioned translation models, aiming tomodel how well a target/source sentence is gener-ated over a source/target sentence with respect to adomain.5Meanwhile, P?D(?| D) can be thought ofas the domain-conditioned language models (LMs),aiming to model how fluent a source/target sentencewith respect to a domain.
For simplicity, once thedomain-conditioned LMs are trained, they will stayfixed during training, i.e., LM probabilities are notparameters in our model.In the M-step of the EM algorithm, we fix the de-rived q?and aim to find the parameter set ?
?thatmaximizes F(q,?)
over the data.
This can be (eas-ily) done by using q?to softly fill in the values of aand D to estimate model parameters.PseudocodeIn summary, the model has three kinds of parame-ters - word translation, word transition, and domainprior parameters.
We now summarize the trainingvia presenting the pseudocode.First, we present expected count notations withrespect to domains for the parameters.
We usec(f | e; f, e, D) to denote the expected counts thatword e aligns to word f .
We use c(i| i?
; f, e, D)to denote the expected counts that two certain con-5Note that P?D(?| ?, D) =?aP?D(?, a| ?, D) and it canbe thus computed efficiently using dynamic programming.secutive source words j and j?1 align to two targetwords i and i?respectively, i.e., j aligns to i and j?1aligns to i?.
Finally, we also use c(D; f, e) to denotethe expected count of domain priors.
Note that allthe expected counts are in the translation (f| e).Figure 3 represents the pseudocode.4 Learning with Partial SupervisionWe now discuss remaining issues on how to guidethe learning with partial supervision, i.e., how touse the given domain information of seed samplesto guide the learning.Number of Domains The values of D ?
[1..(N +1)] depends on theN available seed samples plus theso-called ?out-domain,?
i.e., the part of the heteroge-neous data that is dissimilar to all of the N sampledomains.Parameter Initialization We first discuss how toinitialize the domain prior parameters.
If a sentencepair ?f, e?
belongs to a sample with a pre-specifieddomainDi, we initialize P (Di| f, e) close to 1, and,P (Di?| f, e) close to 0 for other domains i?, i?6= i.Furthermore, we uniformly create the domain priorparameters for the rest of sentence pairs.Uniform initialization for the domain-conditionedalignment parameters is also a reasonable option.Nevertheless, a more effective way is to make use ofthe domain-specific seed samples and the pool of therest sentence pairs in the heterogeneous data.6Thatis, we train the model on each of the samples, assign-6During the initialization, we assume that the pool of therest sentence pairs in the heterogeneous data is the exemplifyingsample of the out-domain.401ing the derived probabilities as the initialization fortheir corresponding domain-conditioned alignmentparameters.
In our implementation, one EM itera-tion is usually dedicated for this.
It should be notedthat we ignore the domain prior parameters in themodel during the period.Parameter Constraints During training, it wouldbe also necessary to keep the domain prior parame-ters fixed for all sentence pairs that belong to seedsamples.
This can be thought of as the constraintsderived from the partial knowledge, guiding thelearning to a desirable parameter space.Domain-conditioned LMs training We now dis-cuss how to train the domain-conditioned LMs withpartial supervision.
It would be reasonable to usethe domain-specific seed samples to train their ex-emplifying domain-conditioned LMs, and the poolof the rest sentence pairs to train the out-domainLMs.
Nevertheless, the out-domain LMs trainedon such a big corpus could dominate the otherdomain-conditioned LMs.
Following Cuong andSima?an (2014b), we rather create a ?pseudo?
out-domain sample to train the out-domain LMs, i.e.,the creation is via an inspired burn-in period.
Inbrief, an EM iteration is dedicated just to computeP (DOUT| f, e) for all sentences, ranking them andselect a small subset with highest score as the (onthe fly) pseudo out-domain sample.Note that our partial learning framework is verysimple.
There are various advanced learning frame-work that are also applicable with the partial su-pervision, e.g., Posterior Regularization (Ganchev etal., 2010).
This leaves much space for future work.5 Domain-conditioned DecodingAt test time, assigning each sentence pair to a sin-gle most likely domain (hard decision) is likely toresult in sub-optimal performance.7Instead we av-erage over domains (soft decision) while predict-ing the translation.
Formally for each sentence pair,?e, f?, we can find their best Viterbi alignment,?a as7Later experiments on word alignment will confirm this.follows:?a = argmaxa?DP (f,a, D|e)= argmaxa?DP (f,a|e, D)P (D|e)= argmaxa?DP (f,a|e, D)P (e|D)P (D).Here, we derive the last equation by applying Bayes?rule to P (D| e), i.e., P (D| e) ?
P (e| D)P (D).
In-terestingly, our Viterbi decoding now relies on a mixof domain-conditioned statistics for each sentencepair.
The computing of term?D(a) for all possi-ble alignments, a, however, is intractable, makingthe search problem difficult.
Inspired by Liang et al(2006), we opt instead for a heuristic objective func-tion as follows8:?a = argmaxa?DP (f, a| e, D)P (e| D)P (D).
(5)Here, note that?p is a lower bound for?p, when0 ?
p ?
1, according to Jensen?s inequality.
WithEq.
5, it is straightforward to design a dynamic pro-gramming algorithm to decoding, e.g., the Viterbialgorithm.
In practice, we observe that the approx-imation yields good results.
Later experiments onword alignment will present this in detail.6 Experimental SetupIn the following experiments, we use three hetero-geneous English-Spanish corpora consisting of 1M ,2M and 4M sentence pairs respectively.
These cor-pora combine two parts.
The first part respectively0.7M , 1.7M and 3.7M is collected from multipledomains and resources including EuroParl (Koehn,2005), Common Crawl, United Nation, News Com-mentary.
The second part consists of three domain-exemplifying samples consisting of roughly 100Ksentence pairs for each one (total 300K).
Each ofthese three samples (manually collected by a com-mercial partner) exemplifies a specific domain re-lated to Legal, Hardware and Pharmacy.Outlook In Section 7 we examine the word align-ment yielded by the HMM alignment model and ourlatent domain HMM alignment model.
In Section 8we proceed further to examine the translation pro-duced by derived SMT systems.8Alternative solutions could be Lagrangian relaxation-baseddecoder (DeNero and Macherey, 2011; Chang et al, 2014).402Model Domain Prior Prec.?
?
Rec.?
?
AER?
?1 MillionModel 4 (ref.)
- 71.56 - 64.59 - 32.10 -Baseline - 66.95 - 61.29 - 36.00 -LatentPharmacy 67.85 +0.90 61.72 +0.43 35.36 -0.64Legal 67.57 +0.62 62.29 +1.00 35.17 -0.83Hardware 69.41 +2.46 63.58 +2.29 33.63 -2.37Legal + Hardware + Software 69.64 +2.69 63.30 +2.01 33.68 -2.322 MillionModel 4 (ref.)
- 74.13 - 65.30 - 30.56 -Baseline - 68.34 - 61.58 - 35.22 -LatentPharmacy 68.85 +0.51 62.58 +1.00 34.43 -0.79Legal 69.98 +1.64 64.01 +2.43 33.13 -2.09Hardware 69.45 +1.11 63.23 +1.65 33.81 -1.41Legal + Hardware + Software 71.51 +3.17 63.87 +2.29 32.53 -2.694 MillionModel 4 (ref.)
- 75.53 - 65.95 - 29.58 -Baseline - 69.37 - 64.30 - 33.26 -LatentPharmacy 69.69 +0.32 62.80 -1.50 33.94 +0.68Legal 70.51 +1.14 63.94 -0.36 32.93 -0.33Hardware 71.75 +2.38 64.44 +0.14 32.10 -1.16Legal + Hardware + Software 72.16 +2.79 64.30 ?0.0 31.99 -1.27Table 1: Alignment accuracy over heterogeneous corpora.7 Word Alignment ExperimentFor alignment accuracy evaluation, we use a data setof 100 sentence pairs with their ?golden?
alignmentfrom Graca et al (2008).
Here, the golden alignmentconsists of sure links (S) and possible links (P ) foreach sentence pair.
Counting the set of generatingalignment links (A), we report the word alignmentaccuracy by precision (|A?P ||P |), recall (|A?S||S|), align-ment error rate (AER) (1 ?|A?P |+|A?S||A|+|S|) (Och andNey, 2003).9For all experiments, we use the same training con-figuration for both the baseline/the latent domainalignment model: 5 iterations for IBM model 1/thelatent domain model; 3 iterations for HMM align-ment model/the latent domain model.
For evalu-ation, we first align the sentence pairs in both di-rections and then symmetrize them using the grow-diag-final heuristic (Koehn et al, 2003).For reference we also report the performance ofa considerably more expressive Model 4, capable ofcapturing more structure, but at the expense of in-tractable inference.
Using MGIZA++ (Gao and Vo-9Note that better results correspond to larger Precision, Re-call and to smaller AER.gel, 2008), we run 5 iterations for training Model 1,3 iterations for training the HMM alignment model,Model 3 and Model 4.7.1 Learning with Single DomainWe first examine the binary case, where we aregiven domain information in advance for each kindof samples only, e.g., Legal, or Pharmacy, or Hard-ware.
For the different sizes of the heterogeneousdata (1M , 2M and 4M ) the seed sample size isthus 10%, 5% and 2.5% respectively.
Note thatin such cases, training the latent domain alignmentmodel induces two domain-conditioned statistics:in-domain vs. out-domain (D1andD2respectively).Once the model is trained, we combine the induceddomain-conditioned statistics together (Eq.
5) andexamine the produced word alignment output.Table 1 presents the results.
Most importantly, itshows that as long as providing domain informationfor reasonably large enough data, learning the latentdomain alignment model notably improves the wordalignment accuracy.
For instance, given in advancethe domain information for a sample of 10%, and5% of the heterogeneous corpora, our model con-sistently improves the word alignment accuracy in403all cases.
Meanwhile, given in advance the domaininformation for a relatively small sample of 2.5%of the heterogeneous data, the results are mixed.We obtain a good performance/slightly better per-formance/worse performance with the case of Hard-ware/Legal/Pharmacy respectively.What do domain-conditioned statistics look like?To have an idea what the induced statisticslook like, we investigate their conditional en-tropy.
Here, we present the conditional entropyfor the domain-confused/-conditioned word trans-lation statistics induced from the HMM alignmentmodel/its latent domain model.
Note that similar re-sults are observed for transition tables.Model Prior Statistics H(F| E)Baseline - Domain-confused 1348.53LatentHardwareD1-conditioned 1124.43D2-conditioned 1354.58LegalD1-conditioned 1104.58D2-conditioned 1385.35PharmacyD1-conditioned 1115.52D2-conditioned 1342.54Table 2: Conditional entropy of the statistics.Formally, for a translation table, ?F, E?, itsconditional entropy, H(F | E) can be estimatedfrom its possible word pairs, ?e, f?
: H(F | E)= ?
?eP (e)?fP (f | e) logP (f | e).
Table 2 re-veals that the induced D1-conditioned statisticsneed much less bits to represent than the induceddomain-confused statistics, e.g., 1124.43, 1104.58,1115.52 vs. 1348.53.
This implies the induced D1-conditioned statistics are much more predictablecompared to the domain-confused statistics.
Mean-while, the inducedD2-conditioned statistics are sim-ilar to the domain-confused statistics in terms of theconditional entropy, e.g., 1354.58, 1385.35, 1342.54vs.
1348.53.7.2 Learning with Multiple DomainsIt would be more interesting to learn the latent do-main alignment model for multiple domains, ratherthan learning with each of them separately.
In detail,using all the seed samples from different domains,we aim to learn four different domain-conditionedstatistics simultaneously.
Under this setting, we ob-tain good results, as described in Table 1.
For thetwo cases with the training corpora of 2M and 4Msentence pairs respectively, learning with the com-bining domain prior knowledge produces the bestword alignment accuracy compared to the rest.
Inthe last case with the training corpus of 1M sen-tence pairs, learning with the combining domainprior knowledge produces compatible with the caseof Hardware, i.e., the best binary domain case.Table 1 also reveals that the performance of ourmodel approaches Model 4, even though Model 4is much more complex and computationally expen-sive.Domain-conditioned statistics combinationWe also investigate the relation between the num-ber of domain-conditioned statistics ?involved?
inthe Viterbi decoding (Eq.
5) and the word align-ment accuracy.
Table 3 presents the results in caseof using only the induced D1-/, D2-/, D3-/, D4-conditioned statistics separately, and also using theirdifferent combinations.
Interestingly, we observethat using more domain-conditioned statistics fordecoding incrementally improves the word align-ment accuracy over the heterogeneous data.
Whilethe domain-conditioned statistics are very differentin their characteristics from each other, the resultsreveal how they are complementary to the others,conveying a mix of domains for each sentence pair.Decoding?s Statistics Prec.?
Rec.?
AER?Hard Decision (ref.)
68.49 62.80 34.48D1(Pharmacy) 64.78 59.86 37.78D2(Legal) 66.54 61.15 36.27D3(Hardware) 66.98 61.36 35.95D4(OUT) 68.46 63.01 34.38D1+ D266.80 61.72 35.84D1+ D2+ D368.54 62.80 34.46D1+ D2+ D3+ D469.64 63.30 33.68Table 3: Domain-conditioned statistics combinationfor Viterbi decoding.
The reported results are for theheterogeneous corpus of 1M sentence pairs.
Similarresults are observed for other training data.Finally, it is also tempting to make a compari-son between the hard vs. soft domain assignmentin Viterbi decoding.
Here, for hard domain decisionwe simply do decoding with the following objec-404tive function:?a = argmaxaP (f, a| e,?D), where?D = argmaxDP (D| e).
Table 3 presents the re-sults.
It reveals that a soft domain assignment on thedomain of sentence pairs results in a better align-ment accuracy than a hard domain assignment.108 Translation ExperimentIn this section, we investigate the contribution of ourmodel in terms of the translation accuracy.
Here,we run experiments on the heterogeneous corporaof 1M, 2M, and 4M sentence pairs, testing the trans-lation accuracy over four different domain-specifictest sets related to News, Pharmacy, Legal, andHardware.We use a standard state-of-the-art phrase-basedsystem as the baseline.
Our dense features includeMOSES (Koehn et al, 2007) baseline features, plushierarchical lexicalized reordering model features(Galley and Manning, 2008), and the word-level fea-ture derived from IBM model 1 score, c.f., (Och etal., 2004).11The interpolated 5-grams LMs withKneser-Ney are trained on a very large monolingualcorpus of 2B words.
We tune the systems using k-best batch MIRA (Cherry and Foster, 2012).
Finally,we use MOSES (Koehn et al, 2007) as decoder.Our system has exactly the same setting with thebaseline, except: (1) To learn the translation, we usethe alignment result derived from our latent domainHMM alignment model, rather than the HMM align-ment model; and (2) We replace the word-level fea-ture with our four domain-conditioned word-levelfeatures derived from the latent domain IBM model1.
Here, note that our latent model is learned withthe supervision from the combining domain knowl-edge of all three domain-specific seed samples.10Note that similar results are also observed for training, inwhich a soft domain assignment using soft EM produces betteralignment accuracy than a hard domain assignment using hardEM.
(See (Gao et al, 2011) for reference to hard domain assign-ment to training data.)
This is perhaps due to the characteristicsof the data we use.
For instance, News sentence pairs are usefulfor translating Legal, Financial or EuroParl to varying degrees.11For every phrase pair ?
?f, e??
with their length of m?fandle?respectively, the lexical feature estimates a probability inModel 1 style between their word pairs ?fj, ei?
(i.e.
P (?f | e?)
=le?
?m?fj=1?le?i=1P (fj|ei)).
Note that adding word-level featuresfrom both translation sides does not help much, as observed by(Och et al, 2004).
We thus add only an one from a translationside.Data System BLEU?
METEOR?
TER?News test1MModel 4 (ref.)
23.6 30.8 58.3Baseline 23.2 30.6 58.9Our System 23.5/+0.3 30.8/+0.2 58.7/-0.22MBaseline 25.9 32.4 56.1Our System 26.3/+0.4 32.6/+0.2 55.6/-0.54MBaseline 26.8 33.0 55.0Our System 27.0/+0.2 33.1/+0.1 54.7/-0.3Pharmacy1MModel 4 (ref.)
54.7 43.8 33.4Baseline 53.9 43.4 34.6Our System 54.4/+0.5 43.8/+0.4 34.0/-0.62MBaseline 54.5 43.7 34.4Our System 55.3/+0.8 44.3/+0.6 33.5/-0.94MBaseline 54.8 43.9 33.8Our System 55.0/+0.2 44.0/+0.1 33.7/-0.1Legal1MModel 4 (ref.)
56.6 44.7 34.1Baseline 56.0 44.2 35.0Our System 57.2/+1.2 44.4/+0.2 34.0/-1.02MBaseline 55.8 43.9 35.4Our System 58.3/+2.5 44.7/+0.8 33.4/-2.04MBaseline 55.9 43.9 34.3Our System 57.3/+1.4 44.4/+0.5 33.4/-0.9Hardware1MModel 4 (ref.)
75.4 53.6 17.7Baseline 74.9 53.1 19.0Our System 76.8/+1.9 53.9/+0.8 17.3/-1.72MBaseline 75.7 53.5 18.6Our System 77.4/+1.7 54.3/+0.8 17.0/-1.64MBaseline 77.1 54.2 17.3Our System 77.9/+0.8 54.5/+0.3 16.7/-0.6Table 4: Metric scores for the systems, which areaverages over multiple runs.
Bold results indicatethat the comparison is significant over the baseline.For the News translation task, we tune systems onthe News-test 2008 of 2, 051 sentence pairs and testthem on the News-test 2013 of 3, 000 sentence pairsfrom the WMT 2013 shared task (Bojar et al, 2013).For the Pharmacy, Legal, and Hardware translationtasks, we tune systems on three domain-specific devsets of 1, 000 sentence pairs and test them on threedomain-specific test sets of 1, 016, 1, 326 and 1, 721sentence pairs.
We report three metrics - BLEU(Papineni et al, 2002), METEOR (Denkowski andLavie, 2011) and TER (Snover et al, 2006), withstatistical significance at 95% confidence interval405under paired bootstrap re-sampling.12For every sys-tem reported, we run the optimizer three times, be-fore running MultEval (Clark et al, 2011) for resam-pling and significance testing.Data BLEU?
METEOR?
TER?1M +1.0 +0.4 -0.92M +1.4 +0.6 -1.34M +0.7 +0.3 -0.5Table 5: Averaged improvements across the tasks.Results are in Table 4, showing significant improve-ments across four different test sets over differentheterogeneous corpora sizes.
Table 5 gives a sum-mary of the improvements.
On average, over hetero-geneous corpora of 1M, 2M and 4M sentence pairs,our system outperforms the baseline by 1.0 BLEU,1.4 BLEU and 0.7 BLEU, respectively.
Finally, weobserve that our system produces comparably goodperformance to the MGIZA++-based system.
When1M data is considered, on three of four tasks, oursystem produces at least compatible translation ac-curacy to the corresponding MGIZA++-based sys-tem.Further analysis reveals that the improvement isdue to not only the reduction in alignment error rate,but also the use of the domain-sensitive lexical fea-tures.
Moreover, the domain-sensitive lexical fea-tures is particularly useful when the domain of thetest data matches with the domain of seed samplers.This is also widely observed in the literature, e.g.,see (Eidelman et al, 2012; Hasler et al, 2014; Hu etal., 2014).9 Related Work and ConclusionIn terms of domain-conditioned statistics for wordalignment, a distantly related research line (Tam etal., 2007; Zhao and Xing, 2008) focuses on usingdocument topics to improve the word alignment.
Interms of learning word alignment with partial su-pervision, another distantly related research line fo-cuses on semi-supervised training with partial man-ual alignments (Fraser and Marcu, 2006; Gao andVogel, 2010; Gao et al, 2010).
Finally, recent12Note that better results correspond to larger BLEU, ME-TEOR and to smaller TER.work also focuses on data selection (Kirchhoff andBilmes, 2014; Cuong and Sima?an, 2014b), mix-ture models (Carpuat et al, 2014), instance weight-ing (Foster et al, 2010) and latent variable mod-els (Cuong and Sima?an, 2014a) over heterogeneouscorpora.One main contribution of this work is the nov-elty of exploring the quality of word alignment inheterogeneous corpora.
This, surprisingly, has notreceived much attention thus far (see the study ofBach et al (2008) and Gao et al (2011) for refer-ence in the literature).
Another major contributionof this work is a learning framework for latent do-main word alignment with partial supervision usingseed domains.
We present its benefits for improv-ing not only the word alignment accuracy, but alsothe translation accuracy resulting SMT systems pro-duce.
We hope this study sparks a new research di-rection for using domain samples, which is cheap togather, but has not been exploited before.One obvious direction for future work might beto integrate the model into fertility-based align-ment models (Brown et al, 1993), as well asother recently advanced alignment frameworks, e.g.,(Simion et al, 2013; Tamura et al, 2014; Chang etal., 2014).
Another interesting direction might be tointegrate our model into advanced mixing multipletranslation models, improving SMT systems trainedon the heterogeneous data (Razmara et al, 2012;Sennrich et al, 2013; Carpuat et al, 2014).
Finally,an open question is whether it is possible to learn thelatent domain alignment model in a fully unsuper-vised style.
This challenge deserves more attentionin future work.AcknowledgementsWe are indebted to Ivan Titov and three anonymousreviewers for their constructive comments on earlierversions.
The first author is supported by the EX-PERT (EXPloiting Empirical appRoaches to Trans-lation) Initial Training Network (ITN) of the Euro-pean Union?s Seventh Framework Programme.
Thesecond author is supported by VICI grant nr.
277-89-002 from the Netherlands Organization for Sci-entific Research (NWO).406ReferencesNguyen Bach, Qin Gao, and Stephan Vogel.
2008.
Im-proving word alignment with language model basedconfidence scores.
In Proceedings of the Third Work-shop on Statistical Machine Translation.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 Workshop onStatistical Machine Translation.
In Proceedings of theEighth Workshop on Statistical Machine Translation.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19:263?311, June.Marine Carpuat, Cyril Goutte, and George Foster.
2014.Linear mixture models for robust machine translation.In Proceedings of the Ninth Workshop on StatisticalMachine Translation.Yin-Wen Chang, Alexander M. Rush, John DeNero, andMichael Collins.
2014.
A constrained viterbi relax-ation for bidirectional word alignment.
In Proceedingsof ACL.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of NAACL HLT.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In Proceedings of HLT: Short Papers.Hoang Cuong and Khalil Sima?an.
2014a.
Latent do-main phrase-based models for adaptation.
In Proceed-ings of EMNLP.Hoang Cuong and Khalil Sima?an.
2014b.
Latent do-main translation models in mix-of-domains haystack.In Proceedings of COLING.John DeNero and Klaus Macherey.
2011.
Model-basedaligner combination using dual decomposition.
InProceedings of ACL.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic metric for reliable optimization and evalu-ation of machine translation systems.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, WMT ?11.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic translationmodel adaptation.
In Proceedings of ACL: Short Pa-pers.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof EMNLP.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.
InProceedings of ACL.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of EMNLP.Kuzman Ganchev, Jo?ao Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
J. Mach.
Learn.
Res.,11:2001?2049, August.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engineer-ing, Testing, and Quality Assurance for Natural Lan-guage Processing, SETQA-NLP ?08.Qin Gao and Stephan Vogel.
2010.
Consensus versusexpertise: A case study of word alignment with me-chanical turk.
In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk, CSLDAMT ?10.Qin Gao, Nguyen Bach, and Stephan Vogel.
2010.
Asemi-supervised word alignment algorithm with par-tial manual alignments.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetricsMATR, WMT ?10.Qin Gao, Will Lewis, Chris Quirk, and Mei-Yuh Hwang.2011.
Incremental training and intentional over-fittingof word alignment.
In Proceedings of MT Summit XIII.Joao Graca, Joana Paulo Pardal, Luisa Coheur, and Dia-mantino Caseiro.
2008.
Building a golden collectionof parallel multi-language word alignment.
In Pro-ceedings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC?08).Joao Graca, Kuzman Ganchev, and Ben Taskar.
2010.Learning tractable word alignment models with com-plex constraints.
Comput.
Linguist., 36(3):481?504.Eva Hasler, Phil Blunsom, Philipp Koehn, and BarryHaddow.
2014.
Dynamic topic adaptation for phrase-based mt.
In Proceedings of EACL.Yuening Hu, Ke Zhai, Vladimir Eidelman, and JordanBoyd-Graber.
2014.
Polylingual tree-based topicmodels for translation domain adaptation.
In Proceed-ings of ACL.Katrin Kirchhoff and Jeff Bilmes.
2014.
Submodularityfor data selection in machine translation.
In Proceed-ings of EMNLP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ond?rej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open source407toolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit, pages 79?86, Phuket, Thailand.
AAMT, MMichi-gan0605 AAMT.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL.Radford M. Neal and Geoffrey E. Hinton.
1999.
Learn-ing in graphical models.
chapter A View of the EM Al-gorithm That Justifies Incremental, Sparse, and OtherVariants, pages 355?368.
MIT Press.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Comput.
Linguist., 29(1):19?51, March.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In HLT-NAACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalu-ation of machine translation.
In Proceedings of ACL.Majid Razmara, George Foster, Baskaran Sankaran, andAnoop Sarkar.
2012.
Mixing multiple translationmodels in statistical machine translation.
In Proceed-ings of ACL.Rico Sennrich, Holger Schwenk, and Walid Aransa.2013.
A multi-domain translation model frameworkfor statistical machine translation.
In Proceedings ofACL.Andrei Simion, Michael Collins, and Cliff Stein.
2013.A convex alternative to ibm model 2.
Proceedings ofEMNLP.Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla,and J. Makhoul.
2006.
A study of translation editrate with targeted human annotation.
In Proceedingsof AMTA.Yik-Cheung Tam, Ian Lane, and Tanja Schultz.
2007.Bilingual lsa-based adaptation for statistical machinetranslation.
Machine Translation, 21(4):187?207.Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.2014.
Recurrent neural networks for word alignmentmodel.
In Proceedings of ACL.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statistical trans-lation.
In Proceedings of COLING.Bing Zhao and Eric P. Xing.
2008.
Hm-bitam: Bilingualtopic exploration, word alignment, and translation.
InProceedings of NIPS.408
