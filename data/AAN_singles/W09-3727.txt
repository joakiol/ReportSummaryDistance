Proceedings of the 8th International Conference on Computational Semantics, pages 277?281,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsComputing implicit entities and eventsfor story understandingRodolfo Delmonte, Emanuele PiantaUniversita` Ca?
Foscari and IRST, Fondazione Bruno Kessler, Veneziadelmont@unive.it, pianta@itc.it1 IntroductionIn order to show that a system for text understanding has produced a soundrepresentation of the semantic and pragmatic contents of a story, it should beable to answer questions about the participants and the events occurring inthe story.
This requires processing linguistic descriptions which are lexicallyexpressed but also unexpressed ones, a task that, in our opinion, can only beaccomplished starting from full-fledged semantic representations.
The over-all task of story understanding requires in addition computing appropriatecoreference and cospecification for entities and events in what is usually re-ferred to as a Discourse Model.
All these tasks have been implemented inthe GETARUNS system, which is subdivided into two main meta-modulesor levels: the Low Level System, containing all modules that operate at sen-tence level; High Level System, containing all the modules that operate atdiscourse level by updating the Discourse Model.
The system is divided upinto a pipeline of sequential but independent modules which realize the sub-division of a parsing scheme as proposed in LFG theory where a c-structureis built before the f-structure can be projected by unification into a DAG(Direct Acyclic Graph).
In this sense we try to apply phrase-structure rulesin a given sequence as they are ordered in the grammar: whenever a syntac-tic constituent is successfully built, it is checked for semantic consistency, asLFG grammaticality principles require [1].GETARUNS has a highly sophisticated linguistically based semanticmodule which is used to build up the Discourse Model.
Semantic process-ing is strongly modularized and distributed amongst a number of differ-ent submodules which take care of Spatio-Temporal Reasoning, DiscourseLevel Anaphora Resolution, and other subsidiary processes like Topic Hier-archy which cooperate to find the most probable antecedent of coreferring277and cospecifying referential expressions when creating semantic individuals.These are then asserted in the Discourse Model (hence the DM), which isthen the sole knowledge representation used to solve nominal coreference.Semantic Mapping is performed in two steps: at first a Logical Form is pro-duced which is a structural mapping from DAGs onto unscoped well-formedformulas.
These are then turned into situational semantics informationalunits, infons which may become facts or sits (non factual situations).
Eachunit has a relation, a list of arguments which in our case receive their se-mantic roles from lower processing - a polarity, a temporal and a spatiallocation index.
Inferences can be drawn on the facts repository as will bediscussed below.2 Implicit entities and implicaturesConversational implicatures and implications in general, are based on anassumption by the addressee that the speaker is obeying the conversationalmaxims (see [2]), in particular the cooperative principle.
We regard themechanism that recovers standard implicatures and conversational implica-tions in general, as a reasoning process that uses the knowledge contained inthe semantic relations actually expressed in the utterance to recover hiddenor implied relations or events as we call them.
This reasoning process canbe partially regarded as a subproduct of an inferential process that takesspatio-temporal locations as the main component and is triggered by theneed to search for coreferent or cospecifiers to a current definite or indef-inite NP head.
This can be interpreted as bridging referential expressionentertaining some semantic relation with previously mentioned entities.
Ifwe consider a classical example from [5] (A: Can you tell me the time?
; B:Well, the milkman has come), we see that the request of the current timeis bound to a spatio-temporal location.
Using the MILKMAN rather thana WATCH to answer the question, is relatable to spatio-temporal triggers.In fact, in order to infer the right approximate time, we need to situate theCOMING event of the milkman in time, given a certain spatial location.Thus, it is just the ?pragmatic restriction?
associated to SPACE and TIMEimplied in the answer, that may trigger the inference.2.1 The restaurant textTo exemplify some of the issues presented above we present a text by [7].
Inthis text, entities may be scenario-dependent characters or main charactersindependent thereof.
Whereas the authors use the text for psychological278experimental reasons, we will focus on its computability.
(0) At the restaurant.
(1) John went into a restaurant.
(2) There was atable in the corner.
(3) The waiter took the order.
(4) The atmosphere waswarm and friendly.
(5) He began to read his book.Sentence (1) introduces both JOHN as the Main Topic in the Topic Hier-archy and RESTAURANT as the Main Location (in the role of LOCATionargument of the governing verb GO and the preposition INTO).
Sentence(2) can potentially introduce TABLE as new main Topic.
This type of sen-tences is called presentational in the linguistic literature, and has the prag-matic role of presenting an entity on the scene of the narration in an abruptmanner, or, as Centering would definite it, with a SHIFT move.
However,the TABLE does not constitute a suitable entity to be presented on thescene and the underlying import is triggering the inference that ?someoneis SITting at a TABLE?.
This inference is guided by the spatio-temporalcomponent of the system.
GETARUNS is equipped with a spatio-temporalinferential module that asserts Main Spatio-Temporal Locations to anchorevents and facts expressed by situational infons.
This happens whenever anexplicit lexical location is present in the text, as in the first sentence (theRESTAURANT).
The second sentence contains another explicit location:the CORNER.
Now, the inferential system will try to establish whether thenew location is either a deictic version of the Main Location, or it is semanti-cally included in the Main Location, or else it is a new unconnected locationthat substitutes the previous one.
The CORNER is in a meronymic seman-tic relation with RESTAURANT and thus it is understood as being a partof it.
This inference triggers the implicature that the TABLE mentioned insentence (2) is a metonymy for the SITting event.
Consequently, the systemwill not assume that the indefinite expression a table has the funciton topresent a new entity TABLE, but that an implicit entity is involved with arelated event.
The entity implied is understood as the Main Topic of thecurrent Topic Hierarchy, i.e.
JOHN.We will now concentrate our attention onto sentence (3).
To accountfor the fact that whenever a waiter takes an order there is always someonethat makes the order, GETARUNS computes TAKE ORDER as a com-pound verb with an optional implicit GOAL argument that is the personORDERing something.
The system then looks for the current Main Topic ofdiscourse or the Focus as computed by the Topic Hierarchy Algorithm, andassociates the semantic identifier to the implicit entity.
This latter procedureis triggered by the existential dummy quantifier associated to the implicit279optional argument.
However, another important process has been activatedautomatically by the presence of a singular definite NP, ?the WAITER?,which is searched at first in the Discourse Model of entities and proper-ties asserted for the previous stretch of text.
Failure in equality matchingactivates the bridging mechanism for inferences which succeeds in identify-ing the WAITER as a Social Role in a RESTAURANT, the current MainLocation.The text includes a sentence (4) that represents a psychological state-ment, that is it expresses the feelings and is viewed from the point of view ofone of the characters in the story.
The relevance of the sentence is its role inthe assignment of the antecedent to the pronominal expressions contained inthe following sentence (5).
Without such a sentence the anaphora resolutionmodule would have no way of computing JOHN as the legitimate antecedentof ?He/his?.
However, in order to capture such information, GETARUNScomputes the Point of View and Discourse Domain on the basis of Informa-tional Structure and Focus Topic by means of a Topic Hierarchy algorithmbased on [3] and [8].2.2 Common sense reasoningGETARUNS is also able to search for unexpressed relations interveningin the current spatio-temporal location.
To solve this problem in a princi-pled way we needed commonsense knowledge organized in a computationallytractable way.
This is what CONCEPTNET 2.1 ([6]) provides.
ConceptNet- available at www.conceptnet.org - is the largest freely available, machine-useable commonsense resource.
Organized as a network of semi-structurednatural language fragments, ConceptNet consists of over 250,000 elementsof commonsense knowledge.
At present it includes instances of 19 semanticrelations, representing categories of, inter alia, temporal, spatial, causal, andfunctional knowledge.
The representation chosen is semi-structured natu-ral language using lemmata rather than inflected words.
The way in whichconcepts are related reminds ?scripts?, where events may be decomposed inPreconditions, Subevents and so on, and has been inspired by Cyc ([4]).ConceptNet can be accessed in different ways; we wanted a strongly con-strained one.
We choose a list of relations from this external resource andcombine them with the information available from the processing of the textto derive Implicit Information.
In other words, we assume that what is be-ing actually said hides additional information which however is implicitelyhinted at.
ConceptNet provides the following relations: SubEventOf, First-SubeventOf, DesiresEvent, Do, CapableOf, FunctionOf, UsedFor, EventRe-280quiresObject, LocationOf.
Let us see how this information can be exploitedto interpret another classical example from the Pragmatics literature: A:I?ve just run out of petrol ; B: Oh, there?s a garage just around the corner.There are a number of missing conceptual links that need to be inferred inthis text, as follows: Inf1 : the CAR has run out of petrol; Inf2 : the CARNEEDS petrol; Inf3 : garages SELL PETROL for cars.In addition, in order to use ConceptNet we need to link petrol and garageto gas/gasoline and gas station respectively.
Now we can query the ontologyand will recover the following facts.
The whole process starts from the firstutterance and uses RUN OUT OF GAS: (Do ?car?
?run out of gas?).
Thenwe can use GAS STATION and CAR to build another query and get (Do?car?
?get fuel at gas station?
), where FUEL and GASoline are in IsArelation.
Eventually we may still get additional information on the reasonwhy this has to be done: (Do ?person?
?don?t want to run out of gas?
),(SubeventOf ?drive car?
?you run out of gas?
), (Do ?car?
?need gas petrolin order to function?
), (Do ?gas station?
?sell fuel for automobile?).
Thesemay all constitute additional commonsense knowledge that may be used tofurther explain and clarify the implicature.References[1] Joan Bresnan.
Lexical-Functional Syntax (Blackwell Textbooks in Linguistics).Blackwell Publisher, September 2000.
[2] H.P.
Grice.
Logic and conversation.
In P. Cole and J.L.
Morgan, editors, Syntaxand Semantics, volume 3.
New York Academic Press, 1975.
[3] B. Grosz.
Focusing and description in natural language dialogues.
CambridgeUniversity Press, 1981.
[4] Douglas B. Lenat.
CYC: A large-scale investment in knowledge infrastructure.Communications of the ACM, 38(11):33?38, 1995.
[5] Stephen Levinson.
Pragmatics.
Cambridge University Press, 1983.
[6] Hugo Liu and Push Singh.
ConceptNet: A practical commonsense reasoningtoolkit.
BT Technology Journal, 22(211?226), 2004.
[7] A.J.
Sanford and S.C. Garrod.
Thematic subjecthood and cognitive constraintson discourse structure.
Journal of Pragmatics, 12(5-6):519?534, 1988.
[8] C. Sidner.
Focusing in the comprehension of definite anaphora.
In M. Bradyand R. Berwick, editors, Computational models of discourse, pages 267?330.MIT Press, 1983.281
