COM PUTATIONAL ('Obl PLEXITY ANDLEXICAL FUNCTIONAL GRAMMARRobert C. BerwickMIT Artificial Intelligence Laboratory, Cambridge, MA1.
INTRODUCTIONAn important goal of ntodent linguistic theory is to characterize as narrowlyas possible the class of natural !anguaooes.
An adequate linguistic theoryshould be broad enough to cover observed variation iu human languages, andyet narrow enough to account for what might be dubbed "cognitivedemands" -- among these, perhaps, the demands of lcarnability andpars,ability.
If cognitive demands are to carry any real theoretical weight, thenpresumably a language may be a (theoretically) pos~ible human language,and yet be "inaccessible" because it is not leanmble or pa~able.Formal results along these lines have already been obtained for certain kindsof'rransformational Generative Grammars: for example, Peters and Ritchie\[I\] showed that Aspeel~-style unrest~ted transtbrmational grammars cangenerate any recursively cnumerablc set: while Rounds (2\] \[31 extended thiswork by demonstrating that modestly r~tricted transformational grammar~(TGs) can generate languages whose recognition time is provhblyexpm~cntial.
(In Rounds" proof, transformatiocs are subject o a "terminallength non-decreasing" condition, as suggested by Peters and Myhill.)
Thus,in the worst case TGs generate languages whose recognition is widelyrecognized to be computatiofrally intrdctable.
Whether this "worst case"complexiw analysis has any real import for actual linguistic study has beenthe subject of ~me debate (for discussion, see Chomsky \[4l; Berwiek andWeinbcrg \[5\]).
Without resolving that cuntroversy here howeser, one thin-g-can be said: to make TGs cmciendy parsable one might providecon~train~ For instance, these additional s'~'ictutes could be roughly of thesort advocated in Marcus' work on patsinB \[6\] -- constraints specifying thatTG-based languages must haw parsers that meet certain "lecalityconditions".
The Marcus' constraints apparently amount to an extension ofKnuth's l.,R(k) locality condition \[7\] to a (restricted) version of a two-stackdeterministic push-down automaton.
(The need tbr LR(k)-like restrictions inorder to ensure efficient processability was also recognized by Rounds \[21.
)Recently, a new theory of grammar has been advanced with the explictiystated aim of meeting the dual demands of tearnability and pa~ability - theLexical Functional Grammars (LFGs) of Bresnan \[!~ I.
The theory of l.exicalFunctional Grammars is claimed to have all the dc~riptive merits oftransformational grammar, but none of its compotational unruliness, Int.FG, there are no transformations (as classically described); the worktbrmerly ascribed to transformations such as "passive" is shouldered byinformation stored in Ibxical entries associated with lexical items.
Theclimmation of transformational power naturally gives rise to the hope that alexically-based system would be computationally simpler than atransformational one.An interesting question then is to determine, as has already been done for thecase of certain brands of transformational grammar, just what the "worgcase" conlputational complexity for the recognition of LFG languages is.
Ifthe recognititm time complexiW for languages generated by the basic LFGrheas can be as complcx as that for languages generated by a modestlyrestricted U'ansfunnational system, then presumably \[.FG will also have toadd additional coastraiuts, beyond those provided in its basic theory, in order',u ensure fficient parsability.The main result of this paper is to show that certain \[.exical FunctionalGrammars can generate languages whose recognition time /s very likelyct~mput.
'xtionally intractable, at Ie,'LSt a~urding to our current understandingof wl~at is or is not rapidly solvable.
Briefly.
the demonstration proceeds byshowing how a problem that is widely conjectured to be cumputationallydimcult -- namely, whether there exists ~n ~%ignment of Us and O's (or '*T"sand "l~'s) to tire litcrals ofa Bta~lcan formula in conjunctive normal form thatmakes the forrnula evaluate to "I" (or "tree") -- can be re-expressed as theprublcm of recognizing whctl~er a particular string is or is uot a member ufthe language generated by a certain lexical functional grammar.
This"reduction" shows that in the worst case the recognitinn of I.FG lanp, uagescan be just as hard as the original Boolean satisfiability problem.
Since k iswidcly conjectured that there cannot be a polynomial-time alguriti'n'n forsatisfiabiliW (the problem is NP-complete), there canno~, be a polynomial-dinerecognition algorithm for LFG's in general either.
Note that this resultsharpens that in Kaplan and Bresnan \[81: there it is shown only that LFG's(weakly) generate some subset of the class of context-sensitive languages(including some strictly context-sensitive languages) and therefore, in theworst case, exponential time is known to be sufficient (though not necessary)to reaognize any LFG language.
The result in \[81 thus does not address thequestion of how much time, in the worst case, is necesmry to recognize LFGlanguages.
The result of this paper indicates that in the worst case more thanpnlynomial time will probably be necessary.
(The reason for the hedlp.
""probably" will become apparent below; it hinges upon the central unsolvedconjecture of current complexity theory.)
In short then, this result places the?
LFG languages more precisely in the complexity hierarchy.It also toms out to be instructive to inquire into just why a lexically-basedapproach can tom out to be compurationally difficult, and howcomputational tractability may be guaranteed.
Advocates of lexically-basedtheories may have thought (and some Pave explicitly stated) that thebanishment of transformations is a compumdonally wise move becausetransformations are computationally "expensive."
Eliminate thetransformations, so this casual argument goes, and one has eliminated allcomptitational problents.
In~guingiy though, when one examines the proofto be given below, the computational work done by transformations in oldertheories re, emerges in the lexical grammar as the problem of choosingbetween alternative categorizations for lexical items - deciding, in a mannerof speaking, whether a particular terminal item is a Noun or a Verb (as withthe word k/ss in English).
This power .of choice, coupled with an ability toexpress co-occurrence constraints over arbitrary distances across terminaltokens in a string (as in Subjeat-Verb number agreement) seems to be all thatis required to make the recognition of LFG languages intr~table.
The workdoee by transformations has been exchanged for work done by lexieM~.hemas.
but the overall computational burden remains mugidy the same.This leaves the question posed in the opening paragraph: jug what sorts ofconstraints on natural anguages are required in order to ensure efficientparsabil)tg?
An infoqrln~ argume.nt can be made that Marcus' work \[6}provides a good first attack on just this kind of characteriza~n.
M~x:us'claim was that languages easily parsed {not "garden-pathed") by o?oole couldbe precisely modeled by the languages easily pm'sed by a certain type ofrestricted, deterministic, two-stack parsing machine.
But this machine can bespawn to be a (weak) non-canonical extension of the I,R(k) grammars, asproposed by Knuth \[51.Finally, this paper will discuss the relevance of this technical result for moredown-to-earth computational linguistics.
As it turns out, even though 2eneralLFG's may well be computationally intractable, it is easy to imagine avarietyof additional constraints for I..FG theory that provide a way to sidesteparovr,d the reduction argument.
All of these additional r~trictions amount tomaking the LFG theory more restricted, in such a way that the reductionargument cannot be made to work.
For example, one effective restriction isto stipulate that there can only be a finite stock of features with which to labelIcxical items.
In any case, the moral of the story is an unsurprising one:specificity and constraints can absolve a theory of computationalintr~tability.
What may be more surprising is that the requisite localityconstraints seem to be useful for a variety of theories of grammar, fromtransformational grmnmar to lexieal functional gr,'unmar.72.
A REVIEW Ok" 131:DU,,eTI'ION ARGUMENTSThe demonstration f the computational complexity of I.FGs rcii~ upon thestandard complexity-theoretic technique of reduction.
Becauso this methodmay be unf.
',,ndiar to many readers, a short review is presented immediatelybelow: this is followed by a sketch of the reduction proper.The idea behind the reduction technique is to take a difficult problem, in thiscase.
the problem of determining the satisfiability of Boolean .rormu/as inconjunctive normal form (CNF), and show that the known problem can bequickly transfumled into the problem whns?
complexity remains to bedetermined, in this case.
the problem of deciding whether a given string is inthe language generated by a given Lexical Functional Grammar.
Before thereduction proper is reviewed, some definitional groundwork must bepresented, A I\]ooleanformula in cenjunctDe normal form is a conjunction ofdisjunctions.
A formula is satisfiable just in case there exkts ome assignmentof T's and \['~s (or t's and 0's) to the Iiterals of the formula X i that fumes theevahmtion of the enure formula to be 1"; oLherwise~ the formula is said to beunsmisfiable.
For cxmnpl?
(X2VX3 VXT)A(XIV~2VX4)A(X3VXIVX 7 )is satisfiable, since the assignment of Xz=T (hence ~'2= F'), X3= F (henceX3='l').
XT=F (.~./=T).
XI=T (XI=F), and X4=F makes the wholeformula cvalute to "T".
The reductioo in the proof below uses a somewhatmore restuictcd format where every term is comprised of the disjunction ofexacdy three \[itcrats, o-called 3-CNF(or "3-SAT").
"l'his restriction entailsno loss of" gcncralit!,, (see Hopcmft and Ullman, \[9\].
Chapter 12), since thisrestricted furmat is also NP-complete.How does a reduction show that the LFG recognition problem must be atleast .
',s hard (computatiomdly speaking) as the original problem of Booleansatisfiability?
Ihe answer is that any decision procedure for LFG recognitioncould be used as'a correspondingly f~st procedure for 3-CNF.
as follows:(1) Given an instance of a 3-CNF problem (the question of whether thereexists a satisl'ying assignment for a given luminia in 3-CNF), apply thetransfi~mlational algurithm provided by the reduction: this algorithm is itself~L%sumed tO execute quickly, in polynomial time or less.
"\]~e algurid'anoutputs a corresponding LFG decision problem, namely: (i) a lexicalfunctional grammar and (ii) a string to be tested lbr membership n thelanguage generated by the I.FG.
The LFG recognition problem r~presents ormimics the decision problem for 3-CNF in the sense that the "yes" and "no ~answers to both ~dsfiability problem and membership roblem mustcoincide (if there is a satisfying ag,;ignmenL then the corresponding LFGdecision problem should give a "yeS" answer, etc.).
(2) Solve the LFG decision problem -- the string-LFG pair - output by Steph if the string is in the LFG language, the original formula was satisfiable; ifnot.
unsadsfiable.
(Note that the grammar and string so constructed depend upon just whatformula is under analysis; that is.
For each different CNF  formula, theprocedure presented above outputs a diffemnt LFG grammar and suingcombination.
In the LFG case it is important to remcmber that "grammar"really means "grammar plus lexicon" - as one might expect in alexically-based theory.
S. Petet~ has observed that a siighdy differentreduction allows one to keep most of the grammar fixed across all possibleinput formulas, constructing only different-sized lexicons for each differentCN\[: Formula; for details, see below.
)To see how a reduction can tell us something about he "worst ca.~" time orspace complexity required to recognize whether a string is or is not in an LFGlanguage, suppose for example that the decision procedure for determiningwhether a string is in an LFG language takes polynomial time (that is, takestime n k on a deterministic "ruling machine, for some integer k, where n= thelength of the input string).
Then.
since the composition of two polynomialalgorithms can be readily shown to take only polynomial time (see \[91Chapter 12), the entire process sketched above, from input of the CHFformula to the decision about its satisfiability, will take only polynomial time.However, CNF (or 3-CNF) has no known polynomial time algorithm, andindeed, it is considered exceedi~zgly unlikely that one could exists.
"Vaerefore,it is just as unJikely that LFG recognition could be done (in general) inpolynomial time,The theory of computational complexity has a much more compact term forproblems like CNF: CNF is NP-cnmolcte.
This label is easily deciphered:(1) CNF is in the class NP.
that is, the class or" languages that can berecognized by a .qD.n-deterministic Tunng machine in Dgivnomial time.
(Hence the abbreviabon "NP", for "non-deterministic polynomial".
To seethat CNF ,', in the class NP, note that one can simply guess all possiblecombinations of truth assignments to iiterab, and check each guess inpolynomial lune.
)(2) CNF  is complete, that is.
all other languages in the class NP can be quicklyreduced to some CNF formula, (Roughly.
one shows that Boolean formulascan be used to "simuiam" any valid computation of a non-determinis~Toting machine,)Since the class of problems solvable in polynomial time on a determinist~Turing machine (conventionally notated.
P) is trivially contained in the clas~so solved by a nondcterministic Turing machine, the class P must be a subsetofdle class NP.
A well-known, v, ell-studicd, and still open question is whtherthe class P is a nroner subset of the class NP.
that is.
whether there areproblems solvable i.t non-deterministic polynomial time that cannot besolved in deterministic polynomial time.. Ik'causc all ofthe several thousandNP-eomplcte problems now catalogued have so far proved recalcitrant todeterministic polynomial time solution, it is widely held that P must indeedIx a proper subsot of NP, and therefore that dte best possible algorithms forsolving NP.complcte problems must take more than polynomial time (ingeneral, the algorithms now known tbr such pmbtems inw~lve exponentialcombinatorial search, in one fashion or another; these are essentially methods'that do no Ixtter than to bnttally simulate -- deterministically, ofcout~e - anon-deterministic machine that "guesses" possible answeix)To repeat the Force of the reduction argument then, it" all LFG rec~it ionproblems were solvable in polynomial time.
then the ability tu quickly reduceCNF Formulas to LFG recognition problems implies that all HP-completeproblems would IX sulvabl?
in polynomial rime.
and that the class P=theclass NP.
This possibility seems extremely remote, tlence, our assumptionthat there is a fast (general) procedure for recognizing whether a string is or isnot in the language generated by an arbitrary LFG grmnmar must be false.In the mrminology of complexity theory, LFG recognition must be NP-hard- "as hard as" any other NP problem, including the NP-complete problems.This means only that LFG recogntion isat least as haedas other NP-complcmproblems -- it could still be more ditlicult (lie in some class that contains theclass NP).
If one could also show that the languages generated by LFC.s arcin the class NP, then LFGs would be shown to be NP-complcte.
This pal~'rstops hort of proving this last claim, but simply conjectures that LFGs are inthe clasa NP.3.A sg~c8 o ~ l g ~To carry out this demonstration in detail one mug explicidy describe thet~nsformauon procedure that takes as input a formula in CHF  and outputs acorresponding LFG decision problem - a string to be tested for membershipin a LFG language and the LFG itself.
One must also show that this can bedone quickly, in a number of stc~ proportional to (at most) the lefigth of theoriginal formula to some polyoomlal power, l~t us dispose of the last pointfirst.
The string to be tested for membership in the LFG language will simplybe the original formula, sans parentheses and logical symbols; the LFGrecognition problem is to lind a well-formed derivation of this string withrespect to the grammar to be provided.
Since the actual grammar and stringone has to wrim down to "simulate" the CNF problem turn out to be noworse than linearly larger than the original formula` an upper bound of say.time n-cubed (where n=length of the original formula) is more thansufficient to construct a corresponding LFG; thus the reduction procedureitself can be done in polynomial time.
as required.
This paper will thereforehave nothing fiarther to say about the time bound on the transformationprocedure.8Some caveats are in order .before mbarking on a proof sketch of thisrednctio?
First of all, the relevant details of the LFG theory will have to becovered on-the-fly; see \[8\] for more  discussion.'
Also, the grammar that isoutput by the reduction procedure will not look very much like a grammarfor a natural language, ~ilthbugh the grammatical devices that will beemployed will in every way be those that are an essential part uf the LFGtheory.
(namely, feature agreement, the lexical analog of Subject or Object"control", lexical ambiguity, and a garden variety context-free grammar.)
Inother words, although it is most unlikely that any namnd language wouldencode the satisfiability probl.cm (and hence be iutractablc) in just themanner oudined below, on the other hand.
no "exotic" LFG machinery isused in the reduction.
Indeed.
some of the more powerful LFG notationalformalisms -- long-distance binding existential nd negative feature operators- have not been exploited.
(An earlier proof made use of an existentialoperator in the feature machinery of LFG, but the reduction presented heredoes not.
)To make good this demonstration e must set out just what the ~tisfiabilityproblem is and what the decision problem for membership n an I..FGlanguage is.
Recall that a formula in conjunctive normal form is satisfiablejust in case every conjunctive term evaluates totrue, that is, at least one literalin each term is true.
The satisfiability problem is to find an assignment of'I"sand Fs to the literals at the bottom (note that the comolcment of literals isalso permitted) such that the root node at the top gets the value "T" (forli31g).
How can we get a lexical functional grammar to represent thisproblem?
What we want is for satisfying a.~ignments o correspond to towell-formed sentences of some corresponding LFG grammar, andnon,satisfvint assignments to correspond to sentences that are notwell-!
'ormed, according to the LFG grammar:.satisftable non-satisfiablefo?
la  w form la|n~Wsentence w' IS sente w" IS NOTin LFG language L(G) in LFG language L(G)Figure I.
A Reduction Must Preserve Soludona to the Original ProblemSince one wants the satisfying/non-satisfying assignments of any particularformula "to map over into well-formed/ill-formed sentences, one mustobviously exploit the LFG machinery for capturing well-formedncmconditions for sentences, First of all, an LFG contains a base context-freem-ammar.
A minimal condition for a sentence (considered as a string) to be inthe language generated by a lexical-functional grammar is that it can begenerated by this base grammar:, such a sentence is then said to have awell-formed constituent s ructure.
For example, if the base roles includedS=bNP VP; Vp=Pv NP, then (glossing over details of Noun Phrase rules)the sentence John kissed the baby would be well-formed but John the babywould not.
Note that this assumes, as usual, the existence of a lexiconthat provides acategorization for each terminal item, e.g., that baby is of theeategury N, k/xr, ed is a V, etc.
Importantly then.
this well-formednesscn/~dition requires us to provide at least one legitimate oarse tree for thecandidate sentence that shows how it may be derived from the underlyingLFG base context-free grammar.
(There could be more than one legitimatetree if the underlying grammar isambiguous.)
Note further that he choice ofcategorization for a lexical item may be crucial.
If baby was assumed tobe ofcategory V, then both sentences above would be ill-formed.A second major component of the LFG theory is the provision for adding aset of se-called functional equations to the base context-free rules.
The~equations ,are used to account for that the co-oecurrence r strictions that areso much a part of natural languages (e,g., Subject-Ve~ agreement).
Roughly,one i s  allowed to associate featur~ with lexical entries and with thenon-terminals of specified context-free rules; these features have values.
Theequation machinery isused to pass features in certain ways around the par,~tree, and conflicting values for the same feature are cause for rejecting acandidate analysis.
To take the Subject-Verb agreement example, considerthe sentence the baby is kissing John.
The lexical entry for baby (consideredas a Noun) might have the Number feature, with the value sinzular.
Thelexieal entry for is might assert that the number feature of the %tbiect aboveit in the parse tree must have the value singular: meanwhile, the featurevalues for Subject are automatically found by another ule (associated withthe Noun Phrase portion ofS=:,NP VP) that grabs whatever features it findsbelow the NP node and copies them up above to the S node.
Thus the S nodegets the Subject feature, with whatever value it has passed from baby below --namely, the value sintadar: this accords with the dicates of the verb/s, and allis well.
Similarly, in the sentence, the boys in the band is kissing John, bayspasses up the number value olural, and this clashes with the verb's constraint;as a result his sentence is judged ill-formed:,lqpTp,/jfeatures?
Subject Number.Singular or Plural?
= CLASHIINumber.plural V *, Number:singularlJthe boys in the band is" kissing John.Figure 2.
Co-eccurrence R strictions are Enforced by Feature Checking in anLFG.It is important to note that the feature comparability check requires (1) aparticular constituent structure trec (a pm~c tree); and (2) an assignment ofterminal items (words) to lexical categories -- e.g., in the first Subject-Verbagreement example above, baby was assigned to be of the category N, aNoun.
The tree is obviously required because the feature checkingmachinery propagates values according to the links specified by thederivation tree; the assignment of terminal items to categories is crucialbecause in most ca~ the values of features are derived from those listed inthe lexical entry for an item (as the value of the numb~er feature was derivedfrtnn the lexical entry for the Noun form of bab~,).
One and the sameterminal item can have two distinct lexical entries, corresponding to distinctlexical categorizations; for example, baby can be both a Noun and a Verb.
Ifwe had picked baby to be a Verb, and hence had adupted ~hatevcr featuresare associated with the Verb entry for baby to be propagated up the tree, thenthe string that was previously well-formed, the baby is kissing John wouldnow be considered eviant.
If a string is ill-formed under all possiblederivation trees and assignments of features From possible lexicalcategorizations, then that string is norin the language generated by the LFG.The possibility of multiple derivation trees and lexical categorizations (andhence multiple feature bundles) for one and the same terminal item plays acrucial role in the reduction proof: it is intended to capture the satisfiabilityproblem of deciding whether to give a literal X i a value of" l "  or "F".Finally, LFG also provides a way to express the familiar patterning ofgrammatical relations (e.g.. "Subject" and "Object") found in naturallanguage.
For example, transitive verl~ must have objects.
This fact of life(expressed inan Aspects.style transformational grammar by subcategorizationre~ictions) is captured in LFG by specifying a so-called ~ (forpredicate) feature with a Verb: the PRED can describe what grammaticalrelations like "Subject" and "Object" must be filled in after feature passinghas taken place in order for the analysis to be well-formed.
For instance, atransitive verb like kiss might have the pattern, kiss((SubjeetXObject)), andthus demand that the Subject and Object (now considered to be "features")have some value in the final analysis.
The values for Subject and Objectmight of course be provided from some other branch of the parse tree, asprovided by the feature propagation machinery; for example, the Obiectfeature could be filled in from the Noun Phrase part of the VP expansion:'SUBJECT: Sue 1S (eatures:lPRED !
*kiss<(SubjeetXObjec0)lJV NP.
sue / Ikm JohnFigure 3.
Predicate Templates Can Demand That a Subject or Object beFilled In.But.
if the Object were not filled in, thee die analysis is declared func#onallyincomplele, and is ruled our.
This device is used tO cast out sentences uch as.t/m baby kL~eg$o much for the LFG machinery that is required for the reduction proo?
(There are additional capabilities in the LFG theory, such as long-distancebinding, but these will nut be called upon in the demonstration below.
)What then does the LFG repmsentador, of die satisfiabillty problem looklike?
Basically, there are three parts to the sausfiability problem that mug bemimicked by the LFG: (I) the assignment ofvaines to literals, e.g., X2-)'r";X4-Y'F"; (2) the co-ordination of value assignments across intervening literalsin the formula; e.g., the literal X 2 can appear in several different erms, butone is nut allowed to assign it the value "1" in one term and the value "F" inanother (and the same goes for the complement of ~, literal: if X 2 has dievalue 'T ' .
"~z cannot have die valu~ "V'): and (3) ~tisfiability mustcorresl~md to LFG wcll-formedness, i.e.
each term has the truth value "r"just in case at least one literal in the tenn is assigned "I" and all terms mustevaluate to " l  TM.Let us now go over how these components may be reproduced in an LFGoone by one.
(t) Assignments: The input string to be tested for membership in the LFGwill simply be the original formula, sans parentheses and logical symbols: theterminal items are thus just a string of Xi's.
Recall that the job of checkingthe string for well-formedn,.-~s involves finding a derivation tree for the suing,solving the ancillary co-oecurrencc equations (by feature propagatiun), andchetking for functional completeness.
Now, the cuntext-fre~ grammarconstructed by the transformation procedure will be set up so ,'ts to generate avirtual copy of the associated formula, down to the point where literals X i area~signed dicir values o f ' r "  or "F".
If the original CNF form had N terms.this part of grammar would look like:S~,T 1 T 2 .. T n (one "l" for each term)Ti=~Yi Yi Yk (one triple of Y's per term)Several comments are in order here.
(I) The context-free base that is built depends upon the original CNFformula that is input, since the number of terms.'
n, varies from formula toformula.
In Stanley Peters' improved version of the reduction proof, thecontext-free base is fixed for all formulas with the rules:S='S S'S'==' T T TorSmT T ForT  F ForT  F Tot_(remaining twelve expansions that have at least one "I" in each triple)The Peters grammar works by recursing until die right number of terms isgenerated (any sentences that are too long or too short cannot be matched tothe input formula).
Thus, the number of terms in the original CNF formulaneed not be explicidy encoded into the base grammar.
(2) The subscripts Lj, and k depend on the actual subscripts in the originalformula.
(3) The Yi are not terminal items, but are non-terminals.
(4) This grammar will have to be slightly modified in order for the reductionto work.
~ will become apparent shordy.Note that so far there are no rules to extend the parse tree down co the levelof terminal items, the X r The next step does this and at the same time addsthe power to choose between " r "  and "F" assignments to literais.
Oneincludes in the context-free base grammar two productions deriving eacJaterminal item Xi, namely, XiT=~X i and XiF'mpX i, corresponding to anassgnment of - r "  or "F" to the formula literal X i (it is important not to getconfused here between the literais of the formula - these are terminalelements in the lexical functional grammar - and die literals of the grammar- the non-terminal" symbols.)
One must also add, obviously, the rulesYi=~XiTlXi F, for each i, and rules corresponding to.
the negations ofvariables, "~ir--'~i Note that these are not "exotic" t.FG rules: exacdy thesame sort of rule is required in the baby case, i.e.. N~baby or V=~.baby,corresponding to whether baby is a Noun or a Verb.
Now.
the lexical entriesfor the "XiT "' categ.rization of X i will look very different from the "XiF'eategodzadon f X i. just as one might expect he N and V forms for baby tobe different.
Here is what the entries for the two categorizations of X i looklike:X~ XiT (Ttmth-assignment)=T(Tassign Xi)=TXl: XiF (Tassign X i) =FThe feature assignments for the negation of the literal X i is simply the dual ofthe entries above (since the sense of"T"  and "I-" is reve~cd):~" .~'iT (T truth-amsignment) = T(fa.~igu X.~: F.x,v :TThe role of the additional "truth-ass/gnment" feature will be explainedbdow.Figure 4.
Sample Lexieal Entries to Reproduce the Ass/gument ofT's and l'~sto a literal X rThe upward-dirked arrows in the entries reflect the LFG re.mumpropagation machinery.
In the case of the X|T entry, for instance, they say to"make the Truth-assitnment feature of the node above XiT have the value"T =.
and make the ~.
pordon of the A~izn feature of the node above havethe value T." This feature propagation device is what reproduces theassignment of T's and Fs to the CNF limrala, \[f we have a triple of sucheicmen~ and at least one of d~m is expanded out to XiT.
then the restorepmpagauon machinery of LFG will merae the common feature names intuone large m~cture for the node above, reflecting the assignments made;moreover, the term ~ll get a tilled-in truth assignment value just in case at~ag one of the expansions selected an XIT path:terminalsuing:T'X ifPnmre s~rtlCtUr?
:i F i kFX X kt ruth'assignment= IXj= L L::aJFigure 5.
The LFG Feature Pmpagatiun Machinery is Used to PercolateFeature Assigumants from the Lexicon.10(The features are passed transparendy through the interveningYi nodes via the LFG ".copy" device.
(T = J.
);this simply means that all the features of the node below the node towhich the "copy" up-add-down arrow'~ are attached are to bethe same as those of the node above the up-and-down arrows.
)It is p!ain that this mechanism mimics the a.~ignment of valueS~'.o literahrequired by the satisfiability problem.
(2) Co-ordination of aasignments: One must also guarantee that the X i valueassigned at one place in the tree is not contradicted by an X| or X i elsewhere.To ensure this, we use the LFG co-occurrence agreement machinery: theAssilzn feature-bundle is pass~ up from each term T i to the highest node inthe parse tree (one simply adds the (i" = \]3 notadon to each T i rule in order toindicate this).
The Assign feature at this node will thus contain the union ofall ~ feature bundles passed up by all terms.
If any X i values conflict,then the resulting structure is judged ill-formed.
Thus, only compatible Xiassignments are well-formed:features: Assign: ~..i =T  or F3.1T~, .
.
.
.
~ Clashl~T X~TI{Tz~gn X~) = T (Tassign X~ = F)Figure 6.
The Feature Comparability Machinery of LFG can Fon:eAssignments to be Co-ordinated Across Terms.
(3) Prt.
'servation f satisfying assignments.
Finally, one has to reproduce theconjunctive chanlcter of the 3-CN F prublem -- that is, a sentence is ~atisfiahle(wcll-formcd) iff each term has at least one literal assigned the value "1".-Part of the disjunctive character of the problcm has already been encoded inthe feature propagation machinery p~?~nted so far: if at least one X i in aterm "\]'j cxpands to the Iexical entry XiT, then the tr~th-a~siRnment featuregets the value T. "\['his just as desired.
Ifone, two, or three of the literais X iin a term select XiT, then Tl's truth-assigument feature is T. and the analysisis well-formed.
But how do we rule out the case where all ~ree Xi's in a lermselect he "F' path.
XiF?
And how do we ensure that all terms have at leastone T below them?Both of these problems can be solved by resorting to the LFG functionalcompleteness constraint.
The ~ck will be to add a Pred feature to a"dummy" node atu~ched tocach term; the sole purpose of this feature will beto refer to the feature "l'mth:a~,~i~,pm.q2.e=.g~ just as the predicate template for thetransitive verb ki.~* mentions thc feature Object.
Since an analysis is notwcll-formcd if the "grmnmatical relations" a Pred mentions are not filled infrom somewhere, this will have the effect of forcing the Tmth-~i=nmentt'cature to gct filled in every term.
Since the "F" lexical entry does not have al'mth-assimlmcnt value, if all the X i in a term triple select the XIF path (allthe litcrais are "F") then no Truth-assignment feature is ever picked up fromthe lexicai entries, and that term never gets a Truth-assignment feature.
Thisviolates what the predicate template demands, and so the whole analysis isthrown out.
(The ill-formednoss i  ex~dy analogous to the case where atransitive verb never gets an ObjeCL) Since this condition is applied to eachterm, we have now guaranteed that each term must have at least one literalbelow it that ~clects the 'T" path -- just as desirea.
Fo actually add the newpredicate template, one simply adds a new (but dummy) branch to each term'1" v with the appropriate predicate constraint a tached toit:/11T, featureJ:,.~ured: "dummy2<(TTruth-assignmen0~Dum~ty2 r / ~ I /lexical entry: i I , ~.
.
.
.
.
.
.
.
.
.
.
.
.
'dummy2': J "~ XtT XtF ~"~vF : ,", ( I' r'dummy2((1' Truth-assignment)> ~, ,X  i |(TTruth-assignmen0 = TFigure 7.
Predicates Can be Used to Force at least one ~ Per Term.There is a final mbde point here: one must prevent the Pred andTruth-assignment features for each term from being passed up to the head"S" node.
The reason is that if these features were passed up, then since theLFG machinery automatically mergea the values of any features with thesame name at the topmost node of the paine tree, the LFG machinery wouldfume the union of the feature values for Pred and Truth-asugnment over allterms in the analysis tree.
The result would be that if any term had .at leastone "I" {hence satisfying the Truth-assignment predicate template in at leastone term), then the Pred and Truth-assignment would get filled in at thetopmost node as well.
The string below would be well-formed if at-least one-term were "T", and this would amount to a disjunction of disjunctions (an"OR" of "OR"s), not quite what is ~ugh?.
To eliminate this possibility, onemust add a final trick: each term T I is given separate Predicate,Truth-assignment.
and Assign features, but only the Assign feature ispropagated to the highest node in the parse tree as such, In contrast, thePredicate and Truth-assignment features for each term are kept "protected"from merger by storing them under separate feature headings labelledT1...'r n. "l~e means by which just the ASSIGN feature bundle is lifted out isthe LFG analogue of the natural language phenomenon f Subject or Object"control".
whereby just the features of the Subject or Object of a lower clauseare lifted out of the lower clause to become the Subject or Object of a matrixsentence; the remaining features stay unmergeable because they stayprotected behind the individually labelled terms.To actu,'dly "implement" this in an LFG one can add two ncw branches toeach Term expansion in the base context-free grammar, as well as two"conttul" equation specificatious that do the actual work of lifting thefeatures from a lower clause to the matrix ~ntence:Natural language case (from \[81, pp.
43-45):The girl persuaded the baby to go.
(part of the) lexicai ena'y forperauaded:V (T VCOMPSubject)=(T OhjecOThe notation (T VCOMP Subjec0=(T Object) - dubbed a "controlequation" -- means that the features of the Object above the V(erb) node amto be the same t~ those of the features of the Subject of the verb complement(VCOMP).
Hence the top-most node of the pa~e tree eventually has afeature bundle something like:~'ubject: {bundle of features for NP subject "the gift"}predicate: 'persuadc<(T Subject)(T ObjectXTVcomp)>'3bjecr \[bundle of features for NP Object "the baby"}"\ COPIED/erb3omplement: ~Subject: {bundle ~f features for NP subject "the baby"a~"VCOMP") ~.Predicate: 'go((TSubject)>' ..JNote l:ow the Object features have been copied from the Subj~'tfeatures of the Verb Complement, via the notation ~k..~cribed above, butthe Predicate features of the Verb Complement were leR behind.The satisfiability analogue of this machinery is almost identical:Phrase structure U'ee:A f  Ti"'~T COMPD U m ~ kOne now attaches a "control equation" to the A i node that forces the Assi=nFeature bundle From the T iCOMP side co be lifted up to gct merged iuto theA.~si~n feature bundle of the T i node (and then, in turn, to become merged atthe topmost node of the tree by the usual Full copy up-and-down arrows):(r TiCOMP Assign) = (TAssign)Note how this is just like the copying of the Subject Features of a VerbComplcmcnt into the Object position of a matrix clause.4.
REI EVANCE OF COMPI.EXITY RESUI.TS ,~N\[') CONCLUSIONSThc demons~ation f the previous section shows that LFGs have enoughpower to "simulate" a probably computationally intractable problem.
Butwhat are we to make of this result?
On the positive side, a complexity resuRsuch as this one places the LFG theory more precisely in the hierarchy ofcomplexity classes.
Ifwe conjecture, as seems reasonable, that LFG languagerecognition is actually in the class NP (that is, LFG recognition can be doneby a non-deterministic Turing machine in polynomial ~rne), then LFGlanguage rccognitiun is NP-complete.
(This conjecture seems reasonablebecause a non-determfnistic "luring machine should be able to "guess" allFeature propagation solutions using its non-deterministic power - includingany "long-distance" binding solutions, an LFG device not discussed here.Since checking candidate solutions is quite rapid - it can be done in n 2 timeor less, as described in \[$\] - r~ognition should be possible in polynomialtime on such a machine.)
Comparing this result to other known languageclaas~ note that context-sensitive language recognition is in the cia~polynomial space ("PSPACE').
since (non-deterministic) linear boundedautomata generate exactly the class of context-sensitive languages.
(Non-deterministic and deterministic polynomial space classes collapsetogether, because of Savitch's wcll-known result \[9\] that any Functioncomputable in non-dcterminL'~ic space N can be computed in demrmini,,,~space N2.)
Funhennore, the class NP is clearly a subset of PSP^CE (since ifa function uses Space N, it must use at least Time N), and it is suspected, butnot known for certain, that NP is a proper subset of PSPACE.
(This being aForm of the P=NP question once again.)
Our conclusion is that it is likelythat LFG's generete a proper subset of the context-sensitive languages.
(In \[81it is shown that this includes ome strictly context-sensitive languages.)
It isimeresting that several other "natural" extensions of the context -~languages - notably, the class of languages generated by the so-called-mdexcd grammars" - also generam a subset of the conteat-sensitivelanguages, including those su'ictly context-sensitive languages shown to begenerable hy LFGs in \[8\], but are provably NP-eomplete (soc \[21 for proofs).Indeed.
acursory look at the power of the indexed grammars atleast sugg~sthat they might subsume the machinery of the LFG theory; this would be agood conjecture to check.On the other ~ide of d~e coin.
how might one restrict \[.FG theory further soaz ~o avoid possible intractability?
Several c~ape hau:hcs immediately cometo mind; thc-ze will simply be listed here.
Note that all of these "fixes" havethe effect of adding additional consu'aints to t~rther restrict the LFG thcory,I.
Rule out "worst case" languages as linguistically irrelevant.
"\['he probable computational inu'actability arises because co-occurrencerestrictions (cumpatible a.~signment of Xi's) can be Fumed across arbitrarydistances in the terminal string in conjunctioo with lexical ambiguity For eachterminal itcm.
\[f some device can be Found in natural languages that filtersout or removes such ambiguity locally (so that the choice of whether an itemis "T" or "1 -~' never depends on other itcms arbitrarily far away in theterminal string), or if natural languages never employ such kinds ofco-~currence r strictions, dlen the reduction is theoretically relevant, butlinguistically irrelevant.
Note that such a finding would be a positivediscovcry, since one would be able to filnhcr r~trict he LFG theory in its12attempt to characterize all and only the natural languages.
This di~"overywould be on a par with, for example, Petcrs and Ritchi?
's observation ~hatalthough the context-sensitive phrase structure roles Formally advanced inlinguistic theory have the power to generate non-context-Free languages, thatpower has apparendy never been used in immediate constituent analysis \[11\].2.
Add "locality principlus" for recognition (or parsing).One could simply stipulate that LFG languages meet some condition knownto ensure efficient recognizability, e.g, Knuth's \[7\] LR(k) restriction, suitablyextended to the case of cuntext-sonsitive languages.
(See \[10\] For more3.
Restrict the lexicon,The reduction depends crucially upon having a n infinite stock oflexieal itemsand an infinite number of Features with which co label them - several foreach literal X r This is necessary because as CNF Formulas grow larger andlarger, the number of Iiterals can grow arbitrarily large.
If, For whateverreason, the stock of lexical items or feature labels is finite, then the reductionmethod must Fail after a certain point.
-\[-his restriction seems ad hoe in thecase ofiexical items, but perhaps less so in dze case of Festures, (Speculating.perhaps features require "grounding" in terms of other language/cognitivesub-systems -- e.8,, a Feature might be required to be one of a finite numberof primitive "basis" elements of a hypothetical conceptual or sensort-motorcognitive system.
)ACKNOWI .ED~ F.MEN'TS\[ would like to thank Run Kapian.
Ray Perrault.
Chrisms Pnpadimimou,andparticularly Sc.,nloy Peters For various discussions about the contents of thispaper.
"This n:pon describes rescarctl done at the A~iticial Intelligence \[aboratoryof" U1c Massachusetts Institute of '\['cchnology.
Support For the Laboratory'sartificial intelligeuce re,catch is provided in part by the Office of Navalgc~il~h under Office of Naval Res~treh contr-'t N00014-80_..-C-0508.~ E\[-'ERENCF.SIll Peters, S. and Ri~hie` R. "On the generative power of ~.nsform~tionalgrammae~."
hffonua?ien Sciences 6, 1973, pp.
49-83.\[2\] Rounds, W. "Complexity of recognition in intermedia~.~.tevet languag?~"Pmcucdings o( the 14th Ann.
Syrup, on Switching Theory and Automat=,19"/3.\[31 Ih)unds W, "A grammatical charactertzadon of" exponential-dinelanguages," Proceedings of the 16th Ann.
Syrup.
on Switching "rheory amiAutomata, 1975. pp.
135-143.\[4\] Chomsky, N. Rules and Representations New York: Columbia UniversityPress, 1980.\[5\[ Befwick, R. and Weinberg, A.
The Role of  Grammars in Model~ ofLanguage Use., unpublished Mrr report, forthcoming, 198L\[6\] Magus, M. A Theory of S~taedc Recognition for Natural Language,Cambridge, MA: MITPreas, 1980.\[7.\] Knuth, D. "On the translation of languages from left to right?,Information and Conm)i, 8, 1965, pp.
607-639.\[8 !
Kaplan.
R. and Bresuan.
.\[.
Lexical-funclional Grommar: A Formal Systemfor Grammatical Representation, Cambridge, MA: MIT Cognitive ScienceOccasional Paper # 13, 1981.
(also Forthcoming in Bresnan, cal., The Men~lRep~seatation f Grammatical Relations, Cambridge, MA: MIT Press, 1981\[9\] HoperoR.
J. and Ulhnan, J.
Introduction to Automata Theory, Languages,and Computation, Reading, MA: Addison-Wesley, 1979.\[10\] Bcrwick, R. Locality Principles and the Acquisition of SyntacticKnowledge, MIT PhD.
cUasenadon, 1981 forthcoming.\[ll\] Peters, S. and Ritchie` R. Context-~ensilive bnnwdime constituentasaal3~is: contexi-free languages revisiled~ Mathematical Systems Theory, 6:4,1973, pp.
324-333.
