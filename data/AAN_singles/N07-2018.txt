Proceedings of NAACL HLT 2007, Companion Volume, pages 69?72,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsClustered Sub-matrix Singular Value DecompositionFang HuangSchool of ComputingRobert Gordon UniversityAberdeen, AB25 1HG, UKf.huang@rgu.ac.ukYorick WilksDepartment of Computer ScienceUniversity of SheffieldSheffield, S1 4DP, UKy.wilks@dcs.shef.ac.ukAbstractThis paper presents an alternative algo-rithm based on the singular value decom-position (SVD) that creates vector rep-resentation for linguistic units with re-duced dimensionality.
The work was mo-tivated by an application aimed to repre-sent text segments for further processingin a multi-document summarization sys-tem.
The algorithm tries to compensatefor SVD?s bias towards dominant-topicdocuments.
Our experiments on measur-ing document similarities have shown thatthe algorithm achieves higher average pre-cision with lower number of dimensionsthan the baseline algorithms - the SVDand the vector space model.1 IntroductionWe present, in this paper, an alternative algorithmcalled Clustered Sub-matrix Singular Value Decom-position(CSSVD) algorithm, which applied cluster-ing techniques before basis vector calculation inSVD (Golub and Loan, 1996).
The work wasmotivated by an application aimed to provide vec-tor representation for terms and text segments in adocument collection.
These vector representationswere then used for further preprocessing in a multi-document summarization system.The SVD is an orthogonal decomposition tech-nique closely related to eigenvector decompositionand factor analysis.
It is commonly used in infor-mation retrieval as well as language analysis appli-cations.
In SVD, a real m-by-n matrix A is decom-posed into three matrices, A = U ?V T .
?
is anm-by-n matrix such that the singular value ?i=?ii isthe square root of the ith largest eigenvalue of AAT ,and ?ij = 0 for i 6= j.
Columns of orthogonal ma-trices U and V define the orthonormal eigenvectorsassociated with eigenvalues of AAT and ATA, re-spectively.
Zeroing out all but the k, k < rank(A),largest singular values yields Ak =?ki=1 ?iuivTi ,which is the closest rank-k matrix to A.
Let A be aterm-document matrix.
Applications such as latentsemantic indexing (Deerwester et al, 1990) applythe rank-k approximation Ak to the original matrixA, which corresponds to projecting A onto the k-dimension subspace spanned by u1, u2, ..., uk.
Be-cause k ?
m, in this k-dimension space, minorterms are ignored, so that terms are not indepen-dent as they are in the traditional vector space model.This allows semantically related documents to be re-lated to each other even though they may not shareterms.However, SVD tends to wipe out outlier(minority-class) documents as well as minor terms(Ando, 2000).
Consequently, topics underlying out-lier documents tend to be lost.
In applications suchas multi-document summarization, a set of relateddocuments are used as the information source.
Typ-ically, the documents describe one broad topic fromseveral different view points or sub-topics.
It is im-portant for each of the sub-topics underlying thedocument collection to be represented well.Based on the above consideration, we propose theCSSVD algorithm with the intention of compensat-69ing for SVD?s tendency to wipe out minor topics.The basic idea is to group the documents into a setof clusters using clustering algorithms.
The SVDis then applied on each of the document clusters.The algorithm thus selects basis vectors by treat-ing equally each of the topics.
Our experimentson measuring document similarities have shown thatthe algorithm achieves higher average precision withlower number of dimensions than the SVD.2 the AlgorithmThe input to the CSSVD algorithm is an m?n term-document matrix A.
Documents in matrix A aregrouped into a set of document clusters.
Here,we adopt single-link algorithm to develop the ini-tial clusters, then use K-means method to refine theclusters.
After clustering, columns in matrix A arepartitioned and regrouped into a set of sub-matricesA1,A2,...,Aq.
Each of these matrices represents adocument cluster.
Assume Ai, 1 ?
i ?
q, is anm?
ni matrix, these sub-matrices are ranked in de-creasing order of their sizes, i.e., n1 ?
n2 ?
... ?nq, then n1 + n2 + ...+ nq = n.The algorithm computes basis vectors as follows:the first basis vector u1 is computed from A1, i.e.,the first left singular vector of A1 is selected.
In or-der to ensure that the basis vectors are orthogonal,singular vectors are actually computed on residualmatrices.
Rij , the residual matrix of Ai after the se-lection of basis vectors u1, u2,..., uj , is defined asRij ={ Ai j = 0Ai ?
proj(Aij) otherwisewhere, proj(Aij) is the orthogonal projection of thedocument vectors in Ai onto the span of u1,u2,...,uj ,i.e.,proj(Aij) =j?k=1ukuTkAithe residual matrix of Ai describes how much thedocument vectors in Ai are excluded from the pro-posed basis vectors u1, u2,..., uj .
For the first ba-sis vector computation, residual matrices are initial-ized as original sub-matrices.
The computation ofthe residual matrix makes the remaining vectors per-pendicular to the previous basis vectors, thus ensuresthat the basis vectors are orthogonal, as the eigen-vector computed next is a linear combination of theremaining vectors.After calculating a basis vector, the algorithmjudges whether the sub-matrices have been well rep-resented by the derived basis vectors.
The residualratio was defined as a criterion for this judgement,rrij = ||Rij ||2Fni ?
(ki + 1)where Rij is the residual matrix of Ai after j basisvectors have been selected1; ni is the number ofthe documents in matrix Ai; ki is the numberof singular vectors that have been selected frommatrix Ai.
Residual ratios of each sub-matrix arecalculated.
The sub-matrix with the largest residualratio is assumed to be the one that contains themost information that has not been represented bythe previous chosen basis vectors.
The first leftsingular vector of this sub-matrix is computed andselected as the next basis vector.
As describedabove, the computation of a basis vector uses thecorresponding residual matrix.
Once a basis vectoris selected, its influence from each sub-matrix issubtracted.
The procedure is repeated until anexpected number of basis vectors have been chosen.The pseudo-code of the algorithm for semanticspace construction is shown as follows:1.
Partition A into matrices A1,...,Aq correspondingto document clusters, where Ai , 1 ?
i ?
q, is anm?
ni (n1 ?
n2 ?
... ?
nq) matrix.2.
For i=1,2,...,q {Ri= Ai; k[i]=0;}3. j=1; r=1;4. ur= the first unit eigenvector of RjRTj ;5.
For i=1,2,...,q Ri= Ri - uruTr Ri;6. k[r]=k[r]+1; r=r+1;7.
For i=1,2,...,q rri= ||Ri||2F(ni?
(k[i]+1)) ;8. j=t if rrt > rrp for p=1,2,...,q and p 6= t;9.
If rrj ?
threshold then stop else goto step 4.For the single-link algorithm used in the CSSVD,we use a threshold 0.2 and cosine measure to cal-culate the similarity between two clusters in our ex-periments.
The performance of the CSSVD is alsorelative to the number of dimensions of the created1||A||F =qPi,j A2ij70subspace.
As described above, the algorithm usesthe residual ratio as a stopping criterion for the basisvector computation.
In each iteration, after a basisvector is created, the residual ratio is compared to athreshold.
Once the residual ratio of each sub-matrixfell below a certain threshold, the process of basis-vector selection is finished.
In our experiments, thethreshold was trained on corpus.After all the k basis vectors are chosen, a term-document vector di can be converted to dki , avector in the k-dimensional space, by multiply-ing the matrix of basis vectors following the stan-dard method of orthogonal transformation,i.e., dki =[u1, u2, ..., uk]Tdi.3 Evaluation3.1 Experimental SetupFor the evaluation of the algorithm, 38 topics fromthe Text REtrieval Conference (TREC) collectionswere used in our experiments.
These topics includeforeign minorities, behavioral genetics, steel pro-duction, etc.
We deleted documents relevant to morethan one topic so that each document is related onlyto one topic.
The total number of documents usedwas 2962.
These documents were split into two dis-joint groups, called ?pool 1?
and ?pool 2?.
The num-ber of documents in ?pool 1?
and ?pool 2?
were 1453and 1509, respectively.
Each of the two groups used19 topics.We generated training and testing data by simu-lating the result obtained by a query search.
Thissimulation is further simplified by selecting docu-ments containing same keywords from each docu-ment group.
Thirty document sets were generatedfrom each of the two document groups, i.e.
60 doc-ument sets in total.
The number of documents foreach set ranges from 51 to 582 with an average of128; the number of topics ranges from 5 to 19 withan average of 12.
Due to the limited number of thedocument sets we created, these sets were used bothfor training and evaluation.
For the evaluation of thedocuments sets from ?pool 1?, ?pool 2?
was used fortraining, and vice versa.To construct the original term-document matrix,the following operations were performed on each ofthe documents: 1) filtering out all non-text tags inthe documents; 2) converting all the characters intolower case; 3) removing stop words - a stoplist con-taining 319 words was used; and 4) term indexing- the tf.idf scheme was used to calculate a term?sweight in a document.
Finally, a document set isrepresented as a matrix A = [aij ], where aij de-notes the normalized weight assigned to term i indocument j.3.2 Evaluation MeasuresOur algorithm was motivated by a multi-documentsummarization application which is mainly basedon measuring the similarities and differences amongtext segments.
Therefore, the basic requisite is to ac-curately measure similarities among texts.
Based onthis consideration, we used the CSSVD algorithm tocreate the document vectors in a reduced space foreach of the document sets; cosine similarities amongthese document vectors were computed; and the re-sults were then compared with the TREC relevancejudgments.
As each of the TREC documents weused has one specific topic.
Assume that similarityshould be higher for any document pair relevant tothe same topic than for any pair relevant to differenttopics.
The algorithm?s accuracy for measuring thesimilarities among documents was evaluated usingaverage precision taken at various recall levels (Har-man, 1995).
Let pi denote the document pair thathas the ith largest similarity value among all pairs ofdocuments in the document set.
The precision for anintra-topic pair pk is calculated byprecision(pk) = number of pj where j ?
kkwhere pj is an intra-topic pair.
The average of theprecision values over all intra-topic pairs is com-puted as the average precision.3.3 ResultsThe algorithms are evaluated by the average preci-sion over 60 document sets.
In order to make a com-parison, two baseline algorithms besides CSSVD areevaluated.
One is the vector space model (VSM)without dimension reduction.
The other is SVD tak-ing the left singular vectors as the basis vectors.To treat the selection of dimensions as a separateissue, we first evaluate the algorithms in terms ofthe best average precision.
The ?best average preci-sion?
means the best over all the possible numbers71of dimensions.
The second row of Table 1 shows thebest average precision of our algorithm, VSM, andSVD.
The best average precision on average over 60document sets of CSSVD is 69.6%, which is 11.5%higher than VSM and 6.1% higher than SVD.measure VSM SVD CSSVDbest averageprecision (%) 58.1 63.5 69.6average DR (%) N/A 54.4 32.1average precision (%) 58.1 59.5 66.8Table 1: the algorithm performanceIn the experiments, we observed that the CSSVD al-gorithm obtained its best performance with the num-ber of dimensions lower than that of SVD.
The Di-mensional Ratio (DR) is defined as the number ofdimensions of the derived sub-space compared withthe dimension number of the original space, i.e.,DR = # of dimensions in derived space# of dimensions in original spaceThe average dimensional ratio is calculated over allthe 60 document sets.
As the algorithms?
computa-tional efficiency is dependent on the number of di-mensions computed, our interest is in getting goodperformance with an average dimensional ratio aslow as possible.
The third row of Table 1 shows theaverage dimensional ratio that yielded the best av-erage precision.
The average dimensional ratio thatCSSVD yielded the best average precision is 32.1%,which is 22.3% lower than that of SVD.
Thus, ouralgorithm has the advantage of being computation-ally inexpensive, assuming that we can find the op-timal number of dimensions.The bottom row of Table 1 shows the averageprecision of the algorithms.
The threshold used inCSSVD algorithm was trained on corpus.
Let p bethe threshold on residual ratio that yielded the bestaverage precision on the training data.
The valueof p is then used as the threshold on the evaluationdata.
For the SVD algorithm, the average dimen-sional ratio that yielded the best average precisionon training data was used as the dimensional ratioto determine the subspace dimensionality in evalua-tion.
The performance shown here are the averageof average precision over 60 document sets.
Again,the CSSVD achieves the best performance, which is7.3% higher than the performance of SVD and 8.7%higher than VSM.4 ConclusionWe have presented an alternative algorithm, theCSSVD, that creates vector representation for lin-guistic units with reduced dimensionality.
The al-gorithm aims to compensate for SVD?s bias towardsdominant-topic documents by grouping documentsinto clusters and selecting basis vectors from eachof the clusters.
It introduces a threshold on the resid-ual ratio of clusters as a stopping criterion of basisvector selection.
It thus treats each topic underly-ing the document collection equally while focuseson the dominant documents in each topic.
The pre-liminary experiments on measuring document simi-larities have shown that the CSSVD achieves higheraverage precision with lower number of dimensionsthan the baseline algorithms.Motivated by a multi-document summarizationapplication, the CSSVD algorithm?s emphasis ontopics and dominant information within each topicmeets the general demand of summarization.
We ex-pect that the algorithm fits the task of summarizationbetter than SVD.
Our future work will focus on morethorough evaluation of the algorithm and integratingit into a summarization system.5 AcknowledgmentsWe would like to thank Mark Sanderson, HoracioSaggion, and Robert Gaizauskas for helpful com-ments at the beginning of this research.ReferencesAndo R.K. 2000 Latent Sementic Space: IterativeScaling Improves Precision of Inter-document Similar-ity Measurement.
Proceedings of ACM SIGIR 2000,Athens, Greece.Deerwester S., Dumais S., Furnas G., and Landauer T.1990.
Indexing by Latent Semantic Analysis.
Jour-nal of the American Society for Information Science,41:391-407.Golub G. and Loan C.V. 1996.
Matrix Computations.Johns-Hopkins University Press, Maryland, US.Harman D.K.
1983.
Overview of the second Text Re-trieval Conference (TREC-2).
Information ProcessingManagement, 31(3):271-289.72
