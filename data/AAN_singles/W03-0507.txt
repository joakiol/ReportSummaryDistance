Text Summarization Challenge 2Text summarization evaluation at NTCIR Workshop 3Manabu OkumuraTokyo Institute of Technologyoku@pi.titech.ac.jpTakahiro FukusimaOtemon Gakuin Universityfukusima@res.otemon.ac.jpHidetsugu NanbaHiroshima City Universitynanba@its.hiroshima-cu.ac.jpAbstractWe describe the outline of Text SummarizationChallenge 2 (TSC2 hereafter), a sequel textsummarization evaluation conducted as one of the tasksat the NTCIR Workshop 3.
First, we describe briefly theprevious evaluation, Text Summarization Challenge(TSC1) as introduction to TSC2.
Then we explainTSC2 including the participants, the two tasks in TSC2,data used, evaluation methods for each task, and briefreport on the results.Keywords: automatic text summarization,summarization evaluation12IntroductionAs research on automatic text summarization is beinga hot topic in NLP, we also see the needs to discuss andclarify the issues on how to evaluate text summarizationsystems.
SUMMAC in May 1998 as a part of TIPSTER(Phase III) project ([1], [2]) and DocumentUnderstanding Conference (DUC) ([3]) in the UnitedStates show the need and importance of the evaluationfor text summarization.In Japan, Text Summarization Challenge (TSC1), atext summarization evaluation, the first of its kind, wasconducted in the years of 1999 to 2000 as a part of theNTCIR Workshop 2.
It was realized in order for theresearchers in the field to collect and share text data forsummarization, and to make clearer the issues ofevaluation measures for summarization of Japanesetexts ([4],[5],[6]).
TSC1 used newspaper articles andhad two tasks for a set of single articles with intrinsicand extrinsic evaluations.
The first task (task A) was toproduce summaries (extracts and free summaries) forintrinsic evaluations.
We used recall, precision and F-measure for the evaluation of the extracts, and content-based as well as subjective methods for the evaluationof the free summaries.The summarization rates for task A were as follows:10, 30, 50% for extracts and 20, 40% for freesummaries.The second task (task B) was to produce summariesfor information retrieval (relevance judgment) task.
Themeasures for evaluation were recall, precision and F-measure to indicate the accuracy of the task, as well asthe time to indicate how long it takes to carry out thetask.We also prepared human-produced summariesincluding key data for the evaluation.
In terms of genre,we used editorials and business news articles at TSC1?sdryrun, and editorials and articles on social issues at theformal run evaluation.As sharable data, we had summaries for 180newspaper articles by spring 2001.
For each article, wehad the following seven types of summaries: importantsentences (10, 30, 50%), important parts specified (20,40%), and free summaries (20, 40%).In comparison, TSC2 uses newspaper articles andhas two tasks (single- and multi-documentsummarization) for two types of intrinsic evaluations.
Inthe following sections, we describe TSC2 in detail.Two Tasks in TSC2 and its ScheduleTSC2 has two tasks.
They are single documentsummarization (task A) and multi-documentsummarization  (task B).Task A: We ask the participants to producesummaries in plain text to be compared with human-prepared summaries from single documents.Summarization rate is a rate between the number ofcharacters in the summary and the total number ofcharacters in the original article.
The rates are about20% and 40%.
This task is the same as task A-2 inTSC1.Task B: In this task, more than two (multiple)documents are summarized for the task.
Given a set ofdocuments, which has been gathered for a pre-definedtopic, the participants produce summaries of the set inplain text format.
The information that was used toproduce the document set, such as queries, as well assummarization lengths are given to the participants.Two summarization lengths are specified, short andlong summaries for one set of documents.The schedule of evaluations at TSC2 was as follows:dryrun was conducted in December 2001 and formal runwas in May 2002.
The final evaluation results werereported to the participants by early July 2002.3 Data Used for TSC2We use newspaper articles from the Mainichinewspaper database of 1998, 1999.
As key data (humanprepared summaries), we prepare the following types ofsummaries.Extract-type summaries:We asked captioners who are well experienced insummarization to select important sentences fromeach article.
The summarization rates are 10%, 30%,and 50%.Abstract-type summaries:We asked the captioners to summarize the originalarticles in two ways.
The first is to choose importantparts of the sentences recognized important inextract-type summaries (abstract-type type1).
Thesecond is to summarize the original articles ?freely?without worrying about sentence boundaries, tryingto obtain the main idea of the articles (abstract-typetype2).
Both types of abstract-type summaries areused for task A.
The summarization rates are 20%and 40%.Both extract-type and abstract-type summaries aresummaries from single articles.Summaries from more than two articles:Given a set of newspaper articles that has beenselected based on a certain topic, the captionersproduced free summaries (short and long summaries)for the set.
Topics are various, from kidnapping caseto Y2K problem.4 Evaluation Methods for each taskWe use summaries prepared by human as key datafor evaluation.
The same two intrinsic evaluationmethods are used for both tasks.
They are evaluation byranking summaries and by measuring the degree ofrevisions.
Here are the details of the two methods.
Weuse 30 articles for task A and 30 sets of documents (30topics) for task B at formal run evaluation.Unfortunately,  due to the limitation of the budget,  onlyan evaluator evaluates a system?s result for an article(ora set).4.1.
Evaluation by rankingThis is basically the same as the evaluation methodused for TSC1 task A-2 (subjective evaluation).
We askhuman judges, who are experienced in producingsummaries, to evaluate and rank the system summariesin terms of two points of views.1.
Content: How much the system summary coversthe important content of the original article.2.
Readability: How readable the system summary is.The judges are given 4 types of summaries to beevaluated and rank them in 1 to 4 scale (1 is the best, 2for the second, 3 for the third best, and 4 for the worst).For task A, the first two types are human-producedabstract-type type1 and type2 summaries.
The third issystem results, and the fourth is summaries produced bylead method.For task B, the first is human-produced freesummaries of the given set of documents, and thesecond is system results.
The third is the results of thebaseline system based on lead method where the firstsentence of each document is used.
The fourth is theresults of the benchmark system using Stein method([7]) whose procedure is as follows:1.
Produce a summary for each document.2.
Group the summaries into several clusters.
Thenumber of clusters is adjusted to be less than thehalf of the number of the documents.3.
Choose the most representative summary as thesummary of the cluster.4.
Compute the similarity among the clusters andoutput the representative summaries in such orderthat the similarity of neighboring summaries ishigh.4.2.
Evaluation by revisionIt is a newly introduced evaluation method in TSC2to evaluate the summaries by measuring the degree ofrevision to system results.
The judges read the originaldocuments and revise the system summaries in terms ofthe content and readability.
The revisions are made byone of three editing operations (insertion, deletion,replacement).
The degree of the revision is computedbased on the number of the operations and the numberof revised characters.
The revisers could be completelyfree in what they did, though they were instructed to dominimum revision.As baseline for task A, lead-method results are used.As reference for task A, human produced summaries(abstract type1 and abstract type 2) are used.
And asbaseline, reference, and benchmark for task B, lead-method results, human produced summaries that aredifferent from the key data, and the results based on theStein method are used respectively.When more than half of the document needs to berevised, the judges can ?give up?
revising the document.5 ParticipantsWe had 4 participating systems for Task A, and 5systems for Task B at dryrun.
We have 8 participatingsystems for Task A and 9 systems for Task B at formalrun.
As group, we had 8 participating groups, which areall Japanese, of universities, governmental researchinstitute or companies in Japan.
Table 1 shows thebreakdown of the groups.University 6Governmentalresearch institute  1Company 2Table 1  Breakdown of Participants(Please note that one group consists of a company and auniversity.
)6 Results6.1.
Results of Evaluation by rankingTable 2 shows the result of evaluation by rankingfor task A and Table 3 shows the result of evaluation byranking for task B.
Each score is the average of thescores for 30 articles for task A, and 30 topics for task Bat formal run.SystemNoContent20%Read-ability20%Content40%Read-ability40%F0101 2.53 2.87 2.60 2.77F0102 2.67 2.97 2.50 2.77F0103 2.80 2.93 2.90 2.90F0104 2.77 2.73 2.80 2.90F0105 2.70 2.73 2.60 2.77F0106 2.73 2.57 2.63 2.67F0107 2.70 2.60 2.50 2.53F0108 2.40 2.83 2.60 2.77TF 3.30 3.30 3.20 3.10Human 2.33 2.20 2.10 2.03Table 2 Ranking evaluation (task A)In Tables 2 and 3, F01* and F02* are labels for thedifferent systems involved, respectively.
In Table 2,?TF?
indicates a baseline system based on term-frequency method, and ?Human?
indicates human-produced summaries that are different from the key dataused in ranking judgement.In Table 3, ?Human?
indicates human-producedsummaries that are different from the key data used inranking judgement.System No ContentShortRead-abilityShortContentLongRead-abilityLongF0201 2.70 3.17 2.50 3.23F0202 2.73 2.70 2.77 2.93F0203 2.60 2.33 2.97 3.03F0204 2.63 2.90 2.80 3.03F0205 2.53 3.10 2.73 3.30F0206 3.20 3.00 3.47 3.30F0207 2.40 2.87 2.63 3.27F0208 2.93 2.70 2.53 2.80F0209 2.83 2.73 2.53 2.87Human 2.00 2.17 1.83 2.33Table 3 Ranking evaluation (task B)In Appendix A, we also show tables giving thefraction of time that each system beats the baseline, onehuman summary, or two human summaries for task A.In Appendix B,  we show tables giving the fraction oftime that each system beats the baseline, the benchmark,or  human summary for task B.Content20%Read-ability20%Content40%Read-ability40%Human(type 1)1.58 1.61 1.67 1.69Human(type 2)1.50 1.57 1.42 1.55Baseline(Lead)3.80 3.60 3.83 3.55Table 4 Ranking evaluation (task A, human andbaseline)ContentShortRead-abilityShortContentLongRead-abilityLongHuman(type 2)1.65 2.38 1.82 2.38Baseline(Lead)2.80 2.20 2.70 2.22Benchmark(Stein)2.48 2.00 2.50 1.99Table 5 Ranking evaluation (task B, human,baseline, and benchmark)In comparison with the system results (Table 2 andTable 3), the scores for the human summaries, thebaseline systems, and the benchmark system(thesummaries to be compared)  are shown in Table 4 andTable 5.6.2.
Results of Evaluation by revisionTable 6 shows the result of evaluation by revision fortask A at rate 40%, and Table 7 shows the result ofevaluation by revision for task A at rate 20%.
Table 8shows the result of evaluation by revision for task Blong, and Table 9 shows the result of evaluation byrevision for task B short.
All the tables show theevaluation results in terms of average number ofrevisions (editing operations) per document.Deletion Insertion ReplacementSystemUIM RD IM RD C RDF0101 2.0  0.1  1.5  0.4  0.5  0.7F0102 1.6  0.4  1.5  0.4  0.4  0.8F0103 2.3  0.2  2.4  0.2  0.4  0.5F0104 2.4  0.4  2.7  0.5  0.4  0.5F0105 2.0  0.3  1.7  0.1  0.7  0.7F0106 2.8  0.2  2.3  0.4  0.3  0.6F0107 2.5  0.6  1.8  0.2  0.1  0.5F0108 2.0  0.4  2.4  0.1  0.4  0.6ld 2.9  0.1  0.7  0.1  0.4  0.1free 0.4  0.4  1.2  0.4  0.1  0.3part 0.7  0.6  0.9  0.3  0.1  0.4edit 0.3 0.1 0.4 0.3 0.1 0.2ALL 1.9  0.3  1.8  0.3  0.3  0.5Table 6 Evaluation by revision (task A 40%)Please note that UIM stands for unimportant, RD forreadability, IM for important, C for content in Tables 6to 9.
They mean the reason for the operations, e.g.?unimportant?
is for deletion operation due to the partjudged to be unimportant, and ?content?
is forreplacement operation due to excess and deficiency ofcontent.In Table 6 and Table 7, ?ld?
means a baseline systemusing lead method, ?free?
is free summaries produced byhuman (abstract type 2), and ?part?
is human-produced(abstract type1) summaries, and these three are baselineand reference scores for task A.Deletion Insertion ReplacementSystemUIM RD IM RD C RDF0101 1.4 0.4 1.3 0.2  0.5  0.3F0102 1.2 0.4 1.0  0.0  0.4  0.5F0103 0.8 0.1 1.2  0.0  0.2  0.1F0104 0.8 0.1 1.2  0.1  0.1  0.2F0105 1.2 0.1 0.7  0.0  0.4  0.2F0106 2.1 0.2 1.7  0.1  0.1  0.2F0107 0.8 0.6 0.9  0.1  0.2  0.1F0108 1.4 0.1 1.1  0.1  0.2  0.6ld 1.9 0.1 1.3  0.0  0.0  0.0free 0.6 0.4 1.1  0.1  0.2  0.1part 0.7 0.3 1.1  0.1  0.1  0.2edit 0.2 0.1 0.5 0.1 0.2 0.2ALL 1.1 0.3 1.1  0.1  0.2  0.3Table 7 Evaluation by revision (task A 20%)Deletion Insertion ReplacementSystemUIM RD IM RD C RDF0201 3.8 0.7 7.2 1.4 1.1 0.9F0202 5.2 0.6 3.5 0.4 0.7 0.5F0203 5.1 0.6 3.8 0.5 0.9 0.6F0204 4.2 0.6 3.4 0.7 1.4 0.7F0205 8.1 0.6 5.4 1.7 3.0 1.3F0206 3.2 0.2 4.7 0.7 0.8 0.6F0207 7.0 1.1 4.1 1.1 1.1 1.1F0208 4.8 0.7 4.0 0.4 0.8 0.9F0209 4.6 0.5 3.9 0.5 0.5 0.5human 3.0 0.9 3.4 7.8 1.0 1.2ld 5.7 0.9 2.9 0.4 0.7 0.5stein 4.0 0.5 2.2 0.3 0.8 0.5edit 3.0 1.2 2.9 0.7 0.7 1.1ALL 4.9 0.7 4.0 1.3 1.1 0.8Table 8 Evaluation by revision (task B long)In Table 8 and Table 9, ?human?
means human-produced summaries which are different from the keydata, and ?ld?
means a baseline system using leadmethod, ?stein?
means a benchmark system using Steinmethod, and these three are baseline,  reference,  andbenchmark scores for task B.To determine the plausibility of the judges?
revision,the revised summaries were again evaluated with theevaluation methods in section 5.
In Tables 6 to 9, `edit?means the evaluation results for the revised summaries.We also measure as degree of revision the number ofrevised characters for the three editing operations, andthe number of documents that are given up revising bythe judges.
Please look at the detailed data at NTCIRWorkshop 3 data booklet.Figure 1 indicates how much the scores for contentand readability vary for the summaries of the samesummarization rate.
It shows that the readability scorestend to be higher than those for content, and it isespecially clearer for 40% summarization.Deletion Insertion ReplacementSystem UIM RD IM RD C RDF0201 3.5 0.5 4.3 0.8 1.1 0.7F0202 3.5 0.4 2.4 0.2 0.7 0.2F0203 3.6 0.3 2.8 0.2 0.5 0.4F0204 2.7 0.5 2.3 0.2 1.2 0.7F0205 5.5 0.4 2.5 0.8 2.0 0.7F0206 2.0 0.4 3.4 0.6 0.4 0.4F0207 3.5 0.4 2.7 0.3 0.6 0.6F0208 2.4 0.5 2.3 0.4 0.2 0.3F0209 2.5 0.5 2.2 0.2 0.3 0.4human 1.9 0.8 2.4 2.0 0.9 0.7ld 2.8 0.7 2.4 0.2 0.5 0.4stein 3.0 0.3 1.8 0.2 0.4 0.3edit 2.2 0.8 2.5 0.6 1.0 1.2ALL 3.1 0.5 2.6 0.5 0.7 0.5-0.300-0.200-0.1000.0000.1000.2000.300F0101F0102F0103F0104F0105F0106F0107F0108TF HumanC20-C40R20-R40Figure 2 Score difference between 20% and 40%summarizations (Task A)Figure 2 shows the differences in scores for thedifferent summarization rates, i.e.
20% and 40% of taskA.
?C20-C40?
means the score for content 20% minusthe score for content 40%.
?R20-R40?
?means the scorefor readability 20% minus the score for readability 40%.Table 9 Evaluation by revision (task B short)7 Discussion  Figure 2 tells us that the ranking scores for 20%summarization tend to be higher than those for 40%,and this is true with the baseline system and humansummaries as well.
7.1.
Discussion for Evaluation by rankingSecond, consider task B.
Figure 3 shows thedifferences in scores for content and readability for eachsystem for task B.
?CS-RS?
means the score for contentshort summaries minus the score for readability shortsummaries.
?CL-RL?
is computed in the same way forlong summaries.We here further look into how the participatingsystems perform by analysing the ranking results interms of differences in scores for content and those forreadability.First, consider task A.
Figure 1 shows the differencesin scores for content and readability for each system.?C20-R20?
means the score for content 20% minus thescore for readability 20%.
?C40-R40?
means the scorefor content 40% minus the score for readability 40%.-0.800-0.600-0.400-0.2000.0000.2000.400F0201F0202F0203F0204F0205F0206F0207F0208F0209Human CS-RSCL-RL-0.500-0.400-0.300-0.200-0.1000.0000.1000.200F0101F0102F0103F0104F0105F0106F0107F0108TF Human C20-R20C40-R40Figure 1 Score difference between Content andReadability (Task A)Figure 3 Score difference between content andreadability (Task B)Figure 3 shows, like Figure 1, that the scores forreadability tend to be higher, thence, the differences arein minus values, than those for content for both shortand long summaries.
In addition, the differences arelarger than the differences we saw for task A, i.e.
inFigure 1.Figure 4 shows the differences in scores for thedifferent summarization lengths, i.e.
short and longsummaries of task B.
?CS-CL?
means the score forcontent short summaries minus the score for contentlong summaries.
?RS-RL?
means the score forreadability short summaries minus the score forreadability long summaries.Figure 4 tells us, unlike Figure2, the scores for shortsummaries tend to be lower than those for longsummaries.
This tendency is very clear for thereadability ranking scores.Figure 1 and 3 show that when we compare theranking scores for content and readability summaries,the readability scores tend to be higher than those forcontent, which means that the evaluation for readabilityis worse than that for content.
Figure 2 and 4 showscontradicting tendencies.
Figure 2 indicates that short(20%) summaries are higher in ranking scores, i.e.worse in evaluation.
However, Figure 4 indicates theother way round.Intuitively longer summaries can have betterreadability since they have more words to deal with, andit is shown in Figure2.
However, it is not the case withtask B ranking results.
Longer summaries had worsescores, especially in readability evaluation.-0.800-0.600-0.400-0.2000.0000.2000.4000.600F0201F0202F0203F0204F0205F0206F0207F0208F0209HumanCS-CLRS-RLFigure 4 Score difference between differentsummarization lengths (Task B)7.2.
Discussion for Evaluation by revisionTo determine the plausibility of the judges?
revision,the revised summaries were again evaluated with theevaluation methods in section 5.
As Tables 6 to 9 show,the degree of the revisions for the revised summaries israther smaller than that for the original ones and isalmost same as that for human summaries.Tables 10 and 11 show the results of evaluation byranking for the revised summaries at task A and Brespectively.
Compared with Tables 2 to 5, Tables 10and 11 show that the scores for the revised summariesare rather smaller than those for the original ones andare almost same as those for human summaries.From these results,  the quality of the revisedsummaries is considered as same as that of humansummaries.System No Content20%Read-ability20%Content40%Read-ability40%edit 2.37 2.43 2.33 2.33Table 10 Ranking evaluation (task A)System No ContentShortRead-abilityShortContentLongRead-abilityLongedit 1.93 2.23 2.13 2.50Table 11 Ranking evaluation (task B)8.
ConclusionsWe have described the outline of the TextSummarization Challenge 2.
In addition to the twoevaluation runs, we held two round-table discussions,one right after dryrun, and the other after formal run.
Atthe second round-table discussion, it was pointed outthat we might need to examine more closely the resultsof evaluation, especially the one by ranking.We are now starting the third evaluation (TSC3).Please see our web page[4]  for the details of the task.References[1] Proceedings of The Tipster Text Program Phase III,Morgan Kaufmann, 1999.
[2] Mani, I., et al The TIPSTER SUMMAC TextSummarization Evaluation, Technical Report, MTR98W0000138,  The MITRE Corp.,  1998.
[3] http://www-nlpir.nist.gov/projects/duc/.
[4] http://oku-gw.pi.titech.ac.jp/tsc/index-en.html.
[5] Takahiro Fukusima and Manabu Okumura, ?TextSummarization Challenge ?Text SummarizationEvaluation at NTCIR Workshop 2?, In Proceedings ofNTCIR Workshop 2, pp.45-50, 2001.
[6] Takahiro Fukusima and Manabu Okumura, ?TextSummarization Challenge ?
Text SummarizationEvaluation in Japan?, North American Association forComputational Linguistics (NAACL2001), Workshopon Automatic Summarization, pp.51-59, 2001.
[7] Gees C. Stein, Tomek Strazalkowski and G. BowdenWise, ?Summarizing Multiple Documents using TextExtraction and Interactive Clustering?, PacificAssociation for Computational Linguistics, pp.200-208, 1999.Appendix A20%readability lead human humansF101 0.767 0.100 0.033F102 0.667 0.100 0.033F103 0.667 0.100 0.033F104 0.733 0.133 0.067F105 0.833 0.233 0.100F106 0.867 0.233 0.133F107 0.733 0.233 0.200F108 0.833 0.067 0.033human 0.933 0.467 0.233tf 0.267 0.067 0.06720%content lead human humansF101 0.867 0.200 0.167F102 0.900 0.200 0.100F103 0.800 0.067 0.033F104 0.767 0.067 0.033F105 0.933 0.200 0.067F106 0.900 0.200 0.100F107 0.800 0.167 0.133F108 1.000 0.267 0.167human 1.000 0.400 0.233tf 0.233 0.000 0.00040%readability lead human humansF101 0.833 0.233 0.033F102 0.700 0.133 0.100F103 0.800 0.100 0.067F104 0.800 0.133 0.033F105 0.767 0.200 0.167F106 0.800 0.167 0.100F107 0.767 0.200 0.167F108 0.833 0.100 0.067human 0.867 0.467 0.300tf 0.400 0.100 0.10040%content lead human humansF101 0.967 0.167 0.100F102 0.900 0.267 0.200F103 0.800 0.100 0.033F104 0.900 0.133 0.067F105 0.867 0.200 0.167F106 0.967 0.200 0.100F107 0.933 0.233 0.167F108 1.000 0.167 0.100human 0.967 0.300 0.267tf 0.367 0.000 0.000Appendix Bshortreadability lead stein humanF201 0.233 0.167 0.333F202 0.333 0.267 0.367F203 0.367 0.333 0.533F204 0.300 0.233 0.300F205 0.200 0.233 0.267F206 0.267 0.233 0.233F207 0.200 0.267 0.400F208 0.367 0.300 0.233F209 0.433 0.167 0.433human 0.667 0.600 0.533shortcontent lead stein humanF201 0.533 0.400 0.267F202 0.433 0.333 0.200F203 0.500 0.500 0.100F204 0.433 0.400 0.200F205 0.500 0.533 0.233F206 0.300 0.200 0.100F207 0.633 0.633 0.233F208 0.400 0.333 0.133F209 0.433 0.267 0.167human 0.700 0.700 0.467longreadability lead stein humanF201 0.167 0.167 0.267F202 0.367 0.333 0.300F203 0.300 0.267 0.367F204 0.233 0.267 0.333F205 0.300 0.100 0.233F206 0.133 0.100 0.233F207 0.200 0.233 0.200F208 0.333 0.300 0.333F209 0.267 0.300 0.367human 0.567 0.533 0.467longcontent lead stein humanF201 0.500 0.500 0.400F202 0.533 0.300 0.167F203 0.433 0.300 0.100F204 0.333 0.400 0.233F205 0.567 0.367 0.300F206 0.200 0.067 0.167F207 0.567 0.500 0.233F208 0.433 0.533 0.200F209 0.567 0.533 0.267human 0.733 0.700 0.567
