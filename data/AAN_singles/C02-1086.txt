Implicit Ambiguity Resolution Using Incremental Clustering inKorean-to-English Cross-Language Information RetrievalKyung-Soon Lee1, Kyo Kageura1, Key-Sun Choi21 NII (National Institute of Informatics)2-1-2 Hitotsubashi, Chiyoda-ku,Tokyo, 101-8430, Japan{kslee, kyo}@nii.ac.jp2 Division of Computer Science, KAIST373-1 Kusung YusongDaejeon, 305-701, Koreakschoi@cs.kaist.ac.krAbstractThis paper presents a method to implicitlyresolve ambiguities using dynamicincremental clustering in Korean-to-Englishcross-language information retrieval.
In theframework we propose, a query in Korean isfirst translated into English by looking upKorean-English dictionary, then documentsare retrieved based on the vector spaceretrieval for the translated query terms.
Forthe top-ranked retrieved documents,query-oriented document clusters areincrementally created and the weight of eachretrieved document is re-calculated by usingclusters.
In experiment on TREC-6 CLIRtest collection, our method achieved 28.29%performance improvement for translatedqueries without ambiguity resolution forqueries.
This corresponds to 97.27% of themonolingual performance for originalqueries.
When we combine our method withquery ambiguity resolution, our methodeven outperforms the monolingual retrieval.1 IntroductionThis paper describes a method of applyingdynamic incremental clustering to the implicitresolution of query ambiguities inKorean-to-English cross-language informationretrieval.
The method uses the clusters ofretrieved documents as a context forre-weighting each retrieved document and forre-ranking the retrieved documents.Cross-language information retrieval (CLIR)enables users to retrieve documents written in alanguage different from a query language.
Themethods used in CLIR fall into two categories:statistical approaches and translation approaches.Statistical methods establish cross-lingualassociations without language translation(Dumais et al 1997; Rehder et al 1997; Yang etal, 1998).
They require large-scale bilingualcorpora.
In translation approach, either queriesor documents are translated.
Though documenttranslation is possible when high qualitymachine translation systems are available (Kwonet al 1997; Oard and Hackett, 1997), it is notvery practical.
Query translation methods (Hulland Grefenstette, 1996; Davis, 1996; Eichmannet al 1998; Yang et al 1998; Jang et al 1999;Chun, 2000) based on bilingual dictionaries,multilingual ontology or thesaurus are muchmore practical.
Many researches adoptdictionary-based query translation because it issimpler and practical, given the wide availabilityof bilingual or multilingual dictionaries.
In orderto achieve a high performance CLIR usingdictionary-based query translation, however, it isnecessary to solve the problem of increasedambiguities of query terms.
One way ofresolving query ambiguities is to use thestatistics, such as mutual information (Churchand Hanks, 1990), to measure associations ofquery terms, on the basis of existing corpora(Jang et al 1999).Document clusters, widely adopted in variousapplications such as browsing and viewing ofdocument results (Hearst and Pedersen, 1996) ortopic detection (Allan et al 1998), also reflectthe association of terms and documents.
Lee etal (2001) showed that incorporating a documentre-ranking method based on document clustersinto the vector space retrieval achieved thesignificant improvement in monolingual IR, as itcontributed to resolving ambiguities caused bypolysemous query terms.The noise or ambiguity produced bydictionary-based query translation in CLIR ismuch larger than the polysemous ambiguities inmonolingual IR.
For example, a Korean term???[eun-haeng]?
is a polysemous term withtwo meanings: ?bank?
and ?ginkgo?.
The Englishterm ?bank?
itself is polysemous, so thetranslated query ends up having magnifiedambiguities.
We will show that the method wepropose, i.e.
implicit ambiguity resolution usingincremental clustering, is highly effective indealing with the increased query ambiguities inCLIR.2 Implicit ambiguity resolution usingincremental clusteringFigure 1 shows the overall architecture of oursystem which incorporates implicit ambiguityresolution method based on query-orienteddocument clusters.
In the system, a query inKorean is first translated into English by lookingup dictionaries, and documents are retrievedbased on the vector space retrieval for thetranslated query.
For the top-ranked retrieveddocuments, document clusters are incrementallycreated and the weight of each retrieveddocument is re-calculated by using clusters withpreference.
This phase is the core of our implicitambiguity resolution method.
Below, we willdescribe each module in the system.2.1 Dictionary-based query translation andambiguitiesQueries are written in natural language inKorean.
We first apply morphological analysisand part-of-speech (POS) tagging to a query,and select keywords based on the POSinformation.
For each keyword, we look upKorean-English dictionaries, and all the Englishtranslations in the dictionaries are chosen asquery terms.
We used a general-purposebilingual dictionary and technical bilingualdictionaries (Chun, 2000).
All in all, they have282,511 Korean entries and 505,003 Englishtranslations.Since a term can have multiple translations,the list of translated query terms can containterms of different meanings as well as synonyms.While synonyms can improve retrievaleffectiveness, terms with different meaningsproduced from the same original term candegrade retrieval performance tremendously.At this stage, we can apply statisticalambiguity resolution method based on mutualinformation.
In the experiment below, we willexamine two cases, i.e.
with and withoutambiguity resolution at this stage.2.2 Document retrieval based on vector spaceretrieval modelFor the query, documents are retrieved based onthe vector space retrieval method.
This methodsimply checks the existence of query terms, andcalculates similarities between the query anddocuments.
The query-document similarity ofeach document is calculated by vector innerproduct of the query and document vectors:ditiqi wwdqsimD ?= ?=1),(              (1)where query and document weight, qiw and diw ,are calculated by ntc-ltn weighting schemewhich yields the best retrieval result in Lee et al(2001) among several weighting schemes usedin SMART system (Salton, 1989).As the translated query can contain noises,non-relevant documents may have higher ranksthan relevant documents.Figure 1.
System architecture of implicitambiguity resolution by incremental clustering.English Query with ambiguitiesTREC AP-news collectionKorean-EnglishDictionariesKorean Queryretrieved top N docsDictionary-basedQuery TranslationVector Space RetrievalEach  document viewre-ranked results?Incremental ClustersDocument context viewReflecting contextto each document2.3 Query-oriented incremental clustering forimplicit ambiguity resolutionIn order to exclude non-relevant documentsfrom higher ranks, we take top N documents tocreate clusters incrementally and dynamically,and use similarities between the clusters and thequery to re-rank the documents.
Basic idea is:Each cluster created by clustering of retrieveddocuments can be seen as giving a context of thedocuments belonging to the cluster; bycalculating the similarity between each clusterand the query, therefore, we can spot therelevant context of the query; documents thatbelong to more relevant context or cluster arelikely to be relevant to the query.It should be noted here that the static globalclustering is not practical in the current setup,because it takes much computational time andthe document space is too sparse (see Anick andVaithyanathan (1997) for the comparison ofstatic and dynamic clustering).2.3.1 Dynamic incremental centroid clusteringWe make clusters based on incremental centroidmethod.
There are a few variations in theagglomerative clustering method.
Theagglomerative centroid method joins the pair ofclusters with the most similar centroid at eachstage (Frakes and Baeza-Yates, 1992).Incremental centroid clustering method isstraightforward.
The input document ofincremental clustering proceeds according to theranks of the top-ranked N documents resultedfrom vector space retrieval for a query.Document and cluster centroid are representedin vectors.
For the first input document (rank 1),create one cluster whose member is itself.
Foreach consecutive document (rank 2, ..., N),compute cosine similarity between the documentand each cluster centroid in the already createdclusters.
If the similarity between the documentand a cluster is above a threshold, then add thedocument to the cluster as a member and updatecluster centroid.
Otherwise, create a new clusterwith this document.
Note that one document canbe a member of several clusters as shown inFigure 2 (sold lines show that the documentbelongs to the cluster).2.3.2 Cluster preferenceSimilarities between the clusters and the query,or query-cluster similarities, are calculated bythe combination of the query inclusion ratio andvector inner product between the query vectorand the centroid vectors of the clusters.citiqiq wwqccqsimC ?
?= ?=1),(          (2)where |q| is the number of terms in the query,|cq| is the number of query terms included in acluster centroid, |cq|/|q| is the query inclusionratio for the cluster.
The documents included inthe same cluster have the same query-clustersimilarity.Cluster preferences are influenced by thequery inclusion ratio, which prefers the clusterwhose centroid includes more various queryterms.
Thus incorporating this information intothe weighting of each document means addinginformation which is related to the behavior ofterms in documents as well as the association ofterms and documents into the evaluation of therelevance of each document; it therefore has theeffect of ambiguity resolution.2.4 Reflecting cluster information to thedocumentsUsing the query-cluster similarity, were-calculate the relevance of each documentaccording to the following equation:),(),(),( cqsimCMAXdqsimDdqsim cd?
?= (3)where simD(q,d) is a query-document similarityby vector space retrieval as defined in equation(1) and simC(q,c) is a query-cluster similarity ofa document d defined in equation (2).
Since eachdocument can be a member of several clusters,we assign the highest query-cluster similarityvalue to the document.
The new documentsimilarity, sim(q,d), is calculated bymultiplication of a query-cluster similarity and aquery-document similarity.
Based on this newFigure 2.
Incremental centroid clustering in orderof the top-ranked N documentssimilarity sim(q,d), we re-rank the retrieveddocuments.
In the equation, we tried to useweighted sum of a query-document similarityand a query-cluster similarity.
The combinationby multiplication showed better performancesthan that of weighted sum.Through this procedure, we can effectivelytake into account the contexts of all the terms ina document as well as of the query terms.
Thus,even if a document which has a lowquery-document similarity can have a highquery-cluster similarity thanks to the effect ofneighboring documents in the same cluster.
Thereverse can be true as well.3 Experiments3.1 Experimental environmentWe evaluated our method on TREC-6 CLIR testcollection which contains 242,918 Englishdocuments (AP news from 1988 to 1990) and 24English queries.
English queries are translated toKorean queries manually.
We use title field ofqueries which consist of three fields such as title,description and narrative.In dictionary-based query translation, onequery term has multiple translations.
Table 3shows the degree of ambiguities.The number of Korean query terms 47The number of translated terms 149The average number of translations 3.2Table 1.
The degree of ambiguities for 24 queries.In our experiment, we only use 14 querieswhich consist of more than one term to observereal effects of our method.
This is because, if aquery consists of more than one term, humancan select the correct meaning of the term by itsneighbours.
But if a query consists of one termsuch as ?bank?
and it is polysemous, no one canresolve ambiguities without consideringadditional external information.
The rest 10queries which consist of one term are used todecide a threshold in incremental clustering.We use SMART system (Salton, 1989)developed at Cornell as a vector space retrieval.3.2 ResultsThe retrieval effectiveness was evaluated usingthe 11-point average precision metric.We compared our method with originalEnglish queries, with translated queries withambiguities, and with translated queries with thebest translation after disambiguation.
Thefollowings are the brief descriptions forcomparison methods:1) monolingual: the performance of vectorspace retrieval system for original Englishqueries as the monolingual baseline.2) tall_base: the performance of vector spaceretrieval system for translated Englishqueries which have all possible translationsin bilingual dictionaries without ambiguityresolution.3) tall_rerank: the performance of proposedmethod using dynamic incremental clustersfor the retrieved documents of tall_base.4) tone_base: the performance of vector spaceretrieval system for translated queries withthe best translations for each query termafter ambiguity resolution based on mutualinformation.5) tone_rerank: the performance of proposedmethod using dynamic incremental clustersfor the retrieved documents of tone_base.?tall_rerank?
and ?tone_rerank?
use ourimplicit disambiguation method.
The number oftop N documents used in dynamic incrementalclustering is 300 and thresholds for incrementalcentroid clustering are set as 0.41 which arelearned from training 10 queries with one termin both tall_rerank and tone_rerank.The main objective of this paper is to observethe performance change by incremental clustersfor translated queries with ambiguities (tall_baseand tall_rerank).Comparison 11-pt avg.
C/M Changeprecision (%) (%)1) monolingual 0.2858 100 -2) tall_base 0.2167 75.82 -3) tall_rerank 0.2780 97.27 +28.294) tone_base 0.2559 89.54 -5) tone_rerank 0.3026 105.87 +18.25Table 2.
The retrieval effectiveness for comparisonmethods.To observe the effect of clusters, wecompared the results after disambiguation basedon mutual information (tone_base andtone_rerank).
We selected the best translationbased on mutual information among alltranslation terms.
Mutual information MI(x,y) isdefined as following (Church and Hanks, 1990):)()(),(log)()(),(log),( 22 yfxfyxfNypxpyxpyxMI ?==  (4)where f(x) and f(y) are frequency of term x andterm y, respectively.
Co-occurrence frequency ofterm x and term y, f(x,y), is taken in window size6 for AP 1988 news documents.The 11-point average precision value,corresponding result to monolingual (C/M), andperformance change are summarized in Table 2.The retrieval effectiveness of tall_rerank is0.2780, corresponding to 97.27% ofmonolingual performance.
The performance oftone_rerank yields 0.3026 (105.87%).
This iseven better than the monolingual performance.The performance of our implicit ambiguityresolution method for all translations(tall_rerank) shows 8.63% improvementcompared with that of ambiguity resolutionbased on mutual information (tone_base).
Theproposed method achieved 28% improvementfor all translation queries and 18% for besttranslation queries compared with the vectorspace retrieval.
Our method afterdisambiguation (tone_rerank) using mutualinformation improved about 39.6% over vectorspace retrieval for all translations queries(tall_base).The cluster-based implicit disambiguationmethod, therefore, is more effective forperformance improvement than the simple querydisambiguation method based on mutualinformation; if used together, it shows yetfurther improvement.3.3 Result analysisWe examined the effects of our method for aquery with ambiguities increased after bilingualdictionary-based term translation.The Korean query is ????[ja-dong-cha]??
[gong-gi] ??[o-yeom]?
whose originalEnglish query is ?automobile air pollution?.
Thetranslated query with all the possible translationsin Korean-English dictionaries for this query isas follows:In this query, the term ????
is polysemouswhich has several meanings such as <air>,<atmosphere>, <jackstone>, <co-occurrence>,and <bowl>.
This is the cause of degradingsystem performance.146 clusters were created for the retrieved 300documents of this query.
The token number ofdocuments in the clusters was 435.
Thedistribution of cluster members is shown inFigure 3.
Most non-relevant documents had atendency to make singleton cluster, and mostrelevant documents made large group clusters.We examined inside the clusters how to seecluster give effects to resolve ambiguity andreflect context.
Cluster C4 in Figure 3 has 60members, which contains 56 relevant documentsand 4 non-relevant documents, among 209relevant documents for this query.
This clustercentroid includes following terms related to thequery:car 0.069automobile 0.127air 0.082atmosphere 0.018pollution 0.196contamination 0.064???
[ja-dong-cha]car, automobile, autocar,motorcar??
[gong-gi]air, atmosphere, empty vessel,bowl, jackstone, pebble, marbles??
[o-yeom]  contamination, pollutionC2C4 C1701020304050607080901000 20 40 60 80 100 120 140cluster ID# of member# of memberFigure 3.
The distribution of cluster membersfor the query with translation ambiguities.Although this centroid includes a noise term?atmosphere?, its weight is low.
The other termsare appropriate to the query; they are synonyms.Since all of the query terms are included in thecentroid, query inclusion ratio is 1 and allsynonyms affect positively to the vector innerproduct value.
Therefore, since this clusterpreference is high, the ranks of all documents inthis cluster changed higher.
The clusterperformed as a context of the documentsrelevant to the query.
Cluster C85 is a singletonwhose centroid includes one of three queryterms:bowl 0.101marble 0.191Since query inclusion ratio is low, the clusterpreference is low.
Therefore this cluster?s effectis weak to the document.Figure 4 presents the rank changes, calculatedby subtracting ranks by our method (tall_rerank)from those by vector space retrieval (tall_base)for each relevant document of the ambiguousquery.
The ranks of most documents arechanged higher through cluster analysis,although the ranks of some documents arechanged lower.
Figure 5 shows recall/precisioncurves for the performances of original Englishquery (monolingual; 0.6783 in 11-pt avg.precision), translated query withoutdisambiguation (tall_base; 0.5635), and ourmethod (tall_rerank; 0.6622).
For increasedquery ambiguity, we could achieve 97.62%performance compared to the monolingualretrieval.These results indicate that cluster analysishelp to resolve ambiguity.
Thus, we couldeffectively take into account the context of allthe terms in a document as well as the queryterms.4 ConclusionWe have proposed the method of applyingdynamic incremental clustering to the implicitresolution of query ambiguities inKorean-to-English cross-language informationretrieval.
The method used the clusters of0.00.10.20.30.40.50.60.70.80.91.00.00.10.20.30.40.50.60.70.80.91.0recallprecisionmonolingualtall_basetall_rerankFigure 5.
The performance comparison for the ambiguous query.Rankchanges(rankoftall_base?rank of tall_rerank )-120-80-40040801201601 17 71 91 882 99 995 47 116 04 077 26 839 14 251 00 62 91 10 82 21 18 10 21 24 99 51 28 74 91 33 25 41 39 77 01 42 58 41 52 76 21 69 32 31 74 09 41 77 57 21 78 69 21 79 71 61 82 65 91 86 11 51 96 07 71 96 98 52 07 81 22 24 25 62 27 46 42 29 47 52 33 70 8Relevant document ID for the queryRankchanges(rankoftall_base?rank of tall_rerank )Figure 4.
The rank changes of tall_rerank from rank of tall_base for each relevant document of the query.retrieved documents as a context forre-weighting each retrieved document and forre-ranking the retrieved documents.Our method was evaluated on TREC-6 CLIRtest collection.
This method achieved 28.29%performance improvement for translated querieswithout ambiguity resolution.
This correspondsto 97.27% of the monolingual performance.When our method was used with the queryambiguity resolution method based on mutualinformation, it showed 105.87% performanceimprovement of the monolingual retrieval.These results indicate that cluster analysis helpto resolve ambiguity greatly, and each clusteritself provide a context for a query.Our method is a language independent modelwhich can be applied to any language retrieval.We expect that our method will furtherimprove the results, although further research isneeded on combining a method to improve recallsuch as query expansion and relevance feedback.ReferencesAllan, J. Carbonell, J., Doddington, G. Yamron.
J.and Yang, Y.
(1998) Topic Detection and TrackingPilot Study: Final Report.
In Proc.
of the DARPABroadcast News Transcription and UnderstandingWorkshop, pp.194-218.Anick, P.G.
and Vaithyanathan, S. (1997) ExploitingClustering and Phrases for Context-BasedInformation Retrieval.
In Proc.
of 20th ACMSIGIR Conference (SIGIR?97).Chun, J.H.
(2000) Resolving Ambiguity and EnglishQuery Supplement using Parallel Corpora onKorean-English CLIR system.
MS thesis, Dept.
ofComputer Science, KAIST (in Korean).Church, K.W.
and Hanks P. (1990) Word AssociationNorms Mutual Information and Lexicography.Computational Linguistics, 16(1), pp.23-29.`Davis, M. (1996) New experiments in cross-languagetext retrieval at NMSU's computing research lab.
InProc.
of the fifth Text Retrieval Conference(TREC-5).Dumais, S.T., Letsche, T.A., Littman, M.L.
andLandauer, T.K.
(1997) Automatic cross-languageretrieval using latent semantic indexing.
In Proc.
ofAAAI Symposium on Cross-Language Text andSpeech Retrieval.Eichmann, D., Ruiz, M.E.
and Srinivasan, P. (1998)Cross-Language Information Retrieval with theUMLS Metathesaurus.
.
In Proc.
of the 21th ACMSIGIR Conference (SIGIR?98).Frakes, W.B., and Baeza-Yates, R. (1992)Information Retrieval: data structures & algorithms.New Jersey: Prentice Hall, pp.435-436.Gilarranz, J., Gonzalo, J. and Verdejo, F. (1997) AnApproach to Conceptual Text Retrieval Using theEuroWordNet Multilingual Semantic Database.
InProc.
of AAAI Spring Symposium onCross-Language Text and Speech Retrieval.Hearst, M.A.
and Pedersen, J.O.
(1996) Reexaminingthe Cluster Hypothesis: Scatter/Gather on RetrievalResults.
In Proc.
of 19th ACM SIGIR Conference(SIGIR?96).Hull, D.A.
and Grefenstette, G. (1996) Queryingacross languages: a dictionary-based approach tomultilingual information retrieval.
In Proc.
of the19th ACM SIGIR Conference (SIGIR?96).Jang, M.G., Myaeng, S.H.
and Park, S.H.
(1999)Using Mutual Information to Resolve QueryTranslation Ambiguities and Query TermWeighting.
In Proc.
of the 37th Annual Meeting ofthe Association for Computational Linguistics.Kwon, O-W., Kang, I.S., Lee, J-H and Lee, G.B.
(1997) Cross-Language Text Retrieval Based onDocument Translation Using Japanese-to-KoreanMT system.
In Proc.
of NLPRS'97, pp.
101-106.Lee, K.S., Park, Y.C., Choi, K.S.
(2001) Re-rankingmodel based on document clusters.
InformationProcessing and Management, 37(1), pp.
1-14.Oard, D.,W.
and Hackett, P. (1997) DocumentTranslation for the Cross-Language Text Retrievalat the University of Maryland.
In  Proc.
of theSixth Text REtrieval Conference (TREC-6).Rehder, B., Littman, M.L., Dumais, S. and Landauer,T.K.
(1997) Automatic 3-language cross-languageinformation retrieval with latent semantic indexing.In Proc.
of the Sixth Text REtrieval Conference(TREC-6).Salton, G. (1989) Automatic Text Processing: TheTransformation, Analysis, and Retrieval ofInformation by Computer.
Addison-Wesley,Reading, Pennsylvania.Voorhees, E.M. (1986) Implementing agglomerativehierarchic clustering algorithms for use indocument retrieval.
Information Processing &Management, 22(6), pp.
465-476.Yang, Y., Carbonell, J.G., Brown, R.D.
andFrederking, R.E.
(1998) Translingual InformationRetrieval: Learning from Bilingual Corpora.
AIJournal special issue, pp.
323-345.
