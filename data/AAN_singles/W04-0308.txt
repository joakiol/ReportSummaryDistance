Incrementality in Deterministic Dependency ParsingJoakim NivreSchool of Mathematics and Systems EngineeringVa?xjo?
UniversitySE-35195 Va?xjo?Swedenjoakim.nivre@msi.vxu.seAbstractDeterministic dependency parsing is a robustand efficient approach to syntactic parsing ofunrestricted natural language text.
In this pa-per, we analyze its potential for incrementalprocessing and conclude that strict incremen-tality is not achievable within this framework.However, we also show that it is possible to min-imize the number of structures that require non-incremental processing by choosing an optimalparsing algorithm.
This claim is substantiatedwith experimental evidence showing that the al-gorithm achieves incremental parsing for 68.9%of the input when tested on a random sampleof Swedish text.
When restricted to sentencesthat are accepted by the parser, the degree ofincrementality increases to 87.9%.1 IntroductionIncrementality in parsing has been advocatedfor at least two different reasons.
The first ismainly practical and has to do with real-timeapplications such as speech recognition, whichrequire a continually updated analysis of the in-put received so far.
The second reason is moretheoretical in that it connects parsing to cog-nitive modeling, where there is psycholinguis-tic evidence suggesting that human parsing islargely incremental (Marslen-Wilson, 1973; Fra-zier, 1987).However, most state-of-the-art parsing meth-ods today do not adhere to the principle of in-crementality, for different reasons.
Parsers thatattempt to disambiguate the input completely?
full parsing ?
typically first employ somekind of dynamic programming algorithm to de-rive a packed parse forest and then applies aprobabilistic top-down model in order to selectthe most probable analysis (Collins, 1997; Char-niak, 2000).
Since the first step is essentiallynondeterministic, this seems to rule out incre-mentality at least in a strict sense.
By contrast,parsers that only partially disambiguate the in-put ?
partial parsing ?
are usually determin-istic and construct the final analysis in one passover the input (Abney, 1991; Daelemans et al,1999).
But since they normally output a se-quence of unconnected phrases or chunks, theyfail to satisfy the constraint of incrementalityfor a different reason.Deterministic dependency parsing has re-cently been proposed as a robust and effi-cient method for syntactic parsing of unre-stricted natural language text (Yamada andMatsumoto, 2003; Nivre, 2003).
In some ways,this approach can be seen as a compromise be-tween traditional full and partial parsing.
Es-sentially, it is a kind of full parsing in that thegoal is to build a complete syntactic analysis forthe input string, not just identify major con-stituents.
But it resembles partial parsing inbeing robust, efficient and deterministic.
Takentogether, these properties seem to make de-pendency parsing suitable for incremental pro-cessing, although existing implementations nor-mally do not satisfy this constraint.
For exam-ple, Yamada and Matsumoto (2003) use a multi-pass bottom-up algorithm, combined with sup-port vector machines, in a way that does notresult in incremental processing.In this paper, we analyze the constraintson incrementality in deterministic dependencyparsing and argue that strict incrementality isnot achievable.
We then analyze the algorithmproposed in Nivre (2003) and show that, giventhe previous result, this algorithm is optimalfrom the point of view of incrementality.
Fi-nally, we evaluate experimentally the degree ofincrementality achieved with the algorithm inpractical parsing.2 Dependency ParsingIn a dependency structure, every word tokenis dependent on at most one other word to-ken, usually called its head or regent, whichPPP?a(In ?advNN60-taletthe-60?s ?prVBm?aladepaintedPNhanhe ?subJJdja?rvabold ?attNNtavlorpictures ?objHPsomwhich ?attVBretadeannoyed? subPMNikitaNikita ?objPMChrusjtjov.Chrustjev.
) ?idFigure 1: Dependency graph for Swedish sentencemeans that the structure can be represented asa directed graph, with nodes representing wordtokens and arcs representing dependency rela-tions.
In addition, arcs may be labeled withspecific dependency types.
Figure 1 shows alabeled dependency graph for a simple Swedishsentence, where each word of the sentence is la-beled with its part of speech and each arc la-beled with a grammatical function.In the following, we will restrict our atten-tion to unlabeled dependency graphs, i.e.
graphswithout labeled arcs, but the results will ap-ply to labeled dependency graphs as well.
Wewill also restrict ourselves to projective depen-dency graphs (Mel?cuk, 1988).
Formally, we de-fine these structures in the following way:1.
A dependency graph for a string of wordsW = w1?
?
?wn is a labeled directed graphD = (W,A), where(a) W is the set of nodes, i.e.
word tokensin the input string,(b) A is a set of arcs (wi, wj) (wi, wj ?
W ).We write wi < wj to express that wi pre-cedes wj in the string W (i.e., i < j); wewrite wi ?
wj to say that there is an arcfrom wi to wj ; we use ??
to denote the re-flexive and transitive closure of the arc re-lation; and we use ?
and ??
for the corre-sponding undirected relations, i.e.
wi ?
wjiff wi ?
wj or wj ?
wi.2.
A dependency graph D = (W,A) is well-formed iff the five conditions given in Fig-ure 2 are satisfied.The task of mapping a string W = w1?
?
?wnto a dependency graph satisfying these condi-tions is what we call dependency parsing.
For amore detailed discussion of dependency graphsand well-formedness conditions, the reader is re-ferred to Nivre (2003).3 Incrementality in DependencyParsingHaving defined dependency graphs, we maynow consider to what extent it is possible toconstruct these graphs incrementally.
In thestrictest sense, we take incrementality to meanthat, at any point during the parsing process,there is a single connected structure represent-ing the analysis of the input consumed so far.In terms of our dependency graphs, this wouldmean that the graph being built during parsingis connected at all times.
We will try to makethis more precise in a minute, but first we wantto discuss the relation between incrementalityand determinism.It seems that incrementality does not by itselfimply determinism, at least not in the sense ofnever undoing previously made decisions.
Thus,a parsing method that involves backtracking canbe incremental, provided that the backtrackingis implemented in such a way that we can alwaysmaintain a single structure representing the in-put processed up to the point of backtracking.In the context of dependency parsing, a case inpoint is the parsing method proposed by Kro-mann (Kromann, 2002), which combines heuris-tic search with different repair mechanisms.In this paper, we will nevertheless restrict ourattention to deterministic methods for depen-dency parsing, because we think it is easier topinpoint the essential constraints within a morerestrictive framework.
We will formalize deter-ministic dependency parsing in a way which isinspired by traditional shift-reduce parsing forcontext-free grammars, using a buffer of inputtokens and a stack for storing previously pro-cessed input.
However, since there are no non-terminal symbols involved in dependency pars-ing, we also need to maintain a representation ofthe dependency graph being constructed duringprocessing.We will represent parser configurations byUnique label (wi r?wj ?
wi r?
?wj) ?
r = r?Single head (wi?wj ?
wk?wj) ?
wi = wkAcyclic ?
(wi?wj ?
wj?
?wi)Connected wi?
?wjProjective (wi?wk ?
wi<wj<wk) ?
(wi?
?wj ?
wk?
?wj)Figure 2: Well-formedness conditions on dependency graphstriples ?S, I, A?, where S is the stack (repre-sented as a list), I is the list of (remaining) inputtokens, and A is the (current) arc relation forthe dependency graph.
(Since the nodes of thedependency graph are given by the input string,only the arc relation needs to be represented ex-plicitly.)
Given an input string W , the parser isinitialized to ?nil,W, ??
and terminates when itreaches a configuration ?S,nil, A?
(for any listS and set of arcs A).
The input string W isaccepted if the dependency graph D = (W,A)given at termination is well-formed; otherwiseW is rejected.In order to understand the constraints onincrementality in dependency parsing, we willbegin by considering the most straightforwardparsing strategy, i.e.
left-to-right bottom-upparsing, which in this case is essentially equiva-lent to shift-reduce parsing with a context-freegrammar in Chomsky normal form.
The parseris defined in the form of a transition system,represented in Figure 3 (where wi and wj arearbitrary word tokens):1.
The transition Left-Reduce combines thetwo topmost tokens on the stack, wi andwj , by a left-directed arc wj ?
wi and re-duces them to the head wj .2.
The transition Right-Reduce combinesthe two topmost tokens on the stack, wiand wj , by a right-directed arc wi ?
wjand reduces them to the head wi.3.
The transition Shift pushes the next inputtoken wi onto the stack.The transitions Left-Reduce and Right-Reduce are subject to conditions that ensurethat the Single head condition is satisfied.
ForShift, the only condition is that the input listis non-empty.As it stands, this transition system is non-deterministic, since several transitions can of-ten be applied to the same configuration.
Thus,in order to get a deterministic parser, we needto introduce a mechanism for resolving transi-tion conflicts.
Regardless of which mechanismis used, the parser is guaranteed to terminateafter at most 2n transitions, given an inputstring of length n. Moreover, the parser is guar-anteed to produce a dependency graph that isacyclic and projective (and satisfies the single-head constraint).
This means that the depen-dency graph given at termination is well-formedif and only if it is connected.We can now define what it means for the pars-ing to be incremental in this framework.
Ide-ally, we would like to require that the graph(W ?
I, A) is connected at all times.
How-ever, given the definition of Left-Reduce andRight-Reduce, it is impossible to connect anew word without shifting it to the stack first,so it seems that a more reasonable condition isthat the size of the stack should never exceed2.
In this way, we require every word to be at-tached somewhere in the dependency graph assoon as it has been shifted onto the stack.We may now ask whether it is possibleto achieve incrementality with a left-to-rightbottom-up dependency parser, and the answerturns out to be no in the general case.
This canbe demonstrated by considering all the possibleprojective dependency graphs containing onlythree nodes and checking which of these can beparsed incrementally.
Figure 4 shows the rele-vant structures, of which there are seven alto-gether.We begin by noting that trees (2?5) can all beconstructed incrementally by shifting the firsttwo tokens onto the stack, then reducing ?
withRight-Reduce in (2?3) and Left-Reduce in(4?5) ?
and then shifting and reducing again ?with Right-Reduce in (2) and (4) and Left-Reduce in (3) and (5).
By contrast, the threeremaining trees all require that three tokens areInitialization ?nil,W, ?
?Termination ?S,nil, A?Left-Reduce ?wjwi|S, I, A?
?
?wj |S, I, A ?
{(wj , wi)}?
?
?wk(wk, wi) ?
ARight-Reduce ?wjwi|S, I, A?
?
?wi|S, I, A ?
{(wi, wj)}?
?
?wk(wk, wj) ?
AShift ?S,wi|I, A?
?
?wi|S, I, A?Figure 3: Left-to-right bottom-up dependency parsing(1) a b c ? ?
(2) a b c ? ?
(3) a b c ? ?
(4) a b c ? ?
(5) a b c ? ?
(6) a b c ? ?
(7) a b c ? ?Figure 4: Projective three-node dependency structuresshifted onto the stack before the first reduction.However, the reason why we cannot parse thestructure incrementally is different in (1) com-pared to (6?7).In (6?7) the problem is that the first two to-kens are not connected by a single arc in thefinal dependency graph.
In (6) they are sisters,both being dependents on the third token; in(7) the first is the grandparent of the second.And in pure dependency parsing without non-terminal symbols, every reduction requires thatone of the tokens reduced is the head of theother(s).
This holds necessarily, regardless ofthe algorithm used, and is the reason why itis impossible to achieve strict incrementality independency parsing as defined here.
However,it is worth noting that (2?3), which are the mir-ror images of (6?7) can be parsed incrementally,even though they contain adjacent tokens thatare not linked by a single arc.
The reason isthat in (2?3) the reduction of the first two to-kens makes the third token adjacent to the first.Thus, the defining characteristic of the prob-lematic structures is that precisely the leftmosttokens are not linked directly.The case of (1) is different in that here theproblem is caused by the strict bottom-up strat-egy, which requires each token to have foundall its dependents before it is combined with itshead.
For left-dependents this is not a problem,as can be seen in (5), which can be processedby alternating Shift and Left-Reduce.
But in(1) the sequence of reductions has to be per-formed from right to left as it were, which rulesout strict incrementality.
However, whereas thestructures exemplified in (6?7) can never be pro-cessed incrementally within the present frame-work, the structure in (1) can be handled bymodifying the parsing strategy, as we shall seein the next section.It is instructive at this point to make a com-parison with incremental parsing based on ex-tended categorial grammar, where the struc-tures in (6?7) would normally be handled bysome kind of concatenation (or product), whichdoes not correspond to any real semantic com-bination of the constituents (Steedman, 2000;Morrill, 2000).
By contrast, the structure in (1)would typically be handled by function compo-sition, which corresponds to a well-defined com-positional semantic operation.
Hence, it mightbe argued that the treatment of (6?7) is onlypseudo-incremental even in other frameworks.Before we leave the strict bottom-up ap-proach, it can be noted that the algorithm de-scribed in this section is essentially the algo-rithm used by Yamada and Matsumoto (2003)in combination with support vector machines,except that they allow parsing to be performedin multiple passes, where the graph produced inone pass is given as input to the next pass.1 Themain motivation they give for parsing in multi-ple passes is precisely the fact that the bottom-up strategy requires each token to have foundall its dependents before it is combined with itshead, which is also what prevents the incremen-tal parsing of structures like (1).4 Arc-Eager Dependency ParsingIn order to increase the incrementality of deter-ministic dependency parsing, we need to com-bine bottom-up and top-down processing.
Moreprecisely, we need to process left-dependentsbottom-up and right-dependents top-down.
Inthis way, arcs will be added to the dependencygraph as soon as the respective head and depen-dent are available, even if the dependent is notcomplete with respect to its own dependents.Following Abney and Johnson (1991), we willcall this arc-eager parsing, to distinguish it fromthe standard bottom-up strategy discussed inthe previous section.Using the same representation of parser con-figurations as before, the arc-eager algorithmcan be defined by the transitions given in Fig-ure 5, where wi and wj are arbitrary word to-kens (Nivre, 2003):1.
The transition Left-Arc adds an arcwj r?
wi from the next input token wjto the token wi on top of the stack andpops the stack.2.
The transition Right-Arc adds an arcwi r?
wj from the token wi on top ofthe stack to the next input token wj , andpushes wj onto the stack.3.
The transition Reduce pops the stack.4.
The transition Shift (SH) pushes the nextinput token wi onto the stack.The transitions Left-Arc and Right-Arc, liketheir counterparts Left-Reduce and Right-Reduce, are subject to conditions that ensure1A purely terminological, but potentially confusing,difference is that Yamada and Matsumoto (2003) use theterm Right for what we call Left-Reduce and the termLeft for Right-Reduce (thus focusing on the positionof the head instead of the position of the dependent).that the Single head constraint is satisfied,while the Reduce transition can only be ap-plied if the token on top of the stack alreadyhas a head.
The Shift transition is the same asbefore and can be applied as long as the inputlist is non-empty.Comparing the two algorithms, we see thatthe Left-Arc transition of the arc-eager algo-rithm corresponds directly to the Left-Reducetransition of the standard bottom-up algorithm.The only difference is that, for reasons of sym-metry, the former applies to the token on topof the stack and the next input token insteadof the two topmost tokens on the stack.
If wecompare Right-Arc to Right-Reduce, how-ever, we see that the former performs no re-duction but simply shifts the newly attachedright-dependent onto the stack, thus makingit possible for this dependent to have right-dependents of its own.
But in order to allowmultiple right-dependents, we must also havea mechanism for popping right-dependents offthe stack, and this is the function of the Re-duce transition.
Thus, we can say that theaction performed by the Right-Reduce tran-sition in the standard bottom-up algorithm isperformed by a Right-Arc transition in combi-nation with a subsequent Reduce transition inthe arc-eager algorithm.
And since the Right-Arc and the Reduce can be separated by anarbitrary number of transitions, this permitsthe incremental parsing of arbitrary long right-dependent chains.Defining incrementality is less straightfor-ward for the arc-eager algorithm than for thestandard bottom-up algorithm.
Simply consid-ering the size of the stack will not do anymore,since the stack may now contain sequences oftokens that form connected components of thedependency graph.
On the other hand, since itis no longer necessary to shift both tokens to becombined onto the stack, and since any tokensthat are popped off the stack are connected tosome token on the stack, we can require thatthe graph (S,AS) should be connected at alltimes, where AS is the restriction of A to S, i.e.AS = {(wi, wj) ?
A|wi, wj ?
S}.Given this definition of incrementality, it iseasy to show that structures (2?5) in Figure 4can be parsed incrementally with the arc-eageralgorithm as well as with the standard bottom-up algorithm.
However, with the new algorithmwe can also parse structure (1) incrementally, asInitialization ?nil,W, ?
?Termination ?S,nil, A?Left-Arc ?wi|S,wj |I, A?
?
?S,wj |I, A ?
{(wj , wi)}?
?
?wk(wk, wi) ?
ARight-Arc ?wi|S,wj |I, A?
?
?wj |wi|S, I, A ?
{(wi, wj)}?
?
?wk(wk, wj) ?
AReduce ?wi|S, I, A?
?
?S, I, A?
?wj(wj , wi) ?
AShift ?S,wi|I, A?
?
?wi|S, I, A?Figure 5: Left-to-right arc-eager dependency parsingis shown by the following transition sequence:?nil, abc, ???
(Shift)?a, bc, ???
(Right-Arc)?ba, c, {(a, b)}??
(Right-Arc)?cba,nil, {(a, b), (b, c)}?We conclude that the arc-eager algorithm is op-timal with respect to incrementality in depen-dency parsing, even though it still holds truethat the structures (6?7) in Figure 4 cannot beparsed incrementally.
This raises the questionhow frequently these structures are found inpractical parsing, which is equivalent to askinghow often the arc-eager algorithm deviates fromstrictly incremental processing.
Although theanswer obviously depends on which languageand which theoretical framework we consider,we will attempt to give at least a partial answerto this question in the next section.
Before that,however, we want to relate our results to someprevious work on context-free parsing.First of all, it should be observed that theterms top-down and bottom-up take on a slightlydifferent meaning in the context of dependencyparsing, as compared to their standard use incontext-free parsing.
Since there are no nonter-minal nodes in a dependency graph, top-downconstruction means that a head is attached toa dependent before the dependent is attachedto (some of) its dependents, whereas bottom-up construction means that a dependent is at-tached to its head before the head is attached toits head.
However, top-down construction of de-pendency graphs does not involve the predictionof lower nodes from higher nodes, since all nodesare given by the input string.
Hence, in terms ofwhat drives the parsing process, all algorithmsdiscussed here correspond to bottom-up algo-rithms in context-free parsing.
It is interest-ing to note that if we recast the problem of de-pendency parsing as context-free parsing with aCNF grammar, then the problematic structures(1), (6?7) in Figure 4 all correspond to right-branching structures, and it is well-known thatbottom-up parsers may require an unboundedamount of memory in order to process right-branching structure (Miller and Chomsky, 1963;Abney and Johnson, 1991).Moreover, if we analyze the two algorithmsdiscussed here in the framework of Abney andJohnson (1991), they do not differ at all as tothe order in which nodes are enumerated, butonly with respect to the order in which arcs areenumerated; the first algorithm is arc-standardwhile the second is arc-eager.
One of the obser-vations made by Abney and Johnson (1991), isthat arc-eager strategies for context-free pars-ing may sometimes require less space than arc-standard strategies, although they may leadto an increase in local ambiguities.
It seemsthat the advantage of the arc-eager strategyfor dependency parsing with respect to struc-ture (1) in Figure 4 can be explained along thesame lines, although the lack of nonterminalnodes in dependency graphs means that thereis no corresponding increase in local ambigui-ties.
Although a detailed discussion of the re-lation between context-free parsing and depen-dency parsing is beyond the scope of this paper,we conjecture that this may be a genuine advan-tage of dependency representations in parsing.Connected Parser configurationscomponents Number Percent0 1251 7.61 10148 61.32 2739 16.63 1471 8.94 587 3.55 222 1.36 98 0.67 26 0.28 3 0.0?
1 11399 68.9?
3 15609 94.3?
8 16545 100.0Table 1: Number of connected components in (S,AS) during parsing5 Experimental EvaluationIn order to measure the degree of incremental-ity achieved in practical parsing, we have eval-uated a parser that uses the arc-eager parsingalgorithm in combination with a memory-basedclassifier for predicting the next transition.
Inexperiments reported in Nivre et al (2004), aparsing accuracy of 85.7% (unlabeled attach-ment score) was achieved, using data from asmall treebank of Swedish (Einarsson, 1976), di-vided into a training set of 5054 sentences anda test set of 631 sentences.
However, in thepresent context, we are primarily interested inthe incrementality of the parser, which we mea-sure by considering the number of connectedcomponents in (S,AS) at different stages dur-ing the parsing of the test data.The results can be found in Table 1, wherewe see that out of 16545 configurations used inparsing 613 sentences (with a mean length of14.0 words), 68.9% have zero or one connectedcomponent on the stack, which is what we re-quire of a strictly incremental parser.
We alsosee that most violations of incrementality arefairly mild, since more than 90% of all configu-rations have no more than three connected com-ponents on the stack.Many violations of incrementality are causedby sentences that cannot be parsed into a well-formed dependency graph, i.e.
a single projec-tive dependency tree, but where the output ofthe parser is a set of internally connected com-ponents.
In order to test the influence of incom-plete parses on the statistics of incrementality,we have performed a second experiment, wherewe restrict the test data to those 444 sentences(out of 613), for which the parser produces awell-formed dependency graph.
The results canbe seen in Table 2.
In this case, 87.1% of allconfigurations in fact satisfy the constraints ofincrementality, and the proportion of configu-rations that have no more than three connectedcomponents on the stack is as high as 99.5%.It seems fair to conclude that, although strictword-by-word incrementality is not possible indeterministic dependency parsing, the arc-eageralgorithm can in practice be seen as a close ap-proximation of incremental parsing.6 ConclusionIn this paper, we have analyzed the potentialfor incremental processing in deterministic de-pendency parsing.
Our first result is negative,since we have shown that strict incrementalityis not achievable within the restrictive parsingframework considered here.
However, we havealso shown that the arc-eager parsing algorithmis optimal for incremental dependency parsing,given the constraints imposed by the overallframework.
Moreover, we have shown that inpractical parsing, the algorithm performs in-cremental processing for the majority of inputstructures.
If we consider all sentences in thetest data, the share is roughly two thirds, but ifwe limit our attention to well-formed output, itis almost 90%.
Since deterministic dependencyparsing has previously been shown to be com-petitive in terms of parsing accuracy (Yamadaand Matsumoto, 2003; Nivre et al, 2004), webelieve that this is a promising approach for sit-uations that require parsing to be robust, effi-cient and (almost) incremental.Connected Parser configurationscomponents Number Percent0 928 9.21 7823 77.82 1000 10.03 248 2.54 41 0.45 8 0.16 1 0.0?
1 8751 87.1?
3 9999 99.5?
6 10049 100.0Table 2: Number of connected components in (S,AS) for well-formed treesAcknowledgementsThe work presented in this paper was sup-ported by a grant from the Swedish Re-search Council (621-2002-4207).
The memory-based classifiers used in the experiments wereconstructed using the Tilburg Memory-BasedLearner (TiMBL) (Daelemans et al, 2003).Thanks to three anonymous reviewers for con-structive comments on the submitted paper.ReferencesSteven Abney and Mark Johnson.
1991.
Mem-ory requirements and local ambiguities ofparsing strategies.
Journal of Psycholinguis-tic Research, 20:233?250.Steven Abney.
1991.
Parsing by chunks.In Principle-Based Parsing, pages 257?278.Kluwer.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings NAACL-2000.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annatual Meeting of theAssociation for Computational Linguistics,pages 16?23, Madrid, Spain.Walter Daelemans, Sabine Buchholz, and JornVeenstra.
1999.
Memory-based shallow pars-ing.
In Proceedings of the 3rd Conferenceon Computational Natural Language Learn-ing (CoNLL), pages 77?89.Walter Daelemans, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
2003.Timbl: Tilburg memory based learner, ver-sion 5.0, reference guide.
Technical ReportILK 03-10, Tilburg University, ILK.Jan Einarsson.
1976.
Talbankens skriftspr?aks-konkordans.
Lund University.Lyn Frazier.
1987.
Syntactic processing: Ev-idence from Dutch.
Natural Language andLinguistic Theory, 5:519?559.Matthias Trautner Kromann.
2002.
Optimalityparsing and local cost functions in Discontin-uous Grammar.
Electronic Notes of Theoret-ical Computer Science, 52.William Marslen-Wilson.
1973.
Linguisticstructure and speech shadowing at very shortlatencies.
Nature, 244:522?533.Igor Mel?cuk.
1988.
Dependency Syntax: The-ory and Practice.
State University of NewYork Press.George A. Miller and Noam Chomsky.
1963.Finitary models of language users.
In R. D.Luce, R. R. Bush, and E. Galanter, editors,Handbook of Mathematical Psychology.
Vol-ume 2.
Wiley.Glyn Morrill.
2000.
Inremental processingand acceptability.
Computational Linguis-tics, 26:319?338.Joakim Nivre, Johan Hall, and Jens Nils-son.
2004.
Memory-based dependency pars-ing.
In Proceedings of the 8th Conferenceon Computational Natural Language Learn-ing (CoNLL), pages 49?56.Joakim Nivre.
2003.
An efficient algorithmfor projective dependency parsing.
In Pro-ceedings of the 8th International Workshopon Parsing Technologies (IWPT), pages 149?160.Mark Steedman.
2000.
The Syntactic Process.MIT Press.Hiroyasu Yamada and Yuji Matsumoto.
2003.Statistical dependency analysis with supportvector machines.
In Proceedings of the 8th In-ternational Workshop on Parsing Technolo-gies (IWPT), pages 195?206.
