Unsupervised methods for developing taxonomies by combining syntacticand statistical informationDominic WiddowsCenter for the Study of Language and Information, Stanford Universitydwiddows@csli.stanford.eduAbstractThis paper describes an unsupervised algo-rithm for placing unknown words into a taxon-omy and evaluates its accuracy on a large andvaried sample of words.
The algorithm worksby first using a large corpus to find semanticneighbors of the unknown word, which we ac-complish by combining latent semantic analy-sis with part-of-speech information.
We thenplace the unknown word in the part of the tax-onomy where these neighbors are most concen-trated, using a class-labelling algorithm devel-oped especially for this task.
This method isused to reconstruct parts of the existing Word-Net database, obtaining results for commonnouns, proper nouns and verbs.
We evaluatethe contribution made by part-of-speech tag-ging and show that automatic filtering using theclass-labelling algorithm gives a fourfold im-provement in accuracy.1 IntroductionThe importance of automatic methods for enriching lex-icons, taxonomies and knowledge bases from free text iswell-recognized.
For rapidly changing domains such ascurrent affairs, static knowledge bases are inadequate forresponding to new developments, and the cost of buildingand maintaining resources by hand is prohibitive.This paper describes experiments which develop auto-matic methods for taking an original taxonomy as a skele-ton and fleshing it out with new terms which are discov-ered in free text.
The method is completely automatic andit is completely unsupervised apart from using the origi-nal taxonomic skeleton to suggest possible classificationsfor new terms.
We evaluate how accurately our meth-ods can reconstruct the WordNet taxonomy (Fellbaum,1998).The problem of enriching the lexical information ina taxonomy can be posed in two complementary ways.Firstly, given a particular taxonomic class (such as fruit)one could seek members of this class (such as apple, ba-nana).
This problem is addressed by Riloff and Shepherd(1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002).
Secondly, given a partic-ular word (such as apple), one could seek suitable tax-onomic classes for describing this object (such as fruit,foodstuff).
The work in this paper addresses the secondof these questions.The goal of automatically placing new words into ataxonomy has been attempted in various ways for at leastten years (Hearst and Schu?tze, 1993).
The process forplacing a word w in a taxonomy T using a corpus C oftencontains some version of the following stages:?
For a word w, find words from the corpus C whoseoccurrences are similar to those of w. Considerthese the ?corpus-derived neighbors?
N(w) of w.?
Assuming that at least some of these neighbors arealready in the taxonomy T , map w to the place inthe taxonomy where these neighbors are most con-centrated.Hearst and Schu?tze (1993) added 27 words to Word-Net using a version of this process, with a 63% ac-curacy at assigning new words to one of a number ofdisjoint WordNet ?classes?
produced by a previous al-gorithm.
(Direct comparison with this result is prob-lematic since the number of classes used is not stated.
)A more recent example is the top-down algorithm ofAlfonseca and Manandhar (2001), which seeks the nodein T which shares the most collocational properties withthe word w, adding 42 concepts taken from The Lord ofthe Rings with an accuracy of 28%.The algorithm as presented above leaves many degreesof freedom and open questions.
What methods shouldbe used to obtain the corpus-derived neighbors N(w)?This question is addressed in Section 2.
Given a col-lection of neighbors, how should we define a ?place inthe taxonomy where these neighbors are most concen-trated??
This question is addressed in Section 3, whichEdmonton, May-June 2003Main Papers , pp.
197-204Proceedings of HLT-NAACL 2003defines a robust class-labelling algorithm for mapping alist of words into a taxonomy.
In Section 4 we describeexperiments, determining the accuracy with which thesemethods can be used to reconstruct the WordNet taxon-omy.
To our knowledge, this is the first such evaluationfor a large sample of words.
Section 5 discusses relatedwork and other problems to which these techniques canbe adapted.2 Finding semantic neighbors: Combininglatent semantic analysis withpart-of-speech information.There are many empirical techniques for recognizingwhen words are similar in meaning, rooted in the idea that?you shall know a word by the company it keeps?
(Firth,1957).
It is certainly the case that words which repeat-edly occur with similar companions often have relatedmeanings, and common features used for determiningthis similarity include shared collocations (Lin, 1999),co-occurrence in lists of objects (Widdows and Dorow,2002) and latent semantic analysis (Landauer and Du-mais, 1997; Hearst and Schu?tze, 1993).The method used to obtain semantic neighbors in ourexperiments was a version of latent semantic analysis,descended from that used by Hearst and Schu?tze (1993,?4).
First, 1000 frequent words were chosen as col-umn labels (after removing stopwords (Baeza-Yates andRibiero-Neto, 1999, p. 167)).
Other words were assignedco-ordinates determined by the number of times they oc-cured within the same context-window (15 words) as oneof the 1000 column-label words in a large corpus.
Thisgave a matrix where every word is represented by a row-vector determined by its co-occurence with frequently oc-curing, meaningful words.
Since this matrix was verysparse, singular value decomposition (known in this con-text as latent semantic analysis (Landauer and Dumais,1997)) was used to reduce the number of dimensionsfrom 1000 to 100.
This reduced vector space is calledWordSpace (Hearst and Schu?tze, 1993, ?4).
Similaritybetween words was then computed using the cosine sim-ilarity measure (Baeza-Yates and Ribiero-Neto, 1999, p.28).
Such techniques for measuring similarity betweenwords have been shown to capture semantic properties:for example, they have been used successfully for recog-nizing synonymy (Landauer and Dumais, 1997) and forfinding correct translations of individual terms (Widdowset al, 2002).The corpus used for these experiments was the BritishNational Corpus, which is tagged for parts-of-speech.This enabled us to build syntactic distinctions intoWordSpace ?
instead of just giving a vector for the stringtest we were able to build separate vectors for the nouns,verbs and adjectives test.
An example of the contribu-tion of part-of-speech information to extracting seman-tic neighbors of the word fire is shown in Table 2.
Ascan be seen, the noun fire (as in the substance/element)and the verb fire (mainly used to mean firing some sortof weapon) are related to quite different areas of mean-ing.
Building a single vector for the string fire confusesthis distinction ?
the neighbors of fire treated just as astring include words related to both the meaning of fire asa noun (more frequent in the BNC) and as a verb.Part of the goal of our experiments was to investi-gate the contribution that this part-of-speech informationmade for mapping words into taxonomies.
As far as weare aware, these experiments are the first to investigatethe combination of latent semantic indexing with part-of-speech information.3 Finding class-labels: Mappingcollections of words into a taxonomyGiven a collection of words or multiword expressionswhich are semantically related, it is often important toknow what these words have in common.
All adults withnormal language competence and world knowledge areadept at this task ?
we know that plant, animal and fun-gus are all living things, and that plant, factory and worksare all kinds of buildings.
This ability to classify objects,and to work out which of the possible classifications of agiven object is appropriate in a particular context, is es-sential for understanding and reasoning about linguisticmeaning.
We will refer to this process as class-labelling.The approach demonstrated here uses a hand-built tax-onomy to assign class-labels to a collection of similarnouns.
As with much work of this nature, the taxonomyused is WordNet (version 1.6), a freely-available broad-coverage lexical database for English (Fellbaum, 1998).Our algorithm finds the hypernyms which subsume asmany as possible of the original nouns, as closely as pos-sible 1.
The concept v is said to be a hypernym of w ifw is a kind of v. For this reason this sort of a taxonomyis sometimes referred to as an ?IS A hierarchy?.
For ex-ample, the possible hypernyms given for the word oak inWordNet 1.6 areoak ?
wood ?
plant material ?
material,stuff ?
substance, matter ?
object, physicalobject ?
entity, something1Another method which could be used for class-labelling is given by the conceptual density algorithm ofAgirre and Rigau (1996), which those authors applied to word-sense disambiguation.
A different but related idea is presentedby Li and Abe (1998), who use a principle from informationtheory to model selectional preferences for verbs using differ-ent classes from a taxonomy.
Their algorithm and goals aredifferent from ours: we are looking for a single class-label forsemantically related words, whereas for modelling selectionalpreferences several classes may be appropriate.fire (string only) fire nn1 fire vvifire 1.000000 fire nn1 1.000000 fire vvi 1.000000flames 0.709939 flames nn2 0.700575 guns nn2 0.663820smoke 0.680601 smoke nn1 0.696028 firing vvg 0.537778blaze 0.668504 brigade nn1 0.589625 cannon nn0 0.523442firemen 0.627065 fires nn2 0.584643 gun nn1 0.484106fires 0.617494 firemen nn2 0.567170 fired vvd 0.478572explosion 0.572138 explosion nn1 0.551594 detectors nn2 0.477025burning 0.559897 destroyed vvn 0.547631 artillery nn1 0.469173destroyed 0.558699 burning aj0 0.533586 attack vvb 0.468767brigade 0.532248 blaze nn1 0.529126 firing nn1 0.459000arson 0.528909 arson nn1 0.522844 volley nn1 0.458717accidental 0.519310 alarms nn2 0.512332 trained vvn 0.447797chimney 0.489577 destroyed vvd 0.512130 enemy nn1 0.445523blast 0.488617 burning vvg 0.502052 alert aj0 0.443610guns 0.487226 burnt vvn 0.500864 shoot vvi 0.443308damaged 0.484897 blast nn1 0.498635 defenders nn2 0.438886Table 1: Semantic neighbors of fire with different parts-of-speech.
The scores are cosine similaritiesoak, oak tree ?
tree ?
woody plant, ligneousplant ?
vascular plant, tracheophyte ?
plant,flora, plant life ?
life form, organism, being,living thing ?
entity, somethingLet S be a set of nouns or verbs.
If the word w ?
S isrecognized by WordNet, the WordNet taxonomy assignsto w an ordered set of hypernyms H(w).Consider the unionH =?w?SH(w).This is the set of all hypernyms of any member of S. Ourintuition is that the most appropriate class-label for theset S is the hypernym h ?
H which subsumes as manyas possible of the members of S as closely as possiblein the hierarchy.
There is a trade-off here between sub-suming ?as many as possible?
of the members of S, andsubsuming them ?as closely as possible?.
This line of rea-soning can be used to define a whole collection of ?class-labelling algorithms?.For each w ?
S and for each h ?
H, define the affinityscore function ?
(w, h) between w and h to be?
(w, h) ={f(dist(w, h)) if h ?
H(w)?g(w, h) if h /?
H(w), (1)where dist(w, h) is a measure of the distance between wand h, f is some positive, monotonically decreasing func-tion, and g is some positive (possibly constant) function.The function f accords ?positive points?
to h if h sub-sumes w, and the condition that f be monotonically de-creasing ensures that h gets more positive points thecloser it is to w. The function g subtracts ?penalty points?if h does not subsume w. This function could depend inmany ways on w and h ?
for example, there could be asmaller penalty if h is a very specific concept than if h isa very general concept.The distance measure dist(w, h) could take manyforms, and there are already a number of distance mea-sures available to use with WordNet (Budanitsky andHirst, 2001).
The easiest method for assigning a distancebetween words and their hypernyms is to count the num-ber of intervening levels in the taxonomy.
This assumesthat the distance in specificity between ontological levelsis constant, which is of course not the case, a problemaddressed by Resnik (1999).Given an appropriate affinity score, it is a simple matterto define the best class-label for a collection of objects.Definition 1 Let S be a set of nouns, let H =?w?S H(w) be the set of hypernyms of S and let ?
(w, h)be an affinity score function as defined in equation (1).The best class-label hmax(S) for S is the node hmax ?
Hwith the highest total affinity score summed over all themembers of S, so hmax is the node which gives the max-imum scoremaxh?H?w?S?
(w, h).Since H is determined by S, hmax is solely determinedby the set S and the affinity score ?.In the event that hmax is not unique, it is customary totake the most specific class-label available.ExampleA particularly simple example of this kind of algorithmis used by Hearst and Schu?tze (1993).
First they parti-tion the WordNet taxonomy into a number of disjoint setswhich are used as class-labels.
Thus each concept hasa single ?hypernym?, and the ?affinity-score?
between aword w and a class h is simply the set membership func-tion, ?
(w, h) = 1 if w ?
h and 0 otherwise.
A collectionof words is assigned a class-label by majority voting.3.1 AmbiguityIn theory, rather than a class-label for related strings, wewould like one for related meanings ?
the concepts towhich the strings refer.
To implement this for a set ofwords, we alter our affinity score function ?
as follows.Let C(w) be the set of concepts to which the word wcould refer.
(So each c ?
C is a possible sense of w.)Then?
(w, h) = maxc?C(w){f(dist(c, h)) if h ?
H(c)?g(w, c) if h /?
H(c), (2)This implies that the ?preferred-sense?
of w with respectto the possible subsumer h is the sense closest to h. Inpractice, our class-labelling algorithm implements thispreference by computing the affinity score ?
(c, h) for allc ?
C(w) and only using the best match.
This selec-tive approach is much less noisy than simply averagingthe probability mass of the word over each possible sense(the technique used in (Li and Abe, 1998), for example).3.2 Choice of scoring functions for theclass-labelling algorithmThe precise choice of class-labelling algorithm dependson the functions f and g in the affinity score function?
of equation (2).
There is some tension here betweenbeing correct and being informative: ?correct?
but unin-formative class-labels (such as entity, something) can beobtained easily by preferring nodes high up in the hier-archy, but since our goal in this work was to classify un-known words in an informative and accurate fashion, thefunctions f and g had to be chosen to give an appropriatebalance.
After a variety of heuristic tests, the function fwas chosen to bef = 1dist(w, h)2 ,where for the distance function dist(w, h) we chose thecomputationally simple method of counting the numberof taxonomic levels between w and h (inclusively toavoid dividing by zero).
For the penalty function g wechose the constant g = 0.25.The net effect of choosing the reciprocal-distance-squared and a small constant penalty function was thathypernyms close to the concept in question received mag-nified credit, but possible class-labels were not penalizedtoo harshly for missing out a node.
This made the algo-rithm simple and robust to noise but with a strong prefer-ence for detailed information-bearing class-labels.
Thisconfiguration of the class-labelling algorithm was used inall the experiments described below.4 Experiments and EvaluationTo test the success of our approach to placing unknownwords into the WordNet taxonomy on a large and signif-icant sample, we designed the following experiment.
Ifthe algorithm is successful at placing unknown words inthe correct new place in a taxonomy, we would expect itto place already known words in their current position.The experiment to test this worked as follows.?
For a word w, find the neighbors N(w) of w inWordSpace.
Remove w itself from this set.?
Find the best class-label hmax(N(w)) for this set(using Definition 1).?
Test to see if, according to WordNet, hmax is a hy-pernym of the original word w, and if so check howclosely hmax subsumes w in the taxonomy.Since our class-labelling algorithm gives a ranked listof possible hypernyms, credit was given for correct clas-sifications in the top 4 places.
This algorithm was testedon singular common nouns (PoS-tag nn1), proper nouns(PoS-tag np0) and finite present-tense verbs (PoS-tagvvb).
For each of these classes, a random sample of wordswas selected with corpus frequencies ranging from 1000to 250.
For the noun categories, 600 words were sam-pled, and for the finite verbs, 420.
For each word w, wefound semantic neighbors with and without using part-of-speech information.
The same experiments were carriedout using 3, 6 and 12 neighbors: we will focus on the re-sults for 3 and 12 neighbors since those for 6 neighborsturned out to be reliably ?somewhere in between?
thesetwo.Results for Common NounsThe best results for reproducing WordNet classifica-tions were obtained for common nouns, and are sum-marized in Table 2, which shows the percentage of testwords w which were given a class-label h which was acorrect hypernym according to WordNet (so for whichh ?
H(w)).
For these words for which a correct clas-sification was found, the ?Height?
columns refer to thenumber of levels in the hierarchy between the target wordw and the class-label h. If the algorithm failed to find aclass-label h which is a hypernym of w, the result wascounted as ?Wrong?.
The ?Missing?
column records thenumber of words in the sample which are not in WordNetat all.The following trends are apparent.
For finding anycorrect class-label, the best results were obtained bytaking 12 neighbors and using part-of-speech informa-tion, which found a correct classification for 485/591 =82% of the common nouns that were included in Word-Net.
This compares favorably with previous experiments,though as stated earlier it is difficult to be sure we arecomparing like with like.
Finding the hypernym whichimmediately subsumes w (with no intervening nodes)exactly reproduces a classification given by WordNet,and as such was taken to be a complete success.
Tak-ing fewer neighbors and using PoS-information both im-proved this success rate, the best accuracy obtained be-ing 86/591 = 15%.
However, this configuration actuallygave the worst results at obtaining a correct classificationoverall.Height 1 2 3 4 5 6 7 8 9 10 Wrong MissingCommon Nouns (sample size 600)3 neighborsWith PoS 14.3 26.1 33.1 37.8 39.8 40.6 41.5 42.0 42.0 42.0 56.5 1.5Strings only 11.8 23.3 31.3 36.6 39.6 41.1 42.1 42.3 42.3 42.3 56.1 1.512 neighborsWith PoS 10.0 21.8 36.5 48.5 59.3 70.0 76.6 78.8 79.8 80.8 17.6 1.5without PoS 8.5 21.5 33.6 46.8 57.1 66.5 72.8 74.6 75.3 75.8 22.6 1.5Proper Nouns (sample size 600)3 neighborsWith PoS 10.6 13.8 15.5 16.5 108 18.6 18.8 18.8 19.1 19.3 25.0 55.6Strings only 9.8 14.3 16.1 18.6 19.5 20.1 20.8 21.1 21.5 21.6 22.1 55.612 neighborsWith PoS 10.5 14.5 16.3 18.1 22.0 23.8 25.5 28.0 28.5 29.3 15.0 55.6Strings only 9.5 13.8 17.5 20.8 22.3 24.6 26.6 30.7 32.5 34.3 10.0 55.6Verbs (sample size 420)3 neighborsWith PoS 17.6 30.2 36.1 40.4 42.6 43.0 44.0 44.0 44.0 44.0 52.6 3.3Strings only 24.7 39.7 43.3 45.4 47.1 48.0 48.3 48.8 49.0 49.0 47.6 3.312 neighborsWith PoS 19.0 36.4 43.5 48.8 52.8 54.2 55.2 55.4 55.7 55.9 40.7 3.3Strings only 28.0 48.3 55.9 60.2 63.3 64.2 64.5 65.0 65.0 65.0 31.7 3.3Table 2: Percentage of words which were automatically assigned class-labels which subsume them in the WordNettaxonomy, showing the number of taxonomic levels between the target word and the class-labelHeight 1 2 3 4 5 6 WrongCommon Nouns 0.799 0.905 0.785 0.858 0.671 0.671 0.569Proper Nouns 1.625 0.688 0.350 0.581 0.683 0.430 0.529Verbs 1.062 1.248 1.095 1.103 1.143 0.750 0.669Table 3: Average affinity score of class-labels for successful and unsuccessful classificationsIn conclusion, taking more neighbors makes thechances of obtaining some correct classification for aword w greater, but taking fewer neighbors increases thechances of ?hitting the nail on the head?.
The use of part-of-speech information reliably increases the chances ofcorrectly obtaining both exact and broadly correct classi-fications, though careful tuning is still necessary to obtainoptimal results for either.Results for Proper Nouns and VerbsThe results for proper nouns and verbs (also in Table2) demonstrate some interesting problems.
On the whole,the mapping is less reliable than for common nouns, atleast when it comes to reconstructing WordNet as it cur-rently stands.Proper nouns are rightly recognized as one of the cat-egories where automatic methods for lexical acquisitionare most important (Hearst and Schu?tze, 1993, ?4).
Itis impossible for a single knowledge base to keep up-to-date with all possible meanings of proper names, and thiswould be undesirable without considerable filtering abil-ities because proper names are often domain-specific.Ih our experiments, the best results for proper nounswere those obtained using 12 neighbors, where a cor-rect classification was found for 206/266 = 77% of theproper nouns that were included in WordNet, using nopart-of-speech information.
Part-of-speech informationstill helps for mapping proper nouns into exactly the rightplace, but in general degrades performance.Several of the proper names tested are geographical,and in the BNC they often refer to regions of the BritishIsles which are not in WordNet.
For example, hampshireis labelled as a territorial division, which as an Englishcounty it certainly is, but in WordNet hampshire is in-stead a hyponym of domestic sheep.
For many of theproper names which our evaluation labelled as ?wronglyclassified?, the classification was in fact correct but a dif-ferent meaning from those given in WordNet.
The chal-lenge for these situations is how to recognize when cor-pus methods give a correct meaning which is differentfrom the meaning already listed in a knowledge base.Many of these meanings will be systematically related(such as the way a region is used to name an item orproduct from that region, as with the hampshire exampleabove) by generative processes which are becoming wellunderstood by theoretical linguists (Pustejovsky, 1995),and linguistic theory may help our statistical algorithmsconsiderably by predicting what sort of new meanings wemight expect a known word to assume through metonymyand systematic polysemy.Typical first names of people such as lisa and ralph al-most always have neighbors which are also first names(usually of the same gender), but these words are not rep-resented in WordNet.
This lexical category is ripe forautomatic discovery: preliminary experiments using thetwo names above as ?seed-words?
(Roark and Charniak,1998; Widdows and Dorow, 2002) show that by takinga few known examples, finding neighbors and removingwords which are already in WordNet, we can collect firstnames of the same gender with at least 90% accuracy.Verbs pose special problems for knowledge bases.
Theusefulness of an IS A hierarchy for pinpointing informa-tion and enabling inference is much less clear-cut thanfor nouns.
For example, sleeping does entail breathingand arriving does imply moving, but the aspectual prop-erties, argument structure and case roles may all be dif-ferent.
The more restrictive definition of troponymy isused in WordNet to describe those properties of verbsthat are inherited through the taxonomy (Fellbaum, 1998,Ch 3).
In practice, the taxonomy of verbs in WordNettends to have fewer levels and many more branches thanthe noun taxonomy.
This led to problems for our class-labelling algorithm ?
class-labels obtained for the verbplay included exhaust, deploy, move and behave, all ofwhich are ?correct?
hypernyms according to WordNet,while possible class-labels obtained for the verb appealincluded keep, defend, reassert and examine, all of whichwere marked ?wrong?.
For our methods, the WordNettaxonomy as it stands appears to give much less reli-able evaluation criteria for verbs than for common nouns.It is also plausible that similarity measures based uponsimple co-occurence are better for modelling similaritybetween nominals than between verbs, an observationwhich is compatible with psychological experiments onword-association (Fellbaum, 1998, p. 90).In our experiments, the best results for verbs wereclearly those obtained using 12 neighbors and no part-of-speech information, for which some correct classifi-cation was found for 273/406 = 59% of the verbs thatwere included in WordNet, and which achieved better re-sults than those using part-of-speech information even forfinding exact classifications.
The shallowness of the tax-onomy for verbs means that most classifications whichwere successful at all were quite close to the word inquestion, which should be taken into account when in-terpreting the results in Table 2.As we have seen, part-of-speech information degradedperformance overall for proper nouns and verbs.
Thismay be because combining all uses of a particular word-form into a single vector is less prone to problems of datasparseness, especially if these word-forms are semanti-cally related in spite of part-of-speech differences 2.
It isalso plausible that discarding part-of-speech information2This issue is reminiscent of the question of whether stem-ming improves or harms information retrieval (Baeza-Yates andRibiero-Neto, 1999) ?
the received wisdom is that stemming(at best) improves recall at the expense of precision and ourfindings for proper nouns are consistent with this.should improve the classification of verbs for the follow-ing reason.
Classification using corpus-derived neighborsis markedly better for common nouns than for verbs, andmost of the verbs in our sample (57%) also occur as com-mon nouns in WordSpace.
(In contrast, only 13% of ourcommon nouns also occur as verbs, a reliable asymmetryfor English.)
Most of these noun senses are semanticallyrelated in some way to the corresponding verbs.
Sinceusing neighboring words for classification is demonstra-bly more reliable for nouns than for verbs, putting theseparts-of-speech together in a single vector in WordSpacemight be expected to improve performance for verbs butdegrade it for nouns.Filtering using Affinity scoresOne of the benefits of the class-labelling algorithm(Definition 1) presented in this paper is that it returns notjust class-labels but an affinity score measuring how welleach class-label describes the class of objects in question.The affinity score turns out to be signficantly correlatedwith the likelihood of obtaining a successful classifica-tion.
This can be seen very clearly in Table 3, whichshows the average affinity score for correct class-labels ofdifferent heights above the target word, and for incorrectclass-labels ?
as a rule, correct and informative class-labels have significantly higher affinity scores than incor-rect class-labels.
It follows that the affinity score can beused as an indicator of success, and so filtering out class-labels with poor scores can be used as a technique forimproving accuracy.To test this, we repeated our experiments using 3neighbors and this time only using class-labels with anaffinity score greater than 0.75, the rest being marked?unknown?.
Without filtering, there were 1143 success-ful and 1380 unsuccessful outcomes: with filtering, thesenumbers changed to 660 and 184 respectively.
Filteringdiscarded some 87% of the incorrect labels and kept morethan half of the correct ones, which amounts to at least afourfold improvement in accuracy.
The improvement wasparticularly dramatic for proper nouns, where filtering re-moved 270 out of 283 incorrect results and still retainedhalf of the correct ones.ConclusionsFor common nouns, where WordNet is most reliable,our mapping algorithm performs comparatively well, ac-curately classifying several words and finding some cor-rect information about most others.
The optimum num-ber of neighbors is smaller if we want to try for an exactclassification and larger if we want information that isbroadly reliable.
Part-of-speech information noticeablyimproves the process of both broad and narrow classifi-cation.
For proper names, many classifications are cor-rect, and many which are absent or incorrect accordingto WordNet are in fact correct meanings which shouldbe added to the knowledge base for (at least) the domainin question.
Results for verbs are more difficult to inter-pret: reasons for this might include the shallowness andbreadth of the WordNet verb hierarchy, the suitability ofour WordSpace similarity measure, and many theoreticalissues which should be taken into account for a successfulapproach to the classification of verbs.Filtering using the affinity score from the class-labelling algorithm can be used to dramatically increaseperformance.5 Related work and future directionsThe experiments in this paper describe one combinationof algorithms for lexical acquisition: both the findingof semantic neighbors and the process of class-labellingcould take many alternative forms, and an exhaustiveevaluation of such combinations is far beyond the scopeof this paper.
Various mathematical models and distancemeasures are available for modelling semantic proxim-ity, and more detailed linguistic preprocessing (such aschunking, parsing and morphology) could be used in avariety of ways.
As an initial step, the way the granularityof part-of-speech classification affects our results for lex-ical acquistion will be investigated.
The class-labellingalgorithm could be adapted to use more sensitive mea-sures of distance (Budanitsky and Hirst, 2001), and corre-lations between taxonomic distance and WordSpace sim-ilarity used as a filter.The coverage and accuracy of the initial taxonomy weare hoping to enrich has a great influence on success ratesfor our methods as they stand.
Since these are preciselythe aspects of the taxonomy we are hoping to improve,this raises the question of whether we can use automati-cally obtained hypernyms as well as the hand-built onesto help classification.
This could be tested by randomlyremoving many nodes from WordNet before we begin,and measuring the effect of using automatically derivedclassifications for some of these words (possibly thosewith high confidence scores) to help with the subsequentclassification of others.The use of semantic neighbors and class-labelling forcomputing with meaning go far beyond the experimen-tal set up for lexical acquisition described in this pa-per ?
for example, Resnik (1999) used the idea of amost informative subsuming node (which can be re-garded as a kind of class-label) for disambiguation, asdid Agirre and Rigau (1996) with the conceptual densityalgorithm.
Taking a whole domain as a ?context?, thisapproach to disambiguation can be used for lexical tun-ing.
For example, using the Ohsumed corpus of medicalabstracts, the top few neighbors of operation are amputa-tion, disease, therapy and resection.
Our algorithm givesmedical care, medical aid and therapy as possible class-labels for this set, which successfully picks out the senseof operation which is most important for the medical do-main.The level of detail which is appropriate for definingand grouping terms depends very much on the domain inquestion.
For example, the immediate hypernyms offeredby WordNet for the word trout includefish, foodstuff, salmonid, malacopterygian,teleost fish, food fish, saltwater fishMany of these classifications are inappropriately fine-grained for many circumstances.
To find a degree ofabstraction which is suitable for the way trout is usedin the BNC, we found its semantic neighbors which in-clude herring swordfish turbot salmon tuna.
The highest-scoring class-labels for this set are2.911 saltwater fish2.600 food fish1.580 fish1.400 scombroid, scombroid0.972 teleost fishThe preferred labels are the ones most humans would an-swer if asked what a trout is.
This process can be usedto select the concepts from an ontology which are ap-propriate to a particular domain in a completely unsuper-vised fashion, using only the documents from that do-main whose meanings we wish to describe.DemonstrationInteractive demonstrations of the class-labelling al-gorithm and WordSpace are available on the web athttp://infomap.stanford.edu/classes andhttp://infomap.stanford.edu/webdemo.
Aninterface to WordSpace incorporating the part-of-speechinformation is currently under consideration.AcknowledgementsThis research was supported in part by the ResearchCollaboration between the NTT Communication ScienceLaboratories, Nippon Telegraph and Telephone Corpora-tion and CSLI, Stanford University, and by EC/NSF grantIST-1999-11438 for the MUCHMORE project.ReferencesE.
Agirre and G. Rigau.
1996.
Word sense disambigua-tion using conceptual density.
In Proceedings of COL-ING?96, pages 16?22, Copenhagen, Denmark.Enrique Alfonseca and Suresh Manandhar.
2001.
Im-proving an ontology refinement method with hy-ponymy patterns.
In Third International Conferenceon Language Resources and Evaluation, pages 235?239, Las Palmas, Spain.Ricardo Baeza-Yates and Berthier Ribiero-Neto.
1999.Modern Information Retrieval.
Addison Wesley /ACM press.A.
Budanitsky and G. Hirst.
2001.
Semantic distance inwordnet: An experimental, application-oriented evalu-ation of five measures.
In Workshop on WordNet andOther Lexical Resources, Pittsburgh, PA. NAACL.Christiane Fellbaum, editor.
1998.
WordNet: An elec-tronic lexical database.
MIT press, Cambridge MA.J.
Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in Linguistic Analysis, Philological So-ciety, Oxford, reprinted in Palmer, F. (ed.
1968) Se-lected Papers of J. R. Firth, Longman, Harlow.Marti Hearst and Hinrich Schu?tze.
1993.
Customizinga lexicon to better suit a computational task.
In ACLSIGLEX Workshop, Columbus, Ohio.T.
Landauer and S. Dumais.
1997.
A solution to plato?sproblem: The latent semantic analysis theory of acqui-sition.
Psychological Review, 104(2):211?240.Hang Li and Naoki Abe.
1998.
Generalizing case framesusing a thesaurus and the mdl principle.
Computa-tional Linguistics, 24(2):217?244.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In ACL:1999, pages 317?324.James Pustejovsky.
1995.
The Generative Lexicon.
MITpress, Cambridge, MA.Philip Resnik.
1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
Journal ofartificial intelligence research, 11:93?130.Ellen Riloff and Jessica Shepherd.
1997.
A corpus-basedapproach for building semantic lexicons.
In ClaireCardie and Ralph Weischedel, editors, Proceedings ofthe Second Conference on Empirical Methods in Natu-ral Language Processing, pages 117?124.
Associationfor Computational Linguistics, Somerset, New Jersey.Brian Roark and Eugene Charniak.
1998.
Noun-phraseco-occurence statistics for semi-automatic semanticlexicon construction.
In COLING-ACL, pages 1110?1116.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In 19th In-ternational Conference on Computational Linguistics,pages 1093?1099, Taipei, Taiwan, August.Dominic Widdows, Beate Dorow, and Chiu-Ki Chan.2002.
Using parallel corpora to enrich multilinguallexical resources.
In Third International Conferenceon Language Resources and Evaluation, pages 240?245, Las Palmas, Spain, May.
