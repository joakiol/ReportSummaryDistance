AN INTEGRATED SYNTACTIC AND SEMANTIC SYSTEM FOR NATURAL LANGUAGE UNDERSTANDINGUN SYSTEME SYNTAXIQUE ET SEMANT|QUE INTEGRE POUR LA COMPREHENSION DU LANGAGENATURELFr&l~rique SEGOND(I) et Karen JENSEN(2)(l)Institut National des T616communications (Evry, France); email: segond at vaxu.int-evry.fr(2)Microsoft Corporation (Redmond, Washington State, USA); email: karenje at microsoft.cornRESUMELa strat6gie pr6sent6e ic iest it l'origine d'un systi~mcint6gr6 de Traitement Automatique du Langage Naturel -le syst~me PLUS (Progressive Language UnderstandingSystem), dans lequel les composantes thdoriquestraditionnelles, yntaxe, s6mantique t diseours, sontli~es pour former un tout.
Le syst~me st ~crit dmls uuseul formalisme, PLNLP (Programming Language forNatural Language Processing; Heidorn 1972), qui fonmitune architecture efficace pour unifier les diff6rentescomposantes.
Le syst~me offre une strat6gie 616gantepour la compr6hension du Langage Naturel h largecouverture, tind6pendante dudomaine d'application.
Arheure actuelle, six eomposantes constituent le syst~mePLUS/PLNLP; elles peuvent 8ire rapidemeut d6crites dela fa~on suivante: (1) syntaxe (PEG, la grammairePLNLP de l'anglais), (2) syntaxe affin6e (rattachementdes constituants), (3) d6rivation d'une forme logique(PEGASUS), (4) d6sambiguisation, (5) normalisationdes relations 6mantiques, (6) module du discours aunivean des Ixaragraphes.Les composantes (1) et (3) sont d6jit assez avancdes etont 6t6 test~es darts le cadre de diff6rentes applications.Les composantes (2) et (4) sont en cours de r6alisatioa.Les techniques pour c r~r  le module du discours ont6tablies mais non encore impi6ment6es.
Cet articleconcentre sur les composantes (3) et (5) avec uneattention plus particuli~re pour (5) qui pose lesfondations grammaticales utilis6es par le module dudiscours.
Des descriptions des autres composantespeuvent $tre trouv6es darts la litt6rature (par exempledans Chanod & al.
1991, Jensen & al.
1992 (il paraitre)).La composante (3), PEGASUS, est un passage d6cisif dela syntaxe it la s6mautique - s6mantique 61m~t eutenducomme impliquant, au minimum, la d6finition de cas ouroles th6matiques (i.e.
structure pr6dicat-argument).
Lameilleure il lustration en est la diff6rence derepr6sentations en entrde et en sortie.
L'entr6e est unarbre syutaxique; la sortie est un graphe 6tiquet6 etorient6.
On arbre est en premier lieu une repr6sentationsyntaxique dans laquelle l'ordre liu6aire et la dominancegrammaticale sont porteurs d'informations.
Un grapheest une repr6seutation s6mantique; l'ordre tin6aire n'estplus significatif ~tant donn6 que l'inforumtion apparaitd6sormais clans les ~tiquettes des arcs du graphe ou dansses attributs.
Afin de d6river la forme Iogique,PEGASUS doit traiter h large 6chelle des ph6nom~uesd'affectation d'arguments y compris darts des cas difficilescomme les d6peudances non born~es (par exempleassocier le bon objet an verbe "ate" darts la phrase "Whatdid Mary say that Johu ate?
"), le contr61e fonctionnel(par exemple, trouver les sujets et les objets dans le casdes infinitives), le,s relations actif/~assif (s'assurer que lesformes actives et passives ont bien les m~mes argmnentssous-jacents), etc...
Le programme doit 6galement faireapparaitre des relations entre les tStes de syntagmes etleurs modifieurs ou adjoints.
11 doit en plus prendre nconsid6ration les anaphores o anaphores nominalescomprenant les pronoms et les r6f6rents de GN d6finis,anaphores verbales : associer les bons arguments et lesbolls constitaants en cas d'ellipse-.
Toute la chained'entr6e &)it ~tre correctement 6valu6e.
Au stade actuel deson d6veloppement, PEGASUS ne prend pas en compteles r6f6reuces d6finies de GN ni la quantification, maistraite tousles autres ph6nom~nes mentionn6s.
L'int6r~tde PEGASUS est de proposer ulm m6thode de calcul desstructures pr6dicat-argumont e  post-traitement, ce qui ledistingue des proc6dures couramarent employ6es clansd'autres syst~mes de TALN.La composante (5) porte sur les relations 6mantiques.Cette composante fair apparaltre l s liens s6mantiquescach6s dans les relations yntaxiques.
Pour cela il vafalloir, entre autres, rassembler les structuress6mantiques paraphrastiques.
Pour ce faire on modifie, ~tl'aide d'une "grammaire de concept" le r6sultat obtenuavec PEGASUS (la structure prgdicat-argument).
Latfiche de la grammaire de concept est de construire unr6seau (bien fond6) dans lequel ies relations 6mantiquessont 6tabfies entre des noeuds de concepts.
Cettegrammaire st un ensemble de procddures 6crites enPLNLP qui accomplissent, sous certaines conWaintes, uncertain hombre d'opdrations sur les graphes.
Les arcs deranMyse sont 6tiquet~s it raide de noms de relations eux-mf~mes d6riv6s de mani~re syst~matique de lacombinaison de la syntaxe t de la s~mantique du texted'entrde.
Les r~gles de cette grammaire sont similaires,pour ce qui est de leur fonne, anx r~gles des composanl~sant6rieures, mais elles op~rent sur diff6rems aspects del'information commune aux structures, analysant lesrelations entre les noeuds dn graphe de la phrase,nonnalisant les structures s6mantiques et les relationslexicales d'uue vari6t6 de domaines yntaxiques, anspour autant perdre acc~s il la structure de surface(contenant les diffdrences yntaxiques).
Cet articlemoutre comment, purlant de la structure pr6dicatargument obtenue n Soltie de PEGASUS, la grammaireproduit des graphes 6mantiques tout en pr6servant lacaracl6ristique du syst~me global : large couverture tind6pendance du donmine.ACRES DE COLING-92.
NANTES, 23-28 Ao~r 1992 8 9 0 PROC.
oi: COL1NG-92, NANTES, AUG. 23-28, 1992I.
Derivation of logical form (PEGASUS)We present PLUS (Progressive languageUnderstanding System), an integrated NLP analysis ystem.In its current state.
PLUS consists of six components,roughly described as: (1) syntax (PEG, the PLNLP EnglishGrammar); (2) corrected syntax (reassignment ofconstituents); (3) derivation of logical form (PEGASUS);(4) sense disambiguation; (5) normalization of semanticrelations; and (6) paragraph (discourse) model.
Thecurrent system architecture is sequential, because thismakes it easier to concentrate on developing techniques forprocessing Irnly unrestricted input.
However, this controlstructure isexpected to become more parallel in the future.The purpose of the third component, PEGASUS, is tosimplify the derivation of a semantic represeattation, orlogical form, for each input sentence or sentence fragment.To do this it computes: (a) the structure of arguments andadjuncts for each clause; Co) NP (pronoun)-anaphora; (c)VP-anaphora (for elided VPs).
Simultaneously it mustmaintain broad coverage (that is, accept and analyzeunrestricted input tex0.
More commonly in NLP systems,the computation of such meaning structures i  consideredimpossible unless aparticular domain is specified.Consider the sentence, "After dinner, Mary gave acake to John."
Figm-e 1 shows the syntactic (tree)representation for that sentence after it has been lZrOcessedby the fn'st two analysis components, and Figure 2 showsthe semantic graph produced by PEGASUS for ate samesentence:.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.DECL I  I .
.
.
.
PP I  l .
.
.
.
.
.
.
PREP I  .
.
.
.
.
"A f te r "\[ I .
.
.
.
.
.
.
NOUN1 * .
.
.
.
"d l  r lne  r ?\[ I .
.
.
.
.
.
.
PUNC1 .
.
.
.
.
, ?I .
.
.
.
NP I  .
.
.
.
.
.
.
.
NOUN2 * - - - "Ma ry  ?I .
.
.
.
VERB1 * .
.
.
.
.
?
gave  ?I .
.
.
.
NP21 .
.
.
.
.
.
DETP I  .
.
.
.
.
.
.
AD J I *  .
.
.
.
"a"I I .
.
.
.
.
.
.
.
NOUN3 * .
.
.
.
cake  ?I .
.
.
.
PP2  l .
.
.
.
.
.
.
PREP2 .
.
.
.
.  "
to"{ I .
.
.
.
.
.
.
NOUN4 * .
.
.
.
.
J ohn"I .
.
.
.
PUNC2 .
.
.
.
.
.
.
.
".
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 1.
Syntactic parse treeo ~DOBJFigure 2.
Semantic graph for tile .sentence in Figure 1A graph is produced by displaying only thoseattributes and values that are defined to be semantic.However, the underlying record structure contains allattributes resulting from the parse.
In this fashion, alllevels and types of information, from morphological tosyntactic to semantic and beyond, are constantly available.This principle of accountability holds throughout ilePLUS/PLNLP system.In a NLP system that uses attribute-value pairs,argument sbuctmes can be produced (a) by def'ming, foreach node, attribute names that correspond to the desiredargument or adjunct ypes, and (b) by assigning values tothose attributes.
It is customary to think of argument nameslike AGENT, PATIENT.
etc.
However.
although theselabels are tantalizingly semantic in nature, there is as yet nouniformly acceptable way of relating sylltaelie structure tothem.
Therefore we avoid such Labels, at least for the timebeing.
We adopt, instead, the notion of "deep" cases orfunctional roles:DSUB: deep subjectDIND: deep indirect objectDOBJ: deep objectDNOM: deep predicate nominativeDCMP: deep object complementAll deep argument attributes arc added to the analysisrecord structure by PEGASUS.
For very simple clauses,deep arguments correspond exactly to the surface syntacticarguments.
For example, in "John ate the cake," the NP"John" fills the roles of both surface and deep subject; "thecake" fills the roles of both surface and deep object In suchsimple cases, the deep argument attributes could as wellhave been assigned by the ,syntax rules; they are assigned byPEGASUS just to simplify the overall system architecture,Each major class node is exanlined, and, if it containsmore than just one single (head) word, each ~sociated wordis evaluated for possible assignment to some deep-structureattribute, lit addition to the deep case Labels, the followingnon-syntactic, sou-argument attributes define the fullyelaborated structure:PRED: predicate (basic term) labelPTCL: particle in two-part verbsOPS: operator, like demonstratives and quantifieesNADJ: adjective modifying anounPAD J: predicate adjectivePROP:.
otherwise unspecified modifier that is a clauseMODS: otherwise unspecified modifier that is not aclause; also, members of a coordinated structureAnd in addition to these, attributes are defined to pointto adjunct preposition',d plwases and subordinate clauses.The names of these attributes are actually rite iemmas ofthose prepositions and conjunctions that begin their phrasesand clauses.
In this fashion, a step is taken toward a moresemantic analysis of these constituents, without thenecessity of going all the way to case Labels like "locative"and "durative.
"The procedure slants by renaming the surfacearguments in all cases, as described previously.
Then itcalls a set of sub.procedures, ach one of which is designedto solve a particular piece of the argument puzzle.
Here isan outline of the flow of control taken for tile specificationof arguments and adjuncts:1.
Assign arguments and modifiers to all UP nodes:A.
Assign arganmnts, in this order:.1) Unbounded ependencies, e.g., in "Whatdid Mary say that John ale?"
tile DOBJ of "ate" is"What.
"2) Fanctioual control, e.g., ill "John wantedto eat the cake," the DSUB of"eat" is "John.
"ACRES ul~ COLlNG-92.
NANIa~S, 23-28 AOUI 1992 8 9 l PROC.
OV COLING-92.
NANTES, AU~.
23-28, 19923) Passives, e.g., in "The cake was eaten byJohn," the DSUB is "John" and the DOBJ is "thecake.
"4) Indirect object paraphrases, e.g., thestructure for "Mary gave a surprise to John" must beidentical to the structure for "Mary gave John asurprise.
"5) Indirect object special cases, e.g., in "1told the story," the syntactic object "the story" is theDOBJ; but in "I told the woman," the syntactic object"the woman" is the DIND.6) Extraposition, e.g., "John ate the cake" isthe DSUB of the sentence "It appears that John atethe cake."B.
Assign modifiers (all adjuncts): prepositional,adjectival and adverb phrases; adverbial noun phrases;subordinate clauses; infinitives; comment clauses;participial modifiers; sentential relative clauses; etc.2.
Assign modifiers (including arguments) toall NP nodes.3.
Assign modifiers to all AJP (adjective phrase) nodes.4.
Assign modifiers to all AVP (adverb phrase) nodes.5.
Clean up the attribute-vaiue structure by deleting someunwanted features,The focus of linguistic interest here is on theassignment ofarguments oVP nodes.
Ordering of the sub-procedures i importanL Long-distance~lependencies mustbe resolved before functional control is assigned, and bothof these maneuvers must be performed before passives arehandled.
The ordering presented here was experimentallydetermined by parsing sentences that contain more than oneof the phenomena noted.Subcategorization features on verbs are used morestrictly here than they are used in the fwst component, hebroad-coverage syntactic sketch.
Also, although selectionalfeatures were not found to be useful in constructing thesyntactic sketch, they are buff, useful and necessary fordefining deep arguments in PEGASUS.
With unboundeddependencies, it is important o distinguish the probablesubeategorization types of verbs in the sentence, and alsosome selectional ("semantic") features on nouns, since theargument struetme will vary depending on the interplaybetween these two pieces of information.The sub-procedure for functional conlrol handles notonly infinitive clauses, but also participial clauses, bothpresent and past.
These consauOJons often requireargument assignment over long intervening stretches oftext.
In the sentence "Mary, just as you predicted, arrivedexcitedly waving her hands," "Mary" is DSUB of thepresent participle "excitedly waving her hands."
In thesentence "Bolstered by an outpouring of public confidence,John accepted the post," "John" is DOBJ of the pastparlieiple "Bolstered by an outpouring..."All of the other sub-procedures for argumentassignment are linguistically interesting to various degrees,but none of them is quite so complex as the lmX:edures forunbounded dependency and functional control.2.
Semantic normalizationSemantic relations are represented by a graph.
Thenodes of the graph contain words; but, since these arelinked with dictionary def'mitions, synonyms, and otherrelated words, it is possible to say that these nodes representconcepts, l It is the job of the concept grammar to constructa well-motivated network in which semantic relations areproperly drawn among concept nodes.In order to do this job, one of the important problemsthat has to be addressed is the problem of showingequivalences between paraphrases.
This problem was firstapproached by PEGASUS, where, for example, both activeand passive forms of a clause are provided with the sameargument structure.
The work is continued by the conceptgrammar, and expanded to handle a much wider set ofparaphrase situations.
The basic intuition remains thesame, however:, different sentences that have essentially thesame meaning (truth-value) will have the same semanticgraph.
And the same principle of accountability applieshere as there: the system will always have access to theoriginal surface syntactic variability, so that no nuances ofexpression need ever be IosLAs an example, all of the following sentences have thesame essential meaning, and therefore should be associatedwith the same semantic graph: "Thero is a blue block";"The block is blue"; "The block is a blue block"; "Theblock is a blue one."
These are not classical syntacticvariants, like active and passive; but they are variants of thesame semantic facts: a block exists, and it is blue.The sentences are analyzed by the syntax andPEGASUS.
(Because our descriptive sentences arepurposely kept very simple, we can avoid using the secondand fourth components, reassignment and sensedisambiguation.)
The result is a graph for each sentence,corresponding to the basic arguments and adjuncts of thatsentence.
The concept grammar examines each sentencegraph, checking for certain configurations that signal thepresence of common underlying conceptual categories.Here is where the syntactic variants will be normalized.The operation of the ommept grammar can becompared to the operation of a syntactic grammar:, syntaxtakes words and phrases, and links them, via commonmorpho-syntactic relationships, into a structtwal whole; theconcept grammar takes arguments and adjuncts, and linksthem, via common semantic relationships, into a conceptualwhole.
Syntax works with syntactic ategory labels; theconcept grammar works with semantic arc labels.2.1.
the "block" sentence paraphrasesConsider the four "block" sentences above.
Theargument and adjunct structures (sentential graphs)provided by PEGASUS for these sentences, and shown inFigure 3, use just four semantic arc labels: DSUB, NADJ,PAl)J, and DNOM (see above): 2See Segood and Jensen 1991 for an explanation of theassignment of NP- and VP-anaphora, a discussion ofadvantages to using a post-processor, and a comparison of ISce Sowa 1984 for an introduction to conceptual graphPEGASUS with other current strategies for deriving structures.predicate-argument StlUCtUres.
2Although only the head lemmas are displayed in the graphnodes, the underlying record structure keeps access to allsyntactic details, such as determiners, tense, etc.ACRES DE COLING-92, NANTES, 23-28 AO~" 1992 8 9 2 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992NADJDSUB" - (~"There is a blue block.
'""The block is blue.
""The block is a blue block.
""II~e block is a blue one.
"Figure 3.
Sentential graphs for tho':'bluek" sentencesThese four sentential graphs am quite different; but,since the sentences have the same meaning, there should bejust one semantic graph for all of them:Figure 4.
Canonical semantic graph for the sentencesin Figure 3This is a case of paraphrase that requiresnormalization.
In order to achieve it, first of all we deletethe node "be" in all graphs.
It is well known that theEnglish copula "be" carries very little semantic weighLRULE 1: Delete the copula "'be.
"Second, if an adjective carries a lexical feature thatmarks it as a "color" word, then we change the arc labelNADJ to the label COLOR.
The effect is to change thename of the relation between the noun and the adjective.RULE 2: Change NADJ flora node with "color" wordto COLORTo achieve the desired semantic graph for "There is ablue block," we apply Rule 1 and Rule 2, deleting the node"be" and changing the name of the relation between thenode "block" and the adjective "blue.
"When the predicate is an adjective If'AD0, there is, inthe argument structure, no direct relation between thesubject (DSUB) and the adjective (PADJ).
Both of them areattributes of the node "be."
In this case, we create a newrelation, NADJ, between the subject and the adjective, anddelete the relation PADJ.
(We will deal later with thedifference between predicative (PADJ) and atlributive(NADJ) adjectives.
)RULE 3: Create NADJ arc between subject andpredicate adjective.Once this new arc is created, rules 1 and 2 willrecognize that the adjective is a "color" word, change thename of the relation NADJ to COLOR, and delete the node"be."
These operations will turn the sentential graph for"The block is blue" into the desired semantic graph inFigure 4.When the predicate is a noun or a noun phrase(DNOM), as in the remaining two "block" sentences, wehave to ask if that predicate nominative is the same term asthe subject (or is an equivalent empty anaphorie team, like"one").
or if it is different from the suhjecL and not empty.In the in'st case we unify the subject and the predicate NPs.All the nodes which point to the first are made to point tothe second, and vice versa.
Once this is done, the problemsof the color adjective and of the empty copula areautomatically handled by existing rnles, and the sententialgraphs for the last two "block" sentences are transformedinto the canonical graph in Figure 4.RULE 4: Unify subject and predicate underappropriate conditions.In the second case, when there is a DNOM that isdifferent from the subject NP, we create a new relationbetween the subject and the predicate.
In the simplest ease,we give this relation the ISA label:RULE 5: Create ISA link under appropriateconditions.Hence the sentence "The block is an object" has thefollowing semantic graph:Figure 5.
Semantic graph for "q"he block is an object"The reader should not conclude from the previousexamples that dealing with paraphrases requires a lot of adhoc solutions.
On the contrary, the rules (or procedures) ofthe concept grammar are general in nature.
"lhey identifyand represent typical semantic relations in a formal way.
Asyntactic grammar does the same thing, but at a differentlevel of structure.
The concept grammar tries to catch whatmight be called "the semantics of the syntax."
Theseoperations are straightforward, just us the operations thatbuild constituent structure in a syntactic grammar amstraightforward.
But this simplicity should not obscure theelegance of what is going on here.
With minimal effort,using easily accessible parse information, we amautomating the creation of a conceptual structure.
Thisconceptual structure will ultimately have a high degree ofabstracmess, generality, and language independence.2.2.
Locative prepositional phrasesConsider the following set of sentences, which shouldall have the same semantic graph (Figure 6):(I)There is a blue block on the red block.The blue block is ou the red block.There is a red block under the blue block.The red block is under the blue bloc.k.ACRES DE COLING-92, NANTES, 23-28 AO~' 1992 8 9 3 PROC.
OF COL1NG-92, NANTES, AUO.
23-28, 1992Figure 6.
Canonical graph for sentences in (1)Figure 7.
Sentential graph for "There is a blue block on thered block"Note the graph node labeled "position."
This word wasnever used in the paraphrase ntences, but the concept wasimplicit in all of them.
(The link between prepositionnames and the word/coucept "position" can be validated indictionaries and thesauri.)
One interesting and significantresult of setting out to normalize these paraphrases is theemergence of what might be called the essential meaning ofthe expressions, namely, a statement of the relative positionof two objects.
In this fashion, rite writing of aconceptglmnmar results naturally, and pragmatically, in theemergence of terms that we might want to'consider as"'semantic pdmitive.s."
It should be emphasized, however,that we are not committed beforehand toany basicconceptual or semantic primitives.
In this example, therelations ONTOP and UNDER appear in the canonicalgraph of the sentence, but this is just for purposes of thepresent exposition.
What we are interested in is to establish,ml appropriate link between the two blocks.
Instead ofONTOP and UNDER we could have ABOVE (or ON) andB I tOW,  etc.It is not necessary to discuss the treatment of each ofthe paraphrases.
The In, st sentence in (I} will serve as anexample.
Figure 7, above, shows its sentential graph.What we want to do is to link the deep subject (''blueblock'3 with the object of the preposition ("red block'3 byusing the relation names ONTOP and UNDER, whichspring from the concept POSITION.
We delete the copula"be," and create the new node POSITION, motivated bydictionary def'mitions for locative prepositions.
Then weadd two attributes, ONTOP and UNDER, to  this node(pointing respectively to the subject and the noun phraseobject of the preposition), and delete the attribute ON in thelist of attributes of the subject.
Notice that if the sentenceread "above" instead of "on," the treatment would be thesame.Of course, this does not mean that looking at thesyntactic relations between words is enough; the semanticsof the wolds themselves are also important.
For instance,the kind of relation involved between a subject NP and theNP object of a PP in the case of a locative prepositionalphrase (e.g.
the eat is in the garden, the cat is under thetable), is not the same as the one involved with the PPwhich is a part of the sentence "'The cat is in love."
Butstill, in all these three sentences, what we are interested inis building the relation between "the cat" and the NP objectof the PP (garden, table, love).
Giving a name to therelation (and, for that purpose, knowing that love is aconcept, garden is a place, and table is an object) is the taskof the sense disambiguation component, which consultsdictionary def'mitions to find the necessary semanticinformation.2.3.
Relative clausesOne way of combining propositions (the block is blue,is on the table, etc.)
into one sentence is to use a relativeclause, We can say:(2) (a) The block that is blue is on the table.
(b) On the table is the block that is blue.
(c) The block, which is on the table, is blue.Figure 8 shows the sentential graph for (2a).
Theattribute PROP points to the semantic structure of therelative clause "that is blue," and the attribute REFidentifies the referent of the relative pronoun "that":Figure 8.
Sentential graph for "The block that is blueis on the table" (2a)In the sentences of (2), we want to relate the deepsubjects of the relative clauses with their predicates.
All wehave to do, in this case, is to unify the DSUB of the PROPwith the REF of the DSUB of the PROP, deleting the REFattribute.
The result is a record, pointed to by PROP, whichhas a DSUB identical to the DSUB of the whole sentence,and therefore possesses both the attributes that it gains fromtile relative clause, and the attributes of the DSUB of thewhole sentence.
Now the system is able to handlerecursively all the other problems (copula, predicateadjective, and spatial prepositional relationships), and weobtain the same graph as is obtained for sentences such as'The blue block is on the table" or "There is a blue block onthe table":Figure 9.
Canonical semantic graph for the sentencesin (2)2.4.
Toward the discourse modelOur work also involves normalizing across sentencehoun -daries.
For instance, from (3a--b):(3) (a) The blue block is on the red block.
(b) The red block is on the black block.we want to be able to infer (3o-d):(3) (c) The blue block is above the black block.
(d) The black block is below the blue block.Inference across sentence boundaries does not differ,in essence, li-om inference within a single .sentence; afterACTES DE COLING-92.
NANq~2S, 23-28 AOt'Zr 1992 8 9 4 I'ROC.
OF COLING-92, NANTES, AUG. 23-28, 1992all, two sentences may l~come one sentence, undercoordination:(3) (a AND b) qlte blue block is on the red blockAND the red block ks on the black block.From an implea~entation point of view, the strategy isthe same.
We consider all nodes called "position."
~llaere isone such node in the graph for (3a), and another in thegraph for (3b).
We look at the records for befit "position"nodes and obtain two lists: one, a list of all ONTOPattributes; and the other, a list of all UNDER attributes.
Welook at the intersection of those li~s.
If they have anelement in common (for instance, in the previous example,"red block" will appear in both of them), then we know thatwe can infer the graph in Figure 10:Figure 10.
Inferential graph for (3d)Figure 10 displays only the inferences in (3c-d),derived from (3a-b).
But the system does not lose access toinformation about the existence and placement of the redblock mentioned in (3a-J0).
;"All the examples given in this paper involve sentenceswith the verb "be?'
"Be" and other state verbs comprise acomplicated and interesting class.
"ll~ey accept a lot ofdifferent constructions (adjectival predicates, nominalpredicates, prepositional phr,xsc complements, etc.
), andprovide a convenient and convincing field for preliminaryinvestigations.
At the same time, much of the work donefor state verbs (coordination, PP relationships, etc.)
can beapplied to other verb classes.3.
ConclusionWe hope to have made two substantial contributions inthis paper: (1) to suggest a novel method l~r computingargument structures in a post-processor, in order to simplifythe derivation of logical fonas for sentences; (2) to showthe birth of a concept grammar, which receives yntacticand semantic information from earlier stages of the systtnn,and autoamtically provides a grammatical foundation forthe next stage, discourse.
We dealt with some linguisticproblems, including different kinds of paraphrases.
We alsosuggested methods for handling logical properties of naturedlanguage, such as the spatial properties of prepositions.
(See Sego~ld and Jensen 1991 for additional constructionshandled by the concept grammar.
)Dealing with locative prepositions i not the same asdealing with the whole of natural anguage, llowever, wehave tried to avoid specific or ad hoc solutions.
The rules ofthe concept grammar are generic in nature.
"lhey expresssemantic facts about English (and, in some cases, aboutlanguage in general), just as a moqtho-syntactic grammarexpresses syntactic facts about English.
Thereh~re they arein no way restricted to a semantic subdomain.This structure of very general relations is one of thesteps leading to an ideal semautic representation ofsentences.
It provides a universal representation,independent front the surface structure but without losingthe information contained in the surface structure.Another contribution of the paper is to illustrate howthis approach leads to an anticulated architecture for auatural anguage anderstanding system.
The architectureprovides both modularity and integration of NLP tasks, andallows for a smooth flow from syntax through semantics todiscourse.
Starting with an initial syntactic sketch, weobtain a conceptual graph step by step, without adding a lotof hand-coded semantic infommtion in the dictionary.AcknowledgmentsWe are grateful to all the people who have helped us.Among these, we ackatowledge here especially thefollowing: George Heidom, who provided us with tools andadvice; Joel Fagan, who initialized rite concept grammarwork (see Fagan 1990); and Wlodek Zadrozny, with whomwe have had lively and interesting conversations aboutsenmntics.
Of course, any errors in this work remain ourre.sponsibility.ReferencesChanod, J.-P., B. ltarriehausen, and S. Montemagni.1991.
"Post-processing multi-lingual rguments ..m'uclures"in Proceedings ofthe l l th International Workshop onExpert Systems and Theh" Applications, Avignon, France.Fagan, Ji.. 1990.
"'Natural Language Text: TheIdeal Knowledge Representation Formalism to SupportContent/MialysLs for Text Retrievar' in P.S.
Jacobs, ed.,Text-based Intelligent System~': Cun'ent Research in TextAnalysis, Information Extraction and Retrieval.
GEResearch and Development Center, Technical InformationSeries, 90CRD198, pl t. 48-52.
(Originally presented atAAAI 1990 Spring Symposimn on Text-based InteUigentSystems, March 27, 28 & 29, 1990, Stanford University,Stanford, California, USA).Heidoru, G.E.
1972.
"'Natural Language Inputs to aSimulation Progvanlming System."
PhD dissertation, YaleUniversity, New ltaven, Connecticut, USA.Jensen, K., G.E.
lteidorn and S.D.
Richardson.
1992,forthcoming.
Natural Language Processing: the PLNLPApproach.
Kluwer Academic Publishers, Boston,Massachusetts, USA.Segond, F. and Jonson, K. 1991.
"An IntegratedSyntactic and Semantic System for Natural LanguageUnderstanding."
IBM RC 16914, T.J. Watson ResearchCenter, Yorktown Heights, New York, LISA.Sowa, J.F.
1984.
Conceptual Structures: InformationProcessing in Mindand Machine.
Addison-Wesley Pub.Co., Reading, M,x~sachusetts, USA.AcrF.s DI::COLING-92, NAN-rEs, 23-28 nO~' 1992 8 9 5 I'P.OC.
OF COLING-92, N^NTI.
:S, AUG. 23-28, 1992
