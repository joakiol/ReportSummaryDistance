Maytag: A multi-staged approach to identifyingcomplex events in textual dataConrad Chang, Lisa Ferro, John Gibson, Janet Hitzeman, Suzi Lubar, Justin Palmer,Sean Munson, Marc Vilain, and Benjamin WellnerThe MITRE Corporation202 Burlington Rd.Bedford, MA 01730 USAcontact: mbv@mitre.org (Vilain)AbstractWe present a novel application of NLPand text mining to the analysis of finan-cial documents.
In particular, we de-scribe an implemented prototype, May-tag, which combines information extrac-tion and subject classification tools in aninteractive exploratory framework.
Wepresent experimental results on their per-formance, as tailored to the financial do-main, and some forward-looking exten-sions to the approach that enables usersto specify classifications on the fly.1 IntroductionOur goal is to support the discovery of complexevents in text.
By complex events, we meanevents that might be structured out of multipleoccurrences of other events, or that might occurover a span of time.
In financial analysis, thedomain that concerns us here, an example ofwhat we mean is the problem of understandingcorporate acquisition practices.
To gauge acompany?s modus operandi in acquiring othercompanies, it isn?t enough to know just that anacquisition occurred, but it may also be impor-tant to understand the degree to which it wasdebt-leveraged, or whether it was performedthrough reciprocal stock exchanges.In other words, complex events are oftencomposed of multiple facets beyond the basicevent itself.
One of our concerns is therefore toenable end users to access complex eventsthrough a combination of their possible facets.Another key characteristic of rich domainslike financial analysis, is that facts and events aresubject to interpretation in context.
To a finan-cial analyst, it makes a difference whether amulti-million-dollar loss occurs in the context ofrecurring operations (a potentially chronic prob-lem), or in the context of a one-time event, suchas a merger or layoff.
A second concern is thusto enable end users to interpret facts and eventsthrough automated context assessment.The route we have taken towards this end is tomodel the domain of corporate finance throughan interactive suite of language processing tools.Maytag, our prototype, makes the followingnovel contribution.
Rather than trying to modelcomplex events monolithically, we provide arange of multi-purpose information extractionand text classification methods, and allow theend user to combine these interactively.
Thinkof it as Boolean queries where the query termsare not keywords but extracted facts, events, en-tities, and contextual text classifications.2 The Maytag prototypeFigure 1, below, shows the Maytag prototypein action.
In this instance, the user is browsing aparticular document in the collection, the 2003securities filings for 3M Corporation.
The userhas imposed a context of interpretation by select-ing the ?Legal matters?
subject code, whichcauses the browser to only retrieve those portionsof the document that were statistically identifiedas pertaining to law suits.
The user has also se-lected retrieval based on extracted facts, in thiscase monetary expenses greater than $10 million.This in turn causes the browser to further restrictretrieval to those portions of the document thatcontain the appropriate linguistic expressions,e.g., ?$73 million pre-tax charge.
?As the figure shows, the granularity of theseoperations in our browser is that of the para-graph, which strikes a reasonable compromisebetween providing enough context to interpretretrieval results, but not too much.
It is also ef-131fective at enabling combination of query terms.Whereas the original document contains 5161paragraphs, the number of these that were taggedwith the ?Legal matters?
code is 27, or .5 percentof the overall document.
Likewise, the query forexpenses greater than $10 million restricts thereturn set to 26 paragraphs (.5 percent).
Theconjunction of both queries yields a commonintersection of only 4 paragraphs, thus preciselytargeting .07 percent of the overall document.Under the hood, Maytag consists of both anon-line component and an off-line one.
The on-line part is a web-based GUI that is connected toa relational database via CGI scripts (html,JavaScript, and Python).
The off-line part of thesystem hosts the bulk of the linguistic and statis-tical processing that creates document meta-data:name tagging, relationship extraction, subjectidentification, and the like.
These processes areapplied to documents entering the text collection,and the results are stored as meta-data tables.The tables link the results of the off-line process-ing to the paragraphs in which they were found,thereby supporting the kind of extraction- andclassification-based retrieval shown in Figure 1.3 Extraction in MaytagAs is common practice, Maytag approachesextraction in stages.
We begin with atomicnamed entities, and then detect structuredentities, relationships, and events.
To do so, werely on both rule-based and statistical means.3.1 Named entitiesIn Maytag, we currently extract named entitieswith a tried-but-true rule-based tagger based onthe legacy Alembic system (Vilain, 1999).
Al-though we?ve also developed more modern sta-tistical methods (Burger et al 1999, Wellner &Vilain, 2006), we do not currently have adequateamounts of hand-marked financial data to trainthese systems.
We therefore found it more con-venient to adapt the Alembic name tagger bymanual hill climbing.
Because this tagger wasoriginally designed for a similar newswire task,we were able to make the port using relativelysmall amounts of training data.
We relied on two100+ page-long Securities filings (singly anno-tated), one for training, and the other for test, onwhich we achieve an accuracy of F=94.We found several characteristics of our finan-cial data to be especially challenging.
The first isthe widespread presence of company name look-alikes, by which we mean phrases like ?HealthCare Markets?
or ?Business Services?
that maylook like company names, but in fact denotebusiness segments or the like.
To circumventthis, we had to explicitly model non-names, ineffect creating a business segment tagger thatcaptures company name look-alikes and preventsthem from being tagged as companies.Another challenging characteristic of these fi-nancial reports is their length, commonly reach-ing hundreds of pages.
This poses a quandaryFigure 1: The Maytag interface132for the way we handle discourse effects.
As withmost name taggers, we keep a ?found names?
listto compensate for the fact that a name may notbe clearly identified throughout the entire span ofthe input text.
This list allows the tagger topropagate a name from clear identifying contextsto non-identified occurrences elsewhere in thediscourse.
In newswire, this strategy boosts re-call at very little cost to precision, but the sheerlength of financial reports creates a dispropor-tionate opportunity for found name lists to intro-duce precision errors, and then propagate them.3.2 Structured entities, relations, and eventsAnother way in which financial writing differsfrom general news stories is the prevalence ofwhat we?ve called structured entities, i.e., name-like entities that have key structural attributes.The most common of these relate to money.
Infinancial writing, one doesn?t simply talk ofmoney: one talks of a loss, gain or expense, ofthe business purpose associated therewith, and ofthe time period in which it is incurred.
Consider:Worldwide expenses for environmentalcompliance [were] $163 million in 2003.To capture such cases as this, we?ve defined arepertoire of structured entities.
Fine-graineddistinctions about money are encoded as color ofmoney entities, with such attributes as their color(in this case, an operating expense), time stamp,and so forth.
We also have structured entities forexpressions of stock shares, assets, and debt.Finally, we?ve included a number of constructsthat are more properly understood as relations(job title) or events (acquisitions).3.3 Statistical trainingBecause we had no existing methods to addressfinancial events or relations, we took this oppor-tunity to develop a trainable approach.
Recentwork has begun to address relation and eventextraction through trainable means, chiefly SVMclassification (Zelenko et al 2003, Zhou et al2005).
The approach we?ve used here is classi-fier-based as well, but relies on maximum en-tropy modeling instead.Most trainable approaches to event extractionare entity-anchored: given a pair of relevant enti-ties (e.g., a pair of companies), the object of theendeavor is to identify the relation that holds be-tween them (e.g., acquisition or subsidiary).
Weturn this around: starting with the head of therelation, we try to find the entities that fill itsconstituent roles.
This is, unavoidably, astrongly lexicalized approach.
To detect anevent such as a merger or acquisition, we startfrom indicative head words, e.g., ?acquire,??purchases,?
?acquisition,?
and the like.The process proceeds in two stages.
Oncewe?ve scanned a text to find instances of our in-dicator heads, we classify the heads to determinewhether their embedding sentence represents avalid instance of the target concept.
In the caseof acquisitions, this filtering stage eliminatessuch non-acquisitions as the use of the word?purchases?
in ?the company purchases raw ma-terials.?
If a head passes this filter, we find thefillers of its constituent roles through a secondclassification stageThe role stage uses a shallow parser to chunkthe sentence, and considers the nominal chunksand named entities as candidate role fillers.
Foracquisition events, for example, these roles in-clude the object of the acquisition, the buyingagent, the bought assets, the date of acquisition,and so forth (a total of six roles).
E.g.In the fourth quarter of 2000 (WHEN), 3M[AGENT] also acquired the multi-layer inte-grated circuit packaging line [ASSETS] ofW.L.
Gore and Associates [OBJECT].The maximum entropy role classifier relies ona range of feature types: the semantic type of thephrase (for named entities), the phrase vocabu-lary, the distance to the target head, and localcontext (words and phrases).Our initial evaluation of this approach hasgiven us encouraging first results.
Based on ahand-annotated corpus of acquisition events,we?ve measured filtering performance at F=79,and role assignment at F=84 for the critical caseof the object role.
A more recent round of ex-periments has produced considerably higher per-formance, which we will report on later this year.4 Subject ClassificationFinancial events with similar descriptions canmean different things depending on where theseevents appear in a document or in what contextthey appear.
We attempt to extract this importantcontextual information using text classificationmethods.
We also use text classification methodsto help users to more quickly focus on an areawhere interesting transactions exist in an interac-tive environment.
Specifically, we classify eachparagraph in our document collection into one ofseveral interested financial areas.
Examples in-clude: Accounting Rule Change, Acquisitionsand Mergers, Debt, Derivatives, Legal, etc.1334.1 ExperimentsIn our experiments, we picked 3 corporate an-nual reports as the training and test document set.Paragraphs from these 3 documents, which arefrom 50 to 150 pages long, were annotated withthe types of financial transactions they are mostrelated to.
Paragraphs that did not fall into acategory of interest were classified as ?other?.The annotated paragraphs were divided into ran-dom 4x4 test/training splits for this test.
The?other?
category, due to its size, was sub-sampled to the size of the next-largest category.As in the work of Nigam et al(2002) or Lodhiet al(2002), we performed a series of experi-ments using maximum entropy and support vec-tor machines.
Besides including the words thatappeared in the paragraphs as features, we alsoexperimented with adding named entity expres-sions (money, date, location, and organization),removal of stop words, and stemming.
In gen-eral, each of these variations resulted in little dif-ference compared with the baseline features con-sisting of only the words in the paragraphs.Overall results ranged from F-measures of 70-75for more frequent categories down to above 30-40 for categories appearing less frequently.4.2 Online LearningWe have embedded our text classificationmethod into an online learning framework thatallows users to select text segments, specifycategories for those segments and subsequentlyreceive automatically classified paragraphs simi-lar to those already identified.
The highest con-fidence paragraphs, as determined by the classi-fier, are presented to the user for verification andpossible re-classification.Figure 1, at the start of this paper, shows theway this is implemented in the Maytag interface.Checkboxes labeled pos and neg are providednext to each displayed paragraph: by selectingone or the other of these checkboxes, users indi-cate whether the paragraph is to be treated as apositive or a negative example of the categorythey are elaborating.
In our preliminary studies,we were able to achieve the peak performance(the highest F1 score) within the first 20 trainingexamples using 4 different categories.5 Discussion and future workThe ability to combine a range of analyticprocessing tools, and the ability to explore theirresults interactively are the backbone of our ap-proach.
In this paper, we?ve covered the frame-work of our Maytag prototype, and have lookedunder its hood at our extraction and classificationmethods, especially as they apply to financialtexts.
Much new work is in the offing.Many experiments are in progress now to as-sess performance on other text types (financialnews), and to pin down performance on a widerrange of events, relations, and structured entities.Another question we would like to address ishow best to manage the interaction between clas-sification and extraction: a mutual feedbackprocess may well exist here.We are also concerned with supporting finan-cial analysis across multiple documents.
Thishas implications in the area of cross-documentcoreference, and is also leading us to investigatevisual ways to define queries that go beyond theparagraph and span many texts over many years.Finally, we are hoping to conduct user studiesto validate our fundamental assumption.
Indeed,this work presupposes that interactive applicationof multi-purpose classification and extractiontechniques can model complex events as well asmonolithic extraction tools ?
laMUC.AcknowledgementsThis research was performed under a MITRECorporation sponsored research project.ReferencesZhou, G., Su J., Zhang, J., and Zhang, M. 2005.
Ex-ploring various knowledge in relation extraction.Proc.
of the 43rd ACL Conf, Ann Arbor, MI.Nigam, K., Lafferty, J., and McCallum, A.
1999.
Us-ing maximum entropy for text classification.
Proc.of IJCAI ?99 Workshop on Information Filtering.Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini,and N., Watkins, C. 2002.
Text classification usingstring kernels.
Journal of Machine Learning Re-search, Vol.
2, pp.
419-444.Vilain, M. and Day, D. 1996.
Finite-state Phrase Pars-ing by Rule Sequences, Proc.
of COLING-96.Vilain, M. 1999.
Inferential information extraction.In Pazienza, M.T.
& Basili, R., Information Ex-traction.
Springer Verlag.Wellner, B., and Vilain, M. (2006) Leveraging ma-chine readable dictionaries in discriminative se-quence models.
Proc.
of LREC 2006 (to appear).Zelenko D., Aone C. and Richardella.
2003.
Kernelmethods for relation extraction.
Journal of Ma-chine Learning Research.
pp1083-1106.134
