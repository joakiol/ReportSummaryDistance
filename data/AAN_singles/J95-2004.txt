Deterministic Part-of-Speech Taggingwith Finite-State TransducersEmmanuel Roche*MERLYves Schabes*MERLStochastic approaches to natural language processing have often been preferred to rule-basedapproaches because of their robustness and their automatic training capabilities.
This was thecase for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging canbe achieved with a rule-based tagger by inferring rules from a training corpus.
However, currentimplementations of the rule-based tagger run more slowly than previous approaches.
In thispaper, we present afinite-state tagger, inspired by the rule-based tagger, that operates in optimaltime in the sense that the time to assign tags to a sentence corresponds to the time required tofollow a single path in a deterministic finite-state machine.
This result is achieved by encodingthe application of the rules found in the tagger as a nondeterministic finite-state transducer andthen turning it into a deterministic transducer.
The resulting deterministic transducer yields apart-of-speech tagger whose speed is dominated by the access time of mass storage devices.
Wethen generalize the techniques to the class of transformation-based systems.1.
IntroductionFinite-state devices have important applications to many areas of computer science, in-cluding pattern matching, databases, and compiler technology.
Although their linguis-tic adequacy to natural language processing has been questioned in the past (Chomsky,1964), there has recently been a dramatic renewal of interest in the application of finite-state devices to several aspects of natural language processing.
This renewal of interestis due to the speed and compactness of finite-state r presentations.
This efficiency isex-plained by two properties: finite-state devices can be made deterministic, and they canbe turned into a minimal form.
Such representations have been successfully applied todifferent aspects of natural anguage processing, such as morphological nalysis andgeneration (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay1994) and speech recognition (Pereira, Riley, and Sproat 1994).
Although finite-statemachines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993;Silberztein 1993), none of these approaches has the same flexibility as stochastic tech-niques.
Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec1992; Cutting et al 1992; Merialdo 1990; DeRose 1988; Weischedel et al 1993), up tonow the knowledge found in finite-state taggers has been handcrafted and was notautomatically acquired.Recently, Brill (1992) described a rule-based tagger that performs as well as taggersbased upon probabilistic models and overcomes the limitations common in rule-basedapproaches to language processing: it is robust and the rules are automatically ac-* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139.
E-mail:rocbe/schabes@merl.com.
(~) 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 2quired.
In addition, the tagger requires drastically less space than stochastic taggers.However, current implementations of Brill's tagger are considerably slower than theones based on probabilistic models since it may require RKn elementary steps to tagan input of n words with R rules requiring at most K tokens of context.Although the speed of current part-of-speech taggers is acceptable for interac-tive systems where a sentence at a time is being processed, it is not adequate forapplications where large bodies of text need to be tagged, such as in information re-trieval, indexing applications, and grammar-checking systems.
Furthermore, the spacerequired for part-of-speech taggers is also an issue in commercial personal computerapplications uch as grammar-checking systems.
In addition, part-of-speech taggersare often being coupled with a syntactic analysis module.
Usually these two modulesare written in different frameworks, making it very difficult to integrate interactionsbetween the two modules.In this paper, we design a tagger that requires n steps to tag a sentence of lengthn, independently of the number of rules and the length of the context hey require.The tagger is represented by a finite-state transducer, a framework that can also bethe basis for syntactic analysis.
This finite-state tagger will also be found useful whencombined with other language components, since it can be naturally extended bycomposing it with finite-state transducers that could encode other aspects of naturallanguage syntax.Relying on algorithms and formal characterizations described in later sections, weexplain how each rule in Brill's tagger can be viewed as a nondeterministic f nite-statetransducer.
We also show how the application of all rules in Brill's tagger is achievedby composing each of these nondeterministic transducers and why nondeterminismarises in this transducer.
We then prove the correctness of the general algorithm fordeterminizing (whenever possible) finite-state transducers, and we successfully applythis algorithm to the previously obtained nondeterministic transducer.
The resultingdeterministic transducer yields a part-of-speech tagger that operates in optimal timein the sense that the time to assign tags to a sentence corresponds to the time requiredto follow a single path in this deterministic finite-state machine.
We also show howthe lexicon used by the tagger can be optimally encoded using a finite-state machine.The techniques used for the construction of the finite-state tagger are then for-malized and mathematically proven correct.
We introduce a proof of soundness andcompleteness with a worst-case complexity analysis for the algorithm for determiniz-ing finite-state transducers.We conclude by proving that the method can be applied to the class of transformation-based error-driven systems.2.
Overview of Brill's TaggerBrill's tagger is comprised of three parts, each of which is inferred from a training cor-pus: a lexical tagger, an unknown word tagger, and a contextual tagger.
For purposesof exposition, we will postpone the discussion of the unknown word tagger and focusmainly on the contextual rule tagger, which is the core of the tagger.The lexical tagger initially tags each word with its most likely tag, estimated byexamining a large tagged corpus, without regard to context.
For example, assumingthat vbn is the most likely tag for the word "killed" and vbd for "shot," the lexicaltagger might assign the following part-of-speech tags: 11 The notation for part-of-speech tags is adapted from the one used in the Brown Corpus (Francis and228Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech TaggingFigure 1Sample rules.1.
vbn vbd PREVTAG np2.
vbd vbn NEXTTAG by(1)(2)(3)Chapman/np killed/vbn John/np Lennon/npJohn/np Lennon/np was/bedz shot/vbd by~by Chapman/npHe/pps witnessed/vbd Lennon/np killed/vbn by~by Chapman/npSince the lexical tagger does not use any contextual information, many words canbe tagged incorrectly.
For example, in (1), the word "killed" is erroneously tagged asa verb in past participle form, and in (2), "shot" is incorrectly tagged as a verb in pasttense.Given the initial tagging obtained by the lexical tagger, the contextual tagger ap-plies a sequence of rules in order and attempts to remedy the errors made by the initialtagging.
For example, the rules in Figure 1 might be found in a contextual tagger.The first rule says to change tag vbn to vbd if the previous tag is np.
The secondrule says to change vbd to tag vbn if the next tag is by.
Once the first rule is applied,the tag for "killed" in (1) and (3) is changed from vbn to vbd and the following taggedsentences are obtained:(4)(5)(6)Chapman/np killed/vbd John/np Lennon/npJohn/np Lennon/np was/bedz shot/vbd by~by Chapman/npHe/pps witnessed/vbd Lennon/np killed/vbd by~by Chapman/npAnd once the second rule is applied, the tag for "shot" in (5) is changed from vbdto vbn, resulting in (8), and the tag for "killed" in (6) is changed back from vbd to vbn,resulting in (9):(7)(8)(9)Chapman/np killed/vbd John/np Lennon/npJohn/np Lennon/np was~be& shot/vbn by~by Chapman/npHe/pps witnessed/vbd Lennon/np killed/vbn by~by Chapman/npIt is relevant o our following discussion to note that the application of the NEXT-TAG rule must look ahead one token in the sentence before it can be applied, and thatthe application of two rules may perform a series of operations resulting in no netchange.
As we will see in the next section, these two aspects are the source of localnondeterminism in Brill's tagger.The sequence of contextual rules is automatically inferred from a training corpus.A list of tagging errors (with their counts) is compiled by comparing the output ofthe lexical tagger to the correct part-of-speech assignment.
Then, for each error, it isdetermined which instantiation of a set of rule templates results in the greatest errorreduction.
Then the set of new errors caused by applying the rule is computed andthe process is repeated until the error reduction drops below a given threshold.Ku~era 1982): pps stands for singular nominative pronoun in third person, vbd for verb in past ense, npfor proper noun, vbn for verb in past participle form, by for the word "by," at for determiner, nnforsingular noun, and bedz for the word "was.
"229Computational Linguistics Volume 21, Number 2A B PREVTAG CA B PREVIOR2OR3TAG CA B PREVIOR2TAG CA B NEXTIOR2TAG CA B NEXTTAG CA B SURROUNDTAG C DA B NEXTBIGRAM C DA B PREVBIGRAM C Dchange A to B if previous tag is Cchange A to B if previous one or two or three tag is Cchange A to B if previous one or two tag is Cchange A to B if next one or two tag is Cchange A to B if next tag is Cchange A to B if surrounding tags are C and Dchange A to B if next bigram tag is C Dchange A to B if previous bigram tag is C DFigure 2Contextual rule templates.iii iilD \] C \[C IA \[IclclAIICIDIClClAI I C ID lii iiI ii iiiilIClCIAI IClClAI(1) (2)Figure 3Partial matches of A B PREVBIGRAM C C on the input C D C C A.
(3)Using the set of contextual rule templates hown in Figure 2, after training onthe Brown Corpus, 280 contextual rules are obtained.
The resulting rule-based taggerperforms as well as state-of-the-art taggers based upon probabilistic models.
It alsoovercomes the limitations common in rule-based approaches to language processing:it is robust, and the rules are automatically acquired.
In addition, the tagger equiresdrastically less space than stochastic taggers.
However, as we will see in the nextsection, Brill's tagger is inherently slow.3.
Complexity of Brill's TaggerOnce the lexical assignment is performed, in Brill's algorithm, each contextual ruleacquired during the training phase is applied to each sentence to be tagged.
For eachindividual rule, the algorithm scans the input from left to right while attempting tomatch the rule.This simple algorithm is computationally inefficient for two reasons.
The first rea-son for inefficiency is the fact that an individual rule is compared at each token of theinput, regardless of the fact that some of the current okens may have been previouslyexamined when matching the same rule at a previous position.
The algorithm treatseach rule as a template of tags and slides it along the input, one word at a time.Consider, for example, the rule A B PREVBIGRAM C C that changes tag A to tag B ifthe previous two tags are C.When applied to the input CDCCA, the pattern CCA is compared three times tothe input, as shown in Figure 3.
At each step no record of previous partial matchesor mismatches i remembered.
In this example, C is compared with the second inputtoken D during the first and second steps, and therefore, the second step could havebeen skipped by remembering the comparisons from the first step.
This method issimilar to a naive pattern-matching algorithm.The second reason for inefficiency is the potential interaction between rules.
Forexample, when the rules in Figure 1 are applied to sentence (3), the first rule results230Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Taggingin a change (6) that is undone by the second rule as shown in (9).
The algorithm maytherefore perform unnecessary computation.In summary, Brill's algorithm for implementing the contextual tagger may requireRKn elementary steps to tag an input of n words with R contextual rules requiring atmost K tokens of context.4.
Construction of the Finite-State TaggerWe show how the function represented by each contextual rule can be representedas a nondeterministic f nite-state transducer and how the sequential application ofeach contextual rule also corresponds to a nondeterministic f nite-state transducerbeing the result of the composition of each individual transducer.
We will then turnthe nondeterministic ransducer into a deterministic transducer.
The resulting part-of-speech tagger operates in linear time independent of the number of rules and thelength of the context.
The new tagger operates in optimal time in the sense that thetime to assign tags to a sentence corresponds to the time required to follow a singlepath in the resulting deterministic finite-state machine.Our work relies on two central notions: the notion of a finite-state transducer andthe notion of a subsequential transducer.
Informally speaking, a finite-state transduceris a finite-state automaton whose transitions are labeled by pairs of symbols.
The firstsymbol is the input and the second is the output.
Applying a finite-state transducer toan input consists of following a path according to the input symbols while storing theoutput symbols, the result being the sequence of output symbols tored.
Section 8.1formally defines the notion of transducer.Finite-state transducers can be composed, intersected, merged with the union op-eration and sometimes determinized.
Basically, one can manipulate finite-state trans-ducers as easily as finite-state automata.
However, whereas every finite-state automa-ton is equivalent to some deterministic finite-state automaton, there are finite-statetransducers that are not equivalent to any deterministic finite-state transducer.
Trans-ductions that can be computed by some deterministic finite-state transducer are calledsubsequential functions.
We will see that the final step of the compilation of our tag-ger consists of transforming a finite-state transducer into an equivalent subsequentialtransducer.We will use the following notation when pictorially describing a finite-state trans-ducer: final states are depicted with two concentric ircles; e represents the emptystring; on a transition from state i to state j, a/b indicates a transition on input symbola and output symbol(s) b; a a question mark (?)
on an input transition (for examplelabeled ?/b) originating at state i stands for any input symbol that does not appear asinput symbol on any other outgoing arc from i.
In this document, each depicted finite-state transducer will be assumed to have a single initial state, namely the leftmoststate (usually labeled 0).We are now ready to construct he tagger.
Given a set of rules, the tagger isconstructed in four steps.The first step consists of turning each contextual rule found in Brill's tagger into afinite-state transducer.
Following the example discussed in Section 2, the functionalityof the rule vbn vbd PREVTAG np is represented by the transducer shown on the left ofFigure 4.2 When multiple output symbols are emitted, a comma symbolizes the concatenation f the outputsymbols.231Computational Linguistics Volume 21, Number 2np/np vbn/vbd?/?
(.~p/npFigure 4Left: Transducer T1 representing the contextual rule vbn vbd PREVTAG np.
Right: Localextension LocExt(T1) of T1.bnFigure 5Left: Transducer T2 representing vbd vbn NEXTTAG by.
Right: Local extension LocExt(T2) of T2.Each contextual rule is defined locally; that is, the transformation it describes mustbe applied at each position of the input sequence.
For instance, the ruleA B PREVIOR2TAG C,which changes A into B if the previous tag or the one before is C, must be appliedtwice on C A A (resulting in the output C B B).
As we have seen in the previous ection,this method is not efficient.The second step consists of turning the transducers produced by the preceding stepinto transducers that operate globally on the input in one pass.
This transformationis performed for each transducer associated with each rule.
Given a function fl thattransforms, say, a into b (i.e.
fl(a) = b), we want to extend it to a function f2 suchthat f2(w) = w / where w' is the word built from the word w where each occurrenceof a has been replaced by b.
We say that f2 is the local extension 3 of fl, and we writef2 = LocExt(fl).
Section 8.2 formally defines this notion and gives an algorithm forcomputing the local extension.Referring to the example of Section 2, the local extension of the transducer for therule vbn vbd PREVTAG np is shown to the right of Figure 4.
Similarly, the transducer forthe contextual rule vbd vbn NEXTTAG by and its local extension are shown in Figure 5.The transducers obtained in the previous step still need to be applied one afterthe other.3 This notion was introduced by Roche (1993).232Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Taggingvbd/vbn~ ~ ~ 4Figure 6Composition T3 = LocExt(T1) oLocExt(T2).a:aFigure 7Example of a transducer not equivalent to any subsequential transducer.The third step combines all transducers into one single transducer.
This corre-sponds to the formal operation of composition defined on transducers.
The formaliza-tion of this notion and an algorithm for computing the composed transducer are wellknown and are described originally by Elgot and Mezei (1965).Returning to our running example of Section 2, the transducer obtained by com-posing the local extension of T2 (right in Figure 5) with the local extension of T1 (rightin Figure 4) is shown in Figure 6.The fourth and final step consists of transforming the finite-state transducer ob-tained in the previous tep into an equivalent subsequential (deterministic) transducer.The transducer obtained in the previous tep may contain some nondeterminism.
Thefourth step tries to turn it into a deterministic machine.
This determinization is not al-ways possible for any given finite-state ransducer.
For example, the transducer shownin Figure 7 is not equivalent to any subsequential transducer.
Intuitively speaking, thistransducer has to look ahead an unbounded istance in order to correctly generatethe output.
This intuition will be formalized in Section 9.2.However, as proven in Section 10, the rules inferred in Brill's tagger can alwaysbe turned into a deterministic machine.
Section 9.1 describes an algorithm for deter-minizing finite-state transducers.
This algorithm will not terminate when applied totransducers epresenting nonsubsequential functions.In our running example, the transducer in Figure 6 has some nondeterministicpaths.
For example, from state 0 on input symbol vbd, two possible emissions arepossible: vbn (from 0 to 2) and vbd (from 0 to 3).
This nondeterminism is due to therule vbd vbn NEXTTAG by, since this rule has to read the second symbol before it canknow which symbol must be emitted.
The deterministic version of the transducer T3 isshown in Figure 8.
Whenever nondeterminism arises in T3, the deterministic machine233Computational Linguistics Volume 21, Number 2Figure 8Subsequential form for T3.
?/vbd,?emits the empty symbol ?, and postpones the emission of the output symbol.
Forexample, from the start state 0, the empty string is emitted on input vbd, while thecurrent state is set to 2.
If the following word is by, the two token string vbn by isemitted (from 2 to 0), otherwise vbd is emitted (depending on the input from 2 to 2 orfrom 2 to 0).Using an appropriate implementation for finite-state transducers ( ee Section 11),the resulting part-of-speech tagger operates in linear time, independently of the num-ber of rules and the length of the context.
The new tagger therefore operates in optimaltime.We have shown how the contextual rules can be implemented very efficiently.
Wenow turn our attention to lexical assignment, he step that precedes the application ofthe contextual transducer.
This step can also be made very efficient.5.
Lexical TaggerThe first step of the tagging process consists of looking up each word in a dictionary.Since the dictionary is the largest part of the tagger in terms of space, a compact rep-resentation is crucial.
Moreover, the lookup process has to be very fast too---otherwisethe improvement in speed of the contextual manipulations would be of little practicalinterest.To achieve high speed for this procedure, the dictionary is represented by a deter-ministic finite-state automaton with both fast access and small storage space.
Supposeone wants to encode the sample dictionary of Figure 9.
The algorithm, as described byRevuz (1991), consists of first building a tree whose branches are labeled by letters andwhose leaves are labeled by a list of tags (such as nn vb), and then minimizing it intoa directed acyclic graph (DAG).
The result of applying this procedure to the sampledictionary of Figure 9 is the DAG of Figure 10.
When a dictionary is represented asa DAG, looking up a word in it consists simply of following one path in the DAG.The complexity of the lookup procedure depends only on the length of the word; inparticular, it is independent of the size of the dictionary.The lexicon used in our system encodes 54, 000 words.
The corresponding DAGtakes 360Kb of space and provides an access time of 12, 000 words per second.
44 The size of the dictionary in plain text (ASCII form) is 742KB.234Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Taggingads nnsbag nn vbbagged vbn vbdbayed vbn vbdbids nnsFigure 9Sample dictionary.a " ~ / d ~,O s ~-~ (nns)_ ~ , /~ 7 .
~ (nn,vb)~-~O----~ ) (vbd,vbn)Figure 10DAG representation f the dictionary of Figure 9.6.
Tagging Unknown WordsThe rule-based system described by Brill (1992) contains a module that operates afterall known words--that is, words listed in the dictionary--have b en tagged with theirmost frequent ag, and before contextual rules are applied.
This module guesses atag for a word according to its suffix (e.g.
a word with an "ing" suffix is likely to bea verb), its prefix (e.g.
a word starting with an uppercase character is likely to be aproper noun), and other relevant properties.This module basically follows the same techniques as the ones used to implementthe lexicon.
Because of the similarity of the methods used, we do not provide furtherdetails about this module.7.
Empirical EvaluationThe tagger we constructed has an accuracy identical s to Brill's tagger and comparableto statistical-based methods.
However, it runs at a much higher speed.
The taggerruns nearly ten times faster than the fastest of the other systems.
Moreover, the finite-state tagger inherits from the rule-based system its compactness compared with astochastic tagger.
In fact, whereas tochastic taggers have to store word-tag, bigram,and trigram probabilities, the rule-based tagger and therefore the finite-state one onlyhave to encode a small number of rules (between 200 and 300).We empirically compared our tagger with Eric Brill's implementation f his tagger,and with our implementation f a trigram tagger adapted from the work of Church(1988) that we previously implemented for another purpose.
We ran the three programson large files and piped their output into a file.
In the times reported, we includedthe time spent reading the input and writing the output.
Figure 11 summarizes theresults.
All taggers were trained on a portion of the Brown corpus.
The experimentswere run on an HP720 with 32MB of memory.
In order to conduct a fair comparison,the dictionary lookup part of the stochastic tagger has also been implemented usingthe techniques described in Section 5.
All three taggers have approximately the same5 Our current implementation is functionally equivalent to the tagger as described by Brill (1992).However, the tagger could be extended to include recent improvements described in more recentpapers (Brill 1994).235Computational Linguistics Volume 21, Number 2Stochastic TaggerSpeed 1,200 w/sSpace 2,158KBRule-Based Tagger500 w/s379KBFinite-State Tagger10,800 w/s815KBFigure 11Overall performance comparison.dictionary lookup unknown wordsSpeed 12,800 w/s  16,600 w/sPercent of the time 85% 6,5%contextual125,100 w/s8.5%Figure 12Speeds of the different parts of the program.precision (95% of the tags are correct).
6 By design, the finite-state tagger producesthe same output as the rule-based tagger.
The rule-based tagger--and the finite-statetagger--do not always produce the exact same tagging as the stochastic tagger (they donot make the same errors); however, no significant difference in performance betweenthe systems was detected.
7Independently, Cutting et aL (1992) quote a performance of 800 words per secondfor their part-of-speech tagger based on hidden Markov models.The space required by the finite-state tagger (815KB) is distributed as follows:363KB for the dictionary, 440KB for the subsequential transducer and 12KB for themodule for unknown words.The speeds of the different parts of our system are shown in Figure 12.
8Our system reaches a performance l vel in speed for which other, very low-levelfactors (such as storage access) may dominate the computation.
At such speeds, thetime spent reading the input file, breaking the file into sentences, breaking the sen-tences into words, and writing the result into a file is no longer negligible.8.
Finite-State TransducersThe methods used in the construction of the finite-state tagger described in the previ-ous sections were described informally.
In the following section, the notion of finite-state transducer and the notion of local extension are defined.
We also provide analgorithm for computing the local extension of a finite-state transducer.
Issues relatedto the determinization of finite-state transducers are discussed in the section followingthis one.8.1 Definition of Finite-State TransducersA finite-state transducer T is a five-tuple (~, Q, i,F, E) where: G is a fn i te alphabet; Q isa finite set of states or vertices; i c Q is the initial state; F C Q is the set of final states;E c Q x (y, u {c}) x ~,* x Q is the set of edges or transitions.6 For evaluation purposes, we randomly selected 90% of the Brown corpus for training purposes and10% for testing.7 An extended iscussion of the precision of the rule-based tagger can be found in Brill (1992).8 In Figure 12, the dictionary lookup includes reading the file, splitting it into sentences, looking up eachword in the dictionary, and writing the final result o a file.
The dictionary lookup and the tagging ofunknown words take roughly the same amount of time, but since the second procedure only applieson unknown words (around 10% in our experiments), the percentage of time it takes is much smaller.236Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging1Figure 13T4: Example of a finite-state ransducer.For instance, Figure 13 is the graphical representation f the transducer:T4 = (Ca, b,c,h,e}, C 0,1, 2,3}, o, {3}, C(0,a, b, 1), (0,a, c, 2), (1, h, h, 3), (2, e, e, 3)}).A finite-state transducer T also defines a function on words in the following way:the extended set of edges F., the transitive closure of E, is defined by the followingrecursive relation:?
i f eEEtheneE/~?
if (q,a,b,q'), (q',a',b',q") E E then (q, aa',bb',q") E E.Then the function f from G* to ~* defined byf(w) = w' iff 3q E F such that (i,w,w',q) E/~ is the function defined by T. One says that T represents f and writes f = ITI.The functions on words that are represented by finite-state transducers are calledrational functions.
If, for some input w, more than one output is allowed (e.g.
f(w) ={Wl, w2 .
.
.
.  })
then f is called a rational transduction.In the example of Figure 13, IT41 is defined by IT4i(ah) = bh and IT4i(ae) = ce.Given a finite-state transducer T = (~, Q, i,F, E), the following additional notionsare useful: its state transition function d that maps Q x (G u {?})
into 2 Q defined byd(q,a) = Cq' E Q I 3w' E G* and (q,a,w',q') E E}; and its emission function ~ that mapsQ x (G u {~}) x Q into 2 ~" defined by 6(q,a,q') = {w' E G* I (q,a,w,',q') E E}.A finite-state transducer could be seen as a finite-state automaton, where eachtransition label is a pair.
In this respect, T4 would be deterministic; however, sincetransducers are generally used to compute a function, a more relevant definitionof determinism consists of saying that both the transition function d and the emis-sion function ~ lead to sets containing at most one element, that is, Id(q,a)I < 1 andI~(q, a, qt)l < 1 (and that these sets are empty for a = ~).
With this notion, if a finite-statetransducer is deterministic, one can apply the function to a given word by determin-istically following a single path in the transducer.
Deterministic transducers are calledsubsequential transducers (Schfitzenberger 1977).
9 Given a deterministic transducer, wecan define the partial functions q?a = q' iff d(q,a) ~ {q~} and q,a = w ~ iff 3q' E Q suchthat q @ a = q~ and 6(q, a, q~) = Cw~}.
This leads to the definition of subsequential trans-ducers: a subsequential transducer T' is a seven-tuple (G, Q,/, F, ?, *, p) where: ~, Q, i, Fare defined as above; ?
is the deterministic state transition function that maps Q xon Q, one writes q?a = q~; * is the deterministic emission function that maps Q x ~ onY,*, one writes q ?
a = w~; and the final emission function p maps F on G*, one writes, (q )  = w.For instance, T4 is not deterministic because d(0,a) = C1,2}, but it is equivalentto T5 represented Figure 14 in the sense that they represent the same function, i.e.9 A sequential transducer is a deterministic transducer for which all states are final.
Sequential transducersare also called generalized sequential machines (Eilenberg 1974).237Computational Linguistics Volume 21, Number 2Figure 14Subsequential transducer T5.h/bh0 a& 1 , / " " - "~ 2b,cFigure 15T6: a finite-state transducer to be extended.a a b c a ba a b c a bb c b ca a b c a bd c aFigure 16Top: Input.
Middle: First factorization.
Bottom: Second factorization.IT4\] =\]Ts\[.
T5 is defined by T5 = ({a,b,c,h,e},(O, 1 2},O,{2},?, , ,p )  where 0?a = 1,0 ,a  = ?, 1 ?h = 2, 1 ,h  = bh, 1@e = 2, 1 ,e  = ce, and p(2) = ~.8.2 Local  Extens ionIn this section, we will see how a function that needs to be applied at all input positionscan be transformed into a global function that needs to be applied once on the input.For instance, consider T6 of Figure 15.
It represents the function f6 = \]T6\[ such thatf6(ab) = bc and f6(bca) = dca.
We want to build the function that, given a word w, eachtime w contains ab (i.e.
ab is a factor of the word) (resp.
bca), this factor is transformedinto its image bc (resp.
dca).
Suppose, for instance, that the input word is w = aabcab, asshown in Figure 16, and that the factors that are in dom(f6) 1?
can be found accordingto two different factorizations: i.e.
w I = a.w2.
c-W211, where w2 -- ab, and wl =aa ?
w3 ?
b, where w3 = bca.
The local extension of f6 will be the transduction that takeseach possible factorization and transforms each factor according to f6, i.e.
f6(w2) =bc and f6(w3) -= dca, and leaves the other parts unchanged; here this leads to twooutputs: abccbc according to the first factorization, and aadcab according to the secondfactorization.The notion of local extension is formalized through the following definition.Definit ionIf f is a rational transduction from G* to G*, the local extension F = LocExt(f) isthe rational transduction from G* on G* defined in the following way: if u =? '
' .  '
F (u )  if E ~* ?
albla2b2 ?
"anbnan+l E G* then v = albla2b 2 ?
"anbnan+l E ai - (G*dom(f) .
~*), bi c dom(f) and b I c f(bi).10 dom(f) denotes the domain of f, that is, the set of words that have at least one output  through f.11 If wi, w2 C ~*,  Wl  - W2 denotes the concatenation of Wl and w 2.
It may  also be written WlW 2,238Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech TaggingLocal Extension ( T' = (G, Q', i', F', E' ) , T = (~., Q, i, F, E ) )1 C'\[0\] = ({i}, identity); q = 0; i' = 0; F' = O; E' = 0; Q' = 0; C'\[1\] = (0, transduction); n = 2;2 do{3 (S, type)= C'\[q\];Q' = Q'u  {q};4 if (type == identity)5 F' = F'U {q};E' = E' u {(q, ?, ?, i')};6 for each w E (~.
U {?})
s.t.
3x E S, d(x,w) # 0 and Vy E S, d(y,w) NF = O7 if 3r E \[0,n - 1\] such that C'\[r\] == ({i} U Ud(x,w),identity)xES8 e=r;9 else10 C'\[e = n + +\] = ({i} U Ud(x,w),identity);xES11 E' = E' U {(q,w,w,e)};12 for each (i, w, w', x) E E13 if 3r E \[0, n - 1\] such that C'\[r\] == ({x}, transduction)14 e=r ;15 else16 C'\[e = n + +\] = ({x}, transduction);17 E' = E' U {(q,w,w',e)};18 for each w E (G U {c}) s.t.
3x E S d(x,w) MF # 0 then E' = E' U {(q,w,w, 1)};19 else if (type == transduction)20 if 3Xl E Q s.t.
S == {Xl}21 if (xi E F) then E' = E' U {(q,~,c,0)};22 for each (xl, w, w', y) E E23 if 3r E \[0, n -- 1\] such that C'\[r\] == ({y}, transduction)24 e = r;25 else26 C'\[e = n + +\] = ({y}, transduction);27 E' = E' U {(q,w,w',e)};28 q++;29 }while(q < n);Figure 17Local extension algorithm.Intuitively, if F = LocExt(f) and w E ~*, each factor of w in dom(f) is t ransformedinto its image by f and the remaining part  of w is left unchanged.
If f is representedby a finite-state transducer T and LocExt(f) is represented by a finite-state transducerT', one writes T' = LocExt(T).It could also be seen that if "YT is the identity function on * - (~* ?
dom(T) ?
~*),then LocExt(T) = "Tr " (T.  "yw)*.
12 Figure 17 gives an algor i thm that computes  the localextension directly.The idea is that an input word  is processed nondeterminist ical ly f rom left to right.Suppose, for instance, that we have the initial t ransducer T7 of Figure 18 and that wewant  to bui ld its local extension, Ts of Figure 19.When the input  is read, if a current input letter cannot be t ransformed at theinitial state of T7 (the letter c for instance), it is left unchanged:  this is expressed bythe looping transition on the initial state 0 of Ts labeled ?/?.13 On the other hand,12 In this last formula, the concatenation ?
stands for the concatenation of the graphs of each function;that is, for the concatenation of the transducers viewed as automata whose labels are of the form a/b.13 As explained before, an input transition labeled by the symbol ?
stands for all transitions labeled witha letter that doesn't  appear as input on any outgoing arc from this state.
A transition labeled ?/?
stands239b,cFigure 18Sample transducer T7.F.dE?/?Computational Linguistics Volume 21, Number 2Figure 19Local extension Ts of TT: T8 = LocExt(T7).if the input symbol, say a, can be processed at the initial state of T7, one doesn'tknow yet whether a will be the beginning of a word that can be transformed (e.g.
ab)or whether it will be followed by a sequence that makes it impossible to apply thetransformation (e.g.
ac).
Hence one has to entertain two possibilities, namely (1) weare processing the input according to T7 and the transitions hould be a/b; or (2) weare within the identity and the transition should be a/a.
This leads to two kind ofstates: the transduction states (marked transduction in the algorithm) and the identitystates (marked identity in the algorithm).
It can be seen in Figure 19 that this leadsto a transducer that has a copy of the initial transducer and an additional part thatprocesses the identity while making sure it could not have been transformed.
In otherwords, the algorithm consists of building a copy of the original transducer and at thesame time the identity function that operates on ~* - ~* ?
dom(T) ?
Y,*.Let us now see how the algorithm of Figure 17 applies step by step to the trans-ducer T7 of Figure 18, producing the transducer T8 of Figure 19.In Figure 17, C'\[0\] = ({i}, identity) of line 1 states that state 0 of the transducer tobe built is of type identity and refers to the initial state i = 0 of T7.
q represents thecurrent state and n the current number of states.
In the loop do{...} while (q < n), onebuilds the transitions of each state one after the other: if the transition points to a statenot already built, a new state is added, thus incrementing n. The program stops whenall states have been inspected and when no additional state is created.
The number ofiterations is bounded by 2 Ilz\]l*2, where \[\[T\[I = \[Q\[ is the number of states of the originaltransducer.
14Line 3 says that the current state within the loop is q and that this statefor all the diagonal pairs a/a s.t.
a is not an input symbol on any outgoing arc from this state.14 In fact, Qr c 2 Qx {transduction,identity}.
Thus,q ~ 2 2\[Q\[.240Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Tagging1?/?Figure 20Local extension T9 of T6:T9 = LocExt(T6).refers to the set of states S and is marked by the type type.
In our example, at thefirst occurrence of this line, S is instantiated to {0} and type = identity.
Line 5 addsthe current identity state to the set of final states and a transition to the initial statefor all letters that do not appear on any outgoing arc from this state.
Lines 6-11 buildthe transitions from and to the identity states, keeping track of where this leads in theoriginal transducer.
For instance, a is a label that verifies the conditions of line 6.
Thusa transition a/a is to be added to the identity state 2, which refers to 1 (because of thetransition a/b of T7) and to i = 0 (because it is possible to start the transduction T7from any identity state).
Line 7 checks that this state doesn't  already exist and adds itif necessary, e = n + + means that the arrival state for this transition, i.e.
d(q, w), will bethe last added state and that the number  of states being built has to be incremented.Line 11 actually builds the transition between 0 and e = 2 labeled a/a.
Lines 12-17describe the fact that it is possible to start a transduction from any identity state.
Herea transition is added to a new state, i.e.
a/b to 3.
The next state to be considered is 2and it is built like state 0, except hat the symbol b should block the current output.
Infact, state 1 means that we already read a with a as output; thus, if one reads b, ab isat the current point, and since ab should be transformed into bc, the current identitytransformation (that is a ~ a) should be blocked: this is expressed by the transition b/bthat leads to state 1 (this state is a "trash" state; that is, it has no outgoing transitionand it is not final).The following state is 3, which is marked as being of type transduction, whichmeans that lines 19-27 should be applied.
This consists simply of copying the transi-tions of the original transducer.
If the original state was final, as for 4 = ({2}, transduction),an ~/~ transition to the initial state is added (to get the behavior of T+).The transducer T9 = LocExt(T6) of Figure 20 gives a more complete (and slightlymore complex) example of this algorithm.241Computational Linguistics Volume 21, Number 29.
DeterminizationThe basic idea behind the determinization algorithm comes from Mehryar Mohri.
isIn this section, after giving a formalization of the algorithm, we introduce a proof ofsoundness and completeness, and we study its worst-case complexity.9.1 Determinization AlgorithmIn the following, for Wl, w 2 E Y~,*, Wl /~ W2 denotes the longest common prefix of wland w2.The finite-state transducers we use in our system have the property that they can bemade deterministic; that is, there exists a subsequential transducer that represents thesame function.
16 If T = (~, Q, i, F, E) is such a finite-state transducer, the subsequentialtransducer T' = (E, Q', i', F', ?, ,, p) defined as follows will be later proved equivalentto T:Q~ c 2 QxE* .
In fact, the determinization of the transducer is related tothe determinization of FSAs in the sense that it also involves a power  setconstruction.
The difference is that one has to keep track of the set ofstates of the original transducer, one might be in and also of the wordswhose emission have been postponed.
For instance, a state{(ql, Wl), (q2,w2)} means that this state corresponds to a path that leadsto q~ and q2 in the original transducer and that the emission of wl (resp.w2) was delayed for ql (resp.
q2).i' = {(i, ~)}.
There is no postponed emission at the initial state.the emission function is defined by:S,a= A A u.6(q,a,q')(q,u)~S q'Ed(q,a)This means that, for a given symbol, the set of possible emissions isobtained by concatenating the postponed emissions with the emission atthe current state.
Since one wants the transition to be deterministic, theactual emission is the longest common prefix of this set.the state transition function is defined by:S?a= U U {(q',(S*a)-l"u'6(q,a,q'))}(q,u)cS q,~d(q,a)Given u, v E E*, u - v denotes the concatenation of u and v andu -1 ?
v -- w, if w is such that u - w -- v, u - I  ?
v = 0 if no such w exists.F '={SEQ' I3 (q ,u )  ESandqCF}if S E F t, p(S) = u s.t.
3q E F, (q, u) C S. We will see in the proof ofcorrectness that p is properly defined.15 Mohri (1994b) also gives a formalization of the algorithm.16 As opposed to automata,  large class of finite-state ransducers do not have any deterministicrepresentation; they cannot be determinized.242Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech TaggingThe determinization algorithm of Figure 21 computes the above subsequentialtransducer.Let us now apply the determinization algorithm of Figure 21 on the finite-statetransducer T4 of Figure 13 and show how it builds the subsequential transducer T10of Figure 22.
Line 1 of the algorithm builds the first state and instantiates it with thepair {(0, e)}.
q and n respectively denote the current state and the number of stateshaving been built so far.
At line 5, one takes all the possible input symbols w; hereonly a is possible, w' of line 6 is the output symbol,w '= e. ( A a(0,a,~')),~'E{1,2}thus w' = a(0,a, 1) A 6(0,a,2) = b A c = e. Line 8 is then computed as follows:s'= U U~ff{0} ~'E{1,2}thus S' = { (1, a (0, a, 1 )) } U { (2, 6 (0, a, 2) } = { (1, b), (2, c) }.
Since no r verifies the conditionon line 9, a new state e is created to which the transition labeled a/w = a/e points andn is incremented.
On line 15, the program goes to the construction of the transitionsof state 1.
On line 5, d and e are then two possible symbols.
The first symbol, h, at line6, is such that w' isw' = A b.
6(1,h,~')) = bh.F/'cd(1,h)={2}Henceforth, the computation of line 8 leads toS'= U U {(q ''(bh)-l"b'h)}={(2"e)}"qE{1} ~'E{2}State 2 labeled {(2, e)} is thus added, and a transition labeled h/bh that points to state2 is also added.
The transition for the input symbol e is computed the same way.The subsequential transducer generated by this algorithm could in turn be min-imized by an~'algorithm described in Mohri (1994a).
However, in our case, the trans-ducer is nearly minimal.9.2 Proof of CorrectnessAlthough it is decidable whether a function is subsequential or not (Choffrut 1977),the determinization algorithm described in the previous section does not terminatewhen run on a nonsubsequential function.Two issues are addressed in this section.
First, the proof of soundness: the fact thatif the algorithm terminates, then the output ransducer is deterministic and representsthe same function.
Second, the proof of completeness: the algorithm terminates in thecase of subsequential functions.Soundness and completeness are a consequence of the main proposition, whichstates that if a transducer T represents a subsequential function f, then the algorithmDeterminizeTransducer d scribed in the previous ection applied on T computes a sub-sequential transducer representing the same function.In order to simplify the proofs, we will only consider transducers that do not havee input transitions, that is E C Q x ~ x ~* x Q, and also without loss of generality,243Computational Linguistics Volume 21, Number 2DeterminizeTransducer(T' = (G, Q', i', F', ?, , ,  p), T = (~I, Q, i, F, E))910111213141516i '=  0;q = 0;n = 1;C'\[0\] = {(0,~)};F' = 0 ;Q '= 0;do {S = C'\[q\];Q' = Q'u  {q};if 3(~, u) ?
S s.t.
~ ?
F then F' = F' U {q} and p(q) = u;foreach w such that 3(~,u) E S and d(~,w)  0 {w,= A A u 61,,w,, ' lG,)es ~'edGw)q*w=w' ;s '= U U(~,u) es 7' edGw)if 3r E \[0,n -- 1\] such that C'\[r\] == S'e=r ;elseC'\[e = n + +\] = S';q@w=e;}q++;}while(q < n);Figure 21Determinization algorithm.h/bhFigure 22Subsequential transducer T10 such that IT10I = IT4I .t ransducers  that are reduced and that are determinist ic in the sense of finite-stateautomata.
17In order  to prove  this proposit ion,  we need to establ ish some pre l iminary  notat ionsand lemmas.First we extend the definit ion of the transit ion funct ion d, the emiss ion funct ion 6,the determinist ic  transit ion funct ion @, and the determinist ic  emiss ion funct ion * onwords  in the classical way.
We then have the fol lowing propert ies:ab) = U a(q',b)6(ql,ab, q2) = U 6(ql, a, q') .
6(q', b, q2){q' Cd(ql,a) \[q2 Cd(q',b ) }q?ab = (q?a)?b17 A transducer defines an automaton whose labels are the pairs "input/output"; this automaton isassumed to be deterministic.244Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Taggingq,ab  = (q ,a ) .
(q?a) ,bFor the following, it is useful to note that if IT I is a function, then 6 is a functiontoo.The following lemma states an invariant hat holds for each state S built withinthe algorithm.
The lemma will later be used for the proof of soundness.Lemma 1Let I = C'\[0\] be the initial state.
At each iteration of the "do" loop in Determinize-Transducer, for each S --- C'\[q\] and for each w E ~* such that I ?
w = S, the followingholds:(i) I ,w= /~ 6(i,w,q)qEd(i,w)(ii) S=I?w={(q ,u)  lqEd( i ,w)andu=( I *w) - l .6 ( i ,w ,q )}Proof(i) and (ii) are obviously true for S = I (since d(i, ~) = i and ~(i, c, i) = c), and wewill show that given some w E ~* if it is true for S = I ?
w, then it is also true for$1 = S @ a = I Q wa for all a E Y..Assuming that (i) and (ii) hold for S and w, then for each a E ~:A ~(i,w,q).
~(q,a,q')qEd(i,w),q' Ed(q,a)= ( I ,w) .
A ')qEd( i,w),q' Ed(q,a )= A(q,u )ES=I?w,q' Ed(q,a)= ( I ,w) .
(S ,a )= I ,w .
( I?w) ,a= I ,waThis proves (i).We now turn to (ii).
Assuming that (i) and (ii) hold for S and w, then for eacha E ~, let $1 = S ?
a; the algorithm (line 8) is such that$1 = ( (q' ,u')  \] 3(q,u)  E S,q'  E d(q,a) and u '= (S ,a )  -1 .
u .
6(q,a,q')  }Let$2 -- {(q',u') I q' E d(i, wa) and u' = (I ?
wa) -1 .
6(i, wa, q')}We show that $1 c $2.
Let (q',u') E $1, then 3(q,u)  E S s.t.
q' E d(q,a) andu' = (S * a ) - l .
u .
6(q, a, q').
Since u = (I ?
w) - I  .
6(i, w, q), then u' = (S * a)- I  .
(I * w)- I  .6( i ,w ,q) .
6(q,a,q');  that is, u' = ( I *wa)  -1 .
6(i, wa, q').
Thus (q',u') E $2.
Hence $1 c $2.We now show that $2 c $1.
Let (q',u') E $2, and let q E d( i ,w)  be s.t.
q' E d(q,a)and u = ( I ,  w) -1 .
6( i ,w,q)  then (q,u) E S and since u' = ( I *  wa) -1 ?
6(i, wa, q') =( s  ,a )  -1 ?
u .
(q ' ,u ' )  E s lThis concludes the proof of (ii).
\[\]245Computational Linguistics Volume 21, Number 2The following lemma states a common property of the state S, which will be usedin the complexity analysis of the algorithm.Lemma 2Each S = C'\[q\] built within the "do" loop is s.t.
Vq E Q, there is at most one pair(q, w) c S with q as first element.ProofSuppose (q, wl) c S and (q, w2) c S, and let w be s.t.
I?w = S. Then Wl = ( I W)  -1  'fi(i, w, q) and w2 = (I ?
w)-I .
6(i, w, q).
Thus W 1 = W 2.
\ [ \ ]The following lemma will also be used for soundness.
It states that the final stateemission function is indeed a function.Lemma 3For each S built in the algorithm, if (q, u), (q', u') c S, then q, q' E F ~ u = u'ProofLet S be one state set built in line 8 of the algorithm.
Suppose (q, u), (q', u') E S and q,q' E F. According to (ii) of lemma 1, u = ( I ,w)  -1 .6(i,w,q) and u' = ( I ,w)  -1.6(i,w,q').Since IT\[ is a function and {6(i,w,q),6(i,w,q')} E ITl(w) then 6(i,w,q) = 6(i,w,q'),therefore u = uq \[\]The following lemma will be used for completeness.Lemma 4Given a transducer T representing a subsequential function, there exists a bound Ms.t.
for each S built at line 8, for each (q,u) E S, lu\[ < M.We rely on the following theorem proven by Choffrut (1978):Theorem 1A function f on G* is subsequential iff it has bounded variations and for any rationallanguage L C ~*, f-1 (L) is also rational.with the following two definitions:DefinitionThe left distance between two strings u and v is I\[u,v\[I = \[u\[ + Iv\[ - 2\[u/~ v\[.DefinitionA function f on G* has bounded variations iff for all k ~ 0, there exists K > 0 s.t.u,v C dom(f), \[\[u,v\[\[ <_ k ~ \]\[f(u),f(v)\[\[ <_ K.Proof of Lemma 4Let f = IT\[.
For each q E Q, let c(q) be a string w s.t.
d(q,w) N F ~ 0 and s.t.
\[w\[ isminimal among such strings.
Note that \[c(q)\[ _< \[IT\[\] where \[IT\[\[ is the number of states246Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Taggingin T. For each q c Q let s(q) E Q be a state s.t.
s(q) c d(q,c(q)) AF.
Let us further defineM1 = maxl6(q,c(q),s(q))\] qEQM2 = max Ic(q)l qEQ i % l ~  lSince f is subsequential, it is of bounded variations, therefore there exists K s.t.
if\]\[u, vi\] ~ aM 2 then I\[f(u),f(v)\] I G K. Let M = K + 2M1.Let S be a state set built at line 8, let w be s.t.
I?w = S and A = I ,w.  Let (ql, u) E S.Let (q2, v) C S be s.t.
u A v = c. Such a pair always exists, since if notthus \[A.\] A u'\] > 0(q',u')ESA u'l = I A .x'u'l>l,', l(q',u')~s (q',u')csThus, because of (ii) in Lemma 1,I A 6(i,w,q')\] > I I ,w lq' Ed(i,w)which contradicts (i) in Lemma 1.Let w = ~(ql, c(ql), s(ql)) and a;' = 6(q2, c(q2), s(q2)).Moreover, for any a,b,c,d E ~*, Iia, ciI <_ \]lab, cd\[I + Ibl + \[d I.
In fact, Ilab, cdiI =\[ab\[ + IcdI- 2Iab A cd I = lal + I c\] + IbI + IdI-  2Iab A cd I = II a, c\]I + 21a A c I + \[b I + \]d I -2 lab A cd Ibut labAcd\] <_ laAcI +Ib\[+\]d\[ and since \]Iab, cd\[I = Ila, cI\[-2(\[abAcd I -\[aAc I - \[b I-IdI) - IbI- IdIone has Iia, cil < I\]ab, cdll + Ib\[ + Idl.Therefore, in particular, luI < \]\[Au, AvI\[ < JiAua;,Avw'\]\[ + \]0; I + Iw'I, thus I u\] < Iif(w ?c(ql)),f(w, c(q2))I\] q- 2M1.
But \]\[w. c(ql),W" c(q2)ll G \]c(ql)\[ + Ic(q2)I ~ 2M2, thus Iif(w ?c(ql)),f(w" c(q2))\[\] < K and therefore I u\] < K + 2M 1 = M. \[\]The time is now ripe for the main proposition, which proves soundness and com-pleteness.PropositionIf a transducer T represents a subsequential function f, then the algorithm Determinize-Transducer described in the previous section applied on T computes a subsequentialtransducer ~- representing the same function.ProofLemma 4 shows that the algorithm always terminates if IT\] is subsequential.Let us show that dom(iTI) c dom(iTI).
Let w E ~* s.t.
w is not in dom(iTI), thend(i, w) M F = 0.
Thus, according to (ii) of Lemma 1, for all (q, u) c I ?
w, q is not in F,thus I ?
w is not terminal and therefore w is not in dom(~-).Conversely, let w E dom(iT\[).
There exists a qf C F s.t.
IT\](w) = 6(i,w, qf) and s.t.qf C d(i,w).
Therefore \]Zi(w ) = ( I ,  w) .
((I* w) -1- 6(i,w, qf)) and according to (ii) ofLemma 1 (qf, (I * w) -I ?
6(i,w, qf)) c I ?
w and since qf E F, Lemma 3 shows thatp( I?
w) = ( I ,w)  -1.
~(i,w, qf), thus ITI(w) = ( I ,w) .
p(I?
w) = ITi(w).
\[\]247Computational Linguistics Volume 21, Number 29.3 Worst-Case ComplexityIn this section we give a worst-case upper bound of the size of the subsequentialtransducer in terms of the size of the input transducer.Let L = {w E G" s.t.
Iw\[ <__ M}, where M is the bound defined in the proofof Lemma 4.
Since, according to Lemma 2, for each state set Q~, for each q E Q, Q'contains at most one pair (q, w), the maximal number N of states built in the algorithmis smaller than the sum of the number of functions from states to strings in L for eachstate set, that isN < ILl IQ'tQ' E2Qwe thus have N _< 2 IQI x ILl iQI -- 2 IQI x 2 \[Qlxl?g2 iLl and therefore N _< 2 IQl(l+l?glLI).Moreover,M+'  - 1ILl = 1 + lye\] + .
.
.
+ ISl M - ISl  - 1 i f  I s \ ]  > 1and ILl = M+I  if = 1.
In this last formula, M = K+2M1, as described in Lemma 4.Note that if P = MAXa~sl6(q,a, q')l is the maximal ength of the simple transitionsemissions, M1 ~ IQI x P, thus M _< K + 2 x IQI x P.Therefore, if \[E I > 1, the number of states N is bounded:i:gl(K+2 x IQI xP+1-1 )N <_ 2 IQIx(l+l?g i~l-,and if lee = 1, N ~ 2 \[QIx(l+l?g(K+2xiQLxP+l))10.
Subsequentiality of Transformation-Based SystemsThe proof of correctness of the determinization algorithm and the fact that the algo-rithm terminates on the transducer encoding Brill's tagger show that the final functionis subsequential nd equivalent to Brill's original tagger.In this section, we prove in general that any transformation-based ystem, such asthose used by Brill, is a subsequential function.
In other words, any transformation-based system can be turned into a deterministic finite-state transducer.We define transformation-based ystems as follows.DefinitionA transformation-based ystem is a finite sequence (f\],.
.
.
,fn) of subsequential func-tions whose domains are bounded.Applying a transformation-based ystem consists of applying each function fi oneafter the other.
Applying one function consists of looking for the first position inthe input at which the function can be triggered.
When the function is triggered,the longest possible string starting at that position is transformed according to thisfunction.
After the string is transformed, the process is iterated starting at the end ofthe previously transformed string.
Then, the next function is applied.
The programends when all functions have been applied.It is not true that, in general, the local extension of a subsequential function issubsequential.
TM For instance, consider the function fa of Figure 23.18 However, the local extensions of the functions we had to compute were subsequentiaL248Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech TaggingFigure 23Function fa.a:b a:b a:bThe local extension of the function fa is not a function.
In fact, consider the inputstring daaaad; it can be decomposed either into d ?
aaa.
ad or into da ?
aaa.
d. The firstdecomposit ion leads to the output dbbbad, and the second one to the output dabbbd.The intended use of the rules in the tagger defined by Brill is to apply eachfunction from left to right.
In addition, if several decomposit ions are possible, the onethat occurs first is the one chosen.
In our previous example, it means that only theoutput dbbbad is generated.This notion is now defined precisely.Let a be the rational function defined by a(a) = a for a c ~, a(\[) = a(\]) = ~ on theadditional symbols '\[' and '\]', with a such that a(u .
v) = a (u) .
a(v).DefinitionLet Y c ~+ and X = ~* - ~*.
Y.
~*, a Y-decomposition of x is a string y E X.
(\[.
Y.
\].
X)*s.t.
a(y) = xFor instance, if Y = dom(fa) -- {aaa}, the set of Y-decompositions of x = daaad is{ d \[aaa \]ad , da \[aaa \]d }.DefinitionLet < be a total order on P, and let ~ = ~ U {\[,\]} be the al _phabet ~ with the twoadditional symbols '\[' and '\]'.
Let extend the order > to N by Va E ~, ' \ [ '< a anda < '\]'.
< defines a lexicographic order on ~* that we also denote <.
Let Y c 2 +and x c N*, the minimal Y-decomposition of x is the Y-decomposition which isminimal in (~*, <).For instance, the minimal dom(fa)-decomposition f daaaad is d\[aaa\]ad.
In fact,d\[aaaJad < da\[aaa\]d.PropositionGiven Y C ~+ finite, the function mdy that to each x c G* associates its minimalY-decomposition, is subsequential nd total.ProofLet dec be defined by dec(w) = u.
\[.
v .
1. dec((uv) -1 .
w), where u, v E P~* are s.t.
v E Y,3v' c ~* with w = uvv' and lul is minimal among such strings and dec(w) -- w if nosuch u, v exists.
The function mdy is total because the function dec always returns anoutput that is a Y-decomposition of w.We shall now prove that the function is rational and then that it has boundedvariations; this will prove according to Theorem 1 that the function is subsequential.In the following X = ~* - P,* ?
Y- P,*.
The transduction Ty that generates the set ofY-decompositions i  defined byTy = Idx.
(eft.
Idy- c/ \] .
Idx)*where Idx (resp.
Idy) stands for the identity function on X (resp.
Y).
Furthermore,249Computational Linguistics Volume 21, Number 2Figure 24Transduction T~,>.C Dthe transduct ion TU,> that to each string w E ~* associates the set of strings strictlygreater than w, that is T~,>(w) = {w' E ~*I w < w'}, is def ined by the transducer of- -  - - 2  - -  Figure 24, in which A = {(x,x)ix E G}, B = {(x,y) E ~2\[x < y}, C = G , D = {?}
xand E = G x {c}.
19Therefore, the r ight-minimal Y-decomposit ion funct ion mdy is def ined by mdy --Ty - (Tu,> o Ty), which proves that mdy is rational.Le tk  > 0.
LetK  = 6xk+6xM,  whereM- -  maxx~yix I.
Let u ,v  E G* bes .
t .Iiu, vII _< k. Let us consider two cases: (i) I u A v I _< M and (ii) lu A v I > M.(i): I u Av I _< M, thus \[uHv I ~ I u Av I + Iiu, v I \ [< M+k.
Moreover,  for each w E Y~*,for each Y-decomposit ion w' of w, Iw'\[ _< 3 x \]w I.
In fact, Y doesn' t  contain ~, thus thenumber  of \[ (resp.
l) in w' is smaller than Iw\[.
Therefore, Imdy(u) I, Imdy(v)l <_ 3 x (M+k)thus \[Imdy(u),mdy(v)lI < K.(ii): u A v = ~ ?
a; with \[a; I = M. Let #, v be s.t.
u = &w# and v = )~a;~.
Let )~',w', #', .~", a;" and v" be s.t.
mdy(u) = )~' J#' ,  mdy(v) -- )~"~;"~,", c~(~') = ~(,V') = ~,c~(a;') = c~(~o") = w, o~(#') = # and ~(~,") = ~,.
Suppose that &' # &", for instance),  < )i,.
Let i be the first indice s.t.
(;f)i < ( ,VI ) i .
20 We have two possible situations:(ii.1) ()~r)i = \[ and ;~" E ~ or (~') i  ---- \].
In that case, since the length of the elements inY is smaller than M = 14,  one has &'~;' = .~1\[.~2\],~3 with \[~ll = i, ;~2 ~ Y and "~3 E .We also have ), 'w" ' ' = /~1/~2/~ 3 with c~()~) = c~(&2) and the first letter of "~2 is differentfrom \[.
Let )~4 be a Y-decomposit ion of ~ 3 y, then &1\[~2\]/~4 is a Y-decomposit ion ofv strictly smaller than ~1 &~)~L," = mdy (v), which contradicts the minimal i ty of mdy (v).The second situation is (ii.2): (&~)i E ~.
and (&')i = \], then we have )~GJ = ~1\[,~2,~3\],~4s.t.
I,~1\[/~2I = i and ),'M" = .,~1\[,~2\],,~&~ s.t.
C~(&~) = C~()~3) and c~(&~) = c~(&4).
Let As bea Y-decomposit ion of ~" ,  then ;~ \[/~2/~3\]/~5 is a Y-decomposit ion of v strictly smallerthan ,V'w"~,", which leads to the same contradiction.
Therefore, &' = )~" and sinceI#'\[+1~,"1 _<3x ( l# l+l~, l ) - -3x  Ilu, vll _< 3x / ,  IImdyCu),mdy(v)ll <_ la;'l+la;"l+l#'l+l~,r'l <2 x M + 3 x k _< K. This proves that mdy has bounded variations and therefore that itis subsequential.
\[\]NWe can now define precisely what  is the effect of a funct ion when one applies itf rom left to right, as was done in the original tagger.19 This construction is similar to the transduction built within the proof of Eilenberg's cross sectiontheorem (Eilenberg 1974).20 (w)i refers to the i th letter in w.250Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech TaggingDefinitionIf f is a rational function with bounded omain, Y = dom(f) c ~.+, the right-minimallocal extension of f, denoted RmLocExt(f), is the composition of a right-minimalY-decomposition mdy with Ida, ?
(\[/~.
f .
\]/~.
Ida,)*.RmLocExt being the composition of two subsequential functions, it is itself subse-quential; this proves the following final proposition, which states that given a rule-based system similar to Brill's system, one can build a subsequential transducer thatrepresents it:PropositionIf (fl .
.
.
.
,fn) is a sequence of subsequential functions with bounded omains and suchthat fi(~) = 0, then RmLocExt(h ) o...  o RmLocExt(fn) is subsequential.We have proven in this section that our techniques apply to the class of transformation-based systems.
We now turn our attention to the implementation f finite-state trans-ducers.11.
Implementation of Finite-State TransducersOnce the final finite-state transducer is computed, applying it to an input is straight-forward: it consists of following the unique sequence of transitions whose left labelscorrespond to the input.
However, in order to have a complexity fully independent ofthe size of the grammar and in particular independent of the number of transitionsat each state, one should carefully choose an appropriate representation for the trans-ducer.
In our implementation, transitions can be accessed randomly.
The transduceris first represented by a two-dimensional t ble whose rows are indexed by states andwhose columns are indexed by the alphabet of all possible input letters.
The contentof the table at line q and at column a is the word w such that the transition from qwith the input label a outputs w. Since only a few transitions are allowed from manystates, this table is very sparse and can be compressed.
This compression is achievedwhile maintaining random access using a procedure for sparse data tables followingthe method given by Tarjan and Yao (1979).12.
ConclusionThe techniques described in this paper are more general than the problem of part-of-speech tagging and are applicable to the class of problems dealing with local transfor-mation rules.We showed that any transformation-based program can be transformed into adeterministic finite-state transducer.
This yields to optimal time implementations oftransformation based programs.As a case study, we applied these techniques to the problem of part-of-speechtagging and presented a finite-state tagger that requires n steps to tag a sentence oflength n, independently of the number of rules and the length of the context heyrequire.
We achieved this result by representing the rules acquired for Brill's taggeras nondeterministic finite-state transducers.
We composed each of these nondetermin-istic transducers and turned the resulting transducer into a deterministic transducer.The resulting deterministic transducer yields a part-of-speech tagger that operates inoptimal time in the sense that the time to assign tags to a sentence corresponds tothe time required to follow a single path in this deterministic finite-state machine.
The251Computational Linguistics Volume 21, Number 2tagger outperforms in speed both Brill's tagger and stochastic taggers.
Moreover, thefinite-state tagger inherits from the rule-based system its compactness compared withstochastic taggers.
We also proved the correctness and the generality of the methods.We believe that this finite-state tagger will also be found useful when combinedwith other language components, since it can be naturally extended by composingit with finite-state transducers that could encode other aspects of natural languagesyntax.AcknowledgmentsWe thank Eric Brill for providing us withthe code of his tagger and for many usefuldiscussions.
We also thank Aravind K.Joshi, Mark Liberman, and Mehryar Mohrifor valuable discussions.
We thank theanonymous reviewers for many helpfulcomments that led to improvements in boththe content and the presentation f thispaper.ReferencesBrill, Eric (1992).
"A simple rule-based partof speech tagger."
In Proceedings, ThirdConference on Applied Natural LanguageProcessing.
Trento, Italy, 152-155.Brill, Eric (1994).
"A report of recentprogress in transformation error-drivenlearning."
In Proceedings, Tenth NationalConference on Artificial Intelligence(AAAI-94).
Seattle, Washington, 722-727.Choffrut, Christian (1977).
"Unecaract6risation des fonctions 6quentielleset des fonctions ous-s6quentielles en tantque relations rationnelles."
TheoreticalComputer Science, 5, 325-338.Choffrut, Christian (1978).
Contributionl'dtude de quelques families remarquables defonctions rationnelles.
Doctoral dissertation,Universit6 Paris VII (Th6se d'Etat).Chomsky, Noam (1964).
Syntactic Structures.Mouton.Church, Kenneth Ward (1988).
"A stochasticparts program and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
Austin, Texas, 136-143.Clemenceau, David (1993).
Structuration dulexique t reconnaissance de mots ddriv&.Doctoral dissertation, Universit6 Paris 7.Cutting, Doug; Kupiec, Julian; Pederson,Jan; and Sibun, Penelope (1992).
"Apractical part-of-speech tagger."
InProceedings, Third Conference on AppliedNatural Language Processing.
Trento, Italy,133-140.DeRose, S. J.
(1988).
"Grammatical categorydisambiguation by statisticaloptimization."
Computational Linguistics,14, 31-39.Eilenberg, Samuel (1974).
Automata,Languages, and Machines.
Academic Press.Elgot, C. C., and Mezei, J. E. (1965).
"Onrelations defined by generalized finiteautomata."
IBM Journal of Research andDevelopment, 9 47-65.Francis, W. Nelson, and Ku~era, Henry(1982).
Frequency Analysis of English Usage.Houghton Mifflin.Kaplan, Ronald M., and Kay, Martin (1994).
"Regular models of phonological rulesystems."
Computational Linguistics, 20(3),331-378.Karttunen, Lauri; Kaplan, Ronald M.; andZaenen, Annie (1992).
"Two-levelmorphology with composition."
InProceedings, 14 th International Conference onComputational Linguistics (COLING-92).Nantes, France, 141-148.Kupiec, J. M. (1992).
"Robust part-of-speechtagging using a hidden Markov model.
"Computer Speech and Language, 6, 225-242.Laporte, Eric (1993).
"Phon6tique ttransducteurs."
Technical report,Universit6 Paris 7, June.Merialdo, Bernard (1990).
"Tagging textwith a probabilistic model."
TechnicalReport RC 15972, IBM Research Division.Mohri, Mehryar (1994a).
"Minimisation ofsequential transducers."
In Proceedings,Fifth Annual Symposium on CombinatorialPattern Matching.
Lecture Notes inComputer Science, Springer-Verlag.Mohri, Mehryar (1994b).
"On someapplications of finite-state automatatheory to natural anguage processing.
"Technical report, Institut Gaspard Monge.Pereira, Fernando C. N.; Riley, Michael; andSproat, Richard W. (1994).
"Weightedrational transductions and theirapplication to human languageprocessing."
In Human Language TechnologyWorkshop.
262-267.
Morgan Kaufmann.Revuz, Dominique (1991).
Dictionnaires etlexiques, m~thodes t algorithmes.
Doctoraldissertation, Universit6 Paris 7.Roche, Emmanuel (1993).
Analyse syntaxiquetransformationelle du fran~ais partransducteurs etlexique-grammaire.
Doctoraldissertation, Universit6 Paris 7.Sch6tzenberger, Marcel Paul (1977).
"Sur252Emmanuel Roche and Yves Schabes Deterministic Part-of-Speech Taggingune variante des fonctions equentielles.
"Theoretical Computer Science, 4, 47-57.Silberztein, Max (1993).
DictionnairesElectroniques tAnalyse Lexicale duFranFais--Le Syst~me INTEX.
Masson.Tapanainen, Pasi, and Voutilainen, Atro(1993).
"Ambiguity resolution in areductionistic parser."
In Proceedings, SixthConference ofthe European Chapter of theACL.
Utrecht, Netherlands, 394-403.Tarjan, Robert Endre, and Chi-Chih Yao,Andrew (1979).
"Storing a sparse table.
"Communications of the ACM, 22(11),606-611.Weischedel, Ralph; Meteer, Marie; Schwartz,Richard; Ramshaw, Lance; and Palmucci,Jeff (1993).
"Coping with ambiguity andunknown words through probabilisticmodels."
Computational Linguistics, 19(2),359-382.253
