Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145?149,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsUnsupervised Semantic Role Induction with Global Role OrderingNikhil GargUniversity of GenevaSwitzerlandnikhil.garg@unige.chJames HendersonUniversity of GenevaSwitzerlandjames.henderson@unige.chAbstractWe propose a probabilistic generative modelfor unsupervised semantic role induction,which integrates local role assignment deci-sions and a global role ordering decision in aunified model.
The role sequence is dividedinto intervals based on the notion of primaryroles, and each interval generates a sequenceof secondary roles and syntactic constituentsusing local features.
The global role orderingconsists of the sequence of primary roles only,thus making it a partial ordering.1 IntroductionUnsupervised semantic role induction has gainedsignificant interest recently (Lang and Lapata,2011b) due to limited amounts of annotated corpora.A Semantic Role Labeling (SRL) system shouldprovide consistent argument labels across differentsyntactic realizations of the same verb (Palmer et al,2005), as in(a.)
[ Mark ]A0 drove [ the car ]A1(b.)
[ The car ]A1 was driven by [ Mark ]A0This simple example also shows that while certainlocal syntactic and semantic features could provideclues to the semantic role label of a constituent, non-local features such as predicate voice could provideinformation about the expected semantic role se-quence.
Sentence a is in active voice with sequence(A0, PREDICATE, A1) and sentence b is in passivevoice with sequence (A1, PREDICATE, A0).
Addi-tional global preferences, such as arguments A0 andA1 rarely repeat in a frame (as seen in the corpus),could also be useful in addition to local features.Supervised SRL systems have mostly used localclassifiers that assign a role to each constituent inde-pendently of others, and only modeled limited cor-relations among roles in a sequence (Toutanova etal., 2008).
The correlations have been modeled viarole sets (Gildea and Jurafsky, 2002), role repeti-tion constraints (Punyakanok et al, 2004), languagemodel over roles (Thompson et al, 2003; Pradhanet al, 2005), and global role sequence (Toutanovaet al, 2008).
Unsupervised SRL systems have ex-plored even fewer correlations.
Lang and Lapata(2011a; 2011b) use the relative position (left/right)of the argument w.r.t.
the predicate.
Grenager andManning (2006) use an ordering of the linking of se-mantic roles and syntactic relations.
However, as thespace of possible linkings is large, language-specificknowledge is used to constrain this space.Similar to Toutanova et al (2008), we propose touse global role ordering preferences but in a gener-ative model in contrast to their discriminative one.Further, unlike Grenager and Manning (2006), wedo not explicitly generate the linking of semanticroles and syntactic relations, thus keeping the pa-rameter space tractable.
The main contribution ofthis work is an unsupervised model that uses globalrole ordering and repetition preferences without as-suming any language-specific constraints.Following Gildea and Jurafsky (2002), previouswork has typically broken the SRL task into (i) argu-ment identification, and (ii) argument classification(Ma`rquez et al, 2008).
The latter is our focus in thiswork.
Given the dependency parse tree of a sentencewith correctly identified arguments, the aim is to as-sign a semantic role label to each argument.145Algorithm 1 Generative process??????
PARAMETERS ?????
?for all predicate p dofor all voice vc ?
{active, passive} dodraw ?orderp,vc ?
Dirichlet(?order)for all interval I dodraw ?SRp,I ?
Dirichlet(?SR)for all adjacency adj ?
{0, 1} dodraw ?STOPp,I,adj ?
Beta(?STOP )for all role r ?
PR ?
SR dofor all feature type f dodraw ?Fp,r,f ?
Dirichlet(?F )????????
DATA ???????
?given a predicate p with voice vc:choose an ordering o ?
Multinomial(?orderp,vc )for all interval I ?
o dodraw an indicator s ?
Binomial(?STOPp,I,0 )while s 6= STOP dochoose a SR r ?
Multinomial(?SRp,I )draw an indicator s ?
Binomial(?STOPp,I,1 )for all generated roles r dofor all feature type f dochoose a value vf ?
Multinomial(?Fp,r,f )2 Proposed ModelWe assume the roles to be predicate-specific.
Webegin by introducing a few terms:Primary Role (PR) For every predicate, we assumethe existence of K primary roles (PRs) denoted byP1, P2, ..., PK .
These roles are not allowed to re-peat in a frame and serve as ?anchor points?
in theglobal role ordering.
Intuitively, the model attemptsto choose PRs such that they occur with high fre-quency, do not repeat, and their ordering influencesthe positioning of other roles.
Note that a PR maycorrespond to either a core role or a modifier role.For ease of explication, we create 3 additional PRs:START denoting the start of the role sequence, ENDdenoting its end, and PRED denoting the predicate.Secondary Role (SR) The roles that are not PRs arecalled secondary roles (SRs).
Given N roles in total,there are (N ?K) SRs, denoted by S1, S2, ..., SN?K .Unlike PRs, SRs are not constrained to occur onlyonce in a frame and do not participate in the globalrole ordering.Interval An interval is a sequence of SRs boundedby PRs, for instance (P2, S3, S5, PRED).Ordering An ordering is the sequence of PRs ob-served in a frame.
For example, if the complete roleFigure 1: Proposed model.
Shaded and unshadednodes represent visible and hidden variables resp.sequence is (START ,P2, S1, S1, PRED, S3, END), theordering is defined as (START , P2, PRED, END).Features We have explored 1 frame level (global)feature (i) voice: active/passive, and 3 argumentlevel (local) features (i) deprel: dependency relationof an argument to its head in the dependency parsetree, (ii) head: head word of the argument, and (iii)pos-head: Part-of-Speech tag of head.Algorithm 1 describes the generative story of ourmodel and Figure 1 illustrates it graphically.
Given apredicate and its voice, an ordering is selected froma multinomial.
This ordering gives us the sequenceof PRs (PR1, PR2, ..., PRN ).
Each pair of consec-utive PRs, PRi, PRi+1, in an ordering correspondsto an interval Ii.
For each such interval, we generate0 or more SRs (SRi1, SRi2, ...SRiM ) as follows.Generate an indicator variable: CONTINUE/STOPfrom a binomial distribution.
If CONTINUE, gen-erate a SR from the multinomial corresponding tothe interval.
Generate another indicator variable andcontinue the process till a STOP has been generated.In addition to the interval, the indicator variable alsodepends on whether we are generating the first SR(adj = 0) or a subsequent one (adj = 1).
For eachrole, primary as well as secondary, we now generatethe corresponding constituent by generating each ofits features independently (F1, F2, ..., FT ).Given a frame instance with predicate p and voicevc, Figure 2 gives (i) Eq.
1: the joint distributionof the ordering o, role sequence r, and constituentsequence f , and (ii) Eq.
2: the marginal distributionof an instance.
The likelihood of the whole corpusis the product of marginals of individual instances.146P (o, r, f |p, vc) = P (o|p, vc)?
??
?ordering?
?
{ri?r?PR}P (fi|ri, p)?
??
?Primary Roles?
?
{I?o}P (r(I), f(I)|I, p)?
??
?Intervals(1)where P (r(I), f(I)|I, p) =?ri?r(I)P (continue|I, p, adj)?
??
?generate indicatorP (ri|I, p)?
??
?generate SRP (fi|ri, p)?
??
?generate features?
P (stop|I, p, adj)?
??
?end of the intervaland P (fi|ri, p) = ?tP (fi,t|ri, p)P (f |p, vc) = ?o?
{r?seq(o)}P (o, r, f |p, vc) where seq(o) = {role sequences allowed under ordering o} (2)Figure 2: ri and fi denote the role and features at position i respectively, and r(I) and f(I) respectivelydenote the SR sequence and feature sequence in interval I .
fi,t denotes the value of feature t at position i.This particular choice of model is inspired fromdifferent sources.
Firstly, making the role order-ing dependent only on PRs aligns with the obser-vation by Pradhan et al (2005) and Toutanova etal.
(2008) that including the ordering informationof only core roles helped improve the SRL perfor-mance as opposed to the complete role sequence.Although our assumption here is softer in that weassume the existence of some roles which definethe ordering which may or may not correspond tocore roles.
Secondly, generating the SRs indepen-dently of each other given the interval is based onthe intuition that knowing the core roles informsus about the expected non-core roles that occur be-tween them.
This intuition is supported by the statis-tics in the annotated data, where we found that if weconsider the core roles as PRs, then most of the in-tervals tend to have only a few types of SRs and agiven SR tends to occur only in a few types of in-tervals.
The concept of intervals is also related tothe linguistic theory of topological fields (Diderich-sen, 1966; Drach, 1937).
This simplifying assump-tion that given the PRs at the interval boundary, theSRs in that interval are independent of the otherroles in the sequence, keeps the parameter space lim-ited, which helps unsupervised learning.
Thirdly,not allowing some or all roles to repeat has beenemployed as a useful constraint in previous work(Punyakanok et al, 2004; Lang and Lapata, 2011b),which we use here for PRs.
Lastly, conditioning the(STOP/CONTINUE) indicator variable on the adja-cency value (adj) is inspired from the DMV model(Klein and Manning, 2004) for unsupervised depen-dency parsing.
We found in the annotated corpusthat if we map core roles to PRs, then most of thetime the intervals do not generate any SRs at all.
So,the probability to STOP should be very high whengenerating the first SR.We use an EM procedure to train the model.
Inthe E-step, we calculate the expected counts of allthe hidden variables in our model using the Inside-Outside algorithm (Baker, 1979).
In the M-step, weadd the counts corresponding to the Bayesian priorsto the expected counts and use the resulting countsto calculate the MAP estimate of the parameters.3 ExperimentsFollowing the experimental settings of Lang and La-pata (2011b), we use the CoNLL 2008 shared taskdataset (Surdeanu et al, 2008), only consider ver-bal predicates, and run unsupervised training on thestandard training set.
The evaluation measures arealso the same: (i) Purity (PU) that measures howwell an induced cluster corresponds to a single goldrole, (ii) Collocation (CO) that measures how wella gold role corresponds to a single induced cluster,and (iii) F1 which is the harmonic mean of PU andCO.
Final scores are computed by weighting eachpredicate by the number of its argument instances.We chose a uniform Dirichlet prior with concentra-tion parameter as 0.1 for all the model parametersin Algorithm 1 (set roughly, without optimization1).50 training iterations were used.3.1 ResultsSince the dataset has 21 semantic roles in total, wefix the total number of roles in our model to be 21.Further, we set the number of PRs to 2 (excludingSTART , END and PRED), and SRs to 21-2=19.1Removing the Bayesian priors completely, resulted in theEM algorithm getting to a local maxima quite early, giving asubstantially lower performance.147Model Features PU CO F10 Baseline2 d 81.6 78.1 79.81a Proposed d 82.3 78.6 80.41b Proposed d,h 82.7 77.2 79.91c Proposed d,p-h 83.5 78.5 80.91d Proposed d,p-h,h 83.2 77.1 80.0Table 1: Evaluation.
d refers to deprel, h refers tohead and p-h refers to pos-head.Table 1 gives the results using different featurecombinations.
Line 0 reports the performance ofLang and Lapata (2011b)?s baseline, which has beenshown difficult to outperform.
This baseline maps20 most frequent deprel to a role each, and the restare mapped to the 21st role.
By just using deprel asa feature, the proposed model outperforms the base-line by 0.6 points in terms of F1 score.
In this con-figuration, the only addition over the baseline is theordering model.
Adding head as a feature leads tosparsity, which results in a substantial decrease incollocation (lines 1b and 1d).
However, just addingpos-head (line 1c) does not cause this problem andgives the best F1 score.
To address sparsity, we in-duced a distributed hidden representation for eachword via a neural network, capturing the semanticsimilarity between words.
Preliminary experimentsimproved the F1 score when using this word repre-sentation as a feature instead of the word directly.Lang and Lapata (2011b) give the results of threemethods on this task.
In terms of F1 score, the La-tent Logistic and Graph Partitioning methods resultin slight reduction in performance over the baseline,while the Split-Merge method results in an improve-ment of 0.6 points.
Table 1, line 1c achieves an im-provement of 1.1 points over the baseline.3.2 Further EvaluationTable 2 shows the variation in performance w.r.t.the number of PRs3 in the best performing config-uration (Table 1, line 1c).
On one extreme, whenthere are 0 PRs, there are only two possible in-tervals: (START,PRED) and (PRED,END) whichmeans that the only context information a SR hasis whether it is to the left or right of the predicate.2The baseline F1 reported by Lang and Lapata (2011b) is79.5 due to a bug in their system (personal communication).3Note that the system might not use all available PRs to labela given frame instance.
#PRs refers to the max #PRs.# PRs PU CO F10 81.67 78.07 79.831 82.91 78.99 80.902 83.54 78.47 80.933 83.68 78.23 80.874 83.72 78.08 80.80Table 2: Performance variation with the number ofPRs (excluding START , END and PRED)With only this additional ordering information, theperformance is the same as the baseline.
Adding just1 PR leads to a big increase in both purity and col-location.
Increasing the number of PRs beyond 1leads to a gradual increase in purity and decline incollocation, with the best F1 score at 2 PRs.
Thisbehavior could be explained by the fact that increas-ing the number of PRs also increases the number ofintervals, which makes the probability distributionsmore sparse.
In the extreme case, where all the rolesare PRs and there are no SRs, the model would justlearn the complete sequence of roles, which wouldmake the parameter space too large to be tractable.For calculating purity, each induced cluster (orrole) is mapped to a particular gold role that hasthe maximum instances in the cluster.
Analyzing theoutput of our model (line 1c in Table 1), we foundthat about 98% of the PRs and 40% of the SRs gotmapped to the gold core roles (A0,A1, etc.).
Thissuggests that the model is indeed following the intu-ition that (i) the ordering of core roles is importantinformation for SRL systems, and (ii) the intervalsbounded by core roles provide good context infor-mation for classification of other roles.4 ConclusionsWe propose a unified generative model for unsu-pervised semantic role induction that incorporatesglobal role correlations as well as local feature infor-mation.
The results indicate that a small number ofordered primary roles (PRs) is a good representationof global ordering constraints for SRL.
This repre-sentation keeps the parameter space small enoughfor unsupervised learning.AcknowledgmentsThis work was funded by the Swiss NSF grant200021 125137 and EC FP7 grant PARLANCE.148ReferencesJ.K.
Baker.
1979.
Trainable grammars for speech recog-nition.
The Journal of the Acoustical Society of Amer-ica, 65:S132.P.
Diderichsen.
1966.
Elementary Danish Grammar.Gyldendal, Copenhagen.E.
Drach.
1937.
Grundstellung der Deutschen Satzlehre.Diesterweg, Frankfurt.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.T.
Grenager and C.D.
Manning.
2006.
Unsupervised dis-covery of a statistical verb lexicon.
In Proceedings ofthe 2006 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics.D.
Klein and C.D.
Manning.
2004.
Corpus-based in-duction of syntactic structure: Models of dependencyand constituency.
In Proceedings of the 42nd AnnualMeeting on Association for Computational Linguis-tics, page 478.
Association for Computational Linguis-tics.J.
Lang and M. Lapata.
2011a.
Unsupervised semanticrole induction via split-merge clustering.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics, Portland, Oregon.J.
Lang and M. Lapata.
2011b.
Unsupervised seman-tic role induction with graph partitioning.
In Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing, pages 1320?1331, Ed-inburgh, Scotland, UK., July.
Association for Compu-tational Linguistics.L.
Ma`rquez, X. Carreras, K.C.
Litkowski, and S. Steven-son.
2008.
Semantic role labeling: an introduc-tion to the special issue.
Computational linguistics,34(2):145?159.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.S.
Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H.
Mar-tin, and D. Jurafsky.
2005.
Support vector learning forsemantic argument classification.
Machine Learning,60(1):11?39.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear programminginference.
In Proceedings of the 20th internationalconference on Computational Linguistics, page 1346.Association for Computational Linguistics.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The conll-2008 shared task on jointparsing of syntactic and semantic dependencies.
InProceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 159?177.Association for Computational Linguistics.C.
Thompson, R. Levy, and C. Manning.
2003.
A gen-erative model for semantic role labeling.
MachineLearning: ECML 2003, pages 397?408.K.
Toutanova, A. Haghighi, and C.D.
Manning.
2008.
Aglobal joint model for semantic role labeling.
Compu-tational Linguistics, 34(2):161?191.149
