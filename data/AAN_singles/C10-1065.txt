Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 572?580,Beijing, August 2010Evaluating N-gram based Evaluation Metrics forAutomatic Keyphrase ExtractionSu Nam Kim, Timothy BaldwinCSSEUniversity of Melbournesunamkim@gmail.com, tb@ldwin.netMin-Yen KanSchool of ComputingNational University of Singaporekanmy@comp.nus.edu.sgAbstractThis paper describes a feasibility studyof n-gram-based evaluation metrics forautomatic keyphrase extraction.
To ac-count for near-misses currently ignoredby standard evaluation metrics, we adaptvarious evaluation metrics developed formachine translation and summarization,and also the R-precision evaluation metricfrom keyphrase evaluation.
In evaluation,the R-precision metric is found to achievethe highest correlation with human anno-tations.
We also provide evidence thatthe degree of semantic similarity varieswith the location of the partially-matchingcomponent words.1 IntroductionKeyphrases are noun phrases (NPs) that are repre-sentative of the main content of documents.
Sincethey represent the key topics in documents, ex-tracting good keyphrases benefits various natu-ral language processing (NLP) applications suchas summarization, information retrieval (IR) andquestion-answering (QA).
Keyphrases can also beused in text summarization as semantic metadata(Barzilay and Elhadad, 1997; Lawrie et al, 2001;D?Avanzo and Magnini, 2005).
In search engines,keyphrases supplement full-text indexing and as-sist users in creating good queries.In the past, a large body of work on keyphraseshas been carried out as an extraction task, uti-lizing three types of cohesion: (1) documentcohesion, i.e.
cohesion between documents andkeyphrases (Frank et al, 1999; Witten et al, 1999;Matsuo and Ishizuka, 2004; Medelyan and Wit-ten, 2006; Nguyen and Kan, 2007; Wan andXiao, 2008); (2) keyphrase cohesion, i.e.
cohe-sion among keyphrases (Turney, 2003); and (3)term cohesion, i.e.
cohesion among terms in akeyphrase (Park et al, 2004).Despite recent successes in keyphrase extrac-tion (Frank et al, 1999; Turney, 2003; Park et al,2004; Medelyan and Witten, 2006; Nguyen andKan, 2007), current work is hampered by the in-flexibility of standard metrics in evaluating differ-ent approaches.
As seen in other fields, e.g.
ma-chine translation (MT) and multi-document sum-marization, the advent of standardized automaticevaluation metrics, combined with standardizeddatasets, has enabled easy comparison of sys-tems and catalyzed the respective research ar-eas.
Traditionally, the evaluation of automatickeyphrase extraction has relied on the numberof exact matches in author-assigned keyphrasesand reader-assigned keyphrases.
The main prob-lem with this approach is that even small vari-ants in the keyphrases are not given any credit.For example, given the gold-standard keyphraseeffective grid computing algorithm, grid com-puting algorithm is a plausible keyphrase candi-date and should be scored appropriately, ratherthan being naively evaluated as wrong.
Addition-ally, author-assigned keyphrases and even reader-assigned keyphrases often have their own prob-lems in this type of evaluation (Medelyan andWit-ten, 2006).
For example, some keyphrases areoften partly or wholly subsumed by other can-didates or may not even occur in the document.Therefore, counting the exactly-matching candi-dates has been shown to be suboptimal (Jarmasz572and Barriere, 2004).Our goal in this paper is to evaluate the relia-bility of automatic evaluation metrics that betteraccount for near-misses.
Prior research based onsemantic similarity (Jarmasz and Barriere, 2004;Mihalcea and Tarau, 2004; Medelyan and Wit-ten, 2006) has taken the approach of using ex-ternal resources such as large corpora, Wikipediaor manually-curated index words.
While we ac-knowledge that these methods can help addressthe near-miss problem, they are impractical dueto the effort required to compile the requisite re-sources for each individual evaluation exercise,and furthermore, the resources tend to be domain-specific.
In order to design a cheap, practical andstable keyphrase evaluation metric, our aim is toproperly account for these near-misses without re-liance on costly external resources.According to our analysis, the degree of se-mantic similarity of keyphrase candidates variesrelative to the location of overlap.
For exam-ple, the candidate grid computing algorithm hashigher semantic similarity than computing algo-rithm with the gold-standard keyphrase effectivegrid computing algorithm.
Also, computing algo-rithm is closer than effective grid to the same gold-standard keyphrase.
From these observations, weinfer that n-gram-based evaluation metrics canbe applied to evaluating keyphrase extraction, butalso that candidates with the same relative n-gramoverlap are not necessarily equally good.Our primary goal is to test the utility of n-grambased evaluation metrics to the task of keyphraseextraction evaluation.
We test the following eval-uation metrics: (1) evaluation metrics from MTand multi-document summarization (BLEU, NIST,METEOR and ROUGE); and (2) R-precision (Zeschand Gurevych, 2009), an n-gram-based evalua-tion metric developed specifically for keyphraseextraction evaluation which has yet to be evalu-ated against humans at the extraction task.
Sec-ondarily, we attempt to shed light on the biggerquestion of whether it is feasible to expect thatn-gram-based metrics without access to externalresources should be able to capture subtle seman-tic differences in keyphrase candidates.
To thisend, we experimentally verify the impact of lex-ical overlap of different types on keyphrase sim-ilarity, and use this as the basis for proposing avariant of R-precision.In the next section, we present a brief primer onkeyphrases.
We then describe the MT and sum-marization evaluation metrics trialled in this re-search, along with R-precision, modified R-precisionand a semantic similarity-based evaluation metricfor keyphrase evaluation (Section 3).
In Section 4,we discuss our gold-standard and candidate ex-traction method.
We compare the evaluation met-rics with human assigned scores for suitability inSection 5, before concluding the paper.2 A Primer on KeyphrasesKeyphrases can be either simplex words (e.g.query, discovery, or context-awareness)1 or largerN-bars/noun phrases (e.g.
intrusion detection,mobile ad-hoc network, or quality of service).The majority of keyphrases are 1?4 words long(Paukkeri et al, 2008).Keyphrases are normally composed of nounsand adjectives, but may occasionally contain ad-verbs (e.g.
dynamically allocated task, or partiallyobservable Markov decision process) or otherparts of speech.
They may also contain hyphens(e.g.
sensor-grouping or multi-agent system) andapostrophes for possessives (e.g.
Bayes?
theoremor agent?s goal).Keyphrases can optionally incorporate PPs (e.g.service quality vs. quality of service).
A variety ofprepositions can be used (e.g.
incentive for coop-eration, inequality in welfare, agent security viaapproximate policy), although the genetive of isthe most common.Keyphrases can also be coordinated, either assimple nouns at the top level (e.g.
performanceand scalability or group and partition) or withinmore complex NPs or between N-bars (e.g.
his-tory of past encounter and transitivity or task andresource allocation in agent system).When candidate phrases get too long, abbre-viations also help to form valid keyphrases (e.g.computer support collaborative work vs. CSCW,or partially observable Markov decision processvs.
POMDP).1All examples in this section are taken from the data setoutlined in Section 4.5733 Evaluation MetricsThere have been various evaluation metrics de-veloped and validated for reliability in fields suchas MT and summarization (Callison-Burch et al,2009).
While n-gram-based metrics don?t cap-ture systematic alternations in keyphrases, they dosupport partial match between keyphrase candi-dates and the reference keyphrases.In this section, we first introduce a range ofpopular n-gram-based evaluation metrics fromthe MT and automatic summarization literature,which we naively apply to the task of keyphraseevaluation.
We then present R-precision, an n-gram-based evaluation metric developed specif-ically for keyphrase evaluation, and propose amodified version of R-precision which weights n-grams according to their relative position in thekeyphrase.
Finally, we present a semantic similar-ity method.3.1 Machine Translation and SummarizationEvaluation MetricsIn this research, we experiment with four popu-lar n-gram-based metrics from the MT and au-tomatic summarization fields ?
BLEU, METEOR,NIST and ROUGE.
The basic task performed by therespective evaluation metrics is empirical determi-nation of how good an approximation is string1 ofstring2?, which is not far removed from the re-quirements of keyphrase evaluation.
We brieflyoutline each of the methods below.One subtle property of keyphrase evaluation isthat there is no a priori preference for shorterkeyphrases over longer keyphrases, unlike MTwhere shorter strings tend to be preferred.
Hence,we use the longer NP as reference and the shorterNP as a translation, to avoid the length penalty inmost MT metrics.2BLEU (Papineni et al, 2002) is an evaluationmetric for measuring the relative similarity be-tween a candidate translation and a set of ref-erence translations, based on n-gram composi-tion.
It calculates the number of overlapping n-grams between the candidate translation and the2While we don?t present the numbers in this paper, theresults were lower for the MT evaluation metrics without thisreordering of the reference and candidate keyphrases.set of reference translations.
In order to avoid hav-ing very short translations receive artificially highscores, BLEU adds a brevity penalty to the scoringequation.METEOR (Agarwal and Lavie, 2008) is similarto BLEU, in that it measures string-level similaritybetween the reference and candidate translations.The difference is that it allows for more matchflexibility, including stem variation and WordNetsynonymy.
The basic metric is based on the num-ber of mapped unigrams found between the twostrings, the total number of unigrams in the trans-lation, and the total number of unigrams in the ref-erence.NIST (Martin and Przybocki, 1999) is onceagain similar to BLEU, but integrates a propor-tional difference in the co-occurrences for all n-grams while weighting more heavily n-grams thatoccur less frequently, according to their informa-tion value.ROUGE (Lin and Hovy, 2003) ?
and its vari-ants including ROUGE-N and ROUGE-L ?
is simi-larly based on n-gram overlap between the can-didate and reference summaries.
For example,ROUGE-N is based on co-occurrence statistics,using higher-order n-grams (n > 1) to esti-mate the fluency of summaries.
ROUGE-L useslongest common subsequence (LCS)-based statis-tics, based on the assumption that the longer thesubstring overlap between the two strings, thegreater the similar Saggion et al (2002).
ROUGE-W is a weighted LCS-based statistic that priori-tizes consecutive LCSes.
In this research, we ex-periment exclusively with the basic ROUGE met-ric, and unigrams (i.e.
ROUGE-1).3.2 R-precisionIn order to analyze near-misses in keyphrase ex-traction evaluation, Zesch and Gurevych (2009)proposed R-precision, an n-gram-based evalua-tion metric for keyphrase evaluation.3 R-precisioncontrasts with the majority of previous work onkeyphrase extraction evaluation, which has usedsemantic similarity based on external resources3Zesch and Gurevych?s R-precision has nothing to do withthe information retrieval evaluation metric of the same name,where P@N is calculated forN equal to the number of rele-vant documents.574(Jarmasz and Barriere, 2004; Mihalcea and Tarau,2004; Medelyan and Witten, 2006).
As our inter-est is in fully automated evaluation metrics whichdon?t require external resources and are domainindependent (for maximal reproducibility of re-sults), we experiment only with R-precision in thispaper.R-precision is based on the number of overlap-ping words between a keyphrase and a candi-date, as well as the length of each.
The met-ric differentiates three types of near-misses: IN-CLUDE, PARTOF and MORPH.
The first twotypes are based on an n-gram approach, whilethe third relies on lexical variation.
As we usestemming, in line with the majority of previouswork on keyphrase extraction evaluation, we fo-cus exclusively on the first two cases, namely IN-CLUDE, and PARTOF.
The final score returnedby R-precision is:number of overlapping word(s)length of keyphrase/candidatewhere the denominator is the longer of thekeyphrase and candidate.Zesch and Gurevych (2009) evaluated R-precision over three corpora (Inspec, DUC and SP)based on 566 non-exact matching candidates.
Inorder to evaluate the human agreement, they hired4 human annotators to rate the near-miss candi-dates, and reported agreements of 80% and 44%for the INCLUDE and PARTOF types, respec-tively.
They did not, however, perform holisticevaluation with human scores to verify its relia-bility in full system evaluation.
This is one of ourcontributions in this paper.3.3 Modified R-precisionIn this section, we describe a modification toR-precision which assigns different weights forcomponent words based on their position in thekeyphrase (unlike R-precision which assigns thesame score for each matching component word).The head noun generally encodes the core seman-tics of the keyphrase, and as a very rough heuris-tic, the further a word is from the head noun,the less semantic import on the keyphrase it has.As such, modified R-precision assigns a score toeach component word relative to its position asCW = 1N?i+1 where N is the number of com-ponent words in the keyphrase and i is the posi-tion of the component word in the keyphrase (1 =leftmost word).For example, AB and BC from ABC would bescored as 13+ 1213+12+11= 511 and12+1113+12+11= 911 , re-spectively.
Thus, with the keyphrase effectivegrid computing algorithm and candidates effec-tive grid, grid computing and computing algo-rithm, modified R-precision assigns different scoresfor each candidate (computing algorithm > gridcomputing > effective grid).
In contrast, the orig-inal R-precision assigns the same score to all can-didates.3.4 Semantic SimilarityIn Jarmasz and Barriere (2004) and Mihalcea andTarau (2004), the authors used a large data setto compute the semantic similarity of two NPsto assign partial credits for semantically similarcandidate keyphrases.
To simulate these meth-ods, we adopted the distributional semantic simi-larity using web documents.
That is, we computedthe similarity between a keyphrase and its sub-string by cosine measure over collected the snip-pets from Yahoo!
BOSS.4 We use the computedsimilarity as our score for near-misses.4 Data4.1 Data CollectionWe constructed a keyphrase extraction dataset us-ing papers across 4 different categories5 of theACM Digital Library.6 In addition to author-assigned keyphrases provided as part of the ACMDigital Library, we generated reader-assignedkeyphrases by assigning 250 students 5 paperseach, a list of candidate keyphrases (see below fordetails), and standardized instructions on how toassign keyphrases.
It took them an average of 15minutes to annotate each paper.
This is the same4http://developer.yahoo.com/search/boss/5C2.4 (Distributed Systems), H3.3 (Information Searchand Retrieval), I2.11 (Distributed Artificial Intelligence ?Multiagent Systems) and J4 (Social and Behavioral Sciences?
Economics).6http://portal.acm.org/dl.cfm575Author Reader TotalTotal 1298/1305 3110/3221 3816/3962NPs 937 2537 3027Average 3.85/4.01 12.44/12.88 15.26/15.85Found 769 2509 2864Table 1: Details of the keyphrase dataset(Rule1) NBAR = (NN*|JJ*)?(NN*)e.g.
complexity, effective algorithm,distributed web-service discovery architecture(Rule2) NBAR IN NBARe.g.
quality of service, sensitivity of VOIP traffic,simplified instantiation of zebroidTable 2: Regular expressions for candidate selec-tiondocument collection and set of keyphrase annota-tions as was used in the SemEval 2010 keyphraseextraction task (Kim et al, 2010).Table 1 shows the details of the final dataset.The numbers after the slashes indicate the numberof keyphrases after including alternate keyphrasesbased on of -PPs.
Despite the reliability of author-assigned keyphrases discussed in Medelyan andWitten (2006), many author-assigned keyphrasesand some reader-assigned keyphrases are notfound verbatim in the source documents because:(1) many of them are substrings of the candidatesor vice versa (about 75% of the total keyphrasesare found in the documents); and (2) our candi-date selection method does not extract keyphrasesin forms such as coordinated NPs or adverbialphrases.4.2 Candidate SelectionDuring preprocessing, we first converted thePDF versions of the papers into text usingpdftotext.
We then lemmatized and POStagged all words using morpha and the LinguaPOS tagger.
Next, we applied the regular expres-sions in Table 2 to extract candidates, based onNguyen and Kan (2007).
Finally, we selected can-didates in terms of their frequency: simplex wordswith frequency ?
2 and NPs with frequency ?
1.We observed that for reader-assigned keyphrases,NPs were often selected regardless of their fre-quency in the source document.
In addition, weallowed variation in the possessive form, nounnumber and abbreviations.Rule1 detects simplex nouns or N-bars as candi-dates.
Rule2 extracts N-bars with post-modifyingPPs.
In Nguyen and Kan (2007), Rule2 was notused to additionally extract N-bars inside modify-ing PPs.
For example, our rules extract not onlyperformance of grid computing as a candidate, butalso grid computing.
However, we did not extendthe candidate selection rules to cover NPs includ-ing adverbs (e.g.
partially-observable Markov de-cision process) or conjunctions (e.g.
behavioralevolution and extrapolation), as they are rare.4.3 Human Assigned ScoreWe hired four graduate students working in NLPto assign human scores to substrings in the gold-standard data.
The scores are between 0 and 4(0 means no semantic overlap between a NP andits substring, while 4 means semantically indistin-guishable).We broke down the candidate?keyphrases pairsinto subtypes, based on where the overlap oc-curs relative to the keyphrase (e.g.
ABCD): (1)Head: the candidate contains the head noun ofthe keyphrase (e.g.
CD); (2) First: the candi-date contains the first word of the keyphrase (e.g.AB); and (3) Middle: the candidate overlaps withthe keyphrase, but contains neither its first wordnor its head word (e.g.
BC).
The average humanscores are 1.94 and 2.11 for First and Head, re-spectively, when the candidate is shorter, whilethey are 2.00, 1.89 and 2.15 for First, Middle, andHead, respectively when the candidate is longer.Note that we did not have Middle instances withcandidates as the shorter string.
The scores areslightly higher for the keyphrases as substringsthan for the candidates as substrings.5 CorrelationTo check the feasibility of metrics for keyphraseevaluation, we checked the Spearman rank corre-lation between the machine-generated score andthe human-assigned score for each keyphrase?candidate pairing.As the percentage of annotators who agree onthe exact score is low (i.e.
2 subjects agree ex-576Human R-precision BLEU METEOR NIST ROUGE SemanticOrig Mod SimilarityAverageAll .4506 .4763 .2840 .3250 .3246 .3366 .3246 .2116L ?
4 .4510 .5264 .2806 .3242 .3238 .3369 .3240 .2050L ?
3 .4551 .4834 .2893 .3439 .3437 .3584 .3437 .1980MajorityAll .4603 .4763 .3438 .3407 .3403 .3514 .3404 .2224L ?
4 .4604 .5264 .3434 .3423 .3421 .3547 .3422 .2168L ?
3 .4638 .4838 .3547 .3679 .3675 .3820 .3676 .2123Table 3: Rank correlation between humans and the different evaluation metrics, based on the humanaverage (top half) and majority (bottom half)Human R-precision BLEU METEOR NIST ROUGEOrig ModLOCATIONFirst .5508 .5032 .5033 .3844 .3844 .4057 .3844Middle .5329 .5741 .5988 .4669 .4669 .4055 .4669Head .3783 .4838 .4838 .3865 .3860 .3780 .3864COMPLEXITYSimple .4452 .4715 .2790 .3653 .3445 .3527 .3445PP .4771 .4814 .1484 .3367 .3122 .3443 .3123CC .3645 .3810 .3140 .3748 .3446 .3384 .3748POS AdjN .4616 .4844 .3507 .3147 .3132 .3115 .3133NN .4467 .4586 .2581 .3321 .3321 .3488 .3322Table 4: Rank correlation between human average judgments and n-gram-based metricsactly on 55%-70% of instances, 3 subjects agreeexactly on 25%-35% of instances), we require amethod for combining the annotations.
We ex-periment with two combination methods: major-ity and average.
The majority is simply the labelwith the majority of annotations associated withit; in the case of a tie, we break the tie by select-ing that annotation which is closest to the median.The average is simply the average score across allannotators.5.1 Overall Correlation with Human ScoresTable 3 presents the correlations between the hu-man scores (acting as an upper bound for thetask), as well as those between human scoreswith machine-generated scores.
We first presentthe overall results, then results over the subset ofkeyphrases of length 4 words or less, and also 3words or less.
We present the results for the anno-tator average and majority in top and bottom half,respectively, of the table.To compute the correlation between the hu-man annotators, we used leave-one-out cross-validation, holding out one annotator, and com-paring them to the combination of the remainingannotators (using either the majority or averagemethod to combine the remaining annotations).This was repeated across all annotators, and theSpearman?s ?
was averaged across the annotators.Overall, we found that R-precision achieved thehighest correlation with humans, above the inter-annotator correlation in all instances.
That is,based on the evaluation methodology employed,it is performing slightly above the average levelof a single annotator.
The relatively low inter-annotator correlation is, no doubt, due to the dif-ficulty of the task, as all of our near-misses have2 or more terms, and the annotators have to makevery fine-grained, and ultimately subjective, deci-sions about the true quality of the candidate.Comparing the n-gram-based methods with thesemantic similarity-based method, the n-gram-based metrics achieved higher correlations acrossthe board, with BLEU, METEOR, NIST and ROUGEall performing remarkably consistently, but well577Human R-precision BLEU METEOR NIST ROUGEOrig ModLOCATIONFirst .5642 .5162 .5163 .4032 .4032 .4297 .4032Middle .5510 .4991 .5320 .4175 .4175 .3653 .4175Head .4147 .5073 .5074 .4156 .4153 .4042 .4156COMPLEXITYSimple .4580 .4869 .3394 .3653 .3651 .3715 .3651PP .4715 .5068 .3724 .3367 .3367 .3652 .3367CC .5777 .5513 .3841 .5745 .5571 .5600 .5745POS AdjN .4501 .4861 .3968 .3266 .3251 .3246 .3252NN .4631 .4733 .3244 .3499 .3499 .3648 .3500Table 5: Rank correlation between human majority and n-gram-based metricsbelow the level of R-precision.
Due to the markedlylower performance of the semantic similarity-based method, we do not consider it for the re-mainder of our experiments.
A general findingwas that as the length of the keyphrase (L) gotlonger, the correlation tended to be higher acrossall n-gram-based metrics.One disappointment at this stage is that the re-sults for modified R-precision are well below thoseof the original, especially over the average of thehuman annotators.5.2 Correlation with Different NP SubtypesTo get a clearer sense of how the different eval-uation metrics are performing, we broke downthe keyphrases according to three syntactic sub-classifications: (1) the location of overlap (seeSection 4.3); (2) the complexity of the NP (doesthe keyphrase contain a preposition [PP], a con-junction [CC] or neither a preposition nor a con-junction [Simple]?
); and (3) the word class se-quence of the keyphrase (is the keyphrase an NN[NN] or an AdjN sequence [AdjN]?).
We presentthe results in Tables 4 and Table 4 for the humanaverage and majority, respectively, presenting re-sults in boldface when the correlation for a givenmethod is higher than for that same method inour holistic evaluation in Table 3 (i.e.
.4506 and.4603, for the average and majority human scores,respectively).All methods, including inter-annotator correla-tion, improve in raw numbers over the subsetsof the data based on overlap location, indicatingthat the data was partitioned into more internally-consistent subsets.
Encouragingly, modified R-precision equalled or bettered the performance ofthe original R-precision over each subset of thedata based on overlap location.
Where modifiedR-precision appears to fall down most noticeablyis over keyphrases including prepositions, as ourassumption about the semantic import based onlinear ordering clearly breaks down in the face ofpost-modifying PPs.
It is also telling that it doesworse over noun?noun sequences than adjective?noun sequences.
In being agnostic to the effectsof syntax, the original R-precision appears to bene-fit overall.
Another interesting effect is that theperformance of BLEU, METEOR and ROUGE isnotably better over candidates which match withnon-initial and non-final words in the keyphrase.We conclude from this analysis that keyphrasescoring should be sensitive to overlap location.Furthermore, our study also shows that n-gram-based MT and summarization metrics are sur-prisingly adept at capturing partial matches inkeyphrases, despite them being much shorter thanthe strings they are standardly applied to.
Morecompellingly, we found that R-precision is the bestoverall performer, and that it matches the perfor-mance of our human annotators across the board.This is the first research to establish this fact.
Ourfindings for modified R-precision were more sober-ing, but its location sensitivity was shown to im-prove over R-precision for instances of overlap inthe middle or with the head of the keyphrase.5786 ConclusionIn this work, we have shown that preexisting n-gram-based evaluation metrics from MT, summa-rization and keyphrase extraction evaluation areable to handle the effects of near-misses, and thatR-precision performs at or above the average levelof a human annotator.
We have also shown thata semantic similarity-based method which usesweb data to model distributional similarity per-formed below the level of all of the n-gram-basedmethods, despite them requiring no external re-sources (web or otherwise).
We proposed a mod-ification to R-precision based on the location ofmatch, but found that while it could achieve betterperformance over certain classes of keyphrases,its net effect was to drag the performance of R-precision down.
Other methods were found to beremarkably consistent across different subtypes ofkeyphrase.AcknowledgementsMany thanks to the anonymous reviewers for theirinsightful comments.
We wish to acknowledgethe generous funding from National ResearchFoundation grant R 252-000-279-325 in support-ing Min-Yen Kan?s work.ReferencesAbhaya Agrwal and Alon Lavie.
METEOR, M-BLEU and M-TER: Evaluation Metrics for High-Correlation with Human Rankings of MachineTranslation Output.
In Proceedings of ACL Work-shop on Statistical Machine Translation.
2008.Ken Barker and Nadia Corrnacchia.
Using nounphrase heads to extract document keyphrases.
InProceedings of BCCSCSI : Advances in ArtificialIntelligence.
2000, pp.96?103.Regina Barzilay and Michael Elhadad.
Using lexi-cal chains for text summarization.
In Proceedingsof ACL/EACL Workshop on Intelligent Scalable TextSummarization.
1997, pp.
10?17.Chris Callison-Burch, Philipp Koehn, Christof Monzand Josh Schroeder.
Proceedings of 4th Workshopon Statistical Machine Translation.
2009.Ernesto D?Avanzo and Bernado Magnini.
A Key-phrase-Based Approach to Summarization: theLAKE System at DUC-2005.
In Proceedings ofDUC.
2005.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin and Craig G. Nevill-Manning.
DomainSpecific Keyphrase Extraction.
In Proceedings ofIJCAI.
1999, pp.668?673.Mario Jarmasz and Caroline Barriere.
Using semanticsimilarity over Tera-byte corpus, compute the per-formance of keyphrase extraction.
In Proceedingsof CLINE.
2004.Su Nam Kim, Olena Medelyan, Min-Yen Kan andTimothy Baldwin.
SemEval-2010 Task 5: Auto-matic Keyphrase Extraction from Scientific Arti-cles.
In Proceedings of SemEval-2: Evaluation Ex-ercises on Semantic Evaluation.
to appear.Dawn Lawrie, W. Bruce Croft and Arnold Rosenberg.Finding Topic Words for Hierarchical Summariza-tion.
In Proceedings of SIGIR.
2001, pp.
349?357.Chin-Yew Lin and Edward H. Hovy.
Automatic Eval-uation of Summaries Using N-gram Co-occurrenceStatistics.
In In Proceedings of HLT-NAACL.
2003.Alvin Martin and Mark Przybocki.
The 1999 NISTSpeaker Recognition Evaluation, Using SummedTwo-Channel Telephone Data for Speaker Detec-tion and Speaker Tracking.
In Proceedings of Eu-roSpeech.
1999.Yutaka Matsuo and Mitsuru Ishizuka.
Keyword Ex-traction from a Single Document using Word Co-occurrence Statistical Information.
InternationalJournal on Artificial Intelligence Tools.
2004,13(1), pp.
157?169.Olena Medelyan and Ian Witten.
Thesaurus basedautomatic keyphrase indexing.
In Proceedings ofACM/IEED-CS JCDL.
2006, pp.
296?297.Rada Mihalcea and Paul Tarau.
TextRank: BringingOrder into Texts.
In Proceedings of EMNLP 2004.2004, pp.
404?411.Guido Minnen, John Carroll and Darren Pearce.
Ap-plied morphological processing of English.
NLE.2001, 7(3), pp.
207?223.Thuy Dung Nguyen and Min-Yen Kan. Key phraseExtraction in Scientific Publications.
In Proceedingof ICADL.
2007, pp.
317?326.Sebastian Pado?, Michel Galley, Dan Jurafsky andChristopher D. Manning.
Textual Entailment Fea-tures for Machine Translation Evaluation.
In Pro-ceedings of ACL Workshop on Statistical MachineTranslation.
2009, pp.
37?41.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
BLEU: a method for automatic evalua-tion of machine translation.
In Proceedings of ACL.2001, pp.
311?318.579Youngja Park, Roy J. Byrd and Branimir Boguraev.Automatic Glossary Extraction Beyond Terminol-ogy Identification.
In Proceedings of COLING.2004, pp.
48?55.Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti Pollaand Timo Honkela.
A Language-Independent Ap-proach to Keyphrase Extraction and Evaluation.
InProceedings of COLING.
2008, pp.
83?86.Horacio Saggion, Dragomir Radev, Simon Teufel,Wai Lam and Stephanie Strassel.
Meta-evaluationof Summaries in a Cross-lingual Environment us-ing Content-based Metrics.
In Proceedings of COL-ING.
2002, pp.
1?7.Peter Turney.
Coherent keyphrase extraction via Webmining.
In Proceedings of IJCAI.
2003, pp.
434?439.Xiaojun Wan and Jianguo Xiao.
CollabRank: to-wards a collaborative approach to single-documentkeyphrase extraction.
In Proceedings of COLING.2008, pp.
969?976.Ian Witten, Gordon Paynter, Eibe Frank, Car Gutwinand Craig Nevill-Manning.
KEA:Practical Auto-matic Key phrase Extraction.
In Proceedings ofACM conference on Digital libraries.
1999, pp.254?256.Torsten Zesch and Iryna Gurevych.
ApproximateMatching for Evaluating Keyphrase Extraction.
In-ternational Conference on Recent Advances in Nat-ural Language Processing.
2009.580
