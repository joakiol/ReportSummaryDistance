Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1871?1881,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsPredicting the Relative Difficulty of Single SentencesWith and Without Surrounding ContextElliot SchumacherCarnegie Mellon Universityeschumac@cs.cmu.eduMaxine EskenaziCarnegie Mellon Universitymax@cs.cmu.eduGwen FrishkoffUniversity of Oregongfrishkoff@gmail.comKevyn Collins-ThompsonUniversity of Michigankevynct@umich.eduAbstractThe problem of accurately predicting rela-tive reading difficulty across a set of sen-tences arises in a number of important natu-ral language applications, such as finding andcurating effective usage examples for intelli-gent language tutoring systems.
Yet whilesignificant research has explored document-and passage-level reading difficulty, the spe-cial challenges involved in assessing aspectsof readability for single sentences have re-ceived much less attention, particularly whenconsidering the role of surrounding passages.We introduce and evaluate a novel approachfor estimating the relative reading difficulty ofa set of sentences, with and without surround-ing context.
Using different sets of lexical andgrammatical features, we explore models forpredicting pairwise relative difficulty using lo-gistic regression, and examine rankings gener-ated by aggregating pairwise difficulty labelsusing a Bayesian rating system to form a finalranking.
We also compare rankings derivedfor sentences assessed with and without con-text, and find that contextual features can helppredict differences in relative difficulty judg-ments across these two conditions.1 IntroductionThe reading difficulty, or readability, of a text isan estimate of linguistic complexity and is typicallybased on lexical and syntactic features, such as textlength, word frequency, and grammatical complex-ity (Collins-Thompson and Callan, 2004; Schwarmand Ostendorf, 2005; Kidwell et al, 2011; Kanungoand Orr, 2009).
Such estimates are often expressedas age- or grade-level measures and are useful fora range of educational and research applications.For example, instructors often wish to select storiesor books that are appropriately matched to studentgrade level.Many measures have been designed to calculatereadability at the document level (e.g., for webpages, articles, or books) (Collins-Thompson andCallan, 2004; Schwarm and Ostendorf, 2005), aswell as the paragraph or passage level (Kidwell etal., 2011; Kanungo and Orr, 2009).
However, muchless work has attempted to characterize the readabil-ity of single sentences (Pila?n et al, 2014).
Thisproblem is challenging because single sentencesprovide less data than is typically required for re-liable estimates, particularly for measures that relyon aggregate statistics.The absence of reliable single-sentence estimatespoints to a gap in natural language processing (NLP)research.
Single sentences are used in a variety ofexperimental and NLP applications: for example,in studies of reading comprehension.
Because read-ability estimates have been shown to predict a sub-stantial portion of variance in comprehension of dif-ferent texts, it would be useful to have measures ofsingle-sentence readability.
Thus, one aim of thecurrent study was to estimate the relative readabil-ity of single sentences with a high degree of accu-racy.
To our knowledge, general-purpose methodsfor computing such estimates for native language(L1) readers have not been developed, and thus ourgoal was to develop a method that would character-ize sentence-level difficulty for that group.The second aim is to compare the readability ofsingle sentences in isolation with the readability of1871these same sentences embedded in a larger context(e.g., paragraph, passage, or document).
When asingle sentence is extracted from a text, it is likely tocontain linguistic elements, such as anaphora (e.g.,?he?
or ?the man?
), that are semantically or syn-tactically dependent on surrounding context.
Notsurprisingly, sentences that contain these contextualdependencies take more effort to comprehend: ananaphoric noun phrase, or NP (e.g., ?he?
), automati-cally triggers the need to resolve reference, typicallyby understanding the link between the anaphor anda full NP from a previous sentence (e.g., ?John?
or?The man that I introduced you to at the party lastnight?
(Perfetti and Frishkoff, 2008).
In general,studies have shown a link between reading com-prehension and the presence of such cross-sentencerelationships in the text (McNamara, 2001; Lieder-holm et al, 2000; Voss and Silfies, 1996).
This im-plies that the very notion of readability at the sen-tence level may depend on context as well as word-and sentence-level features.
Therefore, it is impor-tant to compare readability estimates for single sen-tences that occur in isolation with those that occurwithin a larger passage, particularly if the target sen-tence contains coreferences, implied meanings, orother dependencies with its context.To address these aims, the present study first con-ducted two crowdsourcing experiments.
In the first,?sentence-only?
experiment, workers were asked tojudge which of two ?target?
sentences they thoughtwas more difficult.
In the second, ?sentence-in-passage?
experiment, another group of workers waspresented with the same target sentences that wereused in the first experiment.
However, in the secondexperiment, target sentences were embedded in theiroriginal contexts.Next, we analyzed these judgments of relativereadability for each condition (sentence-only ver-sus sentence-in-passage) by developing models forpredicting pairwise relative difficulty of sentences.These models used a rich representation of targetsentences based on a combination of lexical, syntac-tic, and discourse features.
Significant differenceswere found in readability judgments for sentenceswith and without their surrounding context.
Thisdemonstrates that discourse-level features (i.e., fea-tures related to coherence and cohesion) can affectthe readability of single sentences.2 Related WorkRecent approaches to estimating readability haveused a variety of linguistic features and predic-tion models (Collins-Thompson, 2014).
The Lex-ile Framework (Stenner, 1996) uses word frequencyestimates in a large corpus as a proxy for lexi-cal difficulty, and sentence length as a grammati-cal feature.
Methods based on statistical machinelearning, such as the reading difficulty measures de-veloped by Collins-Thompson and Callan (Collins-Thompson and Callan, 2004) and (Schwarm and Os-tendorf, 2005) used a feature set based on languagemodels.
Later work (Heilman et al, 2008) incorpo-rated grammatical features by parsing the sentencesin a text, and creating subtrees of one- to three-leveldepth as separate features.
Such features allow moredetailed, direct analysis of the sentence structure it-self instead of traditional proxies for syntactic com-plexity likes sentence length.
The linguistic featuresproposed in these works capture specific aspects oflanguage difficulty applied at the document level,whereas our work investigates the effectiveness ofthese feature types for characterizing aspects of dif-ficulty at the sentence level.Methods have been proposed to measure the read-ability of shorter portions of text (e.g.
typically lessthan 100 words), including sentences.
The approachmost similar to ours is the prediction of relative sen-tence difficulty (with associated readability ranking)for the deaf introduced by Inui et al (2001).
Thatwork focused on effective morphosyntactic featuresfor that target population with an SVM binary clas-sifier, whereas our approach (1) is intended for abroader population of L1 learners and thus exploresthe effectiveness of adding a rich, lexically-derivedfeature set, (2) uses a logistic regression model to es-timate class probabilities and interprets the results ofthat model, compared to applying an SVM withoutinterpretation to obtain a binary label, (3) examinesdifferences in predicting sentence difficulty both inand out of passage context, and (4) creates and uses anew dataset based on a crowdsourced approach, us-ing hundreds of non-experts to gather thousands ofpairwise preferences, compared to a questionnairedeployed to a small number of experts.
In other do-mains, a model was proposed to predict the read-ability of short web summaries in Kanungo and Orr2009.
In Kidwell et al (2011), , a set of Age of1872Acquisition estimates for individual words, repre-senting the lexical component of difficulty, was usedto predict the grade levels of passages.
Some ap-proaches have explored the classification of specificaspects of sentences, as opposed to reading difficultyclassification.
For example, (Pila?n et al, 2014) clas-sified individual sentences that would be understoodby second-language learners.
Another work (Kil-garriff et al, 2008) identified sentences that wouldbe good dictionary examples by looking for specificdesirable features.
Davenport et al 2014 used atraditional method of readability (Flesch-Kincaid),within the larger context of exploring relationshipsbetween the difficulty of tweets in a geographic areaand demographics.
Research in text simplificationhas applied sentence-level models of difficulty aspart of simplification-based optimization objectives.For example, Woodsend and Lapata (2011) use wordand syllable count as proxy features for sentence dif-ficulty when implicitly comparing different simpli-fied variants of a sentence.Other approaches have considered the relation-ship of reading difficulty to structures within in thewhole text.
These relationships can include the num-ber of coreferences present in a text.
Coh-Metrix(Graesser et al, 2011) measures text cohesiveness,accounting for both the reading difficulty of the textand other lexical and syntactic measures as well asa measure of prior knowledge needed for compre-hension, and the genre of the text.
Coh-Metrix usesco-reference detection as a factor in the cohesive-ness of a text, typically at the document or passagelevel.
Such cohesiveness factors account for the dif-ficulty of constructing the mental representation oftexts with more complex internal structure.
TextE-valuator (Sheehan et al, 2013; Sheehan et al, 2014)is designed to help educators select materials for in-struction.
The tool includes several components inits evaluation of text, including narrativity, style, andcohesion, beyond traditional difficulty and is againat the whole document level.
This approach illus-trates that the difficulty of a text relies on the rela-tionships within it.
This motivates the need to con-sider context when measuring difficulty.Generating reading difficulty rankings of longertexts from pairwise preferences has been performedin other contexts.
Tanaka-Ishii et al (2010) exploredan approach for sorting texts by readability based onpairwise preferences.
Later, Chen et al (2013) alsoproposed a model to obtain passage readability rank-ing by aggregating pairwise comparisons made bycrowdworkers.
In De Clercq et al(2014), pairwisejudgments of whole passages were obtained fromcrowdworkers and were found to give comparableresults in aggregate to those obtained from experts.A pairwise ranking of text readability was created inPitler and Nenkova (2008) in which readability wasdefined by subjective questions asked to the readerafter finishing the article, such as ?How well-writtenis this article??.
All of the above previous work wasfocused on ordering longer text passages, not singlesentences as we do here.Finally, research in the Machine Translation fieldhas explored pairwise prediction of the best transla-tion between two sentence options.
For example, inSong and Cohn (2011), a pairwise prediction modelwas built using n-gram precision and recall, as wellas function, content, and word counts.
However, un-like pairwise prediction of difficulty, the predictionis done with respect to a reference sentence, or setof reference sentences.3 Data Collection and ProcessingWe now describe methods used to create our datasetof sentences, to collect pairwise assessments of dif-ficulty, and to aggregate these pairwise preferencesinto a complete ranking.3.1 Data SetThe study sentences were drawn from a corpus com-bining the American National Corpus (Reppen etal., 2005), the New York Times Corpus (Sandhaus,2008), and the North American News Text Corpus(McClosky et al, 2008).
The domain of these cor-pora is largely news text, but also includes other top-ics, such as travel guides and other non-fiction.
In to-tal, this database contains 60,663,803 sentences thatserved as initial candidates.
Sentences were filteredout that didn?t include one of the 70 target wordsthat the third author selected for a study on teach-ing vocabulary to 8-14 year-old students.
Other sen-tences were removed based on length, keeping onlysentences of between 6 and 20 words.
Some sen-tences were removed due to the presence of one ormore rare words.
Finally, sentences were annotatedwith the surrounding document reading level, us-1873ing a lexical readability model (Collins-Thompsonand Callan, 2004).The data set gathered by (Collins-Thompson and Callan, 2004) was used in order toadd to the amount of lower-level reading material inthe collected corpora.With these sentences, two crowdsourced taskswere prepared to gather pairwise assessments of sen-tence reading difficulty.
In one task, the sentenceswere presented alone, outside of their original pas-sage context.
In the other task, the same sentenceswere presented within their original passage context.The objective was to generate two sets of pairwisecomparisons of the readability of a sentence.
In to-tal, 120 sentence pairs were used for the first taskand 120 passage pairs were used for the second.Each sentence was compared to five others, whichcreated 300 comparisons in each task.
The five sen-tences matched to each sentence were selected to en-sure that pairs with a range of document level differ-ences would be created.
Within each type of pair, arandom pair was selected.There were several constraints when generatingpairs for comparison.
To allow for sentences to betaken from documents with a range of reading lev-els, sentences were selected evenly from documentsat each reading level.
From the twelve standard U.S.grade levels used in readability, each document wasconsidered to be part of a bin consisting of two ad-jacent grade levels, such as grades 1 and 2, for ex-ample.
Sentences were selected evenly from thosebins.Each sentence needed sufficient context to ensurethere would be equivalent context for each item thatwould be compared, so only passages of sufficientsize were included.
To ensure passages were ofsimilar length, only passages that had between 136and 160 words were included.
Contexts having atleast two sentences before and after the sentencein question were strongly preferred.
Each selectedsentence was paired with one sentence from eachof the other grade level bins.
For example, a sen-tence from grade 1 would be paired with one sen-tence each from grade 3-4, 5-6, 7-8, 9-10, and 11-12.
Finally, each pair of sentences was presented inAB and BA order.
For each pair, there were sevenworker decisions.
There were 296 unique workersfor the sentence-only task, and 355 for the sentence-in-passage task.3.2 CrowdsourcingBoth of these tasks were carried out on the Crowd-flower platform.
The workers were first given in-structions for each task, which included a descrip-tion of the general purpose of the task.
In thesentence-only task, workers were asked to selectwhich of the two sentences was more difficult.
Inthe sentence-within-passage task, workers were sim-ilarly asked to decide which underlined sentencewas more difficult.
The instructions for the latter re-quested that the workers make their judgment onlyon the sentence, not on the whole context.
In bothtasks, there was an option for ?I don?t know or can?tdecide?.
The workers were asked to make their deci-sion based on the vocabulary and grammatical struc-ture of the sentences.
Finally, examples for each taskwere provided with explanations for each answer.For each task, at least 40 gold standard ques-tions were created from pairs of sentences that werejudged to be sufficiently distinct from one anotherso that they could easily be answered correctly.
Forthe sentence-in-passage task, several gold standardquestions were written to verify that the instruc-tions were being followed, since it was possible thata worker might judge the sentences based on thequality of the passage alone.
These gold examplesconsisted of an easier sentence in a difficult pas-sage compared with a difficult sentence within aneasy passage.
For each task, the worker saw threequestions, including one gold standard question.
Aworker needed to maintain an 85% accuracy ratingon gold standard questions to continue, and neededto spend at least 25 seconds per page, which con-tained 3 questions each.A weighted disagreement rate was calculated foreach worker.
If a worker?s response to a ques-tion differed from the most frequent answer to thatquestion, the percentage of agreement was countedagainst the worker.
If a worker, for the sentence-only task, had a disagreement rate (the weighteddisagreement penalty divided by the total questionsthey answered) of 15% or higher, their contributionwas removed from the data set (or 17% or higher forthe sentence in passage task).
The agreement for thesentence-in-passage task is lower than the sentence-only task (88.93% and 90.33% respectively), so thepermitted disagreement level is higher for that task.This resulted in the removal of 5.7% and 4.5% of1874pairwise judgments, respectively.
For each ques-tion, there was an optional text form to allow work-ers to submit feedback.
The sentence-only task paid11 cents per page, and the sentence-in-passage taskpaid 22 cents per page.3.3 Ranking GenerationEach task resulted in 4,200 pairwise preferencejudgments, excluding gold-standard answers.
Toaggregate these pairwise preferences into an over-all ranking of sentences, we use a simple, publiclyavailable approach evaluated by Chen et al as be-ing competitive with their own Crowd-BT aggre-gation method: the Microsoft Trueskill algorithm(Herbrich et al, 2007).
Trueskill is a Bayesian skillrating system that generalized the well-known Elorating system, in that it generates a ranking frompairwise decisions.
As Trueskill?s ranking algorithmdepends on the order in which the samples are pro-cessed, we report the ranking as an average of 50runs.The judgments were not aggregated for each com-parison.
Instead, each of the judgments was treatedindividually.
This allows Trueskill to consider thedegree of agreement between workers, since a sen-tence judgment that has high agreement reflects alarger difference in ranking than one that has loweragreement.
Each sentence was considered a player,and the winner between two, A or B, was the sen-tence considered most difficult.
If a worker chose?I don?t know or can?t tell?, it was considered adraw.
The prediction resulting in ?I don?t know orcan?t tell?
is rare; 2.2% of decisions in the sentenceonly task resulted in a draw, and 2.0% for sentenceswithin passages.
After processing each of the judg-ments, a rating can be built of sentences, rankedfrom least difficult to most difficult.
We can com-pare the resulting rankings for the sentence-only taskand the sentence-in-passage task to see the effect ofcontext on relative sentence difficulty.4 Modeling Pairwise Relative DifficultyOur first step in exploring relative difficulty order-ing for a set of sentences was to develop a modelthat could accurately predict relative difficulty for asingle pair of sentences, corresponding to the pair-wise judgements of relative difficulty we gatheredfrom the crowd.
We did this for both the sentence-only and the sentence-in-passage tasks.
In predict-ing a pairwise judgment for the sentence-only task,the model uses only the sentence texts.
In the modelfor the sentence-in-passage task, the Stanford De-terministic Coreference Resolution System (Raghu-nathan et al, 2010) is used to find coreference chainswithin the passage.
From these coreference chains,sentences with references to and from the target sen-tence can be identified.
If any additional sentencesare found, these are used in a separate feature setthat is included in the model; for all possible fea-tures, they are calculated for the target sentence, andseparately for the additional sentence set.Prior to training the final model, feature selec-tion was done on random splits of the training data.Training data was used to fit a Random Forest Clas-sifier, and based on the resulting classifier, the mostimportant variables were selected using sklearn?sfeature importance method.
The top 2% of the fea-tures (or 1% for the sentence-in-passage with coref-erence, since the feature set size is doubled) were se-lected automatically.
This resulted in a feature sizeof 40-50 features.
We implemented our models us-ing scikit-learn (Pedregosa et al, 2011) in Python.The resulting features were used to train a Logis-tic Regression model.
While other prediction mod-els such as Support Vector Machines have been ap-plied to relative readability prediction (Inui and Ya-mamoto, 2001), we chose Logistic Regression dueto its ability to provide estimates of class prob-abilities (which may be important for reliabilitywhen deploying a system that recommends high-quality items for learners), its connection to theRasch psychometric model used with reading as-sessments (Ehara et al, 2012), and the interpretablenature of the resulting parameter weights.
Since agiven feature has a value for sentence A and B, if afeature was selected for only Sentence A or B, thefeature for the other sentence was also added.
Weused the NLTK library (Bird et al, 2009) to tokenizethe sentence for feature processing.At the sentence level, the familiarity of the wordsis a significant factor to consider in any judgment ofdifficulty.
The grammatical structure of a sentenceis also important to consider: if the sentence usesa more familiar structure, it is likely to be consid-ered less difficult than a sentence with more unusualstructure.
We thus identified two groups of potential1875features: lexical and grammatical, described below.4.1 Lexical FeaturesFor lexical features, based partly on the work of(Song and Cohn 2011) we included the percentageof non-stop words (using NLTK list), the total num-ber of words and the total number of characters asfeatures.
We included the percentage of words inthe text found in the Revised Dale-Chall word list(Dale and Chall, 2000) to capture the presence ofmore difficult words in the sentence.Because sentences that contain rarer sequences ofwords are likely to be more difficult, and the likeli-hood of the sentence based on a large corpus shouldreflect this, we included the n-gram likelihood ofeach sentence, over each of 1-5 n-grams, as a fea-ture.
The Microsoft WebLM service (Wang et al,2010) was used to calculate the n-gram likelihood.In the field of psycholinguistics, Age of Acquisi-tion (AoA) refers to the age at which a word is firstlearned by a child.
A database of 51,715 words col-lected by (Kuperman et al, 2012) provides a rich re-source for use in reading difficulty measures.
Withthis dataset, we computed several additional fea-tures: the average, maximum, and standard devia-tion of the aggregated AoA for all words in a sen-tence that were present in the database.
Since thedata set alo includes the number of syllables in eachword, and as (Kincaid et al, 1975) proposes thatwords with more syllables are more difficult, we alsoincluded the average and maximum syllable count aspotential features.4.2 Syntactic FeaturesWe parsed each sentence in the data set using theBLLIP Parser (Charniak and Johnson, 2005), whichincludes a pre-trained model built on the Wall StreetJournal Corpus.
This provided both a syntactic treeand part of speech tags for the sentence.
As Part ofSpeech tagging is often used as a high-level linguis-tic feature, we computed percentages for each PoStag present, since the percentages might vary be-tween difficult sentences and easier sentences.
Thepercentage for each Part of Speech tag is defined asthe number of times a certain tag occurred, dividedby the total tags.
The diversity of part of speech tagswas used since this might vary between difficult andeasier sentences.Using the syntactic tree provided by the parser,we obtained the likelihood of the parse, and the like-lihood produced by the re-ranker, as syntactic fea-tures.
If a sentence parse has a comparatively highlikelihood, it is likely to be a more common struc-ture and thus more likely to be easier to read.
Thelength and height of the parse were also included asfeatures, since each of these could reflect the diffi-culty of the parse.
Including the entire parse of thesentence would create too much sparsity since syn-tactic parses vary highly from sentence to sentence.Therefore, as was done in (Heilman et al, 2008),subtrees of depth one to three were created from thesyntactic parse, and were added as features.
Thiscreates a smaller feature set, and one that can poten-tially model specific grammatical structures that areassociated with a specific level of difficulty.5 Pairwise Difficulty Prediction ResultsThe performance of the logistic regression modelstrained with different feature sets, for each task, isshown in Table 1.
We reported the mean and stan-dard deviation of the accuracy of each model over200 randomly selected training and testing splits.Each test set consisted of 20% of the data, and con-tained 60 aggregate pairs, all of which are sentences(24 in total) that were not present in the trainingdata.
The test sets for the sentence-in-passage andsentence-only task contain the same sentence pairs,but the individual judgements are different.For comparison, an oracle is included that repre-sents the accuracy a model would achieve if it madethe optimal prediction for each aggregate pair.
Dueto disagreement within the crowd, the oracle cannotreach 100% accuracy.
For example, for some pairA and B, if 10 workers selected A as the more diffi-cult sentence, and 4 workers selected B, the oracle?sprediction for that pair would be that that A is moredifficult.
The judgments of the four workers that se-lected B would be counted as inaccurate, since thefeature set is the same for the judgments with A andthe judgments with B.
Therefore, the oracle repre-sents the highest accuracy a model can achieve, con-sistent with the provided labels, using the featuresprovided.Examining the results in Table 1, we find the bestperforming configuration, Model B, used all featuresas candidates.
The exact number of features selected1876Sentence Only In Passage, With Coref In Passage, No CorefModel Acc.
S.D.
p-value Acc.
S.D.
p-value Acc.
S.D.
p-valueOracle (A) 90.13% 2.71% ?
87.81% 1.84% ?
87.81% 1.84% ?All Features (B) 84.69% 3.46% 0.01 ?
81.66% 3.17% 0.005 ?
81.91% 3.27% ?
0.04AoA + Parse L. (C) 84.33% 3.13% 0.001 ?
81.27% 3.93% 0.001 ?
80.84% 3.61% ?
0.001AoA (D) 79.62% 2.71% 0.001 ?
79.72% 2.86% 0.001 ?
78.99% 2.58% ?
0.001Strat.
Random 50.28% 1.68% ?
50.31% 2.01% ?
50.31% 2.01% ?Table 1: Mean and standard deviation of accuracy on 200 randomized samples of 20% held out data.
?With coref?
indicatescoreference features were used.
The arrow indicates which immediately adjacent accuracy result is used for p-value comparison,e.g.
Model B sentence-only is compared to model C sentence-only, and model B passage, no coref is compared to model B passage,with coref.varied depending on the task.
However, the simplestmodel, the Age of Acquisition model (D) consistingof the average, standard deviation, and maximumAoA features (sentence-only: 6 features, sentence-in-passage: 12 features) performed well, achievingover 78% accuracy on all tasks, showing that mostof the relative difficulty signal at the sentence levelcan be captured with a few lexical difficulty features.The Age of Acquisition + Parse Likelihood model(C) consists of all Age of Acquisition features, plusthe likelihood of the parse (sentence-only: 10 fea-tures, sentence-in-passage: 20 features)1.To assess the contribution of different features tothe model prediction, feature group importances arereported in Table 2.
As features for a given groupare often highly correlated with each other, such asin Age of Acquisition, the importance is calculatedfor feature groups.
Based on the method describedfor Model B, each feature group is removed fromconsideration in the model, and the resulting errorrate from Model B is used to calculate an importancemeasure.
The most important feature is normalizedto have a value of 1.0, with the rest being relative tothe difference in error rate from the original model,averaged across splits.These prediction results show that relative readingdifficulty can be predicted for sentence pairs withhigh accuracy, even with fairly simple feature sets.In particular, the results for AoA model D, whichuses a small number of targeted features, are com-petitive with the best model B that relies on a muchlarger feature set.
The addition of coreference fea-tures did result in small but significant changes in the1The p-value for each accuracy measurement compares itssignificance, using a paired t-test, to the neighboring modelin the direction of the arrow.
For example, the sentence-onlyModel B is compared to sentence-only Model A.Sentence Only Sentence in Passage (with Coref)Feature Imp.
Feature Imp.Age of Acq.
1.00 Age of Acq.
1.00Part of Speech 0.28 Syllables 0.27Syn.
Score 0.22 Part of Speech 0.23Syn.
Other 0.21 Syn.
Tree 0.18Syllables 0.19 Dale Chall 0.17Ngram L. 0.19 Content Word % 0.17Word Len.
0.17 Word Len.
0.16Dale Chall 0.16 Syn.
Other 0.16Content Word % 0.15 Syn.
Score 0.12Syn.
Tree 0.12 Ngram L. 0.10Table 2: Relative feature importance for Model B.
Feature im-portance is the increase in absolute error with a specific featuregroup removed, averaged across cross-validation folds used forTable 1, and normalized relative to the most informative fea-ture.
For Sentence in Passage, feature groups include corefer-ence features.ValueAvg.
Abs.
Diff 9.3Avg.
Abs.
Std Dev 7.7Pearson?s correlation 0.94*Spearman?s correlation 0.94*Table 3: Comparison of rankings generated with and withoutpassage.
Asterisk * indicates p < 0.0001.% Diff Pearson p-val.
Spearman p-val.Reranker -0.33 0.0002 -0.29 0.001Parser -0.33 0.0002 -0.28 0.002Table 4: Correlation between difference in rank and percentagedifference in features.accuracy of the sentence-in-passage task, althoughin one case the accuracy was reduced with corefer-ence features.6 Ranking ResultsUsing the pairwise aggregation method describedin Sec.
3.3, we ranked sentences by relative dif-ficulty for both sentence-only and sentence-in-passage tasks.
By observing how the overall rank or-1877Sentence Sentence-In-PassageAll Gold Only All Gold OnlyPearson Spearman Pearson Spearman Pearson Spearman Pearson SpearmanAoA Avg 0.6971 0.7151 0.7366 0.7598 0.7155 0.7356 0.6220 0.6482AoA Std Dev 0.6366 0.6596 0.7074 0.7385 0.6779 0.7023 0.5742 0.5825AoA Max 0.7084 0.6814 0.8036 0.7877 0.7408 0.7155 0.6215 0.6127Parser L. -0.4942 -0.5297 -0.4605* 0.4920 -0.4172 -0.4465 -0.5099 -0.5157Reranker L. -0.4923 -0.5280 -0.4574* -0.4751 -0.4139 -0.4450 -0.4969 -0.4879*Table 5: Sentence-Only and Sentence-In-Passage Ranking Correlation with Individual Features.
Gold indicates only gold-standardquestions were used to build ranking.
All correlations have p < 0.0001 except those with an asterisk *, which have p < 0.001.dering of sentences changes across these conditions,we can identify differences in how workers judgedthe relative difficulty of sentences with and withoutcontext.6.1 Rank DifferencesWe report differences in ranking in terms of meanand standard deviation of the absolute difference inrank index of each sentence across the two rank-ings, along with Pearson?s coefficient and Spear-man?s rank order coefficients.
Comparisons betweenthe rankings for each task are shown in Table 3.In comparing crowd-generated rankings for thesentence-only and sentence-in-passage task, the re-sults show a statistically significant aggregate dif-ference in how the crowd ranks sentence difficultywith and without the surrounding passage.
Whilethe correlation between the two rankings is high,and the average normalized change in rank posi-tion is 7.7%, multiple sentences exhibited a largechange in ranking.
For example, the sentence ?Asa result, the police had little incentive to make con-cessions.?
was ranked significantly easier when pre-sented out of context than when presented in context(rank change: -30 positions).
For that example, thesurrounding passage explained the complex politicalenvironment referred to indirectly in that sentence.6.2 Feature Correlation with Rank DifferencesTo examine why sentences may be ranked as moreor less difficult, depending on the context, we exam-ined the correlation between a sentence?s change inrank (Sentence-Only Ranking minus the Sentence-in-Passage ranking) and the normalized difference infeature values between the sentence representationand the remaining context representation.
We foundthat percentage change in parser and reranker like-lihoods had the most significant correlation (-0.33)with ranking change, as shown in Table 4.To interpret this result, note that the parser andreranker likelihood represent the probability theparser and reranker models assign to the syntacticparse produced by the sentence.
In other words, theyare a measure of how likely it is that the sentencestructure occurs, based on the model?s training data.If the difficulty of the sentence-in-passage is rankedhigher than the sentence alone, this correlates withthe target sentence having a syntactic structure withhigher likelihood than the average of the surround-ing sentence structures.
This means that if a sen-tence that has a frequently-seen syntactic structure isin a passage with sentences that have less commonstructures, the sentence within the passage is morelikely to be judged as more difficult.
The reverse isalso true: if a sentence that has a more unusual syn-tactic structure is in a passage with sentences withmore familiar structures, the sentence without thesurrounding passage is more likely to be ranked asmore difficult.We also examined the rank correlation of crowd-generated rankings with rankings produced by sort-ing sentences based on the value of individual fea-tures.
In addition to the full rankings, we con-structed a ranking produced only by the gold stan-dard examples, denoted Gold Only and included thisin the comparison.
The gold standard questions con-sist of examples constructed by the authors to have aclear relative difficulty result.
The rank correlationsare shown in Table 5 for both tasks.The reasons for discrepancies in relative diffi-culty assessment between the sentence-only andsentence-in-passage conditions require further ex-ploration.
While the correlation between the per-centage change in probability of the parse and thedifference in ranking is significant, it is not large.It does indicate that despite judges being explicitly1878CrowdPearson SpearmanExpert label 0.85 0.84Document-based label 0.70 0.70Table 6: Correlation between sentence readability labelsand crowd-generated ranking, for expert (sentence-level) anddocument-based labels (from document readability prediction).All correlations have p < 0.0001.told to only consider the sentence, the properties ofthe surrounding passage may indeed influence theperceived relative difficulty of the sentence.6.3 Review of DataThe pairwise prediction results indicate that a largeproportion of the crowdsourced pair orderings canbe decided using vocabulary features, due to thestrong performance of the Age of Acquisition fea-tures.
To identify the relative importance of vocab-ulary and syntax in our data, we reviewed each pairand judged whether the sentence?s syntax or vocab-ulary, or the combination of both, were needed tocorrectly predict the more difficult sentence.
Formany pairs, either syntax or vocabulary could beused to correctly predict the more difficult sentencesince each factor indicated the same sentence wasmore difficult.
We found that 19% of pairs had onlya vocabulary distinction, and 65% of pairs couldbe judged correctly either by vocabulary or syntax.Therefore, 84% of pairs could be judged using vo-cabulary, which explains the high performance ofthe Age of Acquisition features.The level of a sentence?s source document wasused as a proxy for the sentence?s grade level whenbuilding the pairs.
To build a sentence-level goldstandard for this dataset, we asked a teacher with aMaster of Education with a Reading Specialist fo-cus and 30 years of experience in elementary andhigh school reading instruction, to identify the gradelevel of each sentence.
This expert was asked to as-sign either a single grade level or a range of levels toeach of the 120 sentences.
From this, an expert rank-ing was created, using the midpoint of each expert-assigned range.
The correlation between the expertsentence ranking and the crowd ranking can be seenin Table 6, reinforcing the finding that crowdsourcedjudgments can provide an accurate ranking of diffi-culty (De Clercq et al, 2014).7 ConclusionUsing a rich sentence representation based on lex-ical and syntactic features leveraged from previouswork on document-level readability, we introducedand evaluated several models for predicting the rel-ative reading difficulty of single sentences, with andwithout surrounding context.
We found that whilethe best prediction performance was obtained by us-ing all feature classes, simpler representations basedon lexical features such as Age of Acquisition normswere effective.
The accuracy achieved by the bestprediction model came within 6% of the oracle ac-curacy for both tasks.Many of the features identified had a high correla-tion with the rankings produced by the crowd.
Thisindicates that these features can be used to build amodel of sentence difficulty.
With the rankings builtfrom crowdsourced judgments on sentence diffi-culty, small but significant differences were found inhow sentences are ranked with and without the sur-rounding passages.
This result suggests that prop-erties of the surrounding passage of a sentence canchange the perceived difficulty of a sentence.In future work, we plan to increase the numberof sentences in our data set, so that additional morefine-grained features might be considered.
For ex-ample, weights for lexical features could be moreaccurately estimated with more data.
Our use of thecrowd-based labels was intended to reduce noise inthe ranking analysis, but we also intend to use thepairwise predictions produced by the logistic modelas the input to the aggregation model, so that rank-ings can be obtained for previously unseen sentencesin operational settings.
Another goal is to obtain ab-solute difficulty labels for sentences by calibratingordinal ranges based on the relative ranking.
Finally,we are interested in the contribution of context in un-derstanding the meaning of an unknown word.AcknowledgmentsWe thank the anonymous reviewers for their sug-gestions, and Ann Schumacher for serving as gradelevel annotator.
This work was supported in part byDept.
of Education grant R305A140647 to the Uni-versity of Michigan.
Any opinions, findings, conclu-sions or recommendations expressed in this materialare the authors?, and do not necessarily reflect thoseof the sponsors.1879ReferencesSteven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural language processing with Python.
?
O?Reilly Me-dia, Inc.?.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.Xi Chen, Paul N Bennett, Kevyn Collins-Thompson, andEric Horvitz.
2013.
Pairwise ranking aggregation ina crowdsourced setting.
In Proceedings of the sixthACM international conference on Web search and datamining, pages 193?202.
ACM.Kevyn Collins-Thompson and James P Callan.
2004.
Alanguage modeling approach to predicting reading dif-ficulty.
In HLT-NAACL, pages 193?200.Kevyn Collins-Thompson.
2014.
Computational assess-ment of text readability: a survey of current and futureresearch.
International Journal of Applied Linguistics,165(2):97?135.Edgar Dale and Jeanne S Chall.
2000.
Readabilityrevisited: The new dale-chall readability formula.http://opi.mt.gov/Pub/RTI/Forms/School/Choteau/The\%20Dale-Chall\%20Word\%20List.doc.
Accessed: 2016-5-10.James R. A. Davenport and Robert DeLine.
2014.
Thereadability of tweets and their geographic correlationwith education.
CoRR, abs/1401.6058.Orphe?e De Clercq, Veronique Hoste, Bart Desmet, PhilipVan Oosten, Martine De Cock, and Lieve Macken.2014.
Using the crowd for readability prediction.
Nat-ural Language Engineering, 20(03):293?325.Yo Ehara, Issei Sato, Hidekazu Oiwa, and Hiroshi Nak-agawa.
2012.
Mining words in the minds of sec-ond language learners: Learner-specific word diffi-culty.
In COLING 2012, 24th International Confer-ence on Computational Linguistics, Proceedings of theConference: Technical Papers, 8-15 December 2012,Mumbai, India, pages 799?814.Arthur C Graesser, Danielle S McNamara, and Jonna MKulikowich.
2011.
Coh-metrix providing multi-level analyses of text characteristics.
Educational Re-searcher, 40(5):223?234.Michael Heilman, Kevyn Collins-Thompson, and Max-ine Eskenazi.
2008.
An analysis of statistical mod-els and features for reading difficulty prediction.
InProceedings of the Third Workshop on Innovative Useof NLP for Building Educational Applications, EANL?08, pages 71?79, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ralf Herbrich, Tom Minka, and Thore Graepel.
2007.Trueskill(tm): A bayesian skill rating system.
In Ad-vances in Neural Information Processing Systems 20,pages 569?576.
MIT Press, January.Kentaro Inui and Satomi Yamamoto.
2001.
Corpus-based acquisition of sentence readability ranking mod-els for deaf people.
In Proceedings of the Sixth Nat-ural Language Processing Pacific Rim Symposium,November 27-30, 2001, Hitotsubashi Memorial Hall,National Center of Sciences, Tokyo, Japan, pages 159?166.Tapas Kanungo and David Orr.
2009.
Predicting thereadability of short web summaries.
In Proceedingsof the Second ACM International Conference on WebSearch and Data Mining, pages 202?211.
ACM.Paul Kidwell, Guy Lebanon, and Kevyn Collins-Thompson.
2011.
Statistical estimation of wordacquisition with application to readability predic-tion.
Journal of the American Statistical Association,106(493):21?30.Adam Kilgarriff, Milos Husa?k, Katy McAdam, MichaelRundell, and Pavel Rychly`.
2008.
Gdex: Automat-ically finding good dictionary examples in a corpus.In Proceedings of the XIII EURALEX InternationalCongress (Barcelona, 15-19 July 2008), pages 425?432.J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers,and Brad S Chissom.
1975.
Derivation of newreadability formulas (automated readability index, fogcount and flesch reading ease formula) for navy en-listed personnel.
Technical report, DTIC Document.Victor Kuperman, Hans Stadthagen-Gonzalez, and MarcBrysbaert.
2012.
Age-of-acquisition ratings for30,000 english words.
Behavior Research Methods,44(4):978?990.Tracy Liederholm, Michelle Gaddy Everson, Paulvan den Broek, Maureen Mischinski, Alex Crittenden,and Jay Samuels.
2000.
Effects of causal text revi-sions on more-and less-skilled readers?
comprehensionof easy and difficult texts.
Cognition and Instruction,pages 525?556.David McClosky, Eugene Charniak, and Mark Johnson.2008.
Bllip north american news text, complete.
Lin-guistic Data Consortium.Danielle S McNamara.
2001.
Reading both high-coherence and low-coherence texts: Effects of text se-quence and prior knowledge.
Canadian Journal ofExperimental Psychology/Revue canadienne de psy-chologie expe?rimentale, 55(1):51.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.1880Journal of Machine Learning Research, 12:2825?2830.Charles Perfetti and Gwen A. Frishkoff.
2008.
Theneural bases of text and discourse processing.
InB.
Stemmer and H. A. Whitaker (Eds.)
Handbook ofthe Neuroscience of Language, pages 165?174.
Cam-bridge:MA, Elsevier.Ildiko?
Pila?n, Elena Volodina, and Richard Johansson.2014.
Rule-based and machine learning approachesfor second language sentence-level readability.
In Pro-ceedings of the Ninth Workshop on Innovative Use ofNLP for Building Educational Applications.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: A unified framework for predicting text qual-ity.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 186?195, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 492?501.
Associationfor Computational Linguistics.Randi Reppen, Nancy Ide, and Keith Suderman.
2005.American national corpus (anc) second release.
Lin-guistic Data Consortium.Evan Sandhaus.
2008.
The new york times annotatedcorpus.
Linguistic Data Consortium, Philadelphia,6(12):e26752.Sarah E Schwarm and Mari Ostendorf.
2005.
Read-ing level assessment using support vector machinesand statistical language models.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 523?530.
Association forComputational Linguistics.Kathleen M Sheehan, Michael Flor, and Diane Napoli-tano.
2013.
A two-stage approach for generating un-biased estimates of text complexity.
In Proceedingsof the Workshop on Natural Language Processing forImproving Textual Accessibility, pages 49?58.Kathleen M Sheehan, Irene Kostin, Diane Napolitano,and Michael Flor.
2014.
The textevaluator tool.
TheElementary School Journal, 115(2):184?209.Xingyi Song and Trevor Cohn.
2011.
Regression andranking based optimisation for sentence level machinetranslation evaluation.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages123?129.
Association for Computational Linguistics.A Jackson Stenner.
1996.
Measuring reading compre-hension with the lexile framework.Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-ada.
2010.
Sorting texts by readability.
Comput.
Lin-guist., 36(2):203?227, June.James Voss and Laurie Silfies.
1996.
Learning from his-tory text: The interaction of knowledge and compre-hension skill with text structure.
Cognition and In-struction, 14(1):45?68.Kuansan Wang, Christopher Thrasher, Evelyne Viegas,Xiaolong Li, and Paul Hsu.
2010.
An overview ofmicrosoft web n-gram corpus and applications.
June.Kristian Woodsend and Mirella Lapata.
2011.
Learningto simplify sentences with quasi-synchronous gram-mar and integer programming.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 409?420,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.1881
