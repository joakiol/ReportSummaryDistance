Learning theories from textMaria LIAKATA and Stephen PULMANCentre for Linguistics and PhilologyWalton StreetUniversity of OxfordU.K.,maria.liakata@clg.ox.ac.uk, stephen.pulman@clg.ox.ac.ukAbstractIn this paper we describe a method of automati-cally learning domain theories from parsed cor-pora of sentences from the relevant domain anduse FSA techniques for the graphical represen-tation of such a theory.
By a ?domain theory?
wemean a collection of facts and generalisationsor rules which capture what commonly happens(or does not happen) in some domain of interest.As language users, we implicitly draw on suchtheories in various disambiguation tasks, suchas anaphora resolution and prepositional phraseattachment, and formal encodings of domaintheories can be used for this purpose in naturallanguage processing.
They may also be objectsof interest in their own right, that is, as the out-put of a knowledge discovery process.
The ap-proach is generizable to different domains pro-vided it is possible to get logical forms for thetext in the domain.1 IntroductionIt is an old observation that in order to choose thecorrect reading of an ambiguous sentence, we needa great deal of knowledge about the world.
How-ever, the observation that disambiguation decisionsdepend on knowledge of the world can be made tocut both ways: just as we need a lot of knowledgeof the world to make disambiguation decisions, so agiven disambiguation decision can be interpreted astelling us a lot about the way we view the structureof the world.
Our method for inducing domain theo-ries relies on this inversion, since in the general caseit is a much easier job to disambiguate sentencesthan to directly encode the theory that we are draw-ing on in so doing.
Our strategy for trying to build adomain theory is to try to capitalise on the informa-tion that is tacitly contained in those disambiguationdecisions.2 Some background(Pulman, 2000) showed that it was possible to learna simple domain theory from a disambiguated cor-pus: a subset of the ATIS (air travel information ser-vice) corpus (Doddington and Godfrey, 1990).
Am-biguous sentences were annotated as shown to indi-cate the preferred reading:[i,would,like,[the,cheapest,flight,from,washington,to,atlanta]][do,they,[serve,a,meal],on,[the,flight,from,san_francisco,to,atlanta]][i,would,like,[a,flight,from,boston,to,san_francisco,[that,leaves,before,?8:00?
]]]The ?good?
and the ?bad?
parses were used to pro-duce simplified first order logical forms represent-ing the semantic content of the various readings ofthe sentences.
The ?good?
readings were used aspositive evidence, and the ?bad?
readings (or moreaccurately, the bad parts of some of the readings)were used as negative evidence.
Next a particu-lar Inductive Logic Programming algorithm, Pro-gol (Muggleton, 1995), was used to learn a theoryof prepositional relations in this domain: i.e.
whatkinds of entities can be in these relations, and whichcannot:on(+any,+any)from(+any,+any)to(+any,+any)at(+any,+any)The +any declaration says that there are no priorassumptions about sortal restrictions on thesepredicates.
Among others generalisations like thefollowing were obtained (all variables are implicitlyuniversally quantified):fare(A) ?
airline(B) ?
on(A,B)meal(A) ?
flight(B) ?
on(A,B)flight(A) ?
day(B) ?
on(A,B)flight(A) ?
airline(B) ?
on(A,B)This domain theory was then used successfullyin disambiguating a small held-out section of thecorpus, by checking for consistency between logi-cal forms and domain theories.While the numbers of sentences involved in thatexperiment were too small for the results to be sta-tistically meaningful, the experiment proved that themethod works in principle, although of course in re-ality the notion of logical consistency is too stronga test in many cases.
Note also that the results ofthe theory induction process are perfectly compre-hensible - the outcome is a theory with some logicalstructure, rather than a black box.The method requires a fully parsed corpus withcorresponding logical forms.
Using a similar tech-nique, we have experimented with slightly largerdatasets, using the Penn Tree Bank (Marcus et al,1994) since the syntactic annotations for sentencesgiven there are intended to be complete enough forsemantic interpretation, in principle, at least.In practice, (Liakata and Pulman, 2002) report,it is by no means easy to do this.
It is possible torecover partial logical forms from a large propor-tion of the treebank, but these are not complete oraccurate enough to simply replicate the ATIS exper-iment.
In the work reported here, we selected about40 texts containing the verb ?resign?, all reporting,among other things, ?company succession?
events, ascenario familiar from the Message UnderstandingConference (MUC) task (Grishman and Sundheim,1995).
The texts amounted to almost 4000 wordsin all.
Then we corrected and completed someautomatically produced logical forms by hand toget a fairly full representation of the meaningsof these texts (as far as is possible in first orderlogic).
We also resolved by hand some of thesimpler forms of anaphoric reference to individualsto simulate a fuller discourse processing of the texts.To give an example, a sequence of sentences like:J.P. Bolduc, vice chairman of W.R. Grace & Co.(...) was elected a director.
He succeeds TerrenceD.
Daniels,... who resigned.was represented by the following sequence of liter-als:verb(e1,elect).funct_of(?J.P._Bolduc?,x1)....subj(e1,unspecified).obj(e1,x1).description(e1,x1,director,de1).verb(e5,succeed).subj(e5,x1).funct_of(?Terrence_D._Daniels?,x6).obj(e5,x6).verb(e4,resign).subj(e4,x6).The representation is a little opaque, for variousimplementation reasons.
It can be paraphrased asfollows: there is an event, e1, of electing, the sub-ject of which is unspecified, and the object of whichis x1.
x1 is characterised as ?J P Bolduc?, and e1 as-signs the description de1 of ?director?
to x1.
Thereis an event e5 of succeeding, and x1 is the subject ofthat event.
The object of e5 is x6, which is charac-terised as Terrence D Daniels.
There is an event e4of resigning and the subject of that event is x6.The reason for all this logical circumlocution isthat we are trying to learn a theory of the ?verb?predicate, in particular we are interested in relationsbetween the arguments of different verbs, sincethese may well be indicative of causal or other reg-ularities that should be captured in the theory ofthe company succession domain.
If the individualverbs were represented as predicates rather than ar-guments of a ?verb?
predicate we would not be ableto generalise over them: we are restricted to first or-der logic, and this would require higher order vari-ables.We also need to add some background knowl-edge.
We assume a fairly simple flat ontology soas to be able to reuse existing resources.
Some en-tities were assigned to classes automatically usingclustering techniques others had to be done by hand.The set of categories used were:company, financial instrument, financial transac-tion, location, money, number, person, companyposition, product, time, and unit (of organisation).As before, the representation has these categories asan argument of a ?class?
predicate to enable gener-alisation:class(person,x1).class(company,x3).etc.Ideally, to narrow down the hypothesis space forILP, we need some negative evidence.
But in thePenn Tree Bank, only the good parse is represented.There are several possible ways of obtaining nega-tive data, of course: one could use a parser trainedon the Tree Bank to reparse sentences and recoverall the parses.
However, there still remains the prob-lem of recovering logical forms from ?bad?
parses.An alternative would be to use a kind of ?closedworld?
assumption: take the set of predicates and ar-guments in the good logical forms, and assume thatany combination not observed is actually impossi-ble.
One could generate artificial negative evidencethis way.Alternatively, one can try learning from positiveonly data.
The ILP systems Progol (Muggleton,1995) and Aleph (Srinivasan, 1999) are able to learnfrom positive only data, with the appropriate set-tings.
Likewise, so-called ?descriptive?
ILP systemslike WARMR (DeHaspe, 1998) do not always neednegative data: they are in effect data mining en-gines for first order logic, learning generalisationsand correlations in some set of data.3 Domain Theory for CompanySuccession EventsWe found that the most successful method, giventhe absence of negative data, was to use WARMRto learn association rules from the positive data.
Aswith all types of association rule learning, WARMRproduces a huge number of rules, of varying degreesof coverage.
We spent some time writing filters tonarrow down the output to something useful.
Suchfilters consist of constraints ruling out patterns thatare definitely not useful, for example patterns con-taining a verb but no arguments or attributes.
Anexample of such a restriction is provided below:pattern_constraint(Patt):-member(verb(_,E,_A,_,_),Patt),(member(attr(_,E,Attr),Patt)->\+constraint_on_attr(Patt,Attr)).If pattern constraint/1 succeeds for a patternPatt, then Patt is discarded.
Basically, this says thata rule isn?t useful unless it contains a verb and oneof its attributes that satisfies a certain constraint.
Aconstraint might be of the following form:constraint_on_attr(Patt, Attr) :-member(class(_,Attr), Patt).The above states that there should be a classifi-cation of the attribute Attr present in the rule.
Auseful pattern Patt will satisfy such constraints.Some of the filtered output, represented in a morereadable form compatible with the examples aboveare as follows (note that the first argument of theverb/2 predicate refers to an event):Companies report financial transactions:subj(B,C) ?
obj(B,D)?class(fin tran,D) ?
class(company,C) ?verb(B, report)Companies acquire companies:subj(B,C) ?
obj(B,D) ?
class(company,D) ?class(company,C) ?
verb(B, acquire)Companies are based in locations:obj(A,C) ?
class(company,C) ?
in(A,D) ?class(location,D) ?
verb(A, base)If a person is elected, another person resigns:verb(H, elect) ?
obj(H, I) ?
class(person, I)?subj(C,L) ?
class(person,L) ?verb(C, resign)If person C succeeds person E, then someone haselected person C:obj(A,C) ?
class(person,C)?verb(D, succeed)?subj(D,C) ?obj(D,E)?class(person,E) ?
verb(A, elect)If someone elects person C, and person D resigns,then C succeeds D:subj(G,C) ?
verb(A, elect) ?
obj(A,C) ?class(person,C) ?
verb(E, resign)?subj(E,D) ?
class(person,D) ?verb(G, succeed)While there are many other rules learned that areless informative than this, the samples given hereare true generalisations about the type of events de-scribed in these texts: unremarkable, perhaps, butcharacteristic of the domain.
It is noteworthy thatsome of them at least are very reminiscent of thekind of templates constructed for Information Ex-traction in this domain, suggesting a possible furtheruse for the methods of theory induction describedhere.4 Learning weighted finite state automataWhile this experiment was reasonably successful, inthat we were able to induce plausible looking do-main generalisations, the process of selecting thesefrom the output of WARMR requires further super-vision of the learning process.
We therefore tried todevise a method of taking the output directly fromWARMR and processing it in order to automaticallyproduce domain knowledge.
Presenting the data asweighted FSAs serves the twofold purpose of re-ducing the amount of rules output from WARMR,thanks to minimization techniques, while providinga more visualisable representation.
Weighted FSAscan also be seen as a simple kind of probabilisticgraphical model.
We intend to go on to producemore complex models of this type like BayesianNetworks, which are easier to use in a more ro-bust setting, e.g.
for disambiguation purposes, thanthe traditional symbolic knowledge representationmethods presupposed so far.Before explaining the conversion to FSAs we lookin more detail at the representation of the WARMRoutput.5 Representing WARMR OutputEach of the numerous patterns resulting fromWARMR consists of a list of frequently associatedpredicates, found in the flat quasi-logical forms ofthe input sentences.
An example of such a pattern isprovided by the following:freq(6,[verb(A,B,elect,p,d),verb(C,D,succeed,p,d),attr(subj,B,unspecified),attr(obj,D,E),class(cperson,E),attr(subj,D,F),class(cperson,F),attr(obj,B,F)],0.1463).The first argument of the predicate freq/3 showsthe level of the algorithm at which the pattern/querywas acquired (DeHaspe, 1998).
The fact that thepattern was acquired at the sixth level means it wascreated during the sixth iteration of the algorithmtrying to satisfy the constraints input as settings tothe system.
This pattern satisfied four constraints,two of them twice.
The second argument of freq/3is the query itself and the third is its frequency.What is meant by frequency of the query in this in-stance is the number of times it succeeds (i.e.
thenumber of training examples it subsumes), dividedby the number of training examples.
To illustratethe meaning of such a pattern one needs to recon-struct the predicate-argument structures while main-taining the flat format.
Thus, the above pattern isconverted to the following:list(529,0.1463,[elect(A,B,C),cperson(C),succeed(D,C,E),cperson(E)]).It is now easier to understand the pattern as :?Aperson C who is elected succeeds a person E?.
How-ever, it is still not straightforward how one can eval-uate the usefulness of such patterns or indeed howone can incorporate the information they carry intoa system for disambiguation or reasoning.
Thisproblem is further aggravated by the large numberof patterns produced.
Even after employing filtersto discard patterns of little use, for example onescontaining a verb but no classification of its argu-ments, over 26,000 of them were obtained.
Thisis because many of the patterns are overly general:the training set consists of only 372 verb predicatesand a total of 436 clauses.
Such overgeneration is awell known problem of data mining algorithms andrequires sound criteria for filtering and evaluation.Most of the patterns generated are in fact variants ofa much smaller group of patterns.
The question thenarises of how it is possible to merge them so as toobtain a small number of core patterns, representa-tive of the knowledge obtained from the training set.Representing the patterns in a more compact formatalso facilitates evaluation either by a human expertor through incorporation into a pre-existing systemto measure improvement in performance.6 FSA conversionGiven the large amount of shared information inthese outputs, we decided to try to represent it asa set of Finite State Automata, where each transi-tion corresponds to a literal in the original clauses.Since all the literals in the raw output are simplyconjoined, the interpretation of a transition is sim-ply that if one literal is true, the next one is alsolikely to be true.
Our aim was to be able to usestandard FSA minimisation and determination algo-rithms (Aho et al, 1986),(Aho et al, 1974) to re-duce the large set of overlapping clauses to some-thing manageable and visualisable, and to be able touse the frequency information given by WARMR asthe basis for the calculation of weights or probabil-ities on transitions.To convert our patterns into FSAs (and in partic-ular recognizers), we used the package FSA Utili-ties (version FSA6.2.6.5)(van Noord, 2002), whichincludes modules for compiling regular expressionsinto automata (recognizers and transducers) by im-plementing different versions of minimisation anddeterminisation algorithms.
The package also al-lows operations for manipulating automata and reg-ular expressions such as composition, complemen-tation etc.
As the FSA Utilities modules apply toautomata or their equivalent regular expressions, thetask required converting the patterns into regular ex-pressions.
To do this we treat each literal as a sym-bol.
This means each verb and attribute predicatewith its respective arguments is taken to denote asingle symbol.
The literals are implicitly conjoinedand thus ordering does not matter.
Thus we choseto impose an ordering on patterns, whereby themain verb appears first, followed by predicates re-ferring to its arguments.
Any other verbs come next,followed by predicates describing their arguments.This ordering has the advantage over alphanumericordering that it allows filtering out alphabetic vari-ants of patterns where the predicates referring to thearguments of a verb precede the verb and the vari-ables are thus given different names which resultsin different literals.
This ordering on patterns isuseful as it allows common prefixes to be mergedduring minimisation.
Since variable names play animportant role in providing co-indexation betweenthe argument of a verb and a property of that argu-ment, designated by another predicate, terms suchas ?elect(A,B,C)?
and ?elect(D,E,F )?
are con-sidered to be different symbols.
Thus a pattern like:list(768,0.07,[elect(A,B,C),cperson(C),chairman(C,D),old(C,E,F),of(D,G),ccompany(G)]).was converted to the regular expression:macro(x768,[?elect(A,B,C)?,?cperson(C)?,?chairman(C,D)?,?old(C,E,F)?,?of(D,G)?,?ccompany(G)?
]).The first argument of the macro/2 pred-icate is the name of the regular expressionwhereas the second argument states that theregular expression is a sequence of the symbols?elect(A,B,C)?,?cperson(C)?,?chairman(C,D)?
andso on.
Finally, the entire WARMR output can becompiled into an FSA as the regular expressionwhich is the union of all expressions named viaan xnumber identifier.
This is equivalent to sayingthat a pattern can be any of the xnumber patternsdefined.We took all the patterns containing ?elect?
as themain verb and transformed them to regular expres-sions, all of which started with ?elect(A,B,C)?.
Wethen applied determinisation and minimisation tothe union of these regular expressions.
The resultwas an automaton of 350 states and 839 transitions,compared to an initial 2907 patterns.However, an automaton this size is still very hardto visualize.
To circumvent this problem we madeuse of the properties of automata and decomposedthe regular expressions into subexpressions thatcan then be conjoined to form the bigger picture.Patterns containing two and three verbs werewritten in separate files and each entry in the fileswas split into two or three different segments, sothat each segment contained only one verb andpredicates referring to its arguments.
Therefore, anexpression such as:macro(x774,[elect(A,B,C),cperson(C),resign(D,E,F),cperson(E),succeed(G,C,E)]).was transformed into:macro(x774a,[?elect(A,B,C)?,?cperson(C)?]).macro(x774b,[?resign(D,E,F)?,?cperson(E)?]).macro(x774c,[?succeed(G,C,E)?
]).One can then define the automaton xpression1,consisting of the union of all first segment expres-sions, such as x774a, the automaton resign2, con-isting of all expressions where resign is the sec-ond verb and succeed3.
The previous can be com-bined to form the automata [xpression1, resign2]or [xpression1, resign2, succeed3] and so on.
Theautomaton [xpression1, resign2] which represents292 patterns, has 32 states and 105 transitions andis much more manageable.7 Adding weightsThe FSA rules derived from the WARMR patternswould be of more interest if weights were assignedto each transition, indicating the likelihood of anyspecific path/pattern occurring.
For this we neededto obtain weights, equivalent to probabilities foreach predicate-argument term.
Such informationwas not readily available to us.
The only statisticswe have correspond to the frequency of each entirepattern, which is defined as:Freq = number of times the pattern matched the training datanumber of examples in the training setWe took this frequency measure as the proba-bility of patterns consisting of single predicates(e.g.
?elect(A,B,C)?, which is equivalent to ?Belects C?)
whereas the probabilities of all otherpattern constituents have to be conditioned on theprobabilities of terms preceding them.
Thus, theprobability of ?cperson(C)?, given ?elect(A,B,C)?
isdefined by the following:P (?cperson(C)?|?elect(A,B,C)?)
=P (?elect(A,B,C)?,?cperson(C)?
)P (?elect(A,B,C?
)where P (?elect(A,B,C)?,?
cperson(C))?is the frequency of the pattern[?elect(A,B,C)?,?
cperson(C)?]
andP (?elect(A,B,C)?)
is defined as:P (?elect(A,B,C)?)
=?X P (?elect(A,B,C)?, X)That is, the probability of P (?elect(A,B,C)?)
isthe sum of all the probabilities of the patterns thatcontain ?elect(A,B,C)?
followed by another predi-cate.
If such patterns didn?t exist, in which case thesum would be equal to zero, the probability wouldbe just the frequency of the pattern ?elect(A,B,C)?.In principle the frequency ratios described aboveare probabilities but in practice, because of thesize of the dataset, they may not approximatereal probabilities.
Either way they are still validquantities for comparing the likelihood of differentpaths in the FSA.Having computed the conditional probabili-ties/weights for all patterns and constituents,we normalized the distribution by dividing eachprobability in a distribution by the total sum of theprobabilities.
This was necessary in order to makeup for discarded alphabetic variants of patterns.We then verified that the probabilities summed upto 1.
To visualise some of the FSAs (weightedrecognizers) we rounded the weights to the seconddecimal digits and performed determinization andminimization as before.
Rules obtained can befound in Figures 1 and 2 (see figures on last page):The automaton of Figure 1 incorporates the fol-lowing rules:1.
?If a person C is elected, another person E hasresigned and C succeeds E?2.
?If a person C is elected director then anotherperson F has resigned and C succeeds F?3.
?If a person C is elected and another person Epursues (other interests) C succeeds E?The automaton of Figure 2 provides for rules suchas:?If a person is elected chairman of a company E thenC succeeds another person G?.At each stage, thanks to the weights, it is possi-ble to see which permutation of the pattern is morelikely.8 Related WorkRules such as the above express causality and in-terdependence between semantic predicates, whichcan be used to infer information for various linguis-tic applications.
The idea of deriving inference rulesfrom text has been pursued in (Lin and Pantel, 2001)as well, but that approach differs significantly fromthe current one in that it is aimed mainly at discover-ing paraphrases.
In their approach text is parsed intopaths, where each path corresponds to predicate ar-gument relations and rules are derived by comput-ing similarity between paths.
A rule in this caseconstitutes an association between similar paths.This is quite different to the work currently pre-sented, which provides more long range causalityrelations between different predicates, which maynot even occur in adjacent sentences in the origi-nal texts.
Other approaches such as (Collin et al,2002) also aim to learn paraphrases for improving aQuestion-Answering system.
Our work is perhapsmore closely related to the production of causal net-works as in (Subramani and Cooper, 1999), wherethe goal is to learn interdependency relations ofmedical conditions and diseases.
In their work thedependencies only involve key words, but we be-lieve that our techniques could be applied to similarbiomedical domains to discover causal theories withricher inferential structure.9 Conclusions & Future WorkWe have shown that it is possible to induce logicallystructured inference rules from parsed text.
We havealso shown that by using FSA techniques it is pos-sible to construct a weighted automaton for the rep-resentation of rules/patterns generated via a knowl-edge mining process.
This enables merging togetherpermutations of the same pattern and facilitates hu-man evaluation of the pattern.
Furthermore, the factthat we have learned what is in effect a simple prob-abilistic graphical model means that we can nowproduce representations of this knowledge suitablefor more robust inference methods of the type thatwe can deploy to aid reasoning and disambiguationtasks.10 AcknowledgementsWe would particularly like to thank Ashwin Srini-vasan (IBM, New Delhi), Steve Moyle (Oxford),and James Cussens (York) for their help with Alephand Jan Struyf, Hendrik Blockeel and Jan Ra-mon (K.U.
Leuven), for their generous help withWARMR.ReferencesA.H.
Aho, J.E.
Hopcroft, and J.D.
Ullman.
1974.The Design and Analysis of Computer Algo-rithms.
Addison-Wesley Publishing Company.A.H.
Aho, R. Sethi, and J.D.
Ullman.
1986.Compilers - Principles, Techniques, and Tools.Addison-Wesley, Reading, Massachusetts, USA.O.
Collin, F. Duclaye, and F. Yvon.2002.
Learning Paraphrases to Im-prove a Question-Answering System.staff.science.uva.nl/ mdr/NLP4QA/10duclaye-et-al.pdf.Luc DeHaspe.
1998.
Frequent Pattern Discovery inFirst-Order Logic.
Ph.D. thesis, Katholieke Uni-versiteit Leuven.G.
Doddington and C.H.J.
Godfrey.
1990.
TheATIS Spoken Language Systems Pilot Corpus.
InSpeech and Natural Language Workshop, HiddenValley, Pennsylvania.R.
Grishman and B. Sundheim.
1995.
?MessageUnderstanding Conference-6: A Brief History?.www.cs.nyu.edu/cs/projects/proteus/muc/muc6-history-coling.ps.M.
Liakata and S. Pulman.
2002.
From Trees toPredicate-Argument Structures.
In InternationalConference for Computational Linguistics (COL-ING), pages 563?569, Taipei, Taiwan.D.
Lin and P. Pantel.
2001.
Dirt-Discovery of Infer-ence Rules from Text.
In In ACM SIGKDD Con-ference on Knowledge Discovery and Data Min-ing, pages 323?328.M.
Marcus, G. Kim, M. Marcinkiewicz, R. Mac-Intyre, A. Bies, M. Ferguson, K. Katz, andB.
Schasberger.
1994.
The Penn Treebank: An-notating predicate argument structure.
In ARPAHuman Language Technology Workshop.Stephen Muggleton.
1995.
Inverse Entailment andProgol.
New Generation Computing, specialissue on Inductive Logic Programming, 13(3-4):245?286.Stephen Pulman.
2000.
Statistical and Logi-cal Reasoning in Disambiguation.
PhilosophicalTransactions of the Royal Society, 358 number1769:1267?1279.Ashwin Srinivasan.
1999.
?the Aleph Manual?.www.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.M.
Subramani and G.F. Cooper.
1999.
CausalDiscovery from Medical Textual Data.http://www.amia.org/pubs/symposia/D200558.PDF.Gertjan van Noord.
2002.
FSA6 Reference Man-ual.
http://odur.let.rug.nl/ vannoord/Fsa/.Figure 1: The more likely path in this FSA segment is given by the choice of resign(D,E,F ) : 0.15,followed by cperson(E) : 0.64 and finally succeed(G,C,E) : 0.26.
This can be interpreted as follows: ?Ifa person C is elected, another person E has resigned and C succeeds E?Figure 2: Here the more likely path is provided by the sequence:cperson(C) : 0.32, director(C,D) : 0.08, of(D,E) : 0.13, company(E) : 1, succeed(F,C,G) : 0.25,cperson(F ) : 1.
This can be read as: ?If a person C is elected director of a company E then C succeedsanother person G?.Notice the above illustrate only parts of the FSAs, which justifies why the probabilites of arcs leav-ing a node don?t add up to 1
