Jurilinguistic Engineering in Cantonese Chinese:An N-gram-based Speech to Text Transcription SystemB K T'sou, K K Sin, S W K Chan, T B Y Lai, C Lun, K T Ko, G K K Chan, L Y L CheungLanguage hfformation Sciences Research CentreCity University of Itong KongTat Chee Avenue, KowloonHong Kong SAR, ChinaEmail: rlbtsou @nxmail.cityu.edu.hkAbstractA Cantonese Chinese transcription system toautomatically convert stenograph code toChinese characters ix reported.
The majorchallenge in developing such a system is thecritical homocode problem because ofhomonymy.
The statistical N-gram model isused to compute the best combination ofcharacters.
Supplemented with a 0.85 millioncharacter corpus of donmin-specific trainingdata and enhancement measures, the bigramand trigrmn implementations achieve 95%and 96% accuracy respectively, as comparedwith 78% accuracy in the baseline model.
Thesystem perforlnance is comparable with otheradwmced Chinese Speech-to-Text inputapplications under development.
The systemmeets an urgent need o1' the .ludiciary ot: post-1997 Hong Kong.Keyword: Speech to Text, StatisticalModelling, Cantonese, Chinese, LanguageEngineering1.
Introduct ionBritish rule in Hong Kong lnade English the onlyofficial language in the legal domain for over aCentury.
After the reversion of Hong Kongsovereignty to China in 1997, legal bilingualismhas brought on an urgent need to create aComputer-Aided Transcription (CAT) system forCantonese Chinese to produce and maintain themassive legally tenable records of courtproceedings conducted in the local majoritylanguage (T'sou, 1993, Sin and T'sou, 1994, Lunet al, 1995).
With the support fl'om the HongKong Judiciary, we have developed atranscription system for converting stenographcode to Chinese characters.CAT has been widely used for English formany years and awlilable R~r Mandarin Chinese,but none has existed for Cantonese.
AlthonghCantonese is a Chinese dialect, Cantonese andMandarin differ considerably in terms ofphonological struclure, phouotactics, wordmorphology, vocabulary and orthogral)hy.
Mutualintelligibility between the two dialects is generallyvery low.
For example, while Cantonese has lnolethan 700 distinct syllables, Mandarin has onlyabout 400.
Cantonese has 6 tone contours andMandarin only 4.
As for vocabulary, 16.5% of thewords in a 1 million character corpus of courtproceedings in Canlonese cannot be found in acorlms consisting of 30 million characternewspaper texts in Modern Written Chinese(T'sou el al, 1997).
For orthography, MainhmdChina uses the Simplified Chinese character set,and Hong Kong uses the Traditional set l~lus4,702 special local Cantonese Chinese characters(Hong Kong Government, 1999).
Suchdifferences between Cantonese and Mandarinnecessitate the Jtnilinguistic Engineeringundertaking to develop an independent CantoneseCAT system for the local language nvironment.The major challenge in developing aCantonese CAT system lies in the conversion ofphonologically-based stenograph code intoChinese text.
Chinese is a logographic language.Each character or logograph represents a syllable.While the total inventory of Cantonese syllabletypes is about 720, them am at least 14,000Chinese character types.
The limited syllabarycreates many homophones in the language (T'sou,1976).
In a one million character corlms of courtproceedings, 565 distinct syllable types werefound, representing 2,922 distinct character types.Of the 565 syllable types, 470 have 2 or mornhomophonous characters.
In the extreme case, zirepresents 35 homophonous character types.1121Coverage ofl lomophonous and Non-homophous Characters100.00% ?
'o o~ 4:~ ~ ~os 4:e80.00%660.00%# 40.00%c~ 20.00%0.00%?
No.
of High Freq.
Characters %~ t~ p\[EJ HoInophonous Charactcrs \[\] Non-honaophonous chaliactcrs\]Figure I. Covcragc of Homonymous CharactersThese 470 syllables represent 2,810 homophonouscharacter types which account for 94.7% of thetext, as shown in Figure \].
The homocodeproblem nmst be properly resolved to ensuresuccessful conversion.2.
Computer-Aided Transcription (CAT)~dJ lt?
Ico 2F2L .Stcnoma~h code I iSlzg?2 EI Transcription \[ |/ !
EIt?illC ; |h Trigranl ,, Big,an, i ~ '9=, E )!Proofreading\[ i' I' Bigram / Trip, rami Statistical DalaChinese Text i l l :~!Figure 2.
Automatic Transcription ProcessFigure 2 outlines the transcription process in theCantonese CAT system.
Following typicalcourtroom CAT systems, our process is dividedinto three major stages.
In Stage 1, simultaneousto a litigant speaking, a stenographer inputsspeech, i.e.
a sequence of transcribed syllables orstenograph codes, via a stenograph code generator.Each stenograph code basically stands for asyllable.
In Stage 2, the transcription softwareconverts the sequence of stenograph codes \[Sl .
.
.
.
.s,,} into the original character text {q .
.
.
.
.
c,,}.This procedure requires the conversioncomponent to be tightly bound to the phonologyand orthography of a specific language.
Tospecifically address homonymy in Cantonese, theconversion procedure in our system is supportedby bigram and trigram statistical data derivedfrom domain-specific training.
In Stage 3, manualediting of the transcribed texts corrects errorsfrom typing mistakes or his-transcription.3.
System Architecture3.1 Statistical FormulationTo resolve massive ambiguity in speech to textconversion, the N-gram model is used todetermine the most probable character sequence{q .
.
.
.
.
ck} given the input stenograph codesequence {s~ .
.
.
.
.
Sk}.
The conditional probability(1) is to be maximized.
(1) P(q .
.
.
.
.
c~l sl .
.
.
.
.
sk)where {q .
.
.
.
.
c~} stands for a sequence of Ncharacters, and {sl .
.
.
.
.
sk} for a sequence of kinput stenograph codes.The co-occurrence frequencies necessary forcomputation are acquired through training.However, a huge amount of data is needed togenerate reliable statistical estimates for (1) ifN > 3.
Consequently, N-gram probability isapproximated by bigram or trigram estimates.First, rewrite (1) as (2) using Bayes' rule.P(c, ..... c )xP(s, ..... s lc, ..... c , )(2)P(s, ..... s k )As the value of P(s I .
.
.
.
.
st) remains unchangedfor any choice of {q .
.
.
.
.
ct}, one needs only tomaximize the numerator in (2), i.e.
(3).
(3) P(cl ..... Ck) X P(sl ..... s,\[cl ..... ck)(3) can then be approximated by (4) or (5) usingbigram and trigram models respectively.
(4) FL=,...., (P(c,lq_,) x P(s,.Iq))(5) ?P(silci))The transcription program is to compute the bestsequence of {q .
.
.
.
.
c,} so as to maximize (4) or(5).
The advantage of the approximations in (4)and (5) is that P(s,lc,), P(c,lc,.,) and P(c,lc,_2c,_,)can be readily estimated using a training corpus ofmanageable size.3.2 Viterbi AlgorithmThe Viterbi algorithm (Viterbi, 1967) isimplemented toefficiently compute the maxinmmvalue of (4) and (5) for different choices of1122character sequences.
Instead o1' exhaustivelycomputing tile values for all possible charactersequences, the algorithm only keeps track of theprobability of the best character sequenceterminating in each possible character candidatefor a stenograph code.In the trigram implelnentatiou, size limitationin the training cortms makes it impossible toestimate all possible P(c i lc i_2ci .
i )  because some{ci_2, ci_l, q} may never occur there.
FollowingJelinek (1990), P(cil ci.2ci_ i ) is approximated bythe summation of weighted lrigram, bigram andunigram estimates in (6).
(6) p(c i I ci_ 2 ci_ 1)f(ci_ 2 ci- I ci ) f(ci_ 1 c i ) f(c i )= w 3 X + w 2 X + w 1 X -f(ci_ 2 ci_ 1 ) f(ci_ I ) Z f(cj )where (i) w,, w2, w-s _> 0 are weights, (ii)wl-l-w2-{-H; 3 = 1, and (iii) Z f(q) is the stun offrequencies of all characters.
Typically lhe bestresults can be obtained if w:~, the weight fortrigram, is significantly greater than the olher twoweights so that the trigram probability hasdominant effect in the probability expression.
Inour tests, we sot wl=0.01, w2=0.09, aud u;3=0.9.The Viterbi algorithm substantially reduces thecomputational complexity flom O(m") to O(m.-~n)and O(nr~n) using bigram and trigram estimationrc:spectively where n is the number of stenographcode tokens in a sentence, and m is tile upperbound of the number of homophonous charactersfor a stenograph code.To maximize the transcription accuracy, wealso refine the training corpus to ensure that thebigram and trigram statistical models reflect thecomtroom lauguage closely.
This is done byenlarging tile size of tile training corpus and bycompiling domain-specific text corpora.3.,3 Special EncodingAfter some initial trial tests, error analysis wasconducted to investigate the causes of the mis-transcribed characters.
It showed that a noticeableamount of errors were due to high failure rate inthe mtriewtl of seine characters in thetranscription.
The main reason is that highfiequency characters are more likely to interferewith the correct retrieval of other relatively lowerfrequency homophouous characters.
For example,Cantonese, hal ('to be') and hal ('at') arehomophouous in terms of seglnental makeup.Their absolute fiequcucies in our training corpusare 8,695 and 1,614 respectively.
Because of thelarge fi'equency discrepancy, the latter was mis-transcribed as tile former 44% of the times in atrial test.
32 such high fi'equency characters werefound to contribute to about 25% of alltranscription errors.
To minimize the interference,special encoding, which resulted flom shallowlinguistic processing, is applied to the 32characters o that each of them is assigned aunique stenograph code.
This was readilyaccepted by the court stenographers.4.
hnplementation and Results4.1 Compilation of CorporaIn our expreriments, authentic Chinese courtproceedings from the Hong Kong Judiciary wereused fox tile compilation of the training andtesting corpora for the CAT prototypes.
To ensurethat tile training data is comparable with tile datato be transcribed, the training corpus should belarge enough to obtain reliable estimates forP(silc,.
), P(cilci j) and P(cilci_2ci_l).
in our trials,we quickly approached the point of diminishingreturn when the size of the training corpus reachesabout 0.85 million characters.
(See Section 4.2.2.
)To further enhance training, the system alsoexploited stylistic and lexical variations acrossdifferent legal domains, e.g.
tra\[.
'fic, assauh,  andf raud  offences.
Since different case types showdistinct domain-specific legal vocabulary or usage,simply integrating all texts in a single trainingcorpus may obscure the characteristics o1' specificlanguage domains, thus degrading the modelling.Hence domain-specific training corpora were alsocompiled to enhance performance.Two sets of data were created for testing andcomparison: Gener ic  Coqms (GC) and Domain -,specific Cmpus  (DC).
Whereas GC consists oftexts representing various legal case types, DC isrestricted to traffic offence cases.
Each setconsists of a training corpus of 0.85 millioncharacters and a testing corpus of 0.2 millioncharacters.
The training corpus consists ofChinese characters along with the correspondingstenograph codes, and tile testing corpus consistssolely of stenograph codes of the Chinese texts.4.2 Experimental ResultsFor ewfluation, several prototypes were set up to1123test how different factors affected transcriptionaccuracy.
They included (i) use of bigram vs.trigram models, (ii) the size of the trainingcorpora, (iii) domain-specific training, and (iv)special encoding.
To measure conversionaccuracy, the output text was compared with theoriginal Chinese text in each test on a character bycharacter basis, and the percentage of correctlytranscribed characters was computed.
Five sets ofexperiments are reported below.4.2.1 Bigram vs. TrigramThree prototypes were developed: the BigramPrototype, CA Tva2, the Trigram Prototype, CA Tva.~,and the Baseline Prototype, CATo.
CATva2 andCATvA.~ implelnent he conversion engines usingthe bigram and trigram Viterbi algorithmrespectively.
CA7o, was set up to serve as anexperimental control.
Instead of implementing theN-gram model, conversion is accomplished byselecting the highest fiequency item out of thehomophonous character set for each stenographcode.
GC was used throughout the threeexperiments.
The training and testing data sets are0.85 and 0.20 million characters respectively.
Theresults are summarized in Table 1.Corpus GC GC GCAccuracy 78.0% 92.4% 93.6%Table 1.
Different N-gram ModelsThe application of the bigram and trigram modelsoffers about 14% and 15% improvement inaccuracy over Control Prototype, CATo.4.2.2 Size of Training CorporaIn this set of tests, the size of the training corporawas varied to determine the impact of the trainingcorpus size on accuracy.
The sizes tested are 0.20,0.35, 0.50, 0.63, 0.73 and 0.85 million characters.Each corpus is a proper subset of the immediatelylarger corpus so as to ensure the comparability ofhe trainin texts.
CATvA 2 was used in the tests.Training Corpus GC GC GCAccurac~ 89.5% 91.2% 91.8%Training Corpus GC GC GCAccuracy 92.1% 92.3 % 92.4 %Table 2.
Variable Training Data SizeThe results in Table 2 show that increasing thesize of the training corpus enhances the accuracyincrementally.
However, the point of diminishingreturn is reached when the size reaches 0.85million characters.
We also tried doubling thecorpus size to 1.50 million characters.
It onlyyields 0.8% gain over the 0.85 million charactercorpus.4.2.3 Use of Domain-specific TrainingThis set of tests evaluates the effectiveness ofdomain-specific training.
Data fi'oln the twocorpora, GC and DC, are utilized in the training ofthe bigram and trigram prototypes.
The size ofeach training set is 0.85 million characters.
Thesame set of 0.2 million character testing data fromDC is used in all four conversion tests.
Withoutincreasing the size of the training data, setups withdomain-specific training consistently ield about2% improvement.
A more comprehensive set ofcorpora including Tra.lfic, Assault, and Robbeo~ isbein )iled and will be re )ortcd in future.PrototypesTraining DataCATvA; CATvA3 CATvaz CATvA3GC GC DC DCTesting Data DC DC DC DCAccuracy 92.6% 92.8% 94.7% 94.8%Table 3.
Application of Domain-Specificity4.2.4 Special EncodingFollowing shallow linguistic processing, specialencoding assigns unique codes to 32 characters toreduce confusion with other characters.
Anotherround of tests was repeated, identical to theCATvA2 and CATvA 3 tests in Section 4.2.1, exceptfor the use of special encoding.
The use oftraining and testing corpora have 0.85 and 0.20million characters respectiveS ~ i ~  I;:~ :::NOfA~I~Ii~:Prototypes CATw~ CATvA~ CATw2 CATvA3Corpus GC GC GC GCAccuracy 92.4% 93.6% 94.7% 95.6%Table 4.
Application of Special EncodingTable 4 shows that the addition of specialencoding consistently offers about 2% increase inaccuracy.
Special encoding and hence shallowlinguistic processing provide the most significantimprovement in accuracy.4.2.5 Incorporation of Domain-Specificity andSpecial EneodingAs discussed above, both domain-specific trainingand special encoding raise the accuracy oftranscription.
The last set of tests deals with theintegration of the two features.
Special encoding1124is utilized in the training and testing data of DCwhich have 0.85 and 0.20 million charactersrespectively.~raining/Testing, Data DC DCI S. Encoding Applied Applied / zcuracy 95.4 % 96.2 %Table 5.
Integration of D. Specificity and S. EncodingRecall that Domain-Specificity and SpecialEncoding each offers 2% improvelnent.
Table 5shows that combining BOTH features offer about3% improvement over tests without them.
(Seenon-domain-specific tests in Section 4.2.3)The 96.2% accuracy achieved by CATvA 3represents the best performance of our system.The result is conaparable with other relevantadvanced systems for speech to text conversion.For example, Lee (1999) reported 94% accuracyin a Chinese speech to text transcription systemunder developlnent with very large trainingcorpus.5.
ConclusionWe have created a Cantonese Chinese CATsystem which uses the phonologically-basedstenograph machine.
The system deliversencouragingly accurate transcription in a languagewhich has many hon\]ol~honous characters.
Toresolve problematic ambiguity in the conversionfi'on-i a I)honologically-based code to thelogograt)hic Chinese characters, we made use oflhe N-gram statistical model.
The Viterbialgorithm has enabled us to identify the mostprobable sequence of characters from the sels ofpossible homophonous characters.
With theadditional use of special encoding and domain-specific training, the Cantonese CAT system hasattained 96% transcription accuracy.
The successof the Jurilinguistic Engineering project canfurther enhance the efforts by the Hong KongJudiciary to conduct trials in the language of themajority population.
Further improvement to thesystem will include (i) more domain-specifictraining and testing across different case types, (2)firm-tuning for the optimal weights in the trigramformula, and (3) optilnizing the balance betweentraining corpus size and shallow linguisticprocessing.AcknowledgementSupport for the research reported here is providedmainly through the Research Grants Council ofHong Kong under Competitive EarmarkedResearch Grant (CERG) No.
9040326.ReferencesHong Kong Govennnent.
1999.
Hong KongSzq~plemenmry Character Set.
hfformationTechnology Services Department & OfficialLanguages Agency.Jelinek, F. 1990.
"Self-organized Language Modelinglbr Speech Recognition."
In A. Waibel and K.F.Lee, (eds.).
Readings in Speech Recognition.
SanMateo: CA: Morgan Kaufmann Publishers.Lee, K. F. 1999.
"Towards a Multimedia, Multimodal,Multilingual Computer."
Paper presented on behall'of Microsoft Research Institute, China in the 5thNatural Language Processing Pacil'ic RimSymposium held in Belling, China, November 5-7,1999.Lun, S., K. K. Sin, B. K. T'sou and T. A. Cheng.
1997.
"l)iannao Fuzhu Yueyu Suii Fangan."
(TheCantonese Shorthand System for Computer-AidedTranscription) (in Chinese) Proceedings o.f the 5thlntermttional Confere,ce on Cantonese and OtherYue Dialects.
B. H. Zhan (ed).
Guangzhou: JinanUniversity Press.
pp.
217--227.Sin, K. K. and B. K. T'sou.
1994.
"Hong KongCourtroon\] Language: Some Issues on Linguisticsand Language Technology."
Paper presented at lheThird International Conference on ChineseLinguistics.
Hong Kong.T'sou, B. K. 1976.
"Homophony and Internal Changein Chinese."
Computational Analyses of Asian andAfrican Lauguages 3, 67--86.T'sou, t3.
K. 1993.
"Some Issues on Law and Languagein the ltong Kong Special Administrative Region(HKSAR) of China."
Language, Law attd Equality:Proceedings of the 3rd International Conference ofthe International Acaden G of Language Law (IALL).K.
Prinsloo et al Pretoria (cds.
): University of SouthAfrica.
pp.
314-331.T'sou, B. K., H. L. Lin, G. Liu, T. Chan, J. Hu, C. H.Chew, and J. K. P. Tse.
1997.
"A SynchronousChinese Language Corpus fi'om Different SpeechCommunities: Construction and Applications.
"Computational Linguistics and Chinese LanguagePtwcessing 2:91-- 104.Viterbi, A. J.
1967.
"Error Bounds for ConvolutionCodes and an Asymptotically Optimal l)ecodingAlgorithm."
IEEE 7'ransactions on htformationTheoG 13: 260--269.1125
