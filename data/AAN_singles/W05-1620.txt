An Evolutionary Approach to Referring Expression Generation and AggregationRaquel Herva?s and Pablo Gerva?sDepartamento de Sistemas Informa?ticos y Programacio?nUniversidad Complutense de Madrid, Spainraquelhb@fdi.ucm.es,pgervas@sip.ucm.esAbstractThe work presented here is intended as an evolu-tionary task-specific module for referring expres-sion generation and aggregation to be enclosed in ageneric flexible architecture.
Appearances of con-cepts are considered as genes, each one encodingthe type of reference used.
Three genetic opera-tors are used: classic crossover and mutation, plusa specific operator dealing with aggregation.
Fit-ness functions are defined to achieve elementarycoherence and stylistic validity.
Experiments aredescribed and discussed.1 IntroductionIn this paper we present a first approach to the idea of usingNatural Language Generation (NLG) and Evolutionary Algo-rithms (EAs) together.To test the feasibility of our idea, we decided to select onlysome particular features of the text on which to put it to thetest.
Given the complexity of all the changes that are pos-sible to a text, at the levels of syntax, semantics, discoursestructure and pragmatics, it seemed impractical to tackle themall at once.
For the purpose of illustration, we decided thatthe problems of the referring expressions and the aggregationwere the most suitable to be solved using EAs.
ReferringExpression Generation involves deciding how each elementocurring in the input is described in the output text.
Aggre-gation involves deciding how compact the presentation of in-formation should be in a given text.
It operates at several lin-guistic levels, but we only consider it here with respect to con-cepts and their attributes.
For instance, the system must de-cide between generating ?The princess is blonde.
She sleeps.
?and generating ?The blonde princess sleeps.?.
Aggregation isgenerally desirable, but may result in adjective-heavy textswhen the information to impart becomes dense in terms ofattributes, as in ?The pretty blonde princess lived in a strongfancy castle with her stern rich parents.?.
It is necessary tofind the balance between the use of compound or single sen-tences, or in the case of the modifiers of a concept betweenthe description of the attributes of the concept using only aphrase or various.We analysed the features of a human generated text fromthe point of view of the referring expressions, and we foundfive different features of simple texts that might be susceptibleof easy treatment by means of evolutionary techniques.
Theyare described below.Correct Referent.When writing a text, we cannot use a pronoun for somethingthat we have not mentioned before, or readers would get con-fused.
An example could be:She lived in a castle.
A princess was the daugh-ter of parents.In addition, if the full noun reference and the pronoun arefar, the reader can also get confused and be unable to link thetwo occurrences of the same concept, as we can see in thefollowing text:A princess lived in a castle.
She was the daugh-ter of parents.
She loved a knight.
She was pretty.She was blonde.
It had towers.
It was strong.
Theylived in it.Redundant Attributes.When describing a concept in an ?X is Y?
sentence, people donot use the attribute they are going to describe in the referenceto the concept.
Sentences such as the one below are incorrect:The blonde princess was blonde.Reference Repetition.Using always the same reference together with the same set ofattributes results in repetitive text.
For example, it is accept-able to use ?the princess?
every time we refer to the princesscharacter, but it would be striking to use always ?the prettyprincess?, as in this example:A pretty princess lived in a castle.The pretty princess was the daughter of par-ents.
The pretty princess loved a knight.The pretty princess was blonde.To avoid that, repetitive use of references is penalized.Coherence.If we use different subsets of attributes in different referencesto the same concept, the reader may mistakenly assume thatwe are referring to different concepts.
For example, if we use?the pretty princess?
and ?the blonde princess?
in differentplaces, and we have not specified that the princess is bothpretty and blonde, it could seem that there are two princess, apretty one and a blonde one:A princess lived in a castle.
The pretty princesswas the daughter of parents.
The blonde princessloved a knight.Overlooked Information.When processing the conceptual representation of a giveninput, some information about a concept may disappear fromthe final text.
This should be avoided.This paper describe an evolutionary solution that guaran-tees the satisfaction of these restrictions in the conceptualrendition of a given input by means of shallow techniquesthat rely on very little knowledge about the domain and noreasoning or common sense capabilities.2 Natural Language Generation Tasks andEvolutionary AlgorithmsThis section outlines the elementary requirements of the twogeneration tasks addressed in this paper, and sketches the ba-sic principles of the evolutionary techniques that are used.2.1 Referring Expression Generation andAggregationThe correct use of referring expressions to compete with hu-man generated texts involves a certain difficulty.
Possiblesimple algorithms for deciding when to use a pronoun andwhen to use the full noun produce poor results.
Two occur-rences of the same concept in a paragraph can be far apart,and this may confuse the reader.
Knowledge intensive ap-proaches modelled on the way humans do it require a certainmeasure of content understanding that is resource hungry.As shown in [Reiter and Dale, 1992], a referring expressionmust communicate enough information to be able to uniquelyidentify the intended referent in the current discourse context,but avoiding the presence of redundant or otherwise unneces-sary modifiers.
Therefore, it is essential to choose a referencewhich matches these constraints.
Taking into account thesefeatures, Reiter and Dale proposed an algorithm to generatedefinite noun phrases to identify objects in the current focusof attention of the reader or the hearer.
However, Krahmerand Theune [Krahmer and Theune, 2000] argue that due tothe original motivation of the work of Reiter and Dale of mak-ing distinguishing descriptions, various other aspects of thegeneration of definites remained somewhat underdeveloped.In particular they focus on the role of context-sensitivity forreferring expression generation.Kibble and Power [Kibble and Power, 2000] propose a sys-tem which uses Centering Theory [Walker et al, 1998] forplanning of coherent texts and choice of referring expres-sions.
They argue that text and sentence planning need tobe driven in part by the goal of maintaining referential con-tinuity: obtaining a favourable ordering of clauses, and ofarguments within clauses, is likely to increase opportunitiesfor non-ambiguous pronoun use.Aggregation can be seen as the NLG task that involves de-ciding how compact the presentation of information should bein a given text, although there is no exact definition in the lit-erature about what aggregation is [Reape and Mellish, 1999].It operates at several linguistic levels, and due to that Reapeand Mellish make a classification of the different types of ag-gregation: conceptual, discourse, semantic, syntactic, lexicaland referential.
However, the line between them is very nar-row, and in some cases a specific example could be classifiedas different types of aggregation.2.2 Evolutionary AlgorithmsWe propose the use of evolutionary algorithms (EAs) [Hol-land, 1992] to deal with the referring expression generationand aggregation tasks.
Evolutionary algorithms are an ex-tended set of problem resolution techniques inspired by evo-lutionary phenomena and natural evolution.
They work ona population of individuals (representations of possible solu-tions for the problem we are solving) that evolve according toselection rules and genetic operators like crossover and mu-tation.
The fitness function is a metric which allows the eval-uation of each of the possible solutions, in such way that theaverage adaptation of the population would increase in eachgeneration.
Repeating this process hundreds or thousands oftimes it is possible to find very good solutions for the prob-lem.Evolutionary algorithms combine random search, becausethe genetic operators are applied randomly, with orientedsearch, given by the fitness values.
These algorithms findgenerally good solutions, but not always the best ones.
How-ever, this is enough for simple applications.
In the case underconsideration, the main advantage we can find in evolution-ary algorithms is that they do not need specific rules to builda solution, only measurements of its goodness.Evolutionary techniques have been shown in the past to beparticularly well suited for the generation of verse.
The workof Manurung [Manurung, 2003] and Levy [Levy, 2001] pro-posed different computational models of the composition ofverse based on evolutionary approaches.
In both cases, themain difficulty lay in the choice of a fitness function to guidethe process.
Although Levy only addressed a simple modelconcerned with syllabic information, his overall descriptionof the architecture in terms of a population of poem drafts thatevolve, with priority given to those drafts that are evaluatedmore highly, is an important insight.
Levy uses a neural net-work, trained with examples of valid verse, to evaluate thesedrafts.
The work of Manurung addresses the complete task,and it presents a set of evaluators that grade the candidatessolutions according to particular heuristics.Evolutionary algorithms have been also used in text plan-ning.
In [Duboue and McKeown, 2002] the authors presenta technique to learn a tree-like structure for a content plan-ner from an aligned corpus of semantic inputs and corre-sponding, human produced, outputs.
They apply a stochasticsearch mechanism with a two-level fitness function to createthe structure of the planner.
Genetic algorithms are also usedin [Mellish et al, 1998] where the authors state the problemof given a set of facts to convey and a set of rhetorical re-lations that can be used to link them together, how one canarrange this material so as to yield the best possible text.An important conclusion to draw from these efforts is thesuitability of evolutionary techniques for natural languagegeneration tasks in which the form plays a significant role,to the extent of sometimes interfering with the intended con-tent, such as is the case for lyrics generation.3 An Evolutionary Submodule for a SimpleGeneratorThe work presented here is intended to be a module for thetasks of referring expressions generation and aggregation en-closed in the architecture of cFROGS [Garc?
?a et al, 2004].cFROGS is a framework-like library of architectural classesintended to facilitate the development of NLG applications.cFROGS identifies three basic design decisions: what set ofmodules to use, how control should flow between them, andwhat data structures are used to communicate between themodules.We have tested the implementation of the module in an ex-isting application: ProtoPropp [Gerva?s et al, 2004].
Thisis a system for automatic story generation.
The natural lan-guage generator module of ProtoPropp ?
implemented as apipeline architecture of cFROGS modules ?
perform taskssuch as content determination - selecting the particular con-cepts that are relevant - and discourse planning - organisingthem in an orderly fashion.
These tasks are currently carriedout in a traditional manner and simply provide the data for theevolutionary stages.
In the previous prototype of ProtoProppthe referring expression to use for a concrete concept was de-termined using a very simple heuristic: the first time that theconcept appears in the paragraph, the generator uses its fullnoun, in all other cases it uses a pronoun.
When using a fullnoun reference, it is indefinite for the first appearance of theconcept in the text and definite for the rest.The input of the evolutionary algorithm is a basic discoursestructure where each phrase is a message about a relation be-tween two concepts or a description of some attribute of anelement.
Additionally, this submodule has access to a knowl-edge base of conceptual information about the discourse el-ements that appear in the input (characters, locations, at-tributes, relations).In this simple evolutionary algorithm, the appearances ofthe concepts are considered as the genes.
The initial popu-lation is generated randomly, using for each concept its fullnoun or its pronoun.
When using the full noun, a selection ofthe attributes the concept has in the knowledge base is cho-sen.
These attributes will appear just before the noun of theconcept, as it is usual in English.
The system works overthis population for a number of generations determined bythe user.
In each generation three genetic operators are used:crossover, mutation and aggregation.
Finally, at the end ofeach generation each tale is evaluated and a selection of thepopulation is passed to the next one, in such way that the taleswith a higher fitness have more possibilities of being chosen.3.1 Data Representation and GenesWithin the context of the larger cFROGS architecture, dataare represented as complex data structures with generic inter-faces to ensure easy connectivity between different modules[Garc?
?a et al, 2004].
These data follow ideas from the RAGS[Cahill et al, 2001] generic architecture.
However, the no-tation described here corresponds to a representation internalto the module intended to facilitate the operation of the evo-lutionary techniques.Characters, locations and attributes are represented assimple facts containing an unique identifier (to distinguisheach specific character and location from the others) and theirnames.
The identifier in attributes corresponds to the con-cept that holds the attribute, and the name corresponds to theattribute itself.
The current prototype operates over simplelinguistic constructs: the description of a concept using anattribute, or a relation between two concepts.
Pronominalreference is indicated by changing the name of the conceptfor ?pron?, and definite and indefinite reference is indicatedby adding a fact ?ref?
indicating if the reference is definite orindefinite.
Finally, the concepts may go along with some at-tributes preceding the name of the concept, as in ?the prettyblonde princess?.
This list of attributes is represented be-tween -> and <-.A sample part of a draft for the evolutionary algorithmwould be the following:[character(ch26,princess),ref(ind),->attribute(ch26,pretty)<-,relation(ch26,l14,live),location(l14,castle),ref(ind)][character(ch26,pron),relation(ch26,ch25,love),character(ch25,knight),ref(ind)][character(ch26,princess),ref(def),isa(),attribute(ch26,blonde)]In this example, the set of genes would be this:Genes:0: character(ch26,princess),ref(ind),->attribute(ch26,pretty)<-1: location(l14,castle),ref(ind)2: character(ch26,pron)3: character(ch25,knight),ref(ind)4: character(ch26,princess),ref(def)3.2 The Genetic OperatorsThree genetic operators are used: crossover, mutation and ag-gregation.For the crossover operator, two drafts are selected ran-domly and crossed by a random point of their structure.
So,each of the sons will have part of each of the parents.In the case of the mutation operator, some of the genes arechosen randomly to be mutated.
If the gene is a pronoun -as in ?she lived in a castle?
-, it will change into the cor-responding full noun, always associated with a subset of itspossible attributes - for example ?the princess lived in a cas-tle?
or ?the pretty princess lived in a castle?
-.
In case theCorrect Referent error1 =?pronominal references to a concept not referred in full in the two previous genesRedundant Attributes error2 =?
?<adj> X is <adj>?
sentencesReference Repetition error3 =?repeated use of same set of attributes ?
att(geni) ?
to refer to the concept in geniCoherence error4 =?Ni=1(att(geni) ?
I) with I the set of attributes used before for the concept in geniOverlooked Information error5 =?subset of attributes of concept i in the ontology not mentioned in the textTable 1: Definition of fitness functionsgene was a full noun - as in ?the pretty princess?
-, there aretwo options: to change it into a pronoun - in this case ?she?-, or to change the subset of attributes that appear with it -for example ?the princess?
or ?the pretty blonde princess?
-.One of these two options is chosen randomly.The aggregation operator addresses the task of deciding onthe aggregation between concepts and their attributes.
Thisinvolves a certain modification of the structure of the text,because sentences in the text may be deleted if the informa-tion they impart becomes part of a previous sentence.
Theaggregation operator acts only on genes corresponding to ex-plicitly mentioned concepts: concepts referred by pronounsare excluded.
It can act in two directions:?
If the reference to the concept appears with one or moreattributes - as in ?A blonde princess lived in a castle.
?-, the operator disaggregates the attributes by eliminat-ing their mention and adding a corresponding ?X is Y?sentence - resulting in ?A princess lived in a castle.
Shewas blonde.??
If the reference to X has no attributes - as in ?A princesslived in a castle.?
-, the algorithm looks for an ?X is Y?sentence - such as ?The princess was blonde.?
-, addsthe corresponding attributes to the reference, and deletesthe ?X is Y?
sentence - resulting in ?A blonde princesslived in a castle.
?The goal of this definition of the aggregation is to ensurethat the attributes of a concept are mentioned in the appear-ance of a concept or in the correspondent ?X is Y?
sentences,but not in both.
As the aggregation operator is used randomly,the desired result is obtained only in some cases.3.3 The Fitness FunctionThe key to the evolutionary algorithm lies in the choice offitness function.
A simple approach would be to require thatin each generation the user reads all the texts and gives thema fitness value.
The number of generations and individuals inthe population for a simple experiment makes this approachimpractical.We have defined five different fitness functions as shown inTable 1.
This definitions are the results of the analysis of thefeatures of human-generated text.For the evaluation of each of the drafts that form the popu-lation, we use the following formula:fitness = 1/(?ierrori + k)In this way, the fitness would be greater when the error issmaller.
The constant k is used to avoid divisions by zero.
Inour experiments it was set with the value 1, so the maximumpossible fitness was 1.4 Experiments and ResultsTo test the feasibility of the idea of using together NLG andEAs, we have formalized five different fairy tales, mainly dif-ferentiated by their lengths in number of genes, that is, inappearances of concepts.
We must take into account that thenumber of genes shown below are not completely exact, be-cause the aggregation operator can erase or add new sentencesto the tale.
These are the tales formalized and used to do theexperiments:?
Cinderella: 102 genes?
Hansel and Gretel: 90 genes?
The Lioness: 50 genes?
The Dragon: 32 genes?
The Merchant: 31 genesFor each of these tales we have made several experimentsusing different population sizes (10, 25, 50, 100, 200, 300,500) and number of generations (10, 25, 50).
The three ge-netic operators mentioned before (crossover, mutation andaggregation) are applied, and the five fitness functions usedfor the evaluation of the tales.Table 2: Table of numerical resultsIn Table 2 we can see the numerical results of the experi-ments.
For each combination of population size and numberFigure 1: Legend for the talesof generations results shown have been averaged over a num-ber of runs.We can analyse these results taking into account the threedifferent number of generations used.
The legend for the fol-lowing graphics is shown in Figure 1.4.1 10 GenerationsAs we can see in Figure 2, only 10 generations are not enoughfor the bigger tales.
However, in the case of the smaller ones,the fitness values increase with the size of the population, andat certain point they achieve the maximum value of 1.Figure 2: Fitness values of the tales with 10 generations4.2 25 GenerationsIn Figure 3 the fitness values for the bigger tales are higherthan in the case of 10 generations, but still not good enough.For the smaller tales we achieve the maximum fitness valueof 1 quicker than with only 10 generations.4.3 50 GenerationsWe can see in Figure 4 the best values achieved in the ex-periments.
For the smaller tales, we get the maximum fitnessvalue of 1 very quickly.
In the case of the bigger ones, thefitness values are higher than in the previous experiments, butnot very good yet, except in the case of ?The Lioness?, wherethe maximum value of 1 is achieved with 50 generations and500 individuals in the population.5 DiscussionTo start with, EAs seem to be a good approach to solve thetasks addressed, and in all the experiments the results ob-tained are better than the ones achieved using previous heuris-tics.
An example of generated text with the initial simpleheuristic is:Figure 3: Fitness values of the tales with 25 generationsFigure 4: Fitness values of the tales with 50 generationsA princess lived in a castle.
She loved a knight.She was pretty.
She was blonde.
It had towers.
Itwas strong.Using the evolutionary module the same piece of tale isgenerated as follows:A pretty princess lived in a strong castle.
Shewas blonde.
The princess loved a brave knight.
Thecastle had towers.The second example shows that the texts generated by theevolutionary module are richer from the point of view of ad-jectives and structure.Note that depending on the number of genes you need acertain number of individuals and generations to achieve agood fitness value.
For example, ?The Lioness?, with 50genes, gets the maximum fitness with 50 generations and 500individuals, as long as ?Hansel and Gretel?
and ?Cinderella?would need more generations and individuals to get the max-imum fitness.Another important point is that in a specific tale, with aspecific number of genes, you can achieve the same resultsincreasing the number of generations or the size of the popu-lation.
For instance, ?The Merchant?, with 31 genes, gets themaximum fitness with both 25 or 50 generations with smallpopulations or 10 generations with populations of more than100 individuals.Finally, it is important to note that our approach presentssome differences respect to the one of Reiter and Dale [Re-iter and Dale, 1992].
As we have already mentioned, we areworking in the field of the fairy tales, with the specific re-quirements of story generation.
An important point is thatthese are not informative texts, and therefore we can relaxsome constraints taken into account in other works in the areaof referring expressions.6 Conclusions and future workWith respect to both of the tasks addressed, the output textsrespect the specific constraints required for the text to be ac-ceptable, while at the same time showing reasonable variationbetween the different options much as a human-generated textwould.
We are working on extending the system to allow theuse of proper nouns to describe some concepts, as an addi-tional option to pronouns and descriptive references, includ-ing the revision of the genetic operators and the introductionof new evaluation functions to estimate the correct applica-tion of proper nouns.In view of these results, in future work we want to applyEA techniques to other tasks of NLG, such as content de-termination and discourse planning.
The particular advan-tages of evolutionary techniques, combined stage by stage inthis manner, may be an extremely powerful method for solv-ing natural language generation problems while also profitingfrom classic NLG techniques.It would be also interesting to compare our solution withdifferent approaches found in the literature, as for example[Reiter and Dale, 1992] or [Krahmer and Theune, 2000] forthe referring expression generation, and the one of Dalianisand Hovy [Dalianis and Hovy, 1996] for the aggregation.Finally, an evaluation as the one proposed in [Callaway andLester, 2001] would be useful to estimate the goodness of thegenerated texts.
The authors describe the evaluation of STO-RYBOOK, a narrative prose generation system that producesoriginal fairy tales in the Little Red Riding Hood domain.They pretend to evaluate multiple versions of a single storyassuring that the content is identical across them.
Five ver-sions of two separate stories are produced, a pool of twentystudents in English compare them, and at last they are ana-lyzed with an ANOVA test.References[Cahill et al, 2001] L. Cahill, R. Evans, C. Mellish,D.
Paiva, M. Reape, and D. Scott.
The RAGS referencemanual.
Technical Report ITRI-01-07, Information Tech-nology Research Institute, University of Brighton, 2001.
[Callaway and Lester, 2001] C. Callaway and J. Lester.Evaluating the effects of natural language generation tech-niques on reader satisfaction.
In Proceedings of theTwenty-Third Annual Conference of the Cognitive ScienceSociety, Edinburgh, UK, 2001.
[Dalianis and Hovy, 1996] H. Dalianis and E. Hovy.
Ag-gregation in natural language generation.
In G. Ardoniand M. Zock, editors, Trends in Natural Language Gen-eration: an Artificial Intelligence Perspective,EWNLG?93,pages 88?105.
Springer Verlag, 1996.
[Duboue and McKeown, 2002] P.A.
Duboue and K.R.
McK-eown.
Content planner construction via evolutionary algo-rithms and a corpus-based fitness function.
In Proceedingsof the Second International Natural Language GenerationConference (INLG 2002), Ramapo Mountains, NY, 2002.[Garc?
?a et al, 2004] C.
Garc?
?a, R. Herva?s, and P. Gerva?s.Una arquitectura software para el desarrollo de aplica-ciones de generacio?n de lenguaje natural.
SociedadEspan?ola para el Procesamiento del Lenguaje Natural,Procesamiento de Lenguaje Natural, 33:111?118, 2004.
[Gerva?s et al, 2004] P. Gerva?s, B.
D?
?az-Agudo, F. Peinado,and R. Herva?s.
Story plot generation based on CBR.
InAnne Macintosh, Richard Ellis, and Tony Allen, editors,12th Conference on Applications and Innovations in In-telligent Systems, Cambridge, UK, 2004.
Springer, WICSseries.
[Holland, 1992] J.H.
Holland.
Adaptation in Natural andArtificial Systems.
An Introductory Analysis with Applica-tions to Biology, Control and Artificial Intelligence.
MITPress, Cambridge, Massachusetts, Second Edition, 1992.
[Kibble and Power, 2000] R. Kibble and R. Power.
An inte-grated framework for text planning and pronominalization.In Proc.
of the International Conference on Natural Lan-guage Generation (INLG), Israel, 2000.
[Krahmer and Theune, 2000] E. Krahmer and M. Theune.Efficient context-sensitive generation of referring expres-sions, 2000.
[Levy, 2001] R. P. Levy.
A computational model of poeticcreativity with neural network as measure of adaptive fit-ness.
In Proccedings of the ICCBR-01 Workshop on Cre-ative Systems, 2001.
[Manurung, 2003] H.M. Manurung.
An evolutionary algo-rithm approach to poetry generation.
PhD thesis, Schoolof Informatics, University of Edinburgh, 2003.
[Mellish et al, 1998] C. Mellish, A. Knott, J. Oberlander,and M. O?Donnell.
Experiments using stochastic searchfor text planning.
In Eduard Hovy, editor, Proceedingsof the Ninth International Workshop on Natural LanguageGeneration, pages 98?107.
Association for ComputationalLinguistics, New Brunswick, New Jersey, 1998.
[Reape and Mellish, 1999] M. Reape and C. Mellish.
Justwhat is aggregation anyway?
In Proceedings of the7th European Workshop on Natural Language Generation,Toulouse, France, 1999.
[Reiter and Dale, 1992] E. Reiter and R. Dale.
A fast algo-rithm for the generation of referring expressions.
In Pro-ceedings of the 14th conference on Computational linguis-tics, Nantes, France, 1992.
[Walker et al, 1998] M.A.
Walker, A.K.
Joshi, and E.F.Prince.
Centering Theory in Discourse.
Clarendon Press,Oxford, 1998.
