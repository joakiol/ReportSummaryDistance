Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708?717,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPDiscriminative Corpus Weight Estimation for Machine TranslationSpyros Matsoukas and Antti-Veikko I. Rosti and Bing ZhangBBN Technologies, 10 Moulton Street, Cambridge, MA 02138{smatsouk,arosti,bzhang}@bbn.comAbstractCurrent statistical machine translation(SMT) systems are trained on sentence-aligned and word-aligned parallel text col-lected from various sources.
Translationmodel parameters are estimated from theword alignments, and the quality of thetranslations on a given test set dependson the parameter estimates.
There areat least two factors affecting the parame-ter estimation: domain match and trainingdata quality.
This paper describes a novelapproach for automatically detecting anddown-weighing certain parts of the train-ing corpus by assigning a weight to eachsentence in the training bitext so as to op-timize a discriminative objective functionon a designated tuning set.
This way, theproposed method can limit the negative ef-fects of low quality training data, and canadapt the translation model to the domainof interest.
It is shown that such discrim-inative corpus weights can provide sig-nificant improvements in Arabic-Englishtranslation on various conditions, using astate-of-the-art SMT system.1 IntroductionStatistical machine translation (SMT) systems relyon a training corpus consisting of sentences inthe source language and their respective referencetranslations to the target language.
These paral-lel sentences are used to perform automatic wordalignment, and extract translation rules with asso-ciated probabilities.
Typically, a parallel trainingcorpus is comprised of collections of varying qual-ity and relevance to the translation problem of in-terest.
For example, an SMT system applied tobroadcast conversational data may be trained ona corpus consisting mostly of United Nations andnewswire data, with only a very small amount ofin-domain broadcast news/conversational data.
Inthis case, it would be desirable to down-weigh theout-of-domain data relative to the in-domain dataduring the rule extraction and probability estima-tion.
Similarly, it would be good to assign a lowerweight to data of low quality (e.g., poorly alignedor incorrectly translated sentences) relative to dataof high quality.In this paper, we describe a novel discrimina-tive training method that can be used to estimate aweight for each sentence in the training bitext so asto optimize an objective function ?
expected trans-lation edit rate (TER) (Snover et al, 2006) ?
on aheld-out development set.
The training bitext typ-ically consists of millions of (parallel) sentences,so in order to ensure robust estimation we expresseach sentence weight as a function of sentence-level features, and estimate the parameters of thismapping function instead.
Sentence-level fea-tures may include the identifier of the collection orgenre that the sentence belongs to, the number oftokens in the source or target side, alignment infor-mation, etc.
The mapping from features to weightscan be implemented via any differentiable func-tion, but in our experiments we used a simple per-ceptron.
Sentence weights estimated in this fash-ion are applied directly to the phrase and lexicalcounts unlike any previously published method tothe author?s knowledge.
The tuning framework isdeveloped for phrase-based SMT models, but thetuned weights are also applicable to the training ofa hierarchical model.
In cases where the tuning setused for corpus weight estimation is a close matchto the test set, this method yields significant gainsin TER, BLEU (Papineni et al, 2002), and ME-TEOR (Lavie and Agarwal, 2007) scores over astate-of-the-art hierarchical baseline.The paper is organized as follows.
Related workon data selection, data weighting, and model adap-tation is presented in Section 2.
The corpus weight708approach and estimation algorithm are describedin Section 3.
Experimental evaluation of the ap-proach is presented in Sections 4 and 5.
Section 6concludes the paper with a few directions for fu-ture work.2 Related WorkPrevious work related to corpus weighting maybe split into three categories: data selection, dataweighting, and translation model adaptation.
Thefirst two approaches may improve the qualityof the word alignment and prevent phrase-pairswhich are less useful for the domain to be learned.The model adaptation, on the other hand, mayboost the weight of the more relevant phrase-pairs or introduce translations for unseen sourcephrases.Resnik and Smith (2003) mined parallel textfrom the web using various filters to identify likelytranslations.
The filtering may be viewed as adata selection where poor quality translation arediscarded before word alignment.
Yasuda et al(2008) selected subsets of an existing parallel cor-pus to match the domain of the test set.
The dis-carded sentence pairs may be valid translationsbut they do not necessarily improve the translationquality on the test domain.
Mandal et al (2008)used active learning to select suitable training datafor human translation.
Hildebrand et al (2005) se-lected comparable sentences from parallel corporausing information retrieval techniques.Lu et al (2007) proposed weighting compara-ble portions of the parallel text before word align-ment based on information retrieval.
The relevantportions of the parallel text were given a higher in-teger weight in GIZA++ word alignment.
Similareffect may be achieved by replicating the relevantsubset in the training data.Lu et al (2007) also proposed training adaptedtranslation models which were interpolated with amodel trained on the entire parallel text.
Snoveret al (2008) used cross-lingual information re-trieval to identify possible bias-rules to improvethe coverage on the source side.
These rules maycover source phrases for which no translationswere learned from the available parallel text.Koehn and Schroeder (2007) described a pro-cedure for domain adaptation that was using twotranslation models in decoding, one trained onin-domain data and the other on out-of-domaindata.
Phrase translation scores from the two mod-els where combined in a log-linear fashion, withweights estimated based on minimum error ratetraining (Och, 2003) on a designated tuning set.The method described in this paper can also beviewed as data filtering or (static) translation adap-tation, but it has the following advantages overpreviously published techniques:1.
The estimated corpus weights are discrim-inative and are computed so as to directlyoptimize an MT performance metric on apre-defined development set.
Unlike the do-main adaptation technique in (Koehn andSchroeder, 2007), which also estimates theadaptation parameters discriminatively, ourproposed method does not require a man-ual specification of the in-domain and out-of-domain training data collections.
Instead,it automatically determines which collectionsare most relevant to the domain of interest,and increases their weight while decreasingthe weight assigned to less relevant collec-tions.2.
All sentences in the parallel corpus can in-fluence the translation model, as opposedto filtering/discarding data.
However, theproposed method can still assign very lowweights to parts of the corpus, if it determinesthat it helps improve MT performance.3.
The framework used for estimating the cor-pus weights can be easily extended to supportdiscriminative alignment link-level weights,thus allowing the system to automaticallyidentify which portions of the training sen-tences are most useful.Naturally, as with any method, the proposedtechnique has certain limitations.
Specifically, itis only concerned with influencing the translationrule probabilities via the corpus weights; it doesnot change the set of rules extracted.
Thus, it isunable to add new translation rules as in Snoveret al (2008).
Also, it can potentially lead to pa-rameter over-fitting, especially if the function thatmaps sentence features to weights is complex andbased on a large number of parameters, or if thedevelopment set used for estimating the mappingfunction does not match the characteristics of thetest set.7093 Corpus Weights Estimation3.1 Feature ExtractionThe purpose of feature extraction is to identify,for each sentence in the parallel training data, aset of features that can be useful in estimating aweight that is correlated with quality or relevanceto the MT task at hand.
Starting from sentence-aligned, word-aligned parallel training data, onecould extract various types of sentence-level fea-tures.
For example, we could specify features thatdescribe the two sides of the parallel data or thealignment between them, such as collection id,genre id, number of source tokens, number of tar-get tokens, ratio of number of source and targettokens, number of word alignment links, fractionof source tokens that are unaligned, and fractionof target tokens that are unaligned.
Additionally,we could include information retrieval (IR) relatedfeatures that reflect the relevance of a training sen-tence to the domain of interest, e.g., by measur-ing vector space model (VSM) distance of the sen-tence to the current tuning set, or its log likelihhodwith respect to an in-domain language model.Note that the collection and genre identifiers(ids) mentioned above are bit vectors.
Each col-lection in the training set is mapped to a number.A collection may consist of sentences from multi-ple genres (e.g., newswire, web, broadcast news,broadcast conversations).
Genres are also mappedto a unique number across the whole training set.Then, given a sentence in the training bitext, wecan extract a binary vector that contains two non-zero bits, one indicating the collection id, and an-other denoting the genre id.It is worth mentioning that in the experimentsreported later in this paper we made use of only thecollection and genre ids as features, although theframework supports general sentence-level fea-tures.3.2 Mapping Features to WeightsAs mentioned previously, one way to map a fea-ture vector to a weight is to use a perceptron.A multi-layer neural network may also be used,but at the expense of slower training.
In thiswork, all of the experiments carried out made useof a perceptron mapping function.
However, itis also possible to cluster the training sentencesinto classes by training a Gaussian mixture model(GMM) on their respective feature vectors1.
Then,given a feature vector we can compute the (poste-rior) probability that it was generated by one ofthe N Gaussians in the GMM, and use this N-dimensional vector of posteriors as input to theperceptron.
This is similar to having a neural net-work with a static hidden layer and Gaussian acti-vation functions.Given the many choices available in mappingfeatures to weights, we will describe the mappingfunction in general terms.
Let fibe the n ?
1feature vector corresponding to sentence i.
Let?(x;?)
denote a function Rn ?
(0, 1) that is pa-rameterized in terms of the parameter vector ?
andmaps a feature vector x to a scalar weight in (0, 1).The goal of the automatic corpus weight estima-tion procedure is to estimate the parameter vector?
so as to optimize an objective function on a de-velopment set.3.3 Training with Weighted CorporaOnce the sentence features have been mapped toweights, the translation rule extraction and prob-ability estimation can proceed as usual, but withweighted counts.
For example, let wi= ?(fi;?
)be the weight assigned to sentence i.
Let (s, t) bea source-target phrase pair that can be extractedfrom the corpus, and A(s) and B(t) indicating thesets of sentences that s and t occur in.
Then,P (s|t) =?j?A(s)?B(t)wjcj(s, t)?j?B(t)wjcj(t)(1)where cj(?)
denotes the number of occurrences ofthe phrase (or phrase-pair) in sentence j.3.4 Optimizing the Mapping FunctionEstimation of the parameters ?
of the mappingfunction ?
can be performed by directly optimiz-ing a suitable objective function on a developmentset.
Ideally, we would like to estimate the param-eters of the mapping function so as to directly op-timize an automatic MT performance evaluationmetric, such as TER or BLEU on the full transla-tion search space.
However, this is extremely com-putationally intensive for two reasons: (a) opti-mizing in the full translation search space requiresa new decoding pass for each iteration of opti-mization; and (b) a direct optimization of TER or1Note that in order to train such a GMM it may be nec-essary to first apply a decorrelating, dimensionality reducing,transform (e.g., principal component analysis) to the features.710BLEU requires the use of a derivative free, slowlyconverging optimization method such as MERT(Och, 2003), because these objective functions arenot differentiable.In our case, for every parameter vector updatewe need to essentially retrain the translation model(reestimate the phrase and lexical translation prob-abilities based on the updated corpus weights), sothe cost of each iteration is significantly higherthan in a typical MERT application.
For these rea-sons, in this work we chose to minimize the ex-pected TER over a translation N-best on a desig-nated tuning set, which is a continuous and differ-entiable function and can be optimized with stan-dard gradient descent methods in a small numberof iterations.
Note, that using expected TER is notthe only option here; any criterion that can be ex-pressed as a continuous function of the phrase orlexical translation probabilities can be used to op-timize ?.Given an N-best of translation hypotheses overa development set of S sentences, we can definethe expected TER as followsT =?Ss=1?Nsj=1psjsj?Ss=1rs(2)where Nsis the number of translation hypothe-ses available for segment s; sjis the minimumraw edit distance between hypothesis j of seg-ment s (or hsj, for short) and the reference transla-tion(s) corresponding to segment s; rsis the aver-age number of reference translation tokens in seg-ment s, and psjis the posterior probability of hy-pothesis hsjin the N-best.
The latter is computedas followspsj=e?Lsj?Nsk=1e?Lsk(3)where Lsjis the total log likelihood of hypothe-sis hsj, and ?
is a tunable scaling factor that canbe used to change the dynamic range of the likeli-hood scores and hence the distribution of posteri-ors over the N-best.
The hypothesis likelihood Lsjis typically computed as a dot product of a decod-ing weight vector and a vector of various ?feature?scores, such as log phrase translation probability,log lexical translation probability, log n-gram lan-guage model probability, and number of tokens inthe hypothesis.
However, in order to simplify thispresentation we will assume that it contains a sin-gle translation model score, the log phrase transla-tion probability of source given target.
This scoreis a sum of log conditional probabilities, similarto the one defined in Equation 1.
Therefore, Lsjis indirectly a function of the training sentenceweights.In order to minimize the expected TER T , weneed to compute the derivative of T with respectto the mapping function parameters ?.
Using thechain rule, we get equations (4)-(8), where thesummation in Equation 6 is over all source-targetphrase pairs in the derivation of hypothesis hsm, ?is the decoding weight assigned to the log phrasetranslation score, and the summation in Equation7 is over all training sentences2.Thus, in order to compute the derivative ofthe objective function we first need to calculate?
lnP (sk|tk)?
?for every phrase pair (sk, tk) in thetranslation N-best based on Equations 7 and 8,which requires time proportional to the number ofoccurrences of these phrases in the parallel train-ing data.
After that, we can compute ?Lsm?
?foreach hypothesis hsm, based on Equation 6.
Fi-nally, we calculate ?
ln psj?
?and ?T?
?based on Equa-tions 5 and 4, respectively.3.5 Implementation IssuesIn our system, the corpus weights were trainedbased on N-best translation hypotheses generatedby a phrase-based MT system on a designated tun-ing set.
Each translation hypothesis in the N-besthas a score that is a (linear) function of the fol-lowing log translation probabilities: target phrasegiven source phrase, source phrase given targetphrase, and lexical smoothing term.
Additionally,each hypothesis specifies information about itsderivation, i.e., which source-target phrase pairs itconsists of.
Therefore, given an N-best, we canidentify the set of unique phrase pairs and use thisinformation in order to perform a filtered accumu-lation of the statistics needed for calculating thederivative in Equation 8.
This reduces the storageneeded for the sufficient statistics significantly.Minimization of the expected TER of the N-best hypotheses was performed using the limited-memory BFGS algorithm (Liu and Nocedal,1989).
Typically, the parameter vector ?
requiredabout 30 iterations of LBFGS to converge.Since the N-best provides only a limited repre-sentation of the MT hypothesis search space, weregenerated the N-best after every 30 iterations2In the general case where Lsjincludes other translationscores, e.g., lexical translation probabilities, the derivative?Lsm?
?will have to include additional terms.711?T??=S?s=1Ns?j=1?T?
ln psj?
ln psj??=(1?Ss=1rs)S?s=1Ns?j=1psjsj?
ln psj??(4)?
ln psj??=Ns?m=1?
ln psj?Lsm?Lsm?
?= ?(?Lsj???Ns?m=1psm?Lsm??)(5)?Lsm??=?(sk,tk)?hsm?Lsm?
lnP (sk|tk)?
lnP (sk|tk)??=?(sk,tk)?hsm??
lnP (sk|tk)??(6)?
lnP (sk|tk)??=?i?
lnP (sk|tk)?wi?wi??(7)?
lnP (sk|tk)?wi=?j?A(sk)?B(tk)?
(j ?
i) cj(sk, tk)?j?A(sk)?B(tk)wjcj(sk, tk)??j?B(tk)?
(j ?
i) cj(tk)?j?B(tk)wjcj(tk)(8)?
(x) ={1 x = 00 x 6= 0(9)of LBFGS training, merging new hypotheses withtranslations from previous iterations.
The overalltraining procedure is described in more detail be-low:1.
Initialize parameter vector ?
to small randomvalues, so that all training sentences receiveapproximately equal weights.2.
Initialize phrase-based MT decoding weightsto previously tuned values.3.
Perform weighted phrase rule extraction asdescribed in Equation 1, to estimate thephrase and lexical translation probabilities.4.
Decode the tuning set, generating N-best.5.
Merge N-best hypotheses from previous iter-ations to current N-best.6.
Tune decoding weights so as to minimizeTER on merged N-best, using a derivativefree optimization method.
In our case, weused Powell?s algorithm (Powell, 1964) mod-ified by Brent as described in (Brent, 1973) 3.7.
Identify set of unique source-target phrasepairs in merged N-best.8.
Extract sufficient statistics from training datafor all phrases identified in step 7.3This method was first used for N-best based parameteroptimization in (Ostendorf et al, 1991).9.
Run the LBFGS algorithm to minimize theexpected TER in the merged N-best, usingthe derivative equations described previously.10.
Assign a weight to each training sentencebased on the ?
values optimized in 9.11.
Go to step 3.Typically, the corpus weights converge in about4-5 main iterations.
The calculation of the deriva-tive is parallelized to speed up computation, re-quiring about 10 minutes per iteration of LBFGS.4 Experimental SetupIn this section we describe the setup that was usedfor all experiments reported in this paper.
Specif-ically, we provide details about the training data,development sets, and MT systems (phrase-basedand hierarchical).4.1 Training DataAll MT training experiments made use of anArabic-English corpus of approximately 200 mil-lion tokens (English side).
Most of the collectionsin this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly partof the resources specified for the constrained datatrack of the NIST MT evaluation4.4For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf712The corpus includes data from multiple gen-res, as shown in Table 1.
The ?Sakhr?
newswirecollection is a set of Arabic-to-English andEnglish-to-Arabic data provided by Sakhr Soft-ware, totaling about 30.8 million tokens, andis only available to research teams participat-ing in the Defense Advanced Research ProjectsAgency (DARPA) Global Autonomous LanguageExploitation (GALE) program.
The ?LDC Giga-word (ISI)?
collection was produced by automati-cally detecting and extracting portions of paralleltext from the monolingual LDC Arabic and En-glish Gigaword collections, using a method devel-oped at the Information Sciences Institute (ISI) ofthe University of Southern California.Data Origin Style Size(K tokens)LDC pre-GALEU.
Nations 118049Newswire 2700Treebank 685LDC post-GALENewswire 14344Treebank 292Web 478Broad.
News 573Broad.
Conv.
1003Web-found text Lexicons 436Quran 406Sakhr Newswire 30790LDC Gigaword Newswire 29169(ISI)Table 1: Composition of the Arabic-English par-allel corpus used for MT training.It is easy to see that most of the parallel train-ing data are either newswire or from United Na-tions.
The amount of web text or broadcastnews/conversations is only a very small fractionof the total corpus.
In total, there are 31 collec-tions in the training bitext.
Some collections (es-pecially those released recently by LDC for theGALE project) consist of data from multiple gen-res.
The total number of unique genres (or datatypes) in the training set is 10.Besides the above bitext, we also used approxi-mately 8 billion words of English text for languagemodel (LM) training (3.7B words from the LDCGigaword corpus, 3.3B words of web-downloadedtext, and 1.1B words of data from CNN archives).This data was used to train two language mod-els: an entropy-pruned trigram LM, used in decod-ing, and an unpruned 5-gram LM used in N-bestrescoring.
Kneser-Ney smoothing was applied tothe n-grams in both cases.4.2 Development SetsThe development sets used for tuning and testingthe corpus weights and other MT settings werecomprised of documents from previous Arabic-English NIST MT evaluation sets and from GALEdevelopment/evaluation sets.Specifically, the newswire Tune and Test setsconsist of documents from the following col-lections: the newswire portion of NIST MT04,MT05, MT06, and MT08 evaluation sets, theGALE Phase 1 (P1) and Phase 2 (P2) evaluationsets, and the GALE P2 and P3 development sets.The web Tune and Test sets are made of docu-ments from NIST MT06 and MT08, the GALE P1and P2 evaluation sets, the GALE P2 and P3 devel-opment sets, and a held-out portion of the GALEyear 1 quarter 4 web training data release.The audio Tune and Test sets consist of roughlyequal parts of news and conversations broadcastfrom November 2005 through May 2007 by ma-jor Arabic-speaking television and radio stations(e.g., Al-Jazeera, Al-Arabiya, Syrian TV), totalingapproximately 14 hours of speech.
The audio wasprocessed through automated speech recognition(ASR) in order to produce (errorful) transcriptsthat were used as input to all MT decoding experi-ments reported in this paper.
However, the corpusweight estimation was carried out based on N-bestMT of the Arabic audio reference transcriptions(i.e., the transcripts had no speech recognition er-rors, and contained full punctuation).It is important to note that some of the docu-ments in the above devsets have multiple referencetranslations (usually 4), while others have onlyone.
Most of the documents in the newswire setshave 4 references, but unfortunately the web andaudio sets have, on average, less than 2 referencetranslations per segment.
More details are listed inTable 2.Another important note is that, although the au-dio sets consist of both broadcast news (BN) andbroadcast conversations (BC), we did not performBN or BC-specific tuning.
Corpus weights andMT decoding parameters were optimized based ona single Tune set, on a mix of BN and BC data.However, when we report speech translation re-sults in later sections, we break down the perfor-713Genre Tune Test#segs #tokens #refs/seg #segs #tokens #refs/segNewswire 1994 72359 3.94 3149 115700 3.67Web 3278 99280 1.69 4425 125795 2.08Audio BN 897 32990 1.00 1530 53067 1.00Audio BC 765 24607 1.00 1416 44435 1.00Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabicnewswire, web, and audio.
The audio sets include material from both broadcast news and broadcastconversations.mance by genre.4.3 MT SystemsExperiments were performed using two types ofstatistical MT systems: a phrase-based system,similar to Pharaoh (Koehn, 2004), and a state-of-the-art, hierarchical string-to-dependency-treesystem, similar to (Shen et al, 2008).The phrase-based MT system employs a pruned3-gram LM in decoding, and can optionally gen-erate N-best unique translation hypotheses whichare used to estimate the corpus weights, as de-scribed in Section 3.The hierarchical MT system performs decodingwith the same 3-gram LM, generates N-best ofunique translation hypotheses, and then rescoresthem using a large, unpruned 5-gram LM in orderto select the best scoring translation.
It is worthmentioning that this hierarchical MT system pro-vides a very strong baseline; it achieves a case-sensitive BLEU score of 52.20 on the newswireportion of the NIST MT08 evaluation set, whichis similar to the score of the second-best systemthat participated in the unconstrained data track ofthe NIST MT08 evaluation.Both types of models were trained on the sameword alignments generated by GIZA++ (Och andNey, 2003).5 ResultsIn this section we report results on the Arabicnewswire, web, and audio development sets, us-ing both phrase-based and hierarchical MT sys-tems, in terms of TER, BLEU5, and METEOR(Lavie and Agarwal, 2007).
Whenever corpusweights are used, they were estimated on the des-ignated Tune set using the phrase-based MT sys-5The brevity penalty was calculated using the formula inthe original IBM paper, rather than the more recent definitionimplemented in the NIST mteval-v11b.pl script.tem.
Only the collection and genre ids were usedas sentence features in order to estimate the corpusweights.
As mentioned in Section 4.1, the train-ing bitext consists of 31 collections and 10 gen-res, so each training sentence was assigned a 41-dimensional binary vector indicating its particu-lar collection/genre combination.
That vector wasthen mapped into a single weight using a percep-tron.5.1 Phrase-based MTResults using the phrase-based MT system areshown in Table 3.
In all cases, the decodingweights were optimized so as to minimize TERon the designated Tune set.
On newswire, thediscriminative corpus weights provide 0.8% abso-lute gain in TER, in both Tune and Test sets.
Onweb, the TER gain is 0.9% absolute on Tune and0.5% on Test.
On the audio Test set, the TER gainis 0.5% on BN and 1.4% on BC.
Significant im-provements were also obtained in the BLEU andMETEOR scores, on all sets and conditions.5.2 Hierarchical MTResults using the hierarchical MT system areshown in Table 4.
The hierarchical systemused different tuning criteria in each genre.
Onnewswire, the decoding weights were optimizedso as to maximize BLEU, while on web and audiothe tuning was based on 0.5TER+0.5(1?BLEU)(referred to as TERBLEU in what follows).
Notethat these were the criteria for tuning the decodingweights; whenever corpus weights were used, theywere taken from the phrase-based system.It is interesting to see that gains from discrimi-native corpus weights carry over to the more pow-erful hierarchical MT system.
On newswire Test,the gain in BLEU is 0.8; on web Test, the gain inTERBLEU is 0.3.
On the audio Test set, the cor-pus weights provide 0.7 and 0.75 TERBLEU re-duction on BN and BC, respectively.
As with the714Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTRTune No 42.3 48.2 67.5 60.0 21.9 51.3Yes 41.5 49.6 68.7 59.1 22.8 52.3Test No 43.2 46.2 66.5 58.6 24.2 52.2Yes 42.4 47.5 67.8 58.1 25.4 52.9(a) Results on Arabic text.Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTRTune No 56.0 22.9 55.5 57.3 21.7 55.0Yes 55.0 25.0 57.1 56.1 23.6 56.4Test No 53.0 25.3 57.7 55.9 22.9 55.4Yes 52.5 26.6 58.8 54.5 24.7 56.8(b) Results on Arabic audio.Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets.
Decodingweights were optimized on the Tune set in order to directly minimize TER.
Corpus weights were alsooptimized on Tune set, but based on expected TER.phrase-based system, all metrics improve from theuse of corpus weights, in all sets/conditions.6 ConclusionsWe have described a novel approach for estimat-ing a weight for each sentence in a parallel train-ing corpus so as to optimize MT performance of aphrase-based statistical MT system.
The sentenceweights influence MT performance by being ap-plied to the phrase and lexical counts during trans-lation rule extraction and probability estimation.In order to ensure robust training of the weights,we expressed them as a function of sentence-levelfeatures.
Then, we defined the process for opti-mizing the parameters of that function based onthe expected TER of a translation hypothesis N-best on a designated tuning set.The proposed technique was evaluated in thecontext of Arabic-English translation, on multipleconditions.
It was shown that encouraging resultswere obtained by just using collection and genreids as features.
Interestingly, the discriminativecorpus weights were found to be generally appli-cable and provided gains in a state-of-the-art hi-erarchical string-to-dependency-tree MT system,even though they were trained using the phrase-based MT system.Next step is to include other sentence-level fea-tures, as described in Section 3.1.
Finally, thetechnique described in this paper can be extendedto address the estimation of weights at the align-ment link level, based on link-level features.
Webelieve that this will have a larger impact on thelexical and phrase translation probabilities, sincethere is a large number of parallel training sen-tences that are partially correct, i.e., they containparts that are aligned and translated correctly, andparts that are wrong.
The current procedure triesto assign a single weight to such sentences, sothere is no way to distinguish between the ?good?and ?bad?
portions of each sentence.
Pushing theweight estimation at the alignment link level willalleviate this problem and will make the discrimi-native training more targeted.AcknowledgmentsThis work was supported by DARPA/IPTO Con-tract No.
HR0011-06-C-0022 under the GALEprogram.ReferencesRichard P. Brent.
1973.
Algorithms for MinimizationWithout Derivatives.
Prentice-Hall.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the transla-715Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTRTune No 39.5 54.4 70.3 58.2 25.2 53.8Yes 38.8 55.6 71.2 58.0 25.5 54.0Test No 40.7 52.1 69.3 57.0 28.3 54.7Yes 40.1 52.9 69.8 56.6 28.5 55.0(a) Results on Arabic text.Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTRTune No 54.9 27.3 58.0 55.8 26.1 57.4Yes 53.6 28.2 59.0 54.9 26.9 58.0Test No 51.6 29.9 60.0 54.4 27.6 57.7Yes 50.7 30.4 60.7 53.2 27.9 58.7(b) Results on Arabic audio.Table 4: Hierarchical 5-gram rescoring results on the Arabic text and audio development sets.
Decod-ing/rescoring weights were optimized on the Tune set in order to directly maximize BLEU (for newswire)or minimize TERBLEU (for web and audio).
Corpus weights were the same as the ones used in the cor-responding phrase-based decodings.tion model for statistical machine translation basedon information retrieval.
In Proceedings of the 10thAnnual Conference of European Association for Ma-chine Translation, pages 133?142.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine trans-lation.
In Proceedings of the Second Workshop onStatistical Machine Translation, pages 224?227.Philipp Koehn.
2004.
Pharaoh: a beam search de-coder for phrase-based statistical machine transla-tion models.
In Proceedings of the 6th Conferenceof the Association for Machine Translation in theAmericas.Alon Lavie and Abhaya Agarwal.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 228?231.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503?528.Yajuan Lu, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by train-ing data selection and optimization.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 343?350.Arindam Mandal, Dimitra Vergyri, Wen Wang, JingZheng, Andreas Stolcke, Gokhan Tur, DilekHakkani-Tu?r, and Necip Fazil Ayan.
2008.
Effi-cient data selection for machine translation.
In Pro-ceedings of the Second IEEE/ACL Spoken LanguageTechnology Workshop, pages 261?264.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167.M.
Ostendorf, A. Kannan, S. Austin, O. Kimball,R.
Schwartz, and J. R. Rohlicek.
1991.
Integra-tion of diverse recognition methodologies throughreevaluation of nbest sentence hypotheses.
In Pro-ceedings of the DARPA Workshop on Speech andNatural Language, pages 83?87.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics, pages 311?318.M.
J. D. Powell.
1964.
An efficient method for findingthe minimum of a function of several variables with-out calculating derivatives.
The Computer Journal,pages 155?162.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.716Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 577?585.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciula, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Conference of the Associa-tion for Machine Translation in the Americas, pages223?231.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptationusing comparable corpora.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 857?866.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,and Eiichiro Sumita.
2008.
Method of selectingtraining data to build a compact and efficient trans-lation model.
In Proceedings of the Third Interna-tional Joint Conference on Natural Language Pro-cessing, volume II, pages 655?660.717
