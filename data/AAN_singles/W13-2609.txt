Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75?83,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsConcreteness and Corpora: A Theoretical and Practical AnalysisFelix HillComputer LaboratoryUniversity of Cambridgefh295@cam.ac.ukDouwe KielaComputer LaboratoryUniversity of Cambridgedlk427@cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of Cambridgealk23@cam.ac.ukAbstractAn increasing body of empirical evidencesuggests that concreteness is a fundamentaldimension of semantic representation.
By im-plementing both a vector space model and aLatent Dirichlet Allocation (LDA) Model, weexplore the extent to which concreteness is re-flected in the distributional patterns in corpora.In one experiment, we show that that vectorspace models can be tailored to better modelsemantic domains of particular degrees ofconcreteness.
In a second experiment, weshow that the quality of the representations ofabstract words in LDA models can be im-proved by supplementing the training datawith information on the physical properties ofconcrete concepts.
We conclude by discussingthe implications for computational systemsand also for how concrete and abstract con-cepts are represented in the mind1 IntroductionA growing body of theoretical evidence empha-sizes the importance of concreteness to semanticrepresentations.
This fact has not been widelyexploited in NLP systems, despite its clear theo-retical relevance to tasks such as word-sense in-duction and compositionality modeling.
In thispaper, we take a first step towards integratingconcreteness into NLP by testing the extent towhich it is reflected by the superficial (distribu-tional) patterns in corpora.
The motivation isboth theoretical and practical: We consider theimplications for the development of computa-tional systems and also for how concrete and ab-stract concepts are represented in the humanmind.
Experimenting with two popular methodsof extracting lexical representations from text,we show both that these approaches are sensitiveto concreteness and that their performance can beimproved by adapting their implementation tothe concreteness of the domain of application.
Inaddition, our findings offer varying degrees ofsupport to several recent proposals about concep-tual representation.In the following section we review recenttheoretical and practical work.
In Section 3 weexplore the extent to which concreteness is re-flected by Vector-Space Models of meaning(VSMs), and in Section 4 we conduct a similaranalysis for (Bayesian) Latent Dirichlet Alloca-tion (LDA) models.
We conclude, in Section 5,by discussing practical and theoretical implica-tions.2 Related work2.1 ConcretenessEmpirical evidence indicates important cognitivedifferences between abstract concepts, such asguilt or obesity, and concrete concepts, such aschocolate or cheeseburger.
It has been shownthat concrete concepts are more easily learnedand remembered than abstract concepts, and thatlanguage referring to concrete concepts is moreeasily processed (Schwanenflugel, 1991).
Thereare cases of brain damage in which either ab-stract or concrete concepts appear to be specifi-cally impaired (Warrington, 1975), and function-al magnetic resonance imaging (fMRI) studiesimplicate overlapping but partly distinct neuralsystems in the processing of the two concepttypes (Binder et al 2005).
Further, there is in-creasing evidence that concrete concepts arerepresented via intrinsic properties whereas ab-stract representations encode extrinsic relationsto other concepts (Hill et al in press).
However,while these studies together suggest that con-creteness is fundamental to human conceptualrepresentation, much remains to be understoodabout the precise cognitive basis of the ab-stract/concrete distinction.
Indeed, the majorityof theoretically motivated studies of conceptualrepresentation focus on concrete domains, and75comparatively little has been established empiri-cally about abstract concepts.Despite this support for the cognitive impor-tance of concreteness, its application to computa-tional semantics has been limited to date.
Onepossible reason for this is the difficulty in mea-suring lexical concreteness using corpora alone(Kwong, 2008).
Turney et al(2011) overcomethis hurdle by applying a semi-supervised me-thod to quantify noun concreteness.
Using thisdata, they show that a disparity in the concrete-ness between elements of a construction can faci-litate metaphor identification.
For instance, in theexpressions kill the process or black comedy, averb or adjective that generally occurs with aconcrete argument takes an abstract argument.Turney et alshow that a supervised classifiercan exploit this effect to correctly identify 79%of adjective-noun and verb-object constructionsas literal or metaphorical.
Although these resultsare clearly promising, to our knowledge Turneyet als paper is unique in integrating corpus-based methods and concreteness in NLP systems.1.2 Association / similarityA proposed distinction between abstract andconcrete concepts that is particularly importantfor the present work relates to the semantic rela-tions association and (semantic) similarity (seee.g.
Crutch et al2009; Resnik, 1995).
The dif-ference between these relations is exemplified bythe concept pairs {car, petrol} and {car, van}.Car is said to be (semantically) similar to van,and associated with (but not similar to) petrol.Intuitively, the basis for the similarity of car andbike may be their common physical features(wheels) or the fact that they fall within a clearlydefinable category (modes of transport).
In con-trast, the basis for the association between carand petrol may be that they are often found to-gether or the clear functional relationship be-tween them.
The two relations are neither mu-tually exclusive nor independent; bike and carare related to some degree by both associationand similarity.Based on fresults of behavioral experiments,Crutch et al(2009) make the following proposalconcerning how association and similarity inte-ract with concreteness:(C) The conceptual organization of abstract con-cepts is governed by association, whereas theorganization of concrete concepts is governed bysimilarity.Crutch et als hypothesis derives from experi-ments in which participants selected the odd-one-out from lists of five words appearing on ascreen.
The lists comprised either concrete orabstract words (based on ratings of six infor-mants) connected either by similarity (e.g.
dog,wolf, fox etc.
; theft, robbery, stealing etc.)
orassociation (dog, bone, collar etc.
; theft, law, vic-tim etc.
), with an unrelated odd-one-out item ineach list.
Controlling for frequency and position,subjects were both significantly faster and moreaccurate if the related words were either abstractand associated or concrete and similar.
Theseresults support (C) on the basis that decisiontimes are faster when the related items form amore coherent group, rendering the odd-one outmore salient.
Hill et al(in press) tested the samehypothesis on a larger scale, analyzing over18,000 concept pairs scored by human annotatorsfor concreteness as well as the strength of associ-ation between them.
They found a moderate in-teraction between concreteness and the correla-tion between association strength and similarity(as measured using WordNet), but concludedthat the strength of the effect was not sufficientlystrong to either confirm or refute (C).Against this backdrop, the present work ex-amines how association, similarity and concrete-ness are reflected in LDA models and, first,VSMs.
In both cases we test Hypothesis (C) andrelated theoretical proposals, and discuss whetherthese findings can lead to better performing se-mantic models.3 Vector Space ModelsVector space models (VSMs) are perhaps themost common general method of extracting se-mantic representations from corpora (Sahlgren,2006; Turney & Pantel, 2010).
Words arerepresented in VSMs as points in a (geometric)vector space.
The dimensions of the space cor-respond to the model features, which in the sim-plest case are high frequency words from thecorpus.
In such models, the position of a wordrepresentation along a given feature dimensiondepends on how often that word occurs within aspecified proximity to tokens of the feature wordin the corpus.
The exact proximity required is animportant parameter for model implementation,and is referred to as the context window.
Finally,the degree to which two word representations arerelated can be calculated as some function of thedistance between the corresponding points in thesemantic space.763.1 MotivationVSMs are well established as a method of quan-tifying relations between word concepts and haveachieved impressive performance in related NLPtasks (Sahlgren, 2006; Turney & Pantel, 2010).In these studies, however, it is not always clearexactly which semantic relation is best reflectedby the implemented models.
Indeed, researchhas shown that by changing certain parametersettings in the standard VSM architecture, mod-els can be adapted to better reflect one relationtype or another.
Specifically, models withsmaller context windows are reportedly better atreflecting similarity, whereas models with largerwindows better reflect association.
(Agirre et al2009; Peirsman et al 2008)Our experiments in this section aim first tocorroborate these findings by testing how modelsof varying context window sizes perform on em-pirical data of both association and similarity.We then test if this effect differentially affectsperformance on concrete and abstract words.3.2 MethodWe employ a conventional VSM design, extract-ing representations from the (unlemmatised)British National Corpus (Leech et al 1994) withstopwords removed.
In the vector representationof each noun, our dimension features are the50,000 most frequently occurring (non-stopword) words in the corpus.
We experimentwith window sizes of three, five and nine (one,two and four words either side of the noun,counting stopwords).
Finally, we apply point-wise mutual information (PMI) weighting of ourco-occurrence frequencies, and measure similari-ty between weighted noun vectors by the cosineof the angle between them in the vector space.To evaluate modeling of association, we usethe University of South Florida (USF) Free-association Norms (Nelson & McEvoy, 2012).The USF data consist of over 5,000 words pairedwith their free associates.
To elicit free asso-ciates, more than 6,000 participants were pre-sented with cue words and asked to ?write thefirst word that comes to mind that is meaningful-ly related or strongly associated to the presentedword?.
For a cue word c and an associate a, theforward association strength (association) fromc to a is the proportion of participants who pro-duced a when presented with c.  association isthus a measure of the strength of an associaterelative to other associates of that cue.
The USFdata is well suited to our purpose because manycues and associates in the data have a concrete-ness score, taken from either the norms of Paivio,Yuille and Madigan (1968) or Toglia and Battig(1978).
In both cases contributors were asked torate words based on a scale of 1 (very abstract) to7 (very concrete).1  We extracted the all 2,230nouns from the USF data for which concretenessscores were known, yielding a total of 15,195noun-noun pairs together with concreteness andassociation values.Although some empirical word-similarity da-tasets are publically available, they contain few ifany abstract words (Finkelstein et al 2002; Ru-benstein & Goodenough, 1965).
Therefore toevaluate similarity modeling, we use Wu-PalmerSimilarity (similarity) (Wu & Palmer, 1994), aword similarity metric based on the position ofthe senses of two words in the WordNet taxono-my (Felbaum, 1998).
similarity can be applied toboth abstract and concrete nouns and achieves ahigh correlation, with human similarity judg-ments (Wu & Palmer, 1994).23.3 ResultsIn line with previous studies, we observed thatVSMs with smaller window sizes were betterable to predict similarity.
The model with win-dow size 3 achieves a higher correlation withsimilarity (Spearman rank rs  = -0.29) than themodel with window size 9 (rs  = -0.25).
Howev-er, the converse effect for association was notobserved: Model correlation with associationwas approximately constant over all window siz-es.
These effects are illustrated in Fig.
1.1Although concreteness is well understood intuitively, itlacks a universally accepted definition.
It is often describedin terms of reference to sensory experience (Paivio et al1968), but also connected to specificity; rose is often consi-dered more concrete than flora.
The present work does notaddress this ambiguity.2 similarity achieves a Pearson correlation of r  = .80 onthe  30 concrete word pairs in the Miller & Charles (1991)data.0.123 0.1250.120.2860.290.2410.00.10.20.33 5 9WindowCorrelationAssociationSimilarity77Figure 1:  Spearman correlations between VSM out-put and association and similarity for different win-dow sizes.In addressing the theoretical Hypothesis (C) wefocused on the output of our VSM of windowsize five, although the same trends were ob-served over all three models.
Over all 18,195noun-noun pairs the correlation between themodel output and association was significant (rs= 0.13, p < 0.001) but notably lower than the cor-relation with similarity (rs  = -0.29, p < 0.001).To investigate the effect of concreteness, weranked each pair in our sample by the total con-creteness of both nouns, and restricted our analy-sis to the 1000 most concrete and 1000 most ab-stract pairs.
The models captured associationbetter over the abstract pairs than concrete con-cepts, but reflected similarity better over the con-crete concepts.
The strength of this effect is illu-strated in Fig.
2.Figure 2: Spearman correlation values between VSMoutput and similarity and association over subsets ofconcrete and abstract pairs.Given that small window sizes are optimal formodeling similarity, and that WSMs appear tomodel similarity better over concrete conceptsthan over abstract concepts, we explored whetherdifferent window sizes were optimal for eitherabstract or concrete word pairs.
When comparingthe model output to association, no interactionbetween window size and concreteness was ob-served.
However, there was a notable interactionwhen considering performance in modeling simi-larity.
As illustrated in Fig.
3, performance onconcrete word pairs is better for smaller windowsizes, whereas with abstract word pairs a largerwindow size is preferable.Figure 3:  Spearman correlation values between VSMoutput and similarity and association for differentwindow sizes over abstract and concrete word pairsubsets3.4 ConclusionOur results corroborate the body of VSM re-search that reports better performance from smallwindow sizes in modeling similarity.
A likelyexplanation for this finding is that similarity is aparadigmatic relation: Two similar entities canbe plausibly exchanged in most linguistic con-texts.
Small context windows emphasize prox-imity, which loosely reflects structural relation-ships such as verb-object, ensuring that paradig-matically related entities score highly.
Modelswith larger context windows cannot discern pa-radigmatically and syntagmatically related enti-ties in this way.
The performance of our modelson the association dataset did not support theconverse conclusion that larger window sizesperform better.
Overall, each of the three modelswas notably better at capturing similarity thanassociation.
This suggests that the core architec-ture of WSMs is not well suited to modeling as-sociation.
Indeed, ?first order?
models that di-rectly measure word co-occurrences, rather thanconnecting them via features, seem to performbetter at this task (Chaudhari et al 2011).
Thisfact is consistent with the view that association isa more basic or fundamental semantic relationfrom which other more structured relations arederived.The fact that the USF association data re-flects the instinctive first response of participantswhen presented with a cue word is important forinterpreting the results with respect to Hypothe-sis (C).
Our findings suggest that VSMs are bet-ter able to model this data for abstract word pairsthan for concrete word pairs.
This is consistentwith the idea that language fundamentally deter-mines which abstract concepts come to be asso-ciated or connected in the mind.
Conversely, the0.1250.1080.2150.2970.00.10.20.3Association SimilarityRelation TypeCorrelationAbstractConcrete0.2020.2720.2230.2540.140.0860.1040.0990.00.10.23 - Similarity 9 - Similarity 3 - Assoc.
9 - Assoc.Window size - Relation typeCorrelationAbstractConcrete78fact that the model reflects associations betweenconcrete words less well suggests that the impor-tance of extra-linguistic information is lower forconnecting concrete concepts in this instinctiveway.
Indeed, it seems plausible that the processby which concrete concepts become associatedinvolves visualization or some other form of per-ceptual reconstruction.
Consistent with Hypothe-sis (C), this reconstruction, which is not possiblefor abstract concepts, would naturally reflect si-milarity to a greater extent than linguistic contextalone.Finally, when modeling similarity, the ad-vantage of a small window increases as thewords become more concrete.
Similarity be-tween concrete concepts is fundamental to cogni-tive theories involving the well studied notionsof prototype and categorization (Rosch, 1975;Rogers & McClelland, 2003).
In contrast, thecomputation of abstract similarity is intuitively amore complex cognitive operation.
Although theaccurate quantification of abstract similarity maybe beyond existing corpus-based methods, ourresults suggest that a larger context windowcould in fact be marginally preferable shouldVSMs be applied to this task.Overall, our findings show that the design ofVSMs can be tailored to reflect particular seman-tic relations and that this in turn can affect theirperformance on different semantic domains, par-ticularly with respect to concreteness.
In thenext section, we investigate whether the sameconclusions should apply to a different class ofdistributional model.4 Latent Dirichlet Allocation ModelsLDA models are trained on corpora that are di-vided into sections (typically documents), ex-ploiting the principle that words appearing in thesame document are likely to have similar mean-ings.
In an LDA model, the sections are viewedas having been generated by random samplingfrom unknown latent dimensions, which arerepresented as probability distributions (Dirichletdistributions) over words.
Each document canthen be represented by a probability distributionover these dimensions, and by considering themeaning of the dimensions, the meaning of thedocument can be effectively characterized.
Moreimportantly, because each latent dimension clus-ters words of a similar meaning, the output ofsuch models can be exploited to provide highquality lexical representations (Griffiths et al2007).
Such a word representation encodes theextent to which each of the latent dimensionsinfluences the meaning of that word, and takesthe form of a probability distribution over thesedimensions.
The degree to which two words arerelated can then be approximated by any functionthat measures the similarity or difference be-tween distributions.4.1 MotivationIn recent work, Andrews et al(2009) exploreways in which LSA models can be modified toimprove the quality of their lexical representa-tions.
They propose that concepts are acquiredvia two distinct information sources: experientialdata ?
the perceptible properties of objects, anddistributional data ?
the superficial patterns oflanguage.
To test this hypothesis, Andrews et alconstruct three different LDA models, onetrained on experiential data, one trained in theconventional manner on running text, and onetrained on the same text but with the experientialdata appended.
They evaluate the quality of thelexical representations in the three models bycalculating the Kulback-Leibler divergence be-tween the representation distributions to measurehow closely related two words are (Kullback &Leibler, 1951).
When this data was comparedwith the USF association data, the combinedmodel performed better than the corpus-basedmodel, which in turn performed better than thefeatures-only model.
Andrews et alconcludedthat both experiential and distributional data arenecessary for the acquisition of good quality lex-ical representations.As well as suggesting a way to improve theperformance of LDA models on NLP tasks bysupplementing the training data, the approachtaken by Andrews et almay be useful for betterunderstanding the nature of the abstract/concretedistinction.
In recent work, Hill et al(in press)present empirical evidence that concrete con-cepts are represented in terms of intrinsic fea-tures or properties whereas abstract concepts arerepresented in terms of connections to other(concrete and abstract) concepts.
For example,the features [legs], [tail], [fur], [barks] are allcentral aspects of the concrete representation ofdog, whereas the representation of the abstractconcept love encodes connections to other con-cepts such as heart, rose, commitment and hap-piness etc.
If a feature-based representation isunderstood to be constructed from physical orperceptible properties (which themselves may bebasic or fundamental concrete representations),Hill et als characterization of concreteness canbe summarized as follows:79(H) Concreteness correlates with the degree towhich conceptual representations are feature-basedBecause such differences in representation struc-ture would in turn entail differences in the com-putation of similarity, (H) is closely related to aproposal of Markman and Stilwell (2001; seealso Gentner & Markman, 2007):(M) Computing similarity among concrete con-cepts involves a feature-comparison operation,whereas similarity between abstract concepts isa structural, analogy-like, comparison.The findings of Andrews et aldo not address(H) or (M) directly, for two reasons.
Firstly, theyevaluate their model on a set that includes noabstract concepts.
Secondly, they compare theirmodel output to association data without testinghow well it reflects similarity.
In this section wetherefore reconstruct the Andrews models andevaluate how well they reflect both associationand similarity across a larger set of abstract andconcrete concepts.4.2  Method/materialsWe reconstruct two of the three models devel-oped by Andrews et al(2009), excluding thefeatures-only model because of the present focuson corpus-based approaches.
However, whilethe experiential data applied in the Andrews etal.
combined model was that collected by Vig-liocco et al(2004), we use the publicly availableMcRae feature production norms (McRae et al2005).
The McRae data consist of 541 concretenoun concepts together with features for eachelicited from 725 participants.
In the data collec-tion, feature was understood in a very loosesense, so that participants were asked to list bothphysical and functional properties of the nouns inaddition to encyclopedic facts.
However, for thepresent work, we filter out those features thatwere not perceptual properties using McRae etal.
?s feature classes, leaving a total of 1,285 fea-ture types, such as [has_claws] and[made_of_brass].
The importance of each fea-ture to the representation of a given concept isreflected by the proportion of participants whonamed that feature in the elicitation experiment.For each noun concept we therefore extract acorresponding probability distribution over fea-tures.The model design and inference are identicalto those applied by Andrews et al Our distribu-tional model contains 250 latent dimensions andwas trained using a Gibbs Sampling algorithm onapproximately 7,500 sections of the BNC withstopwords removed.3  The combined model con-tains 350 latent dimensions, and was trained onthe same BNC data.
However, for each instanceof one of the 541 McRae concept words, a fea-ture is drawn at random from the probability dis-tribution corresponding to that word and ap-pended to the training data.
The latent dimen-sions in the combined model therefore corres-pond to probability distributions both over wordsand over features.
This leads to an important dif-ference between how words come to be related inthe distributional model and in the combinedmodel.
Both models infer connections betweenwords by virtue of their occurrence either in thesame document or in pairs of documents forwhich the same latent dimensions are prominent.In the distributional model, it is the words in adocument that determines which latent dimen-sions are ultimately prominent, whereas the incombined model it is both the words and the fea-tures in that document.
Therefore, in the com-bined model, two words can come to be relatedbecause they occur not only in documents whosewords are related, but also in documents whosefeatures are related.
For words in the McRaedata, this has the effect of strengthening the rela-tionship between words with common features.More interestingly, because it alters which latentdimensions are most prominent for each docu-ment, it should also influence the relationshipbetween words not in the McRae data.We evaluate the performance of our models inreflecting free association (association) and simi-larity (similarity).
To obtain test items we rankthe 18,195 noun-noun pairs from the USF databy the product of the two (BNC) word frequen-cies and select the 5,000 highest frequency pairs.4.3 ResultsAs expected, the correlation of the combinedmodel output with association was greater thanthe correlation of the distributional model output.Notably, however, as illustrated in Fig.
4, weobserved far greater differences between thecombined and the distributional models whencomparing to similarity.
Over all noun pairs, theaddition of features in the combined model im-3 Code for model implementation was taken from MarkAndrews : http://www.mjandrews.net/code/index.html80proved the correlation with similarity fromSpearman rs  =  0.09  to  rs  =  0.15.Figure 4:  Spearman correlations between distribu-tional and combined model outputs, similarity andassociationIn order to address Hypothesis (C) (Section 2.2),we analyzed the output of the combined modelon subsets of the 1000 most abstract and concreteword pairs in our data as before.
Perhaps surpri-singly, as shown in Fig.
5, when comparing withsimilarity, the model performed better over ab-stract pairs, whereas when comparing with asso-ciation the model performed better over concretepairs.
However, when these concrete pairs wererestricted to those for which at least one of thetwo words was in the McRae data, and hence towhich features had been appended in the corpus,the ability of the model to reflect similarity in-creased significantly.Figure 5:  Spearman correlations between combinedmodel output and similarity and association on differ-ent word pair subsetsFinally, to address hypotheses (H) and (M) wecompared the previous analysis of the combinedmodel output to the equivalent output from thedistributional model.
Surprisingly, as shown inFig.
6, the ability of the model to reflect associa-tion over abstract pairs seemed to reduce with theaddition of features to the training data.
Never-theless, in all other cases the combined modeloutperformed the distributional model.
Interes-tingly, the combined model advantage whencomparing with similarity was roughly the sameover both abstract and concrete pairs.
However,when these pairs contained at least one wordfrom the McRae data, the combined model wasindeed significantly better at modeling similarity,consistent with Hypotheses (M) and (H).Figure 6:  Comparison between distributionalmodel and combined model output correlations withsimilarity and association over different word pairsubsets4.4 ConclusionOur findings corroborate the main conclusion ofAndrews et al that the addition of experientialdata improves the performance of the LDA mod-el in reflecting association.
However, they alsoindicate that the advantage of feature-based LDAmodels is far more significant when the objectiveis to model similarity.The findings are also consistent with, ifnot suggestive of, the theoretical hypotheses (H)and (M).
Clearly, the property features in thecombined model training data enable it to bettermodel both similarity and association betweenthose concepts to which the features correspond.However, this benefit is greater when modelingsimilarity than when modeling association.
Thissuggests that the similarity operation is indeedbased on features to a greater extent than associa-tion.
Moreover, this effect is far greater for theconcrete words for which the features were add-ed than over the other words pairs we tested.Whilst this is not a sound test of hypothesis (H)(no attempt was made to add ?features?
of ab-stract concepts to the model), it is certainly con-sistent with the idea that features or propertiesare a more important aspect of concrete represen-tations than of abstract representations.0.130.090.140.150.000.050.100.15Distributional CombinedModel typeCorrelationAssociationSimilarity0.010.160.20.080.140.360.00.10.20.3Abstract Concrete McRaeWord pair categoryCorrelationAssociationSimilarity0.030.0930.150.0180.110.0250.010.160.20.080.140.360.00.10.20.3Abstract ConcreteDistributional modelMcRae Abstract_ Concrete_Combined modelMcRae_Word pair categoryCorrelationAssociationSimilarity81Perhaps the most interesting aspect of thecombined model is how the addition of featureinformation in the training data for certain wordsinfluences performance on words for which fea-tures were not added.
In this case, our findingssuggest that the benefit when modeling similarityis marginally greater than when modeling associ-ation, an observation consistent with Hypothesis(M).
A less expected observation is that, be-tween words for which features were not added,the advantage of the combined model over thedistributional model in modeling similarity wasequal if not greater for abstract than for concreteconcepts.
We hypothesize that this is becauseabstract representations naturally inherit any re-liance on feature information from the concreteconcepts with which they participate.
In con-trast, highly concrete representations do not en-code relations to other concepts and thereforecannot inherit relevant feature information in thesame way.
Under this interpretation, the con-crete information from the McRae words wouldpropagate more naturally to abstract conceptsthan to other concrete concepts.
As a result, thehighest quality representations in the combinedmodel would be those of the McRae words, fol-lowed by those of the abstract concepts to whichthey closely relate.5 DiscussionThis study has investigated how concreteness isreflected in the distributional patterns found inrunning text corpora.
Our results add to the bodyof evidence that abstract and concrete conceptsare represented differently in the mind.
The factthat VSMs with small windows are particularlyadept at modeling relations between concreteconcepts supports the view that similarity go-verns the conceptual organization of concreteconcepts to a greater extent than for abstract con-cepts.
Further, the performance of our LSAmodels on different tasks and across differentword pairs is consistent with the idea that con-crete representations are built around features,whereas abstract concepts are not.More practically, we have demonstrated thatvector space models can be tailored to reflecteither similarity or association by adjusting thesize of the context window.
This in turn indi-cates a way in which VSMs might be optimizedto either abstract or concrete domains.
Our expe-riments with Latent Dirichlet Allocation corrobo-rate a recent proposal that appending trainingdata with perceptible feature or property infor-mation for a subset of concrete nouns can signif-icantly improve the quality of the model?s lexicalrepresentations.
As expected, this effect wasparticularly salient for representations of wordsfor which features were appended to the trainingdata.
However, the results show that this infor-mation can propagate to words for which fea-tures were not appended, in particular to abstractwords.The fact that certain perceptible aspects ofmeaning are not exhaustively reflected in linguis-tic data is a potentially critical obstacle for cor-pus-based semantic models.
Our findings sug-gest that existing machine learning techniquesmay be able to overcome this by adding the re-quired information for words that refer to con-crete entities and allowing this information topropagate to other elements of language.
In fu-ture work we aim to investigate specificallywhether this hypothesis holds for particular partsof speech.
For example, we would hypothesizethat verbs inherit a good degree of their meaningfrom their prototypical nominal arguments.ReferencesAgirre, E., Alfonseca, E., Hall, K., Kravalova, J.Pasca, K,.
& Soroa,A.
2009.
A Study on Similarityand Relatedness Using Distributional and WordNet-based Approaches.
In Proceedings of NAACL-HLT2009.Andrews, M., Vigliocco, G. & Vinson, D. 2009.Integrating experiential and distributional data tolearn semantic represenations.
Psychological Review,116(3), 463-498.Barsalou, L. 1999.
Perceptual symbol systems.
Be-havioral and Brain Sciences, 22, 577-609.Binder, J., Westbury, C., McKiernan, K., Possing,E., & Medler, D. 2005.
Distinct brain systems forprocessing concrete and abstract concepts.
Journal ofCognitive Neuroscience 17(6), 905-917.Chaudhari, D., Damani, O., & Laxman, S. 2011.Lexical Co-occurrence, Statistical Significance, andWord Association.
EMNLP 2011, 1058-1068.Crutch, S., Connell, S., & Warrington, E. 2009.The different representational frameworks underpin-ning abstract and concrete knowledge: evidence fromodd-one-out judgments.
Quarterly Journal of Experi-mental Psychology, 62(7), 1377-1388.Felbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT Press.Finkelstein, L., Gabrilovich, Matias, Rivlin, Solan,Wolfman & Ruppin.
2002.
Placing Search in Context:The Concept Revisited.
ACM Transactions on Infor-mation Systems, 20(1):116-131.Gentner, D., & Markman, A.
1997.
Structure map-ping in analogy and similarity.
American Psycholo-gist, 52.
45-56.82Griffiths, T., Steyvers, M., & Tenembaum, J.
2007.Topics in semantic representation.
Psychological Re-view, 114 (2), 211-244.Hill, F., Korhonen, A., & Bentz, C. A quantitativeempricial analysis of the abstract/concrete distinction.Cognitive Science.
In press.Kullback, S., & Leibler, R.A. 1951.
On Informa-tion and Sufficiency.
Annals of Mathematical Statis-tics 22 (1): 79?86.Kwong, O, Y.
2008.
A Preliminary study on induc-ing lexical concreteness from dictionary definitions.22nd Pacific Asia Conference on Language, In-formation and Computation, 235?244.Leech, G., Garside, R. & Bryant, R. 1994.
Claws4:The tagging of the British National Corpus.
COL-ING94, Lancaster: UK.Markman, A, & Stilwell, C. 2001.
Role-governedcategories.
Journal of Theoretical and ExperimentalArtificial Intelligence, 13, 329-358.McRae, K., Cree, G. S., Seidenberg, M. S., &McNorgan, C. 2005.
Semantic feature productionnorms for a large set of living and nonliving things.Behavior Research Methods, 37, 547-559Miller, G., & Charles, W. 1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1).Nelson, D., & McEvoy, C. 2012.
The University ofSouth Florida Word Association, Rhyme and WordFragment Norms.
Retrieved online from:http://web.usf.edu/FreeAssociation/Intro.html.Paivio, A., Yuille, J., & Madigan, S. 1968.
Con-creteness, imagery, and meaningfulness values for925 nouns.
Journal of Experimental Psychology Mo-nograph Supplement, 76(1, Pt.
2).Peirsman, y., Heylen, K. & Geeraerts, D. 2008.Size Matters.
Tight and Loose Context Definitions inEnglish Word Space Models.
In Proceedings of theESSLLI Workshop on Distributional Lexical Seman-tics, Hamburg, GermanyResnik, P. 1995.
Using Information Content toEvaluate Semantic Similarity in a Taxonomy.
Pro-ceedings of IJCAI-95.Rogers, T., & McLelland, J.
2003.
Semantic Cog-nition.
Cambridge, Mass: MIT Press.Rosch, E. 1975.
Cognitive representations of se-mantic categories.
Journal of Experimental Psycholo-gy: General, 104(3), (September 1975), pp.
192?233.Rubenstein, H., & Goodenough, J.
1965.
Contex-tual correlates of synonymy.
Communications of theACM 8(10), 627-633.Sahlgren, M. 2006.
The Word-Space Model: Usingdistributional analysis to represent syntagmatic andparadigmatic relations between words in high-dimensional vector spaces.
Ph.D. dissertation, De-partment of Linguistics, Stockholm University.Schwanenflugel, P. 1991.
Why are abstract con-cepts hard to understand?
In P.  Schwanenflugel.The psychology of word meanings (pp.
223-250).Hillsdale, NJ: Erlbaum.Toglia, M., & Battig, W. 1978.
Handbook of se-mantic word norms.
Hillsdale, N.J: Erlbaum.Turney, P, & Pantel, P. 2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research (JAIR), 37, 141-188.Turney,P., Neuman, Y., Assaf,.D, Cohen, Y.
2011.Literal and Metaphorical Sense Identification throughConcrete and Abstract Context.
EMNLP 2011: 680-690Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett,M.
F. 2004.
Reprssenting the meanings of object andaction words: The featural and unitary semantic spacehypothesis.
Cognitive Psychology, 48, 422?488.Warrington, E. (1975).
The selective impairment ofsemantic memory.
Quarterly Journal of ExperimentalPsychology 27(4), 635-657.Wu, Z., Palmer, M. 1994.
Verb semantics and lexi-cal selection.
In: Proceedings of the 32nd AnnualMeeting of the Associations for Computational Lin-guistics.
133?138.83
