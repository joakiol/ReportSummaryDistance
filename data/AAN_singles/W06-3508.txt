Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 57?64,New York City, June 2006. c?2006 Association for Computational LinguisticsSearching for Grammar RightVanessa MicelliEuropean Media LaboratorySchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germany{firstname.lastname@eml-d.villa-bosch.de}AbstractThis paper describes our ongoing work inand thoughts on developing a grammarlearning system based on a constructiongrammar formalism.
Necessary modulesare presented and first results and chal-lenges in formalizing the grammar areshown up.
Furthermore, we point out themajor reasons why we chose constructiongrammar as the most fitting formalism forour purposes.
Then our approach andideas of learning new linguistic phenom-ena, ranging from holophrastic construc-tions to compositional ones, is presented.1 IntroductionSince any particular language1 changes constantly(Cf.
Hopper and Traugott, 2003; Bybee, 1998) ?and even varies across domains, users, registersetc.
?
scalable natural language understanding sys-tems must be able to cope with language variationand change.
Moreover, due to the fact that anynatural language understanding system, which isbased on some formal representation of that lan-guage?s grammar, will always only be able to rep-resent a portion of what is going on in anyparticular language at the present time, we need tofind systematic ways of endowing natural languageunderstanding systems with means of learning new1 This claim also holds within any solidified system of con-ventionalized form-meaning pairings, e.g.
dialects, chro-nolects, sociolects, idiolects, jargons, etc.forms, new meanings and, ultimately, new form-meaning pairings, i.e.
constructions.Constructions are the basic building blocks,posited by a particular grammar framework calledConstruction Grammar, and are defined as follows:?C is a construction iffdef C is a form-meaning pair<Fi, Si> such that some aspect of Fi or some aspectof Si is not strictly predictable from C?s componentparts or from other previously established con-structions.?
(Goldberg, 1995:4).Construction Grammar originated from earlierinsights in functional and usage-based models oflanguage mainly supposed by cognitive linguists(e.g.
Lakoff, 1987; Fillmore and Kay, 1987; Kay,2002; Talmy, 1988; etc.).
It has been devised tohandle actually occurring natural language, whichnotoriously contains non-literal, elliptic, context-dependent, metaphorical or underspecified linguis-tic expressions.
These phenomena still present achallenge for today?s natural language understand-ing systems.
In addition to these advantages, weadhere to principles proposed by other constructiv-ists as e.g.
Tomasello (2003) that language acquisi-tion is a usage-based phenomenon, contrastingapproaches by generative grammarians who as-sume an innate grammar (Chomsky, 1981).
Fur-thermore, we agree to the idea that grammaticalphenomena also contribute to the semantics of asentence which is the reason why syntax cannot bedefined independently of semantics of a grammar.A more detailed outline of construction grammarand the principles we adhered to in formalizing itwill be given in sections 2 and 3.The input to the system is natural language dataas found on the web, as e.g.
in news tickers orblogs, initially restricted to the soccer domain.
As57the learning process develops the input will gradu-ally be extended to other domains.
A description ofthe corpus and its selection process will be given insection 4.
Section 5 provides an outlook on thelearning paradigm, while the last section presentssome future issues and conclusions.2 Grammar FormalismThe most crucial foundation that is needed to builda grammar learning system is a grammar formal-ism.
Therefore, we are designing a new formaliza-tion of construction grammar called ECtoloG(Porzel et al, 2006; Micelli et al, in press).One existing formal computational model ofconstruction grammar is the Embodied Construc-tion Grammar (ECG) (Chang et al, 2002; Bergenand Chang, 2002), with its main focus being onlanguage understanding and later simulation2.
Acongruent and parallel development has led toFCG which simulates the emergence of language(Steels, 2005).
FCG is mainly based on the sameprimitives and operators as ECG is.
We decided toemploy ECG in our model mainly for historicalreasons (see details about its development in thefollowing section), adhering to its main primitivesand operators, but employing the state of the art inknowledge representation.
We adopt insights andmechanisms of FCG where applicable.2.1 Construction Grammar and ECGOne main difference between West Coast Gram-mar (Langacker, 1987; Lakoff, 1987) and EastCoast Grammar (Chomsky, 1965; Katz, 1972) isthe fact that construction grammar offers a vertical?
not a horizontal ?
organisation of any knowledgeconcerning a language?s grammar.
That is, thatgenerative grammars split form from function.Syntax, morphology, a lexicon or other formalcomponents of the grammar constitute form, whilethe conventional function is defined by semantics.All constructions of a language, however, formin Langacker?s terms ?a structured inventory ofconventional linguistic units?
(Langacker,1987:54).
This inventory is network-structured, i.e.there are at least taxonomic links among the con-structions (Diessel, 2004).
This structure presents2 For a detailed ECG analysis of a declarative utterance, i.e.the sentence Harry walked into the cafe, see Bergen andChang (2002).one of the main differences between generativeand construction grammars (Croft, to appear).
Oneof the most cited examples that evidences the ne-cessity, that there can be no explicit separation be-tween syntax and semantics, is Goldberg?sexample sentence (Goldberg, 1995:29):(1) he sneezed the napkin off the table.The whole meaning of this sentence cannot begathered from the meanings of the discrete words.The direct object the napkin is not postulated bythe verb to sneeze.
This intransitive verb wouldhave three arguments in a lexico-semantic theory:?X causes Y to move Z by sneezing?.
Goldbergstates that the additional meaning of caused motionwhich is added to the conventional meaning of theverb sneeze is offered by the respective caused-motion construction.
Based on this backgroundECG ?
a formal computational model of construc-tion grammar ?
was developed within the NeuralTheory of Language project (NTL) and the EDUproject (EDU).While other approaches consider language ascompletely independent from the organism whichuses it, ECG claims that several characteristics ofthe user?s sensorimotor system can influence his orher language (Gallese and Lakoff, 2005).
Theneeded dynamic and inferential semantics in ECGis represented by embodied schemas.
These sche-mas are known under the term of image schemas intraditional cognitive semantics and constituteschematic recurring patterns of sensorimotor ex-perience (Johnson, 1987; Lakoff, 1987).The current ASCII format of ECG is insufficientfor building scalable NLU systems in the long run.Therefore, our attempt at formalizing constructiongrammar results in an ontological model that com-bines two ontological modeling frameworks en-dowed with a construction grammar layer, basedon the main ideas behind ECG.
The following sec-tion describes the resulting ontology, pointing outmain challenges and advantages of that approach.3 Formalizing Construction GrammarThe ontological frameworks mentioned above areDescriptions & Situations (D&S) (Gangemi andMika, 2003) and Ontology of Information Objects(OIO) (Guarino, 2006), which both are extensionsof the Descriptive Ontology for Linguistic and58Cognitive Engineering (DOLCE) (Masolo et al,2003).D&S is an ontology for representing a variety ofreified contexts and states of affairs.
In contrast tophysical objects or events, the extensions of on-tologies to the domain of non-physical objects posea challenge to the ontology engineer.
The reasonfor this lies in the fact that non-physical objects aretaken to have meaning only in combination withsome other ground entity.
Accordingly, their logi-cal representation is generally set at the level oftheories or models and not at the level of conceptsor relations (see Gangemi and Mika, 2003).
It is,therefore, important to keep in mind that the mean-ing of a given linguistic expression emerges onlythrough the combination of both linguistic andconceptual knowledge with ?basic?
ontologicalknowledge, as modeled in such ground ontologies.Next to the support via dedicated editors and in-ference engines, one of the central advantages ofour ensuing ontological model over the currentlyused ASCII-format of ECG lies in its compatibilitywith other ground ontologies developed within theSemantic Web framework.33.1 Modeling of ConstructionsConstructions are modeled in the ECtoloG as in-formation-objects.
According to the specificationof the OIO, information objects have ?
amongstothers ?
the following properties: They are socialobjects realizable by some entity and they can ex-press a description, which represents in this ontol-ogy the ontological equivalent of a meaning or aconceptualization.
Since a construction constitutesa pairing of form and meaning according to theoriginal theory of construction grammar, bothproperties are of advantage for our ontologicalmodel.
To keep the construction?s original struc-ture, the form pole can be modeled with the help ofthe realized-by property4 while the meaning pole isbuilt via the edns:expresses property.
Both proc-esses are described more detailed in the followingsection.Holophrastic ConstructionsThe class of lexical constructions is modeled as asubclass of referringConstruction.
Since it is a3 For more details see Porzel et al (2006).4 We adhere to the convention to present both ontologicalproperties, classes, and instances in italics.subclass of the class information-object it inheritsthe edns:expresses property.
The referringCon-struction class has a restriction on this propertythat denotes, that at least one of the values of theedns:expresses property is of type schema.
Model-ing this restriction is done by means of the built-inowl:someValuesFrom constraint.
The restrictioncounts for all constructions that express a schema.It has no effect on the whole class of constructions,i.e.
it is possible that there exist constructions thatdo not express a single schema, as e.g.
composi-tional ones, whose meaning is a composite of allconstructions and schemas that constitute thatcompositional construction.The form pole of each construction is modeledwith the help of the realized-by property.
Thisproperty designates that a (physical) representation?
as e.g.
the orthographic form of the construction?
realizes a non-physical object ?
in this case ourconstruction.
This property is also inherited fromthe class information-object, the superclass of con-structions.
What fills the range of that property isthe class of edns:physical-realization.
Therefore,we define an instance of inf:writing, which thenfills the form pole of the respective construction.This instance has once more a relation which con-nects it to instances of the class inf:word whichdesignate the realization of the instance of theinf:writing class.This way of modeling the form pole of each lexi-cal construction enables us to automatically popu-late our model with new instances of constructions,as will be described more detailed in section 5.1.Analogous to the modeling of meaning in theoriginal ECG, the meaning pole is ?filled?
with aninstance of the class of image schema.
This can bedone with the help of the edns:expresses relation.This relation is defined, according to the specifica-tion of the D&S ontology, as a relation betweeninformation objects that are used as representations(signs) and their content, i.e.
their meaning or con-ceptualization.
In this ontology, content is reifiedas a description, which offered us the possibility tomodel image schemas as such.
How image sche-mas are modeled will be described in section 3.2.Compositional ConstructionsCompositional constructions are constructionswhich are on a higher level of abstraction thanholophrastic ones.
This means, that there exist con-structions which combine different constructions59into one unit.
ECG designed a so-called construc-tional block, wherein several constructions aresubsumed under and accessible in one more com-plex construction.An example is the DetNoun construction, whichcombines a determiner and a noun to form oneunit.
There is the possibility to model differentconstraints both in the form pole and in the mean-ing pole of a construction.
A form constraint ap-plying to this exact construction is determining thatthe determiner comes before the noun.
This under-standing of before corresponds to Allen?s defini-tion of his interval relations (Allen, 1983), whichstates that they don?t necessarily have to followeach other but that there could be some modifiersin between the two components of this construc-tion.A meaning constraint of this construction deter-mines, that the meaning of the noun, used in thisrespective construction, is assigned to the meaningof the resulting complex construction.5 To be ableto represent these phenomena, we firstly defined aclass construction-parameter, that denotes a sub-class of edns:parameter, a subclass ofedns:concept.
There is a property restriction on theclass that states that all values of the requisite-forproperty have to be of type construction.
This de-termines instances of the class construction-parameter to be used only in constructions on ahigher level of abstraction.
All constructions usedon level 0 of a grammar6, i.e.
lexical constructions,are at the same time instances of the class con-struction-parameter so that they can be used inmore abstract constructions.
The form and mean-ing constraints still need to be modeled in ourframework.
To determine which constructions areused in which more abstract construction, newproperties are defined.
These properties are sub-properties of the requisite-for property.
An exam-ple is the requisite-detnoun-akk-sg property.
Thisproperty defines that the accusative singular de-terminer construction and the corresponding nounconstruction can be requisite-for the compositionalconstruction that combines these two lexical con-structions into one noun phrase.5 For further information about which operators are used tomodel these features in ECG we refer to Bergen and Chang(2002), Chang et al (2002) and Bryant (2004).6 Following Bryant?s (2004) division of constructions into 5levels of different degrees of schematicity.3.2 Modeling of Image SchemasFollowing Johnson and Lakoff (Johnson, 1987;Lakoff and Johnson, 1980; Lakoff, 1987) imageschemas are schematic representations that capturerecurrent patterns of sensorimotor experience.
Ac-cording to ECG, a schema is a description whosepurpose is filling the meaning pole of a construc-tion.
It consists of a list of schematic roles that canserve as simulation parameters.In ECG, schemas can be evoked by or can evokeother schemas, i.e.
particular schematic-roles ofanother schema can be imported.
A schema can,therefore, be defined against the background ofanother schema7.
The property evokes and its in-verse property evoked-by have been defined assubproperties of the dol:generically-dependent-onproperty and its inverse property dol:generic-dependent respectively.
Generic dependence isdefined in the DOLCE ontology as the dependenceon an individual of a given type at some time.The class of image schemas is modeled as a sub-class of edns:description (see definition of descrip-tion in 3.1), in order to enable being employed inthe meaning pole of constructions.Schematic RolesThe class of schematic-roles is a subclass of theedns:concept class.
In the specification of D&S aconcept is classified as a non-physical objectwhich again is defined by a description.
Its func-tion is classifying entities from a ground ontologyin order to build situations that can satisfy the de-scription.
Schematic roles are parameters that al-low other schemas or constructions to refer to theschema's key variable features, e.g.
the role of atrajector in a Trajector Landmark-Schema can beplayed by  the same entity that denotes the moverin e.g.
a caused-motion schema.At the moment, they are modeled with the helpof the edns:defines property.
A schema defines itsschematic roles with this property, denoting a sub-property of the edns:component property.
Accord-ing to the D&S specification, a component is aproper part with a role or function in a system or acontext.
It is also stated, that roles can be differentfor the same entity, and the evaluation of themchanges according to the kind of entity.
Thismeans, that instances of the class schema and its7 To clarify this claim see Langacker?s hypotenuse example(Langacker, 1987:183ff.
).60subclasses can have instances of the class sche-matic-role as their components.
The schematic-roles class has to fulfil the necessary condition,that at least one of the values of the edns:defined-by property is of type schema.The domain of the defines property is a descrip-tion (which can be our schemas) and its range is setto either concepts or figures (which are our sche-matic roles).
The problem occurring hereby is thatthe roles cannot be filled by complete classeswhich is necessary in a lot of cases, since the pa-rameters are not always filled with atomic valuesbut possibly with whole classes of entities.
There-fore, one could think about modeling schematicroles as properties, setting the domain on the corre-sponding schema class and the range on the corre-sponding class whose subclasses and instances canpossibly fill its range.3.3 Linguistic InformationSince linguistic information as e.g.
grammaticalgender, its case, or the part-of-speech of a word isneeded for analyzing natural language texts, thisinformation has to be modeled, as well, in the EC-toloG.
Therefore, we integrated the LingInfomodel (Buitelaar et al, 2006) into the ECtoloG.LingInfo constitutes an ontological model thatprovides other ontologies with linguistic informa-tion for different languages, momentarily for Eng-lish, French, and German.
Main objective of thisontology is to provide a mapping between onto-logical concepts and lexical items.
That is, that thepossibility is offered to assign linguistic informa-tion as e.g.
the orthographic term, its grammaticalgender, its part-of-speech, stem etc.
to classes andproperties.
For our purposes, the LingInfo ontologyhad to be converted from RDFS into OWL-DLformat and then integrated into the ECtoloG.
Forthat reason, a new subclass of owl:class was de-fined: ClassWithLingInfo.
Instances of this meta-class are linked through the linginfo property toLingInfo classes.
The LingInfo class is used to as-sociate a term, a language, and morphosyntacticinformation to classes from the ground ontology;e.g.
a class CafeConstruction, which is an instanceof ClassWithLingInfo, from an ontology proper,can be associated through the property linginfowith Caf?, an instance of the class LingInfo.
Thus,the information that the term is German, its part-of-speech is noun and its grammatical gender neu-ter is obtained.Following this approach, our classes of lexicalconstructions were defined as subclasses ofClassWithLingInfo, being thereby provided with allthe necessary linguistic information as definedabove.
The central challenge resulting from thisapproach is, that through the definition of a meta-class the ontological format is no longer OWL-DLbut goes to OWL-Full which thwarts the employ-ment of Description Logic reasoners.
Reasoningwill not stay computable and decidable.
Futurework will address this challenge by means of inter-twining the LingInfo model with the ECtoloGgrammar model in such a way, that the computa-tional and inferential properties of OWL-DL re-main unchallenged.Another possibility could be obtaining linguisticinformation for lexical items through an externallexicon.4 The Web as a CorpusThe Seed Corpus C: The primary corpus C in thiswork is the portion of the World Wide Web con-fined to web pages containing natural languagetexts on soccer.
To extract natural language textsout of web documents automatically we are usingwrapper agents that fulfil this job (see Porzel et al,2006).
Our first goal is to build a grammar that candeal with all occurring language phenomena ?
i.e.both holophrastic and compositional ones ?
con-tained in that corpus C.Corpus C?
: Next step is the development of a cor-pus C?, where C?
= C + ?
and ?
is constituted by aset of new documents.
This new corpus is not de-signed in an arbitrary manner.
We search similarpages, adding add them to our original corpus C, aswe expect the likelihood of still pretty good cover-age together with some new constructions to bemaximal, thereby enabling our incremental learn-ing approach.
The question emerging hereby is:what constitutes a similar web page?
What, there-fore, has to be explored are various similarity met-rics, defining similarity in a concrete way andevaluate the results against human annotations (seePapineni et al, 2002).4.1 Similarity MetricTo be able to answer the question which texts areactually similar, similarity needs to be defined pre-cisely.
Different approaches could be employed,61i.e.
regarding similarity in terms of syntactic orsemantic phenomena or a combination of both.Since construction grammar makes no separationbetween syntax and semantics, phenomena thatshould be counted are both constructions and im-age schemas.
As for holophrastic constructions thispresents less of a challenge, we rather expectcounting compositional ones being a ?toughcookie?.To detect image schemas in natural text auto-matically, we seek to employ different methodolo-gies, e.g.
LSA (Kintsch and van Dijk, 1978), usingsynonym sets (Fellbaum, 1998) or other ontolo-gies, which could assist in discovering the seman-tics of an unknown word with its correspondingschematic roles and the appropriate fillers.
This ora similar methodology will be applied in the auto-matic acquisition process as well.Another important point is that some terms, orsome constructions, need to get a higher relevancefactor than others, which will highly depend oncontext.
Such a relevance factor can rank terms orconstructions according to their importance in therespective text.
Ranking functions that can be ex-amined are, e.g., the TF/IDF function (e.g.
Salton,1989) or other so called bag of words approaches.Term statistics in general is often used to deter-mine a scalable measure of similarity betweendocuments so it is said to be a good measure fortopical closeness.
Also part-of-speech statisticscould be partly helpful in defining similarity ofdocuments based on the ensuing type/token ratio.The following five steps need to be executed indetermining the similarity of two documents:Step 1: Processing of the document D; analyzingthe text and creating a list of all occurring words,constructions and/or image schemas.
We assumethat the best choice is counting constructions andcorresponding image schemas, since they representthe semantics of the given text.Step 2: Weighing of schemas and constructionsStep 3: Processing of the document D+1; execut-ing of step 1 and 2 for this document.Step 4: Comparing the documents; possibly addingsynonyms of sources as e.g.
WordNet (Fellbaum,1998).Step 5: Calculating the documents?
similarity; de-fining a threshold up to which documents are con-sidered as being similar.
If a document is said to besimilar, it is added to the corpus, which becomesthe new corpus C?.Analysis of the New Corpus C?
: The new corpusC?
is analyzed, whereby the coverage results incoverage A of C?
where:A = 100% - (?h + ?c)?h denotes all the holophrastic phenomena and ?call compositional phenomena not observed in C.5 Grammar LearningTo generate a grammar that covers this new corpusC?
different strategies have to be applied for holo-phrastic items ?h which are lexical constructions inour approach and for compositional ones ?c ?meaning constructions on a higher level of abstrac-tion as e.g.
constructions that capture grammaticalphenomena such as noun phrases or even wholesentences.5.1 Learning Lexical ConstructionsAnalogous to the fast mapping process (Carey,1978) of learning new words based on exposurewithout additional training or feedback on the cor-rectness of its meaning, we are employing amethod of filling our ontology with whole para-digms of new terms8, enabled through the model-ing of constructions described in 3.1.
First stepherein is employing a tool ?
Morphy (Lezius,2002) ?
that enables morphological analysis andsynthesis.
The analysis of a term yields informa-tion about its stem, its part-of-speech, its case, itsnumber, and its grammatical gender.
This informa-tion can then easily be integrated automaticallyinto the ECtoloG.As already mentioned in section 4.3, we are notonly trying to automatically acquire the form poleof the constructions, but also its image schematicmeaning, that means the network of the schemasthat hierarchically form the meaning pole of such aterm, applying ontology learning mechanisms (e.g.Loos, 2006) and methods similar to those de-scribed in section 4.3.
Additionally, investigationsare underway to connect the grammar learningframework proposed herein to a computer visionsystem that provides supplementary feedback con-8 We are aware of the fact that fast mapping in humans is lim-ited to color terms, shapes or texture terms, but are employingthe method on other kinds of terms, nevertheless, since thegrammar learning paradigm in our approach is still in its babyshoes.62cerning the hypothesized semantics of individualforms in the case of multi-media information.5.2 Learning Compositional ConstructionsLearning of compositional constructions still pre-sents an issue which has not been accounted for,yet.
What has already been proposed (Narayanan,inter alia) is that we have to assume a strong induc-tive bias and different learning algorithms, as e.g.some form of Bayesian learning or model merging(Stolcke, 1994) or reinforcement learning (Suttonand Barto, 1998).Another important step that has to be employed isthe (re)organization of the so-called constructicon,i.e.
our inventory of constructions and schemas.These need to be merged, split or maybe thrownout again, depending on their utility, similarity etc.5.3 AmbiguityCurrently the problem of ambiguity is solved byendowing the analyzer with a chart and employingthe semantic density algorithm described in (Bry-ant, 2004).
In the future probabilistic reasoningframeworks as proposed by (Narayanan and Juraf-sky, 2005) in combination with ontology-basedcoherence measures as proposed by (Loos andPorzel, 2004) constitute promising approaches forhandling problems of construal, whether it be on apragmatic, semantic, syntactic or phonologicallevel.6 Concluding RemarksIn this paper we described our ongoing work inand thoughts on developing a grammar learningsystem based on a construction grammar formal-ism used in a question-answering system.
We de-scribed necessary modules and presented firstresults and challenges in formalizing constructiongrammar.
Furthermore, we pointed out our motiva-tion for choosing construction grammar and the,therefore, resulting advantages.
Then our approachand ideas of learning new linguistic phenomena,ranging from holophrastic constructions to compo-sitional ones, were presented.
What should be keptin mind is that our grammar model has to bestrongly adaptable to language phenomena, as e.g.language variation and change, maps, metaphors,or mental spaces.Evaluations in the light of the precision/coveragetrade-off still present an enormous challenge (aswith all adaptive and learning systems).
In the fu-ture we will examine the feasibility of adaptingontology evaluating frameworks, as e.g.
proposedby Porzel and Malaka (2005) for the task of gram-mar learning.
We hope that future evaluations willshow that our resulting system and, therefore, itsgrammar will be robust and adaptable enough to beworth being called ?Grammar Right?.ReferencesAllen, J.F.
1983.
Maintaining Knowledge about Tempo-ral Intervals.
Communications of the ACM, 26(11).Bergen, B. and Chang, N. 2002.
Simulation-Based Lan-guage Understanding in Embodied ConstructionGrammar.
ICSI TR-02-004, Berkeley, CA, USA.Bryant, J.
2004.
Recovering coherent interpretationsusing semantic integration of partial parses.
Proceed-ings of the 3rd ROMAND workshop, Geneva, Switzer-land.Buitelaar, P., Declerck, T., Frank, A., Racioppa, S., Kie-sel, M., Sintek, M., Engel, M., Romanelli, M.,Sonntag, D., Loos, B., Micelli, V., Porzel, R. andCimiano, P. 2006.
LingInfo: Design and Applicationsof a Model for the Integration of Linguistic Informa-tion in Ontologies.
Proceedings of OntoLex 2006.
InPress.
Genoa, Italy.Bybee, J.
1998.
A functionalist approach to grammarand its evolution.
Evolution of Communication 2(2).Carey, S. 1978.
The child as word-learner.
Linguistictheory and psychological reality.
MIT Press, Cam-bridge, MA.Chang, N., Feldman, J., Porzel, R. and Sanders, K.2002.
Scaling Cognitive Linguistics: Formalisms forLanguage Understanding.
Proceedings of the 1stScaNaLU Workshop, Heidelberg, Germany.Chomsky, N. 1965.
Aspects of the theory of syntax.
MITPress.
Cambridge, Mass.Croft, W. To appear.
Logical and typological argumentsfor Radical Construction Grammar.
ConstructionGrammar(s): Cognitive and cross-language dimen-sions (Constructional approaches to Language, 1).John Benjamins, Amsterdam.Diessel, H. 2004.
The Acquisition of Complex Sen-tences.
Cambridge Studies in Linguistics (105).Cambridge University Press, Cambridge.EDU: Even Deeper Understanding http://www.eml-development.de/english/research/edu/index.php (lastaccess: 31/03/06).63Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, Mass.Fillmore, C. and Kay, P. 1987.
The goals of Construc-tion Grammar.
Berkeley Cognitive Science ProgramTR 50.
University of California, Berkeley.Gallese, V. and Lakoff, G. 2005.
The brain?s concepts:the role of the sensory-motor system in conceptualknowledge.
Cognitive Neuropsychology 21/2005.Gangemi A., Mika P. 2003.
Understanding the SemanticWeb through Descriptions and Situations.
Proceed-ings of ODBASE03 Conference, Springer.Goldberg, A.
1995.
Constructions: A ConstructionGrammar Approach to Argument Structure.
Univer-sity of Chicago Press.
Chicago.Guarino, N. 2006.
Ontology Library.
WonderWeb De-liverable D202, I STC-CNR, Padova, Italy.www.loa-cnr.it/Papers/Deliverable%202.pdf (last ac-cess: 31/03/2006).Hopper, P. and Traugott, E. 2003.
Grammaticalization.Cambridge University Press.
Cambridge, UK.Johnson, M. 1987.
The Body in the Mind: The BodilyBasis of Meaning, Imagination, and Reason.
Univer-sity of Chicago Press.
Chicago.Katz, J. J.
1972.
Semantic theory.
Harper & Row.
NewYork.Kay, P. 2002.
An Informal Sketch of a Formal Architec-ture for Construction Grammar.
Grammars 1/5.Kintsch, W. and van Dijk, T. 1978.
Toward a model oftext comprehension and production.
PsychologicalReview, 85 (5).Lakoff, G. and Johnson, M. 1980.
Metaphors We LiveBy.
Chicago University Press.
London.Lakoff, G. 1987.
Women, Fire, and Dangerous Things.University of Chicago Press.
Chicago and London.Langacker, R. 1987.
Foundations of Cognitive Gram-mar, Vol.
1.
University Press.
Stanford.Lezius, W. 2000.
Morphy - German Morphology, Part-of-Speech Tagging and Applications.
Proceedings ofthe 9th EURALEX International Congress, Stuttgart,Germany.Loos, B.
2006.
Scaling natural language understandingvia user-driven ontology learning.
This volume.Loos, B. and Porzel, R. 2004.
Resolution of LexicalAmbiguities in Spoken Dialogue System.
Proceed-ings of the 5th SIGdial Workshop on Discourse andDialogue, Cambridge, Massachusetts, USA.Masolo, C., Borgo, S., Gangemi, A., Guarino, N., andOltramari, A.
2003.
Ontology Library.
WonderWebDeliverable D18, I STC-CNR, Padova, Italy.http://wonderweb.semanticweb.org/deliverables/documents/D18.pdf.Micelli, V., Porzel, R., and Gangemi, A.
In Press.
EC-toloG: Construction Grammar meets the SemanticWeb.
Proceedings of ICCG4, Tokyo, Japan.Narayanan, S. and Jurafsky, D. 2005.
A Bayesian Modelof Human Sentence Processing (in preparation).Narayanan, S. 2006.
Lecture Series given at Interdisci-plinary College, Guenne, Lake Moehne, March 10th?
17th.NTL: Neural Theory of Languagehttp://www.icsi.berkeley.edu/NTL/Papineni, K., Roukos, S., Ward, T. and Zhu, W. 2002.Bleu: a Method for Automatic Evaluation of MachineTranslation.
Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics(ACL), Philadelphia.Porzel, R. and Malaka, R. 2005.
A Task-based Frame-work for Ontology Learning, Population and Evalua-tion.
Ontology Learning from Text: Methods,Evaluation and Applications Frontiers.
Artificial In-telligence and Applications Series, Vol.
123, IOSPress.Porzel, R., Micelli, V., Aras, H., and Zorn, H.-P. 2006.Tying the Knot: Ground Entities, Descriptions andInformation Objects for Construction-based Informa-tion Extraction.
Proceedings of OntoLex 2006.
InPress.
Genoa, Italy.Salton, G. 1989.
Automatic Text Processing: the Trans-formation, Analysis, and Retrieval of Information byComputer.
Addison-Wesley.
Reading, MA.Steels, L. 2005.
The Role of Construction Grammar inFluid Language Grounding.
Submitted.
Elsevier Sci-ence.Stolcke, A.
1994.
Bayesian Learning of ProbabilisticLanguage Models.
Ph.D. Thesis, Computer ScienceDivision, University of California at Berkeley.Sutton, R. and Barto, A.
1998.
Reinforcement Learning:An Introduction.
MIT Press, Cambridge.Talmy, L. 1988.
Force dynamics in language and cogni-tion.
Cognitive Science, 12.Tomasello, M. 2003.
Constructing a Language: A Us-age-Based Theory of Language Acquisition.
HarvardUniversity Press.64
