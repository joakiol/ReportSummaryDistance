Proceedings of the 12th Conference of the European Chapter of the ACL, pages 666?674,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsFlexible Answer Typing with Discriminative Preference RankingChristopher Pinchak?
Dekang Lin?
Davood Rafiei?
?Department of Computing Science ?Google Inc.University of Alberta 1600 Amphitheatre ParkwayEdmonton, Alberta, Canada Mountain View, CA, USA{pinchak,drafiei}@cs.ualberta.ca lindek@google.comAbstractAn important part of question answeringis ensuring a candidate answer is plausi-ble as a response.
We present a flexibleapproach based on discriminative prefer-ence ranking to determine which of a setof candidate answers are appropriate.
Dis-criminative methods provide superior per-formance while at the same time allow theflexibility of adding new and diverse fea-tures.
Experimental results on a set of fo-cused What ...?
and Which ...?
questionsshow that our learned preference rankingmethods perform better than alternativesolutions to the task of answer typing.
Again of almost 0.2 in MRR for both thefirst appropriate and first correct answersis observed along with an increase in pre-cision over the entire range of recall.1 IntroductionQuestion answering (QA) systems have received agreat deal of attention because they provide botha natural means of querying via questions and be-cause they return short, concise answers.
Thesetwo advantages simplify the task of finding in-formation relevant to a topic of interest.
Ques-tions convey more than simply a natural languagequery; an implicit expectation of answer type isprovided along with the question words.
The dis-covery and exploitation of this implicit expectedtype is called answer typing.We introduce an answer typing method that issufficiently flexible to use a wide variety of fea-tures while at the same time providing a high levelof performance.
Our answer typing method avoidsthe use of pre-determined classes that are oftenlacking for unanticipated answer types.
Becauseanswer typing is only part of the QA task, a flexi-ble answer typing model ensures that answer typ-ing can be easily and usefully incorporated into acomplete QA system.
A discriminative preferenceranking model with a preference for appropriateanswers is trained and applied to unseen ques-tions.
In terms of Mean Reciprocal Rank (MRR),we observe improvements over existing systems ofaround 0.2 both in terms of the correct answer andin terms of appropriate responses.
This increasein MRR brings the performance of our model tonear the level of a full QA system on a subset ofquestions, despite the fact that we rely on answertyping features alone.The amount of information given about the ex-pected answer can vary by question.
If the ques-tion contains a question focus, which we defineto be the head noun following the wh-word suchas city in ?What city hosted the 1988 WinterOlympics?
?, some of the typing information is ex-plicitly stated.
In this instance, the answer is re-quired to be a city.
However, there is often addi-tional information available about the type.
In ourexample, the answer must plausibly host a WinterOlympic Games.
The focus, along with the ad-ditional information, give strong clues about whatare appropriate as responses.We define an appropriate candidate answer asone that a user, who does not necessarily knowthe correct answer, would identify as a plausibleanswer to a given question.
For most questions,there exist plausible responses that are not correctanswers to the question.
For our above question,the city of Vancouver is plausible even though itis not correct.
For the purposes of this paper, weassume correct answers are a subset of appropri-ate candidates.
Because answer typing is only in-tended to be a component of a full QA system, werely on other components to help establish the truecorrectness of a candidate answer.The remainder of the paper is organized as fol-lows.
Section 2 presents the application of dis-criminative preference rank learning to answertyping.
Section 3 introduces the models we use666for learning appropriate answer preferences.
Sec-tions 4 and 5 discuss our experiments and their re-sults, respectively.
Section 6 presents prior workon answer typing and the use of discriminativemethods in QA.
Finally, concluding remarks andideas for future work are presented in Section 7.2 Preference RankingPreference ranking naturally lends itself to anyproblem in which the relative ordering betweenexamples is more important than labels or valuesassigned to those examples.
The classic exam-ple application of preference ranking (Joachims,2002) is that of information retrieval results rank-ing.
Generally, information retrieval results arepresented in some ordering such that those higheron the list are either more relevant to the query orwould be of greater interest to the user.In a preference ranking task we have a set ofcandidates c1, c2, ..., cn, and a ranking r such thatthe relation ci <r cj holds if and only if can-didate ci should be ranked higher than cj , for1 ?
i, j ?
n and i 6= j.
The ranking r can forma total ordering, as in information retrieval, or apartial ordering in which we have both ci ?r cjand cj ?r ci.
Partial orderings are useful for ourtask of answer typing because they can be used tospecify candidates that are of an equivalent rank.Given some ci <r cj , preference ranking onlyconsiders the difference between the feature rep-resentations of ci and cj (?
(ci) and ?
(cj), respec-tively) as evidence.
We want to learn some weightvector ~w such that ~w ??
(ci) > ~w ??
(cj) holds forall pairs ci and cj that have the relation ci <r cj .
Inother words, we want ~w ?
(?(ci)??
(cj)) > 0 andwe can use some margin in the place of 0.
In thecontext of Support Vector Machines (Joachims,2002), we are trying to minimize the function:V (~w, ~?)
=12~w ?
~w + C?
?i,j (1)subject to the constraints:?
(ci <r cj) : ~w ?
(?(ci)?
?
(cj)) ?
1?
?i,j (2)?i, j : ?i,j ?
0 (3)The margin incorporates slack variables ?i,j forproblems that are not linearly separable.
Thisranking task is analogous to the SVM classi-fication task on the pairwise difference vectors(?
(ci) ?
?
(cj)), known as rank constraints.
Un-like classification, no explicit negative evidence isrequired as ~w?(?(ci)??
(cj)) = (?1)~w?(?(cj)??(ci)).
It is also important to note that no rankconstraints are generated for candidates for whichno order relation exists under the ranking r.Support Vector Machines (SVMs) have previ-ously been used for preference ranking in thecontext of information retrieval (Joachims, 2002).We adopt the same framework for answer typingby preference ranking.
The SVMlight package(Joachims, 1999) implements the preference rank-ing of Joachims (2002) and is used here for learn-ing answer types.2.1 Application to Answer TypingAssigning meaningful scores for answer typing isa difficult task.
For example, given the question?What city hosted the 1988 Winter Olympics?
?and the candidates New York, Calgary, and theword blue, how can we identify New York andCalgary as appropriate and the word blue as inap-propriate?
Scoring answer candidates is compli-cated by the fact that a gold standard for appropri-ateness scores does not exist.
Therefore, we haveno a priori notion that New York is better than theword blue by some amount v. Because of this, weapproach the problem of answer typing as one ofpreference ranking in which the relative appropri-ateness is more important than the absolute scores.Preference ranking stands in contrast to classifi-cation, in which a candidate is classified as appro-priate or inappropriate depending on the values inits feature representation.
Unfortunately, simpleclassification does not work well in the face of alarge imbalance in positive and negative examples.In answer typing we typically have far more inap-propriate candidates than appropriate candidates,and this is especially true for the experiments de-scribed in Section 4.
This is indeed a problem forour system, as neither re-weighting nor attempt-ing to balance the set of examples with the useof random negative examples were shown to givebetter performance on development data.
This isnot to say that some means of balancing the datawould not provide comparable or superior perfor-mance, but rather that such a weighting or sam-pling scheme is not obvious.An additional benefit of preference ranking overclassification is that preference ranking models thebetter-than relationship between candidates.
Typ-ically a set of candidate answers are all related to aquestion in some way, and we wish to know which667of the candidates are better than others.
In con-trast, binary classification simply deals with theis/is-not relationship and will have difficulty whentwo responses with similar feature values are clas-sified differently.
With preference ranking, viola-tions of some rank constraints will affect the re-sulting order of candidates, but sufficient orderinginformation may still be present to correctly iden-tify appropriate candidates.To apply preference ranking to answer typing,we learn a model over a set of questions q1, ..., qn.Each question qi has a list of appropriate candidateanswers a(i,1), ..., a(i,u) and a list of inappropriatecandidate answers b(i,1), ..., b(i,v).
The partial or-dering r is simply the set?i, j, k : {a(i,j) <r b(i,k)} (4)This means that rank constraints are only gen-erated for candidate answers a(i,j) and b(i,k) forquestion qi and not between candidates a(i,j) andb(l,k) where i 6= l. For example, the candidate an-swers for the question ?What city hosted the 1988Winter Olympics??
are not compared with thosefor ?What colour is the sky??
because our partialordering r does not attempt to rank candidates forone question in relation to candidates for another.Moreover, no rank constraints are generated be-tween a(i,j) and a(i,k) nor b(i,j) and b(i,k) becausethe partial ordering does not include orderings be-tween two candidates of the same class.
Given twoappropriate candidates to the question ?What cityhosted the 1988 Winter Olympics?
?, New Yorkand Calgary, rank constraints will not be createdfor the pair (New York, Calgary).3 MethodsWe begin with the work of Pinchak and Lin (2006)in which question contexts (dependency tree pathsinvolving the wh-word) are extracted from thequestion and matched against those found in a cor-pus of text.
The basic idea is that words that areappropriate as answers will appear in place of thewh-word in these contexts when found in the cor-pus.
For example, the question ?What city hostedthe 1988 Winter Olympics??
will have as one ofthe question contexts ?X hosted Olympics.?
Wethen consult a corpus to discover what replace-ments for X were actually mentioned and smooththe resulting distribution.We use the model of Pinchak and Lin (2006)to produce features for our discriminative model.Table 1: Feature templatesPattern DescriptionE(t, c)Estimated count of term tin context cC(t, c)Observed count of term t incontext c?t?
C(t?, c)Count of all terms appearingin context c?c?
C(t, c?
)Count of term t in allcontextsS(t)Count of the times t occursin the candidate listThese features are mostly based on question con-texts, and are briefly summarized in Table 1.
Fol-lowing Pinchak and Lin (2006), all of our featuresare derived from a limited corpus (AQUAINT);large-scale text resources are not required for ourmodel to perform well.
By restricting ourselvesto relatively small corpora, we believe that our ap-proach will easily transfer to other domains or lan-guages (provided parsing resources are available).To address the sparseness of question contexts,we remove lexical elements from question contextpaths.
This removal is performed after feature val-ues are obtained for the fully lexicalized path; theremoval of lexical elements simply allows manysimilar paths to share a single learned weight.
Forexample, the term Calgary in context X ?
sub-ject ?
host ?
object ?
Olympics (X hostedOlympics) is used to obtain a feature value v thatis assigned to a feature such as C(Calgary, X ?subject ?
?
?
object ?
?)
= v. Removal oflexical elements results in a space of 73 possiblequestion contexts.
To facilitate learning, all countsare log values and feature vectors are normalizedto unit length.The estimated count of term t in context c,E(t, c), is a component of the model of Pinchakand Lin (2006) and is calculated according to:E(t, c) =?
?Pr(?|t)C(?, c) (5)Essentially, this equation computes an expectedcount for term t in question c by observing howlikely t is to be part of a cluster ?
(Pr(?|t)) andthen observing how often terms of cluster ?
oc-cur in context c (C(?, c)).
Although the modelof Pinchak and Lin (2006) is significantly more668complex, we use their core idea of cluster-basedsmoothing to decide how often a term t will oc-cur in a context c, regardless of whether or not twas actually observed in c within our corpus.
ThePinchak and Lin (2006) system is unable to as-sign individual weights to different question con-texts, even though not all question contexts areequally important.
For example, the Pinchak andLin (2006) model is forced to consider a questionfocus context (such as ?X is a city?)
to be of equalimportance to non-focus contexts (such as ?X hostOlympics?).
However, we have observed that it ismore important that candidate X is a city than ithosted an Olympics in this instance.
Appropriateanswers are required to be cities even though notall cities have hosted Olympics.
We wish to ad-dress this problem with the use of discriminativemethods.The observed count features of term t in con-text c, C(t, c), are included to allow for combina-tion with the estimated values from the model ofPinchak and Lin (2006).
Because Pinchak and Lin(2006) make use of cluster-based smoothing, er-rors may occur.
By including the observed countsof term t in context c, we hope to allow for theuse of more accurate statistics whenever they areavailable, and for the smoothed counts in cases forwhich they are not.Finally, we include the frequency of a term t inthe list of candidates, S(t).
The idea here is thatthe correct and/or appropriate answers are likelyto be repeated many times in a list of candidateanswers.
Terms that are strongly associated withthe question and appear often in results are likelyto be what the question is looking for.Both the C(t, c) and S(t) features are exten-sions to the Pinchak and Lin (2006) model and canbe incorporated into the Pinchak and Lin (2006)model with varying degrees of difficulty.
Thevalue of S(t) in particular is highly dependent onthe means used to obtain the candidate list, and thedistribution of words over the candidate list is of-ten very different from the distribution of words inthe corpus.
Because this feature value comes froma different source than our other features, it wouldbe difficult to use in a non-discriminative model.Correct answers to our set of questions areobtained from the TREC 2002-2006 results(Voorhees, 2002).
For appropriateness labels weturn to human annotators.
Two annotators were in-structed to label a candidate as appropriate if thatcandidate was believable as an answer, even if thatcandidate was not correct.
For a question such as?What city hosted the 1988 Winter Olympics?
?,all cities should be labeled as appropriate eventhough only Calgary is correct.
This task comeswith a moderate degree of difficulty, especiallywhen dealing with questions for which appropriateanswers are less obvious (such as ?What kind of acommunity is a Kibbutz??).
We observed an inter-annotator (kappa) agreement of 0.73, which indi-cates substantial agreement.
This value of kappaconveys the difficulty that even human annotatorshave when trying to decide which candidates areappropriate for a given question.
Because of thisvalue of kappa, we adopt strict gold standard ap-propriateness labels that are the intersection of thetwo annotators?
labels (i.e., a candidate is only ap-propriate if both annotators label it as such, other-wise it is inappropriate).We introduce four different models for the rank-ing of appropriate answers, each of which makesuse of appropriateness labels in different ways:Correctness Model: Although appropriatenessand correctness are not equivalent, this modeldeals with distinguishing correct from incorrectcandidates in the hopes that the resulting modelwill be able to perform well on finding both cor-rect and appropriate answers.
For learning, cor-rect answers are placed at a rank above that ofincorrect candidates, regardless of whether or notthose candidates are appropriate.
This representsthe strictest definition of appropriateness and re-quires no human annotation.Appropriateness Model: The correctness modelassumes only correct answers are appropriate.
Inreality, this is seldom the case.
For example,documents or snipppets returned for the question?What country did Catherine the Great rule??
willcontain not only Russia (the correct answer), butalso Germany (the nationality of her parents) andPoland (her modern-day birthplace).
To better ad-dress this overly strict definition of appropriate-ness, we rank all candidates labeled as appropri-ate above those labeled as inappropriate, withoutregards to correctness.
Because we want to learna model for appropriateness, training on appropri-ateness rather than correctness information shouldproduce a model closer to what we desire.Combined Model: Discriminative preferenceranking is not limited to only two ranks.
Wecombine the ideas of correctness and appropri-669ateness together to form a three-rank combinedmodel.
This model places correct answers aboveappropriate-but-incorrect candidates, which arein turn placed above inappropriate-and-incorrectcandidates.Reduced Model: Both the appropriateness modeland the combined model incorporate a large num-ber of rank constraints.
We can reduce the numberof rank constraints generated by simply remov-ing all appropriate, but incorrect, candidates fromconsideration and otherwise following the correct-ness model.
The main difference is that some ap-propriate candidates are no longer assigned a lowrank.
By removing appropriate, but incorrect, can-didates from the generation of rank constraints, weno longer rank correct answers above appropriatecandidates.4 ExperimentsTo compare with the prior approach of Pinchakand Lin (2006), we use a set of what and whichquestions with question focus (questions with anoun phrase following the wh-word).
These area subset of the more general what, which, and whoquestions dealt with by Pinchak and Lin (2006).Although our model can accommodate a widerange of what, which, when, and who questions,the focused what and which questions are an easilyidentifiable subclass that are rarely definitional orotherwise complex in terms of the desired answer.We take the set of focused what and which ques-tions from TREC 2002-2006 (Voorhees, 2002)comprising a total of 385 questions and performed9-fold cross-validation, with one dedicated devel-opment partition (the tenth partition).
The devel-opment partition was used to tune the regulariza-tion parameter of the SVM used for testing.Candidates are obtained by submitting the ques-tion as-is to the Google search engine and chunk-ing the top 20 snippets returned, resulting in anaverage of 140 candidates per question.
Googlesnippets create a better confusion set than simplyrandom words for appropriate and inappropriatecandidates; many of the terms found in Googlesnippets are related in some way to the question.To ensure a correct answer is present (where pos-sible), we append the list of correct answers to thelist of candidates.As a measure of performance, we adopt MeanReciprocal Rank (MRR) for both correct and ap-propriate answers, as well as precision-recall forappropriate answers.
MRR is useful as a mea-sure of overall QA system performance (Voorhees,2002), but is based only on the top correct orappropriate answer encountered in a ranked list.For this reason, we also show the precision-recallcurve to better understand how our models per-form.We compare our models with three alternativeapproaches, the simplest of which is random.
Forrandom, the candidate answers are randomly shuf-fled and performance is averaged over a numberof runs (100).
The snippet frequency approachorders candidates based on their frequency of oc-currence in the Google snippets, and is simply theS(t) feature of our discriminative models in isola-tion.
We remove terms comprised solely of ques-tion words from all approaches to prevent questionwords (which tend to be very frequent in the snip-pets) from being selected as answers.
The last ofour alternative systems is an implementation of thework of Pinchak and Lin (2006) in which the out-put probabilities of their model are used to rankcandidates.4.1 ResultsFigures 1 and 2 show the MRR results andprecision-recall curve of our correctness modelagainst the alternative approaches.
In comparisonto these alternative systems, we show two versionsof our correctness model.
The first uses a linearkernel and is able to outperform the alternative ap-proaches.
The second uses a radial basis function(RBF) kernel and exhibits performance superior tothat of the linear kernel.
This suggests a degreeof non-linearity present in the data that cannot becaptured by the linear kernel alone.
Both the train-ing and running times of the RBF kernel are con-siderably larger than that of the linear kernel.
Theaccuracy gain of the RBF kernel must therefore beweighed against the increased time required to usethe model.Figures 3 and 4 give the MRR results andprecision-recall curves for our additional mod-els in comparison with that of the correctnessmodel.
Although losses in MRR and precisionare observed for both the appropriate and com-bined model using the RBF kernel, the linear ker-nel versions of these models show slight perfor-mance gains.670Figure 1: MRR results for the correctness modelFirst Correct Answer First Appropriate Candidate00.10.20.30.40.50.60.70.80.91MeanReciprocalRank(MRR)RandomSnippet FrequencyPinchak and Lin (2006)Linear KernelRBF Kernel5 Discussion of ResultsThe results of our correctness model, found in Fig-ures 1 and 2 show considerable gains over our al-ternative systems, including that of Pinchak andLin (2006).
The Pinchak and Lin (2006) systemwas specifically designed with answer typing inmind, although it makes use of a brittle generativemodel that does not account for ranking of answercandidates nor for the variable strength of variousquestion contexts.
These results show that our dis-criminative preference ranking approach creates abetter model of both correctness and appropriate-ness via weighting of contexts, preference ranklearning, and with the incorporation of additionalrelated features (Table 1).
The last feature, snippetfrequency, is not particularly strong on its own, butcan be easily incorporated into our discriminativemodel.
The ability to add a wide variety of po-tentially helpful features is one of the strengths ofdiscriminative techniques in general.By moving away from simply correct answersin the correctness model and incorporating labeledappropriate examples in various ways, we are ableto further improve upon the performance of ourapproach.
Training on appropriateness labels in-stead of correct answers results in a loss in MRRfor the first correct answer, but a gain in MRR forthe first appropriate candidate.
Unfortunately, thisdoes not carry over to the entire range of precisionover recall.
For the linear kernel, our three ad-Figure 2: Precision-recall of appropriate candi-dates under the correctness model0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recall00.10.20.30.40.50.60.70.80.91PrecisionRBF KernelLinear KernelPinchak & Lin (2006)Snippet FrequencyRandomditional models (appropriateness, combined, andreduced) show consistent improvements over thecorrectness model, but with the RBF kernel onlythe reduced model produces a meaningful change.The precision-recall curves of Figures 2 and 4show remarkable consistency across the full rangeof recall, despite the fact that candidates exist forwhich feature values cannot easily be obtained.Due to tagging and chunking errors, ill-formedcandidates may exist that are judged appropriateby the annotators.
For example, ?explorer Her-nando Soto?
is a candidate marked appropriateby both annotators to the question ?What Span-ish explorer discovered the Mississippi River?
?However, our context database does not includethe phrase ?explorer Hernando Soto?
meaning thatonly a few features will have non-zero values.
De-spite these occasional problems, our models areable to rank most correct and appropriate candi-dates high in a ranked list.Finally, we examine the effects of training setsize on MRR.
The learning curve for a single par-titioning under the correctness model is presentedin Figure 5.
Although the model trained withthe RBF kernel exhibits some degree of instabil-ity below 100 training questions, both the linearand RBF models gain little benefit from additionaltraining questions beyond 100.
This may be dueto the fact that the most common unlexicalizedquestion contexts have been observed in the first671Figure 3: MRR results (RBF kernel)First Correct Answer First Appropriate Candidate00.10.20.30.40.50.60.70.80.91MeanReciprocalRank(MRR)Correctness ModelAppropriateness ModelCombined ModelReduced Model100 training examples and so therefore additionalquestions simply repeat the same information.
Re-quiring only a relatively small number of trainingexamples means that an effective model can belearned with relatively little input in the form ofquestion-answer pairs or annotated candidate lists.6 Prior WorkThe expected answer type can be captured in anumber of possible ways.
By far the most com-mon is the assignment of one or more prede-fined types to a question during a question anal-ysis phase.
Although the vast majority of the ap-proaches to answer type detection make use ofrules (either partly or wholly) (Harabagiu et al,2005; Sun et al, 2005; Wu et al, 2005; Molla?
andGardiner, 2004), a few notable learned methodsfor answer type detection exist.One of the first attempts at learning a model foranswer type detection was made by Ittycheriah etal.
(2000; 2001) who learn a maximum entropyclassifier over the Message Understanding Confer-ence (MUC) types.
Those same MUC types arethen assigned by a named-entity tagger to iden-tify appropriate candidate answers.
Because of thepotential for unanticipated types, Ittycheriah et al(2000; 2001) include a Phrase type as a catch-allclass that is used when no other class is appropri-ate.
Although the classifier and named-entity tag-ger are shown to be among the components withFigure 4: Precision-recall of appropriate (RBFkernel)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recall00.10.20.30.40.50.60.70.80.91PrecisionCorrectness ModelAppropriateness ModelCombined ModelReduced Modelthe lowest error rate in their QA system, it is notclear how much benefit is obtained from using arelatively coarse-grained set of classes.The approach of Li and Roth (2002) is sim-ilar in that it uses learning for answer type de-tection.
They make use of multi-class learningwith a Sparse Network of Winnows (SNoW) anda two-layer class hierarchy comprising a total offifty possible answer types.
These finer-grainedclasses are of more use when computing a notionof appropriateness, although one major drawbackis that no entity tagger is discussed that can iden-tify these types in text.
Li and Roth (2002) alsorely on a rigid set of classes and so run the risk ofencountering a new question of an unseen type.Pinchak and Lin (2006) present an alternative inwhich the probability of a term being appropriateto a question is computed directly.
Instead of as-signing an answer type to a question, the questionis broken down into a number of possibly overlap-ping contexts.
A candidate is then evaluated as tohow likely it is to appear in these contexts.
Un-fortunately, Pinchak and Lin (2006) use a brittlegenerative model when combining question con-texts that assumes all contexts are equally impor-tant.
This assumption was dealt with by Pinchakand Lin (2006) by discarding all non-focus con-texts with a focus context is present, but this is notan ideal solution.Learning methods are abundant in QA research672Figure 5: Learning curve for MRR of the first cor-rect answer under the correctness model10 25 50 100 150 200 310Training Set Size00.10.20.30.40.50.60.70.80.91MeanReciprocalRank(MRR)RBF KernelLinear KernelSnippet FrequencyPinchak & Lin (2006)Randomand have been applied in a number of differentways.
Ittycheriah et al (2000) created an en-tire QA system based on maximum entropy com-ponents in addition to the question classifier dis-cussed above.
Ittycheriah et al (2000) were ableto obtain reasonable performance from learnedcomponents alone, although future versions of thesystem use non-learned components in addition tolearned components (Prager et al, 2003).
TheJAVELIN I system (Nyberg et al, 2005) usesa SVM during the answer/information extractionphase.
Although learning is applied in many QAtasks, very few QA systems rely solely on learn-ing.
Compositional approaches, in which multipledistinct QA techniques are combined, also showpromise for improving QA performance.
Echihabiet al (2003) use three separate answer extractionagents and combine the output scores with a max-imum entropy re-ranker.Surdeanu et al (2008) explore preference rank-ing for advice or ?how to?
questions in which aunique correct answer is preferred over all othercandidates.
Their focus is on complex-answerquestions in addition to the use of a collection ofuser-generated answers rather than answer typing.However, their use of preference ranking mirrorsthe techniques we describe here in which the rela-tive difference between two candidates at differentranks is more important than the individual candi-dates.7 Conclusions and Future WorkWe have introduced a means of flexible answertyping with discriminative preference rank learn-ing.
Although answer typing does not represent acomplete QA system, it is an important componentto ensure that those candidates selected as answersare indeed appropriate to the question being asked.By casting the problem of evaluating appropriate-ness as one of preference ranking, we allow forthe learning of what differentiates an appropriatecandidate from an inappropriate one.Experimental results on focused what andwhich questions show that a discriminativelytrained preference rank model is able to outper-form alternative approaches designed for the sametask.
This increase in performance comes fromboth the flexibility to easily combine a number ofweighted features and because comparisons onlyneed to be made between appropriate and inappro-priate candidates.
A preference ranking model canbe trained from a relatively small set of examplequestions, meaning that only a small number ofquestion/answer pairs or annotated candidate listsare required.The power of an answer typing system liesin its ability to identify, in terms of some givenquery, appropriate candidates.
Applying the flexi-ble model described here to a domain other thanquestion answering could allow for a more fo-cused set of results.
One straight-forward appli-cation is to apply our model to the process of in-formation or document retrieval itself.
Ensuringthat there are terms present in the document ap-propriate to the query could allow for the intel-ligent expansion of the query.
In a related vein,queries are occasionally comprised of natural lan-guage text fragments that can be treated similarlyto questions.
Rarely are users searching for sim-ple mentions of the query in pages; we wish toprovide them with something more useful.
Ourmodel achieves the goal of finding those appropri-ate related concepts.AcknowledgmentsWe would like to thank Debra Shiau for her as-sistance annotating training and test data and theanonymous reviewers for their insightful com-ments.
We would also like to thank the AlbertaInformatics Circle of Research Excellence and theAlberta Ingenuity Fund for their support in devel-oping this work.673ReferencesA.
Echihabi, U. Hermjakob, E. Hovy, D. Marcu,E.
Melz, and D. Ravichandran.
2003.
Multiple-Engine Question Answering in TextMap.
In Pro-ceedings of the Twelfth Text REtrieval Conference(TREC-2003), Gaithersburg, Maryland.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden,A.
Hickl, and P. Wang.
2005.
Employing TwoQuestion Answering Systems in TREC-2005.
InProceedings of the Fourteenth Text REtrieval Con-ference (TREC-2005), Gaithersburg, Maryland.A.
Ittycheriah, M. Franz, W-J.
Zhu, A. Ratnaparkhi,and R. Mammone.
2000.
IBM?s Statistical Ques-tion Answering System.
In Proceedings of the 9thText REtrieval Conference (TREC-9), Gaithersburg,Maryland.A.
Ittycheriah, M. Franz, and S. Roukos.
2001.
IBM?sStatistical Question Answering System ?
TREC-10.In Proceedings of the 10th Text REtrieval Confer-ence (TREC-10), Gaithersburg, Maryland.T.
Joachims.
1999.
Making Large-Scale SVM Learn-ing Practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods -Support Vector Learning.
MIT-Press.T.
Joachims.
2002.
Optimizing Search Engines Us-ing Clickthrough Data.
In Proceedings of the ACMConference on Knowledge Discovery and Data Min-ing (KDD).
ACM.X.
Li and D. Roth.
2002.
Learning Question Clas-sifiers.
In Proceedings of the International Confer-ence on Computational Linguistics (COLING 2002),pages 556?562.D.
Molla?
and M. Gardiner.
2004.
AnswerFinder -Question Answering by Combining Lexical, Syntac-tic and Semantic Information.
In Proceedings of theAustralian Language Technology Workshop (ALTW2004, pages 9?16, Sydney, December.E.
Nyberg, R. Frederking, T. Mitamura, M. Bilotti,K.
Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita,V.
Pedro, and A. Schlaikjer.
2005.
JAVELIN I andII Systems at TREC 2005.
In Proceedings of theFourteenth Text REtrieval Conference (TREC-2005),Gaithersburg, Maryland.C.
Pinchak and D. Lin.
2006.
A Probabilistic AnswerType Model.
In Proceedings of the Eleventh Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL 2006), Trento,Italy, April.J.
Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-cheriah, and R. Mahindru.
2003.
IBM?s PIQUANTin TREC2003.
In Proceedings of the Twelfth TextREtrieval Conference (TREC-2003), Gaithersburg,Maryland.R.
Sun, J. Jiang, Y.F.
Tan, H. Cui, T-S. Chua, and M-Y.Kan.
2005.
Using Syntactic and Semantic RelationAnalysis in Question Answering.
In Proceedingsof the Fourteenth Text REtrieval Conference (TREC-2005), Gaithersburg, Maryland.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2008.Learning to rank answers on large online QA collec-tions.
In Proceedings of the 46th Annual Meeting forthe Association for Computational Linguistics: Hu-man Language Technologies (ACL-08: HLT), pages719?727, Columbus, Ohio, June.
Association forComputational Linguistics.E.M.
Voorhees.
2002.
Overview of the TREC 2002Question Answering Track.
In Proceedings ofTREC 2002, Gaithersburg, Maryland.M.
Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-lkowski.
2005.
ILQUA ?
An IE-Driven Ques-tion Answering System.
In Proceedings of theFourteenth Text REtrieval Conference (TREC-2005),Gaithersburg, Maryland.674
