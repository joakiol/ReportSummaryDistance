Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 519?528,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA generative model for unsupervised discovery of relations and argumentclasses from clinical textsBryan Rink and Sanda HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX, USA{bryan,sanda}@hlt.utdallas.eduAbstractThis paper presents a generative model forthe automatic discovery of relations betweenentities in electronic medical records.
Themodel discovers relation instances and theirtypes by determining which context tokens ex-press the relation.
Additionally, the valid se-mantic classes for each type of relation are de-termined.
We show that the model producesclusters of relation trigger words which bet-ter correspond with manually annotated re-lations than several existing clustering tech-niques.
The discovered relations reveal someof the implicit semantic structure present inpatient records.1 IntroductionSemantic relations in electronic medical records(EMRs) capture important meaning about the as-sociations between medical concepts.
Knowledgeabout how concepts such as medical problems, treat-ments, and tests are related can be used to improvemedical care by speeding up the retrieval of relevantpatient information or alerting doctors to critical in-formation that may have been overlooked.
Whendoctors write progress notes and discharge sum-maries they include information about how treat-ments (e.g., aspirin, stent) were administered forproblems (e.g.
pain, lesion) along with the out-come, such as an improvement or deterioration.
Ad-ditionally, a doctor will describe the tests (e.g., x-ray, blood sugar level) performed on a patient andwhether the tests were conducted to investigate aknown problem or revealed a new one.
These textualdescriptions written in a patient?s record encode im-portant information about the relationships betweenthe problems a patients has, the treatments taken forthe problems, and the tests which reveal and investi-gate the problems.The ability to accurately detect semantic rela-tions in EMRs, such as Treatment-Administered-for-Problem, can aid in querying medical records.
Af-ter a preprocessing phase in which the relations aredetected in all records they can be indexed and re-trieved later as needed.
A doctor could search forall the times that a certain treatment has been usedon a particular problem, or determine all the treat-ments used for a specific problem.
An additionalapplication is the use of the relational informationto flag situations that merit further review.
If a pa-tient?s medical record indicates a test that was foundto reveal a critical problem but no subsequent treat-ment was performed for the problem, the patient?srecord could be flagged for review.
Similarly, ifa Treatment-Worsens-Problem relation is detectedpreviously in a patient?s record, that information canbe brought to the attention of a doctor who advisessuch a treatment in the future.
By considering allof the relations present in a corpus, better medicalontologies could be built automatically or existingones can be improved by adding additional connec-tions between concepts that have a relation in text.Given the large size of EMR repositories, we ar-gue that it is quite important to have the ability toperform relation discovery between medical con-cepts.
Relations between medical concepts benefittranslational medicine whenever possible relationsare known.
Uzuner et al (2011) show that super-519vised methods recognize such relations with high ac-curacy.
However, large sets of annotated relationsneed to be provided for this purpose.
To addressboth the problem of discovering unknown relationsbetween medical concepts and the related problemof generating examples for known relations, we havedeveloped an unsupervised method.
This approachhas the advantages of not requiring an expensive an-notation effort to provide training data for seman-tic relations, which is particularly difficult for medi-cal records, characterized by many privacy concerns.Our analysis shows a high level of overlap betweenthe manually annotated relations and those that werediscovered automatically.
Our experimental resultsshow that this approach improves upon simpler clus-tering techniques.The remainder of this paper is organized as fol-lows.
Section 2 discusses the related work.
Section3 reports our novel generative model for discoveringrelations in EMRs, Section 4 details the inferenceand parameter estimation of our method.
Section5 details our experiments, Section 6 discusses ourfindings.
Section 7 summarizes the conclusions.2 Related WorkPrevious methods for unsupervised relation dis-covery have also relied on clustering techniques.One technique uses the context of entity argumentsto cluster, while another is to perform a post-processing step to cluster relations found using anexisting relation extraction system.
The approachesmost similar to ours have taken features from thecontext of pairs of entities and used those features toform a clustering space.
In Hasegawa et al (2004),those features are tokens found within a context win-dow of the entity pair.
Distance between entity pairsis then computed using cosine similarity.
In anotherapproach, Rosenfeld and Feldman (2007) use hierar-chical agglomerative clustering along with featuresbased on token patterns seen in the context, againcompared by cosine similarity.Other approaches to unsupervised relation dis-covery have relied on a two-step process where anumber of relations are extracted, usually from apredicate-argument structure.
Then similar relationsare clustered together since synonymous predicatesshould be considered the same relation (e.g.
?ac-quire?
and ?purchase?).
Yates (2009) considers theoutput from an open information extraction system(Yates et al, 2007) and clusters predicates and argu-ments using string similarity and a combination ofconstraints.
Syed and Viegas (2010) also perform aclustering on the output of an existing relation ex-traction system by considering the number of timestwo relations share the same exact arguments.
Sim-ilar relations are expected to have the same pairsof arguments (e.g.
?Ford produces cars?
and ?Fordmanufactures cars?).
These approaches and others(Agichtein and Gravano, 2000; Pantel and Pennac-chiotti, 2006) rely on an assumption that relationsare context-independent, such as when a person isborn, or the capital of a nation.
Our method willdiscover relations that can depend on the context aswell.
For instance, ?penicillin?
may be causally re-lated to ?allergic reaction?
in one patient?s medicalrecord but not in another.
The relation between thetwo entities is not globally constant and should beconsidered only within the scope of one patient?srecords.Additionally, these two-step approaches tendto rely on predicate-argument structures such assubject-verb-object triples to detect arbitrary rela-tions (Syed and Viegas, 2010; Yates et al, 2007).Such approaches can take advantage of the largebody of research that has been done on extractingsyntactic parse structure and semantic role infor-mation from text.
However, these approaches canoverlook relations in text which do not map easilyonto those structures.
Unlike these approaches, ourmodel can detect relations that are not expressed asa verb, such as ?
[cough] + [green sputum]?
to ex-press a conjunction or ?
[Cl] 119 mEq / L [High]?
toexpress that a test reading is indicating a problem.The 2010 i2b2/VA Challenge (Uzuner et al,2011) developed a set of annotations for medicalconcepts and relations on medical progress notesand discharge summaries.
One task at the challengeinvolved developing systems for the extraction ofeight types of relations between concepts.
We usethis data set to compare our unsupervised methodwith others.The advantage of our work over existing unsu-pervised approaches is the simultaneous clusteringof both argument words and relation trigger words.These broad clusters handle: (i) synonyms, (ii) argu-520ment semantic classes, and (iii) words belonging tothe same relation.3 A Generative Model for DiscoveringRelations3.1 Unsupervised Relation DiscoveryA simple approach to discovering relations betweenmedical entities in clinical texts uses a clustering ap-proach, e.g.
Latent Dirichlet Allocation (LDA) (Bleiet al, 2003).
We start with an assumption that rela-tions exist between two entities, which we call argu-ments, and may be triggered by certain words be-tween those entities which we call trigger words.For example, given the text ?
[x-ray] revealed [lungcancer]?, the first argument is x-ray, the second ar-gument is lung cancer, and the trigger word is re-vealed.
We further assume that the arguments mustbelong to a small set of semantic classes specific tothe relation.
For instance, x-ray belongs to a classof medical tests, whereas lung cancer belongs to aclass of medical problems.
While relations may ex-ist between distant entities in text, we focus on thosepairs of entities in text which have no other entitiesbetween them.
This increases the likelihood of a re-lation existing between the entities and minimizesthe number of context words (words between the en-tities) that are not relevant to the relation.With these assumptions we build a baseline rela-tion discovery using LDA.
LDA is used as a baselinebecause of its similarities with our own generativemodel presented in the next section.
Each consec-utive pair of entities in text is extracted, along withthe tokens found between them.
Each of the entitiesin a pair is split into tokens which are taken alongwith the context tokens to form a single pseudo-document.
When the LDA is processed on all suchpseudo-documents, clusters containing words whichco-occur are formed.
Our assumption that relationarguments come from a small set of semantic classesshould lead to clusters which align with relationssince the two arguments of a relation will co-occurin the pseudo-documents.
Furthermore, those argu-ment tokens should co-occur with relation triggerwords as well.This LDA-based approach was examined on elec-tronic medical records from the 2010 i2b2/VA Chal-lenge data set (Uzuner et al, 2011).
The data setCluster 1Words: secondary, due, likely, patient, disease,liver, abdominal, cancer, pulmonary, respiratory,elevated, volume, chronic, edema, related?Correct?
instances: [Metastatic colon cancer]with [abdominal carcinomatosis]; [symptoms]were due to [trauma]?Incorrect?
instances: [mildly improving symp-toms] , plan will be to continue with [his cur-rent medicines]; [prophylaxis] against [peptic ul-cer disease]Cluster 2:Words: examination, no, positive, culture, exam,blood, patient, revealed, cultures, physical, out,urine, notable, showed, cells?Correct?
instances: [a blood culture] grew out[Staphylococcusaureus]; [tamponade] by [exam-ination]?Incorrect?
instances: [the intact drain] drain-ing [bilious material]; [a Pseudomonas cellulitis]and [subsequent sepsis]Figure 1: Two clusters found by examining the mostlikely words under two LDA topics.
The instances arepseudo-documents whose probability of being assignedto that cluster was over 70%contains manually annotated medical entities whichwere used to form the pairs of entities needed.
Forexample, Figure 1 illustrates examples of two clus-ters out of 15 discovered automatically using LDAon the corpus.
The first cluster appears to containwords which indicate a relation whose two argu-ments are both medical problems (e.g.
?disease?,?cancer?, ?edema?).
The trigger words seem to in-dicate a possible causal relation (e.g., ?due?, ?re-lated?, ?secondary?).
The second cluster containswords relevant to medical tests (e.g.
?examination?,?culture?)
and their findings (?revealed?, ?showed?,?positive?).
As illustrated in Figure 1, some of thecontext words are not necessarily related to the re-lation.
The word ?patient?
for instance is presentin both clusters but is not a trigger word becauseit is likely to be seen in the context of any rela-tion in medical text.
The LDA-based model treatsall words equally and cannot identify which wordsare likely trigger words and which ones are generalwords, which merely occur frequently in the context521of a relation.In addition, while the LDA approach can de-tect argument words which co-occur with triggerwords (e.g., ?examination?
and ?showed?
), the clus-ters produced with LDA do not differentiate betweencontextual words and words which belong to the ar-guments of the relation.
An approach which mod-els arguments separately from context words couldlearn the semantic classes of those arguments andthus better model relations.
Considering the exam-ples from Figure 1, a model which could cluster?examination?, ?exam?, ?cultures?, and ?culture?into one medical test cluster and ?disease?, ?cancer?and ?edema?
into a medical problem cluster separatefrom the relation trigger words and general wordsshould model relations more accurately by better re-flecting the implicit structure of the text.
Because ofthese limitations many relations discovered in thisway are not accurate, as can be seen in Figure 1.3.2 Relation Discovery Model (RDM)The limitations identified in the LDA-based ap-proach are solved by a novel relation discoverymodel (RDM) which jointly models relation argu-ment semantic classes and considers them separatelyfrom the context words.
Relations triggered by pairsof medical entities enable us to consider three ob-servable features: (A1) the first argument; (A2)the second argument; and (CW) the context wordsfound between A1 and A2.For instance, in sentence S1 the arguments areA1=?some air hunger?
and A2=?his tidal volume?while the context words are ?last?, ?night?, ?when?,?I?, and ?dropped?.S1: He developed [some air hunger]PROB last nightwhen I dropped [his tidal volume]TREAT from 450to 350.In the RDM, the contextual words are assumed tocome from a mixture model with 2 mixture compo-nents: a relation trigger word (x = 0), or a generalword (x = 1), where x is a variable representingwhich mixture component a word belongs to.
Insentence S1 for example, the word ?dropped?
canbe seen as a trigger word for a Treatment-Causes-Problem relation.
The remaining words are not trig-ger words and hence are seen as general words.Under the RDM?s mixture model, the probabilityof a context word is:P (wC |tr, z) =P (wC |tr, x = 0) ?
P (x = 0|tr) +P (wC |z, x = 1) ?
P (x = 1|tr)Where wC is a context word, the variable tr isthe relation type, and z is the general word class.The variable x chooses whether a context wordcomes from a relation-specific distribution of trig-ger words, or from a general word class.
In theRDM, the two argument classes are modeled jointlyas P (c1, c2|tr), where c1 and c2 are two semanticclasses associated with a relation of type tr.
How-ever the assignment of classes to arguments dependson a directionality variable d. If d = 0, then the firstargument is assigned semantic class c1 and the sec-ond is assigned class c2.
When d = 1 however, theclass assignments are swapped.
This models the factthat a relation?s arguments do not come in a fixedorder, ?
[MRI] revealed [tumor]?
is the same type ofrelation as ?
[tumor] was revealed by [x-ray]?.
Fig-ure 2 shows the graphical model for the RDM.
Eachcandidate relation is modeled independently, with atotal of I relation candidates.
Variable w1 is a wordobserved from the first argument, and w2 is a wordobserved from the second argument.
The modeltakes parameters for the number of relations types(R), the number of argument semantic classes (A),and the number of general word classes (K).
Thegenerative process for the RDM is:1.
For relation type r = 1..R:(a) Draw a binomial distribution ?r fromBeta(?x) representing the mixture distri-bution for relation r(b) Draw a joint semantic class distribution?1,2r ?
RC?C from Dirichlet(?1,2).2.
Draw a categorical word distribution ?zz?
fromDirichlet(?z) for each general word classz?
= 1..K3.
Draw a categorical word distribution ?rr?
fromDirichlet(?r) for each r?
= 1..R4.
for semantic class a?
= 1..A:(a) Draw categorical word distributions?1a?
and ?2a?
from Dirichlet(?1) andDirichlet(?2) for the first and secondarguments, respectively.522trdc1,2w1w2xzwC????
?1,2?z?r?x?d?1,2?z?r?1?2?z?r?1?2W1W2WCIRRRKRAAFigure 2: Graphical model for the RDM.
c1,2 represents the joint generation of c1 and c2P (tr, d|tr?i,d?i, c1,2?i ,x?i,z?i,wC?i,w1?i,w2?i;?,?)
?
u1 ?
u2 ?
u3u1 = f(tr)+?rI+R?r ?f(tr ,d)+?ddf(tr)+?d0+?d1?
f(tr ,c1,c2)+?1,2f(tr)+C?C?1,2u2 =?WCjfi(zj)+?zWC+K?z ?f(tr ,xi)+?xf(tr)+2?x ?
(1x=0f(tr ,wCj )+?rf(tr)+W?r + 1x=1f(zj ,wCj )+?zf(zj)+W?z )u3 =?W1jf(a1,w1j )+?1f(a1)+W?1 ?
?W2jf(a2,w2j )+?2f(a2)+W?2Figure 3: Gibbs sampling update equation for variables tr and d for the ith relation candidate.
The variables a1 = c1and a2 = c2 if d = 0, or a1 = c2 and a2 = c1 if d = 1.
W is the size of the vocabulary.
f(?)
is the count ofthe number of times that event occurred, excluding assignments for the relation instance being sampled.
For instance,f(tr, d) =?Ik 6=i I[trk = tri ?
dk = di]5.
Draw a categorical relation type distribution ?from Dirichlet(?r)6.
For each pair of consecutive entities in the cor-pus, i = 1..I:(a) Sample a relation type tr from ?
(b) Jointly sample semantic classes c1 and c2for the first and second arguments from?1,2tr(c) Draw a general word class categorical dis-tribution ?
from Dirichlet(?z)(d) For each token j = 1..W1 in the first ar-gument: Sample a word w1j from ?1c1 ifd = 0 or ?2c2 if d = 1(e) For each token j = 1..W2 in the secondargument: Sample a word w2j from ?2c2 ifd = 0 or ?1c1 if d = 1(f) For each token j = 1..WC in the contextof the entities:i.
Sample a general word class z from ?ii.
Sample a mixture component x from?triii.
Sample a word from ?rtr if x = 0 or?zz if x = 1.In the RDM, words from the arguments are in-formed by the relation through an argument seman-tic class which is sampled from P (c1, c2|tr) = ?1,2tr .Furthermore, words from the context are informedby the relation type.
These dependencies enablemore coherent relation clusters to form during pa-rameter estimation because argument classes and re-lation trigger words are co-clustered.We chose to model two distinct sets of entitywords (?1 and ?2) depending on whether the entityoccurred in the first argument or the second argu-ment of the relation.
The intuition for using disjointsets of entities is based on the observation that anentity may be expressed differently if it comes firstor second in the text.4 Inference and Parameter EstimationAssignments to the hidden variables in RDM canbe made by performing collapsed Gibbs sampling(Griffiths and Steyvers, 2004).
The joint probabilityof the data is:523P (wC,w1,w2;?,?)
?P (?|?x)P (?|?r)P (?|?d)P (?1,2|?1,2)?P (?z|?z)P (?r|?r)P (?1|?1)P (?2|?2)?
?Ii [P (?i|?z)P (tri |?
)P (di|tr, ?tr )P (c1i , c2i |tr, ?1,2)?
?WC,ij P (zi,j |?i)P (xi,j |tri , ?tri )P (wCi,j |xi,j , tri , zi,j)?
?W1,ij P (w1j |di, c1,2i , ?1)?
?W2,ij P (w2j |di, c1,2i , ?2)]We need to sample variables tr, d, c1,2, x, andz.
We sample tr and d jointly while each of theother variables is sampled individually.
Afterintegrating out the multinomial distributions, wecan sample tr and d from the equation in Figure 3The update equations for the remaining variablescan be derived from the same equation by droppingterms which are constant across changes in that vari-able.In our experiments the hyperparameters were setto ?x = 1.0, ?z = 1.0, ?1,2 = 1.0, ?d0 = 2, ?d1 =1, ?r = 0.01, ?z = 0.01, ?1 = 1.0, ?2 = 1.0.Changing the hyperparameters did not significantlyaffect the results.5 Experimental Results5.1 Experimental SetupWe evaluated the RDM using a corpus of electronicmedical records provided by the 2010 i2b2/VAChallenge (Uzuner et al, 2011).
We used thetraining set, which consists of 349 medical recordsfrom 4 hospitals, annotated with medical concepts(specifically problems, treatments, and tests),along with any relations present between thoseconcepts.
We used these manually annotatedrelations to evaluate how well the RDM performsat relation discovery.
The corpus is annotatedwith a set of eight relations: Treatment-Addresses-Problem, Treatment-Causes-Problem, Treatment-Improves-Problem, Treatment-Worsens-Problem,Treatment-Not-Administered-due-to-Problem, Test-Reveals-Problem, Test-Conducted-for-Problem, andProblem-Indicates-Problem.
The data contains13,460 pairs of consecutive concepts, of which3,613 (26.8%) have a relation belonging to the listabove.
We assess the model using two versions ofthis data set consisting of: those pairs of consecutiveRelation 1 Relation 2 Relation 3 Relation 4mg ( due showedp.r.n. )
consistent nop.o.
Working not revealedhours ICD9 likely evidenceprn Problem secondary doneq Diagnosis patient 2007needed 30 ( performedday cont started demonstratedq.
): most without4 closed s/p normal2 SNMCT seen showsevery **ID-NUM related foundone PRN requiring showingtwo mL including negative8 ML felt wellFigure 4: Relation trigger words found by the RDMentities which have a manually annotated relation(DS1), and secondly, all consecutive pairs of entities(DS2).
DS1 allows us to assess the RDM?s cluster-ing without the noise introduced from those pairslacking a true relation.
Evaluations on DS2 willindicate the level of degradation caused by largenumbers of entity pairs that have no true relation.We also use a separate test set to assess how wellthe model generalizes to new data.
The test setcontains 477 documents comprising 9,069 manuallyannotated relations.5.2 AnalysisFigure 4 illustrates four of the fifteen trigger wordclusters (most likely words according to ?r) learnedfrom dataset DS1 using the best set of parametersaccording to normalized mutual information (NMI)as described in section 5.3.
These parameters were:R = 9 relations, K = 15 general word classes, andA = 15 argument classes.
Examination of the mostlikely words reveals a variety of trigger words, be-yond obvious explicit ones.
Example sentences forthe relation types from Figure 4 are presented in Fig-ure 5 and discussed below.Relation Type 1Instances of this discovered relation are often foundembedded in long lists of drugs prescribed to thepatient.
Tokens such as ?p.o.?
and ?p.r.n.
?, mean-ing respectively ?by mouth?
and ?when necessary?,are indicative of a prescription relation.
The learnedrelation specifically considers arguments of a drug524Instances of Relation Type 11.
Haldol 0.5-1 milligrams p.o.
q.6-8h.
p.r.n.
agitation2.
plavix every day to prevent failure of these stents3.
KBL mouthwash , 15 ccp .o.
q.d.
prn mouth discomfort4.
Miconazole nitrate powder tid prn for groin rash5.
AmBisome 300 mg IV q.d.
for treatment of her hepatic candidiasisInstances of Relation Type 21.
MAGNESIUM HYDROXIDE SUSP 30 ML ) , 30 mL , Susp , By Mouth , At Bedtime , PRN , For Constipation2.
Depression , major ( ICD9 296.00 , Working , Problem ) cont NOS home meds3.
Diabetes mellitus type II ( ICD9 250.00 , Working , Problem ) cont home meds4.
ASCITES ( ICD9 789.5 , Working , Diagnosis ) on spironalactone5.
*Dilutional hyponatremia ( SNMCT **ID-NUM , Working , Diagnosis ) improved with fluid restrictionInstances of Relation Type 31.
ESRD secondary to her DM2.
slightly lightheaded and with increased HR3.
a 40% RCA , which was hazy4.
echogenic kidneys consistent with renal parenchymal disease5.
*Librium for alcohol withdrawalInstances of Relation Type 41.
V-P lung scan was performed on May 24 2007 , showed low probability of PE2.
a bedside transthoracic echocardiogram done in the Cardiac Catheterization laboratory without evidence ofan effusion3.
exploration of the abdomen revealed significant nodularity of the liver4.
Echocardiogram showed moderate dilated left atrium5.
An MRI of the right leg was done which was equivocal for osteomyelitisFigure 5: Examples for four of the discovered relations.
Those marked with an asterisk have a different manuallychosen relation than the othersand a symptom treated by that drug.
The closestmanually chosen relation is Treatment-Addresses-Problem which included drugs as treatments.Relation Type 2Relation 2 captures a similar kind of relation to Re-lation 1.
All five examples for Relation 1 in Fig-ure 5 came from a different set of hospitals than theexamples for Relation 2.
This indicates the modelis detecting stylistic differences in addition to se-mantic differences.
This is one of shortcomings ofsimple generative models.
Because they cannot re-flect the true underlying distribution of the data theywill model the observations in ways that are irrel-evant to the task at hand.
Relation 2 also containscertain punctuation, such as parentheses which theexamples show are used to delineate a treatmentcode.
Instances of Relation 2 were often markedas Treatment-Addresses-Problem relations by anno-tators.Relation Type 3The third relation captures problems which are re-lated to each other.
The manual annotations containa very similar relation called Problem-Indicates-Problem.
This relation is also similar to Cluster 1from Section 3.1, however under the RDM the wordsare much more specific to the relation.
This relationis difficult to discover accurately because of the in-frequent use of strong trigger words to indicate therelation.
Instead, the model must rely more on thesemantic classes of the arguments, which in this casewill both be types of medical problems.Relation Type 4The fourth relation is detecting instances where amedical test has revealed some problem.
This cor-responds to the Test-Reveals-Problem relation fromthe data.
Many good trigger words for that relationhave high probability under Relation 4.
A compar-ison of the RDM?s Relation 4 with LDA?s cluster 2from Figure 1 shows that many words not relevantto the relation itself are now absent.Argument classesFigure 6 shows the 3 most frequent semantic classes525Concept 1 Concept 2 Concept 3CT pain Percocetscan disease Hgbchest right Hctx-ray left Anionexamination renal VicodinChest patient RDWEKG artery BiliMRI - RBCculture symptoms Cahead mild GapFigure 6: Concept words found by the RDMfor the first argument of a relation (?1).
Most of theother classes were assigned rarely, accounting foronly 19% of the instances collectively.
Human an-notators of the data set chose three argument classes:Problems, Treatments, and Tests.
Concept 1 alignsclosely with a test semantic class.
Concept 2 seemsto be capturing medical problems and their descrip-tions.
Finally, Concept 3 appears to be a combina-tion of treatments (drugs) and tests.
Tokens such as?Hgb?, ?Hct?, ?Anion?, and ?RDW?
occur almostexclusively in entities marked as tests by annotators.It is not clear why this cluster contains both typesof words, but many of the high ranking words be-yond the top ten do correspond to treatments, such as?Morphine?, ?Albumin?, ?Ativan?, and ?Tylenol?.Thus the discovered argument classes show somesimilarity to the ones chosen by annotators.5.3 EvaluationFor a more objective analysis of the relations de-tected, we evaluated the discovered relation typesby comparing them with the manually annotatedones from the data using normalized mutual infor-mation (NMI) (Manning et al, 2008).
NMI is aninformation-theoretic measure of the quality of aclustering which indicates how much informationabout the gold classes is obtained by knowing theclustering.
It is normalized to have a range from 0.0to 1.0.
It is defined as:NMI(?
;C) = I(?;C)[H(?)
+H(C)]/2where ?
is the system-produced clustering, C is thegold clustering, I is the mutual information, and His the entropy.
The mutual information of two clus-terings can be defined as:I(?,C) =?k?j|?k ?
cj |N log2N |?k ?
cj ||?k||cj |where N is the number of items in the clustering.The entropy is defined asH(?)
= ?
?k|?k|N log2|?k|NThe reference clusters consist of all relations an-notated with the same relation type.
The predictedclusters consist of all relations which were assignedthe same relation type.In addition to NMI, we also compute the F mea-sure (Amigo?
et al, 2009).
The F measure is com-puted as:F =?i|Li|n maxj{F (Li, Cj)}whereF (Li, Cj) =2 ?Recall(Li, Cj) ?
Precision(Li, Cj)Recall(Li, Cj) + Precision(Li, Cj)and Precision is defined as:Precision(Ci, Lj) =|Ci ?
Lj||Ci|while Recall is simply precision with the argumentsswapped:Recall(L,C) = Precision(C,L)Table 1 shows the NMI and F measure scores forseveral baselines along with the RDM.
Evaluationwas performed on both DS1 (concept pairs havinga manually annotated relation) and DS2 (all con-secutive concept pairs).
For DS2 we learned themodels using all of the data, and evaluated on thoseentity pairs which had a manual relation annotated.The LDA-based model from Section 3.1 is used asone baseline.
Two other baselines are K-means andComplete-Link hierarchical agglomerative cluster-ing using TF-IDF vectors of the context and argu-ment words (similar to Hasegawa et al (2004)).526Method DS1 DS2NMI F NMI FTrain setComplete-link 4.2 37.8 N/A N/AK-means 8.25 38.0 5.4 38.1LDA baseline 12.8 23.0 15.6 26.2RDM 18.2 39.1 18.1 37.4Test setLDA baseline 10.0 26.1 11.5 26.3RDM 11.8 37.7 14.0 36.4Table 1: NMI and F measure scores for the RDM andbaselines.
The first two columns of numbers show thescores when evaluation is restricted to only those pairsof concepts which had a relation identified by annotators.The last two columns are the NMI and F measure scoreswhen each method clusters all consecutive entity pairs,but is only evaluated on those with a relation identifiedby annotators.Complete-link clustering did not finish on DS2because of the large size of the data set.
This high-lights another advantage of the RDM.
Hierarchicalagglomerative clustering is quadratic in the size ofthe number of instances to be clustered, while theRDM?s time and memory requirements both growlinearly in the number of entity pairs.
The scoresshown in Table 1 use the best parameterization ofeach model as measured by NMI.
For DS1 thebest LDA-based model used 15 clusters.
K-meansachieved the best result with 40 clusters, while thebest Complete-Link clustering was obtained by us-ing 40 clusters.
The best RDM model used parame-ters R = 9 relation, K = 15 general word classes,and A = 15 argument classes.
For DS2 the bestnumber of clusters for LDA was 10, while K-meansperformed best with 58 clusters.
The best RDMmodel used R = 100 relations, K = 50 generalword classes, and A = 15 argument classes.
TheLDA-based approach saw an improvement when us-ing the larger data set, however the RDM still per-formed the best.To assess how well the RDM performs on unseendata we also evaluated the relations extracted by themodel on the test set.
Only the RDM and LDA mod-els were evaluated as clusters produced by K-meansand hierarchical clustering are valid only for the dataused to generate the clusters.
Generative models onthe other hand can provide an estimate of the proba-bility for each relation type on unseen text.
For eachmodel we generate 10 samples after a burn in pe-riod of 30 iterations and form clusters by assigningeach pair of concepts to the relation assigned mostoften in the samples.
The results of this evaluationare presented in Table 1.
While these cluster scoresare lower than those on the data used to train themodels, they still show the RDM outperforming theLDA baseline model.6 DiscussionThe relation and argument clusters determined bythe RDM provide a better unsupervised relation dis-covery method than the baselines.
The RDM doesthis using no knowledge about syntax or semanticsoutside of that used to determine concepts.
Theanalysis shows that words highly indicative of rela-tions are detected and clustered automatically, with-out the need for prior annotation of relations or eventhe choice of a predetermined set of relation types.The discovered relations can be interpreted by a hu-man or labeled automatically using a technique suchas the one presented in Pantel and Ravichandran(2004).
The fact that the discovered relations and ar-gument classes align well with those chosen by an-notators on the same data justify our assumptionsabout relations being present and discoverable bythe way they are expressed in text.
Table 1 showsthat the model does not perform as well when manyof the pairs of entities do not have a relation, but itstill performs better than the baselines.While the RDM relies in large part on triggerwords for making clustering decisions it is also ca-pable of including examples which do not containany contextual words between the arguments.
In ad-dition to modeling trigger words, a joint distributionon argument semantic classes is also incorporated.This allows the model to determine a relation typeeven in the absence of triggers.
For example, con-sider the entity pair ?
[lung cancer] [XRT]?, whereXRT stands for external radiation therapy.
By deter-mining the semantic classes for the arguments (lungcancer is a Problem, and XRT is a test), the set ofpossible relations between the arguments can be nar-rowed down.
For instance, XRT is unlikely to bein a causal relationship with a problem, or to make527a problem worse.
A further aid is the fact that thelearned relationships may be specialized.
For in-stance, there may be a learned relation type suchas ?Cancer treatment addresses cancer problem?.
Inthis case, seeing a type of cancer (lung cancer) and atype of cancer treatment (XRT) would be strong ev-idence for that type of relation, even without triggerwords.7 ConclusionsWe presented a novel unsupervised approach to dis-covering relations in the narrative of electronic med-ical records.
We developed a generative modelwhich can simultaneously cluster relation triggerwords as well as relation arguments.
The modelmakes use of only the tokens found in the con-text of pairs of entities.
Unlike many previous ap-proaches, we assign relations to entities at the lo-cation those entities appear in text, allowing us todiscover context-sensitive relations.
The RDM out-performs baselines built using Latent Dirichlet Allo-cation and traditional clustering methods.
The dis-covered relations can be used for a number of ap-plications such as detecting when certain treatmentswere administered or determining if a necessary testhas been performed.
Future work will include trans-forming the RDM into a non-parametric model byusing the Chinese Restaurant Process (CRP) (Blei etal., 2010).
The CRP can be used to determine thenumber of relations, argument classes, and generalword classes automatically.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:extracting relations from large plain-text collections.In Proceedings of the Fifth ACM Conference on Digi-tal libraries, pages 85?94, San Antonio, Texas, UnitedStates.
ACM.E.
Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo.
2009.
Acomparison of extrinsic clustering evaluation metricsbased on formal constraints.
Information Retrieval,12(4):461?486.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.David M. Blei, Thomas L. Griffiths, and Michael I. Jor-dan.
2010.
The nested chinese restaurant process andbayesian nonparametric inference of topic hierarchies.J.
ACM, 57(2):1?30.T.
L Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences of the United States of America, 101(Suppl1):5228.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, ACL ?04, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
ACM ID: 1219008.C.
D Manning, P. Raghavan, and H. Schu?tze.
2008.
In-troduction to information retrieval, volume 1.
Cam-bridge University Press.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging generic patterns for automatically harvesting se-mantic relations.
In Annual Meeting Association forComputational Linguistics, volume 44, page 113.P.
Pantel and D. Ravichandran.
2004.
Automati-cally labeling semantic classes.
In Proceedings ofHLT/NAACL, volume 4, page 321?328.Benjamin Rosenfeld and Ronen Feldman.
2007.
Clus-tering for unsupervised relation identification.
In Pro-ceedings of the sixteenth ACM conference on Con-ference on information and knowledge management,CIKM ?07, page 411?418, New York, NY, USA.ACM.
ACM ID: 1321499.Z.
Syed and E. Viegas.
2010.
A hybrid approach tounsupervised relation discovery based on linguisticanalysis and semantic typing.
In Proceedings of theNAACL HLT 2010 First International Workshop onFormalisms and Methodology for Learning by Read-ing, page 105?113.Ozlem Uzuner, Brett South, Shuying Shen, and Scott Du-Vall.
2011.
2010 i2b2/VA challenge on concepts, as-sertions, and relations in clinical text.
Accepted forpublication.A.
Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-head, and S. Soderland.
2007.
TextRunner: open in-formation extraction on the web.
In Proceedings ofHuman Language Technologies: The Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, page 25?26.Alexander Yates.
2009.
Unsupervised resolution of ob-jects and relations on the web.
Journal of ArtificialIntelligence Research, 34(1).528
