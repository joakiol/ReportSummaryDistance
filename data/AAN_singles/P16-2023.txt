Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 137?142,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsImproving cross-domain n-gram language modelling with skipgramsLouis OnrustCLS, Radboud University NijmegenESAT-PSI, KU Leuvenl.onrust@let.ru.nlAntal van den BoschCLS, Radboud University Nijmegena.vandenbosch@let.ru.nlHugo Van hammeESAT-PSI, KU Leuvenhugo.vanhamme@esat.kuleuven.beAbstractIn this paper we improve over the hierarch-ical Pitman-Yor processes language modelin a cross-domain setting by adding skip-grams as features.
We find that addingskipgram features reduces the perplexity.This reduction is substantial when modelsare trained on a generic corpus and testedon domain-specific corpora.
We alsofind that within-domain testing and cross-domain testing require different backoffstrategies.
We observe a 30-40% reductionin perplexity in a cross-domain languagemodelling task, and up to 6% reductionin a within-domain experiment, for bothEnglish and Flemish-Dutch.1 IntroductionSince the seminal paper on hierarchical Bayesianlanguage models based on Pitman-Yor processes(Teh, 2006), Bayesian language modelling has re-gained an interest.
Although Bayesian languagemodels are not new (MacKay and Peto, 1995),previously proposed models were reported to beinferior compared to other smoothing methods.Teh?s work was the first to report on improve-ments over interpolated Kneser-Ney smoothing(Teh, 2006).To overcome the traditional problems of over-estimating the probabilities of rare occurrencesand underestimating the probabilities of unseenevents, a range of smoothing algorithms havebeen proposed in the literature (Goodman, 2001).Most methods take a heuristic-frequentist ap-proach combining n-gram probabilities for vari-ous values of n, using back-off schemes or inter-polation.Teh (2006) showed that MacKay and Peto?s(1995) research on parametric Bayesian languagemodels with a Dirichlet prior could be extendedto give better results, but also that one of thebest smoothing methods, interpolated Kneser-Ney(Kneser and Ney, 1995), can be derived as an ap-proximation of the Hierarchical Pitman-Yor pro-cess language model (HPYLM).The success of the Bayesian approach to lan-guage modelling is due to the use of statistical dis-tributions such as the Dirichlet distribution, anddistributions over distributions, such as the Dirich-let process and its two-parameter generalisation,the Pitman-Yor process.
Both are widely stud-ied in the statistics and probability theory com-munities.
Interestingly, language modelling hasacquired the status of a ?fruit fly?
problem in thesecommunities, to benchmark the performance ofstatistical models.
In this paper we approach lan-guage modelling from a computational linguisticspoint of view, and consider the statistical methodsto be the tool with the future goal of improvinglanguage models for extrinsic tasks such as speechrecognition.We derive our model from Teh (2006), and pro-pose an extension with skipgrams.
A frequentistapproach to language modelling with skipgrams isdescribed by Pickhardt et al (2014), who intro-duce an approach using skip-n-grams which areinterpolated using modified Kneser-Ney smooth-ing.
In this paper we show that a Bayesian skip-n-gram approach outperforms a frequentist skip-n-gram model.2 MethodTraditionally, the most widely used pattern in lan-guage modelling is the n-gram, which represents137a pattern of n contiguous words, of which we callthe first (n ?
1) words the history or context, andthe nth word the focus word.
The motivation forusing n-grams can be traced back to the distribu-tional hypothesis of Harris (Harris, 1954; Sahl-gren, 2008).
Although n-grams are small patternswithout any explicit linguistic annotation, they aresurprisingly effective in many tasks, such as lan-guage modelling in machine translation, automaticspeech recognition, and information retrieval.One of the main limitations of n-grams istheir contiguity, because this limits the express-ive power to relations between neighboring words.Many patterns in language span a range that islonger than the typical length of n; we call theserelations long-distance relations.
Other patternsmay be within the range of n, but are still non-contiguous; they skip over positions.
Both typesof relations may be modelled with (syntactic)dependencies, and modelling these explicitly re-quires a method to derive a parser, e.g.
a depend-ency parser, from linguistically annotated data.To be able to model long-distance and othernon-contiguous relations between words withoutresorting to explicitly computing syntactic de-pendencies, we use skipgrams.
Skipgrams area generalisation of n-grams.
They consist of ntokens, but now each token may represent a skipof at least one word, where a skip can match anyword.
Let {m} be a skip of lengthm, then the {1}house can match ?the big house?, or ?the yellowhouse?, etc.
We do not allow skips to be at thebeginning or end of the skipgram, so for n > 2skipgrams are a generalisation of n-grams (Good-man, 2001; Shazeer et al, 2015; Pickhardt et al,2014).Pitman-Yor Processes (PYP) belong to the fam-ily of non-parametric Bayesian models.
Let W bea fixed and finite vocabulary of V words.
For eachwordw ?W letG(w) be the probability ofw, andG = [G(w)]w?Wbe the vector of word probabil-ities.
Since word frequencies generally follow apower-law distribution, we use a Pitman-Yor pro-cess, which is a distribution over partitions withpower-law distributions.
In the context of a lan-guage model this means that for a space P (u),with c(u?)
elements (tokens), we want to parti-tion P (u) in V subsets such that the partition isa good approximation of the underlying data, inwhich c(uw) is the size of subset w of P (u).
Weassume that the training data is an sample of theunderlying data, and for this reason we seek to findan approximation, rather than using the partitionsprecisely as found in the training data.Since we also assume that a power-law distribu-tion on the words in the underlying data, we placea PYP prior on G:G ?
PY(d, ?,G0),with discount parameter 0 ?
d < 1, a strengthparameter ?
> ?d and a mean vector G0=[G0(w)]w?W.
G0(w) is the a-priori probability ofword w, which we set uniformly: G0(w) = 1/Vfor all w ?
W .
In general, there is no known ana-lytic form for the density of PY(d, ?,G0) whenthe vocabulary is finite.
However, we are inter-ested in the distribution over word sequences in-duced by the PYP, which has a tractable form, andis sufficient for the purpose of language modelling.Let G and G0be distributions over W , andx1, x2, .
.
.
be a sequence of words drawn i.i.d.from G. The PYP is then described in terms of agenerative procedure that takes x1, x2, .
.
.
to pro-duce a separate sequence of i.i.d.
draws y1, y2, .
.
.from the mean distributionG0as follows.
The firstword x1is assigned the value of the first draw y1from G0.
Let t be the current number of drawsfrom G0, ckthe number of words assigned thevalue of draw ykand c?=?tk=1ckthe number ofdraws from G0.
For each subsequent word xc?+1,we either assign it the value of a previous draw yk,with probabilityck?d?+c?, or assign it the value of anew draw from G0with probability?+dt?+c?.For an n-gram language model we use a hier-archical extension of the PYP.
The hierarchicalextension describes the probabilities over the cur-rent word given various contexts consisting of upto n ?
1 words.
Given a context u, let Gu(w)be the probability of the current word taking onvalue w. A PYP is used as the prior for Gu=[Gu(w)]w?W:Gu?
PY(d|u|, ?|u|, Gpi(u)),where pi(u) is the suffix of u consisting of allbut the first word, and |u| being the length of u.The priors are recursively placed with parameters?|pi(u)|, d|pi(u)|and mean vector Gpi(pi(u)), until weget to G?:G??
PY(d0, ?0, G0),with G0being the uniformly distributed globalmean vector for the empty context ?.1383 Backoff StrategiesIn this paper we investigate three backoffstrategies: ngram, limited, and full.
ngram is thetraditional n-gram backoff method as described byTeh (2006); limited and full are extensions thatalso incorporate skipgram probabilities.
The fullbackoff strategy is similar to ngram in that it al-ways backs off recursively to the word probabilit-ies, while limited halts as soon as a probability isknown for a pattern.
The backoff strategies can beformalised as follows.
For all strategies, we havethat p(w|u) = G0(w) if u = ?.
For ngram, theother case is defined as:p(w|u) =cuw??
d|u|tuw?
?|u|+ cu?
?+?|u|+ d|u|tu??
?|u|+ cu?
?p(w|pi(u))with cuw?being the number of uw tokens, andcu?
?the number of patterns starting with contextu.
Similarly, tuwkis 1 if draw the kth from Guwas w, 0 otherwise.
tuw?then denotes if there is apattern uw, and tu?
?is the number of types follow-ing context u.Now let ?nbe the operator that adds a skipto a pattern u on the nth position if there is notalready a skip.
Then ?
(u) = [?n(u)]|u|n=2is theset of patterns with one skip more than the num-ber of skips currently in u.
The number of gen-erated patterns is ?
= |?(u)|.
We also introducethe indicator function S, which for the full backoffstrategy always returns its argument: Suw(y) = y.The full backoff strategy is defined as follows, withux= ?x(u), and discount frequency ?u= 1:p(w|u) =??m=1{1?
+ 1[cumw??
?umd|um|tumw?
?um?|um|+ cum?
?+Sumw(?|um|+ d|um|tum??
?um?|um|+ cum??p(w|pi(um)))]}+1?
+ 1[cuw??
?ud|u|tuw?
?u?|u|+ cu?
?+Suw(?|u|+ d|u|tu??
?u?|u|+ cu?
?p(w|pi(u)))]The limited backoff strategy is an extension ofthe full backoff strategy that stops the recursion ifa test pattern uw has already occurred in the train-ing data.
This means that the count is not zero,and hence at training time a probability has beenassigned to that pattern.
S is the indicator functionwhich tells if a pattern has been seen during train-ing: Suw(?)
= 0 if count(uw) > 0, 1 otherwise;and ?u= V ??w?WSuw(?).
Setting Suw(?)
= 0stops the recursion.4 DataIn this section we give an overview of the data setswe use for the English and Flemish-Dutch experi-ments.4.1 English DataFor the experiments on English we use four cor-pora: two large generic mixed-domain corporaand two smaller domain-specific corpora.
We trainon the largest of the two mixed-domain corpora,and test on all four corpora.The first generic corpus is the Google 1 billionwords shuffled web corpus of 769 million tokens(Chelba et al, 2013).
For training we use sets 1through 100, out of the 101 available training sets;for testing we use all available 50 test sets (8Mtokens).
The second generic corpus, used as testdata, is a Wikipedia snapshot (368M tokens) ofNovember 2013 as used and provided by Pickhardtet al (2014).
The first domain-specific corpus isfrom JRC-Acquis v3.0 (Steinberger et al, 2006),which contains legislative text of the EuropeanUnion (8M tokens).
The second domain-specificcorpus consists of documents from the EuropeanMedicines Agency, EMEA (Tiedemann, 2009).We shuffled all sentences, and selected 20% ofthem as the test set (3M tokens).Since the HPYLM uses a substantial amount ofmemory, even with histogram-based sampling, wecannot model the complete 1bw data set withoutthresholding the patterns in the model.
We useda high occurrence threshold of 100 on the uni-grams, yielding 99,553 types that occur above thisthreshold.
We use all n-grams and skipgrams thatoccurred at least twice, consisting of the includedunigrams as focus words, with UNKs occupyingthe positions of words not in the vocabulary.
Notethat because these settings are different from mod-els competing on this benchmark, the results inthis paper cannot be compared to those results.4.2 Flemish-Dutch DataFor the experiments on Flemish-Dutch data, weuse the Mediargus corpus as training material.
It139contains 5 years of newspaper texts from 12 Flem-ish newspapers and magazines, totaling 1.3 billionwords.For testing we use the Flemish part of theSpoken Dutch Corpus (CGN) (Oostdijk, 2000)(3.2M words), divided over 15 components, ran-ging from spontaneous speech to books readaloud.
CGN also contains two components whichare news articles and news, which from a domainperspective are similar to the training data of Me-diargus.
We report on each component separately.Similarly to the 1bw models, we used a thres-hold on the word types, such that we have a sim-ilar size of vocabulary (100k types), which we pro-duced with a threshold of 250.
We used the sameoccurrence threshold of 2 on the n- and skipgrams.5 Experimental SetupWe train 4-gram language model on the two train-ing corpora, the Google 1 billion word benchmarkand the Mediargus corpus.
We do not performany preprocessing on the data except tokenisation.The models are trained with a HPYLM.
We do notuse sentence beginning and end markers.
The res-ults for the ngram backoff strategy are obtainedby training without skipgrams; for limited and fullwe added skipgram features during training.At the core of our experimental framework weuse cpyp,1which is an existing library for non-parametric Bayesian modelling with PY priorswith histogram-based sampling (Blunsom et al,2009).
This library has an example application toshowcase its performance with n-gram based lan-guage modelling.
Limitations of the library, suchas not natively supporting skipgrams, and the lackof other functionality such as thresholding and dis-carding of certain patterns, led us to extend the lib-rary with Colibri Core,2a pattern modelling lib-rary.
Colibri Core resolves the limitations, and to-gether the libraries are a complete language modelthat handles skipgrams: cococpyp.3Each model is run for 50 iterations (withoutan explicit burn-in phase), with hyperparameters?
= 1.0 and ?
= 0.8.
The hyperparameters areresampled every 30 iterations with slice sampling(Walker, 2007).
We test each model on differenttest sets, and we collect their intrinsic perform-ance by means of perplexity.
Words in the test set1https://github.com/redpony/cpyp2http://proycon.github.io/colibri-core/3https://github.com/naiaden/cococpypTest ngram limited ?% full ?%1bw 171 141 6 199 -16jrc 1232 994 19 728 41emea 1749 1304 25 1069 39wp 724 635 12 542 25Table 1: Results of the full and limited back-off systems, trained on 1bw, tested on 1bw (in-domain), and cross-domain sets jrc, emea, and wp.
?% is the relative reduction in perplexity for thecolumn to its left.Comp.
ngram limited ?% full ?%a 1280 1116 13 828 35b 847 785 7 639 24c 1501 1272 15 946 37d 1535 1306 15 975 36f 708 647 9 572 19g 479 445 7 440 8h 1016 916 10 718 29i 1075 990 8 783 27j 469 434 7 442 6k 284 253 11 333 -17l 726 639 12 629 13m 578 538 7 512 11n 895 794 11 664 26o 1017 887 13 833 18Table 2: Results of the full and limited backoffsystems, trained on Mediargus, tested on CGN.Components range from spontaneous (a) to non-spontaneous (o), with components j (news reports)and k (news) being in-domain for the trainingcorpus, and the other components being out-of-domain.
?% is the relative reduction in perplexityfor the column to its left.that were unseen in the training data are ignored incomputing the perplexity on test data.6 ResultsThe results are reported in terms of perplexity, inTable 1 for English, and in Table 2 for Flemish-Dutch.
We computed baseline perplexity scoreswith SRILM (Stolcke, 2002) for 1bw.
We usedan interpolated modified Kneser-Ney languagemodel, with Good-Turing discounting to mimicour thresholding options.
Although the modelsare not comparable, this is arguably the closest ap-proximation in SRILM of our HPYLM.
For 1bwthe baseline is 147; for jrc, emea, and wp, 1391,1430, and 1403 respectively.
In some cases the140baseline is better compared to the ngram back-off strategy.
With adding skipgrams we alwaysoutperform the baseline, especially on the out-of-domain test sets.We find that with large data sets adding skip-grams lowers the perplexity, for both languages, inboth within- and cross-domain experiments.
ForEnglish, we observe absolute perplexity reduc-tions up to 680 (a relative reduction of 39%) ina cross-domain setting, and absolute perplexityreductions of 10 (relative reduction of 6%) in awithin-domain setting.
For Flemish-Dutch we ob-serve similar results with absolute reductions upto 560 (relative reduction of 36%) and 31 (relativereduction 11%), respectively.If we consider the three backoff strategies in-dividually, we can see the following effects onboth English and Flemish-Dutch data.
In a within-domain experiment limited backoff is the beststrategy.
In a cross-domain setting, the full back-off strategy yields the lowest perplexity and largestperplexity reductions.
In the first case, stoppingthe backoff when there is a pattern probabilityfor the word and its context yields a more certainprobability than when the probability is diffusedby more uncertain backoff probabilities.Upon inspection of the model sizes, we observethat the skipgram model contains almost five timesas many parameters as the n-gram model.
Thisdifference is explained by the addition of skip-grams of length 3 and 4, and the bigrams andunigrams derived from these skipgrams.
Each 4-gram can be deconstructed into three skipgramsof length 4, and one of these skipgrams yields askipgram of length 3.
Tests with ngram backoffon skipgram models show that the performance isworse compared to ngram backoff in pure n-grammodels because of the extra bigrams and unigrams(ngram ignores the skipgrams).
Yet, the exper-imental results also indicate that with sufficientdata, skipgram models outperform n-gram mod-els.
Because the difference in parameters is onlynoticeable in terms of memory, and it hardly im-pacts the run-time, this makes the skipgram modelthe favourable model.7 ConclusionsIn this paper we showed that by adding skipgrams,a straightforward but powerful generalisation of n-gram word patterns, we can reduce the perplex-ity of a Bayesian language model, especially in across-domain language modelling task.
By chan-ging the backoff strategy we can also improve ona within-domain task.
We found this effect in twolanguages.ReferencesP Blunsom, T Cohn, S Goldwater, and M Johnson.2009.
A note on the implementation of hierarch-ical Dirichlet processes.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337?340.
Association for Computational Linguistics.C Chelba, T Mikolov, M Schuster, Q Ge, T Brants,P Koehn, and T Robinson.
2013.
One billion wordbenchmark for measuring progress in statistical lan-guage modeling.
Technical Report Google Tech Re-port 41880.JT Goodman.
2001.
A bit of progress in lan-guage modeling.
Computer Speech & Language,15(4):403?434.ZS Harris.
1954.
Distributional structure.
Word.R Kneser and H Ney.
1995.
Improved backing-offfor m-gram language modeling.
In In Proceedingsof the IEEE International Conference on Acoustics,Speech and Signal Processing, volume I, pages 181?184, May.DJC MacKay and LCB Peto.
1995.
A hierarchicalDirichlet language model.
Natural language engin-eering, 1(3):289?308.N Oostdijk.
2000.
The spoken Dutch corpus.
Over-view and first evaluation.
In LREC.R Pickhardt, T Gottron, M K?orner, PG Wagner,T Speicher, and S Staab.
2014.
A generalized lan-guage model as the combination of skipped n-gramsand modified Kneser-Ney smoothing.
arXiv pre-print arXiv:1404.3377.M Sahlgren.
2008.
The distributional hypothesis.Italian Journal of Linguistics, 20(1):33?54.N Shazeer, J Pelemans, and C Chelba.
2015.
Sparsenon-negative matrix language modeling for skip-grams.
In Proceedings of Interspeech, pages 1428?1432.R Steinberger, B Pouliquen, A Widiger, C Ignat, T Er-javec, D Tufis, and D Varga.
2006.
The JRC-Acquis: A multilingual aligned parallel corpus with20+ languages.
Proceedings of the 5th InternationalConference on Language Resources and Evaluation.A Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing,pages 257?286, November.141YW Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 985?992.
Association for Computational Lin-guistics.J Tiedemann.
2009.
News from OPUS ?
A collectionof multilingual parallel corpora with tools and in-terfaces.
In Recent Advances in Natural LanguageProcessing, volume 5, pages 237?248.SG Walker.
2007.
Sampling the Dirichlet mixturemodel with slices.
Communications in Statistics ?Simulation and Computation, 36(1):45?54.142
