R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
957 ?
968, 2005.?
Springer-Verlag Berlin Heidelberg 2005An Empirical Study on Language Model AdaptationUsing a Metric of Domain SimilarityWei Yuan1, Jianfeng Gao2, and Hisami Suzuki31Shanghai Jiao Tong University, 1954 Huashan Road, Shanghai 200030sunnyuanovo@sjtu.edu.cn2Microsoft Research, Asia, 49 Zhichun Road, Haidian District, Beijing 100080jfgao@microsoft.com3Microsoft Research, One Microsoft Way, Redmond WA 98052hisamis@microsoft.comAbstract.
This paper presents an empirical study on four techniques of lan-guage model adaptation, including a maximum a posteriori (MAP) method andthree discriminative training models, in the application of Japanese Kana-Kanjiconversion.
We compare the performance of these methods from various anglesby adapting the baseline model to four adaptation domains.
In particular, we at-tempt to interpret the results given in terms of the character error rate (CER) bycorrelating them with the characteristics of the adaptation domain measured us-ing the information-theoretic notion of cross entropy.
We show that such a met-ric correlates well with the CER performance of the adaptation methods, andalso show that the discriminative methods are not only superior to a MAP-basedmethod in terms of achieving larger CER reduction, but are also more robustagainst the similarity of background and adaptation domains.1   IntroductionLanguage model (LM) adaptation attempts to adjust the parameters of a LM so that itperforms well on a particular domain of data.
This paper presents an empirical studyof several LM adaptation methods on the task of Japanese text input.
In particular, wefocus on the so-called cross-domain LM adaptation paradigm, i.e.
to adapt a LMtrained on one domain to a different domain, for which only a small amount of train-ing data is available.The LM adaptation methods investigated in this paper can be grouped into twocategories: maximum a posterior (MAP) and discriminative training.
Linear interpola-tion is representative of the MAP methods [1].
The other three methods, including theboosting [2] and perceptron [3] algorithms and minimum sample risk (MSR) method[4], are discriminative methods, each of which uses a different training algorithm.We carried out experiments over many training data sizes on four distinct adapta-tion corpora, the characteristics of which were measured using the information-theoretic notion of cross entropy.
We found that discriminative training methods1This research was conducted while the author was visiting Microsoft Research Asia.958 W. Yuan, J. Gao, and H. Suzukioutperformed the LI method in all cases, and were more robust across different train-ing sets of different domains and sizes.
However, none of the discriminative trainingmethods was found to outperform the others in our experiments.The paper is organized as follow.
Section 2 introduces the task of IME and the roleof LM.
In Section 3, we review related work.
After a description of the LM adaptationmethods in our experiments in Section 4, Sections 5 and 6 present experimental re-sults and their discussions.
We conclude our paper in Section 7.2   Language Model and the Task of IMEOur study falls into the context of Asian language (Japanese in this study) text input.The standard method for doing this is that the users first input the phonetic strings,which are then converted into the appropriate word string by software.
The task ofautomatic conversion is called IME in this paper, which stands for Input Method Edi-tor, based on the name of the commonly used Windows-based application.The performance of IME is typically measured in terms of the character error rate(CER), which is the number of characters wrongly converted from the phonetic stringdivided by the number of characters in the correct transcript.
Current Japanese IMEsystems exhibit about 5-15% CER in conversion of real-world data in a wide varietyof domains.
In the following, we argue that the IME is a similar problem to speechrecognition but is a better choice for evaluating language modeling techniques.Similar to speech recognition, IME can also be viewed as a Bayesian decisionproblem.
Let A be the input phonetic string (which corresponds to the acoustic signalin speech).
The task of IME is to choose the most likely word string W* among thosecandidates that could have been converted from A:)|()(maxarg)(),(maxarg)|(maxarg*)()()(WAPWPAPAWPAWPWAGENWAGENWAGENW ??
?===(1)where GEN(A) denotes the candidate set given A.Unlike speech recognition, there is almost no acoustic ambiguity in IME, becausethe phonetic string is provided directly by users.
Moreover, we can assume a many-to-one mapping from W to A in IME, i.e.
P(A|W) = 1.
So the decision of Equation (1)depends solely upon P(W), making IME a more direct evaluation test-bed for LMthan speech recognition.
Another advantage is that it is relatively easy to convert W toA, making it possible to obtain a large amount of training data for discriminativelearning, as described later.3   Related WorkOur goal is to quantify the characteristics of different domains of text, and to correlatethem with the performance of various techniques for LM adaptation to compare theireffectiveness and robustness.
This relates our work to the study of domain similaritycalculation and to different techniques for LM adaptation.An Empirical Study on Language Model Adaptation 9593.1 Measuring Domain SimilarityStatistical language modeling (SLM) assumes that language is generated from under-lying distributions.
When we discuss different domains of text, we assume that thetext from each of these domains is generated from a different underlying distribution.We therefore consider the problem of distributional similarity in this paper.Cross entropy is a widely used measure in evaluating LM.
Given a language Lwith its true underlying probability distribution p and another distribution q (e.g.
aSLM) which attempts to model L, the cross entropy of L with respect to q is????=nwwnnnwwqwwpnqLH...111)...(log)...
(1lim),(  (2)where w1?wn is a word string in L. However, in reality, the underlying p is neverknown and the corpus size is never infinite.
We therefore make the assumption that Lis an ergodic and stationary process [5], and approximate the cross entropy by calcu-lating it for a sufficiently large n instead of calculating it for the limit.)...
(log1),( 1 nwwqnqLH ??
(3)This measures how well a model approximates the language L.The KL divergence, or relative entropy, is another measure of distributional simi-larity that has been widely used in NLP and IR [6].
Given the two distributions p andq above, the KL divergence is defined as?=nww nnnnnwwqwwpwwpwwqwwpD...1 11111 )...()...(log)...())...(||)...((  (4)The cross entropy and the KL divergence are related notions.
Given the notationsof L, p and q above, [5] shows that)||()(),( qpDLHqLH +=(5)In other words, the cross entropy takes into account both the similarity between twodistributions (given by KL divergence) and the entropy of the corpus in question, bothof which contribute to the complexity of a LM task.
In this paper we are interested inmeasuring the complexity of the LM adaptation task.
We therefore define the similar-ity between two domains using the cross entropy.
We will also use the metric thatapproximates the entropy of the corpus to capture the in-domain diversity of a corpusin Section 5.2.23.2 LM Adaptation MethodsIn this paper, two major approaches to cross-domain adaptation have been investi-gated: maximum a posteriori (MAP) estimation and discriminative training methods.2There are other well-known metrics of similarity within NLP literature, such as the mutualinformation or cosine similarity [7], which we do not discuss in this paper.960 W. Yuan, J. Gao, and H. SuzukiIn MAP estimation methods, adaptation data is used to adjust the parameters of thebackground model so as to maximize the likelihood of the adaptation data [1].
Dis-criminative training methods to LM adaptation, on the other hand, aim at using theadaptation data to directly minimize the errors on the adaptation data made by thebackground model.
These techniques have been applied successfully to the task oflanguage modeling in non-adaptation [8] as well as adaptation scenarios [9] for speechrecognition.
But most of them focused on the investigation of performance of certainmethods for LM adaptation, without analyzing in detail the underlying reasons ofdifferent performance achieved by different methods.
In this paper we attempt toinvestigate the effectiveness of different discriminative methods in an IME adaptationscenario, with a particular emphasis on correlating their performance with the charac-teristics of adaptation domain.4   LM Adaptation MethodsWe implement four methods in our experiments.
The Linear Interpolation (LI) fallsinto the framework of MAP while the boosting, the perceptron and the MSR methodsfall into that of discriminative training.4.1   Linear Interpolation (MAP)In MAP estimation methods, adaptation data is used to adjust the parameters of thebackground model so as to maximize the likelihood of the adaptation data.The linear interpolation is a special case of MAP according to [10].
At first, wegenerate trigram models on background data and adaptation data respectively.
Thetwo models are then combined into one as:)|()1()|()|( hwPhwPhwP iAiBi ??
?+=  (6)where PB is the probability of the background model, PA is the probability of the adap-tation model and the history h corresponds to two preceding words.
For simplicity, wechose a single ?
for all histories and tuned it on held-out data.4.2   Discriminative TrainingDiscriminative training follows the general framework of linear models [2][3].
We usethe following notation in the rest of the paper.?
Training data is a set of example input/output pairs {Ai, WiR} for i = 1?M, whereeach Ai is an input phonetic string and each WiR is the reference transcript of Ai.?
We assume a set of D+1 features, fd(W), for d=0?D, where each feature is a func-tion that maps W to a real value.
Using vector notation, we have f(W)={ f0(W),f1(W)?fD(W)} and f(W) .1+?
DR  Without loss of generality, f0(W) is called the basemodel feature, and is defined in this paper as the log probability that the back-ground trigram model assigns to W. fd(W), for d=1?D, are defined as the counts ofthe word n-gram in W, where n = 1 and 2 in our case.An Empirical Study on Language Model Adaptation 961?
Finally, the parameters of the model form a vector of D + 1 dimensions, each forone feature function, ?= {?0, ?1?
?D}.
The likelihood score of a word string W is?===Dddd WfWWScore0)()(),( ??f?
(7)Then the decision rule of Equation (1) can be re-written as),(maxarg),((A)* ?
?GENWScoreAWW?=(8)Assume that we can measure the number of conversion errors in W by comparing itwith a reference transcript WR using an error function Er(WR, W), which is an editdistance in our case.
We call the sum of conversion errors over the training data assample risk (SR).
Discriminative training methods strive to minimize the SR by opti-mizing the model parameters, as defined in Equation (9).
?===MiiiRi* AWWErSR...1)),(,(minarg)(minarg ?????
(9)However, SR(.)
cannot be optimized easily since Er(.)
is a piecewise constant (orstep) function of ?
and its gradient is undefined.
Therefore, discriminative methodsapply different approaches that optimize it approximately.
As we shall describe be-low, the boosting and perceptron algorithms approximate SR(.)
by loss functions thatare suitable for optimization, while MSR uses a simple heuristic training procedure tominimize SR(.)
directly without applying any approximated loss function.
We nowdescribe each of the discriminative methods in turn.The boosting algorithm [2] uses an exponential function to approximate SR(.).
Wedefine a ranking error in a case where an incorrect candidate conversion W gets ahigher score than the correct conversion WR.
The margin of the pair (WR, W) withrespect to the model ?
is estimated as),(),(),( ??
WScoreWScoreWWM RR ?=  (10)Then we define an upper bound to the number of ranking errors as the loss function,?
?= ?
?=Mi AWiRiiiWWM...1 )()),(exp()ExpLoss(GEN?
(11)Now, ExpLoss(.)
is convex with respect to ?, so there are no problems with localminima when optimizing it.
The boosting algorithm can be viewed as an iterativefeature selection method: at each iteration, the algorithm selects from all possiblefeatures the one that is estimated to have the largest impact on reducing the ExpLossfunction with respect to the current model, and then optimizes the current model byadjusting only the parameter of the selected feature while keeping the parameters ofother features fixed.The perceptron algorithm [3] can be viewed as a form of incremental training pro-cedure that optimizes a minimum square error (MSE) loss function, which is an ap-proximation of SR(.).
As shown in Figure 1, it starts with an initial parameter settingand adapts it each time a training sample is wrongly converted.962 W. Yuan, J. Gao, and H. Suzuki1 Initialize all parameters in the model, i.e.
?0 = 1 and ?d = 0 for d=1?D2 For t = 1?T, where T is the total number of iterationsFor each training sample (Ai, WiR), i = 1?MUse current model ?
to choose some Wi from GEN(Ai) by Equation (8)For d = 1 ?
D?d  = ?d + ?
(fd (WiR)- fd (Wi)), where ?
is the size of the learning stepFig.
1.
The standard perceptron algorithm with delta ruleIn our experiments, we used the average perceptron algorithm in [3], a simple refine-ment to the algorithm in Figure 1, which has been proved to be more robust.
Let ?dt,ibe the value for the dth parameter after the ith training sample has been processed inpass t over the training data.
Then the ?average parameters?
are defined as in Equa-tion (12).
)/()()(1 1, MTTtMiitdavgd ?= ?
?= =??
(12)The minimum sample risk (MSR) method [4] can be viewed as a greedy stage-wise learning algorithm that minimizes the sample risk SR(.)
directly as it appears inEquation (9).
Similar to the boosting method, it is an iterative procedure.
In eachiteration, MSR selects a feature that is estimated to be most effective in terms of re-ducing SR(.
), and then optimizes the current model by adjusting the parameters of theselected feature.
MSR, however, differs from the boosting method in that MSR triesto optimize the sample risk directly while the boosting optimizes the loss function thatis an upper bound of the sample risk.As mentioned earlier, SR(.)
can be optimized using regular gradient-based optimi-zation algorithms.
MSR therefore uses a particular implementation of line search,originally proposed in [11], to optimize the current model by adjusting the parameterof a selected feature while keeping other parameters fixed.Assuming fd is the selected feature, its parameter ?d is optimized by line search asfollows.
Recall that Er(WR,W) is the function that measures the number of conversionerrors in W versus its reference transcript WR.
The value of SR(.)
is the sum of Er(.
)over all training samples.
For each A in training set, let GEN(A) be the set of n-bestcandidate word strings that could be converted from A.
By adjusting ?d, we obtain foreach training sample an ordered sequence of ?d intervals.
For ?d in each interval, aparticular candidate would be selected according to Equation (8).
Then the corre-sponding Er(.)
is associated with the interval.
As a result, for each training sample, weobtain a sequence of ?d intervals and their corresponding Er(.)
values.
By combiningthe sequences over all training samples, we obtain a global sequence of ?d intervals,each of which is associated with a SR(.)
value.
Therefore we can find the optimalinterval of ?d as well as its corresponding sample risk by traversing the sequence andtaking the center of the interval as the optimal value of ?d.An Empirical Study on Language Model Adaptation 9635   Experimental Results5.1   DataThe data used in our experiments stem from five distinct sources of text.
A 36-million-word Nikkei newspaper corpus was used as the background domain.
We usedfour adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpuscontaining newspaper and other sources of text), Encarta (encyclopedia) and Shincho(collection of novels).For the computation of domain characteristics (Section 5.2), we extracted 1 millionwords from the training data of each domain respectively (corresponding to 13K to78K sentences depending on the domain).
For this experiment, we also used a lexiconconsisting of the words in our baseline lexicon (167,107 words) plus all words in thecorpora used for this experiment (that is, 1M words times 5 domains), which included216,565 entries.
The use of such a lexicon was motivated by the need to eliminate theeffect of out-of-vocabulary (OOV) items.For the experiment of LM adaptation (Section 5.3), we created training data con-sisting of 72K sentences (0.9M~1.7M words) and test data of 5K sentences(65K~120K words) from each adaptation domain.
The first 800 and 8,000 sentencesof each adaptation training data were also used to show how different sizes of adapta-tion training data affected the performances of various adaptation methods.
Another5K-sentence subset was used as held-out data for each domain.
For domain adaptationexperiments, we used our baseline lexicon consisting of 167,107 entries.5.2   Computation of Domain CharacteristicsThe first domain characteristic we computed was the similarity between two domainsfor the task of LM.
As discussed in Section 3, we used the cross entropy as the metric:we first trained a word trigram model using the system described in [12] on the 1-million-word corpus of domain B, and used it in the computations of the cross entropyH(LA, qB) following equation (3).
For simplicity, we denote H(LA, qB) as H(A,B).Table 1 displays the cross entropy between two domains of text.
Note that the crossentropy is not symmetric, i.e., H(A,B) is not necessarily the same as H(B,A).
In orderto have a representative metric of similarity between two domains, we computed theaverage cross entropy between two domains, shown in Table 2, and used this quantityas the metric for domain similarity.Along the main diagonal in the tables below, we also have the cross entropy com-puted for H(A,A), i.e., when two domains we compare are the same (in boldface).
Thisvalue, which we call self entropy for convenience, is an approximation of the entropyof the corpus A, and measures the amount of information per word, i.e., the diversityof the corpus.
Note that the self entropy increases in the order of Nikkei ?
Yomiuri?
Encarta ?
TuneUp ?
Shincho.
This indeed reflects the in-domain variability oftext: Nikkei, Yomiuri and Encarta are highly edited text, following style guidelines;they also tend to have repetitious content.
In contrast, Shincho is a collection of nov-els, on which no style or content restriction is imposed.
We expect that the LM task to964 W. Yuan, J. Gao, and H. Suzukibe more difficult as the corpus is more diverse; we will further discuss the effect ofdiversity in Section 6.3Table 1.
Cross entropy (rows: corpora; column: models)Nikkei Yomiuri TuneUp Encarta ShinchoNikkei 3.94 7.46 7.65 9.81 10.10Yomiuri 7.93 4.09 7.62 9.26 9.97TuneUp 8.25 8.03 4.41 9.04 9.06Encarta 8.79 8.66 8.60 4.40 9.30Shincho 8.70 8.61 8.07 9.10 4.61Table 2.
Average cross entropyNikkei Yomiuri TuneUp Encarta ShinchoNikkei 3.94 7.69 7.95 9.30 9.40Yomiuri  4.09 7.82 8.96 9.29TuneUp   4.41 8.82 8.56Encarta    4.40 9.20Shincho     4.615.3   Results of LM AdaptationWe trained our baseline trigram model on our background (Nikkei) corpus using thesystem described in [12].
The CER (%) of this model on each adaptation domain is inthe second column of Table 3.
For the LI adaptation method (the third column ofTable 3), we trained a word trigram model on the adaptation data, and linearly com-bined it with the background model, as described in Equation (6).For the discriminative methods (the last three columns in Table 3), we produced  acandidate word lattice for each input phonetic string in the adaptation training setusing the background trigram model mentioned above.
For efficiency purposes, wekept only the best 20 hypotheses from the lattice as the candidate conversion set fordiscriminative training.
The lowest CER hypothesis in the lattice, rather than the ref-erence transcript, was used as the gold standard.To compare the performances of different discriminative methods, we fixed thefollowing parameter settings: we set the number of iterations N to be 2,000 for theboosting and MSR methods (i.e., at most 2,000 features in the final models); for theperceptron algorithm, we set T = 40 (in Figure 1).
These settings might lead to an3Another derivative notion from Table 1 is the notion of balanced corpus.
In Table 1, thesmallest cross entropy for each text domain (rows) is the self entropy (in boldface), as ex-pected.
Note, however, that the second smallest cross entropy (underlined) is always obtainedfrom the TuneUp model (except for Nikkei, for which Yomiuri provides the second smallestcross entropy).
This reflects the fact that the TuneUp corpus was created by collecting sen-tences from various sources of text, in order to create a representative test corpus.
Using thenotion of cross entropy, such a characteristic of a test corpus can also be quantified.An Empirical Study on Language Model Adaptation 965unfair comparison, as the perceptron algorithm will select far more features than theboosting and MSR algorithm.
However, we used these settings as they all convergedunder these settings.
All other parameters were tuned empirically.In evaluating both MAP and discriminative methods, we used an N-best rescoringapproach.
That is, we created N best hypotheses using the background trigram model(N=100 in our experiments) for each sentence in test data, and used domain-adaptedmodels to rescore the lattice.
The oracle CERs (i.e., the minimal possible CER giventhe hypotheses in the lattice) ranged from 1.45% to 5.09% depending on the adapta-tion domain.
Table 3 below summarizes the results of various adaptation methods interms of CER (%) and CER reduction (in parentheses) over the baseline model.
In thefirst column, the numbers in parentheses next to the domain name indicates the num-ber of training sentences used for adaptation.Table 3.
CER (%) and CER reduction over BaselineDomain Baseline LI Boosting Perceptron MSRYomiuri (800) 3.70 3.70 (0.00%) 3.13 (15.41%) 3.18 (14.05%) 3.17 (14.32%)Yomiuri (8K) 3.70 3.69 (0.27%) 2.88 (22.16%) 2.85 (22.97%) 2.88 (22.16%)Yomiuri (72K) 3.70 3.69 (0.27%) 2.78 (24.86%) 2.78 (24.86%) 2.73 (26.22%)TuneUp (800) 5.81 5.81 (0.00%) 5.69 (2.07%) 5.69 (2.07%) 5.70 (1.89%)TuneUp (8K) 5.81 5.70 (1.89%) 5.47 (5.85%) 5.47 (5.85%) 5.47 (5.85%)TuneUp (72K) 5.81 5.47 (5.85%) 5.33 (8.26%) 5.20 (10.50%) 5.15 (11.36%)Encarta (800) 10.24 9.60 (6.25%) 9.82 (4.10%) 9.43 (7.91%) 9.44 (7.81%)Encarta (8K) 10.24 8.64 (15.63%) 8.54 (16.60%) 8.34 (18.55%) 8.42 (17.77%)Encarta (72K) 10.24 7.98 (22.07%) 7.53 (26.46%) 7.44 (27.34%) 7.40 (27.73%)Shincho (800) 12.18 11.86 (2.63%) 11.91 (2.22%) 11.90 (2.30%) 11.89 (2.38%)Shincho (8K) 12.18 11.15 (8.46%) 11.09 (8.95%) 11.20 (8.05%) 11.04 (9.36%)Shincho (72K) 12.18 10.76 (11.66%) 10.25 (15.85%) 10.18 (16.42%) 10.16 (16.58%)6   Discussion6.1   Domain Similarity and CERThe first row of Table 2 shows that the average cross entropy with respect to thebackground domain (Nikkei) increases in the following order: Yomiuri ?
TuneUp ?Encarta ?
Shincho.
This indicates that among the adaptation domains, Yomiuri is themost similar to Nikkei, closely followed by TuneUp; Shincho and Encarta are theleast similar to Nikkei.
This is consistent with our intuition, since Nikkei and Yomiuriare both newspaper corpora, and TuneUp, which is a manually constructed corpusfrom various representative domains of text, contains newspaper articles.966 W. Yuan, J. Gao, and H. SuzukiThis metric of similarity correlates perfectly with the CER.
In Table 3, we see thatfor all sizes of training data for all adaptation methods, the following order of CERperformance is observed, from better to worse: Yomiuri ?
TuneUp ?
Encarta ?Shincho.
In other words, the more similar the adaptation domain is to the backgrounddomain, the better the CER results are.6.2   Domain Similarity and the Effectiveness of Adaptation MethodsThe effectiveness of a LM adaptation method is measured by the relative CER reduc-tion over the baseline model.
Figure 3 shows the CER reduction of various methodsfor each domain when the training data size was 8K.40.00%5.00%10.00%15.00%20.00%25.00%Yomiuri TuneUp Encarta ShinchoLIBoostingPerceptronMSRFig.
2.
CER reduction by different adaptation methodsIn Figure 2 the X-axis is arranged in the order of domain similarity with the back-ground domain, i.e., Yomiuri ?
TuneUp ?
Encarta ?
Shincho.
The first thing wenote is that the discriminative methods outperform LI in all cases: in fact, for all rowsin Table 3, MSR outperforms LI in a statistically significant manner (p < 0.01 using t-test);5 the differences among the three discriminative methods, on the other hand, arenot statistically significant in most cases.We also note that the performance of LI is greatly influenced by domain similarity.More specifically, when the adaptation domain is similar to the background domain(i.e., for Yomiuri and TuneUp corpora), the contribution of the LI model is extremelylimited.
This can be explained as follows: if the adaptation data is too similar to thebackground, the difference between the two underlying distributions is so slight thatadding adaptation data leads to no or very small improvements.Such a limitation is not observed with the discriminative methods.
For example, alldiscriminative methods are quite effective on Yomiuri, achieving more than 20%CER reduction.
We therefore conclude that discriminative methods, unlike LI, arerobust against the similarity between background and adaptations domains.4Essentially the same trend is observed with other training data sizes.5The only exception to this was Shincho (800).An Empirical Study on Language Model Adaptation 9676.3   Adaptation Data Size and CER ReductionWe have seen in Table 3 that in all cases, discriminative methods outperform LI.Among the discriminative methods, an interesting characteristic regarding the CERreduction and the data size is observed.
Figure 3 displays the self entropy of fouradaptation corpora along the X-axis, and the improvement in CER reduction when72K-sentence adaptation data is used over when 800 sentences are used along the Y-axis.
In other words, for each adaptation method, each point in the figure correspondsto the CER reduction ratio on a domain (corresponding to Yomiuri, Encarta, TuneUp,Shincho from left to right) when 90 times more adaptation data was available.012345674 4.2 4.4 4.6 4.8self entropyCERreductionratio(%)BoostingPerceptronMSRFig.
3.
Improvement in CER reduction for discriminative methods by increasing the adaptationdata size from 800 to 72K sentencesFrom this figure, we can see that there is a positive correlation between the diversityof the adaptation corpus and the benefit of having more training data available.
Thishas an intuitive explanation: the less diverse the adaptation data is, the less distincttraining examples it will include for discriminative training.
This result is useful inguiding the process of adaptation data collection.7   Conclusion and Future WorkIn this paper, we have examined the performance of various LM adaptation methodsin terms of domain similarity and diversity.
We have found that (1) the notion ofcross-domain similarity, measured by the cross entropy, correlates with the CER of allmodels (Section 6.1), and (2) the notion of in-domain diversity, measured by the selfentropy, correlates with the utility of more adaptation training data for discriminativetraining methods (Section 6.3).
In comparing discriminative methods with a MAP-based method, we have also found that (1) the former uniformly achieve better CERperformance than the latter, and (2) are more robust against the similarity of back-ground and adaptation data (Section 6.2).968 W. Yuan, J. Gao, and H. SuzukiThough we believe these results are useful in designing the future experiments indomain adaptation, some results and correlations indicated in the paper are still incon-clusive.
We hope to run additional experiments to confirm these findings.
We also didnot fully investigate into characterizing the differences among the three discriminativemethods; such an investigation is also left for future research.References1.
Bellagarda, J.
An Overview of Statistical Language Model Adaptation.
In ITRW on Adap-tation Methods for Speech Recognition (2001): 165-174.2.
Collins, M. Ranking Algorithms for Name-Entity Extraction: Boosting and the Voted Per-ceptron.
ACL (2002).3.
Collins, M. Discriminative Training Methods for Hidden Markov Models: Theory and Ex-periments with Perceptron Algorithms.
EMNLP (2002).4.
Gao.
J., H. Yu, P. Xu, and W. Yuan.
Minimum Sample Risk Methods for Language Mod-eling.
To Appear (2005).5.
Manning, C.D., and H. Sch?tze.
Foundations of Statistical Natural Language Processing.The MIT Press (1999).6.
Dagan, I., L. Lee, and F. Pereira.
Similarity-based models of cooccurrence probabilities.Machine Learning, 34(1-3): 43-69 (1999).7.
Lee, L. Measures of distributional similarity.
ACL (1999): 25-32.8.
Roark, B, M. Saraclar and M. Collins.
Corrective Language Modeling for Large Vocabu-lary ASR with the Perceptron Algorithm.
ICASSP (2004): 749-752.9.
Bacchiani, M., B. Roark and M. Saraclar.
Language Model Adaptation with MAP Estima-tion and the Perceptron Algorithm.
HLT-NAACL (2004): 21-24.10.
Bacchiani, M. and B. Roark.
Unsupervised language model adaptation.
ICASSP (2003):224-22711.
Och, F.J.
Minimum error rate training in statistical machine translation.
ACL (2003): 160-167.12.
Gao, J., J. Goodman, M. Li, and K.F.
Lee.
Toward a unified approach to statistical lan-guage modeling for Chinese.
ACM Transactions on Asian Language Information Process-ing l-1 (2002): 3-33.
