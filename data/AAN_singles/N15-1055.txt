Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 526?536,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsRobust Morphological Tagging with Word RepresentationsThomas M?uller and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanymuellets@cis.lmu.deAbstractWe present a comparative investigation ofword representations for part-of-speech (POS)and morphological tagging, focusing on sce-narios with considerable differences betweentraining and test data where a robust approachis necessary.
Instead of adapting the modeltowards a specific domain we aim to build arobust model across domains.
We developeda test suite for robust tagging consisting of sixlanguages and different domains.
We find thatrepresentations similar to Brown clusters per-form best for POS tagging and that word rep-resentations based on linguistic morphologicalanalyzers perform best for morphological tag-ging.1 IntroductionMost natural language processing (NLP) tasks canbe better solved if a preprocessor tags each word inthe natural language input with a label like ?noun,singular?
or ?verb, past tense?
that gives some in-dication of the syntactic role that the word plays inits context.
The most common form of such pre-processing is POS tagging.
However, for morpho-logically rich languages, a large subset of the lan-guages of the world, POS tagging in its original form?
where labels are syntactic categories with littleor no morphological information ?
does not makemuch sense.
The reason is that POS and morpho-logical properties are mutually dependent, so solv-ing only one task or solving the tasks sequentially isinadequate.
The most important dependence of thistype is that POS can be read off morphology in manycases; e.g., the morphological suffix ?-iste?
is a reli-able indicator of the informal second person singu-lar preterite indicative form of a verb in Spanish.
Inwhat follows, we use the term ?morphological tag-ging?
to refer to ?morphological and POS tagging?since morphological tags generally include POS in-formation.The importance of morphological tagging as partof the computational linguistics processing pipelinemotivated us to conduct the research reported in thispaper.
The specific setting that we address is in-creasingly recognized as the setting in which mostpractical NLP takes place: We look at scenarios withconsiderable differences between the training dataand the application data, i.e., between the data thatthe tagger is trained on and the data that it is ap-plied to.
This type of scenario is frequent because ofthe great diversity and variability of natural languageand because of the high cost of annotation ?
whichmakes it impossible to create large training sets foreach new domain.
For this reason, we address mor-phological tagging in a setting in which training andapplication data differ.The most common approach to this setting isdomain adaptation.
Domain adaptation has beendemonstrated to have good performance in scenarioswith differently distributed training/test data.
How-ever, it has two disadvantages.
First, it requires theavailability of data from the target domain.
Second,we need to do some extra work in domain adaptation?
consisting of taking target domain data and using itto adapt our NLP system to the target domain ?
andwe end up with a number of different versions of ourNLP system.
The extra work required and the pro-526liferation of different versions increase the possibil-ity of errors and increase the complexity of deploy-ing NLP technology.
Similar to other recent work(Zhang and Wang, 2009), we therefore take an ap-proach that is different from domain adaptation.
Webuild a system that is robust across domains withoutany modification.
As a result, no extra work is re-quired when the system is applied to a new domain:there is only one system and we can use it for alldomains.The key to making NLP components robust acrossdomains is the use of powerful domain-independentrepresentations for words.
One of the main contri-butions of this paper is that we compare the perfor-mance of the most important representations that canbe used for this purpose.
We find that two of theseare best suited for robust tagging.
MarLiN (Mar-tin et al, 1998) clusters ?
a derivative of Brownclusters ?
perform best for POS tagging.
MarLiNclusters are also an order of magnitude more effi-cient to induce than the original Brown clusters.
Weprovide an open source implementation of MarLiNclustering as part of this publication (Section 8).
Wecompare the word representations to MorphologicalAnalyzers (MAs), which are finite-state transducersthat find the stems of a form and use them to de-rive all its possible morphological readings.
MAsproduce the best results in our experiments on mor-phological tagging.
Our initial expectation was thatdomain differences and lack of coverage would putmanually created MAs at a disadvantage when com-pared to learning algorithms that are run on verylarge text corpora.
However, our results clearly showthat MA-based representations are the best represen-tations to use for robust morphological tagging.The motivation for our work is that both mor-phological tagging and the ?robust?
application set-ting are important areas of research in NLP.
To sup-port this research, we created an extensive evalua-tion set for six languages.
This involved identify-ing morphologically rich languages in which usabledata sets with different distributional properties wereavailable, designing mappings between different tagsets, organizing a manual annotation effort for one ofthe six languages and preparing large ?general?
(notdomain-specific) data sets for unsupervised learningof word representations.
The preparation and publi-cation (Section 8) of this test suite is in itself a sig-nificant contribution.The remainder of this paper is structured as fol-lows.
Section 2 discusses related work.
Section 3presents the representations we tested.
Section 4 de-scribes the data sets and the annotation and conver-sion efforts required to create the in-domain (ID) andout-of-domain (OOD) data sets.
In Section 5, we de-scribe the experiments and discuss our findings.
InSection 6, we provide an analysis of our results.
Sec-tion 7 summarizes our findings and contributions.2 Related WorkMorphological tagging (Oflazer and Kuru?oz, 1994;Haji?c and Hladk?a, 1998) is the task of assigning amorphological reading to a token in context.
Themorphological reading consists of features such ascase, gender, person and tense and is represented asa single tag.
This allows for the application of stan-dard sequence labeling algorithms such as Condi-tional Random Fields (CRFs) (Lafferty et al, 2001),but also puts an upper bound on the accuracy as onlyreadings occurring in the training set can be pro-duced.
It is still the standard approach to morpho-logical disambiguation as the number of readingsthat cannot be produced is usually small.The related work can be divided in systemsthat try to exploit certain properties of a language(Habash and Rambow, 2005; Yuret and T?ure, 2006)and language-independent systems (Haji?c, 2000;Smith et al, 2005).
In this paper, we adopt alanguage-independent approach.Semi-supervised learning attempts to increase theaccuracy of a machine learning system by using ad-ditional unlabeled data.
Word representations, es-pecially Brown clusters, have been extensively usedfor named entity recognition (NER) (Miller et al,2004), parsing (Koo et al, 2008) and POS tagging(Collobert and Weston, 2008; Huang et al, 2009).In these papers, word representations were shown toyield consistent improvements and to often outper-form traditional semi-supervised methods such asself-training.
Prior work on semi-supervised train-ing for morphological tagging includes Spoustov?a etal.
(2009) and Chrupala (2011).
In contrast to thisearlier work on morphological tagging, we study anumber of morphologically more complex and di-verse languages.
We also compare learned represen-527tations to representations obtained from MAs.Domain adaptation (DA) attempts to adapt a modeltrained on a source domain to a target domain.
DAcan be broadly divided into supervised and unsuper-vised approaches depending on whether labeled tar-get domain data is available or not.
Among unsu-pervised approaches to DA, representation learning(Ando and Zhang, 2005; Blitzer et al, 2006) uses theunlabeled target domain data to induce a structurethat is suitable for transferring information from thelabeled source domain to the target domain.
Simi-lar to representation learning for DA, we attempt toinclude word representations into the model.
How-ever, we induce the representation from a generaldomain in an attempt to obtain a model that has ro-bust high accuracy across domains, for the sourcedomain as well as for the target domains, for whichneither labeled nor unlabeled training data is avail-able.3 RepresentationsWe survey the following distributional represen-tations: (i) count vectors reduced by a SingularValue Decomposition (SVD), (ii) word clusters in-duced using the likelihood of a class-based languagemodel, (iii) distributed embeddings trained using aneural network and (iv) accumulated tag counts, atask-specific representation obtained from an auto-matically tagged corpus.Singular value decomposition of word-featurecooccurrence matrices (Sch?utze, 1995) has beenfound to be a fast and efficient way to obtain dis-tributed embeddings.
The approach selects a subsetof the vocabulary as so-called feature words, usuallyby including words up to a certain frequency rank.Every word form can then be represented by the ac-cumulated counts of feature words occurring to itsleft and right.
Then an SVD is applied to the cooc-currence matrix as a form of dimension reductionand to reduce sparsity.We also experimented with unreduced count vec-tors, but they did not give better results than SVDreduced count vectors.
SVD-based representationshave been used in English POS induction (Lamar etal., 2010) as well as as features in English POS tag-ging and syntactic chunking (Huang et al, 2009);they have a similar level of accuracy as unsupervisedHidden Markov Models (HMMs) in these studies.Language model-based (LM-based) word clus-ters were introduced by Brown et al (1992) andlater found to be helpful in a range of NLP tasks.The basic idea is to find the optimal clustering withrespect to the likelihood of a class-based languagemodel:g = argmaxg|D|?i=1p(g(xi)|g(xi?1)) ?
p(xi|g(xi))where g(x) is the cluster assignment function thatmaps a word form x to a cluster and |D| denotesthe length of the training set.
Brown et al (1992)propose a greedy bottom-up algorithm for the opti-mization that merges the pair of clusters that yieldsthe smallest loss in likelihood; as well as a moreefficient approximation of that algorithm that limitsthe number of clusters under consideration and stillworks well in practice.
It is used by most work inthe literature (Liang, 2005; Turian et al, 2010; Kooet al, 2008).We, however, found the algorithm proposed byMartin et al (1998) to be faster and to give slightlybetter results.
The algorithm is similar to K-meansin that it starts with an initial clustering and greed-ily improves the objective function by moving sin-gle words to their optimal cluster.
In contrast toK-means, it updates the objective function immedi-ately.
The algorithm has also been shown to workwell in unsupervised POS induction (Clark, 2003;Blunsom and Cohn, 2011).
Our implementationof this algorithm is called MarLiN and has beenmade available as open-source software (Section 8).Miller et al (2004) use tags of different granular-ity induced from unlabeled text to improve the per-formance of an averaged perceptron tagger (Collins,2002) on an English NER task.The Brown algorithm induces a tree where leavesrepresent a single word form and the root nodethe entire vocabulary.
Intermediate nodes representclusters of different sizes and can be addressed bya binary string specifying the path from the rootnode to the cluster.
Brown clusters are also usedby Koo et al (2008) to improve dependency pars-ing for English and Czech.
Chrupala (2011) com-pare Brown clusters to a Latent Dirichlet Allocation528(LDA) model on Spanish and French morphologicaltagging and find them to yield similar performance.1Neural networks have been used by Collobert andWeston (2008) to train embeddings for POS tag-ging as well as other NLP tasks.
These embed-dings ?
henceforth CW embeddings ?
are trainedby building a neural network that given contextsof a word as input is trained to discriminate be-tween the correct center word and a random word.The proposed training algorithm is reported to needseveral days or even weeks, but has been reimple-mented by Al-Rfou et al (2013), who induced em-beddings for the Wikipedias of more than 100 lan-guages.
Turian et al (2010) find that the perfor-mance of Brown clusters is competitive with moretraining intensive embeddings like CW.
In our exper-iments, we find that MarLiN clusters slightly outper-form CW.
We do not evaluate bag-of-words modelssuch as WORD2VEC (Mikolov et al, 2013), becausethe ordering of words is essential for finding mor-phological properties.Accumulated tag counts (ACT) are a form of task-specific sparse representation.
The unlabeled cor-pus is first annotated by a tagger; for each occurringword form, the number of times a specific tag wasassigned can then be used as a representation.
Gold-berg and Elhadad (2013) and (Sz?ant?o and Farkas,2014) show that using such information in the word-preterminal emission probabilities of PCFGs canimprove parsing accuracy.
Specifically, Sz?ant?o andFarkas (2014) show that this approach performs aswell as an MA in some cases.
We find MAs to bemore effective than the accumulated count embed-dings; this is not a contradiction as we try to improvethe performance of the tagger itself.4 Data PreparationOur test suite consists of data sets for six differ-ent languages: Czech (cs), English (en), German(de), Hungarian (hu), Spanish (es) and Latin (la).Czech, German, Hungarian and Latin are morpho-logically rich.
We chose these languages because1The authors claim that LDA Gibbs sampling is faster thanthe induction of Brown clusters because it only depends linearlyon the number of clusters.
We, however, could not train theirmodels on our bigger data sets as the sampling depends linearlyon the number of tokens.they represent different families: Germanic (En-glish, German), Romance (Latin, Spanish), Slavic(Czech) and Finno-Ugric (Hungarian) and differ-ent degrees of morphological complexity and syn-cretism.
For example, English and Spanish rarelymark case while the other languages do; and as anagglutinative language, Hungarian features a lownumber of possible readings for a word form whilelanguages like German can have more than 40 dif-ferent readings for a word form.
An additional crite-rion was to have a sufficient amount of labeled OODdata.
The data sets also feature an interesting selec-tion of domain differences.
For example, for Latinwe have texts from different epochs while the En-glish data contains canonical and non-canonical text.Labeled Data.
This section describes the annotationand conversion we performed to create consistent IDand OOD data sets.2No conversion was required forHungarian, English and Latin as the data is alreadyannotated in a consistent way.For Hungarian we use the (multi-domain) SzegedDependency Treebank (Vincze et al, 2010).
We usethe part that was used in the SPMRL 2013 sharedtask (Seddah et al, 2013) as ID data (news-wire) andan excerpt from the novel 1984 and a Windows 2000manual as OOD data.For Latin we use the PROIEL treebank (Haug andJ?hndal, 2008).
It consists of data from the Vulgate(bible text, ?
380 AD), Commentarii de Bello Gal-lico (?
50 BC), Letters from Cicero to his friend At-ticus (?
50 BC) and The Pilgrimage of Aetheria (?380 AD).
We use the biggest text source (Vulgate)as ID data and the remainder as OOD data.For English we use the SANCL shared task data(Petrov and McDonald, 2012), which consists ofOntonotes 4.0 as ID data and five OOD domainsfrom the Google Web treebank: Yahoo!
Answers,weblogs, news groups, business reviews and emails.For Czech we use the part of the Prague Depen-dency Treebank (PDT) (B?ohmov?a et al, 2003) thatwas used in the CoNLL 2009 shared tasks (Haji?cet al, 2009) as ID data.
We use the Czech partof the Multext East (MTE) corpus (Erjavec, 2010)as OOD data.
MTE consists of translations of the2Table 5 of the appendix provides a structured overview overthe domains and resources used for each language.
The ap-pendix can be found at http://cistern.cis.lmu.de/marmot/naacl2015/appendix.pdf.529novel 1984 that have been annotated morphologi-cally.
PDT and MTE have been annotated usingtwo different guidelines that without further anno-tation effort could only be merged by reducing themto a common subset.
Specifically, we removed fea-tures such as sub POS tags as well as markers for(in)animacy.
The PDT features a number of tags thatare ambiguous and could not always be resolved.The gender feature Q for example can mean femi-nine or neuter.
If we could not disambiguate sucha tag, we removed it; this results in morphologicaltags that are not present in the MTE corpus and arelatively high number of unseen tags.
Instead ofdescribing the conversion process in greater detailwe refer to our conversion scripts (Section 8).For Spanish we use the part of the AnCora corpus(Taul?e et al, 2008) of CoNLL 2009 and the IULAtreebank (Marimon et al, 2012), which consists offive domains: law, economics, medicine, computerscience and environment.
We use the AnCora cor-pus as ID data set and IULA as OOD data set.
Thetwo treebanks have been annotated using the sameannotation scheme, but slightly different guidelines.Similar to Czech we merged the data sets by delet-ing features that could not be merged or were notpresent in one of the treebanks.
Again we refer tothe conversion script for further details (Section 8).For German we use the Tiger treebank (Brants etal., 2002) in the same split as M?uller et al (2013) asID data and the Smultron corpus (Volk et al, 2010)as OOD data.
Smultron consists of four parts: adescription of Alpine hiking routes, a DVD man-ual, an excerpt of Sophie?s World and economicstexts.
It has been annotated with POS and syntax,but not with morphological features.
We annotatedSmultron following the Tiger guidelines.
The anno-tation process was similar to Marimon et al (2012)in that the data sets were automatically tagged withthe MORPH tagger MarMoT (M?uller et al, 2013)and then manually corrected by two annotators.
Thistagger is a strong baseline as we could include fea-tures based on gold lemma, POS and syntax (Seekerand Kuhn, 2013).
The agreement of the annotatorswas .9628 and the ?
agreement .64.3As most of the3For calculating ?, we assume that random agreement oc-curs when both annotators agree with the reading proposed bythe tagger.
We then estimate the probability of random agree-ment by multiplying the individual estimated probabilities ofdifferences between the annotators were cases whereonly one of the annotators had corrected an obviouserror that the other had overlooked, the differenceswere resolved by the annotators themselves.We used the provided segmentation if availableand otherwise split ID data 8/1/1 into training, devel-opment and test sets and OOD data 1/1 into devel-opment and test sets if not mentioned otherwise.
Wethus have a classical setup of in-domain news papertext vs. prose, medical, law, economic or technicaltexts for Czech, German, Spanish and Hungarian.For English we have canonical vs. non-canonicaldata and for Latin data of different epochs (ca.
400AD vs 50 BC).
Additionally, for German one of thetest domains is written in Swiss German.Looking at some statistics of the labeled datasets,4we find that: Hungarian and Latin are the lan-guages with the highest OOV rates (27% and 37%,which for reasons of consistency we will henceforthwrite as follows: .27 and .37); Hungarian has avery productive agglutinative morphology while thehigh number of Latin OOVs can be explained bythe small training set (<60,000); Czech features thehighest unknown tag rate (.05) as well as the highestunseen word-tag rate (.16).
This can be explained bythe limits of the conversion procedure we discussedabove, e.g., ambiguous features like Q.Unlabeled Data.
As unlabeled data we useWikipedia dumps from 2014 for all languages ex-cept for Latin for which we use the PatrologiaLatina, a collection of clerical texts from ca.
100AD to 1200 AD from Corpus Corporum (Roelli,2014).
We do not use the Latin version of Wikipediabecause it is written by enthusiasts, not by nativespeakers, and contains many errors.We preprocessed the Wikipedia dumps withWIKIPEDIAEXTRACTOR (Attardi and Fuschetto,2013) and NLTK?S (Bird et al, 2009) implemen-tation of PUNKT (Kiss and Strunk, 2006) to de-tect sentence boundaries.
Tokenization was per-formed using MAGYARLANC (Hungarian, Zsibrita etal.
(2013)), STANFORD TOKENIZER (English, Man-ning et al (2014)), FREELING (Spanish, Padr?o andStanilovsky (2012)) and CZECHTOK5(Czech).
Forchanging the proposed tagging.
This yields a random agree-ment probability of .8965.4Complete tables are in the appendix: Tables 1 and 2.5http://sourceforge.net/projects/530Latin, we removed punctuation because PROIELdoes not contain punctuation.
We also split off theclitics ne, que and ve if the resulting token was ac-cepted by LATMOR (Springmann et al (2014)).
Fol-lowing common practice, we normalized the text byreplacing digits with 0s.6In our experiments, we extract representations forthe 250,000 most frequent word types.
This vo-cabulary size is comparable to other work; e.g.,Turian et al (2010) use 269,000 types.
This thresh-old yields low fractions of uncovered tokens7forEnglish and Latin (.009 and .02).
For the otherlanguages, this fraction rises to .04.
We also ex-tract the morphological readings of the words in thisvocabulary using MAGYARLANC (Hungarian, Zsib-rita et al (2013)), FREELING (English and Spanish,Padr?o and Stanilovsky (2012)), SMOR (German,Schmid et al (2004)), an MA from Charles Uni-versity (Czech, Haji?c (2001)) and LATMOR (Latin,Springmann et al (2014)).
Throughout this paperwe extract one feature for each cluster id or MAreading of the current word form.
For example,SMOR produces two readings for the German wordform erhielt ?received?
: <1><SG><PAST><IND>and <2><SG><PAST><IND>, we thus fire twofeatures representing the respective tags whenevererhielt is seen in the data.
We also experimentedwith cluster indexes of neighboring uni/bigrams, butobtained no consistent improvement.
For the denseembeddings we analogously extract the vector of thecurrent word form.5 ExperimentsFor all our experiments we use MarMoT (M?ulleret al, 2013) a joint POS and morphological tag-ger.8The CRF tagger employs a pruning strategy onforward-backward lattices to efficiently handle bigtag sets and higher orders.
Its feature set is similarto Ratnaparkhi (1996) and Toutanova et al (2003)and includes prefixes, suffixes, immediate lexicalcontext and shape features based on capitalization,special characters and digits.
MarMoT was shownto be a competitive POS and morphological taggerczechtok/6For statistics of the unlabeled data sets cf.
Table 3 of theappendix.7Cf.
Table 4 in the appendix.8http://cistern.cis.lmu.de/marmot/across six languages (M?uller et al, 2013).
In orderto make sure that it is also robust in an OOD setupwe compare it to the two popular taggers SVM-Tool (Gim?enez and Marquez, 2004) and Morfette(Chrupa?a et al, 2008).
The results are summarizedin Table 1.MarMoT uses stochastic gradient descent andproduces different results in each training run.
Wetherefore always report the average of five runs.
TheOOD numbers are macro-averages over the differentOOD data sets of a language.9The tables in thispaper are based on the development sets; the onlyexception to this is Table 5, which is based on thetest set.
MarMoT outperforms SVMTool and Mor-fette on every language and setup (ID / OOD) ex-cept for the Spanish OOD data set.
For Czech, Ger-man and Latin the improvements over the best base-line are >1.
Different orders of MarMoT behaveas expected: higher-order models (order>1) outper-form first-order models.
The only exception to thisis Latin.
This suggests a drastic difference of thetag transition probabilities between the Latin ID andOOD data sets.
Given the results in Table 1 and forsimplicity we use an second-order MarMoT modelin all subsequent experiments.LM-based clustering.
We first compare differentimplementations of LM-based clustering.
The im-plementation of Brown clustering by Liang (2005) ismost commonly used.
Its hierarchical binary struc-ture can be used to extract clusterings of varyinggranularity by selecting different prefixes of the pathfrom the root to a specific word form.
Follow-ing other work (Ratinov and Roth, 2009; Turian etal., 2010), we induce 1000 clusters and select pathlengths 4, 6, 10 and 20.
We call this representa-tion Brown path.
We compare Brown path to mk-cls10(Och, 1999) and MarLiN.
These implementa-tions just induce flat clusterings of a certain size; wethus run them for cluster sizes 100, 200, 500 and1000 to also obtain cluster ids of different sizes.
Thecluster sizes roughly resemble the granularity ob-tained in Brownpath.
We call the corresponding mod-9Throughout this paper we use the approximate randomiza-tion test (Yeh, 2000) to establish significance.
To this end, wecompare the output of the medians of the five independent mod-els.
We regard p-values <.05 as significant.10mkcls implements a similar training algorithm as MarLiN,but uses simulated annealing, not greedy maximization.531MarMoT (1) MarMoT (2) MarMoT (3) Morfette SVMToolID OOD ID OOD ID OOD ID OOD ID OODmorphcs 93.27 77.83 93.89 78.52 93.86 78.55 91.48 76.56 91.06 75.41de 88.90 82.74 90.26 84.19 90.54?84.30 85.89 80.28 85.98 78.08es 98.21 93.24 98.22 93.62 98.16 93.42 97.95 93.97?97.96 91.36hu 96.11 89.78 96.07 89.83 95.92 89.70 95.47 89.18 94.72 88.44la 86.09 67.90?86.44 67.47 86.47 67.40 83.68 65.06 84.09 65.65Table 1: Baseline experiments comparing MarMoT models of different orders with Morfette and SVMTool.
Num-bers denote average accuracies on ID and OOD development sets on the full morphological tagging task.
A resultsignificantly better than the other four ID (resp.
OOD) results in its row is marked with ?.BrownflatBrownpathMarLiN mkclsID OOD ID OOD ID OOD ID OODposcs 99.19 97.25 99.18 97.21 99.19 97.26 99.21 97.26de 98.08 93.42 98.07 93.47 98.10 93.44 98.11 93.64?en 96.99 91.67 97.02 91.71 97.01 91.71 97.03 91.86?es 98.84 97.91 98.84 97.97 98.87 97.97 98.84 97.90hu 97.95 93.40 97.89 93.39 97.98 93.36 97.99 93.42la 96.78 86.49 96.62 86.60 96.91 87.24 96.95 87.19morphcs 94.20 78.95 94.23 79.01 94.35 79.14 94.32 79.11de 90.71 85.39 90.75 85.44 90.78 85.58 90.68 85.47es 98.47 95.08 98.47 95.12 98.48 95.15 98.48 95.13hu 96.60 90.57 96.52 90.54 96.60 90.64 96.61 90.66la 87.53 71.69 87.44 71.60 87.87 72.08 87.67 71.88Table 2: Tagging results for LM-based modelsels Brownflat, mkcls and MarLiN.
The runtime of theBrown algorithm depends quadratically on the num-ber of clusters while mkcls and MarLiN have linearcomplexity.
This is reflected in the training times:For German the Brown algorithm takes?
5000min,mkcls ?
2000min and MarLiN ?
500min.Table 2 shows that the absolute differences be-tween systems are small, but overall MarLiN andmkcls are better.11We conclude that systems basedon the algorithm of Martin et al (1998) are slightlymore accurate for tagging and are several timesfaster than the more frequently used version ofBrown et al (1992).
We thus use MarLiN for theremainder of this paper.Neural Network Representations.
We compareMarLiN with the implementation of CW by Al-Rfouet al (2013).
They extracted 64-dimensional repre-sentations for only the most frequent 100,000 wordforms.
To make the comparison fair, we use the in-tersection of our and their representation vocabular-ies.12The results in Table 3 show that MarLiN is11Brownpathreaches the same performance as MarLiN in onecase: pos/es/OOD.12We also use representations from Wikipedia (instead ofCorpus Corporum) for Latin to increase the similarity of theBaseline MarLiN CWID OOD ID OOD ID OODposcs 99.00 96.80 99.16?97.06 99.12 97.00de 97.87 92.21 98.03 93.35?98.03 93.02en 96.92 91.12 97.05 91.72 97.00 91.86?es 98.62 96.70 98.79 97.82?98.80 97.31hu 97.49 92.79 97.94 93.30 97.88 93.40la 95.80 81.92 96.35?85.52?95.88 84.50morphcs 93.89 78.52 94.23?78.91 94.10 78.80de 90.26 84.19 90.54 85.08 90.59 85.21es 98.22 93.62 98.44 94.97?98.44 94.32hu 96.07 89.83 96.47 90.60 96.48 90.95?la 86.44 67.47 86.95 70.30?86.76 69.32Table 3: Tagging results for CWbest in 15 out of 22 cases and significantly better ineight.
CW is best in 9 out of 22 cases and signif-icantly better in two.
We conclude that LM-basedrepresentations are more suited for tagging as theycan be induced faster, are smaller and give better re-sults.SVD and ACT Representations.
For the SVD-based representation we use feature ranks out of{500, 1000} and dimensions out of {50, 100, 200,500}.
We found that l1-normalizing the vectors be-fore and after the SVD improved results slightly.For the accumulated tag counts (ACT) we annotatethe data with our baseline model and extract word-tag probabilities.
The probabilities are then used assparse real-valued features.
Table 4 shows that allrepresentations outperform the baseline.
Improve-ments are biggest for Latin.
Overall, SVD outper-forms ACT and is outperformed by MarLiN andMA.
MarLiN gives the best representations for POStagging while MA outperforms MarLiN in MORPHtagging.
Table 5 shows that the findings for the base-line, MarLiN and MA also hold for the test set.training data.532Baseline ACT MarLiN MA SVDID OOD ID OOD ID OOD ID OOD ID OODposcs 99.00 96.80 99.11 97.03 99.19 97.26 99.18 97.25 99.11 97.09de 97.87 92.21 98.00 92.92 98.10 93.44?98.00 92.87 98.09 92.88en 96.92 91.12 96.97 91.47 97.01 91.71 96.99 91.57 97.00 91.75es 98.62 96.70 98.79 97.09 98.87 97.97 98.87 97.89 98.80 97.16hu 97.49 92.79 97.84 93.15 97.98 93.36 98.12?93.77?97.86 93.30la 95.80 81.92 96.17 83.40 96.91 87.24?96.81 86.31 96.36 85.01morphcs 93.89 78.52 94.16 78.75 94.35 79.14 94.48?79.41?94.14 78.94de 90.26 84.19 90.56 84.78 90.78 85.58 90.75 85.75 90.69 85.15es 98.22 93.62 98.38 93.92 98.48 95.15 98.56?95.43?98.40 94.18hu 96.07 89.83 96.25 90.07 96.60 90.64 96.83?91.14?96.46 90.50la 86.44 67.47 86.96 68.61 87.87 72.08 88.40?73.23?87.45 70.81Table 4: Tagging results for the baseline and four different representationsBaseline MarLiN MAID OOD ID OOD ID OODposcs 98.88 96.43 99.11?96.94 99.06 96.95de 97.32 91.10 97.73?92.00?97.60 91.49en 97.36 89.81 97.58?90.65?97.47 90.51es 98.66 97.94 98.94?98.33 98.87 98.38hu 96.84 92.11 97.08 92.95 97.46?93.25?la 93.02 81.35 95.20 87.58?95.11 86.45morphcs 93.93 77.50 94.33 78.12 94.50?78.37?de 88.41 82.78 89.18 83.91 89.32?84.09es 98.30 95.65 98.53 95.92 98.54 96.33?hu 94.82 88.82 95.46 89.98 95.85?90.46?la 82.09 65.59 84.67 71.25 85.91?72.42?Table 5: Test set results for: baseline, MarLiN, MAf = 0 0 < f < 10 f >= 10morphcs MarLiN 0.29 0.22 0.11MA 0.37 0.35 0.16de MarLiN 1.02 0.17 0.19MA 0.85 0.29 0.42es MarLiN 1.36 0.15 0.02MA 1.50 0.27 0.04hu MarLiN 0.62 0.18 0.00MA 1.07 0.20 0.03la MarLiN 3.76 0.80 0.06MA 4.98 0.69 0.09Table 6: Improvement compared to the baseline for dif-ferent frequency ranges of words on OOD6 AnalysisWe now analyze why MarLiN and MA perform bet-ter than the baseline.
First we compare the improve-ments in absolute error rate over the baseline bygrouping word forms by their training set frequencyf .
The number are shown in Table 6.
We find thatmost of the improvement comes from OOV words.Rare words (frequency<10) show a smaller, but stillimportant contribution while the contribution of fre-morphcs MarLiN gen 0.70 cas 0.41 pos 0.35MA gen 0.85 cas 0.51 pos 0.31de MarLiN gen 1.23 pos 1.14 num 0.62MA gen 1.37 pos 0.63 num 0.59es MarLiN sub 1.49 gen 1.21 pos 1.07MA sub 1.34 gen 1.24 pos 1.10hu MarLiN cas 0.71 sub 0.66 pos 0.52MA cas 0.88 sub 0.84 pos 0.76la MarLiN pos 5.19 cas 3.46 gen 3.25MA pos 4.68 gen 3.85 cas 3.01Table 7: Improvement compared to the baseline for dif-ferent featuresquent words can be almost neglected for four lan-guages.
The exception is German where frequentwords contribute more to the error reduction thanrare words.
This could be caused by syncretismssuch as in plural noun phrases where the gender isnot marked in determiner and adjective and can onlybe derived from the head noun; e.g., the adjectivesin schwere Schulf?acher ?difficult school subjects?and verd?achtige Personen ?suspect persons?
are un-marked for gender and the correct genders (neutervs.
feminine) cannot be inferred from distributionalinformation or suffixes for the nouns (although gen-der is easy to infer distributionally for singular formsof nouns).Looking at the morphological features with thehighest improvement in absolute error rate (Table 7)we find, that the features with the highest improve-ment are POS, SUB-POS (a finer division of POS,e.g., nouns are split into proper / common nouns),gender, case and number.
For all languages POS and?
if part of the annotation ?
SUB-POS are among the533three features with the highest improvements.
Gen-der is also always among the three features with thehighest improvements for the four languages thathave gender (es, de, la, cs).
We just discussed anexample for German where gender could not be de-rived from context or inflectional suffixes.
Otherlanguages also have word forms that do not markgender, e.g., Spanish masculine ave ?bird?
vs. femi-nine llave ?key?.
The gender can, however, easily bederived if the word representation encodes whether aword form has been seen with a specific determineror adjective on its right or left.Lastly, we use Jaccard similarity13to compare thesets of gold and predicted morphological features.Jaccard can be interpreted as a soft variant of ac-curacy: If the two tags are identical it yields 1 andotherwise it corresponds to the number of correctlypredicted features divided by the size of the union ofgold and predicted features.morph cs de es hu laaccuracy 79.41 85.72 95.43 91.14 73.23Jaccard 89.89 90.71 96.77 93.52 83.68This table demonstrates that the evaluation measurewe have used throughout this paper ?
a tag counts ascompletely wrong if a single feature was misidenti-fied even though all others are correct ?
is conserva-tive.
On a feature-by-feature basis accuracy wouldbe much higher.
The difference is largest for Czechand Latin.7 ConclusionWe have presented a test suite for morphologicaltagging consisting of in-domain (ID) and out-of-domain (OOD) data sets for six languages: Czech,English, German, Hungarian, Latin and Spanish.We converted some of the data sets to obtain a rea-sonably consistent annotation and manually anno-tated the German part of the Smultron treebank.
Wesurveyed four different word representations: SVD-reduced count vectors, LM-based clusters, accumu-lated tag counts and CW embeddings.
We found thatthe LM-based clusters outperformed the other rep-resentations for POS and MORPH tagging, ID andOOD data sets and all languages.
We also showedthat our implementation of MarLiN (Martin et al,1998) is an order-of-magnitude more efficient and13Jaccard(U, V ) = |U ?
V |/|U ?
V |performs slightly better than the implementation byLiang (2005).
We also compared the learned repre-sentations to manually created Morphological Ana-lyzers (MAs).
We found that MarLiN outperformsMAs in POS tagging, but that it is substantiallyworse in morphological tagging.
In our analysis ofthe results, we showed that both MarLiN and MAsdecrease the error most for out-of-vocabulary wordsand for the features POS and gender.8 ResourcesAs part of this publication we also release the fol-lowing resources at http://cistern.cis.lmu.de/marmot/: (i) our implementation of MarLiNas open-source (ii) the morphological layer of theGerman part of the SMULTRON corpus.
For eas-ier reproducibility, we also made (iii) the prepro-cessed Wikipedia dumps and the induced represen-tation dictionaries available.
(iv) Morphological dic-tionaries were released to the extent this was com-patible with the usage agreement.
(v) We also pub-lished the conversion code for unifying the Spanishand Czech annotations.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir comments.
The first author is a recipient ofthe Google Europe Fellowship in Natural LanguageProcessing, and this research is supported by thisGoogle Fellowship.
The annotation of the SMUL-TRON data was supported by Deutsche Forschungs-gemeinschaft (grant DFG 2246/2, Wordgraph).ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013.Polyglot: Distributed word representations for multi-lingual NLP.
In Proceedings of CoNLL.Rie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for textchunking.
In Proceedings of ACL.Giuseppe Attardi and Antonia Fuschetto.
2013.Wikipedia Extractor.
http://medialab.di.unipi.it/wiki/Wikipedia_Extractor.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural language processing with Python.
?
O?Reilly Me-dia, Inc.?.534John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal Pitman-Yor process HMM for unsupervised part ofspeech induction.
In Proceedings of ACL-HLT.Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and BarboraHladk?a.
2003.
The Prague dependency treebank.
InProceedings of Treebanks.
Springer.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the workshop on treebanksand linguistic theories.Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics.Grzegorz Chrupa?a, Georgiana Dinu, and Josef Van Gen-abith.
2008.
Learning morphology with Morfette.
InProceedings of LREC.Grzegorz Chrupala.
2011.
Efficient induction of prob-abilistic word classes with LDA.
In Proceedings ofIJCNLP.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of EACL.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML.Toma?z Erjavec.
2010.
MULTEXT-East version 4: Mul-tilingual morphosyntactic specifications, lexicons andcorpora.
In Proceedings of LREC.Jes?us Gim?enez and Lluis Marquez.
2004.
SVMTool: Ageneral POS tagger generator based on support vectormachines.
In Proceedings of LREC.Yoav Goldberg and Michael Elhadad.
2013.
Word seg-mentation, unknown-word resolution, and morpholog-ical agreement in a Hebrew parsing system.
Computa-tional Linguistics.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofACL.Jan Haji?c and Barbora Hladk?a.
1998.
Tagging inflectivelanguages: Prediction of morphological categories fora rich, structured tagset.
In Proceedings of Coling.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, et al 2009.
The CoNLL-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proceedings of CoNLL.Jan Haji?c.
2000.
Morphological tagging: Data vs. dictio-naries.
In Proceedings of NAACL.Jan Haji?c.
2001.
Czech Free Morphology.Dag TT Haug and Marius J?hndal.
2008.
Creating aparallel treebank of the old Indo-European bible trans-lations.
In Proceedings of LaTeCH.Zhongqiang Huang, Vladimir Eidelman, and MaryHarper.
2009.
Improving a simple bigram hmm part-of-speech tagger by latent annotation and self-training.In Proceedings of NAACL.Tibor Kiss and Jan Strunk.
2006.
Unsupervised multi-lingual sentence boundary detection.
ComputationalLinguistics.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML.Michael Lamar, Yariv Maron, Mark Johnson, and ElieBienenstock.
2010.
SVD and clustering for unsuper-vised POS tagging.
In Proceedings of ACL.Percy Liang.
2005.
Semi-supervised learning for natu-ral language.
Ph.D. thesis, Massachusetts Institute ofTechnology.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of ACL: SystemDemonstrations.Montserrat Marimon, Beatriz Fisas, N?uria Bel, MartaVillegas, Jorge Vivaldi, Sergi Torner, Merc`e Lorente,Silvia V?azquez, and Marta Villegas.
2012.
The IULAtreebank.
In Proceedings of LREC.Sven Martin, Jorg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.Speech communication.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
In Proceedings of ICLR: Work-shop.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrimi-native training.
In Proceedings of NAACL-HLT.Thomas M?uller, Helmut Schmid, and Hinrich Sch?utze.2013.
Efficient higher-order CRFs for morphologicaltagging.
In Proceedings of EMNLP.Franz J. Och.
1999.
An efficient method for determiningbilingual word classes.
In Proceedings of EACL.535Kemal Oflazer and`Ilker Kuru?oz.
1994.
Tagging andmorphological disambiguation of turkish text.
In Pro-ceedings of the Applied natural language processing.Llu?
?s Padr?o and Evgeny Stanilovsky.
2012.
Freeling3.0: Towards wider multilinguality.
In Proceedingsof LREC.Slav Petrov and Ryan McDonald.
2012.
Overview of the2012 shared task on parsing the web.
In Proceedingsof SANCL.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of CoNLL.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP.Phillip Roelli.
2014.
Corpus Corporum.
http://www.mlat.uzh.ch/MLS/.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.SMOR: A German computational morphology cover-ing derivation, composition and inflection.
In Pro-ceedings of LREC.Hinrich Sch?utze.
1995.
Distributional part-of-speechtagging.
In Proceedings of EACL.Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, MarieCandito, Jinho Choi, Rich?ard Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,et al 2013.
Overview of the SPMRL 2013 sharedtask: Cross-framework evaluation of parsing morpho-logically rich languages.
In SPMRL.
Association forComputational Linguistics.Wolfgang Seeker and Jonas Kuhn.
2013.
The effects ofsyntactic features in automatic prediction of morphol-ogy.
In Proceedings of EMNLP.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proceedings of EMNLP.Drahom?
?ra ?johanka?
Spoustov?a, Jan Haji?c, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervised train-ing for the averaged perceptron pos tagger.
In Pro-ceedings of EACL.Uwe Springmann, Dietmar Najock, Hermann Morgen-roth, Helmut Schmid, Annette Gotscharek, and Flo-rian Fink.
2014.
OCR of historical printings of Latintexts: problems, prospects, progress.
In Proceedingsof DATeCH.Zsolt Sz?ant?o and Rich?ard Farkas.
2014.
Special tech-niques for constituent parsing of morphologically richlanguages.
In Proceedings of EACL.Mariona Taul?e, Maria Ant`onia Mart?
?, and Marta Re-casens.
2008.
Ancora: Multilevel annotated corporafor Catalan and Spanish.
In Proceedings of LREC.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of NAACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of ACL.Veronika Vincze, D?ora Szauter, Attila Alm?asi, Gy?orgyM?ora, Zolt?an Alexin, and J?anos Csirik.
2010.
Hun-garian dependency treebank.
In Proceedings of LREC.Martin Volk, Anne G?ohring, Torsten Marek, and YvonneSamuelsson.
2010.
SMULTRON (version 3.0) ?
TheStockholm MULtilingual parallel TReebank.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof COLING.Deniz Yuret and Ferhan T?ure.
2006.
Learning morpho-logical disambiguation rules for Turkish.
In Proceed-ings of NAACL.Yi Zhang and Rui Wang.
2009.
Cross-domain depen-dency parsing using a deep linguistic grammar.
In Pro-ceedings of ACL-AFNLP.J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas.2013.
magyarlanc: A toolkit for morphological anddependency parsing of Hungarian.
In Proceedings ofRANLP.536
