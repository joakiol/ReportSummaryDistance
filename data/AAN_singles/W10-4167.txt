LSTC System for Chinese Word Sense InductionPeng Jin, Yihao Zhang, Rui SunLaboratory of Intelligent Information Processing and ApplicationLeshan Teachers?
Collegejandp@pku.edu.cn,yhaozhang@163.com,dram_218@163.comAbstractThis paper presents the Chinese wordsense Induction system of LeshanTeachers?
College.
The systemparticipates in the Chinese word senseInduction of task 4 in Back offsorganized by the Chinese InformationProcessing Society of China (CIPS) andSIGHAN.
The system extracts neighborwords and their POSs centered in thetarget words and selected the best one offour cluster algorithms: Simple KMeans,EM, Farthest First and HierarchicalCluster based on training data.
Weobtained the F-Score of 60.5% on thetraining data otherwise the F-Score is57.89% on the test data provided byorganizers.1.
IntroductionAutomatically obtain the intended sense ofpolysemous words according to its context hasbeen shown to improve performance ininformation retrieval?
information extractionand machine translation.
There are two ways toresolve this problem in view of machinelearning, one is supervised classification andthe other is unsupervised classification i.e.clustering.
The former is word sensedisambiguation (WSD) which relies on largescale, high quality manually annotated sensecorpus, but building a sense-annotated corpusis a time-consuming and expensive project.Even the corpus were constructed, the systemtrained from this corpus show the lowperformance on different domain test corpus.The later is word sense induction (WSI) whichneeds not any training data, and it has becomeone of the most important topics in currentcomputational linguistics.Chinese Information Processing Society ofChina (CIPS) and SIGHAN organized a task isintended to promote the research on ChineseWSI.
We built a WSI system namedLSTC-WSI system for this task.
This systemtried four cluster algorithms, i.e.
SimpleKMeans?EM?Farthest First and HierarchicalCluster implemented by weak 3.7.1 [6], andfound Simple KMeans compete the other threeones according to their performances ontraining data.
Finally, the results returned bySimple KMeans were submitted.2.
Features SelectionFollowing the feature selection in word sensedisambiguation, we extract neighbor words andtheir POSs centered in the target words.
Wordsegmented and POS-tag tool adapted ChineseLexical Analysis System developed by Instituteof Computing Technology.
No other resource isused in the system.
The window size of thecontext is set to 5 around the ambiguous word.The neighbor words which occur only oncewere removed.
Each sample is represented as avector, and feature form is binary: if it occursin is 1 otherwise is 0.3.
Clusters AlgorithmsFour cluster algorithms were tried in oursystem.
I will introduce them simply in thenext respectively.K-means clustering [1] is one of the simplestunsupervised learning algorithms that solve thewell known clustering problem.
The main ideais to define k centroids, one for each cluster.These centroids should be placed in a cunningway because of different location causesdifferent result.
So, the better choice is to placethem as much as possible far away from eachother.EM algorithm[2] is a method for findingmaximum likelihood estimates of parameters instatistical models, where the model depends onunobserved latent variables.
EM is an iterativemethod which alternates between performingan expectation (E) step, which computes theexpectation of the log-likelihood evaluatedusing the current estimate for the latentvariables, and maximization (M) step, whichcomputes parameters maximizing the expectedlog-likelihood found on the E step.The Farthest First algorithm [3] is animplementation of the ?Farthest First TraversalAlgorithm?
by Hochbaum and Shmoys (1985).It finds fast, approximate clusters and may beuseful as an initialiser for k-means.A hierarchical clustering [4] is the guaranteethat for every k, the induced k clustering hascost at most eight times that of the optimalk-clustering.
A hierarchical clustering of n datapoints is a recursive partitioning of the datainto 2, 3, 4, .
.
.
and finally n, clusters.
Eachintermediate clustering is made morefine-grained by dividing one of its clusters.4.
Development4.1 Evaluation methodWe consider the gold standard as a solution tothe clustering problem.
All examples taggedwith a given sense in the gold standard form aclass.
For the system output, the clusters areformed by instances assigned to the same sensetag.
We will compare clusters output by thesystem with the classes in the gold standardand compute F-score as usual [5].
F-score iscomputed with the formula below.Suppose  is a class of the gold standard,andCrSi is a cluster of the system generated, then)/(**2),( RPRPScoreF SC ir +=?
(1)sizecluster totalcluster afor  examples labeledcorrectly  ofnumber  the=psizecluster  totalcluster afor  examples labeledcorrectly  ofnumber  theR =Then for a given class Cr,)),(()( max SCC irr scoreFSscoreFi?=?
)(1CrcrFScorennrScoreF ?==?
(2)where c is total number of classes, is the sizeof classnrCr , and is the total size.
Participantswill be required to induce the senses of thetarget word using only the dataset provided bythe organizers.n4.2 Data SetThe organizers provide 50 Chinese trainingdata of SIGHAN2010-WSI-SampleData.
Thetraining data contain 50 Chinese words; eachword has 50 example sentences, and gives eachword the total number of sense.
The totalnumber of sense is ranging from 2 to 21, butmore cases are 2.
In order to facilitate the teamparticipating in the contest to do experiment,the organizers also provide answer to eachword.In order to evaluating the system?sperformance of all participating team, theorganizers provide 100 test word and eachword have 50 example sentences, the system ofeach participating team need to run out theresults which the organizers need.4.3  System SetupWe developed the LSTC-WSI system based onWeka.
Firstly, we implemented the evaluationalgorithm described in section 4.1.
Then, theinstances were represented as vectors accordingto the feature selection.
Thirdly, four clusteralgorithms from Weka were tried and setdifferent thresholds for feature frequency.Because of paper length constraints, we couldnot list all the experience data we get.
Table 1listed system performance when frequencythreshold set two and without POSinformation.Table 1: The Performance on test dataTargetwordSimpleKmeansEMFarthestFirstHierarchical??
0.618 0.680 0.538 0.649??
0.404 0.365 0.400 0.327??
0.711 0.557 0.672 0.636??
0.626 0.700 0.536 0.570??
0.571 0.555 0.572 0.573??
0.789 0.596 0.680 0.548??
0.704 0.617 0.704 0.682??
0.568 0.495 0.461 0.583??
0.5679 0.679 0.625 0.688??
0.601 0.590 0.648 0.603??
0.578 0.554 0.662 0.616??
0.621 0.537 0.615 0.627??
0.560 0.429 0.466 0.527??
0.627 0.537 0.643 0.603??
0.610 0.538 0.643 0.638??
0.643 0.607 0.648 0.632??
0.615 0.545 0.662 0.603??
0.621 0.616 0.615 0.658??
0.538 0.583 0.569 0.609??
0.603 0.540 0.632 0.569??
0.653 0.557 0.657 0.603??
0.627 0.622 0.652 0.690??
0.421 0.438 0.454 0.453??
0.609 0.528 0.583 0.627??
0.634 0.667 0.486 0.652??
0.574 0.546 0.577 0.584?
0.462 0.429 0.518 0.501??
0.661 0.584 0.584 0.602??
0.430 0.501 0.549 0.418??
0.596 0.644 0.647 0.654??
0.614 0.580 0.672 0.708??
0.666 0.600 0.615 0.595??
0.638 0.590 0.540 0.678??
0.841 0.734 0.662 0.618??
0.613 0.562 0.670 0.568??
0.635 0.617 0.646 0.649??
0.603 0.594 0.615 0.577??
0.644 0.635 0.661 0.560??
0.599 0.595 0.624 0.638??
0.588 0.575 0.587 0.508??
0.699 0.723 0.673 0.643??
0.585 0.596 0.666 0.603??
0.643 0.639 0.666 0.656??
0.624 0.537 0.663 0.608??
0.632 0.525 0.629 0.617??
0.451 0.472 0.490 0.477??
0.613 0.625 0.6723 0.625??
0.601 0.640 0.646 0.661??
0.591 0.585 0.663 0.639??
0.536 0.505 0.477 0.532We tried two ways for feature selection: thefrequency of features and neighbor words?
POSwere taken into account or not.
Table 2 showsthe average performance on the test data viavarying the parameter setting.
Observing theresults returned by Hierarchical cluster is veryimbalance, we set the options ?-L WARD?
inorder to balance the number.Table 2: The Average Performance of 50 Training DataFeaturesSimpleKmeansEMFarthestFirstHierarchicalWord,Windows 50.555 0.566 0.607 0.558Word,Windows 5,Frequency 10.583 0.567 0.599 0.582Word,Windows 5,Frequency 20.605 0.575 0.605 0.598Word,Windows 5,Frequency 30.598 0.590 0.600 0.599Word+POSs,Windows 50.562 0.582 0.618 0.569Word+POSs,Windows 5,Frequency 10.589 0.580 0.610 0.594Word+POSs,Windows 5,Frequency 20.589 0.580 0.610 0.594Compared with the average performance of the50 test data, we find the performance is best1when considering word only and setting thefrequency is two at the same time simpleKMeans was adapted.
So, we use the sameparameters setting and clustered the test databy simple KMeans.
As table 2 shows, theF-Score is 60.5% on training data.
But on testdata, our system?s F-Score is 57.89% officiallyevaluated by task organizers.5.
Conclusion and Future WorksFour cluster algorithms are tried for Chineseword sense induction: Simple KMeans, EM,1 Although ?Farthest First?
got the highest score, theresults of ?Farthest First?
are too imbalance.Farthest First and Hierarchical Cluster.
Weconstruct different feature spaces and select outthe best combination of cluster and featurespace.
Finally, we apply the best system to thetest data.In the future, we will look for better clusteralgorithms for word sense induction.Furthermore, we observe that it is differentfrom word sense disambiguation, different partof speech will cause the polysemy.
We willmake use of this character to improve oursystem.AcknowledgementsThis work is supported by the OpenProjects Program of Key Laboratory ofComputational Linguistics(PekingUniversity)?Ministry of Education, GrantNo.
KLCL-1002.
This work is alsosupported by Leshan Teachers?
College,Grant No.
Z1046.References[1] Dekang Lin, Xiaoyun Wu.
Phrase Clustering forDiscriminative Learning.
Proceedings ofACL ,2009.
[2]Neal R, & Hinton G. A view of the EM algorithmthat justifies incremental, sparse, and othervariants.
Learning in Graphical Models, 89,355?368.
[3] Jon Gibson, Firat Tekiner, Peter Halfpenny.NCeSS Project: Data Mining for Social Scientists.Research Computing Services, University ofManchester, U.K.[4] Sanjoy Dasgupta, Philip M. Long.
Performanceguarantees for hierarchical clustering.
Journal ofComputer and System Sciences, 555?569, 2005.
[5] Eneko Agirre, Aitor Soroa.
Semeval-2007 Task02:Evaluating Word Sense Induction andDiscrimination Systems.
Proceedings ofSemEval-2007, pages 7?12, 2007.
[6] http://www.cs.waikato.ac.nz/ml/weka/
