Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386?389,Prague, June 2007. c?2007 Association for Computational LinguisticsUIUC: A Knowledge-rich Approach to Identifying Semantic Relationsbetween NominalsBrandon Beamer,1,4 Suma Bhat,2,4 Brant Chee,3,4 Andrew Fister,1,4 Alla Rozovskaya,1,4Roxana Girju1,4Department of Linguistics1,Department of Electrical and Computer Engineering2,Department of Library and Information Science3,Beckman Institute4,University of Illinois at Urbana-Champaign{bbeamer, spbhat2, chee, afister2, rozovska, girju}@uiuc.eduAbstractThis paper describes a supervised,knowledge-intensive approach to the auto-matic identification of semantic relationsbetween nominals in English sentences.The system employs different sets of newand previously used lexical, syntactic, andsemantic features extracted from variousknowledge sources.
At SemEval 2007 thesystem achieved an F-measure of 72.4% andan accuracy of 76.3%.1 IntroductionThe SemEval 2007 task on Semantic Relations be-tween Nominals is to identify the underlying se-mantic relation between two nouns in the contextof a sentence.
The dataset provided consists of adefinition file and 140 training and about 70 testsentences for each of the seven relations consid-ered: Cause-Effect, Instrument-Agency, Product-Producer, Origin-Entity, Theme-Tool, Part-Whole,and Content-Container.
The task is defined as abinary classification problem.
Thus, given a pairof nouns and their sentential context, the classifierdecides whether the nouns are linked by the targetsemantic relation.
In each training and test exam-ple sentence, the nouns are identified and manu-ally labeled with their corresponding WordNet 3.0senses.
Moreover, each example is accompanied bythe heuristic pattern (query) the annotators used toextract the sentence from the web and the positionof the arguments in the relation.
(1) 041 ?He derives great joy and <e1>happiness</e1>from <e2>cycling</e2>.?
WordNet(e1) =?happiness%1:12:00::?, WordNet(e2) = ?cy-cling%1:04:00::?, Cause-Effect(e2,e1) = ?true?,Query = ?happiness from *?Based on the information employed, systems canbe classified in four types of classes: (A) systemsthat use neither the given WordNet synsets nor thequeries, (B) systems that use only WordNet senses,(C) systems that use only the queries, and (D) sys-tems that use both.In this paper we present a type-B system that re-lies on various sets of new and previously used lin-guistic features employed in a supervised learningmodel.2 Classification of Semantic RelationsSemantic relations between nominals can be en-coded by different syntactic constructions.
Weextend here over previous work that has focusedmainly on noun compounds and other noun phrases,and noun?verb?noun constructions.We selected a list of 18 lexico-syntactic and se-mantic features split here into three sets: feature set#1 (core features), feature set #2 (context features),and the feature set #3 (special features).
Table 1shows all three sets of features along with their defi-nitions; a detailed description is presented next.
Forsome features, we list previous works where theyproved useful.
While features F1 ?
F4 were selectedfrom our previous experiments, all the other featuresare entirely the contribution of this research.Feature set #1: Core featuresThis set contains six features that were employedin all seven relation classifiers.
The features takeinto consideration only lexico-semantic information386No.
Feature DefinitionFeature Set #1: Core featuresF1 Argument position indicates the position of the arguments in the semantic relation(Girju et al, 2005; Girju et al, 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole).F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic(Girju et al, 2005; Girju et al, 2006) specialization procedure.F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations(Girju et al, 2004) or not.
Specifically, we distinguish here between agential nouns,other nominalizations, and neither.F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location.Feature Set #2: Context featuresF7, F8 Grammatical role describes the grammatical role of e1 (F7) and e2 (F8).
There are threepossible values: subject, direct object, or neither.F9 PP Attachment applies to NP PP constructions and indicates if the prepositional phrasecontaining e2 attaches to the NP containing e1.F10, F11 Semantic Role is concerned with the semantic role of the phrase containingeither e1 (F10) or e2 (F11).
In particular, we focused on three semanticroles: Time, Location, Manner.
The feature is set to 1 if the target nounis part of a phrase of that type and to 0 otherwise.F12, F13, Inter-noun context sequence is a set of three features.
F12 captures the sequence of stemmedF14 words between e1 and e2, while F13 lists the part of speech sequence inbetween the target nouns.
F14 is a scoring weight (with possible values1, 0.5, 0.25, and 0.125) which measures the similarity of an unseensequence to the set of sequence patterns associated with a relation.Feature Set #3: Special featuresF15, F16 Psychological feature is used in the Theme-Tool classifier; indicates if e1 (F15) or e2 (F16)belong or not to a predefined set of psychological features.F17 Instrument semantic role is used for the Instrument-Agency relation and indicates whetherthe phrase containing e1 is labeled as em Instrument or not.F18 Syntactic attachment is used for the Instrument-Agent relation and indicates whether the phrasecontaining the Instrument role attaches to a noun or a verbTable 1: The three sets of features used for the automatic semantic relation classification.about the two target nouns.Argument position (F1) indicates the position ofthe semantic arguments in the relation.
This infor-mation is very valuable, since some relations have aparticular argument arrangement depending on thelexico-syntactic construction in which they occur.For example, most of the noun compounds encod-ing Stuff-Object / Part-Whole relations have e1 asthe part and e2 as the whole (e.g., silk dress).Semantic specialization (F2) is a binary featurerepresenting the prediction of a semantic specializa-tion learning model.
The method consists of a setof iterative procedures of specialization of the train-ing examples on the WordNet IS-A hierarchy.
Thus,after all the initial noun?noun pairs are mappedthrough generalization to entity ?
entity pairs inWordNet, a set of necessary specialization iterationsis applied until it finds a boundary that separates pos-itive and negative examples.
This boundary is testedon new examples for relation prediction.The nominalization features (F3, F4) indicate ifthe target noun is a nominalization and, if yes, ofwhat type.
We distinguish here between agentialnouns, other nominalizations, and neither.
Thefeatures were identified based on WordNet andNomLex-Plus1 and were introduced to filter someof negative examples, such as car owner/THEME.Spatio?Temporal features (F5, F6) were also in-troduced to recognize some near miss examples,such as Temporal and Location relations.
For in-stance, activation by summer (near-miss for Cause-Effect) and mouse in the field (near-miss for Content-Container).
Similarly, for Theme-Tool, a word act-ing as a Theme should not indicate a period of time,as in <e1>the appointment</e1> was for morethan one <e2>year</e2>.
For this we used the in-formation provided by WordNet and special classesgenerated from the works of (Herskovits, 1987),(Linstromberg, 1997), and (Tyler and Evans, 2003).1NomLex-Plus is a hand-coded database of 5,000 verb nom-inalizations, de-adjectival, and de-adverbial nouns.http://nlp.cs.nyu.edu/nomlex/index.html387Feature set #2: Context featuresThis set takes advantage of the sentence context toidentify features at different linguistic levels.The grammatical role features (F7, F8) determineif e1 or e2 is the subject, direct object, or neither.This feature helps filter out some instances with poorcontext, such as noun compounds and identify somenear-miss examples.
For example, a restriction im-posed by the definition of Theme-Tool indicates thatin constructions such as Y/Tool is used for V-ingX/Theme, neither X nor Y can be the subject ofthe sentence, and hence Theme-Tool(X, Y) would befalse.
This restriction is also captured by the nomi-nalization feature in case X or Y is an agential noun.PP attachment (F9) is defined for NP PP construc-tions, where the prepositional phrase containing thenoun e2 attaches or not to the NP (containing e1).The rationale is to identify negative instances wherethe PP attaches to any other word before NP in thesentence.
For example, eat <e1>pizza</e1> with<e2>a fork</e2>, where with a fork attaches tothe verb to eat (cf.
(Charniak, 2000)).Furthermore, we implemented and used two se-mantic role features which identify the semantic roleof the phrase in a verb?argument structure, phrasecontaining either e1 (F10) or e2 (F11).
In particular,we focus on three semantic roles: Time, Location,Manner.
The feature is set to 1 if the target nounis part of a semantic role phrase and to 0 otherwise.The idea is to filter out near-miss examples, expe-cially for the Instrument-Agency relation.
For this,we used ASSERT, a semantic role labeler developedat the University of Colorado at Boulder2 which wasqueried through a web interface.Inter-noun context sequence features (F12, F13)encode the sequence of lexical and part of speechinformation between the two target nouns.
FeatureF14 is a weight feature on the values of F12 andF13 and indicates how similar a new sequence is tothe already observed inter-noun context associatedwith the relation.
If there is a direct match, then theweight is set to 1.
If the part-of-speech pattern of thenew substring matches that of an already seen sub-string, then the weight is set to 0.5.
Weights 0.25and 0.125 are given to those sequences that overlapentirely or partially with patterns encoding other se-2http://oak.colorado.edu/assert/mantic relations in the same contingency set (e.g.,semantic relations that share syntactic pattern se-quences).
The value of the feature is the summationof the weights thus obtained.
The rationale is thatthe greater the weight, the more representative is thecontext sequence for that relation.Feature set #3: Special featuresThis set includes features that help identify specificinformation about some semantic relations.Psychological feature was defined for the Theme-Tool relation and indicates if the target noun (F15,F16) belongs to a list of special concepts.
This fea-ture was obtained from the restrictions listed in thedefinition of Theme-Tool.
In the example need formoney, the noun need is a psychological feature, andthus the instance cannot encode a Theme-Tool rela-tion.
A list of synsets from WordNet subhierarchyof motivation and cognition constituted the psycho-logical factors.
This was augmented with precondi-tions such as foundation and requirement since theywould not be allowed as tools for the theme.The Instrument semantic role is used for theInstrument-Agency relation as a boolean feature(F17) indicating whether the argument identified asInstrument in the relation (e.g., e1 if Instrument-Agency(e1, e2)) belongs to an instrument phrase asidentified by a semantic role tool, such as ASSERT.The syntactic attachment feature (F18) is a fea-ture that indicates whether the argument identifiedas Instrument in the relation attaches to a verb or toa noun in the syntactically parsed sentence.3 Learning Model and ExperimentalSettingFor our experiments we chose libSVM, an opensource SVM package3.
Since some of our featuresare nominal, we followed the standard practice ofrepresenting a nominal feature with n discrete val-ues as n binary features.
We used the RBF kernel.We built a binary classifier for each of the sevenrelations.
Since the size of the task training data perrelation is small, we expanded it with new examplesfrom various sources.
We added a new corpus of3,000 sentences of news articles from the TREC-9text collection (Girju, 2003) encoding Cause-Effect(1,320) and Product-Producer (721).
Another col-3http://www.csie.ntu.edu.tw/?cjlin/libsvm/388Relation P R F Acc Total Base-F Base-Acc Best featuresCause-Effect 69.5 100.0 82.0 77.5 80 67.8 51.2 F1, F2, F5, F6, F12?F14Instrument-Agency 68.2 78.9 73.2 71.8 78 65.5 51.3 F7, F8, F10, F11, F15?F18Product-Producer 84.5 79.0 81.7 76.3 93 80.0 66.7 F1?F4, F12?F14Origin-Entity 86.4 52.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12?F14Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1?F6, F15, F16Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1?F4Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1?F6, F12?F14Average 79.7 69.8 72.4 76.3 78.4Table 2: Performance obtained per relation.
Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macro-averaged for system?s performance on all 7 relations.
Base-F shows the baseline F measure (all true), while Base-Acc shows thebaseline accuracy score (majority).lection of 3,129 sentences from Wall Street Journal(Moldovan et al, 2004; Girju et al, 2004) was con-sidered for Part-Whole (1,003), Origin-Entity (167),Product-Producer (112), and Theme-Tool (91).
Wealso extracted 552 Product-Producer instances fromeXtended WordNet4 (noun entries and their glossdefinition).
Moreover, for Theme-Tool and Content-Container we used special lists of constraints5.
Be-sides the selectional restrictions imposed on thenouns by special features such as F15 and F16 (psy-chological feature), we created lists of containersfrom various thesauri6 and identified selectional re-strictions that differentiate between containers andlocations relying on taxonomies of spatial entitiesdiscussed in detail in (Herskovits, 1987) and (Tylerand Evans, 2003).Each instance in this text collection had the tar-get nouns identified and annotated with WordNetsenses.
Since the annotations used different Word-Net versions, senses were mapped to sense keys.4 Experimental ResultsTable 2 shows the performance of our system foreach semantic relation.
Base-F indicates the base-line F-measure (all true), while Base-Acc shows thebaseline accuracy score (majority).
The Averagescore of precision, recall, F-measure, and accuracyis macroaveraged over all seven relations.
Overall,all features contributed to the performance, with adifferent contribution per relation (cf.
Table 2).5 ConclusionsThis paper describes a method for the automaticidentification of a set of seven semantic relations4http://xwn.hlt.utdallas.edu/5The Instrument-Agency classifier was trained only on thetask dataset.6Thesauri such as TheFreeDictionary.com.based on support vector machines (SVMs).
The ap-proach benefits from an extended dataset on whichbinary classifiers were trained for each relation.
Thefeature sets fed into the SVMs produced very goodresults.AcknowledgmentsWe would like to thank Brian Drexler for his valu-able suggestions on the set of semantic relations.ReferencesE.
Charniak.
2000.
A Maximum-entropy-inspired Parser.
Inthe Proceedings of the 1st NAACL Conference.R.
Girju, A. Giuglea, M. Olteanu, O. Fortu, O. Bolohan, andD.
Moldovan.
2004.
Support vector machines applied tothe classification of semantic relations in nominalized nounphrases.
In the Proceedings of the HLT/NAACL Workshopon Computational Lexical Semantics.R.
Girju, D. Moldovan, M. Tatu, and D. Antohe.
2005.
Onthe semantics of noun compounds.
Computer Speech andLanguage, 19(4):479?496.R.
Girju, A. Badulescu, and D. Moldovan.
2006.
Automaticdiscovery of part-whole relations.
Computational Linguis-tics, 32(1).R.
Girju.
2003.
Automatic detection of causal relations forquestion answering.
In the Proceedings of the ACL Work-shop on ?Multilingual Summarization and Question Answer-ing - Machine Learning and Beyond?.A.
Herskovits.
1987.
Language and spatial cognition: An in-terdisciplinary study of the prepositions in English.
Cam-bridge University Press.S.
Linstromberg.
1997.
English Prepositions Explained.
JohnBenjamins Publishing Co., Amsterdam/Philaderphia.D.
Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.2004.
Models for the semantic classification of nounphrases.
In the Proceedings of the HLT/NAACL Workshopon Computational Lexical Semantics.A.
Tyler and V. Evans.
2003.
The Semantics of English Prepo-sitions: Spatial Sciences, Embodied Meaning, and Cogni-tion.
Cambridge University Press.389
