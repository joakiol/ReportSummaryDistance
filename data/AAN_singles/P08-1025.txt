Proceedings of ACL-08: HLT, pages 209?217,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEfficient Multi-pass Decoding for Synchronous Context Free GrammarsHao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractWe take a multi-pass approach to ma-chine translation decoding when using syn-chronous context-free grammars as the trans-lation model and n-gram language models:the first pass uses a bigram language model,and the resulting parse forest is used in thesecond pass to guide search with a trigram lan-guage model.
The trigram pass closes mostof the performance gap between a bigram de-coder and a much slower trigram decoder, buttakes time that is insignificant in comparisonto the bigram pass.
An additional fast de-coding pass maximizing the expected countof correct translation hypotheses increases theBLEU score significantly.1 IntroductionStatistical machine translation systems basedon synchronous grammars have recently showngreat promise, but one stumbling block to theirwidespread adoption is that the decoding, or search,problem during translation is more computationallydemanding than in phrase-based systems.
This com-plexity arises from the interaction of the tree-basedtranslation model with an n-gram language model.Use of longer n-grams improves translation results,but exacerbates this interaction.
In this paper, wepresent three techniques for attacking this problemin order to obtain fast, high-quality decoders.First, we present a two-pass decoding algorithm,in which the first pass explores states resulting froman integrated bigram language model, and the sec-ond pass expands these states into trigram-basedstates.
The general bigram-to-trigram techniqueis common in speech recognition (Murveit et al,1993), where lattices from a bigram-based decoderare re-scored with a trigram language model.
We ex-amine the question of whether, given the reorderinginherent in the machine translation problem, lowerorder n-grams will provide as valuable a searchheuristic as they do for speech recognition.Second, we explore heuristics for agenda-basedsearch, and present a heuristic for our second passthat combines precomputed language model infor-mation with information derived from the first pass.With this heuristic, we achieve the same BLEUscores and model cost as a trigram decoder with es-sentially the same speed as a bigram decoder.Third, given the significant speedup in theagenda-based trigram decoding pass, we can rescorethe trigram forest to maximize the expected count ofcorrect synchronous constituents of the model, us-ing the product of inside and outside probabilities.Maximizing the expected count of synchronous con-stituents approximately maximizes BLEU.
We finda significant increase in BLEU in the experiments,with minimal additional time.2 Language Model Integrated Decodingfor SCFGWe begin by introducing Synchronous Context FreeGrammars and their decoding algorithms when ann-gram language model is integrated into the gram-matical search space.A synchronous CFG (SCFG) is a set of context-free rewriting rules for recursively generating stringpairs.
Each synchronous rule is a pair of CFG rules209with the nonterminals on the right hand side of oneCFG rule being one-to-one mapped to the other CFGrule via a permutation pi.
We adopt the SCFG nota-tion of Satta and Peserico (2005).
Superscript in-dices in the right-hand side of grammar rules:X ?
X(1)1 ...X(n)n , X(pi(1))pi(1) ...X(pi(n))pi(n)indicate that the nonterminals with the same indexare linked across the two languages, and will eventu-ally be rewritten by the same rule application.
EachXi is a variable which can take the value of any non-terminal in the grammar.In this paper, we focus on binary SCFGs andwithout loss of generality assume that only the pre-terminal unary rules can generate terminal stringpairs.
Thus, we are focusing on Inversion Transduc-tion Grammars (Wu, 1997) which are an importantsubclass of SCFG.
Formally, the rules in our gram-mar include preterminal unary rules:X ?
e/ffor pairing up words or phrases in the two languagesand binary production rules with straight or invertedorders that are responsible for building up upper-level synchronous structures.
They are straight ruleswritten:X ?
[Y Z]and inverted rules written:X ?
?Y Z?.Most practical non-binary SCFGs can be bina-rized using the synchronous binarization techniqueby Zhang et al (2006).
The Hiero-style rules of(Chiang, 2005), which are not strictly binary but bi-nary only on nonterminals:X ?
yu X(1) you X(2); have X(2) with X(1)can be handled similarly through either offline bi-narization or allowing a fixed maximum number ofgap words between the right hand side nonterminalsin the decoder.For these reasons, the parsing problems for morerealistic synchronous CFGs such as in Chiang(2005) and Galley et al (2006) are formally equiva-lent to ITG.
Therefore, we believe our focus on ITGfor the search efficiency issue is likely to generalizeto other SCFG-based methods.Without an n-gram language model, decoding us-ing SCFG is not much different from CFG pars-ing.
At each time a CFG rule is applied on the in-put string, we apply the synchronized CFG rule forthe output language.
From a dynamic programmingpoint of view, the DP states are X[i, j], where Xranges over all possible nonterminals and i and jrange over 0 to the input string length |w|.
Eachstate stores the best translations obtainable.
Whenwe reach the top state S[0, |w|], we can get the besttranslation for the entire sentence.
The algorithm isO(|w|3).However, when we want to integrate an n-gramlanguage model into the search, our goal is search-ing for the derivation whose total sum of weightsof productions and n-gram log probabilities ismaximized.
Now the adjacent span-parameterizedstates X[i, k] and X[k, j] can interact with eachother by ?peeping into?
the leading and trailingn ?
1 words on the output side for each state.Different boundary words differentiate the span-parameterized states.
Thus, to preserve the dynamicprogramming property, we need to refine the statesby adding the boundary words into the parameter-ization.
The LM -integrated states are representedas X[i, j, u1,..,n?1, v1,..,n?1].
Since the number ofvariables involved at each DP step has increased to3 + 4(n ?
1), the decoding algorithm is asymptoti-cally O(|w|3+4(n?1)).
Although it is possible to usethe ?hook?
trick of Huang et al (2005) to factor-ize the DP operations to reduce the complexity toO(|w|3+3(n?1)), when n is greater than 2, the com-plexity is still prohibitive.3 Multi-pass LM-Integrated DecodingIn this section, we describe a multi-pass progres-sive decoding technique that gradually augments theLM -integrated states from lower orders to higherorders.
For instance, a bigram-integrated state[X, i, j, u, v] is said to be a coarse-level state of atrigram-integrate state [X, i, j, u, u?, v?, v], becausethe latter state refines the previous by specifyingmore inner words.Progressive search has been used for HMM?s inspeech recognition (Murveit et al, 1993).
The gen-210eral idea is to use a simple and fast decoding algo-rithm to constrain the search space of a followingmore complex and slower technique.
More specif-ically, a bigram decoding pass is executed forwardand backward to figure out the probability of eachstate.
Then the states can be pruned based on theirglobal score using the product of inside and outsideprobabilities.
The advanced decoding algorithm willuse the constrained space (a lattice in the case ofspeech recognition) as a grammatical constraint tohelp it focus on a smaller search space on whichmore discriminative features are brought in.The same idea has been applied to forests for pars-ing.
Charniak and Johnson (2005) use a PCFG to doa pass of inside-outside parsing to reduce the statespace of a subsequent lexicalized n-best parsing al-gorithm to produce parses that are further re-rankedby a MaxEnt model.We take the same view as in speech recognitionthat a trigram integrated model is a finer-grainedmodel than bigram model and in general we can doan n ?
1-gram decoding as a predicative pass forthe following n-gram pass.
We need to do inside-outside parsing as coarse-to-fine parsers do.
How-ever, we use the outside probability or cost informa-tion differently.
We do not combine the inside andoutside costs of a simpler model to prune the spacefor a more complex model.
Instead, for a given finer-gained state, we combine its true inside cost withthe outside cost of its coarse-level counter-part toestimate its worthiness of being explored.
The useof the outside cost from a coarser-level as the out-side estimate makes our method naturally fall in theframework of A* parsing.Klein and Manning (2003) describe an A* pars-ing framework for monolingual parsing and admis-sible outside estimates that are computed using in-side/outside parsing algorithm on simplified PCFGscompared to the original PCFG.
Zhang and Gildea(2006) describe A* for ITG and develop admissibleheuristics for both alignment and decoding.
Bothhave shown the effectiveness of A* in situationswhere the outside estimate approximates the truecost closely such as when the sentences are short.For decoding long sentences, it is difficult to comeup with good admissible (or inadmissible) heuris-tics.
If we can afford a bigram decoding pass, theoutside cost from a bigram model is conceivably avery good estimate of the outside cost using a tri-gram model since a bigram language model and atrigram language model must be strongly correlated.Although we lose the guarantee that the bigram-passoutside estimate is admissible, we expect that it ap-proximates the outside cost very closely, thus verylikely to effectively guide the heuristic search.3.1 Inside-outside Coarse Level DecodingWe describe the coarse level decoding pass in thissection.
The decoding algorithms for the coarselevel and the fine level do not necessarily have tobe the same.
The fine level decoding algorithm is anA* algorithm.
The coarse level decoding algorithmcan be CKY or A* or other alternatives.Conceptually, the algorithm is finding the short-est hyperpath in the hypergraph in which the nodesare states like X[i, j, u1,..,n?1, v1,..,n?1], and the hy-peredges are the applications of the synchronousrules to go from right-hand side states to left-handside states.
The root of the hypergraph is a specialnode S?
[0, |w|, ?s?, ?/s?]
which means the entire in-put sentence has been translated to a string startingwith the beginning-of-sentence symbol and endingat the end-of-sentence symbol.
If we imagine a start-ing node that goes to all possible basic translationpairs, i.e., the instances of the terminal translationrules for the input, we are searching the shortest hy-per path from the imaginary bottom node to the root.To help our outside parsing pass, we store the back-pointers at each step of exploration.The outside parsing pass, however, starts from theroot S?
[|w|, ?s?, ?/s?]
and follows the back-pointersdownward to the bottom nodes.
The nodes need tobe visited in a topological order so that whenevera node is visited, its parents have been visited andits outside cost is over all possible outside parses.The algorithm is described in pseudocode in Algo-rithm 1.
The number of hyperedges to traverse ismuch fewer than in the inside pass because not ev-ery state explored in the bottom up inside pass canfinally reach the goal.
As for normal outside parsing,the operations are the reverse of inside parsing.
Wepropagate the outside cost of the parent to its chil-dren by combining with the inside cost of the otherchildren and the interaction cost, i.e., the languagemodel cost between the focused child and the otherchildren.
Since we want to approximate the Viterbi211outside cost, it makes sense to maximize over allpossible outside costs for a given node, to be con-sistent with the maximization of the inside pass.
Forthe nodes that have been explored in the bottom uppass but not in the top-down pass, we set their out-side cost to be infinity so that their exploration ispreferred only when the viable nodes from the firstpass have all been explored in the fine pass.3.2 Heuristics for Fine-grained DecodingIn this section, we summarize the heuristics for finerlevel decoding.The motivation for combining the true insidecost of the fine-grained model and the outside es-timate given by the coarse-level parsing is to ap-proximate the true global cost of a fine-grained stateas closely as possible.
We can make the approx-imation even closer by incorporating local higher-order outside n-gram information for a state ofX[i, j, u1,..,n?1, v1,..,n?1] into account.
We call thisthe best-border estimate.
For example, the best-border estimate for trigram states is:hBB(X, i, j, u1, u2, v1, v2)=[maxs?S(i,j)Plm(u2 | s, u1)]?
[maxs?S(i,j)Plm(s | v1, v2)]where S(i, j) is the set of candidate target languagewords outside the span of (i, j).
hBB is the prod-uct of the upper bounds for the two on-the-bordern-grams.This heuristic function was one of the admissibleheuristics used by Zhang and Gildea (2006).
Thebenefit of including the best-border estimate is to re-fine the outside estimate with respect to the innerwords which refine the bigram states into the trigramstates.
If we do not take the inner words into consid-eration when computing the outside cost, all statesthat map to the same coarse level state would havethe same outside cost.
When the simple best-borderestimate is combined with the coarse-level outsideestimate, it can further boost the search as will beshown in the experiments.
To summarize, our recipefor faster decoding is that using?
(X[i, j, u1,..,n?1, v1,..,n?1])+ ?
(X[i, j, u1, vn?1])+ hBB(X, i, j, u1,...,n, v1,...,n) (1)where ?
is the Viterbi inside cost and ?
is the Viterbioutside cost, to globally prioritize the n-gram inte-grated states on the agenda for exploration.3.3 Alternative Efficient Decoding AlgorithmsThe complexity of n-gram integrated decoding forSCFG has been tackled using other methods.The hook trick of Huang et al (2005) factor-izes the dynamic programming steps and lowers theasymptotic complexity of the n-gram integrated de-coding, but has not been implemented in large-scalesystems where massive pruning is present.The cube-pruning by Chiang (2007) and the lazycube-pruning of Huang and Chiang (2007) turn thecomputation of beam pruning of CYK decoders intoa top-k selection problem given two columns oftranslation hypotheses that need to be combined.The insight for doing the expansion top-down lazilyis that there is no need to uniformly explore everycell.
The algorithm starts with requesting the firstbest hypothesis from the root.
The request translatesinto requests for the k-bests of some of its childrenand grandchildren and so on, because re-ranking ateach node is needed to get the top ones.Venugopal et al (2007) also take a two-pass de-coding approach, with the first pass leaving the lan-guage model boundary words out of the dynamicprogramming state, such that only one hypothesis isretained for each span and grammar symbol.4 Decoding to Maximize BLEUThe ultimate goal of efficient decoding to find thetranslation that has a highest evaluation score usingthe least time possible.
Section 3 talks about utiliz-ing the outside cost of a lower-order model to esti-mate the outside cost of a higher-order model, boost-ing the search for the higher-order model.
By doingso, we hope the intrinsic metric of our model agreeswith the extrinsic metric of evaluation so that fastsearch for the model is equivalent to efficient decod-ing.
But the mismatch between the two is evident,as we will see in the experiments.
In this section,212Algorithm 1 OutsideCoarseParsing()for all X[i, j, u, v] in topological order dofor all children pairs pointed to by the back-pointers doif X ?
[Y Z] then the two children are Y [i, k, u, u?]
and Z[k, j, v?, v]?
(Y [i, k, u, u?])
= max {?
(Y [i, k, u, u?]),?
(X[i, j, u, v]) + ?
(Z[k, j, v?, v]) + rule(X ?
[Y Z]) + bigram(u?, v?)}?
(Z[k, j, v?, v]) = max {?
(Z[k, j, v?, v]),?
(X[i, j, u, v]) + ?
(Y [i, k, u, u?])
+ rule(X ?
[Y Z]) + bigram(u?, v?
)}end ifif X ?
?Y Z?
then the two children are Y [i, k, v?, v] and Z[k, j, u, u?]?
(Y [i, k, v?, v]) = max {?
(Y [i, k, v?, v]),?
(X[i, j, u, v]) + ?
(Z[k, j, u, u?])
+ rule(X ?
?Y Z?)
+ bigram(u?, v?)}?
(Z[k, j, u, u?])
= max {?
(Z[k, j, u, u?]),?
(X[i, j, u, v]) + ?
(Y [i, k, v?, v]) + rule(X ?
?Y Z?)
+ bigram(u?, v?
)}end ifend forend forwe deal with the mismatch by introducing anotherdecoding pass that maximizes the expected countof synchronous constituents in the tree correspond-ing to the translation returned.
BLEU is based onn-gram precision, and since each synchronous con-stituent in the tree adds a new 4-gram to the trans-lation at the point where its children are concate-nated, the additional pass approximately maximizesBLEU.Kumar and Byrne (2004) proposed the frameworkof Minimum Bayesian Risk (MBR) decoding thatminimizes the expected loss given a loss function.Their MBR decoding is a reranking pass over an n-best list of translations returned by the decoder.
Ouralgorithm is another dynamic programming decod-ing pass on the trigram forest, and is similar to theparsing algorithm for maximizing expected labelledrecall presented by Goodman (1996).4.1 Maximizing the expected count of correctsynchronous constituentsWe introduce an algorithm that maximizes the ex-pected count of correct synchronous constituents.Given a synchronous constituent specified by thestate [X, i, j, u, u?, v?, v], its probability of being cor-rect in the model isEC([X, i, j, u, u?, v?, v])= ?
([X, i, j, u, u?, v?, v]) ?
?
([X, i, j, u, u?, v?, v]),where ?
is the outside probability and ?
is the in-side probability.
We approximate ?
and ?
using theViterbi probabilities.
Since decoding from bottomup in the trigram pass already gives us the insideViterbi scores, we only have to visit the nodes inthe reverse order once we reach the root to computethe Viterbi outside scores.
The outside-pass Algo-rithm 1 for bigram decoding can be generalized tothe trigram case.
We want to maximize over alltranslations (synchronous trees) T in the forest af-ter the trigram decoding pass according tomaxT?
[X,i,j,u,u?,v?,v]?TEC([X, i, j, u, u?, v?, v]).The expression can be factorized and computed us-ing dynamic programming on the forest.5 ExperimentsWe did our decoding experiments on the LDC 2002MT evaluation data set for translation of Chinesenewswire sentences into English.
The evaluationdata set has 10 human translation references for eachsentence.
There are a total of 371 Chinese sentencesof no more than 20 words in the data set.
Thesesentences are the test set for our different versionsof language-model-integrated ITG decoders.
Weevaluate the translation results by comparing themagainst the reference translations using the BLEUmetric.213The word-to-word translation probabilities arefrom the translation model of IBM Model 4 trainedon a 160-million-word English-Chinese parallel cor-pus using GIZA++.
The phrase-to-phrase transla-tion probabilities are trained on 833K parallel sen-tences.
758K of this was data made available byISI, and another 75K was FBIS data.
The languagemodel is trained on a 30-million-word English cor-pus.
The rule probabilities for ITG are trained usingEM on a corpus of 18,773 sentence pairs with a to-tal of 276,113 Chinese words and 315,415 Englishwords.5.1 Bigram-pass Outside Cost as Trigram-passOutside EstimateWe first fix the beam for the bigram pass, and changethe outside heuristics for the trigram pass to showthe difference before and after using the first-passoutside cost estimate and the border estimate.
Wechoose the beam size for the CYK bigram pass to be10 on the log scale.
The first row of Table 1 showsthe number of explored hyperedges for the bigrampass and its BLEU score.
In the rows below, wecompare the additional numbers of hyperedges thatneed to be explored in the trigram pass using differ-ent outside heuristics.
It takes too long to finish us-ing uniform outside estimate; we have to use a tightbeam to control the agenda-based exploration.
Us-ing the bigram outside cost estimate makes a hugedifference.
Furthermore, using Equation 1, addingthe additional heuristics on the best trigrams that canappear on the borders of the current hypothesis, onaverage we only need to explore 2700 additional hy-peredges per sentence to boost the BLEU score from21.77 to 23.46.
The boost is so significant that over-all the dominant part of search time is no longer thesecond pass but the first bigram pass (inside pass ac-tually) which provides a constrained space and out-side heuristics for the second pass.5.2 Two-pass decoding versus One-passdecodingBy varying the beam size for the first pass, we canplot graphs of model scores versus search time andBLEU scores versus search time as shown in Fig-ure 1.
We use a very large beam for the second passdue to the reason that the outside estimate for thesecond pass is discriminative enough to guide theDecoding Method Avg.
Hyperedges BLEUBigram Pass 167K 21.77Trigram PassUNI ?
?BO + 629.7K=796.7K 23.56BO+BB +2.7K =169.7K 23.46Trigram One-pass,with Beam 6401K 23.47Table 1: Speed and BLEU scores for two-pass decoding.UNI stands for the uniform (zero) outside estimate.
BOstands for the bigram outside cost estimate.
BB stands forthe best border estimate, which is added to BO.Decoder Time BLEU Model ScoreOne-pass agenda 4317s 22.25 -208.849One-pass CYK 3793s 22.89 -207.309Multi-pass, CYK firstagenda second pass 3689s 23.56 -205.344MEC third pass 3749s 24.07 -203.878Lazy-cube-pruning 3746s 22.16 -208.575Table 2: Summary of different trigram decoding strate-gies, using about the same time (10 seconds per sen-tence).search.
We sum up the total number of seconds forboth passes to compare with the baseline systems.On average, less than 5% of time is spent in the sec-ond pass.In Figure 1, we have four competing decoders.bitri cyk is our two-pass decoder, using CYK asthe first pass decoding algorithm and using agenda-based decoding in the second pass which is guidedby the first pass.
agenda is our trigram-integratedagenda-based decoder.
The other two systems arealso one-pass.
cyk is our trigram-integrated CYKdecoder.
lazy kbest is our top-down k-best-style de-coder.1Figure 1(left) compares the search efficiencies ofthe four systems.
bitri cyk at the top ranks first.
cykfollows it.
The curves of lazy kbest and agenda cross1In our implementation of the lazy-cube-pruning based ITGdecoder, we vary the re-ranking buffer size and the the top-klist size which are the two controlling parameters for the searchspace.
But we did not use any LM estimate to achieve earlystopping as suggested by Huang and Chiang (2007).
Also, wedid not have a translation-model-only pruning pass.
So the re-sults shown in this paper for the lazy cube pruning method isnot of its best performance.214and are both below the curves of bitri cyk and cyk.This figure indicates the advantage of the two-passdecoding strategy in producing translations with ahigh model score in less time.However, model scores do not directly translateinto BLEU scores.
In Figure 1(right), bitri cyk isbetter than CYK only in a certain time window whenthe beam is neither too small nor too large.
Butthe window is actually where we are interested ?
itranges from 5 seconds per sentence to 20 secondsper sentence.
Table 2 summarizes the performanceof the four decoders when the decoding speed is at10 seconds per sentence.5.3 Does the hook trick help?We have many choices in implementing the bigramdecoding pass.
We can do either CYK or agenda-based decoding.
We can also use the dynamic pro-gramming hook trick.
We are particularly interestedin the effect of the hook trick in a large-scale systemwith aggressive pruning.Figure 2 compares the four possible combinationsof the decoding choices for the first pass: bitri cyk,bitri agenda, bitri cyk hook and bitri agenda hook.bitri cyk which simply uses CYK as the first passdecoding algorithm is the best in terms of perfor-mance and time trade-off.
The hook-based de-coders do not show an advantage in our experiments.Only bitri agenda hook gets slightly better than bi-tri agenda when the beam size increases.
So, it isvery likely the overhead of building hooks offsets itsbenefit when we massively prune the hypotheses.5.4 Maximizing BLEUThe bitri cyk decoder spends little time in theagenda-based trigram pass, quickly reaching thegoal item starting from the bottom of the chart.
Inorder to maximize BLEU score using the algorithmdescribed in Section 4, we need a sizable trigramforest as a starting point.
Therefore, we keep pop-ping off more items from the agenda after the goalis reached.
Simply by exploring more (200 timesthe log beam) after-goal items, we can optimize theViterbi synchronous parse significantly, shown inFigure 3(left) in terms of model score versus searchtime.However, the mismatch between model score andBLEU score persists.
So, we try our algorithmof maximizing expected count of synchronous con-stituents on the trigram forest.
We find signifi-cant improvement in BLEU, as shown in Figure 3(right) by the curve of bitri cyk epass me cons.
bi-tri cyk epass me cons beats both bitri cyk and cykin terms of BLEU versus time if using more than1.5 seconds on average to decode each sentence.
Ateach time point, the difference in BLEU betweenbitri cyk epass me cons and the highest of bitri cykand cyk is around .5 points consistently as we varythe beam size for the first pass.
We achieve therecord-high BLEU score 24.34 using on average 21seconds per sentence, compared to the next-highestscore of 23.92 achieved by cyk using on average 78seconds per sentence.6 ConclusionWe present a multi-pass method to speed up n-gram integrated decoding for SCFG.
We use an in-side/outside parsing algorithm to get the Viterbi out-side cost of bigram integrated states which is used asan outside estimate for trigram integrated states.
Thecoarse-level outside cost plus the simple estimate forborder trigrams speeds up the trigram decoding passhundreds of times compared to using no outside es-timate.Maximizing the probability of the synchronousderivation is not equivalent to maximizing BLEU.We use a rescoring decoding pass that maximizes theexpected count of synchronous constituents.
Thistechnique, together with the progressive search atprevious stages, gives a decoder that produces thehighest BLEU score we have obtained on the data ina very reasonable amount of time.As future work, new metrics for the final pass maybe able to better approximate BLEU.
As the bigramdecoding pass currently takes the bulk of the decod-ing time, better heuristics for this phase may speedup the system further.Acknowledgments This work was supported byNSF ITR-0428020 and NSF IIS-0546554.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In ACL.215-224-222-220-218-216-214-212-210-208-206-20410  100  1000  10000  100000logscoretotal secsbitri_cykcykagendalazy kbest0.170.180.190.20.210.220.230.2410  100  1000  10000  100000bleutotal secsbitri_cykcykagendalazy kbestFigure 1: We compare the two-pass ITG decoder with the one-pass trigram-integrated ITG decoders in terms of bothmodel scores vs. time (left) and BLEU scores vs. time (right).
The model score here is the log probability of thedecoded parse, summing up both the translation model and the language model.
We vary the beam size (for the firstpass in the case of two-pass) to search more and more thoroughly.-222-220-218-216-214-212-210-208-206-204100  1000  10000  100000logscoretotal secsbitri_cykbitri_cyk_hookbitri_agendabitri_agenda_hook0.170.180.190.20.210.220.230.24100  1000  10000  100000bleutotal secsbitri_cykbitri_cyk_hookbitri_agendabitri_agenda_hookFigure 2: We use different first-pass decoding algorithms, fixing the second pass to be agenda-based which is guidedby the outside cost of the first pass.
Left: model score vs. time.
Right: BLEU score vs. time.-222-220-218-216-214-212-210-208-206-204-202100  1000  10000  100000logscoretotal secsbitri_cyk delayed-stoppingbitri_cyk0.170.180.190.20.210.220.230.240.2510  100  1000  10000  100000bleutotal secsbitri_cyk_epass_me_consbitri_cykcykFigure 3: Left: improving the model score by extended agenda-exploration after the goal is reached in the best-firstsearch.
Right: maximizing BLEU by the maximizing expectation pass on the expanded forest.216David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Conference of the Association forComputational Linguistics (ACL-05), pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the International Conference on ComputationalLinguistics/Association for Computational Linguistics(COLING/ACL-06), pages 961?968, July.Joshua Goodman.
1996.
Parsing algorithms and metrics.In Proceedings of the 34th Annual Conference of theAssociation for Computational Linguistics (ACL-96),pages 177?183.Liang Huang and David Chiang.
2007.
Faster algorithmsfor decoding with integrated language models.
In Pro-ceedings of ACL, Prague, June.Liang Huang, Hao Zhang, and Daniel Gildea.
2005.Machine translation as lexicalized parsing with hooks.In International Workshop on Parsing Technologies(IWPT05), Vancouver, BC.Dan Klein and Christopher D. Manning.
2003.
A* pars-ing: Fast exact Viterbi parse selection.
In Proceed-ings of the 2003 Meeting of the North American chap-ter of the Association for Computational Linguistics(NAACL-03).Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In Daniel Marcu Susan Dumais and Salim Roukos,editors, HLT-NAACL 2004: Main Proceedings, pages169?176, Boston, Massachusetts, USA, May 2 - May7.
Association for Computational Linguistics.Hy Murveit, John W. Butzberger, Vassilios V. Digalakis,and Mitchel Weintraub.
1993.
Large-vocabulary dic-tation using SRI?s decipher speech recognition system:Progressive-search techniques.
In Proceedings of theIEEE International Conference on Acoustics, Speech,& Signal Processing (IEEE ICASSP-93), volume 2,pages 319?322.
IEEE.Giorgio Satta and Enoch Peserico.
2005.
Some com-putational complexity results for synchronous context-free grammars.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP), pages 803?810, Vancouver, Canada,October.Ashish Venugopal, Andreas Zollmann, and Stephan Vo-gel.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In NAACL07,Rochester, NY, April.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Hao Zhang and Daniel Gildea.
2006.
Efficient search forinversion transduction grammar.
In 2006 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), Sydney.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the 2006 Meeting of theNorth American chapter of the Association for Com-putational Linguistics (NAACL-06), pages 256?263.217
