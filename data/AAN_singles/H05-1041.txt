Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 323?330, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Practically Unsupervised Learning Method to Identify Single-SnippetAnswers to Definition Questions on the WebIon Androutsopoulos and Dimitrios GalanisDepartment of InformaticsAthens University of Economics and BusinessPatission 76, GR-104 34, Athens, GreeceAbstractWe present a practically unsupervisedlearning method to produce single-snippetanswers to definition questions in ques-tion answering systems that supplementWeb search engines.
The method exploitson-line encyclopedias and dictionaries togenerate automatically an arbitrarily largenumber of positive and negative definitionexamples, which are then used to train anSVM to separate the two classes.
We showexperimentally that the proposed methodis viable, that it outperforms the alterna-tive of training the system on questionsand news articles from TREC, and that ithelps the search engine handle definitionquestions significantly better.1 IntroductionQuestion answering (QA) systems for document col-lections typically aim to identify in the collectionstext snippets (e.g., 50 or 250 characters long) or ex-act answers (e.g., names, dates) that answer natu-ral language questions submitted by their users.
Al-though they are commonly evaluated on newspaperarchives, as in the TREC QA track, QA systems canalso supplement Web search engines, to help themreturn snippets, as opposed to Web pages, that pro-vide more directly the information users require.Most current QA systems first classify the inputquestion into one of several categories (e.g., ques-tions asking for locations, persons, etc.
), producingexpectations for types of named entities that mustbe present in the answer (locations, person names,etc.).
Using the question?s terms as a query, an infor-mation retrieval (IR) system identifies relevant doc-uments.
Snippets of these documents are then se-lected and ranked, using criteria such as whether ornot they contain the expected types of named enti-ties, the percentage of the question?s terms they con-tain, etc.
The system then outputs the most highly-ranked snippets, or named entities therein.The approach highlighted above performs poorlywith definition questions (e.g., ?What is gasohol?
?,?Who was Duke Ellington??
), because definitionquestions do not generate expectations for particulartypes of named entities, and they typically containonly a single term.
Definition questions are particu-larly common; in the QA track of TREC-2001, wherethe distribution of question types reflected that ofreal user logs, 27% of the questions were requestsfor definitions.
Of course, answers to many defini-tion questions can be found in on-line encyclopediasand dictionaries.1 There are always, however, newor less widely used terms that are not included insuch resources, and this is also true for many namesof persons and products.
Hence, techniques to dis-cover definitions in ordinary Web pages and otherdocument collections are valuable.
Definitions ofthis kind are often ?hidden?
in oblique contexts (e.g.,?He said that gasohol, a mixture of gasoline andethanol, has been great for his business.?
).In recent work, Miliaraki and Androutsopoulos(2004), hereafter M&A, proposed a method we call1See, for example, Wikipedia (http://www.wikipedia.org/).WordNet?s glosses are another on-line source of definitions.323DEFQA, which handles definition questions.
Themethod assumes that a question preprocessor sep-arates definition from other types of questions, andthat in definition questions this module also identi-fies the term to be defined, called the target term.2The input to DEFQA is a (possibly multi-word) tar-get term, along with the r most highly ranked docu-ments that an IR system returned for that term.
Theoutput is a list of k 250-character snippets from ther documents, at least one of which must contain anacceptable short definition of the target term, muchas in the QA track of TREC-2000 and TREC-2001.3We note that since 2003, TREC requires defini-tion questions to be answered by lists of comple-mentary snippets, jointly providing a range of in-formation nuggets about the target term (Voorhees,2003).
In contrast, here we focus on locating single-snippet definitions.
We believe this task is still in-teresting and of practical use.
For example, a listof single-snippet definitions accompanied by theirsource URLs is a good starting point for users ofsearch engines wishing to obtain definitions.
Single-snippet definitions can also be useful in informationextraction, where the templates to be filled in oftenrequire short entity descriptions.
We also note thatthe post-2003 TREC task has encountered evaluationproblems, because it is difficult to agree on whichnuggets should be included in the multi-snippet def-initions (Hildebrandt et al, 2004).
In contrast, ourexperimental results of Section 4 indicate stronginter-assessor agreement for single-snippet answers,suggesting that it is easier to agree upon what con-stitutes an acceptable single-snippet definition.DEFQA relies on an SVM, which is trained to clas-sify 250-character snippets that have the target termat their centre, hereafter called windows, as accept-able definitions or non-definitions.4 To train theSVM, a collection of q training target terms is used;M&A used the target terms of definition questionsfrom TREC-2000 and TREC-2001.
The terms aresubmitted to an IR system, which returns the r most2Alternatively, the user can be asked to specify explicitly thequestion type and target term via a form-based interface.3Definition questions were not considered in TREC-2002.4See, for example, Scholkopf and Smola (2002) for in-formation on SVMs.
Following M&A, we use a linear SVM,as implemented by Weka?s SMO class (http://www.cs.waikato.ac.nz/ml/weka/).
The windows may be shorter than 250 charac-ters, when the surrounding text is limited.highly ranked documents per target term.
The win-dows of the q ?
r resulting documents are taggedas acceptable definitions or non-definitions, and be-come the training instances of the SVM.
At run time,when a definition question is submitted, the r top-ranked documents are obtained, their windows arecollected, and for each window the SVM returns ascore indicating how confident it is that the windowis a definition.
The k windows with the highest con-fidence scores are then reported to the user.The SVM actually operates on vector representa-tions of the windows, that comprise the verdicts orattributes of previous methods by Joho and Sander-son (2000) and Prager et al (2002), as well asattributes corresponding to automatically acquiredlexical patterns.
On TREC-2000 and TREC-2001data, M&A found that DEFQA clearly outperformedthe original methods of Joho and Sanderson andPrager et al Their best configuration answered cor-rectly 73% of 160 definition questions in a cross-validation experiment with k = 5, r = 50, q = 160.A limitation of DEFQA is that it cannot be trainedeasily on new document collections, because it re-quires the training windows to be tagged as defini-tions or non-definitions.
In the experiments of M&A,there were 18,473 training windows.
Tagging themwas easy, because the windows were obtained fromTREC questions and documents, and the TREC or-ganizers provide Perl patterns that can be used tojudge whether a snippet from TREC?s documents isamong the acceptable answers of a TREC question.5For non-TREC questions and document collections,however, where such patterns are unavailable, sep-arating thousands of training windows into the twocategories by hand is a laborious task.In this paper, we consider the case where DE-FQA is used as an add-on to a Web search engine.There are three training alternatives in this setting:(i) train DEFQA on TREC questions and documents;(ii) train DEFQA on a large collection of manuallytagged training windows obtained from Web pagesthat the search engine returned for training targetterms; or (iii) devise techniques to tag automaticallythe training windows of (ii).
We have developed atechnique along alternative (iii), which exploits on-5The patterns?
judgements are not always perfect, which in-troduces some noise in the training examples.324line encyclopedias and dictionaries.
This allows usto generate and tag automatically an arbitrarily largenumber of training windows, in effect convertingDEFQA to an unsupervised method.
We show ex-perimentally that the new unsupervised method isviable, that it outperforms alternative (i), and that ithelps the search engine handle definition questionssignificantly better than on its own.2 Attributes of DEFQADEFQA represents each window as a vector compris-ing the values of the following attributes:6SN: The ordinal number of the window in thedocument, in our case Web page, it originates from.The intuition is that windows that mention the targetterm first in a document are more likely to define it.WC: What percentage of the 20 words that aremost frequent across all the windows of the tar-get term are present in the particular window rep-resented by the vector.
A stop-list and a stemmer areapplied first when computing WC .7 In effect, the 20most frequent words form a simplistic centroid of allthe candidate answers, and WC measures how closethe vector?s window is to this centroid.RK: The ranking of the Web page the windoworiginates from, as returned by the search engine.Manual patterns: 13 binary attributes, each sig-nalling whether or not the window matches a differ-ent manually constructed lexical pattern (e.g., ?tar-get, a/an/the?, as in ?Tony Blair, the British primeminister?).
The patterns are those used by Joho andSanderson, and four more added by M&A.
They areintended to perform well across text genres.Automatic patterns: A collection of m binary at-tributes, each showing if the window matches a dif-ferent automatically acquired lexical pattern.
Thepatterns are sequences of n tokens (n ?
{1, 2, 3})that must occur either directly before or directly af-ter the target term (e.g., ?target, which is?).
Thesepatterns are acquired as follows.
First, all the n-grams that occur directly before or after the targetterms in the training windows are collected.
The n-grams that have been encountered at least 10 timesare considered candidate patterns.
From those, the6SN and WC originate from Joho and Sanderson (2000).7We use the 100 most frequent words of the British NationalCorpus as the stop-list, and Porter?s stemmer.m patterns with the highest precision scores are re-tained, where precision is the number of trainingdefinition windows the pattern matches divided bythe total number of training windows it matches.
Weset m to 200, the value that led to the best resultsin the experiments of M&A.
The automatically ac-quired patterns allow the system to detect definitioncontexts that are not captured by the manual pat-terns, including genre-specific contexts.M&A also explored an additional attribute, whichcarried the verdict of Prager et al?s WordNet-basedmethod (2002).
However, they found the additionalattribute to lead to no significant improvements, and,hence, we do not use it.
We have made no attempt toextend the attribute set of M&A; for example, withattributes showing if the window contains the targetterm in italics, if the window is part of a list thatlooks like a glossary, or if the window derives froman authority Web page.
We leave such extensionsfor future work.
Our contribution is the automaticgeneration of training examples.3 Generating training examplesWhen training DEFQA on windows from Web pages,a mechanism to tag the training windows as defi-nitions or non-definitions is required.
Rather thantagging them manually, we use a measure of howsimilar the wording of each training window is tothe wording of definitions of the same target termobtained from on-line encyclopedias and dictionar-ies.
This is possible because we pick training targetterms for which there are several definitions in dif-ferent on-line encyclopedias and dictionaries; here-after we call these encyclopedia definitions.8 Train-ing windows whose wording is very similar to thatof the corresponding encyclopedia definitions aretagged as definition windows (positive examples),while windows whose wording differs significantlyfrom the encyclopedia definitions are tagged as non-definitions (negative examples).
Training windowsfor which the similarity score does not indicate greatsimilarity or dissimilarity to the wording of the en-cyclopedia definitions are excluded from DEFQA?s8We use randomly selected entries from the index ofhttp://www.encyclopedia.com/ as training terms, and Google?s?define:?
feature, that returns definitions from on-line encyclo-pedias and dictionaries, to obtain the encyclopedia definitions.325training, as they cannot be tagged as positive or neg-ative examples with sufficiently high confidence.Note that encyclopedia definitions are used onlyto tag training windows.
Once the system has beentrained, it can be used to discover on ordinary Webpages definitions of terms for which there are no en-cyclopedia definitions, and indeed this is the mainpurpose of the system.
Note also that we train DE-FQA on windows obtained from Web pages returnedby the search engine for training terms.
This allowsit to learn characteristics of the particular search en-gine being used; for example, what weight to as-sign to RK , depending on how much the searchengine succeeds in ranking pages containing defi-nitions higher.
More importantly, it allows DEFQAto select lexical patterns that are indicative of def-initions in Web pages, as opposed to patterns thatare indicative of definitions in electronic encyclope-dias and dictionaries.
The latter explains why wedo not train DEFQA directly on encyclopedia defi-nitions; another reason is that DEFQA requires bothpositive and negative examples, while encyclopediadefinitions provide only positive ones.We now explain how we compute the similar-ity between a training window and the collectionC of encyclopedia definitions for the window?s tar-get term.
We first remove stop-words, punctua-tion, other non-alphanumeric characters and the tar-get term from the training window, and apply a stem-mer, leading to a new form W of the training win-dow.
We then compute the similarity of W to C as:sim(W,C) = 1/|W | ?
?|W |i=1sim(wi, C)where |W | is the number of distinct words in W , andsim(wi, C) is the similarity of the i-th distinct wordof W to C, defined as follows:sim(wi, C) = fdef (wi, C) ?
idf (wi)fdef (wi, C) is the percentage of definitions in C thatcontain wi, and idf (wi) is the inverse document fre-quency of wi in the British National Corpus (BNC):idf (wi) = 1 + logNdf(wi)N is the number of documents in BNC, and df (wi)the number of BNC documents where wi occurs; ifwi does not occur in BNC, we use the lowest df scoreof BNC.
sim(wi, C) is highest for words that oc-cur in all the encyclopedia definitions and are usedrarely in English.
A training window with a largeproportion of such words most probably defines thetarget term.
More formally, given two thresholds t+and t?
with t?
?
t+, we tag W as a positive ex-ample if sim(W,C) ?
t+, as a negative example ifsim(W,C) ?
t?, and we exclude it from the train-ing of DEFQA if t?
< sim(W,C) < t+.
Hereafter,we refer to this method of generating training exam-ples as the similarity method.To select reasonable values for t+ and t?, we con-ducted a preliminary experiment for t?
= t+ = t;i.e., both thresholds were set to the same value tand no training windows were excluded.
We usedq = 130 training target terms from TREC definitionquestions, for which we had multiple encyclopediadefinitions.
For each term, we collected the r = 10most highly ranked Web pages.9 To alleviate theclass imbalance problem, whereby the positive ex-amples (definitions) are much fewer than the nega-tive ones (non-definitions), we kept only the first 5windows from each Web page (SN ?
5), based onthe observation that windows with great SN scoresare almost certainly non-definitions; we do the samein the training stage of all the experiments of this pa-per, and at run-time, when looking for windows toreport, we ignore windows with SN > 5.
From theresulting collection of training windows, we selectedrandomly 400 windows, and tagged them both man-ually and via the similarity method, with t rangingfrom 0 to 1.
Figures 1 and 2 show the precision andrecall of the similarity method on positive and neg-ative training windows, respectively, for varying t.Here, positive precision is the percentage of trainingwindows the similarity method tagged as positiveexamples (definitions) that were indeed positive; thetrue classes of the training windows were taken to bethose assigned by the human annotators.
Positive re-call is the percentage of truly positive examples thatthe similarity method tagged as positive.
Negativeprecision and negative recall are defined similarly.Figures 1 and 2 indicate that there is no singlethreshold t that achieves both high positive preci-sion and high negative precision.
To be confident9In all our experiments, we used the Altavista search engine.32600.20.40.60.810 0.2 0.4 0.6 0.8 1similarity thresholdprecision-recallprecision of positive examplesrecall of positive examplesFigure 1: Positive precision and recallthat the training windows the similarity method willtag as positive examples are indeed positive (highpositive precision), one has to set t close to 1; and tobe confident that the training windows the similar-ity method will tag as negative examples are indeednegative (high negative precision), t has to be setclose to 0.
This is why we use two separate thresh-olds and discard the training windows whose simi-larity score is between t?
and t+.
Figures 1 and 2also indicate that in both positive and negative ex-amples the similarity method achieves perfect pre-cision only at the cost of very low recall; i.e., if weinsist that all the resulting training examples musthave been tagged correctly (perfect positive and neg-ative precision), the resulting examples will be veryfew (low positive and negative recall).
There is alsoanother consideration when selecting t?
and t+: theratio of positive to negative examples that the sim-ilarity method generates must be approximately thesame as the true ratio before discarding any trainingwindows, in order to avoid introducing an artificialbias in the training of DEFQA?s SVM; the true ratioamong the 400 training windows before discardingany windows was approximately 0.37 : 1.Based on the considerations above, in the remain-ing experiments of this paper we set t+ to 0.5.
InFigure 1, this leads to a positive precision of 0.72(and positive recall 0.49), which does not improvemuch by adopting a larger t+, unless one is willingto set t+ at almost 1 at the price of very low posi-tive recall.
In the case of t?, setting it to any valueless than 0.34 leads to a negative precision above0.9, though negative recall drops sharply as t?
ap-proaches 0 (Figure 2).
For example, setting t?
to00.20.40.60.810 0.2 0.4 0.6 0.8 1similarity thresholdprecision-recallprecision of negative examplesrecall of negative examplesFigure 2: Negative precision and recall0.32, leads to 0.92 negative precision, 0.75 negativerecall, and approximately the same positive to nega-tive ratio (0.31 : 1) as the true observed ratio.
In theexperiments of Section 4, we keep t+ fixed to 0.5,and set t?
to the value in the range (0, 0.34) thatleads to the positive to negative ratio that is closestto the true ratio we observed in the 400 windows.The high negative precision we achieve (> 0.9)suggests that the resulting negative examples are al-most always truly negative.
In contrast, the lowerpositive precision (0.72) indicates that almost one inevery four resulting positive examples is in reality anon-definition.
This is a point where our similaritymethod needs to be improved; we return to this pointin Section 6.
Our experiments, however, show thatdespite this noise, the similarity method already out-performs the alternative of training DEFQA on TRECdata.
Note also that once the thresholds have beenselected, we can generate automatically an arbitrar-ily large set of training examples, by starting with asufficiently large number q of training terms to com-pensate for discarded training examples.4 EvaluationWe tested two different forms of DEFQA.
The firstone, dubbed DEFQAt , was trained on the q = 160definition questions of TREC-2000 and TREC-2001and the corresponding TREC documents, resulting in3,800 training windows.10 The second form of DE-10For each question, the TREC organizers provide the 50 mosthighly ranked documents that an IR engine returned from theTREC document collection.
We keep the top r = 10 of thesedocuments, while M&A kept all 50.
Furthermore, as discussedin Section 3, we retain up to the first 5 windows from each doc-327FQA, dubbed DEFQAs , was trained via the similaritymethod, with q = 480 training target terms, leadingto 7,200 training windows; as discussed in Section 3,one of the advantages of the similarity method is thatone can generate an arbitrarily large set of trainingwindows.
As in the preliminary experiment of Sec-tion 3, r (Web pages per target term) was set to 10in both systems.
To simplify the evaluation and testDEFQA in a more demanding scenario, we set k to1, i.e., the systems were allowed to return only onesnippet per question, as opposed to the more lenientk = 5 in the experiments of M&A.We also wanted a measure of how well DEFQAtand DEFQAs perform compared to a search engineon its own.
For this purpose, we compared the per-formance of the two systems to that of a baseline,dubbed BASE1 , which always returns the first win-dow of the Web page the search engine ranked first.In a search engine that highlights question termsin the returned documents, the snippet returned byBASE1 is presumably the first snippet a user wouldread hoping to find an acceptable definition.
Tostudy how much DEFQAt and DEFQAs improve uponrandom behaviour, we also compared them to a sec-ond baseline, BASEr , which returns a randomly se-lected window among the first five windows of all rWeb pages returned by the search engine.All four systems were evaluated on 81 unseen tar-get terms.
Their responses were judged indepen-dently by two human assessors, who had to markeach response as containing an acceptable short def-inition or not.
As already pointed out, DEFQAt andDEFQAs consult encyclopedia definitions only dur-ing training, and at run time the systems are in-tended to be used with terms for which no ency-clopedia definitions are available.
During this eval-uation, however, we deliberately chose the 81 testterms from the index of an on-line encyclopedia.This allowed us to give the encyclopedia?s defini-tions to the assessors, to help them judge the accept-ability of the single-snippet definitions the systemslocated on Web pages; many terms where related to,for example, medicine or biology, and without theencyclopedia?s definitions the assessors would notbe aware of their meanings.
The following is a snip-pet returned correctly by DEFQAs for ?genome?:ument.
This is why we have fewer training windows than M&A.discipline comparative genomics functional genomics bioinfor-matics the emergence of genomics as a discipline in 1920 , theterm genome was proposed to denote the totality of all genes onall chromosomes in the nucleus of a cell .
biology has.
.
.while what follows is a non-definition snippet re-turned wrongly by BASE1 :what is a genome national center for biotechnology informationabout ncbi ncbi at a glance a science primer databases.
.
.The examples illustrate the nature of the snippetsthat the systems and assessors had to consider.
Thesnippets often contain phrases that acted as links inthe original pages, or even pieces of programmingscripts that our rudimental preprocessing failed toremove.
(We remove only HTML tags, and applya simplistic tokenizer.)
Nevertheless, in most casesthe assessors had no trouble agreeing whether ornot the resulting snippets contained acceptable shortdefinitions.
KCo was 0.80, 0.81, 0.90, 0.89, and0.86 in the assessment of the responses of DEFQAs ,DEFQAt , BASEr , BASE1 , and all responses, respec-tively, indicating strong inter-assessor agreement.11The agreement was slightly lower in DEFQAs andDEFQAt , because there were a few marginally ac-ceptable or truncated definitions the assessors wereuncertain about.
There were also 4 DEFQAs answersand 3 BASE1 answers that defined secondary mean-ings of the target terms; e.g., apart from a kind oflizard, ?gecko?
is also the name of a graphics engine,and ?Exodus?
is also a programme for ex-offenders.Such answers were counted as wrong, though thismay be too strict.
With a larger k, there would bespace to return both the main and secondary mean-ings, and the evaluation could require this.Table 1 shows that DEFQAs answered correctlyapproximately 6 out of 10 definition questions.
Thisis lower than the score reported by M&A (73%),but remarkably high given that in our evaluationthe systems were allowed to return only one snip-pet per question; i.e., the task was much harder thanin M&A?s experiments.
DEFQAs answered correctlymore than twice as many questions as DEFQAt , de-spite the fact that its training data contained a lot ofnoise.
(Single-tailed difference-of-proportions testsshow that all the differences of Table 1 are statisti-11We follow the notation of Di Eugenio and Glass (2004).The KS&C figures were identical.
The 2 ?
P (A) ?
1 figureswere 0.80, 0.85, 0.95, 0.95, and 0.89 respectively.328assessor 1 assessor 2 averageBASEr 14.81 (12) 14.81 (12) 14.81 (12)BASE1 14.81 (12) 12.35 (10) 13.58 (11)DEFQAt 25.93 (21) 25.93 (21) 25.93 (21)DEFQAs 55.56 (45) 60.49 (49) 58.02 (47)Table 1: Percentage of questions answered correctlycally significant at ?
= 0.001.)
The superiority ofDEFQAs appears to be mostly due to its automati-cally acquired patterns.
DEFQAt too was able to ac-quire several good patterns (e.g., ?by target?, ?knownas target?, ?target, which is?, ?target is used in?
), butits pattern set alo comprises a large number of irrel-evant n-grams; this had also been observed by M&A.In contrast, the acquired pattern set of DEFQAs ismuch cleaner, with much fewer irrelevant n-grams,which is probably due to the largest, almost double,number of training windows.
Furthermore, the pat-tern set of DEFQAs contains many n-grams that areindicative of definitions on the Web.
For example,many Web pages that define terms contain text of theform ?What is a target?
A target is.
.
.
?, and DEFQAshas discovered patterns of the form ?what is a/an/thetarget?, ??
A/an/the target?, etc.
It has also discov-ered patterns like ?FAQ target?, ?home page target?,?target page?
etc., that seem to be good indicationsof Web windows containing definitions.Overall, DEFQA?s process of acquiring lexical pat-terns worked better in DEFQAs than in DEFQAt , andwe believe that the performance of DEFQAs could beimproved further by acquiring more than 200 pat-terns; we hope to investigate this in future work,along with an investigation of how the performanceof DEFQAs relates to q, the number of training targetterms.
Finally, note that the scores of both baselinesare very poor, indicating that DEFQAs performs sig-nificantly better than picking the first, or a randomsnippet among those returned by the search engine.5 Related workDefinition questions have recently attracted severalQA researchers.
Many of the proposed approaches,however, rely on manually crafted patterns or heuris-tics to identify definitions, and do not employ learn-ing algorithms (Liu et al, 2003; Fujii and Ishikawa,2004; Hildebrandt et al, 2004; Xu et al, 2004).Ng et al (2001) use machine learning (C5 withboosting) to classify and rank candidate answers ina general QA system, but they do not treat defi-nition questions in any special way; consequently,their worst results are for ?What.
.
.
??
questions,that presumably include definition questions.
Itty-cheriah and Roukos (2002) employ a maximum en-tropy model to rank candidate answers in a general-purpose QA system.
Their maximum entropy modeluses a very rich set of attributes, that includes 8,500n-gram patterns.
Unlike our work, their n-grams arefive or more words long, they are coupled to two-word question prefixes, and, in the case of definitionquestions, they do not need to be anchored at the tar-get term.
The authors, however, do not provide sep-arate performance figures for definition questions.Blair-Goldensohn et al (2003) focus on defini-tion questions, but aim at producing coherent multi-snippet definitions, rather than single-snippet defi-nitions.
The heart of their approach is a compo-nent that uses machine learning (Ripper) to identifysentences that can be included in the multi-sentencedefinition.
This component plays a role similar tothat of our SVM, but it is intended to admit a largerrange of sentences, and appears to employ only at-tributes conveying the ordinal number of the sen-tence in its document and the frequency of the targetterm in the sentence?s context.Since TREC-2003, several researchers have pro-posed ways to generate multi-snippet definitions(Cui et al, 2004; Fujii and Ishikawa, 2004; Hilde-brandt et al, 2004; Xu et al, 2004).
The typicalapproach is to locate definition snippets, much asin our work, and then report definition snippets thatare sufficiently different; most of the proposals usesome form of clustering to avoid reporting redun-dant snippets.
Such methods could also be appliedto DEFQA, to extend it to the post-2003 TREC task.On-line encyclopedias and dictionaries have beenused to handle definition questions in the past, butnot as in our work.
Hildebrandt et al (2004) look uptarget terms in encyclopedias and dictionaries, andthen, knowing the answers, try to find supportingevidence for them in the TREC document collection.Xu et al (2004) collect from on-line encyclopediasand dictionaries words that co-occur with the tar-get term; these words and their frequencies are thenused as a centroid of the target term, and candidate329answers are ranked by computing their similarity tothe centroid.
This is similar to our WC attribute.Cui et al (2004) also employ a form of centroid,comprising words that co-occur with the target term.The similarity to the centroid is taken into consider-ation when ranking candidate answers, but it is alsoused to generate training examples for a learningcomponent that produces soft patterns, in the sameway that we use the similarity method to producetraining examples for the SVM.
As in our work, thetraining examples that the centroid generates maybe noisy, but the component that produces soft pat-terns manages to generalize over the noise.
To thebest of our knowledge, this is the only other unsu-pervised learning approach for definition questionsthat has been proposed.
We hope to compare thetwo approaches experimentally in future work.
Forthe moment, we can only point out that Cui et al?scentroid approach generates only positive examples,while our similarity method generates both positiveand negative ones; this allows us to use a principledSVM learner, as opposed to Cui et al?s more ad hocsoft patterns that incorporate only positive examples.6 Conclusions and future workWe presented an unsupervised method to learn to lo-cate single-snippet answers to definition questionsin QA systems that supplement Web search en-gines.
The method exploits on-line encyclopediasand dictionaries to generate automatically an arbi-trarily large number of positive and negative defini-tion examples, which are then used to train an SVMto separate the two classes.
We have shown experi-mentally that the proposed method is viable, that itoutperforms training the QA system on TREC data,and that it helps the search engine handle definitionquestions significantly better than on its own.We have already pointed out the need to improvethe positive precision of the training examples.
Oneway may be to combine our similarity method withCui et al?s centroids.
We also plan to study the ef-fect of including more automatically acquired pat-terns and using more training target terms.
Finally,our method can be improved by including attributesfor the layout and authority of Web pages.ReferencesS.
Blair-Goldensohn, K.R.
McKeown, and A.H. Schlaik-jer.
2003.
A hybrid approach for answering defi-nitional questions.
Technical Report CUCS-006-03,Columbia University.H.
Cui, M.-Y.
Kan, and T.-S. Chua.
2004.
Unsupervisedlearning of soft patterns for generating definitions fromonline news.
In Proceedings of WWW-2004, pages90?99, New York, NY.B.
Di Eugenio and M. Glass.
2004.
The kappa statistic:A second look.
Comput.
Linguistics, 30(1):95?101.A.
Fujii and T. Ishikawa.
2004.
Summarizing encyclo-pedic term descriptions on the Web.
In Proceedings ofCOLING-2004, pages 645?651, Geneva, Switzerland.W.
Hildebrandt, B. Katz, and J. Lin.
2004.
An-swering definition questions using multiple knowledgesources.
In Proceedings of HLT-NAACL 2004, pages49?56, Boston, MA.A.
Ittycheriah and S. Roukos.
2002.
IBM?s statisticalquestion answering system ?
TREC-11.
In Proceed-ings of TREC-2002.H.
Joho and M. Sanderson.
2000.
Retrieving descriptivephrases from large amounts of free text.
In Proc.
of the9th ACM Conference on Information and KnowledgeManagement, pages 180?186, McLean, VA.B.
Liu, C.W.
Chin, and H.T.
Ng.
2003.
Mining topic-specific concepts and definitions on the Web.
In Pro-ceedings of WWW-2003, Budapest, Hungary.S.
Miliaraki and I. Androutsopoulos.
2004.
Learning toidentify single-snippet answers to definition questions.In Proceedings of COLING-2004, pages 1360?1366,Geneva, Switzerland.H.T.
Ng, J.L.P.
Kwan, and Y. Xia.
2001.
Question an-swering using a large text database: A machine learn-ing approach.
In Proceedings of EMNLP-2001, pages67?73, Pittsburgh, PA.J.
Prager, J. Chu-Carroll, and K. Czuba.
2002.
Use ofWordNet hypernyms for answering what-is questions.In Proceedings of TREC-2001.B.
Scholkopf and A. Smola.
2002.
Learning with ker-nels.
MIT Press.E.M.
Voorhees.
2003.
Evaluating answers to definitionquestions.
In Proceedings of HLT-NAACL 2003, pages109?111, Edmonton, Canada.J.
Xu, R. Weischedel, and A. Licuanan.
2004.
Eval-uation of an extraction-based approach to answeringdefinitional questions.
In Proceedings of SIGIR-2004,pages 418?424, Sheffield, U.K.330
