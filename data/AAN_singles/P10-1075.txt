Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 729?738,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDetecting Errors in Automatically-Parsed Dependency RelationsMarkus DickinsonIndiana Universitymd7@indiana.eduAbstractWe outline different methods to detect er-rors in automatically-parsed dependencycorpora, by comparing so-called depen-dency rules to their representation in thetraining data and flagging anomalous ones.By comparing each new rule to every rel-evant rule from training, we can identifyparts of parse trees which are likely erro-neous.
Even the relatively simple methodsof comparison we propose show promisefor speeding up the annotation process.1 Introduction and MotivationGiven the need for high-quality dependency parsesin applications such as statistical machine transla-tion (Xu et al, 2009), natural language generation(Wan et al, 2009), and text summarization evalu-ation (Owczarzak, 2009), there is a correspondingneed for high-quality dependency annotation, forthe training and evaluation of dependency parsers(Buchholz and Marsi, 2006).
Furthermore, pars-ing accuracy degrades unless sufficient amountsof labeled training data from the same domainare available (e.g., Gildea, 2001; Sekine, 1997),and thus we need larger and more varied anno-tated treebanks, covering a wide range of domains.However, there is a bottleneck in obtaining an-notation, due to the need for manual interven-tion in annotating a treebank.
One approach isto develop automatically-parsed corpora (van No-ord and Bouma, 2009), but a natural disadvantagewith such data is that it contains parsing errors.Identifying the most problematic parses for humanpost-processing could combine the benefits of au-tomatic and manual annotation, by allowing a hu-man annotator to efficiently correct automatic er-rors.
We thus set out in this paper to detect errorsin automatically-parsed data.If annotated corpora are to grow in scale and re-tain a high quality, annotation errors which arisefrom automatic processing must be minimized, aserrors have a negative impact on training and eval-uation of NLP technology (see discussion and ref-erences in Boyd et al, 2008, sec.
1).
There is workon detecting errors in dependency corpus annota-tion (Boyd et al, 2008), but this is based on findinginconsistencies in annotation for identical recur-ring strings.
This emphasis on identical strings canresult in high precision, but many strings do not re-cur, negatively impacting the recall of error detec-tion.
Furthermore, since the same strings often re-ceive the same automatic parse, the types of incon-sistencies detected are likely to have resulted frommanual annotation.
While we can build from theinsight that simple methods can provide reliableannotation checks, we need an approach which re-lies on more general properties of the dependencystructures, in order to develop techniques whichwork for automatically-parsed corpora.Developing techniques to detect errors in parsesin a way which is independent of corpus andparser has fairly broad implications.
By usingonly the information available in a training corpus,the methods we explore are applicable to annota-tion error detection for either hand-annotated orautomatically-parsed corpora and can also provideinsights for parse reranking (e.g., Hall and Nova?k,2005) or parse revision (Attardi and Ciaramita,2007).
Although we focus only on detecting errorsin automatically-parsed data, similar techniqueshave been applied for hand-annotated data (Dick-inson, 2008; Dickinson and Foster, 2009).Our general approach is based on extractinga grammar from an annotated corpus and com-paring dependency rules in a new (automatically-annotated) corpus to the grammar.
Roughly speak-ing, if a dependency rule?which represents all thedependents of a head together (see section 3.1)?does not fit well with the grammar, it is flagged aspotentially erroneous.
The methods do not haveto be retrained for a given parser?s output (e.g.,729Campbell and Johnson, 2002), but work by com-paring any tree to what is in the training grammar(cf.
also approaches stacking hand-written ruleson top of other parsers (Bick, 2007)).We propose to flag erroneous parse rules, usinginformation which reflects different grammaticalproperties: POS lookup, bigram information, andfull rule comparisons.
We build on a method todetect so-called ad hoc rules, as described in sec-tion 2, and then turn to the main approaches in sec-tion 3.
After a discussion of a simple way to flagPOS anomalies in section 4, we evaluate the dif-ferent methods in section 5, using the outputs fromtwo different parsers.
The methodology proposedin this paper is easy to implement and independentof corpus, language, or parser.2 ApproachWe take as a starting point two methods for detect-ing ad hoc rules in constituency annotation (Dick-inson, 2008).
Ad hoc rules are CFG productionsextracted from a treebank which are ?used for spe-cific constructions and unlikely to be used again,?indicating annotation errors and rules for ungram-maticalities (see also Dickinson and Foster, 2009).Each method compares a given CFG rule to allthe rules in a treebank grammar.
Based on thenumber of similar rules, a score is assigned, andrules with the lowest scores are flagged as poten-tially ad hoc.
This procedure is applicable whetherthe rules in question are from a new data set?as inthis paper, where parses are compared to a trainingdata grammar?or drawn from the treebank gram-mar itself (i.e., an internal consistency check).The two methods differ in how the comparisonsare done.
First, the bigram method abstracts arule to its bigrams.
Thus, a rule such as NP ?JJ NN provides support for NP ?
DT JJ JJ NN,in that it shares the JJ NN sequence.
By con-trast, in the other method, which we call the wholerule method,1 a rule is compared in its totalityto the grammar rules, using Levenshtein distance.There is no abstraction, meaning all elements arepresent?e.g., NP?
DT JJ JJ NN is very similarto NP ?
DT JJ NN because the sequences differby only one category.While previously used for constituencies, whatis at issue is simply the valency of a rule, whereby valency we refer to a head and its entire set1This is referred to whole daughters in Dickinson (2008),but the meaning of ?daughters?
is less clear for dependencies.of arguments and adjuncts (cf.
Przepio?rkowski,2006)?that is, a head and all its dependents.
Themethods work because we expect there to be reg-ularities in valency structure in a treebank gram-mar; non-conformity to such regularities indicatesa potential problem.3 Ad hoc rule detection3.1 An appropriate representationTo capture valency, consider the dependency treefrom the Talbanken05 corpus (Nilsson and Hall,2005) in figure 1, for the Swedish sentence in (1),which has four dependency pairs.2(1) Detitga?rgoesbarajustintenotihoptogether.
?It just doesn?t add up.
?SS MA NA PLDet ga?r bara inte ihopPO VV AB AB ABFigure 1: Dependency graph exampleOn a par with constituency rules, we define agrammar rule as a dependency relation rewritingas a head with its sequence of POS/dependentpairs (cf.
Kuhlmann and Satta, 2009), as in fig-ure 2.
This representation supports the detectionof idiosyncracies in valency.31.
TOP?
root ROOT:VV2.
ROOT?
SS:PO VV MA:AB NA:AB PL:AB3.
SS?
PO 5.
NA?
AB4.
MA?
AB 6.
PL?
ABFigure 2: Rule representation for (1)For example, for the ROOT category, the headis a verb (VV), and it has 4 dependents.
Theextent to which this rule is odd depends uponwhether comparable rules?i.e., other ROOT rulesor other VV rules (see section 3.2)?have a simi-lar set of dependents.
While many of the otherrules seem rather spare, they provide useful infor-mation, showing categories which have no depen-dents.
With a TOP rule, we have a rule for every2Category definitions are in appendix A.3Valency is difficult to define for coordination and is spe-cific to an annotation scheme.
We leave this for the future.730head, including the virtual root.
Thus, we can findanomalous rules such as TOP ?
root ROOT:AVROOT:NN, where multiple categories have beenparsed as ROOT.3.2 Making appropriate comparisonsIn comparing rules, we are trying to find evidencethat a particular (parsed) rule is valid by examiningthe evidence from the (training) grammar.Units of comparison To determine similarity,one can compare dependency relations, POS tags,or both.
Valency refers to both properties, e.g.,verbs which allow verbal (POS) subjects (depen-dency).
Thus, we use the pairs of dependency re-lations and POS tags as the units of comparison.Flagging individual elements Previous workscored only entire rules, but some dependenciesare problematic and others are not.
Thus, ourmethods score individual elements of a rule.Comparable rules We do not want to com-pare a rule to all grammar rules, only to thosewhich should have the same valents.
Compara-bility could be defined in terms of a rule?s depen-dency relation (LHS) or in terms of its head.
Con-sider the four different object (OO) rules in (2).These vary a great deal, and much of the variabil-ity comes from the fact that they are headed bydifferent POS categories, which tend to have dif-ferent selectional properties.
The head POS thusseems to be predictive of a rule?s valency.
(2) a. OO?
POb.
OO?
DT:EN AT:AJ NN ET:VVc.
OO?
SS:PO QV VG:VVd.
OO?
DT:PO AT:AJ VNBut we might lose information by ignoring ruleswith the same left-hand side (LHS).
Our approachis thus to take the greater value of scores whencomparing to rules either with the same depen-dency relation or with the same head.
A rule hasmultiple chances to prove its value, and low scoreswill only be for rules without any type of support.Taking these points together, for a given rule ofinterest r, we assign a score (S) to each element eiin r, where r = e1...em by taking the maximumof scores for rules with the same head (h) or sameLHS (lhs), as in (3).
For the first element in (2b),for example, S(DT:EN) = max{s(DT:EN, NN),s(DT:EN, OO)}.
The question is now how we de-fine s(ei, c) for the comparable element c.(3) S(ei) = max{s(ei, h), s(ei, lhs)}3.3 Whole rule anomalies3.3.1 MotivationThe whole rule method compares a list of a rule?sdependents to rules in a database, and then flagsrule elements without much support.
By using alldependents as a basis for comparison, this methoddetects improper dependencies (e.g., an adverbmodifying a noun), dependencies in the wrongoverall location of a rule (e.g., an adverb beforean object), and rules with unnecessarily long ar-gument structures.
For example, in (4), we havean improper relation between skall (?shall?)
andsambeskattas (?be taxed together?
), as in figure 3.It is parsed as an adverb (AA), whereas it shouldbe a verb group (VG).
The rule for this part of thetree is +F ?
++:++ SV AA:VV, and the AA:VVposition will be low-scoring because the ++:++ SVcontext does not support it.
(4) Makarsspouses?o?vrigaotherinkomsterincomesa?rareB-inkomsterB-incomesochandskallshallsomastidigarepreviouslysambeskattasbe taxed togeher..?The other incomes of spouses are B-incomes andshall, as previously, be taxed together.
?++ +F UK KA VGoch skall som tidigare sambeskattas++ SV UK AJ VV++ +F UK SS AAoch skall som tidigare sambeskattas++ SV UK AJ VVFigure 3: Wrong label (top=gold, bottom=parsed)3.3.2 ImplementationThe method we use to determine similarity arisesfrom considering what a rule is like without aproblematic element.
Consider +F ?
++:++ SVAA:VV from figure 3, where AA should be a dif-ferent category (VG).
The rule without this er-ror, +F ?
++:++ SV, starts several rules in the731training data, including some with VG:VV as thenext item.
The subrule ++:++ SV seems to bereliable, whereas the subrules containing AA:VV(++:++ AA:VV and SV AA:VV) are less reliable.We thus determine reliability by seeing how ofteneach subsequence occurs in the training rule set.Throughout this paper, we use the term subruleto refer to a rule subsequence which is exactly oneelement shorter than the rule it is a componentof.
We examine subrules, counting their frequencyas subrules, not as complete rules.
For example,TOP rules with more than one dependent are prob-lematic, e.g., TOP ?
root ROOT:AV ROOT:NN.Correspondingly, there are no rules with three ele-ments containing the subrule root ROOT:AV.We formalize this by setting the score s(ei, c)equal to the summation of the frequencies of allcomparable subrules containing ei from the train-ing data, as in (5), where B is the set of subrulesof r with length one less.
(5) s(ei, c) =?sub?B:ei?subC(sub, c)For example, with c = +F, the frequency of +F?
++:++ SV as a subrule is added to the scoresfor ++:++ and SV.
In this case, +F ?
++:++SV VG:BV, +F ?
++:++ SV VG:AV, and +F?
++:++ SV VG:VV all add support for +F ?++:++ SV being a legitimate subrule.
Thus, ++:++and SV are less likely to be the sources of anyproblems.
Since +F ?
SV AA:VV and +F ?++:++ AA:VV have very little support in the train-ing data, AA:VV receives a low score.Note that the subrule count C(sub, c) is differ-ent than counting the number of rules containinga subrule, as can be seen with identical elements.For example, for SS?
VN ET:PR ET:PR, C(VNET:PR, SS) = 2, in keeping with the fact that thereare 2 pieces of evidence for its legitimacy.3.4 Bigram anomalies3.4.1 MotivationThe bigram method examines relationships be-tween adjacent sisters, complementing the wholerule method by focusing on local properties.
For(6), for example, we find the gold and parsed treesin figure 4.
For the long parsed rule TA ?
PRHD:ID HD:ID IR:IR AN:RO JR:IR, all elementsget low whole rule scores, i.e., are flagged as po-tentially erroneous.
But only the final elementshave anomalous bigrams: HD:ID IR:IR, IR:IRAN:RO, and AN:RO JR:IR all never occur.
(6) Na?rwhendetitga?llerconcernsinkomsta?retthe income year19711971((taxeringsa?retassessment year19721972))skallshallbarnetthe child.
.
.. .
.
?Concerning the income year of 1971 (assessment year1972), the child .
.
.
?3.4.2 ImplementationTo obtain a bigram score for an element, we sim-ply add together the bigrams which contain the el-ement in question, as in (7).
(7) s(ei, c) = C(ei?1ei, c) + C(eiei+1, c)Consider the rule from figure 4.
With c =TA, the bigram HD:ID IR:IR never occurs, soboth HD:ID and IR:IR get 0 added to their score.HD:ID HD:ID, however, is a frequent bigram, soit adds weight to HD:ID, i.e., positive evidencecomes from the bigram on the left.
If we look atIR:IR, on the other hand, IR:IR AN:RO occurs 0times, and so IR:IR gets a total score of 0.Both scoring methods treat each element inde-pendently.
Every single element could be given alow score, even though once one is corrected, an-other would have a higher score.
Future work canexamine factoring in all elements at once.4 Additional informationThe methods presented so far have limited defini-tions of comparability.
As using complementaryinformation has been useful in, e.g., POS error de-tection (Loftsson, 2009), we explore other simplecomparable properties of a dependency grammar.Namely, we include: a) frequency information ofan overall dependency rule and b) information onhow likely each dependent is to be in a relationwith its head, described next.4.1 Including POS informationConsider PA ?
SS:NN XX:XX HV OO:VN, asillustrated in figure 5 for the sentence in (8).
Thisrule is entirely correct, yet the XX:XX position haslow whole rule and bigram scores.
(8) Uppgiftinformationomofvilkawhichorterneighborhoodsomwhoharhasutko?rningdeliveryfinnerfindNiyouocksa?alsoiin.
.
.. .
.
?You can also find information about which neighbor-hoods have delivery services in .
.
.
?732AA HD HD DT PA IR DT AN JR ...Na?r det ga?ller inkomsta?ret 1971 ( taxeringsa?ret 1972 ) ...PR ID ID NN RO IR NN RO IR ...TA HD HD PA ET IR DT AN JR ...Na?r det ga?ller inkomsta?ret 1971 ( taxeringsa?ret 1972 ) ...PR ID ID NN RO IR NN RO IR ...Figure 4: A rule with extra dependents (top=gold, bottom=parsed)ET DT SS XX PA OOUppgift om vilka orter som har utko?rningNN PR PO NN XX HV VNFigure 5: Overflagging (gold=parsed)One method which does not have this problemof overflagging uses a ?lexicon?
of POS tag pairs,examining relations between POS, irrespective ofposition.
We extract POS pairs, note their depen-dency relation, and add a L/R to the label to in-dicate which is the head (Boyd et al, 2008).
Ad-ditionally, we note how often two POS categoriesoccur as a non-depenency, using the label NIL, tohelp determine whether there should be any at-tachment.
We generate NILs by enumerating allPOS pairs in a sentence.
For example, from fig-ure 5, the parsed POS pairs include NN PR 7?
ET-L, NN PO 7?
NIL, etc.We convert the frequencies to probabilities.
Forexample, of 4 total occurrences of XX HV in thetraining data, 2 are XX-R (cf.
figure 5).
A proba-bility of 0.5 is quite high, given that NILs are oftenthe most frequent label for POS pairs.5 EvaluationIn evaluating the methods, our main question is:how accurate are the dependencies, in terms ofboth attachment and labeling?
We therefore cur-rently examine the scores for elements functioningas dependents in a rule.
In figure 5, for example,for har (?has?
), we look at its score within ET ?PR PA:HV and not when it functions as a head, asin PA?
SS:NN XX:XX HV OO:VN.Relatedly, for each method, we are interestedin whether elements with scores below a thresh-old have worse attachment accuracy than scoresabove, as we predict they do.
We can measurethis by scoring each testing data position belowthe threshold as a 1 if it has the correct head anddependency relation and a 0 otherwise.
These aresimply labeled attachment scores (LAS).
Scoringseparately for positions above and below a thresh-old views the task as one of sorting parser outputinto two bins, those more or less likely to be cor-rectly parsed.
For development, we also report un-labeled attachement scores (UAS).Since the goal is to speed up the post-editing ofcorpus data by flagging erroneous rules, we alsoreport the precision and recall for error detection.We count either attachment or labeling errors asan error, and precision and recall are measuredwith respect to how many errors are found belowthe threshold.
For development, we use two F-scores to provide a measure of the settings to ex-amine across language, corpus, and parser condi-tions: the balanced F1 measure and the F0.5 mea-sure, weighing precision twice as much.
Precisionis likely more important in this context, so as toprevent annotators from sorting through too manyfalse positives.
In practice, one way to use thesemethods is to start with the lowest thresholds andwork upwards until there are too many non-errors.To establish a basis for comparison, we compare733method performance to a parser on its own.4 Byexamining the parser output without any automaticassistance, how often does a correction need to bemade?5.1 The dataAll our data comes from the CoNLL-X SharedTask (Buchholz and Marsi, 2006), specifically the4 data sets freely available online.
We use theSwedish Talbanken data (Nilsson and Hall, 2005)and the transition-based dependency parser Malt-Parser (Nivre et al, 2007), with the default set-tings, for developing the method.
To test acrosslanguages and corpora, we use MaltParser on theother 3 corpora: the Danish DDT (Kromann,2003), Dutch Alpino (van der Beek et al, 2002),and Portuguese Bosque data (Afonso et al, 2002).Then, we present results using the graph-basedparser MSTParser (McDonald and Pereira, 2006),again with default settings, to test the methodsacross parsers.
We use the gold standard POS tagsfor all experiments.5.2 Development dataIn the first line of table 1, we report the baselineMaltParser accuracies on the Swedish test data,including baseline error detection precision (=1-LASb), recall, and (the best) F-scores.
In the restof table 1, we report the best-performing resultsfor each of the methods,5 providing the numberof rules below and above a particular threshold,along with corresponding UAS and LAS values.To get the raw number of identified rules, multiplythe number of corpus position below a threshold(b) times the error detection precision (P ).
For ex-ample, the bigram method with a threshold of 39leads to finding 283 errors (455 ?
.622).Dependency elements with frequency below thelowest threshold have lower attachment scores(66.6% vs. 90.1% LAS), showing that simply us-ing a complete rule helps sort dependencies.
How-ever, frequency thresholds have fairly low preci-sion, i.e., 33.4% at their best.
The whole rule andbigram methods reveal greater precision in iden-tifying problematic dependencies, isolating ele-ments with lower UAS and LAS scores than withfrequency, along with corresponding greater pre-4One may also use parser confidence or parser revisionmethods as a basis of comparison, but we are aware of no sys-tematic evaluation of these approaches for detecting errors.5Freq=rule frequency, WR=whole rule, Bi=bigram,POS=POS-based (POS scores multiplied by 10,000)cision and F-scores.
The bigram method is morefine-grained, identifying small numbers of rule el-ements at each threshold, resulting in high errordetection precision.
With a threshold of 39, for ex-ample, we find over a quarter of the parser errorswith 62% precision, from this one piece of infor-mation.
For POS information, we flag 23.6% ofthe cases with over 60% precision (at 81.6).Taking all these results together, we can beginto sort more reliable from less reliable dependencytree elements, using very simple information.
Ad-ditionally, these methods naturally group casestogether by linguistic properties (e.g., adverbial-verb dependencies within a particualr context), al-lowing a human to uncover the principle behindparse failure and ajudicate similar cases at thesame time (cf.
Wallis, 2003).5.3 DiscussionExamining some of the output from the Tal-banken test data by hand, we find that a promi-nent cause of false positives, i.e., correctly-parsedcases with low scores, stems from low-frequencydependency-POS label pairs.
If the dependencyrarely occurs in the training data with the partic-ular POS, then it receives a low score, regardlessof its context.
For example, the parsed rule TA?
IG:IG RO has a correct dependency relation(IG) between the POS tags IG and its head RO, yetis assigned a whole rule score of 2 and a bigramscore of 20.
It turns out that IG:IG only occurs144 times in the training data, and in 11 of thosecases (7.6%) it appears immediately before RO.One might consider normalizing the scores basedon overall frequency or adjusting the scores to ac-count for other dependency rules in the sentence:in this case, there may be no better attachment.Other false positives are correctly-parsed ele-ments that are a part of erroneous rules.
For in-stance, in AA?
UK:UK SS:PO TA:AJ AV SP:AJOA:PR +F:HV +F:HV, the first +F:HV is correct,yet given a low score (0 whole rule, 1 bigram).The following and erroneous +F:HV is similarlygiven a low score.
As above, such cases mightbe handled by looking for attachments in otherrules (cf.
Attardi and Ciaramita, 2007), but thesecases should be relatively unproblematic for hand-correction, given the neighboring error.We also examined false negatives, i.e., errorswith high scores.
There are many examples of PRPA:NN rules, for instance, with the NN improp-734Score Thr.
b a UASb LASb UASa LASa P R F1 F0.5None n/a 5656 0 87.4% 82.0% 0% 0% 18.0% 100% 30.5% 21.5%Freq 0 1951 3705 76.6% 66.6% 93.1% 90.1% 33.4% 64.1% 43.9% 36.9%WR 0 894 4762 64.7% 54.0% 91.7% 87.3% 46.0% 40.5% 43.0% 44.8%6 1478 4178 71.1% 60.9% 93.2% 89.5% 39.1% 56.9% 46.4% 41.7%Bi 0 56 5600 10.7% 7.1% 88.2% 82.8% 92.9% 5.1% 9.7% 21.0%39 455 5201 51.6% 37.8% 90.6% 85.9% 62.2% 27.9% 38.5% 49.9%431 1685 3971 74.1% 63.7% 93.1% 89.8% 36.3% 60.1% 45.2% 39.4%POS 0 54 5602 27.8% 22.2% 87.4% 82.6% 77.8% 4.1% 7.9% 17.0%81.6 388 5268 48.5% 38.4% 90.3% 85.3% 61.6% 23.5% 34.0% 46.5%763 1863 3793 75.4% 65.8% 93.3% 90.0% 34.2% 62.8% 44.3% 37.7%Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.
))erly attached, but there are also many correct in-stances of PR PA:NN.
To sort out the errors, oneneeds to look at lexical knowledge and/or other de-pendencies in the tree.
With so little context, fre-quent rules with only one dependent are not primecandidates for our methods of error detection.5.4 Other corporaWe now turn to the parsed data from three othercorpora.
The Alpino and Bosque corpora are ap-proximately the same size as Talbanken, so we usethe same thresholds for them.
The DDT data isapproximately half the size; to adjust, we simplyhalve the scores.
In tables 2, 3, and 4, we presentthe results, using the best F0.5 and F1 settings fromdevelopment.
At a glance, we observe that the bestmethod differs for each corpus and depending onan emphasis of precision or recall, with the bigrammethod generally having high precision.Score Thr.
b LASb LASa P RNone n/a 5585 73.8% 0% 26.2% 100%Freq 0 1174 43.2% 81.9% 56.8% 45.6%WR 0 483 32.5% 77.7% 67.5% 22.3%6 787 39.4% 79.4% 60.6% 32.6%Bi 39 253 33.6% 75.7% 66.4% 11.5%431 845 45.6% 78.8% 54.4% 31.4%POS 81.6 317 51.7% 75.1% 48.3% 10.5%763 1767 53.5% 83.2% 46.5% 56.1%Table 2: MaltParser results for AlpinoFor Alpino, error detection is better with fre-quency than, for example, bigram scores.
This islikely due to the fact that Alpino has the small-est label set of any of the corpora, with only 24dependency labels and 12 POS tags (cf.
64 and41 in Talbanken, respectively).
With a smaller la-bel set, there are less possible bigrams that couldbe anomalous, but more reliable statistics about aScore Thr.
b LASb LASa P RNone n/a 5867 82.2% 0% 17.8% 100%Freq 0 1561 61.2% 89.9% 38.8% 58.1%WR 0 693 48.1% 86.8% 51.9% 34.5%6 1074 54.4% 88.5% 45.6% 47.0%Bi 39 227 15.4% 84.9% 84.6% 18.4%431 776 51.0% 87.0% 49.0% 36.5%POS 81.6 369 33.3% 85.5% 66.7% 23.6%763 1681 60.1% 91.1% 39.9% 64.3%Table 3: MaltParser results for BosqueScore Thr.
b LASb LASa P RNone n/a 5852 81.0% 0% 19.0% 100%Freq 0 1835 65.9% 88.0% 34.1% 56.4%WR 0 739 53.9% 85.0% 46.1% 30.7%3 1109 60.1% 85.9% 39.9% 39.9%Bi 19.5 185 25.4% 82.9% 74.6% 12.4%215.5 884 56.8% 85.4% 43.2% 34.4%POS 40.8 179 30.2% 82.7% 69.8% 11.3%381.5 1214 62.5% 85.9% 37.5% 41.0%Table 4: MaltParser results for DDTwhole rule.
Likewise, with fewer possible POStag pairs, Alpino has lower precision for the low-threshold POS scores than the other corpora.For the whole rule scores, the DDT data isworse (compare its 46.1% precision with Bosque?s45.6%, with vastly different recall values), whichcould be due to the smaller training data.
Onemight also consider the qualitative differences inthe dependency inventory of DDT compared to theothers?e.g., appositions, distinctions in names,and more types of modifiers.5.5 MSTParserTurning to the results of running the methodson the output of MSTParser, we find similar butslightly worse values for the whole rule and bi-gram methods, as shown in tables 5-8.
What is735most striking are the differences in the POS-basedmethod for Bosque and DDT (tables 7 and 8),where a large percentage of the test corpus is un-derneath the threshold.
MSTParser is apparentlypositing fewer distinct head-dependent pairs, asmost of them fall under the given thresholds.
Withthe exception of the POS-based method for DDT(where LASb is actually higher than LASa) thedifferent methods seem to be accurate enough tobe used as part of corpus post-editing.Score Thr.
b LASb LASa P RNone n/a 5656 81.1% 0% 18.9% 100%Freq 0 3659 65.2% 89.7% 34.8% 64.9%WR 0 4740 55.7% 86.0% 44.3% 37.9%6 4217 59.9% 88.3% 40.1% 53.9%Bi 39 5183 38.9% 84.9% 61.1% 27.0%431 3997 63.2% 88.5% 36.8% 57.1%POS 81.6 327 42.8% 83.4% 57.2% 17.5%763 1764 68.0% 87.0% 32.0% 52.7%Table 5: MSTParser results for TalbankenScore Thr.
b LASb LASa P RNone n/a 5585 75.4% 0% 24.6% 100%Freq 0 1371 49.5% 83.9% 50.5% 50.5%WR 0 453 40.0% 78.5% 60.0% 19.8%6 685 45.4% 79.6% 54.6% 27.2%Bi 39 226 39.8% 76.9% 60.2% 9.9%431 745 48.2% 79.6% 51.8% 28.1%POS 81.6 570 60.4% 77.1% 39.6% 16.5%763 1860 61.9% 82.1% 38.1% 51.6%Table 6: MSTParser results for AlpinoScore Thr.
b LASb LASa P RNone n/a 5867 82.5% 0% 17.5% 100%Freq 0 1562 63.9% 89.3% 36.1% 55.0%WR 0 540 50.6% 85.8% 49.4% 26.0%6 985 58.0% 87.5% 42.0% 40.4%Bi 39 117 34.2% 83.5% 65.8% 7.5%431 736 56.4% 86.3% 43.6% 31.3%POS 81.6 2978 75.8% 89.4% 24.2% 70.3%763 3618 74.3% 95.8% 25.7% 90.7%Table 7: MSTParser results for BosqueScore Thr.
b LASb LASa P RNone n/a 5852 82.9% 0% 17.1% 100%Freq 0 1864 70.3% 88.8% 29.7% 55.3%WR 0 624 60.6% 85.6% 39.4% 24.6%3 1019 65.4% 86.6% 34.6% 35.3%Bi 19.5 168 28.6% 84.5% 71.4% 12.0%215.5 839 61.6% 86.5% 38.4% 32.2%POS 40.8 5714 83.0% 79.0% 17.0% 97.1%381.5 5757 82.9% 80.0% 17.1% 98.1%Table 8: MSTParser results for DDT6 Summary and OutlookWe have proposed different methods for flag-ging the errors in automatically-parsed corpora, bytreating the problem as one of looking for anoma-lous rules with respect to a treebank grammar.The different methods incorporate differing typesand amounts of information, notably comparisonsamong dependency rules and bigrams within suchrules.
Using these methods, we demonstrated suc-cess in sorting well-formed output from erroneousoutput across language, corpora, and parsers.Given that the rule representations and compar-ison methods use both POS and dependency in-formation, a next step in evaluating and improv-ing the methods is to examine automatically POS-tagged data.
Our methods should be able to findPOS errors in addition to dependency errors.
Fur-thermore, although we have indicated that differ-ences in accuracy can be linked to differences inthe granularity and particular distinctions of theannotation scheme, it is still an open question asto which methods work best for which schemesand for which constructions (e.g., coordination).AcknowledgmentsThanks to Sandra Ku?bler and Amber Smith forcomments on an earlier draft; Yvonne Samuels-son for help with the Swedish translations; the IUComputational Linguistics discussion group forfeedback; and Julia Hockenmaier, Chris Brew, andRebecca Hwa for discussion on the general topic.A Some Talbanken05 categoriesPOS tags++ coord.
conj.AB adverbAJ adjectiveAV vara (be)EN indef.
articleHV ha(va) (have)ID part of idiomIG punctuationIR parenthesisNN nounPO pronounPR prepositionRO numeralQV kunna (can)SV skola (will)UK sub.
conj.VN verbal nounVV verbXX unclassifiableDependencies++ coord.
conj.+F main clause coord.AA adverbialAN appositionAT nomainl pre-modifierDT determinerET nominal post-modifierHD headIG punctuationIR parenthesisJR second parenthesisKA comparative adverbialMA attitude adverbialNA negation adverbialOO objectPA preposition comp.PL verb particleSS subjectTA time adverbialUK sub.
conj.VG verb groupXX unclassifiable736ReferencesAfonso, Susana, Eckhard Bick, Renato Haber andDiana Santos (2002).
Floresta Sinta?
(c)tica: atreebank for Portuguese.
In Proceedings ofLREC 2002.
Las Palmas, pp.
1698?1703.Attardi, Giuseppe and Massimiliano Ciaramita(2007).
Tree Revision Learning for DependencyParsing.
In Proceedings of NAACL-HLT-07.Rochester, NY, pp.
388?395.Bick, Eckhard (2007).
Hybrid Ways to ImproveDomain Independence in an ML DependencyParser.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007.
Prague,Czech Republic, pp.
1119?1123.Boyd, Adriane, Markus Dickinson and DetmarMeurers (2008).
On Detecting Errors in Depen-dency Treebanks.
Research on Language andComputation 6(2), 113?137.Buchholz, Sabine and Erwin Marsi (2006).CoNLL-X Shared Task on Multilingual Depen-dency Parsing.
In Proceedings of CoNLL-X.New York City, pp.
149?164.Campbell, David and Stephen Johnson (2002).
Atransformational-based learner for dependencygrammars in discharge summaries.
In Proceed-ings of the ACL-02 Workshop on Natural Lan-guage Processing in the Biomedical Domain.Phildadelphia, pp.
37?44.Dickinson, Markus (2008).
Ad Hoc TreebankStructures.
In Proceedings of ACL-08.
Colum-bus, OH.Dickinson, Markus and Jennifer Foster (2009).Similarity Rules!
Exploring Methods for Ad-Hoc Rule Detection.
In Proceedings of TLT-7.Groningen, The Netherlands.Gildea, Daniel (2001).
Corpus Variation andParser Performance.
In Proceedings ofEMNLP-01.
Pittsburgh, PA.Hall, Keith and Va?clav Nova?k (2005).
CorrectiveModeling for Non-Projective Dependency Pars-ing.
In Proceedings of IWPT-05.
Vancouver, pp.42?52.Kromann, Matthias Trautner (2003).
The DanishDependency Treebank and the underlying lin-guistic theory.
In Proceedings of TLT-03.Kuhlmann, Marco and Giorgio Satta (2009).
Tree-bank Grammar Techniques for Non-ProjectiveDependency Parsing.
In Proceedings of EACL-09.
Athens, Greece, pp.
478?486.Loftsson, Hrafn (2009).
Correcting a POS-TaggedCorpus Using Three Complementary Methods.In Proceedings of EACL-09.
Athens, Greece,pp.
523?531.McDonald, Ryan and Fernando Pereira (2006).Online learning of approximate dependencyparsing algorithms.
In Proceedings of EACL-06.
Trento.Nilsson, Jens and Johan Hall (2005).
Recon-struction of the Swedish Treebank Talbanken.MSI report 05067, Va?xjo?
University: School ofMathematics and Systems Engineering.Nivre, Joakim, Johan Hall, Jens Nilsson, AtanasChanev, Gulsen Eryigit, Sandra Ku?bler, Sve-toslav Marinov and Erwin Marsi (2007).
Malt-Parser: A language-independent system fordata-driven dependency parsing.
Natural Lan-guage Engineering 13(2), 95?135.Owczarzak, Karolina (2009).
DEPEVAL(summ):Dependency-based Evaluation for AutomaticSummaries.
In Proceedings of ACL-AFNLP-09.Suntec, Singapore, pp.
190?198.Przepio?rkowski, Adam (2006).
What to ac-quire from corpora in automatic valence ac-quisition.
In Violetta Koseska-Toszewa andRoman Roszko (eds.
), Semantyka a kon-frontacja jezykowa, tom 3, Warsaw: Slawisty-czny Os?rodek Wydawniczy PAN, pp.
25?41.Sekine, Satoshi (1997).
The Domain Dependenceof Parsing.
In Proceedings of ANLP-96.
Wash-ington, DC.van der Beek, Leonoor, Gosse Bouma, RobertMalouf and Gertjan van Noord (2002).
TheAlpino Dependency Treebank.
In Proceedingsof CLIN 2001.
Rodopi.van Noord, Gertjan and Gosse Bouma (2009).Parsed Corpora for Linguistics.
In Proceed-ings of the EACL 2009 Workshop on the In-teraction between Linguistics and Computa-tional Linguistics: Virtuous, Vicious or Vacu-ous?.
Athens, pp.
33?39.Wallis, Sean (2003).
Completing Parsed Corpora.In Anne Abeille?
(ed.
), Treebanks: Building andusing syntactically annoted corpora, Dordrecht:Kluwer Academic Publishers, pp.
61?71.Wan, Stephen, Mark Dras, Robert Dale and Ce?cileParis (2009).
Improving Grammaticality in Sta-737tistical Sentence Generation: Introducing a De-pendency Spanning Tree Algorithm with an Ar-gument Satisfaction Model.
In Proceedings ofEACL-09.
Athens, Greece, pp.
852?860.Xu, Peng, Jaeho Kang, Michael Ringgaard andFranz Och (2009).
Using a Dependency Parserto Improve SMT for Subject-Object-Verb Lan-guages.
In Proceedings of NAACL-HLT-09.Boulder, Colorado, pp.
245?253.738
