Orthographic Errors in Web Pages:Toward Cleaner Web CorporaChristoph Ringlstetter?Klaus U. Schulz?CIS, University of MunichStoyan Mihov?Bulgarian Academy of Science, SofiaSince the Web by far represents the largest public repository of natural language texts, recentexperiments, methods, and tools in the area of corpus linguistics often use the Web as a corpus.For applications where high accuracy is crucial, the problem has to be faced that a non-negligiblenumber of orthographic and grammatical errors occur in Web documents.
In this article we in-vestigate the distribution of orthographic errors of various types in Web pages.
As a by-product,methods are developed for efficiently detecting erroneous pages and for marking orthographicerrors in acceptable Web documents, reducing thus the number of errors in corpora and linguisticknowledge bases automatically retrieved from the Web.1.
IntroductionThe automated analysis of large corpora has many useful applications (Church andMercer 1993).
Suitable language repositories can be used for deriving models of agiven natural language, as needed for speech recognition (Ostendorf, Digalakis,and Kimball 1996; Jelinek 1997; Chelba and Jelinek 2002), language generation (Ohand Rudickny 2000), and text correction (Kukich 1992; Amengual and Vidal 1998;Strohmaier et al 2003b).
Other corpus-based methods determine associations betweenwords (Grefenstette 1992; Dunning 1993; Lin et al 1998), which yields a basis for com-puting thesauri, or dictionaries of terminological expressions and multiword lexemes(Gaizauskas, Demetriou, and Humphreys 2000; Grefenstette 2001).From multilingual texts, translation lexica can be generated (Gale and Church1991; Kupiec 1993; Kumano and Hirakawa 1994; Boutsis, Piperidis, and Demiros 1999;Grefenstette 1999).
The analysis of technical texts is used to automatically build dictio-naries of acronyms for a given field (Taghva and Gilbreth 1999; Yeates, Bainbridge, andWitten 2000), and related methods help to compute dictionaries that cover the specialvocabulary of a given thematic area (Strohmaier et al 2003a).
In computer-assisted lan-guage learning (CALL), mining techniques for corpora are used to create individualizedand user-centric exercises for grammar and text understanding (Schwartz, Aikawa, andPahud 2004; Brown and Eskenazi 2004; Fletcher 2004a).By Zipf?s law, most words, phrases, and specific grammatical constructions havea very low frequency.
Furthermore, the number of text genres and special thematic?
Funded by German Research Foundation (DFG)?
Funded by VolkswagenStiftungSubmission received: 21 January 2005; revised submission received: 3 August 2005; accepted forpublication: 10 December 2005.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 3areas that come with their own picture of language is large.
This explains that mostof the aforementioned applications can only work when built on top of huge heteroge-neous corpora.
Since the Web represents by far the largest public repository for naturallanguage texts, and since Web search engines such as Google offer simple access topages where language material of a given orthographic, grammatical, or thematic kindis found, many recent experiments and technologies use the Web as a corpus (Kehoe andRenouf 2002; Morley, Renouf, and Kehoe 2003; Kilgarriff and Grefenstette 2003; Resnikand Smith 2003; Way and Gough 2003; Fletcher 2004b).One potential problem for Web-based corpus linguistics is caused by the fact thatwords and phrases occurring in Web pages are sometimes erroneous.
Typing errorsrepresent one widespread phenomenon.
Many Web pages, say, in English, are writtenby non-native speakers, or by persons with very modest language competence.
As aconsequence, spelling errors and grammatical bugs result.
The character sets that areused for writing Web pages are often not fully adequate for the alphabet of a givenlanguage, which represents another systematic source for inaccuracies.
Furthermore, asmall number of texts found in the Web is obtained via optical character recognition(OCR), which may again lead to garbled words.
As a consequence of these and othererror sources, the Web contains a considerable number of ?bad?
pages with languagematerial that is inappropriate for corpus construction.In one way or the other, all the aforementioned applications are affected by theseinadequacies.
While the problem is probably not too serious for approaches that merelycollect statistical information about given language items, the construction of dictio-naries and related linguistic knowledge bases?which are, after all, meant to be usedin different scenarios of automated language processing?becomes problematic if toomany erroneous entries are retrieved from Web pages.
Obviously, in computer-assistedlanguage learning it is a principal concern that words and phrases from the Web thatare presented to the user are error free.In discussions we found that problems resulting from erroneous language materialin Web pages for distinct applications are broadly acknowledged (see also Section 4.4 ofKilgarriff and Grefenstette [2003]).
Still, to the best of our knowledge, a serious analysisof the frequency and distribution of orthographic errors in the Web is missing, and nogeneral methods have been developed that help to detect and exclude pages with toomany erroneous words.
In this article we first report on a series of experiments that tryto answer the following questions:1.
What are important types of orthographic errors found in Web pages?2.
How frequent are errors of a given kind?
For a given error level(percentage of erroneous tokens) ?, which percentage of Web pagesexceeds error level ??3.
How do these figures depend on the language, on the thematic area,and on the genre of the Web pages that are considered?
How dothese figures depend on the document format of the Web pages thatare considered?We then look at the problem indicated above.4.
Which methods help to automatically detect Web pages with manyorthographic errors?Which methods help to mark orthographic errors found in Web pages?296Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTo answer questions 1?3, we retrieved and analyzed a collection of large Englishand German corpora from the Web, using suitable queries to Web search engines.
In ourerror statistics we wanted to distinguish between (1) ?general?
Web pages collectedwithout any specific thematic focus on the one hand and Web pages from specificthematic areas on the other hand, and (2) between Web pages written in HTML andWeb documents written in PDF.
To cover the first difference, for both languages weretrieved two general corpora as well as a number of corpora for specific thematicareas.
All these corpora only contain HTML pages.
A parallel series of general cor-pora was collected that are composed of PDF documents.
Details are provided inSection 2.Special Vocabulary.
Web pages often contain tokens that do not belong to the standardvocabulary of the respective language.
Typical categories are, for example, specialnames, slang, archaic language, expressions from foreign languages, and special ex-pressions from computer science/programming.
Classification and detection of specialvocabulary is outside the scope of the present article.
Since sometimes a clear separationbetween special vocabulary and errors is difficult, we briefly come back to this problemin Section 5.4.Proper Errors.
Focusing on garbled standard vocabulary, tokens may be seriously dam-aged in an ?unexplainable?
way.
Most of the remaining errors can be assigned to one ofthe four classes mentioned above: typing errors (i.e., errors caused by a confusion of keys when typing adocument), spelling errors (?cognitive?
errors resulting from insufficient languagecompetence), errors resulting from inadequate character encoding, and OCR errors.In order to estimate the number of errors of a given kind in the corpora, specialerror dictionaries were built.
These dictionaries, which only list garbled words ofa given language that do not accidentally represent correct words, try to cover ahigh number of the conventional errors of each type that are typically found in Webpages and other documents.
Section 3 motivates the use of error dictionaries for er-ror detection.
Details of the construction of the error dictionaries are discussed inSection 4.In Section 5, we estimate the number of orthographic errors in the corpora thatremain undetected because they do not occur in the error dictionaries.
We also estimatethe percentage of correct tokens of the corpora that are erroneously treated as errorssince they appear in the error dictionaries.
Our results show that the number of tokensof a text that appear in the error dictionaries can be considered as a lower approximationof the number of real orthographic errors.In Section 6, we describe the distribution of orthographic errors of the types dis-tinguished above in the general test corpora, counting occurrences of entries of theerror dictionaries.
Section 7 summarizes the most important differences that arise whenusing PDF corpora, or corpora for special thematic areas.
Section 8 presents various297Computational Linguistics Volume 32, Number 3results that illuminate the relationship between the error rate of a document andits genre.In our experiments we observed in all corpora a rich spectrum of error rates, rangingfrom perfect documents to a small number of clearly unacceptable pages.
This moti-vates the design of filters that efficiently recognize and reject pages with an error ratebeyond a user-specified threshold.
The construction of appropriate filters is describedin Section 9, where we also demonstrate the effect of using these filters, comparingthe figures obtained in Section 6 with the corresponding figures for filtered corpora.Filters work surprisingly well due to a Zipf-like distribution of error frequencies inWeb pages.In Section 10, we present two experiments that exemplify how the methods de-veloped in the article may in fact help to improve corpus-based methods.
The generalquestion of how deeply distinct methods from computational linguistics based on Webcorpora are affected by orthographic errors in Web pages and to what extent the meth-ods developed in the article help to remedy these deficiencies are too complex to bediscussed here.The main insights and contributions are summarized in the Conclusion (Section 11)where we also comment on future work and on some practical difficulties one has toface when collecting and analyzing large corpora from the Web.2.
CorporaThe basis for the evaluations described below is a collection of corpora, each composedof Web pages retrieved with Web search engines (Google/AllTheWeb).
In order tostudy how specific features of a language might influence the distribution of ortho-graphic errors, all corpora were built up in two variants.
The English and Germanvariant, respectively, contain Web pages that were classified as English and German Webpages by the search engine.
As described above, for both languages we collected generalcorpora with Web pages without any thematic focus and, in addition, corpora that coverfive specific thematic areas to be described below.
Statements on the ?representative-ness?
of corpora derived from the Web are notoriously difficult.
The composition ofcorpora retrieved with Web search engines depends on the kind of queries that are used,on the ranking mechanisms of the engine, and on the details of the collection strategy.We mainly concentrated on simple queries and straightforward collection strategies.Still, the large number of subcorpora and pages that were evaluated should guaranteethat accidental results are avoided.2.1 General Web CorporaIn a first attempt, we tried to obtain a general German HTML corpus using the mean-ingless query der die das, i.e., the three German definite articles.
However, queriesof this and a similar form did not lead to satisfactory results: As a consequence ofGoogle?s ranking mechanism, which prefers ?authorities?
(Brin and Page 1998), mainlyportals of big organizations, companies, and others were retrieved.
These pages areoften dominated by graphical elements.
Portions of text are usually small and carefullyedited, which means that orthographic errors are less frequent than in other ?lessofficial?
pages.To achieve a more realistic scenario we randomly generated quintuples, each col-lecting five terms of the 10,000 top frequent German words.
We used Google to retrieve298Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages10 pages per query (quintuple) until we obtained 1,000 pages.
A considerable numberof the URLs were found to be inactive.
After conversion to ASCII and a preliminaryanalysis of error rates with methods described below, some of the remaining pages werefound to contain very large lists of general keywords, including many orthographicerrors.
Apparently these lists and errors were only added to improve the ranking ofthe page in search engines, even for ill-formed queries.
We excluded these pages.
Theremaining documents represent the ?primary?
general German HTML corpus.
Since wewanted to know how results depend on the peculiarities of the selected set of pages, asecond series of queries of the same type was sent to Google to retrieve a ?secondary?general German HTML corpus with a completely disjoint set of pages.Similar procedures were used to obtain a primary and a secondary general EnglishHTML corpus, a general German PDF corpus, and a general English PDF corpus.
Thetranslation from PDF to ASCII was found to be error prone, in particular for Germandocuments (cf.
Gartner 2003).
Due to this process, some converted PDF documentswere seriously damaged.
Since we focus on errors in original Web pages (as opposedto converted versions of such pages), these files were excluded as well.
We foundthese pages when computing error rates based on error dictionaries as described inSections 6 and 7.The number of Web pages and the number of normal tokens (i.e., tokens composedof standard letters only) in the resulting six corpora are shown in Table 1.
Numbers (1)and (2) stand for the primary and secondary corpora, respectively.2.2 Web Corpora for Specific Thematic AreasWe looked at the thematic areas ?Middle Ages,?
?Holocaust,?
?Fish,?
?Mushrooms,?and ?Neurology.?
The given selection of topics tries to cover scientific areas as well ashistory and hobbies.Simple Crawl.
A first series of corpora was collected by sending a query with 25?terminological?
keywords mechanically found in a small corpus of the given area tothe AllTheWeb search engine and collecting the answer set.
For example, the queriesmushrooms mushroom pine edible harvesting morels harvested harvesters driedchanterelle matsutake poisonous flavor chanterelles caps fungi drying stuffinghumidity varieties boletes recipes spores conifers pickersTable 1Number of Web pages, number of normal tokens (tokens composed of standard letters only),and sizes in megabytes of the ?general?
corpora.
Numbers (1) and (2) refer to primary andsecondary corpora, respectively.General corpora Web pages Normal tokens Size (MB)English HTML (1) 829 7,900,337 157English HTML (2) 929 7,152,783 188German HTML (1) 618 9,525,484 189German HTML (2) 857 11,539,035 284English PDF 570 2,193,598 393German PDF 603 1,528,914 240299Computational Linguistics Volume 32, Number 3disorder disorders anxiety self hallucinations delusions anatomy cortex delusionneuroscience disturbance conscious psychotic stimulus hallucination unconsciouslyreceptors cognitive psychoanalytic unconscious consciously stimuli ego schizophreniaimpairmentwere respectively used for collecting the corpora Mushrooms E and Neurology E. Theranking mechanism of AllTheWeb prefers pages containing hits for several keywordsof a disjunctive query.
Since this form of corpus construction is straightforward, not allpages in the resulting corpora belong to the given thematic area.Refined Crawl.
We wanted to see how results are affected when using less naive crawl-ing methods.
For the three areas ?Fish,?
?Mushrooms,?
and ?Neurology,?
the sec-ondary corpora were retrieved using the following refined procedure: Starting froma small tagged seed corpus for the given domain, we mechanically extracted termino-logical open compounds for English (Sornlertlamvanich and Tanaka 1996; Smadja andMcKeown 1990) and compound nouns for German.
Examples are amino group, actionpotential, defense mechanism (English, neurology), truffle species, morel areas, harvesting tips(English, mushrooms), Koffeinstoffwechsel, and Eisenkonzentration (German, neurology).Each of these expressions was sent as a query to Google.
From each answer set wecollected a maximum of 30 top-ranked hits (many answer sets were smaller).
For eachdocument in the resulting corpus, the similarity with the seed corpus was controlled,using a cosine measure (in practice, almost all documents passed the similarity filter).Our method can be considered as a variant of Baroni and Bernardini?s (2004) and leadsto corpora with a strong thematic focus.The statistics for all thematic corpora are summarized in Table 2.
Numbers (1) and(2) stand for corpora crawled with the simple and the refined crawling strategy, respec-tively.
The numbers indicate one interesting effect: Documents in the thematic corporaobtained with the refined crawling strategy turned out to be typically rather short.
Sincewe only used the 30 top-ranked documents for each single query, this probably pointsto a special feature of Google?s ranking mechanism.
A manual inspection of hundredsof documents for both the simple and the refined crawl did not lead to additionalinsights.3.
Error DetectionFor detecting orthographic errors of a particular type in texts, two naive base methodsmay be distinguished.1.
A representative list of errors of the respective type is created andmanually checked.
Each token of the text appearing in the list representsan error (lower approximation).2.
A spell checker or a large-scale dictionary is used to detect ?suspicious?words (error candidates).
For each such token W we manually checkif W really represents an error and we determine its type (upperapproximation).For large corpora, both methods have serious deficiencies.
With Method 1, only a smallpercentage of all errors is detected.
On this basis, it is difficult to estimate the realnumber of errors.
When using Method 2, the number of tokens that have to be manually300Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 2Selected topics and statistics of English (E) and German (G) corpora for specific thematic areas.Numbers (1) and (2) refer to corpora crawled with the simple and the refined strategy,respectively.Topic/Language Web pages Normal tokens Size (MB)Middle Ages E 710 5,069,796 172Fish E (1) 510 10,090,682 266Fish E (2) 940 547,407 22Holocaust E 699 8,797,882 199Mushrooms E (1) 676 7,876,067 197Mushrooms E (2) 933 734,337 22Neurology E (1) 624 8,765,899 197Neurology E (2) 923 779,699 24Middle Ages G 614 6,774,794 195Fish G (1) 655 7,621,579 199Fish G (2) 804 688,882 32Holocaust G 616 5,659,924 160Mushrooms G (1) 527 5,951,305 147Mushrooms G (2) 614 538,575 28Neurology G (1) 486 4,322,952 115Neurology G (2) 323 345,070 12checked becomes too large.
In practice, a large number of error candidates representcorrect tokens.
This is mainly due to special names and other types of nonstandardvocabulary found in Web pages, as mentioned in the introduction.We decided to use a third strategy, which can be considered as a synthesis andcompromise between the above two approaches.
As a starting point, we took stan-dard dictionaries of English, D(English); German, D(German); French, D(French);and Spanish, D(Spanish); and a dictionary of geographic entities, D(Geos); a dictio-nary of proper names, D(Names); and a dictionary of abbreviations and acronyms,D(Abbreviations).1 The number of entries in the dictionaries is described in Table 3.The German dictionary contains compound nouns, which explains the large numberof entries.From these standard dictionaries, we derived special error dictionaries that wereused in the experiments described later.
First, for each of the four error types mentionedabove we manually collected a number of general patterns that ?explain?
possiblemutations from correct words to erroneous entries.
In a second step, these patterns wereused to garble the words of the given background dictionaries.
Third, garbled wordsthat were found to correspond to correct words (entries of the above dictionaries) wereexcluded (filtering step).
Collecting the remaining erroneous strings, we obtained largeerror dictionaries for each type of orthographic error.Experiments described in Section 5 show that our error dictionaries cover the majorpart of all orthographic errors occurring in the English and German Web pages.
At1 These dictionaries are nonpublic.
They have been built up at the Centre for Information and LanguageProcessing (CIS) during the last two decades (Maier-Meyer 1995; Guenthner 1996).
Each entry comeswith a frequency value that describes the number of occurrences in a 1.5-terabyte subcorpus of the Webfrom 1999.
Dictionaries for French and Spanish were included to improve the filtering step.
Suitabledictionaries for other languages were not available.301Computational Linguistics Volume 32, Number 3Table 3Size of background dictionaries.Dictionary Number of entriesD(English) 315, 300D(German) 2, 235, 136D(French) 85, 895D(Spanish) 69, 634D(Geos) 195, 700D(Names) 372, 628D(Abbreviations) 2, 375the same time, the number of tokens that are erroneously treated as errors due to theunavoidable incompleteness of the filtering step remains acceptable.
On this basis, anestimate of the number of conventional orthographic errors occurring in Web pages ispossible, counting the number of occurrences of entries of the error dictionaries.2 Beforewe comment on these points, we describe the construction of the error dictionaries inmore detail.
In the remainder of the article, by Dconv we denote the union of all theconventional dictionaries listed above.4.
Construction of Error DictionariesFor the construction of error dictionaries, the most important error patterns for eachtype of error were determined.
For typing errors and errors caused by character en-coding problems, error patterns were obtained analytically.
For spelling errors andOptical Character Recognition (OCR) errors, important mutation patterns were col-lected empirically.
As a general rule, all error dictionaries were restricted to entries oflength >4.
Many tokens of length ?4 occurring in texts represent acronyms, specialnames, and abbreviations, and it is difficult to mechanically distinguish between thisspecial kind of vocabulary and errors.4.1 Error Dictionaries for Typing ErrorsTyping errors can be partitioned into transpositions, deletions, substitutions, and inser-tions.
Transpositions of two letters occur if two keys are hit in the wrong order.
Deletionsresult if a key is not properly pushed down.
Substitutions occur if a neighbor key ispressed down instead of the intended one.
Horizontal and vertical shifts of fingers maybe distinguished.
If a finger hits the middle between two keys, a neighbor key may bepressed in addition to the intended one.
The wrong letter may occur before or after thecorrect letter.Transpositions, deletions, substitutions, and insertions cover most of the typingerrors discussed in the literature (Kukich 1992).
We ignored homologous errors, thatis, substitutions that are traced back to a confusion of the left and right hand.
Since2 Note that we do not capture false friends, that is, garbled strings that accidentally represent correctwords of the dictionary.
Detection of false friends is known to be notoriously difficult and outside thescope of this article.302Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pagesthere are many possible positions for both hands, this kind of error leads to largeconfusion sets.Since we did not find other patterns in the texts, only mutation variants that areexclusively composed of standard letters (as opposed to digits and other special sym-bols) were taken into account.
Furthermore, since typing errors in general do not affectthe first letter of a word,3 we left this letter unmodified.
We analyzed the number ofmutated variants of a given word.
Both for the American and for the German keyboardwe have approximately 16l variants for a word of length l. This shows that the abovepatterns for typing errors are very productive.
It is not possible to garble all the wordsof our background dictionary for constructing the error dictionaries.
For the generationof the dictionary of English typing errors, Derr(English,typing), we took the 100,000entries of the English background dictionary with the highest frequency.
Applying theabove mutation patterns we generated 10,785,675 strings.
After removal of duplicatesand deletion of words in Dconv (filtering step), we obtained 9, 427, 051 entries for thedictionary Derr(English,typing).The same procedure was used for creating the dictionary of German typing errors,Derr(German,typing).
Since the average length of German words is large, we obtained13, 656, 866 entries.4.2 Error Dictionaries for Spelling ErrorsEnglish.
In order to find the most characteristic patterns for English spelling errors, abootstrapping procedure was used to compute an initial list of errors.
We started withthe misspelled English words verry, noticable, arguement, and inteligence.
For each termwe retrieved 20 Web documents.
After conversion to ASCII we computed the list of allnormal tokens occurring in these documents.
The resulting list was sorted by frequency,and words in Dconv were filtered out.
After a manual selection of new errors with highGoogle counts, the procedure was iterated until we did not find new erroneous wordswith high frequency.
During the bootstrapping procedure, we also found Web pagesthat listed some ?common misspelled words?
of English.
The most frequent errorsmentioned in these lists were also added.
Table 4 presents some strings that were foundwith a large number of Google hits.4Most of the errors that we found can be traced back to a rule set partially describedin Table 5.
The full rule set contains 95 rules.
We applied each rule to D(English),introducing one error at the first possible position, for each entry of the appropriateform.
As a result we obtained a list with 1,223,128 garbled strings.
After applying thestandard filtering procedure, we obtained the dictionary Derr(English,spell) of Englishspelling errors with 1,202,997 entries.German.
Similarly as for English, we built an initial error list.
Bootstrapping was startedwith the misspelled German terms na?hmlich, addresse, resourcen, and vorraus.
Table 6shows some of the resulting German words, the misspelled variant, and the numberof Google hits of the garbled version.
From the initial error list, we obtained a set of 65rules partially described in Table 7.
We applied these rules to D(German), introducingone error for each entry of the appropriate form.
Each rule was applied to each entryusing the first possible position for mutation.
For example, for the lexical entry Adresse of3 A phenomenon often discussed in the literature; see, for example, Kukich (1992), page 388f.4 It is well-known that the number of Google hits for a phrase can vary from one day to the next.303Computational Linguistics Volume 32, Number 3Table 4Some frequently misspelled English words and the number of Google hits of the correctand misspelled forms.Word Google hits Transformation Misspelled variant Google hitsaccommodate 5,800,000 mm ?
m accomodate 559,000category 109,000,000 teg ?
tag catagory 525,000definitely 10,800,000 itely ?
ately definately 1,270,000independent 25,700,000 dent ?
dant independant 523,000millennium 10,500,000 nn ?
n millenium 2,540,000occurrence 4,640,000 rr ?
r occurence 279,000receive 57,000,000 ie ?
ei recieve 1,260,000recommend 31,400,000 mm ?
m recomend 707,000separate 26,300,000 ara ?
era seperate 1,340,000the German standard dictionary we obtained the following error terms: adrese, ahdresse,adrehsse, addresse, adrresse.
As a result we obtained a list with 19, 265, 271 strings.
Thelarge size is mainly caused by the rules for reduplication of consonants, which arenot restricted by word context.
The filtering procedure led to an error dictionary with18, 970, 716 entries.Table 5Rule set (incomplete) for the generation of English spelling errors with examples for eachtransformation class.Deletion of doubled consonantscc ?
c occasionally ?
ocasionallynn ?
n drunkenness ?
drunkenessDeletion of consecutive consonantsmn ?
m column ?
columrh ?
r rhythm ?
rythmDeletion of doubled vowelsee ?
e exceed ?
exceduu ?
u vacuum ?
vacumDeletion in vowel pairaison ?
ason liaison ?
liasonou ?
o mischievous ?
mischievosievous ?
evious mischievous ?
mischeviousDeletion of silent vowels?ed ?
?d maintained ?
maintaindSubstitution of consonantssede ?
cede supersede ?
supercededent ?
dant independent ?
independantSubstitution of vowelsitely ?
ately definitely ?
definatelyteg ?
tag category ?
catagoryInsertion/reduplication of consonants?
?
{c,d,f,l,n,m,p,r,s,t} ?
??
always ?
allwaysTransposition of consonantsght ?
gth right ?
rigthTransposition of vowelsie ?
ei believe ?
beleive304Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 6Some frequently misspelled German words and the number of Google hits of themisspelled version.Word Google hits Transformation Misspelled version Google hitsWeihnachten 5,450,000 ih ?
i Weinachten 99,600Adresse 8,040,000 d ?
dd Addresse 676,000Videothek 581,000 th ?
t Videotek 18,300Kamera 10,900,000 mm ?
m Kammera 14,200deshalb 8,330,000 s ?
ss desshalb 33,900ziemlich 2,970,000 i ?
ih ziehmlich 48,900ekelig 20,600 lig ?
lich ekelich 17,200na?mlich 1,620,000 a?
?
a?h na?hmlich 53,800Maschine 1,840,000 i ?
ie Maschiene 28,300direkt 18,200,000 ek ?
eck direckt 20,600danach 5,100,000 n ?
nn dannach 46,200voraus 1,960,000 r ?
rr vorraus 214,0004.3 Error Dictionaries for OCR ErrorsAs a starting point we used a list of typical OCR errors that we found in a corpus with200 pages of OCR output (Ringlstetter 2003).
Error types are shown in Table 8.Table 7Rule set (incomplete) for the generation of German spelling errors.
The symbol ?t means that t isnot the preceding letter.Deletion of doubled consonantsdd ?
d Kuddelmuddel ?
Kudelmuddelmm ?
m Kommando ?
KomandoSpecial rules for deletion of consonantsmn ?
m Kolumne ?
Kolumea?h ?
a?
a?hnlich ?
a?nlichDeletion of vowelsie ?
i ziemlich ?
zimlichaa ?
a Aal ?
AlSubstitution of consonantsnt ?
nd eigentlich ?
eigendlichrd ?
rt Standard ?
StandartSubstitution of vowelsa?
?
e Empfa?nger ?
Empfengerera ?
ara Temperatur ?
TemparaturInsertion/reduplication of consonants[aeioua?o?u?]
?
[aeioua?o?u?
]h viel ?
viehl[aeioua?o?u?
]k ?
[aeioua?o?u?
]ck direkt ?
direckt?
?
{d,f,l,n,m,p,r,t} ?
??
Gro?britannien ?
Gro?brittannien?tz ?
tz Schweiz ?
SchweitzInsertion of vowelsi ?
ie Maschine ?
MaschieneShiftinga?u ?
au?
a?u?erst ?
au?
?erstllel ?
lell parallel ?
paralell305Computational Linguistics Volume 32, Number 3Table 8List of typical OCR errors.Character substitutions Character merges Character splitsl ?
i rn ?
m m ?
rni ?
l ri ?
n n ?
rig ?
q cl ?
d u?
?
iio ?
pl ?
tv ?
yy ?
vo ?
ce ?
cl ?
1English.
The error dictionary Derr(English,ocr) was generated by applying to the en-tries of D(English) the transformation rules listed in Table 8.
The transformation ofD(English) with its 315, 300 entries led to a list of 1,697,189 entries.
The filtering pro-cedure where we erase words from Dconv led to the error dictionary Derr(English, ocr)with 1, 532, 741 entries.
Table 9 shows some of the most frequent English words, thetransformation result, and the number of Google hits of the garbled variant.German.
When scanning German texts, vowels a?, o?, and u?
are often replaced bytheir counterparts a, o, u.
However, even more frequently, this kind of replacementoccurs as the result of a character encoding problem (see below).
Since we wantedto avoid having our statistics for OCR errors being heavily overloaded with errorscaused by character encoding problems, we did not add these patterns to the list oftypical OCR errors for German texts.
This means that we applied to D(German) thetransformation rules mentioned in Table 8.
The transformation of D(German) with its2, 235, 136 entries led to a list of 11, 623, 989 strings.
After filtering, we obtained the errordictionary Derr(German,ocr) with 10, 608, 635 entries.
Table 10 shows some frequentGerman words, the transformation result, and the number of Google hits of the garbledvariant.Table 9Some members of the top 1,000 most frequent English words transformed by typical OCR errortransformations and the number of Google hits of a garbled version.Word Transformation Garbled result Google hitscompany m?rn cornpany 1.220from m ?
rn frorn 5,310government rn ?
m governrnent 705many m ?
rn rnany 541market m ?
rn rnarket 282more m ?
rn rnore 707most m ?
rn rnost 1,540only y ?
v onlv 4,080said d ?
cl saicl 172system m ?
rn systern 2,060time m ?
rn tirne 2,090will ll ?
11 wi11 3,570306Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 10Some members of the top 1,000 most frequent German words transformed by typical OCR errortransformations and the number of Google hits of a garbled version.Word Transformation Garbled result Google hitsDipl-Ing l ?
i Dipi-Ing 213u?ber u?
?
ii iiber 2,360vorne rn ?
m vome 1,110davon o ?
p davpn 96lager g ?
q laqer 164ferner rn ?
m femer 8414.4 Error Dictionaries with Erroneous Character Encoding of German WordsIn character sets used for the encoding of Web pages, often the German letters A?, O?, U?,a?, o?, u?, and ?
(?sharp s?)
are not available.
In many of these cases, vocals are replaced,following the substitution scheme (e-transformation):A?
?
Ae, O?
?
Oe, U?
?
Ue, a?
?
ae, o?
?
oe, u?
?
ue.In other Web pages, the aforementioned vocals are replaced using the following scheme:A?
?
A, O?
?
O, U?
?
U, a?
?
a, o?
?
o, u?
?
u.This transformation, which is typically found in Web pages written by non-nativespeakers of German, will be called -transformation.Table 11 shows some transformed terms of the top 1,000 German words and givesthe number of Google hits for correct and incorrect spellings.
The right-hand side ofthe table gives the corresponding numbers for PDF documents.
The numbers show thatmisspellings caused by e-transformation are a widespread phenomenon.
Note that thequality of PDF corpora is much better in this respect.When applying the e- or -transformation, letter ?
is typically replaced by ss(s-transformation).
For two reasons, the distinction between ?
and ss is a delicate matter.Since the Swiss spelling is ss, a string representing an erroneous German word may be acorrect Swiss word.
To make things even more complicated, the correct spelling of manyGerman words has been changed during the so-called ?Rechtschreibereform?
someyears ago, which affected the selection between ?
and ss (e.g., Mi?versta?ndnis becameMissversta?ndnis).
Still (and unofficially), the old spelling variant is broadly used.
In whatfollows, a token written with ss that is officially written with ?
is treated as an error.We built two error dictionaries respectively representing errors introduced viae-transformation and -transformation.
All vowels of the form a?, o?, u?
(or upper-casevariants) in the German dictionary were replaced by their images under the respectivetransformation.
Letter ?
occurring in the entries was categorically replaced by ss.
Forthe e-transformation we obtained a list of 436, 198 strings.
The filtering procedure led toan error dictionary Derr(German, enc-e) with 432, 987 entries.Applying the -transformation and the usual filtering step, we generated the errordictionary Derr(German,enc-) with 407, 013 entries.
A considerable number of well-formed words was generated and filtered out.
The rules of German morphology yield a307Computational Linguistics Volume 32, Number 3partial explanation: For so-called strong verbs some paradigmatic forms only differ bya mutation of vowels (mo?chte-mochte).An extra error dictionary Derr(German,enc-s) was built by replacing ?
by ss inGerman dictionary entries without occurrences of vocals A?, O?, U?, a?, o?, u?.
The dictionaryhas 42, 340 entries.4.5 Summary and Maximal Error DictionariesUsing the union of all error dictionaries for both languages, we constructed the maximalerror dictionaries Derr(English,all) and Derr(German,all).
Table 12 summarizes the sizesof all error dictionaries.5.
Error Overproduction and UnderproductionBefore we analyze the number of tokens in the corpora that represent entries of theerror dictionaries, we comment on the limitations of this kind of analysis.
Obviously,not all orthographic errors of a given type occur in the respective error dictionary(underproduction).
On the other hand, some tokens classified as errors by the errordictionary might in fact be correct words (overproduction) due to the incompleteness ofTable 11Most frequent German words with vowels a?, o?, u?
; frequencies of correct spelling and frequencyafter applying e-transformation.
Frequencies are counted in arbitrary Web pages (left part of thetable) and in PDF documents in the Web.Word Norm.
Transf.
Percentage PDF norm.
PDF transf.
Percentagefu?r 19,000,000 5,140,000 27.05 4,050,000 30,900 0.76u?ber 17,800,000 2,330,000 13.08 3,610,000 16,000 0.44ko?nnen 14,500,000 290,000 2.00 1,790,000 3,960 0.22mu?ssen 7,420,000 177,000 2.38 1,090,000 2,060 0.18wa?re 3,500,000 173,000 4.94 590,000 631 0.11fu?nf 2,470,000 291,000 11.78 541,000 570 0.10ko?nnte 2,900,000 165,000 5.69 570,000 618 0.11ha?tten 815,000 43,100 5.28 234,000 315 0.13dafu?r 3,580,800 124,000 3.46 814,000 865 0.11wu?rde 3,770,000 162,000 4.30 601,000 693 0.11Table 12Size of error dictionaries.Error dictionary Entries Error dictionary EntriesDerr(English,typing) 9, 427, 051 Derr(German,typing) 13, 656, 866Derr(English,spell) 1, 202, 997 Derr(German,spell) 18, 970, 716Derr(English,ocr) 1, 532, 741 Derr(German,ocr) 10, 608, 635Derr(German,enc-e) 432, 987Derr(German,enc-) 407, 013Derr(German,enc-s) 42, 340Derr(English,all) 11, 884, 284 Derr(German,all) 43, 688, 771308Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pagesthe final filtering step in the construction of the error dictionaries.
From the constructionof the error dictionaries we may expect that incompleteness/underproduction is mainlycaused by missing patterns for spelling errors and OCR errors, and the fact that we do not seriously damage words when constructing theerror dictionaries.For both English and German, to estimate under/overproduction of the error dictio-naries, the primary general HTML corpus was split into four subclasses.
The classBest contains all documents where the number of hits (tokens representing entriesof the maximal error dictionary) per 1,000 tokens is ?1.
For class Good (Bad, Worst,respectively), the number of hits per 1,000 tokens is 1?5 (5?10, >10, respectively).
Thenumber of documents in each class is found in Tables 13 and 14.5.1 Estimating UnderproductionTo estimate underproduction of the English error dictionaries, the English generalHTML corpus was split into subfiles, each containing 300 tokens.
We then randomlyselected such subfiles and analyzed the proper errors found in these portions.
Sincewe wanted to avoid an unbalanced selection where most errors are from the doc-ument class Worst, a maximum of three errors from each subfile was used for theanalysis.
Error candidates were found with the help of a spell checker and usingour standard dictionaries as a second control.
Slang and special vocabulary werenot used for the statistics.
We also excluded errors where two words were merged.We found that most of the latter errors were caused by the conversion process fromHTML to ASCII.
Each candidate was manually controlled; in difficult cases we con-sulted Merriam-Webster Online.
We continued the search until 1,000 proper errors wereisolated.
From these, 624 (62.4%) turned out to be entries of the maximal English errordictionary.Table 13 refines these statistics and shows the number of errors and the percentageof errors found in the error dictionary for the four quality classes of documents.
As atendency, recall of the error dictionary is better in ?bad?
documents.The same procedure was used for German and confirmed this tendency.
From 1,000errors in the German general HTML corpus, 638 (63.80%) were found in the maximalGerman error dictionary.
The statistics for the four quality classes of documents ispresented in Table 14.5.2 Estimating OverproductionIn our first experiment with English texts we found that a considerable number of hitscorresponded to special names introduced in the documents.
Many of these names areartificial (e.g., Hitty).
In order to avoid all difficulties with special names we decidedto restrict the error analysis in English texts to words starting with a lowercase letter.In each of the four classes, 1,000 hits of this form were randomly selected.
We thenmanually checked which of these tokens represent correct words, reading contexts andconsulting Merriam-Webster Online in difficult cases.The results are presented in Table 15 and show a clear tendency.
The percentage ofproper errors is larger in documents with a large number of hits.
In the class Worst, 95%309Computational Linguistics Volume 32, Number 3Table 13Underproduction of the maximal error dictionary in the primary English general HTML corpus.Document class Documents Errors found Entries of error dict.
%Worst 24 248 166 66.93Bad 39 194 131 67.53Good 226 389 242 62.21Best 540 169 85 50.29Table 14Underproduction of the maximal error dictionary in the primary German general HTML corpus.Document class Documents Errors found Entries of error dict.
%Worst 50 389 307 78.92Bad 42 166 101 60.84Good 297 385 201 52.21Best 229 60 29 48.33of all hits are proper errors; in the class Best, only 60% of the hits represent orthographicerrors.
Most of the remaining hits could be assigned to one of the following categories:correct standard expressions (missing entries of the standard dictionaries), names andgeographic expressions, foreign language expressions, archaic and literary word forms,and abbreviations.
The number of hits in each category is found in Table 15.
The largenumber of standard words among the hits in the class Best is caused by an incomplete-ness of our English dictionary, which does not always contain both the British and theAmerican spelling variants.In the German general HTML corpus, where we could not restrict the experimentto tokens starting with a lowercase letter, a more shallow picture is obtained (Table 16).For the classes Best (61% proper errors), Good (62% proper errors), and Worst (88%proper errors), results are similar to the English case and confirm the above-mentionedgeneral tendency.
Due to the large number of names, foreign language expressions, andarchaic/literary word forms that are found in class Bad, we here have only 56% propererrors.
The results show that overproduction could be considerably reduced when filter-ing error dictionaries with better standard dictionaries for geographic entities, personalnames, foreign language expressions, and archaic and literary word forms.Table 15Overproduction of the maximal error dictionary in the English general HTML corpus.Document class Best Good Bad WorstHits 1,000 1,000 1,000 1,000Percentage proper errors 72 86 89 95Proper errors 722 856 894 952Standard words 206 31 21 5Personal names and geographic entities 23 35 24 27Foreign language expressions 32 42 36 12Archaic and literary word forms 9 28 1 1Abbreviations 8 6 24 2310Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages5.3 Summary So FarFrom the above percentages we obtain a naive estimate for the ratio between the realnumber of errors and the number of hits of the error dictionaries, which is presented inTable 17.
The results show that the number of hits can be seen as a lower approximationof the real number of errors.
The ratio between both numbers is larger for English.
Itdoes not differ dramatically between the distinct quality classes.
However, since bothover- and underproduction are larger for ?good?
documents, error estimates for theseclasses come with a larger degree of uncertainty.5.4 DifficultiesThe above analysis turned out to be much more time-consuming and difficult thanit might appear.
One problem is caused by the fact that nonstandard vocabulary anderrors do not represent disjoint categories.
Orthographic errors are sometimes ?abused?as slang expressions.
A separation between archaic/foreign language expressions andorthographic errors is often only possible when taking the sentence context intoaccount.
These and other examples explain that demarcation issues are sometimesdifficult to solve.
The construction of special dictionaries for slang, foreign languageexpressions, special names, and archaic word forms represents an important stepfor future work.
Using these dictionaries in the filtering step of the construction ofthe error dictionaries, overproduction may probably be reduced in a significant way.Furthermore, these dictionaries should help to detect Web pages with nonstandardvocabulary of a particular type.Table 16Overproduction of the maximal error dictionary in the German general HTML corpus.Document class Best Good Bad WorstHits 1,000 1,000 1,000 1,000Percentage proper errors 61 62 56 88Proper errors 615 624 564 884Standard words 126 123 47 3Names and geos 201 147 193 49Foreign language expressions 31 46 103 37Archaic and literary word forms 18 44 82 24Abbreviations 9 16 11 3Table 17Naive estimates of the ratio between the real number of errors and the number of hits of theerror dictionaries for distinct quality classes.English GermanBest 0.72/0.5029 = 1.43 Best 0.61/0.4833 = 1.26Good 0.86/0.6221 = 1.38 Good 0.62/0.5221 = 1.19Bad 0.89/0.6753 = 1.32 Bad 0.56/0.6084 = 0.92Worst 0.95/0.6693 = 1.42 Worst 0.88/0.7892 = 1.12311Computational Linguistics Volume 32, Number 36.
Distribution of Orthographic Errors in the General HTML CorporaWe define the error rate of a text with respect to an error dictionary Derr as the averagenumber of entries of Derr that are found among 1,000 tokens of the text.
In this sectionwe describe the distribution of error rates for all types of errors in the general HTMLcorpora.
Experiments for other corpora are summarized in the following section.
Theresults of the previous section indicate that the error rate represents a reasonable lowerapproximation for the real number of errors per 1,000 tokens in the document.
Incom-pleteness of the rule sets for generating spelling errors and OCR errors should be keptin mind.
Recall that in English documents, only words starting with a lowercase letterare taken into account.6.1 Distribution of Error Rates for Orthographic ErrorsIn the first test, we consider orthographic errors, that is, errors of arbitrary type.
Ac-cordingly, error rates for documents are computed with respect to the maximal errordictionaries.
For a coarse survey, as in the previous section, we distinguish four qualityclasses Best, Good, Bad, Worst, respectively, covering pages with error rates in theintervals [0, 1), [1, 5), [5, 10), and [10,?).English.
The histograms in Figure 1 show the percentage of documents in each class inthe primary (left-hand side) and secondary (right-hand side) English corpora.
Remark-ably, the differences between the two corpora are almost negligible.
In both cases, mostdocuments belong to class Best; only a small percentage of documents belongs to classesBad and Worst.Table 18 presents the average error rate for various document classes.
As to thelength of documents in the corpora, drastic differences exist.
We did not find a cor-relation between document length and error rates, with the following eye-catchingexception: small (larger) documents of an excellent quality tend to have an error rate0 (close to 0, but >0).5 In order to avoid a dominating influence of long documents,we simply computed the arithmetic mean of all error rates obtained for the singledocuments.
The class Best 80% collects 80% of all documents with lowest error rate,and similarly for the class Best 90%.Note that a significant difference exists between the average rate for all documents(2.47, 2.24, respectively) and the means for the Best 80% classes (0.67, 0.68, respectively).These numbers point to an effect that will be found again in other figures and exper-iments: The large majority of all documents in the corpora have a very good quality.Yet, at the ?bad end?
of the spectrum we find a considerable number of unacceptabledocuments with a very large number of errors.
The phenomenon becomes even moreapparent in Figure 2 (left diagram) where we depict the error rates of all documents.In what follows we often describe mean error rates for all documents and for the classBest 80%.
When comparing distinct corpora, the two values help to see if deviationsconcern the class of all documents or if they are rather caused by a small number of?bad?
documents.Note also that all corresponding average error rates obtained for the primary andsecondary corpora are almost identical.
This gives at least some evidence to the conjec-5 This explains the special effect seen in Figures 14 and 15 where the refined crawl produces many shortdocuments.312Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesFigure 1Percentage of documents in the four quality classes for the primary (left-hand side) andsecondary (right-hand side) English corpora.
The four quality classes cover distinct errorrates for orthographic errors.ture that for corpora crawled with similar queries and collection strategies, error rateswill not differ too much.
As we see next, the situation for the German corpora is morecomplex.German.
The histogram in Figure 3 shows the percentage of documents in each classof the primary (left-hand side) and secondary (right-hand side) German corpora.
Alarge number of documents belongs to class Good.
We now find a larger differencebetween the primary and secondary corpora.
Several phenomena might be responsible.As mentioned above, for the German corpora we did not restrict the analysis to tokensstarting with a lowercase letter.
Hence, documents with many names can cause specialeffects and lead to differences between corpora.
Second, errors caused by encodingof special characters represent an important extra source for errors in German docu-ments where numbers may vary from one corpus to another.
This is seen in Table 20where we analyze all error types occurring in the primary and secondary Germancorpus.
The means for e-transformation are 0.62 for the primary corpus and 1.40 for thesecondary corpus.The average error rates obtained for distinct documents classes of the Germancorpus, which are presented in Table 19, show that for all classes we have more errors than in the English documents, and for different corpora, sometimes nontrivial deviations must be expected.Table 18Mean error rate for arbitrary orthographic errors in various document classes; results for thegeneral English HTML corpus.Document class Best Good Bad Worst Best 80% Best 90% AllE (1) 0.30 2.31 8.83 23.23 0.67 1.06 2.47E (2) 0.27 2.19 6.77 21.61 0.68 1.03 2.24313Computational Linguistics Volume 32, Number 3Figure 2Distribution of error rates for arbitrary orthographic errors in the primary English (left diagram,829 documents, mean error rate 2.47) and the German (right diagram, 618 documents, meanerror rate 3.86) general HTML corpora.
On the x-axis, documents are ordered by error rates;documents with high rates are found on the right-hand side.
In the left diagram, 7 documentswith error rates ranging from 42.99 to 64.22 have been omitted to simplify scaling.
In the rightdiagram, one document with error rate 40.07 is omitted.A more detailed picture of the error rates in the primary German corpus is givenin Figure 2 (right diagram).
The two curves of the figure show that despite the afore-mentioned differences between English and German, basic features of the error ratedistribution are very similar.6.2 Error Rates for Particular Error ClassesTypographic Errors.
The most widespread subclass of errors found in the corporaare typographic errors.
For the primary English corpus, as many as 2.31 of 2.47 hits(93.5%) can be classified as typing errors.6 The percentage is lower in the German corpus(2.15/3.86, 55.7%) where e-transformation, -transformation, and s-transformation rep-resent additional important sources for errors (see below).
In absolute numbers, errorrates for typographic errors observed in the two corpora are similar.The histograms in Figure 4 show the percentage of documents with error rates fortypographic errors in the four intervals [0, 1), [1, 5), [5, 10), and [10,?)
for the primaryand secondary English corpora (upper diagrams of Figure 4) and the correspondingGerman corpora (lower diagrams of Figure 4).
Note again the close similarity betweenthe two English corpora.
The detailed distribution curves, which are similar to thecurves obtained for orthographic errors in Figure 2, are omitted.Spelling Errors.
The two diagrams in Figure 5 show that the error rates found inthe primary English corpus (mean 0.39) are similar to the ones found in the primaryGerman corpus (mean 0.45).
The results presented in Section 5.1 indicate that our errordictionaries for spelling errors are incomplete.
Hence the real number of spelling errorsis probably larger.
We also computed error rates for spelling errors in the secondary6 Recall that the error type of a garbled token may be ambiguous.314Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesFigure 3Percentage of documents in the four quality classes for the primary (left-hand side) andsecondary (right-hand side) German corpora.
The four quality classes cover distinct errorrates for orthographic errors.corpora; results are presented in Table 20.
The tendency observed earlier for ortho-graphic errors was confirmed: the difference between the two English corpora (mean0.39 versus mean 0.38) is negligible; for the two German corpora, the difference is larger(mean 0.45 versus mean 0.58).OCR Errors.
The diagrams in Figure 6 show that most documents do not contain anyOCR errors.
Of course this result is not surprising.
Probably not all errors that contributeto the two diagrams are really caused by wrong character recognition.
Although someof the documents with the highest errors were explicitly marked to contain scannedTable 19Mean error rate for arbitrary orthographic errors in various document classes; results for thegeneral German HTML corpus.Document class Best Good Bad Worst Best 80% Best 90% AllG (1) 0.41 2.61 7.30 15.15 1.89 2.58 3.86G (2) 0.48 2.57 7.21 24.38 2.40 3.09 5.40Table 20Mean of error rates for all error types in primary and secondary general HTML corpora.Error type Mean error rate Mean error rate Mean error rate Mean error rateEnglish corpus English corpus German corpus German corpusHTML (1) HTML (2) HTML (1) HTML (2)arbitrary 2.47 2.24 3.86 5.40typographic 2.31 2.03 2.15 2.79spelling 0.39 0.38 0.45 0.58OCR 0.06 0.07 0.13 0.18e-transformation 0.003 0.004 0.62 1.40-transformation 0.02 0.01 0.19 0.24s-transformation 0.00003 0.00 0.76 0.96315Computational Linguistics Volume 32, Number 3Figure 4Typographic errors: the percentage of documents in the four quality classes in the generalEnglish (upper part) and German (lower part) HTML corpora.
Quality classes refer to error ratesfor typographic errors.text, it is natural to assume that the total number of such documents in the corpus isvery small.
Ambiguous error types might explain some of the errors found in Figure 6;see the discussion below.
As a matter of fact, the number of OCR errors will grow whenanalyzing corpora with many OCRed pages.Figure 5Distribution of error rates for spelling errors in the primary English (left diagram, mean error rate0.39) and German (right diagram, mean error rate 0.45) general HTML corpora.
In the left (right)diagram, one document with error rate 14.95 (11.31) is omitted.316Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesFigure 6Distribution of error rates for OCR errors in the primary English (left diagram, mean error rate0.06) and the German (right diagram, mean error rate 0.13) general HTML corpora.e-transformation and -transformation.
Figures 7 and 8 show some interesting differ-ences between the use of both transformations in German Web pages.
In the primaryGerman corpus, e-transformation errors are concentrated in a small class of documents(documents with rank >480) where we have a nontrivial number of occurrences, lead-ing to a mean error rate of 0.62.
The mean error rate for -transformation is muchsmaller (0.19).
Still, there are more documents containing an -transformation error.This indicates that e-transformation is applied more systematically.
The small plateauin Figure 7 is generated by some portion of text that was found in several documents.The error rates that arise when applying e-transformation in a completely systematicway are typically larger.
In the corpus we found some documents of this kind; since therates are too high, these documents are not depicted in the figure.We also looked for e- and -transformation errors in the documents of the En-glish general HTML corpus.
These errors, which mutate German words, only occurFigure 7Distribution of error rates for e-transformation in the primary German general HTML corpus.Mean: 0.62.
Here 7 documents with error rates ranging from 13.16 to 34.10 are omitted.317Computational Linguistics Volume 32, Number 3Figure 8Distribution of error rates for -transformation in the primary German general HTML corpus.Mean 0.19.in a small number of English documents.
Whereas German writers strongly prefer thee-transformation in situations where the correct characters are not available, we find aclear preference for the -transformation in the English documents.s-Transformation.
Figure 9 shows the distribution of error rates for s-transformationin the primary German general HTML corpus.
Since the corpus contains some Swissdocuments, where ???
is categorically written ?ss?
(cf.
Section 4.4), the high mean (0.76)has to be relativized.Overview.
Table 20 summarizes the error rates of all types of errors in the general HTMLcorpora.
The numbers show that not all errors can be traced back to a unique errortype.Figure 9Distribution of error rates for s-transformation in the primary German general HTML corpus.Mean 0.76.
One document with error rate 11.46 is omitted.318Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages6.3 Summary So FarFor both languages, the large majority of all documents has a small number of ortho-graphic errors.
On the other hand, at the ?bad end?
of the spectrum, a considerablenumber of unacceptable documents with high error rates is found.
Mean values forerror rates are strongly influenced by the latter documents; the average error rate forthe Best 80% class is usually much lower.
The latter rate should also be considered whencomparing results obtained for two corpora.Phenomena observed in English corpora seem to be more stable than those forGerman: Results obtained for the primary and the secondary English general HTMLcorpus are almost identical.
Differences between the two German corpora may partiallybe explained by names occurring in texts and by special character encoding problems.Table 20 illustrates this effect, showing the mean error rates for all error types in theprimary and secondary HTML corpora.The most important error class are typographic errors.
In the German documents,e-transformation and s-transformation represent another typical error source.
Whereasthe number of spelling errors is significant, OCR errors do not play an essential role.Interestingly, the basic form of the distribution curves in Figure 2 is found again inall corresponding curves for other error types and other corpora (see also Figures 14and 15); although the absolute numbers for error rates and details are of course distinct.The close similarity of all distribution curves is striking and gives some evidence to theassumption that relevant features of the error rate distribution are stable, regardless ofthe corpora that are investigated.7.
Differences for Special CorporaWe summarize the error rates that we found in PDF corpora and in corpora for specialthematic fields.
In Figures 14 and 15, we present a small selection of distribution curvesfor error rates.
Similarities of the distribution curves mentioned in the previous sectionshould also be noted.7.1 Distribution of Orthographic Errors in the General PDF CorporaFigure 10 presents the mean error rates for distinct error types found in the generalPDF and (primary) HTML corpus for English.
The results show that PDF documents ingeneral have a better quality than HTML documents.
Whereas we have a mean errorrate of 2.47 for orthographic errors in the HTML documents, the corresponding meanis only 1.38 for PDF.
For the Best 80% documents the means are 0.67 (HTML) and 0.38(PDF).In principle, the same tendency was observed in the documents of the parallelGerman corpora.
However, special effects polluted the picture.
As we mentioned inSection 2.1, the conversion of the German PDF documents to ASCII is very errorprone.
Although we excluded all converted documents that were obviously garbledby the conversion, we also found in the remaining documents examples of errors thatwere caused by the conversion process.
In this sense, the error rates in the originalPDF documents are probably smaller.
Mean error rates are 2.15 (HTML) versus 2.04(PDF) for typographic errors, 0.45 versus 0.41 for spelling errors, 0.13 versus 0.09for OCR errors, 0.62 versus 0.07 for e-transformation errors, and 0.19 versus 0.16 for-transformation errors.
Since the conversion tool categorically replaces letter ???
by319Computational Linguistics Volume 32, Number 3Figure 10PDF versus HTML: mean error rates for distinct error types in the general corpora (English).Black rectangles describe mean error rates for the Best 80% subclass.
?ss?, a very high number of s-transformation errors led to the effect that the overallmean error rate for the German PDF (3.95) is even larger than the rate for the GermanHTML (3.86).7.2 Distribution of Orthographic Errors in Distinct Thematic CorporaFigure 11 describes the average error rates for orthographic errors and spelling errorsin the English corpora.
In almost all thematic areas, mean error rates are larger thanthe corresponding means in the general corpora; the differences are significant andremarkable.
With a mean error rate of 2.05 (0.30) for orthographic (spelling) errors,the English Neurology corpus is very clean and represents an exception.
For the Fishcorpus, even the mean error rate for the Best 80% subclass is 2.72.
We conjecture thatcorpora that are collected without a special thematic focus often contain a large numberof ?professional?
and carefully edited Web pages.
Web pages for special thematic areasFigure 11Thematic corpora versus general corpora: mean error rates for orthographic errors and spellingerrors in distinct English corpora.
All results refer to the primary thematic corpora crawled withthe simple strategy (cf.
Section 2.2).
Black rectangles represent mean error rates for the Best 80%subclass.320Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pagesare perhaps less ?publicity oriented.?
Furthermore, as a rule of thumb, documents inthematic fields related to hobbies (e.g.
Fish) contain more orthographic errors thandocuments in scientific fields (Holocaust, Neurology).
Corpora with a focus on historyseem to occupy a middle position.In the German corpora we have the means for orthographic/spelling error ratespresented in Table 21; numbers in brackets refer to the Best 80% subclass.
The secondcolumn shows that, by and large, the ranking order for thematic areas induced by meanerror rates observed in the English corpora is found again in the German part.
TheGerman corpus Neurology, with its high error rate, does not follow this line.
The highmeans for the Best 80% subclasses in the German corpora are remarkable and show thatthe low quality is not caused by a small number of bad documents.7.3 Differences between the Two Crawling StrategiesTable 22 summarizes the differences for the English corpora retrieved with the simplestrategy on the one hand and the corpora retrieved with the refined strategy on the otherhand.
Numbers represent average error rates for the corpora.
Numbers in brackets referto the Best 80% subclass.Surprisingly, all corpora crawled with the refined strategy always have a better(smaller) average error rate than those retrieved with the simple strategy, pointing toa significant difference between the two types of collection strategies.
An analysis of thedocument genres found in the two types of corpora presented in Section 8 offers a goodexplanation; see Table 26.Table 21Mean error rates for orthographic errors and spelling errors in thematic German corpora.German Orthographic errors Spelling errorsGeneral PDF 3.95 (2.31) 0.41 (0.06)Neurology G (1) (HTML) 6.94 (4.48) 0.51 (0.26)General HTML (1) 3.86 (1.81) 0.45 (0.16)Holocaust G (HTML) 4.97 (3.03) 0.50 (0.27)Mushrooms G (1) (HTML) 7.91 (3.69) 0.78 (0.32)Middle Ages G (HTML) 7.84 (4.30) 0.96 (0.38)Fish G (1) (HTML) 9.34 (4.47) 1.35 (0.52)Table 22Dependency of mean error rates on the crawling strategy for distinct English thematic corpora.Orthographic errors Spelling errors(1) (2) (1) (2)English Simple crawl Refined crawl Simple crawl Refined crawlFish E 7.08 (2.72) 3.39 (0.35) 0.98 (0.27) 0.47 (0)Mushrooms E 4.10 (1.49) 2.58 (0.32) 0.52 (0.13) 0.50 (0)Neurology E 2.05 (0.79) 1.77 (0.25) 0.30 (0.05) 0.26 (0)321Computational Linguistics Volume 32, Number 3Table 23Dependency of mean error rates on the crawling strategy for distinct German thematic corpora.Orthographic errors Spelling errors(1) (2) (1) (2)German Simple crawl Refined crawl Simple crawl Refined crawlFish G 9.34 (4.67) 7.71 (3.31) 1.35 (0.52) 1.00 (0.17)Mushrooms G 7.91 (3.69) 8.51 (3.50) 0.78 (0.32) 0.76 (0.08)Neurology G 6.94 (4.48) 7.08 (2.86) 0.51 (0.26) 0.47 (0.00)Figures 14 and 15 show that the corpora crawled with the refined strategy have alarge number of documents with error rate 0.
This special effect is caused by the largenumber of short documents that are obtained.
For example, the mean length of all thedocuments with error rate 0 in the corpus Fish E (2) is 322 (number of lowercase normaltokens), whereas the average length of the documents in the corpus Fish E (1) is 14,196(cf.
Table 2).The relative order between the three thematic fields was not affected by the crawlingstrategy.
For both crawls, the Neurology corpus achieves the best results, followed byMushrooms and Fish.
The excellent quality of the Best 80% classes obtained with therefined crawl are remarkable.For the German variant of the corpora, as Table 23 shows, a more shallow pictureis obtained.
For two thematic areas, the simple crawl even leads to lower error rates,although the difference is small.
The ranking order between the three thematic areasobtained from the two crawls is not the same.Figure 14 presents the error rates for orthographic errors in the English HTMLcorpora Fish, Mushrooms, and Neurology, comparing the simple strategy (left-handside diagrams) with the refined strategy (right-hand side diagrams).
Figure 15 givesthe error rates for spelling errors in the German HTML corpora Fish, Mushrooms, andNeurology, again comparing the simple and the refined strategies.7.4 Summary So FarPDF corpora were found to have lower error rates.
Corpora covering pages from non-scientific thematic areas often have higher error rates than corpora crawled without afixed thematic focus.
Error rates in the corpora are influenced by the crawling strategy.For English texts, refined crawling strategies that collect pages with a strong thematicfocus seem to produce better corpora.8.
Error Rates and Document GenreClassifying Web documents by genre (Kessler, Nunberg, and Schu?tze 1997; Finn andKushmerick 2003; Dimitrova et al 2003) represents one possible way to improve Websearch techniques.
Web-based corpus linguistics may benefit from these techniquessince they enable a better control of the kind of language material that is added toa collection.
In this section we want to see which kind of correlation exists betweenthe error rates observed in a document and its genre.
After manual inspection of322Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesFigure 12Zipf curves with logarithmic frequencies for English (upper diagram, 1,175,894 entries) andGerman (lower diagram, 454,709 entries) ranked error lists.
The diagrams respectivelyillustrate the frequency of particular orthographic errors in English and German Web pagesfrom a 1.4-terabyte subcorpus of the Web.hundreds of Web pages, we decided to use the following set of document genres forour investigations: The class Prof contains all Web pages with professional texts fromorganizations, enterprises, and administrations.
Also, scientific texts,professional literature, and fiction are added to this class. The class Priv contains private homepages and texts written from apersonal point of view.
A clue term is the personal pronoun I. Texts of thisform may dominate in a Web page run by an organization.
In this case, thepage was classified as Priv. The class Chat contains chat and related collections of private statementsand contributions such as guest books, mailing lists, and so forth.323Computational Linguistics Volume 32, Number 3Figure 13Distribution of error rates in documents (passed/rejected) by the filter F3 for threshold ?
= 5(English test corpus).
The left (right) diagram describes the distribution of documents passed(rejected) by the filter.
The average error rate for accepted (rejected) documents is 1.08 (16.81). The class Junk contains documents where the language is ?polluted,?
forexample, by large lists of erroneous keywords, lists of foreign languageexpressions, dominating subparts only consisting of program code, archaiclanguage, and so forth. The class Other contains all other documents.
In practice we tried to assignto each document one of the above four classes, and most documents inthe class Other are (almost) empty files.Even with this small number of classes, separation issues were sometimes difficult toaddress.
We did not introduce finer subclasses since we expected that the number ofambiguous and problematic cases would be multiplied.Our experiments on document genre were restricted to English corpora.
We lookedat the primary general English HTML corpus and on the English corpora for thedomains Fish, Mushrooms, and Neurology.
For each of the latter three domains, boththe corpus obtained with the simple crawling strategy and the corpus retrieved with therefined crawl were taken into account.
Hence, a total of 7 corpora were investigated.8.1 Genre Distribution of the Four Quality ClassesFor each corpus, all documents in the classes Worst and Bad were manually classi-fied, assigning one of the classes Prof, Priv, Chat, Junk, or Other to the document.From the classes Good and Best, 100 documents were randomly selected and clas-sified in the same way.
Table 24 presents the classification results for the primaryEnglish general HTML corpus.
Not surprisingly, classes Chat and Junk dominate atthe bad end of the quality spectrum, whereas class Prof dominates for good doc-uments.
The same tendency was found for all corpora, although the percentage ofProf documents in distinct quality classes was often larger.
To add one further typicalexample, Table 25 presents the result for the corpus Fish E (1) retrieved with the simplecrawling strategy.
Note that even for the Bad class, 50.62% of the documents are oftype Prof.324Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesFigure 14Distribution of error rates for arbitrary orthographic errors in the 6 English HTML corpora:Fish E (1) and Fish E (2) (upper diagrams), Mushrooms E (1) and Mushrooms E (2) (middle), andNeurology E (1) and Neurology E (2) (bottom diagrams).
Letters (1) (diagrams on the left-handside) refer to corpora retrieved with the simple crawling strategy.
Letters (2) (diagrams on theright-hand side) stand for the refined crawling strategy.
From the refined crawl (right-handsides) a large number of documents without any error hit is obtained.
Corpora crawled with therefined strategy typically contain a large number of short documents (cf.
Sections 2.2 and 7.3),and short documents of good quality often have an error rate 0.
A comparison along the verticaldimension illuminates differences between the three thematic areas: corpora Fish E contain moreerrors than corpora Mushrooms E, which contain more errors than the corpora Neurology E.Mean error rates are 7.08/3.39 [Fish E (1)/Fish E (2)]; 4.10/2.58 [Mushrooms E (1)/Mushrooms E(2)]; and 2.05/1.77 [Neurology E (1)/Neurology E (2)].
In the diagrams, some documents withhigh error rates are omitted to simplify scaling.325Computational Linguistics Volume 32, Number 3Figure 15Distribution of error rates for spelling errors in the 6 German HTML corpora: Fish G (1) and FishG (2) (upper diagrams), Mushrooms G (1) and Mushrooms G (2) (middle), and Neurology G (1)and Neurology G (2) (bottom diagrams).
Letters (1) (diagrams on the left-hand side) refer tocorpora retrieved with the simple crawling strategy.
Letters (2) (diagrams on the right-handside) stand for the refined crawling strategy.
The latter strategy leads to a large number ofshort documents without any hits in the error dictionaries.
See the discussion in Section 7.3.Similarly as for English HTML, corpora Fish G contain more errors than corpora Mushrooms G,which contain more errors than the corpora Neurology G. Mean error rates are 1.35/1.00[Fish G (1)/Fish G (2)]; 0.78/0.76 [Mushrooms G (1)/Mushrooms G (2)]; and 0.51/0.47[Neurology G (1)/Neurology G (2)].
In the diagrams, some documents with high error ratesare omitted to simplify scaling.326Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 24Genre distribution of the four quality classes for the primary general English HTML corpus.English HTML (1) Worst (%) Bad (%) Good (%) Best (%)Chat 42.31 56.41 24.00 1.00Junk 38.46 5.13 1.00 0.00Priv 3.85 10.26 14.00 9.00Prof 15.38 28.20 61.00 90.00Other 0.00 0.00 0.00 0.00Table 25Genre distribution of the four quality classes for the corpus Fish E (1).Fish E (1) Worst (%) Bad (%) Good (%) Best (%)Chat 37.39 20.99 4.00 3.00Junk 26.09 6.17 0.00 6.00Priv 8.70 22.22 9.00 3.00Prof 27.82 50.62 84.00 84.00Other 0.00 0.00 3.00 4.008.2 Genre Distribution: Simple Crawl versus Refined CrawlThe analysis of genres presented in Table 26 illuminates an important difference be-tween the thematic corpora retrieved with the simple and the refined crawling strategy:In the latter corpora, the percentage of documents of type Chat and Junk is lower;differences are significant.
At the same time, corpora retrieved with the refined strategycontain more documents of type Prof. We conjecture that the open compounds thatwere used in the queries for the refined crawl (cf.
Section 2.2) represent a kind of ?high-level language expressions?
that are typically used in a professional or scientific context.With the above background, it is not surprising that the refined crawling strategy leadsto better error rates.8.3 Error Rates for GenresTable 27 presents estimates for the mean error rates obtained for the distinct documentgenres in the seven corpora.
These numbers represent estimates since not all documentsTable 26Composition of corpora retrieved with the simple (1) and the refined (2) crawling strategies.
Therefined strategy (2) helps to avoid documents of type Chat and Junk, attracting documents oftype Prof at the same time.Crawls Fish E Fish E Mushr.
E Mushr.
E Neur.
E Neur.
E(1) (%) (2) (%) (1) (%) (2) (%) (1) (%) (2) (%)Chat 13.86 2.69 8.63 3.52 3.87 2.87Junk 9.10 0.88 5.40 3.15 2.97 0.11Priv 8.79 16.13 12.70 11.96 7.49 2.44Prof 66.03 80.30 73.27 80.01 82.83 94.58Other 2.22 0.00 0.00 1.36 2.84 0.00327Computational Linguistics Volume 32, Number 3Table 27Mean error rates (estimates) for distinct document genres in seven corpora.Crawls English Fish E Fish E Mushr.
E Mushr.
E Neur.
E Neur.
EHTML (1) (1) (2) (1) (2) (1) (2)Chat 6.90 13.05 14.29 10.71 6.27 4.94 11.22Junk 27.31 23.61 59.05 12.37 16.00 4.59 3.15Priv 2.82 7.85 3.16 3.34 3.37 3.79 5.89Prof 1.26 3.68 2.04 2.94 1.20 1.67 1.31of the classes Good and Best were classified.
In all corpora, the mean error rate for classProf is better than the rate for class Priv, which is better than the rate for class Chat.The results indicate that the error rate of a document might be an interesting featurefor genre classification: High error rates typically point to documents of the genres Junkand Chat; excellent error rates typically point to documents of type Prof.
Results forthe Neurology corpora indicate that ?scientific Chat/Junk?
may come with low errorrates.8.4 Summary So FarAn obvious correlation exists between the genre of a document and its error rate.
Errorrates might be used as one feature for genre classification.
The analysis of genres helpsone to understand the differences between corpora retrieved with distinct crawlingstrategies and the error rates observed in the corpora.9.
Filtering MethodsThe figures seen in the previous sections show that corpora collected from the Webtypically contain a non-negligible number of documents with an unacceptable numberof orthographic errors.
We now turn to the question of how to use error dictionaries forrecognizing and filtering Web pages with a high percentage of errors, thus excludingthem from the corpus construction process.
The question of what should be consideredas a ?high percentage?
has to be answered for each application.
Generally speaking wewould like to exclude at least those documents that are found at the right end of thediagrams presented in the previous sections.DefinitionBy a filter, we mean a pair F = ?D,??
consisting of an error dictionary, D, and afilter threshold, ?.
The filter rejects a text document (Web page) T iff the averagenumber of entries of D that are found among 1,000 tokens of T exceeds ?.As a matter of fact, we may use the maximal error dictionaries for filtering.
Forsome applications, small error dictionaries, which occupy less space and are easierto handle, may be advantageous.
The results presented below show that when oneuses a more rigid filter threshold ?, the filtering effect achieved with ?small?
errordictionaries is very similar to the effect when using the maximal error dictionaries.With an obvious interpolation, this observation supports the assumption that the328Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pagesincompleteness of our maximal error dictionaries does not seriously reduce their fil-tering capacities.9.1 Distribution of Error FrequenciesSince error dictionaries are necessarily incomplete in the sense that not all possibleerrors can be covered, it is natural to ask if filters of the above-mentioned formcan work.
We shall see below that even filters with small error dictionaries are use-ful.
The reason is that the frequency of orthographic errors in the Web follows aZipf-like7 distribution.
Since a relatively small number of erroneous tokens alreadycovers a substantial number of all error occurrences, it should not be surprising thateven small error dictionaries help to identify pages with many errors.
In Figure 12, weshow the logarithmic frequencies of errors in a 1.4-terabyte subcorpus retrieved from theWeb in 1999 (?Web-in-a-box?).
The upper diagram shows the distribution of all errorsfrom the maximal English error dictionary, Derr(English,all), in English Web pages.
Onlyerrors with at least two occurrences are covered.
Similarly the lower diagram shows thedistribution of errors from Derr(German,all) in German Web pages.9.2 Basic Filter ScenarioSuppose we are given a collection of Web pages, C. We may fix a user-defined threshold?
in terms of the average number of errors per 1,000 tokens that we are willing to acceptin a document to be added to our corpus.
A document where the average number oferrors per 1,000 tokens does not exceed ?
is called acceptable, other documents arecalled unacceptable.
In practice, since we cannot count real errors, a token is considerederroneous if and only if it occurs in one of our error dictionaries.
In Section 5, we haveseen that the number of entries of the error dictionary found in a text yields a lowerapproximation for the real number of errors.In terms of information retrieval, acceptable documents can be considered as rele-vant documents that we would like to retrieve for ?query?
?.
To extend this analogy,we define the answer set of a filter F w.r.t.
C as the set of all documents in C that arepassed by F .
With these notions we may now define the parameters?
precision andrecall.DefinitionLet ?, C, and F as above.
The precision of F with respect to ?
and C is thepercentage of acceptable documents in the answer set of F .
The recall of F withrespect to ?
and C is the number of acceptable documents in the answer set of Fdivided by the number of all acceptable documents in C.In the remainder of the section, we define and evaluate filters for the English andGerman general HTML corpora, which are denoted CE and CG, respectively.
We considerthree user-defined thresholds: ?
= 10, ?
= 5, and ?
= 1.
The first bound is meant toexclude a small number of documents with an extraordinary number of orthographicerrors.
The second bound is more ambitious.
The third bound might be used in7 Zipf?s law describes the frequency of words in large corpora.
It states that the i-th most frequent wordappears as many times as the most frequent one divided by i?, for some constant ?
?
1.329Computational Linguistics Volume 32, Number 3situations where high accuracy is needed and we want to retrieve only documents witha negligible number of orthographic errors.9.3 Automated Filter ConstructionWe define a hierarchy of filtersF1 = ?D1,?1?,F2 = ?D2,?2?,F3 = ?D3,?3?, .
.
.Filters Fk with higher index k generally lead to better results.
On the negative side, theyare more complex in terms of the number of entries of Dk.
In the following descriptionwe generally assume that a user-defined threshold ?
has been fixed.
For simplicity, werefer to the construction of filters for the English corpus, CE.
The same construction wasused, mutatis mutandis, for CG.
All filters are computed automatically on the basis oftraining data.
For training, two inputs were used.1.
Ranked error list.
We computed a list of all entries of the maximal Englisherror dictionary, Derr(English,all), that occur at least twice in the corpusWeb-in-a-box (cf.
Section 9.1).
The list was ordered by descendingfrequency of occurrence, as in Figure 12.
The resulting ranked error listcontains 1, 175, 894 entries.2.
2.
Training corpus.
The corpus CE was randomly split into a trainingsubcorpus (427 documents) and a test subcorpus (407 documents).8From the training corpus, all documents were excluded that did notcontain at least five distinct errors from the ranked error list, leaving384 documents.Definition of Filters.
The error dictionary Dk for filter Fk was defined as the minimalinitial segment S of the ranked error list such that each unacceptable document in thetraining corpus contains at least k distinct entries of the segment S. The threshold ?kwas defined as the minimal average number of occurrences of entries of Dk per 1,000tokens in an unacceptable document of the training corpus.
These entries need notbe distinct.Clearly, with the given threshold we achieve a precision of 100% on the trainingcorpus.
The philosophy behind this selection of a threshold is simple: We do not wantto add any unacceptable document to the corpus to be built.
Precision is much moreimportant than recall, as long as a substantial number of documents is retrieved.
Asa matter of fact, we cannot expect a 100% precision on the test corpus.
However, ourresults show that the loss of precision is not significant.9.4 Filtering Results for English General HTML CorpusIn what follows we consider the three user-defined thresholds ?
= 10, ?
= 5, and ?
= 1.For each of the filters F1 = (D1,?1), .
.
.
,F5 = (D5,?5), as defined earlier, Table 28 shows8 The distinct sizes of both corpora seem to indicate that the random selection was not perfectly balanced.We ignored this problem, which does not influence the construction.330Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 28Evaluation of filters Fk, 1 ?
k ?
5, for English general HTML corpus, user-defined threshold?
= 10 (top), ?
= 5 (middle), and ?
= 1 (bottom).|Dk| ?k PTrain (%) RTrain (%) PTest (%) RTest (%)?
= 10k = 1 12,217 0.91 100.00 85.42 99.67 80.00k = 2 21,037 1.83 100.00 89.79 99.69 84.73k = 3 46,111 2.19 100.00 91.83 99.40 87.63k = 4 110,201 4.63 100.00 93.87 99.71 91.31k = 5 291,309 5.62 100.00 93.00 99.70 89.21?
= 5k = 1 34,322 1.23 100.00 87.42 99.34 86.00k = 2 47,747 2.19 100.00 95.70 98.50 94.00k = 3 90,160 3.53 100.00 98.77 97.47 97.42k = 4 110,201 3.71 100.00 98.77 97.70 97.42k = 5 291,309 4.83 100.00 100.00 96.15 100.00?
= 1k = 1 37,994 0.13 100.00 51.15 93.43 55.89k = 2 169,507 0.49 100.00 78.35 96.75 78.16k = 3 279,543 0.63 100.00 86.14 97.02 85.58k = 4 299,397 0.67 100.00 90.90 97.10 87.77k = 5 580,330 0.89 100.00 97.40 96.06 96.91the size of the filter dictionary Dk (second column), the filter threshold ?k (third column),and the precision and recall values achieved with the filter on the training and testcorpora (columns 4, 5, 6, 7).Baselines.
When treating the complete test corpus as a ?naive?
answer set (recall 100%),we obtain1.
for ?
= 10, a precision of 94.76%, corresponding to 380 acceptable and 21unacceptable documents,2.
for ?
= 5, a precision of 87.28%, corresponding to 350 acceptable and 51unacceptable documents.3.
for ?
= 1, a precision of 57.10%, corresponding to 229 acceptable and 172unacceptable documents.For ?
= 10, with a precision (recall) of 99.40% (87.63%) on the test corpus, the filterF3 represents a good compromise between size and quality.
Precision is almost optimal.The answer set for the filter contains only one unacceptable document with an error rateof 13.24, which is very close to the threshold.For ?
= 5, using the filter F3 we obtain a precision (recall) of 97.47% (97.42%).
Aninspection of the nine unacceptable documents in the answer set of the filter shows thatthey come very close to the bound ?
= 5.
Note that error dictionaries D1, D2, and D3are larger than the corresponding dictionaries for the threshold k = 10 due to the largernumber of unacceptable documents in the training corpus.For ?
= 1, using the filter F3 we obtain a precision (recall) of 97.02% (85.58%).
Thereare six unacceptable documents in the answer set, all with an error rate below 2.
Thenumbers in Table 28 show how a more rigid (smaller) filter threshold compensates for331Computational Linguistics Volume 32, Number 3the reduced size of error dictionaries essentially without sacrificing precision and witha modest loss of recall.
To illustrate the effect of filtering, yet from another perspective,Figure 13 presents the distribution of error rates (number of entries from the maximalEnglish error dictionary Derr(English,all) per 1,000 tokens) in the answer set and in theset of documents rejected by the filter F3 constructed for the user-defined threshold ?
=5.
The filter was evaluated on the test subcorpus.
The figure shows that almost all docu-ments passed (rejected) by the filter have an error frequency below (beyond) 5 errors per1,000 tokens.9.5 Filtering Results for the German General HTML CorpusFor computing the ranked error list, a list with the frequencies of 18, 624, 436 tokens inGerman Web pages was used.
Via intersection with the list of all entries of the maximalGerman error dictionary, Derr(German,all), we obtained a ranked error list with 454, 709entries.
The training and test corpora contain 314 and 308 documents, respectively, fromthe German general HTML corpus.
Since the results are similar to the English case,we only point to some differences.
Frequencies decrease more rapidly in the Germanranked error list, as may be seen in Figure 12.
In the German list, the top-ranked part isdominated by e/-transformation errors and errors where the letter ?
is replaced by ss.The 10 top-ranked entries and their frequencies are shown in Table 29.
This special classof frequent errors leads to small filter dictionaries.
For example, the filter dictionary for?
= 10, k = 5 has 16,277 entries, and the dictionary for ?
= 5, k = 5 has 127,023 entries.On the other hand, the recall values achieved with the dictionaries in general are lowerthan in the English case.10.
Example ApplicationsObviously, the methods described above are very useful for all corpus tools that visuallypresent linguistic data from Web pages (words, n-grams, concordances, phrases, sen-tences, aligned bilingual material, etc.)
to the user.
Filters help to exclude inappropriatepages.
In the remaining data, tokens that represent entries of the error dictionaries canbe marked.
Depending on the application, the system may then decide to suppress thismaterial or to add a warning when presenting it.
In the remainder of this section, twocase studies are presented that demonstrate the usefulness of filtering techniques anderror dictionaries in distinct applications.10.1 Text Correction with Crawled DictionariesIt has often been observed that fixed handcrafted dictionaries only have a modestcoverage when applied to new texts and corpora.9 Still, for various text processing tasks,dictionaries with high coverage are needed.
The generation of crawled dictionaries thatcollect the vocabulary of appropriate Web pages is one way to obtain a better coverage.As a matter of fact, the quality of these dictionaries suffers from orthographic errors inthe analyzed pages.
Using the above filters helps to reduce the number of errors that are9 Kukich (1992) describes an experiment by Walker and Amsler (1986): ?Nearly two thirds (61%) of thewords in the Merriam-Webster Seventh Collegiate Dictionary did not appear in an eight million word corpusof New York Times news wire text, and, conversely, almost two-thirds (64%) of the words in the text werenot in the dictionary.
?332Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 29Top-ranked errors in German ranked error list and their frequencies.Entry of error list Correct word Error frequencyUniversitaet Universita?t 131,494grossen gro?en 107,904koennen ko?nnen 107,730knnen ko?nnen (kennen?)
87,167heisst hei?t 76,667andern a?ndern (anderen?)
73,972Gruss Gru?
51,721ausser au?er 42,410waere wa?re 37,071muessen mu?ssen 35,864imported.
In order to further improve a crawled dictionary, we may either eliminate allwords that represent entries of the error dictionaries, or we may mark these words fora manual inspection.
In what follows we report on an experiment in the area of lexicaltext correction where these techniques improved:1. the quality of crawled dictionaries by avoiding erroneous entries,2.
the accuracy of lexical text correction achieved with these dictionaries,using a high-level text correction system (Strohmaier et al 2003a, 2003b).Correction Strategy.
Ignoring details, we used the following correction strategy10: Foreach token11 of the input text, the most similar words are retrieved from the dictionaryas a set of correction candidates.
In many cases the token will be found in the dictionaryand represents a correction candidate with optimal similarity.
Based on (1) the similaritybetween text token and correction candidate and (2) the frequency of the correctioncandidate in a corpus, each candidate receives a score.
If the score of the best candidateexceeds a given threshold ?, the token is replaced by this candidate.
In the other case, thetoken is left unmodified.
A good balance between similarity and frequency informationin the score is obtained via training.
The threshold, which is also optimized via training,guarantees that the input token is only replaced if additional confidence is availablethat the best correction candidate in fact represents the corrected version of the token.In the experiment described below, the system was trained on a corpus for the domainMushrooms.
The evaluation corpus is from the domain Fish.
Hence, the two corporaare disjoint and cover distinct thematic areas.
More details on the correction system canbe found in Strohmaier et al (2003b).Garbled Input Text for Correction.
We collected 10 texts from the domain Fish, allcontaining a nontrivial number of errors.
Texts were retrieved from the Web, usingqueries to Google with spelling errors, such as fish anglers infomation realy.
We checkedthat the texts do not contain paragraphs that are also found in the documents of thecorpora Fish E introduced in Section 2.2.
The concatenation of the 10 texts was used as10 To simplify evaluation, a fully automated variant of text correction was considered.11 In what follows, by a token, we always mean a token composed of standard letters only.333Computational Linguistics Volume 32, Number 3input to the text correction system.
For the evaluation, a corrected version of the full textwas manually created.
The full text contains 17,697 tokens of which 418 (2.36%) werefound to be erroneous.Background Dictionaries for Correction.
As a baseline, a first crawled dictionaryDcrawl with 505,652 entries was built, collecting all words from the documents in thecorpus Fish E (1).
A second dictionary D+Fcrawl used only those pages that were notrejected by the filter for threshold ?
= 2, based on the maximal English error dic-tionary Derr(English,all).12 In this case, 324 documents passed the filter, whereas 186were rejected.
In this case we obtained 291,065 entries.
Deleting in D+Fcrawl all words thatrepresent entries of Derr(English,all), a third dictionary D+F+EDcrawl with 269,079 entries wascomputed.Note that we did not extend D+Fcrawl and D+F+EDcrawl by analyzing an additional set offiltered Web pages.
Hence, D+Fcrawl is in fact a subdictionary of Dcrawl, and similarly forD+F+EDcrawl and D+Fcrawl.
This explains why the coverage of D+Fcrawl (D+F+EDcrawl ) is smaller thanthe coverage of Dcrawl (D+Fcrawl); see below.
With an extended filtered crawl, even bettercoverage and accuracy results would probably be possible.Evaluation Results.
We then compared the lexical coverage (percentage of tokens ofthe correct version of the input text found in the dictionary) and correction accuracy(percentage of correct tokens after automated correction) for each of the three dictio-naries.
The results are presented in Table 30.
The accuracy of the input text is 97.64%.The fifth column gives the improvement in accuracy, taking the input text as a baseline.The last column mentions the number of erroneous tokens in the text that are found inthe respective error dictionary.Note that the use of the filtered corpus leads to a measurable improvement incorrection accuracy.
The second step in which we eliminate all entries of the errordictionaries in the correction dictionary leads to an additional gain.Overproduction and Underproduction of the Error Dictionary.
As mentioned above,418 tokens of the input text represented proper errors.
From these, 254 (60.77%) turnedout to be entries of the maximal English error dictionary Derr(English,all).
Note thatthis value for underproduction is very compatible with our estimates in Section 5.Remarkably, only seven of the correct tokens of the input text occurred in the errordictionary.Analyzing the Effect of Using Filters and Error Dictionaries.
The most important errorsource in the correction process are erroneous tokens of the text that?by accident?represent entries of the crawled dictionaries.
Using the above strategy, these falsefriends are only replaced by another word w of the correction dictionary if overwhelm-ing frequency information is available that leads to a preference of w after computingthe balanced score for similarity and frequency.
The dictionary Dcrawl contains 262 of the418 erroneous tokens of the text.
The dictionary D+Fcrawl, which collects the vocabularyof filtered pages, contains only 92 erroneous tokens.
After eliminating all entries of themaximal error dictionary, the new dictionary D+F+EDcrawl contains only 49 false friends.Note that the latter tokens represent errors not contained in the error dictionary.
Avery interesting additional number is the following: when eliminating in Dcrawl all12 Other filter thresholds for ?
= 1, 0.5, and 0 were also tested and led to very similar accuracy values.334Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesTable 30Measuring the quality of distinct dictionaries for text correction.
Dcrawl is produced by anunfiltered crawl, D+Fcrawl by a filtered crawl.
For D+F+EDcrawl , a filtered crawl is used and remainingentries of error dictionaries are eliminated.Dictionary Entries Coverage (%) Accuracy (%) ?
(%) False friendsDcrawl 505,652 99.08 98.45 0.81 262D+Fcrawl 291,065 98.77 98.61 0.97 92D+F+EDcrawl 269,079 98.75 98.74 1.10 49the entries that are found in Derr(English,all), the resulting dictionary contains 105erroneous tokens of the text.
This shows that the filtering step eliminates 56 (= 105 ?
49)erroneous tokens of the text that are not found in the error dictionary and proves thata two-step procedure?first using filters for crawling pages, then eliminating entries oferror dictionaries afterwards?leads to optimal results.10.2 Generating Translation Data from Parallel CorporaParallel texts represent an important resource for automatic acquisition of bilingualdictionaries.
Since only a small number of large parallel corpora are available, whichare moreover specialized both with respect to form and contents, the Web representsan important archive for mining parallel texts (Resnik and Smith 2003).
When buildingup bilingual dictionaries for machine translation, or when presenting parallel phrasesto users, correctness is an important issue.
Hence, it is interesting to see how errordictionaries help to reduce errors in parallel corpora.
Our methods can be applied toany kind of parallel corpus.
For our experiments we used the freely available Europarlcorpus.13 The corpus covers the proceedings of the European Parliament 1996?2001 in11 official languages of the European Union.
We only analyzed the English and Germanversions of the parallel texts.
The 488 documents in the corpus are of an excellent quality.Our goal was to find English and German texts with a nontrivial number of errors (ifany) and to detect these errors.
Since the overproduction of error dictionaries in veryaccurate texts is high, the problem is challenging.
The maximal error dictionaries forthe two languages were used to determine the error rate of each document.
Table 31shows the twenty documents with the highest error rates for both the English and theGerman subcollection of the corpora.
Columns 4 and 5 describe the number of tokensthat represent entries of the respective error dictionary and the number of real errorsamong these hits.
The results show that when analyzing very accurate texts, the errorrate is not always a safe indicator for a corresponding number of real errors.
Still, theexperiment isolates 246 real errors, only looking at 40 documents.
When collectingtranslation correspondences, we may simply discard all phrases/sentences with a hitin an error dictionary, together with their aligned counterparts.
Many translation pairswith errors will be avoided.
Given the length of the documents, the number of hits ofthe error dictionaries is small, hence the loss of recall is not essential.
In this way our13 The corpus, which was also used by Koehn, Och, and Marcu (2003), is available athttp://www.isi.edu/k?oehn/europarl/.335Computational Linguistics Volume 32, Number 3Table 31English (E) and German (G) documents of the Europarl corpora, sizes, error rates w.r.t.
maximalEnglish and German error dictionaries, numbers of hits of the error dictionaries, and numbers ofreal errors among hits.Documents Tokens Error rate Hits Real errors Percentageep-96-09-20.txt (E) 9,945 1.31 13 2 15.38ep-97-04-24.txt (E) 8,074 0.99 8 8 100.00ep-97-09-19.txt (E) 3,230 0.93 3 0 0.00ep-97-02-21.txt (E) 5,830 0.86 5 5 100.00ep-99-01-28.txt (E) 5,347 0.75 4 0 0.00ep-97-06-25.txt (E) 20,012 0.70 14 11 78.57ep-96-07-19.txt (E) 4,383 0.68 3 3 100.00ep-97-04-23.txt (E) 21,930 0.64 14 14 100.00ep-97-12-04.txt (E) 9,463 0.63 6 6 100.00ep-99-02-12.txt (E) 5,426 0.55 3 3 100.00ep-00-03-29.txt (E) 22,252 0.54 12 12 100.00ep-96-07-17.txt (E) 34,381 0.52 18 14 77.77ep-99-03-10.txt (E) 31,509 0.51 16 0 0.00ep-00-11-15.txt (E) 35,167 0.48 17 1 5.88ep-97-04-10.txt (E) 16,653 0.48 8 6 75.00ep-97-05-15.txt (E) 20,942 0.48 10 2 20.00ep-97-10-20.txt (E) 8,601 0.46 4 4 100.00ep-97-04-11.txt (E) 6,857 0.44 3 1 33.33ep-99-01-15.txt (E) 9,193 0.43 4 0 0.00ep-96-06-18.txt (E) 32,768 0.43 14 6 42.86ep-03-01-13.txt (G) 15,926 2.57 41 2 4.89ep-97-05-16.txt (G) 12,344 1.94 24 15 62.50ep-02-09-02.txt (G) 14,845 1.62 24 1 4.16ep-98-11-05.txt (G) 15,035 1.46 22 3 13.64ep-99-01-28.txt (G) 6,798 1.32 9 0 0.00ep-02-04-25.txt (G) 10,842 1.29 14 4 28.57ep-97-10-02.txt (G) 13,650 1.25 17 9 52.94ep-99-07-20.txt (G) 2,431 1.23 3 0 0.00ep-00-03-15.txt (G) 34,904 1.20 42 31 73.81ep-96-06-21.txt (G) 8,474 1.18 10 9 90.00ep-96-06-17.txt (G) 9,408 1.17 11 2 18.18ep-99-04-16.txt (G) 8,667 1.15 10 9 90.00ep-96-04-19.txt (G) 8,694 1.15 10 2 20.00ep-00-12-15.txt (G) 6,964 1.15 8 3 37.50ep-00-09-08.txt (G) 4,374 1.14 5 0 0.00ep-96-07-04.txt (G) 10,975 1.09 12 11 91.66ep-01-04-05.txt (G) 26,941 1.08 29 20 68.96ep-97-06-09.txt (G) 11,152 1.08 12 12 100.00ep-97-07-14.txt (G) 11,180 1.07 12 5 41.66ep-97-07-18.txt (G) 10,392 1.06 11 10 90.90methods may help to improve the generation of translation data even from collectionsof very accurate parallel texts.11.
ConclusionIn this article we investigated the distribution of orthographic errors of distinct types inthe English and German Web.
Experiments based on a variety of very large error dic-tionaries showed that Web corpora typically contain a non-negligible number of pages336Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pageswith an unacceptable number of orthographic errors.
Typing errors represent the mostimportant subclass.
In German Web pages, errors resulting from character encodingproblems represent another important category.
In our experiments, PDF documentswere found to contain less orthographic errors than HTML documents, and corporacovering specific thematic areas were found to contain more errors than collections of?general?
pages without such a focus.
Some differences were remarkable; in particular,our corpora for special thematic areas related to hobbies contain many pages with ahigh number of orthographic errors.
We also found that mean error rates are influencedby the collection strategy.
Specific crawling strategies help to avoid chat and junk whileattracting professional documents.
Since document genre and error rates are correlated,refined crawling strategies may help to reduce mean error rates.Error dictionaries, even subdictionaries of modest size, can be used as filters thathelp to detect and eliminate pages with many orthographic errors.
Filters with user-defined thresholds work well for both languages.
Obviously, the possibility of deletingpages with many orthographic errors and of marking all entries of error dictionariesin the remaining documents opens a wide range of interesting applications in distinctareas of corpus linguistics.
To exemplify possible applications we showed how to im-prove the quality of Web-crawled dictionaries for text correction.
With these filtered dic-tionaries, higher values for correction accuracy were obtained than with those directlyobtained from Web crawls.
In a second experiment, we showed how error dictionar-ies may be used to improve the automated collection of translation correspondences,avoiding translation pairs with orthographic errors.Going beyond corpus linguistics, it might be interesting to design (special modes of)Web search engines where the error rate of a given document is used as one parameterin the ranking of answers.
In many search scenarios, answer documents with a largenumber of orthographic errors appear to be less reliable, and the user might wish toconcentrate on ?professional?
or carefully edited Web pages.In our practical work we found that the collection and analysis of very large Webcorpora is difficult for many reasons.
For example, it is not clear how to treat pageswith artificial vocabulary that is only introduced to obtain a better ranking.
We learnedthat often these junk lists are intensionally enriched with many orthographic errors toobtain a better ranking, in particular for erroneous queries.
In our experiments, someof these pages were found immediately, looking at error rates, and excluded.
Later,when inspecting documents for genre classification, other less eye-catching exampleswere found.
Some portions of text occurred in several documents.
The conversion ofWeb pages into ASCII represents a potential source for new errors.
In particular theconversion of German PDF documents to ASCII turned out to be very error prone.Nonstandard vocabulary (special names, foreign language expressions, archaic lan-guage, programming code, slang, etc.)
is another source that makes various pagesinappropriate for corpus construction.One step for future work is the development of special dictionaries for frequentforeign language expressions, archaic language, programming code, and slang.
Specialdictionaries for these expressions would not only help to detect and exclude pages witha high amount of nonstandard vocabulary, but they could also be used as additionalfilters in the construction of error dictionaries.
The results in Section 5.2 indicate thatthe overproduction of our error dictionaries could be reduced in a significant way byeliminating entries that represent expressions of the earlier-mentioned type.
As a matterof fact, new types of spelling errors were found during the experiments describedearlier.
It might be interesting to enlarge the error dictionaries for spelling errors, takingthe new patterns into account.337Computational Linguistics Volume 32, Number 3We also found that enlarged error dictionaries that store with each garbled entrythe correct word from which it was derived are very useful for error correction.
Incontrast to our first intuitions, the number of ambiguities arising from this correctionstrategy is small, and the predictive power of enlarged error dictionaries is high.More details on text correction with error dictionaries will be given in a forthcomingpaper.AcknowledgmentsThe authors thank the anonymous referees ofComputational Linguistics.
Their remarks andsuggestions helped to improve the contentsand presentation of the article.
Special thanksto Annette Gotscharek and Uli Reffle for alltheir help.ReferencesAmengual, Juan Carlos and Enrique Vidal.1998.
Efficient error-correcting viterbiparsing.
IEEE Transactions on PAMI,20(10):1?109.Baroni, Marco and Silvia Bernardini.
2004.BootCaT: Bootstrapping corpora and termsfrom the web.
In Proceedings of LREC 2004,pages 1313?1316, Lisbon.Boutsis, Sotiris, Stelious Piperidis, and IasonDemiros.
1999.
Generating translationlexica from multilingual texts.
AppliedArtificial Intelligence, 13(6):583?606.Brin, Sergey and Lawrence Page.
1998.
Theanatomy of a large-scale hypertextual Websearch engine.
Computer Networks andISDN Systems, 30:107?117.Brown, Jonathan and Maxine Eskenazi.
2004.Retrieval of authentic documents forreader-specific lexical practice.
InProceedings of the InSTIL/ICALL2004Symposium on Computer Aided LanguageLearning, pages 25?28, Venice.Chelba, Ciprian and Frederick Jelinek.2002.
Recognition performance ofa structured language model.
InProceedings of Sixth European Conferenceon Speech Communication and Technology(EUROSPEECH?99), pages 1567?1570,Budapest.Church, Kenneth W. and Robert L. Mercer.1993.
Introduction to the special issue oncomputational linguistics using largecorpora.
Computational Linguistics,19(1):1?24.Dimitrova, Maya, Nicholas Kushmerick,Petia Radeva, and Joan Jose Villanueva.2003.
User assessment of a visual Webgenre classifier.
In Third InternationalConference on Visualization, Imaging, andImage Processing, Malaga.Dunning, Ted.
1993.
Accurate models for thestatistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Finn, Aidan and Nicholas Kushmerick.
2003.Learning to classify documents accordingto genre.
In IJCAI-03 Workshop onComputational Approaches to Text Style andSynthesis, Acapulco.
Journal of the AmericanSociety for Information Science andTechnology (in press).Fletcher, William H. 2004a.
Facilitating thecompilation and dissemination of ad-hocweb corpora.
In Guy Aston, SilviaBernardini, and Dominic Stewart, editors,Corpora and Language Learners, number 17in Studies in Corpus Linguistics.
JohnBenjamins Publishing Company,Amsterdam.Fletcher, William H. 2004b.
Making the webmore useful as a source for linguisticcorpora.
In U. Connor and T. Upton,editors, Corpus Linguistics in North America2002.
Rodopi, Amsterdam.Gaizauskas, Robert, George Demetriou, andKevin Humphreys.
2000.
Term recognitionin biological science journal articles.
InProceedings of the Workshop onComputational Terminology for Medical andBiological Applications, 2nd InternationalConference on Natural Language Processing(NLP-2000), pages 37?44, Patras.Gale, William A. and Kenneth W. Church.1991.
Identifying word correspondencesin parallel texts.
In Proceedings ofFourth DARPA Workshop on Speechand Natural Language, pages 152?157,Pacific Grove, CA.Gartner, Hans-Ju?rgen.
2003.
Extraktionvon semantischer Information ausLayout-orientierten Daten.
Master?sthesis, Technical University of Graz.Grefenstette, Gregory.
1992.
Use of syntacticcontext to produce term association listsfor text retrieval.
In Proceedings of the 15thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval, pages 89?97, Copenhagen.Grefenstette, Gregory.
1999.
The WWW as aresource for example-based MT tasks.Paper presented at ASLIB ?Translatingand the Computer?
conference, London.338Ringlstetter, Schulz, and Mihov Orthographic Errors in Web PagesGrefenstette, Gregory.
2001.
Very largelexicons.
In Computational Linguistics in theNetherlands 2000: Selected Papers from theEleventh CLIN Meeting, Language andComputers, Amsterdam.Guenthner, Franz.
1996.
Electronic lexica andcorpora research at CIS.
InternationalJournal of Corpus Linguistics, 1(2):287?301.Jelinek, Frederick.
1997.
Statistical Methodsfor Speech Recognition.
MIT Press,Cambridge, MA.Kehoe, Andrew and Antoinette Renouf.
2002.WebCorp: Applying the web to linguisticsand linguistics to the Web.
In PosterProceedings of the 11th International WorldWide Web Conference, WWW02, Honolulu.Kessler, Brett, Geoffrey Nunberg, andHinrich Schu?tze.
1997.
Automateddetection of text genre.
In Proceedings of the35th Annual Meeting of the Association forComputational Linguistics and the 8thMeeting of the European Chapter of theAssociation for Computational Linguistics,pages 32?38, Madrid.Kilgarriff, Adam and Gregory Grefenstette.2003.
Introduction.
ComputationalLinguistics?Special Issue on the Web asCorpus, 29(3):333?348.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of the HumanLanguage Technology and North AmericanAssociation for Computational LinguisticsConference (HLT/NAACL), Edmonton.Kukich, Karen.
1992.
Techniques forautomatically correcting words intexts.
ACM Computing Surveys,24(4):377?439.Kumano, Akira and Hideki Hirakawa.
1994.Building a MT dictionary from paralleltexts based on linguistic and statisticalinformation.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics (COLING?94), pages 76?81,Kyoto.Kupiec, Julian.
1993.
An algorithm forfinding noun phrase correspondences inbilingual corpora.
In Proceedings of the 31stAnnual Meeting of the Association forComputational Linguistics (ACL?93),pages 17?22, Columbus, OH.Lin, Shian-Hua, Chi-Sheng Shih,Meng Chang Chen, Jan-Ming Ho,Ming-Tat Ko, and Yueh-Ming Huang.1998.
Extracting classification knowledgeof internet documents with mining termassociations: A semantic approach.
InProceedings of the 21st International ACMSIGIR Conference on Research andDevelopment in Information Retrieval, pages241?249, Melbourne, Australia.Maier-Meyer, Petra.
1995.
Lexikon undautomatische Lemmatisierung.
Ph.D. thesis,CIS, University of Munich.Morley, Barry, Antoinette Renouf, andAndrew Kehoe.
2003.
Linguistic researchwith the XML/RDF aware WebCorp tool.In Poster Proceedings of the 12th InternationalWorld Wide Web Conference, WWW03,Budapest.Oh, Alice H. and Alexander I. Rudickny.2000.
Stochastic language generationfor spoken dialogue systems.
InANLP/NAACL 2000 Workshop onConversational Systems, pages 27?32,Seattle.Ostendorf, Mari, Vassilios V. Digalakis, andOwen A. Kimball.
1996.
From HMMs tosegment models: A unified view ofstochastic modeling for speechrecognition.
IEEE Transactions Speech andAudio Processing, 4(5):360?378.Resnik, Philip and Noah A. Smith.
2003.
Theweb as a parallel corpus.
ComputationalLinguistics - Special Issue on the Web asCorpus, 29(3):349?380.Ringlstetter, Christoph.
2003.
OCR-Korrektur und Bestimmung vonLevenshtein-Gewichten.
Master?sthesis, LMU, University of Munich.Schwartz, Lee, Takako Aikawa, andMichel Pahud.
2004.
Dynamic languagelearning tools.
In Proceedings of theInSTIL/ICALL2004 Symposium onComputer Aided Language Learning,pages 107?110, Venice.Smadja, Frank A. and Kathleen R. McKeown.1990.
Automatically extracting andrepresenting collocations for languagegeneration.
In Proceedings of the 28th AnnualMeeting of the Association for ComputationalLinguistics, pages 252?259, Pittsburgh, PA.Sornlertlamvanich, Virach and HozumiTanaka.
1996.
The automatic extractionof open compounds from text corpora.In Proceedings of the 16th Conference onComputational Linguistics, pages 1143?1146,Copenhagen.Strohmaier, Christian, Christoph Ringlstetter,Klaus U. Schulz, and Stoyan Mihov.
2003a.Lexical postcorrection of OCR-results: Theweb as a dynamic secondary dictionary?In Proceedings of the Seventh InternationalConference on Document Analysis andRecognition (ICDAR 03), pages 1133?1137,Edinburgh.Strohmaier, Christian, Christoph Ringlstetter,Klaus U. Schulz, and Stoyan Mihov.339Computational Linguistics Volume 32, Number 32003b.
A visual and interactive tool foroptimizing lexical postcorrection ofOCR results.
In Proceedings of the IEEEWorkshop on Document Image Analysisand Recognition, DIAR?03, Madison, WI.Taghva, Kazem and Jeff Gilbreth.
1999.Recognizing acronyms and theirdefinitions.
International Journal onDocument Analysis and Recognition,1(4):191?198.Walker, Donald E. and Robert A. Amsler.1986.
The use of machine-readabledictionaries in sublanguage analysis.
InAnalyzing Language in Restricted Domains:Sublanguage Description and Processing.Lawrence Erlbaum, Hillsdale, NJ,pages 69?83.Way, Andy and Nano Gough.
2003.wEBMT: Developing and validatingan example-based machine translationsystem using the world wide web.Computational Linguistics?Special Issueon the Web as Corpus, 29(3):421?458.Yeates, Stuart, David Bainbridge, andIan H. Witten.
2000.
Using compressionto identify acronyms in text.
In Proceedingsof the Conference on Data Compression,page 582, Snowbird, UT.340
