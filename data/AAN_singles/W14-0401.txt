Felix Bildhauer & Roland Sch?fer (eds.
), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 1?8,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsFinding viable seed URLs for web corpora: a scouting approach andcomparative study of available sourcesAdrien BarbaresiICAR LabENS Lyon & University of Lyon15 parvis Ren?e Descartes, 69007 Lyon, Franceadrien.barbaresi@ens-lyon.frAbstractThe conventional tools of the ?web as cor-pus?
framework rely heavily on URLs ob-tained from search engines.
Recently, thecorresponding querying process becamemuch slower or impossible to perform on alow budget.
I try to find acceptable substi-tutes, i.e.
viable link sources for web cor-pus construction.
To this end, I performa study of possible alternatives, includ-ing social networks as well as the OpenDirectory Project and Wikipedia.
Fourdifferent languages (Dutch, French, In-donesian and Swedish) taken as exam-ples show that complementary approachesare needed.
My scouting approach usingopen-source software leads to a URL di-rectory enriched with metadata which maybe used to start a web crawl.
This ismore than a drop-in replacement for exist-ing tools since said metadata enables re-searchers to filter and select URLs that fitparticular needs, as they are classified ac-cording to their language, their length anda few other indicators such as host- andmarkup-based data.1 Introduction1.1 The ?web as corpus?
paradigm and itsURL seeds problemThe state of the art tools of the ?web as corpus?framework rely heavily on URLs obtained fromsearch engines.
The BootCaT method (Baroni andBernardini, 2004) consists in repeated search en-gine queries using several word seeds that are ran-domly combined, first coming from an initial listand later from unigram extraction over the cor-pus itself.
As a result, so-called ?seed URLs?are gathered which are used as a starting point forweb crawlers.
This approach is not limited to En-glish: it has been successfully used by Baroni et al.
(2009) and Kilgarriff et al.
(2010) for major worldlanguages.Until recently, the BootCaT method could beused in free web corpus building approaches.
Tomy best knowledge it is now pass?e because of in-creasing limitations on the search engines?
APIs,which make the querying process on a low budgetmuch slower or impossible.
Other technical diffi-culties include diverse and partly unknown searchbiases due in part to search engine optimizationtricks as well as undocumented PageRank adjust-ments.
All in all, the APIs may be too expensiveand/or too unstable to support large-scale corpusbuilding projects.API changes are combined with an evolv-ing web document structure and a slow but in-escapable shift from ?web as corpus?
to ?webfor corpus?
due to the increasing number of webpages and the necessity of using sampling meth-ods at some stage.
This is what I call the post-BootCaT world in web corpus construction.1Moreover, the question whether the methodused so far, i.e.
randomizing keywords, providesa good overview of a language is still open.
It nowseems reasonable to look for alternatives, so thatresearch material does not depend on a single datasource, as this kind of black box effect combinedwith paid queries really impedes reproducibilityof research.
Using diverse sources of URL seedscould at least ensure that there is not a single bias,but several.Additionally, the lack of interest and project fi-nancing when dealing with certain less-resourcedlanguages makes it necessary to use light-weight1Note that the proponents of the BootCaT method seem toacknowledge this evolution, see for example Marco Baroni?stalk at this year?s BootCaTters of the world unite (BOTWU)workshop: ?My love affair with the Web... and why it?sover!
?1approaches where costs are lowered as much aspossible (Scannell, 2007).
In this perspective, apreliminary light scouting approach and a full-fledged focused crawler like those used by theSpiderling (Suchomel and Pomik?alek, 2012) orthe COW (Sch?afer and Bildhauer, 2012) projectsare complementary.
A ?web for corpus?
crawlingmethod using a seed set enriched with metadata asdescribed in this article may yield better results,e.g.
ensure a more diverse and less skewed sam-ple distribution in a population of web documents,and/or reach faster a given quantitative goal.1.2 Looking for alternatives, what issues dowe face?Search engines have not been taken as a sourcesimply because they were convenient.
They actu-ally yield good results in terms of linguistic qual-ity.
The main advantage was to outsource oper-ations such as web crawling and website qualityfiltering, which are considered to be too costly ortoo complicated to deal with while the main pur-pose is actually to build a corpus.In fact, it is not possible to start a web crawlfrom scratch, so the main issue to tackle can beput this way: where may we find web pages whichare bound to be interesting for corpus linguists andwhich in turn contain many links to other interest-ing web pages?Researchers in the machine translation fieldhave started another attempt to outsource compe-tence and computing power, making use of datagathered by the CommonCrawl project2to findparallel corpora (Smith et al., 2013).
Nonetheless,the quality of the links may not live up to theirexpectations.
First, purely URL-based approachesare a trade-off in favor of speed which sacrificesprecision, and language identification tasks area good example of this phenomenon (Baykan etal., 2008).
Second, machine-translated content isa major issue, so is text quality in general, es-pecially when it comes to web texts (Arase andZhou, 2013).
Third, mixed-language documentsslow down text gathering processes (King and Ab-ney, 2013).
Fourth, link diversity is a also prob-lem, which in my opinion has not got the atten-tion it deserves.
Last, the resource is constantlymoving.
There are not only fast URL changesand ubiquitous redirections.
Following the ?web2.0?
paradigm, much web content is being injected2http://commoncrawl.org/from other sources, so that many web pages arenow expected to change any time.3Regular ex-ploration and re-analysis could be the way to go toensure the durability of the resource.In the remainder of this paper, I introduce ascouting approach which considers the first issue,touches on the second one, provides tools and met-rics to address the third and fourth, and adapts tothe last.
In the following section I describe mymethodology, then I show in detail which metricsI decided to use, and last I discuss the results.2 Method2.1 Languages studiedI chose four different languages in order to see ifmy approach generalizes well: Dutch, French, In-donesian and Swedish.
It enables me to compareseveral language-dependent web spaces whichought to have different if not incompatible char-acteristics.
In fact, the ?speaker to website quan-tity?
ratio is probably extremely different when itcomes to Swedish and Indonesian.
I showed in aprevious study that this affects greatly link discov-ery and corpus construction processes (Barbaresi,2013a).French is spoken on several continents andDutch is spoken in several countries (Afrikaanswas not part of this study).
Indonesian offers aninteresting point of comparison, as the chances tofind web pages in this language during a crawl atrandom are scarce.
For this very reason, I explic-itly chose not to study English or Chinese becausethey are clearly the most prominently representedlanguages on the web.2.2 Data sourcesI use two reference points, the first one beingthe existing method depending on search enginequeries, upon which I hope to cast a new lightwith this study.
The comparison grounds on URLsretrieved using the BootCaT seed method on themeta-engine E-Tools4at the end of 2012.
The sec-ond reference point consists of social networks,to whose linguistic structure I already dedicateda study (Barbaresi, 2013b) where the method usedto find the URLs is described in detail.
I choseto adopt a different perspective, to re-examine theURLs I gathered and to add relevant metadata3This is the reason why Marco Baroni states in the talkmentioned above that his ?love affair with the web?
is over.4http://www.etools.ch/2in order to see how they compared to the othersources studied here.I chose to focus on three different networks:FriendFeed, an aggregator that offers a broaderspectrum of retrieved information; identi.ca, a mi-croblogging service similar to Twitter; and Red-dit, a social bookmarking and microblogging plat-form.
Perhaps not surprisingly, these data sourcesdisplay the issues linked to API instability men-tioned above.
The example of identi.ca is telling:until March 2013, when the API was closed af-ter the company was bought, it was a social mi-croblogging service built on open source tools andopen standards, the advantages compared to Twit-ter include the Creative Commons license of thecontent, and the absence of limitations on the totalnumber of pages seen.Another data source is the Open DirectoryProject (DMOZ5), where a selection of links is cu-rated according to their language and/or topic.
Thelanguage classification is expected to be adequate,but the amount of viable links is an open question,as well as the content.Last, the free encyclopedia Wikipedia is anotherspam-resilient data source in which the quality oflinks is expected to be high.
It is acknowledgedthat the encyclopedia in a given language editionis a useful resource, the open question resides inthe links pointing to the outside world, as it is hardto get an idea of their characteristics due to thelarge number of articles, which is rapidly increas-ing even for an under-resourced language such asIndonesian.2.3 Processing pipelineThe following sketch describes how the results be-low were obtained:1.
URL harvesting: queries or archive/dumptraversal, filtering of obvious spam and non-text documents.2.
Operations on the URL queue: redirectionchecks, sampling by domain name.3.
Download of the web documents and ana-lysis: collection of host- and markup-baseddata, HTML code stripping, document valid-ity check, language identification.Links pointing to media documents were ex-cluded from this study, as its final purpose is5http://www.dmoz.org/to enable construction of a text corpus.
TheURL checker removes non-http protocols, images,PDFs, audio and video files, ad banners, feeds andunwanted hostnames like twitter.com, google.com,youtube.com or flickr.com.
Additionally, a properspam filtering is performed on the whole URL (us-ing basic regular expressions) as well as at do-main name level using a list of blacklisted domainscomparable to those used by e-mail services to fil-ter spam.
As a page is downloaded or a query isexecuted, links are filtered on-the-fly using a se-ries of heuristics described below, and finally therest of the links are stored.There are two other major filtering operations tobe aware of.
The first concerns the URLs, whichare sampled prior to the download.
The main goalof this operation is strongly related to my scout-ing approach.
Since I set my tools on an explo-ration course, this allows for a faster executionand provides us with a more realistic image ofwhat awaits a potential exhaustive crawler.
Be-cause of the sampling approach, the ?big picture?cannot easily be distorted by a single website.
Thisalso avoids ?hammering?
a particular server un-duly and facilitates compliance with robots.txt aswell as other ethical rules.
The second filter dealswith the downloaded content: web pages are dis-carded if they are too short.
Web documents whichare more than a few megabytes long are also dis-carded.Regarding the web pages, the software fetchesthem from a list, strips the HTML code, sends rawtext to a server instance of langid.py (descriptionbelow) and retrieves the server response, on whichit performs a basic heuristic tests.3 MetadataThe metadata described in this section can be usedin classificatory or graph-based approaches.
I usesome of them in the results below but did not ex-haust all the possible combinations in this study.There are nine of them in total, which can bedivided in three categories: corpus size metrics,which are related to word count measures, webscience metrics, which ought to be given a higherimportance in web corpus building, and finally thelanguage identification, which is performed usingan external tool.33.1 Corpus size metricsWeb page length (in characters) was used as a dis-criminating factor.
Web pages which were tooshort (less than 1,000 characters long after HTMLstripping) were discarded in order to avoid docu-ments containing just multimedia (pictures and/orvideos) or microtext collections for example, asthe purpose was to simulate the creation of ageneral-purpose text corpus.The page length in characters after strippingwas recorded, as well as the number of tokens,so that the total number of tokens of a web cor-pus built on this URL basis can be estimated.
Thepage length distribution is not normal, with a ma-jority of short web texts and a few incredibly longdocuments at the end of the spectrum, which isemphasized by the differences between mean andmedian values used in the results below and justi-fies the mention of both.3.2 Web science metricsHost sampling is a very important step becausethe number of web pages is drastically reduced,which makes the whole process more feasible andmore well-balanced, i.e.
less prone to host biases.IP-based statistics corroborate this hypothesis, asshown below.The deduplication operation is elementary, ittakes place at document level, using a hash func-tion.
The IP diversity is partly a relevant indicator,as it can be used to prove that not all domain nameslead to the same server.
Nonetheless, it cannot de-tect the duplication of the same document acrossmany different servers with different IPs, which inturn the elementary deduplication is able to reveal.Links that lead to pages within the same domainname and links which lead to other domains areextracted from the HTML markup.
The first num-ber can be used to find possible spam or irrelevantlinks, with the notable exception of websites likeAmazon or Wikipedia, which are quite easy to list.The latter may be used to assess the richness (or ata given level the suspiciousness) of a website bythe company it keeps.
While this indicator is notperfect, it enables users to draw conclusions with-out fetching all the downstream URLs.Moreover, even if I do not take advantage of thisinformation in this study, the fetcher also recordsall the links it ?sees?
(as an origin-destinationpair), which enables graph-based approaches suchas visualization of the gathered network or the as-sessment of the ?weight?
of a website in the URLdirectory.
Also, these metadata may very well beuseful for finding promising start URLs.3.3 Language identificationI consider the fact that a lot of web pages havecharacteristics which make it hard for ?classical?NLP approaches like web page language identifi-cation based on URLs (Baykan et al., 2008) to pre-dict the languages of the links with certainty.
Thatis why mature NLP tools have to be used to qualifythe incoming URLs and enable a language-basedfiltering based on actual facts.The language identification tool I used islangid.py (Lui and Baldwin, 2012).
It is open-source, it incorporates a pre-trained model and itcovers 97 languages, which is ideal for tacklingthe diversity of the web.
Its use as a web ser-vice makes it a fast solution enabling distant ordistributed work.As the software is still under active develop-ment, it can encounter difficulties with rare encod-ings.
As a result, the text gets falsely classified asfor example Russian or Chinese.
The languages Istudied are not affected by these issues.
Still, lan-guage identification at document level raises a fewproblems regarding ?parasite?
languages (Scan-nell, 2007).Using a language identification system has afew benefits: it enables finding ?regular?
texts interms of statistical properties and excluding cer-tain types of irregularities such as encoding prob-lems.
Web text collections are smoothed out inrelation to the statistical model applied for eachlanguage target, which is a partly destructive butinteresting feature.There are cases where the confidence intervalof the language identifier is highly relevant, for in-stance if the page is multi-lingual.
Then there aretwo main effects: on one hand the confidence in-dicator gets a lower value, so that it is possible toisolate pages which are likely to be in the targetlanguage only.
On the other hand, the languageguessed is the one with the largest number of iden-tifiable words: if a given web page contains 70 %Danish and 30 % English, then it will be classifiedas being written in Danish, with a low confidenceinterval: this information is part of the metadata Iassociate with each web page.
Since nothing par-ticular stood out in this respect I do not mention itfurther.4URLs% intargetLengthTokens(total)DifferentIPs (%)analyzed retained mean medianDutch 12,839 1,577 84.6 27,153 3,600 5,325,275 73.1French 16,763 4,215 70.2 47,634 8,518 19,865,833 50.5Indonesian 110,333 11,386 66.9 49,731 8,634 50,339,311 18.6Swedish 179,658 24,456 88.9 24,221 9,994 75,328,265 20.0Table 1: URLs extracted from search engines queries4 Results4.1 Characteristics of the BootCaT approachFirst of all, I let my toolchain run on URLs ob-tained using the BootCaT approach, in order toget a glimpse of its characteristics.
I let theURL extractor run for several weeks on Indone-sian and Swedish and only a few days for Dutchand French, since I was limited by the constraintsof this approach, which becomes exponentiallyslower as one adds target languages.6The resultscommented below are displayed in table 1.The domain name reduction has a substantialimpact on the set of URLs, as about a quarter ofthe URLs at best (for French) have different do-main names.
This is a first hint at the lack ofdiversity of the URLs found using the BootCaTtechnique.Unsurprisingly, the majority of links appear tobe in the target language, although the languagefilters do not seem to perform very well.
As theadequate matching of documents to the user?s lan-guage is paramount for search engines, it is prob-ably a bias of the querying methodology and itsrandom tuples of tokens.
In fact, it is not rare tofind unexpected and undesirable documents suchas word lists or search engine optimization traps.The length of web documents is remarkable, itindicates that there are likely to contain long texts.Moreover, the median length seems to be quiteconstant across the three languages at about 8,000tokens, whereas it is less than half that (3,600) forDutch.
All in all, it appears to be an advantagewhich clearly explains why this method has beenconsidered to be successful.
The potential cor-pus sizes are noteworthy, especially when enoughURLs where gathered in the first place, which was6The slow URL collection is explained by the cautioushandling of this free and reliable source, implying a queryrate limiting on my side.
The scouting approach by itself is amatter of hours.already too impracticable in my case to be consid-ered a sustainable option.The number of different IPs, i.e.
the diversityin terms of hosts, seems to get gradually loweras the URL list becomes larger.
The fact thatthe same phenomenon happens for Indonesian andSwedish, with one host out of five being ?new?,indicates a strong tendency.4.2 Social networksDue to the mixed nature of the experimental set-ting, no conclusions can be drawn concerning thesingle components.
The more than 700,000 URLsthat were analyzed give an insight regarding theusefulness of these sources.
About a tenth of it re-mained as responding websites with different do-main names, which is the lowest ratio of this study.It may be explained by the fast-paced evolution ofmicroblogs and also by the potential impurity ofthe source compared to the user-reviewed directo-ries whose results I describe next.As I did not target the studied languages duringthe URL collection process, there were merely afew hundred different domain names to be found,with the exception of French, which was a lot moreprominent.Table 2 provides an overview of the results.
Themean and median lengths are clearly lower thanin the search engine experiment.
In the case ofFrench, with a comparable number of remainingURLs, the corpus size estimate is about 2.5 timessmaller.
The host diversity is comparable, anddoes not seem to be an issue at this point.All in all, social networks are probably a goodcandidate for web corpora, but they require a fo-cused approach of microtext to target a particularcommunity of speakers.4.3 DMOZAs expected, the number of different domainnames on the Open Directory project is high, giv-5% in targetURLsretainedLengthTokens(total)DifferentIPs (%)mean medianDutch 0.6 465 7,560 4,162 470,841 68.8French 5.9 4,320 11,170 5,126 7,512,962 49.7Indonesian 0.5 336 6,682 4,818 292,967 50.9Swedish 1.1 817 13,807 7,059 1,881,970 58.5Table 2: URLs extracted from a blend of social networks crawls (FriendFeed, identi.ca, and Reddit) withno language target.
738,476 URLs analyzed, 73,271 URLs retained in the global process.ing the best ratio in this study between unfilteredand remaining URLs.
The lack of web pages writ-ten in Indonesian is a problem for this source,whereas the other languages seem to be far bet-ter covered.
The adequacy of the web pages withrespect to their language is excellent, as shown intable 3.
These results underline the quality of theresource.On the other hand, document length is thebiggest issue here.
The mean and median val-ues indicate that this characteristic is quite ho-mogeneous throughout the document collection.This may easily be explained by the fact that theURLs which are listed on DMOZ mostly leadto corporate homepages for example, which areclear and concise, the eventual ?real?
text contentbeing somewhere else.
What?s more, the web-sites in question are not text reservoirs by nature.Nonetheless, the sheer quantity of listed URLscompensates for this fact.
The corpus sizes forDutch and French are quite reasonable if one bearsin mind that the URLs were sampled.The relative diversity of IPs compared to thenumber of domain names visited is another indica-tor that the Open Directory leads to a wide range ofwebsites.
The directory performs well comparedto the sources mentioned above, it is also mucheasier to crawl.
It did not cost us more than a fewlines of code followed by a few minutes of runtimeto gather the URLs.4.4 WikipediaThe characteristics of Wikipedia are quite simi-lar, since the free encyclopedia also makes dumpsavailable, which are easily combed through in or-der to gather start URLs.
Wikipedia also com-pares favorably to search engines or social net-works when it comes to the sampling operationand page availability.
It is a major source of URLs,with numbers of gathered URLs in the millions forlanguages like French.
As Wikipedia is not a URLdirectory by nature, it is interesting to see what arethe characteristics of the pages it links to are.
Theresults are shown in table 3.First, the pages referenced in a particular lan-guage edition of Wikipedia often point to webpages written in a foreign language.
According tomy figures, this is a clear case, all the more sinceweb pages in Indonesian are rare.
Still, with a to-tal of more than 4,000 retained web texts, it faresa lot better than DMOZ or social networks.The web pages are longer than the ones fromDMOZ, but shorter than the rest.
This may also berelated to the large number of concise homepagesin the total.
Nonetheless, the impressive num-ber of URLs in the target language is decisive forcorpus building purposes, with the second-biggestcorpus size estimate obtained for French.The IP-related indicator yields good results withrespect to the number of URLs that were retrieved.Because to the high number of analyzed URLs thefigures between 30 and 46% give an insight intothe concentration of web hosting providers on themarket.5 DiscussionI also analyzed the results regarding the num-ber of links that lead out of the page?s domainname.
For all sources, I found no consistent re-sults across languages, with figures varying by afactor of three.
Nonetheless, there seem to be atendency towards a hierarchy in which the searchengines are on top, followed by social networks,Wikipedia and DMOZ.
This is one more hint atthe heterogeneous nature of the data sources I ex-amined with respect to the criteria I chose.This hierarchy is also one more reason why6URLs% intargetLengthTokens(total)DifferentIPs (%)analyzed retained mean medianDMOZDutch 86,333 39,627 94.0 2,845 1,846 13,895,320 43.2French 225,569 80,150 90.7 3,635 1,915 35,243,024 33.4Indonesian 2,336 1,088 71.0 5,573 3,922 540,371 81.5Swedish 27,293 11,316 91.1 3,008 1,838 3,877,588 44.8WikipediaDutch 489,506 91,007 31.3 4,055 2,305 15,398,721 43.1French 1,472,202 201,471 39.4 5,939 2,710 64,329,516 29.5Indonesian 204,784 45,934 9.5 6,055 4,070 3,335,740 46.3Swedish 320,887 62,773 29.7 4,058 2,257 8,388,239 32.7Table 3: URLs extracted from DMOZ and Wikipediasearch engines queries are believed to be fast andreliable in terms of quantity.
This method wasfast, as the web pages are long and full of links,which enables to rapidly harvest a large numberof web pages without having to worry about goinground in circles.
The researchers using the Boot-CaT method probably took advantage of the undo-cumented but efficient filtering operations whichsearch engines perform in order to lead to reli-able documents.
Since this process takes place ina competitive sector where this kind of informa-tion can be sold, it may explain why the companiesnow try to avoid giving it away for free.In the long run, several questions regardingURL quality remain open.
As I show using a high-credibility source such as Wikipedia, the searchengines results are probably closer to the maxi-mum amount of text that is to be found on a givenwebsite than the other sources, all the more whenthe sampling procedure chooses a page at randomwithout analyzing the rest of a website and thuswithout maximizing its potential in terms of to-kens.
Nonetheless, confrontation with the con-stantly increasing number of URLs to analyze andnecessarily limited resources make a website sam-pling by domain name useful.This is part of my cost-efficient approach, wherethe relatively low performance of Wikipedia andDMOZ is compensated by the ease of URL ex-traction.
Besides, the size of the potential corporamentioned here could increase dramatically if onewas to remove the domain name sampling processand if one was to select the web pages with themost out-domain links for the crawl.What?s more, DMOZ and Wikipedia are likelyto improve over time concerning the number ofURLs they reference.
As diversity and costs (tem-poral or financial) are real issues, a combined ap-proach could take the best of all worlds and pro-vide a web crawler with distinct and distant start-ing points, between the terse web pages referencedin DMOZ and the expected ?freshness?
of socialnetworks.
This could be a track to consider, asthey could provide a not inconsiderable amount ofpromising URLs.Finally, from the output of the toolchain toa full-fledged web corpus, other fine-grained in-struments as well as further decisions processes(Sch?afer et al., 2013) will be needed.
The fact thatweb documents coming from several sources al-ready differ by our criteria does not exclude fur-ther differences regarding text content.
By wayof consequence, future work could include a fewmore linguistically relevant text quality indicatorsin order to go further in bridging the gap betweenweb data, NLP and corpus linguistics.6 ConclusionI evaluated several strategies for finding texts onthe web.
The results distinguish no clear win-ner, complementary approaches are called for.
Inlight of these results, it seems possible to replaceor at least to complement the existing BootCaTapproach.
It is understandable why search en-gine queries have been considered a useful datasource.
However, I revealed that they lack diver-7sity at some point, which apart from their imprac-ticality may provide sufficient impetus to look foralternatives.I discussed how I address several issues in or-der to design robust processing tools which (com-bined to the diversity of sources and usable meta-data) enable researchers to get a better glimpse ofthe course a crawl may take.
The problem of linkdiversity has not been well-studied in a corpus lin-guistics context; I presented metrics to help quan-tify it and I showed a possible way to go in orderto gather a corpus using several sources leading toa satisfying proportion of different domain namesand hosts.As a plea for a technicalities-aware corpus cre-ation, I wish to bring to linguists?
attention that thefirst step of web corpus construction in itself canchange a lot of parameters.
I argue that a minimumof web science knowledge among the corpus lin-guistics community could be very useful to fullycomprehend all the issues at stake when dealingwith corpora from the web.The toolchain used to perform these experi-ments is open-source and can be found online.7The resulting URL directory, which includes themetadata used in this article, is available upon re-quest.
The light scouting approach allows for reg-ular updates of the URL directory.
It could alsotake advantage of the strengths of other tools inorder to suit the needs of different communities.AcknowledgmentsThis work has been partially supported by an in-ternal grant of the FU Berlin as well as machinepower provided by the COW (COrpora from theWeb) project at the German Grammar Depart-ment.
Thanks to Roland Sch?afer for letting me usethe URLs extracted from E-Tools and DMOZ.ReferencesYuki Arase and Ming Zhou.
2013.
Machine Trans-lation Detection from Monolingual Web-Text.
InProceedings of the 51th Annual Meeting of the ACL,pages 1597?1607.Adrien Barbaresi.
2013a.
Challenges in web cor-pus construction for low-resource languages in apost-BootCaT world.
In Zygmunt Vetulani andHans Uszkoreit, editors, Proceedings of the 6th Lan-guage & Technology Conference, Less ResourcedLanguages special track, pages 69?73, Pozna?n.7FLUX: Filtering and Language-identification for URLCrawling Seeds ?
https://github.com/adbar/flux-toolchainAdrien Barbaresi.
2013b.
Crawling microbloggingservices to gather language-classified URLs.
Work-flow and case study.
In Proceedings of the 51th An-nual Meeting of the ACL, Student Research Work-shop, pages 9?15.Marco Baroni and Silvia Bernardini.
2004.
BootCaT:Bootstrapping corpora and terms from the web.
InProceedings of LREC, pages 1313?1316.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky WideWeb: A collection of very large linguistically pro-cessed web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.E.
Baykan, M. Henzinger, and I. Weber.
2008.
WebPage Language Identification Based on URLs.
Pro-ceedings of the VLDB Endowment, 1(1):176?187.Adam Kilgarriff, Siva Reddy, Jan Pomik?alek, and PVSAvinesh.
2010.
A Corpus Factory for Many Lan-guages.
In Proceedings of LREC, pages 904?910.Ben King and Steven Abney.
2013.
Labeling the Lan-guages of Words in Mixed-Language Documents us-ing Weakly Supervised Methods.
In Proceedings ofNAACL-HLT, pages 1110?1119.Marco Lui and Timothy Baldwin.
2012. langid.py:An Off-the-shelf Language Identification Tool.
InProceedings of the 50th Annual Meeting of the ACL,pages 25?30.Kevin P. Scannell.
2007.
The Cr?ubad?an Project:Corpus building for under-resourced languages.
InBuilding and Exploring Web Corpora: Proceedingsof the 3rd Web as Corpus Workshop, volume 4,pages 5?15.Roland Sch?afer and Felix Bildhauer.
2012.
Buildinglarge corpora from the web using a new efficient toolchain.
In Proceedings of LREC, pages 486?493.Roland Sch?afer, Adrien Barbaresi, and Felix Bildhauer.2013.
The Good, the Bad, and the Hazy: DesignDecisions in Web Corpus Construction.
In StefanEvert, Egon Stemle, and Paul Rayson, editors, Pro-ceedings of the 8th Web as Corpus Workshop, pages7?15.Jason R. Smith, Herve Saint-Amand, Magdalena Pla-mada, Philipp Koehn, Chris Callison-Burch, andAdam Lopez.
2013.
Dirt Cheap Web-Scale Paral-lel Text from the Common Crawl.
In Proceedingsof the 51th Annual Meeting of the ACL, pages 1374?1383.V?
?t Suchomel and Jan Pomik?alek.
2012.
Efficient We-bcrawling for large text corpora.
In Adam Kilgarriffand Serge Sharoff, editors, Proceedings of the 7thWeb as Corpus Workshop, pages 40?44.8
