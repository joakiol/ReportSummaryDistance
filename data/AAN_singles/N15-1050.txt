Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 472?482,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsModeling Word Meaning in Context with Substitute VectorsOren MelamudComputer Science Dept.Bar-Ilan Universitymelamuo@cs.biu.ac.ilIdo DaganComputer Science Dept.Bar-Ilan Universitydagan@cs.biu.ac.ilJacob GoldbergerFaculty of EngineeringBar-Ilan Universitygoldbej@eng.biu.ac.ilAbstractContext representations are a key element indistributional models of word meaning.
Incontrast to typical representations based onneighboring words, a recently proposed ap-proach suggests to represent a context of a tar-get word by a substitute vector, comprising thepotential fillers for the target word slot in thatcontext.
In this work we first propose a vari-ant of substitute vectors, which we find partic-ularly suitable for measuring context similar-ity.
Then, we propose a novel model for rep-resenting word meaning in context based onthis context representation.
Our model outper-forms state-of-the-art results on lexical substi-tution tasks in an unsupervised setting.1 IntroductionFollowing the distributional hypothesis (Firth,1957), distributional models represent the meaningof a word type as an aggregation of its contexts.A recent line of work addresses polysemy of wordtypes by representing the meaning (or sense) of eachword instance individually as induced by its partic-ular context.
The context-sensitive meaning of aword instance is commonly called the word meaningin context, as opposed to the word meaning out-of-context of a word type.A key element of distributional models is thechoice of context representation.
A context ofa word instance is typically represented by anunordered collection of its first-order neighboringwords, called bag-of-words (BOW).
In contrast, Yat-baz et al (2012) proposed to represent this contextas a second-order substitute vector.
Instead of theneighboring words themselves, a substitute vectorour proposals will unlock new ways of raisingthe finance needed to develop businesses.representations of contextBOW proposals, raising, unlock, wayssubstitutes alternate, proposing, infinite, variousrepresentation of word meaning in contextparaphrases new, innovative, different,alternative, novelTable 1: Example for BOW and substitute vector repre-sentations for a context of the target word new.
The para-phrase vector is the representation learned by our modelfor the meaning of new in this context.
Only the first fewentries for each vector are shown.includes the potential filler words for the target wordslot, weighted according to how ?fit?
they are to fillthe target slot given the neighboring words.
For ex-ample, the substitute vector representing the con-text ?I my smartphone.?
(target slot underlined),would typically include potential slot fillers such aslove, lost, upgraded, etc.Melamud et al (2014) argued that substitute vec-tors are potentially more informative than tradi-tional context representations since the fitness ofthe fillers is estimated using an n-gram languagemodel, thereby capturing information embedded inthe neighboring word order.
They showed promisingresults on measuring word similarity out-of-contextwith a distributional model based on this approach.In this paper we first propose a variant of sub-stitute vectors as a context representation, whichwe find particularly suitable for measuring contextsimilarity.
Then, we extend the work in Mela-mud et al (2014) by proposing a novel distribu-tional model for representing word meaning in con-text, based on this context representation.
Like sub-472stitute vectors, the word representations learned byour model are second-order vectors, which we callparaphrase vectors.
Table 1 illustrates the differencebetween BOW and substitute vector context repre-sentations, as well as our representation for a wordin context.
Our model outperforms state-of-the-artresults on lexical substitution tasks while learningfrom plain text in an unsupervised setting.12 BackgroundA key element in distributional models of wordmeaning is the choice of context representation.
Tra-ditional out-of-context distributional models aggre-gate the observed contexts of a target word type toderive its representation.
More recent models ofword meaning in context typically bias such a wordtype representation towards the context of each par-ticular word instance using context similarity mea-sures.
The typical choice for context representationis a bag-of-words (BOW) context vector.
In this vec-tor each neighboring word type is assigned with aweight, such as its count (Erk and Pad?o, 2010) or tf-idf (Reisinger and Mooney, 2010).
A more recentvariant of this approach is the continuous bag-of-words (CBOW) vector, where context is representedas an average, or tf-idf weighted average, of denselow dimensional vector representations of the neigh-boring words (Huang et al, 2012).
Context similar-ity is typically computed using vector Cosine.Several types of models of word meaning in con-text were recently proposed in the literature, mostlybased on variants of BOW context representations.Thater et al (2011) aggregate the contexts of a tar-get word type into a sparse syntax-based context fea-ture vector.
Then, they generate a biased vector rep-resentation by reducing the weight of each contextfeature the less similar it is to the context of thegiven word instance.
Reisinger and Mooney (2010)and Huang et al (2012) use context clustering to in-duce multiple word senses for a target word type,where each sense is represented by a different con-text feature vector.
Then, they choose for each wordinstance the sense vector most similar to its givencontext.
?O S?eaghdha and Korhonen (2014) use LDA(Blei et al, 2003) to aggregate word collocations to1Our source code is publicly available at: www.cs.biu.ac.il/nlp/resources/downloads/word2parvec/distributions over topics.
Then, word meaning isrepresented by distributions that can be conditionedon (and hence biased towards) the given context.3 Substitute VectorsIn this work we propose to use a little-known contextmodeling paradigm, representing a context wordwindow as a substitute vector (Yatbaz et al, 2012).Unlike traditional context representations, a substi-tute vector does not comprise the first-order neigh-boring words of the target word.
Instead it includesthe second-order potential fillers for the target wordslot, weighted according to how ?fit?
they are to fillthe target slot given the neighboring words.
Moreformally, we denote a word window context arounda target word slot as c, and the substitute vector rep-resenting c as ~sc.
~sc[v] is the fitness weight of wordv to fill the target slot in context c, for every word vin the target word vocabulary.
For example, the sub-stitute vector ~sc= [big 0.35, good 0.28, bold 0.05,...] may represent the context c = ?It?s a move.
?.Yatbaz et al (2012) used a Kneser-Ney smoothedn-gram language model (Kneser and Ney, 1995) toestimate conditional probability as the fitness weight~sc[v] = p(v|c).
Using a smoothed language model isessential since context-word collocation counts aretoo sparse when the context considered is an entireword window.
We note that given the nature of n-gram language models this representation is sensi-tive to word order.
This property makes it appealingas it is potentially more informative than the tradi-tional unordered BOW context representations.4 A Model for Word Meaning in ContextThe main contribution of this paper is in propos-ing a model for word meaning in context, whichis based on substitute vector context representationsinstead of the traditional bag-of-words representa-tions.
To achieve this we first propose a variantof substitute vectors as our context representation.Then, we consider an out-of-context representationfor a word type as the average of the substitute vectorrepresentations of its observed contexts.
Finally, thein-context representation for a given word instanceis a weighted average of its observed contexts.
Inthis weighted average, contexts are weighted higherthe more similar they are to the given context, using473... the very essence or heart of being a coach .c ... the very essence or heart of being a .~scchristian, non-smoker, traitor, grandparent~pucoach, bus, train, boat~pu,ccoach, teacher, writer, managerTable 2: The substitute vector ~scfor context c, the out-of-context paraphrase vector ~pufor the word type u = coach,and the in-context paraphrase vector ~pu,cfor u in con-text c, as learned by our model.
Only the first few vectorentries (weights omitted) are shown.
~sc, ~puand ~pu,caredefined formally in sections 4.1 and 4.2.a substitute vector similarity measure.
Intuitively,with this weighting scheme, we wish to considermostly the word type contexts that induce a sensesimilar to that of the given word instance, under thepremise that similar contexts induce similar wordsenses.
The resulting word representation is biasedtowards the given context on one hand due to thecontext weighting scheme, and is bounded to the tar-get word type spectrum of meanings on the otherhand as only contexts of that word type are takeninto consideration.Table 2 exemplifies a context substitute vector andboth out-of-context and in-context word representa-tions learned by our model for a word instance.
Itis evident in this case that our in-context represen-tation comprises suitable paraphrases in contrast tothe out-of-context representation.
We evaluate theseword representations quantitatively in Section 6.
Wenext describe our model in more detail.4.1 Context representationWe wish the context representation in our modelto be optimized for measuring context similarity,which is typically used to bias in-context word rep-resentations towards a given context.For the purpose of measuring similarity betweencontexts, we consider in this section the contextsas ?targets?.
Accordingly, we observe that the sub-stitute vector of a word window context c can beconsidered as a vector of first-order co-occurrencefeatures of c, as it consists of slot filler words thatare likely to co-occur with this context.
Hence, wefollow prior work and propose to use Positive PMI(PPMI) as our substitute vector feature weights, in-stead of the conditional probabilities used by Yat-baz et al (2012), and vector Cosine as our contextQ: the transcendental meditation people advertisedthis: meditation can fix many sicknesses.substitutes relieve, circumvent, alleviateRsub: use the results of your analysis to suggestdesign changes that would fix these problems.substitutes overcome, solve, alleviateRcbow: fix in your mind a picture ofheavenly worship that is real and eternal.substitutes echoing, send, stickTable 3: Example for a context of the word fix, Q, and thetwo contexts of fix, Rsuband Rcbow, most similar to it,based on substitute vector and CBOW similarity, respec-tively.
The substitute vectors are illustrated below eachcontext (selected substitutes in the top-10 entries shown).similarity function (Bullinaria and Levy, 2007):~sc[v] = PPMI(c, v) = max(0, PMI(c, v)) (1)sim(c, c?)
= cos(~sc, ~sc?)
(2)where ~sc[v] is the fitness weight for word v in thesubstitute vector of context c, PMI is point-wisemutual information (Church and Hanks, 1990), andsim(c, c?)
is our context similarity measure.
Wenote that a context c in our setting stands for an en-tire word window rather than a single context word.We therefore follow Yatbaz et al (2012) using ann-gram language model to estimate PMI(c, v), asdetailed in Section 5.Table 3 illustrates an example of a given contextand the contexts most similar to it, as retrieved byour substitute vector and continuous bag-of-wordscontext similarity measures.
It is evident in this casethat our measure correlates with the induced sensesbetter than the bag-of-words measure.
We suggestthis context similarity measure as a standalone con-tribution, which may be useful in other settings aswell.
We evaluate it quantitatively in Section 5.4.2 Modeling word meaningWord meaning out-of-context We first define ourout-of-context representation for target word type u,as an average of the substitute vectors of its contexts:(3)~pu=1|Cu|?i?Cu~siwhere Cuis a collection of the contexts observedfor target word type u in a learning corpus, and ~siare their substitute vectors.474Word meaning in context Next, following Erkand Pad?o (2010), to represent the meaning of word uin context c, we would like to alter the out-of-contextrepresentation by theoretically averaging only overcontexts that induce a word sense similar to that ofthe given context.
To approximate this objective weuse a weighted average of all contexts of u, wherecontexts are weighted according to their similarityto the given context:(4)~pu,c=1Z?i?Ccusim(c, i) ?
~siwhere Ccu= Cu?
c (u?s corpus contexts plus thegiven context) and Z is a normalization factor.~pu,c[v] =1Z?i?Ccusim(c, i) ?
~si[v] is the av-erage fitness of v within the contexts of u, biasedto those similar to c. We consider this a context-sensitive similarity score, indicative of the likeli-hood of v to be a paraphrase of u in context c.2Thus,we name our in-context representation for a word in-stance as its paraphrase vector.Finally, ~pmu,cdenotes a word representation as de-fined in Equation (4), where only the top-m percentof the contexts in Ccumost similar to c are averaged.Using low values for m means injecting a strongerbias in our model towards the given context.5 Evaluating Context RepresentationsAs described in Section 4, our model for word mean-ing in context utilizes a context similarity measureunder the premise that similar contexts induce simi-lar target word senses.
In this section we describe afocused evaluation of our proposed similarity mea-sure and prior methods with respect to this objective.This evaluation suggests that our measures may beuseful as a component in other models as well.5.1 Task descriptionGiven a word window context c of a target word u,we wish to evaluate context similarity measures ontheir ability to retrieve other contexts of u from Cuthat induce a similar sense.
To perform such an eval-uation we want a dataset of target words with thou-sands of sense tagged contexts in Cufor each tar-get word u.
Since available manually sense-tagged2This can be considered an in-context extension of the out-of-context similarity score proposed by Melamud et al (2014).datasets, such as SemCor (Mihalcea, 1998), are notlarge enough for this purpose, we adopted a pseudo-word approach, with which we can automaticallygenerate as many tagged contexts as we wish.Pseudo-word methods consider a set of realwords as pseudo-senses of an artificial pseudo-word(Pilehvar and Navigli, 2014).
Specifically, weadopted a simple approach following Otrusina andSmrz (2010) to generate our pseduo-words.
First,we sampled 100 words randomly from our learn-ing corpus, ukWaC (Ferraresi et al, 2008).
Thenwe constructed a pseudo-word based on each ofthese words as follows.
We used WordNet (Fell-baum, 2010) to identify all of the word?s synsets.Next, for each synset we chose the surface wordwhich is the least polysemous yet occurs in ourlearning corpus at least 1,000 times, as a represen-tative for this synset.
Then, we created a pseudo-word whose pseudo-senses are the set of the rep-resentative words.
For example, the pseudo-wordthat was generated based on the word promoteis elevate encourage advertise.
Finally, we sam-pled from our learning corpus 1,000 contexts foreach pseudo-sense word, and for each pseudo-wordwe mixed together all contexts of its pseudo-sensewords.
The original pseudo-sense word for eachcontext was recorded as its sense tag.Next, for each pseudo-word, we sampled a singlequery context from all of its mixed contexts and thenranked the remaining contexts according to each ofthe compared context similarity measures.
We com-puted precision at top-1, top-1% and average pre-cision for the ranked lists, where a true-positive isa context with an identical sense tag as the querycontext.
We repeated this procedure, sampling 100different query contexts and computed the mean pre-cision values.
Finally, we report for each comparedmethod, the average of the mean precision valuesfor all 100 pseudo-words.
Our pseudo-word datasetconsists on average around 4.5 senses and 4,500tagged contexts per pseudo-word.35.2 Compared methodsAll compared methods are unsupervised and use theplain text of ukWaC (Ferraresi et al, 2008), a two3Our pseudo-word dataset is available at: www.cs.biu.ac.il/nlp/resources/downloads/word2parvec/475billion word web corpus, as their learning corpus.We converted every word that occurs less than 100times in the corpus to a special rare-word token, andall numbers to a special number token, obtaining avocabulary of a little under 200K word types.5.2.1 Substitute vector similarityWe learned a 5-gram Kneser-Ney language modelfrom our learning corpus using KenLM (Heafieldet al, 2013).
Following Yatbaz et al (2012),we used FASTSUBS (Yuret, 2012) with this lan-guage model to efficiently generate substitute vec-tors pruned to their top-n substitutes, v1..vn, andnormalized such that?i=1..np(vi|c) = 1.
In or-der to make our substitute vectors compatible withthe pseudo-word setting, for each substitute vec-tor we replaced the entries of all of the pseudo-sense words with a single pseudo-word entry, andassigned it with the sum of the conditional proba-bilities of the pseudo-sense words.
Next, we com-puted the Positive PMI weights for the substitutes,~sc[vi] = PPMI(vi, c) = max(0, log(p(vi|c)p(vi))),where p(vi) is the unigram probability of the word viin our learning corpus.
The unigram probability ofa pseudo-word is the sum of the probabilities of itspseudo-sense words.
Finally, we computed contextsimilarity as substitute vector Cosine.SUBweight,ndenotes our similarity measure,where n is the pruning factor and weight ?
{cond, ppmi} denotes conditional probabilities andPPMI fitness weights, respectively.
We note that byusing a 5-gram language model we consider a con-text word window of 4 words on each side of thetarget word.5.2.2 Bag-of-words similarityBag-of-words similarity between two contextword windows is computed as vector Cosine be-tween their bag-of-words vector representations.We use BOWweight,lto denote these context sim-ilarity measures, where l is the size of the con-text word window on each side of the target word(we use sent to denote the entire sentence), andweight ?
{tf, tfidf} stands for term frequency andtf-idf weights, respectively.
CBOWweight,lis usedto denote the same for the continuous bag-of-wordsmethod, which is based on averaging the densevector word representations.
We used word2vecMethod P@1 P@1% AvgPrecSUB ?
CBOW 80.6 67.1 44.5SUBppmi,100073.1 60.0 44.0SUBppmi,10074.5 61.0 42.8CBOWtfidf,8w68.4 58.0 43.5SUBcond,100063.6 53.0 38.6SUBcond,10063.1 52.7 38.5BOWtfidf,sent62.8 51.3 34.6Random 30.4 30.4 30.4Table 4: Precision values for compared context similaritymeasures.
Only the best performing configurations forBOW and CBOW are shown.
(Mikolov et al, 2013) to learn these dense vectors.45.3 ResultsThe results presented in Table 4 support our hypoth-esis that our proposed substitute vector similaritymeasure is particularly suitable for measuring con-text similarity, at least in our setting.
Our similaritymeasures outperform both CBOW and BOW base-lines on P@1 and P@1%, with statistical signifi-cance at p < 0.01 for SUBppmi,100, and p < 0.05for SUBppmi,1000, on a paired t-test.
On averageprecision they perform similarly to CBOW.
Withinthe substitute vector measures, our proposed PPMIweights significantly outperform the previously usedconditional probabilities, and the choice of pruningfactor has a small impact.
Within the bag-of-wordsmeasures, the CBOW measure significantly outper-forms the BOW measure, with an optimal windowsize of 8 (on each side).
This suggests that CBOW?sability to capture context word similarities via itsdense word representations is beneficial.Finally, SUB?CBOW denotes a combined simi-larity measure, which is the geometrical mean be-tween the scores of the best configurations of therespective methods.
We see that this combinationyields substantial improvement, outperforming allother baselines across all precision categories, withp < 0.0001 for P@1 and P@1%.
We hypothesizethat this is due to the synergy between the word or-der sensitivity of SUB and the word similarities andlarger window size captured by CBOW.4We experimented with various parameters of word2vec,observing small differences in performance.
We report here theresults with the best configuration (cbow 1, negative sampling15, window size 8).4766 Evaluating Word RepresentationsModels of word meaning in context are commonlyevaluated in lexical substitution tasks on predict-ing paraphrases of a target word that preserve itsmeaning in a given context.
Conveniently, our para-phrase vector representations for words include ex-actly these predictions.
We evaluated our model ontwo lexical substitution datasets under two types oftasks and compared it to the state-of-the-art as de-scribed next.6.1 Lexical substitution datasetsThe dataset introduced in the lexical substitutiontask of SemEval 2007 (McCarthy and Navigli,2007), denoted here LS07, is the most widely usedfor the evaluation of lexical substitution.
It consistsof 10 sentences extracted from a web corpus for eachof 201 target words (nouns, verbs, adjectives and ad-verbs), or altogether 2,010 word instances in senten-tial context, split into 300 trial sentences and 1,710test sentences.
The gold standard provided with thisdataset is a weighted lemmatized paraphrase list foreach word instance, based on manual annotations.A more recent dataset (Kremer et al, 2014), de-noted LS14, provides the same kind of data as LS07,but instead of target words that were specifically se-lected to be ambiguous as in LS07, the target wordshere are simply all the content words in text doc-uments extracted from news and fiction corpora.LS14 is also much larger than LS07 with over 15Ktarget word instances.6.2 Predicting lexical substitutions6.2.1 Task descriptionIn SemEval 2007 the lexical substitution task or-ganizers evaluated participant systems on their abil-ity to predict the paraphrases in the gold standard ofthe LS07 test-set in a few subtasks (1) best and best-mode - evaluate the quality of the best predictions(2) oot and oot-mode (out of ten) - evaluate the cov-erage of the gold paraphrase list by the top ten bestpredictions.5We performed this evaluation on boththe LS07 and LS14 datasets.5For brevity we do not describe the details of these subtasks.We report only recall scores as in this task recall=precision forall methods that predict paraphrases to all of the instances in thedataset, as we did.6.2.2 Compared methodsWe used the same learning corpus and substitutevector generation procedure as described in Sec-tion 5.
For every target word type u (not lemma-tized) in the LS07 and LS14 datasets, we sampled acollection of 20K sentence contexts from our learn-ing corpus (or less for word types with lower fre-quency), denoted Cu.6Next, we generated substi-tute vectors, pruned to top-100 entries, for these con-texts.
For every target word type u, we discardedthe contexts in Cuwhere u itself is not in the top-100 predicted substitutes, assuming that either thesecontexts are not typical to u, or that the quality ofthe predicted substitutes is low.
This omits approx-imately 25% of all contexts.
Finally, we generatedtop-100 and top-1000 substitute vectors for all of theinstances in the LS07 and LS14 datasets.We used a generalization of PPMI, called ShiftedPPMI (Levy and Goldberg, 2014), as our substi-tute vector fitness weights: SPPMI(v, c; s) =max(0, PPMI(v, c) ?
s), where s is a global shiftconstant.
Levy and Goldberg (2014) showed thatSPPMI outperformed PPMI on various semantictasks.
We tuned the value of s ?
{0.0, 1.0, 2.0, 3.0}on the trial portion of the LS07 dataset and used thebest value, 2.0, on the LS07 test set and on LS14.We omit results based on conditional probabilityweights for brevity as they were substantially worse.Next, for every LS07/LS14 instance of word u incontext c we generated a paraphrase vector accord-ing to Equation (4).
We used the paraphrase vectorssorted by entry scores as our paraphrase predictions.Pinn(in-context) denotes this method, where n is thepruning factor used for generating the substitute vec-tors of the lexical substitution datasets.7As baseline, we generated our meaning out-of-context paraphrase vectors according to Equa-tion (3), denoted Pout.
We also used word2vec(Mikolov et al, 2013) to generate dense word vec-tors for all word types in our learning corpus.8Para-6In general, we observed that the results improve the morecontexts are sampled up to ?
10K contexts per word type.7For the learning corpus contexts we always used top-100substitute vectors to reduce computational complexity.8We experimented with 600-dimension vectors, negativesampling value 15, both skip and cbow options, and variouswindow sizes, and tuned these parameters on the trial portion ofthe LS07 dataset.477phrase predictions for this baseline, denoted w2vout,were computed as the words most similar to the tar-get word based on dense vector Cosine similarities,ignoring the context (out-of-context).In the best subtasks we used only the top rankedlemmatized paraphrase (best prediction) suggestedby each of the compared methods.
In the oot sub-tasks we used the top-10 lemmatized paraphrases.To the best of our knowledge, Biemann andRiedl (2013) is the only prior work that attemptedto perform the original SemEval 2007 task on theLS07 dataset, learning only from corpus data likewe do.
They merged Gigaword (Parker et al, 2011)and LLC (Richter et al, 2006) as their learning cor-pus, which is similar in size to ours.
We denote byBiemanninand Biemannoutthe reported results fortheir in-context and out-of-context methods, respec-tively.
There is no previously reported result for thistask on LS14.6.2.3 ResultsThe results are shown in Table 5.
First, we notethat our out-of-context method significantly out-performs the out-of-context word2vec baseline onall subtasks in both LS07 and LS14, showing thatour model performs well on predicting paraphraseseven out-of-context.
Furthermore, our meaning in-context methods show significant additional gains inperformance on LS07, with top-1000 pruning per-forming a little better than top-100.
On LS14 wesee smaller gains, which may be due to the fact thatits target words are less ambiguous by construction.This behavior is consistent with similar findings inKremer et al (2014).
Finally, both BiemannoutandBiemanninexhibit substantially lower performanceon LS07 than our methods, achieving scores that areclose to the word2vec baseline.We note that all ten systems that participated inthe original SemEval 2007 task on the LS07 datasetfollowed a two-step scheme (1) generating para-phrase candidates using a manually constructed the-saurus, such as WordNet; (2) ranking the candidatesaccording to the given context based on data fromvarious learning corpora.
We stress that in con-trast to all these systems, our model does not uti-lize manually constructed thesauri, and therefore ad-dresses a much harder problem of predicting para-phrase substitutes out of the entire vocabulary, ratherMethod best best-m oot oot-mLS07 test-setPin100012.72 21.71 36.37 52.03Pin10012.25 20.73 35.54 50.98Pout10.68 18.29 32.58 46.34w2voutskip,4w8.25 13.41 29.27 39.92Biemanninn/a n/a 27.48 37.19Biemannoutn/a n/a 27.02 37.35LS14 allPin10008.07 17.37 26.67 46.23Pin1007.93 16.97 26.24 45.58Pout7.80 16.90 25.57 44.66w2voutskip,4w5.99 12.21 22.66 36.98Table 5: best and oot subtasks scores for all comparedmethods.
best-m and oot-m stand for the mode scores.than merely ranking a small given set of candidates.Even so, in the best subtasks we achieve top resultswith respect to the reported score range of these sys-tems, 2.98-12.90 for best, and 4.72-20.73 for best-mode.
In comparison to the reported oot score range,our results are lower than average.6.3 Ranking lexical substitutions6.3.1 Task descriptionMost works that used the LS07 dataset after Se-mEval 2007, as well as the results reported for LS14,focused only on candidate ranking.
Instead of us-ing a thesaurus, they obtained the set of paraphrasecandidates for each target type by pooling the an-notated gold-standard paraphrases from all of its in-stances.9The quality of the rankings with respect tothe gold standard was measured using GeneralizedAverage Precision (GAP) (Kishida, 2005).
Further-more, all of the works compared in this section dis-carded multi-word expression substitutes from theoriginal gold standard, and omitted instances whothus remained with no gold paraphrases.
We followthe same evaluation settings for this task.6.3.2 Compared methodsWe observe that in this task, the objective isto rank candidates that are known to be semanti-cally similar to the target word in some context.Therefore, we hypothesize that possibly more fo-cus should be given in this case to assessing the9A target type is defined as the pair (word lemma, pos),where pos ?
{noun, verb, adjective, adverb}.478compatibility between the candidates and the givencontext versus their semantic similarity to the targetword.
To this end, we explore strategies with differ-ent points of balance between these two factors.
Onone hand we evaluate the scores assigned to the can-didates by our out-of-context paraphrase vector rep-resentation, which is based only on semantic simi-larity.
Similarly, we also evaluate rankings based onword2vec similarity scores, with a range of learn-ing parameters (same range as used in Section 6.2)and report the best results that we were able to ob-tain.
On the other hand, we ignore the target wordidentity and consider only context compatibility byranking candidates based on their conditional prob-abilities to fill the target word slot, as reflected inthe respective context substitute vector representa-tion, denoted Scond,1000.Finally, we rank the candidates using the scores inour in-context paraphrase vectors from Section 6.2.However, this time we check the effect of injectinga stronger bias towards the given context c, by aver-aging only the top-m percent contexts most similarto c, for m ?
{1%, 5%, 10%, 100%}, as describedin Section 4.2.
We denote this as Pin,mn.
We donot report results based on conditional probabilityweights, as they perform substantially worse thanour SPPMI weights.
We also report the most recentstate-of-the-art results on both LS07 and LS14.
OnLS07, we report our results both on the test-set andon the entire dataset (trial+test).6.3.3 ResultsThe results are shown in Table 6.
Looking firstat the results on the LS07 dataset, we see that notsurprisingly both our out-of-context method, Pout,and the word2vec baseline, w2voutskip,2w, which ig-nore the given context, achieve relatively low results.Scond,1000that considers only the context compati-bility performs a little better.
Next, we see that ourin-context method, Pin,100%1000outperforms all of theabove, but Pin,5%1000, which is more strongly biasedto context compatibility performs even better.
Forbrevity, we report only the results for m = 5%,which performed best on the trial portion of LS07,but the results are almost as good for m = 1% andm = 10%.
This supports our hypothesis that in theranking task more focus is to be given to contextcompatibility.
As in the prediction task in SectionMethod Resources LS07 LS07 LS14test all allPin,5%1000UW55.2 55.1 50.2Pin,100%100052.0 51.7 50.0Scond,1000UW48.6 48.4 46.4Pout46.6 45.9 47.9w2voutskip,2w45.2 45.2 46.5Random 29.7 30.0 33.8Kremer, GW n/a 52.5 47.82014?Thater, GW n/a 51.7 n/a2011S?eaghdha, WP,BN n/a 49.5 n/a2014Moon, UW,BN,WN n/a 47.1 n/a2013 GW,WN n/a 46.7 n/aSzarvas, LLC,WN n/a 55.0* n/a2013bSzarvas, LLC,WN n/a 52.4* n/a2013aTable 6: GAP scores for compared methods.UW = ukWaC; GW = Gigaword; WP = Wikipedia;WN = WordNet; BN = British National Corpus (Astonand Burnard, 1998).
?A re-implementation of the model in Thater, 2011.
* Obtained by a supervised method.6.2, the pruning factor of 100 performed slightly (upto half a point) worse than 1000.In comparison to previous results, our methodachieves the best reported GAP score to date, on parwith Szarvas et al (2013b).
However, we note thatboth Szarvas et al (2013b) and Szarvas et al (2013a)follow a supervised approach, training on the LS07gold standard with 10-fold cross validation, as wellas incorporate features from WordNet.
Therefore,they cannot be directly compared with unsupervisedmodels, such as our own.
Our model and previousworks used different learning corpora that are sim-ilar in size.
Moon and Erk (2013) reported resultsfor both Gigaword and ukWaC, showing minor dif-ferences in performance.The results on LS14 exhibit a similar behaviorwith our method outperforming the state-of-the-art.However, as also reported in Kremer et al (2014),the performance gain achieved by taking the givencontext into consideration is smaller than in LS07.Again, this seems to be due to the nature of LS14,which is not biased to ambiguous target words.479Benchmark Orig 1000 100best 12.7 12.1 11.2best-m 21.7 20.9 19.7oot 36.3 34.8 32.5oot-m 52.0 50.4 46.4GAP test 55.2 52.0 50.9GAP all 55.1 51.8 50.7Table 7: The effect of context clustering on our model?sperformance on LS07.
Orig stands for best configurationof our model with no clustering, and 1000/100 stand forthe same configuration with that number of clusters.6.4 Computational efficiency and clusteringTo generate our in-context paraphrase predictionsfor an instance of a target word u, our model per-forms a weighted average over all of its context sub-stitute vectors in Cu.
The run-time complexity ofthis procedure is reasonably efficient at O(|Cu|?n),where n is the vector pruning factor.
This is com-parable to the complexity of the state-of-the-art al-gorithm before this work (Thater et al, 2011) andeven to word2vec?s dense vector computations.
Asa point of reference, in our experiments it took?300 msec to generate an in-context paraphrasevector for a given word instance on a modest sin-gle core, which was only about 3 times slower thanthe word2vec computation.Memory consumption of our model is not an issuewhen operating in an ?offline?
mode.
In this mode allthe target word instances in a test set (such as LS07or LS14), can first be sorted according to their wordtype.
Then, while processing all instances of thesame word type u one after the other, only the sub-stitute vectors in Cuneed to be loaded into memory.In contrast, in an ?online?
mode, to be ready for anyarbitrary word instance input, our model would needto keep in memory substitute vectors for all the wordtypes in the vocabulary V .
The space complexity inthis case is O(|Cu|?n ?
|V |), which can easily reachmemory consumptions in the order of ?100 GB ormore, requiring a large-scale server.To address this challenge, we present a morecoarse-grained variant of our model, where for eachword type u we keep only k substitute vectorsinstead of all individual context vectors, therebybounding the memory consumption to O(k ?n ?
|V |).To this end, for each word type u we used spheri-cal k-means to cluster the ?20,000 substitute vec-tors in Cuinto either 100 or 1000 clusters.
Then, in-stead of Cu, we used the collection of its cluster cen-troids, pruned to their top-100 entries.
Table 7 showsthe results when applying this to our best perform-ing configurations on the LS07 dataset.
The resultsshow relative performance degradation when fewerclusters are used, indicating that some relevant in-formation may be lost in this process.
However, ab-solute performance remains competitive, suggestingthat this is a viable option when memory consump-tion is a concern.
The results on the LS14 datasetshow similar trends.7 Discussion and Future WorkWe proposed a model for word meaning in contextwhose main novelty is in representing contexts assubstitute vectors.
Our model outperformed state-of-the-art baselines in both predicting and rankingparaphrases of words in context in two different lex-ical substitution tasks.
As another potential contri-bution, the context similarity measures used in ourmodel performed well on a targeted evaluation, sug-gesting that they may be useful as a component inother applications as well.Substitute vectors were successfully used earlierfor performing part-of-speech and word sense induc-tion tasks (Baskaya et al, 2013; Yatbaz et al, 2014),not addressed in this work.
These works took a dif-ferent approach, embedding words in a low dimen-sional space, based on target-substitute pairs sam-pled from substitute vectors.
It would be interestingto explore how our approach applies to these tasks.Finally, a preliminary qualitative analysis showedthat low quality substitute vectors may be a factorlimiting our model?s performance.
This suggeststhat generating substitute vectors with better lan-guage models, such as neural language models, isa potential path to further improvements.AcknowledgmentsWe thank our reviewers for their helpful remarks.This work was partially supported by the Israel Sci-ence Foundation grant 880/12, the German ResearchFoundation through the German-Israeli Project Co-operation (DIP, grant DA 1600/1-1), and the Euro-pean Community?s Seventh Framework Programme(FP7/2007-2013) grant 287923 (EXCITEMENT).480ReferencesGuy Aston and Lou Burnard.
1998.
The BNC hand-book: exploring the British National Corpus withSARA.
Capstone.Osman Baskaya, Enis Sert, Volkan Cirik, and DenizYuret.
2013.
Ai-ku: Using substitute vectors and co-occurrence modeling for word sense induction and dis-ambiguation.
In Proceedings of the SemEval.Chris Biemann and Martin Riedl.
2013.
Text: Now in2d!
a framework for lexical expansion with contextualsimilarity.
Journal of Language Modelling, 1(1):55?95.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.John A. Bullinaria and Joseph P. Levy.
2007.
Extract-ing semantic representations from word co-occurrencestatistics: A computational study.
Behavior ResearchMethods, 39(3):510?526.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.Katrin Erk and Sebastian Pad?o.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof ACL (short papers).Christiane Fellbaum.
2010.
WordNet.
Springer.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of English.In Proceedings of the 4th Web as Corpus Workshop(WAC-4).John R. Firth.
1957.
A synopsis of linguistic theory1930-1955.
Studies in linguistic analysis, pages 1?32.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark,and Philipp Koehn.
2013.
Scalable modified Kneser-Ney language model estimation.
In Proceedings ofACL.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of ACL.Kazuaki Kishida.
2005.
Property of average precisionand its generalization: An examination of evaluationindicator for information retrieval experiments.
Na-tional Institute of Informatics Tokyo, Japan.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of ICASSP.Gerhard Kremer, Katrin Erk, Sebastian Pad?o, and StefanThater.
2014.
What substitutes tell us-analysis of anall-words lexical substitution corpus.
In Proceedingsof EACL.Omer Levy and Yoav Goldberg.
2014.
Neural word em-beddings as implicit matrix factorization.
In Proceed-ings of NIPS.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of SemEval.Oren Melamud, Ido Dagan, Jacob Goldberger, IdanSzpektor, and Deniz Yuret.
2014.
Probabilistic mod-eling of joint-context in distributional similarity.
InProceedings of CoNLL.Rada Mihalcea.
1998.
Semcor semantically tagged cor-pus.
Unpublished manuscript.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Taesun Moon and Katrin Erk.
2013.
An inference-basedmodel of word meaning in context as a paraphrase dis-tribution.
ACM Trans.
Intell.
Syst.
Technol., 4(3):42:1?42:28.Diarmuid?O S?eaghdha and Anna Korhonen.
2014.
Prob-abilistic distributional semantics with latent variablemodels.
Computational Linguistics, 40(3):587?631.Lubomir Otrusina and Pavel Smrz.
2010.
A new ap-proach to pseudoword generation.
In Proceedings ofLREC.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English gigaword fifth edition,june.
Linguistic Data Consortium, LDC2011T07.Mohammad Taher Pilehvar and Roberto Navigli.
2014.A large-scale pseudoword-based evaluation frame-work for state-of-the-art word sense disambiguation.Computational Linguistics, 40(4):837?881.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics.Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd?ottir,and Chris Biemann.
2006.
Exploiting the Leipzig cor-pora collection.
Proceesings of the IS-LTC.Gy?orgy Szarvas, Chris Biemann, Iryna Gurevych, et al2013a.
Supervised all-words lexical substitution us-ing delexicalized features.
In Proceedings of HLT-NAACL.Gy?orgy Szarvas, R?obert Busa-Fekete, and EykeH?ullermeier.
2013b.
Learning to rank lexicalsubstitutions.
In Proceedings of EMNLP.Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effec-tive vector model.
In Proceedings of IJCNLP.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.Learning syntactic categories using paradigmatic rep-resentations of word context.
In Proceedings ofEMNLP.481Mehmet Ali Yatbaz, Enis R?fat Sert, and Deniz Yuret.2014.
Unsupervised instance-based part of speech in-duction using probable substitutes.
In Proceedings ofCOLING.Deniz Yuret.
2012.
FASTSUBS: An efficient and exactprocedure for finding the most likely lexical substitutesbased on an n-gram language model.
Signal Process-ing Letters, IEEE, 19(11):725?728.482
