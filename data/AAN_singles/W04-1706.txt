E-Assessment using Latent Semantic Analysis in the Computer ScienceDomain: A Pilot StudyPete Thomas, Debra Haley, Anne deRoeck, Marian PetreComputing Research Centre, Department of ComputingThe Open University, Walton Hall, Milton Keynes, UK MK7 6AAP.G.Thomas;D.T.Haley;A.Deroeck;M.Petre [at] open.ac.ukAbstractLatent Semantic Analysis (LSA)  is a statisticalNatural Language Processing (NLP) technique forinferring meaning from a text.
Existing LSA-basedapplications focus on formative assessment ingeneral domains.
The suitability of LSA forsummative assessment in the domain of computerscience is not well known.
The results from thepilot study reported in this paper encourage us topursue further research in the use of LSA in thenarrow, technical domain of computer science.This paper explains the theory behind LSA,describes some existing LSA applications, andpresents some results using LSA for automaticmarking of short essays for a graduate class inarchitectures of computing systems.1 IntroductionThis paper describes a pilot study undertaken toinvestigate the feasibility of using Latent SemanticAnalysis (LSA) for automatic marking of shortessays in the domain of computer science.
Theseshort essays are free-form answers to examquestions - not multiple choice questions (MCQ).Exams in the form of MCQs, although easy tomark, do not provide the opportunity for deeperassessment made possible with essays.This study employs LSA in several areas that areunder-researched.
First, it uses very small corpora?
less than 2,000 words compared to  about 11million words in one of the existing, successfulapplications (Wade-Stein & Kintsch, 2003).Second, it involves the specific, technical domainof computer science.
LSA research usuallyinvolves more heterogeneous text with a broadvocabulary.
Finally, it focuses on summativeassessment where the accuracy of results isparamount.
Most LSA research has involvedformative assessment for which more generalevaluations are sufficient.The study investigates one of the shortcomingsof LSA mentioned by Manning and Sch?tze (1999,p.
564).
They report that LSA has high recall butlow precision.
The precision declines because ofspurious co-occurrences.
They claim that LSAdoes better on heterogeneous text with a broadvocabulary.
Computer science is a technicaldomain with a more homogeneous vocabulary,which results, possibly, in fewer spurious co-occurrences.
A major question of this research ishow LSA will behave when the technique isstretched by applying it to a narrow domain.Section 2 gives the history of LSA and explainshow it works.
Section 3 describes several existingLSA applications related to e-assessment.
Section4 provides the motivation for more LSA researchand reports on a pilot study undertaken to assessthe feasibility of using LSA for automatic markingof short essays in the domain of computer science.Section 5 lists several open issues and areas forimprovement that future studies will address.Finally, Section 6 summarises the paper.2 What is Latent Semantic Analysis?
?Latent Semantic Analysis is a theory andmethod for extracting and representing thecontextual-usage meaning of words by statisticalcomputations applied to a large corpus of text?
(Landauer, Foltz & Laham, 1998).
It is astatistical-based natural language processing (NLP)method for inferring meaning from a text1.
It wasdeveloped by researchers at Bellcore as aninformation retrieval technique (Deerwester,Dumais, Furnas, Landauer & Harshman, 1990) inthe late 1980s.
The earliest application of LSA wasLatent Semantic Indexing (LSI) (Furnas, et al,1988; Deerwester, et al, 1990).
LSI provided anadvantage over keyword-based methods in that itcould induce associative meanings of the query(Foltz, 1996) rather than relying on exact matches.Landauer and Dumais (1997) promoted LSA asa model for the human acquisition of knowledge.They developed their theory after creating aninformation retrieval tool and observingunexpected results from its use.
They claimed that1 The researchers originally used the term LSI (LatentSemantic Indexing) to refer to the method.
Theinformation retrieval community continues to use theterm LSI.LSA solves Plato?s problem, that is, how do peoplelearn so much when presented with so little?
Theiranswer is the inductive process: LSA ?inducesglobal knowledge indirectly from local co-occurrence data in a large body of representativetext?
(Landauer & Dumais, 1997).From the original application for retrievinginformation, the applications of LSA have evolvedto systems that more fully exploit its ability toextract and represent meaning.
Recent applicationsbased on LSA compare a sample text with a pre-existing, very large corpus to judge the meaning ofthe sample.To use LSA, researchers amass a suitable corpusof text.
They create a term-by-document matrixwhere the columns are documents and the rows areterms (Deerwester, et al, 1990).
A term is asubdivision of a document; it can be a word,phrase, or some other unit.
A document can be asentence, a paragraph, a textbook, or some otherunit.
In other words, documents contain terms.
Theelements of the matrix are weighted word counts ofhow many times each term appears in eachdocument.
More formally, each element, aij in an ix j matrix is the weighted count of term i indocument j.LSA decomposes the matrix into three matricesusing Singular Value Decomposition (SVD), awell-known technique (Miller, 2003) that is thegeneral case of factor analysis.
Deerwester et.
al.,(1990) describe the process as follows.Let t = the number of terms, or rowsd =  the number of documents, or columnsX = a t by d matrixThen, after applying SVD, X = TSD, wherem = the number of dimensions, m <= min(t,d)T =  a t by m matrixS = an m by m diagonal matrix, i.e., onlydiagonal entries have non-zero valuesD =  an m by d matrixLSA reduces S, the diagonal matrix created bySVD, to an appropriate number of dimensions k,where k << m, resulting in S'.
The product of TS'Dis the least-squares best fit to X, the original matrix(Deerwester, et al, 1990).The literature often describes LSA as analyzingco-occurring terms.
Landauer and Dumais (1997)argue it does more and explain that the new matrixcaptures the ?latent transitivity relations?
amongthe terms.
Terms not appearing in an originaldocument are represented in the new matrix as ifthey actually were in the original document(Landauer & Dumais, 1997).
LSA?s ability toinduce transitive meanings is considered especiallyimportant given that Furnas et.
al.
(1982) reportfewer than 20% of paired individuals will use thesame term to refer to the same common concept.LSA exploits what can be named the transitiveproperty of semantic relationships: If A?B andB?C, then A?C (where ?
stands for issemantically related to).
However, the similarity tothe transitive property of equality is not perfect.Two words widely separated in the transitivitychain can have a weaker relationship than closerwords.
For example, LSA might find that copy ?duplicate ?
double ?
twin ?
sibling.
Copy andduplicate are much closer semantically than copyand sibling.Finding the correct number of dimensions for thenew matrix created by SVD is critical; if it is toosmall, the structure of the data is not captured.Conversely, if it is too large, sampling error andunimportant details remain, e.g., grammaticalvariants (Deerwester, et al, 1990; Miller, 2003;Wade-Stein & Kintsch, 2003).
Empirical workinvolving very large corpora shows the correctnumber of dimensions to be about 300 (Landauer& Dumais, 1997; Wade-Stein & Kintsch, 2003).Creating the matrices using SVD and reducingthe number of dimensions, often referred to astraining the system, requires a lot of computingpower; it can take hours or days to complete theprocessing (Miller, 2003).
Fortunately, once thetraining is complete, it takes just seconds for LSAto evaluate a text sample (Miller, 2003).3 Using LSA for assessment3.1 Types of assessmentElectronic feedback, or e-assessment, is animportant component of e-learning.
LSA, with itsability to provide immediate, accurate,personalised, and content-based feedback, can bean important component of an e-learningenvironment.Formative assessment provides direction, focus,and guidance concurrent with the learner engagingin some learning process.
E-assessment canprovide ample help to a learner without requiringadded work by a human tutor.
A learner canbenefit from private, immediate, and convenientfeedback.Summative assessment, on the other hand,happens at the conclusion of a learning episode oractivity.
It evaluates a learner?s achievement andcommunicates that achievement to interestedparties.
Summative assessment using LSA sharesthe virtues of formative assessment and canproduce more objective grading results than thosethat can occur when many markers are assessinghundreds of student essays.The applications described in the next sectionuse LSA to provide formative  assessment.
Section4 discusses a pilot study that focuses on summativeassessment.3.2 Existing applicationsMuch work is being done in the area of usingLSA to mark essays automatically and to providecontent-based feedback.
One of the greatadvantages of automatic assessment of essays is itsability to provide helpful, immediate feedback tothe learner without burdening the teacher.
Thisapplication is particularly suited to distanceeducation, where opportunities for one-on-onetutoring are infrequent or non-existent (Steinhart,2001).
Existing systems include Apex (Lemaire &Dessus, 2001), Autotutor (Wiemer-Hastings,Wiemer-Hastings & Graesser, 1999), IntelligentEssay Assessor (Foltz, Laham & Landauer, 1999),Select-a-Kibitzer (Miller, 2003), and SummaryStreet (Steinhart, 2001; Wade-Stein & Kintsch,2003).
They differ in details of audience addressed,subject domain, and advanced training required bythe system (Miller, 2003).
They are similar in thatthey are LSA-based, web-based, and providescaffolding, feedback, and unlimited practiceopportunities without increasing a teacher?sworkload (Steinhart, 2001).
All of them claim thatLSA correlates as well to human markers as humanmarkers correlate to one another.
See (Miller,2003) for an excellent analysis of these systems.4 E-Assessment pilot studyAlthough research using Latent SemanticAnalysis (LSA) to assess essays automatically hasshown promising results (Chung & O'Neil, 1997;Foltz, et al, 1999; Foltz, 1996; Lemaire & Dessus,2001; Landauer, et al, 1998; Miller, 2003;Steinhart, 2001; Wade-Stein & Kintsch, 2003), notenough research has been done on using LSA forinstructional software (Lemaire & Dessus, 2001).Previous studies involved both young students anduniversity-age students, and several differentknowledge domains.
An open question is how LSAcan be used to improve the learning of university-age, computer science students.
This section offersthree characteristics that distinguish this researchfrom existing research involving the use of LSA toanalyse expository writing texts and reports on apilot study to determine the feasibility of usingLSA to mark students?
short essay answers toexam questions.4.1 Focuses of the experimentThis subsection describes three facets of theexperiment that involve under-researched areas, inthe cases of the domain and the type of assessment,and an unsolved research question in the case ofthe appropriate dimension reduction value forsmall corpora.The study involves essays written by computerscience (CS) students.
CS, being a technicaldomain, has a limited, specialist vocabulary.
Thus,essays written for CS exams are thought to have amore restricted terminology than do the expositorywriting texts usually analysed by LSA researchers.Nevertheless, the essays are written in Englishusing a mixture of technical terms and generalterms.
Will LSA produce valid results?Accuracy is paramount in summativeassessment.
Whereas formative assessment can begeneral and informative, summative assessmentrequires a high degree of precision.
Can LSAproduce results with a high degree of correlationwith human markers?The consensus among LSA researchers, whocustomarily use very large corpora, is that thenumber of dimensions that produces the best resultis about 300.
But because this study involved  just17 graded samples, the number of reduceddimensions has to be less than 17.
Can LSA workwith many fewer dimensions than 300?
A broaderquestion is whether LSA can work with a smallcorpus in a restricted domain.4.2 The DataThe data for this experiment consisted ofanswers from six students to three questions in asingle electronic exam held at the Open Universityin April 2002.
The answers are free-form shortessays.
The training corpus for each questioncomprised 16 documents consisting of studentanswers to the same question and a specimensolution.
Table 1 gives the average size (in words)of both the student answers graded by LSA and thecorpus essays.QuestionAQuestionBQuestionCCorpusdocuments 112 35 131Studentanswers 108 31 88Table 1: Average document sizeThe corpus training documents had been markedpreviously by three trained human markers.
Theaverage marks were assigned to each corpusdocument.
To provide a standard on which tojudge the LSA results, each of the answers fromthe six students was marked by three humanmarkers and awarded the average mark.4.3 The LSA MethodThe following steps were taken three times, oncefor each question on the exam.?
Determine the words, or terms, in the corpusdocuments after removing punctuation andstop words.
(No attempt has yet been madeto deal with synonyms or word forms, suchas plurals, via stemming.)?
Construct a t x d term frequency matrix M,where t is the number of terms  in the corpusand d is the number of documents ?
17 inthis experiment.
Each entry tfij is the numberof times term i appears in document j.?
Weight each entry tfij in M using the simpleweighting scheme: 1 + log(tfij).?
Perform singular value decomposition of theweighted term frequency matrix resulting inMweighted = TSDT.?
Choose an optimum dimension, k, to reduceMweighted.
(see the next subsection for details)?
Compute B = SDT - the reduced weightedfrequency document?
Construct a vector, a, of weighted termfrequencies in a student-answer document.?
Compute the reduced student-answer vectora' = aTST?
Determine the corpus document that bestmatches the student-answer by comparing a'with the column vectors in B.?
Award the student-answer the markassociated with the most similar corpusdocument using the cosine similaritymeasure.4.4 Determining the optimum dimensionreduction (k)?
This experiment reduced the SVD matricesusing k = 2 .. number of corpus documents ?1, or k = 2 .. 16.
For each value of k, theLSA method produced a mark for eachstudent-answer.?
The experiment compared the six LSAmarks for the student-answers with thecorresponding average human mark usingEuclidean distance.?
The experiment revealed that, for thiscorpus, k = about 10 gave the best matchesacross the three questions.4.5 ResultsThe four graphs below show the results obtained.Question A012345671 2 3 4 5 6StudentPointsAwardedGraded by HumanGraded by LSAFigure 1: LSA marks for question AQuestion B012345671 2 3 4 5 6StudentPointsAwarded Graded by HumanGraded by LSAFigure 2: LSA marks for question BQuestion C0123456781 2 3 4 5 6StudentPointsAwardedGraded by HumanGraded by LSAFigure 3: LSA marks for question CTotal02468101214161 2 3 4 5 6StudentPointsAwardedGraded by HumanGraded by LSAFigure 4: LSA marks for total4.6 DiscussionThis experiment investigated the feasibility ofusing LSA to assess short essay answers.
Theresults shown in Figures 1 ?
3 suggest that LSA-marked answers were similar to human-markedanswers in 83% (15 of 18) of the answers tested.LSA seemed to work well on five of the sixstudent-answers for Question A, all the answers forQuestion B, and four of the six answers forQuestion C. For the three clearly incorrectanswers, LSA gave a higher score than did thehuman markers for the answer to question A andone higher mark and one lower mark than did thehuman markers for the answers to question C.To quantify these visual impressions, the studyused the Spearman?s rho statistical test for each ofthe three questions.
Only one of the three questionsshows a statistical correlation between LSA andhuman marks: question B shows a statisticalcorrelation significant at the 95% level.These results, while unacceptable for a real-world application, are encouraging given theextremely small corpus size of only 17 documents,or about 2,000 words for questions A and C andabout 600 words for question B.
This pilot studysolidified our understanding of how to use LSA,the importance of a large corpus, and how toapproach further research to improve the resultsand increase the applicability of the results of thispilot study.5 A roadmap for further research5.1 The corpusLSA results depend on both corpus size andcorpus content.5.1.1 Corpus sizeExisting LSA research stresses the need for alarge corpus.
The corpora for the pilot studydescribed in this paper were very small.
Inaddition, the documents are too few in number tobe  representative of the student population.
Anideal corpus would provide documents that give aspread of marks across the mark range and avariety of answers for each mark.
Future studieswill use a larger corpus.5.1.2 Corpus contentWiemer-Hastings, et.
al (1999) report that size isnot the only important characteristic of the corpus.Not surprisingly, the composition of the corpuseffects the results of essay grading by LSA.
Inaddition to specific documents directly related totheir essay questions, Wiemer-Hastings, et.
al usedmore general documents.
They found the bestcomposition to be about 40% general documentsand 60% specific documents.The corpora used for this pilot study comprisedonly specific documents - the human marked shortessays.
Future work will involve adding sections oftext books to enlarge and enrich the corpus withmore general documents.5.2 Weighting functionThe pilot study used local weighting - the mostbasic form of term weighting.
Local weighting isdefined as tfij (the number of times term i is foundin document j) dampened by the log function: localweighting = 1 + log (tfij ).
This dampening reflectsthe fact that a term that appears in a document xtimes more frequently than another term is not xtimes more important.The study selected this simple weightingfunction  to provide a basis on which to comparemore sophisticated functions in future work.
Manyvariations of weighting functions exist; two aredescribed next.5.2.1 Log-entropyDumais (1991) recommended using log-entropyweighting, which is local weighting times globalweighting.
Global weighting is defined as 1 ?
theentropy or noise.
Global weighting attempts toquantify the fact that a term appearing in manydocuments is less important than a term appearingin fewer documents.The log-entropy term weight for term i in doc j =( ) ( )????????????
??
?+?numdocsgftfgftftf iijiijij loglog11logwhereijtf  ?
term frequency ?
the frequency of term i indocument jigf  ?
global frequency ?
the total number oftimes term i occurs in the whole collection5.2.2 tfidfSebastiani (2002) claims the most commonweighting is tfidf, or term frequency inversedocument frequency.
( )jk dttfidf ,  = ( ) ( )kjk tTrTrdt#log,# ?where #( tk, dj ) denotes the number of times tkoccurs in dj#Tr(tk) denotes the document frequency of term tk,that is, the number of documents in Tr in which tkoccurs.Future studies will examine the effects ofapplying various term weighting functions.5.3 Similarity measuresThe pilot study used two different similaritymeasures.
It used the cosine measure to comparethe test document with the corpus documents.
Itused Euclidean distance to choose k, the number ofreduced dimensions that produced the best resultsoverall.
Other measures exist and will be tried infuture studies.Ljungstrand and Johansson (1998) define thefollowing similarity measures:Inner product (dot) measure:M( X, Y ) =  ?=niii yx1Cosine measure:M( X, Y ) =??
?===niiniiniiiyxyx111Manhattan distance measure:M( X, Y ) = ?=?niii yx1Euclidean distance measure (2-norm):M( X, Y ) = ( )?=?niii yx12m-norm measure:M( X, Y ) = ( ) mnimii yx11???????
?=,  m ?
NWhere X = (x1,x2,...,xn) and Y = (y1,y2,...,yn) are twon-dimensional vectors.Figure 5.
Similarity Measures5.4 Corpus pre-processingRemoving stop words is one type of pre-processing performed for this study.
Explicitlyadding synonym knowledge and stemming are twoadditional ways of preparing the corpus that futureresearch will consider.
Stemming involvesconflating word forms to a common string, e.g.,write, writing, writes, written, writer would berepresented in the corpus as writ.5.5 Dimension reductionChoosing the appropriate dimension, k, forreducing the matrices in LSA is a well known openissue.
The current consensus is that k should beabout 300.
No theory yet exists to suggest theappropriate value for k. Currently, researchersdetermine k by  empirically testing various valuesof k and selecting the best one.
The only heuristicsays that k << min(terms, documents).
Aninteresting result from the study reported in thispaper is that even though k had to be less than 17,the number of documents in our corpora and thusmuch less than the recommended value of 300,LSA produced statistically significant results forone of the three questions tested.Future studies will continue to investigate therelationship among k, the size of the corpus, thenumber of documents in the corpus, and the type ofdocuments in the corpus.6 SummaryThis paper introduced and explained LSA andhow it can be used to provide e-assessment by bothformative and summative assessment.
It providedexamples of existing research that uses LSA for e-assessment.
It reported the results of a pilot studyto determine the feasibility of using LSA to assessautomatically essays written in the domain ofcomputer science.
Although just one of the threeessay questions tested showed that LSA markswere statistically correlated to the average of threehuman marks, the results are promising becausethe experiment used very small corpora.Future studies will attempt to improve the resultsof LSA by increasing the size of the corpora,improving the content of the corpora,experimenting with different weighting functionsand similarity measures, pre-processing the corpus,and using various values of k for dimensionreduction.7 AcknowledgementsThe work reported in this study was partially supported bythe European Community under the Innovation SocietyTechnologies (IST) programme of the 6th FrameworkProgramme for RTD - project ELeGI, contract IST-002205.This document does not represent the opinion of the EuropeanCommunity, and the European Community is not responsiblefor any use that might be made of data appearing therein.8 ReferencesChung, G., & O'Neil, G. (1997).
Methodologicalapproaches to online scoring of essays (Centerfor the Study of Evaluation, CRESST No.
461).Los Angeles.Deerwester, S., Dumais, S. T., Furnas, G. W.,Landauer, T. K., & Harshman, R. (1990).Indexing by Latent Semantic Analysis.
Journalof the American Society for Information Science,41(6), 391-407.Dumais, S. T. (1991).
Improving the retrieval ofinformation from external sources.
BehavioralResearch Methods, Instruments & Computers,23(2), 229-236.Foltz, P. W. (1996).
Latent semantic analysis fortext-based research.
Behavior Research Methods,Instruments and Computers, 28(2), 197-202.Foltz, P. W., Laham, D., & Landauer, T. K. (1999).Automated Essay Scoring: Applications toEducational Technology.
In Proceedings ofEdMedia '99.Furnas, G. W., Deerwester, S., Dumais, S. T.,Landauer, T. K., Harshman, R. A., Streeter, L.A., et al (1988).
Information retrieval using asingular value decomposition model of latentsemantic structure.
ACM, pp.
465-480.Furnas, G. W., Gomez, L. M., Landauer, T. K., &Dumais, S. T. (1982).
Statistical semantics: Howcan a computer use what people name things toguess what things people mean when they namethings?
In Proceedings of the SIGCHIConference on Human Factors in ComputingSystems (pp.
251-253).
ACM.Landauer, T. K., & Dumais, S. T. (1997).
Asolution to Plato's problem: The Latent SemanticAnalysis theory of acquisition, induction andrepresentation of knowledge.
PsychologicalReview, 104(2), 211-240.Landauer, T. K., Foltz, P. W., & Laham, D. (1998).An introduction to Latent Semantic Analysis.Discourse Processes, 25, 259-284.Lemaire, B., & Dessus, P. (2001).
A system toassess the semantic content of student essays.Journal of Educational Computing Research,24(3), 305-320.Ljungstrand, P., & Johansson, H. (1998, May).Intranet indexing using semantic documentclustering.
Retrieved 5/4/2004, fromhttp://www.handels.gu.se/epc/archive/00002294/01/ljungstrand.IA7400.pdf.Manning, C., & Sch?tze, H. (1999).
Foundationsof Statistical Natural Language Processing.Cambridge, Massachusetts: MIT Press.Miller, T. (2003).
Essay assessment with LatentSemantic Analysis.
Journal of EducationalComputing Research, 28.Sebastiani, F. (2002, March).
Machine Learning inAutomated Text Categorization.
ACMComputing Surveys, 34(1), 1-47.Steinhart, D. J.
(2001).
Summary Street: Anintelligent tutoring system for improving studentwriting through the use of Latent SemanticAnalysis.
Unpublished doctoral dissertation,University of Colorado, Boulder, Department ofPsychology.Wade-Stein, D., & Kintsch, E. (2003).
SummaryStreet: Interactive computer support for writing(Tech Report from the Institute for CognitiveScience).
University of Colorado, USA.Wiemer-Hastings, P., Wiemer-Hastings, K., &Graesser, A. C. (1999).
Improving an intelligenttutor's comprehension of students with LatentSemantic Analysis.
In S. Lajoie & M.
Vivet(Eds.
), Artificial Intelligence in Education.Amsterdam: IOS Press.
