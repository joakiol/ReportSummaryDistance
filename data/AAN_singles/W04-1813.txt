Determining the Specificity of Terms based on Information TheoreticMeasuresPum-Mo Ryu and Key-Sun ChoiDept.
EECS/KORTERM KAIST373-1 Guseong-dong Yuseong-gu305-701 DaejeonKoreapmryu@world.kaist.ac.kr, kschoi@world.kaist.ac.krAbstractThis paper introduces new specificitydetermining methods for terms based oninformation theoretic measures.
Thespecificity of terms represents the quantity ofdomain specific information that is containedin the terms.
Compositional and contextualinformation of terms are used in proposedmethods.
As the methods don?t rely on domaindependent information, they can be applied toother domains without extra processes.Experiments showed very promising resultswith the precision 82.0% when applied to theterms in MeSH thesaurus.1 IntroductionThe specificity of terms represents the quantity ofdomain specific information contained in the terms.If a term has large quantity of domain specificinformation, the specificity of the term is high.
Thespecificity of a term X is quantified to positive realnumber as equation (1).
( )Spec X R+?
(1)The specificity is a kind of necessary conditionfor term hierarchy, i.e., if X1 is one of ancestors ofX2, then Spec(X1) is less than Spec(X2).
Thus thiscondition can be applied to automatic constructionor evaluation of term hierarchy.
The specificityalso can be applied to automatic term recognition.Many domain specific terms are multiwordterms.
When domain specific concepts arerepresented as multiword terms, the terms areclassified into two categories based on compositionof unit words.
In the first category, new terms arecreated by adding modifiers to existing terms.
Forexample ?insulin-dependent diabetes mellitus?
wascreated by adding modifier ?insulin-dependent?
toits hypernym ?diabetes mellitus?
as in Table 1.
InEnglish, the specific level terms are verycommonly compounds of the generic level termand some modifier (Croft, 2004).
In this case,compositional information is important to getmeaning of the terms.
In the second category, newterms are independent of existing terms.
Forexample, ?wolfram syndrome?
is semanticallyrelated to its ancestor terms as in Table 1.
But itshares no common words with its ancestor terms.In this case, contextual information is important toget meaning of the terms.Node Number TermsC18.452.297 diabetes mellitusC18.452.297.267 insulin-dependent diabetes mellitusC18.452.297.267.960 wolfram syndromeTable 1 Subtree of MeSH1 thesaurus.
Nodenumbers represent hierarchical structure of termsContextual information has been mainly used torepresent the meaning of terms in previous works.
(Grefenstette, 1994) (Pereira, 1993) and(Sanderson, 1999) used contextual information tofind hyponymy relation between terms.
(Caraballo,1999) also used contextual information todetermine the specificity of nouns.
Contrary,compositional information of terms has not beencommonly discussed.
We propose new specificitymeasuring methods based on both compositionaland contextual information.
The methods areformulated as information theory like measures.This paper consists as follow; new specificitymeasuring methods are introduced in section 2, andthe experiments and evaluation on the methods arediscussed in section 3, finally conclusions aredrawn in section 4.2 Specificity Measuring MethodsIn this section, we describe information theory likemethods to measure the specificity of terms.
Here,we call information theory like methods, becausesome probability values used in these methods are1 MeSH is available at http://www.nlm.nih.gov/mesh.MeSH 2003 was used in this research.CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 87not real probability, rather they are relative weightof terms or words.In information theory, when a low probabilitymessage occurs on channel output, the quantity ofsurprise is large, and the length of bits to representthe message becomes long.
Thus the large quantityof information is gained by the message (Haykin,1994).
If we regard the terms in corpus as themessages of channel output, the informationquantity of the terms can be measured usinginformation theory.
A set of target terms is definedas equation (2) for further explanation.
{ |1 }kT t k n= ?
?
(2)where tk is a term.
In next step, a discrete randomvariable X is defined as equation (3).
{ |1 }   ( ) Prob( )k k kX x k n p x X x= ?
?
= =    (3)where xk is an event of tk is observed in corpus,p(xk) is the probability of xk.
The informationquantity, I(xk), gained after observing xk, is used asthe specificity of tk as equation (4).
( ) ( ) log ( )k k kSpec t I x p x?
= ?
(4)By equation (4), we can measure the specificityof tk, by estimating p(xk).
We describe threeestimating methods for p(xk) in following sections.2.1 Compositional Information based Method(Method 1)By compositionality, the meaning of a term can bestrictly predicted from the meaning of theindividual words (Manning, 1999).
This method isdivided into two steps: In the first step, thespecificity of each word is measured independently.In the second step, the specificity of compositewords is summed up.
For detail description, weassume that tk consists of one or more words asequation (5).1 2...k mt w w w=                           (5)where wi is i-th word in tk.
In next step, a discreterandom variable Y is defined as equation (6).
{ |1 }   ( ) Prob( )i i iY y i m p y Y y= ?
?
= =       (6)where yi is an event of wi occurs in term tk, p(yi) isthe probability of yi.
Information quantity, I(xk), inequation (4) is redefined as equation (7) based onprevious assumption.1( ) ( ) log ( )mk i iiI x p y p y== ??
(7)where I(xk) is average information quantity of allwords in tk.
In this mechanism, p(yi) of informativewords should be smaller than that of noninformative words.
Two information sources, wordfrequency, tf.idf are used to estimate p(yi)independently.We assume that if a term is composed of lowfrequency words, the term have large quantity ofdomain information.
Because low frequency wordsappear in limited number of terms, they have highdiscriminating ability.
On this assumption, p(yi) inequation (7) is estimated as relative frequency of wiin corpus.
In this estimation, P(yi) for lowfrequency words becomes small.tf.idf is widely used term weighting scheme ininformation retrieval (Manning, 1999).
We assumethat if a term is composed of high tf.idf words, theterm have domain specific information.
On thisassumption, p(yi) in equation (7) is estimated asequation (8).
( )( ) ( ) 1( )ii MLE ijjtf idf wp y p wtf idf w??
= ?
??
(8)where tf?idf(w) is tf.idf value of w. In this equation,p(yi) of high tf.idf words becomes small.If the modifier-head structure is known, thespecificity of the term is calculated incrementallystarting from head noun.
In this manner, thespecificity of the term is always larger than that ofthe head term.
This result answers to theassumption that more specific term has higherspecificity.
We use simple nesting relationsbetween terms to analyze modifier-head structureas follows (Frantzi, 2000):Definition 1 If two terms X and Y are terms insame semantic category and X is nested in Y asW1XW2, then X is head term, and W1 and W2 aremodifiers of X.For example, because ?diabetes mellitus?
isnested in ?insulin dependent diabetes mellitus?
andtwo terms are all disease names, ?diabetesmellitus?
is head term and ?insulin dependent?
ismodifier.
The specificity of Y is measured asequation (9).1 2( ) ( ) ( ) ( )Spec Y Spec X Spec W Spec W?
?= + ?
+ ?
(9)where Spec(X), Spec(W1), and Spec(W2) are thespecificity of X, W1, W2 respectively.
?
and ?
areweighting schemes for the specificity of modifiers.They are found by experimentally.2.2 Contextual Information based Method(Method 2)There are some problems that are hard to addressusing compositional information alone.
Firstly,although two disease names, ?wolfram syndrome?and ?insulin-dependent diabetes mellitus?, shareCompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology88many common features in semantic level, theydon?t share any common words in lexical level.
Inthis case, it is unreasonable to compare twospecificity values based on compositionalinformation.
Secondly, when several words arecombined into one term, there are additionalsemantic components that are not predicted by unitwords.
For example, ?wolfram syndrome?
is a kindof ?diabetes mellitus?.
We can not predict themeaning of ?diabetes mellitus?
from two separatewords ?wolfram?
and ?syndrome?.
Thus we usecontextual information to address these problems.General terms are frequently modified by otherwords in corpus.
Because domain specific termshave sufficient information in themselves, they arerarely modified by other words, (Caraballo, 1999).Under this assumption, we use probabilitydistribution of modifiers as contextual information.Collecting sufficient modifiers from given corpusis very important in this method.
To this end, weuse Conexor functional dependency parser(Conexor, 2004) to analyze the structure ofsentences.
Among many dependency functionsdefined in the parser, ?attr?
and ?mod?
functionsare used to extract modifiers from analyzedstructures.
This method can be applied the termsthat are modified by other words in corpus.Entropy of modifiers for a term is defined asequation (10).
( ) ( , ) log ( , )mod k i k i kiH t p mod t p mod t= ??
(10)where p(modi,tk) is the probability of modi modifiestk and it is estimated as relative frequency of modiin all modifiers of tk.
The entropy calculated byequation (10) is the average information quantityof all (modi,tk) pairs.
Because domain specificterms have simple modifier distributions, theentropy of the terms is low.
Therefore inversedentropy is assigned to I(xk) in equation (4) to makespecific terms get large quantity of information.1( ) max( ( )) ( )k mod i mod ki nI x H t H t?
??
?
(11)where the first term of approximation is themaximum modifier entropy of all terms.2.3 Hybrid Method (Method 3)In this section, we describe hybrid method toovercome shortcomings of previous two methods.In this method the specificity is measured asequation (12).1( ) 1 1( ) (1 )( )( ) ( )kCmp k Ctx kI xI x I x?
?
?+ ?
(12)where ICmp(xk) and ICtx(xk) are information quantitymeasured by method1 and method 2 respectively.They are normalized value between 0 and 1.
(0 1)?
??
?
is weight of two values.
If 0.5?
= , theequation is harmonic mean of two values.Therefore I(xk) becomes large when two values areequally large.3 Experiments and EvaluationIn this section, we describe our experiments andevaluate proposed methods.We select a subtree of MeSH thesaurus for theexperiment.
?metabolic diseases(C18.452)?
node isroot of the subtree, and the subtree consists of 436disease names which are target terms forspecificity measuring.
We used MEDLINE 2database corpus (170,000 abstracts, 20,000,000words) to extract statistical information.Each method was evaluated by two criteria,coverage and precision.
Coverage is the fraction ofthe terms which have the specificity by givenmethod.
Method 2 gets relatively lower coveragethan method 1, because method 2 can measure thespecificity only when both the terms and theirmodifiers occur in corpus.
Method 1 can measurethe specificity whenever parts of composite wordsappear in corpus.
Precision is the fraction ofcorrect specificity relations values as equation (13).#   ( , )#    ( , )of R p c with correct specificitypof all R p c=   (13)where R(p,c) is a parent-child relation in MeSHthesaurus.
If child term c has larger specificity thanthat of parent term p, then the relation is said tohave correct specificity.
We divided parent-childrelations into two types.
Relations where parentterm is nested in child term are categorized as typeI.
Other relations are categorized as type II.
Thereare 43 relations in type I and 393 relations in typeII.
The relations in type I always have correctspecificity provided modifier-head informationmethod described in section 2.1 is applied.We tested prior experiment for 10 humansubjects to find out the upper bound of precision.The subjects are all medical doctors of internalmedicine, which is closely related division to?metabolic diseases?.
They were asked to identifyparent-child relationship for given term pairs.
Theaverage precisions of type I and type II were96.6% and 86.4% respectively.
We set these valuesas upper bound of precision for suggested methods.2  MEDLINE is a database of biomedical articlesserviced by National Library of Medicine, USA.
(http://www.nlm.nih.gov)CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 89The specificity of terms was measured withmethod 1, method 2, and method 3 as Table 2.Two additional methods, based on term frequencyand term tf.idf, were experimented to comparecompositionality based methods and term basedmethods.Method 1 showed better performance than termbased methods.
This result illustrate basicassumption of this paper that specific concepts arecreated by adding information to existing concepts,and new concepts are expressed as new terms byadding modifiers to existing terms.
Word tf.idfbased method showed better precision than wordfrequency based method.
This result illustrate thattf.idf of words is more informative than frequencyof words.Method 3 showed the best precision, 82.0%,because the two methods interactedcomplementary.
In hybrid method, the weightvalue 0.8?
=  indicates that compositionalinformation is more informative than contextualinformation for the specificity of domain specificterms.One reason of the errors is that the names ofsome internal nodes in MeSH thesaurus arecategory names rather disease names.
For example,as ?acid-base imbalance (C18.452.076)?
is nameof disease category, it doesn't occur as frequentlyas other real disease names.
Other predictablereason is that we didn?t consider various surfaceforms of same term.
For example, although?NIDDM?
is acronym of ?non insulin dependentdiabetes mellitus?, the system counted two termsseparately.
Therefore the extracted statistics can?tproperly reflect semantic level information.4 ConclusionThis paper proposed specificity measuringmethods for terms based on information theory likemeasures using compositional and contextualinformation of terms.
The methods areexperimented on the terms in MeSH thesaurus.Hybrid method showed the best precision of 82.0%,because two methods complemented each other.As the proposed methods don't use domaindependent information, they can be adapted toother domains without extra processes.In the future, we will modify the system tohandle various term formations such as abbreviatedform.
Finally we will apply the proposed methodsto the terms of other specific domains.5 AcknowledgementsThis work was supported in part by Ministry ofScience & Technology, Ministry of Culture &Tourism of Korean government, and KoreaScience & Engineering Foundation.ReferencesCaraballo, S. A., Charniak, E. 1999.
Determiningthe Specificity of Nouns from Text.
Proceedingsof the Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing andVery Large CorporaConexor.
2004.
Conexor Functional DependencyGrammar Parser.
http://www.conexor.comCroft, W. 2004.
Typology and Universals.
2nd ed.Cambridge Textbooks in Linguistics, CambridgeUniv.
PressFrantzi, K., et.
al.
2000.
Automatic recognition ofmulti-word terms: the C-value/NC-value method.Journal of Digital Libraries, vol.
3, num.
2Grefenstette, G. 1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer AcademicPublishersHaykin, S. 1994.
Neural Network.
IEEE PressManning, C. D. and Schutze, H. 1999.Foundations of Statistical Natural LanguageProcessing.
The MIT PresssPereira, F., Tishby, N., and Lee, L. 1993.Distributational clustering of English words.Proceedings of ACLSanderson, M. 1999.
Deriving concept hierarchiesfrom text.
Proceedings of ACM S1GIRPrecisionMethodsType I Type II TotalCoverageHuman subjects(Average) 96.6 86.4 87.4Term frequency 100.0 53.5 60.6 89.5Term tf?idf 52.6 59.2 58.2 89.5Word Freq.
37.2 72.5 69.0 100.0Word Freq.+Structure (?=?=0.2) 100.0 72.8 75.5 100.0Word tf?idf 44.2 75.3 72.2 100.0CompositionalInformationMethod(Method 1) Word tf?idf +Structure (?=?=0.2) 100.0 76.6 78.9 100.0Contextual Information Method (Method 2) (mod cnt>1) 90.0 66.4 70.0 70.2Hybrid Method (Method 3)  (tf?idf + Struct, ?=0.8) 95.0 79.6 82.0 70.2Table 2.
Experimental results (%)CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology90
