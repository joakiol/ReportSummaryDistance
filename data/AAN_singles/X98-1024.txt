Automatic Text Summarization in TIPSTERThdr~se  F i rmin  lnder jeet  Man iDepartment of Defense The MITRE Corporation, W6409800 Savage Road 11493 Sunset Hills RoadFort George G. Meade, Maryland 20755-6000 Reston, Virginia 22090tt'irmi n @ romulus.ncsc.mil imani@mitre.orgAutomatic Text Summarization was added as amajor research thrust of the TIPSTER programduring TIPSTER Phase III, 1996-1998.
It is anatural extension of the previously supportedresearch efforts in Information Extraction (IE)and Information Retrieval (IR).
There isconsiderable interest in automatically producingsummaries due, in large part, to the growth of theInternet and the World Wide Web.
TheTIPSTER program sponsored seven researchefforts into text summarization, all with differentapproaches tothe problem.?
Carnegie Group lnc and Carnegie MellonUniversity teamed up to investigate atechnique called "Maximal MarginalRelevance" or MMR, which producessummaries of documents by identifying keyrelevant, non-redundant i formation foundwithin the document, intended primarily forvery long documents.?
Cornell University and SabIR Research,Inc.
used the document ranking and passageretrieval capabilities of the SMART IRengine to effectively identify relevantrelated passages in a document.?
GE Research & Development identifiedthe discourse macro structure for eachdocument and selected the passages fromeach component that scored well using bothcontent and contextual c ues.?
New Mexico State University also usedinformation about he document structurecombined with part of speech and propername recognition to weight and selectsentences to be included in their summaries.?
Textwise LLC primarily concentrated onsummaries of multiple documents.
Theyassigned subject field coding of documentsas an initial indicator of document contentand identified the most relevant paragraphs,combining statistical information about ermfrequency with linguistic information.?
University of Pennsylvania used co-reference resolution as the basis for theirsummaries, finding information within adocument that is naturally linked together byreferring to the same individual,organization, orevent and extracting thatrelated information to generate a summary.USC Information Sciences Institute (ISI)used a multi-faceted approach, including theoptimal position of a sentence within a text,which varies based on text type, andbuilding thematic representations of textsbased on external ontologies.Each of the systems is described in detailelsewhere in the proceedings.In coordination with the various researchefforts, DARPA sponsored an evaluation of textsummarization systems.
This evaluation wasconducted in two phases.
In September 1997, adry-run was held to validate the evaluationmethodology and establish a baseline forperformance.
Participation i the dry-run waslimited to TIPSTER researchers only.
A formalevaluation was conducted inMay 1998 and wasopen to all interested parties.The summaries can be characterized andevaluated by many features, including:?
Coverage - m summary can cover asingle document or a group of relateddocuments.?
Focus - A generic summary capturesthe main theme(s) of a document,whereas auser-directed summary isgeared towards aparticular topic ofinterest indicated by the user.?
h~tent - An indicative summaryprovides aquick overview of thecontent of the full text, but is notintended to serve as a substitute.
Aninformative summary should captureenough relevant information to be areplacement for the full text document.The evaluation consisted of four tasksdesigned to assess performance ofautomaticsummaries used in real world tasks and toleverage off of previous evaluations in IR and IE,the Text REtrieval Conferences and MessageUnderstanding Conferences, respectively.
Itprimarily addressed indicative summa_izies ofsingle documents.179In the adhoc task, analysts read the output ofthe summarization systems intermixed with fulltext and baseline (lead sentence) summaries andassessed the relevance of each text presented tothem with respect o a given topic (each systemsummary was a user-directed summary).In the categorization task, analysts revieweda different set of summaries, full text andbaselines and had to determine which topic,among a set of five related topics, bestrepresented the theme of the text (each summarywas a generic summary).For the question-and-answer task, each user-directed summary was evaluated to determine ifit included the answers to 4-5 questionsconsidered essential for a document to berelevant o a given topic.
The percentage ofquestions answered was compared to that of afull text document.
A summary that successfullycaptures all of the relevant concepts in adocument could be considered a goodinformative summary of that document.The acceptability task simply asked theanalysts to read each of 30 summaries and thecorresponding full text and determine if thesummary was a good summary for the document.This task was not conducted uring the dry-run.For the adhoc and categorization tasks,participants submitted two summaries.
One wasrestricted to 10% of the length of the document,the other could vary in length and was intendedto capture the best summary a system couldproduce.
The question-and-answer andacceptability tasks used only these bestsummaries.TREC documents and relevance data wereused for all tasks.
This provided a good basis forthe initial evaluations, however futureevaluations should evolve beyond singledocument summarization of newspaper-styletext.The formal evaluation was held in the springof 1998 and in addition to the seven TIPSTERcontractors included participants from around theworld:* British Telecomm's ProSum?
Center for Information Research(Russia)?
Intelligent Algorithm's Infogist?
IBM 'Thomas J. Watson Research?
Lexis-Nexis?
National Taiwan University?
SRA International?
University of Massachusetts Center forIntelligent Information Retrieval?
University of SurreyThere was not a large difference inperformance between the various systems ineither evaluation, however the results do showsome encouraging trends.
The analysts wereable to process the best summaries more quicklythan the full text without a significant loss inaccuracy, and they preferred reading documentsthat were shorter in length than the typical longfull text articles.
The results from the dry-runwere discussed uring the TIPSTER meeting inOctober 1997 and are documented in \[Firminand Chrzanowski 1998\].
The results of theformal evaluation were discussed in May 1998and are documented in \[Mani et al, 1998\].Interest in summarization continues to grow.Two frequently mentioned applications includesummarization of the results of an IR query (e.g.from Altavista or Infoseek) or combiningsummarization with a traditional text processingapplication such as Microsoft Word.
Researchefforts are moving towards summarization acrossdocuments and summarization i languagesother than English.
As this research continuesand more applications come on the market, itwill be useful to have a benchmark against whichto evaluate the utility of the systems.
TheDARPA evaluation was a first step in thatdirection.The dry-run and formal evaluations wereconducted by the Department of Defense,SPAWAR Systems Center and The MITRECorp.
under DARPA sponsorship through theTIPSTER program.References\[Firmin and Chrzanowski 1998\] Firmin, T. andChrzanowski, M. J., An Evaluation of AutomaticText Summarization Systems, in Mani, I. andand Maybury, M. Advances in Automatic TextSummarization, MIT Press, 1998.\[Mani et al, 1998\] Mani, I., Firmin, T., House,D., Chrzanowski, M., Klein, G., Sundheim, B.,Hirschman, L., Obrst, L. (1998), The TIPSTERText Summarization Evaluation: Final Report,http://www.tipster.orJ.180
