Detection of Question-Answer Pairs in Email ConversationsLokesh Shrestha and Kathleen McKeownColumbia UniversityComputer Science DeparmentNew York, NY 10027,USA,lokesh@cs.columbia.edu, kathy@cs.columbia.eduAbstractWhile sentence extraction as an approach tosummarization has been shown to work in docu-ments of certain genres, because of the conver-sational nature of email communication whereutterances are made in relation to one madepreviously, sentence extraction may not capturethe necessary segments of dialogue that wouldmake a summary coherent.
In this paper, wepresent our work on the detection of question-answer pairs in an email conversation for thetask of email summarization.
We show that var-ious features based on the structure of email-threads can be used to improve upon lexicalsimilarity of discourse segments for question-answer pairing.1 IntroductionIn this paper, we discuss work on the detection ofquestion and answer pairs in email threads, i.e., co-herent exchanges of email messages among severalparticipants.
Email is a written medium of asyn-chronous multi-party communication.
This meansthat, as in face-to-face spoken dialog, the emailthread as a whole is a collaborative effort with inter-action among the discourse participants.
However,unlike spoken dialog, the discourse participants arenot physically co-present, so that the written word isthe only channel of communication.
Furthermore,replies do not happen immediately, so that respon-ders need to take special precautions to identify rel-evant elements of the discourse context (for exam-ple, by quoting previous messages).
Thus, email isa distinct linguistic genre that poses its own chal-lenges to summarization.With the increasing popularity of email as ameans of communication, an increasing number ofmeetings are scheduled, events planned, issues re-solved, and questions answered through emails.
Asthe number of emails in one?s mailbox increases, in-Regarding ?acm home/bjarney?, on Apr 9,2001, Muriel Danslop wrote:Two things: Can someone be responsible forthe press releases for Stroustrup?Responding to this on Apr 10, 2001, TheresaFeng wrote:I think Phil, who is probably a better writerthan most of us, is writing up something fordang and Dave to send out to various ACMchapters.
Phil, we can just use that as our?press release?, right?In another subthread, on Apr 12, 2001, KevinDanquoit wrote:Are you sending out upcoming events for thisweek?Figure 1: Sample summary obtained with sentenceextractionformation from past conversations becomes increas-ingly inaccessible and difficult to manage.
For ex-ample, a number of emails can be used in schedul-ing a meeting, and a search for information on themeeting may retrieve all the intermediate emails,thus hindering one?s access to the required informa-tion.
Access to required information could be dra-matically improved by querying summaries of emailconversationsWhile summarization of email conversationsseems a natural way to improve upon currentmethods of email management, research on emailsummarization is in early stages.
Consider anexample summary of a thread of email conver-sation produced by a sentence extraction basedemail thread summarization system developed atColumbia (Rambow et al, 2004) shown in Figure 1.While this summary does include an answer to thefirst question, it does not include answers to thetwo questions posed subsequently even though theanswers are present in the thread.
This exampledemonstrates one of the inadequacies of sentenceextraction based summarization modules: namely,the absence of discourse segments that would havemade the summaries more readable and complete.A summarization module that includes answers toquestions posed in extractive summaries, then, be-comes very useful.Further, questions are a natural means of resolv-ing any issue.
This is especially so of email conver-sations through which most of our issues, whetherprofessional or personal, get resolved.
And, theasynchronous nature of email conversation makes itpossible for users to pursue several questions in par-allel.
In fact, in our corpus of email exchanges, wefound that about 20% of all email threads focus pri-marily on a question-answer exchange, whether onequestion is posed and multiple people respond orwhether multiple questions are posed and multipleresponses given.
For these type of email exchanges,a summary that can highlight the main question(s)asked and the response(s) given would be useful.Being able to distinguish questions pertaining to dif-ferent issues in an email thread and being able toassociate the answers with their questions is a ne-cessity in order to generate this type of summary.In this paper, we present our work on the detec-tion of question and answer pairs in email conver-sations.
The question-answer detection system wepresent will ultimately serve as one component of afull email summarization system, providing a por-tion of summary content.
We developed one ap-proach for the detection of questions in email mes-sages, and a separate approach to detect the corre-sponding answers.
These are described in turn be-low.2 Previous and Related Work(Muresan et al, 2001) describe work on summariz-ing individual email messages using machine learn-ing approaches to learn rules for salient noun phraseextraction.
In contrast, our work aims at summariz-ing whole threads and at capturing the interactivenature of email.
(Nenkova and Bagga, 2003) present work on gen-erating extractive summaries of threads in archiveddiscussions.
A sentence from the root message andfrom each response to the root is extracted usingad-hoc algorithms crafted by hand.
This approachworks best when the subject of the root email bestdescribes the ?issue?
of the thread, and when theroot email does not discuss more than one issue.
Inour work, we do not make any assumptions aboutthe nature of the email, and try to learn strategies tolink question and answer segments for summariza-tion.
(Newman and Blitzer, 2003) also address theproblem of summarizing archived discussion lists.They cluster messages into topic groups, and thenextract summaries for each cluster.
The summary ofa cluster is extracted using a scoring metric based onsentence position, lexical similarity of a sentence tocluster centroid, and a feature based on quotation,among others.
Because the summaries are extrac-tive in nature, this approach still suffers from thepossibility of incomplete summaries.
(Lam et al, 2002) present work on email summa-rization by exploiting the thread structure of emailconversation and common features such as namedentities and dates.
They summarize the messageonly, though the content of the message to be sum-marized is ?expanded?
using the content from its an-cestor messages.
The expanded message is passedto a document summarizer which is used as a blackbox to generate summaries.
Our work, in contrast,aims at summarizing the whole thread, and we areprecisely interested in changing the summarizationalgorithm itself, not in using a black box summa-rizer.In addition, there has been some work on summa-rizing meetings.
As discussed in Section 1, email isdifferent in important respects from multi-party di-alog.
However, some important aspects are related.
(Zechner and Lavie, 2001), for example, presentsa spoken dialogue summarization system that takesinto consideration local cross-speaker coherence bylinking question answer pairs, and uses this infor-mation to generate extract based summaries withcomplete question-answer regions.
While we haveused a similar question detection approach, our ap-proach to answer detection is different.
We get backto this in Section 4.
(Rambow et al, 2004) show that sentence ex-traction techniques can work for summarizing emailthreads, but profit from email-specific features.
Inaddition, they show that the presentation of the sum-mary should take into account the dialogic structureof email communication.
However, since their ap-proach does not try to detect question and answerpairs, the extractive summaries suffer from the pos-sibility of incomplete summaries.3 Automatic Question DetectionWhile the detection of questions in email messagesis not as difficult a problem as in speech conversa-tions where features such as the question mark char-acter are absent, relying on the use of question markcharacter for identifying questions in email mes-sages is not adequate.
The need for special attentionin detecting questions in email messages arises dueto three reasons.
First, the use of informal languagemeans users might use the question mark characterin cases other than questions (for example, to de-note uncertainty) and may overlook using a questionmark after a question.
Second, a question may bestated in a declarative form, as in, ?I was wonderingif you are free at 5pm today.?
Third, not every ques-tion, whether in an interrogative form or in a declar-ative form, is meant to be answered.
For example,rhetorical questions are used for purposes other thanto obtain the information the question asked, and arenot required to be associated with answer segments.We used supervised rule induction for the de-tection of interrogative questions.
Training exam-ples were extracted from the transcribed SWITCH-BOARD corpus annotated with DAMSL tags.1This particular corpus was chosen not only be-cause an adequate number of training examplescould be extracted from the manual annotations,but also because of the use of informal languagein speech that is also characteristic of email con-versation.
Utterances with DAMSL tags ?sv?
(speech act ?statement-opinion?)
and ?sd?
(speechact ?statement-non-opinion?)
were used to extract5,000 negative examples.
Similarly, utteranceswith tags ?qy?
(?yes-no-question?
), ?qw?
(?Wh-question?
), and ?qh?
(?rhetorical-question?)
wereused to extract 5,000 positive examples.
Each utter-ance was then represented by a feature vector whichincluded the following features:?
POS tags for the first five terms (shorter utter-ances were padded with dummies)?
POS tags for the last five terms (shorter utter-ances were padded with dummies)?
length of the utterance1From the Johns Hopkins University LVCSRSummer Workshop 1997, available fromhttp://www.colorado.edu/ling/jurafsky/ws97/Scheme Ripper Ripper+Recall 0.56 0.72Precision 0.96 0.96F1-score 0.70 0.82Table 1: Test results for detection of questions ininterrogative form?
POS-bigrams from a list of 100 most discrimi-nating POS-bigrams list.The list of most discriminating POS-bigrams wasobtained from the training data by following thesame procedure that (Zechner and Lavie, 2001)used.We then used Ripper (Cohen, 1996) to learnrules for question detection.
Like many learningprograms, Ripper takes as input the classes to belearned, a set of feature names and possible values,and training data specifying the class and featurevalues for each training example.
In our case, thetraining examples are the speech acts extracted fromthe SWITCHBOARD corpus as described above.Ripper outputs a classification model for predictingthe class (i.e., whether a speech act is a questionor not) of future examples; the model is expressedas an ordered set of if-then rules.
For testing, wemanually extracted 300 questions in interrogativeform and 300 statements in declarative form fromthe ACM corpus.2 We show our test results with re-call, precision and F1-score3 in Table 1 on the firstcolumn.While the test results show that the precision wasvery good, the recall score could be further im-proved.
Upon further investigation on why the re-call was so low, we found that unlike the positiveexamples we used in our training data, most of thequestions in the test data that were missed by therules learned by Ripper started with a declarativephrase.
For example, both ?I know its on 108th, butafter that not sure, how exactly do we get there?
?,and ?By the way, are we shutting down clic??
be-gin with declarative phrases and were missed by theRipper learned rules.
Following this observation,2More information on the ACM corpus will be provided inSection 4.1.
At the time of the development of the questiondetection module the annotations were not available to us, sowe had to manually extract the required test speech acts.3F1-score = 2PRP+R , where P=Precision and R=Recallwe manually updated our question detection mod-ule to break a speech act that was not initially pre-dicted as question into phrases separated by commacharacters.
Then we applied the rules on the firstphrase of the speech act and if that failed on the lastphrase.
For example, the rules would fail on thephrase ?I know its on 108th?, but would be able toclassify the phrase ?how exactly do we get there?as a question.
In doing this we were able to increasethe recall score to 0.72, leading to a F1-score of 0.82as shown in Table 1 in the second column.4 Automatic Answer DetectionWhile the automatic detection of questions in emailmessages is relatively easier than the detection ofthe same in speech conversations, the asynchronousnature of email conversations makes detection andpairing of question and answer pairs a more diffi-cult task.
Whereas in speech a set of heuristics canbe used to identify answers as shown by (Zechnerand Lavie, 2001), such heuristics cannot be readilyapplied to the task of question and answer pairing inemail conversations.
First, more than one topic canbe discussed in parallel in an email thread, whichimplies that questions relating to more than a singletopic can be pursued in parallel.
Second, even whenan email thread starts with the discussion of a singletopic, the thread may eventually be used to initiate adifferent topic just because the previous topic?s listof recipients closely matched those required for thenewly initiated topic.
Third, because of the use of?Reply?
and ?ReplyAll?
functions in email clients,a user may be responding to an issue posed earlierin the thread while using one of the email messagessubsequent to the message posing the issue to ?replyback?
to that issue.
So, while it may seem from thestructure of the email thread that a person is reply-ing back to a certain email, the person may actuallybe replying back to an email earlier in the thread.This implies that when several persons answer aquestion, there may be answers which appear sev-eral emails after the email posing the question.
Fi-nally, the fact that the question and its correspond-ing answers may have few words in common fur-ther complicates answer detection.
This is possiblewhen a person uses the context of the email conver-sation to ask questions and make answers, and thesemantics of such questions and answers have to beinterpreted with respect to the context they appearin.
Such context is readily available for a readerthrough the use of quoted material from past emailmessages.
All of these make the task of detectingand linking question and answer pairs in email con-versations a complicated task.
However, this taskis not as complicated a task as automatic questionanswering where the search space for candidate an-swers is much wider and more sophisticated mea-sures than those based on lexical similarity have tobe employed.Our approach to automatic answer detection inemail conversations is based on the observation thatwhile a number of issues may be pursued in paral-lel, users tend to use separate paragraphs to addressseparate issues in the same email message.
Whilea more complicated approach to segmentation ofemail messages could be possible, we have used thisbasic observation to delineate discourse segments inemail messages.
Further, because discourse seg-ments contain more lexical context than their in-dividual sentences, our approach detects associa-tions between pairs of discourse segments ratherthan pairs of sentences.We now present our machine learning approachto automatic detection of question and answer pairs.4.1 CorpusOur corpus consists of approximately 300 threads ofabout 1000 individual email messages sent duringone academic year among the members of the boardof the student organization of the ACM at ColumbiaUniversity.
The emails dealt mainly with planningevents of various types, though other issues werealso addressed.
On average, each thread contained3.25 email messages, with all threads containing atleast two messages, and the longest thread contain-ing 18 messages.
Threads were constructed fromthe individual email messages using the ?In-Reply-To?
header information to link parent and childemail messages.Two annotators (DB and GR) each were askedto highlight and link question and answer pairs inthe corpus.
Our work presented here is based onthe work these annotators had completed at the timeof this writing.
GR has completed work on 200threads of which there are 81 QA threads (threadswith question and answer pairs), 98 question seg-ments, and 142 question and answer pairs.
DB hascompleted work on 138 threads of which there are62 QA threads, 72 question segments, and 92 ques-tion and answer pairs.
We consider a segment tobe a question segment if a sentence in that segmenthas been highlighted as a question.
Similarly, weconsider a segment to be an answer segment if asentence in that segment has been paired with aquestion to form a question and answer pair.
Thekappa statistic (Carletta, 1996) for identifying ques-tion segments is 0.68, and for linking question andanswer segments given a question segment is 0.81.4.2 FeaturesFor each question segment in an email message,we make a list of candidate answer segments.This is basically just a list of original (contentthat is not quoted from past emails)4 segments inall the messages in the thread subsequent to themessage of the question segment.
Let the thread inconsideration be called t, the container message ofthe question segment be called mq, the containermessage of the candidate answer segment be calledma, the question segment be called q, and thecandidate answer segment be called a.
For eachquestion and candidate answer pair, we computethe following sets of features:4.2.1 Some Standard Features(a) number of non stop words in segment q and seg-ment a;(b) cosine similarity5 and euclidean distance6 be-tween segment q and a;4.2.2 Features derived from the structure ofthe thread t(c) the number of intermediate messages betweenmq and ma in t;(d) the ratio of the number of messages in t sent ear-4While people do use quoted material to respond to specificsegments of past emails, a phenomenon more common is dis-cussion lists, because the occurrence of such is rare in the ACMcorpus we decided not to use them as a feature.5cosine sim(x, y) =?Ni=1(cxi ?
cyi)?
?Nj=1 c2xj ?
?Nj=1 c2xjwhere cxi is the count of word i in segment x, and cyi is thecount of word i in segment y.6euclidean dis(x, y) =???
?N?i=1(c2xi ?
c2yi)where cxi is the count of word i in segment x, and cyi is thecount of word i in segment y.lier than mq and all the messages in t, and similarlyfor ma;(e) whether a is the first segment in the list of candi-date answer segments of q (this is true if a segmentis the first segment in the first message sent in replyto mq);4.2.3 Features based on the other candidateanswer segments of q(f) number of candidate answer segments of q andthe number of candidate answer segments of q aftera (a segment x is considered to be after another seg-ment y if x is from a message sent later than that ofy, or if x appears after y in the same message);(g) the ratio of the number of candidate answer seg-ments before a and the number of all candidate an-swer segments (a segment x is considered to be be-fore another segment y if x is from a message sentearlier than that of y, or if x appears before y in thesame message); and(h) whether q is the most similar segment of aamong all segments from ancestor messages of mabased on cosine similarity (the list of ancestor mes-sages of a message is computed by recursively fol-lowing the ?In-Reply-To?
header information thatpoints to the parent message of a message).While the contribution of a single feature to theclassification task may not be intuitively apparent,we hope that a combination of a subset of thesefeatures, in some way, would help us in detectingquestion-answer pairs.
For example, when the num-ber of candidate answer segments for a questionsegment is less than or equal to two, feature (e) maybe the best contributor to the classification task.
But,when the number of candidate answer segments ishigh, a collection of some features may be the bestcontributor.We categorized each feature vector for the pair qand a as a positive example if a has been marked asan answer and linked with q.4.3 Training DataWe computed four sets of training data.
Two foreach of the annotators separately, which we call DBand GR according to the label of their respectiveannotator.
One taking the union of the annotators,which we call Union, and another taking the inter-section, which we call Inter.
For the first two sets,we collected the threads that had at least one ques-tion and answer pair marked by the respective anno-tator.
For each question that has an answer markedData Set DB GR Union Interdatapoints 259 355 430 181positives 89 139 168 59questions 72 98 118 52threads 62 81 97 46Table 2: Summary of training data: number of in-stancesData Set Precision Recall F1-scoreDB 0.6 0.27 0.372GR 0.629 0.439 0.517Union 0.61 0.429 0.503Inter 0.571 0.407 0.475Table 3: Baseline results(some of the highlighted questions do not have cor-responding answers in the thread), we computed alist of feature vectors as described above with all ofits candidate answer segments.
For the union set,we collected all the threads that had question andanswer pairs marked by either annotator, and com-puted the feature vectors for each such question seg-ment.
A feature vector was categorized positive ifany of the two annotators have marked the respec-tive candidate answer segment as an answer.
Forthe intersection set, we collected all the threads thathad question and answer pairs marked by both an-notators.
Here, a feature vector was labelled posi-tive only if both the annotators marked the respec-tive candidate answer segment as an answer.
Ta-ble 2 summarizes the information on the four setsof training data.4.4 Experiments and ResultsThis section describes experiments using Ripper toautomatically induce question and candidate answerpair classifiers, using the features described in Se-cion 4.2.
We obtained the results presented here us-ing five-fold cross-validation.Table 3 shows the precision, recall and F1-scorefor the four datasets using the cosine similarity fea-ture only.
We use these results as the baselineagainst which we compare the results for the fullset of features shown in Table 4.
While precisionusing the full feature set is comparable to that ofthe baseline measure, we get a significant improve-ment on recall with the full feature set.
The base-Data Set Precision Recall F1-scoreDB 0.69 0.652 0.671GR 0.68 0.612 0.644Union 0.698 0.619 0.656Inter 0.6 0.508 0.55Table 4: Summary of resultsline measure predicts that the candidate answer seg-ment whose similarity with the question segment isabove a certain threshold will be an actual answersegment.
Our results suggest that lexical similaritycannot alone capture the rules associated with ques-tion and answer pairing, and that the use of variousfeatures based on the structure of the thread of emailconversations can be used to improve upon lexicalsimilarity of discourse segments.
Further, while theresults do not seem to suggest a clear preference forthe data set DB over the data set GR (this could beexplained by their high kappa score of 0.81), takingthe union of the two datasets does seem to be bet-ter than taking the intersection of the two datasets.This could be because the intersection greatly re-duces the number of positive data points from whatis available in the union, and hence makes the learn-ing of rules more difficult with Inter.Finally, on observing that some questions had atmost 2 candidate answers, and others had quite afew, we investigated what happens when we dividethe data set Union into two data sets, one for ques-tion segments with 2 or less candidate answer seg-ments which we call the data set Union a, and theother with the rest of the data set which we call thedata set Union b.
Union a has, on average, 1.5 can-didate answer segments, while Union b has 5.7.
Weshow the results of this experiment with the full fea-ture set in Table 5.
Our results show that it is mucheasier to learn rules for questions with the data setUnion a, which we show in the first row, than other-wise.
We compare our results for the baseline mea-sure of predicting the majority class, which we showin the second row, to demonstrate that the resultsobtained with the dataset Union a were not due tomajority class prediction.
While the results for theother subset, Union b, which we show in the thirdrow, compare well with the results for Union, whenthe results for the data sets Union a and Union bare combined, which we show in the fourth row,we achieve better results than without the splitting,Data Set Precision Recall F1-score positives datapointsUnion a 0.879 0.921 0.899 63 79Baseline for Union a 0.797 1.0 0.887 63 79Union b 0.631 0.619 0.625 105 351Combined 0.728 0.732 0.730 168 430Union 0.698 0.619 0.656 168 430Table 5: Summary of results with the split data set compared with a baseline and the unsplit data setshown in the last row.5 Conclusion and Future WorkWe have presented an approach to detect question-answer pairs with good results.
Our approach isthe first step towards a system which can highlightquestion-answer pairs in a generated summary.
Ourapproach works well with interrogative questions,but we have not addressed the automatic detectionof questions in the declarative form and rhetoricalquestions.
People often pose their requests in adeclarative form in order to be polite among otherreasons.
Such requests could be detected with theiruse of certain key phrases some of which include?Please let me know...?, ?I was wondering if...?, and?If you could....that would be great.?.
And, becauserhetorical questions are used for purposes other thanto obtain the information the question asked, suchquestions do not require to be paired with answers.The automatic detection of these question types arestill under investigation.Further, while the approach to the detection ofquestion-answer pairs in threads of email conver-sation we have presented here is quite effective, asshown by our results, the use of such pairs of dis-course segments for use in summarization of emailconversations is an area of open research to us.As we discussed in Section 1, generation of sum-maries for email conversations that are devoted toquestion-answer exchanges and that integrate iden-tified question-answer pairs as part of a full sum-mary is also needed.6 AcknowledgementsWe are grateful to Owen Rambow for his helpful ad-vice.
We also thank Andrew Rosenberg for his dis-cussion on kappa statistic as it relates to the ACMcorpus.
This work was supported by the NationalScience Foundation under the KDD program.
Anyopinions, findings, and conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views of theNational Science Foundation.ReferencesJean Carletta.
1996.
Assessing agreement on clas-sification tasks: The kappa statistic.
Computa-tional Linguistics, 22(2):249?254.William Cohen.
1996.
Learning trees and ruleswith set-valued features.
In Fourteenth Confer-ence of the American Association of Articial In-telligence.
AAAI.Derek Lam, Steven L. Rohall, Chris Schmandt, andMia K. Stern.
2002.
Exploiting e-mail structureto improve summarization.
In ACM 2002 Confer-ence on Computer Supported Cooperative Work(CSCW2002), Interactive Posters, New Orleans,LA.Smaranda Muresan, Evelyne Tzoukermann, and Ju-dith Klavans.
2001.
Combining Linguistic andMachine Learning Techniques for Email Sum-marization.
In Proceedings of the CoNLL 2001Workshop at the ACL/EACL 2001 Conference.Ani Nenkova and Amit Bagga.
2003.
Facilitatingemail thread access by extractive summary gen-eration.
In Proceedings of RANLP, Bulgaria.Paula Newman and John Blitzer.
2003.
Summariz-ing archived discussions: a beginning.
In Pro-ceedings of Intelligent User Interfaces.Owen Rambow, Lokesh Shrestha, John Chen, andChristy Lauridsen.
2004.
Summarizaing emailthreads.
In Proceedings of HLT-NAACL 2004:Short Papers.Klaus Zechner and Alon Lavie.
2001.
Increasingthe coherence of spoken dialogue summaries bycross-speaker information linking.
In Proceed-ings of the NAACL-01 Workshop on AutomaticSummarization, Pittsburgh, PA.
