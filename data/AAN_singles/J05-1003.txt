ArticlesDiscriminative Reranking for NaturalLanguage ParsingMichael CollinsMassachusetts Institute of TechnologyTerry KooMassachusetts Institute of TechnologyThis article considers approaches which rerank the output of an existing probabilistic parser.The base parser produces a set of candidate parses for each input sentence, with associatedprobabilities that define an initial ranking of these parses.
A second model then attempts toimprove upon this initial ranking, using additional features of the tree as evidence.
The strengthof our approach is that it allows a tree to be represented as an arbitrary set of features, withoutconcerns about how these features interact or overlap and without the need to define aderivation or a generative model which takes these features into account.
We introduce a newmethod for the reranking task, based on the boosting approach to ranking problems described inFreund et al (1998).
We apply the boosting method to parsing the Wall Street Journal treebank.The method combined the log-likelihood under a baseline model (that of Collins [1999]) withevidence from an additional 500,000 features over parse trees that were not included in theoriginal model.
The new model achieved 89.75% F-measure, a 13% relative decrease in F-measure error over the baseline model?s score of 88.2%.
The article also introduces a newalgorithm for the boosting approach which takes advantage of the sparsity of the feature space inthe parsing data.
Experiments show significant efficiency gains for the new algorithm over theobvious implementation of the boosting approach.
We argue that the method is an appealingalternative?in terms of both simplicity and efficiency?to work on feature selection methodswithin log-linear (maximum-entropy) models.
Although the experiments in this article are onnatural language parsing (NLP), the approach should be applicable to many other NLPproblems which are naturally framed as ranking tasks, for example, speech recognition, machinetranslation, or natural language generation.1.
IntroductionMachine-learning approaches to natural language parsing have recently shown somesuccess in complex domains such as news wire text.
Many of these methods fall intothe general category of history-based models, in which a parse tree is represented as aderivation (sequence of decisions) and the probability of the tree is then calculated as aproduct of decision probabilities.
While these approaches have many advantages, itcan be awkward to encode some constraints within this framework.
In the ideal case,the designer of a statistical parser would be able to easily add features to the model* 2005 Association for Computational Linguistics MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), the Stata Center, Building 32,32 Vassar Street, Cambridge, MA 02139.
Email: mcollins@csail.mit.edu, maestro@mit.edu.Submission received: 15th October 2003; Accepted for publication: 29th April 2004that are believed to be useful in discriminating among candidate trees for a sentence.In practice, however, adding new features to a generative or history-based model canbe awkward: The derivation in the model must be altered to take the new features intoaccount, and this can be an intricate task.This article considers approaches which rerank the output of an existingprobabilistic parser.
The base parser produces a set of candidate parses for eachinput sentence, with associated probabilities that define an initial ranking of theseparses.
A second model then attempts to improve upon this initial ranking, usingadditional features of the tree as evidence.
The strength of our approach is that itallows a tree to be represented as an arbitrary set of features, without concerns abouthow these features interact or overlap and without the need to define a derivationwhich takes these features into account.We introduce a new method for the reranking task, based on the boostingapproach to ranking problems described in Freund et al (1998).
The algorithm can beviewed as a feature selection method, optimizing a particular loss function (theexponential loss function) that has been studied in the boosting literature.
We appliedthe boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus,Santorini, and Marcinkiewicz 1993).
The method combines the log-likelihood under abaseline model (that of Collins [1999]) with evidence from an additional 500,000features over parse trees that were not included in the original model.
The baselinemodel achieved 88.2% F-measure on this task.
The new model achieves 89.75% F-measure, a 13% relative decrease in F-measure error.Although the experiments in this article are on natural language parsing, theapproach should be applicable to many other natural language processing (NLP)problems which are naturally framed as ranking tasks, for example, speechrecognition, machine translation, or natural language generation.
See Collins (2002a)for an application of the boosting approach to named entity recognition, and Walker,Rambow, and Rogati (2001) for the application of boosting techniques for ranking inthe context of natural language generation.The article also introduces a new, more efficient algorithm for the boostingapproach which takes advantage of the sparse nature of the feature space in theparsing data.
Other NLP tasks are likely to have similar characteristics in terms ofsparsity.
Experiments show an efficiency gain of a factor of 2,600 for the new algorithmover the obvious implementation of the boosting approach.
Efficiency issues areimportant, because the parsing task is a fairly large problem, involving around onemillion parse trees and over 500,000 features.
The improved algorithm can perform100,000 rounds of feature selection on our task in a few hours with current processingspeeds.
The 100,000 rounds of feature selection require computation equivalent toaround 40 passes over the entire training set (as opposed to 100,000 passes for the??naive??
implementation).The problems with history-based models and the desire to be able to specifyfeatures as arbitrary predicates of the entire tree have been noted before.
In particular,previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, DellaPietra, and Lafferty 1997; Johnson et al 1999; Riezler et al 2002) has investigated theuse of Markov random fields (MRFs) or log-linear models as probabilistic models withglobal features for parsing and other NLP tasks.
(Log-linear models are often referredto as maximum-entropy models in the NLP literature.)
Similar methods have also beenproposed for machine translation (Och and Ney 2002) and language understanding indialogue systems (Papineni, Roukos, and Ward 1997, 1998).
Previous work (Friedman,Hastie, and Tibshirani 1998) has drawn connections between log-linear models and26Computational Linguistics Volume 31, Number 127boosting for classification problems.
One contribution of our research is to drawsimilar connections between the two approaches to ranking problems.We argue that the efficient boosting algorithm introduced in this article is anattractive alternative to maximum-entropy models, in particular, feature selectionmethods that have been proposed in the literature on maximum-entropy models.
Theearlier methods for maximum-entropy feature selection methods (Ratnaparkhi,Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra,Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) requireseveral full passes over the training set for each round of feature selection, suggestingthat at least for the parsing data, the improved boosting algorithm is several orders ofmagnitude more efficient.1 In section 6.4 we discuss our approach in comparison tothese earlier methods for feature selection, as well as the more recent work ofMcCallum (2003); Zhou et al (2003); and Riezler and Vasserman (2004).The remainder of this article is structured as follows.
Section 2 reviews history-based models for NLP and highlights the perceived shortcomings of history-basedmodels which motivate the reranking approaches described in the remainder of thearticle.
Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000;Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty2001; Collins, Schapire, and Singer 2002) that derives connections between boostingand maximum-entropy models for the simpler case of classification problems; thiswork forms the basis for the reranking methods.
Section 4 describes how theseapproaches can be generalized to ranking problems.
We introduce loss functions forboosting and MRF approaches and discuss optimization methods.
We also derive theefficient algorithm for boosting in this section.
Section 5 gives experimental results,investigating the performance improvements on parsing, efficiency issues, and theeffect of various parameters of the boosting algorithm.
Section 6 discusses related workin more detail.
Finally, section 7 gives conclusions.The reranking models in this article were originally introduced in Collins (2000).
Inthis article we give considerably more detail in terms of the algorithms involved, theirjustification, and their performance in experiments on natural language parsing.2.
History-Based ModelsBefore discussing the reranking approaches, we describe history-based models (Blacket al 1992).
They are important for a few reasons.
First, several of the best-performingparsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997,1999; Henderson 2003) are cases of history-based models.
Many systems applied topart-of-speech tagging, speech recognition, and other language or speech tasks also fallinto this class of model.
Second, a particular history-based model (that of Collins[1999]) is used as the initial model for our approach.
Finally, it is important to describehistory-based models?and to explain their limitations?to motivate our departurefrom them.Parsing can be framed as a supervised learning task, to induce a function f : XYYgiven training examples ?xi, yi?, where xi Z X , yi Z Y.
We define GEN?x?
?Y to be theset of candidates for a given input x.
In the parsing problem x is a sentence, and1 Note, however, that log-linear models which employ regularization methods instead of featureselection?see, for example, Johnson et al (1999) and Lafferty, McCallum, and Pereira (2001)?are likelyto be comparable in terms of efficiency to our feature selection approach.
See section 6.3 for morediscussion.Collins and Koo Discriminative Reranking for NLPGEN?x?
is a set of candidate trees for that sentence.
A particular characteristic of theproblem is the complexity of GEN?x?
: GEN?x?
can be very large, and each member ofGEN?x?
has a rich internal structure.
This contrasts with ??typical??
classification prob-lems in which GEN?x?
is a fixed, small set, for example, f1;?1g in binaryclassification problems.In probabilistic approaches, a model is defined which assigns a probability P?x, y?to each ?x, y?
pair.2 The most likely parse for each sentence x is then arg maxyZGEN(x)P?x, y?.
This leaves the question of how to define P?x, y?.
In history-based approaches,a one-to-one mapping is defined between each pair ?x, y?
and a decision sequencebd1::: dn?.
The sequence bd1::: dn?
can be thought of as the sequence of moves that build?x, y?
in some canonical order.
Given this mapping, the probability of a tree can bewritten asP?x, y?
?Yi?1:::nP?di)F?d1::: di1?
?Here, ?d1::: di1?
is the history for the ith decision.
F is a function which groups histo-ries into equivalence classes, thereby making independence assumptions in the model.Probabilistic context-free grammars (PCFGs) are one example of a history-basedmodel.
The decision sequence bd1::: dn?
is defined as the sequence of rule expansions ina top-down, leftmost derivation of the tree.
The history is equivalent to a partially builttree, and F picks out the nonterminal being expanded (i.e., the leftmost nonterminal inthe fringe of this tree), making the assumption that P?dijd1::: di1?
depends only onthe nonterminal being expanded.
In the resulting model a tree with rule expansionsbAiYbi?
is assigned a probabilityQi?1n P?bijAi?.Our base model, that of Collins (1999), is also a history-based model.
It can be con-sidered to be a type of PCFG, where the rules are lexicalized.
An example rule would beVP?saw?
> VBD?saw?
NPC?her?
NP?today?Lexicalization leads to a very large number of rules; to make the number of parametersmanageable, the generation of the right-hand side of a rule is broken down into a numberof decisions, as follows:First the head nonterminal (VBD in the above example) is chosen.Next, left and right subcategorization frames are chosen({} and {NP-C}).Nonterminal sequences to the left and right of the VBD are chosen(an empty sequence to the left, bNP-C, NP?
to the right).Finally, the lexical heads of the modifiers are chosen (her and today).282 To be more precise, generative probabilistic models assign joint probabilities P?x, y?
to each ?x, y?
pair.Similar arguments apply to conditional history-based models, which define conditional probabilitiesP?y j x?
through a definitionP?y j x?
?Yi?1:::nP?di j F?d1::: di1, x?
?where d1 .
.
.
dn are again the decisions made in building a parse, and F is a function that groups historiesinto equivalence classes.
Note that x is added to the domain of F (the context on which decisions areconditioned).
See Ratnaparkhi (1997) for one example of a method using this approach.Computational Linguistics Volume 31, Number 129Figure 1 illustrates this process.
Each of the above decisions has an associatedprobability conditioned on the left-hand side of the rule (VP(saw)) and other infor-mation in some cases.History-based approaches lead to models in which the log-probability of a parsetree can be written as a linear sum of parameters ak multiplied by features hk.
Eachfeature hk?x, y?
is the count of a different ??event??
or fragment within the tree.
As anexample, consider a PCFG with rules bAkYbk?
for 1km.
If hk?x, y?
is the numberof times bAkYbk?
is seen in the tree, and ak ?
log P?bkjAk?
is the parameter associatedwith that rule, thenlog P?x, y?
?Xmk?1akhk?x, y?All models considered in this article take this form, although in the boosting modelsthe score for a parse is not a log-probability.
The features hk define an m-dimensionalvector of counts which represent the tree.
The parameters ak represent the influence ofeach feature on the score of a tree.A drawback of history-based models is that the choice of derivation has aprofound influence on the parameterization of the model.
(Similar observations havebeen made in the related cases of belief networks [Pearl 1988], and language modelsfor speech recognition [Rosenfeld 1997].)
When designing a model, it would bedesirable to have a framework in which features can be easily added to the model.Unfortunately, with history-based models adding new features often requires amodification of the underlying derivations in the model.
Modifying the derivation toinclude a new feature type can be a laborious task.
In an ideal situation we would beable to encode arbitrary features hk, without having to worry about formulating aderivation that included these features.To take a concrete example, consider part-of-speech tagging using a hiddenMarkov model (HMM).
We might have the intuition that almost every sentence has atleast one verb and therefore that sequences including at least one verb should haveincreased scores under the model.
Encoding this constraint in a compact way in anHMM takes some ingenuity.
The obvious approach?to add to each state theinformation about whether or not a verb has been generated in the history?doublesFigure 1The sequence of decisions involved in generating the right-hand side of a lexical rule.Collins and Koo Discriminative Reranking for NLPthe number of states (and parameters) in the model.
In contrast, it would be trivial toimplement a feature hk?x, y?
which is 1 if y contains a verb, 0 otherwise.3.
Logistic Regression and BoostingWe now turn to machine-learning methods for the ranking task.
In this section wereview two methods for binary classification problems: logistic regression (ormaximum-entropy) models and boosting.
These methods form the basis for thereranking approaches described in later sections of the article.
Maximum-entropymodels are a very popular method within the computational linguistics community;see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article whichintroduces the models and motivates them.
Boosting approaches to classification havereceived considerable attention in the machine-learning community since the intro-duction of AdaBoost by Freund and Schapire (1997).Boosting algorithms, and in particular the relationship between boostingalgorithms and maximum-entropy models, are perhaps not familiar topics in theNLP literature.
However there has recently been much work drawing connectionsbetween the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffyand Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001;Collins, Schapire, and Singer 2002); in this section we review this work.
Much of thiswork has focused on binary classification problems, and this section is also restrictedto problems of this type.
Later in the article we show how several of the ideas can becarried across to reranking problems.3.1 Binary Classification ProblemsThe general setup for binary classification problems is as follows:The ?
?input domain??
(set of possible inputs) is X .The ?
?output domain??
(set of possible labels) is simply a set of twolabels, Y = {1, +1}.3The training set is an array of n labeled examples,b?x1, y1?, ?x2; y2?, : : : , ?xn, yn?
?, where each xi ZX , yi ZY.Input examples are represented through m ??features,??
which arefunctions hk : XY < for k = 1, .
.
.
, m. It is also sometimes convenientto think of an example x as being represented by an m-dimensional?
?feature vector??
7?x?
?
bh1?x?, h2?x?, : : : , hm?x?
?.Finally, there is a parameter vector, a ?
ba1, : : : ,am?,where each ak Z <, hence a?
is an m-dimensional real-valued vector.We show that both logistic regression and boosting implement a linear, or hyperplane,classifier.
This means that given an input example x and parameter values a?, theoutput from the classifier issign ?F?x, a???
?1?303 It turns out to be convenient to define Y = {1, +1} rather than Y = {0, +1}, for example.Computational Linguistics Volume 31, Number 131whereF?x, a??
?Xmk?1akhk?x?
?
a?
I 7?x?
?2?Here a?
I 7?x?
is the inner or dot product between the vectors a?
and 7?x?, and sign(z) =1 if z  0, sign(z) = 1 otherwise.
Geometrically, the examples x are represented asvectors 7(x) in some m-dimensional vector space, and the parameters a?
define ahyperplane which passes through the origin4 of the space and has a?
as its normal.Points lying on one side of this hyperplane are classified as +1; points on the other sideare classified as 1.
The central question in learning is how to set the parameters a?,given the training examples b?x1, y1?, ?x2, y2?, : : : ,?xn, yn??.
Logistic regression andboosting involve different algorithms and criteria for training the parameters a?, butrecent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy andHelmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins,Schapire, and Singer 2002) has shown that the methods have strong similarities.
Thenext section describes parameter estimation methods.3.2 Loss Functions for Logistic Regression and BoostingA central idea in both logistic regression and boosting is that of a loss function, whichdrives the parameter estimation methods of the two approaches.
This section describesloss functions for binary classification.
Later in the article, we introduce loss functionsfor reranking tasks which are closely related to the loss functions for classification tasks.First, consider a logistic regression model.
The parameters of the model a?
are usedto define a conditional probabilityP?y j x, a??
?
eyF?x, a?
?1 ?
eyF?x, a??
?3?where F?x, a??
is as defined in equation (2).
Some form of maximum-likelihoodestimation is often used for parameter estimation.
The parameters are chosen tomaximize the log-likelihood of the training set; equivalently: we talk (to emphasize thesimilarities to the boosting approach) about minimizing the negative log-likelihood.The negative log-likelihood, LogLoss(a?
), is defined asLogLoss ?a??
?
Xni?1log P?yi j xi, a???
Xni?1logeyiF?xi, a?
?1 ?
eyiF?xi, a?
? ?Xni?1log 1 ?
eyiF?xi, a?
? ?4?There are many methods in the literature for minimizing LogLoss(a?)
with respect toa?, for example, generalized or improved iterative scaling (Berger, Della Pietra, and4 It might seem to be a restriction to have the hyperplane passing through the origin of the space.
Howeverif a constant ??bias??
feature hm?1?x?
?
1 for all x is added to the representation, a hyperplane passingthrough the origin in this new space is equivalent to a hyperplane in general position in the originalm-dimensional space.Collins and Koo Discriminative Reranking for NLPDella Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradientmethods (Malouf 2002).
In the next section we describe feature selection methods, asdescribed in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra,and Lafferty (1997).Once the parameters a?
are estimated on training examples, the output for anexample x is the most likely label under the model,arg maxyZYP?y j x, a??
?
arg maxyZf1,?1gyF?x, a??
?
sign?F?x, a???
?5?where as before, sign ?z?
?
1 if z  0, sign ?z?
?
1 otherwise.
Thus we see that thelogistic regression model implements a hyperplane classifier.In boosting, a different loss function is used, namely, ExpLoss(a?
), which is definedasExpLoss?a??
?Xni?1eyiF?xi, a??
?6?This loss function is minimized using a feature selection method, which we describe inthe next section.There are strong similarities between LogLoss (equation (4)) and ExpLoss(equation (6)).
In making connections between the two functions, it is useful toconsider a third function of the parameters and training examples,Error?a??
?Xni?1gyiF?xi, a??0?
?7?where gp?
is one if p is true, zero otherwise.
Error(a?)
is the number of incorrectlyclassified training examples under parameter values a?.Finally, it will be useful to define the margin on the ith training example, givenparameter values a?, asMi?a??
?
yiF?xi, a??
?8?With these definitions, the three loss functions can be written in the following form:LogLoss?a??
?Xni?1f ?Mi?a??
?, where f ?z?
?
log?1 ?
ez?ExpLoss?a??
?Xni?1f ?Mi?a??
?, where f ?z?
?
ezError?a??
?Xni?1f ?Mi?a??
?, where f ?z?
?
gz0?The three loss functions differ only in their choice of an underlying ??potentialfunction??
of the margins, f(z).
This function is f(z) = log (1 + ez), f(z) = ez, or32Computational Linguistics Volume 31, Number 133f ?z?
?
gz0?
for LogLoss, ExpLoss, and Error, respectively.
The f(z) functions penalizenonpositive margins on training examples.
The simplest function, f ?z?
?
gz0?, givesa cost of one if a margin is negative (an error is made), zero otherwise.
ExpLoss andLogLoss involve definitions for f?z?
which quickly tend to zero as z Y V but heavilypenalize increasingly negative margins.Figure 2 shows plots for the three definitions of f ?z?.
The functions f ?z?
?
ez andf ?z?
?
log ?1 ?
ez?
are both upper bounds on the error function, so that mini-mizing either LogLoss or ExpLoss can be seen as minimizing an upper bound onthe number of training errors.
(Note that minimizing Error(a?)
itself is known to beat least NP-hard if no parameter settings can achieve zero errors on the trainingset; see, for example, Hoffgen, van Horn, and Simon [1995].)
As zYV, the func-tions f ?z?
?
ez and f ?z?
?
log?1 ?
ez?
become increasingly similar, because log?1 ?
ez?Y ez as ez Y 0.
For negative z, the two functions behave quite differently.f ?z?
?
ez shows an exponentially growing cost function as zY V. In contrast, aszYV it can be seen that log?1 ?
ez?Y log?ez?
?
z, so this function showsasymptotically linear growth for negative z.
As a final remark, note that both f ?z?
?ez and f ?z?
?
log?1 ?
ez?
are convex in z, with the result that LogLoss?a??
andExpLoss?a??
are convex in the parameters a?.
This means that there are no problemswith local minima when optimizing these two loss functions.3.3 Feature Selection MethodsIn this article we concentrate on feature selection methods: algorithms which aim tomake progress in minimizing the loss functions LogLoss?a??
and ExpLoss?a??
whileusing a small number of features (equivalently, ensuring that most parameter values inFigure 2Potential functions underlying ExpLoss, LogLoss, and Error.
The graph labeled ExpLoss is a plotof f ?z?
?
ez for z ?
?1:5 : : :1:5; LogLoss shows a similar plot for f ?z?
?
log?1 ?
ez?
; Error is aplot of f ?z?
?
gz0?.Collins and Koo Discriminative Reranking for NLPa?
are zero).
Roughly speaking, the motivation for using a small number of features isthe hope that this will prevent overfitting in the models.Feature selection methods have been proposed in the maximum-entropy literatureby several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, andDella Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, andWard 1997, 1998; McCallum 2003; Zhou et al 2003; Riezler and Vasserman 2004).
Themost basic approach?for example see Ratnaparkhi, Roukos, and Ward (1994) andBerger, Della Pietra, and Della Pietra (1996)?involves selection of a single feature ateach iteration, followed by an update to the entire model, as follows:Step 1: Throughout the algorithm, maintain a set of active features.
Initialize this set tobe empty.Step 2: Choose a feature from outside of the set of active features which has the largestestimated impact in terms of reducing the loss function LogLoss, and add this to theactive feature set.Step 3: Minimize LogLoss?a??
with respect to the set of active features; that is, allowonly the active features to take nonzero parameter values when minimizing LogLoss.Return to Step 2.Methods in the boosting literature (see, for example, Schapire and Singer [1999]) canbe considered to be feature selection methods of the following form:Step 1: Start with all parameter values set to zero.Step 2: Choose a feature which has largest estimated impact in terms of reducing theloss function ExpLoss.Step 3: Update the parameter for the feature chosen at Step 2 in such a way as tominimize ExpLoss?a??
with respect to this one parameter.
All other parameter valuesare left fixed.
Return to Step 2.The difference with this latter ??boosting??
approach is that in Step 3, only oneparameter value is adjusted, namely, the parameter corresponding to the newly chosenfeature.
Note that in this framework, the same feature may be chosen at more than oneiteration.5 The maximum-entropy feature selection method can be quite inefficient,as the entire model is updated at each step.
For example, Ratnaparkhi (1998) quotestimes of around 30 hours for 500 rounds of feature selection on a prepositional-phrase attachment task.
These experiments were performed in 1998, when pro-cessors were no doubt considerably slower than those available today.
However,the PP attachment task is much smaller than the parsing task that we are address-ing: Our task involves around 1,000,000 examples, with perhaps a few hundred featuresper example, and 100,000 rounds of feature selection; this compares to 20,000 exam-ples, 16 features per example, and 500 rounds of feature selection for the PP attach-ment task in Ratnaparkhi (1998).
As an estimate, assuming that computationalcomplexity scales linearly in these factors,6 our task is 1,000,00020,000  32016 100,000500 ?
200,000345 That is, the feature may be repeatedly updated, although the same feature will never be chosenin consecutive iterations, because after an update the model is minimized with respect to theselected feature.6 We believe this is a realistic assumption, as each round of feature selection takes O?nf ?
time, where nis the number of training examples, and f is the number of active features on each example.Computational Linguistics Volume 31, Number 135as large as the PP attachment task.
These figures suggest that the maximum-entropyfeature selection approach may be infeasible for large-scale tasks such as the one in thisarticle.The fact that the boosting approach does not update the entire model at eachround of feature selection may be a disadvantage in terms of the number of features orthe test data accuracy of the final model.
There is reason for concern that Step 2 will atsome iterations mistakenly choose features which are apparently useful in reducingthe loss function, but which would have little utility if the entire model had beenoptimized at the previous iteration of Step 3.
However, previous empirical results forboosting have shown that it is a highly effective learning method, suggesting that thisis not in fact a problem for the approach.
Given the previous strong results for theboosting approach, and for reasons of computational efficiency, we pursue theboosting approach to feature selection in this article.3.4 Statistical Justification for the MethodsMinimization of LogLoss is most often justified as a parametric, maximum-likelihood(ML) approach to estimation.
Thus this approach benefits from the usual guaranteesfor ML estimation: If the distribution generating examples is within the class ofdistributions specified by the log-linear form, then in the limit as the sample size goesto infinity, the model will be optimal in the sense of convergence to the true underlyingdistribution generating examples.
As far as we are aware, behavior of the models forfinite sample sizes is less well understood.
In particular, while feature selectionmethods have often been proposed for maximum-entropy models, little theoreticaljustification (in terms of guarantees about generalization) has been given for them.
Itseems intuitive that a model with a smaller number of parameters will require fewersamples for convergence, but this is not necessarily the case, and at present thisintuition lacks a theoretical basis.
Feature selection methods can probably bemotivated either from a Bayesian perspective (through a prior favoring models witha smaller number of nonzero parameters) or from a frequentist/goodness-of-fitperspective (models with fewer parameters are less likely to fit the data by chance), butthis requires additional research.The statistical justification for boosting approaches is quite different.
Boostingalgorithms were originally developed within the PAC framework (Valiant 1984) formachine learning, specifically to address questions regarding the equivalence of weakand strong learning.
Freund and Schapire (1997) originally introduced AdaBoost andgave a first set of statistical guarantees for the algorithm.
Schapire et al (1998) gave asecond set of guarantees based on the analysis of margins on training examples.
Bothpapers assume that a fixed distribution D(x, y) is generating both training and testexamples and that the goal is to find a hypothesis with a small number of expectederrors with respect to this distribution.
The form of the distribution is not assumed tobe known, and in this sense the guarantees are nonparametric, or ?
?distribution free.?
?Freund and Schapire (1997) show that if the weak learning assumption holds (i.e.,roughly speaking, a feature with error rate better than chance can be found for anydistribution over the sample space X  {1, +1}), then the training error for theExpLoss method decreases rapidly enough for there to be good generalization to testexamples.
Schapire et al (1998) show that under the same assumption, minimization ofExpLoss using the feature selection method ensures that the distribution of margins ontraining data develops in such a way that good generalization performance on testexamples is guaranteed.Collins and Koo Discriminative Reranking for NLP3.5 Boosting with Complex Feature SpacesThus far in this article we have presented boosting as a feature selection approach.
Inthis section, we note that there is an alternative view of boosting in which it isdescribed as a method for combining multiple models, for example, as a method forforming a linear combination of decision trees.
We consider only the simpler, featureselection view of boosting in this article.
This section is included for completeness andbecause the more general view of boosting may be relevant to future work on boostingapproaches for parse reranking (note, however, that the discussion in this section is notessential to the rest of the article, so the reader may safely skip this section if she or hewishes to do so).In feature selection approaches, as described in this article, the set of possiblefeatures hk?x?
for k = 1, .
.
.
, m is taken to be a fixed set of relatively simple functions.
Inparticular, we have assumed that m is relatively small (for example, small enough foralgorithms that require O(m) time or space to be feasible).
More generally, however,boosting can be applied in more complex settings.
For example, a common use ofboosting is to form a linear combination of decision trees.
In this case each example x isrepresented as a number of attribute-value pairs, and each ??feature??
hk(x) is a completedecision tree built on predicates over the attribute values in x.
In this case the numberof features m is huge: There are as many features as there are decision trees over thegiven set of attributes, thus m grows exponentially quickly with the number ofattributes that are used to represent an example x.
Boosting may even be applied insituations in which the number of features is infinite.
For example, it may be used toform a linear combination of neural networks.
In this case each feature hk(x)corresponds to a different parameter setting within the (infinite) set of possibleparameter settings for the neural network.In more complex settings such as boosting of decision trees or neural networks, itis generally not feasible to perform an exhaustive search (with O(m) time complexity)for the feature which has the greatest impact on the exponential7 loss function.
Instead,an approximate search is performed.
In boosting approaches, this approximate searchis achieved through a protocol in which at each round of boosting, a ??distribution?
?over the training examples is maintained.
The distribution can be interpreted asassigning an importance weight to each training example, most importantly givinghigher weight to examples which are incorrectly classified.
At each round of boostingthe distribution is passed to an algorithm such as a decision tree or neural networklearning method, which attempts to return a feature (a decision tree, or a neuralnetwork parameter setting) which has a relatively low error rate with respect to thedistribution.
The feature that is returned is then incorporated into the linearcombination of features.
The algorithm which generates a classifier given adistribution over the examples (for example, the decision tree induction method) isusually referred to as ?
?the weak learner.??
The weak learner generally uses anapproximate (for example, greedy) method to find a function with a low error ratewith respect to the distribution.
Freund and Schapire (1997) show that provided that ateach round of boosting the weak learner returns a feature with greater than (50 + &) %accuracy for some fixed &, the number of training errors falls exponentially quicklywith the number of rounds of boosting.
This fast drop in training errors translates tostatistical bounds on generalization performance (Freund and Schapire 1997).367 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedmanet al (2000) and Duffy and Helmbold (1999).Computational Linguistics Volume 31, Number 137Under this view of boosting, the feature selection methods in this article are aparticularly simple case in which the weak learner can afford to exhaustively searchthrough the space of possible features.
Future work on reranking approaches mightconsider other approaches?such as boosting of decision trees?which can effectivelyconsider more complex features.4.
Reranking ApproachesThis section describes how the ideas from classification problems can be extended toreranking tasks.
A baseline statistical parser is used to generate N-best output both forits training set and for test data sentences.
Each candidate parse for a sentence isrepresented as a feature vector which includes the log-likelihood under the baselinemodel, as well as a large number of additional features.
The additional features can inprinciple be any predicates over sentence/tree pairs.
Evidence from the initial log-likelihood and the additional features is combined using a linear model.
Parameterestimation becomes a problem of learning how to combine these different sources ofinformation.
The boosting algorithm we use is related to the generalization of boostingmethods to ranking problems in Freund et al (1998); we also introduce an approachrelated to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994),Papineni, Roukos, and Ward (1997, 1998), Johnson et al (1999), Riezler et al (2002), andOch and Ney (2002).Section 4.1 gives a formal definition of the reranking problem.
Section 4.2introduces loss functions for reranking that are analogous to the LogLoss and ExpLossfunctions in section 3.2.
Section 4.3 describes a general approach to feature selectionmethods with these loss functions.
Section 4.4 describes a first algorithm for theexponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithmfor the case of ExpLoss.
Finally, section 4.6 describes issues in feature selectionalgorithms for the LogLoss function.4.1 Problem DefinitionWe use the following notation in the rest of this article:si is the ith sentence in the training set.
There are n sentences in trainingdata, so that 1in.xi, j is the jth parse of the ith sentence.
There are ni parses for the ithsentence, so that 1in and 1jni.
Each xi, j contains both thetree and the underlying sentence (i.e., each xi, j is a pair bsi, ti, j?,where si is the ith sentence in the training data, and ti, j is the jth treefor this sentence).
We assume that the parses are distinct, that is, thatxi, j m xi, j?
for j m j?.Score?xi, j?
is the ??score??
for parse xi, j, a measure of the similarity ofxi, j to the gold-standard parse.
For example, Score?xi, j?
might be theF-measure accuracy of parse xi, j compared to the gold-standard parsefor si.Q?xi, j) is the probability that the base parsing model assigns to parse xi, j.L?xi, j?
?
log Q?xi, j?
is the log-probability.Collins and Koo Discriminative Reranking for NLPWithout loss of generality, we assume xi,1 to be the highest-scoringparse for the ith sentence.8 More precisely, for all i, 2jni,Score?xi,1?
> Score?xi, j?.
Note that xi,1 may not be identical to thegold-standard parse; in some cases the parser may fail to proposethe correct parse anywhere in its list of candidates.9Thus our training data consist of a set of parses, fxi, j : i ?
1, : : : , n, j ?
1, : : : , nig,together with scores Score?xi, j?
and log-probabilities L?xi, j?.We represent candidate parse trees through m features, hk for k ?
1, : : : , m: Each hkis an indicator function, for example,hk?x?
?
1 if x contains the rule bS Y NP VP?0 otherwiseWe show that the restriction to binary-valued features is important for the simplicityand efficiency of the algorithms.10 We also assume a vector of m + 1 parameters, a?
={a0, a1, .
.
.
, am}.
Each ai can take any value in the reals.
The ranking function for aparse tree x implied by a parameter vector a?
is defined asF?x, a??
?
a0L?x?
?Xmk?1akhk?x?Given a new test sentence s, with parses xj for j = 1, : : : , N, the output of the model isthe highest-scoring tree under the ranking functionarg maxxZfx1 : : : xNgF?x, a?
?Thus F?x, a??
can be interpreted as a measure of how plausible a parse x is, with higherscores meaning that x is more plausible.
Competing parses for the same sentence areranked in order of plausibility by this function.
We can recover the base rankingfunction?the log-likelihood L?x?
?by setting a0 to a positive constant and setting allother parameter values to be zero.
Our intention is to use the training examples to pickparameter values which improve upon this initial ranking.We now discuss how to set these parameters.
First we discuss loss functionsLoss?a??
which can be used to drive the training process.
We then go on to describefeature selection methods for the different loss functions.388 In the event that multiple parses get the (same) highest score, the parse with the highest value of log-likelihood L under the baseline model is taken as xi, 1.
In the event that two parses have the same scoreand the same log-likelihood?which occurred rarely if ever in our experiments?we make a randomchoice between the two parses.9 This is not necessarily a significant issue if an application using the output from the parser is sensitive toimprovements in evaluation measures such as precision and recall that give credit for partial matchesbetween the parser?s output and the correct parse.
In this case, it is important only that the precision/recall for xi, 1 is significantly higher than that of the baseline parser, that is, that there is some ?
?head room?
?for the reranking module in terms of precision and recall.10 In particular, this restriction allows closed-form parameter updates for the models based on ExpLoss thatwe consider.
Note that features tracking the counts of different rules can be simulated through severalfeatures which take value one if a rule is seen  1 time,  2 times  3 times, and so on.Computational Linguistics Volume 31, Number 1394.2 Loss Functions for Ranking Problems4.2.1 Ranking Errors and Margins.
The loss functions we consider are all related tothe number of ranking errors a function F makes on the training set.
The ranking errorrate is the number of times a lower-scoring parse is (incorrectly) ranked above the bestparse:Error ?a??
?XiXnij?2gF?xi,1, a?
?F?xi,j, a???
?XiXnij?2gF?xi,1, a??
 F?xi,j, a?
?0?where again, gp?
is one if p is true, zero otherwise.
In the ranking problem we define themargin for each example xi,j such that i = 1, : : : , n, j = 2, : : : , ni, asMij?a??
?
F?xi,1, a??
 F?xi,j, a?
?Thus Mij?a??
is the difference in ranking score between the correct parse of a sentenceand a competing parse xi,j.
It follows thatError?a??
?XiXnij?2gMij?a?
?0?The ranking error is zero if all margins are positive.
The loss functions we discuss allturn out to be direct functions of the margins on training examples.4.2.2 Log-Likelihood.
The first loss function is that suggested by Markov randomfields.
As suggested by Ratnaparkhi, Roukos, and Ward (1994) and Johnson et al(1999), the conditional probability of xi,q being the correct parse for the ith sentence isdefined asP?xi,q j si, a??
?eF?xi,q, a?
?Pnij?1eF?xi,j, a?
?Given a new test sentence s, with parses xj for j = 1, : : : , N, the most likely tree isarg maxxjeF?xj,a??PNq?1eF?xq,a???
arg maxxjF?xj, a?
?Hence once the parameters are trained, the ranking function is used to order candidatetrees for test examples.The log-likelihood of the training data isXilog P?xi,1 j si, a??
?XilogeF?xi,1, a?
?Pj?1ni eF?xi,j, a?
?Under maximum-likelihood estimation, the parameters a?
would be set to maxi-mize the log-likelihood.
Equivalently, we again talk about minimizing the negativeCollins and Koo Discriminative Reranking for NLPlog-likelihood.
Some manipulation shows that the negative log-likelihood is a functionof the margins on training data:LogLoss ?a??
?Xilog eF?xi,1, a?
?Pj?1ni eF?xi,j, a??
?Xilog 1Pj?1ni e?F?xi,1, a?
?F?xi,j, a???
?Xilog?1 ?Xnij?2e?F?xi,1, a?
?F?xi,j, a????
?Xilog?1 ?Xnij?2eMi,j?a???
?9?Note the similarity of equation (9) to the LogLoss function for classification in equa-tion (4).4.2.3 Exponential Loss.
The next loss function is based on the boosting methoddescribed in Schapire and Singer (1999).
It is a special case of the general rankingmethods described in Freund et al (1998), with the ranking ??feedback??
being a simplebinary distinction between the highest-scoring parse and the other parses.
Again, theloss function is a function of the margins on training data:ExpLoss?a??
?XiXnij?2e?F?xi,1, a?
?F?xi,j, a???
?XiXnij?2eMi,j?a??
?10?Note the similarity of equation (10) to the ExpLoss function for classificationin equation (6).
It can be shown that ExpLoss?a??
 Error?a?
?, so that minimizingExpLoss?a??
is closely related to minimizing the number of ranking errors.11 Thisfollows from the fact that for any x, ex  gx < 0?, and therefore thatXiXnij?2eMi,j?a??
XiXnij?2gMi,j?a?
?0?We generalize the ExpLoss function slightly, by allowing a weight for each examplexi,j, for i = 1, .
.
.
, n, j = 2, .
.
.
, ni.
We use Si,j to refer to this weight.
In particular, in someexperiments in this article, we use the following definition:Si,j ?
Score?xi,1?
 Score?xi,j?
?11?4011 Note that LogLoss is not a direct upper bound on the number of ranking errors, although it can be shownthat it is a (relatively loose) upper bound on the number of times the correct parse is not the highest-ranked parse on the model.
The latter observation follows from the property that the correct parse mustbe highest ranked if its probability is greater than 0.5.Computational Linguistics Volume 31, Number 141where, as defined in section 4.1, Score?xi, j?
is some measure of the ??goodness??
of aparse, such as the F-measure (see section 5 for the exact definition of Score used in ourexperiments).
The definition for ExpLoss is modified to beExpLoss?a??
?XiXnij?2Si,jeMi,j?a?
?This definition now takes into account the importance, Si,j, of each example.
It is anupper bound on the following quantity:XiXnij?2Si,jgMi,j?a?
?0?which is the number of errors weighted by the factors Si,j.
The original definition ofExpLoss in equation (10) can be recovered by setting Si,j = 1 for all i, j (i.e., by givingequal weight to all examples).
In our experiments we found that a definition of Si,j suchas that in equation (11) gave improved performance on development data, presumablybecause it takes into account the relative cost of different ranking errors in training-data examples.4.3 A General Approach to Feature SelectionAt this point we have definitions for ExpLoss and LogLoss which are analogous to thedefinitions in section 3.2 for binary classification tasks.
Section 3.3 introduced the ideaof feature selection methods; the current section gives a more concrete description ofthe methods used in our experiments.The goal of feature selection methods is to find a small subset of the features thatcontribute most to reducing the loss function.
The methods we consider are greedy, ateach iteration picking the feature hk with additive weight d which has the most impacton the loss function.
In general, a separate set of instances is used in cross-validation tochoose the stopping point, that is, to decide on the number of features in the model.At this point we introduce some notation concerning feature selection methods.We define Upd?a?, k, d?
to be an updated parameter vector, with the same parametervalues as a?
with the exception of ak, which is incremented by d:Upd?a?, k, d?
?
fa0,a1, : : : ,ak ?
d, : : : ,amgThe d parameter can potentially take any value in the reals.
The loss for the updatedmodel is Loss?Upd?a?, k, d??.
Assuming we greedily pick a single feature with someweight to update the model, and given that the current parameter settings are a?, theoptimal feature/weight pair ?k, d?
is?k, d?
?
arg mink, dLoss?Upd?a?, k, d?
?Collins and Koo Discriminative Reranking for NLPThe feature selection algorithms we consider take the following form (a?t is theparameter vector at the tth iteration):Step 1: Initialize a?0 to some value.
(This will generally involve values of zero fora1, .
.
.
, am and a nonzero value for a0, for example, a?0 = {1, 0, 0, .
.
.}.
)Step 2: For t = 1 to N (the number of iterations N will be chosen by cross-validation)a: Find ?k, d?
= arg mink,d Loss?Upd ?a?t1, k, d?
?b: Set a?t = Upd?a?t1, k, d?Note that this is essentially the idea behind the ??boosting??
approach to feature se-lection introduced in section 3.3.
In contrast, the feature selection method of Berger,Della Pietra, and Della Pietra (1996), also described in section 3.3, would involveupdating parameter values for all selected features at step 2b.The main computation for both loss functions involves searching for the optimalfeature/weight pair ?k, d?.
In both cases we take a two-step approach to solving thisproblem.
In the first step the optimal update for each feature hk is calculated.
Wedefine BestWt?k, a??
as the optimal update for the kth feature (it must be calculated forall features k = 1, .
.
.
, m):BestWt?k, a??
?
arg mindLoss?Upd?a?, k, d?
?The next step is to calculate the Loss for each feature with its optimal update, whichwe will callBestLoss?k, a??
?
mindLoss?Upd?a?, k, d??
?
Loss?Upd?a?, k, BestWt?k, a???
?BestWt and BestLoss for each feature having been computed, the optimal feature/weight pair can be found:k ?
arg minkBestLoss?k, a?
?, d ?
BestWt?k, a?
?The next sections describe how BestWt and BestLoss can be computed for the two lossfunctions.4.4 Feature Selection for ExpLossAt the first iteration, a0 is set to optimize ExpLoss (recall that L?xi,j?
is the log-likelihood for parse xi,j under the base parsing model):a0 ?
arg minaXiXnij?2Si,je?a?L?xi,1?L?xi,j??
?12?In initial experiments we found that this step was crucial to the performance of themethod (as opposed to simply setting a0 ?
1, for example).
It ensures that the42Computational Linguistics Volume 31, Number 143contribution of the log-likelihood feature is well-calibrated with respect to theexponential loss function.
In our implementation a0 was optimized using simple brute-force search.
All values of a0 between 0.001 and 10 at increments of 0.001 were tested,and the value which minimized the function in equation (12) was chosen.12Feature selection then proceeds to search for values of the remaining param-eters, a1, .
.
.
, am.
(Note that it might be preferable to also allow a0 to be adjusted asfeatures are added; we leave this to future work.)
This requires calculation of theterms BestWt?k, a??
and BestLoss?k, a??
for each feature.
For binary-valued featuresthese values have closed-form solutions, which is computationally very convenient.We now describe the form of these updates.
See appendix A for how the updatescan be derived (the derivation is essentially the same as that in Schapire and Singer[1999]).First, we note that for any feature, ?hk?xi,1?
 hk?xi,j? can take on three values: +1,1, or 0 (this follows from our assumption of binary-valued feature values).
For eachk we define the following sets:A?k ?
f?i,j?
: ?hk?xi,1?
 hk?xi,j? ?
1gAk ?
f?i,j?
: ?hk?xi,1?
 hk?xi,j? ?
1gThus A+k is the set of training examples in which the kth feature is seen in the correctparse but not in the competing parse; Ak is the set in which the kth feature is seen inthe incorrect but not the correct parse.Based on these definitions, we next define W?k and Wk as follows:W?k ?X?i,j?ZA?kSi,jeMi,j?a??
?13?Wk ?X?i,j?ZAksi,jeMi,j?a??
?14?Given these definitions, it can be shown (see appendix A) thatBestWt?k, a??
?
12logW?kWk?15?andBestLoss?k, a??
?
Z ffiffiffiffiffiffiffiffiW?kqffiffiffiffiffiffiffiffiWkq 2?16?12 A more precise approach, for example, binary search, could also be used to solve this optimizationproblem.
We used the methods that searches through a set of fixed values for simplicity, implicitlyassuming that a precision of 0.001 was sufficient for our problem.Collins and Koo Discriminative Reranking for NLPwhere Z ?PiPnij?2 Si,jeMi,j?a??
?
ExpLoss?a??
is a constant (for fixed a?)
which appearsin the BestLoss for all features and therefore does not affect their ranking.As Schapire and Singer (1999) point out, the updates in equation (15) can beproblematic, as they are undefined (infinite) when either W?k or Wk is zero.
FollowingSchapire and Singer (1999), we introduce smoothing through a parameter & and thefollowing new definition of BestWt:BestWt?k, a??
?
12logW?k ?
&ZWk ?
&Z?17?The smoothing parameter & is chosen through optimization on a development set.See Figure 3 for a direct implementation of the feature selection method forExpLoss.
We use an array of valuesGk ?
jffiffiffiffiffiffiffiffiW?kqffiffiffiffiffiffiffiffiWkqjto indicate the gain of each feature (i.e., the impact that choosing this feature will haveon the ExpLoss function).
The features are ranked by this quantity.
It can be seen thatalmost all of the computation involves the calculation of Z and W?k and Wk for eachfeature hk.
Once these values have been computed, the optimal feature and its updatecan be chosen.4.5 A New, More Efficient Algorithm for ExpLossThis section presents a new algorithm which is equivalent to the ExpLoss algo-rithm in Figure 3, but can be vastly more efficient for problems with sparse fea-ture spaces.
In the experimental section of this article we show that it is almost2,700 times more efficient for our task than the algorithm in Figure 3.
The efficiencyof the different algorithms is important in the parsing problem.
The training data weeventually used contained around 36,000 sentences, with an average of 27 parses persentence, giving around 1,000,000 parse trees in total.
There were over 500,000 dif-ferent features.The new algorithm is also applicable, with minor modifications, to boostingapproaches for classification problems in which the representation also involves sparsebinary features (for example, the text classification problems in Schapire and Singer[2000]).
As far as we are aware, the new algorithm has not appeared elsewhere in theboosting literature.Figure 4 shows the improved boosting algorithm.
Inspection of the algorithm inFigure 3 shows that only margins on examples in the sets A?k and Ak are modifiedwhen a feature k is selected.
The feature space in many NLP problems is very sparse(most features only appear on relatively few training examples, or equivalently, mosttraining examples will have only a few nonzero features).
It follows that in many cases,the sets A?k and Ak will be much smaller than the overall size of the training set.Therefore when updating the model from a?
to Upd?a?, k, d?, the values W?k and Wkremain unchanged for many features and do not need to be recalculated.
In fact, only44Computational Linguistics Volume 31, Number 145features which co-occur with k* on some example must be updated.
The algorithm inFigure 4 recalculates the values of A?k and Ak only for those features which co-occurwith the selected feature k*.To achieve this, the algorithm relies on a second pair of indices.
For all i, 2jni,we defineB?i,j ?
fk : ?hk?xi,1?
 hk?xi,j? ?
1 gBi,j ?
fk : ?hk?xi,1?
 hk?xi,j? ?
1g ?18?Figure 3A naive algorithm for the boosting loss function.Collins and Koo Discriminative Reranking for NLP46Figure 4An improved algorithm for the boosting loss function.Computational Linguistics Volume 31, Number 147So Bi,j+ and Bi,j are indices from training examples to features.
With the algorithm inFigure 4, updating the values of W?k and Wk for the features which co-occur with k*involves the following number of steps:C ?X?i,j?ZA?k?jB?i,j j ?
jBi,j j?
?X?i,j?ZAk?jB?i,j j ?
jBi,j j?
?19?In contrast, the naive algorithm requires a pass over the entire training set, whichrequires the following number of steps:T ?Xni?1Xnij?2?jB?i,j j ?
jBi,j j?
?20?The relative efficiency of the two algorithms depends on the value of C/T at eachiteration.
In the worst case, when every feature chosen appears on every trainingexample, then C/T = 1, and the two algorithms essentially have the same running time.However in sparse feature spaces there is reason to believe that C/T will be small formost iterations.
In section 5.4.3 we show that this is the case for our experiments.4.6 Feature Selection for LogLossWe now describe an approach that was implemented for LogLoss.
At the first iteration,a0 is set to one.
Feature selection then searches for values of the remaining parameters,a1, : : : ,am.
We now describe how to calculate the optimal update for a feature k withthe LogLoss function.
First we recap the definition of the probability of a particularparse xi,q given parameter settings a?
:P?xi,q j si, a??
?eF?xi,q,a?
?Pj?1ni eF?xi,j,a?
?Recall that the log-loss isLogLoss?a??
?Xi log P?xi,1 j si, a?
?Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWtdoes not exist.
However, we can define an iterative solution using techniques fromiterative scaling (Della Pietra, Della Pietra, and Lafferty 1997).
We first define h?k, thenumber of times that feature k is seen in the best parse, and p?k?a?
?, the expected numberof times under the model that feature k is seen:h?k ?Xihk?xi,1?, p?k?a??
?XiXnij?1hk?xi,j?P?xi,j j si, a?
?Collins and Koo Discriminative Reranking for NLPIterative scaling then defines the following update d?d?
?
log h?kp?k?a?
?While in general it is not true that d?
?
BestWt?k, a?
?, it can be shown that this updateleads to an improvement in the LogLoss (i.e., that LogLoss?Upd?a?, k, d??
?LogLoss a??
),with equality holding only when a?k is already at the optimal value, in other words,when arg mind LogLoss?Upd?a?, k, d??
= 0.
This suggests the following iterative methodfor finding BestWt?k, a??
:Step 1: Initialization: Set d = 0 and a??
?
a?, calculate h?k.Step 2: Repeat until convergence of d:a: Calculate p?k(a??
).b: d@ d?
log h?kp?k?a???
.c : a??
@Upd?a?, k, d?.Step 3: Return BestWt?k, a??
?
d.Given this method for calculating BestWt?k, a?
?, BestLoss?k, a??
can be calculated asLoss?k, BestWt?k, a???.
Note that this is only one of a number of methods for findingBestWt?k, a??
: Given that this is a one-parameter, convex optimization problem, it is afairly simple task, and there are many methods which could be used.Unfortunately there does not appear to be an efficient algorithm for LogLoss that isanalogous to the ExpLoss algorithm in Figure 4 (at least if the feature selection methodis required to pick the feature with highest impact on the loss function at eachiteration).
A similar observation for LogLoss can be made, in that when the model isupdated with a feature/weight pair ?k, d?, many features will have their values forBestWt and BestLoss unchanged.
Only those features which co-occur with k on someexample will need to have their values of BestWt and BestLoss updated.
However, thisobservation does not lead to an efficient algorithm: Updating these values is muchmore expensive than in the ExpLoss case.
The procedure for finding the optimal valueBestWt?k, a??
must be applied for each feature which co-occurs with the chosen featurek.
For example, the iterative scaling procedure described above must be applied for anumber of features.
For each feature, this will involve recalculation of the distributionfP?xi, 1 j si?, P?xi,2 j si?, : : : , P?xi,ni j si?g for each example i on which the feature occurs.13 Ittakes only one feature that is seen on all training examples for the algorithm to involverecalculation of P?xi,j j si?
for the entire training set.
This contrasts with the simpleupdates in the improved boosting algorithm ?W?k ?
W?k ?
D and Wk ?
Wk ?
D?.
Infact in the parsing experiments, we were forced to give up on the LogLoss featureselection methods because of their inefficiency (see section 6.4 for more discussion aboutefficiency).4813 This is not a failure of iterative scaling alone: Given that in the general case, closed-form solutions forBestWt and BestLoss do not exist, it is hard to imagine a method that computes these values exactlywithout some kind of iterative method which requires repeatedly visiting the examples on which afeature is seen.Computational Linguistics Volume 31, Number 149Note, however, that approximate methods for finding the best feature andupdating its weight may lead to efficient algorithms.
Appendix B gives a sketch of onesuch approach, which is based on results from Collins, Schapire, and Singer (2002).
Wedid not test this method; we leave this to future work.5.
Experimental Evaluation5.1 Generation of Parsing Data SetsWe used the Penn Wall Street Journal treebank (Marcus, Santorini, andMarcinkiewicz 1993) as training and test data.
Sections 2?21 inclusive (around40,000 sentences) were used as training data, section 23 was used as the final test set.Of the 40,000 training sentences, the first 36,000 were used as the main training set.The remaining 4,000 sentences were used as development data and to cross-validatethe number of rounds (features) in the model.
Model 2 of Collins (1999) was used toparse both the training and test data, producing multiple hypotheses for eachsentence.
We achieved this by disabling dynamic programming in the parser andchoosing a relatively narrow beam width of 1,000.
The resulting parser returns allparses that fall within the beam.
The number of such parses varies sentence bysentence.In order to gain a representative set of training data, the 36,000 training sentenceswere parsed in 2,000 sentence chunks, each chunk being parsed with a model trainedon the remaining 34,000 sentences (this prevented the initial model from beingunrealistically ??good??
on the training sentences).
The 4,000 development sentenceswere parsed with a model trained on the 36,000 training sentences.
Section 23 wasparsed with a model trained on all 40,000 sentences.In the experiments we used the following definition for the Score of the parse:Score?xi,j?
?Fmeasure?xi,j?100 Size?xi,j?where F-measure(xi,j) is the F1 score14 of the parse when compared to the gold-standard parse (a value between 0 and 100), and Size(xi,j) is the number of constituentsin the gold-standard parse for the ith sentence.
Hence the Score function is sensitive toboth the accuracy of the parse, and also the number of constituents in the gold-standard parse.5.2 FeaturesThe following types of features were included in the model.
We will use the rule VP YPP VBD NP NP SBAR with head VBD as an example.
Note that the output of ourbaseline parser produces syntactic trees with headword annotations (see Collins[1999]) for a description of the rules used to find headwords).14 Note that in the rare cases in which the baseline parser produces no constituents, the precision isundefined; in these cases we defined the F-measure to be 0.Collins and Koo Discriminative Reranking for NLPRules.
These include all context-free rules in thetree, for example, VP Y PP VBD NP NP SBAR.Bigrams.
These are adjacent pairs of nonterminalsto the left and right of the head.
As shown, theexample rule would contribute the bigrams(Right,VP,NP,NP), (Right,VP,NP,SBAR),(Right,VP,SBAR,STOP) to the right of the headand (Left,VP,PP,STOP) to the left of the head.Grandparent rules.
Same as Rules, but alsoincluding the nonterminal above the rule.Grandparent bigrams.
Same as Bigrams,but also including the nonterminal abovethe bigrams.Lexical bigrams.Same as Bigrams,but with the lexicalheads of the twononterminals alsoincluded.Two-level rules.
Same as Rules, but alsoincluding the entire rule above the rule.Two-level bigrams.
Same as Bigrams, butalso including the entire rule above the rule.Trigrams.
All trigrams within the rule.
Theexample rule would contribute the trigrams(VP, STOP, PP, VBD!
), (VP, PP, VBD!, NP),(VP, VBD!, NP, NP), (VP, NP, NP, SBAR),and (VP,NP, SBAR, STOP) (!
is used to markthe head of the rule).50Computational Linguistics Volume 31, Number 151Head Modifiers.
All head-modifier pairs, withthe grandparent nonterminal also included.An adj flag is also included, which is one ifthe modifier is adjacent to the head, zerootherwise.
As an example, say the nonterminaldominating the example rule is S. Theexample rule would contribute (Left, S, VP,VBD, PP, adj = 1), (Right, S, VP, VBD, NP,adj = 1), (Right, S, VP, VBD, NP, adj = 0),and (Right, S, VP, VBD, SBAR, adj = 0).PPs.
Lexical trigrams involving the headsof arguments of prepositional phrases.The example shown at right wouldcontribute the trigram (NP, NP, PP, NP,president, of, U.S.), in addition to therelation (NP, NP, PP, NP, of, U.S.), whichignores the headword of the constituentbeing modified by the PP.
The threenonterminals (for example, NP, NP, PP)identify the parent of the entire phrase,the nonterminal of the head of the phrase,and the nonterminal label for the PP.Distance head modifiers.
Features involving the distance betweenheadwords.
For example, assume dist is the number of words betweenthe headwords of the VBD and SBAR in the (VP, VBD, SBAR) head-modifierrelation in the above rule.
This relation would then generate features(VP, VBD, SBAR, = dist), and (VP, VBD, SBAR,x) for all distx9 and(VP, VBD, SBAR,  x) for all 1xdist.Further lexicalization.
In order to generate more features, a second passwas made in which all nonterminals were augmented with their lexicalheads when these headwords were closed-class words.
All featuresapart from head modifiers, PPs, and distance head modifiers were thengenerated with these augmented nonterminals.All of these features were initially generated, but only features seen on at leastone parse for at least five different sentences were included in the final model (thiscount cutoff was implemented to keep the number of features down to a tractablenumber).5.3 Applying the Reranking MethodsThe ExpLoss method was trained with several values for the smoothing parameter &:{0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075}.
For each value of &, themethod was run for 100,000 rounds on the training data.
The implementation wassuch that the feature updates for all 100,000 rounds for each training run wererecorded in a file.
This made it simple to test the model on development data for allvalues of N between 0 and 100,000.Collins and Koo Discriminative Reranking for NLPThe different values of & and N were compared on development data through thefollowing criterion:XiScore?zi?
?21?where Score is as defined above, and zi is the output of the model on the ithdevelopment set example.
The &, N values which maximized this quantity were usedto define the final model applied to the test data (section 23 of the treebank).
Theoptimal values were & ?
0.0025 and N ?
90,386, at which point 11,673 features hadnonzero values (note that the feature selection techniques may result in a given featurebeing updated more than once).
The computation took roughly 3?4 hours on amachine with a 1.6 GHz pentium processor and around 2 GB of memory.Table 1 shows results for the method.
The model of Collins (1999) was the basemodel; the ExpLoss model gave a 1.5% absolute improvement over this method.
Themethod gives very similar accuracy to the model of Charniak (2000), which also uses arich set of initial features in addition to Charniak?s (1997) original model.The LogLoss method was too inefficient to run on the full data set.
Instead wemade some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parsetrees) and 52,294 features.15 On an older machine (an order of magnitude or moreslower than the machine used for the final tests) the boosting method took 40 minutesfor 10,000 rounds on this data set.
The LogLoss method took 20 hours to complete3,500 rounds (a factor of about 85 times slower).
This was in spite of various heuristicsthat were implemented in an attempt to speed up LogLoss: for example, selectingmultiple features at each round or recalculating the statistics for only the best Kfeatures for some small K at the previous round of feature selection.
In initialexperiments we found ExpLoss to give similar, perhaps slightly better, accuracy thanLogLoss.5.4 Further ExperimentsThis section describes further experiments investigating various aspects of theboosting algorithm: the effect of the & and N parameters, learning curves, the choiceof the Si,j weights, and efficiency issues.5.4.1 The Effect of the & and N Parameters.
Figure 5 shows the learning curve ondevelopment data for the optimal value of & (0.0025).
The accuracy shown is theperformance relative to the baseline method of using the probability from thegenerative model alone in ranking parses, where the measure in equation (21) is usedto measure performance.
For example, a score of 101.5 indicates a 1.5% increase in thisscore.
The learning curve is initially steep, eventually flattening off, but reaching itspeak value after a large number (90,386) of rounds of feature selection.Table 2 indicates how the peak performance varies with the smoothing parameter&.
Figure 6 shows learning curves for various values of &.
It can be seen that valuesother than & ?
0.0025 can lead to undertraining or overtraining of the model.5215 All features described above except distance head modifiers and further lexicalization were included.Computational Linguistics Volume 31, Number 153Figure 5Learning curve on development data for the optimal value for & (0.0025).
The y-axis is the level ofaccuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.Table 1Results on section 23 of the WSJ Treebank.
??LR??
is labeled recall; ??LP??
is labeled precision;??CBs??
is the average number of crossing brackets per sentence; ?
?0 CBs??
is the percentage ofsentences with 0 crossing brackets; ?
?2 CBs??
is the percentage of sentences with two or morecrossing brackets.
All the results in this table are for models trained and tested on the same data,using the same evaluation metric.
Note that the ExpLoss results are very slightly different fromthe original results published in Collins (2000).
We recently reimplemented the boosting codeand reran the experiments, and minor differences in the code and & values tested ondevelopment data led to minor improvements in the results.Model40 Words (2,245 sentences)LR LP CBs 0 CBs 2 CBsCharniak 1997 87.5% 87.4% 1.00 62.1% 86.1%Collins 1999 88.5% 88.7% 0.92 66.7% 87.1%Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6%ExpLoss 90.2% 90.4% 0.73 71.2% 90.2%Model100 Words (2,416 sentences)LR LP CBs 0 CBs 2 CBsCharniak 1997 86.7% 86.6% 1.20 59.5% 83.2%Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% ?Collins 1999 88.1% 88.3% 1.06 64.0% 85.1%Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7%ExpLoss 89.6% 89.9% 0.86 68.7% 88.3%Collins and Koo Discriminative Reranking for NLP5.4.2 The Effect of the Si,j Weights on Examples.
In section 4.2.3 we introduced theidea of weights Si,j representing the importance of examples.
Thus far, in theexperiments in this article, we have used the definitionSi,j ?
Score?xi,1?
 Score?xi,j?
?22?thereby weighting examples in proportion to their difference in score from the correctparse for the sentence in question.
In this section we compare this approach to adefault definition of Si,j, namely,Si,j ?
1 ?23?Using this definition, we trained the ExpLoss method on the same training set forseveral values of the smoothing parameter & and evaluated the performance ondevelopment data.
Table 3 compares the peak performance achieved under the twodefinitions of Si,j on the development set.
It can be seen that the definition in equation(22) outperforms the simpler method in equation (23).
Figure 7 shows the learningcurves for the optimal values of & for the two methods.
It can be seen that the learningcurve for the definition of Si,j in equation (22) consistently dominates the curve for thesimpler definition.5.4.3 Efficiency Gains.
Section 4.5 introduced an efficient algorithm for optimizingExpLoss.
In this section we explore the empirical gains in efficiency seen on theparsing data sets in this article.We first define the quantity T as follows:T ?XiXnij?2?jB?i,j j ?
jBi,j j?54Table 2Peak performance achieved for various values of &.
?
?Best N??
refers to the number ofrounds at which peak development set accuracy was reached.
?
?Best score??
indicates therelative performance, compared to the baseline method, at the optimal value for N.& Best N Best score0.0001 29,471 101.7430.00025 22,468 101.8490.0005 48,795 101.8450.00075 43,386 101.8090.001 43,975 101.8490.0025 90,386 101.8640.005 66,378 101.8240.0075 80,746 101.722Computational Linguistics Volume 31, Number 155Figure 6Learning curves on development data for various values of &.
In each case the y-axis is thelevel of accuracy (100 is the baseline score), and the x-axis is the number of rounds ofboosting.
The three graphs compare the curve for & = 0.0025 (the optimal value) to (from topto bottom) & = 0.0001, & = 0.0075, and & = 0.001.
The top graph shows that & = 0.0001 leads toundersmoothing (overtraining).
Initially the graph is higher than that for & = 0.0025, but on laterrounds the performance starts to decrease.
The middle graph shows that & = 0.0075 leads tooversmoothing (undertraining).
The graph shows consistently lower performance than that for& = 0.0025.
The bottom graph shows that there is little difference in performance for & = 0.001versus & = 0.0025.Collins and Koo Discriminative Reranking for NLPThis is a measure of the number of updates to the W?k and Wk variables required inmaking a pass over the entire training set.
Thus it is a measure of the amount ofcomputation that the naive algorithm for ExpLoss, presented in Figure 3, requires foreach round of feature selection.Next, say the improved algorithm in Figure 4 selects feature k* on the t th round offeature selection.
Then we define the following quantity:Ct ?X?i,j?&A?k?jB?i,j j ?
jBi,j j?
?X?i,j?&Ak?jB?i,j j ?
jBi,j j?This is a measure of the number of summations required by the improved algorithm inFigure 4 at the t th round of feature selection.56Figure 7Performance versus number of rounds of boosting for Si,j ?
Score?xi,1?
 Score?xi, j?
(curve labeled ??Weighted??)
and Si,j ?
1 (curve labeled ?
?Not Weighted??
).Table 3Peak performance achieved for various values of & for Si, j ?
Score?xi,1?
 Score?xi, j?
(columnlabeled ??weighted??)
and Si,j ?
1 (column labeled ??unweighted??
).& Best score (weighted) Best score (unweighted)0.0001 101.743 101.7440.00025 101.849 101.7540.0005 101.845 101.7780.00075 101.809 101.7620.001 101.849 101.7780.0025 101.864 101.6990.005 101.824 101.6100.0075 101.722 101.604Computational Linguistics Volume 31, Number 157We are now in a position to compare the running times of the two algorithms.
Wedefine the following quantities:Work?n?
?Xnt?1CtT?24?Savings?n?
?
nTPnt?1Ct?25?Savings?a, b?
?
?1 ?
b  a?TPbt?aCt?26?Here, Work?n?
is the computation required for n rounds of feature selection, wherea single unit of computation corresponds to a pass over the entire training set.Savings?n?
tracks the relative efficiency of the two algorithms as a function of thenumber of features, n. For example, if Savings?100?
?
1,200, this signifies that for thefirst 100 rounds of feature selection, the improved algorithm is 1,200 times as efficientas the naive algorithm.
Finally, Savings?a, b?
indicates the relative efficiency betweenrounds a and b, inclusive, of feature selection.
For example, Savings?11, 100?
?
83signifies that between rounds 11 and 100 inclusive of the algorithm, the improvedalgorithm was 83 times as efficient.Figures 8 and 9 show graphs of Work?n?
and Savings?n?
versus n. The savingsfrom the improved algorithm are dramatic.
In 100,000 rounds of feature selection,the improved algorithm requires total computation that is equivalent to a mere 37.1passes over the training set.
This is a saving of a factor of 2,692 over the naive algorithm.Table 4 shows the value of Savings?a,b?
for various values of ?a,b?.
It can beseen that the performance gains are significantly larger in later rounds of featureselection, presumably because in later stages relatively infrequent features are beingselected.
Even so, there are still savings of a factor of almost 50 in the early stages ofthe method.6.
Related Work6.1 History-Based Models with Complex FeaturesCharniak (2000) describes a parser which incorporates additional features into apreviously developed parser, that of Charniak (1997).
The method gives substantialimprovements over the original parser and results which are very close to the resultsof the boosting method we have described in this article (see section 5 for experimentalresults comparing the two methods).
Our features are in many ways similar to those ofCharniak (2000).
The model in Charniak (2000) is quite different, however.
Theadditional features are incorporated using a method inspired by maximum-entropymodels (e.g., the model of Ratnaparkhi [1997]).Ratnaparkhi (1997) describes the use of maximum-entropy techniques ap-plied to parsing.
Log-linear models are used to estimate the conditional probabilitiesP?di j F ?d1, : : : , di1??
in a history-based parser.
As a result the model can take intoaccount quite a rich set of features in the history.Collins and Koo Discriminative Reranking for NLP58Figure 8Work?n??yaxis?
versus n ?xaxis?.Figure 9Savings?n??y-axis?
versus n?x-axis?.Computational Linguistics Volume 31, Number 159Both approaches still rely on decomposing a parse tree into a sequence ofdecisions, and we would argue that the techniques described in this article have moreflexibility in terms of the features that can be included in the model.6.2 Joint Log-Linear ModelsAbney (1997) describes the application of log-linear models to stochastic head-driven phrase structure grammars (HPSGs).
Della Pietra, Della Pietra, and Lafferty(1997) describe feature selection methods for log-linear models, and Rosenfeld(1997) describes application of these methods to language modeling for speechrecognition.
These methods all emphasize models which define a joint proba-bility over the space of all parse trees (or structures in question): For this reasonwe describe these approaches as ?
?Joint log-linear models.??
The probability of a treexi,j isP?xi,j?
?eF?xi,j?PxZZeF?x?
?27?Here Z is the (infinite) set of possible trees, and the denominator cannot becalculated explicitly.
This is a problem for parameter estimation, in which an estimateof the denominator is required, and Monte Carlo methods have been proposed(Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as atechnique for estimation of this value.
Our sense is that these methods can becomputationally expensive.
Notice that the joint likelihood in equation (27) is not adirect function of the margins on training examples, and its relation to errorrate is therefore not so clear as in the discriminative approaches described in thisarticle.6.3 Conditional Log-Linear ModelsRatnaparkhi, Roukos, and Ward (1994), Johnson et al (1999), and Riezler et al (2002)suggest training log-linear models (i.e., the LogLoss function in equation (9)) forparsing problems.
Ratnaparkhi, Roukos, and Ward (1994) use feature selectiontechniques for the task.
Johnson et al (1999) and Riezler et al (2002) do not use afeature selection technique, employing instead an objective function which includes aTable 4Values of Savings (a, b) for various values of a, b.a?b Savings (a, b)1?100,000 2,692.71?10 48.611?100 83.5101?1,000 280.01,001?10,000 1,263.910,001?50,000 2,920.250,001?100,000 4,229.8Collins and Koo Discriminative Reranking for NLPGaussian prior on the parameter values, thereby penalizing parameter values whichbecome too large:a? ?
arg mina ?LogLoss?a??
?
Xk?0 : : :ma2k72k?
?28?Closed-form updates under iterative scaling are not possible with this objectivefunction; instead, optimization algorithms such as gradient descent or conjugategradient methods are used to estimate parameter values.In more recent work, Lafferty, McCallum, and Pereira (2001) describe the use ofconditional Markov random fields (CRFs) for tagging tasks such as named entityrecognition or part-of-speech tagging (hidden Markov models are a common methodapplied to these tasks).
CRFs employ the objective function in equation (28).
A keyinsight of Lafferty, McCallum, and Pereira (2001) is that when features are of asignificantly local nature, the gradient of the function in equation (28) can be calculatedefficiently using dynamic programming, even in cases in which the set of candidatesinvolves all possible tagged sequences and is therefore exponential in size.
See also Shaand Pereira (2003) for more recent work on CRFs.Optimizing a log-linear model with a Gaussian prior (i.e., choosing parametervalues which achieve the global minimum of the objective function in equation (28)) isa plausible alternative to the feature selection approaches described in the currentarticle or to the feature selection methods previously applied to log-linear models.
TheGaussian prior (i.e., thePk a2k=72k penalty) has been found in practice to be veryeffective in combating overfitting of the parameters to the training data (Chen andRosenfeld 1999; Johnson et al 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al2002).
The function in equation (28) can be optimized using variants of gradientdescent, which in practice require tens or at most hundreds of passes over the trainingdata (see, e.g., Sha and Pereira 2003).
Thus log-linear models with a Gaussian prior arelikely to be comparable in terms of efficiency to the feature selection approachdescribed in this article (in the experimental section, we showed that for the parse-reranking task, the efficient boosting algorithm requires computation that is equivalentto around 40 passes over the training data).Note, however, that the two methods will differ considerably in terms of thesparsity of the resulting reranker.
Whereas the feature selection approach leads toaround 11,000 (2%) of the features in our model having nonzero parameter values,log-linear models with Gaussian priors typically have very few nonzero parameters(see, e.g., Riezler and Vasserman 2004).
This may be important in some domains, forexample, those in which there are a very large number of features and this largenumber leads to difficulties in terms of memory requirements or computation time.6.4 Feature Selection MethodsA number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al 2003;Riezler and Vasserman 2004) describe feature selection approaches for log-linearmodels applied to NLP problems.
Earlier work (Berger, Della Pietra, and Della Pietra1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggestedmethods that added a feature at a time to the model and updated all parameters in thecurrent model at each step (for more detail, see section 3.3).60Computational Linguistics Volume 31, Number 161Assuming that selection of a feature takes one pass over the training set and thatfitting a model takes p passes over the training set, these methods require f  ?p + 1?passes over the training set, where f is the number of features selected.
In ourexperiments, f , 10,000.
It is difficult to estimate the value for p, but assuming (veryconservatively) that p = 2, selecting 10,000 features would require 30,000 passes overthe training set.
This is around 1,000 times as much computation as that required forthe efficient boosting algorithm applied to our data, suggesting that the featureselection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998),and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for theparsing task.More recent work (McCallum 2003; Zhou et al 2003; Riezler and Vasserman 2004)has considered methods for speeding up the feature selection methods described inBerger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, DellaPietra, and Lafferty (1997).
McCallum (2003) and Riezler and Vasserman (2004)describe approaches that add k features at each step, where k is some constant greaterthan one.
The running time for these methods is therefore O?
f  ?p ?
1?=k?.
Riezlerand Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimalperformance.
McCallum (2003) uses a value of k = 1,000.
Zhou et al (2003) use adifferent heuristic that avoids having to recompute the gain for every feature at everyiteration.We would argue that the alternative feature selection methods in the currentarticle may be preferable on the grounds of both efficiency and simplicity.
Even withlarge values of k in the approach of McCallum (2003) and Riezler and Vasserman(2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient asthese alternative approaches.
In terms of simplicity, the methods in McCallum (2003)and Riezler and Vasserman (2004) require selection of a number of free parametersgoverning the behavior of the algorithm: the value for k, the value for a regularizerconstant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and theprecision with which the model is optimized at each stage of feature selection(McCallum [2003] describes using ?
?just a few BFGS iterations??
at each stage).
Incontrast, our method requires a single parameter to be chosen (the value for the &smoothing parameter) and makes a single approximation (that only a single feature isupdated at each round of feature selection).
The latter approximation is particularlyimportant, as it leads to the efficient algorithm in Figure 4, which avoids a pass overthe training set at each iteration of feature selection (note that in sparse feature spaces, frounds of feature selection in our approach can take considerably fewer than f passesover the training set, in contrast to other work on feature selection within log-linearmodels).Note that there are other important differences among the approaches.
Both DellaPietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods thatinduce conjunctions of ??base??
features, in a way similar to decision tree learners.
Thusa relatively small number of base features can lead to a very large number of possibleconjoined features.
In future work it might be interesting to consider these kinds ofapproaches for the parsing problem.
Another difference is that both McCallum, andRiezler and Vasserman, describe approaches that use a regularizer in addition tofeature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use aone-norm regularizer.Finally, note that other feature selection methods have been proposed within themachine-learning community: for example, ??filter??
methods, in which featureselection is performed as a preprocessing step before applying a learning method,Collins and Koo Discriminative Reranking for NLPand backward selection methods (Koller and Sahami 1996), in which initially allfeatures are added to the model and features are then incrementally removed from themodel.6.5 Boosting, Perceptron, and Support Vector Machine Approaches for RankingProblemsFreund et al (1998) introduced a formulation of boosting for ranking problems.
Theproblem we have considered is a special case of the problem in Freund et al (1998), inthat we have considered a binary distinction between candidates (i.e., the best parsevs.
other parses), whereas Freund et al consider learning full or partial orderings overcandidates.
The improved algorithm that we introduced in Figure 4 is, however, a newalgorithm that could perhaps be generalized to the full problem of Freund et al (1998);we leave this to future research.Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003)describe experiments on tagging tasks using the ExpLoss function, in contrast to theLogLoss function used in Lafferty, McCallum, and Pereira (2001).
Altun, Hofmann,and Johnson (2003) describe how dynamic programming methods can be used tocalculate gradients of the ExpLoss function even in cases in which the set of candidatesagain includes all possible tagged sequences, a set which grows exponentially in sizewith the length of the sentence being tagged.
Results in Altun, Johnson, and Hofmann(2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impacton accuracy for the tagging task in question.Perceptron-based algorithms, or the voted perceptron approach of Freund andSchapire (1999), are another alternative to boosting and LogLoss methods.
See Collins(2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptronalgorithm.
Collins (2002b) gives convergence proofs for the methods; Collins (2002a)directly compares the boosting and perceptron approaches on a named entity task;and Collins and Duffy (2001, 2002) use a reranking approach with kernels, whichallow representations of parse trees or labeled sequences in very-high-dimensionalspaces.Shen, Sarkar, and Joshi (2003) describe support vector machine approaches toranking problems and apply support vector machines (SVMs) using tree-adjoininggrammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we havedescribed in this article, with good empirical results.See Collins (2004) for a discussion of many of these methods, including anoverview of statistical bounds for the boosting, perceptron, and SVM methods, as wellas a discussion of the computational issues involved in the different algorithms.7.
ConclusionsThis article has introduced a new algorithm, based on boosting approaches in machinelearning, for ranking problems in natural language processing.
The approach gives a13% relative reduction in error on parsing Wall Street Journal data.
While in this articlethe experimental focus has been on parsing, many other problems in natural languageprocessing or speech recognition can also be framed as reranking problems, so themethods described should be quite broadly applicable.
The boosting approach toranking has been applied to named entity segmentation (Collins 2002a) and naturallanguage generation (Walker, Rambow, and Rogati 2001).
The key characteristics ofthe approach are the use of global features and of a training criterion (optimization62Computational Linguistics Volume 31, Number 163problem) that is discriminative and closely related to the task at hand (i.e., parseaccuracy).In addition, the article introduced a new algorithm for the boosting approachwhich takes advantage of the sparse nature of the feature space in the parsing data thatwe use.
Other NLP tasks are likely to have similar characteristics in terms of sparsity.Experiments show an efficiency gain of a factor of over 2,600 on the parsing data forthe new algorithm over the obvious implementation of the boosting approach.
Wewould argue that the improved boosting algorithm is a natural alternative tomaximum-entropy or (conditional) log-linear models.
The article has drawn connec-tions between boosting and maximum-entropy models in terms of the optimizationproblems that they involve, the algorithms used, their relative efficiency, and theirperformance in empirical tests.Appendix A: Derivation of Updates for ExpLossThis appendix gives a derivation of the optimal updates for ExpLoss.
The derivation isvery close to that in Schapire and Singer (1999).
Recall that for parameter values a?, weneed to compute BestWt?k, a??
and BestLoss?k, a??
for k ?
1, .
.
.
, m, whereBestWt?k, a??
?
arg mindExpLoss?Upd?a?, k, d?
?andBestLoss?k, a??
?
ExpLoss?Upd?a?, k, BestWt?k, a???
?The first thing to note is that an update in parameters from a?
to Upd?a?, k, d?
?results in a simple additive update to the ranking function F:F?xi,j, Upd?a?, k, d??
?
F?xi,j,a?
?
dhk?xi,j?It follows that the margin on example ?i, j?
also has a simple update:Mi,j?Upd?a?, k, d??
?
F?xi,1, Upd?a?, k, d??
 F?xi,j, Upd?a?, k, d???
F?xi,1, a??
 F?xi,j, a??
?
d?hk?xi,1?
 hk?xi,j??
Mi,j?a??
?
d?hk?xi,1?
 hk?xi,j?The updated ExpLoss function can then be written asExpLoss ?Upd?a?,k, d??
?XiXnij?2Si,jeMi,j?Upd?a?, k, d??
?XiXnij?2Si,jeMi,j?a?d?hk?xi,1?hk?xi,j?Next, we note that ?hk?xi,1?
 hk?xi,j? can take on three values: +1, 1, or 0.
We split thetraining sample into three sets depending on this value:A?k ?
f?i,j?
: ?hk?xi,1?
 hk?xi,j? ?
1gCollins and Koo Discriminative Reranking for NLPAk ?
f?i,j?
: ?hk?xi,1?
 hk?xi,j? ?
1gA0k ?
f?i,j?
: ?hk?xi,1?
 hk?xi,j? ?
0gGiven these definitions, we define Wk+, Wk, and Wk0 asW?k ?X?i,j?&A?kSi,jeMi,j?a?
?Wk ?X?i,j?&AkSi,jeMi,j?a?
?W0k ?X?i,j?&A0kSi,jeMi,j?a?
?ExpLoss is now rewritten in terms of these quantities:ExpLoss?Upd?a?, k, d??
?X?i,j?&A?kSi,jeMi,j?a?
?d ?X?i,j?&AkSi,jeMi,j?a??
?d ?X?i,j?&A0kSi,jeMi,j?a???
edW?k ?
edWk ?
W0k ?A:1?To find the value of d that minimizes this loss, we set the derivative of (A.1) withrespect to d to zero, giving the following solution:BestWt?k, a??
?
12logW?kWkPlugging this value of d back into (A.1) gives the best loss:BestLoss?k, a??
?
2ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiW?k Wkq?
W0k?
2ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiW?k Wkq?
Z  W?k  Wk?
Z ffiffiffiffiffiffiffiffiW?kqffiffiffiffiffiffiffiffiWkq 2?A:2?where Z ?
ExpLoss?a??
?PiPnij?2 Si,jeMi,j?a??
is a constant (for constant a?)
whichappears in the BestLoss for all features and therefore does not affect their ranking.Appendix B: An Alternative Method for LogLossIn this appendix we sketch an alternative approach for feature selection in LogLossthat is potentially an efficient method, at the cost of introducing an approximation64Computational Linguistics Volume 31, Number 165in the feature selection method.
Until now, we have defined BestLoss?k, a??
to be theminimum of the loss given that the kth feature is updated an optimal amount:BestLoss?k, a??
?
mindLogLoss?Upd?a?,k, d?
?In this section we sketch a different approach, based on results from Collins, Schapire,and Singer (2002), which leads to an algorithm very similar to that for ExpLoss inFigures 3 and 4.
Take the following definitions (note the similarity to the definitions inequations (13), (14), (15), and (16), with only the definitions for Wk+ and Wk beingaltered):W?k ?X?i,j?&A?kqi,j, Wk ?X?i,j?&Akqi,j, where qi,j ?eMi,j?a?
?1 ?Pniq?2 eMi,q?a??
?B:1?BestWt?k, a??
?
12logW?kWk?B:2?BestLoss?k, a??
?
LogLoss?a??
ffiffiffiffiffiffiffiffiW?kqffiffiffiffiffiffiffiffiWkq 2?B:3?Note that the ExpLoss computations can be recovered by replacing qi,j in equation(B.1) with qi,j ?
eMi,j?a??.
This is the only essential difference between the newalgorithm and the ExpLoss method.Results from Collins, Schapire and Singer (2002) show that under these definitionsthe following guarantee holds:LogLoss?Upd?a?,k, BestWt?k, a???
?BestLoss?k, a?
?So it can be seen that the update from a?
to Upd?a?, k, BestWt?k, a???
is guaranteed todecrease LogLoss by at leastffiffiffiffiffiffiffiffiW?kqffiffiffiffiffiffiffiffiWkq 2.
From these results, the algorithms in Figures 3and 4 could be altered to take the revised definitions of W?k and Wk into account.Selecting the feature with the minimum value of BestLoss?k, a??
at each iteration leadsto the largest guaranteed decrease in LogLoss.
Note that this is now an approximation,in that BestLoss?k, a?)
is an upper bound on the log-likelihood which may or may not betight.
There are convergence guarantees for the method, however, in that as thenumber of rounds of feature selection goes to infinity, the LogLoss approaches itsminimum value.The algorithms in Figures 3 and 4 could be modified to take the alternativedefinitions of W?k and Wk into account, thereby being modified to optimize LogLossinstead of ExpLoss.
The denominator terms in the qi,j definitions in equation (B.1) maycomplicate the algorithms somewhat, but it should still be possible to derive relativelyefficient algorithms using the technique.For a full derivation of the modified updates and for quite technical convergenceproofs, see Collins, Schapire and Singer (2002).
We give a sketch of the argument here.First, we show thatLogLoss?Upd?a?
; k; d?
?LogLoss?a? W?k  Wk ?
W?k ed ?
Wk ed?
?B:4?Collins and Koo Discriminative Reranking for NLPThis can be derived as follows (in this derivation we use gk?xi,j?
?
hk?xi,1?
 hk?xi,j??
:LogLoss?Upd?a?,k, d???
LogLoss?a??
?
LogLoss?Upd?a?, k, d??
 LogLoss?a???
LogLoss?a??
?Xilog1 ?Pnij?2 eMi,j?a?
?dgk?xi,j?1 ?Pnij?2 eMi,j?a??!?
LogLoss?a??
?Xilog11 ?Pnij?2 eMi,j?a??
?Pnij?2 eMi,j?a?
?dgk?xi,j?1 ?Pnij?2 eMi,j?a??!?
LogLoss?a??
?Xilog 1 Xnij?2qi,j ?Xnij?2qi,jedgk?xi,j?0@1A?B:5?LogLoss ?a??
XiXnij?2qi,j ?XiXnij?2qi,jedgk?xi,j??
LogLoss?a??
 ?W0k ?
W?k ?
Wk ?
?
W0k ?
W?k ed ?
Wk ed?
LogLoss?a??
 W?k  Wk ?
W?k ed ?
Wk ed?B:6?Equation (B.6) can be derived from equation (B.5) through the bound log?1 + x?xfor all x.The second step is to minimize the right-hand side of the bound in equation (B.4)with respect to d. It can be verified that the minimum is found atd ?
12logW?kWkat which value the right-hand side of equation (B.4) is equal toLogLoss?d?
ffiffiffiffiffiffiffiffiW?kqffiffiffiffiffiffiffiffiWkq 266AcknowledgmentsThanks to Rob Schapire and Yoram Singer foruseful discussions on boosting algorithms andto Mark Johnson for useful discussions aboutlinear models for parse ranking.
Steve Abneyand Fernando Pereira gave useful feedback onearlier drafts of this work.
Finally, thanks tothe anonymous reviewers for several usefulcomments.ReferencesAbney, Steven.
1997.
Stochasticattribute-value grammars.
ComputationalLinguistics, 23(4):597?618.Altun, Yasemin, Thomas Hofmann, andMark Johnson.
2003.
Discriminativelearning for label sequences via boosting.In Advances in Neural Information ProcessingSystems (NIPS 15), Vancouver.Altun, Yasemin, Mark Johnson, andThomas Hofmann.
2003.
Loss functionsand optimization methods fordiscriminative learning of label sequences.In Proceedings of Empirical Methods inNatural Language Processing (EMNLP 2003),Sapporo, Japan.Berger, Adam L., Stephen A. Della Pietra, andVincent J. Della Pietra.
1996.
A maximumentropy approach to natural languageComputational Linguistics Volume 31, Number 167processing.
Computational Linguistics,22(1):39?71.Black, Ezra, Frederick Jelinek, John Lafferty,David Magerman, Robert Mercer, and SalimRoukos.
1992.
Towards history-basedgrammars: Using richer models forprobabilistic parsing.
In Proceedings of theFifth DARPA Speech and Natural LanguageWorkshop, Harriman, NY.Charniak, Eugene.
1997.
Statistical parsingwith a context-free grammar and wordstatistics.
Proceedings of the 14th NationalConference on Artificial Intelligence, MenloPark, CA.
AAAI Press/MIT Press.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of NAACL-2000, Seattle.Chen, Stanley F., and Ronald Rosenfeld.1999.
A gaussian prior for smoothingmaximum entropy models.
TechnicalReport CMU-CS-99-108, ComputerScience Department, Carnegie MellonUniversity.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguisticsand Eighth Conference of the European Chapterof the Association for ComputationalLinguistics, pages 16?23, Madrid.Collins, Michael.
1999.
Head-DrivenStatistical Models for Natural LanguageParsing.
Ph.D. thesis, University ofPennsylvania, Philadelphia.Collins, Michael.
2000.
Discriminativereranking for natural language parsing.In Proceedings of the 17th InternationalConference on Machine Learning (ICML 2000),Stanford, CA.
Morgan Kaufmann,San Francisco.Collins, Michael.
2002a.
Ranking algorithmsfor named-entity extraction: Boosting andthe voted perceptron.
In ACL 2002:Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics,Philadelphia.Collins, Michael.
2002b.
Discriminativetraining methods for hidden Markovmodels: Theory and experiments with theperceptron algorithm.
In Proceedings ofEMNLP 2002, Philadelphia.Collins, Michael.
2004.
Parameter estimationfor statistical parsing models: Theory andpractice of distribution-free methods.
InHarry Bunt, John Carroll, and Giorgio Satta,editors, New Developments in ParsingTechnology.
Kluwer.Collins, Michael and Nigel Duffy.
2001.Convolution kernels for natural language.In Advances in Neural Information ProcessingSystems (NIPS 14), Vancouver.Collins, Michael, and Nigel Duffy.
2002.New ranking algorithms for parsing andtagging: Kernels over discrete structures,and the voted perceptron.
In ACL 2002:Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics,Philadelphia.Collins, Michael, Robert E. Schapire, andYoram Singer.
2002.
Logistic regression,AdaBoost and Bregman distances.
MachineLearning, 48(1/2/3):253?285.Della Pietra, Stephen, Vincent Della Pietra,and John Lafferty.
1997.
Inducing featuresof random fields.
IEEE Transactions onPattern Analysis and Machine Intelligence,19(4):380?393.Duffy, Nigel and David Helmbold.
1999.Potential boosters?
In Advances inNeural Information Processing Systems(NIPS 12), Denver.Freund, Yoav, Raj Iyer, Robert E. Schapire,and Yoram Singer.
1998.
An efficientboosting algorithm for combiningpreferences.
In Machine Learning:Proceedings of the 15th InternationalConference, Madison, WI.Freund, Yoav and Robert E. Schapire.
1997.
Adecision-theoretic generalization of on-linelearning and an application to boosting.Journal of Computer and System Sciences,55(1):119?139.Freund, Yoav and Robert E. Schapire.
1999.Large margin classification using theperceptron algorithm.
Machine Learning,37(3):277?296.Friedman, Jerome H., Trevor Hastie, andRobert Tibshirani.
2000.
Additive logisticregression: A statistical view of boosting.Annals of Statistics, 38(2):337?374.Henderson, James.
2003.
Inducing historyrepresentations for broad coveragestatistical parsing.
In Proceedings of the JointMeeting of the North American Chapter of theAssociation for Computational Linguistics andthe Human Language Technology Conference(HLT-NAACL 2003), pages 103?110,Edmonton, Alberta, Canada.Hoffgen, Klauss U., Kevin S. van Horn, andHans U. Simon.
1995.
Robust trainability ofsingle neurons.
Journal of Computer andSystem Sciences, 50:114?125.Johnson, Mark, Stuart Geman, StephenCanon, Zhiyi Chi, and Stefan Riezler.1999.
Estimators for stochastic??unification-based??
grammars.
InProceedings of ACL 1999, CollegePark, MD.Collins and Koo Discriminative Reranking for NLP68Joshi, Aravind K., Leon S. Levy, and MasakoTakahashi.
1975.
Tree adjunct grammars.Journal of Computer and System Science 10,no.
1:136?163.Koller, Daphne, and Mehran Sahami.
1996.Toward optimal feature selection.
InProceedings of the 13th InternationalConference on Machine Learning (ICML),pages 284?292, Bari, Italy, July.Lafferty, John.
1999.
Additive models,boosting, and inference for generalizeddivergences.
In Proceedings of the 12thAnnual Conference on Computational LearningTheory (COLT?99), Santa Cruz, CA.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.In Proceedings of ICML 2001,Williamstown, MA.Lebanon, Guy and John Lafferty.
2001.Boosting and maximum likelihood forexponential models.
In Advances inNeural Information Processing Systems(NIPS 14), Vancouver.Malouf, Robert.
2002.
A comparison ofalgorithms for maximum entropyparameter estimation.
In Proceedings of theSixth Conference on Natural LanguageLearning (CoNNL-2002), Taipei, Taiwan.Marcus, Mitchell, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: The PennTreebank.
Computational Linguistics,19(2):313?330.Mason, Llew, Peter L. Bartlett, and JonathanBaxter.
1999.
Direct optimization of marginsimproves generalization in combinedclassifiers.
In Advances in Neural InformationProcessing Systems (NIPS 12), Denver.McCallum, Andrew.
2003.
Efficientlyinducing features of conditional randomfields.
In Proceedings of the Conference onUncertainty in Artificial Intelligence (UAI2003), Acapulco.Och, Franz Josef, and Hermann Ney.
2002.Discriminative training and maximumentropy models for statistical machinetranslation.
In ACL 2002: Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 295?302,Philadelphia.Papineni, Kishore A., Salim Roukos, and R. T.Ward.
1997.
Feature-based languageunderstanding.
In Proceedings ofEuroSpeech?97, vol.
3, pages 1435?1438,Rhodes, Greece.Papineni, Kishore A., Salim Roukos, andR.
T. Ward.
1998.
Maximum likelihoodand discriminative training of directtranslation models.
In Proceedings of the1998 IEEE International Conference onAcoustics, Speech and Signal Processing,vol.
1, pages 189?192, Seattle.Pearl, Judea.
1988.
Probabilistic Reasoning inIntelligent Systems.
Morgan Kaufmann, SanMateo, CA.Ratnaparkhi, Adwait.
1997.
A linear observedtime statistical parser based on maximumentropy models.
In Proceedings of the SecondConference on Empirical Methods in NaturalLanguage Processing, Brown University,Providence, RI.Ratnaparkhi, Adwait.
1998.
Maximum EntropyModels for Natural Language AmbiguityResolution.
Ph.D. thesis, University ofPennsylvania, Philadelphia.Ratnaparkhi, Adwait, Salim Roukos, andR.
T. Ward.
1994.
A maximum entropymodel for parsing.
In Proceedings of theInternational Conference on Spoken LanguageProcessing, pages 803?806, Yokohama,Japan.Riezler, Stefan, Tracy H. King,Ronald M. Kaplan, Richard Crouch,John T. Maxwell III, and Mark Johnson.2002.
Parsing the Wall Street Journalusing a lexical-functional grammarand discriminative estimationtechniques.
In ACL 2002: Proceedingsof the 40th Annual Meeting of theAssociation for Computational Linguistics,Philadelphia.Riezler, Stefan and Alexander Vasserman.2004.
Incremental feature selection andl1 regularization for relaxedmaximum-entropy modeling.
In Proceedingsof the 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP?04),Barcelona, Spain.Rosenfeld, Ronald.
1997.
A whole sentencemaximum entropy language model.
InProceedings of the IEEE Workshop onSpeech Recognition and Understanding,Santa Barbara, CA, December.Schapire, Robert E., Yoav Freund, PeterBartlett, and W. S. Lee.
1998.
Boosting themargin: A new explanation for theeffectiveness of voting methods.
Annals ofStatistics, 26(5):1651?1686.Schapire, Robert E. and Yoram Singer.
1999.Improved boosting algorithms usingconfidence-rated predictions.
MachineLearning, 37(3):297?336.Schapire, Robert E. and Yoram Singer.
2000.BoosTexter: A boosting-based system fortext categorization.
Machine Learning,39(2/3):135?168.Computational Linguistics Volume 31, Number 169Sha, Fei and Fernando Pereira.
2003.
Shallowparsing with conditional random fields.
InProceedings of HLT-NAACL 2003,Edmonton, Alberta, Canada.Shen, Libin, Anoop Sarkar, and Aravind K.Joshi.
2003.
Using LTAG based features inparse reranking.
In Proceedings of the 2003Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2003),Sapporo, Japan.Valiant, Leslie G. 1984.
A theory of thelearnable.
Communications of the ACM,27(11):1134?1142.Walker, Marilyn, Owen Rambow, andMonica Rogati.
2001.
SPoT: A trainablesentence planner.
In Proceedings of the SecondMeeting of the North American Chapter of theAssociation for ComputationalLinguistics (NAACL 2001), Pittsburgh.Zhou, Yaqian, Fuliang Weng, Lide Wu, andHauke Schmidt.
2003.
A fast algorithm forfeature selection in conditional maximumentropy modeling.
In Proceedings of the 2003Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2003),Sapporo, Japan.Collins and Koo Discriminative Reranking for NLP
