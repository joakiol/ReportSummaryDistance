Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 177?185,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsImportance-Driven Turn-Bidding for Spoken Dialogue SystemsEthan O. Selfridge and Peter A. HeemanCenter for Spoken Language UnderstandingOregon Health & Science University20000 NW Walker Rd., Beaverton, OR, 97006selfridg@ohsu.edu, heemanp@ohsu.eduAbstractCurrent turn-taking approaches for spokendialogue systems rely on the speaker re-leasing the turn before the other can take it.This reliance results in restricted interac-tions that can lead to inefficient dialogues.In this paper we present a model we re-fer to as Importance-Driven Turn-Biddingthat treats turn-taking as a negotiative pro-cess.
Each conversant bids for the turnbased on the importance of the intendedutterance, and Reinforcement Learning isused to indirectly learn this parameter.
Wefind that Importance-Driven Turn-Biddingperforms better than two current turn-taking approaches in an artificial collabo-rative slot-filling domain.
The negotiativenature of this model creates efficient dia-logues, and supports the improvement ofmixed-initiative interaction.1 IntroductionAs spoken dialogue systems are designed toperform ever more elaborate tasks, the needfor mixed-initiative interaction necessarily grows.Mixed-initiative interaction, where agents (bothartificial and human) may freely contribute toreach a solution efficiently, has long been a focusof dialogue systems research (Allen et al, 1999;Guinn, 1996).
Simple slot-filling tasks mightnot require the flexible environment that mixed-initiative interaction brings but those of greatercomplexity, such as collaborative task comple-tion or long-term planning, certainly do (Fergu-son et al, 1996).
However, translating this interac-tion into working systems has proved problematic(Walker et al, 1997), in part to issues surround-ing turn-taking: the transition from one speaker toanother.Many computational turn-taking approachesseek to minimize silence and utterance overlapduring transitions.
This leads to the speaker con-trolling the turn transition.
For example, systemsusing the Keep-Or-Release approach will not at-tempt to take the turn unless it is sure the userhas released it.
One problem with this approachis that the system might have important informa-tion to give but will be unable to get the turn.The speaker-centric nature of current approachesdoes not enable mixed-initiative interaction andresults in inefficient dialogues.
Primarily, theseapproaches have been motivated by smooth tran-sitions reported in the human turn-taking studiesof Sacks et al (1974) among others.Sacks et al also acknowledge the negotiativenature of turn-taking, stating that the ?the turn asunit is interactively determined?(p.
727).
Otherstudies have supported this, suggesting that hu-mans negotiate the turn assignment through theuse of cues and that these cues are motivated bythe importance of what the conversant wishes tocontribute (Duncan and Niederehe, 1974; Yangand Heeman, 2010; Schegloff, 2000).
Giventhis, any dialogue system hoping to interact withhumans efficiently and naturally should have anegotiative and importance-driven quality to itsturn-taking protocol.
We believe that, by focus-ing on the rationale of human turn-taking be-havior, a more effective turn-taking system maybe achieved.
We propose the Importance-DrivenTurn-Bidding (IDTB) model where conversantsbid for the turn based on the importance of theirutterance.
We use Reinforcement Learning to mapa given situation to the optimal utterance and bid-ding behavior.
By allowing conversants to bid forthe turn, the IDTB model enables negotiative turn-taking and supports true mixed-initiative interac-tion, and with it, greater dialogue efficiency.We compare the IDTB model to current turn-taking approaches.
Using an artificial collab-orative dialogue task, we show that the IDTBmodel enables the system and user to complete177the task more efficiently than the other approaches.Though artificial dialogues are not ideal, they al-low us to test the validity of the IDTB model be-fore embarking on costly and time-consuming hu-man studies.
Since our primary evaluation criteriais model comparison, consistent user simulationsprovide a constant needed for such measures andincrease the external validity of our results.2 Current Turn-Taking ApproachesCurrent dialogue systems focus on the release-turnas the most important aspect of turn-taking, inwhich a listener will only take the turn after thespeaker has released it.
The simplest of these ap-proaches only allows a single utterance per turn,after which the turn necessarily transitions to thenext speaker.
This Single-Utterance (SU) modelhas been extended to allow the speaker to keep theturn for multiple utterances: the Keep-Or-Release(KR) approach.
Since the KR approach gives thespeaker sole control of the turn, it is overwhelm-ingly speaker-centric, and so necessarily unnego-tiative.
This restriction is meant to encouragesmooth turn-transitions, and is inspired by the or-der, smoothness, and predictability reported in hu-man turn-taking studies (Duncan, 1972; Sacks etal., 1974).Systems using the KR approach differ on howthey detect the user?s release-turn.
Turn releasesare commonly identified in two ways: either us-ing a silence-threshold (Sutton et al, 1996), orthe predictive nature of turn endings (Sacks et al,1974) and the cues associated with them (e.g.
Gra-vano and Hirschberg, 2009).
Raux and Eskenazi(2009) used decision theory with lexical cues topredict appropriate places to take the turn.
Simi-larly, Jonsdottir, Thorisson, and Nivel (2008) usedReinforcement Learning to reduce silences be-tween turns and minimize overlap between utter-ances by learning the specific turn-taking patternsof individual speakers.
Skantze and Schlangan(2009) used incremental processing of speech andprosodic turn-cues to reduce the reaction time ofthe system, finding that that users rated this ap-proach as more human-like than a baseline system.In our view, systems built using the KR turn-taking approach suffer from two deficits.
First,the speaker-centricity leads to inefficient dialoguessince the speaker may continue to hold the turneven when the listener has vital information togive.
In addition, the lack of negotiation forcesthe turn to necessarily transition to the listener af-ter the speaker releases it.
The possibility that thedialogue may be better served if the listener doesnot get the turn is not addressed by current ap-proaches.Barge-in, which generally refers to allowingusers to speak at any time (Stro?m and Seneff,2000), has been the primary means to create amore flexible turn-taking environment.
Yet, sincebarge-in recasts speaker-centric systems as user-centric, the system?s contributions continue to belimited.
System barge-in has also been investi-gated.
Sato et al (2002) used decision trees to de-termine whether the system should take the turn ornot when the user pauses.
An incremental methodby DeVault, Sagae, and Traum (2009) found pos-sible points that a system could interrupt withoutloss of user meaning, but failed to supply a rea-sonable model as to when to use such information.Despite these advances, barge-in capable systemslack a negotiative turn-taking method, and con-tinue to be deficient for reasons similar to thosedescribed above.3 Importance-Driven Turn-Bidding(IDTB)We introduce the IDTB model to overcome the de-ficiencies of current approaches.
The IDTB modelhas two foundational components: (1) The impor-tance of speaking is the primary motivation behindturn-taking behavior, and (2) conversants use turn-cue strength to bid for the turn based on this impor-tance.
Importance may be broadly defined as howwell the utterance leads to some predeterminedconversational success, be it solely task comple-tion or encompassing a myriad of social etiquettecomponents.Importance-Driven Turn-Bidding is motivatedby empirical studies of human turn-conflict res-olution.
Yang and Heeman (2010) found an in-crease of turn conflicts during tighter time con-straints, which suggests that turn-taking is in-fluenced by the importance of task completion.Schlegoff (2000) proposed that persistent utter-ance overlap was indicative of conversants hav-ing a strong interest in holding the turn.
Walkerand Whittaker (1990) show that people will inter-rupt to remedy some understanding discrepancy,which is certainly important to the conversation?ssuccess.
People communicate the importance oftheir utterance through turn-cues.
Duncan and178Niederehe (1974) found that turn-cue strength wasthe best predictor of who won the turn, and thisfinding is consistent with the use of volume to winturns found by Yang and Heeman (2010).The IDTB model uses turn-cue strength to bidfor the turn based on the importance of the utter-ance.
Stronger turn-cues should be used when theintended utterance is important to the overall suc-cess of the dialogue, and weaker ones when it isnot.
In the prototype described in Section 5, boththe system and user agents bid for the turn after ev-ery utterance and the bids are conceptualized hereas utterance onset: conversants should be quickto speak important utterances but slow with lessimportant ones.
This is relatively consistent withYang and Heeman (2010).
A mature version ofour work will use cues in addition to utterance on-set, such as those recently detailed in Gravano andHirshberg (2009).1A crucial element of our model is the judgmentand quantization of utterance importance.
We useReinforcement Learning (RL) to determine impor-tance by conceptualizing it as maximizing the re-ward over an entire dialogue.
Whatever actionslead to a higher return may be thought of as moreimportant than ones that do not.2 By using RL tolearn both the utterance and bid behavior, the sys-tem can find an optimal pairing between them, andchoose the best combination for a given conversa-tional situation.4 Information State Update andReinforcement LearningWe build our dialogue system using the Informa-tion State Update approach (Larsson and Traum,2000) and use Reinforcement Learning for actionselection (Sutton and Barto, 1998).
The systemarchitecture consists of an Information State (IS)that represents the agent?s knowledge and is up-dated using a variety of rules.
The IS also usesrules to propose possible actions.
A condensedand compressed subset of the IS ?
the Reinforce-ment Learning State ?
is used to learn which pro-posed action to take (Heeman, 2007).
It has beenshown that using RL to learn dialogue polices isgenerally more effective than ?hand crafted?
di-1Our work (present and future) is distinct from some re-cent work on user pauses (Sato et al, 2002) since we treatturn-taking as an integral piece of dialogue success.2We gain an inherent flexibility in using RL since the re-ward can be computed by a wide array of components.
Thisis consistent with the broad definition of importance.alogue policies since the learning algorithm maycapture environmental dynamics that are unat-tended to by human designers (Levin et al, 2000).Reinforcement Learning learns an optimal pol-icy, a mapping between a state s and action a,where performing a in s leads to the lowest ex-pected cost for the dialogue (we use minimumcost instead of maximum reward).
An -greedysearch is used to estimate Q-scores, the expectedcost of some state?action pair, where the systemchooses a random action with  probability and theargminaQ(s, a) action with 1- probability.
ForQ-learning, a popular RL algorithm and the oneused here,  is commonly set at 0.2 (Sutton andBarto, 1998).
Q-learning updates Q(s, a) basedon the best action of the next state, given by thefollowing equation, with the step size parameter?
= 1/?N(s, a) where N(s, a) is the number oftimes the s, a pair has been seen since the begin-ning of training.Q(st, at) = Q(st, at) + ?
[costt+1+ argminaQ(st+1, a)?Q(st, at)]The state space should be formulated as aMarkov Decision Process (MDP) for Q-learningto update Q-scores properly.
An MDP relies ona first-order Markov assumption in that the transi-tion and reward probability from some st, at pairis completely contained by that pair and is unaf-fected by the history st?1at?1, st?2at?2, .
.
.. Forthis assumption to be met, care is required whendeciding which features to include for learning.The RL State features we use are described in thefollowing section.5 Domain and Turn-Taking ModelsIn this section, we show how the IDTB ap-proach can be implemented for a collaborativeslot filling domain.
We also describe the Single-Utterance and Keep-Or-Release domain imple-mentations that we use for comparison.5.1 Domain TaskWe use a food ordering domain with two partici-pants, the system and a user, and three slots: drink,burger, and side.
The system?s objective is to fillall three slots with the available fillers as quicklyas possible.
The user?s role is to specify its de-sired filler for each slot, though that specific fillermay not be available.
The user simulation, whileintended to be realistic, is not based on empiricaldata.
Rather, it is designed to provide a rich turn-179taking domain to evaluate the performance of dif-ferent turn-taking designs.
We consider this a col-laborative slot-filling task since both conversantsmust supply information to determine the intersec-tion of available and desired fillers.Users have two fillers for each slot.3 A user?stop choice is either available, in which case we saythat the user has adequate filler knowledge, or theirsecond choice will be available, in which we sayit has inadequate filler knowledge.
This assuresthat at least one of the user?s filler is available.Whether a user has adequate or inadequate fillerknowledge is probabilistically determined basedon user type, which will be described in Section5.2.Table 1: Agent speech actsAgent ActionsSystem query slot, inform [yes/no],inform avail.
slot fillers,inform filler not available, byeUser inform slot filler,query filler availabilityWe model conversations at the speech act level,shown in Table 1, and so do not model the actualwords that the user and system might say.
Eachagent has an Information State that proposes possi-ble actions.
The IS is made up of a number of vari-ables that model the environment and is slightlydifferent for the system and the user.
Shared vari-ables include QUD, a stack which manages thequestions under discussion; lastUtterance, the pre-vious utterance, and slotList, a list of the slotnames.
The major system specific IS variablesthat are not included in the RL State are availSlot-Fillers, the available fillers for each slot; and threeslotFiller variables that hold the fillers given by theuser.
The major user specific IS variables are threedesiredSlotFiller variables that hold an ordered listof fillers, and unvisitedSlots, a list of slots that theuser believes are unfilled.The system has a variety of speech actions: in-form [yes/no], to answer when the user has asked afiller availability question; inform filler not avail-able, to inform the user when they have specifiedan unavailable filler; three query slot actions (onefor each slot), a query which asks the user for afiller and is proposed if that specific slot is unfilled;3We use two fillers so as to minimize the length of train-ing.
This can be increased without substantial effort.three inform available slot fillers actions, whichlists the available fillers for that slot and is pro-posed if that specific slot is unfilled or filled withan unavailable filler; and bye, which is always pro-posed.The user has two actions.
They can inform thesystem of a desired slot filler, inform slot filler, orquery the availability of a slot?s top filler, queryfiller availability.
A user will always respond withthe same slot as a system query, but may changeslots entirely for all other situations.
Additionaldetails on user action selection are given in Section5.2.Specific information is used to produce an in-stantiated speech action, what we refer to as anutterance.
For example, the speech action informslot filler results in the utterance of ?inform drinkd1.?
A sample dialogue fragment using the Single-Utterance approach is shown in Table 2.
Noticethat in Line 3 the system informs the user thattheir first filler, d1, is unavailable.
The user thenasks asks about the availability of its second drinkchoice, d2 (Line 4), and upon receiving an affirma-tive response (Line 5), informs the system of thatfiller preference (Line 6).Table 2: Single-Utterance dialogueSpkr Speech Action Utterance1 S: q. slot q. drink2 U: i. slot filler i. drink d13 S: i. filler not avail i. not have d14 U: q. filler avail q. drink have d25 S: i. slot i. yes6 U: i. slot filler i. drink d27 S: i. avail slot fillers i. burger have b1Implementation in RL: The system uses RL tolearn which of the IS proposed actions to take.
Inthis domain we use a cost function based on dia-logue length and the number of slots filled with anavailable filler: C = Number of Utterances + 25 ?unavailablyFilledSlots.
In the present implemen-tation the system?s bye utterance is costless.
Thesystem chooses the action that minimizes the ex-pected cost of the entire dialogue from the currentstate.The RL state for the speaker has seven vari-ables:4 QUD-speaker, the stack of speakers whohave unresolved questions; Incorrect-Slot-Fillers,4We experimented with a variety of RL States and this oneproved to be both small and effective.180a list of slot fillers (ordered chronologically onwhen the user informed them) that are unavail-able and have not been resolved; Last-Sys-Speech-Action, the last speech action the system per-formed; Given-Slot-Fillers, a list of slots that thesystem has performed the inform available slotfiller action on; and three booleans variables, slot-RL, that specify whether a slot has been filled cor-rectly or not (e.g.
Drink-RL).5.2 User TypesWe define three different types of users ?
Experts,Novices, and Intermediates.
User types differprobabilistically on two dimensions: slot knowl-edge, and slot belief strength.
We define experts tohave a 90 percent chance of having adequate fillerknowledge, intermediates a 50 percent chance,and novices a 10 percent chance.
These proba-bilities are independent between slots.
Slot beliefstrength represents the user?s confidence that it hasadequate domain knowledge for the slot (i.e.
thetop choice for that slot is available).
It is eithera strong, warranted, or weak belief (Chu-Carrolland Carberry, 1995).
The intuition is that expertsshould know when their top choice is available,and novices should know that they do not knowthe domain well.Initial slot belief strength is dependent on usertype and whether their filler knowledge is ade-quate (their initial top choice is available).
Ex-perts with adequate filler knowledge have a 70,20, and 10 percent chance of having Strong, War-ranted, and Weak beliefs respectfully.
Similarly,intermediates with adequate knowledge have a 50,25, and 25 percent chance of the respective beliefstrengths.
When these user types have inadequatefiller knowledge the probabilities are reversed todetermine belief strength (e.g.
Experts with inad-equate domain knowledge for a slot have a 70%chance of having a weak belief).
Novice users al-ways have a 10, 10, and 80 percent chance of therespective belief strengths.The user choses whether to use the query orinform speech action based on the slot?s beliefstrength.
A strong belief will always result in aninform, a warranted belief resulting in an informwith p = 0.5, and weak belief will result in an in-form with p = 0.25.
If the user is informed of thecorrect fillers by the system?s inform, that slot?sbelief strength is set to strong.
If the user is in-formed that a filler is not available, than that filleris removed from the desired filler list and the beliefremains the same.55.3 Turn-Taking ModelsWe now discuss how turn-taking works for theIDTB model and the two competing models thatwe use to evaluate our approach.
The systemchooses its turn action based on the RL state andwe add a boolean variable turn-action to the RLState to indicate when the system is performing aturn action or a speech action.
The user uses beliefto choose its turn action.Turn-Bidding: Agents bid for the turn at theend of each utterance to determine who will speaknext.
Each bid is represented as a value between 0and 1, and the agent with the lower value (strongerbid) wins the turn.
This is consistent with theuse of utterance onset.
There are 5 types of bids,highest, high, middle, low, and lowest, which arespread over a portion of the range as shown in Fig-ure 1.
The system uses RL to choose a bid anda random number (uniform distribution) is gener-ated from that bid?s range.
The users?
bids are de-termined by their belief strength, which specifiesthe mean of a Gaussian distribution, as shown inFigure 1 (e.g Strong belief implies a ?
= 0.35).Computing bids in this fashion leads to, on av-erage, users with strong beliefs bidding highest,warranted beliefs bidding in the middle, and weakbeliefs bidding lowest.
The use of the probabil-ity distributions allows us to randomly decide tiesbetween system and user bids.Figure 1: Bid Value Probability DistributionSingle-Utterance: The Single-Utterance (SU)approach, as described in Section 2, has a rigid5In this simple domain the next filler is guaranteed to beavailable if the first is not.
We do not model this with beliefstrength since it is probably not representative of reality.181turn-taking mechanism.
After a speaker makes asingle utterance the turn transitions to the listener.Since the turn transitions after every utterance thesystem must only choose appropriate utterances,not turn-taking behavior.
Similarly, user agents donot have any turn-taking behavior and slot beliefsare only used to choose between a query and aninform.Keep-Or-Release Model: The Keep-Or-Release (KR) model, as described in Section2, allows the speaker to either keep the turn tomake multiple utterances or release it.
Taking thesame approach as English and Heeman (2005),the system learns to keep or release the turn aftereach utterance that it makes.
We also use RLto determine which conversant should begin thedialogue.
While the use of RL imparts someimportance onto the turn-taking behavior, itis not influencing whether the system gets theturn when it did not already have it.
This is ancrucial distinction between KR and IDTB.
IDTBallows the conversants to negotiate the turn usingturn-bids motivated by importance, whereas inKR only the speaker determines when the turncan transition.Users in the KR environment choose whether tokeep or release the turn similarly to bid decisions.6After a user performs an utterance, it chooses theslot that would be in the next utterance.
A number,k, is generated from a Gaussian distribution usingbelief strength in the same manner as the IDTBusers?
bids are chosen.
If k ?
0.55 then the userkeeps the turn, otherwise it releases it.5.4 Preliminary Turn-Bidding SystemWe described a preliminary turn-bidding systemin earlier work presented at a workshop (Selfridgeand Heeman, 2009).
A major limitation was anoverly simplified user model.
We used two usertypes, expert and novice, who had fixed bids.
Ex-perts always bid high and had complete domainknowledge, and the novices always bid low andhad incomplete domain knowledge.
The system,using all five bid types, was always able to out bidand under bid the simulated users.
Among otherthings, this situation gives the system completecontrol of the turn, which is at odds with the nego-tiative nature of IDTB.
The present contribution isa more realistic and mature implementation.6We experimented with a few different KR decisionstrategies, and chose the one that performed the best.6 Evaluation and DiscussionWe now evaluate the IDTB approach by compar-ing it against the two competing models: Single-Utterance and Keep-Or-Release.
The three turn-taking approaches are trained and tested in fouruser conditions: novice, intermediate, expert, andcombined.
In the combined condition, one of thethree user types is randomly selected for each dia-logue.
We train ten policies for each condition andturn-taking approach.
Policies are trained using Q-learning, and ?greedy search for 10000 epochs(1 epoch = 100 dialogues, after which the Q-scoresare updated) with  = 0.2.
Each policy is thenran over 10000 test dialogues with no exploration( = 0), and the mean dialogue cost for that pol-icy is determined.
The 10 separate policy valuesare then averaged to create the mean policy cost.The mean policy cost between the turn-taking ap-proaches and user conditions are shown in Table 3.Lower numbers are indicative of shorter dialogues,since the system learns to successfully completethe task in all cases.Table 3: Mean Policy Cost for Model and Usercondition7Model Novice Int.
Expert CombinedSU 7.61 7.09 6.43 7.05KR 6.00 6.35 4.46 6.01IDTB 6.09 5.77 4.35 5.52Single User Conditions: Single user conditionsshow how well each turn-taking approach can op-timize its behavior for specific user populationsand handle slight differences found in those pop-ulations.
Table 3 shows that the mean policy costof the SU model is higher than the other two mod-els which indicates longer dialogues on average.Since the SU system must respond to every userutterance and cannot learn a turn-taking strategyto utilize user knowledge, the dialogues are neces-sarily longer.
For example, in the expert conditionthe best possible dialogue for a SU interaction willhave a cost of five (three user utterances for eachslot, two system utterances in response).
This costis in contrast to the best expert dialogue cost ofthree (three user utterances) for KR and IDTB in-teractions.The IDTB turn-taking approach outperformsthe KR design in all single user conditions ex-7SD between policies ?
0.04182cept for novice (6.09 vs. 6.00).
In this condi-tion, the KR system takes the turn first, informsthe available fillers for each slot, and then releasesthe turn.
The user can then inform its filler eas-ily.
The IDTB system attempts a similar dialoguestrategy by using highest bids but sometimes losesthe turn when users also bid highest.
If the useruses the turn to query or inform an unavailablefiller the dialogue grows longer.
However, this isquite rare as shown by small difference in perfor-mance between the two models.
In all other singleuser conditions, the IDTB approach has shorter di-alogues than the KR approach (5.77 and 4.35 vs.6.35 and 4.46).
A detailed explanation of IDTB?sperformance will be given in Section 6.1.Combined User Condition: We next measureperformance on the combined condition thatmixes all three user types.
This condition is morerealistic than the other three, as it better mimicshow a system will be used in actual practice.
TheIDTB approach (mean policy cost = 5.52) outper-forms the KR (mean policy cost = 6.01) and SU(mean policy cost = 7.05) approaches.
We alsoobserve that KR outperforms SU.
These resultssuggest that the more a turn-taking design can beflexible and negotiative, the more efficient the dia-logues can be.Exploiting User bidding differences: It fol-lows that IDTB?s performance stems from its ne-gotiative turn transitions.
These transitions are dis-tinctly different than KR transitions in that there isinformation inherent in the users bids.
A user thathas a stronger belief strength is more likely to behave a higher bid and inform an available filler.Policy analysis shows that the IDTB system takesadvantage of this information by using moderatebids ?neither highest nor lowest bids?
to filterusers based on their turn behavior.
The distribu-tion of bids used over the ten learned policies isshown in Table 4.
The initial position refers tothe first bid of the dialogue; final position, the lastbid of the dialogue; and medial position, all otherbids.
Notice that the system uses either the low ormid bids as its initial policy and that 67.2% of di-alogue medial bids are moderate.
These distribu-tions show that the system has learned to use theentire bid range to filter the users, and is not seek-ing to win or lose the turn outright.
This behavioris impossible in the KR approach.Table 4: Bid percentages over ten policies in theCombined User condition for IDTBPosition H-est High Mid Low L-estInitial 0.0 0.0 70.0 30.0 0.0Medial 20.5 19.4 24.5 23.3 12.3Final 49.5 41.0 9.5 0.0 0.06.1 IDTB Performance:In our domain, performance is measured by dia-logue length and solution quality.
However, sincesolution quality never affects the dialogue cost fora trained system, dialogue length is the only com-ponent influencing the mean policy cost.The primary cause of longer dialogues are un-available filler inform and query (UFI?Q) utter-ances by the user, which are easily identified.These utterances lengthen the dialogue since thesystem must inform the user of the available fillers(the user would otherwise not know that the fillerwas unavailable) and then the user must then in-form the system of its second choice.
The meannumber of UFI?Q utterance for each dialogue overthe ten learned policies are shown for all user con-ditions in Table 5.
Notice that these numbers areinversely related to performance: the more UFI?Q utterances, the worse the performance.
For ex-ample, in the combined condition the IDTB usersperform 0.38 UFI?Q utterances per dialogue (u/d)compared to the 0.94 UFI?Q u/d for KR users.While a KR user will release the turn if its plannedTable 5: Mean number of UFI?Q utterances overpoliciesModel Novice Int.
Expert CombinedKR 0.0 1.15 0.53 0.94IDTB 0.1 0.33 0.39 0.38utterance has a weak belief, it may select that weakutterance when first getting the turn (either after asystem utterance or at the start of the dialogue).This may lead to a UFI?Q utterance.
The IDTBsystem, however, will outbid the same user, result-ing in a shorter dialogue.
This situation is shownin Tables 6 and 7.
The dialogue is the same un-til utterance 3, where the IDTB system wins theturn with a mid bid over the user?s low bid.
In theKR environment however, the user gets the turnand performs an unavailable filler inform, whichthe system must react to.
This is an instance ofthe second deficiency of the KR approach, where183Table 6: Sample IDTB dialogue in Combined Usercondition; Cost=6Sys Usr Spkr Utt1 low mid U: inform burger b12 h-est low S: inform burger have b33 mid low S: inform side have s14 mid h-est U: inform burger b35 mid high U: inform drink d16 l-est h-est U: inform side s17 high mid S: byeTable 7: Sample KR dialogue in Combined Usercondition; Cost=7Agent Utt Turn-Action1 U: inform burger b1 Release2 S: inform burger have b3 Release3 U: inform side s1 Keep4 U: inform drink d1 Keep5 U: inform burger b3 Release6 S: inform side have s2 Release7 U: inform side s2 Release8 S: byethe speaking system should not have released theturn.
The user has the same belief in both scenar-ios, but the negotiative nature of IDTB enables ashorter dialogues.
In short, the IDTB system canwin the turn when it should have it, but the KRsystem cannot.A lesser cause of longer dialogues is an instanceof the first deficiency of the KR systems; the lis-tening user cannot get the turn when it should haveit.
Usually, this situation presents itself when theuser releases the turn, having randomly chosen theweaker of the two unfilled slots.
The system thenhas the turn for more than one utterance, inform-ing the available fillers for two slots.
However,the user already had a strong belief and availabletop filler for one of those slots, and the systemhas increased the dialogue length unnecessarily.
Inthe combined condition, the KR system produces0.06 unnecessary informs per dialogue, whereasthe IDTB system produces 0.045 per dialogue.The novice and intermediate conditions mirror this(IDTB: 0.009, 0.076 ; KR: 0.019, 0.096 respect-fully), but the expert condition does not (IDTB:0.011, KR: 0.0014).
In this case, the IDTB systemwins the turn initially using a low bid and informsone of the strong slots, whereas the expert user ini-tiates the dialogue for the KR environment and un-necessary informs are rarer.
In general, however,the KR approach has more unnecessary informssince the KR system can only infer that one of theuser?s beliefs was probably weak, otherwise theuser would not have released the turn.
The IDTBsystem handles this situation by using a high bid,allowing the user to outbid the system as its con-tribution is more important.
In other words, theIDTB user can win the turn when it should have it,but the KR user cannot.7 ConclusionThis paper presented the Importance-Driven Turn-Bidding model of turn-taking.
The IDTB model ismotivated by turn-conflict studies showing that theinterest in holding the turn influences conversantturn-cues.
A computational prototype using Re-inforcement Learning to choose appropriate turn-bids performs better than the standard KR and SUapproaches in an artificial collaborative dialoguedomain.
In short, the Importance-Driven Turn-Bidding model provides a negotiative turn-takingframework that supports mixed-initiative interac-tions.In the previous section, we showed that the KRapproach is deficient for two reasons: the speak-ing system might not keep the turn when it shouldhave, and might release the turn when it shouldnot have.
This is driven by KR?s speaker-centricnature; the speaker has no way of judging thepotential contribution of the listener.
The IDTBapproach however, due to its negotiative quality,does not have this problem.Our performance differences arise from situa-tions when the system is the speaker and the useris the listener.
The IDTB model also excels in theopposite situation, when the system is the listenerand the user is the speaker, though our domain isnot sophisticated enough for this situation to oc-cur.
In the future we hope to develop a domainwith more realistic speech acts and a more diffi-cult dialogue task that will, among other things,highlight this situation.
We also plan on imple-menting a fully functional IDTB system, using anincremental processing architecture that not onlydetects, but generates, a wide array of turn-cues.AcknowledgmentsWe gratefully acknowledge funding from theNational Science Foundation under grant IIS-0713698.184ReferencesJ.E Allen, C.I.
Guinn, and Horvitz E. 1999.
Mixed-initiative interaction.
IEEE Intelligent Systems,14(5):14?23.Jennifer Chu-Carroll and Sandra Carberry.
1995.
Re-sponse generation in collaborative negotiation.
InProceedings of the 33rd annual meeting on Asso-ciation for Computational Linguistics, pages 136?143, Morristown, NJ, USA.
Association for Compu-tational Linguistics.David DeVault, Kenji Sagae, and David Traum.
2009.Can i finish?
learning when to respond to incre-mental interpretation results in interactive dialogue.In Proceedings of the SIGDIAL 2009 Conference,pages 11?20, London, UK, September.
Associationfor Computational Linguistics.S.J.
Duncan and G. Niederehe.
1974.
On signallingthat it?s your turn to speak.
Journal of ExperimentalSocial Psychology, 10:234?247.S.J.
Duncan.
1972.
Some signals and rules for takingspeaking turns in conversations.
Journal of Person-ality and Social Psychology, 23:283?292.M.
English and Peter A. Heeman.
2005.
Learningmixed initiative dialog strategies by using reinforce-ment learning on both conversants.
In Proceedingsof HLT/EMNLP, pages 1011?1018.G.
Ferguson, J. Allen, and B. Miller.
1996.
TRAINS-95: Towards a mixed-initiative planning assistant.In Proceedings of the Third Conference on ArtificialIntelligence Planning Systems (AIPS-96), pages 70?77.A.
Gravano and J. Hirschberg.
2009.
Turn-yieldingcues in task-oriented dialogue.
In Proceedings of theSIGDIAL 2009 Conference: The 10th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, pages 253?261.
Association for Compu-tational Linguistics.C.I.
Guinn.
1996.
Mechanisms for mixed-initiativehuman-computer collaborative discourse.
In Pro-ceedings of the 34th annual meeting on Associationfor Computational Linguistics, pages 278?285.
As-sociation for Computational Linguistics.P.A.
Heeman.
2007.
Combining reinforcement learn-ing with information-state update rules.
In Pro-ceedings of the Annual Conference of the NorthAmerican Association for Computational Linguis-tics, pages 268?275, Rochester, NY.Gudny Ragna Jonsdottir, Kristinn R. Thorisson, andEric Nivel.
2008.
Learning smooth, human-liketurntaking in realtime dialogue.
In IVA ?08: Pro-ceedings of the 8th international conference on In-telligent Virtual Agents, pages 162?175, Berlin, Hei-delberg.
Springer-Verlag.S.
Larsson and D. Traum.
2000.
Information state anddialogue managment in the trindi dialogue move en-gine toolkit.
Natural Language Engineering, 6:323?340.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
Astochastic model of human-machine interaction forlearning dialog strategies.
IEEE Transactions onSpeech and Audio Processing, 8(1):11 ?
23.A.
Raux and M. Eskenazi.
2009.
A finite-state turn-taking model for spoken dialog systems.
In Pro-ceedings of HLT/NAACL, pages 629?637.
Associa-tion for Computational Linguistics.H.
Sacks, E.A.
Schegloff, and G. Jefferson.
1974.
Asimplest systematics for the organization of turn-taking for conversation.
Language, 50(4):696?735.R.
Sato, R. Higashinaka, M. Tamoto, M. Nakano, andK.
Aikawa.
2002.
Learning decision trees to de-termine turn-taking by spoken dialogue systems.
InICSLP, pages 861?864, Denver, CO.E.A.
Schegloff.
2000).
Overlapping talk and the orga-nization of turn-taking for conversation.
Languagein Society, 29:1 ?
63.E.
O. Selfridge and Peter A. Heeman.
2009.
A biddingapproach to turn-taking.
In 1st International Work-shop on Spoken Dialogue Systems.G.
Skantze and D. Schlangen.
2009.
Incremental di-alogue processing in a micro-domain.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 745?753.
Association for Computational Lin-guistics.N.
Stro?m and S. Seneff.
2000.
Intelligent barge-in inconversational systems.
In Sixth International Con-ference on Spoken Language Processing.
Citeseer.R.
Sutton and A. Barto.
1998.
Reinforcement Learn-ing.
MIT Press.S.
Sutton, D. Novick, R. Cole, P. Vermeulen, J. de Vil-liers, J. Schalkwyk, and M. Fanty.
1996.
Build-ing 10,000 spoken-dialogue systems.
In ICSLP,Philadelphia, Oct.M.
Walker and S. Whittaker.
1990.
Mixed initiativein dialoge: an investigation into discourse segmen-tation.
In Proceedings of the 28th Annual Meet-ing of the Association for Computational Linguis-tics, pages 70?76.M.
Walker, D. Hindle, J. Fromer, G.D. Fabbrizio, andC.
Mestel.
1997.
Evaluating competing agentstrategies for a voice email agent.
In Fifth EuropeanConference on Speech Communication and Technol-ogy.Fan Yang and Peter A. Heeman.
2010.
Initiative con-flicts in task-oriented dialogue?.
Computer SpeechLanguage, 24(2):175 ?
189.185
