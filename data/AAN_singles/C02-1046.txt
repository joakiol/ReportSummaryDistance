Translation Selection through Source Word Sense Disambiguationand Target Word SelectionHyun Ah Leeyzand Gil Chang KimyyDept.
of EECS, Korea Advanced Institute of Science and Technology (KAIST),373-1 Kusong-Dong Yusong-Gu Taejon 305-701 Republic of KoreazDaumsoft Inc., Daechi-Dong 946-12 Kangnam-Gu Seoul 135-280 Republic of Koreahalee@csone.kaist.ac.kr, gckim@cs.kaist.ac.krAbstractA word has many senses, and each sense canbe mapped into many target words.
Therefore,to select the appropriate translation with a cor-rect sense, the sense of a source word should bedisambiguated before selecting a target word.Based on this observation, we propose a hybridmethod for translation selection that combinesdisambiguation of a source word sense and se-lection of a target word.
Knowledge for transla-tion selection is extracted from a bilingual dic-tionary and target language corpora.
Dividingtranslation selection into the two sub-problems,we can make knowledge acquisition straightfor-ward and select more appropriate target words.1 IntroductionIn machine translation, translation selection isa process that selects an appropriate target lan-guage word corresponding to a word in a sourcelanguage.
Like other problems in natural lan-guage processing, knowledge acquisition is cru-cial for translation selection.
Therefore, manyresearchers have endeavored to extract knowl-edge from existing resources.As masses of language resources become avail-able, statistical methods have been attemptedfor translation selection and shown practical re-sults.
Some of the approaches have used a bilin-gual corpus as a knowledge source based on theidea of Brown et al (1990), but they are notpreferred in general since a bilingual corpus ishard to come by.
Though some latest approach-es have exploited word co-occurrence that is ex-tracted from a monolingual corpus in a targetlanguage (Dagan and Itai, 1994; Prescher et al,2000; Koehn and Knight, 2001), those methodsoften fail in selecting appropriate words becausethey do not consider sense ambiguity of a targetword.In a bilingual dictionary, senses of a word areclassied into several sense divisions and also itstranslations are grouped by each sense division.Therefore, when one looks up the translationof a word in a dictionary, she/he ought to re-solve the sense of a word in a source languagesentence, and then choose a target word amongtranslations corresponding to the sense.
In thispaper, the fact that a word has many sensesand each sense can be mapped into many tar-get words (Lee et al, 1999) is referred to as the`word-to-sense and sense-to-word' relationship,based on which we propose a hybrid method fortranslation selection.In our method, translation selection is takenas the combined problem of sense disambigua-tion of a source language word and selection ofa target language word.
To disambiguate thesense of a source word, we employ both a dic-tionary based method and a target word co-occurrence based method.
In order to select atarget word, the co-occurrence based method isalso used.We introduce three measures for translationselection: sense preference, sense probabilityand word probability.
In a bilingual dictionary,example sentences are listed for each sense di-vision of a source word.
The similarity betweenthose examples and an input sentence can serveas a measure for sense disambiguation, which wedene as sense preference.
In the bilingual dic-tionary, target words are also recorded for eachsense division.
Since the set of those words canmodel each sense, we can calculate the probabil-ity of the sense by applying the co-occurrencebased method to the set of words.
We denethe estimated probability as sense probability,which is taken for the other measure of sensedisambiguation.
Using co-occurrence, the prob-ability of selecting a word from the set of trans-lations can be calculated.
We dene it as wordprobability, which is a measure for word selec-tion.
Merging sense preference, sense probabili-ty and word probability, we compute preferencefor each translation, and then choose a targetword with the highest translation preference asa translation.2 Translation Selection based on`word-to-sense and sense-to-word'The `word-to-sense and sense-to-word' relation-ship means that a word in a source languagecould have multiple senses, each of which mightbe mapped into various words in a target lan-guage.
As shown in examples below, a Koreanverb `meok-da' has many senses (work, deaf, eat,etc.).
Also `kkae-da' has three senses (break,hatch, wake), among which the break sense of`kkae-da' is mapped into multiple words suchas `break', `smash' and `crack'.
In that case, ifbreakkkae-dawakehatchbreak smash hatch awakewake_up  (jeobsi-reul kkae-da)?
(X) hatch a dish?
(O) break a dish?
(O) smash a dish?
(O) crack a dishcrackworkmeok-daeatdeafsaw bite deaf takeeatcut?.dye have 	 (jeomsim-eul meok-da)?
(X) saw a lunch?
(?)
eat a lunch?
(O) have a lunch?
(O) take a lunch`hatch' is selected as a translation of `kkae-da' ina sentence `jeobsi-reul kkae-da', the translationmust be wrong because the sense of `kkae-da'in the sentence is break rather than hatch.
Incontrast, any word of the break sense of `kkae-da' forms a well-translated sentence.
Howev-er, selecting a correct sense does not alwaysguarantee a correct translation.
If the sense of`meok-da' in `jeomsim-eul meok-da' is correctlydisambiguated as eat but an inappropriate tar-get word is selected, an improper or unnaturaltranslation will be produced like `eat a lunch'.Therefore, in order to get a correct transla-tion, such a target word must be selected thathas the right sense of a source word and form-s a proper target language sentence.
Previousapproaches on translation selection usually tryto translate a source word directly into a targetword without considering the sense.
Thus, theyincrease the complexity of the problem and suf-TargetLanguageCorpusInputSentenceGetSensePreferenceGetWordProbabilitySense Definition &Example SentenceBilingualDictionarySense & TagetWord MappingWord-levelTranslationGetSenseProbabilityCombinespf, sp, wpWordNet Target WordCooccurrenceSenseDisambiguationWordSelectionFigure 1: Our process of translation selectionfer from the problem of knowledge acquisitionand incorrect selection of translation.In this paper, we propose a method fortranslation selection that reects `word-to-senseand sense-to-word' relationship.
We divide theproblem of translation selection into sense dis-ambiguation of a source word and selection ofa target word.
Figure 1 shows our process ofselecting translation.
We introduce three mea-sures for translation selection: sense preference(spf) and sense probability (sp) for sense disam-biguation, and word probability (wp) for wordselection.Figure 2 shows a part of an English-English-Korean Dictionary (EssEEK, 1995)1.
As shownin the gure, example sentences and denitionsentences are listed for each sense of a sourceword.
In the gure, example sentences of thedestroy sense of `break' consist of words such as`cup', `piece', and `thread', and those for the vio-late sense consist of words such as `promise' and`contract', which can function as indicators forthe sense of `break'.
We calculate sense pref-erence of a word for each sense by estimatingsimilarity between words in an input sentenceand words in example and denition sentences.Each sense of a source word can be mappedinto a set of translations.
As shown in the ex-ample, the break sense of `kkae-da' is mappedinto the set f`break', `smash', `crack'g and somewords in the set can replace each other in atranslated sentence like `break/smash/crack a1The English-English-Korean Dictionary used here isan English-Korean one, where English denitions arepaired with Korean translations.break breikvbroke, brok "envt1  cause (something) to come topieces by a blow or strain; destroy; crack; smashL 	 & ~ a cup   ~ a glass to [into] piece     ~ a thread   ~ one's arm   ~ a stick in two  I heard the rope ~.
!"
# $ %& The river broke itsbank.
'"()2  hurt; injureL *+, -./.
0&~ the skin 12 *+3  put (something) out of order; make useless byrough handling, etc.
L 345 6.78-.
0& ~ a watch 9:78 ~ a line ; < ~ the peace => 344  fail to keepor follow; act against (a rule, law, etc) ; violate; disobey?
@ "A BC  ~one's promise DE ~ a contract :D CF05 ?.Figure 2: A part of an EEK Dictionarydish'.
Therefore, we can expect the behaviorof those words to be alike or meaningfully re-lated in a corpus.
Based on this expectation,we compute sense probability by applying thetarget word co-occurrence based method to allwords in the same sense division.Word probability represents how likely a tar-get word is selected from the set of words in thesame sense division.
In the example `jeomsim-eul meok-da', the sense of `meok-da' is eat,which is mapped into three words f`eat', `have',`take'g.
Among them, `have' forms a naturalphrase while `eat' makes an awkward phrase.We could judge the naturalness from the fac-t that `have' and `lunch' co-occurs in a corpusmore frequently than `eat' and `lunch'.
In oth-er words, the level of naturalness or awkward-ness of a phrase can be captured by word co-occurrence.
Based on this observation, we usetarget word co-occurrence to get word probabil-ity.3 Calculation of each measure3.1 Sense PreferenceGiven a source word s and its k-th sense sk,we calculate sense preference spf(sk) using theequation (1).
In the equation, SNT is a set ofall content words except s in an input sentence.DEFskis a set of all content words in denitionsentences of sk, and EXskis a set of all contentwords in example sentences of sk, both of whichare extracted from the dictionary.spf(sk) =Xwi2SNTmaxwd2DEFsksim(wi; wd) (1)+Xwi2SNTmaxwe2EXsksim(wi; we)Sense preference is obtained by calculating sim-ilarity between words in SNT and words in DE-F and EX.
For all words in an input sentence(wi2SNT), we sum up the maximum similaritybetween wiand all clue words (i.e.
wdand we).To get similarity between words (sim(wi; wj)),we use WordNet and the metric proposed inRigau et al (1997).Senses in a dictionary are generally orderedby frequency.
To reect distribution of sensesin common text, we use a weight factor (sk)that is inversely proportional to the order of asense skin a dictionary.
Then, we calculatenormalized sense preference to combine sensepreference and sense probability.spfw(sk) = (sk) spf(sk)Pispf(si)(2)spfnorm(sk) =spfw(sk)Pispfw(si)(3)3.2 Sense ProbabilitySense probability represents how likely targetwords with the same sense co-occur with trans-lations of other words within the input sentence.Let us suppose that the i-th word in an inputsentence is siand the k-th sense of siis ski.Then, sense probability sp(ski) is computed asfollows:n(tkiq) =X(sj;m;c)2(si)mXp=1f(tkiq; tjp; c)f(tkiq) + f(tjp)(4)sp(ski) = p^sense(ski) =Pqn(tkiq)PxPyn(txiy)(5)In the equation, (si) signies a set of word-s that co-occur with sion syntactic relations.In an element (sj;m; c) of (si), sjis a wordthat co-occurs with siin an input sentences, cis a syntactic relation2between siand sj, andm is the number of translations of sj.
Providedthat the set of translations of a sense skiis Tkiand a member of Tkiis tkiq, the frequency of co-occurring tkiqand tjpwith a syntactic relationc is denoted as f(tkiq; tjp; c), which is extract-ed from target language corpora.3Therefore,2We guess 37 syntactic relations in English sen-tences based on verb pattern information in the dic-tionary and the results of the memory based shallowparser (Daelemans et al, 1999): subj-verb, obj-verb,modifier-noun, adverb-modifiee, etc.3When `jeobsi' has 4 translations (`plate', `dish',`saucer', `platter'), (kkae-da) has a member (jeob-n(tkiq) in the equation (4) represents how fre-quently tkiqco-occurs with translations of sj.
Bysumming up n(tkiq) for all target words in Tki, weobtain sense probability of ski.3.3 Word ProbabilityWord probability represents how frequently atarget word in a sense division co-occurs in acorpus with translations of other words withinthe input sentence.
We denote word probabilityas wp(tkiq) that is a probability of selecting tkiqfrom Tki.
Using n(tkiq) in the equation (4), wecalculate word probability as follows:wp(tkiq) = p^word(tkiq) =n(tkiq)Pxn(tkix)(6)3.4 Translation PreferenceTo select a target word among all translationsof a source word, we compute translation pref-erence (tpf) by merging sense preference, senseprobability and word probability.
We simplysum up the value of sense preference and senseprobability as a measure of sense disambigua-tion, and then multiply it with word probabilityas follows:tpf(tkiq) = (?
spfnorm(ski) + (1 ?
)sp(ski)) (7) wp(tkiq)=(Tki)In the equation, (Tki) is a normalizing factorfor wp(tkiq)4, and ?
is a weighting factor for sensepreference.
We select a target word with thehighest tpf as a translation.When all of the n(tkiq) in the equation (4) are0, we use the following equation for smoothing,which uses only frequency of a target word.n(tkiq) =  f(tkiq)Ppf(tkip)(8)si, 4, verb-obj) in the example of `jeobsi-reul kkae-da', and then the following frequencies will be used:f(break, plate, verb-obj), f(break, dish, verb-obj),...,f(crack, platter, verb-obj).4Because wp(tkiq) is a probability of tkiqwithin Tki,wp(tkiq) becomes 1 when Tkihas only one element.
(Tki)is used to make the maximum value of wp(tkiq)=(Tki) to1.
Hence, (Tki) = maxjwp(tkij).
Using (Tki), we canprevent discounting translation preference of a word, thesense of which has many corresponding target words.4 EvaluationWe evaluated our method on English-to-Koreantranslation.
EssEEK (1995) was used as a bilin-gual dictionary.
We converted this English-English Korean dictionary into a machine read-able form, which has about 43,000 entries,34,000 unique words and 80,000 senses.
Fromthe dictionary, we extracted sense denition-s, example sentences and translations for eachsense.
Co-occurrence of Korean was extract-ed from Yonsei Corpus, KAIST corpus and Se-jong Corpus using the method proposed in Yoon(2001).
The number of extracted co-occurrenceis about 600,000.To exclude any kind of human interventionduring knowledge extraction and evaluation, weevaluated our method similarly with Koehn andKnight (2001).
English-Korean aligned sen-tences were collected from novels, textbooks forhigh school students and sentences in a Korean-to-English bilingual dictionary.
Among thosesentences, we extracted sentence pairs in whichwords in the English sentence satisfy the fol-lowing condition: if the paired Korean sentencecontains a word that is dened in the dictio-nary as a translation of the English word.
Werandomly chose 945 sentences as an evaluationset, in which 3,081 words in English sentencessatisfy the condition.
Among them, 1,653 arenouns, 990 are verbs, 322 are adjectives, and116 are adverbs.We used Brill's tagger (Brill, 1995) andMemory-Based Shallow Parser (Daelemans etal., 1999) to analyze English sentences.
To an-alyze Korean sentences, we used a POS taggerfor Korean (Kim and Kim, 1996).First, we evaluated the accuracies of sensepreference (spf) and sense probability (sp).
Ifany target word of the sense with the highestvalue is included in an aligned target languagesentence, we regarded it as correct result.The accuracy of sense preference is shown inTable 1.
In the table, a w/o DEF row showsthe result produced by removing the DEF cluein the equation (1), and a w/o ORDER row isthe result produced without the weight factor(sk) of the equation (2).
As shown in the table,the result with the weight factor and without asense denition sentence is best for all cases.
Wewill discuss this result in the next section.The accuracy of sense probability is shown inTable 1: Accuracy of sense preference (spf)noun verb adj.
adv.
allw/o ORDER w/o DEF 59.23% 42.93% 45.03% 34.48% 52.47%with DEF 57.65% 40.20% 49.07% 33.62% 51.14%with ORDER5w/o DEF 66.30% 48.48% 59.32% 44.83% 59.94%with DEF 65.76% 45.56% 56.52% 42.24% 58.41%Table 2: Accuracy of sense probability (sp)noun verb adj.
adv.
allwith all cooc word 52.49% 30.49% 38.94% 59.35% 46.52%with case cooc word 49.62% 31.33% 40.12% 60.98% 45.41%Table 3: Accuracy of translation selectionnoun verb adj.
adv.
allrandom selection 11.11% 3.92% 7.41% 11.12% 6.77%1st translation of 1st sense 12.89% 12.12% 7.45% 2.59% 11.86%most frequent 46.34% 27.58% 31.99% 37.93% 38.68%spf 53.72% 38.18% 39.75% 44.83% 46.98%with sp 42.88% 19.28% 26.25% 39.02% 35.04%all wp 51.66% 31.41% 32.92% 46.55% 43.10%cooc spwp 51.97% 31.92% 33.54% 50.00% 43.62%word spfwp 55.23% 40.00% 41.30% 50.00% 50.17%(spf+sp)wp654.99% 34.55% 36.34% 50.86% 46.42%with sp 38.90% 20.20% 25.96% 37.40% 33.05%case wp 48.70% 34.04% 35.40% 45.69% 42.58%cooc spwp 49.12% 33.74% 36.34% 50.86% 43.01%word spfwp 53.60% 42.22% 42.86% 50.86% 49.93%(spf+sp)wp651.18% 36.46% 38.20% 53.45% 45.28%Table 2.
In the equation (4), we proposed to usesyntactic relations to get sense probability.
Inthe table, the result obtained by using words ona syntactic relation (with case cooc word) is com-pared to that obtained by using all words with-in a sentence (with all cooc word).
As shown,the accuracy for nouns is higher when using allwords in a sentence, whereas the accuracy forothers is higher when considering syntactic re-lations.Table 3 shows the result of translation se-lection.7We set three types of baseline - ran-dom selection, most frequent, 1st translation of1st sense8.
We conducted the experiment al-5In the case of (1st sense)=1.5, (2nd sense)=1.3,(3rd sense)=1.15, and (remainder)=16In the case of ?=0.6, which shows the best result inthe experiment.7To get spf, we use only the best combination of clues- with the weight factor and without sense denition sen-tences.8Result of selecting translation that is recorded atrst place in the rst sense division of a source word.tering the combination of spf, sp and wp.
Anspf row shows the accuracy of selecting the rsttranslation of the sense with the highest spf, andan sp row shows the accuracy of selecting therst translation of the sense with the highestsp.
A wp row shows the accuracy of using onlyword probability.
In other words, it is obtainedthrough assuming all translations of a sourceword to have the same sense.
Therefore, theresult in a wp row can be regarded as a resultproduced by the method that uses only targetword co-occurrence.
An spwp row is the resultobtained without spfnorm, and an spfwp rowis the result obtained without sp in the equa-tion (8).
An (spf+sp)wp row is the result ofcombining all measures.For the best case, the accuracy for nouns is55.23%, that for verbs is 42.22% and that foradjectives is 42.86%.
Although we use a simplemethod for sense disambiguation, our results oftranslation selection are superior to the resultsthat are produced using only target word co-occurrence (wp rows).5 DiscussionWe could summarize the advantage of ourmethod as follows: reduction of the problem complexity simplifying knowledge acquisition selection of more appropriate translation use of a mono-bilingual dictionary integration of syntactic relations guarantee of robust resultThe gure below shows the average number ofsenses and translations for an English word inEssEEK (1995).
The left half of the graph isfor all English words in the dictionary, and theright half is for the words that are marked asfrequent or signicant words in the dictionary.On the average, a frequently appearing Englishword has 2.82 senses, each sense has 1.96 Ko-rean translations, and consequently an Englishword has 5.55 Korean translations.
Moreover,in our evaluation set, a word has 6.86 senses anda word has 14.78 translations.
Most approacheson translation selection have tried to translatea source word directly into a target word, thusthe complexity of their method grows propor-tional to the number of translations per word.In contrast, the complexity of our method isproportional to only the number of senses perword or the number of translations per sense.Therefore, we could reduce the complexity oftranslation selection.When the degree of ambiguity of a source lan-guage is n and that of a target language is m,01234567all noun verb adj adv all noun verb adj advall word frequent wordsense per word translation per sense translation per wordFigure 3: The average number of senses andtranslations for an English wordthe complexity of translation is nearly nm.Therefore, knowledge acquisition for translationis more complicated than other problems of nat-ural language processing.
Although the com-plexity of knowledge acquisition of the meth-ods based on a target language corpus is on-ly m, ignorance of a source language results inselecting inappropriate target words.
In con-trast, by splitting translation selection into twosub-problems, our method uses only existing re-sources - a bilingual dictionary and a target lan-guage monolingual corpus.
Therefore, we couldalleviate the complexity of knowledge acquisi-tion to around n+m.As shown in the previous section, our methodcould select more appropriate translation thanthe method based on target word co-occurrence,although we used coarse knowledge for sensedisambiguation.
It is because the co-occurrencebased method does not consider sense ambigu-ity of a target word.
Consider the translationof the English phrase `have a car'.
A word `car'is translated into a Korean word `cha', whichhas many meanings including car and tea.
Aword `have' has many senses - posses, eat, drink,etc., among which the drink sense is mapped in-to a Korean verb `masi-da'.
Because of the teasense of `cha', the frequency of co-occuring `cha'and `masi-da' is dominant in a corpus over thatof all other translations of `car' and `have'.
Inthat case, the method that uses only word co-occurrence (wp in our experiment) translated`have a car' into an incorrect sentence `cha-reulmasi-da' that means `have a tea'.
In contrast,our method produced a correct translation `cha-reul kaji-da', because we disambiguate the senseof `have' as posses by employing sense preferencebefore using co-occurrence.In the experiment, the combination spfwpgets higher accuracy than (spf+sp)wp for mostcases.
It is due to the low accuracy of sp, whichis also caused by the ambiguity of a target wordas shown in the example of `have a car'.
Nev-ertheless, we do not think it means sense prob-ability is useless.
The accuracies of spwp arealmost better than those of wp, therefore senseprobability can work as a countermeasure whenno knowledge of a source language exists for dis-ambiguating the sense of a word.In this paper, we used a mono-bilingual dic-tionary, which has more information than acommon bilingual dictionary including sensedenitions in a source language and syntacticpatterns for the predicate.
While previous re-search on sense disambiguation that exploitsthose kinds of information has shown reliableresults, in our experiment, the use of sense def-initions lowers the accuracy.
It is likely becausewe used sense denitions too primitively.
There-fore, we expect that we could increase the ac-curacy by properly utilizing additional informa-tion in the dictionary.Many studies on translation selection haveconcerned with translation of only nouns.
Inparticular, some of them use all co-occurringwords within a sentence, and others use a few re-stricted types of syntactic relations.
Even more,each syntactic relation is utilized independently.In this paper, we proposed a statistical modelthat integrates various syntactic relations, withwhich the accuracies for verbs, adjectives, andadverbs increase.
Nevertheless, since the accu-racy for nouns is higher when using all wordsin a sentence, it seems that syntactic relationsmay be dierently used for each case.The other advantage of our method is ro-bustness.
Since our method disambiguates thesense of a source language word using reliableknowledge in a dictionary, we could avoid select-ing or generating a target word the meaning ofwhich is denitely improper in given sentenceslike `hatch a dish' for 'jeobsi-reul kkae-da' and`cha-reul masi-da' for `have a car'.6 ConclusionIn this paper, we proposed a hybrid method fortranslation selection.
By dividing translationselection into sense disambiguation of a sourceword and selection of a target word, we couldsimplify knowledge acquisition and select moreappropriate translation.AcknowledgementWe are grateful to Sabine Buchholz, BertjanBusser at Tilburg University and professor Wal-ter Daelemans at University of Antwerp for theaid to use the Memory-Based Shallow Parser(MBSP).ReferencesEric Brill.
1995.
Transformation-based error-driven learning and natural language process-ing : A case study in part of speech tagging.Computational Linguistics, 21(4).Peter F. Brown, John Cocke, Vincent DellaPietra, Stephen Della Pietra, Frederick Je-linek, John D. Laerty, Robert L. Mercer,and Paul S. Roossin.
1990.
A statistical ap-proach to machine translation.
Computation-al Linguistics, 16(2).Walter Daelemans, Sabine Buchholz, and JornVeenstra.
1999.
Memory-based shallow pars-ing.
In Proceedings of the 3rd Internation-al Workshop on Computational Natural Lan-guage Learning (CoNLL-'99).Ido Dagan and Alon Itai.
1994.
Word sense dis-ambiguation using a second language mono-ligual corpus.
Computational Linguistics,20(4).EssEEK, 1995.
Essence English-English KoreanDictionary.
MinJung SeoRim.Jae-Hoon Kim and Gil Chang Kim.
1996.Fuzzy network model for part-of-speech tag-ging under small training data.
Natural Lan-guage Engineering, 2(2).Philipp Koehn and Kevin Knight.
2001.Knowledge sources for word-level translationmodels.
In Proceedings of Empirical Method-s in Natural Language Processing conference(EMNLP-2001).Hyun Ah Lee, Jong C. Park, and Gil ChangKim.
1999.
Lexical selection with a targetlanguage monolingual corpus and an mrd.
InProceedings of the 8th International Confer-ence on Theoretical and Methodological Issuesin Machine Translation (TMI-'99).Detlef Prescher, Stefan Riezler, and MatsRooth.
2000.
Using a probabilistic class-based lexicon for lexical ambiguity resolu-tion.
In Proceedings of the 18th Internation-al Conference on Computational Linguistics(COLING-2000).German Rigau, Jordi Atserias, and Eneko A-girre.
1997.
Combining unsupervised lexi-cal knowledge methods for word sense disam-biguation.
In Proceedings of the 35th AnnualMeeting of the Association for ComputationalLinguistics (ACL-'97).Juntae Yoon.
2001.
E?cient dependency analy-sis for korean sentences based on lexical asso-ciation and multi-layered chunking.
Literaryand Linguistic Computing, 16(3).
