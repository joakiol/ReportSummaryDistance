Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1234?1243,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAntecedent Selection for Sluicing: Structure and ContentPranav AnandLinguisticsUC Santa Cruzpanand@ucsc.eduDaniel HardtIT ManagementCopenhagen Business Schooldh.itm@cbs.dkAbstractSluicing is an elliptical process where the ma-jority of a question can go unpronounced aslong as there is a salient antecedent in previ-ous discourse.
This paper considers the taskof antecedent selection: finding the correctantecedent for a given case of sluicing.
Weargue that both syntactic and discourse rela-tionships are important in antecedent selec-tion, and we construct linguistically sophis-ticated features that describe the relevant re-lationships.
We also define features that de-scribe the relation of the content of the an-tecedent and the sluice type.
We develop a lin-ear model which achieves accuracy of 72.4%,a substantial improvement over a strong man-ually constructed baseline.
Feature analysisconfirms that both syntactic and discourse fea-tures are important in antecedent selection.1 IntroductionEllipsis involves sentences with missing subparts,where those subparts must be interpretatively filledin by the hearer.
How this is possible has been a ma-jor topic in linguistic theory for decades (Sag, 1976;Chung et al, 1995; Merchant, 2001).
One widelystudied example is verb phrase ellipsis (VPE), ex-emplified by (1).
(1) Harry traveled to southern Denmark to studybotany .
Tom did too .In the second sentence (Tom did too) the verb phraseis entirely missing, yet the hearer effortlessly ?re-solves?
(understands) its content to be traveled tosouthern Denmark to study botany.Another widely studied case of ellipsis is sluicing,in which the majority of a question is unpronounced,as in (2).
(2) Harry traveled to southern Denmark to studybotany .
I want to know why .Here the content of the question, introduced by theWH-phrase why, is missing, yet it is understood bythe hearer to be why did Harry travel to southernDenmark to study botany?.
In both of these cases,ellipsis resolution is made possible by the presenceof an antecedent, material in prior discourse that, in-formally speaking, is equivalent to what is missing.Ellipsis poses an important challenge for manyapplications in language technology, as variousforms of ellipsis are known to be frequent in a va-riety of languages and text types.
This is perhapsmost evident in the case of question-answering sys-tems, since elliptical questions and elliptical answersare both very common in discourse.
A computa-tional system that can effectively deal with ellipsisinvolves three subtasks (Nielsen, 2005): ellipsis de-tection, in which a case of ellipsis is identified, an-tecedent selection, in which the antecedent for a caseof ellipsis is found, and ellipsis resolution, where thecontent of the ellipsis is filled in with reference tothe antecedent and the context of the ellipsis.
Here,we focus on antecedent selection for sluicing.
Inaddressing this problem of antecedent selection, wemake use of a newly available annotated corpus ofsluice occurrences (Anand and McCloskey, 2015).This corpus consists of 4100 automatically parsedand annotated examples from the New York Timessubset of the Gigaword Corpus, of which 2185 are1234publicly available.Sluicing antecedent selection might appear simple?
after all, it typically involves a sentential expres-sion in the nearby context.
However, analysis of theannotated corpus data reveals surprising ambiguityin the identification of the antecedent for sluicing.In what follows, we describe a series of algo-rithms and models for antecedent selection in sluic-ing.
Following section 2 on background, we de-scribe our dataset in section 3.
Then in section 4,we describe the structural factors that we have iden-tified as relevant for antecedent selection.
In sec-tion 5, we look at ways in which the content of thesluice and the content of the antecedent tend to berelated to each other: we address lexical overlap,as well as the probabilistic relation of head verbs toWH-phrase types, and the relation of correlate ex-pressions to sluice types.
In section 6 we presenttwo manually constructed baseline classifiers, andthen we describe an approach to automatically tun-ing weights for the complete set of features.
In sec-tion 7 we present the results of these algorithms andmodels, including results involving various subsetsof features, to better understand their contributionsto the overall results.
Finally in section 8 we discussthe results in light of plans for future work.2 Background2.1 Sluicing and ellipsisSluicing is formally defined in theoretical linguis-tics as ellipsis of a question, leaving only a WH-phrase remnant.
While VPE is licensed only bya small series of auxiliaries (e.g., modals, do, seeLobeck (1995)), sluicing can occur wherever ques-tions can, both in unembedded ?root?
environments(e.g., Why?)
or governed by the range of expres-sions that embed questions, like know in (2).
Sluic-ing is argued to be possible principally in contextswhere there is uncertainty or vagueness about an is-sue (Ginzburg and Sag, 2000).
In some cases, thismanifests as a correlate, an overt indefinite expres-sion whose value is not further specified, like one ofthe candidates in (3).
But in many others, like that in(2) or (4), there is no correlate, and the uncertaintyis implicit.
(3) They ?ve made an offer to [cor one of the can-didates ] , but I ?m not sure which one(4) They were firing , but at what was unclearThe existence of correlate-sluices suggests an obvi-ous potential feature type for antecedent detection.However, the annotated sluices in (Anand and Mc-Closkey, 2015) have correlates only 22% of the time,making this process considerably harder.
We returnto the question of correlates in section 5.1.2.2 Related WorkThe first large-scale study of ellipsis is due to Hardt(1997), which addresses VPE.
Examining 644 casesof VPE in the Penn Treebank, Hardt presents amanually constructed algorithm for locating the an-tecedent for VPE, and reports accuracy of 75% to94.8%, depending on whether the metric used re-quires exact match or more liberal overlap or con-tainment.
Several preference factors for choosingVPE antecedents are identified (Recency, ClausalRelations, Parallelism, and Quotation).
One of thecentral components of the analysis is the identifi-cation of structural constraints which rule out an-tecedents that improperly contain the ellipsis site,an issue we also address here for sluicing.
Draw-ing on 1510 instances of VPE in both the BritishNational Corpus (BNC) and the Penn Treebank,Nielsen (2005) shows that a maxent classifier usingrefinements of Hardt?s features can achieve roughlysimilar results to Hardt?s, but that additional lexicalfeatures do not help appreciably.Nielsen chooses to optimize for Hardt?s HeadOverlap metric, which assigns success to any candi-date containing/contained in the correct antecedent.There are thus many ?correct?
antecedents for agiven instance of VPE, which mitigates the class im-balance problem.
However, the approach does notprovide a way to discriminate between these con-taining candidates, an important step in the eventualgoal of resolving the ellipsis.There is no similar work on antecedent selec-tion for sluicing, though there have been small-scale corpora gathered for sluices (Nykiel, 2010;Beecher, 2008).
In addition, Fernandez et al (2005)build rule-based and memory-based classifiers forthe pragmatic import of root (unembedded) sluicesin the BNC, based on the typology of Ginzburg andSag (2000).
Using features for the type of WH-1235phrase, markers of mood (declarative/interrogative)and polarity (positive/negative) as well as the pres-ence of correlate-like material (e.g., quantifiers, defi-nites, etc.
), they can diagnose the purpose of a sluicein a dataset of 300 root sluices with 79% averageF-score, a 5% improvement over the MLE.
Fernan-dez et al (2007) address the problem of identify-ing sluices and other non-sentential utterances.
Wedon?t address that problem in the current work.
Fur-thermore, Fernandez et al (2007) and Fernandezet al (2008) address the general problem of non-sentential utterances or fragments in dialogue, in-cluding sluices.
Sluicing in dialogue differs fromsluicing in written text in various ways: there is ahigh proportion of root sluices, and antecedent se-lection is likely mitigated by the length of utterancesand the order of conversation.
As we discuss, manyof our newswire sluices evince difficult patterns ofcontainment inside the antecedent (particularly whatwe call interpolated and cataphoric sluices), and itdoes not appear from inspection that root sluicesever participate in such processes.Looking more generally, there is an obvious po-tential connection between antecedent selection forellipsis and the problem of coreference resolution(see Hardt (1999) for an explicit theoretical link be-tween the two).
However, entity coreference reso-lution is a problem with two major differences fromellipsis antecedent detection: a) the antecedent andanaphor often share a variety of syntactic, semantic,and morphological characteristics that can be featu-rally exploited; b) entity expressions in a text are of-ten densely coreferent, which can help provide prox-ies for discourse salience of an entity.In contrast, abstract anaphora, particularly dis-course anaphora (this/that anaphora to somethingsentential), may offer a more parallel case to ours.Here, Kolhatkar et al (2013) use a combinationof syntactic type, syntactic/word context, length,and lexical features to identify the antecedents ofanaphoric shell nouns (this fact) with precisionfrom 0.35-0.72.
Because of the sparsity of thesecases, Kolhatkar et al use Denis and Baldridge?s(2008) candidate ranking model (versus a standardmention-pair model (Soon et al, 2001)), in whichall potential candidates for an anaphor receive a rela-tive rank in the overall candidate pool.
In this paper,we will pursue a hillclimbing approach to antecedentselection, inspired by the candidate ranking scheme.3 Data3.1 The Annotated DatasetOur dataset, described in Anand and McCloskey(2015), consists of 4100 sluicing examples from theNew York Times subset of the Gigaword Corpus,2nd edition.
This dataset is the first systematic, ex-haustive corpus of sluicing.1 Each example is an-notated with four main tags, given in terms of tokensequence offsets: the sluice remnant, the antecedent,and then inside the antecedent the main predicateand the correlate, if any.
The annotations also pro-vide a free-text resolution.
Of the 4100 annotated,2185 sluices have been made publicly available; weuse that smaller dataset here.
We make use of theannotation of the antecedent and remnant tags.
SeeAnand and McCloskey (2015) for additional infor-mation on the dataset and the annotation scheme.For the feature extraction in section 4, we rely onthe the token, parsetree, and dependency parse in-formation in Annotated Gigaword (extracted fromStanford CoreNLP).3.2 Defining the Correct AntecedentBecause of disagreements with the automatic parsesof their data, Anand and McCloskey (2015) hadannotators tag token sequences, not parsetree con-stituents.
As a result, 10% of the annotations are notsentence-level (i.e., S, SBAR, SBARQ) constituents,such as the VP antecedent in (5), and 15% are notconstituents at all, such as the case of (6), where theparse lacks an S node excluding the initial tempo-ral clause.
We describe two different ways to definewhat will count as the correct antecedent in buildingand assessing our models.3.2.1 Constituent-Based AccuracyLinguists generally agree that the antecedent forsluicing is a sentential constituent (see Merchant(2001) and references therein).
Thus, it is straight-forward to define the antecedent as the minimal14100 sluices works out to roughly 0.14% of WH-phrasesin the NYT portion of Gigaword.
However, note that this in-cludes all uses of WH-phrases (e.g., clefts and relative clauses),whereas sluicing is only possible for WH-questions.
It?s notclear how many questions there are in the dataset (distinguish-ing questions and other WH-phrases is non-trivial).1236sentence-level constituent containing the token se-quence marked as the antecedent.
Then we defineCONACCURACY as the percentage of cases in whichthe system selects the correct antecedent, as definedhere.While it is linguistically appealing to uniformlydefine candidates as sentential constituents, the an-notator choices are sometimes not parsed that way,as in the following examples:(5) ?
I do n?t know how , ?
said Mrs. Kitayeva ,?
but [S we want [V P to bring Lydia home] , in any condition ] .
?
(6) [S [SBAR When Brown , an all-Americatight end , was selected in the first round in1992 ] he was one of the highest rated play-ers on the Giants ?
draft board ]In such cases, there is a risk that we will not accu-rately assess the performance of our systems, sincethe system choice and annotator choice will onlypartially overlap.3.2.2 Token-Based Precision and RecallHere we define a metric which calculates the pre-cision and recall of individual token occurrences,following Bos and Spenader (2011) (see also Kol-hatkar and Hirst (2012)).
This will accurately re-flect the discrepancy in examples like (5) ?
accord-ing to ConAccuracy, a system choice of we want tobring Lydia home in any condition is simply con-sidered correct, as it is the smallest sentential con-situent containing the annotator choice.
Accordingto the Token-Based metric, we see that the systemachieves recall of 1; however, since the system in-cludes six extraneous tokens, precision is .4.
We de-fine TOKF as the harmonic mean of Token-BasedPrecision and Recall; for (5), TokF is .57.3.3 Development and Test DataThe dataset consists of 2185 sluices extracted fromthe New York Times between July 1994 and De-cember 2000.
For feature development, we seg-mented the data into a development set (DS) of the453 sluices from July 1994 to December 1995.
Theexperiments in section 6 were carried out on a testset (TS) of the 1732 sluices in the remainder of thedataset, January 1996 to December 2000.4 StructureUnder our assumptions, the candidate antecedentset for a given sluice is the set of all sentence-level parsetree constituents within a n-sentence ra-dius around the sluice sentence (based on DS, weset n = 2).
Because sentence-level constituents em-bed, in DS there are on average 6.4 candidate an-tecedents per sluice.
However, because ellipsis res-olution involves identification of an antecedent, weassume that it, like anaphora resolution, should besensitive to the overall salience of the antecedent.This means that there should be, in principle, proxiesfor salience that we can exploit to diagnose the plau-sibility of a candidate for sluicing in general.
Weconsider four principle kinds of proxies: measuresof candidate-sluice distance, measures of candidate-sluice containment, measures of candidate ?mainpoint?, and candidate-sluice discourse relation mark-ers.4.1 DistanceWithin DS, 63% of antecedents are within the samesentence as the sluice site, and 33% are in the im-mediately preceding sentence.
In terms of candi-dates, the antecedent is on average the 5th candidatefrom the end of the n-sentence window.
The pos-itive integer-valued feature DISTANCE tracks thesenotions of recency, where DISTANCE is 1 if the can-didate is the candidate immediately preceding or fol-lowing the sluice site (DISTANCE is defined to be 0only for infinitival Ss like S0 in (7) below).
The fea-ture FOLLOWS marks whether a candidate followsthe sluice.4.2 ContainmentAs two-thirds of the antecedents are in the samesentence as the sluice, we need measures to distin-guish the candidates internal to the sentence con-taining the sluice.
In general, we want to excludeany candidate that ?contains?
(i.e., dominates) thesluice, such as S0 and S-1 in (7).
One might havethought that we want to always exclude the entiresentence (here, S-4) as well, but there are severalcases where the smallest sentence-level constituentcontaining the annotated antecedent dominates thesluice, including: parenthetical sluices inside the an-tecedent (8), sluices in subordinating clauses (9), or1237sluice VPs coordinated with the antecedent VP (10).We thus need features to mark when such candidatesare ?non-containers?.
(7) [S?4 [S?3 I have concluded that [S?2 I cannot support the nomination ] , and [S?1 Ineed [S0 to explain why ] ].
](8) [S?2 A major part of the increase in coverage, [S?1 though Mitchell ?s aides could not sayjust how much , ] would come from a pro-vision providing insurance for children andpregnant women .
](9) [S?3 Weltlich still plans [S?2 to go , [S?1although he does n?t know where ] ] ](10) [S?2 State regulators have ordered 20thCentury Industries Inc. [S?1 to begin pay-ing $ 119 million in Proposition 103 rebatesor explain why not by Nov. 14 .
]]Conceptually, what renders S-3 in (9), S-2 in (8),and S-1 in (10) non-containers is that in all threecases the sluice is semantically dissociable from therest of the sentence.
We provide three features tomark this.
First, the boolean feature SLUICEINPAR-ENTHETICAL marks when the sluice is dominatedby a parenthetical (a PRN node in the parse or an(al)though SBAR delimited by punctuation).
Sec-ond, SLUICEINCOORDVP marks the configurationexemplified (10).We also compute a less structure-specific mea-sure of whether the candidate is meaningful oncethe sluice (and material dependent on it) is removed.This means determining, for example, that S-4 in (7)is meaningful once to explain why .
is removed butS-1 is not.
But the latter result follows from thefact that the main predicate of S-1, need takes thesluice govering verb explain as an argument, andhence removing that argument renders it semanti-cally incomplete.
We operationalize this in termsof complement dependency relations.
We first lo-cate the largest subgraph containing the sluice in achain of ccomp and xcomp relations.
This gives usgovmax, the highest such governor (i.e., explain) inFig.
1.
The subgraph dependent on govmax is thenremoved, as indicated by the grayed boxes in Fig 1.If the resulting subgraph contains a verbal governor,the candidate is meaningful and CONTAINSSLUICEis false.
By this logic, S-4 in (7) is meaningful be-cause it contains concluded, but S-1 is not, becausethere is no verbal material remaining.4.3 Discourse StructureIt has often been suggested (Asher, 1993; Hardt,1997; Hardt and Romero, 2004) that the antecedentselection process is very closely tied to discourse re-lations, in the sense that there is a strong preferenceor even a requirement for a discourse relation be-tween the antecedent and ellipsis.Here we define several features that indicate eitherthat a discourse relation is present or is not present.We begin with features indicating that a dis-course relation is not present: the theoretical lin-guistics literature on sluicing has noted that an-tecedents not in the ?main point?
of an assertion (e.g.,ones in appositives (AnderBois, 2014) or relativeclauses (Cantor, 2013)) are very poor antecedentsfor sluices, presumably because their content is notvery salient.
The boolean features CANDINPAREN-THETICAL (determined as for the sluice above) andCANDINRELCLAUSE mark these patterns.2We also define features that would tend to indicatethe presence of a discourse relation.
These have todo with antecedents that occur after the sluice.
Al-though antecedents overwhelmingly occur prior tosluices, we observe one prominent cataphoric pat-tern in DS, where the sentence containing the sluiceis coordinated with a contrastive discourse relation;this is exemplified in (11).
(11) ?
I do n?t know why , but I like JimmyCarter .
?Three features are designed to capture this pattern:COORDWITHSLUICE indicates whether the sluiceand candidate are connected by a coordination de-pendency, AFTERINITIALSLUICE marks the con-junctive condition where the candidate follows asluice initial in its sentence, and IMMEDAFTERINI-TIALSLUICE marks a candidate that is the closestfollowing candidate to an initial sluice.2This feature might be seen as an analog to the appositionfeatures used in nominal coreference resolution (Bengtson andRoth, 2008), but there it is used to link appositives, whereashere it is to exclude candidates.1238I have concluded that ... the nomination , and I need to explain whyVBN govmaxnsubjauxconj:andccnsubjxcompmark ccompI need to explain whygovmaxnsubjxcompmark ccompFigure 1: Sluice containment for S-4 and S-1 in (7).
Starting at the governor of the sluice, explain, find govmax need and delete itstransitive dependents.
The candidate does not contain the sluice if the remaining graph contains verbal governors.5 ContentIn addition to the structural features above, we alsocompute several features relating the content of thesluice site and the antecedent.
The intuition be-hind these relational features is the following: eachsluice type (why, who, how much, etc.)
representsa certain type of question, and each candidate rep-resents a particular type of predication.
For a givena sluice type, some predications might fit more nat-urally than others.
More generally, it is a commonview that an elliptical expression and its antecedentcontain matching ?parallel elements?.3Below we describe three approaches to this: onesimply looks for lexical overlap ?
words that occurboth in the sluice expression and in the candidate.The second involves a more general notion of how apredication fits with a sluice type.
To capture this,we gather co-occurrence counts of main verb andsluice types.
The third approach compares potentialcorrelates in candidates with the type of sluice.5.1 OverlapOne potential candidate for overlap information isthe presence of a correlate in the antecedent.
How-ever, 75% of of sluices involve WH-phrases that typ-ically involve no correlate (e.g., how, when, why).The pertinent exception to this are extent sluices (ones where the remnant is how (much|many|JJ)),which have been argued to heavily favor a correlate(Merchant, 2001), such as (12) below (though see(13) for a counterexample).3This term is from Dalrymple et al (1991); a similar gen-eral view about parallelism in ellipsis arises in many differenttheories, such as Pru?st et al (1994) and Asher (1993).
(12) The 49ers are [corr very good ] .It ?s hard to know how good because theCowboys were the only team in the leaguewho could test them .We thus compute the number of tokens of OVER-LAP between the content terms in the WH-phrasesluice (non-WH, non prepositional) and the entireantecedent.5.2 Wh-PredicateEven for correlate-less sluices, the WH-phrase mustsemantically cohere with the main predicate of theantecedent.
Thus, in (13), S-3 is a more likely an-tecedent than S-2 because increase is more likely totake an implicit extent than predict.
Although wecould have consulted a lexically rich resource (e.g,VerbNet, FrameNet), our hope was that this generalapproach could carry over to less argument-specificcombinations such as how with complete and raisein (14).
(13) [S?3 Deliveries would increase as a resultof the acquisition ] , [S?2 he predicted ] ,but [S?1 he would not say by how much ](14) [S?4 [S?3 Once the city and team completea contract ] , the Firebirds will begin to raise$ 9 million ] , [S?2 team president Yountsaid ] , [S?1 but he would not say how ] .Our assumption is that some main predicates aremore likely than others for a given sluice type, andwe wish to gather data that reveals these probabil-ities.
This is somewhat similar to the approach ofHindle and Rooth (1993), who gather probabilities1239that reflect the association of verbal and nominalheads with prepositions to disambiguiate preposi-tional phrase attachment.One way to collect these would be to use oursluicing data, which consists of a total of 2185 an-notated examples.
However, the probabilities of in-terest are not about sluicing per se.
Rather, they areabout how well a given predication fits with a giventype of question.
Thus instead of using our com-paratively small set of annotated sluicing examples,we used overt WH-constructions in Gigaword toobserve cooccurrences between question types andmain predicates.
To find overt WH-constructions,we extracted all instances where a WH-phrase is:a) a dependent (to exclude cases like Who?)
andb) not at the right edge of a VP (to exclude sluiceslike know who, per Anand and McCloskey (2015)).To further ensure that we were not overlapping withour dataset, we did this only for the non=NYT sub-sets of Gigaword (i.e., AFP, APW, CNA, and LTW).This procedure generated 687,000 WH-phrase in-stances, and 79,753 WH-phrase-governor bigramtypes.
From these bigrams, we calculated WH-PREDICATE, the normalized pmi of WH-phrase typeand governor lemma in Annotated Gigaword.5.3 Correlate OverlapTwenty-two percent of our data has correlates, andthese correlates should be discriminative for partic-ular sluice types.
For example, temporal (when)sluices have timespan correlates (e.g., tomorrow,later), while entity (who/what) sluices have individ-uals as correlates (e.g., someone, a book).
We ex-tracted four potential integer-valued correlate fea-tures from each candidate: LOCATIVECORR is thenumber of primarily locative prepositions (thosewith a locative MLE in The Preposition Project(Litowski and Hargraves, 2005)).
ENTITYCORR isthe number of nominals in the candidate that are in-definite (bare nominals or ones with a determiner re-lation to a, an and weak quantifiers (some, many,much, few, several).TEMPORALCORR is the num-ber of lexical patterns in the candidate for TIMEX3annotations in Timebank 1.2 (Pustejovesky et al,2016).
WHICHCORR is the pattern for entities plusor.distance DISTANCE, FOLLOWScontainment CONTAINSSLUICEISDOMINATED-BYSLUICEdiscourse structure COORDWITHSLUICE,AFTERINITIALSLUICE,IMMEDAFTERINI-TIALSLUICE, CAND-INPARENTHETICAL,CANDINRELCLAUSEcontent OVERLAP, WH-PREDICATEcorrelate LOCATIVECORR,ENTITYCORR, TEM-PORALCORR, WHICH-CORRTable 1: Summary of features used in experiments.6 AlgorithmsMention-pair coreference models reduce corefer-ence resolution to two steps: a local binary clas-sification, and a global resolution of coreferencechains.
We may see antecedent selection as a sim-ilar two-stage process: classification on the proba-bility a given candidate is an antecedent, and thenselection of the most likely candidate for a givensluice.
As Denis and Baldridge (2008) note, onelimitation of this approach is that the overall rankof the candidates is never directly learned.
Theyinstead propose to learn the rank of a candidate cfor antecedent a, modeled as the log-linear scoreof a candidate across a set of coreference modelsm, (exp?j wjmj(c, a)), normalized by the sum ofcandidate scores.
We apply the same approach toour problem, viewing each feature in Table 1 as amodel, and estimating weights for the features byhill-climbing.
We begin by defining constructedbaselines which are implemented by manually as-signing weights.
We then consider the results of amaxent classifier over the features.
Finally, we de-termine the weights directly by hill-climbing withrandom restarts.6.1 Manual BaselinesRandom simply selects candidates at random.
Clstchooses the closest candidate that starts before thesluice.
This is done by assigning a weight of -1to DISTANCE and -10 to FOLLOWING (to exclude1240the following candidate), and 0 to all other fea-tures.
ClstBef chooses the closest candidate that en-tirely precedes the sluice (i.e., starts before and doesnot contain the sluice site).
To construct ClstBef,we change the weight of CONTAINSSLUICE to -10,which means that candidates containing the sluicewill never be chosen.6.2 A maxent modelWe trained a maxent classifier on the features in Ta-ble 1 for the binary antecedent-not antecedent task.With 10-fold cross-validation on the test set, themaxent model achieved an average accuracy on thebinary antecedent task of 87.1 and an F-score of53.8 (P=63.9, R=46.5).
We then constructed an an-tecedent selector that chose the candidate with thehighest classifier score.6.3 Hill-ClimbingWe define a procedure to hill-climb over weightsin order to maximize ConAccuracy over the entiretraining set (maximizing TokF yielded similar re-sults, and is not reported here).
Weights are initial-ized with random values in the interval [-10,10].
Atiteration i, the current weight vector is compared toalternatives differing from it by the current step sizeon one weight, and the best new vector is selected.For the results reported here, we performed 13 ran-dom restarts and exponential step size 10?i.5 (valuesthat maximized performance on the DS).7 ResultsWe performed 10-fold cross-validation over TS onthe hill-climbed and maxent models above, produc-ing average ConAccuracy and TokF as shown in Ta-ble 2, which also gives results of the three base-lines on the entire dataset.
The hill-climbed ap-proach with all features substantially outperformedthe baselines, achieving a ConAccuracy of 72.4%.We investigated the performance of our hill-climbing procedure with ablation of several featuresubsets.
We ablated features by group, as in Table 1.Table 2 shows the results for using four groups andonly one group, as well as the top two three groupand two group combinations.Features fall in three tiers.
Distance features arethe most predictive: all the top systems use them,and they alone perform reasonably well (like Clst).A:Tr F:Tr A:Tes F:TesHC-DCSNR 73.8 72.4 72.4 71.5HC-CSNR 41.8 51.8 40.3 51.6HC-DCSR 72.9 71.6 72.1 71.0HC-DSNR 53.5 59.1 52.7 58.3HC-DCNR 65.8 67.1 64.6 65.9HC-DCSN 73.3 72.1 72.7 71.8HC-DCS 72.7 71.5 72.5 71.3HC-DCN 65.6 67.8 64.3 66.8HC-DC 63.3 65.4 63.0 65.4HC-DS 51.2 57.2 50.9 57.1HC-D 41.6 51.6 41.5 51.6HC-C 30.6 45.1 28.9 45.3HC-S 30.7 43.0 27.0 42.0HC-N 30.5 38.6 30.7 38.2HC-R 23.6 35.9 22.2 33.1Maxent 65.3 70.2 64.2 68.0Random 19.4 44.1 19.5 46.3Clst 41.2 52.1 na naClstBef 56.5 67.9 na naTable 2: Average (Con)A(ccuracy) and (Tok)F(-Score) forTr(ain) and Tes(t) splits on 10-fold cross-validation of data.Feature groups: Distance, Containment, Discourse Structure,coNtent, coRrelate.
(Red marks results not significantly differ-ent (via paired t-test) from HC-DCSNR.
)Containment and then Discourse Structure featuresare the next most helpful.
The full system has aConAccuracy of 72.4 on the TS, not reliably dif-ferent from several systems without Content and/orCorrelate features.
At the same time, the scores forthese feature types on their own show that they arepredictive of the antecedent: The Correlate featureR has a score of 22.2, which is a rather modest, butstatistically significant, improvement over Random.The Content feature N improves quite substantially,up to 30.7.
This suggests that there is some redun-dancy with the other features, so that the contribu-tions of Content and Correlate are not observed incombination with them.
(HC-N and HC-R?s lowerthan Random TokF is a result of precision: Randommore often selects very small candidates inside thecorrect antecedent, leading to a higher precision.
)The Content and Correlate features concern re-lations between the type of sluice and the contentof the antecedent; since other features do not cap-ture this, it is puzzling that these provide no fur-ther improvement.
To better understand why thisis, we investigated the performance of our feature1241sets by sluice type.
For the top performing systems,we found that antecedent selection for sluices overextents (e.g, how much, how tall) performed 11%better than average and those over reasons (why)and manner (how) performed 13% worse than aver-age; no other WH-phrase types differed significantlyfrom average.
Importantly, this finding was consis-tent even for the systems without Content or Cor-relate features, which we extracted in large part tohelp highlight possible correlate material for extentsluices as well as entity (who/what) and temporal(when) sluices.We also examined systems knocking out our bestperforming features, Distance, Containment, andDiscourse Structure.
When Distance features wereomitted, we saw a bimodal distribution: reason andmanner sluice antecedent selection was 31% betterthan expected (based on the full system differencesdiscussed above), and the other sluices performed22% worse.
When Containment features were omit-ted, reason sluices performed 10% better than ex-pected, while extent ones were 10% worse.
Finally,when Discourse Structure features were removed,entity and temporal sluices had half the error ratewe would expect.
While it is hard to provide a cleartakeaway from these differences, they do point tothe relative difficulty in locating sluice antecedentsbased on WH-phrase type, and they also suggest thatdifferent sluice types present quite different chal-lenges.
This suggests that one promising line mightbe to learn different featural weights for each sluicetype.8 ConclusionWe have addressed the problem of sluicing an-tecedent selection by defining linguistically sophis-ticated features describing the structure and contentof candidates.
We described a hill-climbed modelwhich achieves accuracy of 72.4%, a substantial im-provement over a strong manually constructed base-line.
We have shown that both syntactic and dis-course relationships are important in antecedent se-lection.
In future work, we hope to improve the per-formance of several of our features.
Notable amongthese are the discourse structural proxies we foundto make a contribution to the model.
These featuresconstitute a quite limited view of discourse struc-ture, and we suspect that a better representation ofdiscourse structure might well lead to further im-provements.
One potential path would be to lever-age data where discourse relations are explicitly an-notated, such as that in the Penn Discourse Treebank(Prasad et al, 2008).
In addition, although our Con-tent and Correlate features were not useful alongsidethe others, we hope that more refined versions ofthose could provide some assistance.
We also notedthat our performance was impacted by WH-types,and therefore it might be helpful to learn differentfeatural weights per type.In closing, we would like to return to the largerquestion of effectively handling ellipsis.
The solu-tion to antecedent selection that we have presentedhere provides a starting point for addressing theproblem of resolution, in which the content of thesluice is filled in.
However, even if the correct an-tecedent is selected, the missing content is not al-ways an exact copy of the antecedent ?
often sub-stantial modifications will be required ?
and an ef-fective resolution system will have to negotiate suchmismatches.
As it turns out, many incorrect an-tecedents differ from the correct antecedent in wayshighly reminiscent of these mismatches.
Thus, someof the errors of our selection algorithm may be mostnaturally addressed by the resolution system, and itmay be that the relative priority of the specific chal-lenges we identified here will become clearer as weaddress the next step down in the overall pipeline.AcknowledgmentsWe gratefully acknowledge the work of Jim Mc-Closkey in helping to create the dataset investi-gated here, as well as the principal annotators onthe project.
We thank Jordan Boyd-Graber, EllenRiloff, and three incisive reviewers for helpful com-ments.
This research has been sponsored by NSFgrant number 1451819.ReferencesPranav Anand and Jim McCloskey.
2015.
Annotatingthe implicit content of sluices.
In The 9th LinguisticAnnotation Workshop held in conjuncion with NAACL2015, page 178.Scott AnderBois.
2014.
The semantics of sluicing: Be-yond truth conditions.
Language, 90(4):887?926.1242Nicholas Asher.
1993.
Reference to Abstract Objects inEnglish.
Dordrecht.Henry Beecher.
2008.
Pramatic inference in the interpre-tation of sluiced Prepositional Phrases.
In San DiegoLinguistic Papers, volume 3, pages 2?10.
Departmentof Linguistics, UCSD, La Jolla, California.Eric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 294?303.
Asso-ciation for Computational Linguistics.Johan Bos and Jennifer Spenader.
2011.
An annotatedcorpus for the analysis of vp ellipsis.
Language Re-sources and Evaluation, 45(4):463?494.Sara Cantor.
2013.
Ungrammatical double-island sluic-ing as a diagnostic of left-branch positioning.S.
Chung, W. Ladusaw, and J. McCloskey.
1995.
Sluic-ing and logical form.
Natural Language Semantics,3:1?44.Mary Dalrymple, Stuart Shieber, and Fernando Pereira.1991.
Ellipsis and higher-order unification.
Linguis-tics and Philosophy, 14(4), August.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669.Raquel Ferna?ndez, Jonathan Ginzburg, and ShalomLappin.
2005.
Automatic bare sluice disambiguationin dialogue.
In Proceedings of the IWCS-6 (Sixth In-ternational Workshop on Computational Semantics),pages 115?127, Tilburg, the Netherlands, January.Available at:http://www.dcs.kcl.ac.uk/staff/lappin/recent_papers_index.html.Raquel Ferna?ndez, Jonathan Ginzburg, and Shalom Lap-pin.
2007.
Classifying non-sentential utterances in di-alogue: A machine learning approach.
ComputationalLinguistics, 33(3):397?427.Raquel Ferna?ndez, Jonathan Ginzburg, Howard Gregory,and Shalom Lappin.
2008.
Shards: Fragment resolu-tion in dialogue.
In Computing Meaning, pages 125?144.
Springer.Jonathan Ginzburg and Ivan Sag.
2000.
InterrogativeInvestigations: The Form, Meaning and Use of EnglishInterrogatives.
CSLI Publications, Stanford, Calif.Daniel Hardt and Maribel Romero.
2004.
Ellipsisand the structure of discourse.
Journal of Semantics,24(5):375?414.Daniel Hardt.
1997.
An empirical approach to vp ellip-sis.
Computational Lingusitics, 23(4):525?541.Daniel Hardt.
1999.
Dynamic interpretation of verbphrase ellipsis.
Linguistics & Philosophy, 22(2):187?221.Donald Hindle and Mats Rooth.
1993.
Structural ambi-guity and lexical relations.
Computational linguistics,19(1):103?120.Varada Kolhatkar and Graeme Hirst.
2012.
Resolv-ing ?this-issue?
anaphora.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 1255?65.Varada Kolhatkar, Heike Zinmeister, and Graeme Hirst.2013.
Interpreting anaphoric shell nouns using an-tecedents of ctaphoric shell nouns as training data.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing.Ken Litowski and Orin Hargraves.
2005.
The prepo-sition project.
In ACL-SIGSEM Workshop on ?TheLinguistic Dimension of Prepositions and Their Usein Computational Linguistic Formalisms and Applica-tions, pages 171?179.Anne Lobeck.
1995.
Ellipsis: Functional heads, licens-ing and identification.
Oxford University Press.Jason Merchant.
2001.
The syntax of silence: Sluicing,islands, and identity in ellipsis.
Oxford.Leif Nielsen.
2005.
A Corpus-Based Study of VerbPhrase Ellipsis Identification and Resolution.
Ph.D.thesis, King?s College London.Johanna Nykiel.
2010.
Whatever happened to Old En-glish sluicing.
In Robert A. Cloutier, Anne MarieHamilton-Brehm, and Jr. William A. Kretzschmar, ed-itors, Studies in the History of the English Language V:Variation and Change in English Grammar and Lexi-con: Contemporary Approaches, pages 37?59.
Walterde Gruyter.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie LWebber.
2008.
The penn discourse treebank 2.0.
InLREC.
Citeseer.Hub Pru?st, Remko Scha, and Martin van den Berg.
1994.A discourse perspective on verb phrase anaphora.
Lin-guistics and Philosophy, 17(3):261?327.James Pustejovesky, Marc Verhagen, Roser Sauri, Jes-sica Littman, Robert Gaizauskas, Graham Katz, Inder-jeet Mani, Robert Knippen, and Andrea Setzer.
2016.Timebank 1.2.
LDC2006T08, April.Ivan A.
Sag.
1976.
Deletion and Logical Form.
Ph.D.thesis, Massachusetts Institute of Technology.
(Pub-lished 1980 by Garland Publishing, New York).W.
M. Soon, H.T.
Ng, and D. C. Y Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Computational Linguistics, 27(4):521?44.1243
