Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 17?22,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Feature-Rich Constituent Context Model for Grammar InductionDave GollandUniversity of California, Berkeleydsg@cs.berkeley.eduJohn DeNeroGoogledenero@google.comJakob UszkoreitGoogleuszkoreit@google.comAbstractWe present LLCCM, a log-linear variant of theconstituent context model (CCM) of grammarinduction.
LLCCM retains the simplicity ofthe original CCM but extends robustly to longsentences.
On sentences of up to length 40,LLCCM outperforms CCM by 13.9% brack-eting F1 and outperforms a right-branchingbaseline in regimes where CCM does not.1 IntroductionUnsupervised grammar induction is a fundamentalchallenge of statistical natural language processing(Lari and Young, 1990; Pereira and Schabes, 1992;Carroll and Charniak, 1992).
The constituent con-text model (CCM) for inducing constituency parses(Klein and Manning, 2002) was the first unsuper-vised approach to surpass a right-branching base-line.
However, the CCM only effectively modelsshort sentences.
This paper shows that a simple re-parameterization of the model, which ties togetherthe probabilities of related events, allows the CCMto extend robustly to long sentences.Much recent research has explored dependencygrammar induction.
For instance, the dependencymodel with valence (DMV) of Klein and Manning(2004) has been extended to utilize multilingual in-formation (Berg-Kirkpatrick and Klein, 2010; Co-hen et al, 2011), lexical information (Headden III etal., 2009), and linguistic universals (Naseem et al,2010).
Nevertheless, simplistic dependency modelslike the DMV do not contain information present ina constituency parse, such as the attachment order ofobject and subject to a verb.Unsupervised constituency parsing is also an ac-tive research area.
Several studies (Seginer, 2007;Reichart and Rappoport, 2010; Ponvert et al, 2011)have considered the problem of inducing parsesover raw lexical items rather than part-of-speech(POS) tags.
Additional advances have come frommore complex models, such as combining CCMand DMV (Klein and Manning, 2004) and model-ing large tree fragments (Bod, 2006).The CCM scores each parse as a product of prob-abilities of span and context subsequences.
It wasoriginally evaluated only on unpunctuated sentencesup to length 10 (Klein and Manning, 2002), whichaccount for only 15% of the WSJ corpus; our exper-iments confirm the observation in (Klein, 2005) thatperformance degrades dramatically on longer sen-tences.
This problem is unsurprising: CCM scoreseach constituent type by a single, isolated multino-mial parameter.Our work leverages the idea that sharing infor-mation between local probabilities in a structuredunsupervised model can lead to substantial accu-racy gains, previously demonstrated for dependencygrammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al, 2010).
Our model, Log-LinearCCM (LLCCM), shares information between theprobabilities of related constituents by expressingthem as a log-linear combination of features trainedusing the gradient-based learning procedure of Berg-Kirkpatrick et al (2010).
In this way, the probabil-ity of generating a constituent is informed by relatedconstituents.Our model improves unsupervised constituencyparsing of sentences longer than 10 words.
On sen-tences of up to length 40 (96% of all sentences inthe Penn Treebank), LLCCM outperforms CCM by13.9% (unlabeled) bracketing F1 and, unlike CCM,outperforms a right-branching baseline on sentenceslonger than 15 words.172 ModelThe CCM is a generative model for the unsuper-vised induction of binary constituency parses oversequences of part-of-speech (POS) tags (Klein andManning, 2002).
Conditioned on the constituency ordistituency of each span in the parse, CCM generatesboth the complete sequence of terminals it containsand the terminals in the surrounding context.Formally, the CCM is a probabilistic model thatjointly generates a sentence, s, and a bracketing,B, specifying whether each contiguous subsequenceis a constituent or not, in which case the span iscalled a distituent.
Each subsequence of POS tags,or SPAN, ?, occurs in a CONTEXT, ?, which is anordered pair of preceding and following tags.
Abracketing is a boolean matrix B, indicating whichspans (i, j) are constituents (Bij = true) and whichare distituents (Bij = false).
A bracketing is con-sidered legal if its constituents are nested and form abinary tree T (B).The joint distribution is given by:P(s,B) = PT (B) ?
?i,j?T (B)PS (?
(i, j, s)|true) PC (?
(i, j, s)|true) ?
?i,j 6?T (B)PS (?
(i, j, s)|false) PC (?
(i, j, s)|false)The prior over unobserved bracketings PT (B) isfixed to be the uniform distribution over all legalbracketings.
The other distributions, PS (?)
andPC (?
), are multinomials whose isolated parametersare estimated to maximize the likelihood of a set ofobserved sentences {sn} using EM (Dempster et al,1977).12.1 The Log-Linear CCMA fundamental limitation of the CCM is that it con-tains a single isolated parameter for every span.
Thenumber of different possible span types increases ex-ponentially in span length, leading to data sparsity asthe sentence length increases.1As mentioned in (Klein and Manning, 2002), the CCMmodel is deficient because it assigns probability mass to yieldsand spans that cannot consistently combine to form a valid sen-tence.
Our model does not address this issue, and hence it issimilarly deficient.The Log-Linear CCM (LLCCM) reparameterizesthe distributions in the CCM using intuitive featuresto address the limitations of CCM while retainingits predictive power.
The set of proposed featuresincludes a BASIC feature for each parameter of theoriginal CCM, enabling the LLCCM to retain thefull expressive power of the CCM.
In addition, LL-CCM contains a set of coarse features that activateacross distinct spans.To introduce features into the CCM, we expresseach of its local conditional distributions as a multi-class logistic regression model.
Each local distri-bution, Pt(y|x) for t ?
{SPAN,CONTEXT}, condi-tions on label x ?
{true, false} and generates anevent (span or context) y.
We can define each lo-cal distribution in terms of a weight vector, w, andfeature vector, fxyt, using a log-linear model:Pt(y|x) =exp ?w, fxyt??y?
exp?w, fxy?t?
(1)This technique for parameter transformation wasshown to be effective in unsupervised models forpart-of-speech induction, dependency grammar in-duction, word alignment, and word segmentation(Berg-Kirkpatrick et al, 2010).
In our case, replac-ing multinomials via featurized models not only im-proves model accuracy, but also lets the model applyeffectively to a new regime of long sentences.2.2 Feature TemplatesIn the SPAN model, for each span y = [?1, .
.
.
, ?n]and label x, we use the following feature templates:BASIC: I [y = ?
?
x = ?
]BOUNDARY: I [?1 = ?
?
?n = ?
?
x = ?
]PREFIX: I [?1 = ?
?
x = ?
]SUFFIX: I [?n = ?
?
x = ?
]Just as the external CONTEXT is a signal of con-stituency, so too is the internal ?context.?
For exam-ple, there are many distinct noun phrases with differ-ent spans that all begin with DT and end with NN; afact expressed by the BOUNDARY feature (Table 1).In the CONTEXT model, for each context y =[?1, ?2] and constituent/distituent decision x, we usethe following feature templates:BASIC: I [y = ?
?
x = ?
]L-CONTEXT: I [?1 = ?
?
x = ?
]R-CONTEXT: I [?2 = ?
?
x = ?
]18Consider the following example extracted fromthe WSJ:0 The 1DTVenezuelan 2JJcurrency 3NNNP-SBJplummeted 4VBDthis 5DTyear 6NNNP-TMPVPSBoth spans (0, 3) and (4, 6) are constituents corre-sponding to noun phrases whose features are shownin Table 1:Feature Name (0,3) (4, 6)spanBASIC-DT-JJ-NN: 1 0BASIC-DT-NN: 0 1BOUNDARY-DT-NN: 1 1PREFIX-DT: 1 1SUFFIX-NN: 1 1contextBASIC--VBD: 1 0BASIC-VBD-: 0 1L-CONTEXT-: 1 0L-CONTEXT-VBD: 0 1R-CONTEXT-VBD: 1 0R-CONTEXT-: 0 1Table 1: Span and context features for constituent spans (0, 3)and (4, 6).
The symbol  indicates a sentence boundary.Notice that although the BASIC span features areactive for at most one span, the remaining featuresfire for both spans, effectively sharing informationbetween the local probabilities of these events.The coarser CONTEXT features factor the contextpair into its components, which allow the LLCCMto more easily learn, for example, that a constituentis unlikely to immediately follow a determiner.3 TrainingIn the EM algorithm for estimating CCM parame-ters, the E-Step computes posteriors over bracket-ings using the Inside-Outside algorithm.
The M-Step chooses parameters that maximize the expectedcomplete log likelihood of the data.The weights, w, of LLCCM are estimated to max-imize the data log likelihood of the training sen-tences {sn}, summing out all possible bracketingsB for each sentence:L(w) =?snlog?BPw(sn, B)We optimize this objective via L-BFGS (Liu andNocedal, 1989), which requires us to compute theobjective gradient.
Berg-Kirkpatrick et al (2010)showed that the data log likelihood gradient is equiv-alent to the gradient of the expected complete loglikelihood (the objective maximized in the M-step ofEM) at the point from which expectations are com-puted.
This gradient can be computed in three steps.First, we compute the local probabilities of theCCM, Pt(y|x), from the current w using Equa-tion (1).
We approximate the normalization over anexponential number of terms by only summing overspans that appeared in the training corpus.Second, we compute posteriors over bracketings,P(i, j|sn), just as in the E-step of CCM training,2 inorder to determine the expected counts:exy,SPAN =?sn?ijI [?
(i, j, sn) = y] ?
(x)exy,CONTEXT =?sn?ijI [?
(i, j, sn) = y] ?
(x)where ?
(true) = P(i, j|sn), and ?
(false) = 1 ??
(true).We summarize these expected count quantities as:exyt ={exy,SPAN if t = SPANexy,CONTEXT if t = CONTEXTFinally, we compute the gradient with respect tow, expressed in terms of these expected counts andconditional probabilities:?L(w) =?xytexytfxyt ?G(w)G(w) =?xt(?yexyt)?y?Pt(y|x)fxy?tFollowing (Klein and Manning, 2002), we initializethe model weights by optimizing against posteriorprobabilities fixed to the split-uniform distribution,which generates binary trees by randomly choosinga split point and recursing on each side of the split.32We follow the dynamic program presented in Appendix A.1of (Klein, 2005).3In Appendix B.2, Klein (2005) shows this posterior can beexpressed in closed form.
As in previous work, we start the ini-tialization optimization with the zero vector, and terminate after10 iterations to regularize against achieving a local maximum.193.1 Efficiently Computing the GradientThe following quantity appears in G(w):?t(x) =?yexytWhich expands as follows depending on t:?SPAN(x) =?y?sn?ijI [?
(i, j, sn) = y] ?
(x)?CONTEXT(x) =?y?sn?ijI [?
(i, j, sn) = y] ?
(x)In each of these expressions, the ?
(x) term canbe factored outside the sum over y.
Each fixed(i, j) and sn pair has exactly one span and con-text, hence the quantities?y I [?
(i, j, sn) = y] and?y I [?
(i, j, sn) = y] are both equal to 1.?t(x) =?sn?ij?
(x)This expression further simplifies to a constant.The sum of the posterior probabilities, ?
(true), overall positions is equal to the total number of con-stituents in the tree.
Any binary tree over N ter-minals contains exactly 2N ?
1 constituents and12(N ?
2)(N ?
1) distituents.
?t(x) ={?sn (2|sn| ?
1) if x = true12?sn(|sn| ?
2)(|sn| ?
1) if x = falsewhere |sn| denotes the length of sentence sn.Thus, G(w) can be precomputed once for the en-tire dataset at each minimization step.
Moreover,?t(x) can be precomputed once before all iterations.3.2 Relationship to SmoothingThe original CCM uses additive smoothing in its M-step to capture the fact that distituents outnumberconstituents.
For each span or context, CCM adds10 counts: 2 as a constituent and 8 as a distituent.4We note that these smoothing parameters are tai-lored to short sentences: in a binary tree, the numberof constituents grows linearly with sentence length,whereas the number of distituents grows quadrati-cally.
Therefore, the ratio of constituents to dis-tituents is not constant across sentence lengths.
Incontrast, by virtue of the log-linear model, LLCCMassigns positive probability to all spans or contextswithout explicit smoothing.4These counts are specified in (Klein, 2005); Klein andManning (2002) added 10 constituent and 50 distituent counts.LengthBaselineCCMLLCCM RightbranchingUpperboundInitialization1015202530354071.9 72.0 61.7 88.1 49.853.0 64.6 53.1 86.8 39.846.6 60.0 48.2 86.3 34.242.7 56.2 44.9 85.9 30.639.9 50.3 42.6 85.7 28.537.5 49.2 41.3 85.6 27.333.7 47.6 40.5 85.5 26.8025507510010 15 20 25 30 35 4072.064.660.056.250.349.247.671.953.046.642.739.937.533.7Binary branching upper boundLog-linear CCMStandard CCMRight branchingMaximum sentence lengthBracketingF1Figure 1: CCM and LLCCM trained and tested on sentences ofa fixed length.
LLCCM performs well on longer sentences.
Thebinary branching upper bound correponds to UBOUND from(Klein and Manning, 2002).4 ExperimentsWe train our models on gold POS sequences fromall sections (0-24) of the WSJ (Marcus et al, 1993)with punctuation removed.
We report bracketingF1 scores between the binary trees predicted by themodels on these sequences and the treebank parses.We train and evaluate both a CCM implementa-tion (Luque, 2011) and our LLCCM on sentences upto a fixed length n, for n ?
{10, 15, .
.
.
, 40}.
Fig-ure 1 shows that LLCCM substantially outperformsthe CCM on longer sentences.
After length 15,CCM accuracy falls below the right branching base-line, whereas LLCCM remains significantly betterthan right-branching through length 40.5 ConclusionOur log-linear variant of the CCM extends robustlyto long sentences, enabling constituent grammar in-duction to be used in settings that typically includelong sentences, such as machine translation reorder-ing (Chiang, 2005; DeNero and Uszkoreit, 2011;Dyer et al, 2011).AcknowledgmentsWe thank Taylor Berg-Kirkpatrick and Dan Kleinfor helpful discussions regarding the work on whichthis paper is based.
This work was partially sup-ported by the National Science Foundation througha Graduate Research Fellowship to the first author.20ReferencesTaylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic grammar induction.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1288?1297, Uppsala, Sweden, July.Association for Computational Linguistics.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590, Los Angeles,California, June.
Association for Computational Lin-guistics.Rens Bod.
2006.
Unsupervised parsing with U-DOP.In Proceedings of the Conference on ComputationalNatural Language Learning.Glenn Carroll and Eugene Charniak.
1992.
Two experi-ments on learning probabilistic dependency grammarsfrom corpora.
In Workshop Notes for Statistically-Based NLP Techniques, AAAI, pages 1?13.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 263?270, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.Shay B. Cohen and Noah A. Smith.
2009.
Shared logis-tic normal distributions for soft parameter tying in un-supervised grammar induction.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 74?82,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 50?61, Edinburgh, Scotland,UK., July.
Association for Computational Linguistics.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 39(1):1?38.John DeNero and Jakob Uszkoreit.
2011.
Inducing sen-tence structure from parallel corpora for reordering.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 193?203, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.Chris Dyer, Kevin Gimpel, Jonathan H. Clark, andNoah A. Smith.
2011.
The CMU-ARK German-English translation system.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages337?343, Edinburgh, Scotland, July.
Association forComputational Linguistics.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages101?109, Boulder, Colorado, June.
Association forComputational Linguistics.Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages128?135, Philadelphia, Pennsylvania, USA, July.
As-sociation for Computational Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics, Main Volume, pages 478?485, Barcelona,Spain, July.Dan Klein.
2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis.Karim Lari and Steve J.
Young.
1990.
The estimationof stochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language,4:35?56.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory method for large scale optimization.
Mathe-matical Programming B, 45(3):503?528.Franco Luque.
2011.
Una implementacio?n del mod-elo DMV+CCM para parsing no supervisado.
In 2doWorkshop Argentino en Procesamiento de LenguajeNatural.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Tahira Naseem and Regina Barzilay.
2011.
Using se-mantic cues to learn syntax.
In AAAI.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1234?1244, Cambridge,MA, October.
Association for Computational Linguis-tics.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.21In Proceedings of the 30th Annual Meeting of the As-sociation for Computational Linguistics, pages 128?135, Newark, Delaware, USA, June.
Association forComputational Linguistics.Elias Ponvert, Jason Baldridge, and Katrin Erk.
2011.Simple unsupervised grammar induction from raw textwith cascaded finite state models.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 1077?1086, Portland, Oregon, USA, June.Association for Computational Linguistics.Roi Reichart and Ari Rappoport.
2010.
Improved fullyunsupervised parsing with zoomed learning.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 684?693,Cambridge, MA, October.
Association for Computa-tional Linguistics.Yoav Seginer.
2007.
Fast unsupervised incremental pars-ing.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 384?391, Prague, Czech Republic, June.
Association forComputational Linguistics.22
