Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 93?102,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsNot All Character N -grams Are Created Equal: A Study inAuthorship AttributionUpendra Sapkota and Steven BethardThe University of Alabama at Birmingham1300 University BoulevardBirmingham, AL 35294, USA{upendra,bethard}@cis.uab.eduManuel Montes-y-G?omezInstituto Nacional de Astrof?
?sicaOptica y Electr?onicaPuebla, Mexicommontesg@ccc.inaoep.mxThamar SolorioUniversity of Houston4800 Calhoun RdHouston, TX, 77004, USAsolorio@cs.uh.eduAbstractCharacter n-grams have been identified asthe most successful feature in both single-domain and cross-domain Authorship Attribu-tion (AA), but the reasons for their discrimina-tive value were not fully understood.
We iden-tify subgroups of character n-grams that corre-spond to linguistic aspects commonly claimedto be covered by these features: morpho-syntax, thematic content and style.
We evaluatethe predictiveness of each of these groups intwo AA settings: a single domain setting anda cross-domain setting where multiple topicsare present.
We demonstrate that character n-grams that capture information about affixesand punctuation account for almost all of thepower of character n-grams as features.
Ourstudy contributes new insights into the use ofn-grams for future AA work and other classifi-cation tasks.1 IntroductionAuthorship Attribution (AA) tackles the problem ofdetermining who, among a set of authors, wrote thedocument at hand.
AA has relevant applications rang-ing from plagiarism detection (Stamatatos, 2011) toForensic Linguistics, such as identifying authorshipof threatening emails or malicious code.
Applied ar-eas such as law and journalism can also benefit fromauthorship attribution, where identifying the true au-thor of a piece of text (such as a ransom note) mayhelp save lives or catch the offenders.We know from state of the art research in AA thatthe length of the documents and the number of po-tential candidate authors have an important effect onthe accuracy of AA approaches (Moore, 2001; Luy-ckx and Daelemans, 2008; Luyckx and Daelemans,2010).
We can also point out the most common fea-tures that have been used successfully in AA work,including: bag-of-words (Madigan et al, 2005; Sta-matatos, 2006), stylistic features (Zheng et al, 2006;Stamatatos et al, 2000), and word and character leveln-grams (Kjell et al, 1994; Keselj et al, 2003; Penget al, 2003; Juola, 2006).The utility of bag-of-words features is well under-stood: they effectively capture correlations betweenauthors and topics (Madigan et al, 2005; Kaster et al,2005).
The discriminative value of these features isthus directly related to the level of content divergenceamong authors and among train and test sets.The utility of stylistic features is also well under-stood: they model author preferences for the useof punctuation marks, emoticons, white spaces, andother traces of writing style.
Such preferences areless influenced by topic, and directly reflect some ofthe unique writing patterns of an author.Character n-grams are the single most successfulfeature in authorship attribution (Koppel et al, 2009;Frantzeskou et al, 2007; Koppel et al, 2011), but thereason for their success is not well understood.
Onehypothesis is that character n-grams carry a little bitof everything: lexical content, syntactic content, andeven style by means of punctuation and white spaces(Koppel et al, 2011).
While this argument seemsplausible, it falls short of a rigorous explanation.In this paper, we investigate what in the make-up93of these small units of text makes them so power-ful.
Our goal is two-fold: on the one hand we wantto have a principled understanding of character n-grams that will inform their use as features for AAand other tasks; on the other hand we want to makeAA approaches more accessible to non-experts sothat, for example, they could be acceptable pieces ofevidence in criminal cases.The research questions we aim to answer are:?
Are all character n-grams equally important?For example, are the prefix of ?there?, the suffixof ?breathe?
and the whole word ?the?
all equiv-alent?
More generally, are character n-gramsthat capture morpho-syntactic information, the-matic information and style information equallyimportant??
Are the character n-grams that are most impor-tant for single-domain settings also the mostimportant for cross-domain settings?
Whichcharacter n-grams are more like bag-of-wordsfeatures (which tend to track topics), and whichare more like stylistic features (which tend totrack authors)??
Do different classifiers agree on the importanceof the different types of character n-grams?
Aresome character n-grams consistently the bestregardless of the learning algorithm??
Are some types of character n-grams irrelevantin AA tasks?
Are there categories of charactern-grams that we can exclude and get similar(or better) performance than using all n-grams?If there are, are they the same for both single-domain and cross-domain AA settings?Our study shows that using the default bag-of-words representation of char n-grams results in col-lapsing sequences of characters that correspond todifferent linguistic aspects, and that this yields subop-timal prediction performance.
We further show thatwe can boost accuracy by loosing some categories ofn-grams.
Char n-grams closely related to thematiccontent can be completely removed without loss ofaccuracy, even in cases where the train and test setshave the same topics represented, a counter-intuitiveargument.
Given the wide spread use of char n-gramsin text classification tasks, our findings have signifi-cant implications for future work in related areas.2 Categories of Character N -gramsTo answer our research questions and explore thevalue of character n-grams in authorship attribution,we propose to separate character n-grams into ten dis-tinct categories.
Unlike previous AA work where allcharacter n-grams were combined into a single bag-of-n-grams, we evaluate each category separatelyto understand its behavior and effectiveness in AAtasks.
These categories are related to the three linguis-tic aspects hypothesized to be represented by char-acter n-grams: morpho-syntax (as represented byaffix-like n-grams), thematic content (as representedby word-like n-grams) and style (as represented bypunctuation-based n-grams).
We refer to these threeaspects as super categories (SC).The following sections describe the different typesof n-grams.
We use the sentence in Table 1 as arunning example for the classes and in Table 2 weshow the resulting n-grams in that sentence.
For easeof understanding, we replace spaces in n-grams withunderscores ( ).The actors wanted to see if the pact seemed like anold-fashioned one.Table 1: Example sentence to demonstrate the selectionof different n-gram categories.2.1 Affix n-gramsCharacter n-grams are generally too short to repre-sent any deep syntax, but some of them can reflectmorphology to some degree.
In particular, we con-sider the following affix-like features by looking atn-grams that begin or end a word:prefix A character n-gram that covers the first ncharacters of a word that is at least n+ 1 charac-ters long.suffix A character n-gram that covers the last n char-acters of a word that is at least n + 1 characterslong.space-prefix A character n-gram that begins with aspace.94SC Category Character n-gramsaffixprefixact wan pac see lik fassuffixors ted act med ike nedspace-prefixac wa to se if th pa lian ol onspace-suffixhe rs ed to ee if ct keanwordwhole-word The see the old onemid-wordcto tor ant nte eem eme ash shihio ion onemulti-worde a s w d t o s e i f t e p t sd l n o d opunctbeg-punct -famid-punct d-fend-punct ld- ne.Table 2: Example of the n-gram categories (n = 3) for thesentence in Table 1.
The first column represents the supercategory (SC).
The n-grams that appear in more than onecategory are in bold.space-suffix A character n-gram that ends with aspace.2.2 Word n-gramsWhile character n-grams are often too short to cap-ture entire words, some types can capture partialwords and other word-relevant tokens.
We considerthe following such features:whole-word A character n-gram that covers all char-acters of a word that is exactly n characters long.mid-word A character n-gram that covers n charac-ters of a word that is at least n + 2 characterslong, and that covers neither the first nor the lastcharacter of the word.multi-word N -grams that span multiple words,identified by the presence of a space in the mid-dle of the n-gram.2.3 Punctuation n-gramsThe main stylistic choices that character n-grams cancapture are the author?s preferences for particularpatterns of punctuation.
The following features char-acterize punctuation by its location in the n-gram.beg-punct A character n-gram whose first characteris punctuation, but middle characters are not.mid-punct A character n-gram with at least onepunctuation character that is neither the firstnor the last character.end-punct A character n-gram whose last characteris punctuation, but middle characters are not.The above ten categories are intended to be dis-joint, so that a character n-gram belongs to exactlyone of the categories.
For n-grams that contain bothspaces and punctuation, we first categorize by punc-tuation and then by spaces.
For example, ?e, ?
isassigned to the mid-punct category, not the space-suffix category.We have observed that in our data almost 80% ofthe n-grams in the punct-beg and punct-mid cate-gories contain a space.
This tight coupling of punc-tuation and spaces is due to the rules of English or-thography: most punctuation marks require a spacefollowing them.
The 20% of n-grams that have punc-tuation but no spaces correspond mostly to the ex-ceptions to this rule: quotation marks, mid-word hy-phens, etc.
An interesting experiment for future workwould be to split out these two types of punctuationinto separate feature categories.3 DatasetsWe consider two corpora, a single-domain corpus,where there is only one topic that all authors arewriting about, and a multi-domain corpus, wherethere are multiple different topics.
The latter allowsus to test the generalization of AA models, by testingthem on a different topic from that used for training.The first collection is the CCAT topic class, a sub-set of the Reuters Corpus Volume 1 (Lewis et al,2004).
Although this collection was not gatheredfor the goal of doing authorship attribution studies,previous work has reported results for AA with 10and 50 authors (Stamatatos, 2008; Plakias and Sta-matatos, 2008; Escalante et al, 2011).
We refer tothese as CCAT 10 and CCAT 50, respectively.
BothCCAT 10 and CCAT 50 belong to CCAT category(about corporate/industrial news) and are balancedacross authors, with 100 documents sampled for eachauthor.
Manual inspection of the dataset revealedthat some of the authors in this collection consis-tently used signatures at the end of documents.
Also,we noticed some writers use quotations a lot.
Con-95Corpus #authors#docs #sentences #words/author/topic /doc /docCCAT 10 10 100 19 425CCAT 50 50 100 19 415Guardian1 13 13 53 1034Guardian2 13 65 10 207Table 3: Some statistics about the datasets.sidering these parts of text for measuring the fre-quencies of character n-grams is not a good ideabecause signatures provide direct clues about the au-thorship of document and quotations do not reflectthe author?s writing style.
Therefore, to clean up theCCAT collection, we preprocessed it to remove sig-natures and quotations from each document.
Sincethe CCAT collection contains documents belongingto only corporate/industrial topic category, this willbe our single-domain collection.The other collection consists of texts publishedin The Guardian daily newspaper written by 13 au-thors in four different topics (Stamatatos, 2013).
Thisdataset contains opinion articles on the topics: World,U.K., Society, and Politics.
Following prior work,to make the collection balanced across authors, wechoose at most ten documents per author for each ofthe four topics.
We refer to this corpus as Guardian1.We also consider a variation of this corpus that makesit more challenging but that more closely matchesrealistic scenarios of forensic investigation that dealwith short texts such as tweets, SMS, and emails.We chunk each of the documents by sentence bound-aries into five new short documents.
We refer to thiscorpus as Guardian2.Table 3 shows some of the statistics of the CCATand Guardian corpora and Table 4 presents some ofthe top character n-grams for each category (takenfrom an author in the Guardian data, but the top n-grams look qualitatively similar for other authors).4 Experimental SettingsWe performed various experiments using differentcategories of character n-grams.
We chose n=3 sinceour preliminary experiments found character 3-gramsto be more effective than other higher level charactern-grams.
For each category, we considered onlythose 3-grams that occur at least five times in thetraining documents.The performance of different authorship attribu-SC Category N -gramsaffixprefix tha the wit con havsuffix ing hat ion ent ersspace-prefix th of to an inspace-suffix he of to ed ngwordwhole-word the and for was notmid-word tio ati iti men entmulti-word e t s a t t s t n tpunctbeg-punct .
T ?s , t , a .
Imid-punct s, e, s. e?s y?send-punct es, on.
on, es.
er,Table 4: Top character 3-grams in each category for author?Catherine Bennet?
in the cross-domain training data.tion models was measured in terms of accuracy.
Inthe single-domain CCAT experiments, accuracy wasmeasured using the train/test partition of prior work.In the cross-domain Guardian experiments, accuracywas measured by considering all 12 possible pairingsof the 4 topics, treating one topic as training data andthe other as testing data, and averaging accuracy overthese 12 scenarios.
This ensured that in the cross-domain experiments, the topics of the training datawere always different from that of the test data.We trained support vector machine (SVM) clas-sifiers using the Weka implementation (Witten andFrank, 2005) with default parameters.
We also ransome comparative experiments with the Weka im-plementation of naive Bayes classifiers and the Lib-SVM implementation of SVMs.
In the results below,when performance of a single classifier is presented,it is the result of Weka?s SVM, which generally gavethe best performance.
When performance of otherclassifiers are presented, the classifiers are explicitlyindicated.5 Experimental Results and EvaluationIn this section, we present various results on author-ship attribution tasks using both single as well ascross-domain datasets.
We will explore character n-grams in depth and try to understand why they are soeffective in discriminating authors.5.1 Which n-gram Categories are MostAuthor-Discriminative?After breaking character n-grams into ten disjoint cat-egories, we empirically illustrate what categories are96affix word punctDataset prefix suffix space-prefix space-suffix multi-word whole-word mid-word beg-punct mid-punct end-punctCCAT 10 74.6 71.0 71.2 66.0 65.8 48.0 70.0 60.2 35.4 56.2CCAT 50 61.9 59.6 57.0 51.0 51.2 35.4 61.0 39.7 12.4 36.5(a) Single Domainaffix word punctDataset prefix suffix space-prefix space-suffix multi-word whole-word mid-word beg-punct mid-punct end-punctGuardian1 41.6 36.7 41.9 38.1 32.2 38.1 37.8 43.5 46.1 37.3Guardian2 31.0 26.9 29.7 27.0 23.2 26.8 27.2 33.6 33.5 24.5(b) Cross-DomainTable 5: Accuracy of AA classifiers trained on each of the character n-gram categories.
The top four accuracies foreach dataset are in bold.prefix suffixspace-prefixspace-suffixmulti-wordwhole-wordmid-wordbeg-punctmid-punctend-punct246810132569471084652Single Domain (CCAT)CCAT 10CCAT 50prefix suffixspace-prefixspace-suffixmulti-wordwhole-wordmid-wordbeg-punctmid-punctend-punct2468104.56.84.45.58.56.25.83.82.96.63.273.96.49.36.6622.28.3Cross Domain (Guardian)Guardian1Guardian2Figure 1: Average rank of the performance of each n-gram category on the single-domain CCAT tasks (top) and thecross-domain Guardian tasks (bottom).most discriminative.
Table 5 shows the accuracy ofeach type of n-gram for each of the different corpora.Table 5(a) shows that the top four categories forsingle-domain AA are: prefix, suffix, space-prefix,and mid-word.
These four categories have the bestperformance on both CCAT 10 and CCAT 50.
Incontrast, Table 5(b) shows that the top four categoriesfor cross-domain AA are: prefix, space-prefix, beg-punct, and mid-punct.For both single-domain and cross-domain AA, pre-fix and space-prefix are strong features, and are gen-erally better than the suffix features, perhaps becauseauthors have more control over prefixes in English,while suffixes are often obligatory for grammaticalreasons.
For cross-domain AA, beg-punct and mid-punct are the top features, likely because an author?s97use of punctuation is consistent even when the topicchanges.
For single-domain AA, mid-word was alsoa good feature, probably because it captured lexicalinformation that correlates with authors?
preferencestowards writing about specific topics.Figure 1 shows an alternate view of these results,graphing the rank of each n-gram type.
For com-puting the rank, the accuracies of the ten differentn-gram type classifiers are sorted in decreasing or-der and ranked from 1 to 10 respectively with tiesgetting the same rank.
For the Guardian corpora,the average rank of each n-gram category was com-puted by averaging its rank across the 12 possibletest/train cross-domain combinations.
In both of thesingle-domain CCAT corpora, the classifier based onprefix n-grams had the top accuracy (rank 1), andthe classifier based on mid-punct had the worst accu-racy (rank 10).
In both of the cross-domain Guardiancorpora, on the other hand, mid-punct was amongthe top-ranked n-gram categories.
This suggests thatpunctuation features generalize the best across topic,but if AA is more of a topic classification task (asin the single-domain CCAT corpora), then punctua-tion adds little over other features that more directlycapture the topic.Since our cross-domain datasets are small, weperformed a small number of planned comparisonsusing a two-tailed t-test over the accuracies on theGuardian1 and Guardian2 corpora.
We found that inboth corpora, the best punctuation category (punct-mid) is better than the best word category (whole-word) with p < 0.001.
In the Guardian2 corpus, thebest affix category (space-prefix) is also better thanthe best word category (whole-word) with p < 0.05,but this does not hold in the Guardian1 corpus(p = 0.14).
Also, we observed that in both Guardian1and Guardian2 datasets, both punct-mid and space-prefix are better than multi-word (p < 0.01).Overall, we see that affix n-grams are generallyeffective in both single-domain and cross-domainsettings, punctuation n-grams are effective in cross-domain settings, and mid-word is the only effectiveword n-gram, and only in the single-domain setting.5.2 Do Different Classifiers Agree on theImportance of Different n-gram Types?The previous experiments have shown, for example,that prefix n-grams are universally predictive in AAComparisonCCAT GuardianWeka SVM vs LibSVM 0.93 0.81Weka SVM vs Naive Bayes 0.73 0.57LibSVM vs Naive Bayes 0.77 0.44Table 6: Spearman?s rank correlation coefficient (?)
foreach pair of classifiers on the single-domain (CCAT) andcross-domain (Guardian) settings.tasks, that mid-word n-grams are good predictors insingle-domain settings, and that beg-punct n-gramsare good predictors in cross-domain settings.
Butare these facts about the n-gram types themselves,or are these results only true for the specific SVMclassifiers we trained?To see whether certain types of n-grams are funda-mentally good or bad, regardless of the classifier, wecompare performance of the different n-gram typesfor three classifiers: Weka SVM classifiers (as usedin our other experiments), LibSVM classifiers andWeka?s naive Bayes classifiers1.
Figure 2 shows then-gram category rankings for all these classifiers2forboth the single-domain CCAT and the cross-domainGuardian settings.Across the different classifiers, the pattern of fea-ture rankings are similar.
Table 6 shows the Spear-man?s rank correlation coefficient (?)
for the per-n-gram-type accuracies of each pair of classifiers.
Weobserve fairly high correlations, with ?
above 0.70for all single-domain pairings, and between 0.44 and0.81 for cross-domain pairings.As in Section 5.1, prefix and space-prefix areamong the most predictive n-gram types.
In thesingle-domain settings, we again see that suffix andmid-word are also highly predictive, while in thecross-domain settings, we again see that beg-punctand mid-punct are highly predictive.
These results allconfirm that some types of n-grams are fundamen-tally more predictive than others, and our results arenot specific to the particular type of classifier used.1Weka SVM and LibSVM are both support vector machineclassifiers, but Weka uses Platt?s sequential minimal optimizationalgorithm while LibSVM uses working set selection with sec-ond order information.
The result is that they achieve differentperformance on our AA tasks.2We also tried a decision tree classifier, C4.5 (J48) fromWEKA, and it produced similar patterns (not shown).98prefix suffixspace-prefixspace-suffixmulti-wordwhole-wordmid-wordbeg-punctmid-punctend-punct24681013 35.5 5.59371082.54568.52.58.593.56.59.56.59.585Single Domain (CCAT)WekaSVMLIBSVMNaiveBayesprefix suffixspace-prefixspace-suffixmulti-wordwhole-wordmid-wordbeg-punctmid-punctend-punct2468103.96.94.168.96.45.92.92.57.555.72.63.79.277.13.23.87.86.88.26.24.98.56.16.71.6 1.64.4Cross Domain (Guardian)Figure 2: Average rank of the performance of each n-gram category across different types of classifiers on thesingle-domain CCAT task (top) and the cross-domain Guardian task (bottom).5.3 Are Some Character N -grams Irrelevant?In the previous sections, we have seen that sometypes of character n-grams are more predictive thanothers - affix n-grams performed well in both singledomain and cross-domain settings and punctuationn-grams performed well in cross-domain settings.In general, word n-grams were not as predictive asother types of n-grams (with the one exception be-ing mid-word n-grams in the single domain setting).Given this poor performance of word n-grams, anatural question is: could we exclude these featuresentirely and achieve similar performance?Our goal then is to compare a model trained onaffix n-grams and punct n-grams against a modeltrained on ?all?
n-grams.
We consider two definitionsof ?all?
:all-untyped The traditional approach to extractingn-grams where n-gram types are ignored (e.g.,?the?
as a whole word is no different from ?the?in the middle of a word)all-typed The approach discussed in this paper,where n-grams of different types are dis-tinguished (equivalent to the set of all af-fix+punct+word n-grams).We compare these models trained on all the n-gramsto our affix+punct model.Table 7 shows this analysis.
For either definitionof ?all?, the model that discards all word featuresachieves performance as high or higher than themodel with all of the features, and does so with onlyabout two thirds of the features.
This is not too sur-prising in the cross-domain Guardian tasks, wherethe word n-grams were among the worst features.On the single-domain CCAT tasks this result is moresurprising, since we have discarded the mid-wordn-grams, which was one of the best single-domainn-gram types.
This indicates that whatever informa-tion mid-word is capturing it is also being capturedin other ways via affix and punct n-grams.
Of all1024 possible combinations of features, we tried a99Datasetall-untyped all-typed affix+punctAcc N Acc N Acc NCCAT 10 77.8 8245 77.2 9715 78.8 5474CCAT 50 69.2 14461 69.1 17062 69.3 9966Guardian1 55.6 5689 53.6 6966 57.0 3822Guardian2 45.9 5687 45.6 6965 48.0 3820Table 7: Results of excluding word n-grams, comparedto using all n-grams, either in the traditional approach(untyped n-grams) or in the approach of this paper (typedn-grams).
Accuracy (Acc) and the number of features(N in italics) are reported for each classifier.
The bestaccuracy for each dataset is in bold.number of different combinations and were unable toidentify one that outperformed affix+punct.
Overall,this experiment gives compelling evidence that affixand punct n-grams are more important than wordn-grams.6 AnalysisWe did a manual exploration of our datasets.
Inour cross-domain dataset, the character 3-gram ?sti?shows up as both prefix and mid-word.
All 13 authorsuse ?sti?
frequently as a mid-word n-gram in wordssuch as institution, existing, justice, and distinction.For example:?
The government?s story is that the existing war-heads might be deteriorating.?
For all the justice of many of his accusations,the result is occasionally as dreadful as his titlesuggests.But only six authors use ?sti?
as a prefix, in exampleslike:?
Their mission was to convince tourists thatBritain was still open for business.?
There aren?t even any dead people on it, sinceby the very act of being dead and still famous,they assert their long-term impact.Thus ?sti?
as a prefix is predictive of authorship eventhough ?sti?
as a mid-word n-gram is not.
Notably, un-der the traditional untyped bag-of-n-grams approach,both versions of ?sti?
would have been treated thesame, and this discriminative power would have beenlost.To use old-fashioned language, she is motherly - aplump, rosy-cheeked woman of Kent, whom natureseemed to have created to raise children.To use old-fashioned language, she is motherly - aplump, rosy-cheeked woman of Kent, whom natureseemed to have created to raise children.Table 8: Example sentence showing the opacity of eachcharacter.
Darkness of character is determined by thenumber of categories it belongs to (lowest=lighter, high-est=darkest color).
Categories in word are discarded.As already demonstrated in Section 5 that af-fix+punct features perform better than using all thefeatures, we would like to use an example from ourdataset to visualize the text when features in SC wordare discarded.
Out of seven categories in affix andpunct, we computed in how many of them each char-acter belongs to, three being the maximum possiblevalue.
Therefore, we show each character with differ-ent opacity level depending on number of categoriesit belongs to: zero will get white color (word relatedn-grams), one will get 33% black, two will get 67%black, and three will get 100% black.
In Table 8,we show an example sentence before (first row ofTable 8) and after (second row of Table 8) showingthe opacity level of each character.
It is clear thatthe darkest characters are those around the punctua-tion characters and those around spaces are seconddarkest, while the lightest (with 0% darkness) are theones in the middle of long words.
This gives us anidea about the characters in a text that are importantfor AA tasks.7 DiscussionVarious hypotheses have been put forth to explain the?black magic?
(Kestemont, 2014) behind the successof character n-gram features in authorship attribution.Kestemont (2014) conjectured that their utility wasin capturing function words and morphology.
Koppelet al (2009) suggested that they were capturing topicinformation in single domain settings, and style andsyntactic information in cross-domain settings.
Ourstudy provides empirical evidence for testing theseclaims.
We did indeed find that the ability of char-acter n-grams to capture morphology is useful, asreflected in the high prediction performance of af-100fix n-grams in both single-domain and cross-domainsettings.
And we found that word n-grams (captur-ing topic information) were useful in single domainsettings, while puct n-grams (capturing style infor-mation) were useful in cross-domain settings.
Wefurther found that word n-grams are unnecessary,even in single-domain settings.
Models based onlyon affix and punct n-grams performed as well asmodels with all n-grams regardless of whether it wasa single-domain or cross-domain authorship attribu-tion task.Our findings on the value of selecting n-grams ac-cording to the linguistic aspect they represent mayalso be beneficial in other classification tasks wherecharacter n-grams are commonly used.
Promisingtasks are those related to the stylistic analysis of texts,such as native language identification, document sim-ilarity and plagiarism detection.Morphologically speaking, English is a poor lan-guage.
The fact that we identified significant differ-ences in performance by selecting n-gram categoriesthat are related to affixation in this poorly inflectedlanguage suggests that we may find even larger dif-ferences in performance in morphologically richerlanguages.
We leave this research question for futurework.AcknowledgementsThis research was partially supported by NSF awards1462141 and 1254108.
It was also supported in partby the CONACYT grant 134186 and the WIQ-EIIRSES project (grant no.
269180) within the FP 7Marie Curie.ReferencesH.
J. Escalante, T. Solorio, and M. Montes-y Gomez.2011.
Local histograms of character n-grams for au-thorship attribution.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 288?298,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.G.
Frantzeskou, E. Stamatatos, S. Gritzalis, and C. E.Chaski.
2007.
Identifying authorship by byte-level n-grams: The source code author profile (SCAP) method.Journal of Digital Evidence, 6(1).P.
Juola.
2006.
Authorship attribution.
Foundations andTrends in Information Retrieval, 1(3):233?334, Decem-ber.A.
Kaster, S. Siersdorfer, and G. Weikum.
2005.
Com-bining text and linguistic document representations forauthorship attribution.
In SIGIR Workshop: StylisticAnalysis of Text for Information Access (STYLE, pages27?35.V.
Keselj, F. Peng, N. Cercone, and C. Thomas.
2003.N-gram based author profiles for authorship attribution.In Proceedings of the Pacific Association for Computa-tional Linguistics, pages 255?264.M.
Kestemont.
2014.
Function words in authorship attri-bution.
From black magic to theory?
In CLFL, pages59?66, Gothenburg, Sweden, April.Bradley Kjell, W.Addison Woods, and Ophir Frieder.1994.
Discrimination of authorship using visualization.Information Processing & Management, 30(1):141 ?150.M.
Koppel, J. Schler, and S. Argamon.
2009.
Computa-tional methods in authorship attribution.
Journal of theAmerican Society for Information Science and Technol-ogy, 60(1):9?26.M.
Koppel, J. Schler, and S. Argamon.
2011.
Author-ship attribution in the wild.
Language Resources andEvaluation, 45:83?94.D.
D. Lewis, Y. Yang, T. G. Rose, and F. Li.
2004.
RCV1:A new benchmark collection for text categorization re-search.
Journal of Machine Learning Research, 5:361?397.K.
Luyckx and W. Daelemans.
2008.
Authorship attri-bution and verification with many authors and limiteddata.
In Proceedings of the 22nd International Confer-ence on Computational Linguistics (COLING 2008),pages 513?520, Manchester, UK, August.K.
Luyckx and W. Daelemans.
2010.
The effect of authorset size and data size in authorship attribution.
Literaryand Linguistic Computing, pages 1?21, August.D.
Madigan, A. Genkin, S. Argamon, D. Fradkin, andL.
Ye.
2005.
Author identification on the large scale.In Proceedings of CSNA/Interface 05.R.
Moore.
2001.
There?s no data like more data (butwhen will enough be enough?).
In Proceedings ofthe IEEE International Workshop on Intelligent SignalProcessing, Budapest, Hungary.F.
Peng, D. Schuurmans, V. Keselj, and S. Wang.
2003.Language independent authorship attribution usingcharacter level language models.
In 10th Conferenceof the European Chapter of the Association for Compu-tational Linguistics, EACL, pages 267?274.S.
Plakias and E. Stamatatos.
2008.
Tensor space modelsfor authorship attribution.
In Proceedings of the 5thHellenic Conference on Artificial Intelligence: Theo-ries, Models and Applications, volume 5138 of LNCS,pages 239?249, Syros, Greece.101E.
Stamatatos, G. Kokkinakis, and N. Fakotakis.
2000.Automatic text categorization in terms of genre andauthor.
Computational Linguistics, 26(4):471?495, De-cember.E.
Stamatatos.
2006.
Authorship attribution based on fea-ture set subspacing ensembles.
International Journalon Artificial Intelligence tools, 15(5):823?838.E.
Stamatatos.
2008.
Author identification: Using textsampling to handle the class imbalance problem.
Infor-mation Processing and Managemement, 44:790?799.E.
Stamatatos.
2011.
Plagiarism detection using stopwordn-grams.
Journal of the American Society for Informa-tion Science and Technology, 62(12):2512?2527.E.
Stamatatos.
2013.
On the robustness of authorshipattribution based on character n-gram features.
Journalof Law & Policy, 21(2):421 ?
439.I.
H. Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques.
MorganKauffmann, 2nd edition.R.
Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang.2006.
A framework for authorship identification of on-line messages: Writing-style features and classificationtechniques.
J.
Am.
Soc.
Inf.
Sci.
Technol., 57(3):378?393, February.102
