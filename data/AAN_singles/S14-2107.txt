Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 610?614,Dublin, Ireland, August 23-24, 2014.SZTE-NLP: Aspect Level Opinion Mining Exploiting Syntactic CuesViktor Hangya1, G?abor Berend1, Istv?an Varga2?, Rich?ard Farkas11University of SzegedDepartment of Informatics{hangyav,berendg,rfarkas}@inf.u-szeged.hu2NEC Corporation, JapanKnowledge Discovery Research Laboratoriesvistvan@az.jp.nec.comAbstractIn this paper, we introduce our contribu-tions to the SemEval-2014 Task 4 ?
As-pect Based Sentiment Analysis (Pontiki etal., 2014) challenge.
We participated inthe aspect term polarity subtask wherethe goal was to classify opinions relatedto a given aspect into positive, negative,neutral or conflict classes.
To solve thisproblem, we employed supervised ma-chine learning techniques exploiting a richfeature set.
Our feature templates ex-ploited both phrase structure and depen-dency parses.1 IntroductionThe booming volume of user-generated contentand the consequent popularity growth of online re-view sites has led to vast amount of user reviewsthat are becoming increasingly difficult to grasp.There is desperate need for tools that can automat-ically process and organize information that mightbe useful for both users and commercial agents.Such early approaches have focused on deter-mining the overall polarity (e.g., positive, nega-tive, neutral, conflict) or sentiment rating (e.g.,star rating) of various entities (e.g., restaurants,movies, etc.)
cf.
(Ganu et al., 2009).
While theoverall polarity rating regarding a certain entityis, without question, extremely valuable, it failsto distinguish between various crucial dimensionsbased on which an entity can be evaluated.
Evalu-ations targeting distinct key aspects (i.e., function-ality, price, design, etc) provide important cluesthat may be targeted by users with different priori-ties concerning the entity in question, thus holding?The work was done while this author was working as aguest researcher at the University of SzegedThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/much greater value in one?s decision making pro-cess.In this paper, we introduce our contribution tothe SemEval-2014 Task 4 ?
Aspect Based Sen-timent Analysis (Pontiki et al., 2014) challenge.We participated in the aspect term polarity sub-task where the goal was to classify opinions whichare related to a given aspect into positive, nega-tive, neutral or conflict classes.
We employed su-pervised machine learning techniques exploiting arich feature set for target polarity detection, witha special emphasis on features that deal with thedetection of aspect scopes.
Our system achievedan accuracy of 0.752 and 0.669 for the restaurantand laptop domains, respectively.2 ApproachWe employed a four-class supervised (positive,negative, neutral and conflict) classifier here.
Asa normalization step, we converted the given textsinto their lowercased forms.
Bag-of-words fea-tures comprised the basic feature set for our max-imum entropy classifier, which was shown to behelpful in polarity detection (Hangya and Farkas,2013).In the case of aspect-oriented sentiment detec-tion, we found it important to locate text partsthat refer to particular aspects.
For this, we usedseveral syntactic parsing methods and introducedparse tree based features.2.1 Distance-weighted Bag-of-words FeaturesInitially, we used n-gram token features (unigramsand bigrams).
It could be helpful to take into con-sideration the distance between the token in ques-tion and the mention of the target aspect.
Thecloser a token is to an entity the more plausiblethat the given token is related to the aspect.610<ROOT> The food was great but the service was awful .DT NN VBD JJ CC DT NN VBD JJ .ROOTSBJNMOD PRDCOORDPCONJNMOD SBJ PRDFigure 1: Dependency parse tree (MATE parser).For this we used weighted feature vectors, andweighted each n-gram feature by its distance in to-kens from the mention of the given aspect:1e1n|i?j|,where n is the length of the review and the valuesi, j are the positions of the actual word and thementioned aspect.2.2 Polarity LexiconTo examine the polarity of the words comprisinga review, we incorporated the SentiWordNet sen-timent lexicon (Baccianella et al., 2010) into ourfeature set.In this resource, synsets ?
i.e.
sets of wordforms sharing some common meaning ?
are as-signed positivity, negativity and objectivity scores.These scores can be interpreted as the probabilitiesof seeing some representatives of the synsets ina positive, negative and neutral meaning, respec-tively.
However, it is not unequivocal to deter-mine automatically which particular synset a givenword belongs to with respect its context.
Considerthe word form great for instance, which mighthave multiple, fundamentally different sentimentconnotations in different contexts, e.g.
in expres-sions such as ?great food?
and ?great crisis?.We determined the most likely synset a particu-lar word form belonged to based on its contexts byselecting the synset, the members of which werethe most appropriate for the lexical substitutionof the target word.
The extent of the appropri-ateness of a word being a substitute for anotherword was measured relying on Google?s N-GramCorpus, using the indexing framework describedin (Ceylan and Mihalcea, 2011).We look up the frequencies of the n-grams thatwe derive from the context by replacing the tar-get words with its synonyms(great) from varioussynsets, e.g.
good versus big.
We count down thefrequency of the phrases food is good and food isbig in a huge set of in-domain documents (Cey-lan and Mihalcea, 2011).
Than we choose themeaning which has the highest probability, goodin this case.
This way we assign a polarity valuefor each word in a text and created three new fea-tures for the machine learning algorithm, whichare the number of positive, negative and objectivewords in the given document.2.3 Negation Scope DetectionSince negations are quite frequent in user reviewsand have the tendency to flip polarities, we tookspecial care of negation expressions.
We collecteda set of negation expressions, like not, don?t, etc.and a set of delimiters and, or, etc.
It is reasonableto think that the scope of a negation starts whenwe detect a negation word in the sentence and itlasts until the next delimiter.
If an n-gram was ina negation scope we added a NOT prefix to thatfeature.2.4 Syntax-based FeaturesIt is very important to discriminate between textfragments that are referring to the given aspect andthe fragments that do not, within the same sen-tence.
To detect the relevant text fragments, weused dependency and constituency parsers.
Sinceadjectives are good indicators of opinion polarity,we add the ones to our feature set which are inclose proximity with the given aspect term.
Wedefine proximity between an adjective and an as-pect term as the length of the non-directional pathbetween them in the dependency tree.
We gatheradjectives in proximity less than 6.Another feature, which is not aspect specific butcan indicate the polarity of an opinion, is the polar-ity of words?
modifiers.
We defined a feature tem-plate for tokens whose syntactic head is present in611ROOTS..SVPADJPJJawfulVBDwasNPNNserviceDTtheCCbutSVPADJPJJgreatVBDwasNPNNfoodDTTheFigure 2: Constituency parse tree (Stanford parser).our positive or negative lexicon.
For dependencyparsing we used the MATE parser (Bohnet, 2010)trained on the Penn Treebank (penn2malt conver-sion), an example can be seen on Figure 1.Besides using words that refer to a given aspect,we tried to identify subsentences which refers tothe aspect mention.
In a sentence we can expressour opinions about more than one aspect, so it isimportant not to use subsentences containing opin-ions about other aspects.
We developed a sim-ple rule based method for selecting the appropri-ate subtree from the constituent parse of the sen-tence in question (see Figure 2).
In this method,the root of this subtree is the leaf which containsthe given aspect initially.
In subsequent steps thesubtree containing the aspect in its yield gets ex-panded until the following conditions are met:?
The yield of the subtree consists of at leastfive tokens.?
The yield of the subtree does not contain anyother aspect besides the five-token windowframe relative to the aspect in question.?
The current root node of the subtree is eitherthe non-terminal symbol PP or S.Relying on these identified subtrees, we intro-duced a few more features.
First, we creatednew n-gram features from the yield of the sub-tree.
Next, we determined the polarity of this sub-tree with a method proposed by Socher et al.
()and used it as a feature.
We also detected thosewords which tend to take part in sentences con-veying subjectivity, using the ?2statistics calcu-lated from the training data.
With the help of thesewords, we counted the number of opinion indica-tor words in the subtree as additional features.
Weused the Stanford constituency parser (Klein andManning, 2003) trained on the Penn Treebank forthese experiments.2.5 ClusteringAspect mentions can be classified into a few dis-tinct topical categories, such as aspects regardingthe price, service or ambiance of some product orservice.
Our hypothesis was that the distributionof the sentiment categories can differ significantlydepending on the aspect categories.
For instance,people might tend to share positive ideas on theprice of some product rather than expressing neg-ative, neutral or conflicting ideas towards it.
Inorder to make use of this assumption, we automat-ically grouped aspect mentions based on their con-texts as different aspect target words can still referto the very same aspect category (e.g.
?deliciousfood?
and ?nice dishes?
).Clustering of aspect mentions was performedby determining a vector for each aspect term basedon the words co-occurring with them.
6, 485 dis-tinct lemmas were found to co-occur with any ofthe aspect phrases in the two databases, thus con-text vectors originally consisted of that many el-ements.
Singular value decomposition was thenused to project these aspect vectors into a lower di-mensional ?semantic?
space, where k-means clus-tering (with k = 10) was performed over the datapoints.
For each classification instance, we re-garded the cluster ID of the particular aspect termas a nominal feature.6123 ResultsIn this section, we will report our results on theshared task database which consists of Englishproduct reviews.
There are 3, 000 laptop andrestaurant related sentences, respectively.
Aspectswere annotated in these sentences, resulting in atotal of 6, 051 annotated aspects.
In our experi-ments, we used maximum entropy classifier withthe default parameter settings of the Java-basedmachine learning framework MALLET (McCal-lum, ).weightingcluster-polarityparserssentiment0.70.720.740.760.78systemsfull-systembaselineFigure 3: Accuracy on the restaurant test data.weightingcluster-polarityparserssentiment0.620.640.660.680.7systemsfull-systembaselineFigure 4: Accuracy on the laptop test data.Our accuracy measured on the restaurant andlaptop test databases can be seen on figures 3 and4.
On the x-axis the accuracy loss can be seencomparing to our baseline (n-gram features only)and full-system, while turning off various sets offeatures.
Firstly, the weighting of n-gram featuresare absent, then features based on aspect clusteringand words which indicate polarity in texts.
After-wards, features that are created using dependencyand constituency parsing are turned off and lastlysentiment features based on the SentiWordNet lex-icon are ignored.
It can be seen that omitting thefeatures based on parsing results in the most seri-ous drop in performance.
We achieved 1.1 and 2.6error reduction on the restaurant and laptop testdata using these features, respectively.In Table 1 the results of several other participat-ing teams can be seen on the restaurant and laptoptest data.
There were more than 30 submissions,from which we achieved the sixth and third bestresults on the restaurants and laptop domains, re-spectively.
At the bottom of the table the officialbaselines for each domain can be seen.Team restaurant laptopDCU 0.809 0.704NRC-Canada 0.801 0.704SZTE-NLP 0.752 0.669UBham 0.746 0.666USF 0.731 0.645ECNU 0.707 0.611baseline 0.642 0.510Table 1: Accuracy results of several other partici-pants.
Our system is named SZTE-NLP.4 ConclusionsIn this paper, we presented our contribution to theaspect term polarity subtask of the SemEval-2014Task 4 ?
Aspect Based Sentiment Analysis chal-lenge.
We proposed a supervised machine learn-ing technique that employs a rich feature set tar-geting aspect term polarity detection.
Among thefeatures designed here, the syntax-based featuregroup for the determination of the scopes of the as-pect terms showed the highest contribution.
In theend, our system was ranked as 6thand 3rd, achiev-ing an 0.752 and 0.669 accuracies for the restau-rant and laptop domains, respectively.613AcknowledgmentsViktor Hangya and Istv?an Varga were funded inpart by the European Union and the EuropeanSocial Fund through the project FuturICT.hu(T?AMOP-4.2.2.C-11/1/KONV-2012-0013).G?abor Berend and Rich?ard Farkas was partiallyfunded by the ?Hungarian National ExcellenceProgram?
(T?AMOP 4.2.4.A/2-11-1-2012-0001),co-financed by the European Social Fund.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10).Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.
Coling 2010 Organizing Com-mittee.Hakan Ceylan and Rada Mihalcea.
2011.
An efficientindexer for large n-gram corpora.
In ACL (SystemDemonstrations), pages 103?108.
The Associationfor Computer Linguistics.Gayatree Ganu, Noemie Elhadad, and Amelie Marian.2009.
Beyond the stars: Improving rating predic-tions using review text content.
In WebDB.Viktor Hangya and Richard Farkas.
2013.
Target-oriented opinion mining from tweets.
In CognitiveInfocommunications (CogInfoCom), 2013 IEEE 4thInternational Conference on, pages 251?254.
IEEE.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st ACL, pages 423?430.Andrew Kachites McCallum.
Mallet: A machinelearning for language toolkit.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe International Workshop on Semantic Evaluation,SemEval ?14.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, October.614
