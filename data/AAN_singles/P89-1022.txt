AUTOMATIC  ACQUIS IT ION OF THE LEXICAL SEMANTICS  OF VERBSFROM SENTENCE FRAMES*Mort Webster and Mitch MarcusDepartment of Computer and Information ScienceUniversity of Pennsylvania200 S. 33rd StreetPhiladelphia, PA 19104ABSTRACTThis paper presents a computational model of verbacquisition which uses what we will call the princi-ple of structured overeommitment to eliminate theneed for negative evidence.
The learner escapesfrom the need to be told that certain possibili-ties cannot occur (i.e., are "ungrammatical") byone simple expedient: It assumes that all proper-ties it has observed are either obligatory or for-bidden until it sees otherwise, at which point itdecides that what it thought was either obliga-tory or forbidden is merely optional.
This modelis built upon a classification of verbs based upona simple three-valued set of features which repre-sents key aspects of a verb's syntactic structure,its predicate/argument structure, and the map-ping between them.1 INTRODUCTIONThe problem of how language is learned is per-haps the most difficult puzzle in language under-standing.
It is necessary to understand learning inorder to understand how people use and organizelanguage.
To build truly robust natural anguagesystems, we must ultimately understand how toenable our systems to learn new forms themselves.Consider the problem of learning new lexicalitems in context.
To take a specific example, howis it that a child can learn the difference betweenthe verbs look and see (inspired by Landau andGleitman(1985) )?
They clearly have similar coremeanings, namely ~perceive by sight".
One ini-tially attractive and widely-held hypothesis i that*This work was partially supported by the DARPAgrant N00014-85-K0018, and Alto grant DAA29-84-9-0027.
The authors also wish to thank Beth Levin and theanonymotm reviewers of this paper for many helpful com-ments.
We ~ b~efit~l greatly from disctumion ofissuesof verb acquisition in children with Lila Gleitman.word meaning is learned directly by observationof the surrounding non-linguistic context.
Whilethis hypothesis ultimately only begs the question,it also runs into immediate substantive difficultieshere, since there is usually looking going on at thesame time as seeing and vice versa.
But how canone learn that these verbs differ in that look isan active verb and see is stative?
This difference,although difficult to observe in the environment,is clearly marked in the different syntactic framesthe two verbs are found in.
For example, see, be-ing a stative perception verb, can take a sentencecomplement:(1) John saw that Mary was reading.while look cannot:(2) * John looked that Mary was reading.Also look can be used in an imperative,(3) Look at the ball!while it sounds a bit strange to command someoneto see,(4) ?
See the ball!
(Examples like "look Jane, see Spot run!"notwithstanding.)
This difference reflects the factthat one can command someone to direct theireyes (look) but not to mentally perceive whatsomeone else perceives (see).
As this exampleshows, there are clear semantic differences betweenverbs that are reflected in the syntax, but not ob-vious by observation alone.
The fact that childrenare able to correctly learn the meanings of look andsee, as well as hundreds of other verbs, with mini-mal exposure suggests that there is some correla-tion between syntax and semantics that facilitatesthe learning of word meaning.Still, this and similar arguments ignore the factthat children do not have access to the negative177evidence crucial to establishing the active/stativedistinction of the look/see pair.
Children cannotknow that sentences like (2) and (4) do not oc-cur, and it is well established that children arenot corrected for syntactic errors.
Such evidencerenders highly implausible models like that ofPinker(198?
), which depend crucially on negativeexamples.
How then can this semantic/syntacticcorrelation be exploited?STRUCTURED OVERCOM-MITMENT AND A LEARNINGALGORITHMIn this paper, we will present a computationalmodel of verb acquisition which uses what we willcall the principle of structured o~ercomrnitment toeliminate the need for such negative vidence.
Inessence, our learner learns by initially jumping tothe strongest conclusions it can, simply assum-ing that everything within its descriptive systemthat it hasn't seen will never occur, and then laterweakening its hypotheses when faced with contra-dictory evidence.
Thus, the learner escapes fromthe need to be told that certain possibilities can-not occur (i.e.
are"ungrammatical') by the simpleexpedient of assuming that all properties it has ob-served are either always obligatory or always for-bidden.
If and when the learner discovers thatit was wrong about such a strong assumption, itreclassifies the property from either obligatory orforbidden to merely optional.Note that this learning principal requires thatno intermediate analysis is ever abandoned; anal-yses are only further refined by the weakening ofuniversals (X ALWAYS has property P) to existen-rials (X SOMETIMES has property P).
It is in thissense that the overcommitment is"structured.
"For such a learning strategy to work, it must bethe case that the set of features which underlies thelearning process are surface observable; the learnermust be able to determine of a particular instanceof (in this case) a verb structure whether someproperty is true or false of it.
This would seem toimply, as far as we can tell, a commitment to thenotion of em learning as selection widely presup-posed in the linguistic study of generative gram-mar (as surveyed, for example, in Berwick(1985).Thus, we propose that the problem of learning thecategory of a verb does not require that a natu-ral language understanding system synthesize mde novo a new structure to represent i s seman-tic class, but rather that it determine to which ofa predefined, presumably innate set of verb cate-gories a given verb belongs.
In what follows below,we argue that a relevant classification of verb cat-egories can be represented by simple conjunctionsof a finite number of predefined quasi-independentfeatures with no need for disjunction or complexboolean combinations of features.Given such a feature set, the Principal of Struc-tured Overcommitment defines a partial ordering(or, if one prefers, a tangled hierarchy) of verbs asfollows: At the highest level of the hierarchy is aset of verb classes where all the primary four fea-tures, where defined, are either obligatory or for-bidden.
Under each of these "primary" categoriesthere are those categories which differ from it onlyin that some category which is obligatory or for-bidden in the higher class is optional in the lowerclass.
Note that both obligatory and forbiddencategories at one level lead to the same optionalcategory at the next level down.The learning system, upon encountering a verbfor the first time, will necessarily classify that verbinto one of the ten top-level categories.
This is be-cause the learner assumes, for example, that if averb is used with an object upon first encounter,that it always has an object; if it has no object,that it never has an object, etc.
The learner willleave each verb classification unchanged upon en-countering new verb instances until a usage occursthat falsifies at least one of the current feature val-ues.
When encountering such a usage i.e.
a verbframe in which a property that is marked obliga-tory is missing, or a property that is marked for-bidden is present (there are no other possibilities)- then the learner reclassifies the verb by mov-ing down the hierarchy at least one level replacingthe OBLIGATORY or FORBIDDEN value of thatfeature with OPTIONAL.Note that, for each verb, the learner's classifica.tion moves monotonically lower on this hierarchy,until it eventually remains unchanged because thelearner has arrived at the correct value.
(Thusthis learner embodies a kind of em learning in thelimit.3 THE FEATURE SET  AND THEVERB H IERARCHYAs discussed above, our learner describes eachverb by means of a vector of features.
Someof these features describe syntactic propertiesof the verb (e.g.
"Takes an Object"), others de-scribe aspects of the theta-structure (the predi-cate/argument structure) of the verb (e.g.
"Takes178an Agent",~Ikkes a Theme"), while others de-scribe some key properties of the mapping be-tween theta-structure and syntactic structure(e.g.
"Theme Appears As Surface Object").
Mostof these features are three-valued; they de-scribe properties that are either always true (e.g.that"devour" always Takes An Object), alwaysfalse (e.g.
that "fall" never Takes An Object) orproperties that are optionally true (e.g.
that"eat"optionally Takes An Object).
Always true valueswill be indicated as"q-" below, always false valuesas" - "  and optional values as~0 ".All verbs are specified for the first threefeatures mentioned above: "Takes an Object"(OBJ),"Takes an Agent" (AGT), and"Takes aTheme" (THEME).
All verbs that allow OBJ andTHEME are specified for"Theme Appears As Ob-ject" (TAO), otherwise TAO is undefined.
At thehighest level of the hierarchy is a set of verb classeswhere all these primary features, where defined,are either obligatory or forbidden.
Thus there areat most 10 primary verb types; of the eight for thefirst three features, only two (-I--q-, and -H-+)split for TAO.The full set of features we assume include theprimary set of features (OBJ, AGT, THEME, andTAO), as described above,,and a secondary set offeatures which play a secondary role in the learn-ing algorithm, as will be discussed below.
Thesesecondary features are either thematic properties,or correlations between thematic and syntacticroles.
The thematic properties are: LOC - takes alocative; INST  - takes an instrument; and DAT -takes a dative.
The first thematic-syntactic map-ping feature "Instrument as Subject" is fake ifno instrument can.
appear in subject position (or,true if the subject is always an instrument, al-"though this is never the case.)
The second suchfeature "Theme as Chomeuf  (TAC) is the onlynon-trinary-valued feature in our learner; it spec-ifies what preposition marks the theme when it isnot realized as subject or object.
This feature, ifnot - ,  either takes a lexical item (a preposition,actually, as its value, or else the null string.
Wetreat verbs with double objects (e.g.
"John gaveMary the ball.")
as having a Dative as object, andthe theme as either marked by a null prepositionor, somewhat alternatively, as a bare NP chomeur.
(The facts we deal with here don't decide betweenthese two analyses.
)Note that this analysis does not make explictwhat can appear as object; it is a claim of theanalysis that if the verb is OBJ:?
or OBJ:0 and isTAO:-  or TAO:0, then whatever other thematicroles may occur can be realized as the object.
Thismay well be too strong, but we are still seeking acounterexample.Figure 1 shows our classification of some verbclasses of English, given this feature set.
(Thisclassification owes much to Levin(1985), as well asto Grimshaw(1983) and Jackendoff(1983).)
This isonly the beginning of such a classification, clearly;for example, we have concentrated our effortssolely on verbs that take simple NPs as comple-ments.
Our intention is merely to provide a richenough set of verb classes to show that our clas-sification scheme has merit, and that the learningalgorithm works.
We believe that this set of fea-tures is rich enough to describe not only the verbclasses covered here but other similar classes.
It isalso our hope that an analysis of verbs with richercomplement structures will extend the set of fea-tures without changing the analysis of the classescurrently handled.It is interesting to note that although the partialordering of verb classes is defined in terms of fea-tures defined over syntactic and theta structures,that there appears to be at least a very strong se-mantic reflex to the network.
Due to lack of space,we label verb cla-~ses in Figure 1 only with exem-plars; here we give a list of either typical verbs inthe class, and/or a brief description of the class,in semantic terms:?
Spray, load, inscribe, sow: Verbs of physicalcontact hat show the completive/noncomple-tire 1 alternation.
If completive, like "fill".?
Clear, empty: Similar to spray/load, but ifcompletive, like "empty".?
Wipe: Like clear, but no completive pattern.?
Throw: The following four verb classes all in-volve an object and a trajectory.
'~rhrow"verbs don't require a terminus of the trajec-tory.?
Present: Like "throw", as far as we can tell.?
Give: Requires a terminus.z This is the differ~ce between:I \]osded.the ay on the truck.sadI loaded the truck with hay.In the second case, but not the first, them is a implicationthat the truck is completely full.179SPRAY,LOADEMPTYSEARCHBREAK,DESTROYTOUCHPUTDEVOURFLYBREATHEF ILLG IVETFLOWERIO'+IA?TI+*'--+ IT+O II++01?
'TID TI ,*+T, I s~ II + I + n o i o ii o i o i - i ~th  i o iI m ~ E i l H i l i ~ i N / _ i iJR  i E ~ i , i  i + i l U  i ' P i  B ~ ~; ' H i  - -  ~ , i ~ _ _  - - i - - - ' - - - - + - - ~ ~I I n  - -  - -  - - | i l l i  - -  - - I ~ , J+i  mmmi  ~ imP i l i - im m , i  i l - i  i - e m i R~ ~ ~ i + i  I E m i l i ~ m  i - in  i i .
i  imi .
.
.
.
.
.
.
.
| /  i i I b-m i ~  ~ i r J i l  ~ i o  i i l - i  i ~  i , .
i l  i i| D ~ N I ~ E?, | n - -  i a i l i o |  .
.
.
.t .m,__~__: I  + - i , l  .
.
.
.
.
I - - \ [ + l ~ - -: ' m i , i  i ~ m , i m  .
.
.
.
~ ~~m ~ ~ m-ro l l  m i l l  m i ra  m m  mmlm iF igure  1: Some verb  feature  descr ip t ions .
( .
.
.
.
)I~wAYs  l1.
( - -0 .
)( -+- - )IS~-IM.mm~(+++o)IALWAYS +m~.~.l( *+.0 .
)1+1(00+0)( .
.~  . )
( .
.??)
(+-++)( +,,..*+ .,,.
0 ) ( .+.
O.t.
+. )"
Ik( 0 + .,,+ O) iqJSH (++00)F j=-++ 110+001~ t  ~ qlmulIF igure  2: The  verb  h ie rarchy .180?
Poke, jab, stick, touch: Some object follows atrajectory, resulting in surface contact.?
Hug: Surface contact, no trajectory.?
Fill: Inherently ?ompletive verbs.?
Search: Verbs that show a completive/non-completive alternation that doesn't involvephysical contact.?
Die, flower: Change of state.
Inherently non-agentive.?
Break: Change of state, undergoing causitivealternation.?
Destroy: Verbs of destruction.?
Pierce: Verbs of destruction i volving a tra-jectory.
* Devour, dynamite: Verbs of destruction withincorporated instruments?
Put: Simple change of location.?
Eat: Verbs of ingesting allowing instruments?
Breathe: Verbs of ingesting that incorporateinstrument?
Fall, swim: Verbs of movement with incorpo-rated theme and incorporated manner.?
Push: Exerting force; maybe somethingmoves, maybe not.?
Stand: Like "break s, but at a location.?
Rain: Verbs which have no agent, and incor-porate their patient.The set of verb classes that we have investigatedinteracts with our learning algorithm to define thepartial order of verb classes illustrated schemati-cally in Figure 2.For simplicity, this diagram is organized by thevalues of the four principle features of our system.Each subsystem shown in brackets shares the sameprinciple features; the individual verbs within eachsubsystem differ in secondary features as shown.If one of the primary features is made optional,the learning algorithm will map all verbs in eachsubsystem into the same subordinate subsystemas shown; of course, secondary feature values aremaintained as well.
In some cases, a sub-hierarchywithin a subsystem shows the learning of a sec-ondary feature.We should note that several of the primary verbclasses in Figure 2 are unlabelled because they cor-respond to no English verbs: The class " - - - - "would be the class of rain if it didn't allow formslike ~hail stones rained from the sky", while theclass '~+--I--t-" would be the class of verbs like "de-strof' if they only took instruments as subjects.Such classes may be artifacts of our analysis, orthey may be somewhat unlikely classes that arefilled in languages other than English.Note that sub-patterns in the primary featuresubvector seem to signal semantic properties in astraightforward way.
So, for example, it appearsthat verbs have the pattern {OBJ:+, THEME:+,TAO:-} only if they are inherently completive;consider "search" and "fill".
Similarly, the rareverbs that have the pattern {OBJ:-, THEME:-},i.e those that are truly intransitive, appear to in-corporate their theme into their meaning; a typi-cal case here is =swim".
Verbs that are {OBJ:-,AGT: -}  (e.g.
=die") are inherently stative; theyallow no agency.
Those verbs that are {AGT:+}incorporate the instrument of the operation intotheir meaning.
We will have to say about this be-low.4 THE LEARNING ALGORITHMAT WORKLet us now see how the learning algorithm worksfor a few verbs.Our model presupposes that the learner receivesas input a parse of the sentence from which to de-rive the subject and object grammatical relations,and a representation of what NPs serve as agent,patient, instrument and location.
This may beseen as begging the question of verb acquisition,because, it may be asked, how could an intelligentlearner know what entities function as agent, pa-tient, etc.
without understanding the meaning ofthe verb?
Our model in fact presupposes that alearner can distinguish between such general cat-egories as animate, inanimate, instrument, andlocative from direct observation of the environ-ment, without explicit support from verb meaning;i.e.
that it will be clear from observation em who isacting on em what em where.
This assumption isnot unreasonable; there is strong experimental ev-idence that children do in fact perceive even some-thing as subtle as the difference between animateand inanimate motion well before the two wordstage (see Golinkoff et al 1984).
Thisnotion thatagent, patient and the like can be derived fromdirect observation (perhaps focussed by what NPs181appear in the sentence) is a weak form of whatis sometimes called the em semantic bootstrap-ping hypothesis (Pinker(1984)).
The theory thatwe present here is actually a combination of thisweak form of semantic bootstrapping with what iscalled em syntactic bootstrapping, the notion thatsyntactic frames alone offer enough information toclassify verbs (see Naigles, Gleitman, and Gleit-man (in press) and Fisher, Gleitman and Gleit-man(1988).
)With this preliminary out of the way, let's turnto a simple example.
Suppose the learner encoun-ters the verb "break", never seen before, in thecontext(6) The window broke.The learner sees that the referent of "the window"is inanimate, and thus is the theme.
Given thisand the syntactic fzarne of (6), the learner can seethat em break (a) does not take an object, in thiscase, (b) does not take an agent, and (c) takesa patient.
By Structured Overcommitment, thelearner therefore assumes that em break em nevertakes an object, em never takes a subject, and emalways takes a patient.
Thus, it classifies em breakas {OBJ:-, AGT: - ,  THEME:+,  TAO: -}  ( i fTAOis undefined, it is assigned "-').
It also assumesthat em break is {DAT:-,  LOC:- ,  INST:-, ... }for similar reasons.
This is the class of DIE, oneof the toplevel verb classes.Next, suppose it sees(7) John broke the window.and sees from observation that the referent of"John" is an agent, the referent of "the window"a patient, and from syntax that "John" is sub-ject, and "the window" object.
That em breaktakes an object conflicts with the current view thatem break NEVER takes an object, and thereforethis strong assumption isweakened to say thatem break SOMETIMES takes an object.
Simi-larly, the learner must fall back to the positionthat em break SOMETIMES can have the themeserve as object, and can SOMETIMES have anagent.
This takes {OBJ:-, AGT: - ,  THEME:+,TAO: -}  to {OBJ:0, AGT:0, THEME:+,  TAO:0},which is the class of both em break and em stand.However, since it has never seen a locative for ernbreak, it assumes that em break falls into exactlythe category we have labelled as "break".22And how would it distinguish betweenThe vase stood on the table.madThere are, of course, many other possible ordersin which the learner might encounter the verb embreak.
Suppose the learner first encounters thepattern(8) John broke the window.beR)re any other occurrences of this verb.
Givenonly (8), it will assume that em break always takesan object, always takes an agent, always has a pa-tient, and always has the patient serving as ob-ject.
The learner will also assume that em breaknever takes a location, a dative, etc.
This willgive it the initial description of {OBJ:+, AGT:+,THEME:+,  TAO:+,  ..., LOC:-) ,  which causesthe learner to classify em break as falling intothe toplevel verb class of DEVOUR,  verbs of de-struction with the instrument incorporated intothe verb meaning.Next, suppose the learner sees(9) The hammer  broke the window.where the learner observes that '~hammer" is aninanimate object, and therefore must serve as in-strument, not agent.
This means that the earlierassumption that agent is necessary was an over-commitment (as was the unmentioned assump-tion that an instrument was forbidden).
Thelearner therefore weakens the description of embreak to {OBJ:+, AGT:0, THEME:-{-, TAO:+,..., LOC:- ,  INST:0}, which moves em break intothe verb class of DESTROY,  destruction withoutincorporated instrument.Finally (as it turns out), suppose the learnersees(10) The window broke.Now it discovers that the object is not obliga-tory, and also that the theme can appear as sub-ject, not object, which means that TAO is op-tional, not obligatory.
This now takes em break to{OBJ:0, AGT:0, THEME:+,  TAO:0, ... }, whichis the verb class of break.We interposed (9) between (8) and (10) in thissequence just to exercise the learner.
If (10) fol-lowed (8) directly, the learner would have taken embreak to verb class BREAK all the more quickly.Although we will not explicitly go through the ex-ercise here, it is important to our claims that anypermutation of the potential sentence frames of embreak will take the learner to BREAK,  althoughsome combinations require verb classes not shownThe base broke on the table?This is a probl~n we discuss at the end of this paper.182on our chart for the sake of simplicity (e.g.
theclass {OBJ:0, AGT: - ,  THEME:+, TAO:0} if ithasn't yet seen an agent as subject.
).We were somewhat surprised to note that thetrajectory of em break takes the learner through asequence of states whose semantics are useful ap-proximations of the meaning of this verb.
In thefirst case above, the learner goes through the classof "change of state without agency", into the classof BREAK, i.e.
"change of state involving no lo-cation".
In the second case, the trajectory takesthe learner through "destroy with an incorporatedinstrument", and then DESTROY into BREAK.In both of these cases, it happens that the trajec-tory of em break through our hierarchy causes itto have a meaning consistent with its final mean-ing at each point of the way.
While this will notalways be true, it seems that it is quite often thecase.
We find this property of our verb classifica-tion very encouraging, particularly given its gene-sis in our simple learning principle.We now consider a similar example for a dif-ferent verb, the verb em load, in somewhat terserform.
And again, we have chosen asomewhat indi-rect route to the final derived verb class to demon-strate complex trajectories through the space ofverb classes.
Assume the learner first encounters( I I )  John loads the hay onto the truck.From (11), the learner builds the representa-tion {OBJ:+, AGT:+, THEME:+, TAO:+, .
.
.
,LOC:+, .
.
.
,  DAT:-}, which lands the learner intothe class of PUT, i.e.
"simple change of location".We aasume that the learner can derive that "thetruck" is a locative both from the prepositionalmarking, and from direct observation.Next the learner encounters(12) John loads the hay.From this, the learner discovers that the locationis not obligatory, but merely optional, shiftingit to {OBJ:+, AGT:+, THEME:+, TAO:+, .
.
.
,LOC:O .
.
.
,  DAT:-}, the verb class of HUG, withthe general mean/ng of "surface contact with notrajectory.
"The next sentence ncountered is(13) John loads the truck with hay.This sentence tells the learner that the theme needonly optionally serve as object, that it can be ?shifted to a non-argument position marked withthe preposition em with.
This gives em loadthe description of {OBJ:+, AGT:+, THEME:+,TAO:0, TAC:with, .
.
.
,  LOC:0 .
.
.
.
DAT:-}.
Thisnew description takes em load now into the verbclass of POKE/TOUCH, surface contact by anobject that has followed some trajectory.
(Wehave explicitly indicated in our description herethat {DAT:-} was part of the verb description,rather than leaving this fact implicit, because weknew, of course, that this feature would be neededto distinguish between the verb classes of GIVEand POKE/TOUCH.
We should stress that thisand many other features are encoded as " - "  untilencountered by the learner; we have simply sup-pressed explicitly representing such features in ouraccount here unless needed.
)Finally, the learner encounters the sentence(14) John loads the truck.which makes it only optional that the thememust occur, shifting the verb representation to{OBJ:+, AGT:+, THEME:0, TAO:0, TAC:with,.
.
.
,  LOC:0 .
.
.
,  DAT:-}.
The principle four fea-tures of this description put the verb into the gen-eral area of WIPE, CLEAR and SPRAY/LOAD,but the optional locative, and the fact that thetheme can be marked with em with select for theclass of SPRAY/LOAD, verbs of physical contactthat show the completive/noncompletive alt rna-tion:Note that in this case again, the semantics of theverb classes along the learning trajectory are rea-sonable successive approximations to the meaningof the verb.5 FURTHER RESEARCH ANDSOME PROBLEMSOne difficulty with this approach which we havenot yet confronted is that real data is somewhatnoisy.
For example, although it is often claimedthat Motherese is extremely clean, one researcherhas observed that the verb "put", which requiresboth a location and an object to be fully grammat-ical, has been observed in Motherese (althoughextremely infrequently) without a location.
Westrongly suspect, of course, that the assumptionthat one instance suffices to change the learner'smodel is too strong.
It would be relatively easyto extend the model we give here with a coupleof bits to count the number of counterexamplesseen for each obligatory or forbidden feature, withtwo or three examples needed within some limitedtime period to shift the feature to optional.Can the model we describe here be taken as apsychological model?
At first glance, clearly not,183because this model appears to be deeply conser-vative, and as Pinker(1987) demonstrates, chil-dren freely use verbs in patterns that they havenot seen.
In our terms, they use verbs as if theyhad moved them down the hierarchy without ev-idence.
The facts as currently understood can beaccounted for by our model given one simple as-sumption: While children summarize their expo-sure to verb usages as discussed above, they willuse those verbs in highly productive alternations(as if they were in lower categories) for some pe-riod after exposure to the verb.
The claim is thattheir em usage might be non-conservative, evenif their representations of verb class are.
By thismodel, the child would restrict he usage of a givenverb to the represented usages only after some pe-riod of time.
The mechanisms for deriving criteriafor productive usage of verb patterns described byPinker(1987) could also be added to our modelwithout difficulty.
In essence, one would thenhave a non-conservative learner with a conserva-tive core.REFERENCES\[1\]\[2\]Berwick, 1t.
(1985) The Acquisition of Syntac-tic Knowledge.
Cambridge, MA: MIT Press.Fisher, C.; Gleitman, H.; and Gleitman, L.(1988) Relations between verb syntax andverb semantics: On the semantic ontent ofsubcategorization frames.
Submitted for pub-lication.\[3\] Golinkoff, R.M.
; Harding, C.G.
; Carson, V.;and Sexton, M.E.
(1984) The infant's percep-tion of causal events: the distinction betweenanimate and inanimate object.
In L.P. Lip-sitt and C. Rovee-Collier (Eds.)
Advances inInfancy Research 3: 145-65.\[4\] Grirnshaw, J.
(1983) Subcategorization a dgrammatical relations.
In A. Zaenen (Ed.
),Subjects and other subjects.
Evanston: Indi-ana University Linguistics Club.\[5\] Jackendoff, I~.
(1983) Semantics and cogni-tion.
Cambridge, MA: The MIT Press.\[6\] Landau, B. and Gleitman, L.R.
(1985) Lan-guage and ezperience: Evidence from theblind child.
Cambridge, MA: Harvard Univer-sity Press.\[7\] Levin, B.
(1985) Lexical semantics in review:An introduction.
In B. Levin (Ed.
), Lexicalsemantics in review.
Lezicon Project WorkingPapers, 1.
Cambridge, MA: MIT Center forCognitive Science.\[8\] Naigles, L.; Gleitman, H.; and Gleitman,L.R.
(in press) Children acquire word mean-ing components from syntactic evidence.
InE.
Dromi (Ed.)
Linguistic and conceptual de-velopment.
Ablex.\[9\] Pinker, S. (1984) Language Learnability andLanguage Development.
Cambridge, MA:Harvard University Press.\[10\] Pinker, S. (1987) Resolving a learnabilityparadox in the acquisition of the verb lexi-con.
Lezicon project working papers 17.
Cam-bridge, MA: MIT Center for Cognitive Sci-ence.184
