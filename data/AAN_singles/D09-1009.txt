Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 81?90,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPActive Learning by Labeling FeaturesGregory DruckDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003gdruck@cs.umass.eduBurr SettlesDept.
of Biostatistics &Medical InformaticsDept.
of Computer SciencesUniversity of WisconsinMadison, WI 53706bsettles@cs.wisc.eduAndrew McCallumDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003mccallum@cs.umass.eduAbstractMethods that learn from prior informa-tion about input features such as general-ized expectation (GE) have been used totrain accurate models with very little ef-fort.
In this paper, we propose an ac-tive learning approach in which the ma-chine solicits ?labels?
on features ratherthan instances.
In both simulated and realuser experiments on two sequence label-ing tasks we show that our active learningmethod outperforms passive learning withfeatures as well as traditional active learn-ing with instances.
Preliminary experi-ments suggest that novel interfaces whichintelligently solicit labels on multiple fea-tures facilitate more efficient annotation.1 IntroductionThe application of machine learning to new prob-lems is slowed by the need for labeled trainingdata.
When output variables are structured, an-notation can be particularly difficult and time-consuming.
For example, when training a condi-tional random field (Lafferty et al, 2001) to ex-tract fields such as rent, contact, features, and utilitiesfrom apartment classifieds, labeling 22 instances(2,540 tokens) provides only 66.1% accuracy.1Recent work has used unlabeled data and lim-ited prior information about input features to boot-strap accurate structured output models.
For ex-ample, both Haghighi and Klein (2006) and Mannand McCallum (2008) have demonstrated resultsbetter than 66.1% on the apartments task de-scribed above using only a list of 33 highly dis-criminative features and the labels they indicate.However, these methods have only been appliedin scenarios in which the user supplies such priorknowledge before learning begins.1Averaged over 10 randomly selected sets of 22 instances.In traditional active learning (Settles, 2009), themachine queries the user for only the labels of in-stances that would be most helpful to the machine.This paper proposes an active learning approach inwhich the user provides ?labels?
for input features,rather than instances.
A labeled input feature de-notes that a particular input feature, for examplethe word call, is highly indicative of a particularlabel, such as contact.
Table 1 provides an excerptof a feature active learning session.In this paper, we advocate using generalizedexpectation (GE) criteria (Mann and McCallum,2008) for learning with labeled features.
We pro-vide an alternate treatment of the GE objectivefunction used by Mann and McCallum (2008) anda novel speedup to the gradient computation.
Wethen provide a pool-based feature active learningalgorithm that includes an option to skip queries,for cases in which a feature has no clear label.We propose and evaluate feature query selectionalgorithms that aim to reduce model uncertainty,and compare to several baselines.
We evaluateour method using both real and simulated user ex-periments on two sequence labeling tasks.
Com-pared to previous approaches (Raghavan and Al-lan, 2007), our method can be used for both classi-fication and structured tasks, and the feature queryselection methods we propose perform better.We use experiments with simulated labelers onreal data to extensively compare feature query se-lection algorithms and evaluate on multiple ran-dom splits.
To make these simulations more re-alistic, the effort required to perform different la-beling actions is estimated from additional exper-iments with real users.
The results show that ac-tive learning with features outperforms both pas-sive learning with features and traditional activelearning with instances.In the user experiments, each annotator activelylabels instances, actively labels features one at atime, and actively labels batches of features orga-81accuracy 46.5?
60.5feature labelPHONE* contactcall contactdeposit rentmonth rentpets restrict.lease rentappointment contactparking featuresEMAIL* contactinformation contactaccuracy 60.5?
67.1feature labelwater utilitiesclose neighbor.garbage utilitiesincluded utilitiesfeaturesshopping neighbor.bart neighbor.downtown neighbor.TIME* contactbath sizeTable 1: Two iterations of feature active learning.Each table shows the features labeled, and the re-sulting change in accuracy.
Note that the word in-cluded was labeled as both utilities and features, andthat ?
denotes a regular expression feature.nized using a ?grid?
interface.
The results supportthe findings of the simulated experiments and pro-vide evidence that the ?grid?
interface can facili-tate more efficient annotation.2 Conditional Random FieldsIn this section we describe the underlying proba-bilistic model for all methods in this paper.
Wefocus on sequence labeling, though the describedmethods could be applied to other structured out-put or classification tasks.
We model the proba-bility of the label sequence y ?
Ynconditionedon the input sequence x ?
Xn, p(y|x; ?)
usingfirst-order linear-chain conditional random fields(CRFs) (Lafferty et al, 2001).
This probability isp(y|x; ?)
=1Zxexp(?i?j?jfj(yi, yi+1,x, i)),where Zxis the partition function and featurefunctions fjconsider the entire input sequenceand at most two consecutive output variables.The most probable output sequence and transitionmarginal distributions can be computed using vari-ants of Viterbi and forward-backward.Provided a training data distribution p?, we es-timate CRF parameters by maximizing the condi-tional log likelihood of the training data.L(?)
= Ep?
(x,y)[log p(y|x; ?
)]We use numerical optimization to maximize L(?
),which requires the gradient of L(?)
with respectto the parameters.
It can be shown that the par-tial derivative with respect to parameter j is equalto the difference between the empirical expecta-tion of Fjand the model expectation of Fj, whereFj(y,x) =?ifj(yi, yi+1,x, i).???jL(?)
= Ep?(x,y)[Fj(y,x)]?
Ep?(x)[Ep(y|x;?
)[Fj(y,x)]].We also include a zero-mean variance ?2= 10Gaussian prior on parameters in all experiments.22.1 Learning with missing labelsThe training set may contain partially labeled se-quences.
Let z denote missing labels.
We esti-mate parameters with this data by maximizing themarginal log-likelihood of the observed labels.LMML(?)
= Ep?
(x,y)[log?zp(y, z|x; ?
)]We refer to this training method as maximummarginal likelihood (MML); it has also been ex-plored by Quattoni et al (2007).The gradient of LMML(?)
can also be writtenas the difference of two expectations.
The first isan expectation over the empirical distribution of xand y, and the model distribution of z.
The secondis a double expectation over the empirical distribu-tion of x and the model distribution of y and z.???jLMML(?)
= Ep?(x,y)[Ep(z|y,x;?
)[Fj(y, z,x)]]?
Ep?(x)[Ep(y,z|x;?
)[Fj(y, z,x)]].We train models using LMML(?)
with expectedgradient (Salakhutdinov et al, 2003).To additionally leverage unlabeled data, wecompare with entropy regularization (ER).
ERadds a term to the objective function that en-courages confident predictions on unlabeled data.Training of linear-chain CRFs with ER is de-scribed by Jiao et al (2006).3 Generalized Expectation CriteriaIn this section, we give a brief overview of gen-eralized expectation criteria (GE) (Mann and Mc-Callum, 2008; Druck et al, 2008) and explain howwe can use GE to learn CRF parameters with esti-mates of feature expectations and unlabeled data.GE criteria are terms in a parameter estimationobjective function that express preferences on the210 is a default value that works well in many settings.82value of a model expectation of some function.Given a score function S, an empirical distributionp?
(x), a model distribution p(y|x; ?
), and a con-straint function Gk(x,y), the value of a GE crite-rion is G(?)
= S(Ep?(x)[Ep(y|x;?
)[Gk(x,y)]]).GE provides a flexible framework for parameterestimation because each of these elements can takean arbitrary form.
The most important differencebetween GE and other parameter estimation meth-ods is that it does not require a one-to-one cor-respondence between constraint functions Gkandmodel feature functions.
We leverage this flexi-bility to estimate parameters of feature-rich CRFswith a very small set of expectation constraints.Constraint functions Gkcan be normalized sothat the sum of the expectations of a set of func-tions is 1.
In this case, S may measure the di-vergence between the expectation of the constraintfunction and a target expectation?Gk.G(?)
=?Gklog(E[Gk(x,y)]), (1)where E[Gk(x,y)] = Ep?(x)[Ep(y|x;?
)[Gk(x,y)]].It can be shown that the partial derivative ofG(?)
with respect to parameter j is proportional tothe predicted covariance between the model fea-ture function Fjand the constraint function Gk.3???jG(?)
=?GkE[Gk(x,y)]?
(2)(Ep?(x)[Ep(y|x;?)[Fj(x,y)Gk(x,y)]?
Ep(y|x;?)[Fj(x,y)]Ep(y|x;?
)[Gk(x,y)]])The partial derivative shows that GE learns pa-rameter values for model feature functions basedon their predicted covariance with the constraintfunctions.
GE can thus be interpreted as a boot-strapping method that uses the limited training sig-nal to learn about parameters for related modelfeature functions.3.1 Learning with feature-label distributionsMann and McCallum (2008) apply GE to a linear-chain, first-order CRF.
In this section we providean alternate treatment that arrives at the same ob-jective function from the general form describedin the previous section.Often, feature functions in a first-order linear-chain CRF f are binary, and are the conjunction3If we use squared error for S, the partial derivative is thecovariance multiplied by 2(?Gk?
E[Gk(x,y)]).of an observational test q(x, i) and a label pair test1{yi=y?,yi+1=y??
}.4f(yi, yi+1,x, i) = 1{yi=y?,yi+1=y??
}q(x, i)The constraint functions Gkwe use here decom-pose and operate similarly, except that they onlyinclude a test for a single label.
Single label con-straints are easier for users to estimate and makeGE training more efficient.
Label transition struc-ture can be learned automatically from single la-bel constraints through the covariance-based pa-rameter update of Equation 2.
For convenience,we can write Gykto denote the constraint func-tion that combines observation test k with a testfor label y.
We also add a normalization constantCk= Ep?
(x)[?iqk(x, i)],Gyk(x,y) =?i1Ck1{yi=y}qk(x, i)Under this construction the expectation of Gykisthe predicted conditional probability that the labelat some arbitrary position i is y when the observa-tional test at i succeeds, p?
(yi=y|qk(x, i)=1; ?
).If we have a set of constraint functions {Gyk:y ?
Y}, and we use the score function in Equa-tion 1, then the GE objective function specifies theminimization of the KL divergence between themodel and target distributions over labels condi-tioned on the success of the observational test.
Ingeneral the objective function will consist of manysuch KL divergence penalties.Computing the first term of the covariance inEquation 2 requires a marginal distribution overthree labels, two of which will be consecutive, butthe other of which could appear anywhere in thesequence.
We can compute this marginal usingthe algorithm of Mann and McCallum (2008).
Aspreviously described, this algorithm is O(n|Y|3)for a sequence of length n. However, we makethe following novel observation: we do not needto compute the extra lattices for feature label pairswith?Gyk= 0, since this makes Equation 2 equalto zero.
In Mann and McCallum (2008), probabil-ities were smoothed so that ?y?Gyk> 0.
If weassume that only a small number of labels m havenon-zero probability, then the time complexity ofthe gradient computation is O(nm|Y|2).
In thispaper typically 1 ?m?
4, while |Y| is 11 or 13.4We this notation for an indicator function that returns 1if the condition in braces is satisfied, and 0 otherwise.83In experiments in this paper, using this optimiza-tion does not significantly affect final accuracy.We use numerical optimization to estimatemodel parameters.
In general GE objective func-tions are not convex.
Consequently, we initial-ize 0th-order CRF parameters using a sliding win-dow logistic regression model trained with GE.We also include a Gaussian prior on parameterswith ?2= 10 in the objective function.3.2 Learning with labeled featuresThe training procedure described above requiresa set of observational tests or input features withtarget distributions over labels.
Estimating a dis-tribution could be a difficult task for an annotator.Consequently, we abstract away from specifyinga distribution by allowing the user to assign labelsto features (c.f.
Haghighi and Klein (2006) , Drucket al (2008)).
For example, we say that the wordfeature call has label contact.
A label for a featuresimply indicates that the feature is a good indicatorof the label.
Note that features can have multiplelabels, as does included in the active learning ses-sion shown in Table 1.
We convert an input featurewith a set of labels L into a distribution by assign-ing probability 1/|L| for each l ?
L and probabil-ity 0 for each l /?
L. By assigning 0 probability tolabels l /?
L, we can use the speed-up described inthe previous section.3.3 Related WorkOther proposed learning methods use labeled fea-tures to label unlabeled data.
The resultingpartially-labeled corpus can be used to train a CRFby maximizing MML.
Similarly, prototype-drivenlearning (PDL) (Haghighi and Klein, 2006) opti-mizes the joint marginal likelihood of data labeledwith prototype input features for each label.
Ad-ditional features that indicate similarity to the pro-totypes help the model to generalize.
In a previ-ous comparison between GE and PDL (Mann andMcCallum, 2008), GE outperformed PDL withoutthe extra similarity features, whose constructionmay be problem-specific.
GE also performed bet-ter when supplied accurate label distributions.Additionally, both MML and PDL do not natu-rally generalize to learning with features that havemultiple labels or distributions over labels, as inthese scenarios labeling the unlabeled data is notstraightforward.
In this paper, we attempt to ad-dress this problem using a simple heuristic: whenthere are multiple choices for a token?s label, sam-ple a label.
In Section 5 we use this heuristic withMML, but in general obtain poor results.Raghavan and Allan (2007) also propose sev-eral methods for learning with labeled features,but in a previous comparison GE gave better re-sults (Druck et al, 2008).
Additionally, the gen-eralization of these methods to structured outputspaces is not straightforward.
Chang et al (2007)present an algorithm for learning with constraints,but this method requires users to set weights byhand.
We plan to explore the use of the recentlydeveloped related methods of Bellare et al (2009),Grac?a et al (2008), and Liang et al (2009) in fu-ture work.
Druck et al (2008) provide a surveyof other related methods for learning with labeledinput features.4 Active Learning by Labeling FeaturesFeature active learning, presented in Algorithm 1,is a pool-based active learning algorithm (Lewisand Gale, 1994) (with a pool of features ratherthan instances).
The novel components of thealgorithm are an option to skip a query and thenotion that skipping and labeling have differentcosts.
The option to skip is important when us-ing feature queries because a user may not knowhow to label some features.
In each iteration themodel is retrained using the train procedure, whichtakes as input a set of labeled features C and un-labeled data distribution p?.
For the reasons de-scribed in Section 3.3, we advocate using GE forthe train procedure.
Then, while the iteration costc is less than the maximum cost cmax, the featurequery q that maximizes the query selection met-ric ?
is selected.
The accept function determineswhether the labeler will label q.
If q is labeled, itis added to the set of labeled features C, and thelabel cost clabelis added to c. Otherwise, the skipcost cskipis added to c. This process continues forN iterations.4.1 Feature query selection methodsIn this section we propose feature query selectionmethods ?.
Queries with a higher scores are con-sidered better candidates.
Note again that by fea-tures we mean observational tests qk(x, i).
It isalso important to note these are not feature selec-tion methods since we are determining the featuresfor which supervisory feedback will be most help-ful to the model, rather than determining whichfeatures will be part of the model.84Algorithm 1 Feature Active LearningInput: empirical distribution p?, initial feature constraintsC, label cost clabel, skip cost cskip, max cost per iterationcmax, max iterations NOutput: model parameters ?for i = 1 to N do?
= train(p?, C)c = 0while c < cmaxdoq = argmaxqk?
(qk)if accept(q) thenC = C ?
label(q)c = c+ clabelelsec = c+ cskipend ifend whileend for?
= train(p?, C)We propose to select queries that provide thelargest reduction in model uncertainty.
We notatepossible responses to a query qkas g?.
The Ex-pected Information Gain (EIG) of a query is theexpectation of the reduction in model uncertaintyover all possible responses.
Mathematically, IG is?EIG(qk) = Ep(g?|qk;?)[Ep?
(x)[H(p(y|x; ?
)?H(p(y|x; ?g?
)]],where ?g?are the new model parameters if the re-sponse to qkis g?.
Unfortunately, this method iscomputationally intractable.
Re-estimating ?g?willtypically involve retraining the model, and do-ing this for each possible query-response pair isprohibitively expensive for structured output mod-els.
Computing the expectation over possible re-sponses is also difficult, as in this paper users mayprovide a set of labels for a query, and more gen-erally g?
could be a distribution over labels.Instead, we propose a tractable strategy for re-ducing model uncertainty, motivated by traditionaluncertainty sampling (Lewis and Gale, 1994).
Weassume that when a user responds to a query, thereduction in uncertainty will be equal to the To-tal Uncertainty (TU), the sum of the marginal en-tropies at the positions where the feature occurs.
?TU(qk) =?i?jqk(xi, j)H(p(yj|xi; ?
))Total uncertainty, however, is highly biased to-wards selecting frequent features.
A mean un-certainty variant, normalized by the feature?scount, would tend to choose very infrequent fea-tures.
Consequently we propose a tradeoff be-tween the two extremes, called weighted uncer-tainty (WU), that scales the mean uncertainty bythe log count of the feature in the corpus.
?WU(qk) = log(Ck)?TU(qk)Ck.Finally, we also suggest an uncertainty-based met-ric called diverse uncertainty (DU) that encour-ages diversity among queries by multiplying TUby the mean dissimilarity between the feature andpreviously labeled features.
For sequence labelingtasks, we can measure the relatedness of featuresusing distributional similarity.5?DU(qk) = ?TU(qk)1|C|?j?C1?sim(qk, qj)We contrast the notion of uncertainty describedabove with another type of uncertainty: the en-tropy of the predicted label distribution for the fea-ture, or expectation uncertainty (EU).
As abovewe also multiply by the log feature count.
?EU(qk) = log(Ck)H(p?
(yi= y|qk(x, i)=1; ?
))EU is flawed because it will have a large value fornon-discriminative features.The methods described above require the modelto be retrained between iterations.
To verify thatthis is necessary, we compare against query selec-tion methods that only consider the previously la-beled features.
First, we consider a feature queryselection method called coverage (cov) that aimsto select features that are dissimilar from existinglabeled features, increasing the labeled features??coverage?
of the feature space.
In order to com-pensate for choosing very infrequent features, wemultiply by the log count of the feature.
?cov(qk) = log(Ck)1|C|?j?C1?
sim(qk, qj)Motivated by the feature query selection methodof Tandem Learning (Raghavan and Allan, 2007)(see Section 4.2 for further discussion), we con-sider a feature selection metric similarity (sim)that is the maximum similarity to a labeled fea-ture, weighted by the log count of the feature.
?sim(qk) = log(Ck)maxj?Csim(qk, qj)5sim(qk, qj) returns the cosine similarity between contextvectors of words occurring in a window of ?3.85Features similar to those already labeled are likelyto be discriminative, and therefore likely to be la-beled (rather than skipped).
However, insufficientdiversity may also result in an inaccurate model,suggesting that coverage should select more use-ful queries than similarity.Finally, we compare with several passive base-lines.
Random (rand) assigns scores to featuresrandomly.
Frequency (freq) scores input featuresusing their frequency in the training data.
?freq(qk) =?i?jqk(xi, j)Top LDA (LDA) selects the top words from 50topics learned from the unlabeled data using la-tent Dirichlet alocation (LDA) (Blei et al, 2003).More specifically, the words w generated by eachtopic t are ranked using the conditional probabilityp(w|t).
The word feature is assigned its maximumrank across all topics.
?LDA(qk) = maxtrankLDA(qk, t)This method will select useful features if the top-ics discovered are relevant to the task.
A similarheuristic was used by Druck et al (2008).4.2 Related WorkTandem Learning (Raghavan and Allan, 2007) isan algorithm that combines feature and instanceactive learning for classification.
The algorithm it-eratively queries the user first for instance labels,then for feature labels.
Feature queries are selectedaccording to their co-occurrence with importantmodel features and previously labeled features.
Asnoted in Section 3.3, GE is preferable to the meth-ods Tandem Learning uses to learn with labeledfeatures.
We address the mixing of feature and in-stance queries in Section 4.3.In order to better understand differences in fea-ture query selection methodology, we proposed afeature query selection method motivated6by themethod used in Tandem Learning in Section 4.1.However, this method performs poorly in the ex-periments in Section 5.Liang et al (2009) simultaneously developeda method for learning with and actively selecting6The query selection method of Raghavan and Allan(2007) requires a stack that is modified between querieswithin each iteration.
Here query scores are only updatedafter each iteration of labeling.measurements, or target expectations with associ-ated noise.
The measurement selection methodproposed by Liang et al (2009) is based onBayesian experimental design and is similar tothe expected information gain method describedabove.
Consequently this method is likely to beintractable for real applications.
Note that Lianget al (2009) only use this method in synthetic ex-periments, and instead use a method similar to to-tal uncertainty for experiments in part-of-speechtagging.
Unlike the experiments presented in thispaper, Liang et al (2009) conduct only simulatedactive learning experiments and do not considerskipping queries.Sindhwani (Sindhwani et al, 2009) simultane-ously developed an active learning method thatqueries for both instance and feature labels thatare then used in a graph-based learning algorithm.They find that querying certain features outper-forms querying uncertain features, but this is likelybecause their query selection method is similarto the expectation uncertainty method describedabove, and consequently non-discriminative fea-tures may be queried often (see also the discus-sion in Section 4.1).
It is also not clear how thisgraph-based training method would generalize tostructured output spaces.4.3 Expectation Constraint Active LearningThroughout this paper, we have focussed on label-ing input features.
However, the proposed meth-ods generalize to queries for expectation estimatesof arbitrary functions, for example queries for thelabel distributions for input features, labels for in-stances (using a function that is non-zero only fora particular instance), partial labels for instances,and class priors.
The uncertainty-based query se-lection methods described in Section 4.1 applynaturally to these new query types.
Importantlythis framework would allow principled mixing ofdifferent query types, instead of alternating be-tween them as in Tandem Learning (Raghavan andAllan, 2007).
When mixing queries, it will beimportant to use different costs for different an-notation types (Vijayanarasimhan and Grauman,2008), and estimate the probability of obtaining auseful response to a query.
We plan to pursue thesedirections in future work.
This idea was also pro-posed by Liang et al (2009), but no experimentswith mixed active learning were presented.865 Simulated User ExperimentsIn this section we experiment with an automatedoracle labeler.
When presented an instance query,the oracle simply provides the true labels.
Whenpresented a feature query, the oracle first decideswhether to skip the query.
We have found thatusers are more likely to label features that are rel-evant for only a few labels.
Therefore, the oraclelabels a feature if the entropy of its per occurrencelabel expectation, H(p?
(yi= y|qk(x, i) = 1; ?))
?0.7.
The oracle then labels the feature using aheuristic: label the feature with the label whoseexpectation is highest, as well as any label whoseexpectation is at least half as large.We estimate the effort of different labeling ac-tions with preliminary experiments in which weobserve users labeling data for ten minutes.
Userstook an average of 4 seconds to label a feature, 2seconds to skip a feature, and 0.7 seconds to la-bel a token.
We setup experiments such that eachiteration simulates one minute of labeling by set-ting cmax= 60, cskip= 2 and clabel= 4.
Forinstance active learning, we use Algorithm 1 butwithout the skip option, and set clabel= 0.7.
Weuse N = 10 iterations, so the entire experimentsimulates 10 minutes of annotation time.
For ef-ficiency, we consider the 500 most frequent unla-beled features in each iteration.
To start, ten ran-domly selected seed labeled features are provided.We use random (rand) selection, uncertaintysampling (US) (using sequence entropy, normal-ized by sequence length) and information den-sity (ID) (Settles and Craven, 2008) to select in-stance queries.
We use Entropy Regularization(ER) (Jiao et al, 2006) to leverage unlabeled in-stances.7We weight the ER term by choosing thebest8weight in {10?3, 10?2, 10?1, 1, 10} multi-plied by#labeled#unlabeledfor each data set and query se-lection method.
Seed instances are provided suchthat the simulated labeling time is equivalent to la-beling 10 features.We evaluate on two sequence labeling tasks.The apartments task involves segmenting 300apartment classified ads into 11 fields includingfeatures, rent, neighborhood, and contact.
We usethe same feature processing as Haghighi and Klein(2006), with the addition of context features in awindow of ?3.
The cora references task is to ex-tract 13 BibTeX fields such as author and booktitle7Results using self-training instead of ER are similar.8As measured by test accuracy, giving ER an advantage.method apartments coramean final mean finalER rand 48.1 53.6 75.9 81.1ER US 51.7 57.9 76.0 83.2ER ID 51.4 56.9 75.9 83.1MML rand 47.7 51.2 58.6 64.6MML WU 57.6 60.8 61.0 66.2GE rand 59.0 64.8?77.6 83.7GE freq 66.5?71.6?68.6 79.8GE LDA 65.7?71.4?74.9 85.0GE cov 68.2?
?72.6?73.5 83.3GE sim 57.8 65.9?67.1 79.2GE EU 66.5?71.6?68.6 79.8GE TU 70.1??73.6?
?76.9 88.2?
?GE WU 71.6??74.6??80.3??88.1?
?GE DU 70.5??74.4??78.4?87.5?
?Table 2: Mean and final token accuracy results.A?or?denotes that a GE method significantlyoutperforms all non-GE or passive GE methods,respectively.
Bold entries significantly outperformall others.
Methods in italics are passive.from 500 research paper references.
We use a stan-dard set of word, regular expressions, and lexiconfeatures, as well as context features in a windowof ?3.
All results are averaged over ten random80:20 splits of the data.5.1 ResultsTable 2 presents mean (across all iterations) andfinal token accuracy results.
On the apartmentstask, GE methods greatly outperform MML9andER methods.
Each uncertainty-based GE methodalso outperforms all passive GE methods.
On thecora task, only GE with weighted uncertainty sig-nificantly outperforms ER and passive GE meth-ods in terms of mean accuracy, but all uncertainty-based GE methods provide higher final accuracy.This suggests that on the cora task, active GEmethods are performing better in later iterations.Figure 1, which compares the learning curves ofthe best performing methods of each type, showsthis phenomenon.
Further analysis reveals that theuncertainty-based methods are choosing frequentfeatures that are more likely to be skipped thanthose selected randomly in early iterations.We next compare with the results of relatedmethods published elsewhere.
We cannot makeclaims about statistical significance, but the results9Only the best MML results are shown.87illustrate the competitiveness of our method.
The74.6% final accuracy on apartments is higher thanany result obtained by Haghighi and Klein (2006)(the highest is 74.1%), higher than the supervisedHMM results reported by Grenager et al (2005)(74.4%), and matches the results of Mann and Mc-Callum (2008) with GE with more accurate sam-pled label distributions and 10 labeled examples.Chang et al (2007) only obtain better results than88.2% on cora when using 300 labeled examples(two hours of estimated annotation time), 5000 ad-ditional unlabeled examples, and extra test time in-ference constraints.
Note that obtaining these re-sults required only 10 simulated minutes of anno-tation time, and that GE methods are provided noinformation about the label transition matrix.6 User ExperimentsAnother advantage of feature queries is that fea-ture names are concise enough to be browsed,rather than considered individually.
This allowsthe design of improved interfaces that can furtherincrease the speed of feature active learning.
Webuilt a prototype interface that allows the user toquickly browse many candidate features.
The fea-tures are split into groups of five features each.Each group contains features that are related, asmeasured by distributional similarity.
The featureswithin each group are sorted according to the ac-tive learning metric.
This interface, displayed inFigure 3, may be useful because features in thesame group are likely to have the same label.We conduct three types of experiments.
First, auser labels instances selected by information den-sity, and models are trained using ER.
The in-stance labeling interface allows the user to labeltokens quickly by extending the current selectionone token at a time and only requiring a singlekeystroke to label an entire segment.
Second,the user labels features presented one-at-a-time byweighted uncertainty, and models are trained us-ing GE.
To aid the user in understanding the func-tion of the feature quickly, we provide several ex-amples of the feature occurring in context and themodel?s current predicted label distribution for thefeature.
Finally, the user labels features organizedusing the grid interface described in the previousparagraph.
Weighted uncertainty is used to sortfeature queries within each group, and GE is usedto train models.
Each iteration of labeling laststwo minutes, and there are five iterations.
Retrain-ing with ER between iterations takes an averageof 5 minutes on cora and 3 minutes on apart-ments.
With GE, the retraining times are on av-erage 6 minutes on cora and 4 minutes on apart-ments.
Consequently, even when viewed with to-tal time, rather than annotation time, feature activelearning is beneficial.
While waiting for models toretrain, users can perform other tasks.Figure 2 displays the results.
User 1 labeledapartments data, while Users 2 and 3 labeled coradata.
User 1 was able to obtain much better resultswith feature labeling than with instance labeling,but performed slightly worse with the grid inter-face than with the serial interface.
User 1 com-mented that they found the label definitions forapartments to be imprecise, so the other experi-ments were conducted on the cora data.
User 2obtained better results with feature labeling thaninstance labeling, and obtained higher mean ac-curacy with the grid interface.
User 3 was muchbetter at labeling features than instances, and per-formed especially well using the grid interface.7 ConclusionWe proposed an active learning approach in whichfeatures, rather than instances, are labeled.
Wepresented an algorithm for active learning withfeatures and several feature query selection meth-ods that approximate the expected reduction inmodel uncertainty of a feature query.
In simu-lated experiments, active learning with featuresoutperformed passive learning with features, anduncertainty-based feature query selection outper-formed other baseline methods.
In both simulatedand real user experiments, active learning withfeatures outperformed passive and active learningwith instances.
Finally, we proposed a new label-ing interface that leverages the conciseness of fea-ture queries.
User experiments suggested that thisgrid interface can improve labeling efficiency.AcknowledgmentsWe thank Kedar Bellare for helpful discussions and Gau-rav Chandalia for providing code.
This work was supportedin part by the Center for Intelligent Information Retrievaland the Central Intelligence Agency, the National SecurityAgency and National Science Foundation under NSF grant#IIS-0326249.
The second author was supported by a grantfrom National Human Genome Research Institute.
Any opin-ions, findings and conclusions or recommendations are theauthors?
and do not necessarily reflect those of the sponsor.882 4 6 8 1035404550556065707580simulated annotation time (minutes)tokenaccuracyapartmentsER + uncertaintyMML + weighted uncertaintyGE + frequencyGE + weighted uncertainty 2 4 6 8 1045505560657075808590simulated annotation time (minutes)tokenaccuracycoraER + uncertaintyMML + weighted uncertaintyGE + randomGE + weighted uncertaintyFigure 1: Token accuracy vs. time for best performing ER, MML, passive GE, and active GE methods.2 4 6 8 105101520253035404550556065annotation time (minutes)tokenaccuracyuser 1 ?
apartmentsER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 10303540455055606570annotation time (minutes)tokenaccuracyuser 2 ?
coraER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 103540455055606570758085annotation time (minutes)tokenaccuracyuser 3 ?
coraER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid)Figure 2: User experiments with instance labeling and feature labeling with the serial and grid interfaces.Figure 3: Grid feature labeling interface.
Boxes on the left contain groups of features that appear insimilar contexts.
Features in the same group often receive the same label.
On the right, the model?scurrent expectation and occurrences of the selected feature in context are displayed.89ReferencesKedar Bellare, Gregory Druck, and Andrew McCal-lum.
2009.
Alternating projections for learning withexpectation constraints.
In UAI.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent dirichlet alocation.Journal of Machine Learning Research, 3:2003.Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007.Guiding semi-supervision with constraint-drivenlearning.
In ACL, pages 280?287.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2008.
Learning from labeled features usinggeneralized expectation criteria.
In SIGIR.Joao Grac?a, Kuzman Ganchev, and Ben Taskar.
2008.Expectation maximization and posterior constraints.In J.C. Platt, D. Koller, Y.
Singer, and S. Roweis,editors, Advances in Neural Information ProcessingSystems 20.
MIT Press.Trond Grenager, Dan Klein, and Christopher D. Man-ning.
2005.
Unsupervised learning of field segmen-tation models for information extraction.
In ACL.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In HTL-NAACL.Feng Jiao, Shaojun Wang, Chi-Hoon Lee, RussellGreiner, and Dale Schuurmans.
2006.
Semi-supervised conditional random fields for improvedsequence segmentation and labeling.
In ACL, pages209?216.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In ICML.David D. Lewis and William A. Gale.
1994.
A sequen-tial algorithm for training text classifiers.
In SIGIR,pages 3?12, New York, NY, USA.
Springer-VerlagNew York, Inc.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning from measurements in exponential fami-lies.
In ICML.Gideon Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learn-ing of conditional random fields.
In ACL.A.
Quattoni, S. Wang, L.-P Morency, M. Collins, andT.
Darrell.
2007.
Hidden conditional random fields.IEEE Transactions on Pattern Analysis and MachineIntelligence, 29:1848?1852, October.Hema Raghavan and James Allan.
2007.
An interac-tive algorithm for asking and incorporating featurefeedback into support vector machines.
In SIGIR,pages 79?86.Ruslan Salakhutdinov, Sam Roweis, and ZoubinGhahramani.
2003.
Optimization with em andexpectation-conjugate-gradient.
In ICML, pages672?679.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In EMNLP.Burr Settles.
2009.
Active learning literature survey.Technical Report 1648, University of Wisconsin -Madison.Vikas Sindhwani, Prem Melville, and Richard D.Lawrence.
2009.
Uncertainty sampling and trans-ductive experimental design for active dual supervi-sion.
In ICML.Sudheendra Vijayanarasimhan and Kristen Grauman.2008.
Multi-level active prediction of useful imageannotations for recognition.
In NIPS.90
