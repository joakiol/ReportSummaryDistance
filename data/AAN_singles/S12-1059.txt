First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435?440,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUKP: Computing Semantic Textual Similarity byCombining Multiple Content Similarity MeasuresDaniel Ba?r?, Chris Biemann?, Iryna Gurevych?
?, and Torsten Zesch??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universita?t Darmstadt?Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationwww.ukp.tu-darmstadt.deAbstractWe present the UKP system which performedbest in the Semantic Textual Similarity (STS)task at SemEval-2012 in two out of three met-rics.
It uses a simple log-linear regressionmodel, trained on the training data, to combinemultiple text similarity measures of varyingcomplexity.
These range from simple char-acter and word n-grams and common sub-sequences to complex features such as Ex-plicit Semantic Analysis vector comparisonsand aggregation of word similarity based onlexical-semantic resources.
Further, we em-ploy a lexical substitution system and statisti-cal machine translation to add additional lex-emes, which alleviates lexical gaps.
Our finalmodels, one per dataset, consist of a log-linearcombination of about 20 features, out of thepossible 300+ features implemented.1 IntroductionThe goal of the pilot Semantic Textual Similarity(STS) task at SemEval-2012 is to measure the de-gree of semantic equivalence between pairs of sen-tences.
STS is fundamental to a variety of tasksand applications such as question answering (Linand Pantel, 2001), text reuse detection (Clough etal., 2002) or automatic essay grading (Attali andBurstein, 2006).
STS is also closely related to tex-tual entailment (TE) (Dagan et al, 2006) and para-phrase recognition (Dolan et al, 2004).
It differsfrom both tasks, though, insofar as those operate onbinary similarity decisions while STS is defined asa graded notion of similarity.
STS further requires abidirectional similarity relationship to hold betweena pair of sentences rather than a unidirectional en-tailment relation as for the TE task.A multitude of measures for computing similar-ity between texts have been proposed in the pastbased on surface-level and/or semantic content fea-tures (Mihalcea et al, 2006; Landauer et al, 1998;Gabrilovich and Markovitch, 2007).
The exist-ing measures exhibit two major limitations, though:Firstly, measures are typically used in separation.Thereby, the assumption is made that a singlemeasure inherently captures all text characteristicswhich are necessary for computing similarity.
Sec-ondly, existing measures typically exclude similar-ity features beyond content per se, thereby implyingthat similarity can be computed by comparing textcontent exclusively, leaving out any other text char-acteristics.
While we can only briefly tackle the sec-ond issue here, we explicitly address the first one bycombining several measures using a supervised ma-chine learning approach.
With this, we hope to takeadvantage of the different facets and intuitions thatare captured in the single measures.In the following section, we describe the featurespace in detail.
Section 3 describes the machinelearning setup.
After describing our submitted runs,we discuss the results and conclude.2 Text Similarity MeasuresWe now describe the various features we have tried,also listing features that did not prove useful.2.1 Simple String-based MeasuresString Similarity Measures These measures op-erate on string sequences.
The longest common435substring measure (Gusfield, 1997) compares thelength of the longest contiguous sequence of char-acters.
The longest common subsequence measure(Allison and Dix, 1986) drops the contiguity re-quirement and allows to detect similarity in caseof word insertions/deletions.
Greedy String Tiling(Wise, 1996) allows to deal with reordered text partsas it determines a set of shared contiguous sub-strings, whereby each substring is a match of maxi-mal length.
We further used the following measures,which, however, did not make it into the final mod-els, since they were subsumed by the other mea-sures: Jaro (1989), Jaro-Winkler (Winkler, 1990),Monge and Elkan (1997), and Levenshtein (1966).Character/word n-grams We compare charactern-grams following the implementation by Barro?n-Ceden?o et al (2010), thereby generalizing the orig-inal trigram variant to n = 2, 3, .
.
.
, 15.
We alsocompare word n-grams using the Jaccard coefficientas previously done by Lyon et al (2001), and thecontainment measure (Broder, 1997).
As high n ledto instabilities of the classifier due to their high in-tercorrelation, only n = 1, 2, 3, 4 was used.2.2 Semantic Similarity MeasuresPairwise Word Similarity The measures forcomputing word similarity on a semantic level op-erate on a graph-based representation of words andthe semantic relations among them within a lexical-semantic resource.
For this system, we used the al-gorithms by Jiang and Conrath (1997), Lin (1998a),and Resnik (1995) on WordNet (Fellbaum, 1998).In order to scale the resulting pairwise word sim-ilarities to the text level, we applied the aggregationstrategy by Mihalcea et al (2006): The sum of theidf -weighted similarity scores of each word with thebest-matching counterpart in the other text is com-puted in both directions, then averaged.
In our ex-periments, the measure by Resnik (1995) proved tobe superior to the other measures and was used in allword similarity settings throughout this paper.Explicit Semantic Analysis We also used the vec-tor space model Explicit Semantic Analysis (ESA)(Gabrilovich and Markovitch, 2007).
Besides Word-Net, we used two additional lexical-semantic re-sources for the construction of the ESA vector space:Wikipedia and Wiktionary1.Textual Entailment We experimented with usingthe BIUTEE textual entailment system (Stern andDagan, 2011) for generating entailment scores toserve as features for the classifier.
However, thesefeatures were not selected by the classifier.Distributional Thesaurus We used similaritiesfrom a Distributional Thesaurus (similar to Lin(1998b)) computed on 10M dependency-parsed sen-tences of English newswire as a source for pairwiseword similarity, one additional feature per POS tag.However, only the feature based on cardinal num-bers (CD) was selected in the final models.2.3 Text Expansion MechanismsLexical Substitution System We used the lexicalsubstitution system based on supervised word sensedisambiguation (Biemann, 2012).
This system au-tomatically provides substitutions for a set of about1,000 frequent English nouns with high precision.For each covered noun, we added the substitutionsto the text and computed the pairwise word similar-ity for the texts as described above.
This feature al-leviates the lexical gap for a subset of words.Statistical Machine Translation We used theMoses SMT system (Koehn et al, 2007) to trans-late the original English texts via three bridge lan-guages (Dutch, German, Spanish) back to English.Thereby, the idea was that in the translation pro-cess additional lexemes are introduced which allevi-ate potential lexical gaps.
The system was trained onEuroparl made available by Koehn (2005), using thefollowing configuration which was not optimized forthis task: WMT112 baseline without tuning, withMGIZA alignment.
The largest improvement wasreached for computing pairwise word similarity (asdescribed above) on the concatenation of the origi-nal text and the three back-translations.2.4 Measures Related to Structure and StyleIn our system, we also used measures which gobeyond content and capture similarity along thestructure and style dimensions inherent to texts.However, as we report later on, for this content-1www.wiktionary.org20-5-grams, grow-diag-final-and alignment, msd-bidirec-tional-fe reodering, interpolation and kndiscount436oriented task they were not selected by the classifier.Nonetheless, we briefly list them for completeness.Structural similarity between texts can be de-tected by computing stopword n-grams (Sta-matatos, 2011).
Thereby, all content-bearing wordsare removed while stopwords are preserved.
Stop-word n-grams of both texts are compared using thecontainment measure (Broder, 1997).
In our experi-ments, we tested n-gram sizes for n = 2, 3, .
.
.
, 10.We also compute part-of-speech n-grams forvarious POS tags which we then compare using thecontainment measure and the Jaccard coefficient.We also used two similarity measures betweenpairs of words (Hatzivassiloglou et al, 1999): Wordpair order tells whether two words occur in thesame order in both texts (with any number of wordsin between), word pair distance counts the numberof words which lie between those of a given pair.To compare texts along the stylistic dimension,we further use a function word frequencies mea-sure (Dinu and Popescu, 2009) which operates on aset of 70 function words identified by Mosteller andWallace (1964).
Function word frequency vectorsare computed and compared by Pearson correlation.We also include a number of measures whichcapture statistical properties of texts, such as type-token ratio (TTR) (Templin, 1957) and sequentialTTR (McCarthy and Jarvis, 2010).3 System DescriptionWe first run each of the similarity measures intro-duced above separately.
We then use the resultingscores as features for a machine learning classifier.Pre-processing Our system is based on DKPro3,a collection of software components for naturallanguage processing built upon the Apache UIMAframework.
During the pre-processing phase, we to-kenize the input texts and lemmatize using the Tree-Tagger implementation (Schmid, 1994).
For somemeasures, we additionally apply a stopword filter.Feature Generation We now compute similarityscores for the input texts with all measures and forall configurations introduced in Section 2.
This re-sulted in 300+ individual score vectors which servedas features for the following step.3http://dkpro-core-asl.googlecode.comRun Features1 Greedy String TilingLongest common subsequence (2 normalizations)Longest common substringCharacter 2-, 3-, and 4-gramsWord 1- and 2-grams (Containment, w/o stopwords)Word 1-, 3-, and 4-grams (Jaccard)Word 2- and 4-grams (Jaccard, w/o stopwords)Word Similarity (Resnik (1995) on WordNetaggregated according to Mihalcea et al (2006);2 variants: complete texts + difference only)Explicit Semantic Analysis (Wikipedia, Wiktionary)Distributional Thesaurus (POS: Cardinal numbers)2 All Features of Run 1Lexical Substitution for Word Sim.
(complete texts)SMT for Word Sim.
(complete texts as above)3 All Features of Run 2Random numbers from [4.5, 5] for surprise datasetsTable 1: Feature sets of our three system configurationsFeature Combination The feature combinationstep uses the pre-computed similarity scores, andcombines their log-transformed values using a linearregression classifier from the WEKA toolkit (Hall etal., 2009).
We trained the classifier on the trainingdatasets of the STS task.
During the developmentcycle, we evaluated using 10-fold cross-validation.Post-processing For Runs 2 and 3, we applied apost-processing filter which stripped all charactersoff the texts which are not in the character range[a-zA-Z0-9].
If the texts match, we set their similar-ity score to 5.0 regardless of the classifier?s output.4 Submitted RunsRun 1 During the development cycle, we identi-fied 19 features (see Table 1) which achieved thebest performance on the training data.
For eachof the known datasets, we trained a separate clas-sifier and applied it to the test data.
For the surprisedatasets, we trained the classifier on a joint datasetof all known training datasets.Run 2 For the Run 2, we were interested in theeffects of two additional features: lexical substitu-tion and statistical machine translation.
We addedthe corresponding measures to the feature set of Run1 and followed the same evaluation procedure.Run 3 For the third run, we used the same featureset as for Run 2, but returned random numbers from[4.5, 5] for the sentence pairs in the surprise datasets.437Dim.
Text Similarity Features PAR VID SEBest Feature Set, Run 1 .711 .868 .735Best Feature Set, Run 2 .724 .868 .742Content Pairwise Word Similarity .564 .835 .527Character n-grams .658 .771 .554Explicit Semantic Analysis .427 .781 .619Word n-grams .474 .782 .619String Similarity .593 .677 .744Distributional Thesaurus .494 .481 .365Lexical Substitution .228 .554 .483Statistical Machine Translation .287 .652 .516Structure Part-of-speech n-grams .193 .265 .557Stopword n-grams .211 .118 .379Word Pair Order .104 .077 .295Style Statistical Properties .168 .225 .325Function Word Frequencies .179 .142 .189Table 2: Best results for single measures, grouped by di-mension, on the training datasets MSRpar, MSRvid, andSMTeuroparl, using 10-fold cross-validation5 Results on Training DataEvaluation was carried out using the official scorerwhich computes Pearson correlation of the humanrated similarity scores with the the system?s output.In Table 2, we report the results achieved oneach of the training datasets using 10-fold cross-validation.
The best results were achieved for thefeature set of Run 2, with Pearson?s r = .724,r = .868, and r = .742 for the datasets MSR-par, MSRvid, and SMTeuroparl, respectively.
Whileindividual classes of content similarity measuresachieved good results, a different class performedbest for each dataset.
However, text similarity mea-sures related to structure and style achieved onlypoor results on the training data.
This was to be ex-pected due to the nature of the data, though.6 Results on Test DataBesides the Pearson correlation for the union of alldatasets (ALL), the organizers introduced two addi-tional evaluation metrics after system submission:ALLnrm computes Pearson correlation after the sys-tem outputs for each dataset are fitted to the goldstandard using least squares, and Mean refers to theweighted mean across all datasets, where the weightdepends on the number of pairs in each dataset.In Table 3, we report the offical results achievedon the test data.
The best configuration of our systemwas Run 2 which was ranked #1 for the evaluation#1 #2 #3 Sys.
r1 r2 r3 PAR VID SE WN SN1 2 1 UKP2 .823 .857 .677 .683 .873 .528 .664 .4932 3 5 TL .813 .856 .660 .698 .862 .361 .704 .4683 1 2 TL .813 .863 .675 .734 .880 .477 .679 .3984 4 4 UKP1 .811 .855 .670 .682 .870 .511 .664 .4675 6 13 UNT .784 .844 .616 .535 .875 .420 .671 .403....................................87 85 70 B/L .311 .673 .435 .433 .299 .454 .586 .390Table 3: Official results on the test data for the top 5participating runs out of 89 which were achieved on theknown datasets MSRpar, MSRvid, and SMTeuroparl, aswell as on the surprise datasets OnWN and SMTnews.
Wereport the ranks (#1: ALL, #2: ALLnrm, #3: Mean) andthe corresponding Pearson correlation r according to thethree offical evaluation metrics (see Sec.
6).
The providedbaseline is shown at the bottom of this table.metrics ALL (r = .823)4 and Mean (r = .677), and#2 for ALLnrm (r = .857).
An exhaustive overviewof all participating systems can be found in the STStask description (Agirre et al, 2012).7 Conclusions and Future WorkIn this paper, we presented the UKP system, whichperformed best across the three official evaluationmetrics in the pilot Semantic Textual Similarity(STS) task at SemEval-2012.
While we did notreach the highest scores on any of the single datasets,our system was most robust across different data.
Infuture work, it would be interesting to inspect theperformance of a system that combines the outputof all participating systems in a single linear model.We also propose that two major issues with thedatasets are tackled in future work: (a) It is unclearhow to judge similarity between pairs of texts whichcontain contextual references such as on Mondayvs.
after the Thanksgiving weekend.
(b) For severalpairs, it is unclear what point of view to take, e.g.
forthe pair An animal is eating / The animal is hopping.Is the pair to be considered similar (an animal is do-ing something) or rather not (eating vs. hopping)?Acknowledgements This work has been sup-ported by the Volkswagen Foundation as part of theLichtenberg-Professorship Program under grant No.I/82806, and by the Klaus Tschira Foundation underproject No.
00.133.2008.499% confidence interval: .807 ?
r ?
.837438ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: A Pi-lot on Semantic Textual Similarity.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tion, in conjunction with the 1st Joint Conference onLexical and Computational Semantics.Lloyd Allison and Trevor I. Dix.
1986.
A bit-stringlongest-common-subsequence algorithm.
InformationProcessing Letters, 23:305?310.Yigal Attali and Jill Burstein.
2006.
Automated es-say scoring with e-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3).Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, andGorka Labaka.
2010.
Plagiarism Detection acrossDistant Language Pairs.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics, pages 37?45.Chris Biemann.
2012.
Creating a System for Lexi-cal Substitutions from Scratch using Crowdsourcing.Language Resources and Evaluation: Special Issueon Collaboratively Constructed Language Resources,46(2).Andrei Z. Broder.
1997.
On the resemblance and con-tainment of documents.
Proceedings of the Compres-sion and Complexity of Sequences, pages 21?29.Paul Clough, Robert Gaizauskas, Scott S.L.
Piao, andYorick Wilks.
2002.
METER: MEasuring TExtReuse.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages152?159.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual EntailmentChallenge.
In Machine Learning Challenges, LectureNotes in Computer Science, pages 177?190.
Springer.Liviu P. Dinu and Marius Popescu.
2009.
Ordinal mea-sures in authorship identification.
In Proceedings ofthe 3rd PAN Workshop.
Uncovering Plagiarism, Au-thorship and Social Software Misuse, pages 62?66.William B. Dolan, Chris Quirk, and Chris Brockett.2004.
Unsupervised Construction of Large ParaphraseCorpora: Exploiting Massively Parallel News Sources.In Proceedings of the 20th International Conferenceon Computational Linguistics, pages 350?356.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting Semantic Relatedness using Wikipedia-basedExplicit Semantic Analysis.
In Proceedings of the20th International Joint Conference on Artificial In-telligence, pages 1606?1611.Dan Gusfield.
1997.
Algorithms on Strings, Trees andSequences: Computer Science and Computational Bi-ology.
Cambridge University Press.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11(1):10?18.Vasileios Hatzivassiloglou, Judith L. Klavans, andEleazar Eskin.
1999.
Detecting text similarity overshort passages: Exploring linguistic feature combina-tions via machine learning.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, pages203?212.Matthew A. Jaro.
1989.
Advances in record linkagemethodology as applied to the 1985 census of TampaFlorida.
Journal of the American Statistical Associa-tion, 84(406):414?420.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxonomy.In Proceedings of the 10th International Conferenceon Research in Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the ACL 2007 Demo and Poster Sessions,pages 177?180.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings of the10th Machine Translation Summit, pages 79?86.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes, 25(2):259?284.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.Dekang Lin and Patrick Pantel.
2001.
Discovery of In-ference Rules for Question Answering.
Natural Lan-guage Engineering, 7(4):343?360.Dekang Lin.
1998a.
An information-theoretic definitionof similarity.
In Proceedings of International Confer-ence on Machine Learning, pages 296?304.Dekang Lin.
1998b.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics, pages 768?774.Caroline Lyon, James Malcolm, and Bob Dickerson.2001.
Detecting short passages of similar text in largedocument collections.
In Proceedings of Conferenceon Empirical Methods in Natural Language Process-ing, pages 118?125.Philip M. McCarthy and Scott Jarvis.
2010.
MTLD,vocd-D, and HD-D: A validation study of sophisti-439cated approaches to lexical diversity assessment.
Be-havior research methods, 42(2):381?92.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and Knowledge-based Measuresof Text Semantic Similarity.
In Proceedings of the 21stNational Conference on Artificial Intelligence, pages775?780.Alvaro Monge and Charles Elkan.
1997.
An efficientdomain-independent algorithm for detecting approxi-mately duplicate database records.
In Proceedings ofthe SIGMOD Workshop on Data Mining and Knowl-edge Discovery, pages 23?29.Frederick Mosteller and David L. Wallace.
1964.
In-ference and disputed authorship: The Federalist.Addison-Wesley.Philip Resnik.
1995.
Using Information Content to Eval-uate Semantic Similarity in a Taxonomy.
In Proceed-ings of the 14th International Joint Conference on Ar-tificial Intelligence, pages 448?453.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49.Efstathios Stamatatos.
2011.
Plagiarism detectionusing stopword n-grams.
Journal of the Ameri-can Society for Information Science and Technology,62(12):2512?2527.Asher Stern and Ido Dagan.
2011.
A ConfidenceModel for Syntactically-Motivated Entailment Proofs.In Proceedings of the International Conference on Re-cent Advances in Natural Language Processing, pages455?462.Mildred C. Templin.
1957.
Certain language skills inchildren.
University of Minnesota Press.William E. Winkler.
1990.
String Comparator Metricsand Enhanced Decision Rules in the Fellegi-SunterModel of Record Linkage.
In Proceedings of the Sec-tion on Survey Research Methods, pages 354?359.Michael J.
Wise.
1996.
YAP3: Improved detection ofsimilarities in computer program and other texts.
InProceedings of the 27th SIGCSE technical symposiumon Computer science education, pages 130?134.440
