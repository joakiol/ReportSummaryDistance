Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 280?284,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLearning Better Rule Extraction with Translation Span AlignmentJingbo Zhu    Tong Xiao   Chunliang ZhangNatural Language Processing LaboratoryNortheastern University, Shenyang, China{zhujingbo,xiaotong,zhangcl}@mail.neu.edu.cnAbstractThis paper presents an unsupervised ap-proach to learning translation span align-ments from parallel data that improvessyntactic rule extraction by deleting spuri-ous word alignment links and adding newvaluable links based on bilingual transla-tion span correspondences.
Experiments onChinese-English translation demonstrateimprovements over standard methods fortree-to-string and tree-to-tree translation.1 IntroductionMost syntax-based statistical machine translation(SMT) systems typically utilize word alignmentsand parse trees on the source/target side to learnsyntactic transformation rules from parallel data.The approach suffers from a practical problem thateven one spurious (word alignment) link can pre-vent some desirable syntactic translation rules fromextraction, which can in turn affect the quality oftranslation rules and translation performance (Mayand Knight 2007; Fossum et al 2008).
To addressthis challenge, a considerable amount of previousresearch has been done to improve alignment qual-ity by incorporating some statistics and linguisticheuristics or syntactic information into wordalignments (Cherry and Lin 2006; DeNero andKlein 2007; May and Knight 2007; Fossum et al2008; Hermjakob 2009; Liu et al 2010).Unlike their efforts, this paper presents a simpleapproach that automatically builds the translationspan alignment (TSA) of a sentence pair by utiliz-ing a phrase-based forced decoding technique, andthen improves syntactic rule extraction by deletingspurious links and adding new valuable links basedon bilingual translation span correspondences.
Theproposed approach has two promising properties.SVPADVPNNSimportsVBZhaveDTVBNRBfallendrasticallythe??jianshao???dafudu?
?jinkou?leNNVV ASAD VPVPSNP Frontier nodeWord alignmentFigure 1.
A real example of Chinese-English sentencepair with word alignment and both-side parse trees.Some blocked Tree-to-string Rules:r1: AS(?)
?
haver2: NN(??)
?
the importsr3: S (NN:x1 VP:x2) ?
x1 x2Some blocked Tree-to-tree Rules:r4: AS(?)
?
VBZ(have)r5: NN(??)
?
NP(DT(the) NNS(imports))r6: S(NN:x1 VP:x2) ?
S(NP:x1 VP:x2)r7: VP(AD:x1 VP(VV:x2 AS:x3))?
VP(VBZ:x3 ADVP(RB:x1 VBN:x2))Table 1.
Some useful syntactic rules are blocked due tothe spurious link between ???
and ?the?.Firstly, The TSAs are constructed in an unsuper-vised learning manner, and optimized by the trans-lation model during the forced decoding process,without using any statistics and linguistic heuristicsor syntactic constraints.
Secondly, our approach isindependent of the word alignment-based algo-rithm used to extract translation rules, and easy toimplement.2 Translation Span Alignment ModelDifferent from word alignment, TSA is a processof identifying span-to-span alignments betweenparallel sentences.
For each translation span pair,2801.
Extract phrase translation rules R from the parallelcorpus with word alignment, and construct a phrase-based translation model M.2.
Apply M to implement phrase-based forced decodingon each training sentence pair (c, e), and output itsbest derivation d* that can transform c into e.3.
Build a TSA of each sentence pair (c, e) from its bestderivation d*, in which each rule r in d* is used toform a translation span pair {src(r)<=>tgt(r)}.Figure 2.
TSA generation algorithm.
src(r) and tgt(r)indicate the source and target side of rule r.its source (or target) span is a sequence of source(or target) words.
Given a source sentence c=c1...cn,a target sentence e=e1...em, and its word alignmentA, a translation span pair ?
is a pair of source span(ci...cj) and target span (ep...eq))( qpji ec ?=?where ?
indicates that the source span (ci...cj) andthe target span (ep...eq) are translational equivalent.We do not require that ?
must be consistent withthe associated word alignment A in a TSA model.Figure 2 depicts the TSA generation algorithmin which a phrase-based forced decoding tech-nique is adopted to produce the TSA of each sen-tence pair.
In this work, we do not apply syntax-based forced decoding (e.g., tree-to-string) becausephrase-based models can achieve the state-of-the-art translation quality with a large amount of train-ing data, and are not limited by any constituentboundary based constraints for decoding.Formally, given a sentence pair (c, e), thephrase-based forced decoding technique aims tosearch for the best derivation d* among all consis-tent derivations that convert the given source sen-tence c into the given target sentence e with respectto the current translation model induced from thetraining data, which can be expressed by)|)((Prmaxarg)(),(* cdTGTdedTGTecDd?=?
?=          (1)where D(c,e) is the set of candidate derivations thattransform c to e, and TGT(d) is a function that out-puts the yield of a derivation d. ?
indicates parame-ters of the phrase-based translation model learnedfrom the parallel corpus.The best derivation d* produced by forced de-coding can be viewed as a sequence of translationsteps (i.e., phrase translation rules), expressed bykrrrd ??
?= ...* 21 ,c = ??
???
??
?e =  the imports have drastically fallenThe best derivation d* produced by forced decoding:r1: ??
?
the importsr2: ???
??
?
drastically fallenr3: ?
?
haveGenerating TSA from d*:[??
]<=>[the imports][???
??
]<=>[drastically fallen][?
]<=>[have]Table 2.
Forced decoding based TSA generation on theexample sentence pair in Fig.
1.where ri indicates a phrase rule used to form d*.
?is a composition operation that combines rules{r1...rk} together to produce the target translation.As mentioned above, the best derivation d* re-spects the input sentence pair (c, e).
It means thatfor each phrase translation rule ri used by d*, itssource (or target) side exactly matches a span ofthe given source (or target) sentence.
The sourceside src(ri) and the target side tgt(ri) of each phrasetranslation rule ri in d* form a translation span pair{src(ri)<=>tgt(ri)} of (c,e).
In other words, theTSA of (c,e) is a set of translation span pairs gen-erated from phrase translation rules used by thebest derivation d*.
The forced decoding based TSAgeneration on the example sentence pair in Figure1 can be shown in Table 2.3 Better Rule Extraction with TSAsTo better understand the particular task that wewill address in this section, we first introduce adefinition of inconsistent with a translation spanalignment.
Given a sentence pair (c, e) with theword alignment A and the translation span align-ment P, we call a link (ci, ej)?A inconsistent withP, if  ci and ej are covered respectively by two dif-ferent translation span pairs in P and vice versa.
(ci, ej)?A inconsistent with P  ?)()(:)()(:??????tgtesrccPORtgtesrccPjiji?????????
?where src(?)
and tgt(?)
indicate the source and tar-get span of a translation span pair ?.By this, we will say that a link (ci, ej)?A is aspurious link if it is inconsistent with the givenTSA.
Table 3 shows that an original link (4?1)are covered by two different translation span pairs281Source Target WA TSA1: ??
1: the 1?2 [1,1]<=>[1,2]2: ???
2: imports 2?4 [2,3]<=>[4,5]3: ??
3: have 3?5 [4,4]<=>[3,3]4: ?
4: drastically 4?15: fallen (null)?3Table 3.
A sentence pair with the original word align-ment (WA) and the translation span alignment (TSA).
([4,4]<=>[3,3]) and ([1,1] <=>[1,2]), respectively.In such a case, we think that this link (4?1) is aspurious link according to this TSA, and should beremoved for rule extraction.Given a resulting TSA P, there are four differenttypes of translation span pairs, such as one-to-one,one-to-many, many-to-one, and many-to-manycases.
For example, the TSA shown in Table 3contains a one-to-one span pair ([4,4]<=>[3,3]), aone-to-many span pair ([1,1]<=>[1,2]) and amany-many span pair ([2,3]<=>[4,5]).
In such acase, we can learn a confident link from a one-to-one translation span pair that is preferred by thetranslation model in the forced decoding basedTSA generation approach.
If such a confident linkdoes not exist in the original word alignment, weconsider it as a new valuable link.Until now, a natural way is to use TSAs to di-rectly improve word alignment quality by deletingsome spurious links and adding some new confi-dent links, which in turn improves rule quality andtranslation quality.
In other words, if a desirabletranslation rule was blocked due to some spuriouslinks, we will output this translation rule.
Let?srevisit the example in Figure 1 again.
The blockedtree-to-string r3 can be extracted successfully afterdeleting the spurious link (?, the), and a new tree-to-string rule r1 can be extracted after adding a newconfident link (?, have) that is inferred from aone-to-one translation span pair [4,4]<=>[3,3].4 Experiments4.1 SetupWe utilized a state-of-the-art open-source SMTsystem NiuTrans (Xiao et al 2012) to implementsyntax-based models in the following experiments.We begin with a training parallel corpus of Chi-nese-English bitexts that consists of 8.8M Chinesewords and 10.1M English words in 350K sentencepairs.
The GIZA++ tool was used to perform theMethod Prec% Rec% F1% Del/Sent Add/SentBaseline 83.07 75.75 79.25 - -TSA 84.01 75.46 79.51 1.5 1.1Table 4.
Word alignment precision, recall and F1-scoreof various methods on 200 sentence pairs of Chinese-English data.bi-directional word alignment between the sourceand the target sentences, referred to as the baselinemethod.
For syntactic translation rule extraction,minimal GHKM (Galley et al, 2004) rules are firstextracted from the bilingual corpus whose sourceand target sides are parsed using the Berkeleyparser (Petrov et al 2006).
The composed rules arethen generated by composing two or three minimalrules.
A 5-gram language model was trained on theXinhua portion of English Gigaword corpus.
Beamsearch and cube pruning techniques (Huang andChiang 2007) were used to prune the search spacefor all the systems.
The base feature set used for allsystems is similar to that used in (Marcu et al2006), including 14 base features in total such as 5-gram language model, bidirectional lexical andphrase-based translation probabilities.
All featureswere log-linearly combined and their weights wereoptimized by performing minimum error rate train-ing (MERT) (Och 2003).
The development data setused for weight training comes from NIST MT03evaluation set, consisting of 326 sentence pairs ofless than 20 words in each Chinese sentence.
Twotest sets are NIST MT04 (1788 sentence pairs) andMT05 (1082 sentence pairs) evaluation sets.
Thetranslation quality is evaluated in terms of the case-insensitive IBM-BLEU4 metric.4.2 Effect on Word AlignmentTo investigate the effect of the TSA method onword alignment, we designed an experiment toevaluate alignment quality against gold standardannotations.
There are 200 random chosen andmanually aligned Chinese-English sentence pairsused to assert the word alignment quality.
Forword alignment evaluation, we calculated precision,recall and F1-score over gold word alignment.Table 4 depicts word alignment performance ofthe baseline and TSA methods.
We apply the TSAsto refine the baseline word alignments, involvingspurious link deletion and new link insertion op-erations.
Table 4 shows our method can yield im-provements on precision and F1-score, onlycausing a little negative effect on recall.2824.3 Translation QualityMethod # of Rules MT03 MT04 MT05Baseline (T2S) 33,769,071 34.10 32.55 30.15TSA (T2S) 32,652,26134.61+(+0.51)33.01+(+0.46)30.66+(+0.51)Baseline (T2T) 24,287,206 34.51 32.20 31.78TSA (T2T) 24,119,71934.85(+0.34)32.92*(+0.72)32.22+(+0.44)Table 5.
Rule sizes and IBM-BLEU4 (%) scores ofbaseline and our method (TSA) in tree-to-string (T2S)and tree-to-tree (T2T) translation on Dev set (MT03)and two test sets (MT04 and MT05).
+ and * indicatesignificantly better on performance comparison at p<.05and p<.01, respectively.Table 5 depicts effectiveness of our TSA methodon translation quality in tree-to-string and tree-to-tree translation tasks.
Table 5 shows that our TSAmethod can improve both syntax-based translationsystems.
As mentioned before, the resulting TSAsare essentially optimized by the translation model.Based on such TSAs, experiments show that spuri-ous link deletion and new valuable link insertioncan improve translation quality for tree-to-stringand tree-to-tree systems.5 Related WorkPrevious studies have made great efforts to incor-porate statistics and linguistic heuristics or syntac-tic information into word alignments (Ittycheriahand Roukos 2005; Taskar et al 2005; Moore et al2006; Cherry and Lin 2006; DeNero and Klein2007; May and Knight 2007; Fossum et al 2008;Hermjakob 2009; Liu et al 2010).
For example,Fossum et al (2008) used a discriminativelytrained model to identify and delete incorrect linksfrom original word alignments to improve string-to-tree transformation rule extraction, which incor-porates four types of features such as lexical andsyntactic features.
This paper presents an approachto incorporating translation span alignments intoword alignments to delete spurious links and addnew valuable links.Some previous work directly models the syntac-tic correspondence in the training data for syntacticrule extraction (Imamura 2001; Groves et al 2004;Tinsley et al 2007; Sun et al 2010a, 2010b; Paulset al 2010).
Some previous methods infer syntac-tic correspondences between the source and thetarget languages through word alignments and con-stituent boundary based syntactic constraints.
Sucha syntactic alignment method is sensitive to wordalignment behavior.
To combat this, Pauls et al(2010) presented an unsupervised ITG alignmentmodel that directly aligns syntactic structures forstring-to-tree transformation rule extraction.
Onemajor problem with syntactic structure alignmentis that syntactic divergence between languages canprevent accurate syntactic alignments between thesource and target languages.May and Knight (2007) presented a syntactic re-alignment model for syntax-based MT that usessyntactic constraints to re-align a parallel corpuswith word alignments.
The motivation behind theirmethods is similar to ours.
Our work differs from(May and Knight 2007) in two major respects.First, the approach proposed by May and Knight(2007) first utilizes the EM algorithm to obtainViterbi derivation trees from derivation forests ofeach (tree, string) pair, and then produces Viterbialignments based on obtained derivation trees.
Ourforced decoding based approach searches for thebest derivation to produce translation span align-ments that are used to improve the extraction oftranslation rules.
Translation span alignments areoptimized by the translation model.
Secondly, theirmodels are only applicable for syntax-based sys-tems while our method can be applied to bothphrase-based and syntax-based translation tasks.6 ConclusionThis paper presents an unsupervised approach toimproving syntactic transformation rule extractionby deleting spurious links and adding new valuablelinks with the help of bilingual translation spanalignments that are built by using a phrase-basedforced decoding technique.
In our future work, it isworth studying how to combine the best of our ap-proach and discriminative word alignment modelsto improve rule extraction for SMT models.AcknowledgmentsThis research was supported in part by the NationalScience Foundation of China (61073140), the Spe-cialized Research Fund for the Doctoral Programof Higher Education (20100042110031) and theFundamental Research Funds for the Central Uni-versities in China.283ReferencesColin Cherry and Dekang Lin.
2006.
Soft syntactic con-straints for word alignment through discriminativetraining.
In Proc.
of ACL.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Proc.of ACL.Victoria Fossum, Kevin Knight and Steven Abney.2008.
Using syntax to improve word alignment pre-cision for syntax-based machine translation.
In Proc.of the Third Workshop on Statistical Machine Trans-lation, pages 44-52.Michel Galley, Mark Hopkins, Kevin Knight and DanielMarcu.
2004.
What's in a translation rule?
In Proc.
ofHLT-NAACL 2004, pp273-280.Declan Groves, Mary Hearne and Andy Way.
2004.Robust sub-sentential alignment of phrase-structuretrees.
In Proc.
of COLING, pp1072-1078.Ulf Hermjakob.
2009.
Improved word alignment withstatistics and linguistic heuristics.
In Proc.
of EMNLP,pp229-237Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
of ACL, pp144-151.Kenji Imamura.
2001.
Hierarchical Phrase AlignmentHarmonized with Parsing.
In Proc.
of NLPRS,pp377-384.Abraham Ittycheriah and Salim Roukos.
2005.
A maxi-mum entropy word aligner for Arabic-English ma-chine translation.
In Proc.
of HLT/EMNLP.Yang Liu, Qun Liu and Shouxun Lin.
2010.
Discrimina-tive word alignment by linear modeling.
Computa-tional Linguistics, 36(3):303-339Daniel Marcu, Wei Wang, Abdessamad Echihabi andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proc.
of EMNLP, pp44-52.Jonathan May and Kevin Knight.
2007.
Syntactic re-alignment models for machine translation.
In Proc.
ofEMNLP-CoNLL.Robert C. Moore, Wen-tau Yih and Andreas Bode.
2006.Improved discriminative bilingual word alignment.In Proc.
of ACLFranz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL.Adam Pauls, Dan Klein, David Chiang and KevinKnight.
2010.
Unsupervised syntactic alignment withinversion transduction grammars.
In Proc.
of NAACL,pp118-126Slav Petrov, Leon Barrett, Roman Thibaux and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of ACL, pp433-440.Jun Sun, Min Zhang and Chew Lim Tan.
2010a.
Explor-ing Syntactic Structural Features for Sub-TreeAlignment Using Bilingual Tree Kernels.
In Proc.
ofACL, pp306-315.Jun Sun, Min Zhang and Chew Lim Tan.
2010b.
Dis-criminative Induction of Sub-Tree Alignment usingLimited Labeled Data.
In Proc.
of COLING, pp1047-1055.Ben Taskar, Simon Lacoste-Julien and Dan Klein.
2005.A discriminative matching approach to word align-ment.
In Proc.
of HLT/EMNLPJohn Tinsley, Ventsislav Zhechev, Mary Hearne andAndy Way.
2007.
Robust language pair-independentsub-tree alignment.
In Proc.
of MT Summit XI.Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li.
2012.NiuTrans: An Open Source Toolkit for Phrase-basedand Syntax-based Machine Translation.
In Proceed-ings of ACL, demonstration session284
