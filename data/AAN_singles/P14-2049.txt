Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 296?301,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproving sparse word similarity models with asymmetric measuresJean Mark GawronSan Diego State Universitygawron@mail.sdsu.eduAbstractWe show that asymmetric models based onTversky (1977) improve correlations withhuman similarity judgments and nearestneighbor discovery for both frequent andmiddle-rank words.
In accord with Tver-sky?s discovery that asymmetric similarityjudgments arise when comparing sparseand rich representations, improvement onour two tasks can be traced to heavilyweighting the feature bias toward the rarerword when comparing high- and mid-frequency words.1 IntroductionA key assumption of most models of similarity isthat a similarity relation is symmetric.
This as-sumption is foundational for some conceptions,such as the idea of a similarity space, in whichsimilarity is the inverse of distance; and it is deeplyembedded into many of the algorithms that buildon a similarity relation among objects, such asclustering algorithms.
The symmetry assumptionis not, however, universal, and it is not essentialto all applications of similarity, especially when itcomes to modeling human similarity judgments.Citing a number of empirical studies, Tversky(1977) calls symmetry directly into question, andproposes two general models that abandon sym-metry.
The one most directly related to a largebody of word similarity work that followed is whathe calls the ratio model, which defines sim(a, b)as:f(A ?
B)f(A ?
B) + ?f(A\B) + ?f(B\A)(1)Here A and B represent feature sets for the objectsa and b respectively; the term in the numerator is afunction of the set of shared features, a measure ofsimilarity, and the last two terms in the denomina-tor measure dissimilarity: ?
and ?
are real-numberweights; when ?
6= ?, symmetry is abandoned.To motivate such a measure, Tversky presentsexperimental data with asymmetric similarity re-sults, including similarity comparisons of coun-tries, line drawings of faces, and letters.
Tverskyshows that many similarity judgment tasks havean inherent asymmetry; but he also argues, fol-lowing Rosch (1975), that certain kinds of stimuliare more naturally used as foci or standards thanothers.
Goldstone (in press) summarizes the re-sults succinctly: ?Asymmetrical similarity occurswhen an object with many features is judged asless similar to a sparser object than vice versa; forexample, North Korea is judged to be more likeChina than China is [like] North Korea.?
Thus,one source of asymmetry is the comparison ofsparse and dense representations.The relevance of such considerations to wordsimilarity becomes clear when we consider thatfor many applications, word similarity measuresneed to be well-defined when comparing very fre-quent words with infrequent words.
To make thisconcrete, let us consider a word representationin the word-as-vector paradigm (Lee, 1997; Lin,1998), using a dependency-based model.
Sup-pose we want to measure the semantic similarityof boat, rank 682 among the nouns in the BNCcorpus studied below, which has 1057 nonzerodependency features based on 50 million wordsof data, with dinghy, rank 6200, which has only113 nonzero features.
At the level of the vec-tor representations we are using, these are eventsof very different dimensionality; that is, there areten times as many features in the representation ofboat as there are in the representation of dinghy.
Ifin Tversky/Rosch terms, the more frequent wordis also a more likely focus, then this is exactlythe kind of situation in which asymmetric similar-ity judgments will arise.
Below we show that an296asymmetric measure, using ?
and ?
biased in fa-vor of the less frequent word, greatly improves theperformance of a dependency-based vector modelin capturing human similarity judgments.Before presenting these results, it will be help-ful to slightly reformulate and slightly generalizeTversky?s ratio model.
The reformulation will al-low us to directly draw the connection betweenthe ratio model and a set of similarity measuresthat have played key roles in the similarity litera-ture.
First, since Tversky has primarily additive fin mind, we can reformulate f(A ?
B) as followsf(A ?
B) =?f?A?Bwght(f) (2)Next, since we are interested in generalizing fromsets of features, to real-valued vectors of features,w1, w2, we define?SI(w1, w2) =?f?w1?w2SI(w1[f ], w2[f ]).
(3)Here SI is some numerical operation on real-number feature values (SI stands for shared infor-mation).
If the operation is MIN and w1[f ] andw2[f ] both contain the feature weights for f , then?f?A?Bwght(f)= ?MIN(w1, w2)=?f?w1?w2MIN(w1[f ], w2[f ]),so with SI set to MIN, Equation (3) includes Equa-tion (2) as a special case.
Similarly, ?
(w1, w1)represents the summed feature weights of w1, andtherefore,f(w1\w2) = ?
(w1, w1) ?
?
(w1, w2)In this generalized form, then, (1) becomes?(w1,w2)?(w1,w2)+?[?(w1,w1)??(w1,w2)]+?[?(w2,w2)??(w1,w2)]=?(w1,w2)??(w1,w1)+??(w2,w2)+?(w1,w2)?(?+?)?
(w1,w2)(4)Thus, if ?
+ ?
= 1, Tversky?s ratio model be-comes simply:sim(w1, w2) =?(w1,w2)??(w1,w1)+(1??)?
(w2,w2)(5)The computational advantage of this reformula-tion is that the core similarity operation ?
(w1, w2)is done on what is generally only a small numberof shared features, and the ?
(wi, wi) calculations(which we will call self-similarities), can be com-puted in advance.
Note that sim(w1, w2) is sym-metric if and only if ?
= 0.5.
When ?
> 0.5,sim(w1, w2) is biased in favor of w1as the refer-ent; When ?
< 0.5, sim(w1, w2) is biased in favorof w2.Consider four similarity functions that haveplayed important roles in the literature on similar-ity:DICE PROD(w1, w2) =2?w1?w2?w1?2+?w2?2DICE?
(w1, w2) =2?
?f?w1?w2min(w1[f ], w2[f ])?w1[f ]+?w2[f ]LIN(w1, w2) =?f?w1?w2w1[f ]+ w2[f ]?w1[f ]+?w2[f ]COS(w1, w2) = DICE PROD appliedto unit vectors(6)The function DICE PROD is not well known in theword similarity literature, but in the data miningliterature it is often just called Dice coefficient, be-cause it generalized the set comparison functionof Dice (1945).
Observe that cosine is a specialcase of DICE PROD.
DICE?was introduced in Cur-ran (2004) and was the most successful functionin his evaluation.
Since LIN was introduced in Lin(1998); several different functions have born thatname.
The version used here is the one used inCurran (2004).The three distinct functions in Equation 6 havea similar form.
In fact, all can be defined in termsof ?
functions differing only in their SI operation.Let ?SIbe a shared feature sum for operation SI,as defined in Equation (3).
We define the Tversky-normalized version of ?SI, written TSI, as:1TSI(w1, w2) =2 ?
?SI(w1, w2)?SI(w1, w1) + ?SI(w2, w2)(7)Note that TSIis just the special case of Tversky?sratio model (5) in which ?
= 0.5 and the similaritymeasure is symmetric.We define three SI operations ?PROD2, ?MIN, and?AVGas follows:SI ?SI(w1, w2)PROD?f?w1?w2w1[f ] ?
w2[f ]AVG?f?w1?w2w1[f ]+w2[f ]2MIN?f?w1?w2MIN(w1[f ], w2[f ])1Paralleling (7) is Jaccard-family normalization:?JACC(w1, w2) =?
(w1, w2)?
(w1, w1) + ?
(w2, w2)?
?
(w1, w2)It is easy to generalize the result from van Rijsbergen (1979)for the original set-specific versions of Dice and Jaccard, andshow that all of the Tversky family functions discussed aboveare monotonic in Jaccard.2?PROD, of course, is dot product.297This yields the three similarity functions citedabove:DICE PROD(w1, w2) =TPROD(w1, w2)DICE?
(w1, w2) =TMIN(w1, w2)LIN(w1, w2) =TAVG(w1, w2)(8)Thus, all three of these functions are special casesof symmetric ratio models.
Below, we investigateasymmetric versions of all three, which we writeas T?,SI(w1, w2), defined as:?SI(w1, w2)?
?
?SI(w1, w1) + (1 ?
?)
?
?SI(w2, w2)(9)Following Lee (1997), who investigates a differentfamily of asymmetric similarity functions, we willrefer to these as ?-skewed measures.We also will look at a rank-biased family ofmeasures:R?,SI(w1, w2) = T?,SI(wh, wl)wherewl= argminw?
{w1,w2}Rank(w)wh= argmaxw?
{w1,w2}Rank(w)(10)Here, T?,SI(wh, wl) is as defined in (9), and the ?-weighted word is always the less frequent word.For example, consider comparing the 100-featurevector for dinghy to the 1000 feature vector forboat: if ?
is high, we give more weight to the pro-portion of dinghy?s features that are shared thanwe give to the proportion of boat?s features thatare shared.In the following sections we present data show-ing that the performance of a dependency-basedsimilarity system in capturing human similarityjudgments can be greatly improved with rank-bias and ?-skewing.
We will investigate the threeasymmetric functions defined above.3We arguethat the advantages of rank bias are tied to im-proved similarity estimation when comparing vec-tors of very different dimensionality.
We thenturn to the problem of finding a word?s nearestsemantic neighbors.
The nearest neighbor prob-lem is a rather a natural ground in which to tryout ideas on asymmetry, since the nearest neigh-bor relation is itself not symmetrical.
We showthat ?-skewing can be used to improve the qualityof nearest neighbors found for both high- and mid-frequency words.3Interestingly, Equation (9) does not yield an asymmetricversion of cosine.
Plugging unit vectors into the ?-skewedversion of DICE PROD still leaves us with a symmetric func-tion (COS), whatever the value of ?.2 Systems1.
We parsed the BNC with the Malt Depen-dency parser (Nivre, 2003) and the Stanfordparser (Klein and Manning, 2003), creatingtwo dependency DBs, using basically the de-sign in Lin (1998), with features weighted byPMI (Church and Hanks, 1990).2.
For each of the 3 rank-biased similarity sys-tems (R?,SI) and cosine, we computed corre-lations with human judgments for the pairsin 2 standard wordsets: the combined Miller-Charles/Rubenstein-Goodenough word sets(Miller and Charles, 1991; Rubenstein andGoodenough, 1965) and the Wordsim 353word set (Finkelstein et al, 2002), as well asto a subset of the Wordsim set restricted toreflect semantic similarity judgments, whichwe will refer to as Wordsim 201.3.
For each of 3 ?-skewed similarity systems(T?,SI) and cosine, we found the nearestneighbor from among BNC nouns (of anyrank) for the 10,000 most frequent BNCnouns using the the dependency DB createdin step 2.4.
To evaluate of the quality of the nearestneighbors pairs found in Step 4, we scoredthem using the Wordnet-based PersonalizedPagerank system described in Agirre (2009)(UKB), a non distributional WordNet basedmeasure, and the best system in Table 1.3 Human correlationsTable 1 presents the Spearman?s correlationwith human judgments for Cosine, UKB, andour 3 ?-skewed models using Malt-parserbased vectors applied to the combined Miller-Charles/Rubenstein-Goodenough word sets, theWordsim 353 word set, and the Wordsim 202word set.The first of each of the column pairs is a sym-metric system, and the second a rank-biased vari-ant, based on Equation (10).
In all cases, the bi-ased system improves on the performance of itssymmetric counterpart; in the case of DICE?andDICE PROD, that improvement is enough for thebiased system to outperform cosine, the best ofthe symmetric distributionally based systems.
Thevalue .97 was chosen for ?
because it produced thebest ?-system on the MC/RG corpus.
That value298MC/RG Wdsm201 Wdsm353?
= .5 ?
= .97 ?
= .5 ?
= .97 ?
= .5 ?
= .97Dice DICE PROD .59 .71 .50 .60 .35 .44LIN .48 .62 .42 .54 .29 .39DICE?.58 .67 .49 .58 .34 .43Euc Cosine .65 NA .56 NA .41 NAWN UKB WN .80 NA .75 NA .68 NATable 1: System/Human correlations.
Above the line: MALT Parser-based systems0.5 0.6 0.7 0.8 0.9 1.0?
value0.340.360.380.400.42correlationFigure 1: Scores monotonically increase with ?is probably probably an overtrained optimum.
Thepoint is that ?-skewing always helps: For all threesystems, the improvement shown in raising ?
from.5 to whatever the optimum is is monotonic.
Thisis shown in Figure 1.
Table 2 shows very simi-lar results using the Stanford parser, demonstrat-ing the pattern is not limited to a single parsingmodel.In Table 3, we list the pairs whose rerankingon the MC/RG dataset contributed most to the im-provement of the ?
= .9 system over the default?
= .5 system.
In the last column an approxi-mation of the amount of correlation improvementprovided by that pair (?
):4Note the 3 of the 5items contributing the most improvement this sys-tem were pairs with a large difference in rank.Choosing ?
= .9, weights recall toward the rarerword.
We conjecture that the reason this helps isTversky?s principle: It is natural to use the sparser4The approximation is based on the formula for comput-ing Spearman?s R with no ties.
If n is the number of items,then the improvement on that item is:6 ?
[(baseline ?
gold)2?
(test?
gold)2]n ?
(n2?
1)Word 1 Rank Word 2 Rank ?automobile 7411 car 100 0.030asylum 3540 madhouse 14703 0.020coast 708 hill 949 0.018mound 3089 stove 2885 0.017autograph 10136 signature 2743 0.009Table 3: Pairs contributing the biggest improve-ment, MC/RG word setrepresentation as the focus in the comparison.4 Nearest neighborsFigure 2 gives the results of our nearest neighborstudy on the BNC for the case of DICE PROD.
Thegraphs for the other two ?-skewed systems arenearly identical, and are not shown due to spacelimitations.
The target word, the word whosenearest neighbor is being found, always receivesthe weight 1 ?
?.
The x-axis shows target wordrank; the y-axis shows the average UKB simi-larity scores assigned to nearest neighbors every50 ranks.
All the systems show degraded nearestneighbor quality as target words grow rare, but atlower ranks, the ?
= .04 nearest neighbor systemfares considerably better than the symmetric ?
=.50 system; the line across the bottom tracks thescore of a system with randomly generated near-est neighbors.
The symmetric DICE PROD sys-tem is as an excellent nearest neighbor system athigh ranks but drops below the ?
= .04 system ataround rank 3500.
We see that the ?
= .8 systemis even better than the symmetric system at highranks, but degrades much more quickly.We explain these results on the basis of the prin-ciple developed for the human correlation data: Toreflect natural judgments of similarity for compar-isons of representations of differing sparseness, ?should be tipped toward the sparser representation.Thus, ?
= .80 works best for high rank tar-get words, because most nearest neighbor candi-299MC/RG Wdsm201 Wdsm353?
= .5 opt opt ?
?
= .5 opt opt ?
?
= .5 opt opt ?DICE PROD .65 .70 .86 .42 .57 .99 .36 .44 .98LIN .58 .68 .90 .41 .56 .94 .30 .41 .99DICE?.60 .71 .91 .43 .53 .99 .32 .43 .99Table 2: System/Human correlations for Stanford parser systems0 2000 4000 6000 8000 10000Word rank0.0000.0050.0100.0150.0200.0250.0300.035Avgsimilarityalpha04alpha_50alpha_80randomFigure 2: UKB evaluation scores for nearestneighbor pairs across word ranks, sampled every50 ranks.dates are less frequent, and ?
= .8 tips the bal-ance toward the nontarget words.
On the otherhand, when the target word is a low ranking word,a high ?
weight means it never receives the high-est weight, and this is disastrous, since most goodcandidates are higher ranking.
Conversely, ?
=.04 works better.5 Previous workThe debt owed to Tversky (1977) has been madeclear in the introduction.
Less clear is the debtowed to Jimenez et al (2012), which also pro-poses an asymmetric similarity framework basedon Tversky?s insights.
Jimenez et al showed thecontinued relevance of Tversky?s work.Motivated by the problem of measuring howwell the distribution of one word w1captures thedistribution of another w2, Weeds and Weir (2005)also explore asymmetric models, expressing sim-ilarity calculations as weighted combinations ofseveral variants of what they call precision and re-call.
Some of their models are also Tverskyan ratiomodels.
To see this, we divide (9) everywhere by?
(w1, w2):TSI(w1, w2) =1???(w1,w1)?(w1,w2)+(1??)??(w2,w2)?
(w1,w2)If the SI is MIN, then the two terms in the de-nominator are the inverses of what W&W calldifference-weighted precision and recall:PREC(w1, w2) =?MIN(w1,w2)?MIN(w1,w1)REC(w1, w2) =?MIN(w1,w2)?MIN(w2,w2),So for TMIN, (9) can be rewritten:1?PREC(w1,w2)+1?
?REC(w1,w2)That is, TMINis a weighted harmonic mean ofprecision and recall, the so-called weighted F-measure (Manning and Schu?tze, 1999).
W&W?sadditive precision/recall models appear not to beTversky models, since they compute separatesums for precision and recall from the f ?
w1?w2, one using w1[f ], and one using w2[f ].Long before Weed and Weir, Lee (1999) pro-posed an asymmetric similarity measure as well.Like Weeds and Weir, her perspective was to cal-culate the effectiveness of using one distribution asa proxy for the other, a fundamentally asymmetricproblem.
For distributions q and r, Lee?s ?-skewdivergence takes the KL-divergence of a mixtureof q and r from q, using the ?
parameter to definethe proportions in the mixture.6 ConclusionWe have shown that Tversky?s asymmetric ratiomodels can improve performance in capturinghuman judgments and produce better nearestneighbors.
To validate these very preliminaryresults, we need to explore applications compat-ible with asymmetry, such as the TOEFL-likesynonym discovery task in Freitag et al (2005),and the PP-attachment task in Dagan et al (1999).AcknowledgmentsThis work reported here was supported by NSFCDI grant # 1028177.300ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova,M.
Pasca, and A. Soroa.
2009.
A study on similar-ity and relatedness using distributional and wordnet-based approaches.
In Proceedings of NAACL-HLT09, Boulder, Co.K.W.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational linguistics, 16(1):22?29.J.R.
Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.College of Science and Engineering.
School of In-formatics.I.
Dagan, L. Lee, and F.C.N.
Pereira.
1999.
Similarity-based models of word cooccurrence probabilities.Machine Learning, 34(1):43?69.L.R.
Dice.
1945.
Measures of the amount of ecologicassociation between species.
Ecology, 26(3):297?302.L.
Finkelstein, E. Gabrilovich, Yossi Matias, EhudRivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-pin.
2002.
Placing search in context: The conceptrevisited.
ACM Transactions on Information Sys-tems, 20(1):116?131.D.
Freitag, M. Blume, J. Byrnes, E. Chow, S. Kapadia,R.
Rohwer, and Z.Wang.
2005.
New experiments indistributional representations of synonymy.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning, pages 25?32.
Associa-tion for Computational Linguistics.R.
L. Goldstone.
in press.
Similarity.
In R.A. WilsonWilson and F. C. Keil, editors, MIT Encylcopedia ofCognitive Sciences.
MIT Press, Cambridge, MA.S.
Jimenez, C. Becerra, and A. Gelbukh.
2012.
Softcardinality: A parameterized similarity function fortext comparison.
In Proceedings of the First JointConference on Lexical and Computational Seman-tics, pages 449?453.
Association for ComputationalLinguistics.D.
Klein and Christopher D. Manning.
2003.
Fast ex-act inference with a factored model for natural lan-guage parsing.
In Advances in Neural InformationProcessing Systems 15 (NIPS 2002), pages 3?10,Cambridge, MA.
MIT Press.L.
Lee.
1997.
Similarity-based approaches to naturallanguage processing.
Ph.D. thesis, Harvard Univer-sity.L.
Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Com-putational Linguistics, pages 25?32.
Association forComputational Linguistics.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Annual Meeting-Association forComputational Linguistics, volume 36, pages 768?774.
Association for Computational Linguistics.C.D.
Manning and H. Schu?tze.
1999.
Foundations ofstatistical natural language processing.
MIT Press,Cambridge.G.A.
Miller and W.G.
Charles.
1991.
Contextual cor-relates of semantic similarity.
Language and Cogni-tive Processes, 6(1):1?28.J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of the 8th Inter-national Workshop on Parsing Technologies (IWPT03), pages 149?160.E.
Rosch and C. B. Mervis.
1975.
Family resem-blances: Studies in the internal structure of cate-gories.
Cognitive psychology, 7(4):573?605.H.
Rubenstein and J.B. Goodenough.
1965.
Contex-tual correlates of synonymy.
Communications of theACM, 8:627?633.A.
Tversky.
1977.
Features of similarity.
Psychologi-cal Review, 84:327?352.C.
J. van Rijsbergen.
1979.
Information retrieval.Butterworth-Heinemann, Oxford.J.
Weeds and D. Weir.
2005.
Co-occurrence retrieval:A flexible framework for lexical distributional simi-larity.
Computational Linguistics, 31(4):439?475.301
