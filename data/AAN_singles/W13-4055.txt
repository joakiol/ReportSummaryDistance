Proceedings of the SIGDIAL 2013 Conference, pages 354?356,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsIntegration and test environment for an in-vehicle dialogue system in theSIMSI projectStaffan Larsson, Sebastian BerlinUniversity of GothenburgBox 200SE-405 30 GothenburgSwedensl@ling.gu.sesebastian.berlin@gu.seAnders EliassonMecel ABBox 140 44SE-400 20 GothenburgSwedenanders.eliasson@mecel.seFredrik KronlidTalkamatic ABFo?rsta la?nggatan 18SE-413 28 GothenburgSwedenfredrik@talkamatic.seAbstractThe goal of the SIMSI (Safe In-vehicleMultimodal Speech Interaction) project isthreefold.
Firstly, to integrate a dialoguesystem for menu-based dialogue with aGUI-driven in-vehicle infotainment sys-tem.
Secondly, to further improve the in-tegrated system with respect to driver dis-traction, thus making the system safer touse while driving.
Thirdly, to verify thatthe resulting system decreases visual dis-traction and cognitive load during interac-tion.
This demo paper describes the inte-gration of the two existing systems, andthe test environment designed to enableevaluation of the system.1 Background1.1 Driver distraction and safetyDriver distraction is one common cause of acci-dents, and is often caused by the driver interact-ing with technologies such as mobile phones, me-dia players or navigation systems.
The so-called100-car study (Neale et al 2005) revealed thatsecondary task distraction is the largest cause ofdriver inattention, and that the handling of wire-less devices is the most common secondary task.The goal of SIMSI is to design systems which en-able safe interaction with technologies in vehicles,by reducing the cognitive load imposed by the in-teraction and minimizing head-down time.1.2 The Talkamatic Dialogue ManagerBased on Larsson (2002) and later work, Talka-matic AB has developed the Talkamatic DialogueManager (TDM) with the goal of being the mostcompetent and usable dialogue manager on themarket, both from the perspective of the user andfrom the perspective of the HMI developer.
TDMprovides a general interaction model founded inhuman interaction patterns, resulting in a high de-gree of naturalness and flexibility which increasesusability.
Also, TDM reduces complexity for de-velopers and users, helping them to reach theirgoals faster and at a lower cost.A major problem with the current state-of-the-art in-vehicle spoken dialogue systems is that theyare either too simplistic to be useful to the enduser, or alternatively that they are fairly sophisti-cated but unmanageable for the manufacturer dueto the size and complexity of the implementation.TDM offers sophisticated multi-modal interactionmanagement solutions which allow for easy modi-fication and development, allowing interaction de-signers to easily explore new solutions and re-ducing overhead for new dialogue applications interms of code and development man-hours.TDM deals with several interaction patternswhich are basic to human-human linguistic in-teraction, and offers truly integrated multimodal-ity which allows user to freely switch between(or combine) modalities.
All these solutions aredomain-independent which means that they neednot be implemented in each application.
UsingTalkamatic technology, dialogue behaviour can bealtered without touching application properties,and application properties can be updated withouttouching the dialogue logic.
This makes testing ofdifferent dialogue strategies, prompts etc.
consid-erably quicker and easier than when using regularstate-machine-based dialogue systems.In addition, as the dialogue strategy is separatedfrom the application logic, development time fornew dialogue applications can be significantly re-duced.
Furthermore, the developer designing theapplication does not need to be a dialogue expertas the dialogue design is built into the dialoguemanager.3541.3 Integrated multimodality in TDMThere are reasons to believe that multi-modal in-teraction is more efficient and less distracting thanuni-modal interaction (Oviatt et al 2004).
TDMsupports multi-modal interaction where voice out-put and input (VUI) is combined with a traditionalmenu-based GUI with graphical output and hap-tic input.
In cases where a GUI already exists,TDM can replace the GUI-internal interaction en-gine, thus adding speech while keeping the origi-nal GUI design.
All system output is realized bothverbally and graphically, and the user can switchfreely between uni-modal (voice or screen/keys)and multi-modal interaction.To facilitate the browsing of lists (a well knowninteraction problem for dialogue systems), Talka-matic has developed its Voice-Cursor technology1(Larsson et al 2011).
It allows a user to browsea list in a multi-modal dialogue system withoutlooking at a screen and without being exposed tolarge chunks of readout information.A crucial property of TDM?s integrated multi-modality is the fact that it enables the driver of avehicle to carry out all interactions without everlooking at the screen, either by speaking to the sys-tem, by providing haptic input, or by combiningthe two.
We are not aware of any current mul-timodal in-vehicle dialogue system offering thiscapability.
Additional information is available atwww.talkamatic.se.1.4 Mecel PopulusWhile TDM offers full menu-based multimodalinteraction, the GUI itself is fairly basic and doesnot match the state of the art when it comes tographical design.
By contrast, Mecel Populus isan commercial-grade HMI (Human Machine In-terface) with professionally designed visual out-put.
The Mecel Populus suite is a complete toolchain for designing, developing and deployinguser interfaces for distributed embedded systems.It minimizes the time and cost of producing eye-catching, full-featured HMIs.The Mecel Populus concept has several uniquefeatures compared to traditional HMI develop-ment.
These features, when combined, remove thebarriers that traditionally exist between the peo-ple working with requirements, system engineer-ing, HMI design and implementation.
An HMIis created and verified in Mecel Populus Editor1Patent PendingFigure 1: SIMSI system overviewwithout having to write any software.
The HMI isthen downloaded to the target environment whereMecel Populus Engine executes it.
Mecel Popu-lus has been designed for the automotive industryto deliver high performance user interfaces with ashort time-to-market and to enable efficient soft-ware life cycle management.
Additional informa-tion is available at www.mecel.se/products.2 System integrationThe goal of this part of SIMSI is to provide aproject-specific integration of TDM and the Me-cel Populus platform.
In this way, we estab-lish a commercial-grade HMI for experiments anddemonstrations.
At the same time, the integrationof TDM and Populus increases the commercial po-tential of both platforms, since it integrates a state-of-the-art HMI tool without voice capabilities anda dialogue manager with limited graphical capa-bilities.The major problem in integrating Populus andTDM is that both systems keep track of the cur-rent state of the interaction and manage transitionsbetween states resulting from user or system ac-tions.
Hence, there is a need to keep the systems insync at all times.
This is managed by a TransitionQueue (TQ) module which keeps a lock which canbe grabbed by either system at any time, unlessit has already been grabbed by the other system.The systems then enter into a master-slave rela-tion where the master is the system which ownsthe lock.
The master tells the slave how the in-teraction state is to be updated, and the slave onlywaits for messages from the master until the lockhas been returned to the TQ.355Figure 2: SIMSI test environment overview3 Test environmentThe purpose of this part of the project is to conductecologically valid test of the applications, and tobegin and continue an iterative development cycleof testing - evaluation - development.
We want tofind the best interaction solutions in cases where itis not intuitively clear what is best.
This involvesimplementing variants of a behaviour, testing themon naive users, collecting data from these interac-tions, and establishing statistically significant re-sults based on the collected data.The test environment consists of two parts, apartfrom the dialogue system: a driving simulator(SCANeR from Octal) and an eye tracker (SmartEye Pro from Smarteye).
In later tests we will alsoinclude instruments for measuring cognitive load.In our setup we have three monitors, giving theuser a wide field of view.
We also have a gamingsteering wheel, including pedals, gear lever and adriver?s seat.
These are used mainly to control thedriving simulator, but there are also a number ofbuttons on the steering wheel which are used tobrowse the menus in the HMI and as Push-to-talk(PTT).
An Android tablet (Asus Eee Pad Trans-former TF101) showing the HMI GUI is placed infront of the user, trying to match the position of adisplay in a car.
Both TDM and Populus run onthe same desktop computer as the driving simula-tor, and a Populus Android app runs on the tablet.The app allows the user to select items by tappingthem, as well as scrolling in lists in normal smartphone fashion.
The eye tracker runs on a sepa-rate desktop computer, as it requires a substantialamount of processing power.Figure 3: SIMSI test environment in actionStudio software that comes with the drivingsimulator is used to design and run scenarios.
Thescenarios govern how autonomous traffic shouldbehave and events, such as weather change andthe state of traffic signals.
The simulator logs datafor the environment and each vehicle.
Data likelane deviation (where in the lane the vehicle is)and how the user handles instruments, e.g.
steer-ing wheel and pedals, can be used to measure cog-nitive load.
At a later stage this kind of data canalso be used to trigger behaviour in the dialoguesystem.The eye tracker uses three cameras to track theuser?s eyes and head at 60 Hz.
The cameras arespaced to give good tracking in the middle of thescene, where you typically look when you?re driv-ing, and at the same time capture head movementto the side.
As we are interested in when the user islooking at the tablet, we placed one of the camerasspecifically to improve eye tracking in this area.ReferencesStaffan Larsson, Alexander Berman, and JessicaVilling.
2011.
Adding a speech cursor to a mul-timodal dialogue system.
In INTERSPEECH 2011,12th Annual Conference of the International SpeechCommunication Association, Florence, Italy, 2011,pages 3319?3320.Staffan Larsson.
2002.
Issue-based Dialogue Manage-ment.
Ph.D. thesis, Go?teborg University.Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer,Jeremy Sudweeks, and Michael Goodman.
2005.An overview of the 100-car naturalistic study andfindings.Sharon L. Oviatt, Rachel Coulston, and RebeccaLunsford.
2004.
When do we interact multi-modally?
: cognitive load and multimodal commu-nication patterns.
In ICMI, pages 129?136.356
