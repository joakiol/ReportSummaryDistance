Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300?308,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsStructural, Transitive and Latent Models for Biographic Fact ExtractionNikesh Garera and David YarowskyDepartment of Computer Science, Johns Hopkins UniversityHuman Language Technology Center of ExcellenceBaltimore MD, USA{ngarera,yarowsky}@cs.jhu.eduAbstractThis paper presents six novel approachesto biographic fact extraction that modelstructural, transitive and latent proper-ties of biographical data.
The ensem-ble of these proposed models substantiallyoutperforms standard pattern-based bio-graphic fact extraction methods and per-formance is further improved by modelinginter-attribute correlations and distribu-tions over functions of attributes, achiev-ing an average extraction accuracy of 80%over seven types of biographic attributes.1 IntroductionExtracting biographic facts such as ?Birthdate?,?Occupation?, ?Nationality?, etc.
is a critical stepfor advancing the state of the art in informationprocessing and retrieval.
An important aspect ofweb search is to be able to narrow down searchresults by distinguishing among people with thesame name leading to multiple efforts focusingon web person name disambiguation in the liter-ature (Mann and Yarowsky, 2003; Artiles et al,2007, Cucerzan, 2007).
While biographic facts arecertainly useful for disambiguating person names,they also allow for automatic extraction of ency-lopedic knowledge that has been limited to man-ual efforts such as Britannica, Wikipedia, etc.Such encyploedic knowledge can advance verti-cal search engines such as http://www.spock.comthat are focused on people searches where one canget an enhanced search interface for searching byvarious biographic attributes.
Biographic facts arealso useful for powerful query mechanisms suchas finding what attributes are common betweentwo people (Auer and Lehmann, 2007).Figure 1: Goal: extracting attribute-value bio-graphic fact pairs from biographic free-textWhile there are a large quantity of biographic textsavailable online, there are only a few biographicfact databases available1, and most of them havebeen created manually, are incomplete and areavailable primarily in English.This work presents multiple novel approachesfor automatically extracting biographic facts suchas ?Birthdate?, ?Occupation?, ?Nationality?, and?Religion?, making use of diverse sources of in-formation present in biographies.In particular, we have proposed and evaluated thefollowing 6 distinct original approaches to this1E.g.
: http://www.nndb.com, http://www.biography.com,Infoboxes in Wikipedia300task with large collective empirical gains:1.
An improvement to the Ravichandran andHovy (2002) algorithm based on PartiallyUntethered Contextual Pattern Models2.
Learning a position-based model using ab-solute and relative positions and sequentialorder of hypotheses that satisfy the domainmodel.
For example, ?Deathdate?
very oftenappears after ?Birthdate?
in a biography.3.
Using transitive models over attributes viaco-occurring entities.
For example, otherpeople mentioned person?s biography pagetend to have similar attributes such as occu-pation (See Figure 4).4.
Using latent wide-document-context modelsto detect attributes that may not be mentioneddirectly in the article (e.g.
the words ?song,hits, album, recorded,..?
all collectively indi-cate the occupation of singer or musician inthe article.5.
Using inter-attribute correlations, for filter-ing unlikely biographic attribute combina-tions.
For example, a tuple consisting of <?Nationality?
= India, ?Religion?
= Hindu >has a higher probability than a tuple consist-ing of < ?Nationality?
= France, ?Religion?= Hindu >.6.
Learning distributions over functions of at-tributes, for example, using an age distri-bution to filter tuples containing improbable<deathyear>-<birthyear> lifespan values.We propose and evaluate techniques for exploitingall of the above classes of information in the nextsections.2 Related WorkThe literature for biography extraction falls intotwo major classes.
The first one deals with iden-tifying and extracting biographical sentences andtreats the problem as a summarization task (Cowieet al, 2000, Schiffman et al, 2001, Zhou etal., 2004).
The second and more closely relatedclass deals with extracting specific facts such as?birthplace?, ?occupation?, etc.
For this task,the primary theme of work in the literature hasbeen to treat the task as a general semantic-classlearning problem where one starts with a fewseeds of the semantic relationship of interest andlearns contextual patterns such as ?<NAME>was born in <Birthplace>?
or ?<NAME> (born<Birthdate>)?
(Hearst, 1992; Riloff, 1996; The-len and Riloff, 2002; Agichtein and Gravano,2000; Ravichandran and Hovy, 2002; Mann andYarowsky, 2003; Jijkoun et al, 2004; Mann andYarowsky, 2005; Alfonseca et al, 2006; Pasca etal., 2006).
There has also been some work on ex-tracting biographic facts directly from Wikipediapages.
Culotta et al (2006) deal with learningcontextual patterns for extracting family relation-ships from Wikipedia.
Ruiz-Casado et al (2006)learn contextual patterns for biographic facts andapply them to Wikipedia pages.While the pattern-learning approach extends wellfor a few biography classes, some of the bio-graphic facts like ?Gender?
and ?Religion?
do nothave consistent contextual patterns, and only afew of the explicit biographic attributes such as?Birthdate?, ?Deathdate?, ?Birthplace?
and ?Oc-cupation?
have been shown to work well in thepattern-learning framework (Mann and Yarowsky,2005; Alfonesca, 2006; Pasca et al, 2006).Secondly, there is a general lack of work that at-tempts to utilize the typical information sequenc-ing within biographic texts for fact extraction, andwe show how the information structure of biogra-phies can be used to improve upon pattern basedmodels.
Furthermore, we also present additionalnovel models of attribute correlation and age dis-tribution that aid the extraction process.3 ApproachWe first implement the standard pattern-based ap-proach for extracting biographic facts from the rawprose in Wikipedia people pages.
We then presentan array of novel techniques exploiting differentclasses of information including partially-tetheredcontextual patterns, relative attribute position andsequence, transitive attributes of co-occurring en-tities, broad-context topical profiles, inter-attributecorrelations and likely human age distributions.For illustrative purposes, we motivate each tech-nique using one or two attributes but in practicethey can be applied to a wide range of attributesand empirical results in Table 4 show that theygive consistent performance gains across multipleattributes.3014 Contextual Pattern-Based ModelA standard model for extracting biographic factsis to learn templatic contextual patterns such as<NAME> ?was born in?
<Birthplace>.
Suchtemplatic patterns can be learned using seed ex-amples of the attribute in question and, there hasbeen a plethora of work in the seed-based boot-strapping literature which addresses this problem(Ravichandran and Hovy, 2002; Thelen and Riloff,2002; Mann and Yarowsky, 2005; Alfonseca et al,2006; Pasca et al, 2006)Thus for our baseline we implemented a stan-dard Ravichandran and Hovy (2002) patternlearning model using 100 seed2 examples froman online biographic database called NNDB(http://www.nndb.com) for each of the biographicattributes: ?Birthdate?, ?Birthplace?, ?Death-date?, ?Gender?, ?Nationality?, ?Occupation?
and?Religion?.
Given the seed pairs, patterns foreach attribute were learned by searching for seed<Name,Attribute Value> pairs in the Wikipediapage and extracting the left, middle and right con-texts as various contextual patterns3.While the biographic text was obtained fromWikipedia articles, all of the 7 attribute valuesused as seed and test person names could notbe obtained from Wikipedia due to incompleteand unnormalized (for attribute value format) in-foboxes.
Hence, the values for training/evaluationwere extracted from NNDB which provides acleaner set of gold truth, and is similar to an ap-proach utilizing trained annotators for marking upand extracting the factual information in a stan-dard format.
For consistency, only the peoplenames whose articles occur in Wikipedia whereselected as part of seed and test sets.Given the attribute values of the seed names andtheir text articles, the probability of a relationshipr(Attribute Name), given the surrounding context?A1 p A2 q A3?, where p and q are <NAME>and <Attrib Val> respectively, is given using therote extractor model probability as in (Ravichan-dran and Hovy, 2002; Mann and Yarowsky 2005):2The seed examples were chosen randomly, with a biasagainst duplicate attribute values to increase training diver-sity.
Both the seed and test names and data will be madeavailable online to the research community for replicationand extension.3We implemented a noisy model of coreference resolu-tion by resolving any gender-correct pronoun used in theWikipedia page to the title person name of the article.
Genderis also extracted automatically as a biographic attribute.P (r(p, q)|A1pA2qA3) =?x,y?rc(A1xA2yA3)?x,zc(A1xA2zA3)Thus, the probability for each contextual patternis based on how often it correctly predicts a re-lationship in the seed set.
And, each extractedattribute value q using the given pattern can thusbe ranked according to the above probability.
Wetested this approach for extracting values for eachof the seven attributes on a test set of 100 held-outnames and report Precision, Pseudo-recall and F-score for each attribute which are computed in thestandard way as follows, for say Attribute ?Birth-place (bplace)?
:Precisionbplace =# people with bplace correctly extracted# of people with bplace extractedPseudo-recbplace =# people with bplace correctly extracted# of people with bplace in test setF-scorebplace =2?Precisionbplace?Pseudo-recbplacePrecisionbplace + Pseudo-recbplaceSince the true values of each attribute are obtainedfrom a cleaner and normalized person-database(NNDB), not all the attribute values maybe presentin the Wikipedia article for a given name.
Thus,we also compute accuracy on the subset of namesfor which the value of a given attribute is also ex-plictly stated in the article.
This is denoted as:Acctruth pres =# people with bplace correctly extracted# of people with true bplace stated in articleWe further applied a domain model for each at-tribute to filter noisy targets extracted from lex-ical patterns.
Our domain models of attributesinclude lists of acceptable values (such as listsof places, occupations and religions) and struc-tural constraints such as possible date formats for?Birthdate?
and ?Deathdate?.
The rows with sub-script ?RH02?in Table 4 shows the performanceof this Ravichandran and Hovy (2002) model withadditional attribute domain modeling for each at-tribute, and Table 3 shows the average perfor-mance across all attributes.5 Partially Untethered TemplaticContextual PatternsThe pattern-learning literature for fact extractionoften consists of patterns with a ?hook?
and?target?
(Mann and Yarowsky, 2005).
For ex-ample, in the pattern ?<Name> was born in<Birthplace>?, ?<NAME>?
is the hook and?<Birthplace>?
is the target.
The disadvantageof this approach is that the intervening dually-tethered patterns can be quite long and highlyvariable, such as ?<NAME> was highly influ-302Figure 2: Distribution of the observed documentmentions of Deathdate, Nationality and Religion.ential in his role as <Occupation>?.
We over-come this problem by modeling partially unteth-ered variable-length ngram patterns adjacent toonly the target, with the only constraint beingthat the hook entity appear somewhere in the sen-tence4.
Examples of these new contextual ngramfeatures include ?his role as <Occupation>?
and?role as <Occupation>?.
The pattern probabilitymodel here is essentially the same as in Ravichan-dran and Hovy, 2002 and just the pattern repre-sentation is changed.
The rows with subscript?RH02imp?
in tables 4 and 3 show performancegains using this improved templatic-pattern-basedmodel, yielding an absolute 21% gain in accuracy.6 Document-Position-Based ModelOne of the properties of biographic genres is thatprimary biographic attributes5 tend to appear incharacteristic positions, often toward the begin-ning of the article.
Thus, the absolute position(in percentage) can be modeled explicitly using aGaussian parametric model as follows for choos-ing the best candidate value v?
for a given attributeA:v?
= argmaxv?domain(A)f(posnv|A)where,f(posnv|A)= N (posnv; ?
?A, ?
?2A)= 1??A?2pie?(posnv???A)2/2?
?A24This constraint is particularly viable in biographic text,which tends to focus on the properties of a single individual.5We use the hyperlinked phrases as potential values for allattributes except ?Gender?.
For ?Gender?
we used pronounsas potential values ranked according to the their distance fromthe beginning of the page.In the above equation, posnv is the absoluteposition ratio (position/length) and ?
?A, ?
?A2 arethe sample mean and variance based on the sam-ple of correct position ratios of attribute valuesin biographies with attribute A.
Figure 2, forexample, shows the positional distribution of theseed attribute values for deathdate, nationality andreligion in Wikipedia articles, fit to a Gaussiandistribution.
Combining this empirically derivedposition model with a domain model6 of accept-able attribute values is effective enough to serveas a stand-alone model.Attribute Best rank P(Rank)in seed setBirthplace 1 0.61Birthdate 1 0.98Deathdate 2 0.58Gender 1 1.0Occupation 1 0.70Nationality 1 0.83Religion 1 0.80Table 1: Majority rank of the correct attributevalue in the Wikipedia pages of the seed namesused for learning relative ordering among at-tributes satisfying the domain model6.1 Learning Relative Ordering in thePosition-Based ModelIn practice, for attributes such as birthdate, thefirst text pattern satisfying the domain model isoften the correct answer for biographical articles.Deathdate also tends to occur near the beginningof the article, but almost always some pointafter the birthdate.
This motivates a second,sequence-based position model based on the rankof the attribute values among other values in thedomain of the attribute, as follows:v?
= argmaxv?domain(A)P (rankv|A)where P (rankv|A) is the fraction of biographieshaving attribute a with the correct value occuringat rank rankv, where rank is measured accordingto the relative order in which the values belongingto the attribute domain occur from the beginning6The domain model is the same as used in Section 4 andremains constant across all the models developed in this paper303of the article.
We use the seed set to learn the rel-ative positions between attributes, that is, in theWikipedia pages of seed names what is the rank ofthe correct attribute.Table 1 shows the most frequent rank of the correctattribute value and Figure 3 shows the distribu-tion of the correct ranks for a sample of attributes.We can see that 61% of the time the first loca-tion mentioned in a biography is the individuals?sbirthplace, while 58% of the time the 2nd datein the article is the deathdate.
Thus, ?Deathdate?often appears as the second date in a Wikipediapage as expected.
These empirical distributionsfor the correct rank provide a direct vehicle forscoring hypotheses, and the rows with ?rel.
posn?as the subscript in Table 4 shows the improvementin performance using the learned relative order-ing.
Averaging across different attributes, table3 shows an absolute 11% average gain in accu-racy of the position-sequence-based models rela-tive to the improved Ravichandran and Hovy re-sults achieved here.Figure 3: Empirical distribution of the relative po-sition of the correct (seed) answers among all textphrases satisfying the domain model for ?birth-place?
and ?death date?.7 Implicit ModelsSome of the biographic attributes such as ?Nation-ality?, ?Occupation?
and ?Religion?
can be ex-tracted successfully even when the answer is notdirectly mentioned in the biographic article.
Wepresent two such models for doing so in the fol-lowing subsections:7.1 Extracting Attributes Transitively usingNeighboring Person-NamesAttributes such as ?Occupation?
are transitive innature, that is, the people names appearing closeto the target name will tend to have the sameoccupation as the target name.
Based on thisintution, we implemented a transitive model thatpredicts occupation based on consensus voting viathe extracted occupations of neighboring names7as follows:v?
= argmaxv?domain(A)P (v|A,Sneighbors)where,P (v|A,Sneighbors) =# neighboring names with attrib value v# of neighboring names in the articleThe set of neighboring names is representedas Sneighbors and the best candidate value foran attribute A is chosen based on the the fractionof neighboring names having the same valuefor the respective attribute.
We rank candidatesaccording to this probability and the row labeled?trans?
in Table 4 shows that this model helps insubsantially improving the recall of ?Occupation?and ?Religion?, yielding a 7% and 3% averageimprovement in F-measure respectively, on top ofthe position model described in Section 6.7.2 Latent Model based on Document-WideContext ProfilesIn addition to modeling cross-entity attributetransitively, attributes such as ?Occupation?
canalso be modeled successfully using a document-wide context or topic model.
For example, thedistribution of words occuring in a biography7We only use the neighboring names whose attributevalue can be obtained from an encylopedic database.
Fur-thermore, since we are dealing with biographic pages thattalk about a single person, all other person-names mentionedin the article whose attributes are present in an encylopediawere considered for consensus voting304Figure 4: Illustration of modeling ?occupation?
and ?nationality?
transitively via consensus from at-tributes of neighboring namesof a politician would be different from that ofa scientist.
Thus, even if the occupation is notexplicitly mentioned in the article, one can inferit using a bag-of-words topic profile learned fromthe seed examples.Given a value v, for an attribute A, (for ex-ample v = ?Politician?
and A = ?Occupation?
),we learn a centroid weight vector:Cv = [w1,v, w2,v, ..., wn,v] where,wt,v = 1N tft,v ?
log|A||t?A|tft,v is the frequency of word t in the articles of Peoplehaving attribute A = v|A| is the total number of values of attribute A|t ?
A| is the total number of values of attribute A, such thatthe articles of people having one of those values contain theterm tN is the total number of People in the seed setGiven a biography article of a test name andan attribute in question, we compute a similarword weight vector C ?
= [w?1, w?2, ..., w?n] forthe test name and measure its cosine similarityto the centroid vector of each value of the givenattribute.
Thus, the best value a?
is chosen as:v?
=argmaxvw?1?w1,v+w?2?w2,v+....+w?n?wn,v?w?21 +w?22 +...+w?2n?w21,v+w22,v+...+w2n,vTables 3 and 4 show performance using the la-tent document-wide-context model.
We see thatthis model by itself gives the top performanceon ?Occupation?, outperforming the best alterna-tive model by 9% absolute accuracy, indicatingthe usefulness of implicit attribute modeling viabroad-context word frequencies.This latent model can be further extended us-ing the multilingual nature of Wikipedia.
Wetake the corresponding German pages of the train-ing names and model the German word distribu-tions characterizing each seed occupation.
Table4 shows that English attribute classification can besuccessful using only the words in a parallel Ger-man article.
For some attributes, the performanceof latent model modeled via cross-language (notedas latentCL) is close to that of English suggestingpotential future work by exploiting this multilin-gual dimension.It is interesting to note that both the transitivemodel and the latent wide-context model do notrely on the actual ?Occupation?
being explicitlymentioned in the article, they still outperform ex-305Occupation Weight VectorEnglishPhysicist <magnetic:32.7, electromagnetic:18.2, wire: 18.2, electricity: 17.7, optical:14.5, discovered:11.2>Singer <song:40, hits:30.5, hit:29.6, reggae:23.6, album:17.1, francis:15.2, music:13.8, recorded:13.6, ...>Politician <humphrey:367.4, soviet: 97.4, votes: 70.6, senate: 64.7, democratic: 57.2, kennedy: 55.9, ...>Painter <mural:40.0, diego:14.7, paint:14.5, fresco:10.9. paintings:10.9, museum of modern art:8.83, ...>Auto racing <renault:76.3, championship:32.7. schumacher:32.7, race:30.4, pole:29.1, driver:28.1 >GermanPhysicist <faraday:25.4, chemie:7.3, vorlesungsserie:7.2, 1846:5.8, entdeckt:4.5, rotation:3.6 ...>Singer <song:16.22, jamaikanischen:11.77, platz:7.3, hit: 6.7, solou?nstler:4.5, album:4.1, widmet:4.0, ...>Politician <konservativen:26.5, wahlkreis:26.5, romano:21.8, stimmen:18.6, gewa?hlt:18.4, ...>Painter <rivera:32.7, malerin:7.6, wandgema?lde:7.3, kunst:6.75, 1940:5.8, maler:5.1, auftrag:4.5, ...>Auto racing <team:29.4,mclaren:18.1,teamkollegen:18.1,sieg:11.7, meisterschaft:10.9, gegner:10.9, ...>Table 2: Sample of occupation weight vectors in English and German learned using the latent model.plicit pattern-based and position-based models.This implicit modeling also helps in improving therecall of less-often directly mentioned attributessuch as a person?s ?Religion?.8 Model CombinationWhile the pattern-based, position-based, transitiveand latent models are all stand-alone models, theycan complement each other in combination as theyprovide relatively orthogonal sources of informa-tion.
To combine these models, we perform a sim-ple backoff-based combination for each attributebased on stand-alone model performance, and therows with subscript ?combined?
in Tables 3 and 4shows an average 14% absolute performance gainof the combined model relative to the improvedRavichandran and Hovy 2002 model.9 Further Extensions: Reducing FalsePositivesSince the position-and-domain-based models willalmost always posit an answer, one of the prob-lems is the high number of false positives yieldedby these algorithms.
The following subsections in-troduce further extensions using interesting prop-erties of biographic attributes to reduce the effectof false positives.9.1 Using Inter-Attribute CorrelationsOne of the ways to filter false positives is byfiltering empirically incompatible inter-attributepairings.
The motivation here is that the at-tributes are not independent of each other whenmodeled for the same individual.
For example,P(Religion=Hindu | Nationality=India) is higherthan P(Religion=Hindu | Nationality=France) andModel Fscore AcctruthpresRavichandran and Hovy, 2002 0.37 0.43Improved RH02 Model 0.54 0.64Position-Based Model 0.53 0.75Combinedabove 3+trans+latent+cl 0.59 0.78Combined + Age Dist + Corr 0.62 0.80(+24%) (+37%)Table 3: Average Performance of different modelsacross all biographic attributessimilarly we can find positive and negative cor-relations among other attribute pairings.
For im-plementation, we consider all possible 3-tuplesof (?Nationality?, ?Birthplace?, ?Religion?
)8 andsearch on NNDB for the presence of the tuple forany individual in the database (excluding the testdata of course).
As an agressive but effective filter,we filter the tuples for which no name in NNDBwas found containing the candidate 3-tuples.
Therows with label ?combined+corr?
in Table 4 andTable 3 shows substantial performaance gains us-ing inter-attribute correlations, such as the 7% ab-solute average gain for Birthplace over the Section8 combined models, and a 3% absolute gain forNationality and Religion.9.2 Using Age DistributionAnother way to filter out false positives is to con-sider distributions on meta-attributes, for example:while age is not explicitly extracted, we can usethe fact that age is a function of two extracted at-tributes (<Deathyear>-<Birthyear>) and use theage distribution to filter out false positives for8The test of joint-presence between these three attributeswere used since they are strongly correlated306Figure 5: Age distribution of famous people on theweb (from www.spock.com)<Birthdate> and<Deathdate>.
Based on the agedistribution for famous people9 on the web shownin Figure 5, we can bias against unusual candi-date lifespans and filter out completely those out-side the range of 25-100, as most of the probabil-ity mass is concentrated in this range.
Rows withsubscript ?comb+ age dist?
in Table 4 shows theperformance gains using this feature, yielding anaverage 5% absolute accuracy gain for Birthdate.10 ConclusionThis paper has shown six successful novel ap-proaches to biographic fact extraction using struc-tural, transitive and latent properties of biographicdata.
We first showed an improvement to the stan-dard Ravichandran and Hovy (2002) model uti-lizing untethered contextual pattern models, fol-lowed by a document position and sequence-basedapproach to attribute modeling.Next we showed transitive models exploiting thetendency for individuals occurring together in anarticle to have related attribute values.
We alsoshowed how latent models of wide document con-text, both monolingually and translingually, cancapture facts that are not stated directly in a text.Each of these models provide substantial per-formance gain, and further performance gain isachived via classifier combination.
We alsoshowed how inter-attribution correlations can be9Since all the seed and test examples were used fromnndb.com, we use the age distribution of famous people onthe web: http://blog.spock.com/2008/02/08/age-distribution-of-people-on-the-web/Attribute Prec P-Rec Fscore AcctruthpresBirthdateRH02 0.86 0.38 0.53 0.88BirthdateRH02imp 0.52 0.52 0.52 0.67Birthdaterel.
posn 0.42 0.40 0.41 0.93Birthdatecombined 0.58 0.58 0.58 0.95Birthdatecomb+age dist 0.63 0.60 0.61 1.00DeathdateRH02 0.80 0.19 0.30 0.36DeathdateRH02imp 0.50 0.49 0.49 0.59Deathdaterel.
posn 0.46 0.44 0.45 0.86Deathdatecombined 0.49 0.49 0.49 0.86Deathdatecomb+age dist 0.51 0.49 0.50 0.86BirthplaceRH02 0.42 0.38 0.40 0.42BirthplaceRH02imp 0.41 0.41 0.41 0.45Birthplacerel.
posn 0.47 0.41 0.44 0.48Birthplacecombined 0.44 0.44 0.44 0.48Birthplacecombined+corr 0.53 0.50 0.51 0.55OccupationRH02 0.54 0.18 0.27 0.26OccupationRH02imp 0.38 0.34 0.36 0.48Occupationrel.
posn 0.48 0.35 0.40 0.50Occupationtrans 0.49 0.46 0.47 0.50Occupationlatent 0.48 0.48 0.48 0.59OccupationlatentCL 0.48 0.48 0.48 0.54Occupationcombined 0.48 0.48 0.48 0.59NationalityRH02 0.40 0.25 0.31 0.27NationalityRH02imp 0.75 0.75 0.75 0.81Nationalityrel.
posn 0.73 0.72 0.71 0.78Nationalitytrans 0.51 0.48 0.49 0.49Nationalitylatent 0.56 0.56 0.56 0.56NationalitylatentCL 0.55 0.48 0.51 0.48Nationalitycombined 0.75 0.75 0.75 0.81Nationalitycomb+corr 0.77 0.77 0.77 0.84GenderRH02 0.76 0.76 0.76 0.76GenderRH02imp 0.99 0.99 0.99 0.99Genderrel.
posn 1.00 1.00 1.00 1.00Gendertrans 0.79 0.75 0.77 0.75Genderlatent 0.82 0.82 0.82 0.82GenderlatentCL 0.83 0.72 0.77 0.72Gendercombined 1.00 1.00 1.00 1.00ReligionRH02 0.02 0.02 0.04 0.06ReligionRH02imp 0.55 0.18 0.27 0.45Religionrel.
posn 0.49 0.24 0.32 0.73Religiontrans 0.38 0.33 0.35 0.48Religionlatent 0.36 0.36 0.36 0.45ReligionlatentCL 0.30 0.26 0.28 0.22Religioncombined 0.41 0.41 0.41 0.76Religioncombined+corr 0.44 0.44 0.44 0.79Table 4: Attribute-wise performance comparisonof all the models across several biographic at-tributes.modeled to filter unlikely attribute combinations,and how models of functions over attributes, suchas deathdate-birthdate distributions, can furtherconstrain the candidate space.
These approachescollectively achieve 80% average accuracy on atest set of 7 biographic attribute types, yielding a37% absolute accuracy gain relative to a standardalgorithm on the same data.307ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: ex-tracting relations from large plain-text collections.Proceedings of ICDL, pages 85?94.E.
Alfonseca, P. Castells, M. Okumura, and M. Ruiz-Casado.
2006.
A rote extractor with edit distance-based generalisation and multi-corpora precisioncalculation.
Proceedings of COLING-ACL, pages9?16.J.
Artiles, J. Gonzalo, and S. Sekine.
2007.
Thesemeval-2007 weps evaluation: Establishing abenchmark for the web people search task.
In Pro-ceedings of SemEval, pages 64?69.S.
Auer and J. Lehmann.
2007.
What have Innsbruckand Leipzig in common?
Extracting Semantics fromWiki Content.
Proceedings of ESWC, pages 503?517.A.
Bagga and B. Baldwin.
1998.
Entity-Based Cross-Document Coreferencing Using the Vector SpaceModel.
Proceedings of COLING-ACL, pages 79?85.R.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
Pro-ceedings of EACL, pages 3?7.J.
Cowie, S. Nirenburg, and H. Molina-Salgado.
2000.Generating personal profiles.
The InternationalConference On MT And Multilingual NLP.S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
Proceedings ofEMNLP-CoNLL, pages 708?716.A.
Culotta, A. McCallum, and J. Betz.
2006.
Integrat-ing probabilistic extraction models and data miningto discover relations and patterns in text.
Proceed-ings of HLT-NAACL, pages 296?303.E.
Filatova and J. Prager.
2005.
Tell me what you doand I?ll tell you what you are: Learning occupation-related activities for biographies.
Proceedings ofHLT-EMNLP, pages 113?120.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of COLING,pages 539?545.V.
Jijkoun, M. de Rijke, and J. Mur.
2004.
Infor-mation extraction for question answering: improv-ing recall through syntactic patterns.
Proceedings ofCOLING, page 1284.G.S.
Mann and D. Yarowsky.
2003.
Unsupervisedpersonal name disambiguation.
In Proceedings ofCoNLL, pages 33?40.G.S.
Mann and D. Yarowsky.
2005.
Multi-field in-formation extraction and cross-document fusion.
InProceedings of ACL, pages 483?490.A.
Nenkova and K. McKeown.
2003.
References tonamed entities: a corpus study.
Proceedings of HLT-NAACL companion volume, pages 70?72.M.
Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Organizing and searching the World WideWeb of Facts Step one: The One-Million Fact Ex-traction Challenge.
Proceedings of AAAI, pages1400?1405.D.
Ravichandran and E. Hovy.
2002.
Learning sur-face text patterns for a question answering system.Proceedings of ACL, pages 41?47.Y.
Ravin and Z. Kazi.
1999.
Is Hillary Rodham Clin-ton the President?
Disambiguating Names acrossDocuments.
Proceedings of ACL.M.
Remy.
2002.
Wikipedia: The Free Encyclopedia.Online Information Review Year, 26(6).E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
Proceedings of AAAI,pages 1044?1049.M.
Ruiz-Casado, E. Alfonseca, and P. Castells.2005.
Automatic extraction of semantic relation-ships for wordnet by means of pattern learning fromwikipedia.
Proceedings of NLDB 2005.M.
Ruiz-Casado, E. Alfonseca, and P. Castells.
2006.From Wikipedia to semantic relationships: a semi-automated annotation approach.
Proceedings ofESWC.B.
Schiffman, I. Mani, and K.J.
Concepcion.
2001.Producing biographical summaries: combining lin-guistic knowledge with corpus statistics.
Proceed-ings of ACL, pages 458?465.M.
Thelen and E. Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extrac-tion pattern contexts.
In Proceedings of EMNLP,pages 14?21.N.
Wacholder, Y. Ravin, and M. Choi.
1997.
Disam-biguation of proper names in text.
Proceedings ofANLP, pages 202?208.C.
Walker, S. Strassel, J. Medero, and K. Maeda.
2006.Ace 2005 multilingual training corpus.
LinguisticData Consortium.R.
Weischedel, J. Xu, and A. Licuanan.
2004.
AHybrid Approach to Answering Biographical Ques-tions.
New Directions In Question Answering, pages59?70.M.
Wick, A. Culotta, and A. McCallum.
2006.
Learn-ing field compatibilities to extract database recordsfrom unstructured text.
In Proceedings of EMNLP,pages 603?611.L.
Zhou, M. Ticrea, and E. Hovy.
2004.
Multidoc-ument biography summarization.
Proceedings ofEMNLP, pages 434?441.308
