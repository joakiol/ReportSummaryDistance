Proceedings of the 7th Workshop on Statistical Machine Translation, pages 76?83,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSPEDE: Probabilistic Edit Distance Metrics for MT EvaluationMengqiu Wang and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305 USA{mengqiu,manning}@cs.stanford.eduAbstractThis paper describes Stanford University?s sub-mission to the Shared Evaluation Task of WMT2012.
Our proposed metric (SPEDE) com-putes probabilistic edit distance as predictionsof translation quality.
We learn weighted editdistance in a probabilistic finite state machine(pFSM) model, where state transitions corre-spond to edit operations.
While standard editdistance models cannot capture long-distanceword swapping or cross alignments, we rectifythese shortcomings using a novel pushdownautomaton extension of the pFSM model.
Ourmodels are trained in a regression framework,and can easily incorporate a rich set of linguis-tic features.
Evaluated on two different pre-diction tasks across a diverse set of datasets,our methods achieve state-of-the-art correla-tion with human judgments.1 IntroductionWe describe the Stanford Probabilistic Edit DistanceEvaluation (SPEDE) metric, which makes predic-tions of translation quality by computing weightededit distance.
We model weighted edit distance ina probabilistic finite state machine (pFSM), wherestate transitions correspond to edit operations.
Theweights of the edit operations are then automaticallylearned in a regression framework.
One of the ma-jor contributions of this paper is a novel extensionof the pFSM model into a probabilistic PushdownAutomaton (pPDA), which enhances traditional edit-distance models with the ability to model phrase shiftand word swapping.
Furthermore, we give a new log-linear parameterization to the pFSM model, whichallows it to easily incorporate rich linguistic features.We conducted extensive experiments on a di-verse set of standard evaluation data sets (NISTOpenMT06, 08; WMT06, 07, 08).
Our modelsachieve or surpass state-of-the-art results on all testsets.2 Related WorkResearch in automatic machine translation (MT) eval-uation metrics has been a key driving force behindthe recent advances of statistical machine transla-tion (SMT) systems.
The early seminal work onautomatic MT metrics (e.g., BLEU and NIST) islargely based on n-gram matches (Papineni et al,2002; Doddington, 2002).
Despite their simplicity,these measures have shown good correlation with hu-man judgments, and enabled large-scale evaluationsacross many different MT systems, without incurringthe huge labor cost of human evaluation (Callison-Burch et al (2009; 2010; 2011), inter alia).Later metrics that move beyond n-grams achievehigher accuracy and improved robustness from re-sources like WordNet synonyms (Miller et al, 1990),paraphrasing (Zhou et al, 2006; Snover et al, 2009;Denkowski and Lavie, 2010), and syntactic parsestructures (Liu et al, 2005; Owczarzak et al, 2008;He et al, 2010).
But a common problem in thesemetrics is they typically resort to ad-hoc tuning meth-ods instead of principled approaches to incorporatelinguistic features.
Recent models use linear orSVM regression and train them against human judg-ments to automatic learn feature weights, and haveshown state-of-the-art correlation with human judg-ments (Albrecht and Hwa, 2007a; Albrecht and Hwa,2007b; Sun et al, 2008; Pado et al, 2009).
Thedrawback, however, is they rely on time-consuming76Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set(Doc:AFP CMN 20070703.0005, system09, sent 1).
The three rows below are the best state transition (edit) sequencesthat transforms REF to SYS, according to the three proposed models.
The corresponding alignments generated by themodels (pFSM, pPDA, pPDA+f ) are shown with different styled lines, with later models in the order generating strictlymore alignments than earlier ones.
The gold human evaluation score is 6.5, and model predictions are: pPDA+f 5.5,pPDA 4.3, pFSM 3.1, METEORR 3.2, TERR 2.8.preprocessing modules to extract linguistic features(e.g., a full end-to-end textual entailment system wasneeded in Pado et al (2009)), which severely lim-its their practical use.
Furthermore, these modelsemploy a large number of features (on the order ofhundreds), and consequently make the model predic-tions opaque and hard to analyze.3 pFSMs for MT RegressionWe start off by framing the problem of machine trans-lation evaluation in terms of weighted edit distancecalculated using probabilistic finite state machines(pFSMs).
A FSM defines a language by accepting astring of input tokens in the language, and rejectingthose that are not.
A probabilistic FSM defines theprobability that a string is in a language, extending onthe concept of a FSM.
Commonly used models suchas HMMs, n-gram models, Markov Chains, proba-bilistic finite state transducers and PCFGs all fall inthe broad family of pFSMs (Knight and Al-Onaizan,1998; Eisner, 2002; Kumar and Byrne, 2003; Vidalet al, 2005).
Unlike all the other applications ofFSMs where tokens in the language are words, inour language tokens are edit operations.
A string oftokens that our FSM accepts is an edit sequence thattransforms a reference translation (denoted as ref )into a system translation (sys).Our pFSM has a unique start and stop state, andone state per edit operation (i.e., Insert, Delete, Sub-stitution).
The probability of an edit sequence e isgenerated by the model is the product of the state tran-sition probabilities in the pFSM, formally describedas:w(e | s,r) =1Z|e|?i=1exp ?
?
f(ei?1,ei,s,r) (1)We featurize each of the state changes with a log-linear parameterization; f is a set of binary featurefunctions defined over pairs of neighboring states(by the Markov assumption) and the input sentences,and ?
are the associated feature weights; r and s areshorthand for ref and sys; Z is a partition function.In this basic pFSM model, the feature functions aresimply identity functions that emit the current state,and the state transition sequence of the previous stateand the current state.The feature weights are then automatically learnedby training a global regression model where sometranslational equivalence judgment score (e.g., hu-man assessment score, or HTER (Snover et al,2006)) for each sys and ref translation pair is theregression target (y?).
Since the ?gold?
edit sequenceare not given at training or prediction time, we treatthe edit sequences as hidden variables and sum over77them in our model.
We introduce a new regressionvariable y ?
R which is the log-sum of the unnormal-ized weights (Eqn.
(1)) of all edit sequences, formallyexpressed as:y = log ?e?
?e?|e?|?i=1exp ?
?
f(ei?1,ei,s,r) (2)The sum over an exponential number of edit se-quences in e?
is solved efficiently using a forward-backward style dynamic program.
Any edit sequencethat does not lead to a complete transformation ofthe translation pair has a probability of zero in ourmodel.
Our regression target then seeks to minimizethe least squares error with respect to y?, plus a L2-norm regularizer term parameterized by ?
:?
?
= min?
{?si,ri[y?i ?
(y|si|+ |ri|+?
)]2 +???
?2}(3)The |si|+ |ri| is a length normalization term for theith training instance, and ?
is a scaling constant foradjusting to different scoring standards (e.g., 7-pointscale vs. 5-point scale), whose value is automaticallylearned.
At test time, y/(|s|+ |r|)+?
is computedas the predicted score.We replaced the standard substitution edit opera-tion with three new operations: Sword for same wordsubstitution, Slemma for same lemma substitution, andSpunc for same punctuation substitution.
In otherwords, all but the three matching-based substitutionsare disallowed.
The start state can transition into anyof the edit states with a constant unit cost, and eachedit state can transition into any other edit state ifand only if the edit operation involved is valid at thecurrent edit position (e.g., the model cannot transi-tion into Delete state if it is already at the end of ref ;similarly it cannot transition into Slemma unless thelemma of the two words under edit in sys and refmatch).
When the end of both sentences are reached,the model transitions into the stop state and ends theedit sequence.
The first row in Figure 1 starting withpFSM shows a state transition sequence for an exam-ple sys/ref translation pair.
There exists a one-to-onecorrespondence between substitution edits and wordalignments.
Therefore this example state transitionsequence correctly generates an alignment for theword 43 and people.It is helpful to compare with the TER met-ric (Snover et al, 2006), which is based on the ideaof word error rate measured in edit distance, to betterunderstand the intuition behind our model.
Thereare two major improvements in our model: 1) theedit operations in our model are weighted, as definedby the feature functions and weights; 2) the weightsare automatically learned, instead of being uniformor manually set; and 3) we model state transitions,which can be understood as a bigram extension ofthe unigram edit distance model used in TER.
Forexample, if in our learned model the feature for twoconsecutive Sword states has a positive weight, thenour model would favor consecutive same word sub-stitutions, whereas in the TER model the order ofthe substitution does not matter.
The extended TER-plus (Snover et al, 2009) metric addresses the firstproblem but not the other two.3.1 pPDA ExtensionA shortcoming of edit distance models is that theycannot handle long-distance word swapping ?
apervasive phenomenon found in most natural lan-guages.
1 Edit operations in standard edit distancemodels need to obey strict incremental order in theiredit position, in order to admit efficient dynamic pro-gramming solutions.
The same limitation is sharedby our pFSM model, where the Markov assumptionis made based on the incremental order of edit po-sitions.
Although there is no known solution to thegeneral problem of computing edit distance wherelong-distance swapping is permitted (Dombb et al,2010), approximate algorithms do exist.
We presenta simple but novel extension of the pFSM modelto a probabilistic pushdown automaton (pPDA), tocapture non-nested word swapping within limiteddistance, which covers a majority of word swappingin observed in real data (Wu, 2010).A pPDA, in its simplest form, is a pFSM whereeach control state is equipped with a stack (Esparzaand Kucera, 2005).
The addition of stacks for eachtransition state endows the machine with memory,extending its expressiveness beyond that of context-free formalisms.
By construction, at any stage in anormal edit sequence, the pPDA model can ?jump?1The edit distance algorithm described in Cormen etal.
(2001) can only handle adjacent word swapping (transpo-sition), but not long-distance swapping.78forward within a fixed distance (controlled by a maxdistance parameter) to a new edit position on eitherside of the sentence pair, and start a new edit subse-quence from there.
Assuming the jump was made onthe sys side, 2 the machine remembers its current editposition in sys as Jstart , and the destination positionon sys after the jump as Jlanding.We constrain our model so that the only edit op-erations that are allowed immediately following a?jump?
are from the set of substitution operations(e.g., Sword).
And after at least one substitutionhas been made, the device can now ?jump?
backto Jstart , remembering the current edit position asJend .
Another constraint here is that after the back-ward ?jump?, all edit operations are permitted exceptfor Delete, which cannot take place until at least onesubstitution has been made.
When the edit sequenceadvances to position Jlanding, the only operation al-lowed at that point is another ?jump?
forward opera-tion to position Jend , at which point we also clear allmemory about jump positions and reset.An intuitive explanation is that when pPDA makesthe first forward jump, a gap is left in sys that hasnot been edited yet.
It remembers where it left off,and comes back to it after some substitutions havebeen made to complete the edit sequence.
The sec-ond row in Figure 1 (starting with pPDA) illustratesan edit sequence in a pPDA model that involves three?jump?
operations, which are annotated and indexedby number 1-3 in the example.
?Jump 1?
creates anun-edited gap between word 43 and western, aftertwo substitutions, the model makes ?jump 2?
to goback and edit the gap.
The only edit permitted imme-diately after ?jump 2?
is deleting the comma in ref,since inserting the word 43 in sys before any substi-tution is disallowed.
Once the gap is completed, themodel resumes at position Jend by making ?jump 3?,and completes the jump sequence.The ?jumps?
allowed the model to align wordssuch as western India, in addition to the alignmentsof 43 people found by the pFSM.
In practice, wefound that our extension gives a big boost to modelperformance (cf.
Section 5.1), with only a modestincrease in computation time.
32Recall that we transform ref into sys, and thus on the sysside, we can only insert but not delete.
The argument appliesequally to the case where the jump was made on the other side.3The length of the longest edit sequence with jumps only3.2 Parameter EstimationSince the least squares operator preserves convexity,and the inner log-sum-exponential function is con-vex, the resulting objective function is also convex.For parameter learning, we used the limited memoryquasi-newton method (Liu and Nocedal, 1989) to findthe optimal feature weights and scaling constant forthe objective.
We initialized ?
=~0, ?
= 0, and ?
= 5.We also threw away features occurring fewer thanfive times in training corpus.
Gradient calculationwas similar to other pFSM models, such as HMMs,we omitted the details here, for brevity.4 Rich Linguistic FeaturesWe add new substitution operations beyond those in-troduced in Section 3, to capture synonyms and para-phrase in the translations.
Synonym relations are de-fined according to WordNet (Miller et al, 1990), andparaphrase matches are given by a lookup table usedin TERplus (Snover et al, 2009).
To better take ad-vantage of paraphrase information at the multi-wordphrase level, we extended our substitution operationsto match longer phrases by adding one-to-many andmany-to-many bigram block substitutions.5 ExperimentsThe goal of our experiments is to test both the ac-curacy and robustness of the proposed new models.We then show that modeling word swapping and richlinguistics features further improve our results.To better situate our work among past researchand to draw meaningful comparison, we use exactlythe same standard evaluation data sets and metricsas Pado et al (2009), which is currently the state-of-the-art result for regression-based MT evaluation.We consider four widely used MT metrics (BLEU,NIST, METEOR (Banerjee and Lavie, 2005) (v0.7),and TER) as our baselines.
Since our models aretrained to regress human evaluation scores, to makea direct comparison in the same regression setting,we also train a small linear regression model for eachbaseline metric in the same way as descried in Padoet al (2009).
These regression models are strictlymore powerful than the baseline metrics and showhigher robustness and better correlation with humanincreased by 0.5 ?max(|s|, |r|) in the worst case, and by andlarge swapping is rare in comparison to basic edits.79Data Set Our Metrics Baseline Metricstrain test pFSM pPDA pPDA+f BLEUR NISTR TERR METR MTR RTER MT+RTERA+C U 54.6 55.3 57.2 49.9 49.5 50.1 49.1 50.1 54.5 55.6A+U C 59.9 63.8 65.7 53.9 53.1 50.3 61.1 57.3 58.0 62.7C+U A 61.2 60.4 59.8 52.5 50.4 54.5 60.1 55.2 59.9 61.1MT08 MT06 65.2 63.4 64.5 57.6 55.1 63.8 62.1 62.6 62.2 65.2Table 1: Overall results on OpenMT08 and OpenMT06 evaluation data sets.
The R (as in BLEUR) refers to theregression model trained for each baseline metric, same as Pado et al (2009).
The first three rows are round-robintrain/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu).
The last row are results trainedon entire OpenMT08 (A+C+U) and tested on OpenMT06.
Numbers in this table are Spearman?s rank correlation ?between human assessment scores and model predictions.
The pPDA column describes our pPDA model with jumpdistance limit 5.
METR is shorthand for METEORR.
+f means the model includes synonyms and paraphrase features(cf.
Section 4).
Best results and scores that are not statistically significantly worse are highlighted in bold in each row.judgments.
4 We also compare our models with thestate-of-the-art linear regression models reported inPado et al (2009) that combine features from mul-tiple MT evaluation metrics (MT), as well as richlinguistic features from a textual entailment system(RTE).In all of our experiments, each reference and sys-tem translation sentence pair is tokenized using thePTB (Marcus et al, 1993) tokenization script, andlemmatized by the Porter Stemmer (Porter, 1980).Statistical significance tests are performed using thepaired bootstrap resampling method (Koehn, 2004).We divide our experiments into two sections, basedon two different prediction tasks ?
predicting abso-lute scores and predicting pairwise preference.5.1 Exp.
1: Predicting Absolute ScoresThe first task is to evaluate a system translationon a seven point Likert scale against a single ref-erence.
Higher scores indicate translations that arecloser to the meaning intended by the reference.
Hu-man ratings in the form of absolute scores are avail-able for standard evaluation data sets such as NISTOpenMT06,08.5 Since our model makes predictionsat the granularity of a whole sentence, we focus onsentence-level evaluation.
A metric?s goodness isjudged by how well it correlates with human judg-ments, and Spearman?s rank correlation (?)
is re-ported for all experiments in this section.We used the NIST OpenMT06 corpus for develop-ment purposes, and reserved the NIST OpenMT08corpus for post-development evaluation.
The4See Pado et al (2009) for more discussion.5Available from http://www.nist.gov.OpenMT06 data set contains 1,992 English trans-lations of Arabic newswire text from 8 MT systems.For development, we used a 2-fold cross-validationscheme with splits at the first 1,000 and last 992 sen-tences.
The OpenMT08 data set contains Englishtranslations of newswire text from three languages(Arabic has 2,769 pairs from 13 MT systems; Chi-nese has 1,815 pairs from 15; and Urdu has 1,519pairs, from 7).
We followed the same experimentalsetup as Pado et al (2009), using a ?round robin?training/testing scheme, i.e., we train a model on datafrom two languages, making predictions for the third.We also show results of models trained on the entireOpenMT08 data set and tested on OpenMT06.Overall ComparisonResults of our proposed models compared againstthe baseline models described in Pado et al (2009)are shown in Table 1.
The pFSM and pPDA mod-els do not use any additional information other thanwords and lemmas, and thus make a fair comparisonwith the baseline metrics.
6 We can see from the ta-ble that pFSM significantly outperforms all baselineson Urdu and Arabic, but trails behind METEORRon Chinese by a small margin (1.2 point in Spear-man?s ?).
On Chinese data set, the pPDA exten-sion gives results significantly better than the bestbaseline metrics for Chinese (2.7 better than METE-ORR).
It is also significantly better than pFSM (by6METEORR actually has an unfair advantage in this compari-son, since it uses synonym information from WordNet; TERRon the other hand has a disadvantage because it does not uselemmas.
Lemma is added later in the TERplus extension (Snoveret al, 2009).803.9 points), suggesting that modeling word swappingis particularly rewarding for Chinese language.
Onthe other hand, pPDA model does not perform bet-ter than the pFSM model on Arabic in MT08 andOpenMT06 (which is also Arabic-to-English).
Thisobservation is consistent with findings in earlier workthat Chinese-English translations exhibit much moremedium and long distance reordering than languageslike Arabic (Birch et al, 2009).Both the pFSM and pPDA models also signifi-cantly outperform the MTR linear regression modelthat combines the outputs of all four baselines, on allthree source languages.
This demonstrates that ourregression model is more robust and accurate than astate-of-the-art system combination linear-regressionmodel.
The RTER and MT+RTER linear regressionmodels benefit from the rich linguistic features in thetextual entailment system?s output.
It has access toall the features in pPDA+f such as paraphrase and de-pendency parse relations, and many more (e.g., NormBank, part-of-speech, negation, antonyms).
However,our pPDA+f model rivals the performance of RTERand MT+RTER on Arabic (with no statistically sig-nificant difference from RTER), and greatly improveover these two models on Urdu and Chinese.
Mostnoticeably, pPDA+f is 7.7 points better than RTERon Chinese.5.2 Exp.
2: Predicting Pairwise PreferencesTo further test our model?s robustness, we evaluateit on WMT data sets with a different prediction taskin which metrics make pairwise preference judg-ments between translation systems.
The WMT06-08 data sets are much larger in comparison to theOpenMT06 and 08 data.
They contain MT outputs ofover 40 systems from five different source languages(French, German, Spanish, Czech, and Hungarian).The WMT06, 07 and 08 sets contains 10,159, 5,472and 6,856 sentence pairs, respectively.
We used por-tions of WMT 06 and 07 data sets 7 that are annotatedwith absolute scores on a five point scale for training,and the WMT08 data set annotated with pairwisepreference for testing.To generate pairwise preference predictions, wefirst predict an absolute score for each system trans-lation, then compare the scores between each system7Available from http://www.statmt.org.pair, and give preference to the higher score.
Weadopt the sentence-level evaluation metric used inPado et al (2009), which measures the consistency(accuracy) of metric predictions with human prefer-ences.
The random baseline for this task on WMT08data set is 39.8%.Models WMT06 WMT07 WMT06+07pPDA+f 51.6 52.4 52.0BLEUR 49.7 49.5 49.6METEORR 51.4 51.4 51.5NISTR 50.0 50.3 50.2TERR 50.9 51.0 51.2MTR 50.8 51.5 51.5RTER 51.8 50.7 51.9MT+RTER 52.3 51.8 52.5Table 2: Pairwise preference prediction results on WMT08test set.
Each column shows a different training data set.Numbers in this table are model?s consistency with humanpairwise preference judgments.
Best result on each testset is highlighted in bold.Results are shown in Table 2.
Similar to the resultson OpenMT experiments, our model consistently out-performed BLEUR, METEORR, NISTR and TERR.Our model also gives better performance than theMTR ensemble model on all three tests; and ties withRTER in two out of the three tests but performs sig-nificantly better on the other test.
The MT+RTERensemble model is better on two tests, but worseon the other.
But overall the two systems are quitecomparable, with less than 0.6% accuracy difference.The results also show that our method is stable acrossdifferent training sets, with test accuracy differencesless than 0.4%.6 ConclusionWe described the SPEDE metric for sentence levelMT evaluation.
It is based on probabilistic finite statemachines to compute weighted edit distance.
Ourmodel admits a rich set of linguistic features, andcan be trained to learn feature weights automaticallyby optimizing a regression objective.
A novel push-down automaton extension was also presented forcapturing long-distance word swapping.
Our metricsachieve state-of-the-art results on a wide range ofstandard evaluations, and are much more lightweightthan previous regression models.81AcknowledgementsWe gratefully acknowledge the support of DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the support of the DARPA BroadOperational Language Translation (BOLT) programthrough IBM.
Any opinions, findings, and conclusionor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe view of the DARPA, AFRL, or the US govern-ment.ReferencesJ.
Albrecht and R. Hwa.
2007a.
A re-examination ofmachine learning approaches for sentence-level MTevaluation.
In Proceedings of ACL.J.
Albrecht and R. Hwa.
2007b.
Regression for sentence-level MT evaluation with pseudo references.
In Pro-ceedings of ACL.S.
Banerjee and A. Lavie.
2005.
Meteor: An automaticmetric for MT evaluation with improved correlationwith human judgments.
In Proceedings of ACL Work-shop on Intrinsic and Extrinsic Evaluation Measures.A.
Birch, P. Blunsom, and M. Osborne.
2009.
A quantita-tive analysis of reordering phenomena.
In Proceedingsof WMT 09.C.
Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.2009.
Findings of the 2009 Workshop on StatisticalMachine Translation.
In Proceedings of the FourthWorkshop on Statistical Machine Translation.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O. Zaidan.
2010.
Findings of the2010 joint workshop on Statistical Machine Translationand metrics for Machine Translation.
In Proceedingsof Joint WMT 10 and MetricsMatr Workshop at ACL.C.
Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.2011.
Findings of the 2011 workshop on statistical ma-chine translation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation.T.
H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.2001.
Introduction to Algorithms, Second Edition.
MITPress.M.
Denkowski and A. Lavie.
2010.
Extending the ME-TEOR machine translation evaluation metric to thephrase level.
In Proceedings of HLT/NAACL.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram cooccurrence statistics.In Proceedings of HLT.Y.
Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.2010.
The approximate swap and mismatch edit dis-tance.
Theoretical Computer Science, 411(43).J.
Eisner.
2002.
Parameter estimation for probabilisticfinite-state transducers.
In Proceedings of ACL.J.
Esparza and A. Kucera.
2005.
Quantitative analysisof probabilistic pushdown automata: Expectations andvariances.
In Proceedings of the 20th Annual IEEESymposium on Logic in Computer Science.Y.
He, J.
Du, A.
Way, and J. van Genabith.
2010.
TheDCU dependency-based metric inWMT-MetricsMATR2010.
In Proceedings of Joint WMT 10 and Metrics-Matr Workshop at ACL.K.
Knight and Y. Al-Onaizan.
1998.
Translation withfinite-state devices.
In Proceedings of AMTA.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of EMNLP.S.
Kumar and W. Byrne.
2003.
A weighted finite statetransducer implementation of the alignment templatemodel for statistical machine translation.
In Proceed-ings of HLT/NAACL.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory BFGS method for large scale optimization.
Math.Programming, 45:503?528.D.
Liu, , and D. Gildea.
2005.
Syntactic features for eval-uation of machine translation.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of english: thePenn Treebank.
Computational Linguistics, 19(2):313?330.G.
A.Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.Miller.
1990.
WordNet: an on-line lexical database.International Journal of Lexicography, 3(4).F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL.K.
Owczarzak, J. van Genabith, and A.
Way.
2008.
Evalu-ating machine translation with LFG dependencies.
Ma-chine Translation, 21(2):95?119.S.
Pado, M. Galley, D. Jurafsky, and C. D. Manning.
2009.Robust machine translation evaluation with entailmentfeatures.
In Proceedings of ACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL.M.F.
Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings ofAMTA.M.
Snover, , N. Madnani, B. Dorr, and R. Schwartz.
2009.Fluency, adequacy, or HTER?
exploring different hu-man judgments with a tunable MT metric.
In Proceed-ings of WMT09 Workshop.82S.
Sun, Y. Chen, and J. Li.
2008.
A re-examination onfeatures in regression based approach to automatic MTevaluation.
In Proceedings of ACL.E.
Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,and R. C. Carrasco.
2005.
Probabilistic finite-state ma-chines part I. IEEE Transactions on Pattern Analysisand Machine Intelligence, 27(7):1013?1025.D.
Wu, 2010.
CRC Handbook of Natural Language Pro-cessing, chapter How to Select an Answer String?,pages 367?408.
CRC Press.L.
Zhou, C.Y.
Lin, and E. Hovy.
2006.
Re-evaluatingmachine translation results with paraphrase support.
InProceedings of EMNLP.83
