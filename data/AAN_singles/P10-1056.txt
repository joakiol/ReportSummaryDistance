Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 544?554,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAutomatic Evaluation of Linguistic Quality in Multi-DocumentSummarizationEmily Pitler, Annie Louis, Ani NenkovaComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USAepitler,lannie,nenkova@seas.upenn.eduAbstractTo date, few attempts have been madeto develop and validate methods for au-tomatic evaluation of linguistic quality intext summarization.
We present the firstsystematic assessment of several diverseclasses of metrics designed to capture var-ious aspects of well-written text.
We trainand test linguistic quality models on con-secutive years of NIST evaluation data inorder to show the generality of results.
Forgrammaticality, the best results come froma set of syntactic features.
Focus, coher-ence and referential clarity are best evalu-ated by a class of features measuring localcoherence on the basis of cosine similaritybetween sentences, coreference informa-tion, and summarization specific features.Our best results are 90% accuracy for pair-wise comparisons of competing systemsover a test set of several inputs and 70%for ranking summaries of a specific input.1 IntroductionEfforts for the development of automatic text sum-marizers have focused almost exclusively on im-proving content selection capabilities of systems,ignoring the linguistic quality of the system out-put.
Part of the reason for this imbalance is theexistence of ROUGE (Lin and Hovy, 2003; Lin,2004), the system for automatic evaluation of con-tent selection, which allows for frequent evalua-tion during system development and for report-ing results of experiments performed outside ofthe annual NIST-led evaluations, the DocumentUnderstanding Conference (DUC)1 and the TextAnalysis Conference (TAC)2.
Few metrics, how-ever, have been proposed for evaluating linguistic1http://duc.nist.gov/2http://www.nist.gov/tac/quality and none have been validated on data fromNIST evaluations.In their pioneering work on automatic evalua-tion of summary coherence, Lapata and Barzilay(2005) provide a correlation analysis between hu-man coherence assessments and (1) semantic re-latedness between adjacent sentences and (2) mea-sures that characterize how mentions of the sameentity in different syntactic positions are spreadacross adjacent sentences.
Several of their modelsexhibit a statistically significant agreement withhuman ratings and complement each other, yield-ing an even higher correlation when combined.Lapata and Barzilay (2005) and Barzilay andLapata (2008) both show the effectiveness ofentity-based coherence in evaluating summaries.However, fewer than five automatic summarizerswere used in these studies.
Further, both setsof experiments perform evaluations of mixed setsof human-produced and machine-produced sum-maries, so the results may be influenced by theease of discriminating between a human and ma-chine written summary.
Therefore, we believe it isan open question how well these features predictthe quality of automatically generated summaries.In this work, we focus on linguistic quality eval-uation for automatic systems only.
We analyzehow well different types of features can rank goodand poor machine-produced summaries.
Goodperformance on this task is the most desired prop-erty of evaluation metrics during system develop-ment.
We begin in Section 2 by reviewing thevarious aspects of linguistic quality that are rel-evant for machine-produced summaries and cur-rently used in manual evaluations.
In Section 3,we introduce and motivate diverse classes of fea-tures to capture vocabulary, sentence fluency, andlocal coherence properties of summaries.
We eval-uate the predictive power of these linguistic qual-ity metrics by training and testing models on con-secutive years of NIST evaluations (data described544in Section 4).
We test the performance of differ-ent sets of features separately and in combinationwith each other (Section 5).
Results are presentedin Section 6, showing the robustness of each classand their abilities to reproduce human rankings ofsystems and summaries with high accuracy.2 Aspects of linguistic qualityWe focus on the five aspects of linguistic qual-ity that were used to evaluate summaries in DUC:grammaticality, non-redundancy, referential clar-ity, focus, and structure/coherence.3 For each ofthe questions, all summaries were manually ratedon a scale from 1 to 5, in which 5 is the best.The exact definitions that were provided to thehuman assessors are reproduced below.Grammaticality: The summary should have no datelines,system-internal formatting, capitalization errors or obviouslyungrammatical sentences (e.g., fragments, missing compo-nents) that make the text difficult to read.Non-redundancy: There should be no unnecessary repeti-tion in the summary.
Unnecessary repetition might take theform of whole sentences that are repeated, or repeated facts,or the repeated use of a noun or noun phrase (e.g., ?Bill Clin-ton?)
when a pronoun (?he?)
would suffice.Referential clarity: It should be easy to identify who or whatthe pronouns and noun phrases in the summary are referringto.
If a person or other entity is mentioned, it should be clearwhat their role in the story is.
So, a reference would be un-clear if an entity is referenced but its identity or relation tothe story remains unclear.Focus: The summary should have a focus; sentences shouldonly contain information that is related to the rest of the sum-mary.Structure and Coherence: The summary should be well-structured and well-organized.
The summary should not justbe a heap of related information, but should build from sen-tence to sentence to a coherent body of information about atopic.These five questions get at different aspects ofwhat makes a well-written text.
We therefore pre-dict each aspect of linguistic quality separately.3 Indicators of linguistic qualityMultiple factors influence the linguistic quality oftext in general, including: word choice, the ref-erence form of entities, and local coherence.
Weextract features which serve as proxies for each ofthe factors mentioned above (Sections 3.1 to 3.5).In addition, we investigate some models of gram-maticality (Chae and Nenkova, 2009) and coher-ence (Graesser et al, 2004; Soricut and Marcu,2006; Barzilay and Lapata, 2008) from prior work(Sections 3.6 to 3.9).3http://www-nlpir.nist.gov/projects/duc/duc2006/quality-questions.txtAll of the features we investigate can be com-puted automatically directly from text, but somerequire considerable linguistic processing.
Severalof our features require a syntactic parse.
To extractthese, all summaries were parsed by the Stanfordparser (Klein and Manning, 2003).3.1 Word choice: language modelsPsycholinguistic studies have shown that peopleread frequent words and phrases more quickly(Haberlandt and Graesser, 1985; Just and Carpen-ter, 1987), so the words that appear in a text mightinfluence people?s perception of its quality.
Lan-guage models (LM) are a way of computing howfamiliar a text is to readers using the distributionof words from a large background corpus.
Bigramand trigram LMs additionally capture grammati-cality of sentences using properties of local tran-sitions between words.
For this reason, LMs arewidely used in applications such as generation andmachine translation to guide the production of sen-tences.
Judging from the effectiveness of LMs inthese applications, we expect that they will pro-vide a strong baseline for the evaluation of at leastsome of the linguistic quality aspects.We built unigram, bigram, and trigram lan-guage models with Good-Turing smoothing overthe New York Times (NYT) section of the EnglishGigaword corpus (over 900 million words).
Weused the SRI Language Modeling Toolkit (Stol-cke, 2002) for this purpose.
For each of the threengram language models, we include the min, max,and average log probability of the sentences con-tained in a summary, as well as the overall logprobability of the entire summary.3.2 Reference form: Named entitiesThis set of features examines whether named enti-ties have informative descriptions in the summary.We focus on named entities because they appearoften in summaries of news documents and are of-ten not known to the reader beforehand.
In addi-tion, first mentions of entities in text introduce theentity into the discourse and so must be informa-tive and properly descriptive (Prince, 1981; Frau-rud, 1990; Elsner and Charniak, 2008).We run the Stanford Named Entity Recognizer(Finkel et al, 2005) and record the number ofPERSONs, ORGANIZATIONs, and LOCATIONs.First mentions to people Feature exploration onour development set found that under-specified545references to people are much more disruptiveto a summary than short references to organiza-tions or locations.
In fact, prior work in Nenkovaand McKeown (2003) found that summaries thathave been rewritten so that first mentions of peo-ple are informative descriptions and subsequentmentions are replaced with more concise referenceforms are overwhelmingly preferred to summarieswhose entity references have not been rewritten.In this class, we include features that reflectthe modification properties of noun phrases (NPs)in the summary that are first mentions to people.Noun phrases can include pre-modifiers, apposi-tives, prepositional phrases, etc.
Rather than pre-specifying all the different ways a person expres-sion can be modified, we hoped to discover thebest patterns automatically, by including featuresfor the average number of each Part of Speech(POS) tag occurring before, each syntactic phraseoccurring before4, each POS tag occurring after,and each syntactic phrase occurring after the headof the first mention NP for a PERSON.
To measureif the lack of pre or post modification is particu-larly detrimental, we also include the proportionof PERSON first mention NPs with no words be-fore and with no words after the head of the NP.Summarization specific Most summarizationsystems today are extractive and create summariesusing complete sentences from the source docu-ments.
A subsequent mention of an entity in asource document which is extracted to be the firstmention of the entity in the summary is proba-bly not informative enough.
For each type ofnamed entity (PERSON, ORGANIZATION, LO-CATION), we separately record the number of in-stances which appear as first mentions in the sum-mary but correspond to non-first mentions in thesource documents.3.3 Reference form: NP syntaxSome summaries might not include people andother named entities at all.
To measure how en-tities are referred to more generally, we includefeatures about the overall syntactic patterns foundin NPs: the average number of each POS tag andeach syntactic phrase occurring inside NPs.4We define a linear order based on a preorder traversal ofthe tree, so syntactic phrases which dominate the head areconsidered occurring before the head.3.4 Local coherence: Cohesive devicesIn coherent text, constituent clauses and sentencesare related and depend on each other for their in-terpretation.
Referring expressions such as pro-nouns link the current utterance to those where theentities were previously mentioned.
In addition,discourse connectives such as ?but?
or ?because?relate propositions or events expressed by differ-ent clauses or sentences.
Both these categoriesare known cohesive or linking devices in human-produced text (Halliday and Hasan, 1976).
Themere presence of such items in a text would be in-dicative of better structure and coherence.We compute a number of shallow features thatprovide a cheap way of capturing the above intu-itions: the number of demonstratives, pronouns,and definite descriptions as well as the number ofsentence-initial discourse connectives.3.5 Local coherence: ContinuityThis class of linguistic quality indicators is a com-bination of factors related to coreference, adjacentsentence similarity, and summary-specific contextof surface cohesive devices.Summarization specific Extractive multi-document summaries often lack appropriateantecedents for pronouns and proper context forthe use of discourse connectives.In fact, early work in summarization (Paice,1980; Paice, 1990) has pointed out that the pres-ence of cohesive devices described in the previoussection might in fact be the source of problems.A manual analysis of automatic summaries (Ot-terbacher et al, 2002) also revealed that anaphoricreferences that cannot be resolved and unclear dis-course relations constitute more than 30% of allrevisions required to manually rewrite summariesinto a more coherent form.To identify these potential problems, we adaptthe features for surface cohesive devices to indi-cate whether referring expressions and discourseconnectives appear in the summary with the samecontext as in the input documents.For each of the cohesive devices discussed inSection 3.4?demonstratives, pronouns, definitedescriptions, and sentence-initial discourse con-nectives?we compare the previous sentence inthe summary with the previous sentence in the in-put article.
Two features are computed for eachtype of cohesive device: (1) number of times thepreceding sentence in the summary is the same546as the preceding sentence in the input and (2) thenumber of times the preceding sentence in sum-mary is different from that in the input.
Sincethe previous sentence in the input text often con-tains the antecedent of pronouns in the currentsentence, if the previous sentence from the inputis also included in the summary, the pronoun ishighly likely to have a proper antecedent.We also compute the proportion of adjacent sen-tences in the summary that were extracted from thesame input document.Coreference Steinberger et al (2007) compare thecoreference chains in input documents and in sum-maries in order to locate potential problems.
Weinstead define a set of more general features re-lated to coreference that are not specific to sum-marization and are applicable for any text.
Ourfeatures check the existence of proper antecedentsfor pronouns in the summary without reference tothe text of the input documents.We use the publicly available pronoun reso-lution system described in Charniak and Elsner(2009) to mark possible antecedents for pronounsin the summary.
We then compute as features thenumber of times an antecedent for a pronoun wasfound in the previous sentence, in the same sen-tence, or neither.
In addition, we modified the pro-noun resolution system to also output the probabil-ity of the most likely antecedent and include theaverage antecedent probability for the pronounsin the text.
Automatic coreference systems aretrained on human-produced texts and we expecttheir accuracies to drop when applied to automat-ically generated summaries.
However, the predic-tions and confidence scores still reflect whetheror not possible antecedents exist in previous sen-tences that match in gender/number, and so maystill be useful for coherence evaluation.Cosine similarity We use cosine similarity tocompute the overlap of words in adjacent sen-tences si and si+1 as a measure of continuity.cos?
=vsi .vsi+1||vsi ||||vsi+1 ||(1)The dimensions of the two vectors (vsi andvsi+1) are the total number of word types fromboth sentences si and si+1.
Stop words were re-tained.
The value of each dimension for a sentenceis the number of tokens of that word type in thatsentence.
We compute the min, max, and averagevalue of cosine similarity over the entire summary.While some repetition is beneficial for cohe-sion, too much repetition leads to redundancy inthe summary.
Cosine similarity is thus indicativeof both continuity and redundancy.3.6 Sentence fluency: Chae and Nenkova(2009)We test the usefulness of a suite of 38 shallowsyntactic features studied by Chae and Nenkova(2009).
These features are weakly but signif-icantly correlated with the fluency of machinetranslated sentences.
These include sentencelength, number of fragments, average lengths ofthe different types of syntactic phrases, total lengthof modifiers in noun phrases, and various othersyntactic features.
We expect that these structuralfeatures will be better at detecting ungrammaticalsentences than the local language model features.Since all of these features are calculated over in-dividual sentences, we use the average value overall the sentences in a summary in our experiments.3.7 Coh-Metrix: Graesser et al (2004)The Coh-Metrix tool5 provides an implementationof 54 features known in the psycholinguistic lit-erature to correlate with the coherence of human-written texts (Graesser et al, 2004).
These includecommonly used readability metrics based on sen-tence length and number of syllables in constituentwords.
Other measures implemented in the sys-tem are surface text properties known to contributeto text processing difficulty.
Also included aremeasures of cohesion between adjacent sentencessuch as similarity under a latent semantic analysis(LSA) model (Deerwester et al, 1990), stem andcontent word overlap, syntactic similarity betweenadjacent sentences, and use of discourse connec-tives.
Coh-Metrix has been designed with thegoal of capturing properties of coherent text andhas been used for grade level assessment, predict-ing student essay grades, and various other tasks.Given the heterogeneity of features in this class,we expect that they will provide reasonable accu-racies for all the linguistic quality measures.
Inparticular, the overlap features might serve as ameasure of redundancy and local coherence.5http://cohmetrix.memphis.edu/5473.8 Word coherence: Soricut and Marcu(2006)Word co-occurrence patterns across adjacent sen-tences provide a way of measuring local coherencethat is not linguistically informed but which canbe easily computed using large amounts of unan-notated text (Lapata, 2003; Soricut and Marcu,2006).
Word coherence can be considered as theanalog of language models at the inter-sentencelevel.
Specifically, we used the two features in-troduced by Soricut and Marcu (2006).Soricut and Marcu (2006) make an analogy tomachine translation: two words are likely to betranslations of each other if they often appear inparallel sentences; in texts, two words are likely tosignal local coherence if they often appear in ad-jacent sentences.
The two features we computedare forward likelihood, the likelihood of observ-ing the words in sentence si conditioned on si?1,and backward likelihood, the likelihood of observ-ing the words in sentence si conditioned on sen-tence si+1.
?Parallel texts?
of 5 million adjacentsentences were extracted from the NYT section ofGigaWord.
We used the GIZA++6 implementa-tion of IBM Model 1 to align the words in adjacentsentences and obtain all relevant probabilities.3.9 Entity coherence: Barzilay and Lapata(2008)Linguistic theories, and Centering theory (Groszet al, 1995) in particular, have hypothesized thatthe properties of the transition of attention fromentities in one sentence to those in the next, play amajor role in the determination of local coherence.Barzilay and Lapata (2008), inspired by Center-ing, proposed a method to compute the local co-herence of texts on the basis of the sequences ofentity mentions appearing in them.In their Entity Grid model, a text is representedby a matrix with rows corresponding to each sen-tence in a text, and columns to each entity men-tioned anywhere in the text.
The value of a cellin the grid is the entity?s grammatical role in thatsentence (Subject, Object, Neither, or Absent).
Anentity transition is a particular entity?s role in twoadjacent sentences.
The actual entity coherencefeatures are the fraction of each type of these tran-sitions in the entire entity grid for the text.
Onewould expect that coherent texts would containa certain distribution of entity transitions which6http://www.fjoch.com/GIZA++.htmlwould differ from those in incoherent sequences.We use the Brown Coherence Toolkit7 (Elsneret al, 2007) to construct the grids.
The tool doesnot perform full coreference resolution.
Instead,noun phrases are considered to refer to the sameentity if their heads are identical.Entity coherence features are the only ones thathave been previously applied with success for pre-dicting summary coherence.
They can thereforebe considered to be the state-of-the-art approachfor automatic evaluation of linguistic quality.4 Summarization dataFor our experiments, we use data from themulti-document summarization tasks of the Doc-ument Understanding Conference (DUC) work-shops (Over et al, 2007).Our training and development data comes fromDUC 2006 and our test data from DUC 2007.These were the most recent years in which thesummaries were evaluated according to specificlinguistic quality questions.
Each input consistsof a set of 25 related documents on a topic and thetarget length of summaries is 250 words.In DUC 2006, there were 50 inputs to be sum-marized and 35 summarization systems which par-ticipated in the evaluation.
This included 34 au-tomatic systems submitted by participants, and abaseline system that simply extracted the lead-ing sentences from the most recent article.
InDUC 2007, there were 45 inputs and 32 differentsummarization systems.
Apart from the leadingsentences baseline, a high performance automaticsummarizer from a previous year was also usedas a baseline.
All these automatic systems are in-cluded in our evaluation experiments.4.1 System performance on linguistic qualityEach summary was evaluated according to thefive linguistic quality questions introduced in Sec-tion 2: grammaticality, non-redundancy, referen-tial clarity, focus, and structure.
For each of thesequestions, all summaries were manually rated on ascale from 1 to 5, in which 5 is the best.The distributions of system scores in the 2006data are shown in Figure 1.
Systems are currentlythe worst at structure, middling at referential clar-ity, and relatively better at grammaticality, focus,7http://www.cs.brown.edu/?melsner/manual.html548Figure 1: Distribution of system scores on the fivelinguistic quality questionsGram Non-redun Ref Focus StructContent .02 -.40 * .29 .28 .09Gram .38 * .25 .24 .54 *Non-redun -.07 -.09 .27Ref .89 * .76 *Focus .80 *Table 1: Spearman correlations between the man-ual ratings for systems averaged over the 50 inputsin 2006; * p < .05and non-redundancy.
Structure is the aspect of lin-guistic quality where there is the most room forimprovement.
The only system with an averagestructure score above 3.5 in DUC 2006 was theleading sentences baseline system.As can be expected, people are unlikely to beable to focus on a single aspect of linguistic qualityexclusively while ignoring the rest.
Some of thelinguistic quality ratings are significantly corre-lated with each other, particularly referential clar-ity, focus, and structure (Table 1).More importantly, the systems that producesummaries with good content8 are not necessar-ily the systems producing the most readable sum-maries.
Notice from the first row of Table 1 thatnone of the system rankings based on these mea-sures of linguistic quality are significantly posi-tively correlated with system rankings of content.The development of automatic linguistic qualitymeasurements will allow researchers to optimizeboth content and linguistic quality.8as measured by summary responsiveness ratings on a 1to 5 scale, without regard to linguistic quality5 Experimental setupWe use the summaries from DUC 2006 for train-ing and feature development and DUC 2007served as the test set.
Validating the results on con-secutive years of evaluation is important, as resultsthat hold for the data in one year might not carryover to the next, as happened for example in Con-roy and Dang (2008)?s work.Following Barzilay and Lapata (2008), we re-port summary ranking accuracy as the fraction ofcorrect pairwise rankings in the test set.We use a Ranking SVM (SV M light (Joachims,2002)) to score summaries using our features.
TheRanking SVM seeks to minimize the number ofdiscordant pairs (pairs in which the gold stan-dard has x1 ranked strictly higher than x2, but thelearner ranks x2 strictly higher than x1).
The out-put of the ranker is always a real valued score, so aglobal rank order is always obtained.
The defaultregularization parameter was used.5.1 Combining predictionsTo combine information from the different featureclasses, we train a meta ranker using the predic-tions from each class as features.First, we use a leave-one out (jackknife) pro-cedure to get the predictions of our features forthe entire 2006 data set.
To predict rankings ofsystems on one input, we train all the individualrankers, one for each of the classes of features in-troduced above, on data from the remaining in-puts.
We then apply these rankers to the sum-maries produced for the held-out input.
By repeat-ing this process for each input in turn, we obtainthe predicted scores for each summary.Once this is done, we use these predicted scoresas features for the meta ranker, which is trained onall 2006 data.
To test on a new summary pair in2007, we first apply each individual ranker to getits predictions, and then apply the meta ranker.In either case (meta ranker or individual featureclass), all training is performed on 2006 data, andall testing is done on 2007 data which guaranteesthe results generalize well at least from one yearof evaluation to the next.5.2 Evaluation of rankingsWe examine the predictive power of our featuresfor each of the five linguistic quality questions intwo settings.
In system-level evaluation, we wouldlike to rank all participating systems according to549their performance on the entire test set.
In input-level evaluation, we would like to rank all sum-maries produced for a single given input.For input-level evaluation, the pairs are formedfrom summaries of the same input.
Pairs in whichthe gold standard ratings are tied are not included.After removing the ties, the test set consists of 13Kto 16K pairs for each linguistic quality question.Note that there were 45 inputs and 32 automaticsystems in DUC 2007.
So, there are a total of45?
(322)= 22, 320 possible summary pairs.For system-level evaluation, we treat the real-valued output of the SVM ranker for each sum-mary as the linguistic quality score.
The 45 indi-vidual scores for summaries produced by a givensystem are averaged to obtain an overall score forthe system.
The gold-standard system-level qual-ity rating is equal to the average human ratings forthe system?s summaries over the 45 inputs.
At thesystem level, there are about 500 non-tied pairs inthe test set for each question.For both evaluation settings, a random baselinewhich ranked the summaries in a random orderwould have an expected pairwise accuracy of 50%.6 Results and discussion6.1 System-level evaluationSystem-level accuracies for each class of featuresare shown in Table 2.
All classes of features per-form well, with at least a 20% absolute increasein accuracy over the random baseline (50% ac-curacy).
For each of the linguistic quality ques-tions, the corresponding best class of featuresgives prediction accuracies around 90%.
In otherwords, if these features were used to fully auto-matically compare systems that participated in the2007 DUC evaluation, only one out of ten com-parisons would have been incorrect.
These resultsset a high standard for future work on automaticsystem-level evaluation of linguistic quality.The state-of-the-art entity coherence featuresperform well but are not the best for any of the fiveaspects of linguistic quality.
As expected, sentencefluency is the best feature class for grammatical-ity.
For all four other questions, the best featureset is Continuity, which is a combination of sum-marization specific features, coreference featuresand cosine similarity of adjacent sentences.
Conti-nuity features outperform entity coherence by 3 to4% absolute difference on referential quality, fo-cus, and coherence.
Accuracies from the languageFeature set Gram.
Redun.
Ref.
Focus Struct.Lang.
models 87.6 83.0 91.2 85.2 86.3Named ent.
78.5 83.6 82.1 74.0 69.6NP syntax 85.0 83.8 87.0 76.6 79.2Coh.
devices 82.1 79.5 82.7 82.3 83.7Continuity 88.8 88.5 92.9 89.2 91.4Sent.
fluency 91.7 78.9 87.6 82.3 84.9Coh-Metrix 87.2 86.0 88.6 83.9 86.3Word coh.
81.7 76.0 87.8 81.7 79.0Entity coh.
90.2 88.1 89.6 85.0 87.1Meta ranker 92.9 87.9 91.9 87.8 90.0Table 2: System-level prediction accuracies (%)model features are within 1% of entity coherencefor these three aspects of summary quality.Coh-Metrix, which has been proposed as a com-prehensive characterization of text, does not per-form as well as the language model and the en-tity coherence classes, which contain considerablyfewer features related to only one aspect of text.The classes of features specific to named enti-ties and noun phrase syntax are the weakest pre-dictors.
It is apparent from the results that conti-nuity, entity coherence, sentence fluency and lan-guage models are the most powerful classes of fea-tures that should be used in automation of evalu-ation and against which novel predictors of textquality should be compared.Combining all feature classes with the metaranker only yields higher results for grammatical-ity.
For the other aspects of linguistic quality, it isbetter to use Continuity by itself to rank systems.One certainly unexpected result is that featuresdesigned to capture one aspect of well-written textturn out to perform well for other questions aswell.
For instance, entity coherence and continuityfeatures predict grammaticality with very high ac-curacy of around 90%, and are surpassed only bythe sentence fluency features.
These findings war-rant further investigation because we would notexpect characteristics of local transitions indica-tive of text structure to have anything to do withsentence grammaticality or fluency.
The resultsare probably due to the significant correlation be-tween structure and grammaticality (Table 1).6.2 Input-level evaluationThe results of the input-level ranking experimentsare shown in Table 3.
Understandably, input-level prediction is more difficult and the results arelower compared to the system-level predictions:even with wrong predictions for some of the sum-maries by two systems, the overall judgment that550one system is better than the other over the entiretest set can still be accurate.While for system-level predictions the metaranker was only useful for grammaticality, at theinput level it outperforms every individual featureclass for each of the five questions, obtaining ac-curacies around 70%.These input-level accuracies compare favorablywith automatic evaluation metrics for other nat-ural language processing tasks.
For example, atthe 2008 ACL Workshop on Statistical MachineTranslation, all fifteen automatic evaluation met-rics, including variants of BLEU scores, achievedbetween 42% and 56% pairwise accuracy with hu-man judgments at the sentence level (Callison-Burch et al, 2008).As in system-level prediction, for referentialclarity, focus, and structure, the best feature classis Continuity.
Sentence fluency again is the bestclass for identifying grammaticality.Coh-Metrix features are now best for determin-ing redundancy.
Both Coh-Metrix and Continuity(the top two features for redundancy) include over-lap measures between adjacent sentences, whichserve as a good proxy for redundancy.Surprisingly, the relative performance of thefeature classes at input level is not the same asfor system-level prediction.
For example, the lan-guage model features, which are the second bestclass for the system-level, do not fare as well atthe input-level.
Word co-occurrence which ob-tained good accuracies at the system level is theleast useful class at the input level with accuraciesjust above chance in all cases.6.3 Components of continuityThe class of features capturing sentence-to-sentence continuity in the summary (Section 3.5)are the most effective for predicting referentialclarity, focus, and structure at the input level.We now investigate to what extent each of itscomponents?summary-specific features, corefer-ence, and cosine similarity between adjacentsentences?contribute to performance.Results obtained after excluding each of thecomponents of continuity is shown in Table 4;each line in the table represents Continuity mi-nus a feature subclass.
Removing cosine over-lap causes the largest drop in prediction accuracy,with results about 10% lower than those for thecomplete Continuity class.
Summary specific fea-Feature set Gram.
Redun.
Ref.
Focus Struct.Lang.
models 66.3 57.6 62.2 60.5 62.5Named ent.
52.9 54.4 60.0 54.1 52.5NP Syntax 59.0 50.8 59.1 54.5 55.1Coh.
devices 56.8 54.4 55.2 52.7 53.6Continuity 61.7 62.5 69.7 65.4 70.4Sent.
fluency 69.4 52.5 64.4 61.9 62.6Coh-Metrix 65.5 67.6 67.9 63.0 62.4Word coh.
54.7 55.5 53.3 53.2 53.7Entity coh.
61.3 62.0 64.3 64.2 63.6Meta ranker 71.0 68.6 73.1 67.4 70.7Table 3: Input-level prediction accuracies (%)tures, which compare the context of a sentencein the summary with the context in the originaldocument where it appeared, also contribute sub-stantially to the success of the Continuity class inpredicting structure and referential clarity.
Accu-racies drop by about 7% when these features areexcluded.
However, the coreference features donot seem to contribute much towards predictingsummary linguistic quality.
The accuracies of theContinuity class are not affected at all when thesecoreference features are not included.6.4 Impact of summarization methodsIn this paper, we have discussed an analysis of theoutputs of current research systems.
Almost allof these systems still use extractive methods.
Thesummarization specific continuity features rewardsystems that include the necessary preceding con-text from the original document.
These featureshave high prediction accuracies (Section 6.3) oflinguistic quality, however note that the support-ing context could often contain less important con-tent.
Therefore, there is a tension between strate-gies for optimizing linguistic quality and for op-timizing content, which warrants the developmentof abstractive methods.As the field moves towards more abstractivesummaries, we expect to see differences in botha) summary linguistic quality and b) the featurespredictive of linguistic aspects.As discussed in Section 4.1, systems are cur-rently worst at structure/coherence.
However,grammaticality will become more of an issue assystems use sentence compression (Knight andMarcu, 2002), reference rewriting (Nenkova andMcKeown, 2003), and other techniques to producetheir own sentences.The number of discourse connectives is cur-rently significantly negatively correlated withstructure/coherence (Spearman correlation of r =551Ref.
Focus Struct.Continuity 69.7 65.4 70.4- Sum-specific 63.9 64.2 63.5- Coref 70.1 65.2 70.6- Cosine 60.2 56.6 60.7Table 4: Ablation within the Continuity class;pairwise accuracy for input-level predictions (%)-.06, p = .008 on DUC 2006 system summaries).This can be explained by the fact that they of-ten lack proper context in an extractive summary.However, an abstractive system could plan a dis-course structure and insert appropriate connectives(Saggion, 2009).
In this case, we would expect thepresence of discourse connectives to be a mark ofa well-written summary.6.5 Results on human-written abstractsSince abstractive summaries would have markedlydifferent properties from extracts, it would be in-teresting to know how well these sets of featureswould work for predicting the quality of machine-produced abstracts.
However, since current sys-tems are extractive, such a data set is not available.Therefore we experiment on human-written ab-stracts to get an estimate of the expected per-formance of our features on abstractive systemsummaries.
In both DUC 2006 and DUC 2007,ten NIST assessors wrote summaries for the var-ious inputs.
There are four human-written sum-maries for each input and these summaries werejudged on the same five linguistic quality aspectsas the machine-written summaries.
We train on thehuman-written summaries from DUC 2006 andtest on the human-written summaries from DUC2007, using the same set-up as in Section 5.These results are shown in Table 5.
We only re-port results on the input level, as we are interestedin distinguishing between the quality of the sum-maries, not the NIST assessors?
writing skills.Except for grammaticality, the prediction accu-racies of the best feature classes for human ab-stracts are better than those at input level for ma-chine extracts.
This result is promising, as it showsthat similar features for evaluating linguistic qual-ity will be valid for abstractive summaries as well.Note however that the relative performance ofthe feature sets changes between the machine andhuman results.
While for the machines Continu-ity feature class is the best predictor of referentialclarity, focus, and structure (Table 3), for humans,language models and sentence fluency are best forFeature set Gram.
Redun.
Ref.
Focus Struct.Lang.
models 52.1 60.8 76.5 71.9 78.4Named ent.
62.5 66.7 47.1 43.9 59.1NP Syntax 64.6 49.0 43.1 49.1 58.0Coh.
devices 54.2 68.6 66.7 49.1 64.8Continuity 54.2 49.0 62.7 61.4 71.6Sent.
fluency 54.2 64.7 80.4 71.9 72.7Coh-Metrix 54.2 52.9 68.6 56.1 69.3Word coh.
62.5 58.8 62.7 70.2 60.2Entity coh.
45.8 49.0 54.9 52.6 56.8Meta ranker 62.5 56.9 80.4 50.9 67.0Table 5: Input-level prediction accuracies forhuman-written summaries (%)these three aspects of linguistic quality.
A possi-ble explanation for this difference could be that insystem-produced extracts, incoherent organizationinfluences human perception of linguistic qualityto a great extent and so local coherence featuresturned out very predictive.
But in human sum-maries, sentences are clearly well-organized andhere, continuity features appear less useful.
Sen-tence level fluency seems to be more predictive ofthe linguistic quality of these summaries.7 ConclusionWe have presented an analysis of a wide varietyof features for the linguistic quality of summaries.Continuity between adjacent sentences was con-sistently indicative of the quality of machine gen-erated summaries.
Sentence fluency was useful foridentifying grammaticality.
Language model andentity coherence features also performed well andshould be considered in future endeavors for auto-matic linguistic quality evaluation.The high prediction accuracies for input-levelevaluation and the even higher accuracies forsystem-level evaluation confirm that questions re-garding the linguistic quality of summaries can beanswered reasonably using existing computationaltechniques.
Automatic evaluation will make test-ing easier during system development and enablereporting results obtained outside of the cycles ofNIST evaluation.AcknowledgmentsThis material is based upon work supported undera National Science Foundation Graduate ResearchFellowship and NSF CAREER award 0953445.We would like to thank Bonnie Webber for pro-ductive discussions.552ReferencesR.
Barzilay and M. Lapata.
2008.
Modeling local co-herence: An entity-based approach.
ComputationalLinguistics, 34(1):1?34.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2008.
Further meta-evaluation of ma-chine translation.
In Proceedings of the Third Work-shop on Statistical Machine Translation, pages 70?106.J.
Chae and A. Nenkova.
2009.
Predicting the fluencyof text with shallow structural features: case studiesof machine translation and human-written text.
InProceedings of EACL, pages 139?147.E.
Charniak and M. Elsner.
2009.
EM works for pro-noun anaphora resolution.
In Proceedings of EACL,pages 148?156.J.M.
Conroy and H.T.
Dang.
2008.
Mind the gap: dan-gers of divorcing evaluations of summary contentfrom linguistic quality.
In Proceedings of COLING,pages 145?152.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Lan-dauer, and R. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Societyfor Information Science, 41:391?407.M.
Elsner and E. Charniak.
2008.
Coreference-inspired coherence modeling.
In Proceedings ofACL/HLT: Short Papers, pages 41?44.M.
Elsner, J. Austerweil, and E. Charniak.
2007.
Aunified local and global model for discourse coher-ence.
In Proceedings of NAACL/HLT.J.R.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by gibbs sampling.
In Proceed-ings of ACL, pages 363?370.K.
Fraurud.
1990.
Definiteness and the processing ofnoun phrases in natural discourse.
Journal of Se-mantics, 7(4):395.A.C.
Graesser, D.S.
McNamara, M.M.
Louwerse, andZ.
Cai.
2004.
Coh-Metrix: Analysis of text on co-hesion and language.
Behavior Research MethodsInstruments and Computers, 36(2):193?202.B.
Grosz, A. Joshi, and S. Weinstein.
1995.
Centering:a framework for modelling the local coherence ofdiscourse.
Computational Linguistics, 21(2):203?226.K.F.
Haberlandt and A.C. Graesser.
1985.
Componentprocesses in text comprehension and some of theirinteractions.
Journal of Experimental Psychology:General, 114(3):357?374.M.A.K.
Halliday and R. Hasan.
1976.
Cohesion inEnglish.
Longman Group Ltd, London, U.K.T.
Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 133?142.M.A.
Just and P.A.
Carpenter.
1987.
The psychologyof reading and language comprehension.
Allyn andBacon Boston, MA.D.
Klein and C.D.
Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL, pages 423?430.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction: A probabilistic approachto sentence compression.
Artificial Intelligence,139(1):91?107.M.
Lapata and R. Barzilay.
2005.
Automatic evalua-tion of text coherence: Models and representations.In International Joint Conference On Artificial In-telligence, volume 19, page 1085.M.
Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedingsof ACL, pages 545?552.C.Y.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of NAACL/HLT, page 78.C.Y.
Lin.
2004.
Rouge: A package for automatic eval-uation of summaries.
In Proceedings of the Work-shop on Text Summarization Branches Out (WAS2004), pages 25?26.A.
Nenkova and K. McKeown.
2003.
References tonamed entities: a corpus study.
In Proceedings ofHLT/NAACL 2003 (short paper).J.
Otterbacher, D. Radev, and A. Luo.
2002.
Revi-sions that improve cohesion in multi-document sum-maries: a preliminary study.
In Proceedings of theWorkshop on Automatic Summarization, ACL.P.
Over, H. Dang, and D. Harman.
2007.
Ducin context.
Information Processing Management,43(6):1506?1520.C.D.
Paice.
1980.
The automatic generation of litera-ture abstracts: an approach based on the identifica-tion of self-indicating phrases.
In Proceedings of the3rd annual ACM conference on Research and devel-opment in information retrieval, pages 172?191.C.D.
Paice.
1990.
Constructing literature abstracts bycomputer: Techniques and prospects.
InformationProcessing Management, 26(1):171?186.E.F.
Prince.
1981.
Toward a taxonomy of given-newinformation.
Radical pragmatics, 223:255.H.
Saggion.
2009.
A Classification Algorithm for Pre-dicting the Structure of Summaries.
Proceedingsof the 2009 Workshop on Language Generation andSummarisation, page 31.553R.
Soricut and D. Marcu.
2006.
Discourse generationusing utility-trained coherence models.
In Proceed-ings of ACL.J.
Steinberger, M. Poesio, M.A.
Kabadjov, and K. Jeek.2007.
Two uses of anaphora resolution in sum-marization.
Information Processing Management,43(6):1663?1680.A.
Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Seventh International Confer-ence on Spoken Language Processing, volume 3.554
