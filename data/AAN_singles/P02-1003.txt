Generation as Dependency ParsingAlexander Koller and Kristina StriegnitzDept.
of Computational Linguistics, Saarland University{koller|kris}@coli.uni-sb.deAbstractNatural-Language Generation from flatsemantics is an NP-complete problem.This makes it necessary to develop al-gorithms that run with reasonable effi-ciency in practice despite the high worst-case complexity.
We show how to con-vert TAG generation problems into de-pendency parsing problems, which is use-ful because optimizations in recent de-pendency parsers based on constraint pro-gramming tackle exactly the combina-torics that make generation hard.
Indeed,initial experiments display promising run-times.1 IntroductionExisting algorithms for realization from a flat inputsemantics all have runtimes which are exponential inthe worst case.
Several different approaches to im-proving the runtime in practice have been suggestedin the literature ?
e.g.
heuristics (Brew, 1992) andfactorizations into smaller exponential subproblems(Kay, 1996; Carroll et al, 1999).
While these solu-tions achieve some measure of success in making re-alization efficient, the contrast in efficiency to pars-ing is striking both in theory and in practice.The problematic runtimes of generation algo-rithms are explained by the fact that realization is anNP-complete problem even using just context-freegrammars, as Brew (1992) showed in the context ofshake-and-bake generation.
The first contribution ofour paper is a proof of a stronger NP-completenessresult: If we allow semantic indices in the grammar,realization is NP-complete even if we fix a singlegrammar.
Our alternative proof shows clearly thatthe combinatorics in generation come from essen-tially the same sources as in parsing for free wordorder languages.
It has been noted in the literaturethat this problem, too, becomes NP-complete veryeasily (Barton et al, 1987).The main point of this paper is to show how toencode generation with a variant of tree-adjoininggrammars (TAG) as a parsing problem with depen-dency grammars (DG).
The particular variant of DGwe use, Topological Dependency Grammar (TDG)(Duchier, 2002; Duchier and Debusmann, 2001),was developed specifically with efficient parsing forfree word order languages in mind.
The mere exis-tence of this encoding proves TDG?s parsing prob-lem NP-complete as well, a result which has beenconjectured but never formally shown so far.
But itturns out that the complexities that arise in gener-ation problems in practice seem to be precisely ofthe sort that the TDG parser can handle well.
Initialexperiments with generating from the XTAG gram-mar (XTAG Research Group, 2001) suggest that ourgeneration system is competitive with state-of-the-art chart generators, and indeed seems to run in poly-nomial time in practice.Next to the attractive runtime behaviour, our ap-proach to realization is interesting because it mayprovide us with a different angle from which tolook for tractable fragments of the general realiza-tion problem.
As we will show, the computation thattakes place in our system is very different from thatin a chart generator, and may be more efficient insome cases by taking into account global informa-tion to guide local choices.Plan of the Paper.
We will define the problem wewant to tackle in Section 2, and then show that it isNP-complete (Section 3).
In Section 4, we sketchthe dependency grammar formalism we use.
Sec-tion 5 is the heart of the paper: We show how toencode TAG generation as TDG parsing, and dis-cuss some examples and runtimes.
We compare ourapproach to some others in Section 6, and concludeand discuss future research in Section 7.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
17-24.Proceedings of the 40th Annual Meeting of the Association for2 The Realization ProblemIn this paper, we deal with the subtask of naturallanguage generation known as surface realization:given a grammar and a semantic representation, theproblem is to find a sentence which is grammaticalaccording to the grammar and expresses the contentof the semantic representation.We represent the semantic input as a multiset(bag) of ground atoms of predicate logic, suchas {buy(e,a,b), name(a,mary) car(b)}.
To en-code syntactic information, we use a tree-adjoininggrammar without feature structures (Joshi and Sch-abes, 1997).
Following Stone and Doran (1997) andKay (1996), we enhance this TAG grammar witha syntax-semantics interface in which nonterminalnodes of the elementary trees are equipped with in-dex variables, which can be bound to individuals inthe semantic input.
We assume that the root node,all substitution nodes, and all nodes that admit ad-junction carry such index variables.
We also assigna semantics to every elementary tree, so that lexi-cal entries are pairs of the form (?, T ), where ?
isa multiset of semantic atoms, and T is an initial orauxiliary tree, e.g.
( {buy(x,y,z)},S:xNP:y  VP:xV:xbuysNP:z )When the lexicon is accessed, x, y, z get boundto terms occurring in the semantic input, e.g.
e, a, bin our example.
Since we furthermore assume thatevery index variable that appears in T also appearsin ?, this means that all indices occurring in T getbound at this stage.The semantics of a complex tree is the multisetunion of the semantics of the elementary trees in-volved.
Now we say that the realization problem ofa grammar G is to decide for a given input semanticsS and an index i whether there is a derivation treewhich is grammatical according to G, is assignedthe semantics S, and has a root node with index i.3 NP-Completeness of RealizationThis definition is the simplest conceivable formal-ization of problems occurring in surface realizationas a decision problem: It does not even require usto compute a single actual realization, just to check?1 B:iN:i  E:keB:k sem: edge(i,k)?2 Ceating C sem: edge(i,k)?3 N:insem: node(i)?4 B:1eat C sem: start-eating?5 Catesem: end-eatingFigure 1: The grammar Gham.whether one exists.
Every practical generation sys-tem generating from flat semantics will have to ad-dress this problem in one form or another.Now we show that this problem is NP-complete.A similar result was proved in the context of shake-and-bake generation by Brew (1992), but he neededto use the grammar in his encoding, which leavesthe possibility open that for every single grammarG, there might be a realization algorithm tailoredspecifically to G which still runs in polynomial time.Our result is stronger in that we define a singlegrammar Ghamwhose realization problem is NP-complete in the above sense.
Furthermore, we findthat our proof brings out the sources of the complex-ity more clearly.
Ghamdoes not permit adjunction,hence the result also holds for context-free gram-mars with indices.1 2 3It is clear that the problem is inNP: We can simply guess the ele-mentary trees we need and how tocombine them, and then check inpolynomial time whether they verbalize the seman-tics.
The NP-hardness proof is by reducing the well-known HAMILTONIAN-PATH problem to the realiza-tion problem.
HAMILTONIAN-PATH is the problemof deciding whether a directed graph has a cycle thatvisits each node exactly once, e.g.
(1,3,2,1) in thegraph shown above.We will now construct an LTAG grammar Ghamsuch that every graph G = (V,E) can be encodedas a semantic input S for the realization problem ofGham, which can be verbalized if and only if G hasa Hamiltonian cycle.
S is defined as follows:S = {node(i) | i ?
V }?
{edge(i, k) | (i, k) ?
E}?
{start-eating, end-eating}.B:1N:1 N:1nE:3eB:3 B:3N:3 N:3nE:2eB:2 B:2N:2 N:2nE:1eB:1 B:1eat C Ceating C CateFigure 2: A derivation with Ghamcorresponding toa Hamiltonian cycle.The grammar Ghamis given in Fig.
1; the startsymbol is B, and we want the root to have index 1.The tree ?1models an edge transition from node ito the node k by consuming the semantic encodingsof this edge and (by way of a substitution of ?3) ofthe node i.
The second substitution node of ?1canbe filled either by another ?1, in which way a paththrough the graph is modelled, or by an ?4, in whichcase we switch to an ?edge eating mode?.
In thismode, we can arbitrarily consume edges using ?2,and close the tree with ?5when we?re done.
Thisis illustrated in Fig.
2, the tree corresponding to thecycle in the example graph above.The Hamiltonian cycle of the graph, if one exists,is represented in the indices of the B nodes.
The listof these indices is a path in the graph, as the ?1treesmodel edge transitions; it is a cycle because it startsin 1 and ends in 1; and it visits each node exactlyonce, for we use exactly one ?1tree for each nodeliteral.
The edges which weren?t used in the cyclecan be consumed in the edge eating mode.The main source for the combinatorics of the re-alization problem is thus the interaction of lexicalambiguity and the completely free order in the flatsemantics.
Once we have chosen between ?1and ?2in the realization of each edge literal, we have deter-mined which edges should be part of the prospectiveHamiltonian cycle, and checking whether it reallyis one can be done in linear time.
If, on the otherhand, the order of the input placed restrictions onthe structure of the derivation tree, we would againhave information that told us when to switch into theedge eating mode, i.e.
which edges should be partpeter likes marysubj objFigure 3: TDG parse tree for ?Peter likes Mary.
?of the cycle.
A third source of combinatorics whichdoes not become so clear in this encoding is the con-figuration of the elementary trees.
Even when wehave committed to the lexical entries, it is conceiv-able that only one particular way of plugging theminto each other is grammatical.4 Topological Dependency GrammarThese factors are exactly the same that make depen-dency parsing for free word order languages diffi-cult, and it seems worthwhile to see whether op-timized parsers for dependency grammars can alsocontribute to making generation efficient.
We nowsketch a dependency formalism which has an effi-cient parser and then discuss some of the importantproperties of this parser.
In the next section, we willsee how to employ the parser for generation.4.1 The Grammar FormalismThe parse trees of topological dependency grammar(TDG) (Duchier and Debusmann, 2001; Duchier,2002) are trees whose nodes correspond one-to-oneto the words of the sentence, and whose edges are la-belled, e.g.
with syntactic relations (see Fig.
3).
Thetrees are unordered, i.e.
there is no intrinsic orderamong the children of a node.
Word order in TDGis initially completely free, but there is a separatemechanism to specify constraints on linear prece-dence.
Since completely free order is what we wantfor the realization problem, we do not need thesemechanisms and do not go into them here.The lexicon assigns to each word a set of lexicalentries; in a parse tree, one of these lexical entrieshas to be picked for each node.
The lexical entryspecifies what labels are allowed on the incomingedge (the node?s labels) and the outgoing edges (thenode?s valency).
Here are some examples:word labels valencylikes ?
{subj, obj, adv?
}Peter {subj, obj} ?Mary {subj, obj} ?The lexical entry for ?likes?
specifies that the corre-sponding node does not accept any incoming edges(and hence must be the root), must have preciselyone subject and one object edge going out, and canhave arbitrarily many outgoing edges with label adv(indicated by ?).
The nodes for ?Peter?
and ?Mary?both require their incoming edge to be labelled witheither subj or obj and neither require nor allow anyoutgoing edges.A well-formed dependency tree for an input sen-tence is simply a tree with the appropriate nodes,whose edges obey the labels and valency restric-tions specified by the lexical entries.
So, the tree inFig.
3 is well-formed according to our lexicon.4.2 TDG ParsingThe parsing problem of TDG can be seen as a searchproblem: For each node, we must choose a lexi-cal entry and the correct mother-daughter relations itparticipates in.
One strength of the TDG approach isthat it is amenable to strong syntactic inferences thattackle specifically the three sources of complexitymentioned above.The parsing algorithm (Duchier, 2002) is statedin the framework of constraint programming (Kollerand Niehren, 2000), a general approach to copingwith combinatorial problems.
Before it explores allchoices that are possible in a certain state of thesearch tree (distribution), it first tries to eliminatesome of the choices which definitely cannot lead to asolution by simple inferences (propagations).
?Sim-ple?
means that propagations take only polynomialtime; the combinatorics is in the distribution stepsalone.
That is, it can still happen that a search treeof exponential size has to be explored, but the timespent on propagation in each of its node is only poly-nomial.
Strong propagation can reduce the size ofthe search tree, and it may even make the whole al-gorithm run in polynomial time in practice.The TDG parser translates the parsing prob-lem into constraints over (variables denoting) fi-nite sets of integers, as implemented efficiently inthe Mozart programming system (Oz DevelopmentTeam, 1999).
This translation is complete: Solutionsof the set constraint can be translated back to cor-rect dependency trees.
But for efficiency, the parseruses additional propagators tailored to the specificinferences of the dependency problem.
For instance,in the ?Peter likes Mary?
example above, one suchpropagator could contribute the information that nei-ther the ?Peter?
nor the ?Mary?
node can be an advchild of ?likes?, because neither can accept an advedge.
Once the choice has been made that ?Peter?
isthe subj child of ?likes?, a propagator can contributethat ?Mary?
must be its obj child, as it is the onlypossible candidate for the (obligatory) obj child.Finally, lexical ambiguity is handled by selectionconstraints.
These constraints restrict which lexicalentry should be picked for a node.
When all pos-sible lexical entries have some information in com-mon (e.g., that there must be an outgoing subj edge),this information is automatically lifted to the nodeand can be used by the other propagators.
Thus itis sometimes even possible to finish parsing withoutcommitting to single lexical entries for some nodes.5 Generation as Dependency ParsingWe will now show how TDG parsing can be used toenumerate all sentences expressing a given input se-mantics, thereby solving the realization problem in-troduced in Section 2.
We first define the encoding.Then we give an example and discuss some runtimeresults.
Finally, we consider a particular restrictionof our encoding and ways of overcoming it.5.1 The EncodingLet G be a grammar as described in Section 2;i.e.
lexical entries are of the form (?, T ), where?
is a flat semantics and T is a TAG elementarytree whose nodes are decorated with semantic in-dices.
We make the following simplifying assump-tions.
First, we assume that the nodes of the elemen-tary trees of G are not labelled with feature struc-tures.
Next, we assume that whenever we can adjoinan auxiliary tree at a node, we can adjoin arbitrarilymany trees at this node.
The idea of multiple adjunc-tion is not new (Schabes and Shieber, 1994), but itis simplified here because we disregard complex ad-junction constraints.
We will discuss these two re-strictions in the conclusion.
Finally, we assume thatevery lexical semantics ?
has precisely one member;this restriction will be lifted in Section 5.4.Now let?s say we want to find the realizations ofthe input semantics S = {?1, .
.
.
, ?n}, using thegrammar G. The input ?sentence?
of the parsingstart mary buy car indef redsubst NP,m,1substS,e,1substN,c,1substNP,c,1adjN,cFigure 4: Dependency tree for ?Mary buys a redcar.
?problem we construct is the sequence {start} ?
S,where start is a special start symbol.
The parsetree will correspond very closely to a TAG deriva-tion tree, its nodes standing for the instantiated ele-mentary trees that are used in the derivation.To this end, we use two types of edge labels ?substitution and adjunction labels.
An edge with asubstitution label substA,i,pfrom the node ?
to thenode ?
(both of which stand for elementary trees)indicates that ?
should be plugged into the p-th sub-stitution node in ?
that has label A and index i. Wewrite subst(A) for the maximum number of occur-rences of A as the label of substitution nodes in anyelementary tree of G; this is the maximum value thatp can take.An edge with an adjunction label adjA,ifrom ?
to?
specifies that ?
is adjoined at some node within ?carrying label A and index i and admitting adjunc-tion.
It does not matter for our purposes to whichnode in ?
?
is adjoined exactly; the choice cannot af-fect grammaticality because there is no feature uni-fication involved.The dependency grammar encodes how an ele-mentary tree can be used in a TAG derivation byrestricting the labels of the incoming and outgoingedges via labels and valency requirements in the lex-icon.
Let?s say that T is an elementary tree of Gwhich has been matched with the input atom ?r, in-stantiating its index variables.
Let A be the labeland i the index of the root of T .
If T is an auxiliarytree, it accepts incoming adjunction edges for A andi, i.e.
it gets the labels value {adjA,i}.
If T is aninitial tree, it will accept arbitrary incoming substi-tution edges for A and i, i.e.
its labels value is{substA,i,p| 1 ?
p ?
subst(A)}In either case, T will require precisely one out-going substitution edge for each of its substitutionnodes, and it will allow arbitrary numbers of outgo-ing adjunction edges for each node where we canadjoin.
That is, the valency value is as follows:{substA,i,p| ex.
substitution node N in T s.t.
Ais label, i is index of N , and N ispth substitution node for A:i in T}?
{adjA,i?
| ex.
node with label A, index iin T which admits adjunction}We obtain the set of all lexicon entries for theatom ?rby encoding all TAG lexicon entries whichmatch ?ras just specified.
The start symbol, start,gets a special lexicon entry: Its labels entry is theempty set (i.e.
it must be the root of the tree), and itsvalency entry is the set {substS,k,1}, where k is thesemantic index with which generation should start.5.2 An ExampleNow let us go through an example to make these def-initions a bit clearer.
Let?s say we want to verbalizethe semantics{name(m, mary), buy(e,m, c),car(c), indef(c), red(c)}The LTAG grammar we use contains the elemen-tary trees which are used in the tree in Fig.
5, alongwith the obvious semantics; we want to generate asentence starting with the main event e. The encod-ing produces the following dependency grammar;the entries in the ?atom?
column are to be read asabbreviations of the actual atoms in the input seman-tics.atom labels valencystart ?
{substS,e,1}buy {substS,e,1} {substNP,c,1, substNP,m,1,adjV P,e?, adjV,e?
}mary {substNP,m,1, {adjNP,1?, adjPN,m?
}substNP,m,2}indef {substNP,c,1, {adjNP,c?
}substNP,c,2}car {substN,c,1} {adjN,c?
}red {adjN,c} ?If we parse the ?sentence?start mary buy car indef redwith this grammar, leaving the word order com-pletely open, we obtain precisely one parse tree,shown in Fig.
4.
Reading this parse as a TAGderivation tree, we can reconstruct the derived treein Fig.
5, which indeed produces the string ?Marybuys a red car?.S:eNP:m NP:mPN:mMaryVP:eV:ebuysNP:c NP:cDetnoad jaN:c N:cAdjnoad jredN:cN:ccarFigure 5: Derived tree for ?Mary buys a red car.
?5.3 Implementation and ExperimentsThe overall realization algorithm we propose en-codes the input problem as a DG parsing problemand then runs the parser described in Section 4.2,which is freely available over the Web, as a blackbox.
Because the information lifted to the nodes bythe selection constraints may be strong enough tocompute the parse tree without ever committing tounique lexical entries, the complete parse may stillcontain some lexical ambiguity.
This is no problem,however, because the absence of features guaranteesthat every combination of choices will be grammat-ical.
Similarly, a node can have multiple childrenover adjunction edges with the same label, and theremay be more than one node in the upper elemen-tary tree to which the lower tree could be adjoined.Again, all remaining combinations are guaranteed tobe grammatical.In order to get an idea of the performance ofour realization algorithm in comparison to the stateof the art, we have tried generating the followingsentences, which are examples from (Carroll et al,1999):(1) The manager in that office interviewed a newconsultant from Germany.
(2) Our manager organized an unusual additionalweekly departmental conference.We have converted the XTAG grammar (XTAGResearch Group, 2001) into our grammar format,automatically adding indices to the nodes of the el-ementary trees, removing features, simplifying ad-junction constraints, and adding artificial lexical se-mantics that consists of the words at the lexical an-chors and the indices used in the respective trees.XTAG typically assigns quite a few elementary treesto one lemma, and the same lexical semantics can of-ten be verbalized by more than hundred elementarytrees in the converted grammar.
It turns out that thedependency parser scales very nicely to this degreeof lexical ambiguity: The sentence (1) is generatedin 470 milliseconds (as opposed to Carroll et al?s 1.8seconds), whereas we generate (2) in about 170 mil-liseconds (as opposed to 4.3 seconds).1 Althoughthese numbers are by no means a serious evaluationof our system?s performance, they do present a firstproof of concept for our approach.The most encouraging aspect of these results isthat despite the increased lexical ambiguity, theparser gets by without ever making any wrongchoices, which means that it runs in polynomialtime, on all examples we have tried.
This is possiblebecause on the one hand, the selection constraint au-tomatically compresses the many different elemen-tary trees that XTAG assigns to one lemma into veryfew classes.
On the other hand, the propagation thatrules out impossible edges is so strong that the freeinput order does not make the configuration prob-lem much harder in practice.
Finally, our treatmentof modification allows us to multiply out the possi-ble permutations in a postprocessing step, after theparser has done the hard work.
A particularly strik-ing example is (2), where the parser gives us a singlesolution, which multiplies out to 312 = 13 ?
4!
dif-ferent realizations.
(The 13 basic realizations corre-spond to different syntactic frames for the main verbin the XTAG grammar, e.g.
for topicalized or pas-sive constructions.
)5.4 More Complex SemanticsSo far, we have only considered TAG grammars inwhich each elementary tree is assigned a semanticsthat contains precisely one atom.
However, thereare cases where an elementary tree either has anempty semantics, or a semantics that contains mul-tiple atoms.
The first case can be avoided by ex-ploiting TAG?s extended domain of locality, see e.g.
(Gardent and Thater, 2001).The simplest possible way for dealing with thesecond case is to preprocess the input into several1A newer version of Carroll et al?s system generates (1) in420 milliseconds (Copestake, p.c.).
Our times were measuredon a 700 MHz Pentium-III PC.different parsing problems.
In a first step, we collectall possible instantiations of LTAG lexical entriesmatching subsets of the semantics.
Then we con-struct all partitions of the input semantics in whicheach block in the partition is covered by a lexical en-try, and build a parsing problem in which each blockis one symbol in the input to the parser.This seems to work quite well in practice, as thereare usually not many possible partitions.
In the worstcase, however, this approach produces an exponen-tial number of parsing problems.
Indeed, using avariant of the grammar from Section 3, it is easyto show that the problem of deciding whether thereis a partition whose parsing problem can be solvedis NP-complete as well.
An alternative approach isto push the partitioning process into the parser aswell.
We expect this will not hurt the runtime allthat much, but the exact effect remains to be seen.6 Comparison to Other ApproachesThe perspective on realization that our system takesis quite different from previous approaches.
In thissection, we relate it to chart generation (Kay, 1996;Carroll et al, 1999) and to another constraint-basedapproach (Gardent and Thater, 2001).In chart based approaches to realization, the mainidea is to minimize the necessary computation byreusing partial results that have been computed be-fore.
In the setting of fixed word order parsing, thisbrings an immense increase in efficiency.
In genera-tion, however, the NP-completeness manifests itselfin charts of worst-case exponential size.
In addition,it can happen that substructures are built which arenot used in the final realization, especially when pro-cessing modifications.By contrast, our system configures nodes into adependency tree.
It solves a search problem, madeup by choices for mother-daughter relations in thetree.
Propagation, which runs in polynomial time,has access to global information (illustrated in Sec-tion 4.2) and can thus rule out impossible mother-daughter relations efficiently; every propagation stepthat takes place actually contributes to zooming inon the possible realizations.
Our system can showexponential runtimes when the distributions span asearch tree of exponential size.Gardent and Thater (2001) also propose a con-straint based approach to generation working witha variant of TAG.
However, the performance of theirsystem decreases rapidly as the input gets largereven when when working with a toy grammar.
Themain difference between their approach and oursseems to be that their algorithm tries to constructa derived tree, while ours builds a derivation tree.Our parser only has to deal with information thatis essential to solve the combinatorial problem, andnot e.g.
with the internal structure of the elementarytrees.
The reconstruction of the derived tree, whichis cheap once the derivation tree has been computed,is delegated to a post-processing step.
Working withderived trees, Gardent and Thater (2001) cannot ig-nore any information and have to keep track of therelationships between nodes at points where they arenot relevant.7 ConclusionGeneration from flat semantics is an NP-completeproblem.
In this paper, we have first given an al-ternative proof for this fact, which works even fora fixed grammar and makes the connection to thecomplexity of free word order parsing clearly visi-ble.
Then we have shown how to translate the re-alization problem of TAG into parsing problems oftopological dependency grammar, and argued howthe optimizations in the dependency parser ?
whichwere originally developed for free word order pars-ing ?
help reduce the runtime for the generation sys-tem.
This reduction shows in passing that the pars-ing problem for TDG is NP-complete as well, whichhas been conjectured, but never proved.The NP-completeness result for the realizationproblem explains immediately why all existing com-plete generation algorithms have exponential run-times in the worst case.
As our proof shows, themain sources of the combinatorics are the interac-tion of lexical ambiguity and tree configuration withthe completely unordered nature of the input.
Mod-ification is important and deserves careful treatment(and indeed, our system deals very gracefully withit), but it is not as intrinsically important as someof the literature suggests; our proof gets by withoutmodification.
If we allow the grammar to be partof the input, we can even modify the proof to showNP-hardness of the case where semantic atoms canbe verbalized more often than they appear in the in-put, and of the case where they can be verbalizedless often.
The case where every atom can be usedarbitrarily often remains open.By using techniques from constraint program-ming, the dependency parser seems to cope ratherwell with the combinatorics of generation.
Propaga-tors can rule out impossible local structures on thegrounds of global information, and selection con-straints greatly alleviate the proliferation of lexicalambiguity in large TAG grammars by making sharedinformation available without having to commit tospecific lexical entries.
Initial experiments with theXTAG grammar indicate that we can generate prac-tical examples in polynomial time, and may be com-petitive with state-of-the-art realization systems interms of raw runtime.In the future, it will first of all be necessary to liftthe restrictions we have placed on the TAG gram-mar: So far, the nodes of the elementary trees areonly equipped with nonterminal labels and indices,not with general feature structures, and we allowonly a restricted form of adjunction constraints.
Itshould be possible to either encode these construc-tions directly in the dependency grammar (which al-lows user-defined features too), or filter out wrongrealizations in a post-processing step.
The effect ofsuch extensions on the runtime remains to be seen.Finally, we expect that despite the general NP-completeness, there are restricted generation prob-lems which can be solved in polynomial time, butstill contain all problems that actually arise for nat-ural language.
The results of this paper open up anew perspective from which such restrictions can besought, especially considering that all the natural-language examples we tried are indeed processedin polynomial time.
Such a polynomial realiza-tion algorithm would be the ideal starting pointfor algorithms that compute not just any, but thebest possible realization ?
a problem which e.g.Bangalore and Rambow (2000) approximate usingstochastic methods.Acknowledgments.
We are grateful to TilmanBecker, Chris Brew, Ann Copestake, Ralph Debus-mann, Gerald Penn, Stefan Thater, and our reviewersfor helpful comments and discussions.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Usingtags, a tree model, and a language model for genera-tion.
In Proc.
of the TAG+5 Workshop, Paris.G.
Edward Barton, Robert C. Berwick, and Eric SvenRistad.
1987.
Computational Complexity and Natu-ral Language.
MIT Press, Cambridge, Mass.Chris Brew.
1992.
Letting the cat out of the bag: Gen-eration for Shake-and-Bake MT.
In Proceedings ofCOLING-92, pages 610?616, Nantes.John Carroll, Ann Copestake, Dan Flickinger, and Vic-tor Poznanski.
1999.
An efficient chart generator for(semi-)lexicalist grammars.
In Proceedings of the 7thEuropean Workshop on NLG, pages 86?95, Toulouse.Denys Duchier and Ralph Debusmann.
2001.
Topolog-ical dependency trees: A constraint-based account oflinear precedence.
In Proceedings of the 39th ACL,Toulouse, France.Denys Duchier.
2002.
Configuration of labeled trees un-der lexicalized constraints and principles.
Journal ofLanguage and Computation.
To appear.Claire Gardent and Stefan Thater.
2001.
Generating witha grammar based on tree descriptions: A constraint-based approach.
In Proceedings of the 39th ACL,Toulouse.Aravind Joshi and Yves Schabes.
1997.
Tree-AdjoiningGrammars.
In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, chapter 2, pages 69?123.
Springer-Verlag, Berlin.Martin Kay.
1996.
Chart generation.
In Proceedings ofthe 34th Annual Meeting of the ACL, pages 200?204,Santa Cruz.Alexander Koller and Joachim Niehren.
2000.
Con-straint programming in computational linguistics.
Toappear in Proceedings of LLC8, CSLI Press.Oz Development Team.
1999.
The Mozart ProgrammingSystem web pages.
http://www.mozart-oz.org/.Yves Schabes and Stuart Shieber.
1994.
An alterna-tive conception of tree-adjoining derivation.
Compu-tational Linguistics, 20(1):91?124.Matthew Stone and Christy Doran.
1997.
Sentence plan-ning as description using tree-adjoining grammar.
InProceedings of the 35th ACL, pages 198?205.XTAG Research Group.
2001.
A lexicalized tree adjoin-ing grammar for english.
Technical Report IRCS-01-03, IRCS, University of Pennsylvania.
