Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 229?238,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Context-Aware Topic Model for Statistical Machine TranslationJinsong Su1, Deyi Xiong2?, Yang Liu3, Xianpei Han4, Hongyu Lin1,Junfeng Yao1, Min Zhang2Xiamen University, Xiamen, China1Soochow University, Suzhou, China2Tsinghua University, Beijing, China3Institute of Software, Chinese Academy of Sciences, Beijing, China4{jssu, hylin, yao0010}@xmu.edu.cn{dyxiong, minzhang}@suda.edu.cnliuyang2011@tsinghua.edu.cnxianpei@nfs.iscas.ac.cnAbstractLexical selection is crucial for statistical ma-chine translation.
Previous studies separatelyexploit sentence-level contexts and document-level topics for lexical selection, neglectingtheir correlations.
In this paper, we proposea context-aware topic model for lexical selec-tion, which not only models local contexts andglobal topics but also captures their correla-tions.
The model uses target-side translationsas hidden variables to connect document top-ics and source-side local contextual words.
Inorder to learn hidden variables and distribu-tions from data, we introduce a Gibbs sam-pling algorithm for statistical estimation andinference.
A new translation probability basedon distributions learned by the model is inte-grated into a translation system for lexical se-lection.
Experiment results on NIST Chinese-English test sets demonstrate that 1) our modelsignificantly outperforms previous lexical se-lection methods and 2) modeling correlationsbetween local words and global topics can fur-ther improve translation quality.1 IntroductionLexical selection is a very important task in statis-tical machine translation (SMT).
Given a sentencein the source language, lexical selection statisticallypredicts translations for source words, based on vari-ous translation knowledge.
Most conventional SMTsystems (Koehn et al, 2003; Galley et al, 2006;Chiang, 2007) exploit very limited context informa-tion contained in bilingual rules for lexical selection.
?Corresponding author.
{stance,  attitude ...}l?ch?ngdu?
g?i    w?nt?
zh?nggu?
b?och?
zh?ngl?
l?ch?ng[Economy topic,  Politics topic ...]{problem,  issue ...}w?nt?Figure 1: A Chinese-English translation example to il-lustrate the effect of local contexts and global topics aswell as their correlations on lexical selection.
Each blackline indicates a set of translation candidates for a Chinesecontent word (within a dotted box).
Green lines point totranslations that are favored by local contexts while bluelines show bidirectional associations between global top-ics and their consistent target-side translations.Previous studies that explore richer information forlexical selection can be divided into two categories:1) incorporating sentence-level contexts (Chan et al,2007; Carpuat and Wu, 2007; Hasan et al, 2008;Mauser et al, 2009; He et al, 2008; Shen et al,2009) or 2) integrating document-level topics (Xi-ao et al, 2011; Ture et al, 2012; Xiao et al, 2012;Eidelman et al, 2012; Hewavitharana et al, 2013;Xiong et al, 2013; Hasler et al, 2014a; Hasler et al,2014b) into SMT.
The methods in these two strandshave shown their effectiveness on lexical selection.However, correlations between sentence- anddocument-level contexts have never been exploredbefore.
It is clear that local contexts and global top-ics are often highly correlated.
Consider a Chinese-English translation example presented in Figure 1.On the one hand, if local contexts suggest that thesource word ??|/l`?ch?ang?
should be translated in-229to ?stance?, they will also indicate that the topicof the document where the example sentence oc-curs is about politics.
The politics topic can be fur-ther used to enable the decoder to select a correc-t translation ?issue?
for another source word ?
?K/w`ent?i?, which is consistent with this topic.
Onthe other hand, if we know that this document main-ly focuses on the politics topic, the candiate trans-lation ?stance?
will be more compatible with thecontext of ??|/l`?ch?ang?
than the candiate transla-tion ?attitude?.
This is because neighboring source-side words ??I/zh?ongu?o?
and ???/zh?ongl`??
of-ten occur in documents that are about internationalpolitics.
We believe that such correlations betweenlocal contextual words and global topics can be usedto further improve lexical selection.In this paper, we propose a unified framework tojointly model local contexts, global topics as well astheir correlations for lexical selection.
Specifically,?
First, we present a context-aware topic mod-el (CATM) to exploit the features mentionedabove for lexical selection in SMT.
To the bestof our knowledge, this is the first work to joint-ly model both local and global contexts for lex-ical selection in a topic model.?
Second, we present a Gibbs sampling algorith-m to learn various distributions that are relatedto topics and translations from data.
The trans-lation probabilities derived from our model areintegrated into SMT to allow collective lexicalselection with both local and global informtion.We validate the effectiveness of our model on astate-of-the-art phrase-based translation system.
Ex-periment results on the NIST Chinese-English trans-lation task show that our model significantly outper-forms previous lexical selection methods.2 Context-Aware Topic ModelIn this section, we describe basic assumptions andelaborate the proposed context-aware topic model.2.1 Basic AssumptionsIn CATM, we assume that each source document dconsists of two types of words: topical words whichare related to topics of the document and contextualwords which affect translation selections of topicalwords.As topics of a document are usually representedby content words in it, we choose source-side nouns,verbs, adjectives and adverbs as topical words.
Forcontextual words, we use all words in a source sen-tence as contextual words.
We assume that they aregenerated by target-side translations of other wordsthan themselves.
Note that a source word may beboth topical and contextual.
For each topical word,we identify its candidate translations from trainingcorpus according to word alignments between thesource and target language.
We allow a target trans-lation to be a phrase of length no more than 3 words.We refer to these translations of source topical word-s as target-side topical items, which can be eitherwords or phrases.
In the example shown in Figure1, all source words within dotted boxes are topicalwords.
Topical word ??|/l`?ch?ang?
is supposed tobe translated into a target-side topical item ?stance?,which is collectively suggested by neighboring con-textual words ?
?I/zh?onggu?o?, ???/zh?ongl`?
?and the topic of the corresponding document.In our model, all target-side topical items in a doc-ument are generated according to the following twoassumptions:?
Topic consistency assumption: All target-sidetopical items in a document should be consis-tent with the topic distribution of the document.For example, the translations ?issue?, ?stance?tend to occur in documents about politics topic.?
Context compatibility assumption: For a top-ical word, its translation (i.e., the counter-part target-side topical item) should be com-patible with its neighboring contextual word-s. For instance, the translation ?stance?
of??|/l`?ch?ang?
is closely related to contextu-al words ??I/zh?ongu?o?
and ???/zh?ongl`?
?.2.2 ModelThe graphical representation of CATM, which visu-alizes the generative process of training data D, isshown in Figure 2.
Notations of CATM are present-ed in Table 1.
In CATM, each document d can begenerated in the following three steps1:1In the following description,Dir(.),Mult(.)
andUnif(.
)denote Dirichlet, Multinomial and Uniform distributions, re-230SymbolMeaning?hyperparameter for ?
?hyperparameter for ?
?hyperparameter for ?
?hyperparameter for ?ftopical wordccontextual worde?target-side topical iteme?
?a sampled target-side topical item used togenerate a source-side contextual word?the topic distribution of document?the distribution of a topic over target-sidetopical items?the translation probability distribution of atarget-side topical item over source-side topicalwords?the generation probability distribution of atarget-side topical item over source-sidecontextual wordsNztopic numberNddocument numberNfthe number of topical wordsNcthe number of contextual wordsNe?the number of target-side topical itemsNf,dthe number of topical words in dNc,dthe number of contextual words in dTable 1: Notations in CATM.1.
Sample a topic distribution ?d?Dir(?).2.
For each position i that corresponds to a topicalword fiin the document:(a) Sample a topic zi?Mult(?d).
(b) Conditioned on the topic zi, sample atarget-side topical item e?i?Mult(?zi).
(c) Conditioned on the target-side topi-cal item e?i, sample the topical wordfi?Mult(?e?i).3.
For each position j that corresponds to a contex-tual word cjin the document:(a) Collect all target-side topical items?esthatare translations of neighboring topicalwords within a window centered at cj(window size ws).
(b) Randomly sample an item from?es,e??j?Unif(?es).
(c) Conditioned on the sampled target-sidetopical item e?
?j, sample the contextualword cj?Mult(?e?
?j).To better illustrate CATM, let us revisit the examplein Figure 1.
We describe how CATM generates top-spectively.NdNc,dNf,dNe???ze?
e?
?f c??
??Nz?
?Figure 2: Graphical representation of our model.ical words ??K/w`ent??
?, ?
?|/l`?ch?ang?, and con-textual word ???/zh?ongl`??
in the following steps:Step 1: The model generates a topic dis-tribution for the corresponding document as{economy0.25, politics0.75}.Step 2: Based on the topic distribution, wechoose ?economy?
and ?politics?
as topic assign-ments for ??K/w`ent???
and ??|/l`?ch?ang?
respec-tively; Then, according to the distributions of the t-wo topics over target-side topical items, we generatetarget-side topical items ?issue?
and ?stance?
; Final-ly, according to the translation probability distribu-tions of these two topical items over source-side top-ical words, we generate source-side topical words??K/w`ent???
and ??|/l`?ch?ang?
for them respec-tively.Step 3: For the contextual word ???/zh?ongl`?
?,we first collect target-side topical items of its neigh-boring topical words such as ??K/w`ent??
?, ??/b?aoch???
and ??|/l`?ch?ang?
to form a target-side topical item set {?issue?,?keep?, ?stance?
},from which we randomly sample one item ?stance?.Next, according to the generation probability dis-tribution of ?stance?
over source contextual words,we finally generate the source contextual word ???/zh?ongl`?
?.In the above generative process, all target-sidetopical items are generated from the underlying top-ics of a source document, which guarantees that se-lected target translations are topic-consistent.
Ad-231ditionally, each source contextual word is derivedfrom a target-side topical item given its generationprobability distribution.
This makes selected targettranslations also compatible with source-side localcontextual words.
In this way, global topics, topicalwords, local contextual words and target-side topi-cal items are highly correlated in CATM that exactlycaptures such correlations for lexical selection.3 Parameter Estimation and InferenceWe propose a Gibbs sampling algorithm to learn var-ious distributions described in the previous section.Details of the learning and inference process are p-resented in this section.3.1 The Probability of Training CorpusAccording to CATM, the total probability of train-ing data D given hyperparameters ?, ?, ?
and ?
iscomputed as follows:P (D;?, ?, ?, ?)
=?dP (fd, cd;?, ?, ?, ?)=?d?
?edP (?ed|?, ?
)P (fd|?ed, ?
)P (cd|?ed, ?)=?
?P (?|?)?
?P (?|?)?d?
?edP (fd|?ed, ?)??
?P (?|?)?
?e?dP (?e?d|?ed)p(cd|?e?d, ?)??
?P (?|?
)P (?ed|?, ?)d?d?d?d?
(1)where fdand?eddenote the sets of topical words andtheir target-side topical item assignments in docu-ment d, cdand?e?dare the sets of contextual word-s and their target-side topical item assignments indocument d.3.2 Parameter Estimation via Gibbs SamplingThe joint distribution in Eq.
(1) is intractable tocompute because of coupled hyperparameters andhidden variables.
Following Han et al (2012),we adapt the well-known Gibbs sampling algorith-m (Griffiths and Steyvers, 2004) to our model.
Wecompute the joint posterior distribution of hiddenvariables, denoted by P (z,?e,?e?|D), and then use thisdistribution to 1) estimate ?, ?, ?
and ?, and 2) pre-dict translations and topics of all documents in D.Specifically, we derive the joint posterior distribu-tion from Eq.
(1) as:P (z,?e,?e?|D) ?
P (z)P (?e|z)P (f|?e)P (?e?|?e)P (c|?e?)
(2)Based on the equation above, we construct a Markovchain that converges to P (z,?e,?e?|D), where each s-tate is an assignment of a hidden variable (includ-ing topic assignment to a topical word, target-sidetopical item assignment to a source topical or con-textual word.).
Then, we sequentially sample eachassignment according to the following three condi-tional assignment distributions:1.
P (zi= z|z?i,?e,?e?,D): topic assignment dis-tribution of a topical word given z?ithat denotes alltopic assignments but zi,?e and?e?that are target-sidetopical item assignments.
It is updated as follows:P (zi= z|z?i,?e,?e?,D) ?CDZ(?i)dz+ ?CDZ(?i)d?+Nz?
?CZ?E(?i)ze?+ ?CZ?E(?i)z?+Ne??
(3)where the topic assignment to a topical word is de-termined by the probability that this topic appears indocument d (the 1st term) and the probability thatthe selected item e?
occurs in this topic (the 2nd ter-m).2.
P (e?i= e?|z,?e?i,?e?,D): target-side topical itemassignment distribution of a source topical word giv-en the current topic assignments z, the current itemassignments of all other topical words?e?i, and thecurrent item assignments of contextual words?e?.
Itis updated as follows:P (e?i= e?|z,?e?i,?e?,D) ?CZ?E(?i)ze?+ ?CZ?E(?i)z?+Ne??
?C?EF(?i)e?f+ ?C?EF(?i)e??+Nf??
(CW?E(?i)we?+ 1CW?E(?i)we?)CW?E?we?
(4)where the target-side topical item assignment to atopical word is determined by the probability thatthis item is from the topic z (the 1st term), the prob-ability that this item is translated into the topicalword f (the 2nd term) and the probability of con-textual words within a wsword window centered atthe topical word f , which influence the selection ofthe target-side topical item e?
(the 3rd term).
It isvery important to note that we use a parallel corpusto train the model.
Therefore we directly identifytarget-side topical items for source topical words viaword alignments rather than sampling.2323.
P (e?
?i= e?|z,?e,?e?
?i,D): target-side topical itemassignment distribution for a contextual word giventhe current topic assignments z, the current item as-signments of topical words?e, and the current itemassignments of all other contextual words?e??i.
It isupdated as follows:P (e?
?i= e?|z,?e,?e?
?i,D) ?CW?Ewe?CW?Ew?
?C?EC(?i)e?c+ ?C?EC(?i)e??+Nc?
(5)where the target-side topical item assignment usedto generate a contextual word is determined by theprobability of this item being assigned to generatecontextual words within a surface window of sizews(the 1st term) and the probability that contextu-al words occur in the context of this item (the 2ndterm).In all above formulas,CDZdzis the number of timesthat topic z has been assigned for all topical wordsin document d, CDZd?=?zCDZdzis the topic numberin document d, and CZ?Eze?, C?EFe?f, CW?Ewe?, CW?E?we?andC?ECe?chave similar explanations.
Based on the abovemarginal distributions, we iteratively update all as-signments of corpus D until the constructed Markovchain converges.
Model parameters are estimatedusing these final assignments.3.3 Inference on Unseen DocumentsFor a new document, we first predict its topics andtarget-side topical items using the incremental Gibb-s sampling algorithm described in (Kataria et al,2011).
In this algorithm, we iteratively update top-ic assignments and translation assignments of anunseen document following the same process de-scribed in Section 3.2, but with estimated model pa-rameters.Once we obtain these assignments, we estimatelexical translation probabilities based on the sam-pled counts of target-side topical items.
Formal-ly, for the position i in the document correspond-ing to the content word f , we collect the sampledcount that translation e?
generates f , denoted byCsam(e?, f).
This count can be normalized to form anew translation probability in the following way:p(e?|f) =Csam(e?, f) + kCsam+ k ?Ne?,f(6)where Csamis the total number of samples duringinference and Ne?,fis the number of candidate trans-lations of f .
Here we apply add-k smoothing to re-fine this translation probability, where k is a tunableglobal smoothing constant.
Under the framework oflog-linear model (Och and Ney, 2002), we use thistranslation probability as a new feature to improvelexical selection in SMT.4 ExperimentsIn order to examine the effectiveness of our mod-el, we carried out several groups of experiments onChinese-to-English translation.4.1 SetupOur bilingual training corpus is from the FBIS cor-pus and the Hansards part of LDC2004T07 cor-pus (1M parallel sentences, 54.6K documents, with25.2M Chinese words and 29M English words).We first used ZPar toolkit2and Stanford toolkit3topreprocess (i.e., word segmenting, PoS tagging) theChinese and English parts of training corpus, andthen word-aligned them using GIZA++ (Och andNey, 2003) with the option ?grow-diag-final-and?.We chose the NIST evaluation set of MT05 as thedevelopment set, and the sets of MT06/MT08 as testsets.
On average, these three sets contain 17.2, 13.9and 14.1 content words per sentence, respectively.We trained a 5-gram language model on the Xinhuaportion of Gigaword corpus using the SRILM Toolk-it (Stolcke, 2002).Our baseline system is a state-of-the-art SMT sys-tem, which adapts bracketing transduction gram-mars (Wu, 1997) to phrasal translation and equip-s itself with a maximum entropy based reorderingmodel (MEBTG) (Xiong et al, 2006).
We used thetoolkit4developed by Zhang (2004) to train the re-ordering model with the following parameters: it-eration number iter=200 and Gaussian prior g=1.0.During decoding, we set the ttable-limit as 20, thestack-size as 100.
The translation quality is eval-uated by case-insensitive BLEU-4 (Papineni et al,2002) metric.
Finally, we conducted paired boot-strap sampling (Koehn, 2004) to test the significancein BLEU score differences.2http://people.sutd.edu.sg/?yue zhang/doc/index.html3http://nlp.stanford.edu/software4http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html233Model MT05CATM (?
6w) 33.35CATM (?
8w) 33.43CATM (?
10w) 33.42CATM (?
12w) 33.49CATM (?
14w) 33.30Table 2: Experiment results on the development set usingdifferent window sizes ws.To train CATM, we set the topic number Nzas25.5For hyperparameters ?
and ?, we empiricallyset ?=50/Nzand ?=0.1, as implemented in (Grif-fiths and Steyvers, 2004).
Following Han et al(2012), we set ?
and ?
as 1.0/Nfand 2000/Nc, re-spectively.
During the training process, we ran 400iterations of the Gibbs sampling algorithm.
For doc-uments to be translated, we first ran 300 rounds in aburn-in step to let the probability distributions con-verge, and then ran 1500 rounds where we collectedindependent samples every 5 rounds.
The longesttraining time of CATM is less than four days on ourserver using 4GB RAM and one core of 3.2GHzCPU.
As for the smoothing constant k in Eq.
(6),we set its values to 0.5 according to the performanceon the development set in additional experiments.4.2 Impact of Window Size wsOur first group of experiments were conducted onthe development set to investigate the impact of thewindow size ws.
We gradually varied window sizefrom 6 to 14 with an increment of 2.Experiment results are shown in Table 2.
Weachieve the best performance when ws=12.
Thissuggests that a ?12-word window context is suf-ficient for predicting target-side translations for am-biguous source-side topical words.
We therefore setws=12 for all experiments thereafter.4.3 Overall PerformanceIn the second group of experiments, in addition tothe conventional MEBTG system, we also comparedCATM with the following two models:Word Sense Disambiguation Model (WSDM)(Chan et al, 2007).
This model improves lexical s-election in SMT by exploiting local contexts.
For5We try different topic numbers from 25 to 100 with an in-crement of 25 each time.
We find thatNz=25 produces a slight-ly better performance than other values on the development set.each content word, we construct a MaxEnt-basedclassifier incorporating local collocation and sur-rounding word features, which are also adopted byChan et al (2007).
For each candidate translatione?
of topical word f , we use WSDM to estimatethe context-specific translation probability P (e?|f),which is used as a new feature in SMT system.Topic-specific Lexicon Translation Model(TLTM) (Zhao and Xing, 2007).
This modelfocuses on the utilization of document-level context.We adapted it to estimate a lexicon translationprobability as follows:p(f |e?, d) ?
p(e?|f, d) ?
p(f |d)=?zp(e?|f, z) ?
p(f |z) ?
p(z|d) (7)where p(e?|f, z) is the lexical translation probabil-ity conditioned on topic z, which can be calculat-ed according to the principle of maximal likelihood,p(f |z) is the generation probability of word f fromtopic z, and p(z|d) denotes the posterior topic distri-bution of document d.Note that our CATM is proposed for lexical se-lection on content words.
To show the strong effec-tiveness of our model, we also compared it againstthe full-fledged variants of the above-mentioned twomodels that are built for all source words.
We referto them as WSDM (All) and TLTM (All), respec-tively.Table 3 displays BLEU scores of different lexicalselection models.
All models outperform the base-line.
Although we only use CATM to predict trans-lations for content words, CATM achieves an aver-age BLEU score of 26.77 on the two test sets, whichis higher than that of the baseline by 1.18 BLEUpoints.
This improvement is statistically significantat p<0.01.
Furthermore, we also find that our modelperforms better than WSDM and TLTM with signif-icant improvements.
Finally, even if WSDM (All)and TLTM (all) are built for all source words, theyare still no better than than CATM that selects de-sirable translations for content words.
These exper-iment results strongly demonstrate the advantage ofCATM over previous lexical selection models.5 AnalysisIn order to investigate why CATM is able to outper-form previous models that explore only local contex-234ModelLocal Context Global Topic MT06 MT08 AvgBaseline?
?
29.66??21.52??25.59WSDM??
30.62?22.05?
?26.34WSDM (All)??
30.92 22.27 26.60TLTM??30.27??21.70?
?25.99TLTM (All)??30.33??21.58??25.96CATM?
?30.97 22.56 26.77Table 3: Experiment results on the test sets.
Avg = average BLEU scores.
WSDM (All) and TLTM (All) are modelsbuilt for all source words.
?
: significantly worse than CATM (p<0.05), ??
: significantly worse than CATM (p<0.01).tual words or global topics, we take a deep look in-to topics, topical items and contextual words learnedby CATM and empirically analyze the effect of mod-eling correlations between local contextual wordsand global topics on lexical selection.5.1 Outputs of CATMWe present some examples of topics learned byCATM in Table 4.
We also list five target-side topi-cal items with the highest probabilities for each top-ic, and the most probable five contextual words foreach target-side topical item.
These examples clear-ly show that target-side topical items tightly connectglobal topics and local contextual words by captur-ing their correlations.5.2 Effect of Correlation ModelingCompared to previous lexical selection models,CATM jointly models both local contextual wordsand global topics.
Such a joint modeling also en-ables CATM to capture their inner correlations at themodel level.
In order to examine the effect of corre-lation modeling on lexical selection, we comparedCATM with its three variants:  CATM (Contex-t) that only uses local context information.
We de-termined target-side topical items for content wordsin this variant by setting the probability distributionthat a topic generates a target-side topical item to beuniform; CATM (Topic) that explores only glob-al topic information.
We identified target-side topi-cal items for content words in the model by settingwsas 0, i.e., no local contextual words being usedat all.
 CATM (Log-linear) is the combinationof the above-mentioned two variants ( and ) ina log-linear manner, which does not capture corre-lations between local contextual words and globaltopics at the model level.ModelMT06 MT08 AvgCATM (Context)30.46??22.02?
?26.24CATM (Topic)30.20??21.90?
?26.05CATM (Log-linear)30.59?22.24?26.42CATM30.97 22.56 26.77Table 5: Experiment results on the test sets.
CATM (Log-linear) is the combination of CATM (Context) and CATM(Topic) in a log-linear manner.Results in Table 5 show that CATM performs sig-nificantlly better than both CATM (Topic) and CAT-M (Context).
Even compared with CATM (Log-linear), CATM still achieves a significant improve-ment of 0.35 BLEU points (p<0.05).
This validatesthe effectiveness of capturing correlations for lexicalselection at the model level.6 Related WorkOur work is partially inspired by (Han and Sun,2012), where an entity-topic model is presented forentity linking.
We successfully adapt this work tolexical selection in SMT.
The related work mainlyincludes the following two strands.
(1) Lexical Selection in SMT.
In order to explorerich context information for lexical selection, someresearchers propose trigger-based lexicon models tocapture long-distance dependencies (Hasan et al,2008; Mauser et al, 2009), and many more re-searchers build classifiers to select desirable trans-lations during decoding (Chan et al, 2007; Carpuatand Wu, 2007; He et al, 2008; Liu et al, 2008).Along this line, Shen et al (2009) introduce fournew linguistic and contextual features for translationselection in SMT.
Recently, we have witnessed anincreasing efforts in exploiting document-level con-text information to improve lexical selection.
Xiaoet al (2011) impose a hard constraint to guarantee235TopicTarget-sideTopical ItemsSource-side Contextual WordsrefugeeUNHCR J?
(refugee) ???
(office) ;(commissioner) ??
(affair) p?
(high-level)republic ??
(union) ??
(democracy) ?
(government) ?d=(Islam) ??
(Central Africa)refugee J?
(refugee) ??
(return) 6l??
(displaced) e?
(repatriate) o(protect)Kosovo r?F?
(Metohija) ?S(territory) ??
(crisis) ??
(situation) l??
(Serbia)federal ?I(republic) Hd.?
(Yugoslavia) ???
(Kosovo) ?
(government) ?
(authority)militarymilitary *(observer) 1?
(action) {I(USA) <(personnel) ??
(army)missile ??
(defense) X?
(system) {I(USA) u(launch) q(*)United States ?I(China) F(Japan) (Taiwan) ?
(military) NMD(National Missile Defense)system ?
?I(United Nations) ??
(build) I(country) I[(country) &E(information)war ?(war) |(?)
?.
(world) u?
(wage) ?
(gulf)economycountry u??
(developing) u?
(developed) ??
(Africa) u?
(development) ?
(China)development ?
?Y(sustainable) ?L(economy) r?
(promote) ?
(society) ?(situation)international ?
(society) |?
(organization) ??
(coorporation) I[(country) ?
?I(United Nations)economic ?
(society) u?
(development) O(growth) I[(country) ?z(globalization)trade u?
(development) IS(international) ?.
(world) ?
](investment) :(point)cross-straitrelationTaiwan ?I(China) ??
(mainland) ?
(authority) {I(USA) ??
(compatriot)China `(say) {I(USA) (Taiwan) K(principle) ?
(*)relation u?
(development) W(*) ?
(China) ?
(*) I(country)cross-strait ?
(*) 'X(relation) (Taiwan) W(*) 6(exchange)issue )?
(settlement) ??
(discuss) ?K(issue) ??
(important) (Taiwan)Table 4: Examples of topics, topical items and contextual words learned by CATM with Nz=25 and Ws=12.
Chinesewords that do not have direct English translations are denoted with ?*?.
Here ?q?
and ?|?
are Chinese quantifiersfor missile and war, respectively; ???
and ?W?
together means cross-starit.the document-level translation consistency.
Ture etal.
(2012) soften this consistency constraint by in-tegrating three counting features into decoder.
Alsorelevant is the work of Xiong et al(2013), who usethree different models to capture lexical cohesion fordocument-level SMT.
(2) SMT with Topic Models.
In this strand, Zhaoand Xing (2006, 2007) first present a bilingual top-ical admixture formalism for word alignment inSMT.
Tam et al (2007) and Ruiz et al (2012) applytopic model into language model adaptation.
Su etal.
(2012) conduct translation model adaptation withmonolingual topic information.
Gong et al (2010)and Xiao et al (2012) introduce topic-based similar-ity models to improve SMT system.
Axelrod et al(2012) build topic-specific translation models fromthe TED corpus and select topic-relevant data fromthe UN corpus to improve coverage.
Eidelman et al(2012) incorporate topic-specific lexical weights in-to translation model.
Hewavitharana et al (2013)propose an incremental topic based translation mod-el adaptation approach that satisfies the causalityconstraint imposed by spoken conversations.
Hasleret al (2014) present a new bilingual variant of LDAto compute topic-adapted, probabilistic phrase trans-lation features.
They also use a topic model to learnlatent distributional representations of different con-text levels of a phrase pair (Hasler et al, 2014b).In the studies mentioned above, those by Zhaoand Xing (2006), Zhao and Xing (2007), Hasler etal.
(2014a), and Hasler et al (2014b) are most relat-ed to our work.
However, they all perform dynam-ic translation model adaptation with topic models.Significantly different from them, we propose a newtopic model that exploits both local contextual word-s and global topics for lexical selection.
To the bestof our knowledge, this is first attempt to capture cor-relations between local words and global topics forbetter lexical selection at the model level.7 Conclusion and Future WorkThis paper has presented a novel context-aware topicmodel for lexical selection in SMT.
Jointly modelinglocal contexts, global topics and their correlations ina unified framework, our model provides an effec-tive way to capture context information at differen-t levels for better lexical selection in SMT.
Experi-ment results not only demonstrate the effectivenessof the proposed topic model, but also show that lex-ical selection benefits from correlation modeling.In the future, we want to extend our model fromthe word level to the phrase level.
We also plan to236improve our model with monolingual corpora.AcknowledgmentsThe authors were supported by National Natural Sci-ence Foundation of China (Grant Nos 61303082 and61403269), Natural Science Foundation of Jiang-su Province (Grant No.
BK20140355), National863 program of China (No.
2015AA011808), Re-search Fund for the Doctoral Program of HigherEducation of China (No.
20120121120046), theSpecial and Major Subject Project of the Industri-al Science and Technology in Fujian Province 2013(Grant No.
2013HZ0004-1), and 2014 Key Projectof Anhui Science and Technology Bureau (GrantNo.
1301021018).
We also thank the anonymousreviewers for their insightful comments.ReferencesAmittai Axelrod, Xiaodong He, Li Deng, Alex Acero,and Mei-Yuh Hwang.
2012.
New methods and eval-uation experiments on translating TED talks in the I-WSLT benchmark.
In Proc.
of ICASSP 2012, pages4945-4648.Rafael E. Banchs and Marta R. Costa-jussa`.
2011.
ASemantic Feature for Statistical Machine Translation.In Proc.
of SSSST-5 2011, pages 126-134.David M. Blei.
2003.
Latent Dirichlet Allocation.
Jour-nal of Machine Learning, pages 993-1022.Marine Carpuat and Dekai Wu.
2007.
Improving Statis-tical Machine Translation Using Word Sense Disam-biguation.
In Proc.
of EMNLP 2007, pages 61-72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word Sense Disambiguation Improves Statistical Ma-chine Translation.
In Proc.
of ACL 2007, pages 33-40.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, pages 201-228.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better Hypothesis Testing for Statis-tical Machine Translation: Controlling for OptimizerInstability.
In Proc.
of ACL 2011, short papers, pages176-181.George Doddington.
2002.
Translation Quality Using N-gram Cooccurrence Statistics.
In Proc.
of HLT 2002,138-145.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic Models for Dynamic Transla-tion Model Adaptation.
In Proc.
of ACL 2012, ShortPapers, pages 115-119.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and Ignacio Thay-er.
2006.
Scalable Inference and Training of Context-Rich Syntactic Translation Models.
In Proc.
of ACL2006, pages 961-968.Zhengxian Gong and Guodong Zhou.
2010.
ImproveSMT with Source-side Topic-Document Distributions.In Proc.
of SUMMIT 2010.Thomas L. Griffiths and Mark Steyvers.
2004.
FindingScientific Topics.
In Proc.
of the National Academy ofSciences 2004.Xianpei Han and Le Sun.
2012.
An Entity-Topic Modelfor Entity Linking.
In Proc.
of EMNLP 2012, pages105-115.Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and Jesu?sAndre?s-Ferrer 2008.
Triplet Lexicon Models for S-tatistical Machine Translation.
In Proc.
of EMNLP2008, pages 372-381.Eva Hasler, Phil Blunsom, Philipp Koehn, and Bar-ry Haddow.
2014.
Dynamic Topic Adaptation forPhrase-based MT.
In Proc.
of EACL 2014, pages 328-337.Eva Hasler, Phil Blunsom, Philipp Koehn, and BarryHaddow.
2014.
Dynamic Topic Adaptation for SMTusing Distributional Profiles.
In Proc.
of WMT 2014,pages 445-456.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Improv-ing Statistical Machine Translation using LexicalizedRule Selection.
In Proc.
of COLING 2008, pages 321-328.Sanjika Hewavitharana, Dennis Mehay, Sankara-narayanan Ananthakrishnan, and Prem Natarajan.2013.
Incremental Topic-based TM Adaptation forConversational SLT.
In Proc.
of ACL 2013, ShortPapers, pages 697-701.Saurabh S. Kataria, Krishnan S. Kumar, and Rajeev Ras-togi.
2011.
Entity Disambiguation with HierarchicalTopic Models.
In Proc.
of KDD 2011, pages 1037-1045.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Proc.of NAACL-HLT 2003, pages 127-133.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
of EMNLP2004, pages 388-395.Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.2008.
Maximum Entropy based Rule Selection Modelfor Syntax-based Statistical Machine Translation.
InProc.
of EMNLP 2008, pages 89-97.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending Statistical Machine Translation with Discrim-inative and Trigger-based Lexicon Models.
In Proc.
ofEMNLP 2009, pages 210-218.237Franz Joseph Och and Hermann Ney.
2002.
Discrimi-native Training and Maximum Entropy Models for S-tatistical Machine Translation.
In Proc.
of ACL 2002,pages 295-302.Franz Joseph Och and Hermann Ney.
2003.
A Systemat-ic Comparison of Various Statistical Alignment Mod-els.
Computational Linguistics, 2003(29), pages 19-51.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of ACL 2003,pages 160-167.Franz Joseph Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical Machine Trans-lation.
Computational Linguistics, 2004(30), pages417-449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2007.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proc.
of ACL2002, pages 311-318.Nick Ruiz and Marcello Federico.
2012.
Topic Adapta-tion for Lecture Translation through Bilingual LatentSemantic Models.
In Proc.
of the Sixth Workshop onStatistical Machine Translation, pages 294-302.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective Use of Lin-guistic and Contextual Information for Statistical Ma-chine Translation.
In Proc.
of EMNLP 2009, pages72-80.Andreas Stolcke.
2002.
Srilm - An Extensible LanguageModeling Toolkit.
In Proc.
of ICSLP 2002, pages 901-904.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xi-aodong Shi, Huailin Dong, and Qun Liu.
2012.
Trans-lation Model Adaptation for Statistical Machine Trans-lation with Monolingual Topic Information.
In Proc.of ACL 2012, pages 459-468.Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz.
2007.Bilingual LSA-based adaptation for statistical machinetranslation.
Machine Translation, 21(4), pages 187-207.Ferhan Ture, DouglasW.
Oard, and Philip Resnik.
2012.Encouraging Consistent Translation Choices.
In Proc.of NAACL-HLT 2012, pages 417-426.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377-403.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level Consistency Verification inMachine Translation.
In Proc.
of MT SUMMIT 2011,pages 131-138.Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, andShouxun Lin.
2012.
A Topic Similarity Model for Hi-erarchical Phrase-based Translation.
In Proc.
of ACL2012, pages 750-758.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model for S-tatistical Machine Translation.
In Proc.
of ACL 2006,pages 521-528.Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lu?,and Qun Liu.
2013.
Modeling Lexical Cohesion forDocument-Level Machine Translation.
In Proc.
of IJ-CAI 2013, pages 2183-2189.Deyi Xiong and Min Zhang.
2014.
A Sense-BasedTranslation Model for Statistical Machine Translation.In Proc.
of ACL 2014, pages 1459-1469.Bing Zhao and Eric P.Xing.
2006.
BiTAM: BilingualTopic AdMixture Models for Word Alignment.
InProc.
of ACL/COLING 2006, pages 969-976.Bing Zhao and Eric P.Xing.
2007.
HM-BiTAM: Bilin-gual Topic Exploration, Word Alignment, and Trans-lation.
In Proc.
of NIPS 2007, pages 1-8.238
