Proceedings of ACL-08: HLT, pages 335?343,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsParsing Noun Phrase Structure with CCGDavid Vadas and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{dvadas1, james}@it.usyd.edu.auAbstractStatistical parsing of noun phrase (NP) struc-ture has been hampered by a lack of gold-standard data.
This is a significant problem forCCGbank, where binary branching NP deriva-tions are often incorrect, a result of the auto-matic conversion from the Penn Treebank.We correct these errors in CCGbank using agold-standard corpus of NP structure, result-ing in a much more accurate corpus.
We alsoimplement novel NER features that generalisethe lexical information needed to parse NPsand provide important semantic information.Finally, evaluating against DepBank demon-strates the effectiveness of our modified cor-pus and novel features, with an increase inparser performance of 1.51%.1 IntroductionInternal noun phrase (NP) structure is not recoveredby a number of widely-used parsers, e.g.
Collins(2003).
This is because their training data, the PennTreebank (Marcus et al, 1993), does not fully anno-tate NP structure.
The flat structure described by thePenn Treebank can be seen in this example:(NP (NN lung) (NN cancer) (NNS deaths))CCGbank (Hockenmaier and Steedman, 2007) isthe primary English corpus for Combinatory Cate-gorial Grammar (CCG) (Steedman, 2000) and wascreated by a semi-automatic conversion from thePenn Treebank.
However, CCG is a binary branch-ing grammar, and as such, cannot leave NP structureunderspecified.
Instead, all NPs were made right-branching, as shown in this example:(N(N/N lung)(N(N/N cancer) (N deaths) ) )This structure is correct for most English NPs andis the best solution that doesn?t require manual re-annotation.
However, the resulting derivations oftencontain errors.
This can be seen in the previous ex-ample, where lung cancer should form a con-stituent, but does not.The first contribution of this paper is to correctthese CCGbank errors.
We apply an automatic con-version process using the gold-standard NP data an-notated by Vadas and Curran (2007a).
Over a quar-ter of the sentences in CCGbank need to be altered,demonstrating the magnitude of the NP problem andhow important it is that these errors are fixed.We then run a number of parsing experiments us-ing our new version of the CCGbank corpus.
Inparticular, we implement new features using NERtags from the BBN Entity Type Corpus (Weischedeland Brunstein, 2005).
These features are targeted atimproving the recovery of NP structure, increasingparser performance by 0.64% F-score.Finally, we evaluate against DepBank (King et al,2003).
This corpus annotates internal NP structure,and so is particularly relevant for the changes wehave made to CCGbank.
The CCG parser now recov-ers additional structure learnt from our NP correctedcorpus, increasing performance by 0.92%.
Applyingthe NER features results in a total increase of 1.51%.This work allows parsers trained on CCGbank tomodel NP structure accurately, and then pass thiscrucial information on to downstream systems.335(a) (b)NN /NcottonNconjandNN /NacetateNfibersNN /NN /NcottonN /N [conj ]conjandN /NacetateNfibersFigure 1: (a) Incorrect CCG derivation from Hockenmaier and Steedman (2007) (b) The correct derivation2 BackgroundParsing of NPs is typically framed as NP bracketing,where the task is limited to discriminating betweenleft and right-branching NPs of three nouns only:?
(crude oil) prices ?
left-branching?
world (oil prices) ?
right-branchingLauer (1995) presents two models to solve this prob-lem: the adjacency model, which compares the as-sociation strength between words 1?2 to words 2?3;and the dependency model, which compares words1?2 to words 1?3.
Lauer (1995) experiments with adata set of 244 NPs, and finds that the dependencymodel is superior, achieving 80.7% accuracy.Most NP bracketing research has used Lauer?sdata set.
Because it is a very small corpus, mostapproaches have been unsupervised, measuring as-sociation strength with counts from a separate largecorpus.
Nakov and Hearst (2005) use search enginehit counts and extend the query set with typographi-cal markers.
This results in 89.3% accuracy.Recently, Vadas and Curran (2007a) annotated in-ternal NP structure for the entire Penn Treebank, pro-viding a large gold-standard corpus for NP bracket-ing.
Vadas and Curran (2007b) carry out supervisedexperiments using this data set of 36,584 NPs, out-performing the Collins (2003) parser.The Vadas and Curran (2007a) annotation schemeinserts NML and JJP brackets to describe the correctNP structure, as shown below:(NP (NML (NN lung) (NN cancer) )(NNS deaths) )We use these brackets to determine new gold-standard CCG derivations in Section 3.2.1 Combinatory Categorial GrammarCombinatory Categorial Grammar (CCG) (Steed-man, 2000) is a type-driven, lexicalised theory ofgrammar.
Lexical categories (also called supertags)are made up of basic atoms such as S (Sentence)and NP (Noun Phrase), which can be combined toform complex categories.
For example, a transitiveverb such as bought (as in IBM bought thecompany) would have the category: (S\NP)/NP .The slashes indicate the directionality of arguments,here two arguments are expected: an NP subject onthe left; and an NP object on the right.
Once thesearguments are filled, a sentence is produced.Categories are combined using combinatory rulessuch as forward and backward application:X /Y Y ?
X (>) (1)Y X \Y ?
X (<) (2)Other rules such as composition and type-raising areused to analyse some linguistic constructions, whileretaining the canonical categories for each word.This is an advantage of CCG, allowing it to recoverlong-range dependencies without the need for post-processing, as is the case for many other parsers.In Section 1, we described the incorrect NP struc-tures in CCGbank, but a further problem that high-lights the need to improve NP derivations is shownin Figure 1.
When a conjunction occurs in an NP, anon-CCG rule is required in order to reach a parse:conj N ?
N (3)This rule treats the conjunction in the same manneras a modifier, and results in the incorrect derivationshown in Figure 1(a).
Our work creates the correctCCG derivation, shown in Figure 1(b), and removesthe need for the grammar rule in (3).Honnibal and Curran (2007) have also madechanges to CCGbank, aimed at better differentiat-ing between complements and adjuncts.
PropBank(Palmer et al, 2005) is used as a gold-standard to in-form these decisions, similar to the way that we usethe Vadas and Curran (2007a) data.336(a) (b) (c)NN /NlungNN /NcancerNdeathsN??????lung???cancer??
?deathsNN /N(N /N )/(N /N )lungN /NcancerNdeathsFigure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags2.2 CCG parsingThe C&C CCG parser (Clark and Curran, 2007b) isused to perform our experiments, and to evaluatethe effect of the changes to CCGbank.
The parseruses a two-stage system, first employing a supertag-ger (Bangalore and Joshi, 1999) to propose lexi-cal categories for each word, and then applying theCKY chart parsing algorithm.
A log-linear model isused to identify the most probable derivation, whichmakes it possible to add the novel features we de-scribe in Section 4, unlike a PCFG.The C&C parser is evaluated on predicate-argument dependencies derived from CCGbank.These dependencies are represented as 5-tuples:?hf , f , s, ha, l?, where hf is the head of the predi-cate; f is the supertag of hf ; s describes which ar-gument of f is being filled; ha is the head of theargument; and l encodes whether the dependency islocal or long-range.
For example, the dependencyencoding company as the object of bought (as inIBM bought the company) is represented by:?bought, (S\NP1 )/NP2 , 2, company,??
(4)This is a local dependency, where company is fill-ing the second argument slot, the object.3 Conversion ProcessThis section describes the process of converting theVadas and Curran (2007a) data to CCG derivations.The tokens dominated by NML and JJP brackets inthe source data are formed into constituents in thecorresponding CCGbank sentence.
We generate thetwo forms of output that CCGbank contains: AUTOfiles, which represent the tree structure of each sen-tence; and PARG files, which list the word?word de-pendencies (Hockenmaier and Steedman, 2005).We apply one preprocessing step on the PennTreebank data, where if multiple tokens are enclosedby brackets, then a NML node is placed around thosetokens.
For example, we would insert the NMLbracket shown below:(NP (DT a) (-LRB- -LRB-)(NML (RB very) (JJ negative) )(-RRB- -RRB-) (NN reaction) )This simple heuristic captures NP structure not ex-plicitly annotated by Vadas and Curran (2007a).The conversion algorithm applies the followingsteps for each NML or JJP bracket:1.
Identify the CCGbank lowest spanning node,the lowest constituent that covers all of thewords in the NML or JJP bracket;2. flatten the lowest spanning node, to remove theright-branching structure;3. insert new left-branching structure;4. identify heads;5. assign supertags;6. generate new dependencies.As an example, we will follow the conversion pro-cess for the NML bracket below:(NP (NML (NN lung) (NN cancer) )(NNS deaths) )The corresponding lowest spanning node, whichincorrectly has cancer deaths as a constituent,is shown in Figure 2(a).
To flatten the node, we re-cursively remove brackets that partially overlap theNML bracket.
Nodes that don?t overlap at all are leftintact.
This process results in a list of nodes (whichmay or may not be leaves), which in our example is[lung, cancer, deaths].
We then insert the cor-rect left-branching structure, shown in Figure 2(b).At this stage, the supertags are still incomplete.Heads are then assigned using heuristics adaptedfrom Hockenmaier and Steedman (2007).
Since weare applying these to CCGbank NP structures ratherthan the Penn Treebank, the POS tag based heuristicsare sufficient to determine heads accurately.337Finally, we assign supertags to the new structure.We want to make the minimal number of changesto the entire sentence derivation, and so the supertagof the dominating node is fixed.
Categories are thenpropagated recursively down the tree.
For a nodewith category X , its head child is also given the cat-egory X .
The non-head child is always treated asan adjunct, and given the category X /X or X \X asappropriate.
Figure 2(c) shows the final result of thisstep for our example.3.1 Dependency generationThe changes described so far have generated the newtree structure, but the last step is to generate new de-pendencies.
We recursively traverse the tree, at eachlevel creating a dependency between the heads ofthe left and right children.
These dependencies arenever long-range, and therefore easy to deal with.We may also need to change dependencies reachingfrom inside to outside the NP, if the head(s) of theNP have changed.
In these cases we simply replacethe old head(s) with the new one(s) in the relevantdependencies.
The number of heads may change be-cause we now analyse conjunctions correctly.In our example, the original dependencies were:?lung,N /N1 , 1, deaths,??
(5)?cancer,N /N1 , 1, deaths,??
(6)while after the conversion process, (5) becomes:?lung, (N /N1 )/(N /N )2 , 2, cancer,??
(7)To determine that the conversion process workedcorrectly, we manually inspected its output forunique tree structures in Sections 00?07.
This iden-tified problem cases to correct, such as those de-scribed in the following section.3.2 Exceptional casesFirstly, when the lowest spanning node covers theNML or JJP bracket exactly, no changes need to bemade to CCGbank.
These cases occur when CCG-bank already received the correct structure duringthe original conversion process.
For example, brack-ets separating a possessive from its possessor weredetected automatically.A more complex case is conjunctions, which donot follow the simple head/adjunct method of as-signing supertags.
Instead, conjuncts are identifiedduring the head-finding stage, and then assigned thesupertag dominating the entire coordination.
Inter-vening non-conjunct nodes are given the same cate-gory with the conj feature, resulting in a derivationthat can be parsed with the standard CCGbank bi-nary coordination rules:conj X ?
X[conj] (8)X X[conj] ?
X (9)The derivation in Figure 1(b) is produced by thesecorrections to coordination derivations.
As a result,applications of the non-CCG rule shown in (3) havebeen reduced from 1378 to 145 cases.Some POS tags require special behaviour.
De-terminers and possessive pronouns are both usuallygiven the supertag NP [nb]/N , and this should notbe changed by the conversion process.
Accordingly,we do not alter tokens with POS tags of DT and PRP$.Instead, their sibling node is given the category Nand their parent node is made the head.
The parent?ssibling is then assigned the appropriate adjunct cat-egory (usually NP\NP ).
Tokens with punctuationPOS tags1 do not have their supertag changed either.Finally, there are cases where the lowest span-ning node covers a constituent that should not bechanged.
For example, in the following NP:(NP(NML (NN lower) (NN court) )(JJ final) (NN ruling) )with the original CCGbank lowest spanning node:(N (N/N lower)(N (N/N court)(N (N/N final) (N ruling) ) ) )the final ruling node should not be altered.It may seem trivial to process in this case, butconsider a similarly structured NP: lower courtruling that the U.S. can bar the useof... Our minimalist approach avoids reanalysingthe many linguistic constructions that can be dom-inated by NPs, as this would reinvent the creationof CCGbank.
As a result, we only flatten thoseconstituents that partially overlap the NML or JJPbracket.
The existing structure and dependencies ofother constituents are retained.
Note that we are stillconverting every NML and JJP bracket, as even inthe subordinate clause example, only the structurearound lower court needs to be altered.1period, comma, colon, and left and right bracket.338the world ?s largest aid donorNP [nb]/N N /N N NP\NP NP\NP NP\NP>N>NP<NP<NP<NPthe world ?s largest aid donorNP [nb]/N N (NP [nb]/N )\NP N /N N /N N> >NP N< >NP [nb]/N N>NP(a) (b)Figure 3: CCGbank derivations for possessives# %Possessive 224 43.75Left child contains DT/PRP$ 87 16.99Couldn?t assign to non-leaf 66 12.89Conjunction 35 6.84Automatic conversion was correct 26 5.08Entity with internal brackets 23 4.49DT 22 4.30NML/JJP bracket is an error 12 2.34Other 17 3.32Total 512 100.00Table 1: Manual analysis3.3 Manual annotationA handful of problems that occurred during the con-version process were corrected manually.
The firstindicator of a problem was the presence of a pos-sessive.
This is unexpected, because possessiveswere already bracketed properly when CCGbankwas originally created (Hockenmaier, 2003, ?3.6.4).Secondly, a non-flattened node should not be as-signed a supertag that it did not already have.
Thisis because, as described previously, a non-leaf nodecould dominate any kind of structure.
Finally, weexpect the lowest spanning node to cover only theNML or JJP bracket and one more constituent to theright.
If it doesn?t, because of unusual punctuationor an incorrect bracket, then it may be an error.
Inall these cases, which occur throughout the corpus,we manually analysed the derivation and fixed anyerrors that were observed.512 cases were flagged by this approach, or1.90% of the 26,993 brackets converted to CCG.
Ta-ble 1 shows the causes of these problems.
The mostcommon cause of errors was possessives, as the con-version process highlighted a number of instanceswhere the original CCGbank analysis was incorrect.An example of this error can be seen in Figure 3(a),where the possessive doesn?t take any arguments.Instead, largest aid donor incorrectly modifies theNP one word at a time.
The correct derivation aftermanual analysis is in (b).The second-most common cause occurs whenthere is apposition inside the NP.
This can be seenin Figure 4.
As there is no punctuation on whichto coordinate (which is how CCGbank treats mostappositions) the best derivation we can obtain is tohave Victor Borge modify the preceding NP.The final step in the conversion process wasto validate the corpus against the CCG grammar,first by those productions used in the existingCCGbank, and then against those actually licensedby CCG (with pre-existing ungrammaticalities re-moved).
Sixteen errors were identified by this pro-cess and subsequently corrected by manual analysis.In total, we have altered 12,475 CCGbank sen-tences (25.5%) and 20,409 dependencies (1.95%).4 NER featuresNamed entity recognition (NER) provides informa-tion that is particularly relevant for NP parsing, sim-ply because entities are nouns.
For example, know-ing that Air Force is an entity tells us that AirForce contract is a left-branching NP.Vadas and Curran (2007a) describe using NE tagsduring the annotation process, suggesting that NER-based features will be helpful in a statistical model.There has also been recent work combining NER andparsing in the biomedical field.
Lewin (2007) exper-iments with detecting base-NPs using NER informa-tion, while Buyko et al (2007) use a CRF to identify339a guest comedian Victor BorgeNP [nb]/N N /N N /N N /N N>N>N>N>NPa guest comedian Victor BorgeNP [nb]/N N /N N (NP\NP)/(NP\NP) NP\NP> >N NP\NP>NP<NP(a) (b)Figure 4: CCGbank derivations for apposition with DTcoordinate structure in biological named entities.We draw NE tags from the BBN Entity TypeCorpus (Weischedel and Brunstein, 2005), whichdescribes 28 different entity types.
These in-clude the standard person, location and organizationclasses, as well person descriptions (generally occu-pations), NORP (National, Other, Religious or Po-litical groups), and works of art.
Some classes alsohave finer-grained subtypes, although we use onlythe coarse tags in our experiments.Clark and Curran (2007b) has a full descriptionof the C&C parser?s pre-existing features, to whichwe have added a number of novel NER-based fea-tures.
Many of these features generalise the headwords and/or POS tags that are already part of thefeature set.
The results of applying these featuresare described in Sections 5.3 and 6.The first feature is a simple lexical feature, de-scribing the NE tag of each token in the sentence.This feature, and all others that we describe here,are not active when the NE tag(s) are O, as there is noNER information from tokens that are not entities.The next group of features is based on the lo-cal tree (a parent and two child nodes) formed byevery grammar rule application.
We add a fea-ture where the rule being applied is combined withthe parent?s NE tag.
For example, when joiningtwo constituents2 : ?five, CD, CARD, N /N ?
and?Europeans, NNPS, NORP, N ?, the feature is:N ?
N /N N + NORPas the head of the constituent is Europeans.In the same way, we implement features that com-bine the grammar rule with the child nodes.
Thereare already features in the model describing eachcombination of the children?s head words and POStags, which we extend to include combinations with2These 4-tuples are the node?s head, POS, NE, and supertag.the NE tags.
Using the same example as above, oneof the new features would be:N ?
N /N N + CARD + NORPThe last group of features is based on the NEcategory spanned by each constituent.
We iden-tify constituents that dominate tokens that all havethe same NE tag, as these nodes will not cause a?crossing bracket?
with the named entity.
For ex-ample, the constituent Force contract, in theNP Air Force contract, spans two differentNE tags, and should be penalised by the model.
AirForce, on the other hand, only spans ORG tags, andshould be preferred accordingly.We also take into account whether the constituentspans the entire named entity.
Combining thesenodes with others of different NE tags should notbe penalised by the model, as the NE must combinewith the rest of the sentence at some point.These NE spanning features are implemented asthe grammar rule in combination with the parentnode or the child nodes.
For the former, one fea-ture is active when the node spans the entire entity,and another is active in other cases.
Similarly, thereare four features for the child nodes, depending onwhether neither, the left, the right or both nodes spanthe entire NE.
As an example, if the Air Forceconstituent were being joined with contract, thenthe child feature would be:N ?
N /N N + LEFT + ORG + Oassuming that there are more O tags to the right.5 ExperimentsOur experiments are run with the C&C CCG parser(Clark and Curran, 2007b), and will evaluate thechanges made to CCGbank, as well as the effective-ness of the NER features.
We train on Sections 02-21, and test on Section 00.340PREC RECALL F-SCOREOriginal 91.85 92.67 92.26NP corrected 91.22 92.08 91.65Table 2: Supertagging resultsPREC RECALL F-SCOREOriginal 85.34 84.55 84.94NP corrected 85.08 84.17 84.63Table 3: Parsing results with gold-standard POS tags5.1 SupertaggingBefore we begin full parsing experiments, we eval-uate on the supertagger alone.
The supertagger isan important stage of the CCG parsing process, itsresults will affect performance in later experiments.Table 2 shows that F-score has dropped by 0.61%.This is not surprising, as the conversion process hasincreased the ambiguity of supertags in NPs.
Previ-ously, a bare NP could only have a sequence of N /Ntags followed by a final N .
There are now morecomplex possibilities, equal to the Catalan numberof the length of the NP.5.2 Initial parsing resultsWe now compare parser performance on our NP cor-rected version of the corpus to that on original CCG-bank.
We are using the normal-form parser modeland report labelled precision, recall and F-score forall dependencies.
The results are shown in Table 3.The F-score drops by 0.31% in our new version ofthe corpus.
However, this comparison is not entirelyfair, as the original CCGbank test data does not in-clude the NP structure that the NP corrected model isbeing evaluated on.
Vadas and Curran (2007a) expe-rienced a similar drop in performance on Penn Tree-bank data, and noted that the F-score for NML andJJP brackets was about 20% lower than the overallfigure.
We suspect that a similar effect is causing thedrop in performance here.Unfortunately, there are no explicit NML and JJPbrackets to evaluate on in the CCG corpus, and so anNP structure only figure is difficult to compute.
Re-call can be calculated by marking those dependen-cies altered in the conversion process, and evaluatingonly on them.
Precision cannot be measured in thisPREC RECALL F-SCOREOriginal 83.65 82.81 83.23NP corrected 83.31 82.33 82.82Table 4: Parsing results with automatic POS tagsPREC RECALL F-SCOREOriginal 86.00 85.15 85.58NP corrected 85.71 84.83 85.27Table 5: Parsing results with NER featuresway, as NP dependencies remain undifferentiated inparser output.
The result is a recall of 77.03%, whichis noticeably lower than the overall figure.We have also experimented with using automat-ically assigned POS tags.
These tags are accuratewith an F-score of 96.34%, with precision 96.20%and recall 96.49%.
Table 4 shows that, unsur-prisingly, performance is lower without the gold-standard data.
The NP corrected model drops an ad-ditional 0.1% F-score over the original model, sug-gesting that POS tags are particularly important forrecovering internal NP structure.
Evaluating NP de-pendencies only, in the same manner as before, re-sults in a recall figure of 75.21%.5.3 NER features resultsTable 5 shows the results of adding the NER fea-tures we described in Section 4.
Performance hasincreased by 0.64% on both versions of the corpora.It is surprising that the NP corrected increase is notlarger, as we would expect the features to be lesseffective on the original CCGbank.
This is becauseincorrect right-branching NPs such as Air Force con-tract would introduce noise to the NER features.Table 6 presents the results of using automati-cally assigned POS and NE tags, i.e.
parsing rawtext.
The NER tagger achieves 84.45% F-score onall non-O classes, with precision being 78.35% andrecall 91.57%.
We can see that parsing F-scorehas dropped by about 2% compared to using gold-standard POS and NER data, however, the NER fea-tures still improve performance by about 0.3%.341PREC RECALL F-SCOREOriginal 83.92 83.06 83.49NP corrected 83.62 82.65 83.14Table 6: Parsing results with automatic POS and NE tags6 DepBank evaluationOne problem with the evaluation in the previous sec-tion, is that the original CCGbank is not expected torecover internal NP structure, making its task eas-ier and inflating its performance.
To remove thisvariable, we carry out a second evaluation againstthe Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al, 2003), as described in Clark andCurran (2007a).
Parser output is made similar to thegrammatical relations (GRs) of the Briscoe and Car-roll (2006) data, however, the conversion remainscomplex.
Clark and Curran (2007a) report an upperbound on performance, using gold-standard CCG-bank dependencies, of 84.76% F-score.This evaluation is particularly relevant for NPs, asthe Briscoe and Carroll (2006) corpus has been an-notated for internal NP structure.
With our new ver-sion of CCGbank, the parser will be able to recoverthese GRs correctly, where before this was unlikely.Firstly, we show the figures achieved using gold-standard CCGbank derivations in Table 7.
In the NPcorrected version of the corpus, performance has in-creased by 1.02% F-score.
This is a reversal of theresults in Section 5, and demonstrates that correctNP structure improves parsing performance, ratherthan reduces it.
Because of this increase to the up-per bound of performance, we are now even closerto a true formalism-independent evaluation.We now move to evaluating the C&C parser it-self and the improvement gained by the NER fea-tures.
Table 8 show our results, with the NP cor-rected version outperforming original CCGbank by0.92%.
Using the NER features has also caused anincrease in F-score, giving a total improvement of1.51%.
These results demonstrate how successfulthe correcting of NPs in CCGbank has been.Furthermore, the performance increase of 0.59%on the NP corrected corpus is more than the 0.25%increase on the original.
This demonstrates that NERfeatures are particularly helpful for NP structure.PREC RECALL F-SCOREOriginal 86.86 81.61 84.15NP corrected 87.97 82.54 85.17Table 7: DepBank gold-standard evaluationPREC RECALL F-SCOREOriginal 82.57 81.29 81.92NP corrected 83.53 82.15 82.84Original, NER 82.87 81.49 82.17NP corrected, NER 84.12 82.75 83.43Table 8: DepBank evaluation results7 ConclusionThe first contribution of this paper is the applicationof the Vadas and Curran (2007a) data to Combina-tory Categorial Grammar.
Our experimental resultshave shown that this more accurate representationof CCGbank?s NP structure increases parser perfor-mance.
Our second major contribution is the intro-duction of novel NER features, a source of semanticinformation previously unused in parsing.As a result of this work, internal NP structure isnow recoverable by the C&C parser, a result demon-strated by our total performance increase of 1.51%F-score.
Even when parsing raw text, without goldstandard POS and NER tags, our approach has re-sulted in performance gains.In addition, we have made possible further in-creases to NP structure accuracy.
New features cannow be implemented and evaluated in a CCG pars-ing context.
For example, bigram counts from a verylarge corpus have already been used in NP bracket-ing, and could easily be applied to parsing.
Sim-ilarly, additional supertagging features can now becreated to deal with the increased ambiguity in NPs.Downstream NLP components can now exploit thecrucial information in NP structure.AcknowledgementsWe would like to thank Mark Steedman andMatthew Honnibal for help with converting the NPdata to CCG; and the anonymous reviewers for theirhelpful feedback.
This work has been supported bythe Australian Research Council under DiscoveryProject DP0665973.342ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?265.Ted Briscoe and John Carroll.
2006.
Evaluating the accu-racy of an unlexicalized statistical parser on the PARCDepBank.
In Proceedings of the COLING/ACL 2006Main Conference Poster Sessions, pages 41?48.
Syd-ney, Australia.Ekaterina Buyko, Katrin Tomanek, and Udo Hahn.2007.
Resolution of coordination ellipses in biologicalnamed entities with conditional random fields.
In Pro-ceedings of the 10th Conference of the Pacific Associa-tion for Computational Linguistics (PACLING-2007),pages 163?171.
Melbourne, Australia.Stephen Clark and James R. Curran.
2007a.
Formalism-independent parser evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL-07), pages 248?255.
Prague, Czech Republic.Stephen Clark and James R. Curran.
2007b.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Michael Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguistics,29(4):589?637.Julia Hockenmaier.
2003.
Data and Models for Statis-tical Parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Julia Hockenmaier and Mark Steedman.
2005.
CCGbankmanual.
Technical Report MS-CIS-05-09, Departmentof Computer and Information Science, University ofPennsylvania.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Matthew Honnibal and James R. Curran.
2007.
Im-proving the complement/adjunct distinction in CCG-bank.
In Proceedings of the 10th Conference ofthe Pacific Association for Computational Linguistics(PACLING-07), pages 210?217.
Melbourne, Australia.Tracy Holloway King, Richard Crouch, Stefan Riezler,Mary Dalrymple, and Ronald M. Kaplan.
2003.
ThePARC700 dependency bank.
In Proceedings of the 4thInternational Workshop on Linguistically InterpretedCorpora (LINC-03).
Budapest, Hungary.Mark Lauer.
1995.
Corpus statistics meet the compoundnoun: Some empirical results.
In Proceedings of the33rd Annual Meeting of the Association for Computa-tional Linguistics, pages 47?54.
Cambridge, MA.Ian Lewin.
2007.
BaseNPs that contain gene names: do-main specificity and genericity.
In Biological, trans-lational, and clinical language processing workshop,pages 163?170.
Prague, Czech Republic.Mitchell Marcus, Beatrice Santorini, and MaryMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Preslav Nakov and Marti Hearst.
2005.
Search enginestatistics beyond the n-gram: Application to nouncompound bracketing.
In Proceedings of the 9th Con-ference on Computational Natural Language Learning(CoNLL-05), pages 17?24.
Ann Arbor, MI.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of seman-tic roles.
Computational Linguistics, 31(1):71?106.Mark Steedman.
2000.
The Syntactic Process.
MIT Press,Cambridge, MA.David Vadas and James R. Curran.
2007a.
Adding nounphrase structure to the Penn Treebank.
In Proceed-ings of the 45th Annual Meeting of the Association ofComputational Linguistics (ACL-07), pages 240?247.Prague, Czech Republic.David Vadas and James R. Curran.
2007b.
Large-scalesupervised models for noun phrase bracketing.
In Pro-ceedings of the 10th Conference of the Pacific Associa-tion for Computational Linguistics (PACLING-2007),pages 104?112.
Melbourne, Australia.Ralph Weischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus.
Technicalreport.343
