Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 31?36,Columbus, June 2008. c?2008 Association for Computational LinguisticsThe role of positive feedback in Intelligent Tutoring SystemsDavide FossatiDepartment of Computer ScienceUniversity of Illinois at ChicagoChicago, IL, USAdfossa1@uic.eduAbstractThe focus of this study is positive feedback inone-on-one tutoring, its computational model-ing, and its application to the design of moreeffective Intelligent Tutoring Systems.
A datacollection of tutoring sessions in the domainof basic Computer Science data structures hasbeen carried out.
A methodology based onmultiple regression is proposed, and some pre-liminary results are presented.
A prototype In-telligent Tutoring System on linked lists hasbeen developed and deployed in a college-level Computer Science class.1 IntroductionOne-on-one tutoring has been shown to be a veryeffective form of instruction (Bloom, 1984).
Theresearch community is working on discovering thecharacteristics of tutoring.
One of the goals is to un-derstand the strategies tutors use, in order to designeffective learning environments and tools to supportlearning.
Among the tools, particular attention isgiven to Intelligent Tutoring Systems (ITSs), whichare sophisticated software systems that can providepersonalized instruction to students, in some respectsimilar to one-on-one tutoring (Beck et al, 1996).Many of these systems have been shown to be veryeffective (Evens and Michael, 2006; Van Lehn et al,2005; Di Eugenio et al, 2005; Mitrovic?
et al, 2004;Person et al, 2001).
In many experiments, ITSs in-duced learning gains higher than those measured ina classroom environment, but lower than those ob-tained with one-on-one interactions with human tu-tors.
The belief of the research community is thatknowing more about human tutoring would help im-prove the design of ITSs.
In particular, the effectiveuse of natural language might be a key element.
Inmost of the studies mentioned above, systems withmore sophisticated language interfaces performedbetter than other experimental conditions.An important form of student-tutor interaction isfeedback.
Negative feedback can be provided by thetutor in response to students?
mistakes.
An effectiveuse of negative feedback can help the student cor-rect a mistake and prevent him/her from repeatingthe same or a similar mistake again, effectively pro-viding a learning opportunity to the student.
Posi-tive feedback is usually provided in response to somecorrect input from the student.
Positive feedback canhelp students reinforce the correct knowledge theyalready have, or successfully integrate new knowl-edge, if the correct input provided by the student wasoriginated by a random or tentative step.The goal of this study is to assess the relevance ofpositive feedback in tutoring, and build a computa-tional model of positive feedback that can be imple-mented in ITSs.
Even though some form of positivefeedback is present in many successful ITSs, the pre-dominant type of feedback generated by those sys-tems is negative feedback, as those systems are de-signed to react to students mistakes.
To date, thereis no systematic study of the role of positive feed-back in ITSs in the literature.
However, there isan increasing amount of evidence that suggests thatpositive feedback may be very important in enhanc-ing students?
learning.
In a detailed study in a con-trolled environment and domain, the letter patternextrapolation task, Corrigan-Halpern (2006) found31that subjects given positive feedback performed bet-ter in an assessment task than subjects receiving neg-ative feedback.
In another study on the same do-main, Lu (2007) found that the ratio of the positiveover negative messages in her corpus of expert tu-toring dialogues is about 4 to 1, and the ratio is evenhigher in the messages presented by her successfulITS modeled after an expert tutor, being about 10to 1.
In the dataset subject of this study, which ison a completely different domain ?Computer Sci-ence data structures?
such a high ratio of positiveover negative feedback messages still holds, in theorder of about 8 to 1.
In a recent study, Barrow et al(2008) showed that a version of their SQL-Tutor en-riched with positive feedback generation helped stu-dents learn faster than another version of the samesystem delivering negative feedback only.What might be the educational value of positivefeedback in ITSs?
First of all, positive feedbackmay be an effective motivational technique (Lepperet al, 1997).
Positive feedback can also have cog-nitive value.
In a problem solving setting, the stu-dent can make a tentative (maybe random) step to-wards the correct solution.
At this point, positivefeedback from the tutor may be important in help-ing the student consolidate this step and learn fromit.
Some researchers outlined the importance of self-explanation in learning (Chi, 1996; Renkl, 2002).Positive feedback has the potential to improve self-explanation, in terms of quantity and effectiveness.Another issue is how students perceive and acceptfeedback (Weaver, 2006), and, in the case of auto-mated tutoring systems, whether students read feed-back messages at all (Heift, 2001).
Positive feed-back might also make students more willing to ac-cept help and advice from the tutor.2 A study of human tutoringThe domain of this study is Computer Science datastructures, specifically linked lists, stacks, and bi-nary search trees.
A corpus of 54 one-on-one tutor-ing sessions has been collected.
Each individual stu-dent participated in only one tutoring session, witha tutor randomly assigned from a pool of two tutors.One of the tutors is an experienced Computer Sci-ence professor, with more than 30 years of teachingexperience.
The other tutor is a senior undergrad-Topic Tutor Avg Stdev t df PListNovice .09 .22 -2.00 23 .057Expert .18 .26 -3.85 29 < .01Both .14 .25 -4.24 53 < .01None .01 .15 -0.56 52 nsiList .09 .17 -3.04 32 < .01StackNovice .35 .25 -6.90 23 < .01Expert .27 .22 -6.15 23 < .01Both .31 .24 -9.20 47 < .01No .05 .17 -2.15 52 < .05TreeNovice .33 .26 -6.13 23 < .01Expert .29 .23 -6.84 29 < .01Both .30 .24 -9.23 53 < .01No .04 .16 -1.78 52 nsTable 1: Learning gains and t-test statisticsuate student in Computer Science, with only onesemester of previous tutoring experience.
The tutor-ing sessions have been videotaped and transcribed.Student took a pre-test right before the tutoring ses-sion, and a post-test immediately after.
An addi-tional group of 53 students (control group) took thepre and post tests, but they did not participate in a tu-toring session, and attended a lecture about a totallyunrelated topic instead.Paired samples t-tests revealed that post-testscores are significantly higher than pre-test scoresin the two tutored conditions for all the topics, ex-cept for linked lists with the less experienced tu-tor, where the difference is only marginally signifi-cant.
If the two tutored groups are aggregated, thereis significant difference for all the topics.
Studentsin the control group did not show significant learn-ing for linked lists and binary search trees, and onlymarginally significant learning for stacks.
Means,standard deviations, and t-test statistic values are re-ported in Table 1.There is no significant difference between the twotutored conditions in terms of learning gain, ex-pressed as the difference between post-score andpre-score.
This is revealed by ANOVA betweenthe two groups of students in the tutored condition.For lists, F (1, 53) = 1.82, P = ns.
For stacks,F (1, 47) = 1.35, P = ns.
For trees, F (1, 53) =0.32, P = ns.The learning gain of students that received tutor-ing is significantly higher than the learning gain ofthe students in the control group, for all the topics.32This is showed by ANOVA between the group oftutored students (with both tutors) and the controlgroup.
For lists, F (1, 106) = 11.0, P < 0.01.
Forstacks, F (1, 100) = 41.4, P < 0.01.
For trees,F (1, 106) = 43.9, P < 0.01.
Means and standarddeviations are reported in Table 1.3 Regression-based analysisThe distribution of scores across sessions shows a lotof variability (Table 1).
In all the conditions, thereare sessions with very high learning gains, and ses-sions with very low ones.
This observation and theprevious results suggest a new direction for subse-quent analysis: instead of looking at the character-istics of a particular tutor, it is better to look at thefeatures that discriminate the most successful ses-sions from the least successful ones.
As advocatedin (Ohlsson et al, 2007), a sensible way to do thatis to adopt an approach based on multiple regressionof learning outcomes per tutoring session onto thefrequencies of the different features.
The followinganalysis has been done adopting a hierarchical, lin-ear regression model.Prior knowledge First of all, we want to factor outthe effect of prior knowledge, measured by the pre-test score.
A linear regression model reveals strongeffect of pre-test scores on learning gain (Table 2).However, the R2 values show that there is a lot ofvariance left to be explained, especially for lists andstacks, although not so much for trees.
Notice thatthe ?
weights are negative.
That means studentswith higher pre-test scores learn less then studentswith lower pre-test scores.
A possible explanationis that students with more previous knowledge haveless learning opportunity than students with less pre-vious knowledge.Time on task Another variable that is recognizedas important by the educational research commu-nity is time on task, and we can approximate it withthe length of the tutoring session.
In the hierarchi-cal regression model, session length follows pre-testscore.
Surprisingly, session length has a significanteffect only on linked lists (Table 2).Student activity Another hypothesis is that thedegree of student activity, in the sense of the amountof student?s participation in the discussion, mightrelate to learning (Lepper et al, 1997; Chi et al,2001).
To test this hypothesis, the following defi-nition of student activity has been adopted:student activity =# of turns?
# of short turnssession lengthTurns are the sequences of uninterrupted speech ofthe student.
Short turns are the student turns shorterthan three words.
The regression analysis revealedno significant effect of this measure of students?
ac-tivity on learning gain.Feedback The dataset has been manually anno-tated for episodes where positive or negative feed-back is delivered.
All the protocols have beenannotated by one coder, and some of them havebeen double-coded by a second one (intercoderagreement: kappa = 0.67).
Examples of feedbackepisodes are reported in Figure 1.The number of positive feedback episodes and thenumber of negative feedback episodes have been in-troduced in the regression model (Table 2).
Themodel showed a significant effect of feedback forlinked lists and stacks, but no significant effect ontrees.
Interestingly, the effect of positive feedback ispositive, but the effect of negative feedback is nega-tive, as can be seen by the sign of the ?
value.4 A tutoring system for linked listsA new ITS in the domain of linked lists, iList, isbeing developed (Figure 2).The iList system is based on the constraint-baseddesign paradigm.
Originally developed from a cog-nitive theory of how people might learn from per-formance errors (Ohlsson, 1996), constraint-basedmodeling has grown into a methodology used tobuild full-fledged ITSs, and an alternative to themodel tracing approach adopted by many ITSs.
In aconstraint-based system, domain knowledge is mod-eled with a set of constraints, logic units composedof a relevance condition and a satisfaction condi-tion.
A constraint is irrelevant when the relevancecondition is not satisfied; it is satisfied when bothrelevance and satisfaction conditions are satisfied; itis violated when the relevance condition is satisfiedbut the satisfaction condition is not.
In the contextof tutoring, constraints are matched against student33T: do you see a problem?T: I have found the node a@l, see here I found the node b@l, andthen I put g@l in after it.Begin + T: here I have found the node a@l and now the link I have tochange is +...S: ++ you have to link e@l <over xxx.> [>]End + T: [<] <yeah> I have to go back to this one.S: *mmhmT: so I *uh once I?m here, this key is here, I can?t go backwards.Begin - S: <so you> [>] <you won?t get the same> [//] would you get thesame point out of writing t@l close to c@l at the top?T: oh, t@l equals c@l.T: no because you would have a type mismatch.End - T: t@l <is a pointer> [//] is an address, and this is contents.Figure 1: Positive and negative feedback (T = tutor, S = student)Topic Model Predictor ?
R2 PList1 Pre-test -.45 .18 < .052Pre-test -.40.28< .05Session length .35 < .053Pre-test -.35.36< .05Session length .33 .05+ feedback .46 .05- feedback -.53 < .05Stack1 Pre-test -.53 .26 < .012Pre-test -.52.24< .01Session length .05 ns3Pre-test -.58.33< .01Session length .01 ns+ feedback .61 < .05- feedback -.55 < .05Tree1 Pre-test -.79 .61 < .012Pre-test -.78.60< .01Session length .03 ns3Pre-test -.77.59< .01Session length .04 ns+ feedback .06 ns- feedback -.12 nsAll1 Pre-test -.52 .26 < .012Pre-test -.54.29< .01Session length .20 < .053Pre-test -.57.32< .01Session length .16 .06+ feedback .30 < .05- feedback -.23 .05Table 2: Linear regressionFigure 2: The iList systemsolutions.
Satisfied constraints correspond to knowl-edge that students have acquired, whereas violatedconstraints correspond to gaps or incorrect knowl-edge.
An important feature is that there is no needfor an explicit model of students?
mistakes, as op-posed to buggy rules in model tracing.
The possibleerrors are implicitly specified as the possible waysin which constraints can be violated.The architecture of iList includes a problemmodel, a constraint evaluator, a feedback manager,and a graphical user interface.
Student model andpedagogical module, important components of acomplete ITS (Beck et al, 1996), have not beenimplemented yet, and will be included in a futureversion.
Currently, the system provides only simplenegative feedback in response to students?
mistakes,as customary in constraint-based ITSs.A first version of the system has been deployed34into a Computer Science class of a partner institu-tion.
33 students took a pre-test before using thesystem, and a post-test immediately afterwards.
Thestudents also filled in a questionnaire about theirsubjective impressions on the system.
The interac-tion of the students with the system was logged.T-test on test scores revealed that students didlearn during the interaction with iList (Table 1).
Thelearning gain is somewhere in between the one ob-served in the control condition and the one of thetutored condition.
ANOVA revealed no significantdifference between the control group and the iListgroup, nor between the iList group and the tutoredgroup, whereas the difference between control andtutored groups is significant.A preliminary analysis of the questionnaires re-vealed that students felt that iList helped them learnlinked lists to a moderate degree (on a 1 to 5 scale:avg = 2.88, stdev = 1.18), but working with iListwas interesting to them (avg = 4.0, stdev = 1.27).Students found the feedback provided by the sys-tem somewhat repetitive (avg = 3.88, stdev = 1.18),which is not surprising given the simple template-based generation mechanism.
Also, the feedbackwas considered not very useful (avg = 2.31, 1.23),but at least not too misleading (avg = 2.22, stdev= 1.21).
Interestingly, students declared that theyread the feedback provided by the system (avg =4.25, stdev = 1.05), but the logs of the system re-veal just the opposite.
In fact, on average, studentsread feedback messages for 3.56 seconds (stdev =2.66 seconds), resulting in a reading speed of 532words/minute (stdev = 224 words/minute).
Accord-ing to Carver?s taxonomy (Carver, 1990), such speedindicates a quick skimming of the text, whereasreading for learning typically has a lower speed, inthe order of 200 words/minute.5 Future workThe main goal of this research is to build a compu-tational model of positive feedback that can be usedin ITSs.
The study of empirical data and the sys-tem design and development will proceed in paral-lel, helping and informing each other as new resultsare obtained.The conditions and the modalities of positivefeedback delivery by tutors will be investigated fromthe human tutoring dataset.
To do so, more codingcategories will be defined, and the data will be anno-tated with these categories.
The results of the statis-tical analysis over the first few coding categories willbe used to guide the definition of more categories,that will be in turn used to annotate the data, andso on.
An example of potential coding category iswhether the student?s action that triggered the feed-back was prompted by the tutor or volunteered bythe student.
Another example is whether the feed-back?s content was a repetition of what the studentjust said or included additional explanation.The first experiment with iList provided a com-prehensive log of the students?
interaction with thesystem.
Additional analysis of this data will be im-portant, especially because the nature of the interac-tion of a student with a computer system differs fromthe interaction with a human tutor.
When workingwith a computer system, most of the interaction hap-pens through a graphical interface, instead of natu-ral language dialogue.
Also, the interaction with acomputer system is mostly student-driven, whereasour human protocols show a clear predominance ofthe tutor in the conversation.
In the CS protocols,on average, 94% of the words belong to the tutor,and most of the tutors?
discourse is some form of di-rect instruction.
On the other hand, the interactionwith the system will mostly consist of actions thatstudents make to solve the problems that they willbe asked to solve, with few interventions from thesystem.
An interesting analysis that could be doneon the logs is the discovery of sequential patterns us-ing data mining algorithms, such as MS-GSP (Liu,2006).
Such patterns could then be regressed againstlearning outcomes, in order to assess their correla-tion with learning.After the relevant features are discovered, a com-putational model of positive feedback will be builtand integrated into iList.
The model will en-code knowledge extracted with machine learning ap-proaches, and such knowledge will inform a dis-course planner, responsible of organizing and gen-erating appropriate positive feedback.
The choicheof the specific machine learning and discourse plan-ning methods will require extensive empirical inves-tigation.
Specifically, among the different machinelearning methods, some are able to provide somesort of human-readable symbolic model, which can35be inspected to gain some insights on how the modelworks.
Decision trees and association rules belongto this category.
Other methods provide a less read-able, black-box type of models, but they may be veryuseful and effective as well.
Examples of such meth-ods include Neural Networks and Markov Models.The ultimate goal of this research is to get both an ef-fective model and to gain insights on tutoring.
Thus,both classes of machine learning methods will betried, with the goal of finding a balance betweenmodel effectiveness and model readability.Finally, the system with enhanced feedback capa-bilities will be deployed and evaluated.AcknowledgmentsThis work is supported by award N00014-07-1-0040from the Office of Naval Research, and additionallyby awards ALT-0536968 and IIS-0133123 from theNational Science Foundation.ReferencesDevon Barrow, Antonija Mitrovic?, Stellan Ohlsson, andMichael Grimley.
2008.
Assessing the impact of pos-itive feedback in constraint-based tutors.
In ITS 2008,The 9th International Conference on Intelligent Tutor-ing Systems, Montreal, Canada.Joseph Beck, Mia Stern, and Erik Haugsjaa.
1996.Applications of AI in education.
ACM crossroads.http://www.acm.org/crossroads/xrds3-1/aied.html.B.
S. Bloom.
1984.
The 2 sigma problem: The search formethods of group instruction as effective as one-to-onetutoring.
Educational Researcher, 13:4?16.Ronald P. Carver.
1990.
Reading Rate: A Review ofResearch and Theory.
Academic Press, San Diego,CA.Michelene T.H.
Chi, Stephanie A. Siler, Heisawn Jeong,Takashi Yamauchi, and Robert G. Hausmann.
2001.Learning from human tutoring.
Cognitive Science,25:471?533.Michelene T.H.
Chi.
1996.
Constructing self-explanations and scaffolded explanations in tutoring.Applied Cognitive Psychology, 10:33?49.Andrew Corrigan-Halpern.
2006.
Feedback in ComplexLearning: Considering the Relationship Between Util-ity and Processing Demands.
Ph.D. thesis, Universityof Illinois at Chicago.Barbara Di Eugenio, Davide Fossati, Dan Yu, SusanHaller, and Michael Glass.
2005.
Aggregation im-proves learning: Experiments in natural language gen-eration for intelligent tutoring systems.
In ACL05,Proceedings of the 42nd Meeting of the Associationfor Computational Linguistics, Ann Arbor, MI.Martha Evens and Joel Michael.
2006.
One-on-oneTutoring by Humans and Machines.
Mahwah, NJ:Lawrence Erlbaum Associates.Trude Heift.
2001.
Error-specific and individualizedfeedback in a web-based language tutoring system: Dothey read it?
ReCALL Journal, 13(2):129?142.M.
R. Lepper, M. Drake, and T. M. O?Donnell-Johnson.1997.
Scaffolding techniques of expert human tutors.In K. Hogan and M. Pressley, editors, Scaffolding stu-dent learning: Instructional approaches and issues,pages 108?144.
Brookline Books, New York.Bing Liu.
2006.
Web Data Mining.
Springer, Berlin.Xin Lu.
2007.
Expert Tutoring and Natural LanguageFeedback in Intelligent Tutoring Systems.
Ph.D. thesis,University of Illinois at Chicago.Antonija Mitrovic?, Pramuditha Suraweera, Brent Mar-tin, and A. Weerasinghe.
2004.
DB-suite: Ex-periences with three intelligent, web-based databasetutors.
Journal of Interactive Learning Research,15(4):409?432.Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-vide Fossati, Xin Lu, and Trina C. Kershaw.
2007.Beyond the code-and-count analysis of tutoring dia-logues.
In AIED07, 13th International Conference onArtificial Intelligence in Education.Stellan Ohlsson.
1996.
Learning from performance er-rors.
Psychological Review, 103:241?262.N.
K. Person, A. C. Graesser, L. Bautista, E. C. Mathews,and the Tutoring Research Group.
2001.
Evaluatingstudent learning gains in two versions of AutoTutor.In J. D. Moore, C. L. Redfield, and W. L. Johnson,editors, Artificial intelligence in education: AI-ED inthe wired and wireless future, pages 286?293.
Amster-dam: IOS Press.Alexander Renkl.
2002.
Learning from worked-out ex-amples: Instructional explanations supplement self-explanations.
Learning and Instruction, 12:529?556.Kurt Van Lehn, Collin Lynch, Kay Schulze, Joel A.Shapiro, Robert H. Shelby, Linwood Taylor, Don J.Treacy, Anders Weinstein, and Mary C. Wintersgill.2005.
The Andes physics tutoring system: Five yearsof evaluations.
In G. I. McCalla and C. K. Looi, ed-itors, Artificial Intelligence in Education Conference.Amsterdam: IOS Press.Melanie R. Weaver.
2006.
Do students value feed-back?
Student perceptions of tutors?
written re-sponses.
Assessment and Evaluation in Higher Edu-cation, 31(3):379?394.36
