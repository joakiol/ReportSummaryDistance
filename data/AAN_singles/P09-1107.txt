Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 949?957,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPIncremental HMM Alignment for MT System CombinationChi-Ho LiMicrosoft Research Asia49 Zhichun Road, Beijing, Chinachl@microsoft.comXiaodong HeMicrosoft ResearchOne Microsoft Way, Redmond, USAxiaohe@microsoft.comYupeng LiuHarbin Institute of Technology92 Xidazhi Street, Harbin, Chinaypliu@mtlab.hit.edu.cnNing XiNanjing University8 Hankou Road, Nanjing, Chinaxin@nlp.nju.edu.cnAbstractInspired by the incremental TER align-ment, we re-designed the Indirect HMM(IHMM) alignment, which is one of thebest hypothesis alignment methods forconventional MT system combination, inan incremental manner.
One crucial prob-lem of incremental alignment is to align ahypothesis to a confusion network (CN).Our incremental IHMM alignment is im-plemented in three different ways: 1) treatCN spans as HMM states and define statetransition as distortion over covered n-grams between two spans; 2) treat CNspans as HMM states and define state tran-sition as distortion over words in compo-nent translations in the CN; and 3) usea consensus decoding algorithm over onehypothesis and multiple IHMMs, each ofwhich corresponds to a component trans-lation in the CN.
All these three ap-proaches of incremental alignment basedon IHMM are shown to be superior to bothincremental TER alignment and conven-tional IHMM alignment in the setting ofthe Chinese-to-English track of the 2008NIST Open MT evaluation.1 IntroductionWord-level combination using confusion network(Matusov et al (2006) and Rosti et al (2007)) is awidely adopted approach for combining MachineTranslation (MT) systems?
output.
Word align-ment between a backbone (or skeleton) translationand a hypothesis translation is a key problem inthis approach.
Translation Edit Rate (TER, Snoveret al (2006)) based alignment proposed in Simet al (2007) is often taken as the baseline, anda couple of other approaches, such as the Indi-rect Hidden Markov Model (IHMM, He et al(2008)) and the ITG-based alignment (Karakos etal.
(2008)), were recently proposed with better re-sults reported.
With an alignment method, eachhypothesis is aligned against the backbone and allthe alignments are then used to build a confusionnetwork (CN) for generating a better translation.However, as pointed out by Rosti et al (2008),such a pair-wise alignment strategy will producea low-quality CN if there are errors in the align-ment of any of the hypotheses, no matter how goodthe alignments of other hypotheses are.
For ex-ample, suppose we have the backbone ?he buys acomputer?
and two hypotheses ?he bought a lap-top computer?
and ?he buys a laptop?.
It will benatural for most alignment methods to produce thealignments in Figure 1a.
The alignment of hypoth-esis 2 against the backbone cannot be consideredan error if we consider only these two translations;nevertheless, when added with the alignment ofanother hypothesis, it produces the low-qualityCN in Figure 1b, which may generate poor trans-lations like ?he bought a laptop laptop?.
While itcould be argued that such poor translations are un-likely to be selected due to language model, thisCN does disperse the votes to the word ?laptop?
totwo distinct arcs.Rosti et al (2008) showed that this problem canbe rectified by incremental alignment.
If hypoth-esis 1 is first aligned against the backbone, theCN thus produced (depicted in Figure 2a) is thenaligned to hypothesis 2, giving rise to the good CNas depicted in Figure 2b.1 On the other hand, the1Note that this CN may generate an incomplete sentence?he bought a?, which is nevertheless unlikely to be selectedas it leads to low language model score.949Figure 1: An example bad confusion network dueto pair-wise alignment strategycorrect result depends on the order of hypotheses.If hypothesis 2 is aligned before hypothesis 1, thefinal CN will not be good.
Therefore, the obser-vation in Rosti et al (2008) that different orderof hypotheses does not affect translation quality iscounter-intuitive.This paper attempts to answer two questions: 1)as incremental TER alignment gives better perfor-mance than pair-wise TER alignment, would theincremental strategy still be better than the pair-wise strategy if the TER method is replaced byanother alignment method?
2) how does transla-tion quality vary for different orders of hypothesesbeing incrementally added into a CN?
For ques-tion 1, we will focus on the IHMM alignmentmethod and propose three different ways of imple-menting incremental IHMM alignment.
Our ex-periments will also try several orders of hypothe-ses in response to question 2.This paper is structured as follows.
After set-ting the notations on CN in section 2, we willfirst introduce, in section 3, two variations of thebasic incremental IHMM model (IncIHMM1 andIncIHMM2).
In section 4, a consensus decodingalgorithm (CD-IHMM) is proposed as an alterna-tive way to search for the optimal alignment.
Theissues of alignment normalization and the order ofhypotheses being added into a CN are discussed insections 5 and 6 respectively.
Experiment resultsand analysis are presented in section 7.Figure 2: An example good confusion networkdue to incremental alignment strategy2 Preliminaries: Notation on ConfusionNetworkBefore the elaboration of the models, let us firstclarify the notation on CN.
A CN is usually de-scribed as a finite state graph with many spans.Each span corresponds to a word position and con-tains several arcs, each of which represents an al-ternative word (could be the empty symbol , ?)
atthat position.
Each arc is also associated with Mweights in an M -way system combination task.Follow Rosti et al (2007), the i-th weight of anarc is ?r 11+r , where r is the rank of the hypothe-sis in the i-th system that votes for the word repre-sented by the arc.
This conception of CN is calledthe conventional or compact form of CN.
The net-works in Figures 1b and 2b are examples.On the other hand, as a CN is an integrationof the skeleton and all hypotheses, it can be con-ceived as a list of the component translations.
Forexample, the CN in Figure 2b can be convertedto the form in Figure 3.
In such an expanded ortabular form, each row represents a componenttranslation.
Each column, which is equivalent toa span in the compact form, comprises the alter-native words at a word position.
Thus each cellrepresents an alternative word at certain word po-sition voted by certain translation.
Each row is as-signed the weight 11+r , where r is the rank of thetranslation of some MT system.
It is assumed thatall MT systems are weighted equally and thus the950Figure 3: An example of confusion network in tab-ular formrank-based weights from different system can becompared to each other without adjustment.
Theweight of a cell is the same as the weight of thecorresponding row.
In this paper the elaborationof the incremental IHMM models is based on suchtabular form of CN.Let EI1 = (E1 .
.
.
EI) denote the backbone CN,and e?J1 = (e?1 .
.
.
e?J) denote a hypothesis beingaligned to the backbone.
Each e?j is simply a wordin the target language.
However, each Ei is a span,or a column, of the CN.
We will also use E(k) todenote the k-th row of the tabular form CN, andEi(k) to denote the cell at the k-th row and thei-th column.
W (k) is the weight for E(k), andWi(k) = W (k) is the weight for Ei(k).
pi(k)is the normalized weight for the cell Ei(k), suchthat pi(k) = Wi(k)?i Wi(k).
Note that E(k) containsthe same bag-of-words as the k-th original trans-lation, but may have different word order.
Notealso that E(k) represents a word sequence withinserted empty symbols; the sequence with all in-serted symbols removed is known as the compactform of E(k).3 The Basic IncIHMM ModelA na?
?ve application of the incremental strategy toIHMM is to treat a span in the CN as an HMMstate.
Like He et al (2008), the conditional prob-ability of the hypothesis given the backbone CNcan be decomposed into similarity model and dis-tortion model in accordance with equation 1p(e?J1 |EI1) =?aJ1J?j=1[p(aj |aj?1, I)p(e?j |eaj )] (1)The similarity between a hypothesis word e?j anda span Ei is simply a weighted sum of the similar-ities between e?j and each word contained in Ei asequation 2:p(e?j |Ei) =?Ei(k)?Eipi(k) ?
p(e?j |Ei(k)) (2)The similarity between two words is estimated inexactly the same way as in conventional IHMMalignment.As to the distortion model, the incrementalIHMM model also groups distortion parametersinto a few ?buckets?
:c(d) = (1 + |d?
1|)?KThe problem in incremental IHMM is when to ap-ply a bucket.
In conventional IHMM, the transi-tion from state i to j has probability:p?
(j|i, I) = c(j ?
i)?Il=1 c(l ?
i)(3)It is tempting to apply the same formula to thetransitions in incremental IHMM.
However, thebackbone in the incremental IHMM has a specialproperty that it is gradually expanding due to theinsertion operator.
For example, initially the back-bone CN contains the option ei in the i-th span andthe option ei+1 in the (i+1)-th span.
After the firstround alignment, perhaps ei is aligned to the hy-pothesis word e?j , ei+1 to e?j+2, and the hypothesisword e?j+1 is left unaligned.
Then the consequentCN have an extra span containing the option e?j+1inserted between the i-th and (i + 1)-th spans ofthe initial CN.
If the distortion buckets are appliedas in equation 3, then in the first round alignment,the transition from the span containing ei to thatcontaining ei+1 is based on the bucket c(1), butin the second round alignment, the same transitionwill be based on the bucket c(2).
It is therefore notreasonable to apply equation 3 to such graduallyextending backbone as the monotonic alignmentassumption behind the equation no longer holds.There are two possible ways to tackle this prob-lem.
The first solution estimates the transitionprobability as a weighted average of different dis-tortion probabilities, whereas the second solutionconverts the distortion over spans to the distortionover the words in each hypothesis E(k) in the CN.3.1 Distortion Model 1: simple weighting ofcovered n-gramsDistortion Model 1 shifts the monotonic alignmentassumption from spans of CN to n-grams coveredby state transitions.
Let us illustrate this point withthe following examples.In conventional IHMM, the distortion probabil-ity p?
(i + 1|i, I) is applied to the transition fromstate i to i+1 given I states because such transition951jumps across only one word, viz.
the i-th word ofthe backbone.
In incremental IHMM, suppose thei-th span covers two arcs ea and ?, with probabili-ties p1 and p2 = 1?
p1 respectively, then the tran-sition from state i to i+ 1 jumps across one word(ea) with probability p1 and jumps across nothingwith probability p2.
Thus the transition probabil-ity should be p1 ?
p?
(i+ 1|i, I) + p2 ?
p?
(i|i, I).Suppose further that the (i + 1)-th span coverstwo arcs eb and ?, with probabilities p3 and p4 re-spectively, then the transition from state i to i+ 2covers 4 possible cases:1. nothing (??)
with probability p2 ?
p4;2. the unigram ea with probability p1 ?
p4;3. the unigram eb with probability p2 ?
p3;4. the bigram eaeb with probability p1 ?
p3.Accordingly the transition probability should bep2p4p?
(i|i, I) + p1p3p?
(i+ 2|i, I) +(p1p4 + p2p3)p?
(i+ 1|i, I).The estimation of transition probability can begeneralized to any transition from i to i?
by ex-panding all possible n-grams covered by the tran-sition and calculating the corresponding probabil-ities.
We enumerate all possible cell sequencesS(i, i?)
covered by the transition from span i toi?
; each sequence is assigned the probabilityP i?i =i?
?1?q=ipq(k).where the cell at the i?-th span is on some rowE(k).
Since a cell may represent an empty word,a cell sequence may represent an n-gram where0 ?
n ?
i?
?
i (or 0 ?
n ?
i ?
i?
in backwardtransition).
We denote |S(i, i?
)| to be the length ofn-gram represented by a particular cell sequenceS(i, i?).
All the cell sequences S(i, i?)
can be clas-sified, with respect to the length of correspondingn-grams, into a set of parameters where each ele-ment (with a particular value of n) has the proba-bilityP i?i (n; I) =?|S(i,i?
)|=nP i?i .The probability of the transition from i to i?
is:p(i?|i, I) =?n[P i?i (n; I) ?
p?
(i+ n|i, I)].
(4)That is, the transition probability of incrementalIHMM is a weighted sum of probabilities of ?n-gram jumping?, defined as conventional IHMMdistortion probabilities.However, in practice it is not feasible to ex-pand all possible n-grams covered by any transi-tion since the number of n-grams grows exponen-tially.
Therefore a length limit L is imposed suchthat for all state transitions where |i?
?
i| ?
L, thetransition probability is calculated as equation 4,otherwise it is calculated by:p(i?|i, I) = maxq p(i?|q, I) ?
p(q|i, I)for some q between i and i?.
In other words, theprobability of longer state transition is estimatedin terms of the probabilities of transitions shorteror equal to the length limit.2 All the state transi-tions can be calculated efficiently by dynamic pro-gramming.A fixed value P0 is assigned to transitions tonull state, which can be optimized on held-outdata.
The overall distortion model is:p?
(j|i, I) ={P0 if j is null state(1?
P0)p(j|i, I) otherwise3.2 Distortion Model 2: weighting ofdistortions of component translationsThe cause of the problem of distortion over CNspans is the gradual extension of CN due to theinserted empty words.
Therefore, the problemwill disappear if the inserted empty words are re-moved.
The rationale of Distortion Model 2 isthat the distortion model is defined over the ac-tual word sequence in each component translationE(k).Distortion Model 2 implements a CN in such away that the real position of the i-th word of the k-th component translation can always be retrieved.The real position of Ei(k), ?
(i, k), refers to theposition of the word represented by Ei(k) in thecompact form of E(k) (i.e.
the form without anyinserted empty words), or, if Ei(k) represents anempty word, the position of the nearest precedingnon-empty word.
For convenience, we also denoteby ??
(i, k) the null state associated with the stateof the real word ?
(i, k).
Similarly, the real length2This limit L is also imposed on the parameter I in distor-tion probability p?
(i?|i, I), because the value of I is growinglarger and larger during the incremental alignment process.
Iis defined as L if I > L.952of E(k), L(k), refers to the number of non-emptywords of E(k).The transition from span i?
to i is then definedasp(i|i?)
= 1?k W (k)?k[W (k) ?
pk(i|i?)]
(5)where k is the row index of the tabular form CN.Depending on Ei(k) and Ei?
(k), pk(i|i?)
iscomputed as follows:1. if both Ei(k) and Ei?
(k) represent realwords, thenpk(i|i?)
= p?(?
(i, k)|?
(i?, k), L(k))where p?
refers to the conventional IHMMdistortion probability as defined by equa-tion 3.2. if Ei(k) represents a real word but Ei?
(k) theempty word, thenpk(i|i?)
= p?(?
(i, k)|??
(i?, k), L(k))Like conventional HMM-based word align-ment, the probability of the transition from anull state to a real word state is the same asthat of the transition from the real word stateassociated with that null state to the other realword state.
Therefore,p?(?
(i, k)|??
(i?, k), L(k)) =p?(?
(i, k)|?
(i?, k), L(k))3. if Ei(k) represents the empty word butEi?
(k) a real word, thenpk(i|i?)
={P0 if?
(i, k) = ?
(i?, k)P0P?(i|i?
; k) otherwisewhere P?(i|i?
; k) = p?(?
(i, k)|?
(i?, k), L(k)).The second option is due to the constraint thata null state is accessible only to itself or thereal word state associated with it.
Therefore,the transition from i?
to i is in fact composedof the first transition from i?
to ?
(i, k) and thesecond transition from ?
(i, k) to the null stateat i.4.
if both Ei(k) and Ei?
(k) represent the emptyword, then, with similar logic as cases 2and 3,pk(i|i?)
={P0 if?
(i, k) = ?
(i?, k)P0P?(i|i?
; k) otherwise4 Incremental Alignment usingConsensus Decoding over MultipleIHMMsThe previous section describes an incrementalIHMM model in which the state space is based onthe CN taken as a whole.
An alternative approachis to conceive the rows (component translations)in the CN as individuals, and transforms the align-ment of a hypothesis against an entire network tothat against the individual translations.
Each in-dividual translation constitutes an IHMM and theoptimal alignment is obtained from consensus de-coding over these multiple IHMMs.Alignment over multiple sequential patterns hasbeen investigated in different contexts.
For ex-ample, Nair and Sreenivas (2007) proposed multi-pattern dynamic time warping (MPDTW) to alignmultiple speech utterances to each other.
How-ever, these methods usually assume that the align-ment is monotonic.
In this section, a consensusdecoding algorithm that searches for the optimal(non-monotonic) alignment between a hypothesisand a set of translations in a CN (which are alreadyaligned to each other) is developed as follows.A prerequisite of the algorithm is a functionfor converting a span index to the correspondingHMM state index of a component translation.
Thetwo functions ?
and ??
s defined in section 3.2 areused to define a new function:??
(i, k) ={??
(i, k) if Ei(k) is null?
(i, k) otherwiseAccordingly, given the alignment aJ1 = a1 .
.
.
aJof a hypothesis (with J words) against a CN(where each aj is an index referring to the spanof the CN), we can obtain the alignment a?k =??
(a1, k) .
.
.
??
(aJ , k) between the hypothesis andthe k-th row of the tabular CN.
The real lengthfunction L(k) is also used to obtain the number ofnon-empty words of E(k).Given the k-th row of a CN, E(k), an IHMM?
(k) is formed and the cost of the pair-wise align-ment, a?k, between a hypothesis h and ?
(k) is de-fined as:C(a?k;h, ?
(k)) = ?
logP (a?k|h, ?
(k)) (6)The cost of the alignment of h against a CN is thendefined as the weighted sum of the costs of the Kalignments a?k:C(a;h,?)
=?kW (k)C(a?k;h, ?
(k))953= ?
?kW (k) logP (a?k|h, ?
(k))where ?
= {?
(k)} is the set of pair-wise IHMMs,and W (k) is the weight of the k-th row.
The op-timal alignment a?
is the one that minimizes thiscost:a?
= argmaxa?kW (k) logP (a?k|h, ?
(k))= argmaxa?kW (k)[?j[logP (??
(aj , k)|??
(aj?1, k), L(k)) +logP (ej |Ei(k))]]= argmaxa?j[?kW (k) logP (??
(aj , k)|??
(aj?1, k), L(k)) +?kW (k) logP (ej |Ei(k))]= argmaxa?j[logP ?
(aj |aj?1) +logP ?
(ej |Eaj )]A Viterbi-like dynamic programming algorithmcan be developed to search for a?
by treating CNspans as HMM states, with a pseudo emissionprobability asP ?
(ej |Eaj ) =K?k=1P (ej |Eaj (k))W (k)and a pseudo transition probability asP ?
(j|i) =K?k=1P (??
(j, k)|??
(i, k), L(k))W (k)Note that P ?
(ej |Eaj ) and P ?
(j|i) are not trueprobabilities and do not have the sum-to-one prop-erty.5 Alignment NormalizationAfter alignment, the backbone CN and the hypoth-esis can be combined to form an even larger CN.The same principles and heuristics for the con-struction of CN in conventional system combina-tion approaches can be applied.
Our incremen-tal alignment approaches adopt the same heuris-tics for alignment normalization stated in He et al(2008).
There is one exception, though.
All 1-N mappings are not converted to N ?
1 ?-1 map-pings since this conversion leads to N ?
1 inser-tion in the CN and therefore extending the net-work to an unreasonable length.
The Viterbi align-ment is abandoned if it contains an 1-N mapping.The best alignment which contains no 1-N map-ping is searched in the N-Best alignments in a wayinspired by Nilsson and Goldberger (2001).
Forexample, if both hypothesis words e?1 and e?2 arealigned to the same backbone span E1, then allalignments aj={1,2} = i (where i 6= 1) will beexamined.
The alignment leading to the least re-duction of Viterbi probability when replacing thealignment aj={1,2} = 1 will be selected.6 Order of HypothesesThe default order of hypotheses in Rosti et al(2008) is to rank the hypotheses in descending oftheir TER scores against the backbone.
This pa-per attempts several other orders.
The first one issystem-based order, i.e.
assume an arbitrary orderof the MT systems and feeds all the translations(in their original order) from a system before thetranslations from the next system.
The rationalebehind the system-based order is that the transla-tions from the same system are much more similarto each other than to the translations from othersystems, and it might be better to build CN byincorporating similar translations first.
The sec-ond one is N-best rank-based order, which means,rather than keeping the translations from the samesystem as a block, we feed the top-1 translationsfrom all systems in some order of systems, andthen the second best translations from all systems,and so on.
The presumption of the rank-based or-der is that top-ranked hypotheses are more reliableand it seemed beneficial to incorporate more reli-able hypotheses as early as possible.
These twokinds of order of hypotheses involve a certain de-gree of randomness as the order of systems is arbi-trary.
Such randomness can be removed by impos-ing a Bayes Risk order on MT systems, i.e.
arrangethe MT systems in ascending order of the BayesRisk of their top-1 translations.
These four ordersof hypotheses are summarized in Table 1.
We alsotried some intuitively bad orders of hypotheses, in-cluding the reversal of these four orders and therandom order.7 EvaluationThe proposed approaches of incremental IHMMare evaluated with respect to the constrainedChinese-to-English track of 2008 NIST Open MT954Order ExampleSystem-based 1:1 .
.
.
1:N 2:1 .
.
.
2:N .
.
.
M:1 .
.
.
M:NN-best Rank-based 1:1 2:1 .
.
.
M:1 .
.
.
1:2 2:2 .
.
.
M:2 .
.
.
1:N .
.
.
M:NBayes Risk + System-based 4:1 4:2 .
.
.
4:N .
.
.
1:1 1:2 .
.
.
1:N .
.
.
5:1 5:2 .
.
.
5:NBayes Risk + Rank-based 4:1 .
.
.
1:1 .
.
.
5:1 4:2 .
.
.
1:2 .
.
.
5:2 .
.
.
4:N .
.
.
1:N .
.
.
5:NTable 1: The list of order of hypothesis and examples.
Note that ?m:n?
refers to the n-th translation fromthe m-th system.Evaluation (NIST (2008)).
In the following sec-tions, the incremental IHMM approaches usingdistortion model 1 and 2 are named as IncIHMM1and IncIHMM2 respectively, and the consensusdecoding of multiple IHMMs as CD-IHMM.
Thebaselines include the TER-based method in Rostiet al (2007), the incremental TER method in Rostiet al (2008), and the IHMM approach in He etal.
(2008).
The development (dev) set comprisesthe newswire and newsgroup sections of MT06,whereas the test set is the entire MT08.
The 10-best translations for every source sentence in thedev and test sets are collected from eight MT sys-tems.
Case-insensitive BLEU-4, presented in per-centage, is used as evaluation metric.The various parameters in the IHMM model areset as the optimal values found in He et al (2008).The lexical translation probabilities used in thesemantic similarity model are estimated from asmall portion (FBIS + GALE) of the constrainedtrack training data, using standard HMM align-ment model (Och and Ney (2003)).
The back-bone of CN is selected by MBR.
The loss functionused for TER-based approaches is TER and thatfor IHMM-based approaches is BLEU.
As to theincremental systems, the default order of hypothe-ses is the ascending order of TER score against thebackbone, which is the order proposed in Rostiet al (2008).
The default order of hypothesesfor our three incremental IHMM approaches isN-best rank order with Bayes Risk system order,which is empirically found to be giving the high-est BLEU score.
Once the CN is built, the finalsystem combination output can be obtained by de-coding it with a set of features and decoding pa-rameters.
The features we used include word con-fidences, language model score, word penalty andempty word penalty.
The decoding parameters aretrained by maximum BLEU training on the devset.
The training and decoding processes are thesame as described by Rosti et al (2007).Method dev testbest single system 32.60 27.75pair-wise TER 37.90 30.96incremental TER 38.10 31.23pair-wise IHMM 38.52 31.65incremental IHMM 39.22 32.63Table 2: Comparison between IncIHMM2 and thethree baselines7.1 Comparison against BaselinesTable 2 lists the BLEU scores achieved bythe three baseline combination methods andIncIHMM2.
The comparison between pairwiseand incremental TER methods justifies the supe-riority of the incremental strategy.
However, thebenefit of incremental TER over pair-wise TER issmaller than that mentioned in Rosti et al (2008),which may be because of the difference betweentest sets and other experimental conditions.
Thecomparison between the two pair-wise alignmentmethods shows that IHMM gives a 0.7 BLEUpoint gain over TER, which is a bit smaller thanthe difference reported in He et al (2008).
Thepossible causes of such discrepancy include thedifferent dev set and the smaller training set forestimating semantic similarity parameters.
De-spite that, the pair-wise IHMM method is still astrong baseline.
Table 2 also shows the perfor-mance of IncIHMM2, our best incremental IHMMapproach.
It is almost one BLEU point higher thanthe pair-wise IHMM baseline and much higherthan the two TER baselines.7.2 Comparison among the IncrementalIHMM ModelsTable 3 lists the BLEU scores achieved bythe three incremental IHMM approaches.
Thetwo distortion models for IncIHMM approachlead to almost the same performance, whereasCD-IHMM is much less satisfactory.For IncIHMM, the gist of both distortion mod-955Method dev testIncIHMM1 39.06 32.60IncIHMM2 39.22 32.63CD-IHMM 38.64 31.87Table 3: Comparison between the three incremen-tal IHMM approachesels is to shift the distortion over spans to the dis-tortion over word sequences.
In distortion model 2the word sequences are those sequences availablein one of the component translations in the CN.Distortion model 1 is more encompassing as it alsoconsiders the word sequences which are combinedfrom subsequences from various component trans-lations.
However, as mentioned in section 3.1,the number of sequences grows exponentially andthere is therefore a limit L to the length of se-quences.
In general the limit L ?
8 would ren-der the tuning/decoding process intolerably slow.We tried the values 5 to 8 for L and the variationof performance is less than 0.1 BLEU point.
Thatis, distortion model 1 cannot be improved by tun-ing L. The similar BLEU scores as shown in Ta-ble 3 implies that the incorporation of more wordsequences in distortion model 1 does not lead toextra improvement.Although consensus decoding is conceptuallydifferent from both variations of IncIHMM, itcan indeed be transformed into a form similar toIncIHMM2.
IncIHMM2 calculates the parametersof the IHMM as a weighted sum of various proba-bilities of the component translations.
In contrast,the equations in section 4 shows that CD-IHMMcalculates the weighted sum of the logarithm ofthose probabilities of the component translations.In other words, IncIHMM2 makes use of the sumof probabilities whereas CD-IHMM makes useof the product of probabilities.
The experimentresults indicate that the interaction between theweights and the probabilities is more fragile in theproduct case than in the summation case.7.3 Impact of Order of HypothesesTable 4 lists the BLEU scores on the test setachieved by IncIHMM1 using different orders ofhypotheses.
The column ?reversal?
shows the im-pact of deliberately bad order, viz.
more than oneBLEU point lower than the best order.
The ran-dom order is a baseline for not caring about or-der of hypotheses at all, which is about 0.7 BLEUnormal reversalSystem 32.36 31.46Rank 32.53 31.56BR+System 32.37 31.44BR+Rank 32.6 31.47random 31.94Table 4: Comparison between various orders ofhypotheses.
?System?
means system-based or-der; ?Rank?
means N-best rank-based order; ?BR?means Bayes Risk order of systems.
The numbersare the BLEU scores on the test set.point lower than the best order.
Among the orderswith good performance, it is observed that N-bestrank order leads to about 0.2 to 0.3 BLEU pointimprovement, and that the Bayes Risk order ofsystems does not improve performance very much.In sum, the performance of incremental alignmentis sensitive to the order of hypotheses, and the op-timal order is defined in terms of the rank of eachhypothesis on some system?s n-best list.8 ConclusionsThis paper investigates the application of the in-cremental strategy to IHMM, one of the state-of-the-art alignment methods for MT output com-bination.
Such a task is subject to the prob-lem of how to define state transitions on a grad-ually expanding CN.
We proposed three differ-ent solutions, which share the principle that tran-sition over CN spans must be converted to thetransition over word sequences provided by thecomponent translations.
While the consensus de-coding approach does not improve performancemuch, the two distortion models for incrementalIHMM (IncIHMM1 and IncIHMM2) give superbperformance in comparison with pair-wise TER,pair-wise IHMM, and incremental TER.
We alsoshowed that the order of hypotheses is importantas a deliberately bad order would reduce transla-tion quality by one BLEU point.ReferencesXiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore 2008.
Indirect-HMM-based Hypothesis Alignment for Combining Out-puts from Machine Translation Systems.
Proceed-ings of EMNLP 2008.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer 2008.
Machine Translation956System Combination using ITG-based Alignments.Proceedings of ACL 2008.Evgeny Matusov, Nicola Ueffing and Hermann Ney.2006.
Computing Consensus Translation from Mul-tiple Machine Translation Systems using EnhancedHypothesis Alignment.
Proceedings of EACL.Nishanth Ulhas Nair and T.V.
Sreenivas.
2007.
JointDecoding of Multiple Speech Patterns for RobustSpeech Recognition.
Proceedings of ASRU.Dennis Nilsson and Jacob Goldberger 2001.
Sequen-tially Finding the N-Best List in Hidden MarkovModels.
Proceedings of IJCAI 2001.NIST 2008.
The NIST Open MachineTranslation Evaluation.
www.nist.gov/speech/tests/mt/2008/doc/Franz J. Och and Hermann Ney 2003.
A SystematicComparison of Various Statistical Alignment Mod-els.
Computational Linguistics 29(1):pp 19-51Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu 2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
Proceedings ofACL 2002Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz 2007.
Improved Word-level System Com-bination for Machine Translation.
Proceedings ofACL 2007.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz 2008.
Incremental Hypoth-esis Alignment for Building Confusion Networkswith Application to Machine Translation SystemCombination.
Proceedings of the 3rd ACL Work-shop on SMT.Khe Chai Sim, William J. Byrne, Mark J.F.
Gales,Hichem Sahbi, and Phil C. Woodland 2007.
Con-sensus Network Decoding for Statistical MachineTranslation System Combination.
Proceedings ofICASSP vol.
4.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla and John Makhoul 2006.
A Study ofTranslation Edit Rate with Targeted Human Anno-tation.
Proceedings of AMTA 2006957
