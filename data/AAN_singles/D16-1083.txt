Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 866?875,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Represent Reviewwith Tensor Decomposition for Spam DetectionXuepeng Wang1,2, Kang Liu1, Shizhu He1 and Jun Zhao1,21 National Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences, Beijing, 100190, China2 University of Chinese Academy of Sciences, Beijing, 100049, China{xpwang, kliu, shizhu.he, jzhao}@nlpr.ia.ac.cnAbstractReview spam detection is a key task in opin-ion mining.
To accomplish this type of de-tection, previous work has focused mainly oneffectively representing fake and non-fake re-views with discriminative features, which arediscovered or elaborately designed by expert-s or developers.
This paper proposes a nov-el review spam detection method that learn-s the representation of reviews automaticallyinstead of heavily relying on experts?
knowl-edge in a data-driven manner.
More specifi-cally, according to 11 relations (generated au-tomatically from two basic patterns) betweenreviewers and products, we employ tensor de-composition to learn the embeddings of the re-viewers and products in a vector space.
Wecollect relations between any two entities (re-viewers and products), which results in muchuseful and global information.
We concate-nate the review text, the embeddings of the re-viewer and the reviewed product as the rep-resentation of a review.
Based on such rep-resentations, the classifier could identify theopinion spam more precisely.
Experimentalresults on an open Yelp dataset show that ourmethod could effectively enhance the spamdetection accuracy compared with the state-of-the-art methods.1 IntroductionWith the development of E-commerce, more andmore customers share their experiences about prod-ucts and services by posting reviews on the we-b.
These reviews could heavily guide the purchas-ing behaviors of customers.
The products whichreceive more positive reviews tend to attract moreconsumers and result in more profits.
Studies onYelp.com have shown that an extra half-star ratingcould cause a restaurant to sell out 19% more prod-ucts (Anderson and Magruder, 2012), and a one-star increase leads to a 5-9% profit increase (Luca,2011).
Therefore, more and more sellers and manu-facturers have begun to place emphasis on analyzingreviews.
However, the question remains: is everyonline review trustful?
It has been reported that upto 25% of the reviews on Yelp.com could be fraudu-lent1.
Due to the great profit or reputation, impostorsor spammers energetically post fake reviews on theweb to promote or defame targeted products (Jindaland Liu, 2008).
Such fake reviews could misleadconsumers and damage the online review websites?reputations.
Therefore, it is necessary and urgent todetect fake reviews (review spam).To accomplish this goal, much work has beenconducted.
They commonly regard this task as aclassification task and most efforts are devoted toexploring useful features for representing target re-views.
Li et al (2013) and Kim et al (2015) rep-resent reviews with linguistic features; Lim et al(2010) and Mukherjee et al (2013c) represent re-views with reviewers?
behavioral features2; Wanget al (2011) and Akoglu et al (2013) exploregraph structure features3; Mukherjee et al (2013b),1http://www.bbc.com/news/technology-242997422Reviewers?
spammer-like behaviors, e.g., if a reviewer con-tinuously posts reviews within a short period of time, (s)hemight be a spammer, and her (his) posted reviews could be s-pam.3A kind of behavioral features which contain much interac-tions between reviewers and products866Rayana and Akoglu (2015) use the combination ofaforementioned features.
According to the exist-ing studies, reviewers?
behavioral features have beenproven to be more effective than reviews?
linguisticfeatures for detecting review spam (Mukherjee et al,2013c).
It is because that foxy spammers could eas-ily disguise their writing styles and forge reviews,discovering discriminative linguistic features is verydifficult.
Recently, most of the researchers (Rayanaand Akoglu, 2015) have focused on the reviewer-s?
behavioral features, the intuition behind which isto capture the reviewers?
actions and supposes thatthose reviews written with spammer-like behaviorswould be spam.Although, the existing work has made signifi-cant progress in combating review spamming, theyalso have several limitations as follows.
(1) Therepresentations of reviews rely heavily on experts?prior knowledge or developers?
ingenuity.
To dis-cover more discriminative features for representingreviews, previous work (Mukherjee et al, 2013b;Rayana and Akoglu, 2015) have spent lots of man-power and time on the statistics of the reviewdatasets.
Besides, experts?
prior knowledge or de-velopers?
ingenuity is not always reliable with thevariations of domains and languages.
For exam-ple, based on the datasets from Dianping site4, Liet al (2015) find that the real users tend to reviewthe restaurants nearby, but the spammers are not re-stricted to the geographical location, they may comefrom anywhere.
However, it is not true in the Yelpdatasets (Mukherjee et al, 2013b).
We found that72% of the Yelp?s review spam is posted from theareas near the restaurants, but only 64% of the au-thentic reviews are near the restaurants.
Therefore,how to learn the representations of reviews direct-ly from data instead of heavily relying on the ex-perts?
prior knowledge or developers?
ingenuity be-comes crucial and urgent.
(2) Furthermore, limitedby the experts?
knowledge, previous work only usespartial information of the review system.
For exam-ple, traditional behavioral features (Lim et al, 2010;Mukherjee et al, 2013c) only utilize the informationof individual reviewer.
Although the work (Wanget al, 2011; Rayana and Akoglu, 2015) have triedto employ graph structure to consider the interac-4http://www.dianping.comtions among the reviewers and products, it is a kindof local interaction defined within the same produc-t review page.
However, the interaction among thereviewers and products from different review pagesalso provides much useful and global information,which is ignored by the previous work.To tackle the problems described above, we pro-pose a novel review spam detection method whichcan learn the representations of reviews instead ofheavily relying on the experts?
knowledge, develop-ers?
ingenuity, or spammer-like assumption, and canreserve the original information with a global man-ner.
Inspired by the work about distributional repre-sentation or embedding for text and knowledge base,we propose a tensor factorization-based model tolearn the representation of each review automatical-ly.
The finally learnt representation of each review isdetermined by the original data, rather than the fea-tures or clues found by experts.
More specifically,we defined two basic patterns without any expert-s?
knowledge, developers?
ingenuity, or spammer-like assumptions.
Based on the two basic patterns,we extended 11 interactive relations between entities(reviewers and products) in terms of time, locations,social contact, etc.
Then, we build a 3-mode tensoron these 11 interactive relations between reviewersand products.
In order to reserve the original infor-mation with a global manner, we collect the relation-s of any two entities regardless of whether they arefrom the same review page.
In this way, we couldreserve the original information of the data as muchas possible, which dispenses with human selection.Next, we utilize tensor factorization to perform ten-sor decomposition, and the representations of re-viewers and products are embedded in a latent vectorspace by collective learning.
Afterward, we couldobtain vector representations (embeddings) for boththe reviewers and products.
Then, we concatenatethe review text (e.g., bigram), the embedding of areviewer and the reviewed product as the represen-tation of a review.
In this way, the representationsof reviews driven by data could be learnt in the en-tire review system in a global manner.
Finally, suchrepresentations are fed into a classifier to detect thereview spam.In summary, this paper makes the following con-tributions:?
It addresses the spam detection issue with a867123?N?1?2 ?
?n ?
1 ?
2??
m?1?2??n?
1?
2??
mkXFactorize??ATA?i?
j?iConcatenate ReviewRepresentationInputClassifier?
jkRReviewDataProduct?
jReviewer?iReviewText?
kRelationsFigure 1: Illustrated of our method.
The ?i denotes the i-th reviewer, and the ?j denotes the j-th product.new perspective.
Specifically, it learns the rep-resentation of reviews directly from the da-ta.
The key advantage is that it can representthe reviews instead of heavily relying on hu-man ingenuity cost, experts?
knowledge or anyspammer-like assumption.?
It collects the relations between any two en-tities regardless of whether they are from thesame review page, which results in much globalinformation.
With the help of tensor factoriza-tion, it could collectively embed the informa-tion of different relations into the final repre-sentations of reviews, and further optimize therepresentations.
Therefore it could faithfullyreflect the original characteristics of the entirereview system with a global manner.?
An extra advantage is that the learnt represen-tations of reviews are embeddings in a latent s-pace.
They are hardly comprehended by humanbeings included spammers.
It?s a robust detec-tion method in contrast to the previous meth-ods in which the reviews are represented bythe explicit detecting clues and features.
Oncehave realized the explicit features that werecaptured, experienced spammers could changetheir spamming strategies.?
The method of this paper renders 89.2%F1-score in detecting restaurant review spamwhich is higher than the F1-score of 86.1%rendered by the method in (Mukherjee et al,2013b) (in hotel domain, it?s 87.0% vs 84.8%).These experimental results give good confi-dence to the proposed approach, and the learntrepresentations of reviews are more robust andeffective than in previous methods.2 The Proposed MethodIn this section, we propose our method (shown inFigure 1) in detail.
Compared with the previouswork, we address the review spam detection issue bylearning the representation of the reviews automati-cally in a latent space without experts?
knowledge.First, we extend 11 interactive relations between en-tities (reviewers and products) from the two basicpatterns in terms of time, locations, social contact,etc.
Then, our method generates 11 relation matricesof the reviewers (?i) and products (?j).
After that,we construct a 3-mode tensor X, where each sliceXk in X denotes the link relationship between thereviewers and products in the relation k. Second, wefactorize the tensor X by employing the algorithmRESCAL (Nickel et al, 2011).
In the factorizationresults, A represents the embeddings of the review-ers (?i) and products (?j) in the latent space withthe collective learning.
Third, we concatenate thereview text (bigram), the embedding of its reviewerand the reviewed product together, as the representa-tion of the review.
Last, the concatenated embeddingof the review is fed into a classifier (e.g., SVM) todetect whether it is a fake or non-fake review.2.1 Relation Matrices GenerationIn the review system, there are two kinds of entities:reviewers and products5.
Each entity has several at-tributes, e.g., the attribute ?location?
of a restaurantis Chicago (the restaurant is regarded as a product).More details are shown in Table 1.To learn the representations of reviews directlyfrom the data instead of experts?
knowledge, we de-fined two basic patterns:5The product refers to a hotel/restaurant in our experiments.868Reviewer Attribute Product Attributeset of reviewed products set of reviewersset of reviews(rating score, time)set of reviews(rating score, time)website joining date average ratingfriend count review countlocation locationTable 1: Entities and AttributesPattern 1:Record the relationships between twoentities.Pattern 2:Record the relationships between at-tributes of two entities.These patterns do not contain any spammer-likeprior assumption, just record the natural relationin the original review system.
Based on the t-wo basic patterns, we extended 11 interactive rela-tions between entities and their attributes (showedin Table 1).
They will be described in detail asfollows.
Meanwhile, we define that avg(ak,i) =1n?nk=1 ak,i.1.
Have reviewed: This relation records whethera reviewer has reviewed a product.
If reviewer?i reviewed product ?j , the value X[i, j, 1] inthis relation matrix X[:, :, 1] is 1, otherwise it?s0.2.
Rating score: What score (1 to 5 star) areviewer-rated product receives.
The valueX[i, j, 2] ?
{1, 2, ..., 5}.3.
Commonly reviewed products: The numberof products that a reviewer commonly reviewedwith other reviewers.
The value X[i, j, 3] =|Pij | , Pij = Pi ?
Pj ; Pi is the product set re-viewed by reviewer ?i.4.
Commonly reviewed time difference: Thetime differences that a reviewer who commonlyreviews with other reviewers on the same prod-ucts.
The valueX[i, j, 4] = ti?
tj , where ti =avg(tk,i); tk,i is the time that the reviewer ?ireviewed the product ?k in the Pij set.5.
Commonly reviewed rating difference: Therating differences that a reviewer who com-monly reviews with other reviewers on thesame products.
The value X[i, j, 5] = ri ?rj , where ri = avg(rk,i); rk,i is the score ofthe reviewer ?i rated the product ?k in Pij set.6.
Date difference of websites joined: The datedifferences of joining review websites betweena reviewer and others.
The value X[i, j, 6] =di?
dj , where di is the date on which reviewer?i joining websites.7.
Average rating difference: The differences inthe average rating of a reviewer over all his re-views compared with other reviewers.
The val-ueX[i, j, 7] = ?ri ?
?rj , where ?ri = avg(?rk,i);?rk,i is the score with which the reviewer ?i rat-ed the product ?k in Pi.The differences in the average rating of aproduct over all its reviews compared with oth-er products.
X[i, j, 7] = ?pi ?
?pj , where ?pi =avg(?pk,i); ?pk,i is the score of review k in R?i ,which is the review set for product ?i.8.
Friend count difference: The differences inthe friend count of a reviewer compared to oth-ers.
At the review website, a reviewer can makefriends with others.
The value X[i, j, 8] =fi?fj ; where fi is the friend count of reviewer?i.9.
Have the same location or not: Whether t-wo reviewers/products are from the same cityor whether a reviewer has the same locationwith a product.
If two entities have the samelocation, the value X[i, j, 9] = 1, otherwiseX[i, j, 9] = 0.10.
Common reviewers: The number of the samereviewers that a product has with other product-s.
The value X[i, j, 10] = |?ij | , where ?ij =?i ?
?j ; ?i is the set of reviewers who re-viewed product ?i.11.
Review count difference: The differences inthe reviews count of any two reviewers.
Thevalue X[i, j, 11] = |R?i | ????R?j??
?, where R?i isthe reviews set of reviewer ?i.
Or the differ-ences in the reviews count of any two products,where X[i, j, 11] =???R?i???????R?j??
?, where R?i isthe reviews set of product ?i.According to the relations that we present above,we build 11 relation matrices among the reviewersand products.
To unify the values of different ma-trices to a reference system, we normalize with the869sigmoid function.
Thus, the value ?0?
will be nor-malized to ?0.5?.
Moreover, we set the values thatmake no sense to ?0?, such as the value between t-wo products in Relation 1: Have reviewed.
Then,we unite the 11 matrices to form the adjacent ten-sor.
Each of the matrices is a slice of the tensor.
Thereviewers and products are regarded as the same en-tities in the tensor.
We build two separate tensors forthe hotel domain and restaurant domain respective-ly.
Next, we perform tensor factorization to learn therepresentations (embeddings) of reviewers and prod-ucts.
Note that the word ?relation?
is normally usedfor binary (0/1) relations, but some values of afore-mentioned relations could be between 0 and 1.
How-ever, our experiments show that this type of rela-tion is actually practicable.
Besides, there is not anyspammer-like assumption in the relations.
Namely,the values of relations don?t indicate how suspiciousthe reviewers are.
The values faithfully reflect theoriginal characteristics of the entire review system.This can help to reduce the need of carefully design-ing expert features and the understanding of domain-s as much as possible.2.2 Learning to Represent ReviewsIn general case, a review contains the text, the re-viewer and the reviewed product.
We firstly learnto represent reviewers and products.
As mentionedabove, based on the relations, we could construct anadjacency tensor X.
Then, we convert the global re-lation information related reviewers and products in-to embeddings through tensor factorization, wherean efficient factorization algorithm called RESCAL(Nickel et al, 2011) is employed.
First, we intro-duce it briefly.To identify latent components in a tensor forcollective learning, Nickel et al (2011) proposedRESCAL, which is a tensor factorization algorithm.Given a tensor Xn??n??m?
, RESCAL aims to havea rank-r approximation, where each slice Xk is fac-torized asXk ?
ARkAT , for all k = 1...m?, (1)A is an n??
r matrix, where the i-th row denotes thei-th entity.
Rk is an asymmetric r ?
r matrix thatdescribes the interactions of the latent componentsaccording to the k-th relation.
Note that while Rkdiffers in each slice, A remains the same.A andRk are derived by minimizing the loss func-tion below.minA,Rkf(A, Rk) + ?
?
g(A, Rk), (2)where f(A, Rk) = 12(?k ?
Xk ?
ARkAT ?2F) is the mean-squared reconstruction error, andg(A, Rk) = 12(?
A ?2F +?k ?
Rk ?2F ) is theregularization term.In our method, sliceXk is the k-th relation above.The i-th entity is the i-th reviewer or product.As mentioned in Section 2.1, in order to obtainmore useful and global information automatically,we collect the relations of any two entities no mat-ter whether they are from the same review page.Then we could embed the informations over multi-relations into the finally learnt representation by thetensor factorization.
As Nickel et al (2011) proved,all the relations have a determining influence on thelearnt latent-component representation of the i-thentity.
It removes the noise of the original data bylearning through the global loss function.
Conse-quently, we get the representation of reviewers andproducts with a further optimization by the collec-tive learning.2.3 Detecting Review Spam in Latent SpaceAfter learning the representations of reviewers andproducts, we begin to represent the reviews that werewritten by reviewers for the products.
Our final pur-pose is to detect the review spam.
We concatenatethe review text (bigram), the embedding of a review-er and the reviewed product as the representation ofa review.
The representations of the review text bybigram have been proved to be effective in sever-al previous work (Mukherjee et al, 2013b; Rayanaand Akoglu, 2015; Kim et al, 2015).
It?s also a kindof data-driven representation.
Then, we take the em-beddings of the reviews as the input to the classifiers.Here, we use the linear kernel SVM model to com-pare with the experimental results in (Mukherjee etal., 2013b) and (Rayana and Akoglu, 2015).3 Experiments3.1 Datasets and Evaluation MetricsDatasets: To evaluate the proposed method, we con-ducted experiments on Yelp dataset that was used in870previous studies (Mukherjee et al, 2013b; Mukher-jee et al, 2013c; Rayana and Akoglu, 2015).
Al-though there are other datasets for evaluation, suchas (Jindal and Liu, 2008), (Lim et al, 2010; Xie etal., 2012) and (Ott et al, 2011), they are generatedby human labeling or crowd sourcing and have beenproved not to be reliable since human labeling fakereviews is quite poor (Ott et al, 2011).
There waslack of real-life and nearly ground truth data, untilMukherjee et al (2013c) proposed the Yelp reviewdataset.
The statistics of the Yelp dataset are listed inTable 2.
The reviewed product here refers to a hotelor restaurant.Evaluation Metrics: We select precision (P), recall(R), F1-Score (F1) and accuracy (A) as metrics.Domain Hotel Restaurantfake 802 8368non-fake 4876 50149%fake 14.1% 14.3%#reviews 5678 58517#reviewers 5124 35593Table 2: Yelp Labeled Dataset Statistics.3.2 Our Method vs.
The State-of-the-artMethodsTo illustrate the effectiveness of the proposed ap-proach, we select several state-of-the-arts for com-parison.
The first one is SPEAGLE+ (Rayana andAkoglu, 2015), which is a kind of graph-basedmethod.
The representations of reviews in (Rayanaand Akoglu, 2015) are combined with linguistic fea-tures, behavioral features and review graph structurefeatures.
It?s a semi-supervised method.
For a faircomparison with our 5-fold CV classification, weset the ratio of labeled data in SPEAGLE+ to 80%.The second one is Mukherjee et al (2013b).
KC andMukherjee (2016) also conduct experiments on therestaurant subset in Table 2.
But they mainly focuson analyzing the effects of temporal dynamics.
It?snot our focus.
So we didn?t take it into comparison.In our experiments, we employ behavioral features(Mukherjee BF) and both of behavioral and linguis-tic features (Mukherjee BF+Bigram) proposed inMukherjee et al (2013b), respectively.
The parame-ters used in these compared methods are same as theoriginal papers.
For our approach, we set the param-eter r to 150, ?
to 10, and the iteration number to100.The compared results are shown in Table 3.
We u-tilize our learnt embeddings of reviewers (Ours RE),both of reviewers?
embeddings and products?
em-beddings (Ours RE+PE), respectively.
Moreover,to perform fair comparison, like Mukherjee et al(2013b), we add representations of the review text inclassifier (Ours RE+PE+Bigram).
From the result-s, we can observe that our method could outperformall state-of-the-arts in both the hotel and restaurantdomains.
It proves that our method is effective.
Fur-thermore, the improvements in both the hotel andrestaurant domains prove that our model possessespreferable domain-adaptability.
It could representthe reviews more accurately and globally by learn-ing from the original data, rather than the experts?knowledge or assumption.3.3 The Effectiveness of Learning to RepresentReviewTo further prove the representations learnt by ourmethod are effective for detecting review spam, wecompare the learnt representation (embeddings) ofreviewers (Ours RE) (Table 3 (a,b) rows 7, 8) withexisting behavioral features of reviewers (Mukher-jee BF) (Mukherjee et al, 2013b) (Table 3 (a,b)rows 3, 4).
In results, using the learnt reviewer-s?
representations in our method, results in around2.0% (in 50:50) and 4.0% (in N.D.) improvement inF1 and A in the hotel domain, and results in around2.1% (in 50:50) and 7.0%(in N.D.) improvement inF1 and A in the restaurant domain.
These resultsshow that our data-driven representations of review-ers are more helpful for review spam detection thanexisting reviewers?
behavioral features, and that newmethod embeds more useful and accurate informa-tion from the original data.
It isn?t limited to expert-s?
knowledge.
Moreover, the latent representationsare more robust because they are hardly perceivedby spammers.
Having realized the explicit existingbehavioral features, crafty spammers tend to changetheir spamming strategies.
Consider the feature ?Re-view Length?, which is used in (Mukherjee et al,2013b), as an example.
They find that the averagereview length of the spammers is quite short com-pared with non-spammers.
However, once a craftyspammer realizes that he left this type of footprint,he could produce a review that is as long as the non-871Method C.D.
P R F1 ASPEAGLE+(80%) 50:50 75.7 83.0 79.1 81.0N.D.
26.5 56.0 36.0 80.4Mukherjee BF 50:50 82.4 85.2 83.7 83.8N.D.
41.4 84.6 55.6 82.4Mukherjee BF+Bigram 50:50 82.8 86.9 84.8 85.1N.D.
46.5 82.5 59.4 84.9Ours RE 50:50 83.3 88.1 85.6 85.5N.D.
47.1 83.5 60.2 85.0Ours RE+PE 50:50 83.6 89.0 86.2 85.7N.D.
47.5 84.1 60.7 85.3Ours RE+PE+Bigram 50:50 84.2 89.9 87.0 86.5N.D.
48.2 85.0 61.5 85.9(a) HotelP R F1 A80.5 83.2 81.8 82.5 150.1 70.5 58.6 82.0 282.8 88.5 85.6 83.3 348.2 87.9 62.3 78.6 484.5 87.8 86.1 86.5 548.9 87.3 62.7 82.3 685.4 90.2 87.7 87.4 756.9 90.1 69.8 85.8 886.0 90.7 88.3 88.0 957.4 89.9 70.1 86.1 1086.8 91.8 89.2 89.9 1158.2 90.3 70.8 87.8 12(b) RestaurantTable 3: Classification results across the behavioral features (BF), the reviewer embeddings (RE) , product embeddings (PE) andbigram of the review texts.
Training uses balanced data (50:50).
Testing uses two class distributions (C.D.
): 50:50 (balanced) andNatural Distribution (N.D.).
Improvements of our method are statistically significant with p<0.005 based on paired t-test.84%86%88% BFRelsRE80%82% P R F1 A(a) Hotel86%88%90% BFRelsRE82%84% P R F1 A(b) RestaurantFigure 2: SVM 5-fold CV classification results across behav-ioral features (BF), 11 relations (Rels) and reviewer embed-dings (RE) in our method.
Both training and testing use bal-anced data (50:50).
Improvements are statistically significantwith p<0.005 based on paired t-test.spammers to pretend to be a normal reviewer.
Be-sides, as there isn?t any spammer-like assumptionin our extended relations (Section 2.1), crafty spam-mers have little influence on them.We also compared existing behavioral features(BF) (Mukherjee et al, 2013b) with detecting re-view spam by only employing the 11 generated re-lations (Rels).
We take the relation matrix row ofeach reviewer as the representations of the reviews.According to the results shown in Figure 2, the 11generated relations (Rels) results in an obvious im-provement than the existing behavioral features (BF)(Mukherjee et al, 2013b) (Table 3 (a,b) row 3) inboth the hotel and restaurant domains.
It proves thatthe generated relations could obtain more useful andglobal informations, as they collect the relations ofany entities (reviewers and products) regardless ofwhether they are from the same review page.
Fur-thermore, Figure 2 also showed that the embeddingsDroppedRelationHotel RestaurantF1 A F1 A1 -2.1 -2.0 -2.0 -3.12 -2.3 -2.1 -1.9 -2.93 -3.9 -4.0 -4.0 -6.34 -3.7 -3.5 -3.6 -5.55 -3.5 -3.6 -2.8 -4.56 -2.5 -2.5 -3.4 -5.27 -3.2 -3.2 -3.3 -5.08 -2.8 -2.6 -3.0 -4.69 -4.0 -3.7 -3.7 -5.410 -2.2 -2.4 -1.8 -2.811 -2.6 -2.4 -2.7 -4.4Table 4: SVM 5-fold CV classification results by dropping re-lations from our method utilizing RE+PE+Bigram.
Both train-ing and testing use balanced data (50:50).
Differences in clas-sification metrics for each dropped relation are statistically sig-nificant with p<0.01 based on paired t-test.of reviewers (RE) learnt by the tensor decomposi-tion perform better than the Rels.
As we mentionedin Section 2.2, the tensor decomposition embeds theinformations over all the relations collectively, andremoves the noise of the original data by learningthrough the global loss function.
Consequently, weget the representations with a further optimization.3.4 The Effectiveness of Product EmbeddingsIn general case, a review contains the review tex-t, the reviewer and the reviewed product.
But mostof the previous work represent the reviews with thereviewers?
behavioral features and the reviews?
lin-guistic features.
The products are seldom represent-ed.
As shown in Table 3 (a,b) rows 9,10 , the rep-resentations which added the products embeddings872perform better than just using the reviewer embed-dings.
Statistics of the datasets suggest that thereare about 1% of spammers who not only write fakereviews, but also write non-fake reviews.
Liu (2015)also proved that some reviewers have contributedmany genuine reviews and have built up their reputa-tion; then they started to spam for some businesses,or even sell their accounts to spammers.
Comparedwith previous work, our method by adding produc-t embeddings could distinguish the reviews of thesame reviewer for different products.3.5 The Effects of Different RelationsWe also drop relations of our method with a grace-ful degradation.
Table 4 shows the performances ofour method utilizing BF+PE+Bigram for hotel andrestaurant domains.
We found that dropping Rela-tions 1, 2 and 10 results in a relatively gentle re-duction (about 2.2%) in F1-score.
According to oursurvey, the sparseness of the slices generated by Re-lation 1, 2 and 10 is about 99.9%.
For this reason,the result is a relatively gentle reduction.
Droppingother relations also result in a 2.5-4.0% performancereduction.
It proves that each relation has an influ-ence on the learning to represent reviews.4 Related WorkJindal and Liu (2008) first propose the problem ofreview spam detection.
They identify three cate-gories of spam: fake reviews (also called untruth-ful opinions), reviews on the brand only, and non-reviews.Stepping studies focus on studying fake re-views because of its difficulty to be detected.
Mostefforts are devoted to represent fake and non-fakereviews with effective features.Linguistic Features Ott et al (2011) apply psy-chological and linguistic clues to identify reviewspam.
They produce the first dataset of gold-standard deceptive review spam, employing crowd-sourcing through the Amazon Mechanical Turk.Harris (2012) explores several human- and machine-based assessment methods with writing style fea-tures.
Feng et al (2012a) investigate syntactic sty-lometry for review spam detection.
Li et al (2013)propose a generative LDA-based topic modeling ap-proach for fake review detection.
They (Li et al,2014b) further investigate the general difference oflanguage usage between deceptive and truthful re-views.
Li et al (2014a) propose a positive-unlabeledlearning method base on unigrams and bigrams.Kim et al (2015) carry out a frame-based deep se-mantic analysis on deceptive opinions.Behavioral Features Lim et al (2010) investigatereviewers?
rating behavioral features.
Jindal etal.
(2010) identify unusual review patterns whichcan represent suspicious behaviors of reviews.
Liet al (2011) provide a two-view semi-supervisedmethod, co-training method base on behavioral fea-tures.
Feng et al (2012b) study the distributions ofbehavioral features.
Xie et al (2012) explore the s-ingleton reviews with abnormal temporal patterns.Mukherjee et al (2012) study the group spammers?behavioral features.
Mukherjee et al (2013a) pro-pose a principal method which models the spamici-ty of reviewers.
Fei et al (2013) model the review-ers?
co-occurrence in review bursts.
Mukherjee etal.
(2013c) prove that reviewers?
behavioral featuresare more effective than reviews?
linguistic featuresfor detecting review spam.
Li et al (2015) explorethe temporal and spatial patterns at Dianping.com.KC and Mukherjee (2016) analyze the temporal dy-namics of opinion spamming.Graph Structure Wang et al (2011) investigate thereview graph features of online store review.
Akogluet al (2013) exploit the network effect among re-viewers and products.Combined Features There are also some workwhich explores methods via the combined featuresreferred above.
Mukherjee et al (2013b) prose amethod base on the linguistic features and behav-ioral features.
Rayana and Akoglu (2015) propose amodel that utilizes clues from review text, reviewer-s?
behaviors and the review graph structure.5 Conclusion and Future WorkThis paper proposes a new review spam detectionmethod that learns the representations of reviews in-stead of heavily relying on experts?
knowledge ina data-driven manner.
A 3-mode tensor is built onthe relations which are generated from two patterns,and a tensor factorization algorithm is used to auto-matically learn the vector representations of review-ers and products.
Afterwards, we concatenate thereview text, the embedding of a reviewer and the873reviewed product as the representation of a review.Then, a classifier is applied to detect the review s-pam.
Experimental results prove the effectiveness ofthe proposed method, which learns more robust re-view representations.
In future work, we plan to ex-plore a more effective way to learn the embeddingsof review text.AcknowledgmentsThis work was supported by the Natural Sci-ence Foundation of China (No.
61533018), theNational Basic Research Program of China (No.2014CB340503) and the National Natural ScienceFoundation of China (No.
61502493).
We wouldlike to thank Prof. Bing Liu for sharing the Yelp re-view dataset with us, and the anonymous reviewersfor their detailed comments and suggestions.ReferencesLeman Akoglu, Rishi Chandy, and Christos Faloutsos.2013.
Opinion fraud detection in online reviews bynetwork effects.
ICWSM, 13:2?11.Michael Anderson and Jeremy Magruder.
2012.
Learn-ing from the crowd: Regression discontinuity esti-mates of the effects of an online review database*.
TheEconomic Journal, 122(563):957?989.Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu,Malu Castellanos, and Riddhiman Ghosh.
2013.
Ex-ploiting burstiness in reviews for review spammer de-tection.
In ICWSM.
Citeseer.Song Feng, Ritwik Banerjee, and Yejin Choi.
2012a.Syntactic stylometry for deception detection.
In Pro-ceedings of the 50th ACL: Short Papers-Volume 2,pages 171?175.
ACL.Song Feng, Longfei Xing, Anupam Gogar, and YejinChoi.
2012b.
Distributional footprints of deceptiveproduct reviews.
In ICWSM.C Harris.
2012.
Detecting deceptive opinion spam usinghuman computation.
In Workshops at AAAI on Artifi-cial Intelligence.Nitin Jindal and Bing Liu.
2008.
Opinion spam and anal-ysis.
In Proceedings of the First WSDM, pages 219?230.
ACM.Nitin Jindal, Bing Liu, and Ee-Peng Lim.
2010.
Find-ing unusual review patterns using unexpected rules.In Proceedings of the 19th CIKM, pages 1549?1552.ACM.Santosh KC and Arjun Mukherjee.
2016.
On the tem-poral dynamics of opinion spamming: Case studies onyelp.
In Proceedings of the 25th International Confer-ence on World Wide Web, pages 369?379.
InternationalWorld Wide Web Conferences Steering Committee.Seongsoon Kim, Hyeokyoon Chang, Seongwoon Lee,Minhwan Yu, and Jaewoo Kang.
2015.
Deep semanticframe-based deceptive opinion spam analysis.
In Pro-ceedings of the 24th CIKM, pages 1131?1140.
ACM.Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.2011.
Learning to identify review spam.
In IJCAIProceedings, volume 22, page 2488.Jiwei Li, Claire Cardie, and Sujian Li.
2013.
Topicspam:a topic-model based approach for spam detection.
InACL (2), pages 217?221.Huayi Li, Bing Liu, Arjun Mukherjee, and Jidong Shao.2014a.
Spotting fake reviews using positive-unlabeledlearning.
Computacio?n y Sistemas, 18(3):467?475.Jiwei Li, Myle Ott, Claire Cardie, and Eduard Hovy.2014b.
Towards a general rule for identifying decep-tive opinion spam.
ACL.Huayi Li, Zhiyuan Chen, Arjun Mukherjee, Bing Liu,and Jidong Shao.
2015.
Analyzing and detectingopinion spam on a large-scale dataset via temporal andspatial patterns.
In Ninth International AAAI Confer-ence on Web and Social Media.Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,and Hady Wirawan Lauw.
2010.
Detecting productreview spammers using rating behaviors.
In Proceed-ings of the 19th CIKM, pages 939?948.
ACM.Bing Liu.
2015.
Sentiment Analysis: Mining Opinion-s, Sentiments, and Emotions.
Cambridge UniversityPress.Michael Luca.
2011.
Reviews, reputation, and revenue:The case of yelp.
com.
Com (September 16, 2011).Harvard Business School NOM Unit Working Paper,(12-016).Arjun Mukherjee, Bing Liu, and Natalie Glance.
2012.Spotting fake reviewer groups in consumer reviews.
InProceedings of the 21st WWW, pages 191?200.
ACM.Arjun Mukherjee, Abhinav Kumar, Bing Liu, JunhuiWang, Meichun Hsu, Malu Castellanos, and Riddhi-man Ghosh.
2013a.
Spotting opinion spammers usingbehavioral footprints.
In Proceedings of the 19th ACMSIGKDD, pages 632?640.
ACM.Arjun Mukherjee, Vivek Venkataraman, Bing Liu, andNatalie Glance.
2013b.
Fake review detection: Classi-fication and analysis of real and pseudo reviews.
Tech-nical report, Technical Report UIC-CS-2013-03, Uni-versity of Illinois at Chicago.Arjun Mukherjee, Vivek Venkataraman, Bing Liu, andNatalie S Glance.
2013c.
What yelp fake review filtermight be doing?
In ICWSM.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collective874learning on multi-relational data.
In Proceedings ofthe 28th ICML, pages 809?816.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Han-cock.
2011.
Finding deceptive opinion spam by anystretch of the imagination.
In Proceedings of the 49thACL: Human Language Technologies-Volume 1, pages309?319.
ACL.Shebuti Rayana and Leman Akoglu.
2015.
Collectiveopinion spam detection: Bridging review networks andmetadata.
In Proceedings of the 21th ACM SIGKD-D International Conference on Knowledge Discoveryand Data Mining, pages 985?994.
ACM.Guan Wang, Sihong Xie, Bing Liu, and Philip S Yu.2011.
Review graph based online store review spam-mer detection.
In Proceedings of the 11th ICDM,pages 1242?1247.
IEEE.Sihong Xie, Guan Wang, Shuyang Lin, and Philip S Yu.2012.
Review spam detection via temporal pattern dis-covery.
In Proceedings of the 18th KDD, pages 823?831.
ACM.875
