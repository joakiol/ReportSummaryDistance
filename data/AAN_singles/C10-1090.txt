Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 797?805,Beijing, August 2010Using Cross-Lingual Projections to Generate Semantic RoleLabeled Corpus for Urdu - A Resource Poor LanguageSmruthi MukundCEDARUniversity at Buffalosmukund@buffalo.eduDebanjan GhoshThomson Reuters R&Ddebanjan.ghosh@thomsonreuters.comRohini K. SrihariCEDARUniversity at Buffalorohini@cedar.buffalo.eduAbstractIn this paper we explore the possibility ofusing cross lingual projections that helpto automatically induce role-semanticannotations in the PropBank paradigmfor Urdu, a resource poor language.
Thistechnique provides annotation projectionsbased on word alignments.
It is relativelyinexpensive and has the potential to re-duce human effort involved in creatingsemantic role resources.
The projectionmodel exploits lexical as well as syntac-tic information on an English-Urdu paral-lel corpus.
We show that our method ge-nerates reasonably good annotations withan accuracy of 92% on short structuredsentences.
Using the automatically gen-erated annotated corpus, we conduct pre-liminary experiments to create a semanticrole labeler for Urdu.
The results of thelabeler though modest, are promising andindicate the potential of our technique togenerate large scale annotations for Urdu.1 IntroductionSemantic Roles (also known as thematic roles)help to understand the semantic structure of adocument (Fillmore, 1968).
At a fundamentallevel, they help to capture the similarities anddifferences in the meaning of verbs via the ar-guments they define by generalizing over surfacesyntactic configurations.
In turn, these roles aidin domain independent understanding as the se-mantic frames and semantic understanding sys-tems do not depend on the syntactic configura-tion for each new application domain.
Identify-ing semantic roles benefit several languageprocessing tasks - information extraction (Sur-deanu et al, 2003), text categorization (Moschitti,2008) and finding relations in textual entailment(Burchardt and Frank 2006).Automatically identifying semantic roles is of-ten referred to as shallow semantic parsing (Gil-dea and Jurafsky, 2002).
For English, thisprocess is facilitated by the existence of twomain SRL annotated corpora ?
FrameNet (Bakeret al, 1998) and PropBank (Palmer et al, 2005).Both datasets mark almost all surface realizationsof semantic roles.
FrameNet has 800 semanticframes that cover 120,000 example sentences1.PropBank has annotations that cover over113,000 predicate-argument structures.
ClearlyEnglish is well supported with resources for se-mantic roles.
However, there are other widelyspoken resource poor languages that are not asprivileged.
The PropBank based resources avail-able for languages like Chinese (Xue and Palmer,2009), Korean (Palmer et al, 2006) and Spanish(Taule, 2008) are only about two-thirds the sizeof the English PropBank.Several alternative techniques have been ex-plored in the literature to generate semantic rolelabeled corpora for resource poor languages asproviding manually annotated data is time con-suming and involves intense human labor.
Am-bati and Chen (2007) have conducted an exten-sive survey and outlined the benefits of usingparallel corpora to transfer annotations.
A widerange of annotations from part of speech (Hi andHwa, 2005) and chunks (Yarowsky et al, 2001)to word senses (Diab and Resnik, 2002), depen-dencies (Hwa et al, 2002) and semantic roles(Pado and Lapata, 2009) have been successfullytransferred between languages.
FrameNet styleannotations in Chinese is obtained by mappingEnglish FrameNet entries directly to conceptslisted in HowNet2 (online ontology for Chinese)with an accuracy of 68% (Fung and Chen, 2004).1 Wikipedia - http://en.wikipedia.org/wiki/PropBank2 http://www.keenage.com/html/e_index.html797Fung et al (2007) analyze an automatically an-notated English-Chinese parallel corpus andshow high cross-lingual agreement for PropBankroles (range of 75%-95% based on the roles).In this paper we explore the possibility of us-ing English-Urdu parallel corpora to generateSRL annotations for Urdu, a less commonlytaught language (LCTL).
Earlier attempts to gen-erate SRL corpora using annotation projectionshave been for languages such as German, French(Pado and Lapata, 2009) and Italian (Moschitti,2009) that have high vocabulary overlap withEnglish.
Also, German belongs to the same lan-guage family as English (Germanic family).
Ur-du on the other hand is an Indic language that isgrammatically very different and shares almostno vocabulary with English.The technique of cross lingual projections war-rants good BLEU score that ensures correct wordalignments.
According to NIST 2008 Open Ma-chine Translation challenge 3 , a 0.2280 bestBLEU score was achieved for Urdu to Englishtranslation.
This is comparable to the BLEUscores achieved for German to English ?
0.253and French to English ?
0.3 (Koehn, 2005).
But,for SRL transfer, perfect word alignment is notmandatory as SRL requires semantic correspon-dence only.
According to Fillmore (1982) se-mantic frames are based on conceptual structures.They are generalizations over surface structuresand hence less prone to syntactic variations.Since English and Urdu have a reasonable se-mantic correspondence (Example 3), we believethat the projections when capped with a postprocessing step will considerably reduce thenoise induced by inaccurate alignments and pro-duce acceptable mappings.Hindi is syntactically similar to Urdu.
Theselanguages are standardized forms of Hindustani.They are free word order languages and follow ageneral SOV (Subject-Object-Verb) structure.Projection approach has been used by (Mukerjeeet al, 2006) and (Sinha, 2009) to transfer verbpredicates from English onto Hindi.
Sinha (2009)achieves a 90% F-Measure in verb predicatetransfer from English to Hindi.
This shows thatusing cross lingual transfer approach to obtainsemantic annotations for Urdu from English is anidea worth exploring.3http://www.itl.nist.gov/iaui/894.01/tests/mt/2008/doc/mt08_official_results_v0.html1.1 ApproachOur approach leverages existing EnglishPropBank annotations provided via the SemLink4corpus.
SemLink provides annotations forVerbNet using the pb (PropBank) attribute.
Byusing English-Urdu parallel corpus we acquireverb predicates and their arguments.
When wetransfer verb predicates (lemmas), we alsotransfer pb attributes.
We obtain annotationprojections from the parallel corpora as follows:1.
Take a pair of sentences E (in English) and U(in Urdu) that are translations of each other.2.
Annotate E with semantic roles.3.
Project the annotations from E onto U usingword alignment information, lexicalinformation and linguistic rules that involvesyntactic information.There are several challenges to the annotationprojection technique.
Dorr (1994) presents somemajor lexical-semantic divergence problemsapplicable in this scenario:(a) Thematic Divergence - In some cases, al-though there exists semantic parallelism, thetheme of the English sentence captured inthe subject changes into an object in the Ur-du sentence (Example 1).
(b) Conflatational Divergence - Sometimes tar-get translations spans over a group of words(Example 1: plays is mapped to kirdar ada).Trying to ascertain this word span for se-mantic roles is difficult as the alignmentscan be incomplete and very noisy.
(c) Demotional divergence and Structural di-vergence - Despite semantic relatedness, insome sentence pairs, alignments obtainedfrom simple projections generate randommatchings as the usage is syntactically dis-similar (Example 2).Handling all challenges adds complexity to ourmodel.
The heuristic rules that we implement areguided by linguistic knowledge of Urdu.
Thisincreases the effectiveness of the alignments.Example 1:I(subject)am Angry at Reheem(object)Raheem(subject)mujhe(object)Gussa dilate hai(Raheem brings anger in me)4 http://verbs.colorado.edu/semlink/798Example 2: (noun phrase to prepositional pharse)Ali attended work todayAli aaj daftar mein haazir tha(Ali was present at work today)2 Generating Parallel CorporaPropBank provides SRL annotated corpora forEnglish.
It uses predicate independent labels(ARG0, ARG1, etc.)
which indicate how a verbrelates to its arguments.
The argument types areconsistent across all uses of a single verb and donot consider the sense of the verb.
We use thePropBank annotations provided for the WallStreet Journal (WSJ) part of the Penn Tree bankcorpus (Marcus et al, 2004).
The arguments of averb are labeled sequentially from ARG0 toARG5 where ARG0 is the proto-typical Agent,ARG1 is the proto-typical patient, ARG2 is therecipient, and so on.
There are other adjunct tagsin the dataset that are indicated by ARGM thatinclude tags for location (ARGM-LOC), tempor-al tags (ARGM-TMP) etc.An Urdu corpus of 6000 sentences corres-ponding to 317 WSJ articles of Penn Tree Bankcorpus is provided by CRULP5 (used in the NIST2008 machine translation task).
We consider2350 English sentences with PropBank annota-tions that have corresponding Urdu translations(CRULP corpus) for our experiments.2.1 Sentence AlignmentSentence alignment is a prerequisite for any pa-rallel corpora processing.
As the first step, wehad to generate a perfect sentence aligned paral-lel corpus as the translated sentences, despitebelonging to the same domain (WSJ ?
Penn treebank), had several errors in demarcating the sen-tence boundaries.Sentence alignment between English and Urduis achieved over two iterations.
In the first itera-tion, the length of each sentence is calculatedbased on the occurrence of words belonging toimportant part of speech categories such as prop-er nouns, adjectives and verbs.
Considering mainPOS categories for length assessment helps over-come the conflatational divergence issue.
Foreach English sentence, Urdu sentences with thesame length are considered to be probable candi-5http://www.crulp.org/dates for alignment.
In the second iteration, anUrdu-English lexicon is used on the Urdu corpusand English translations are obtained.
An Eng-lish-Urdu sentence pair with maximum lexicalmatch is considered to be sentence aligned.Clearly this method is highly dependent on theexistence of an exhaustive Urdu-English dictio-nary.
The lexicons that we use to perform loo-kups are collected by mining Wikipedia and oth-er online resources (Mukund et al, 2010).
How-ever, lexicon lookups will fail for Out-Of-Vocabulary words.
There could also be a colli-sion if Urdu sentences have English transliteratedwords (Example 3, ?office?).
Such errors aremanually verified for correctness.Example 3:Kya  aaj tum office gaye ?Did you go to the office today ?2.2 Word AlignmentIn the case of generating word alignments it isbeneficial to calculate alignments in both transla-tion directions (English ?
Urdu and Urdu - Eng-lish).
This nature of symmetry will help to re-duce alignment errors.
We use the BerkeleyAligner6 word alignment package which imple-ments a joint training model with posterior de-coding (Liang et al, 2006) to consider bidirec-tional alignments.
Predictions are made based onthe agreements obtained by two bidirectionalmodels in the training phase.
The intuitive objec-tive function that incorporates data likelihoodand a measure of agreement between the modelsis maximized using an EM-like algorithm.
Thisalignment model is known to provide 29% re-duction in AER over IBM model 4 predictions.On our data set the word alignment accuracyis 71.3% (calculated over 200 sentence pairs).
Inorder to augment the alignment accuracy, weadded 3000 Urdu-English words and phrases ob-tained from the Urdu-English dictionary to ourparallel corpus.
The alignment accuracy im-proved by 3% as the lexicon affects the word co-occurrence count.Word alignment in itself does not produce ac-curate semantic role projections from English toUrdu.
This is because the verb predicates in Urducan span more than one token.
Semantic roles6 http://nlp.cs.berkeley.edu/Main.html799can cover sentential constituents of arbitrarylength, and simply using word alignments forprojection is likely to result in wrong role spans.Also, alignments are not obtained for all words.This could lead to missing projections.One way to correct these alignment errors is todevise token based heuristic rules.
This is notvery beneficial as writing generic rules is diffi-cult and different errors demand specific rules.We propose a method that considers POS, tenseand chunk information along with word align-ments to project annotations.Figure 1: Projection modelOur proposed approach can be explained intwo stages as shown in figure 1.
In Stage 1 onlyverb predicates are transferred from English toUrdu.
Stage 2 involves transfer of arguments anddepends on the output of Stage 1.
Predicatetransfer cannot rely entirely on word alignments(?3).
Rules devised around the chunk boundariesboost the verb predicate recognition rate.Any verb group sequence consisting of a mainverb and its auxiliaries are marked as a verbchunk.
Urdu data is tagged using the chunk tagset proposed exclusively for Indian languages byBharati et al, (2006).
Table 1 shows the tags thatare important for this task.Verb Chunk DescriptionVGF Verb group is finite  (decided by the auxiliaries)VGNF Verb group for non-finite adverbial and adjectival chunkVGNN Verb group has a gerundTable 1: Verb chunk tags in UrduThe sentence aligned parallel corpora that wefeed as input to our model is POS tagged for bothEnglish and Urdu.
Urdu data is also tagged forchunk boundaries and morphological featureslike tense, gender and number information.Named Entities are also marked on the Urdu dataset as they help in tagging the ARGM arguments.All the NLP taggers (POS, NE, Chunker, andMorphological Analyzer) used in this work aredetailed in Mukund et al, (2010).English data is not chunked using a conven-tional chunk tagger.
Each English sentence issplit into virtual phrases at boundaries deter-mined by the following parts of speech ?
IN, TO,MD, POS, CC, DT, SYM,: (Penn Tree Bank tag-set).
These tags represent positions in a sentencethat typically mark context transitions (they aremostly the closed class words).
We show laterhow these approximate chunks assist in correct-ing predicate mappings.We use an Urdu-English dictionary (?2.1) thatassigns English meanings to Urdu words in eachsentence.
Using translation information from adictionary can help transfer verb predicates whenthe translation equivalent preserves the lexicalmeaning of the source language.The first rule that gets applied for predicatetransfer is based on lexicon lookup.
If the Eng-lish verb is found to be a synonym to an Urduword that is part of a verb chunk, then the lemmaassociated with the English word is transferred tothe entire verb chunk in Urdu.
However not alltranslations?
equivalents are lexically synonym-ous.
Sometimes the word used in Urdu is differ-ent in meaning to that in English but relevant inthe context (lexical divergence).The word alignments considered in proximityto the approximate English chunks come to res-cue in such scenarios.
Here, for all the wordsoccurring in each Urdu verb chunk, correspond-ing English aligned words are found from theword alignments.
If the words that are found be-long to the same approximate English chunk,then the verb predicate of that chunk (if present)is projected onto the verb chunk in Urdu.
Thisheuristic technique increases the verb projectionaccuracy by about 15% as shown in ?4.The Penn tree bank tag set for English part ofspeech has different tags for verbs based on thetense information.
VBD is used to indicate pasttense, and VBP and VBZ for present tense.
Urdu800also has the tense information associated with theverbs in some cases.
We exploit this similarity toproject the verb predicates from English ontoUrdu.The adverbial chunk in Urdu includes pure ad-verbial phrases.
These chunks also form part ofthe verb predicates.SRBP          NP                        VGNFRB         NN   VB     AUXA(?????
?/dobara)     (??
?/jaan)  (???
?/dali)        (??
?/gayi)[English meaning ?
Revitalized]Figure 2: example for demotional divergenceE.g.
consider the English word ?revitalized?
(figure 2).
This is tagged VBD.
However, the Ur-du equivalent of this word is ???????
???
????
????
(dobara jaan daali gayi ~ to put life in again).The POS tags are ?RB, NN, VB, AUXA?
(adverb,noun, verb, aspectual auxiliary).
The word ?do-bara?
is a part of the adverbial chunk RBP andthe infinite verb chunk VGNF spans across thelast two words ?daali gayi?.
?jaan?
is a nounchunk.
This kind of demotional divergence iscommonly observed in languages like Hindi andUrdu.
In order to consider this entire phrase to bethe Urdu equivalent representation of the Englishword ?revitalized?, a rule for adverbial chunk isincluded as the last step to account for un-accommodated English verbs in the projections.In the PropBank corpus, predicate argument re-lations are marked for almost all occurrences ofnon-copula verbs.
We however do not have POStags that help to identify non-copula words.Words that can be auxiliary verbs occur as non-copula verbs in Urdu.
We maintain a list of suchauxiliary verbs.
When the verb chunk in Urducontains only one word and belongs to the list,we simply ignore the verb chunk and proceed tothe next chunk.
This avoids several false posi-tives in verb projections.Stage 2 of the model includes the transfer ofarguments.
In order to see how well our methodworks, we project all argument annotations fromEnglish onto Urdu.
We do not consider wordalignments for arguments with proper nouns.
Thedouble metaphone algorithm (Philips 2000) isapplied on both English NNP (proper noun)tagged words as well as English transliteratedUrdu (NNP) tagged words.
Arguments fromEnglish are mapped onto Urdu for word pairswith the same metaphone code.For other arguments, we consider word align-ments in proximity to verb predicates.
The argu-ment boundaries are determined based on chunkand POS information.
We observe that our me-thod projects the annotations associated withnouns fairly well.
However, when the argumentscontain adjectives, the boundaries are disconti-nuous.
In such cases, we consider the entirechunk without the case marker as a probablecandidate for the projected argument.
We alsohave some issues with the ARGM-MOD argu-ments in that they overlap with the verb predi-cates.
When the verb predicate that it overlapswith is a complex predicate, we consider the en-tire verb chunk to be the Urdu equivalent candi-date argument.
These rules along with wordalignments yield fairly accurate projections.The rules that we propose are dependent on thePOS, chunk and tense information that are lan-guage specific.
Hence our method is languageindependent only to the extent that the new lan-guage considered should have similar syntacticstructure as Urdu.
Indic languages fall in thiscategory.3 Verb PredicatesDetecting verb predicates can be a challengingtask especially if very reliable and efficient toolssuch as POS tagger and chunkers are not availa-ble.
We apply the POS tagger (CRULP tagset,88% F-Score) and Chunker (Hindi tagset, 90%F-Score) provided by Mukund et al, (2010) onthe Urdu data set and show that syntactic infor-mation helps to compensate alignment errors.Stanford POS tagger7 (Penn Tree bank tagset) isapplied on the English data set.Predicates can be simple predicates that liewithin the chunk boundary or complex predicateswhen they span across chunk boundaries.
Whenverbs in English are expressed in Urdu/Hindi, inseveral cases, more than one word is used toachieve perfect translation.
In English the tenseof the verb is mostly captured by the verb mor-pheme such as ?asked?
?said?
?saying?.
In Ur-du the tense is mostly captured by the auxiliaryverbs.
So a single word English verb such as?talking?
would be translated into two words7 http://nlp.stanford.edu/software/tagger.shtml801?batein karna?
where ?karna?~ do is the aux-iliary verb.
However this cannot be generalizedas there are instances when translations are wordto word.
E.g.
?said?
is mapped to a single wordUrdu verb ?kaha?.Complex predicates in Urdu can occur in thefollowing POS combinations.
/oun+Verb, Ad-jective+Verb, Verb+Verb, Adverb+Verb.
Table 2lists the main verb tags present in the Urdu POStagset.
(refer Penn Tree bank POS tagset forEnglish tags).Urdu Tags DescriptionVB VerbVBI Infinitive VerbVBL Light VerbVBLI Infinitive Light VerbVBT Verb to beAUXA Aspectual AuxiliaryAUXT Tense AuxiliaryTable 2: Verb tagsAuxiliary verbs in Urdu occur alongside VB,VBI, VBL or VBLI tags.
Sinha (2009) definescomplex predicates as a group of words consist-ing of a noun (NN/NNP), an adjective (JJ), a verb(VB) or an adverb (RB) followed by a light verb(VBL/VBLI).
Light verbs are those which contri-bute to the tense and agreement of the verb (Buttand Geuder, 2001).
However, despite the exis-tence of a light verb tag, it is noticed that in sev-eral sentences, verbs followed by auxiliary verbsneed to be grouped as a single predicate.
Hence,we consider such combinations as belonging tothe complex predicate category.E1G- According_VBG to_TO some_DT estimates_NNSthe_DT rule_NN changes_NNS would_MD cut_VB insid-er_NN filings_NNS by_IN more_JJR than_IN a_DTthird_JJURD- [Kuch_QN  andaazon_NN  ke_CM  muta-biq_NNCM]_NP [kanoon_NN mein_CM]_NP [tabdee-liayan_NN]_NP[ androni_JJ    drjbndywn_NNko_CM]_NP [ayk_CD thayiy_FR se_CM]_NP [zyada_Ikam_JJ]_JJP [karey_VBL gi_AUXT]_VGFExample 4Example 4 demonstrates the existence of a lightverb in a complex predicate.
The English verb?cut?
is mapped to ???
????
???
(kam karey gi)belonging to the VBF chunk group.EG- Rolls_NNP -_: Royce_NNP Motor_NNPCars_NNPS Inc._NNP said_VBD it_PRP expects_VBZits_PRP$ U.S._NNP sales_NNS to_TO remain_VBsteady_JJ at_IN about_IN 1 200_CD cars_NNS in_IN1990_CDURD - [Rolls  Royce motor car inc_NNPC ne_CM]_NP[kaha_VB]_VBNF [wo_PRP]_NP [apney_PRRFP$]_NP[U.S._NNP ki_CM]_NP [ frwKt_NN ko_CM]_NP[1990_CD mein_CM]_NP [takreeban_RB]_RBP [1200_CDkaron_NN par_CM]_NP [mtwazn_JJ]_JJP [rakhne_VBIki_CM]_VGNN [tawaqqo_NN]_NP [karte_VBhai_AUXT]_VGFExample 5In example 5, ?said?
corresponds to one Urduword ?????
(kaha) that also captures the tense in-formation (past).
However, consider the verb?expects?.
This is a clear case of noun-verbcomplex predicate where ?expects?
is mapped to?????
????
???
(tawaqqo karte hai).E1G- /ot_RB all_PDT those_DT who_WP wrote_VBDoppose_VBP the_DT changes_NNSURD -wo tamaam  jinhon ne likha tabdeeliyon ke [mukha-lif_JJ]_JJP [nahi_RB]_RBP [hain_VBT]_VGFExample 6In example 6, verb predicates are ?wrote?
and?oppose?.
Consider the word ?oppose?.
Thereare two ways of representing this word in Urdu.As a verb chunk the translation would be ?muk-halifat nahi karte?
and as an adjectival chunk?mukhalif nahi hai?.
The latter form of repre-sentation is used widely in the available transla-tion corpus.
The Urdu equivalent of ?oppose?
is??????
????
(mukhalif hai).Another interesting observation in example 6 isthe existence of discontinuous predicates.Though ?oppose?
is one word in English, theUrdu representation has two words that do notoccur together.
The adverb ?nahi?
~?not?
oc-curs between the adjective and the verb.
Statisti-cally dealing with this issue is extremely chal-lenging and affects the boundaries of other ar-guments.
Generalizing the rules needed to identi-fy discontinuous predicates requires more de-tailed analysis of the corpus ?
from the linguisticaspect ?
and has not been attempted in this paper.We however map ?
???
????
?????
?
(mukhalif nahihai) to the predicate ?oppose?.
?nahi?
is treatedas an argument ARG_NEG in PropBank.4 Projection ResultsIt is impossible for us to report our projectionresults on the entire data set as we do not have itmanually annotated.
For the purpose of evalua-tion, we manually annotated 100 long sentences(L) and 100 short sentences (S) from the full2350 sentence set.
All the results are reported on802this 200 set of sentences.
Set L has sentences thateach has more than two verb predicates and sev-eral arguments.
The number of words per sen-tence here is greater than 55.
S; on the otherhand has sentences with about 40 words each andno complex SOV structures.The results shown in Table 3 are for all tags(verbs+args) that are projected from English ontoUrdu.
In order to understand why the perfor-mance over L dips, consider the results in Table4 that are for verb projections only.
Some longsentences in English have Urdu translations thatdo not maintain the same structure.
For examplean English phrase ?
??
might prompt individu-als to get out of stocks altogether?
is written inUrdu in a way that the English representationwould be ?what makes individuals to get out ofstocks is ??.
The Urdu equivalent word for?prompt?
is missing and the associated lemmagets assigned to the Urdu equivalent of ?get?
(the next lemma).
This also affects the argumentprojections.
Another reason is the effect of wordalignments itself.
Clearly longer sentences havegreater alignment errors.All tags8 100 long sentences100 shortsentencesActual Tags 1267 372Correct Tags 943 325Found Tags 1212 353L :  Precision 77.8% Recall 74.4% F-Score 76%S:  Precision 92% Recall 87.4% F-Score 89.7%Table 3: when all tags are consideredComparing the results of Table 4 to Table 3,we see that argument projections affect the re-call.
This is because the projections of argumentsdepend not only on the word alignments but alsoon the verb predicates.
Incorrect verb predicatesaffect the argument projections.Only lemma 100 long sentences100 shortsentencesActual Tags 670 240Correct Tags 490 208Overall Tags 720 257L: Precision 68% Recall 73.1% F-Score 70.45%S : Precision 80.9% Recall 86.6% F-Score 83.65%Table 4: for verb projections onlyTable 5 summarizes the results obtained whenonly the word alignments are considered to8 Tags -  lemma (verb predicates) + arguments, Actual tags?
number of tags in the English set, Found tags ?
number oftags transferred to Urdu, Correct Tags ?
number of tagscorrectly transferredproject all tags.
But when virtual phrase bounda-ries in English are also considered, the F-scoreimproves by 8% (Table 6).
This is because vir-tual boundaries in a way mark context switch andwhen considered in proximity to the word align-ments yield better predicate boundaries.100 long sentences : only alignmentsActual Tags 1267Correct Tags 617Overall Tags 782Precision 78.9% Recall 48.7% F-Score 60.2%Table 5: with only word alignments100 long sentences : alignments + virtual boundariesActual Tags 1267Correct Tags 792Overall Tags 1044Precision 75.8% Recall  62.5% F-Score 68.5%Table 6: with word alignments and virtual boundaries100SentencesARG0ARG1ARG2ARG3ARGMLong 124 271 67 25 140Found 111 203 36 12 114P % 89.5 74.9 53.7 48 81.42Short 34 47 4 2 19Found 30 45 4 2 19P % 88.2 95.7 100 100 100Table 7: results of argument projectionsPrecision (P) on argumentsTable 7 shows the results of argument projec-tions over the first 4 arguments of PropBank ?ARG0, ARG1, ARG2 and ARG3 (out of 24 argu-ments, majority are sparse in our test set) and theadjunct tag set ARGM.5 Automatic DetectionThe size of SRL annotated corpus generated forUrdu is limited with only 2350 sentences.
Toexplore the possibilities of augmenting this dataset, we train verb predicate and argument detec-tion models.
The results show great promise ingenerating large-scale automatic annotations.5.1 Verb Predicate DetectionVerb predicate detection happens in two stag-es.
In the first stage, the predicate boundaries aremarked using a CRF (Lafferty et al, 2001) basedsequence labeling approach.
The training data forthe model is generated by annotating the auto-matically annotated Urdu SRL corpus using BI803annotations.
E.g.
kam B-VG, karne par I-VG.
Thenon-verb predicates are labeled ?-1?.
The modeluses POS, chunk and lexical information as fea-tures.
We report the results on a set of 77 sen-tences containing a mix of short and long sen-tences.Number of verb predicates correctly marked 377Num of verb predicates found 484Actual num of verb predicates 451Precision 77.8% Recall 83.5% F-Score 80.54%Table 8: CRF results for verb boundariesEvery verb predicate is associated with a lemmamapped from the English VerbNet map file9.
E.g.the Urdu verb ???
????
???
(kam karne par) hasthe lemma ?lower?.
The second stage includesassigning these lemmas.
Lemma assignment isbased on lookups from a VerbNet like map file.We have compiled a large set of Urdu verb pre-dicates by mapping translations found in the au-tomatically annotated corpus to the VerbNet mapfile.
This Urdu verb predicate list also accommo-dates complex predicates that occur along withverbs such as ?karna ?
to do?, ?paana ?
to get?,etc.
(along with different variations of theseverbs ?
karte, kiya, paate etc.).
This verb predi-cate list (manually corrected) consists of 800 en-tries.
Since our gold standard test set is verysmall, the lemma assignment for all verb predi-cates is 100% (no pb values and hence nosenses).
This list, however, has to be augmentedfurther to meet the standards of the EnglishVerbNet map file.5.2 Argument DetectionArgument detection (SRL) is done in two steps:(1) argument boundary detection (2) argumentlabel assignment.
We perform tests for step 2 toshow how well a standard SVM role detectionmodel works on the automatically generated Ur-du data set.
For each pair of correct predicate pand an argument i we create a feature representa-tion F p,a  ~ set T of all arguments.
To train a mul-ti-class role-classifier, given the set T of all ar-guments, T can be rationalized as T arg i+  (positiveinstances) and T arg i?
(negative instances) for eachargument i.
In this way, individual ONE-vs-ALL(Gildea and Jurafsky, 2002) classifier for each9 http://verbs.colorado.edu/semlink/semlink1.1/vn-pb/README.TXTargument i is trained.
In the testing phrase, givenan unseen sentence, for each argument Fp,q isgenerated and classified by each individual clas-sifier.We created a set of standard SRL features asshown in table 9.
The results (Tables 10 and 11),though not impressive, are promising.
We be-lieve that by increasing the number of samples(for each argument) in the training set and intel-ligently controlling the negative samples, theresults can be improved significantly.Training ?
2270 sentences with 7315 argument instances.Test ?
77 sentences with 496 argument instances.
(22 dif-ferent role types)BaseLineFeatures(BL)phrase-type (syntactic category; NP, PP etc.
),predicate (in our case, verb group), path (syn-tactic path from the argument constituent tothe predicate), head words (argument and thepredicate respectively), position (whether thephrase is before or after the predicate)DetailedFeaturesBL + POS (of the first word in the predicate),chunk tag of the predicate, POS (of the firstword of the constituent argument), head word(of the verb group in a complex predicate),named entity (whether the argument containsany named entity, such as location, person,organization etc.
)Table 9: Features for SRLKernel/features Precision Recall F-ScoreLK ?
BL 71.88 48.25 57.74LK ?
all 73.91 47.55 57.87PK ?
BL 74.19 48.25 58.47PK ?all (best) 73.47 49.65 59.26Table 10: Arg0 performanceKernel/features Precision Recall F-ScoreLK ?
BL 69.35 22.87 34.40LK ?
all 69.84 23.4 35.05PK ?
BL 73.77 24.14 36.38PK ?all (best) 73.8 26.06 38.52Table 11: Arg1 Performances(PK - polynomial kernel LK ?
Linear kernel)6 ConclusionIn this work, we develop an alignment systemthat is tailor made to fit the SRL problem scopefor Urdu.
Furthermore, we have shown that de-spite English being a totally different language,resources for Urdu can be generated if the subtlegrammatical nuances of Urdu are accounted forwhile projecting the annotations.
We plan towork on argument boundary detection and ex-plore other features for argument detection.
Thelemma set generated for Urdu is being refined forfiner granularity.804ReferencesAmbati, Vamshi and Chen, Wei, 2007.
Cross Lingual Syn-tax Projection for Resource-Poor Languages.
CMU.Baker, Collin .F., Charles J. Fillmore, John B. Lowe.
1998.The Berkeley Frame Net project.
COLI/G-ACL.Bharati, Akshar, Dipti Misra Sharma, Lakshmi Bai andRajeev Sangal.
2006.
AnnCorra: Annotating CorporaGuidelines For POS And Chunk Annotation For IndianLanguage.
Technical Report, Language TechnologiesResearch Centre IIIT, Hyderabad.Burchardt, Aljoscha and Anette Frank.
2006.
Approachingtextual entailment with LFG and FrameNet frames.
RTE-2 Workshop.
Venice, Italy.Butt, Miriam and Wilhelm Geuder.
2001.
On the(semi)lexical status of light verbs.
/orbert Corver andHenk van Riemsdijk, (Eds.
), Semi-lexical Categories: Onthe content of function words and the function of contentwords, Mouton de Gruyter, pp.
323?370, Berlin.Diab, Mona and Philip Resnik.
2002.
An unsupervised me-thod for word sense tagging using parallel corpora.
40thAnnual Meeting of ACL, pp.
255-262, Philadelphia, PA.Dorr, Bonnie, J.
1994.
Machine Translation Divergences: AFormal Description and Proposed Solution.
ACL, Vol.20(4), pp.
597-631.Fillmore, Charles J.
1968.
The case for case.
Bach, &Harms( Eds.
), Universals in Linguistic Theory, pp.
1-88.Holt, Rinehart, and Winston, New York.Fillmore, Charles J.
1982.
Frame semantics.
Linguistics inthe Morning Calm, pp.111-137.
Hanshin, Seoul, S. Ko-rea.Fung, Pascale and Benfeng Chen.
2004.
BiFrameNet: Bilin-gual frame semantics resources construction by cross-lingual induction.
20th International Conference onComputational Linguistics, pp.
931-935, Geneva, Swit-zerland.Fung, Pascale, Zhaojun Wu, Yongsheng Yang and DekaiWu.
2007.
Learning bilingual semantic frames: Shallowsemantic parsing vs. semantic role projection.
11th Con-ference on Theoretical and Methodological Issues inMachine Translation, pp.
75-84, Skovde, Sweden.Gildea, Daniel and Daniel Jurafsky.
2002.
Automatic labe-ling of semantic roles.
Computational Linguistics, Vol.28(3), pp.
245-288.Hi, Chenhai and Rebecca Hwa.
2005.
A backoff model forbootstrapping resources for non-english languages.
JointHuman Language Technology Conference and Confe-rence on EM/LP, pp.
851-858, Vancouver, BC.Hwa, Rebecca, Philip Resnik, Amy Weinberg, and OkanKolak.
2002.
Evaluation translational correspondance us-ing annotation projection.
40th Annual Meeting of ACL,pp.
392-399, Philadelphia, PA.Koehn, Phillip.
2005.
?Europarl: A parallel corpus for statis-tical machine translation,?
MT summit, Citeseer.Lafferty, John D., Andrew McCallum and C.N.
Pereira.2001.
Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
18th Inter-national Conference on Machine Learning, pp.
282-289.Liang, Percy, Ben Taskar, and Dan Klein.
2006.
Alignmentby Agreement, /AACL.Marcus, Mitchell P., Beatrice Santorini and Mary Ann Mar-cinkiewicz.
2004.
Building a large annotated corpus ofEnglish: The Penn Treebank.
Computational Linguistics,Vol.
19(2), pp.
313-330.Moschitti, Alessandro.
2008.
Kernel methods, syntax andsemantics for relational text categorization.
17th ACMCIKM, pp.
253-262, Napa Valley, CA.Mukerjee, Amitabh , Ankit Soni and Achala M. Raina.2006.
Detecting Complex Predicates in Hindi using POSProjection across Parallel Corpora.
Workshop on Multi-word Expressions: Identifying and Exploiting Underly-ing Properties, pp.
11?18.
Sydney.Mukund, S., Srihari, R. K., and Peterson, E. 2010.
An In-formation Extraction System for Urdu ?
A Resource PoorLanguage.
Special Issue on Information Retrieval for In-dian Languages.
TALIP.Pado, Sebastian and Mirella Lapata.
2009.
Cross-Lingualannotation Projection of Semantic Roles.
Journal of Ar-tificial Intelligence Research, Vol.
36, pp.
307-340.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of semanticroles.
Computational Linguistics, Vol.
31(1).Palmer, Martha, Shijong Ryu, Jinyoung Choi, SinwonYoon, and Yeongmi Jeon.
2006.
Korean Propbank.
Lin-guistic data consortium, Philadelphia.Philips, Lawrence.
2000.
The Double Metaphone SearchAlgorithm.
C/C++ Users Journal.Sinha, R. Mahesh K. 2009.
Mining Complex Predicates InHindi Using A Parallel Hindi-English Corpus.
ACL In-ternational Joint Conference in /atural LanguageProcessing, pp 40.Surdeanu, Mihai, Sanda Harabagiu, John Williams and PaulAarseth.
2003.
Using predicate-argument structures forinformation extraction.
41st Annual Meeting of the Asso-ciation for Computational Linguistics, pp.
8-15, Sappo-ro, Japan.Taule, Mariona, M. Antonio Marti, and Marta Recasens.2008.
Ancora: Multi level annotated corpora for Catalanand Spanish.
6th International Conference on LanguageResources and Evaluation, Marrakesh, Morocco.Xue, Nianwen and Martha Palmer.
2009.
Adding semanticroles to the Chinese treebank.
/atural Language Engi-neering, Vol.
15(1), pp.
143-172.Yarowsky, David, Grace Ngai and Richard Wicentowski.2001.
Inducing multi lingual text analysis tools via ro-bust projection across aligned corpora.
1st Human Lan-guage Technology Conference, pp.
161-168, San Fran-cisco, CA.805
