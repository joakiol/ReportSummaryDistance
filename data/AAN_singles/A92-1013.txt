Computational Lexicons: the Neat Examples and the Odd ExemplarsRoberto Basil i , Maria Teresa PazienzaDip.
di Ingegneria Elettronica, Universita'"Tor Vergata", Roma, ItalyPaola VelardiIst.
di Informatica, Universita' di Ancona,Ancona, ItalyAbstractWhen implementing computational lexicons it isimportant o keep in mind the texts that a NLPsystem must deal with.
Words relate to eachother in many different, often queer, ways: thisinformation is rarely found in dictionaries, and itis quite hard to be invented a priori, despite theimagination that linguists exhibit at inventingesoteric examples.In this paper we present the results of anexperiment in learning from corpora the frequentselectional restrictions holding between contentwords.
The method is based on the analysis ofword associations augmented with syntacticmarkers and semantic tags.
Word pairs areextracted by a morphosyntactic analyzer andclustered according to their semantic tags.
Astatistical measure is applied to the data toevaluate the significance of a detected relation.Clustered association data render the study ofword associations more interesting with severalrespects: data are more reliable even for smallercorpora, more easy to interpret, and have manypractical applications in NLP.1.
IntroductionOne of the fundamental property of computationallexicons is an account of the relations between verbsand its arguments.
Arguments are identified by theirposition in a predicate-argument structure, or byconceptual relations names (e.g.
agent, purpose,location, etc).
Arguments are annotated withselectional restrictions, that impose type constraintson the set of content words that may fill a relation.Selectional restrictions often do not provide all thesemantic information that is necessary in NLPsystems, however they are at the basis of themajority of computational pproaches to syntacticand semantic disambiguation.It has been noticed that representing only thesemantics of verbs may be inadequate (Velardi et al1988; Boguraev 1991; Macpherson 1991).
The notion ofspreading the semantic load supports the idea thatevery content word should be represented in thelexicon as the union of all the situations in which itcould potentially participate.
Unfortunately, handwriting selectional restrictions i  not an easy matter,because it is time consuming and it is hard to keepconsistency among the data when the lexicon hasseveral hundred or thousand words.
However themajor difficulty is that words relate to each other inmany different, often domain dependent ways.
Thenowadays vast literature on computational lexiconsis f i l led with neat  examples  of theeat(animate,food) flavour, but in practice in manylanguage domains selectional constraints betweenwords are quite odd.
It is not just a matter of violatingthe semantic expectations, such as in "kill theprocess" or "my car drinks gasoline", neither it isthat kind of fancifulness that linguists exhibit atf inding queer sentences.
Rather, there existstatistically relevant linguistic relations that arehard to imagine a-priori, almost never found indictionaries, and even harder to assign to theappropriate slot in the whatever conceptual structureadopted for lexical representation.
Several examplesof such relations are shown throughout this paper.Ideally, knowledge on word relations should beacquired directly from massive amounts of texts~rather than from hand-crafted rules.
This idea is a!the basis of many recent studies on word associations.The results of these studies have importantapplications in lexicography, to detect lexico-syntactic regularities (Church and Hanks, 19901(Calzolari and Bindi,1990), such as, for example~support verbs (e.g.
"make-decision") prepositionalverbs (e.g.
"rely-upon") idioms, semantic relations(e.g.
"part_of") and fixed expressions (e.g.
"kick thebucket").
In (Hindle,1990; Zernik, 1989; Webster elMarcus, 1989) cooccurrence analyses augmented withsyntactic parsing is used for the purpose of wordclassification.
All these studies are based on th~(strong) assumption that syntactic similarity in wor(~patterns implies semantic similarity.
In (Guthrie elal., 1991), sets of consistently contiguous word~,("neighbourhood") are extracted from machine-readable  d ic t ionar ies ,  to help semanticdisambiguation i information retrieval.
In (Smadj~and McKeown, 1990) statist ical ly collectecassociations provide pragmatic ues for lexical choic(in sentence generation.
For example, we can learrthat "make decision" is a better choice than, say96"have decision" or "take decision".
(Hindle andRooths,  1991) proposes  that a syntact icdisambiguation criterion can be gathered bycomparing the probability of occurrence of noun-preposition and verb-preposition pairs in V NP PPstructures.In general word associations are collected byextracting word pairs in a +-5 window.
In (Calzolariand Bindi, 1990), (Church and Hanks, 1990) thesignificance of an association (x,y) is measured by themutual information I(x,y), i.e.
the probability ofobserving x and y together, compared with theprobability of observing x and y independently.
In(Smadja, 1989), (Zernik and Jacobs, 1990), theassociations are filtered by selecting the word pairs(x,y) whose frequency of occurrence is above f+ks,where f is the average appearance, s is the standarddeviation, and k is an empirically determined factor.
(Hindle, 1990; Hindle and Rooths,1991) and (Smadja,1991) use syntactic markers to increase thesignificance of the data.
(Guthrie et al, 1991\] usesthe subject classification given in machine-readabledictionaries (e.g.
economics, engineering, etc.)
toreinforce cooccurence links.Despite the use of these methods to add evidence tothe data, the major problem with word-pairscollections is that reliable results are obtained onlyfor a small subset of high-frequency words on verylarge corpora, otherwise the association ratiobecomes unstable.
For example, Church run hisexperiment on a corpus with over 20-30 millionswords, and Hindle reports 6 millions words as notbeing an adequate corpus.
In many practical NLP/IRapplications corpora are not so large, and typicallyspan from 500,000 to a few million words.
Theanalysis of associations could be done on widerdomains, but a part for very general words, it is muchmore desirable to collect data from the applicationcorpus.
Information collected from other sources couldadd noise rather than strengthening the data,because in most applications jargon, technical words,and domain-dependent associations are the norm.
In(Smadja, 1989b) it is shown a table of operationalpairs like adjective-noun and verb-object, from whichclearly emerges the very different nature of the twosource domains (Unix Usenet and Jerusalem Post).
Forexample, the noun-noun pairs with "tree" includeassociations such as "parse, grammar, decision" and"olive, Christmas".
If the NLP/ IR application isabout the computer world, associations such as "olivetree" or "Christmas tree" are (at best) useless.A second problem with statistically collected wordpairs is that an analysis based simply on surfacedistribution may produce data at a level ofgranular i ty too fine.
For example, a purelydistributional analysis for word classification, suchas those cited above, might place two verbs intodistinct classes because one is used primarily with anobject olive and the other with the object grape.
Thismay not be appropriate given the application.Abstraction via semantic lasses (e.g.
VEGETABLE),would ensure that the ontology found is appropriatefor the domain.
The model of preposit ionalattachment preference proposed by Hindle is also tooweak if applied only to verb-preposition and noun-preposition pairs.
A preposition may or may not berelated to a verb, even if it frequently cooccurs withit, depending upon the underlying semantic relation.It is the semantic ategory of the noun following apreposition that determines the nature of thesemantic link (e.g.
for+ HUMAN ENTITY =beneficiary, for+ACTION = purpose), and ultimatelyinfluences the choice of  the proper attachment.Semantic abstraction also renders the data morereadable.
Millions of simple word cooccurrences letthe experimenter sink in an ocean of data, withoutproviding much insight of the conceptual nature ofthe detected associations.In this paper, we present a study on word associationsaugmented with syntactic markers and semantictagging.
We call these data clustered associations.Clustered association data are syntactic pairs ortriples (e.g.
N_V(\]ohn,go) V prep_N(go, to,Boston),N_prepN(Boston,by,bus) 1) in which one or bothcontent words are replaced by their semantic tag(e .g .
V_prep_N(PHYS ICAL  ACT- to -PLACE),N_prep_N(PLACE-by-MACHINE) etc.
).Semantic tags are very high-level in order to reducethe cost of hand-tagging.Clustered association data have several advantages:- First, statistically meaningful data can begathered from (relatively) small corpora;- Second, data are presented in a compact form andare much more readable;- Third, clustered association data are useful formany interesting NLP applications, such asconceptual clustering, syntactic and semanticdisambiguation, and semi-automatic learning ofthe relevant selectional restrictions in a givenlanguage domain.In this paper we discuss the results of an experimentin learning selectional restrictions, to provide supportfor the design of computational lexicons.
Otherresults are presented in (Basili et al, 1991; Fabrizi etal., forthcomingl.1 We did not want to schock the reader withqueer examples ince the introduction.97The method is applied to a corpus of economicenterprise descriptions, registered at the Chambersof Commerce in Italy.
The database of thesedescriptions (in total over 1,000,000 descriptions,each spanning from 1 to 100-200 words) is managed inItaly by the Company CERVED.
Sentences describeone or several commercial enterprises carried out by agiven Company.
Examples of these descriptions areprovided throughout the text.
In our experiment, weused only 25,000 descriptions, including about 500,000words.
A second experiment on a legal corpus is underpreparation and will be ready shortly.2 Acqu i r ing  syntact i c  assoc ia t ionsClustered association data are collected by firstextracting from the corpus all the syntacticallyrelated word pairs.Combining statistical and parsing methods has beendone by (Hindle, 1990; Hindle and Rooths,1991) and(Smadja and McKewon, 1990; Smadja,1991).
Thenovel aspect of our study is that we collect not onlyoperational pairs, but triples, such as N_prep N,V_prep_N etc.
In fact, the preposition conveyimportant information on the nature of the semanticlink between syntactically related content words.
Bylooking at the preposition, it is possible to restrictthe set of semantic relations underlying a syntacticrelation (e.g.
for=purpose,beneficiary).To extract syntactic associations two methods havebeen adopted in the literature.
Smadja attempts toapply syntactic information to a set of automaticallycollected collocations (statistics-first).
Hindleperforms syntactic parsing before collocationalanalysis (syntax-first).
In our study, we decided toadopt the syntax-first approach, because:- as remarked above, it is important to extract notonly syntactic pairs, but also triples;- statistically collected associations miss somesyntactic relation between distant words incoordinate constructions (usually the window inwhich word pairs are extracted is +-5) and couplemany words that are not syntactically related.Even though (Smadja,1991) reports goodperformances of his system, it must be noticed thatthe precision and efficiency figures of the parserapply to a set of data that have been already(statistically) processed.
Thus the actualprecision and efficiency in extractingsyntactically related words from the source corpusmay be lower than expected.As in other similar works, the syntactic analyzerused in this study does not rely on a complete Italiangrammar.
The parser only detects the sur facesyntactic relations between words.
A full descriptionof the analyzer is outside the scope of this paper (see(Marziali, 1991) for details).
In short, the parserconsists of a segmentation algorithm to cut texts intophrases (NP, PP, VP etc), and a phrase parser that isable to detect he following 15 links: N_V, V_N,N_ADJ, N N, N_prep_N, V_prep_N, N_prep_V,V_prep_V, N_cong_N, ADJ_cong_ADJ, V_ADV,ADV_cong_ADV, V_cong V, N_prep_ADJ,V_prep_ADJ.The segmentation algorithm is very simple.
If thedomain sublanguage is good Italian, sentence cuttingis based on the presence ofverbs, punctuation, adverbssuch as when, if, because, etc: For more jergaldomains, such as the economic enterprise corpus, textcutting is based on heuristics uch as the detection ofa word classified as "activity" (Fasolo et a1.,1990).In fact, this domain is characterized by absence ofpunctuation, ill formed sentences, long nestedcoordinate constructions.The phrase parser is based on DCG (Pereira andWarren,1980), the most complex part of which is thetreatment of coordination.
The grammar consists ofabout 20 rules.
Rather than a parse tree, the output isa "fiat" set of syntactic relations between contentwords.
For example, parsing the sentence:fabbrica di scarpe per uomo e per bambino(*manufacture of shoes for man and child)produces the following relations:N_prep N(fabbrica ,di,scarpe)N prep_N(fabbrica,per,uomo)N_prep_N(fabbrica,per,bambino)N_prep_N(scarpe,per,uomo)N_prep_N(scarpe,per,bambino)N_cong_N(uomo,e,bambino)Unlike Church and Hindle, we are not interested incollecting binary or ternary relations between wordswithin a sentence, but rather in detecting recurringbinary syntactic associations in the corpus.
For thispurpose it is unnecessary to retrieve even partialparse trees.The complexity of the grammar is O(n2), that makesit computationally attractive for parsing largecorpora.
In (Marziali,1991) the efficiency andprecision of this grammar with respect to the full se!of surface syntactic links detectable by a completeDCG grammar are evaluated to be 85% and 90%.
Thereference output adopted to perform the evaluation isa syntactic graph (Seo and Simmons,1989)'.
Syntacticgraphs include in a unique graph the set of allpossible parse trees.
The evaluation was hand-made98over a set of 100 sentences belonging to three domains:the economic corpus, the legal corpus, and a novel.The performances are better for the legal corpus andthe novel, due to the ungrammaticality of theeconomic orpus.The relatively high efficiency rate, as comparedwith the figures reported in (Brent, 1991), are due tothe fact that Italian morphology is far more complexthan English.
Once a good morphologic analyzer isavailable (the one used in our work is very welltested, and has first described in (Russo,1987)),problems such as verb detection, raised in (Brent,1991), are negligible.
In addition, the text-cuttingalgorithm has positive effects on the precision.Despite this, we verified that about a 35% of thesyntactic associations extracted from the economiccorpus are semantically unrelated, due to syntacticambiguity.
As shown in the following sections,semantic clustering in part solves this problem,because semantically unrelated word pairs do notaccumulate statistical relevance, except for very rareand unfortunate cases.In any case, we need more experiments to verify theeffect of a more severe sentence cutting algorithm onthe precision at detecting semantically related pairs.This issue is particularly relevant for ungrammaticaltexts, as in the economic orpus.3.
Assigning semantic tagsThe set of syntactic associations extracted by theDCG parser are first clustered according to thecooccurring words and the type of syntactic link.
Afurther clustering is performed based on the semantictag associated to the cooccurring words.Clustering association data through semantic tagginghas two important advantages:First, it improves significantly the reliability ofassociation data, even for small corpora;Second, and more importantly, semantic tags make itexplicit the semantic nature of word relations.Manually adding semantic tags to words may appearvery expensive, but in fact it is not, if very broad,domain-dependent classes are selected.In our application, the following 13 categories wereadopted:PHYSICAL_ACT (packaging, travel, build, etc.
)MENTAL_ACT(sell, organize, handle, teach, etc.
)HUMAN_ENTITY (shareholder, company, person,farmer, tailor, etc.
)ANIMAL (cow, sheep, etc.
)VEGETABLE (carrots, grape, rubber, coffee, etc.
)MATERIAL (wood, iron, water, cement, etc.
)BUILDING (mill, shop, house, grocery, etc.
)BYPRODUCT (jam, milk, wine, drink, hide, etc.
)ARTIFACT (item, brickwork, toy, table, wears, etc.
)MACHINE (engine, tractor, grindstone,computer, etc.
)PLACE (ground, field, territory, Italy, sea, etc.
)QUALITY (green, chemical, coaxial, flexible, etc.
)MANNER (chemically, by-hand, retail, etc.
)These categories classify well enough the wordswhich are found in the selected sub-corpus as a test-bed for our research.
Some words received two tags:for example, there are sentences in which aBUILDING metonymically refers to the commercialACT held in that building (e.g.
"trade mills for theproduction.."); some word is both a BY_PRODUCT(e.g.
"wood carving") or a MATERIAL (e.g.
"handicrafts in wood").
Because the domain is veryspecific, double-tagging i s never due to polisemy.Once the definition of a semantic class is clearlystated, and with the help of a simple user interface,hand tagging a word is a matter of seconds.
Weadopted the policy that, whenever assigning a tag isnot obvious, or none of the available tags seemsadequate, the word is simply skipped.
Unclassifiedwords are less than 10% in our corpus.
Overall, weclassified over 5000 words (lemmata).
The activityof classification was absolutely negligible incomparison with all the other activities in thisresearch, both on the ground of time and requiredskill.Domain-dependent tags render the classificationtask more simple and ensure that the clusteredassociation data are appropriate given theapplication.
An obvious drawback is that it isnecessary to re-classify many words  if theapplication domain changes significantly.
Forexample, we are currently prepar ing a newexperiment on a legal corpus.
The domain issemantically more rich, hence we needed 15 classes.A first estimate revealed that about 30-40% of thewords need to be re-classified using more appropriatesemantic tags.4 Acquisition of selectional restrictionsClustered association data are at the basis of ourmethod to detect the important  selectionalrestrictions that hold in a given sublanguage.
Thestatistical significance of the detected relations ismeasured by the probability of cooccurrence of twoclasses C 1 and C 2 in the pattern C 1 synt-rel C2,where synt-rel is one of the syntactic relationsdetectable by the parser summarized in Section 2.99Rather than evaluating the probability Pr(C 1 synt-rel C2), we computed the conditioned probabilityP(C1,C2/synt-rel) estimated by:f(Cl,synt rel,C 2 )(1)f(synt_rel)The reason for using (1) rather than other measuresproposed in the literature, is that what matters hereis to detect all the statistically relevant phenomena,not necessarily all the meaningful associations.
Suchmeasures as the mutual information and the t-score(Church et al, 1991) give emphasis to the infrequentphenomena, because the statistical significance ofthe coupling between C 1 and C 2 is related to theprobability of occurrence of C 1 and C 2 independentlyfrom each other.
This would be useful at detectingrare but meaningful relations if one could rely on thecorrectness of the data.
Unfortunately, due tosyntactic ambiguity and errors in parsing, manysyntactic associations are not semantically related,i.e.
there exists no plausible selectional relationsbetween the to cooccurring words.
Using the mutualinformation, such relations could accumulatestatistical evidence.
The (1) is more conservative, butensures more reliable results.
In any case, we runseveral exper iments  with different statisticalmeasures, without being entirely happy with any ofthese.
Finding more appropriate statistical methodsis one of the future objectives of our work.Clustered association data are used to build tables,one for each syntactic structure, whose element (x,y)represents the statistical significance in the corpus ofa concept pair C 1 C 2.
All the relevant couplingsamong classes are identified by a human operatorwho inspects the tables, and labels concept pairs bythe appropriate conceptual relation.
Finding anappropriate set of conceptual relations is not an easytask.
In labeling concept pairs, we relied on ourpreceding work on semantic representation withConceptual Graph \[Pazienza nd Velardi, 1987\].Four of these tables are presented in the Appendix.The data have been collected from a corpus of about500,000 words.
The morphosyntactic analyzer takesabout 6-10 hours on a Spark station.
Clustering thedata takes about as much.
At first, we extracted onlythe V_N, N_prep N and V_prep_N associations,for a total of 52,155 different syntactic associations.The average is 5 occurrences for each association.At first glance, the data seem quite odd even to anItalian reader, but it turns out that the tables showexactly what the corpus includes.
Let us briefly gothrough the 4 tables.Table 1 summarizes the relations C l -per -C  2(per=for).
Some of the significant associations are:ARTIFACT - PHYSICAL_ACT (e.g.
: articoli per Iosport (*items for sport), attrezzi per giardinaggio(*tools for gardening))ARTIFACT - BUILDING (e.g.
biancheria per la casa(*linens for the house), mobili per negozi (*furnituresfor shops))MACHINE-BUILDING (e.g.
macchinari perlaboratori (*equipments for laboratories), macine permulini (*grindstones for mills))All the above relations subsume the usage (orfigurative_destination) relation.Notice that the "advertised" beneficiary relation isnot very significant in the corpus.
The onlystatistically relevant beneficiary relations areARTIFACT-for-HUMAN_ENTITY ( ( e.g.
calzatureper uomo (*shoes for man), biancheria per signora( * l inens  fo r  lady) )  andHUMAN_ENTITY_ for_HUMANENTITY  (e.g.parrucchire per signora (*hairdresser for lady).
Itappears that in the considered omain, verbs, exceptfor some, poorly relate with the preposition for (thisis the first surprise!
).Table 2 shows the Cl-in-C 2 relations.
Two relationsrepresent the large majority:ARTIFACT-in- BYPRODUCT (e.g.
calzature inpelle (*shoes in leather), guarnizioni in gomma(packings in rubber))ARTIFACT-in-MATERIAL (e.g.
oggetti in legnc(*handicrafts in wood) ringhiere in ferro (*banistersin iron))both subsume a matter elation (this is one of the few"expected" associations we found).Less frequent but interesting are the followin~relations:MENTAL_ACT-in-MENTAL_ACT (e.g.
concedere ir~appalto (*to grant in bid) acquistare in leasing (*tobuy in leasing))ARTIFACT-in-ARTIFACT (e.g.
prodotti in scatok(*products in can))While the second is a "reassuring" location relatior(subsumed also by the in-PLACE associations in th(last column), we are less sure about the semanti(interpretation of the first relation.
Tentatively, w(choosed the manner relation.
The same type o:relation is also found in the CI-a-C 2 (a=to,on) tabh(Table 3): MENTAL_ACT-a-MENTAL_ACT (e.gacquistare a credito (*to buy to (=on) credit)abilitare all'ottenimento (*qualifying to th4attainment), assistenza ll'insegnamento (assistenc,to the teaching))100The first example (on credit) clearly subsumes thesame relation as for in leasing; the following twoseem of a different nature.
We used figurative-destination to label these relations.
This may or maynot be the best interpretation: however, whatmatters here is not so much the human interpretationof the data, but rather the ability of the system atdetecting the relevant semantic associations,whatever their name could be.Once again in this table, notice that the commonrelations, like recipient (to-HUMAN_ENTITY) anddestination (to-PLACE) are less frequent thanexpected.Table 4 shows the C l -da -C  2 re la t ions(da=from,for, to).
The most frequent relations are:MATERIAL-da-PHYSICAL_ACT (e.g materiali daimballaggio (*material from (=for) packing) legnada ardere (*wood from (=to) burn))ARTIFACT-da-ARTIFACT (e.g cera da pavimenti(*wax for floors), lenzuola da letto (*sheets for bed))ARTIFACT-da-PLACE (e.g giocattoli da mare (*toysfor sea) abiti da montagna (*wears for mountain)MENTAL_ACT-da-BUILDING (e.g acquistare dafabbrica (*to buy from firms), comprare da oleifici(*to buy from oil-mills))The first three relations, very frequent in the corpus,all subsume the usage relation.
It is interesting tonotice that in Italian "da+PLACE" commonlysubsumes a source relation, just as in English(from+PLACE).
The table however shows that thisis not the case, at least when "from-PLACE" cooccurswith classes uch as ARTIFACT and MATERIAL.
Theclassical source sense is found in the fourth example.BUILDINGs here metonymically refer to the humanorganization that manages an activity in thebuilding.Currently we are unable to analyze the preposition di(*of), because in the corpus it is used for the largemajority to subsume the direct-object syntacticrelation (e.g.
vendita di frutta *sale of fruit).
It turnsout that the distribution of Cl-di-C 2 is too even toallow an analysis of the data.
Perhaps a less crudeparsing strategy could help at ruling out theseassociations.A new domain is now under examination (a legalcorpus on taxation orms).
A first analysis of the datashows that even for this corpus, despite it is muchless jergal, several unconventional relations holdbetween content words.5.
Final RemarksWe spent some time at illustrating the tables to makethe reader more confident with the data, and to showwith several practical examples the thesis of thispaper, i.e.
that selectional restrictions are morefanciful than what usually appears from theliterature on computational lexicons (and fromdictionaries as well).
The reader should not thinkthat we selected for our application the oddestdomain we could find: similar (as for fancifulness)data are being extracted from a legal corpus which iscurrently under examination.The (semi-)automatic acquisition of selectionalrestrictions is only one ot the things that can belearned using clustered association data.
In aforthcoming paper (Basili et al, forthcoming) thesame data, clustered only by the right-hand word,are at the basis of a very reliable algorithm forsyntactic disambiguation.
We are also experimentingconcept formation algorithms for verb and nounclassification (Fabrizi et al, forthcoming).In summary, clustered associations in our viewgreatly improve the reliability and applicability ofstudies on word associations.
More work is necessary,because of semantic tagging, but there is an evidentpayoff.
In any case, semantic tagging is not at all themost painful manual activity in association studies.Acknowledgements.We thank CERVEDomani for making us availablethe corpus of enterprise descriptions.
This work hasbeen in part supported by the European Community(PRO-ART and NOMOS ESPRIT 1991 n. 5330).ReferencesACL 1990, ProceedingsPennsylvania, 1990.ACL 1991, ProceedingsCalifornia, 1991R.of ACL '90, Pittsburgh,of ACL '91, Berkley,Basili, M. T. Pazienza, P. Velardi, Using wordassociation for syntactic disambiguation, 2nd.Congress of the Italian Association forArtificial Intelligence, Palermo, 1991B.
Boguraev, Building a Lexicon: the Contributionof Computers, IBM Report, T.J. WatsonResearch Center, 1991M.
Brent, Automat ic  Aquis i t ion ofSubcategorization frames from Untagged Texts,in (ACL, 1991)N. Calzolari, R. Bindi, Acquisition of LexicalInformation from Corpus, in (COL,1990)K. W. Church, P. Hanks, Word Association Norms,Mutual Information, and Lexicography,101Computational Linguistics, vol.
16, n. 1, March1990.K.
Church, W. Gale, P. Hanks, D. Hindle, UsingStatistics in Lexical Analysis, in (Zernik,1991).S.
Fabrizi, M.T.Pazienza, P. Velardi, A corpus-driven clustering algorithm for the acquisitionof word ontologies, forthcoming.M.Fasolo, L.Garbuio, N.Guarino, Comprensionedi descrizioni di attivita' economico-produttive espresse in linguaggio naturale,Proc.
of GULP Conference, Padova 1990.Jo Guthrie, L. Guthrie, Y. Wilks, H. Aidinejad,Subject-dependent Co-occurrence and WordSense Disambiguation, in (ACL, 1991).D.
Hindle, Noun classification from predicateargument structures, in (ACL ,1990).D.
Hindle, M. Rooths, Structural Ambiguity andLexical Relations, in (ACL, 1991).
"Lex ica l  Semant ics  and KnowledgeRepresentation" Proc.
of a workshop sponsoredby the Special Interest Group on the Lexicon ofthe ACL, Ed.
J. Pustejovsky, June 1991M.
Macpherson, Redefining the Level of the Word,in (Lexical, 1991).A.
Marziali, "Laurea" dissertation, University ofRoma II, Dept.
of Electrical Engineering, inpreparationM.T.
Pazienza, P. Velardi, A structuredRepresentation of Word Senses for SemanticAnalysis, Third Conference of the EuropeanChapter of ACL, Copenhagen, April 1-3, 1987.F.
Pereira, D. Warren, Definite Clause Grammarsfor Language Analysis A Survey of theFormalism and a Comparison with AugmentedTrans i t ion  Networks,  in Art i f ic ia lIntelligence, n. 13, 1980.M.
Russo, A generative grammar approach for themorphologic and morphosyntactic analysis ofthe Italian language, 3rd.
Conf.
of theEuropean Chapter of the ACL, Copenhaghen,April 1-3 1987J.
Seo, R.F.
Simmons, Syntactic Graphs aRepresentation for the Union of All AmbigousParse Trees, Computational Linguistics, Vol.15, n.1, March, 1989.F.
A. Smadja, Lexical Co-occurrence The MissingLink, Literary and Linguistic Computing, vol.4, n.3, 1989.F.
Smadja, Macrocoding the Lexicon with Co-occurrence Knowledge, First LexicalAcquisition Workshop, August 1989, Detroit,and in (Zernik,1991).F.
Smadja, K. McKewon, Automatically extractingand representing collocations for languagegeneration, in (ACL,1990).F.
Smadja, From N-Grams to Collocations anevaluation of XTRACT, in (ACL,1991).P.
Velardi, M.T.
Pazienza, M. De Giovanetti"Conceptual Graphs for the Analysis andGeneration of Sentences ", in IBM Journal oiR&D, special issue on language processing,March 1988M.
Webster M. Marcus, Automatic Acquisition otlexical semantics of verbs from sentenceframes, Proc.
of ACL89, Vancouver 1989U.
Zernik, Lexical acquisition Learning fromCorpus by capitalizing on Lexical categoriesProc.
of IJCAI 1989, Detroit 1989U.
Zernik, P. Jacobs, Tagging for LearningCollecting Thematic Relations from CorpusProc.
of COLING 90, Helsinki, August 1990.U.
Zernik ed.
"Lexical Acquisition Using On-lin(Resources to Build a Lexicon", Lawrenc(Erlbaum Ass.,1991Appendix: Examples of acquired conceptual associationsper I) 2) 3) 4) 5) 6) 7) 8) 9) i0) ii)i) att mat2) att ment3) manufat to4 )ent i ta_umana5) vegeta le6) cos t ruz ione7) der ivato8) mater ia le9) an imal ii0) macch inar ioll) luoghi0.121 0.0750.041 0.0540.094 0.0680.016 0.0240.003 0.0010.061 0.0300.013 0.0040.027 0.0080.000 0.0010.032 0.0360.004 0.004 0.000 0.0000 031 0.022 0.000 0.039 0.002 0.0020 023 0.018 0.000 0.023 O.OO1 0.0000 045 0.026 0.001 0.057 0.005 0.0050 002 0.024 0.000 0.014 0.000 0.0010 002 0.000 - 0.0000 012 0.010 - 0.013 0.000 0.0000 006 0.001 0.000 0.004 0.001 0.0010 011 0.001 0.000 0.015 0.002 0.002- - - 0.000O.OlO 0.002 O.OO1 0.033 0.001 0.001- 0.001 0.0000.001 0.010- 0.0050.000 0.011- 0 .
0 0 1- 0 .
0 0 1- 0 .
0 0 10.001 0.002- 0.003- 0.0040.0040 0030 0040 0010 0000 0010 0010 0010 0000 0010 000Table 1:Cl -per -C2102in 1) 2) 3) 4) 5) 6) 7) 8) 9) 10) 11)I) at t_mat 0.019 0.048 0.0232) att ment 0.033 0.088 0.038 0.050 0.001 03) manufat to  0.014 0.006 0.066 0.001 0.007 04)ent i ta  umana 0.004 0.009 0.0005) vegeta le  - 0.0106) cos t ruz ione  0.008 0.013 0.0177) der ivato  0.001 0.001 0.0068) mater ia le  0.002 - 0.0089) animal i  - -I0) macch inar io  - 0.001ii) luoghi 0.001 0.003 0.0010.013 0.002 0 013 0 041 0.0850.002 00.001 00.000 00.001 00007 0007 0001 0009 0004 0000 0001 0016 0 048092 0 121004 0 009006 0 000009 0 035005 0 005002 0 005- 0.0030.001 0.002Tab le  2 :C l - in -C2- 0.000 0.053- 0.004 0.074- 0.001 0.009- 0 .
0 1 0- 0.001 0.020- 0.004- 0.008- 0 .
0 0 1  0 .
0 0 0- 0 .
0 0 0i) 2) 3) 4) 5) 6) 7) 8) 9) I0) II)i)2)3) manufat to  0 005 04 )ent i ta_umana 0 001 05) vegeta le  0 001 06) cos t ruz ione  0 007 07) der ivato  0 000 08) mater ia le  0 002 09) an imal i  0i0) macch inar io  0.004 0ii) luoghi 0.001 0a t tmatat t_ment013 0.019 0.000009 0\[001 0.002001 0.002009 0.002 0.002001 0.000 0.001011 - -000005  0.002005 - 0.0070.020 0.084 0.032 0.020 0.018 0.002 0.006 0.001 0.025 0.0360.037 0 351 0.077 0.055 0.008 0.019 0.016 0.005 0.001 0.024 0.0490.004 0.001 0.002 0.000 0 .005 0.0050.002 0.000 0.002 - 0.001 0.0080.001 - 0 .
0 0 1  -0 .002  0.001 0.007 - 0.004 0.0040.001 0.001 - 0.004 0.003- - 0 .
0 0 0  -- - 0 .
0 0 1  -- 0.002 0.002 - 0.003 0.002- - - 0.001 0.017Tab le  3 :C I -a -C2da i) 2) 3) 4) 5) 6) 7) 8) 9) i0) ii)I)2) att_ment  03) manufat to  0~)ent i ta_umana 05) vegeta le  06) cos t ruz ione  07) der ivato  08) mater ia le  09) animal i  0i0) macch inar io  0ii) luoghi 0art mat 0 046 0.023 0.036 0.033 0.001 0.023 0.010 0.008022 0.012 0.047 0.052 0.001 0.037 0.001 0.001023 0.009 0.251 0.009 0.002 0.036 0.007 0.002003 0.004 0.010 0.004 0.001 0.005 0.002002 - 0.001 0.001 0.002008 0.007 0.036 0.003 0.003 0.023 0.004 0.001012 0.001 0.010 0.001 0.001 0.001 0.005012 0.001 0.010 0.001 0.001 0.001 0.005003 - 0.001 0.003 0.003 0.008004 0.001 0.007 0.002 0.001 0.001003 - 0.001 0.002Tab le  4 :C l -da -C20.005 0 0210.004 0 0320.007 0 0590 00~0 0160 0250 0090 0090 0060.007 0 0010 001Legenda:1) att_mat = PHYS ICALACT2) att_ment = MENTALACT3) manufatto = ARTIFACT4) entita umana = HUMAN_ENTITY5) vegetale = VEGETABLE6) costruzione = BUILDING7) derivato = BYPRODUCT8) materiale = MATTER9) animali = ANIMALS10) macchinario = MACHINE11) luoghi = PLACES103
