Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1360?1369, Dublin, Ireland, August 23-29 2014.Sentence Compression for Target-Polarity Word Collocation ExtractionYanyan Zhao1, Wanxiang Che2, Honglei Guo3, Bing Qin2, Zhong Su3and Ting Liu2?1: Department of Media Technology and Art, Harbin Institute of Technology2: Department of Computer Science and Technology, Harbin Institute of Technology3: IBM Research-China{yyzhao, bqin, tliu}@ir.hit.edu.cn, {guohl, suzhong}@cn.ibm.comAbstractTarget-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarilyon syntactic features to identify the relationships between targets and polarity words.
A majorproblem of current research is that this task focuses on customer reviews, which are natural orspontaneous, thus posing a challenge to syntactic parsers.
We address this problem by proposinga framework of adding a sentiment sentence compression (Sent Comp) step before performingT-P collocation extraction.
Sent Comp seeks to remove the unnecessary information for senti-ment analysis, thereby compressing a complicated sentence into one that is shorter and easier toparse.
We apply a discriminative conditional random field model, with some special sentiment-related features, in order to automatically compress sentiment sentences.
Experiments show thatSent Comp significantly improves the performance of T-P collocation extraction.1 IntroductionSentiment analysis deals with the computational treatment of opinion, sentiment and subjectivity in tex-t (Pang and Lee, 2008), and has received considerable attention in recent years (Liu, 2012).
Target-Polarity word (T-P) collocation extraction, which aims to extract the collocation of a target and its cor-responding polarity word in a sentiment sentence, is a basic task in sentiment analysis.
For example,in a sentiment sentence ?????????????
(The camera has a novel appearance), ????
(appearance) is the target, and ????
(novel) is the polarity word that modifies ????
(appearance).According, ??
?, ???
(?appearance, novel?)
is the T-P collocation.
Generally, T-P collocation is abasic and complete sentiment unit, thus is very useful for many sentiment analysis applications.Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (Ab-basi et al., 2008; Duric and Song, 2012).
For example, the syntactic relation ?AdjATTx Noun?, where theATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-Pcollocation ??
?, ???
(?appearance, novel?)
in the above sentiment sentence (Bloom et al., 2007;Qiu et al., 2011; Xu et al., 2013).However, one major problem of these approaches is the ?naturalness?
of sentiment sentences, that is,such sentences are more natural or spontaneous compared with normal sentences, thus posing a challengeto syntactic parsers.
Accordingly, many wrong syntactic features have been produced and these canfurther result in the poor performance of T-P collocation extraction.
Taking the sentence in Figure 1(a)as an example, because the word ????
(fortunately) is so chatty,1 the parsing result is wrong.
Thus,are unable to extract the T-P collocation ???,??
(?keyboard, good?
).To solve the ?naturalness?
problem, we can train a parser on sentiment sentences.
Unfortunately, an-notating such data will cost us a lot of time and effort.
Instead, in this paper we produce a sentencecompression model, Sent Comp, which is designed especially to compress complicated sentiment sen-tences into formal and easier to parse ones, further improving T-P collocation extraction.
?Correspondence author: tliu@ir.hit.edu.cnThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1Note that, in Figure 1, the Chinese word ????
is chatty, although its translated English word ?fortunately?
is not.
In thispaper, we focus on processing the Chinese data.1360??
??
?fortunately keyboard goodSBVVOBROOT(a) before compression??
?keyboard goodSBVROOT(b) after compressionFigure 1: Parse trees before and after compression.This idea is motivated by the observation that, current syntactic parsers usually perform accuratelyfor short, simple and formal sentences, whereas error rates increase for longer, more complex or morenatural and spontaneous sentences (Finkel et al., 2008).
Hence, the improvement in syntactic parsingperformance would have a ripple effect over T-P collocation extraction.
For example, we can compressthe sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part ????(fortunately).
We can see that the shortened sentence is now well-formed (in Chinese) and its parse treeis correct, making it easier to accurately extract T-P collocation.Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining impor-tant information (usually important grammar structure) (Jing, 2000).
For example, the sentence ?Overall,this is a great camera.?
can be compressed into ?This is a camera.?
by removing the adverbial ?overall?and the modifier ?great?.
However, the modifier ?great?
is a polarity word and very important for sen-timent analysis.
Therefore, Sent Comp model for sentiment sentences is different from the traditionalcompression models, because it needs to retain the important sentiment information, such as the polarityword.
Hence, using Sent Comp, the above sentence can be compressed into ?This is a great camera.
?We regard Sent Comp as a sequence labeling task, which can be solved by a conditional randomfields (CRF) model.
Instead of seeking the manual rules on parse trees for compression, as in otherstudies (Vickrey and Koller, 2008), this method is an automatic procedure.
In this work, we introducesome sentiment-related features to retain the sentiment information for Sent Comp.We apply Sent Comp as the first step in the T-P collocation extraction task.
First, we compress thesentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-the-art T-P collocation extraction approach on the compressed sentences.
Experimental results on a Chinesecorpus of four product domains show the effectiveness of our approach.The main contributions of this paper are as follows:?
We present a framework of using sentiment sentence compression preprocessing step to improve T-P collocation extraction.
This framework can better solve the ?over-natural?
problem of sentimentsentences, which poses a challenge to syntactic parsers.
More importantly, the idea of this frame-work can be applied to some other sentiment analysis tasks that rely heavily on syntactic results.?
We develop a simple yet effective compression model Sent Comp for sentiment sentences.
To thebest of our knowledge, this is the first sentiment sentence compression model.2 BackgroundFor our baseline system, we used the state-of-the-art method to extract T-P collocations introduced byQiu et al.
(2011), who proposed a double propagation method.
This idea is based on the observationthat there is a natural syntactic relationship between polarity words and targets owing to the fact thatpolarity words are used to modify targets.
Furthermore, they also found that polarity words and targetsthemselves have relations in some sentiment sentences (Qiu et al., 2011).Based on this idea, in the double propagation method, we first used an initial seed polarity word lexiconand the syntactic relations to extract the targets, which can fall into a new target lexicon.
Then we used thetarget lexicon and the same syntactic relations to extract the polarity words and to subsequently expandthe polarity word lexicon.
This is an iterative procedure, because this method can iteratively produce thenew polarity words and targets back and forth using the syntactic relations.1361??
??
?
?function very powerfulSBVADVROOT(a) syntactic structure 1??
?
?
?powerful functionATTRADROOT(b) syntactic structure 2??
?
??
?function is powerfulSBV VOB RADROOT(c) syntactic structure 3??
?
??
??
?
?function and service very powerfulCOOLAD ADVSBV ROOT(d) syntactic structure 4??
??
??
?
?
?function very powerful and completeSBV COOADV LADROOT(e) syntactic structure 5Figure 2: Example of syntactic structure rules for T-P collocation extraction.
We showed five examplesfrom a total of nine syntactic structures.
For each kind of syntactic structure (a) to (e), the target isshown with a red box and the polarity word is shown with a green box.
Syntactic structures (a) to (c)describe the relations between targets and polarity words.
Syntactic structure (d), which is extendedfrom (a), describes the relation between two targets.
Syntactic structure (e), which is also extended from(a), describes the relation between two polarity words.
Similarly, we can summarize the other four rulesextended from (b) and (c) to describe the relations between two targets or two polarity words.We can see that the syntactic relations are important for this method, and Qiu et al.
(2011) proposedeight rules to describe these relations.
However, their work only focused on English sentences, whereasthe relations for Chinese sentences are different.
Thus, in accordance with Chinese grammar, we pro-posed nine syntactic structure rules between target t and polarity word p in a Chinese T-P collocation?t, p?.2The three main rules are shown below and some example rules are illustrated in Figure 2.Rule 1: tSBVx p, the ?subject-verb?
structure between t and p, such as the example in Figure 2(a).Rule 2: pATTx t, that p is an attribute for t, such as the example in Figure 2(b).Rule 3: tSBVx ?VOBy p, the ?subject-verb-object?
structure between t and p, such as the example inFigure 2(c).
The ?
denotes any word.The other six rules can be extended from the three main rules by obtaining the coordination (COO)relation of t or p, such as tSBVx ?COOy p in Figure 2(e).
Note that the POS for t should be noun and for pshould be adjective.As described above, the T-P collocation extraction relies heavily on syntactic parsers.
Hence, if wecan use the Sent Comp model to improve the performance of parsers, the performance of T-P collocationextraction can also be improved accordingly.3 Sentiment Sentence Compression3.1 Problem AnalysisFirst, we conducted an error analysis for the results of current T-P collocation extraction, from which weobserved that the ?naturalness?
of sentiment sentences is one of the main problems.
For examples:?
Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser.For example, in the sentence ???????
(fortunately the keyboard is good) shown in Figure 1,the usage of the chatty word ????
(fortunately) affects the accuracy of the syntactic parser.2A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as ourdependency parser.
More information about the syntactic relations can be found in their paper.
The state-of-the-art graph-baseddependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05).1362??
??
?besides photo goodADVPOBROOTcomp??
?photo goodSBVROOT(a) parse tree 1 before and after compression??
?
?
?
??
?
?screen for people feel goodSBVATTPOBRAD SBVROOTcomp??
?
?screen goodSBVROOT(b) parse tree 2 before and after compressionFigure 3: ?Naturalness?
problem of sentiment sentences.?
Conjunction word usage: conjunction words are often used in sentiment sentences to show the dis-course relations between two sentences.
However, there are so many conjunction words in Chinese,some of which can cause errors among parsers.
For example, in Figure 3(a), the parse tree of sen-tence ????????
(besides the photo is good) is wrong because of the usage of the conjunctionword ????
(besides).?
Feeling words/phrase usage: in sentiment sentences, people often use some feeling words/phrase,such as ???????
(feel like) in Figure 3(b) or ?????
(smell like).
Given that the currentsyntactic parser cannot handle the feeling words/phrases very well, the T-P collocation ??
?, ???
(?screen, good?)
in Figure 3(b) cannot be extracted correctly.To address the ?naturalness?
problem, we compressed the sentiment sentences into one that are shorterand easier to parse.
Similar to the examples in Figure 1 and 3, the compressed sentences can be easilyand correctly parsed.
The above analysis can be used as the criteria to guide us in compressing sentimentsentences when annotating, and can also help us exploit more useful features for automatic sentimentsentence compression.3.2 Task DefinitionWe focus on studying the methods for extractive sentence compression.3Formally, extractive sentencecompression aims to shorten a sentence x = x1?
?
?xninto a substring y = y1?
?
?
ym, where yi?
{x1, ?
?
?
, xn}, m ?
n.In this paper, similar to Nomoto (2007), we also treated the sentence compression as a sequencelabeling task which can be solved by a CRF model.
We assigned a compression tag tito each word xiinan original sentence x, where ti= N if xi?
y, else ti= Y.A first-order linear-chain CRF is used which defines the following conditional probability:P (t|x) =1Z(x)?iMi(ti, ti?1|x) (1)where x and t are the input and output sequences respectively, Z(x) is the partition function, and Miisthe clique potential for edge clique i.
Here, we used the CRFsuite toolkit to train the CRF model.43.3 FeaturesThe features for Sent Comp are listed in Table 1.
Aside from the basic word (w), POS tag (t) andtheir combination context features (01 ?
04), we introduced some sentiment-related features (05 ?
06)and latent semantic features (07 ?
08) to better handle sentiment analysis data and generalize wordfeatures.
Then we added the syntactic parse features (09), which are commonly used in traditionalsentence compression task.One sentiment-related feature (feeling(?))
indicates whether a word is a feeling word, which is inspiredby the naturalness problem in Figure 3(b).
As discussed above, the current parser often produces wrongparse trees because of these feeling words.
Therefore, the feeling words tend to be removed from a3Generally, there are two kinds of sentence compression methods: extractive method and abstractive method.
Becauseabstractive method needs more resource and is more complicated, in this paper, we only focus on extractive approach.4www.chokkan.org/software/crfsuite/1363Basic Features01: wi+k,?1 ?
k ?
102: wi+k?1?
wi+k, 0 ?
k ?
103: ti+k,?2 ?
k ?
204: ti+k?1?
ti+k,?1 ?
k ?
2Sentiment-related Features05: feeling(wi)06: polarity(wi)Latent Semantic Features07: suffix(wi) if t(wi) == n else prefix(wi)08: cluster(wi)Syntactic Features09: dependency(wi)Table 1: Features of sentiment sentence compressionsentiment sentence for Sent Comp.
We can obtain a feeling word lexicon from HowNet,5a popularChinese sentiment thesaurus, where a feeling word is defined by DEF={perception|??}
tag.
Finally,we collected 38 feeling words, such as??
(realize),??
(find), and??
(think).The other sentiment-related feature (polarity(?))
indicates whether a word is a polarity word.
Oneof the main differences between a sentiment sentence and a formal sentence is that the former oftencontains polarity words.
In contrast to the features of feeling(?
), polarity words (e.g., ?great?
in thesentence ?Overall, this is a great camera?)
tend to be retained, because they are important and specialto sentiment analysis.
In this paper, we treat polarity words as important features, considering that theyare often tagged as modifiers and are easily removed by common sentence compression methods.
Wecan obtain the polarity feature (polarity(?))
from a polarity lexicon, which can also be obtained fromHowNet.To generalize the words in sentiment sentences, we proposed two kinds of semantic features.
Thefirst one is a suffix or prefix character feature (prefix(?)
or suffix(?)).
In contrast to English, the suffix(for noun) or prefix (for non noun) characters of a Chinese word often carry that word?s core semanticinformation.
For example, ???
(bicycle), ??
(car), and ??
(train) are all various kinds of ?
(vehicle), which is also the suffix of the three words.
Given that all of them may become targets, theytend to be retained in compressed sentences.
The verbs, ??
and ?
?, can be denoted by their prefixfeel (?
), and can be removed from original sentences because they are feeling words.We used word clustering features (cluster(?))
as the other latent semantic feature to further improvethe generalization over common words.
Word clustering features contain some semantic informationand have been successfully used in several natural language processing tasks, including NER (Miller etal., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008).
For instance, the words ?
?and ??
(appearance) belong to the same word cluster, although they have a different suffix or prefix.Both words are important for T-P collocation extraction and should be retained.
We used the Brownword clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005).
Raw texts wereobtained from the fifth edition of Chinese Gigaword (LDC2011T13).Finally, similar to McDonald (2006), we also added the dependency relation between a word and itsparent as the syntactic features.
Intuitively, the dependency relations are helpful in carrying out sentencecompression.
For example, the ROOT relation typically indicates that the word should not be removedbecause it is the main verb of a sentence.4 Experiments4.1 Experimental Setup4.1.1 CorpusWe conducted the experiments on a Chinese corpus of four product domains, which came from the Task3of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).6Table 2 describes the corpus,5www.keenage.com6www.ir-china.org.cn/coae2008.html1364Domain # reviews # sentences # collocationsCamera 138 1,249 1,335Car 161 1,172 1,312Notebook 56 623 674Phone 123 1,350 1,479All 478 4,394 4,800Table 2: Corpus statistics for the Chinese corpus of four product domains.where 4,394 sentiment sentences containing 4,800 T-P collocations are manually found and annotatedfrom 478 reviews.We ask annotators to manually compress all the sentiment sentences.
Specifically, the annotatorsremoved some words from a sentiment sentence according to two criteria stated as follows: (1) removingthe word should not change the essential content of the sentence, and (2) removing the word shouldnot change the sentiment orientation of the sentence.
In order to assess the quality of the annotation,we sampled 500 sentences from this corpus and asked two annotators to perform the annotation.
Theresulting word-based Cohen?s kappa (Cohen, 1960) (i.e., a measure of inter-annotator agreement rangingfrom zero to one) of 0.7 indicated a good strength of agreement.4.1.2 EvaluationGenerally, compressions are evaluated using three criteria (McDonald, 2006), namely, grammaticality,importance, and compression rate.
Obviously, the former two are difficult to evaluate objectively.
Previ-ous works used human judgment, which entails a difficult and expensive process.
In this paper, similar toa common sequence labeling task, we simply used the F-score metric of removed words to roughly eval-uate the performance of sentiment sentence compression.
Of course, the final effectiveness of sentencecompression model can be reviewed by the derived T-P collocation extraction task.For T-P collocation extraction, we applied the traditional P, R and F-score for the final evaluations.Specially, a fuzzy matching evaluation is adopted for the T-P collocation extraction.
That is to say,given an extracted T-P collocation ?t, p?, whose standard result is ?ts, ps?, if t is the substring of ts, andmeanwhile p is the substring of ps, we consider the extracted ?t, p?
is a correct T-P collocation.4.2 Sentiment Sentence Compression ResultsFeatures P(%) R(%) F(%)Basic (01 ?
04) 76.4 57.4 65.5+ feeling (05) 75.9 57.6 65.5+ polarity (06) 76.6 57.6 65.7+ suffix or prefix (07) 78.4 56.9 66.0+ cluster (08) 74.9 58.9 65.9+ dependency (09) 75.3 57.2 65.0All (01 ?
08) 77.3 59.1 67.0All - feeling (05) 77.1 58.9 66.8Table 3: The results of sentiment sentence compression with different features.Results of Sent Comp with different features are shown in Table 3.
All results are reported using five-fold cross validation.
We can see that the performance is improved when we added feeling7and polarityfeatures (05 ?
06) respectively, indicating that the sentiment-related features are useful for sentimentsentence compression.
In addition, the latent semantic features (07 ?
08) are also helpful, especially thesuffix or prefix features, which show better performance than the four other kinds of features.Nonetheless, the dependency features (09) have a negative on compression performance due to thespecificity of compression for sentiment sentences.
That is because the lower dependency parsing per-formance on sentiment sentences introduces many wrong dependency relations, which counteract the7In Table 3, although the performance of adding feeling is comparative to the basic system (Basic (01-04)), the systemwithout feeling (All - feeling (05), the last line) is worse than the system using all the features (All (01-08)).
This can illustratethe effectiveness of the feeling feature.1365Domain Method P(%) R(%) F(%)no Comp 74.7 58.4 65.6Camera manual Comp 83.4 62.7 71.6auto Comp 80.4 62.1 70.1no Comp 68.2 53.1 59.7Car manual Comp 76.3 57.7 65.7auto Comp 72.3 56.1 63.2no Comp 74.1 56.8 64.3Notebook manual Comp 82.7 64.5 72.5auto Comp 79.7 62.8 70.2no Comp 77.3 60.9 68.1Phone manual Comp 82.7 65.7 73.2auto Comp 80.3 63.3 70.8no Comp 73.7 57.5 64.6All manual Comp 81.2 62.5 70.6auto Comp 78.1 60.9 68.4Table 4: Results on T-P collocation extraction for four product domains.contribution of the dependency relation features.
This is also the reason why we need to compress sen-timent sentences as the first step for T-P collocation extraction.
Finally, when we combine all of usefulfeatures (01 ?
08), the performance achieves the highest score.It is worth noting that sentiment sentence compression is a new task proposed in this paper.
Forsimplicity, this paper aims to attempt a simple yet effective sentiment sentence compression model.
Wewill polish the Sent Comp model in the future work.4.3 Sent Comp for T-P Collocation ExtractionWe designed three comparative systems to demonstrate the effectiveness of Sent Comp for T-P collo-cation extraction.
Note that, Sent Comp is the first step to process the corpus before T-P collocationextraction.
The method for T-P collocation extraction was based on the state-of-the-art method proposedby Qiu et al.
(2011) as described in Section 2.no Comp - This refers to the system that only uses the T-P collocation extraction method and does notperform sentence compression as the first step.manual Comp - This system manually compresses the corpus into a new one as the first step, and thenapplies the T-P collocation extraction method on the new compressed corpus.auto Comp - This system uses Sent Comp as the first step to automatically compress the corpus into anew one, and then applies the T-P collocation extraction method on the new corpus.From the descriptions above, we can draw a conclusion that the performance of manual Comp can beconsidered as the upper bound for the sentiment sentence compression based T-P collocation extractiontask.Table 4 shows the experimental results of the three systems on T-P collocation extraction for four prod-uct domains.
Here, manual Comp can significantly (p < 0.01) improved the F-score by approximately6%,8compared with no Comp.
This illustrates that the idea of sentiment sentence compression is use-ful for T-P collocation extraction.
Specifically, the proposed method can transform some over-naturalsentences into normal ones, further influencing their final syntactic parsers.
Evidently, because the T-Pcollocation extraction relies heavily on syntactic features, the more correct syntactic parse trees derivedfrom the compressed sentences can help to increase the performance of this task.Compared with no Comp, the auto Comp system also yielded a significantly better results (p < 0.01)that indicated an improvement of 3.8% in the F-score, despite the fact that the automatic sentence com-pression model Sent Comp may wrongly compress some sentences.
This demonstrates the usefulnessof sentiment sentence compression step in the T-P collocation extraction task and further proves theeffectiveness of our proposed model.8We use paired bootstrap resampling significance test (Efron and Tibshirani, 1993).1366Moreover, we can observe that the idea of sentence compression and our Sent Comp are useful forall the four product domains on T-P collocation extraction task, indicating that Sent Comp is domainadaptive.
However, we can find a small gap between auto Comp and manual Comp, which indicatesthat the Sent Comp model can still be improved further.
In the future, we will explore more effectivesentence compression algorithms to bridge the gap between the two systems.5 Related Works5.1 Sentiment AnalysisT-P collocation extraction is a basic task in sentiment analysis.
In order to solve this task, most methodsfocused on identifying relationships between targets and polarity words.
In early studies, researcher-s recognized the target first, and then chose its polarity word within a window of size k (Hu and Liu,2004).
However, considering that this kind of method is too heuristic, the performance proved to be verylimited.
To tackle this problem, many researchers found syntactic patterns that can better describe therelationships between targets and polarity words.
For example, Bloom et al.
(2007) constructed a link-age specification lexicon containing 31 patterns, while Qiu et al.
(2011) proposed a double propagationmethod that introduced eight heuristic syntactic patterns to extract the collocations.
Xu et al.
(2013) usedthe syntactic patterns to extract the collocation candidates in their two-stage framework.Based on the above, we can conclude that syntactic features are very important for T-P collocationextraction.
However, the ?naturalness?
problem can still seriously affect the performance of syntacticparser.
Once our sentiment sentence compression method can improve the quality of parsing, the perfor-mance of T-P collocation extraction task can be improved as well.
Note that, to date, there is no previouswork using a sentence compression model to improve this task.5.2 Sentence CompressionSentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones,while preserving the essential content (Jing, 2000).
There are many applications that can benefit froma robust compression system, such as summarization systems (Li et al., 2013), semantic role label-ing (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on.Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner andCharniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos,2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing thesyntactic tree of the original sentence.
However, the automatic parsing results may not be correct; thus,the compressed tree (after removing constituents from a bad parse) may not produce a good compressedsentence.
McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problemby using discriminative models.Aside from above extractive sentence compression approaches, there is another research line, namely,abstractive approach, which compresses an original sentence by reordering, substituting, and inserting,as well as removing (Cohn and Lapata, 2013).
This method needs more resource and is more complicat-ed.
Therefore, in this paper, we only focus on extractive approach.At present, the current sentence compression methods all focus on formal sentences, and few meth-ods are being proposed to study sentiment sentences.
As discussed in the above sections, the currentcompression models cannot be directly utilized to T-P collocation extraction owing to the specificity ofsentiment sentences.
Therefore, a new compression model for sentiment sentences should be established.6 Conclusion and Future WorkIn this work, we presented a framework that adopted a CRF based sentiment sentence compression mod-el Sent Comp, as a preprocessing step, to improve the T-P collocation extraction task.
Different fromthe existing sentence compression models used for formal sentences, Sent Comp incorporated somesentiment-related features to retain the sentiment information.
Experimental results showed that the sys-tem with the sentence compression step performed better than that without this step, thus demonstratingthe effectiveness of the framework and the compression model Sent Comp.1367Generally, the idea of this framework maybe useful for many sentiment analysis tasks that rely heavilyon syntactic results.
Thus in the future, we will try to apply the Sent Comp model for these tasks.
Besides,the simplicity and effectiveness of this framework motivates us to pursue the study further.
For example,we will polish the Sent Comp model by exploring more sentiment-related features and exploring othertypes of compression models.AcknowledgmentsWe thank the anonymous reviewers for their helpful comments.
This work was supported by NationalNatural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, the Ministryof Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Fundamen-tal Research Funds for the Central Universities via grant No.HIT.NSRIF.2013090 and IBM Research-China Joint Research Project.ReferencesAhmed Abbasi, Hsinchun Chen, and Arab Salem.
2008.
Sentiment analysis in multiple languages: Featureselection for opinion classification in web forums.
ACM Trans.
Inf.
Syst., 26(3):12:1?12:34, June.Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007.
Extracting appraisal expressions.
In HLT-NAACL2007, pages 308?315.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
Comput.
Linguist., 18(4):467?479, December.Wanxiang Che, Zhenghua Li, and Ting Liu.
2010.
Ltp: A chinese language technology platform.
In Coling 2010:Demonstrations, pages 13?16, Beijing, China, August.
Coling 2010 Organizing Committee.Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu.
2013.
Named entity recognition withbilingual constraints.
In Proceedings of the 2013 Conference of the North American Chapter of the Associ-ation for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.Association for Computational Linguistics.James Clarke and Mirella Lapata.
2008.
Global inference for sentence compression: An integer linear program-ming approach.
J. Artif.
Intell.
Res.
(JAIR), 31:399?429.Jacob Cohen.
1960.
A coefficient of agreement for nominal scales.
Educational and Psychological Measurement,20(1):37 ?
46.Trevor Cohn and Mirella Lapata.
2009.
Sentence compression as tree transduction.
Journal of Artificial Intelli-gence Research, 34:637?674.Trevor Cohn and Mirella Lapata.
2013.
An abstractive approach to sentence compression.
ACM Transactions onIntelligent Systems and Technology, 4(3):1?35.Adnan Duric and Fei Song.
2012.
Feature selection for sentiment analysis based on content and syntax models.Decis.
Support Syst., 53(4):704?711, November.B.
Efron and R. J. Tibshirani.
1993.
An Introduction to the Bootstrap.
Chapman & Hall, New York.Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL-08: HLT, pages 959?967, Columbus, Ohio, June.
Association forComputational Linguistics.Dimitrios Galanis and Ion Androutsopoulos.
2010.
An extractive supervised two-stage method for sentencecompression.
In Human Language Technologies: The 2010 Annual Conference of the North American Chapterof the Association for Computational Linguistics, pages 885?893, Los Angeles, California, June.
Associationfor Computational Linguistics.Michel Galley and Kathleen McKeown.
2007.
Lexicalized Markov grammars for sentence compression.
InHuman Language Technologies 2007: The Conference of the North American Chapter of the Association forComputational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.Association for Computational Linguistics.1368Minqing Hu and Bing Liu.
2004.
Mining and summarizing customer reviews.
In Proceedings of KDD-2004,pages 168?177.Hongyan Jing.
2000.
Sentence reduction for automatic text summarization.
In IN PROCEEDINGS OF THE 6THAPPLIED NATURAL LANGUAGE PROCESSING CONFERENCE, pages 310?315.Kevin Knight and Daniel Marcu.
2002.
Summarization beyond sentence extraction: A probabilistic approach tosentence compression.
Artif.
Intell., 139(1):91?107, July.Terry Koo, Xavier Carreras, and Michael Collins.
2008.
Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio, June.
Association for Computational Linguistics.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013.
Document summarization via guided sentence compression.In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,Seattle, Washington, USA, October.
Association for Computational Linguistics.Percy Liang.
2005.
Semi-supervised learning for natural language.
Master?s thesis, MIT.Bing Liu.
2012.
Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technologies.Morgan & Claypool Publishers.Ryan McDonald.
2006.
Discriminative sentence compression with soft syntactic evidence.
In In Proc.
EACL.Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.
Name tagging with word clusters and discriminativetraining.
In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,pages 337?342, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi Tsujii.
2010.
Entity-focused sentence simplificationfor relation extraction.
In Proceedings of the 23rd International Conference on Computational Linguistics(COLING 2010), pages 788?796.Tadashi Nomoto.
2007.
Discriminative sentence compression with conditional random fields.
Information Pro-cessing and Management, 43(6):1571?1587, November.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135,January.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011.
Opinion word expansion and target extraction throughdouble propagation.
Computational Linguistics, 37(1):9?27.Kapil Thadani and Kathleen McKeown.
2013.
Sentence compression with joint structural inference.
In Pro-ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74, Sofia,Bulgaria, August.
Association for Computational Linguistics.Jenine Turner and Eugene Charniak.
2005.
Supervised and unsupervised learning for sentence compression.
InProceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 290?297, Stroudsburg, PA, USA.
Association for Computational Linguistics.David Vickrey and Daphne Koller.
2008.
Sentence simplification for semantic role labeling.
In ACL, pages344?352.
The Association for Computer Linguistics.Kristian Woodsend and Mirella Lapata.
2011.
Learning to simplify sentences with quasi-synchronous grammarand integer programming.
In Proceedings of the 2011 Conference on Empirical Methods in Natural LanguageProcessing, pages 409?420, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao.
2013.
Mining opinion words and opinion targetsin a two-stage framework.
In Proceedings of the 51st Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August.
Association for ComputationalLinguistics.Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang.
2008.
Overview of chinese pinionanalysis evaluation 2008.
In The First Chinese Opinion Analysis Evaluation (COAE) 2008.1369
