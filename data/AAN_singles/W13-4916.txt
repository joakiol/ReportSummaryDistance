Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 135?145,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics(Re)ranking Meets Morphosyntax: State-of-the-art Resultsfrom the SPMRL 2013 Shared Task?Anders Bjo?rkelund?, O?zlem C?etinog?lu?, Richa?rd Farkas?, Thomas Mu?ller?
?, and Wolfgang Seeker?
?Institute for Natural Language Processing , University of Stuttgart, Germany?Department of Informatics, University of Szeged, Hungary?Center for Information and Language Processing, University of Munich, Germany{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.derfarkas@inf.u-szeged.huAbstractThis paper describes the IMS-SZEGED-CIScontribution to the SPMRL 2013 Shared Task.We participate in both the constituency anddependency tracks, and achieve state-of-the-art for all languages.
For both tracks we makesignificant improvements through high qualitypreprocessing and (re)ranking on top of strongbaselines.
Our system came out first for bothtracks.1 IntroductionIn this paper, we present our contribution to the 2013Shared Task on Parsing Morphologically Rich Lan-guages (MRLs).
MRLs pose a number of interestingchallenges to today?s standard parsing algorithms,for example a free word order and, due to their richmorphology, greater lexical variation that aggravatesout-of-vocabulary problems considerably (Tsarfatyet al 2010).Given the wide range of languages encompassedby the term MRL, there is, as of yet, no clear con-sensus on what approaches and features are gener-ally important for parsing MRLs.
However, devel-oping tailored solutions for each language is time-consuming and requires a good understanding ofthe language in question.
In our contribution to theSPMRL 2013 Shared Task (Seddah et al 2013), wetherefore chose an approach that we could apply toall languages in the Shared Task, but that would alsoallow us to fine-tune it for individual languages byvarying certain components.
?Authors in alphabetical order.For the dependency track, we combined the n-best output of multiple parsers and subsequentlyranked them to obtain the best parse.
While thisapproach has been studied for constituency parsing(Zhang et al 2009; Johnson and Ural, 2010; Wangand Zong, 2011), it is, to our knowledge, the firsttime this has been applied successfully within de-pendency parsing.
We experimented with differentkinds of features in the ranker and developed fea-ture models for each language.
Our system rankedfirst out of seven systems for all languages exceptFrench.For the constituency track, we experimentedwith an alternative way of handling unknown wordsand applied a products of Context Free Grammarswith Latent Annotations (PCFG-LA) (Petrov et al2006), whose output was reranked to select the bestanalysis.
The additional reranking step improvedresults for all languages.
Our system beats vari-ous baselines provided by the organizers for all lan-guages.
Unfortunately, no one else participated inthis track.For both settings, we made an effort to automat-ically annotate our data with the best possible pre-processing (POS, morphological information).
Weused a multi-layered CRF (Mu?ller et al 2013) toannotate each data set, stacking with the informationprovided by the organizers when this was beneficial.The high quality of our preprocessing considerablyimproved the performance of our systems.The Shared Task involved a variety of settings asto whether gold or predicted part-of-speech tags andmorphological information were available, as wellas whether the full training set or a smaller (5k sen-135Arabic Basque French German Hebrew Hungarian Korean Polish SwedishMarMoT 97.38/92.22 97.02/87.08 97.61/90.92 98.10/91.80 97.09/97.67 98.72/97.59 94.03/87.68 98.12/90.84 97.27/97.13Stacked 98.23/89.05 98.56/92.63 97.83/97.62Table 1: POS/morphological feature accuracies on the development sets.tences) training set was used for training.
Through-out this paper we focus on the settings with pre-dicted preprocessing information with gold segmen-tation and the full1 training sets.
Unless stated other-wise, all given numbers are drawn from experimentsin this setting.
For all other settings, we refer thereader to the Shared Task overview paper (Seddah etal., 2013).The remainder of the paper is structured as fol-lows: We present our preprocessing in Section 2 andafterwards describe both our systems for the con-stituency (Section 3) and for the dependency tracks(Section 4).
Section 5 discusses the results on theShared Task test sets.
We conclude with Section 6.2 PreprocessingWe first spent some time on preparing the data sets,in particular we automatically annotated the datawith high-quality POS and morphological informa-tion.
We consider this kind of preprocessing to be anessential part of a parsing system, since the qualityof the automatic preprocessing strongly affects theperformance of the parsers.Because our tools work on CoNLL09 format, wefirst converted the training data from the CoNLL06format to CoNLL09.
We thus had to decide whetherto use coarse or fine part-of-speech (POS) tags.
Ina preliminary experiment we found that fine tags arethe better option for all languages but Basque andKorean.
For Korean the reason seems to be that thefine tag set is huge (> 900) and that the same infor-mation is also provided in the feature column.We predict POS tags and morphological featuresjointly using the Conditional Random Field (CRF)tagger MarMoT2 (Mu?ller et al 2013).MarMoT incrementally creates forward-backward lattices of increasing order to prunethe sizable space of possible morphological analy-ses.
We use MarMoT with the default parameters.1Although, for Hebrew and Swedish only 5k sentences wereavailable for training, and the two settings thus coincide.2https://code.google.com/p/cistern/Since morphological dictionaries can improve au-tomatic POS tagging considerably, we also createdsuch dictionaries for each language.
For this, we an-alyzed the word forms provided in the data sets withlanguage-specific morphological analyzers exceptfor Hebrew and German where we just extracted themorphological information from the lattice files pro-vided by the organizers.
For the other languageswe used the following tools: Arabic: AraMorpha reimplementation of Buckwalter (2002), Basque:Apertium (Forcada et al 2011), French: an IMSinternal tool,3 Hungarian: Magyarlanc (Zsibrita etal., 2013), Korean: HanNanum (Park et al 2010),Polish: Morfeusz (Wolin?ski, 2006), and Swedish:Granska (Domeij et al 2000).The created dictionaries were shared with theother Shared Task participants.
We used these dic-tionaries as additional features for MarMoT.For some languages we also integrated the pre-dicted tags provided by the organizers into the fea-ture model.
These stacked models gave improve-ments for Swedish, Polish and Basque (cf.
Table 1for accuracies).For the full setting the training data was annotatedusing 5-fold jackknifing.
In the 5k setting, we addi-tionally added all sentences not present in the parsertraining data to the training data sets of the tagger.This is similar to the predicted 5k files provided bythe organizers, where more training data than the 5kwas also used for prediction.Table 3 presents a comparison between our graph-based baseline parser using the preprocessing ex-plained in this section (denoted mate) and thepreprocessing provided by the organizers (denotedmate?).
Our preprocessing yields improvementsfor all languages but Swedish.
The worse perfor-mance for Swedish is due to the fact that the pre-dictions provided by the organizers were producedby models that were trained on a much larger data3The French morphology was written by Zhenxia Zhou,Max Kisselew and Helmut Schmid.
It is an extension of Zhou(2007) and implemented in SFST (Schmid, 2005).136Arabic Basque French German Hebrew Hungarian Korean Polish SwedishBerkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50Replaced 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52Product 80.30 86.21 81.42 84.56 90.49 89.80 84.15 88.32 79.25Reranked 81.24 87.35 82.49 85.01 90.49 91.07 84.63 88.40 79.53Table 2: PARSEVAL scores on the development sets.set.
The comparison with other parsers demonstratesthat for some languages (e.g., Hebrew or Korean)the improvements due to better preprocessing canbe greater than the improvements due to a betterparser.
For instance, for Hebrew the parser trainedon the provided preprocessing is more than threepoints (LAS) behind the three parsers trained onour own preprocessing.
However, the difference be-tween these three parsers is less than a point.3 Constituency ParsingThe phrase structure parsing pipeline is based onproducts of Context Free Grammars with Latent An-notations (PCFG-LA) (Petrov et al 2006) and dis-criminative reranking.
We further replace rare wordsby their predicted morphological analysis.We preprocess the treebank trees by removing themorphological annotation of the POS tags and thefunction labels of all non-terminals.
We also reducethe 177 compositional Korean POS tags to their firstatomic tag, which results in a POS tag set of 9 tags.PCFG-LAs are incrementally built by split-ting non-terminals, refining parameters using EM-training and reversing splits that only cause smallincreases in likelihood.Running the Berkeley Parser4 ?
the reference im-plementation of PCFG-LAs ?
on the data sets resultsin the PARSEVAL scores given in Table 2 (Berke-ley).
The Berkeley parser only implements a simplesignature-based unknown word model that seems tobe ineffective for some of the languages, especiallyBasque and Korean.We thus replace rare words (frequency < 20) bythe predicted morphological tags of Section 2 (or thetrue morphological tag for the gold setup).
The intu-ition is that our discriminative tagger has a more so-phisticated unknown word treatment than the Berke-ley parser, taking for example prefixes, suffixes and4http://code.google.com/p/berkeleyparser/the immediate lexical context into account.
Further-more, the morphological tag contains most of thenecessary syntactic information.
An exception, forinstance, might be the semantic information neededto disambiguate prepositional attachment.
We thinkthat replacing rare words by tags has an advan-tage over constraining the pre-terminal layer of theparser, because the parser can still decide to assigna different tag, for example in cases were the tag-ger produces errors due to long-distance dependen-cies.
The used frequency threshold of 20 resultsin token replacement rates of 18% (French) to 57%(Korean and Polish), which correspond to 209 (forPolish) to 3221 (for Arabic) word types that are notreplaced.
The PARSEVAL scores for the describedmethod are again given in Table 2 (Replaced).
Themethod yields improvements for all languages ex-cept for French where we observe a drop of 0.06.The improvements range from 0.46 for Arabic to1.02 for Swedish, 3.1 for Polish and more than 10for Basque and Korean.To further improve results, we employ theproduct-of-grammars procedure (Petrov, 2010),where different grammars are trained on the samedata set but with different initialization setups.
Wetrained 8 grammars and used tree-level inference.In Table 2 (Product) we can see that this leads toimprovements from 0.72 for Hungarian to 3.73 forSwedish.On the 50-best output of the product parser,we also carry out discriminative reranking.
Thereranker is trained for the maximum entropy objec-tive function of Charniak and Johnson (2005) anduse the standard feature set ?
without language-specific feature engineering ?
from Charniak andJohnson (2005) and Collins (2000).
We use aslightly modified version of the Mallet toolkit (Mc-Callum, 2002) for reranking.Improvements range from negligible differences(< .1) for Hebrew and Polish to substantial differ-ences (> 1.)
for Basque, French, and Hungarian.137mate parserbest-firstparserturboparsermerged listof 50-100 besttrees/sentencemerged listscored byall parsersrankerptb treesParsing RankingIN OUTscoresscoresscoresfeaturesFigure 1: Architecture of the dependency ranking system.For our final submission, we used the rerankeroutput for all languages except French, Hebrew, Pol-ish, and Swedish.
This decision was based on anearlier version of the evaluation setting provided bythe organizers.
In this setup, reranking did not helpor was even harmful for these four languages.
Thefigures in Table 2 use the latest evaluation script andare thus consistent with the test set results presentedin Section 5.After the submission deadline the Shared Taskorganizers made us aware that we had surprisinglylow exact match scores for Polish (e.g., 1.22 forthe reranked setup).
The reason seems to be thatthe Berkeley parser cannot produce unary chains oflength > 2.
The gold development set contains 1783such chains while the prediction of the reranked sys-tem contains none.
A particularly frequent unarychain with 908 occurences in the gold data is ff ?fwe ?
formaczas.
As this chain cannot be pro-duced the parser leaves out the fwe phrase.
Insertingnew fwe nodes between ff and formacszas nodesraises the PARSEVAL scores of the reranked modelfrom 88.40 to 90.64 and the exact match scores to11.34.
This suggests that the Polish results could beimproved substantially if unary chains were properlydealt with, for example by collapsing unary chains.54 Dependency ParsingThe core idea of our dependency parsing systemis the combination of the n-best output of several5Thanks to Slav Petrov for pointing us to the unary chainlength limit.parsers followed by a ranking step on the com-bined list.
Specifically, we first run two parsers thateach output their 50-best analyses for each sentence.These 50-best analyses are merged together into onesingle n-best list of between 50 and 100 analyses(depending on the overlap between the n-best listsof the two parsers).
We then use the two parsersplus an additional one to score each tree in the n-best lists according to their parsing model, thus pro-viding us with three different scores for each tree inthe n-best lists.
The n-best lists are then given toa ranker, which ranks the list using the three scoresand a small set of additional features in order to findthe best overall analysis.
Figure 1 shows a schematicof the process.As a preprocessing step, we reduced the depen-dency label set for the Hungarian training data.The Hungarian dependency data set encodes ellipsesthrough composite edge labels which leads to a pro-liferation of edge labels (more than 400).
Sincemany of these labels are extremely rare and thus hardto learn for the parsers, we reduced the set of edge la-bels during the conversion.
Specifically, we retainedthe 50 most frequent labels, while reducing the com-posite labels to their base label.For producing the initial n-best lists, we usethe mate parser6 (Bohnet, 2010) and a variant ofthe EasyFirst parser (Goldberg and Elhadad, 2010),which we here call best-first parser.The mate parser is a state-of-the-art graph-baseddependency parser that uses second-order features.6https://code.google.com/p/mate-tools138The parser works in two steps.
First, it uses dy-namic programming to find the optimal projectivetree using the Carreras (2007) decoder.
It thenapplies the non-projective approximation algorithmproposed by McDonald and Pereira (2006) in or-der to produce non-projective parse trees.
The non-projective approximation algorithm is a greedy hillclimbing algorithm that starts from the optimal pro-jective parse and iteratively tries to reattach all to-kens, one at a time, everywhere in the sentence aslong as the tree property holds.
It halts when the in-crease in the score of the tree according to the pars-ing model is below a certain threshold.n-best lists are obtained by applying the non-projective approximation algorithm in a non-greedymanner, exploring multiple possibilities.
All treesare collected in a list, and when no new trees arefound, or newer trees have a significantly lowerscore than the currently best one, search halts.
Then best trees are then retrieved from the list.
Itshould be noted that, in the standard case, the non-projective approximation algorithm may find a localoptimum, and that there may be other trees that havea higher score which were not explored.
Thus thebest parse in the greedy case may not necessarilybe the one with the highest score in the n-best list.Since the parser is trained with the greedy versionof the non-projective approximation algorithm, thegreedily chosen output parse tree is of special in-terest.
We thus flag this tree as the baseline mateparse, in order to use that for features in the ranker.The baseline mate parse is also our overall baselinein the dependency track.The best-first parser deviates from the EasyFirstparser in several small respects: The EasyFirst de-coder creates dependency links between the roots ofadjacent substructures, which gives an O(n log n)complexity, but restricts the output to projectivetrees.
The best-first parser is allowed to choose ashead any node of an adjacent substructure instead ofonly the root, which increases complexity to O(n2),but accounts for a big part of possible non-projectivestructures.
We additionally implemented a swap-operation (Nivre, 2009; Tratz and Hovy, 2011) toaccount for the more complex structures.
The best-first parser relies on a beam-search strategy7 to pur-7Due to the nature of the decoder, the parser can producesue multiple derivations, which we also use to pro-duce the n-best output.In the scoring step, we additionally apply the tur-boparser8 (Martins et al 2010), which is based onlinear programming relaxations.9 We changed allthree parsers such that they would return a score fora given tree.
We use this to extract scores from eachparser for all trees in the n-best lists.
It is impor-tant to have a score from every parser for every tree,as previously observed by Zhang et al(2009) in thecontext of constituency reranking.4.1 RankingTable 3 shows the performance of the individualparsers measured on the development sets.
It alsodisplays the oracle scores over the different n-bestlists, i.e., the maximal possible score over an n-bestlist if the best tree is always selected.The mate parser generally performs best followedby turboparser, while the best-first parser comes last.But we can see from the oracle scores that the best-first parser often shows comparable or even higheroracle scores than mate, and that the combinationof the n-best lists always adds substantial improve-ments to the oracle scores.
These findings show thatthe mate and best-first parsers are providing differ-ent sets of n-best lists.
Moreover, all three parsersrely on different parsing algorithms and feature sets.For these reasons, we hypothesized that the parserscontribute different views on the parse trees and thattheir combination would result in better overall per-formance.In order to leverage the diversity between theparsers we experimented with ranking10 on then-best lists.
We used the same ranking model in-troduced in Section 3 here as well.
The model istrained to select the best parse according to the la-beled attachment score (LAS).
The training data forthe ranker was created by 5-fold jackknifing on thetraining sets.
The feature sets for the ranker forspurious ambiguities in the beam.
If this occurs, only the onewith the higher score is kept.8http://www.ark.cs.cmu.edu/TurboParser/9Ideally we would also extract n-best lists from the tur-boparser, however time prevented us from making the necessarymodifications.10We refrain from calling it reranking in this setting, sincewe are using merged n-best lists and the initial ranking is notentirely clear to begin with.139Arabic Basque French German Hebrew Hungarian Korean Polish SwedishBaseline results for individual parsersmate?
88.50/83.50 88.18/84.49 92.71/90.85 83.63/75.89 87.07/82.84 86.06/82.39 91.17/85.81 83.65/77.16mate 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05bf 87.61/85.32 84.07/75.90 87.45/83.92 92.90/91.10 86.10/79.57 83.85/75.94 86.54/83.97 90.10/83.75 82.27/75.36turbo 87.82/85.35 88.88/83.84 88.24/84.57 93.59/91.54 85.74/78.95 86.86/82.80 88.35/86.23 90.97/85.55 83.24/76.15Oracle scores for n-best listsmate 90.85/88.74 93.39/89.85 90.99/87.81 97.14/95.84 89.05/83.03 91.41/88.19 94.86/92.96 95.19/91.67 87.19/81.66bf 91.47/89.46 91.68/86.46 91.38/88.68 97.40/96.60 91.04/85.67 87.64/81.79 94.90/92.94 96.25/93.74 87.60/82.46merged 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets.
mate?
uses the prepro-cessing provided by the organizers, the other parsers use the preprocessing described in Section 2.each language were optimized manually via cross-validation on the training sets.
The features used foreach language, as well as a default (baseline) fea-ture set, are shown in Table 4.
We now outline thefeatures we used in the ranker:Score from the base parsers ?
denoted B, M,T, for the best-first, mate, and turbo parsers, re-spectively.
We also have indicator features whethera certain parse was the best according to a givenparser, denoted GB, GM, GT, respectively.
Sincethe mate parser does not necessarily assign the high-est score to the baseline mate parse, the GM fea-ture is a ternary feature which indicates whether aparse is the same as the baseline mate parse, or bet-ter, or worse.
We also experimented with transfor-mations and combinations of the scores from theparsers.
Specifically, BMProd denotes the productof B and M; BMeProd denotes the sum of B and Min e-space, i.e., eB+M ; reBMT, reBT, reMT denotethe normalized product of the corresponding scores,where scores are normalized in a softmax fashionsuch that all features take on values in the interval(0, 1).Projectivity features (Hall et al 2007) ?
thenumber of non-projective edges in a tree, denotednp.
Whether a tree is ill-nested, denoted I.
Since ill-nested trees are extremely rare in the treebanks, thishelps the ranker filter out unlikely candidates fromthe n-best lists.
For a definition and further discus-sion of ill-nestedness, we refer to (Havelka, 2007).Constituent features ?
from the constituent trackwe also have constituent trees of all sentences whichcan be used for feature extraction.
Specifically, forevery head-dependent pair, we extract the path in theconstituent tree between the nodes, denoted ptbp.Case agreement ?
on head-dependent pairs thatboth have a case value assigned among their mor-phological features, we mark whether it is the samecase or not, denoted case.Function label uniqueness ?
on each training setwe extracted a list of function labels that generallyoccur at most once as the dependent of a node, e.g.,subjects or objects.
Features are then extracted fromall nodes that have one or more dependents of eachlabel aimed at capturing mistakes such as doublesubjects on a verb.
This template is denoted FL.In addition to the features mentioned above, weexperimented with a variety of feature templates, in-cluding features drawn from previous work on de-pendency reranking (Hall, 2007), e.g., lexical andPOS-based features over edges, ?subcategorization?frames (i.e., the concatenation of POS-tags that areheaded by a certain node in the tree), etc, althoughthese features did not seem to help.
For German wecreated feature templates based on the constraintsused in the constraint-based parser by Seeker andKuhn (2013).
This includes, e.g., violations in caseor number agreement between heads and depen-dents, as well as more complex features that con-sider labels on entire verb complexes.
None of thesefeatures yielded any clear improvements though.
Wealso experimented with features that target somespecific constructions (and specifics of annotationschemes) which the parsers typically cannot fullysee, such as coordination, however, also here we sawno clear improvements.4.2 Effects of RankingIn Table 5, we show the improvements from usingthe ranker, both with the baseline and optimized fea-tures sets for the ranker.
For the sake of comparison,140Arabic Basque French German Hebrew Hungarian Korean Polish SwedishBaseline 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05Ranked-dflt 88.54/86.32 89.99/85.43 88.85/85.39 94.06/92.36 87.28/80.44 88.16/84.54 88.71/86.65 92.26/87.12 84.51/77.83Ranked 88.93/86.74 89.95/85.61 89.37/85.96 94.20/92.68 87.63/81.02 88.38/84.77 89.20/87.12 93.02/87.69 85.04/78.57Oracle 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96Table 5: Performance (UAS/LAS) of the reranker on the development sets.
Baseline denotes our baseline.
Ranked-dfltand Ranked denote the default and optimized ranker feature sets, respectively.
Oracle denotes the oracle scores.default B, M, T, GB, GM, GT, IArabic B, M, T, GB, GM, I, ptbp, reBMTBasque B, M, T, GB, GM, GT, I, ptbp, I, reMT, caseFrench B, M, T, GB, GM, GT, I, ptbpGerman B, M, T, GM, I, BMProd, FLHebrew B, M, T, GB, GM, GT, I, ptbp, FL, BMeProdHungarian B, M, T, GB, GM, GT, I, ptbp, reBM, FLKorean B, M, T, GB, GM, GT, I, ptbp, reMT, FLPolish B, M, T, GB, GM, GT, I, ptbp, npSwedish B, M, T, GB, GM, GT, I, ptbp, reBM, FLTable 4: Feature sets for the dependency ranker for eachlanguage.
default denotes the default ranker feature set.the baseline mate parses as well as the oracle parseson the merged n-best lists are repeated from Table 3.We see that ranking clearly helps, both with a tai-lored feature set, as well as the default feature set.The improvement in LAS between the baseline andthe tailored ranking feature sets ranges from 1.1%(French) to 1.6% (Hebrew) absolute, with the excep-tion of Hungarian, where improvements on the devset are more modest (contrary to the test set results,cf.
Section 5).
Even with the default feature set, theimprovements range from 0.5% (French) to 1.1%(Hebrew) absolute, again setting Hungarian aside.We believe that this is an interesting result consid-ering the simplicity of the default feature set.5 Test Set ResultsIn this section we outline our final results on the testsets.
As previously, we focus on the setting withpredicted tags in gold segmentation and the largesttraining set.
We also present results on Arabic andHebrew for the predicted segmentation setting.
Forthe gold preprocessing and all 5k settings, we referthe reader to the Shared Task overview paper (Sed-dah et al 2013).11In Table 7, we present our results in the con-11Or the results page online: http://www.spmrl.org/spmrl2013-sharedtask-results.htmlstituency track.
Since we were the only participat-ing team in the constituency track, we compare our-selves with the best baseline12 provided by the or-ganizers.
Our system outperforms the baseline forall languages in terms of PARSEVAL F1.
Follow-ing the trend on the development sets, reranking isconsistently helping across languages.13 Despite thelack of other submissions in the shared task, we be-lieve our numbers are generally strong and hope thatthey can serve as a reference for future work on con-stituency parsing on these data sets.Table 8 displays our results in the dependencytrack.
We submitted two runs: a baseline, whichis the baseline mate parse, and the reranked trees.The table also compares our results to the best per-forming other participant in the shared task (denotedOther) as well as the MaltParser (Nivre et al 2007)baseline provided by the shared task organizers (de-noted ST Baseline).
We obtain the highest scoresfor all languages, with the exception of French.
It isalso clear that we make considerable gains over ourbaseline, confirming our results on the developmentsets reported in Section 4.
It is also noteworthy thatour baseline (i.e., the mate parser with our own pre-processing) outperforms the best other system for 5languages.Arabic HebrewOther 90.75/8.48 88.33/12.20Dep.
Baseline 91.13/9.10 89.27/15.01Dep.
Ranked 91.74/9.83 89.47/16.97Constituency 92.06/9.49 89.30/13.60Table 6: Unlabeled TedEval scores (accuracy/exactmatch) for the test sets in the predicted segmentation set-ting.
Only sentences of length ?
70 are evaluated.12It should be noted that the Shared Task organizers com-puted 2 different baselines on the test sets.
The best baselineresults for each language thus come from different parsers.13We remind the reader that our submission decisions are notbased on figures in Table 2, cf.
Section 3.141Arabic Basque French German Hebrew Hungarian Korean Polish SwedishST Baseline 79.19 74.74 80.38 78.30 86.96 85.22 78.56 86.75 80.64Product 80.81 87.18 81.83 80.70 89.46 90.58 83.49 87.55 83.99Reranked 81.32 87.86 82.86 81.27 89.49 91.85 84.27 87.76 84.88Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting.
ST Baseline denotes thebest baseline (out of 2) provided by the Shared Task organizers.
Our submission is underlined.Arabic Basque French German Hebrew Hungarian Korean Polish SwedishST Baseline 83.18/80.36 79.77/70.11 82.49/77.98 81.51/77.81 76.49/69.97 80.72/70.15 85.72/82.06 82.19/75.63 80.29/73.21Other 85.78/83.20 89.19/84.25 89.19/85.86 90.80/88.66 81.05/73.63 88.93/84.97 85.84/82.65 88.12/82.56 87.28/80.88Baseline 86.96/84.81 89.32/84.25 87.87/84.37 90.54/88.37 85.88/79.67 89.09/85.31 87.41/85.51 90.30/85.51 86.85/80.67Ranked 88.32/86.21 89.88/85.14 88.68/85.24 91.64/89.65 86.70/80.89 89.81/86.13 88.47/86.62 91.75/87.07 88.06/82.13Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting.
Other denotes the highestscoring other participant in the Shared Task.
ST Baseline denotes the MaltParser baseline provided by the Shared Taskorganizers.Table 6 shows the unlabeled TedEval (Tsarfaty etal., 2012) scores (accuracy/exact match) on the testsets for the predicted segmentation setting for Ara-bic and Hebrew.
Note that these figures only includesentences of length less than or equal to 70.
SinceTedEval enables cross-framework comparison, wecompare our submissions from the dependency trackto our submission from the constituency track.
Inthese runs we used the same systems that were usedfor the gold segmentation with predicted tags track.The predicted segmentation was provided by theShared Task organizers.
We also compare our re-sults to the best other system from the Shared Task(denoted Other).Also here we obtain the highest results for bothlanguages.
However, it is unclear what syntacticparadigm (dependencies or constituents) is bettersuited for the task.
All in all it is difficult to assesswhether the differences between the best and secondbest systems for each language are meaningful.6 ConclusionWe have presented our contribution to the 2013SPMRL Shared Task.
We participated in both theconstituency and dependency tracks.
In both trackswe make use of a state-of-the-art tagger for POS andmorphological features.
In the constituency track,we use the tagger to handle unknown words and em-ploy a product-of-grammars-based PCFG-LA parserand parse tree reranking.
In the dependency track,we combine multiple parsers output as input for aranker.Since there were no other participants in the con-stituency track, it is difficult to draw any conclusionsfrom our results.
We do however show that the ap-plication of product grammars, our handling of rarewords, and a subsequent reranking step outperformsa baseline PCFG-LA parser.In the dependency track we obtain the best re-sults for all languages except French among 7 partic-ipants.
Our reranking approach clearly outperformsa baseline graph-based parser.
This is the first timemultiple parsers have been used in a dependencyreranking setup.Aside from minor decisions made on the basisof each language, our approach is language agnos-tic and does not target morphology in any particu-lar way as part of the parsing process.
We showthat with a strong baseline and with no languagespecific treatment it is possible to achieve state-of-the-art results across all languages.
Our architec-ture for the dependency parsing track enables the useof language-specific features in the ranker, althoughwe only had minor success with features that targetmorphology.
However, it may be the case that ap-proaches from previous work on parsing MRLs, orthe approaches taken by other teams in the SharedTask, can be successfully combined with ours andimprove parsing accuracy even more.AcknowledgmentsRicha?rd Farkas is funded by the European Union andthe European Social Fund through the project Fu-turICT.hu (grant no.
: TA?MOP-4.2.2.C-11/1/KONV-1422012-0013).
Thomas Mu?ller is supported by aGoogle Europe Fellowship in Natural LanguageProcessing.
The remaining authors are funded bythe Deutsche Forschungsgemeinschaft (DFG) viathe SFB 732, projects D2 and D8 (PI: Jonas Kuhn).We also express our gratitude to the treebankproviders for each language: Arabic (Maamouri etal., 2004; Habash and Roth, 2009; Habash et al2009; Green and Manning, 2010), Basque (Adurizet al 2003), French (Abeille?
et al 2003), He-brew (Sima?an et al 2001; Tsarfaty, 2010; Gold-berg, 2011; Tsarfaty, 2013), German (Brants et al2002; Seeker and Kuhn, 2012), Hungarian (Csendeset al 2005; Vincze et al 2010), Korean (Choiet al 1994; Choi, 2013), Polish (S?widzin?ski andWolin?ski, 2010), and Swedish (Nivre et al 2006).ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.2003.
Building a treebank for french.
In AnneAbeille?, editor, Treebanks.
Kluwer, Dordrecht.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
D?
?az de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In TLT-03, pages 201?204.Bernd Bohnet.
2010.
Top Accuracy and Fast Depen-dency Parsing is not a Contradiction.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.
Coling 2010 Organizing Commit-tee.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Erhard Hinrichs and Kiril Simov, edi-tors, Proceedings of the First Workshop on Treebanksand Linguistic Theories (TLT 2002), pages 24?41, So-zopol, Bulgaria.Tim Buckwalter.
2002.
Buckwalter Arabic Morpholog-ical Analyzer Version 1.0.
Linguistic Data Consor-tium, University of Pennsylvania, 2002.
LDC CatalogNo.
: LDC2002L49.Xavier Carreras.
2007.
Experiments with a Higher-Order Projective Dependency Parser.
In Proceedingsof the CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 957?961, Prague, Czech Republic, June.Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 173?180.Key-Sun Choi, Young S Han, Young G Han, and Oh WKwon.
1994.
Kaist tree bank project for korean:Present and future development.
In Proceedings ofthe International Workshop on Sharable Natural Lan-guage Resources, pages 7?14.
Citeseer.Jinho D. Choi.
2013.
Preparing korean data forthe shared task on parsing morphologically rich lan-guages.
ArXiv e-prints.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
In Proceedings of the Sev-enteenth International Conference on Machine Learn-ing, ICML ?00, pages 175?182.Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?sKocsor.
2005.
The Szeged treebank.
In Va?clav Ma-tous?ek, Pavel Mautner, and Toma?s?
Pavelka, editors,Text, Speech and Dialogue: Proceedings of TSD 2005.Springer.Rickard Domeij, Ola Knutsson, Johan Carlberger, andViggo Kann.
2000.
Granska-an efficient hybrid sys-tem for Swedish grammar checking.
In In Proceed-ings of the 12th Nordic Conference in ComputationalLinguistics.Mikel L Forcada, Mireia Ginest?
?-Rosell, Jacob Nord-falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-tonio Pe?rez-Ortiz, Felipe Sa?nchez-Mart?
?nez, GemaRam?
?rez-Sa?nchez, and Francis M Tyers.
2011.
Aper-tium: A free/open-source platform for rule-based ma-chine translation.
Machine Translation.Yoav Goldberg and Michael Elhadad.
2010.
An Ef-ficient Algorithm for Easy-First Non-Directional De-pendency Parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 742?750, Los Angeles, California, June.Association for Computational Linguistics.Yoav Goldberg.
2011.
Automatic syntactic processing ofModern Hebrew.
Ph.D. thesis, Ben Gurion Universityof the Negev.Spence Green and Christopher D. Manning.
2010.
Bet-ter arabic parsing: Baselines, evaluations, and anal-ysis.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),pages 394?402, Beijing, China, August.
Coling 2010Organizing Committee.Nizar Habash and Ryan Roth.
2009.
Catib: Thecolumbia arabic treebank.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August.
Association for Com-putational Linguistics.Nizar Habash, Reem Faraj, and Ryan Roth.
2009.
Syn-tactic Annotation in the Columbia Arabic Treebank.
InProceedings of MEDAR International Conference onArabic Language Resources and Tools, Cairo, Egypt.143Keith Hall, Jiri Havelka, and David A. Smith.
2007.Log-Linear Models of Non-Projective Trees, k-bestMST Parsing and Tree-Ranking.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,pages 962?966, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Keith Hall.
2007.
K-best Spanning Tree Parsing.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 392?399, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Jiri Havelka.
2007.
Beyond Projectivity: Multilin-gual Evaluation of Constraints and Measures on Non-Projective Structures.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 608?615, Prague, Czech Republic,June.
Association for Computational Linguistics.Mark Johnson and Ahmet Engin Ural.
2010.
Rerank-ing the Berkeley and Brown Parsers.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 665?668, Los An-geles, California, June.
Association for ComputationalLinguistics.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InNEMLAR Conference on Arabic Language Resourcesand Tools.Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,and Mario Figueiredo.
2010.
Turbo Parsers: Depen-dency Parsing by Approximate Variational Inference.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 34?44, Cambridge, MA, October.
Association for Compu-tational Linguistics.Andrew Kachites McCallum.
2002.
?mal-let: A machine learning for language toolkit?.http://mallet.cs.umass.edu.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 81?88, Trento, Italy.
Asso-ciation for Computational Linguistics.Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze.2013.
Efficient Higher-Order CRFs for MorphologicalTagging.
In In Proceedings of EMNLP.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish treebank with phrase structureand dependency annotation.
In Proceedings of LREC,pages 1392?1395, Genoa, Italy.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13:95?135, 6.Joakim Nivre.
2009.
Non-Projective Dependency Pars-ing in Expected Linear Time.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages351?359, Suntec, Singapore, August.
Association forComputational Linguistics.S Park, D Choi, E-k Kim, and KS Choi.
2010.
A plug-incomponent-based Korean morphological analyzer.
InProceedings of HCLT2010.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics, pages 433?440.
Associa-tion for Computational Linguistics.Slav Petrov.
2010.
Products of Random Latent VariableGrammars.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 19?27, Los Angeles, California, June.
Associa-tion for Computational Linguistics.Helmut Schmid.
2005.
A programming language forfinite state transducers.
In FSMNLP.Djame?
Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, IakesGoenaga, Koldo Gojenola, Yoav Goldberg, SpenceGreen, Nizar Habash, Marco Kuhlmann, WolfgangMaier, Joakim Nivre, Adam Przepiorkowski, RyanRoth, Wolfgang Seeker, Yannick Versley, VeronikaVincze, Marcin Wolin?ski, and Alina Wro?blewska.2013.
Overview of the SPMRL 2013 Shared Task: ACross-Framework Evaluation of Parsing Morphologi-cally Rich Languages.
In Proceedings of the 4th Work-shop on Statistical Parsing of Morphologically RichLanguages: Shared Task, Seattle, WA.Wolfgang Seeker and Jonas Kuhn.
2012.
Making El-lipses Explicit in Dependency Conversion for a Ger-man Treebank.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Eval-uation, pages 3132?3139, Istanbul, Turkey.
EuropeanLanguage Resources Association (ELRA).Wolfgang Seeker and Jonas Kuhn.
2013.
Morphologicaland Syntactic Case in Statistical Dependency Parsing.Computational Linguistics, 39(1):23?55.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank forModern Hebrew Text.
In Traitement Automatique desLangues.144Marek S?widzin?ski and Marcin Wolin?ski.
2010.
Towardsa bank of constituent parse trees for Polish.
In Text,Speech and Dialogue: 13th International Conference(TSD), Lecture Notes in Artificial Intelligence, pages197?204, Brno, Czech Republic.
Springer.Stephen Tratz and Eduard Hovy.
2011.
A Fast, Ac-curate, Non-Projective, Semantically-Enriched Parser.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1257?1268, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Reut Tsarfaty, Djame?
Seddah, Yoav Goldberg, SandraKuebler, Yannick Versley, Marie Candito, JenniferFoster, Ines Rehbein, and Lamia Tounsi.
2010.
Sta-tistical Parsing of Morphologically Rich Languages(SPMRL) What, How and Whither.
In Proc.
of theSPMRL Workshop of NAACL-HLT, pages 1?12, LosAngeles, CA, USA.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012.
Joint Evaluation of Morphological Segmen-tation and Syntactic Parsing.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages6?10, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Reut Tsarfaty.
2010.
Relational-Realizational Parsing.Ph.D.
thesis, University of Amsterdam.Reut Tsarfaty.
2013.
A Unified Morpho-SyntacticScheme of Stanford Dependencies.
Proceedings ofACL.Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgyMo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.
Hun-garian dependency treebank.
In LREC.Zhiguo Wang and Chengqing Zong.
2011.
Parse Rerank-ing Based on Higher-Order Lexical Dependencies.
InProceedings of 5th International Joint Conference onNatural Language Processing, pages 1251?1259, Chi-ang Mai, Thailand, November.
Asian Federation ofNatural Language Processing.Marcin Wolin?ski.
2006.
Morfeusz - A practical tool forthe morphological analysis of Polish.
In Intelligent in-formation processing and web mining, pages 511?520.Springer.Hui Zhang, Min Zhang, Chew Lim Tan, and HaizhouLi.
2009.
K-Best Combination of Syntactic Parsers.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1552?1560, Singapore, August.
Association for Com-putational Linguistics.Zhenxia Zhou.
2007.
Entwicklung einer franzo?sischenFinite-State-Morphologie.
Diplomarbeit, Institute forNatural Language Processing, University of Stuttgart.Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.2013.
Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-gyors?
?tott szo?faji egye?rtelmu?s??te?s.
In IX.
MagyarSza?m?
?to?ge?pes Nyelve?szeti Konferencia.145
