Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 348?354,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsLIMSI Submission for WMT?14 QE TaskGuillaume Wisniewski and Nicolas P?echeux and Alexandre Allauzen and Franc?ois YvonUniversit?e Paris Sud and LIMSI-CNRS91 403 ORSAY CEDEX, France{wisniews, pecheux, allauzen, yvon}@limsi.frAbstractThis paper describes LIMSI participationto the WMT?14 Shared Task on Qual-ity Estimation; we took part to the word-level quality estimation task for Englishto Spanish translations.
Our system re-lies on a random forest classifier, an en-semble method that has been shown tobe very competitive for this kind of task,when only a few dense and continuous fea-tures are used.
Notably, only 16 featuresare used in our experiments.
These fea-tures describe, on the one hand, the qual-ity of the association between the sourcesentence and each target word and, on theother hand, the fluency of the hypothe-sis.
Since the evaluation criterion is thef1measure, a specific tuning strategy isproposed to select the optimal values forthe hyper-parameters.
Overall, our systemachieves a 0.67 f1score on a randomly ex-tracted test set.1 IntroductionThis paper describes LIMSI submission to theWMT?14 Shared Task on Quality Estimation.
Weparticipated in the word-level quality estimationtask (Task 2) for the English to Spanish direction.This task consists in predicting, for each word ina translation hypothesis, whether this word shouldbe post-edited or should rather be kept unchanged.Predicting translation quality at the word levelraises several interesting challenges.
First, this isa (relatively) new task and the best way to for-mulate and evaluate it has still to be established.Second, as most works on quality estimation haveonly considered prediction at the sentence level, itis not clear yet which features are really effectiveto predict quality at the word and a set of base-line features has still to be found.
Finally, sev-eral characteristic of the task (the limited numberof training examples, the unbalanced classes, etc.
)makes the use of ?traditional?
machine learning al-gorithms difficult.
This papers describes how weaddressed this different issues for our participationto the WMT?14 Shared Task.The rest of this paper is organized as follows.Section 2 gives an overview of the shared task datathat will justify some of the design decisions wemade.
Section 3 describes the different featureswe have considered and Section 4, the learningmethods used to estimate the classifiers parame-ters.
Finally the results of our models are pre-sented and analyzed in Section 5.2 World-Level Quality EstimationWMT?14 shared task on quality estimation num-ber 2 consists in predicting, for each word of atranslation hypothesis, whether this word shouldbe post-edited (denoted by the BAD label) orshould be kept unchanged (denoted by the OK la-bel).
The shared task organizers provide a bilin-gual dataset from English to Spanish1made oftranslations produced by three different MT sys-tems and by one human translator; these transla-tions have then been annotated with word-level la-bels by professional translators.
No additional in-formation about the systems used, the derivationof the translation (such as the lattices or the align-ment between the source and the best translationhypothesis) or the tokenization applied to identifywords is provided.The distributions of the two labels for the dif-ferent systems is displayed in Table 1.
As itcould be expected, the class are, overall, unbal-anced and the systems are of very different qual-ity: the proportion of BAD and OK labels highlydepends on the system used to produce the transla-tion hypotheses.
However, as our preliminary ex-periments have shown, the number of examples is1We did not consider the other language pairs.348too small to train a different confidence estimationsystem for each system.The distribution of the number of BAD labelsper sentence is very skewed: on average, one wordout of three (precisely 35.04%) in a sentence is la-beled as BAD but the median of the distribution ofthe ratio of word labeled BAD in a sentence is 20%and its standard deviation is pretty high (34.75%).Several sentences have all their words labeled aseither OK or BAD, which is quite surprising as thesentences of the corpus for Task 2 have been se-lected because there were ?near miss translations?that is to say translations that should have con-tained no more that 2 or 3 errors.Another interesting finding is that the propor-tion of word to post-edit is the same across thedifferent parts-of-speech (see Table 2).2Table 1: Number of examples and distribution oflabels for the different systems on the training setSystem #sent.
#words % OK % BAD1 791 19,456 75.48 24.522 621 14,620 59.11 40.893 454 11,012 59.76 40.244 90 2,296 36.85 63.15Total 1,956 47,384 64.90 35.10Table 2: Distribution of labels according to thePOS on the training setPOS % in train % BADNOUN 23.81 35.02ADP 15.06 35.48DET 14.90 32.88VERB 14.64 41.26PUNCT 10.92 27.26ADJ 6.61 35.68CONJ 5.04 30.77PRON 4.58 43.15ADV 4.39 36.56As the classes are unbalanced, prediction per-formance will be evaluated in terms of precision,recall and f1score computed on the BAD label.More precisely, if the number of true positive (i.e.2We used FreeLing (http:nlp.lsi.upc.edu/freeling/) to predict the POS tags of the translationhypotheses and, for the sake of clarity, mapped the 71 tagsused by FreeLing to the 11 universal POS tags of Petrov etal.
(2012).BAD word predicted as BAD), false positive (OKword predicted as BAD) and false negative (BADword predicted as OK) are denoted tpBAD, fpBADand fnBAD, respectively, the quality of a confidenceestimation system is evaluated by the three follow-ing metrics:pBAD=tpBADtpBAD+ fpBAD(1)rBAD=tpBADtpBAD+ fnBAD(2)f1=2 ?
pBAD?
rBADpBAD+ rBAD(3)3 FeaturesIn our experiments, we used 16 features to de-scribe a given target word tiin a translation hy-pothesis t = (tj)mj=1.
To avoid sparsity issues wedecided not to include any lexicalized informationsuch as the word or the previous word identities.As the translation hypotheses were generated bydifferent MT systems, no white-box features (suchas word alignment or model scores) are consid-ered.
Our features can be organized in two broadcategories:Association Features These features measurethe quality of the ?association?
between the sourcesentence and a target word: they characterize theprobability for a target word to appear in a transla-tion of the source sentence.
Two kinds of associa-tion features can be distinguished.The first one is derived from the lexicalizedprobabilities p(t|s) that estimate the probabilitythat a source word s is translated by the targetword tj.
These probabilities are aggregated usingan arithmetic mean:p(tj|s) =1nn?i=1p(tj|si) (4)where s = (si)ni=1is the source sentence (with anextra NULL token).
We assume that p(tj|si) = 0 ifthe words tjand sihave never been aligned in thetrain set and also consider the geometric mean ofthe lexicalized probabilities, their maximum value(i.e.
maxs?sp(tj|s)) as well as a binary featurethat fires when the target word tjis not in the lex-icalized probabilities table.The second kind of association features relieson pseudo-references, that is to say, translationsof the source sentence produced by an indepen-dent MT system.
Many works have considered349pseudo-references to design new MT metrics (Al-brecht and Hwa, 2007; Albrecht and Hwa, 2008)or for confidence estimation (Soricut and Echi-habi, 2010; Soricut and Narsale, 2012) but, to thebest of our knowledge, this is the first time thatthey are used to predict confidence at the wordlevel.Pseudo-references are used to define 3 binaryfeatures which fire if the target word is in thepseudo-reference, in a 2-gram shared between thepseudo-reference and the translation hypothesis orin a common 3-gram, respectively.
The latticesrepresenting the search space considered to gen-erate these pseudo-references also allow us to es-timate the posterior probability of a target wordthat quantifies the probability that it is part of thesystem output (Gispert et al., 2013).
Posteriors ag-gregate two pieces of information for each word inthe final hypothesis: first, all the paths in the lat-tice (i.e.
the number of translation hypotheses inthe search space) where the word appears in areconsidered; second, the decoder scores of thesepaths are accumulated in order to derive a confi-dence measure at the word level.
In our experi-ments, we considered pseudo-references and lat-tices produced by the n-gram based system de-veloped by our team for last year WMT evalu-ation campaign (Allauzen et al., 2013), that hasachieved very good performance.Fluency Features These features measure the?fluency?
of the target sentence and are based ondifferent language models: a ?traditional?
4-gramlanguage model estimated on WMT monolingualand bilingual data (the language model used byour system to generate the pseudo-references); acontinuous-space 10-gram language model esti-mated with SOUL (Le et al., 2011) (also used byour MT system) and a 4-gram language modelbased on Part-of-Speech sequences.
The lattermodel was estimated on the Spanish side of thebilingual data provided in the translation sharedtask in 2013.
These data were POS-tagged withFreeLing (Padr?o and Stanilovsky, 2012).All these language models have been used to de-fine two different features :?
the probability of the word of interest p(tj|h)where h = tj?1, ..., tj?n+1is the historymade of the n?
1 previous words or POS?
the ratio between the probability ofthe sentence and the ?best?
probabil-ity that can be achieved if the targetword is replaced by any other word (i.e.maxv?Vp(t1, ..., tj?1, v, tj+1, ..., tm) wherethe max runs over all the words of thevocabulary).There is also a feature that describes the back-offbehavior of the conventional language model: itsvalue is the size of the largest n-gram of the trans-lation hypothesis that can be estimated by the lan-guage model without relying on back-off probabil-ities.Finally, there is a feature describing, for eachword that appears more than once in the train set,the probability that this word is labeled BAD.
Thisprobability is simply estimated by the ratio be-tween the number of times this word is labeledBAD and the number of occurrences of this word.It must be noted that most of the features weconsider rely on models that are part of a ?clas-sic?
MT system.
However their use for predictingtranslation quality at the word-level is not straight-forward, as they need to be applied to sentenceswith a given unknown tokenization.
Matching thetokenization used to estimate the model to the oneused for collecting the annotations is a tedious anderror-prone process and some of the prediction er-rors most probably result from mismatches in tok-enization.4 Learning Methods4.1 ClassifiersPredicting whether a word in a translation hypoth-esis should be post-edited or not can naturally beframed as a binary classification task.
Based onour experiments in previous campaigns (Singh etal., 2013; Zhuang et al., 2012), we considered ran-dom forest in all our experiments.3Random forest (Breiman, 2001) is an ensem-ble method that learns many classification treesand predicts an aggregation of their result (for in-stance by majority voting).
In contrast with stan-dard decision trees, in which each node is splitusing the best split among all features, in a ran-dom forest the split is chosen randomly.
In spiteof this simple and counter-intuitive learning strat-egy, random forests have proven to be very good?out-of-the-box?
learners.
Random forests haveachieved very good performance in many similar3we have used the implementation provided byscikit-learn (Pedregosa et al., 2011).350tasks (Chapelle and Chang, 2011), in which onlya few dense and continuous features are available,possibly because of their ability to take into ac-count complex interactions between features andto automatically partition the continuous featuresvalue into a discrete set of intervals that achievesthe best classification performance.As a baseline, we consider logistic regres-sion (Hastie et al., 2003), a simple linear modelwhere the parameters are estimated by maximiz-ing the likelihood of the training set.These two classifiers do not produce only a classdecision but yield an instance probability that rep-resents the degree to which an instance is a mem-ber of a class.
As detailed in the next section,thresholding this probability will allow us to di-rectly optimize the f1score used to evaluate pre-diction performance.4.2 Optimizing the f1ScoreAs explained in Section 2, quality prediction willbe evaluated in terms of f1score.
The learn-ing methods we consider can not, as most learn-ing method, directly optimize the f1measure dur-ing training, since this metric does not decomposeover the examples.
It is however possible to takeadvantage of the fact that they actually estimate aprobability to find the largest f1score on the train-ing set.Indeed these probabilities are used with athreshold (usually 0.5) to produce a discrete (bi-nary) decision: if the probability is above thethreshold, the classifier produces a positive out-put, and otherwise, a negative one.
Each thresh-old value produces a different trade-off betweentrue positives and false positives and consequentlybetween recall and precision: as the the thresholdbecomes lower and lower, more and more exam-ple are assigned to the positive class and recall in-crease at the expense of precision.Based on these observations, we propose thefollowing three-step method to optimize the f1score on the training set:1. the classifier is first trained using the ?stan-dard?
learning procedure that optimizes eitherthe 0/1 loss (for random forest) or the likeli-hood (for the logistic regression);2. all the possible trade-offs between recalland precision are enumerated by varyingthe threshold; exploiting the monotonicity ofthresholded classifications,4this enumerationcan be efficiently done in O (n ?
log n) andresults in at most n threshold values, where nis the size of the training set (Fawcett, 2003);3. all the f1scores achieved for the differentthresholds found in the previous step are eval-uated; there are strong theoretical guaran-tees that the optimal f1score that can beachieved on the training set is one of thesevalues (Boyd and Vandenberghe, 2004).Figure 1 shows how f1score varies with the deci-sion threshold and allows to assess the differencebetween the optimal value of the threshold and itsdefault value (0.5).Figure 1: Evolution of the f1score with respect tothe threshold used to transform probabilities intobinary decisions5 ExperimentsThe features and learning strategies described inthe two previous sections were evaluated on theEnglish to Spanish datasets.
As no official devel-opment set was provided by the shared task orga-nizers, we randomly sampled 200 sentences fromthe training set and use them as a test set through-out the rest of this article.
Preliminary experimentsshow that the choice of this test has a very low im-pact on the classification performance.
The dif-ferent hyper-parameters of the training algorithm4Any instance that is classified positive with respect to agiven threshold will be classified positive for all lower thresh-olds as well.351Table 3: Prediction performance for the two learn-ing strategies consideredClassifier thres.
rBADpBADf1Random forest 0.43 0.64 0.69 0.67Logistic regression 0.27 0.51 0.72 0.59were chosen by maximizing classification perfor-mance (as evaluated by the f1score) estimated on150 sentences of the training set kept apart as avalidation set.Results for the different learning algorithmsconsidered are presented in Table 3.
Random for-est clearly outperforms a simple logistic regres-sion, which shows the importance of using non-linear decision functions, a conclusion at pair withour previous results (Zhuang et al., 2012; Singh etal., 2013).The overall performance, with a f1measure of0.67, is pretty low and in our opinion, not goodenough to consider using such a quality estimationsystem in a computer-assisted post-edition con-text.
However, as shown in Table 4, the predictionperformance highly depends on the POS categoryof the words: it is quite good for ?plain?
words(like verb and nouns) but much worse for othercategories.There are two possible explanations for thisobservation: predicting the correctness of somemorpho-syntaxic categories may be intrinsicallyharder (e.g.
for punctuation the choice of whichcan be highly controversial) or depend on infor-mation that is not currently available to our sys-tem.
In particular, we do not consider any in-formation about the structure of the sentence andabout the labels of the context, which may explainwhy our system does not perform well in predict-ing the labels of determiners and conjunctions.
Inboth cases, this result brings us to moderate ourprevious conclusions: as a wrong punctuation signhas not the same impact on translation quality as awrong verb, our system might, regardless of its f1score, be able to provide useful information aboutthe quality of a translation.
This also suggests thatwe should look for a more ?task-oriented?
metric.Finally, Figure 2 displays the importance of thedifferent features used in our system.
Randomforests deliver a quantification of the importanceof a feature with respect to the predictability of thetarget variable.
This quantification is derived fromTable 4: Prediction performance for each POS tagSystem f1VERB 0.73PRON 0.72ADJ 0.70NOUN 0.69ADV 0.69overall 0.67DET 0.62ADP 0.61CONJ 0.57PUNCT 0.56the position of a feature in a decision tree: fea-tures used in the top nodes of the trees, which con-tribute to the final prediction decision of a largerfraction of the input samples, play a more impor-tant role than features used near the leaves of thetree.
It appears that, as for our previous experi-ments (Wisniewski et al., 2013), the most relevantfeature for predicting translation quality is the fea-ture derived from the SOUL language model, evenif other fluency features seem to also play an im-portant role.
Surprisingly enough, features relatedto the pseudo-reference do not seem to be useful.Further experiments are needed to explain the rea-sons of this observation.6 ConclusionIn this paper we described the system submittedfor Task 2 of WMT?14 Shared Task on QualityEstimation.
Our system relies on a binary clas-sifier and consider only a few dense and contin-uous features.
While the overall performance ispretty low, a fine-grained analysis of the errors ofour system shows that it can predict the quality ofplain words pretty accurately which indicates thata more ?task-oriented?
evaluation may be needed.AcknowledgmentsThis work was partly supported by ANR projectTransread (ANR-12-CORD-0015).
Warm thanksto Quoc Khanh Do for his help for training a SOULmodel for Spanish.ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regressionfor sentence-level mt evaluation with pseudo refer-3520 0.02 0.04 0.06 0.08 0.1 0.12 0.14notInIBM1TablewordNotInSearchSpacepseudoRefCommon3grampseudoRefCommon1grampseudoRefCommon2grammaxMatchingNGramSizegeomIBM1diffMaxLMbestAlwordPosteriorarithIBM1posLMtradiLMmaxLMScorepriorProbasoulLMFeature ImportanceFeatureFigure 2: Features considered by our system sorted by their relevance for predicting translation errorsences.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages296?303, Prague, Czech Republic, June.
ACL.Joshua Albrecht and Rebecca Hwa.
2008.
The roleof pseudo references in MT evaluation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 187?190, Columbus, Ohio, June.ACL.Alexandre Allauzen, Nicolas P?echeux, Quoc KhanhDo, Marco Dinarelli, Thomas Lavergne, Aur?elienMax, Hai-Son Le, and Franc?ois Yvon.
2013.
LIMSI@ WMT13.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, pages 62?69, Sofia, Bulgaria, August.
ACL.Stephen Boyd and Lieven Vandenberghe.
2004.
Con-vex Optimization.
Cambridge University Press, NewYork, NY, USA.Leo Breiman.
2001.
Random forests.
Mach.
Learn.,45(1):5?32, October.Olivier Chapelle and Yi Chang.
2011.
Yahoo!
learn-ing to rank challenge overview.
In Olivier Chapelle,Yi Chang, and Tie-Yan Liu, editors, Yahoo!
Learn-ing to Rank Challenge, volume 14 of JMLR Pro-ceedings, pages 1?24.
JMLR.org.Tom Fawcett.
2003.
ROC Graphs: Notes and PracticalConsiderations for Researchers.
Technical ReportHPL-2003-4, HP Laboratories, Palo Alto.Adri`a Gispert, Graeme Blackwood, Gonzalo Iglesias,and William Byrne.
2013.
N-gram posterior prob-ability confidence measures for statistical machinetranslation: an empirical study.
Machine Transla-tion, 27(2):85?114.Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-man.
2003.
The Elements of Statistical Learning.Springer, July.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
Structuredoutput layer neural network language model.
InAcoustics, Speech and Signal Processing (ICASSP),2011 IEEE International Conference on, pages5524?5527.
IEEE.Llu?
?s Padr?o and Evgeny Stanilovsky.
2012.
Freeling3.0: Towards wider multilinguality.
In Proceedingsof the Language Resources and Evaluation Confer-ence (LREC 2012), Istanbul, Turkey, May.
ELRA.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istan-bul, Turkey, may.
European Language ResourcesAssociation (ELRA).Anil Kumar Singh, Guillaume Wisniewski, andFranc?ois Yvon.
2013.
LIMSI submission forthe WMT?13 quality estimation task: an experi-ment with n-gram posteriors.
In Proceedings of theEighth Workshop on Statistical Machine Transla-tion, pages 398?404, Sofia, Bulgaria, August.
ACL.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translations353via ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621, Uppsala, Sweden, July.ACL.Radu Soricut and Sushant Narsale.
2012.
Combiningquality prediction and system selection for improvedautomatic translation output.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 163?170, Montr?eal, Canada, June.
ACL.Guillaume Wisniewski, Anil Kumar Singh, andFranc?ois Yvon.
2013.
Quality estimation for ma-chine translation: Some lessons learned.
MachineTranslation, 27(3).Yong Zhuang, Guillaume Wisniewski, and Franc?oisYvon.
2012.
Non-linear models for confidence es-timation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 157?162,Montr?eal, Canada, June.
ACL.354
