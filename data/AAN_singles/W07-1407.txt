Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 42?47,Prague, June 2007. c?2007 Association for Computational LinguisticsLearning Textual Entailment using SVMs and String Similarity MeasuresProdromos Malakasiotis and Ion AndroutsopoulosDepartment of InformaticsAthens University of Economics and BusinessPatision 76, GR-104 34 Athens, GreeceAbstractWe present the system that we submitted tothe 3rd Pascal Recognizing Textual Entail-ment Challenge.
It uses four Support VectorMachines, one for each subtask of the chal-lenge, with features that correspond to stringsimilarity measures operating at the lexicaland shallow syntactic level.1 IntroductionTextual Entailment is desirable in many natural lan-guage processing areas, such as question answer-ing, information extraction, information retrieval,and multi-document summarization.
In the PascalRecognizing Textual Entailment Challenge (RTE), itis defined as the task of deciding whether or not themeaning of a hypothesis text (H) can be inferredfrom the meaning of another text (T ).1 For instance:T : The drugs that slow down or halt Alzheimer?s diseasework best the earlier you administer them.H: Alzheimer?s disease is treated using drugs.is a correct entailment pair, but the following is not:T : Drew Walker, NHS Tayside?s public health director, said:?It is important to stress that this is not a confirmed caseof rabies.
?H: A case of rabies was confirmed.In previous RTE challenges (Dagan et al, 2006;Bar-Haim et al, 2006), several machine-learning ap-proaches appeared, but their results showed that sig-nificant improvements were still necessary.
In thispaper, we present the system we used in the third1See http://www.pascal-network.org/.RTE challenge.
The latter had four different devel-opment and test sets (QA, IR, IE, SUM), intended toevaluate textual entailment recognition in the fournatural language processing areas mentioned above.2 System overviewOur system uses SVMs (Vapnik, 1998) to determinewhether each T?H pair constitutes a correct tex-tual entailment or not.
In particular, it employs fourSVMs, each trained on the development dataset ofthe corresponding RTE subtask (QA, IR, IE, SUM)and used on the corresponding test dataset.
Pre-liminary experiments indicated that training a singleSVM on all four subsets leads to worse results, de-spite the increased size of the training set, presum-ably because of differences in how the pairs wereconstructed in each subtask, which do not allow asingle SVM to generalize well over all four.The system is based on the assumption that stringsimilarity at the lexical and shallow syntactic levelcan be used to identify textual entailment reason-ably well, at least in question answering, the mainarea we are interested in.
We, therefore, try to cap-ture different kinds of similarity by employing 10different string similarity measures, to be discussedbelow.
In each T?H case, every measure is appliedto the following 8 pairs of strings, producing a totalof 80 measurements:pair 1: two strings with the original words of T andH , respectively; although we refer to ?words?,this and the following string pairs also containnon-word tokens, such as punctuation.22We use OPENNLP?s tokenizer, POS-tagger, and chunker (seehttp://opennlp.sourceforge.net/), and our ownimplementation of Porter?s stemmer.42pair 2: two strings containing the correspondingstems of the words of T and H , respectively;pair 3: two strings containing the part-of-speech(POS) tags of the words of T and H;pair 4: two strings containing the chunk tags (seebelow) of the words of T and H;pair 5: two strings containing only the nouns of Tand H , as identified by a POS-tagger;pair 6: two strings containing only the stems of thenouns of T and H;pair 7: two strings containing only the verbs of Tand H , as identified by a POS-tagger;pair 8: two strings containing only the stems of theverbs of T and H .Chunk tags are of the form B-x, I-x or O, were B andI indicate the initial and other words of the chunks,respectively, whereas O indicates words outside allchunks; x can be NP, VP, or PP, for noun phrase,verb phrase, and prepositional phrase chunks.Partial matches: When applying the string simi-larity measures, one problem is that T may be muchlonger than H , or vice versa.
Consider, for exam-ple, the following T?H pair.
The difference in thelengths of T and H may mislead many similaritymeasures to indicate that the two texts are very dis-similar, even though H is included verbatim in T .T : Charles de Gaulle died in 1970 at the age of eighty.
Hewas thus fifty years old when, as an unknown officer re-cently promoted to the (temporary) rank of brigadier gen-eral, he made his famous broadcast from London reject-ing the capitulation of France to the Nazis after the deba-cle of May-June 1940.H: Charles de Gaulle died in 1970.To address this problem, when we consider a pairof strings (s1, s2), if s1 is longer than s2, we alsocompute the ten values fi(s?1, s2), where fi (1 ?
i ?10) are the string similarity measures, for every s?1that is a substring of s1 of the same length as s2.
Wethen locate the s?1 with the best average similarity tos2, shown below as s?
?1 :s?
?1 = argmaxs?110?i=1fi(s?1, s2)and we keep the ten fi(s?
?1 , s2) values and their aver-age as 11 additional measurements.
Similarly, if s2is longer than s1, we keep the ten fi(s1, s?
?2 ) valuesand their average.
This process could be applied toall pairs 1?8 above, but the system we submitted ap-plied it only to pairs 1?4; hence, there is a total of 44additional measurements in each T?H case.The 124 measurements discussed above provide124 candidate numeric features that can be used bythe SVMs.3 To those, we add the following four:Negation: Two Boolean features, showing if T orH , respectively, contain negation, identified bylooking for words like ?not?, ?won?t?, etc.Length ratio: This is min(LT ,LH)max(LT ,LH) , were LT andLH are the lengths, in words, of T and H .Text length: Binary feature showing if the markupof the dataset flags T as ?long?
or ?short?.Hence, there are 128 candidate features in total.From those, we select a different subset for theSVM of each subtask, as will be discussed in fol-lowing sections.
Note that similarity measures havealso been used in previous RTE systems as fea-tures in machine learning algorithms; see, for ex-ample, Kozareva and Montoyo (2006), Newman etal.
(2006).
However, the results of those systems in-dicate that improvements are still necessary, and webelieve that one possible improvement is the use ofmore and different similarity measures.We did not use similarity measures that operateon parse trees or semantic representations, as we areinterested in RTE methods that can also be applied toless spoken languages, where reliable parsers, factextractors, etc.
are often difficult to obtain.2.1 String similarity measuresWe now describe the ten string similarity measuresthat we use.4 The reader is reminded that the mea-sures are applied to string pairs (s1, s2), where s1and s2 derive from T and H , respectively.Levenshtein distance: This is the minimum num-ber of operations (edit distance) needed to transformone string (in our case, s1) into the other one (s2),3All feature values are normalized in [?1, 1].4We use the SIMMETRICS library; see http://www.dcs.shef.ac.uk/?sam/simmetrics.html.43where an operation is an insertion, deletion, or sub-stitution of a single character.
In pairs of strings thatcontain POS or chunk tags, it would be better to con-sider operations that insert, delete, or substitute en-tire tags, instead of characters, but the system wesubmitted did not do this; we addressed this issue insubsequent work, as will be discussed below.Jaro-Winkler distance: The Jaro-Winkler dis-tance (Winkler, 1999) is a variation of the Jaro dis-tance (Jaro, 1995), which we describe first.
The Jarodistance dj of s1 and s2 is defined as:dj(s1, s2) =m3 ?
l1+m3 ?
l2+m ?
t3 ?
m,where l1 and l2 are the lengths (in characters) of s1and s2, respectively.
The value m is the number ofcharacters of s1 that match characters of s2.
Twocharacters from s1 and s2, respectively, are taken tomatch if they are identical and the difference in theirpositions does not exceed max(l1,l2)2 ?
1.
Finally, tocompute t (?transpositions?
), we remove from s1 ands2 all characters that do not have matching charac-ters in the other string, and we count the number ofpositions in the resulting two strings that do not con-tain the same character; t is half that number.The Jaro-Winkler distance dw emphasizes prefixsimilarity between the two strings.
It is defined as:dw(s1, s2) = dj(s1, s2) + l ?
p ?
[1 ?
dj(s1, s2)],where l is the length of the longest common prefixof s1 and s2, and p is a constant scaling factor thatalso controls the emphasis placed on prefix similar-ity.
The implementation we used considers prefixesup to 6 characters long, and sets p = 0.1.Again, in pairs of strings (s1, s2) that contain POStags or chunk tags, it would be better to apply thismeasure to the corresponding lists of tags in s1 ands2, instead of treating s1 and s2 as strings of char-acters, but the system we submitted did not do this;this issue was also addressed in subsequent work.Soundex: Soundex is an algorithm intended tomap each English name to an alphanumeric code,so that names whose pronunciations are the sameare mapped to the same code, despite spelling dif-ferences.5 Although Soundex is intended to be used5See http://en.wikipedia.org/wiki/Soundex.on names, and in effect considers only the first let-ter and the first few consonants of each name, weapplied it to s1 and s2, in an attempt to capture simi-larity at the beginnings of the two strings; the stringswere first stripped of all white spaces and non-lettercharacters.
We then computed similarity betweenthe two resulting codes using the Jaro-Winkler dis-tance.
A better approach would be to apply Soundexto all words in T and H , forming a 9th pair (s1, s2),on which other distance measures would then be ap-plied; we did this in subsequent work.Manhattan distance: Also known as City Blockdistance or L1, this is defined for any two vectors~x = ?x1, .
.
.
, xn?
and ~y = ?y1, .
.
.
, yn?
in an n-dimensional vector space as:L1(~x, ~y) =n?i=1|xi ?
yi|.In our case, n is the number of distinct words (ortags) that occur in s1 and s2 (in any of the two);and xi, yi show how many times each one of thesedistinct words occurs in s1 and s2, respectively.Euclidean distance: This is defined as follows:L2(~x, ~y) =???
?n?i=1(xi ?
yi)2.In our case, ~x and ~y correspond to s1 and s2, respec-tively, as in the previous measure.Cosine similarity: The definition follows:cos(~x, ~y) =~x ?
~y?~x?
?
?~y?.In our system ~x and ~y are as above, except that theyare binary, i.e., xi and yi are 1 or 0, depending onwhether or not the corresponding word (or tag) oc-curs in s1 or s2, respectively.N-gram distance: This is the same as L1, but in-stead of words we use all the (distinct) character n-grams in s1 and s2; we used n = 3.Matching coefficient: This is |X ?
Y |, where Xand Y are the sets of (unique) words (or tags) of s1and s2, respectively; i.e., it counts how many com-mon words s1 and s2 have.44Dice coefficient: This is the following quantity; inour case, X and Y are as in the previous measure.2 ?
|X ?
Y ||X| + |Y |Jaccard coefficient: This is defined as |X?Y ||X?Y | ;again X and Y are as in the matching coefficient.2.2 SVM tuning and feature selectionAs already noted, we employed four SVMs, one foreach subtask of the challenge (IR, IE, QA, SUM).6In each subtask, feature selection was performed asfollows.
We started with a set of 20 features, whichcorrespond to the ten similarity measures applied toboth words and stems (string pairs 1 and 2 of section1); see table 1.
We then added the 10 features thatcorrespond to the ten similarity measures applied toPOS tags (string pair 3).
In IE and IR, this addi-tion led to improved leave-one-out cross-validationresults on the corresponding development sets, andwe kept the additional features (denoted by ?X?
intable 1).
In contrast, in QA and SUM the additional10 features were discarded, because they led to noimprovement in the cross-validation.
We then addedthe 10 features that corresponded to the ten similar-ity measures applied to chunk tags (string pair 4),which were retained only in the IE SVM, and so on.The order in which we considered the various ex-tensions of the feature sets is the same as the order ofthe rows of table 1, and it reflects the order in whichit occurred to us to consider the corresponding ad-ditional features while preparing for the challenge.We hope to investigate additional feature selectionschemes in further work; for instance, start with all128 features and explore if pruning any groups offeatures improves the cross-validation results.With each feature set that we considered, weactually performed multiple leave-one-out cross-validations on the development dataset, for differentvalues of the parameters of the SVM and kernel, us-ing a grid-search utility.
Each feature set was eval-uated by considering its best cross-validation result.The best cross-validation results for the final featuresets of the four SVMs are shown in table 2.6We use LIBSVM (Chang and Lin, 2001), with a Radial BasisFunction kernel, including LIBSVM?s grid search tuning utility.Subtask Accuracy (%)QA 86.50 (90.00)IR 80.00 (75.50)SUM 73.00 (72.50)IE 62.00 (61.50)all 75.38 (74.88)Table 2: Best cross-validation results of our systemon the development datasets.
Results with subse-quent improvements are shown in brackets.Subtask Accuracy (%) Average Precision (%)QA 73.50 (76.00) 81.03 (81.08)IR 64.50 (63.50) 63.61 (67.28)SUM 57.00 (60.50) 60.88 (61.58)IE 52.00 (49.50) 58.16 (51.57)all 61.75 (62.38) 68.08 (68.28)Table 3: Official results of our system.
Results withsubsequent improvements are shown in brackets.3 Official results and discussionWe submitted only one run to the third RTE chal-lenge.
The official results of our system are shownin table 3.7 They are worse than the best results wehad obtained in the cross-validations on the devel-opment datasets (cf.
table 2), but this was expectedto a large extent, since the SVMs were tuned on thedevelopment datasets; to some extent, the lower of-ficial results may also be due to different types ofentailment being present in the test datasets, whichhad not been encountered in the training sets.As in the cross-validation results, our system per-formed best in the QA subtask; the second and thirdbest results of our system were obtained in IR andSUM, while the worst results were obtained in IE.Although a more thorough investigation is neces-sary to account fully for these results, it appears thatthey support our initial assumption that string simi-larity at the lexical and shallow syntactic level can beused to identify textual entailment reasonably wellin question answering systems.
Some further reflec-tions on the results of our system follow.In the QA subtask of the challenge, it appears thateach T was a snippet returned by a question answer-ing system for a particular question.8 We are notaware of exactly how the T s were selected by the7See the RTE Web site for a definition of ?average precision?.8Consult http://www.pascal-network.org/Challenges/RTE3/Introduction/.45Feature sets features IE IR QA SUMsimilarity measures on words 10 X X X Xsimilarity measures on stems 10 X X X X+ similarity measures on POS tags +10 X X+ similarity measures on chunk tags +10 X X+ average of sim.
measures on words of best partial match +1 X+ average of sim.
measures on stems of best partial match +1 X X+ average of sim.
measures on POS tags of best partial match +1 X X+ average of sim.
measures on chunk tags of best partial match +1 X X+ similarity measures on words of best partial match +10+ similarity measures on stems of best partial match +10 X+ similarity measures on POS tags of best partial match +10 X+ similarity measures on chunk tags of best partial match +10+ negation +2 X+ length ratio +1 X+ similarity measures on nouns +10 X+ similarity measures on noun stems +10+ similarity measures on verbs +10 X+ similarity measures on verb stems +10+ short/long T +1 X XTotal 128 64 31 23 54Table 1: Feature sets considered and chosen in each subtask.systems used, but QA systems typically return T sthat contain the expected answer type of the inputquestion; for instance, if the question is ?When didCharles de Gaulle die?
?, T will typically contain atemporal expression.
Furthermore, QA systems typi-cally prefer T s that contain many words of the ques-tion, preferably in the same order, etc.
(Radev etal., 2000; Ng et al, 2001; Harabagiu et al, 2003).Hence, if the answers are sought in a document col-lection with high redundancy (e.g., the Web), i.e.,a collection where each answer can be found withmany different phrasings, the T s (or parts of them)that most QA systems return are often very similar,in terms of phrasings, to the questions, provided thatthe required answers exist in the collection.In the QA datasets of the challenge, for each T ,which was a snippet returned by a QA system for aquestion (e.g., ?When did Charle de Gaulle die??
),an H was formed by ?plugging into?
the questionan expression of the expected answer type from T .In effect, this converted all questions to propositions(e.g., ?Charle de Gaulle died in 1970.?)
that requirea ?yes?
or ?no?
answer.
Note that this plugging indoes not always produce a true proposition; T maycontain multiple expressions of the expected answertype (e.g., ?Charle de Gaulle died in 1970.
In 1990,a monument was erected.
.
.
?)
and the wrong onemay be plugged into the question (H = ?Charle deGaulle died in 1990.?
).Let us first consider the case where the proposi-tion (H) is true.
Assuming that the document collec-tion is redundant and that the answer to the questionexists in the collection, T (or part of it) will often bevery similar to H , since it will be very similar to thequestion that H was derived from.
In fact, the simi-larity between T andH may be greater than betweenT and the question, since an expression from T hasbeen plugged into the question to form H .
Beingvery similar, T will very often entail H , and, hence,the (affirmative) responses of our system, which arebased on similarity, will be correct.Let us now consider the case whereH is false.
Al-though the same arguments apply, and, hence, onemight again expect T to be very similar to H , thisis actually less likely now, because H is false and,hence, it is more difficult to find a very similarlyphrased T in the presumed trustful document collec-tion.
The reduced similarity between T and H willlead the similarity measures to suggest that the T?Hentailment does not hold; and in most cases, this is acorrect decision, because H is false and, thus, it can-not be entailed by a (true) T that has been extractedfrom a trustful document collection.Similar arguments apply to the IR subtask, whereour system achieved its second best results.
Our re-sults in this subtask were lower than in the QA sub-46task, presumably because the T s were no longer fil-tered by the additional requirement that they mustcontain an expression of the expected answer type.We attribute the further deterioration of our re-sults in the SUM subtask to the fact that, accord-ing to the challenge?s documentation, all the T?Hpairs of that subtask, both true and false entailments,were chosen to have high lexical similarity, whichdoes not allow the similarity measures of our systemto distinguish well between the two cases.
Finally,the lower results obtained in the IE subtask may bedue to the fact that the T?H pairs of that subtaskwere intended to reflect entailments identified by in-formation extraction systems, which specialize onidentifying particular semantic relations by employ-ing more complicated machinery (e.g., named entityrecognizers and matchers, fact extractors, etc.)
thansimple string similarity measures; the results mayalso be partly due to the four different ways thatwere used to construct the T?H pairs of that sub-task.
It is interesting to note (see table 1) that thefeature sets were larger in the subtasks where oursystem scored worse, which may be an indication ofthe difficulties the corresponding SVMs encountered.4 Conclusions and further workWe presented a textual entailment recognition sys-tem that relies on SVMs whose features correspondto string similarity measures applied to the lexicaland shallow syntactic level.
Experimental results in-dicate that the system performs reasonably well inquestion answering (QA), which was our main tar-get, with results deteriorating as we move to infor-mation retrieval (IR), multi-document summariza-tion (SUM), and information extraction (IE).In work carried out after the official submissionof our system, we incorporated two of the possibleimprovements that were mentioned in previous sec-tions: we treated strings containing POS or chunktags as lists of tags; and we applied Soundex to eachword of T and H , forming a 9th pair of strings, onwhich all other similarity measures were applied;feature selection was then repeated anew.
The cor-responding results are shown in brackets in tables2 and 3.
There was an overall improvement in alltasks (QA, IR, SUM), except for IE, where textual en-tailment is more difficult to capture via textual simi-larity, as commented above.
We have suggested twoadditional possible improvements: applying partialmatching to all of the string pairs that we consider,and investigating other feature selection schemes.
Infuture work, we also plan to exploit WordNet to cap-ture synonyms, hypernyms, etc.AcknowledgementsThis work was funded by the Greek PENED 2003 programme,which is co-funded by the European Union (75%), and theGreek General Secretariat for Research and Technology (25%).ReferencesR.
Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo,B.
Magnini, and I. Szpektor.
2006.
The 2nd PASCAL recog-nising textual entailment challenge.
In Proceedings of the2nd PASCAL Challenges Workshop on Recognising TextualEntailment, Venice, Italy.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: a libraryfor Support Vector Machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.I.
Dagan, O. Glickman, and B. Magnini.
2006.
The PASCALrecognising textual entailment challenge.
In Quin?onero-Candela et al, editor, MLCW 2005, LNAI, volume 3904,pages 177?190.
Springer-Verlag.S.M.
Harabagiu, S.J.
Maiorano, and M.A.
Pasca.
2003.
Open-domain textual question answering techniques.
Natural Lan-guage Engineering, 9(3):231?267.M.A.
Jaro.
1995.
Probabilistic linkage of large public healthdata file.
Statistics in Medicine, 14:491?498.Z.
Kozareva and A. Montoyo.
2006.
MLENT: The machinelearning entailment system of the University of Alicante.
InProc.
of 2nd PASCAL Challenges Workshop on RecognisingTextual Entailment, Venice, Italy.E.
Newman, J. Dunnion, and J. Carthy.
2006.
Constructing adecision tree classifier using lexical and syntactic features.In Proc.
of 2nd PASCAL Challenges Workshop on Recognis-ing Textual Entailment, Venice, Italy.H.T.
Ng, J.L.P.
Kwan, and Y. Xia.
2001.
Question answeringusing a large text database: A machine learning approach.
InProc.
of Empirical Methods in Natural Language Process-ing, Carnegie Mellon Univ., PA.D.R.
Radev, J. Prager, and V. Samn.
2000.
Ranking suspectedanswers to natural language questions using predictive an-notation.
In Proc.
of NAACL-ANLP, pages 150?157, Seattle,WA.V.
Vapnik.
1998.
Statistical learning theory.
John Wiley.W.E.
Winkler.
1999.
The state of record linkage and currentresearch problems.
Statistical Research Report RR99/04, USBureau of the Census, Washington, DC.47
