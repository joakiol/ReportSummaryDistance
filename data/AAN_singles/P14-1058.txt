Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613?623,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning to Predict Distributions of Words Across DomainsDanushka BollegalaDepartment of Computer ScienceUniversity of LiverpoolLiverpool,L69 3BX, UKdanushka.bollegala@liverpool.ac.ukDavid WeirDepartment of InformaticsUniversity of SussexFalmer, Brighton,BN1 9QJ, UKd.j.weir@sussex.ac.ukJohn CarrollDepartment of InformaticsUniversity of SussexFalmer, Brighton,BN1 9QJ, UKj.a.carroll@sussex.ac.ukAbstractAlthough the distributional hypothesis hasbeen applied successfully in many naturallanguage processing tasks, systems usingdistributional information have been lim-ited to a single domain because the dis-tribution of a word can vary between do-mains as the word?s predominant mean-ing changes.
However, if it were pos-sible to predict how the distribution ofa word changes from one domain to an-other, the predictions could be used toadapt a system trained in one domain towork in another.
We propose an unsuper-vised method to predict the distribution ofa word in one domain, given its distribu-tion in another domain.
We evaluate ourmethod on two tasks: cross-domain part-of-speech tagging and cross-domain sen-timent classification.
In both tasks, ourmethod significantly outperforms compet-itive baselines and returns results that arestatistically comparable to current state-of-the-art methods, while requiring notask-specific customisations.1 IntroductionThe Distributional Hypothesis, summarised by thememorable line of Firth (1957) ?
You shall knowa word by the company it keeps ?
has inspired adiverse range of research in natural language pro-cessing.
In such work, a word is represented bythe distribution of other words that co-occur withit.
Distributional representations of words havebeen successfully used in many language process-ing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunk-ing (Huang and Yates, 2009), ontology learning(Curran, 2005), computing semantic textual sim-ilarity (Besanc?on et al, 1999), and lexical infer-ence (Kotlerman et al, 2012).However, the distribution of a word often variesfrom one domain1to another.
For example, inthe domain of portable computer reviews the wordlightweight is often associated with positive sen-timent bearing words such as sleek or compact,whereas in the movie review domain the sameword is often associated with negative sentiment-bearing words such as superficial or formulaic.Consequently, the distributional representations ofthe word lightweight will differ considerably be-tween the two domains.
In this paper, given thedistribution wSof a word w in the source domainS, we propose an unsupervised method for pre-dicting its distributionwTin a different target do-main T .The ability to predict how the distribution of aword varies from one domain to another is vitalfor numerous adaptation tasks.
For example, un-supervised cross-domain sentiment classification(Blitzer et al, 2007; Aue and Gamon, 2005) in-volves using sentiment-labeled user reviews fromthe source domain, and unlabeled reviews fromboth the source and the target domains to learna sentiment classifier for the target domain.
Do-main adaptation (DA) of sentiment classificationbecomes extremely challenging when the distribu-tions of words in the source and the target domainsare very different, because the features learnt fromthe source domain labeled reviews might not ap-pear in the target domain reviews that must beclassified.
By predicting the distribution of a wordacross different domains, we can find source do-main features that are similar to the features intarget domain reviews, thereby reducing the mis-match of features between the two domains.We propose a two-step unsupervised approachto predict the distribution of a word across do-mains.
First, we create two lower dimensional la-1In this paper, we use the term domain to refer to a col-lection of documents about a particular topic, for examplereviews of a particular kind of product.613tent feature spaces separately for the source andthe target domains using Singular Value Decom-position (SVD).
Second, we learn a mapping fromthe source domain latent feature space to the tar-get domain latent feature space using Partial LeastSquare Regression (PLSR).
The SVD smoothingin the first step both reduces the data sparseness indistributional representations of individual words,as well as the dimensionality of the feature space,thereby enabling us to efficiently and accuratelylearn a prediction model using PLSR in the sec-ond step.
Our proposed cross-domain word dis-tribution prediction method is unsupervised in thesense that it does not require any labeled data ineither of the two steps.Using two popular multi-domain datasets, weevaluate the proposed method in two predictiontasks: (a) predicting the POS of a word in a tar-get domain, and (b) predicting the sentiment of areview in a target domain.
Without requiring anytask specific customisations, systems based on ourdistribution prediction method significantly out-perform competitive baselines in both tasks.
Be-cause our proposed distribution prediction methodis unsupervised and task independent, it is poten-tially useful for a wide range of DA tasks such en-tity extraction (Guo et al, 2009) or dependencyparsing (McClosky et al, 2010).
Our contribu-tions are summarised as follows:?
Given the distribution wSof a word w in asource domain S, we propose a method forlearning its distribution wTin a target do-main T .?
Using the learnt distribution predictionmodel, we propose a method to learn a cross-domain POS tagger.?
Using the learnt distribution predictionmodel, we propose a method to learn a cross-domain sentiment classifier.To our knowledge, ours is the first successful at-tempt to learn a model that predicts the distribu-tion of a word across different domains.2 Related WorkLearning semantic representations for words us-ing documents from a single domain has receivedmuch attention lately (Vincent et al, 2010; Socheret al, 2013; Baroni and Lenci, 2010).
As we havealready discussed, the semantics of a word variesacross different domains, and such variations arenot captured by models that only learn a single se-mantic representation for a word using documentsfrom a single domain.The POS of a word is influenced both by itscontext (contextual bias), and the domain of thedocument in which it appears (lexical bias).
Forexample, the word signal is predominately usedas a noun in MEDLINE, whereas it appears pre-dominantly as an adjective in the Wall Street Jour-nal (WSJ) (Blitzer et al, 2006).
Consequently, atagger trained on WSJ would incorrectly tag sig-nal in MEDLINE.
Blitzer et al (2006) appendthe source domain labeled data with predicted piv-ots (i.e.
words that appear in both the source andtarget domains) to adapt a POS tagger to a tar-get domain.
Choi and Palmer (2012) proposea cross-domain POS tagging method by trainingtwo separate models: a generalised model and adomain-specific model.
At tagging time, a sen-tence is tagged by the model that is most similarto that sentence.
Huang and Yates (2009) train aConditional Random Field (CRF) tagger with fea-tures retrieved from a smoothing model trained us-ing both source and target domain unlabeled data.Adding latent states to the smoothing model fur-ther improves the POS tagging accuracy (Huangand Yates, 2012).
Schnabel and Sch?utze (2013)propose a training set filtering method where theyeliminate shorter words from the training databased on the intuition that longer words are morelikely to be examples of productive linguistic pro-cesses than shorter words.The sentiment of a word can vary from one do-main to another.
In Structural CorrespondenceLearning (SCL) (Blitzer et al, 2006; Blitzer etal., 2007), a set of pivots are chosen using point-wise mutual information.
Linear predictors arethen learnt to predict the occurrence of those piv-ots, and SVD is used to construct a lower dimen-sional representation in which a binary classifieris trained.
Spectral Feature Alignment (SFA) (Panet al, 2010) also uses pivots to compute an align-ment between domain specific and domain inde-pendent features.
Spectral clustering is performedon a bipartite graph representing domain specificand domain independent features to find a lower-dimensional projection between the two sets offeatures.
The cross-domain sentiment-sensitivethesaurus (SST) (Bollegala et al, 2011) groupstogether words that express similar sentiments in614different domains.
The created thesaurus is used toexpand feature vectors during train and test stagesin a binary classifier.
However, unlike our method,SCL, SFA, or SST do not learn a prediction modelbetween word distributions across domains.Prior knowledge of the sentiment of words, suchas sentiment lexicons, has been incorporated intocross-domain sentiment classification.
He et al(2011) propose a joint sentiment-topic model thatimposes a sentiment-prior depending on the oc-currence of a word in a sentiment lexicon.
Pono-mareva and Thelwall (2012) represent source andtarget domain reviews as nodes in a graph and ap-ply a label propagation algorithm to predict thesentiment labels for target domain reviews fromthe sentiment labels in source domain reviews.
Asentiment lexicon is used to create features for adocument.
Although incorporation of prior senti-ment knowledge is a promising technique to im-prove accuracy in cross-domain sentiment classi-fication, it is complementary to our task of distri-bution prediction across domains.The unsupervised DA setting that we considerdoes not assume the availability of labeled data forthe target domain.
However, if a small amount oflabeled data is available for the target domain, itcan be used to further improve the performance ofDA tasks (Xiao et al, 2013; Daum?e III, 2007).3 Distribution Prediction3.1 In-domain Feature Vector ConstructionBefore we tackle the problem of learning a modelto predict the distribution of a word across do-mains, we must first compute the distribution ofa word from a single domain.
For this purpose, werepresent a word w using unigrams and bigramsthat co-occur with w in a sentence as follows.Given a document H, such as a user-review ofa product, we split H into sentences, and lemma-tize each word in a sentence using the RASP sys-tem (Briscoe et al, 2006).
Using a standard stopword list, we filter out frequent non-content un-igrams and select the remainder as unigram fea-tures to represent a sentence.
Next, we generatebigrams of word lemmas and remove any bigramsthat consists only of stop words.
Bigram featurescapture negations more accurately than unigrams,and have been found to be useful for sentimentclassification tasks.
Table 1 shows the unigramand bigram features we extract for a sentence us-ing this procedure.
Using data from a single do-sentence This is an interesting and well researched bookunigrams this, is, an, interesting, and, well, researched,(surface) bookunigrams this, be, an, interest, and, well, research, book(lemma)unigrams interest, well, research, book(features)bigrams this+be, be+an, an+interest, interest+and,(lemma) and+well, well+research, research+bookbigrams an+interest, interest+and, and+well,(features) well+research, research+bookTable 1: Extracting unigram and bigram features.main, we construct a feature co-occurrence ma-trix A in which columns correspond to unigramfeatures and rows correspond to either unigram orbigram features.
The value of the element aijinthe co-occurrence matrix A is set to the number ofsentences in which the i-th and j-th features co-occur.Typically, the number of unique bigrams ismuch larger than that of unigrams.
Moreover, co-occurrences of bigrams are rare compared to co-occurrences of unigrams, and co-occurrences in-volving a unigram and a bigram.
Consequently,in matrix A, we consider co-occurrences only be-tween unigrams vs. unigrams, and bigrams vs.unigrams.
We consider each row in A as repre-senting the distribution of a feature (i.e.
unigramsor bigrams) in a particular domain over the uni-gram features extracted from that domain (repre-sented by the columns of A).
We apply PositivePointwise Mutual Information (PPMI) to the co-occurrence matrix A.
This is a variation of thePointwise Mutual Information (PMI) (Church andHanks, 1990), in which all PMI values that are lessthan zero are replaced with zero (Lin, 1998; Bul-linaria and Levy, 2007).
Let F be the matrix thatresults when PPMI is applied to A. Matrix F hasthe same number of rows, nr, and columns, nc, asthe raw co-occurrence matrix A.Note that in addition to the above-mentionedrepresentation, there are many other ways to rep-resent the distribution of a word in a particular do-main (Turney and Pantel, 2010).
For example,one can limit the definition of co-occurrence towords that are linked by some dependency relation(Pado and Lapata, 2007), or extend the windowof co-occurrence to the entire document (Baroniand Lenci, 2010).
Since the method we proposein Section 3.2 to predict the distribution of a wordacross domains does not depend on the particular615feature representation method, any of these alter-native methods could be used.To reduce the dimensionality of the featurespace, and create dense representations for words,we perform SVD on F. We use the left singu-lar vectors corresponding to the k largest singularvalues to compute a rank k approximation?F, ofF.
We perform truncated SVD using SVDLIBC2.Each row in?F is considered as representing a wordin a lower k (nc) dimensional feature space cor-responding to a particular domain.
Distributionprediction in this lower dimensional feature spaceis preferrable to prediction over the original fea-ture space because there are reductions in overfit-ting, feature sparseness, and the learning time.
Wecreated two matrices,?FSand?FTfrom the sourceand target domains, respectively, using the abovementioned procedure.3.2 Cross-Domain Feature Vector PredictionWe propose a method to learn a model that canpredict the distribution wTof a word w in thetarget domain T , given its distribution wSinthe source domain S. We denote the set offeatures that occur in both domains by W ={w(1), .
.
.
, w(n)}.
In the literature, such featuresare often referred to as pivots, and they have beenshown to be useful for DA, allowing the weightslearnt to be transferred from one domain to an-other.
Various criteria have been proposed for se-lecting a small set of pivots for DA, such as themutual information of a word with the two do-mains (Blitzer et al, 2007).
However, we do notimpose any further restrictions on the set of pivotsW other than that they occur in both domains.For each word w(i)?
W , we denote the cor-responding rows in?FSand?FTby column vec-tors w(i)Sand w(i)T. Note that the dimensional-ity of w(i)Sand w(i)Tneed not be equal, and wemay select different numbers of singular vectorsto approximate?FSand?FT.
We model distribu-tion prediction as a multivariate regression prob-lem where, given a set {(w(i)S,w(i)T)}ni=1consist-ing of pairs of feature vectors selected from eachdomain for the pivots in W , we learn a mappingfrom the inputs (w(i)S) to the outputs (w(i)T).We use Partial Least Squares Regression(PLSR) (Wold, 1985) to learn a regression modelusing pairs of vectors.
PLSR has been applied in2http://tedlab.mit.edu/?dr/SVDLIBC/Algorithm 1 Learning a prediction model.Input: X, Y, L.Output: Prediction matrix M.1: Randomly select ?lfrom columns in Yl.2: vl= Xl>?l/????Xl>?l???
?3: ?l= Xlvl4: ql= Yl>?l/????Yl>?l???
?5: ?l= Ylql6: If ?lis unchanged go to Line 7; otherwise go to Line 27: cl= ?l>?l/?????l>?l???
?8: pl= Xl>?l/?l>?l9: Xl+1= Xl?
?lpl>and Yl+1= Yl?
cl?lql>.10: Stop if l = L; otherwise l = l + 1 and return to Line 1.11: Let C = diag(c1, .
.
.
, cL), and V = [v1.
.
.vL]12: M = V(P>V)?1CQ>13: return MChemometrics (Geladi and Kowalski, 1986), pro-ducing stable prediction models even when thenumber of samples is considerably smaller thanthe dimensionality of the feature space.
In particu-lar, PLSR fits a smaller number of latent variables(10?
100 in practice) such that the correlation be-tween the feature vectors for pivots in the two do-mains are maximised in this latent space.Let X and Y denote matrices formed by ar-ranging respectively the vectors w(i)Ss and w(i)Tinrows.
PLSR decomposes X and Y into a series ofproducts between rank 1 matrices as follows:X ?L?l=1?lpl>= ?P>(1)Y ?L?l=1?lql>= ?Q>.
(2)Here, ?l, ?l, pl, and qlare column vectors, andthe summation is taken over the rank 1 matricesthat result from the outer product of those vectors.The matrices, ?, ?, P, and Q are constructed re-spectively by arranging ?l, ?l, pl, and qlvectorsas columns.Our method for learning a distribution predic-tion model is shown in Algorithm 1.
It is based onthe two block NIPALS routine (Wold, 1975; Rosi-pal and Kramer, 2006) and iteratively discovers Lpairs of vectors (?l,?l) such that the covariances,Cov(?l,?l), are maximised under the constraint||pl|| = ||ql|| = 1.
Finally, the prediction matrix,M is computed using ?l,?l,pl, ql.
The predicteddistribution?wTof a word w in T is given by?wT= MwS.
(3)616Our distribution prediction learning method is un-supervised in the sense that it does not requiremanually labeled data for a particular task fromany of the domains.
This is an important point,and means that the distribution prediction methodis independent of the task to which it may subse-quently be applied.
As we go on to show in Sec-tion 6, this enables us to use the same distributionprediction method for both POS tagging and sen-timent classification.4 Domain AdaptationThe main reason that a model trained only on thesource domain labeled data performs poorly inthe target domain is the feature mismatch ?
fewfeatures in target domain test instances appear insource domain training instances.
To overcomethis problem, we use the proposed distribution pre-diction method to find those related features in thesource domain that correspond to the features ap-pearing in the target domain test instances.We consider two DA tasks: (a) cross-domainPOS tagging (Section 4.1), and (b) cross-domainsentiment classification (Section 4.2).
Note thatour proposed distribution prediction method canbe applied to numerous other NLP tasks that in-volve sequence labelling and document classifica-tion.4.1 Cross-Domain POS TaggingWe represent each word using a set of featuressuch as capitalisation (whether the first letter of theword is capitalised), numeric (whether the wordcontains digits), prefixes up to four letters, andsuffixes up to four letters (Miller et al, 2011).Next, for each word w in a source domain labeled(i.e.
manually POS tagged) sentence, we select itsneighbours u(i)in the source domain as additionalfeatures.
Specifically, we measure the similarity,sim(u(i)S,wS), between the source domain distri-butions of u(i)and w, and select the top r simi-lar neighbours u(i)for each word w as additionalfeatures for w. We refer to such features as dis-tributional features in this work.
The value of aneighbour u(i)selected as a distributional featureis set to its similarity score sim(u(i)S,wS).
Next,we train a CRF model using all features (i.e.
cap-italisation, numeric, prefixes, suffixes, and distri-butional features) on source domain labeled sen-tences.We train a PLSR model, M, that predicts thetarget domain distribution Mu(i)Sof a word u(i)inthe source domain labeled sentences, given its dis-tribution, u(i)S. At test time, for each word w thatappears in a target domain test sentence, we mea-sure the similarity, sim(Mu(i)S,wT), and selectthe most similar r words u(i)in the source domainlabeled sentences as the distributional features forw, with their values set to sim(Mu(i)S,wT).
Fi-nally, the trained CRF model is applied to a targetdomain test sentence.Note that distributional features are always se-lected from the source domain during both trainand test times, thereby increasing the number ofoverlapping features between the trained modeland test sentences.
To make the inference tractableand efficient, we use a first-order Markov factori-sation, in which we consider all pairwise combi-nations between the features for the current wordand its immediate predecessor.4.2 Cross-Domain Sentiment ClassificationUnlike in POS tagging, where we must individ-ually tag each word in a target domain test sen-tence, in sentiment classification we must classifythe sentiment for the entire review.
We modify theDA method presented in Section 4.1 to satisfy thisrequirement as follows.Let us assume that we are given a set{(x(i)S, y(i))}ni=1of n labeled reviews x(i)Sfor thesource domain S. For simplicity, let us considerbinary sentiment classification where each reviewx(i)is labeled either as positive (i.e.
y(i)= 1) ornegative (i.e.
y(i)= ?1).
Our cross-domain bi-nary sentiment classification method can be easilyextended to the multi-class setting as well.
First,we lemmatise each word in a source domain la-beled review x(i)S, and extract both unigrams andbigrams as features to represent x(i)Sby a binary-valued feature vector.
Next, we train a binary clas-sification model, ?, using those feature vectors.Any binary classification algorithm can be usedto learn ?.
In our experiments, we used L2 reg-ularised logistic regression.Next, we train a PLSR model, M, as describedin Section 3.2 using unlabeled reviews in thesource and target domains.
At test time, we rep-resent a test target review H using a binary-valuedfeature vector h of unigrams and bigrams of lem-mas of the words in H, as we did for source do-main labeled train reviews.
Next, for each featurew(j)extracted from H, we measure the similarity,617sim(Mu(i)S,w(j)T), between the target domain dis-tribution of w(j), and each feature (unigram or bi-gram) u(i)in the source domain labeled reviews.We score each source domain feature u(i)for itsrelatedness to H using the formula:score(u(i),H) =1|H||H|?j=1sim(Mu(i)S,w(j)T) (4)where |H| denotes the total number of features ex-tracted from the test review H. We select the topscoring r features u(i)as distributional features forH, and append those to h. The corresponding val-ues of those distributional features are set to thescores given by Equation 4.
Finally, we classifyh using the trained binary classifier ?.
Note thatgiven a test review, we find the distributional fea-tures that are similar to all the words in the test re-view from the source domain.
In particular, we donot find distributional features independently foreach word in the test review.
This enables us tofind distributional features that are consistent withall the features in a test review.4.3 Model ChoicesFor both POS tagging and sentiment classifica-tion, we experimented with several alternativeapproaches for feature weighting, representation,and similarity measures using development data,which we randomly selected from the training in-stances from the datasets described in Section 5.For feature weighting for sentiment classifica-tion, we considered using the number of occur-rences of a feature in a review and tf-idf weight-ing (Salton and Buckley, 1983).
For representa-tion, we considered distributional features u(i)indescending order of their scores given by Equa-tion 4, and then taking the inverse-rank as the val-ues for the distributional features (Bollegala et al,2011).
However, none of these alternatives re-sulted in performance gains.
With respect to simi-larity measures, we experimented with cosine sim-ilarity and the similarity measure proposed by Lin(1998); cosine similarity performed consistentlywell over all the experimental settings.
The featurerepresentation was held fixed during these similar-ity measure comparisons.For POS tagging, we measured the effect ofvarying r, the number of distributional features,using a development dataset.
We observed thatsetting r larger than 10 did not result in signifi-cant improvements in tagging accuracy, but onlyincreased the train time due to the larger featurespace.
Consequently, we set r = 10 in POS tag-ging.
For sentiment analysis, we used all featuresin the source domain labeled reviews as distri-butional features, weighted by their scores givenby Equation 4, taking the inverse-rank.
In bothtasks, we parallelised similarity computations us-ing BLAS3level-3 routines to speed up the com-putations.
The source code of our implementationis publicly available4.5 DatasetsTo evaluate DA for POS tagging, following Blitzeret al (2006), we use sections 2 ?
21 from WallStreet Journal (WSJ) as the source domain labeleddata.
An additional 100, 000 WSJ sentences fromthe 1988 release of the WSJ corpus are used as thesource domain unlabeled data.
Following Schn-abel and Sch?utze (2013), we use the POS labeledsentences in the SACNL dataset (Petrov and Mc-Donald, 2012) for the five target domains: QA fo-rums, Emails, Newsgroups, Reviews, and Blogs.Each target domain contains around 1000 POSlabeled test sentences and around 100, 000 unla-beled sentences.To evaluate DA for sentiment classification,we use the Amazon product reviews collected byBlitzer et al (2007) for four different product cat-egories: books (B), DVDs (D), electronic items(E), and kitchen appliances (K).
There are 1000positive and 1000 negative sentiment labeled re-views for each domain.
Moreover, each domainhas on average 17, 547 unlabeled reviews.
We usethe standard split of 800 positive and 800 negativelabeled reviews from each domain as training data,and the remainder for testing.6 Experiments and ResultsFor each domain D in the SANCL (POS tag-ging) and Amazon review (sentiment classifica-tion) datasets, we create a PPMI weighted co-occurrence matrix FD.
On average, FDcreatedfor a target domain in the SANCL dataset con-tains 104, 598 rows and 65, 528 columns, whereasthose numbers in the Amazon dataset are 27, 397and 35, 200 respectively.
In cross-domain senti-ment classification, we measure the binary senti-ment classification accuracy for the target domain3http://www.openblas.net/4http://www.csc.liv.ac.uk/?danushka/software.html618test reviews for each pair of domains (12 pairs intotal for 4 domains).
On average, we have 40, 176pivots for a pair of domains in the Amazon dataset.In cross-domain POS tagging, WSJ is alwaysthe source domain, whereas the five domains inSANCL dataset are considered as the target do-mains.
For this setting we have 9822 pivots onaverage.
The number of singular vectors k se-lected in SVD, and the number of PLSR dimen-sions L are set respectively to 1000 and 50 for theremainder of the experiments described in the pa-per.
Later we study the effect of those two param-eters on the performance of the proposed method.The L-BFGS (Liu and Nocedal, 1989) method isused to train the CRF and logistic regression mod-els.6.1 POS Tagging ResultsTable 2 shows the token-level POS tagging accu-racy for unseen words (i.e.
words that appear in thetarget domain test sentences but not in the sourcedomain labeled train sentences).
By limiting theevaluation to unseen words instead of all words,we can evaluate the gain in POS tagging accuracysolely due to DA.
The NA (no-adapt) baseline sim-ulates the effect of not performing any DA.
Specif-ically, in POS tagging, a CRF trained on sourcedomain labeled sentences is applied to target do-main test sentences, whereas in sentiment classi-fication, a logistic regression classifier trained us-ing source domain labeled reviews is applied to thetarget domain test reviews.
The Spredbaseline di-rectly uses the source domain distributions for thewords instead of projecting them to the target do-main.
This is equivalent to setting the predictionmatrix M to the unit matrix.
The Tpredbaselineuses the target domain distribution wTfor a wordw instead of MwS.
If w does not appear in thetarget domain, then wTis set to the zero vector.The Spredand Tpredbaselines simulate the two al-ternatives of using source and target domain dis-tributions instead of learning a PLSR model.
TheDA method proposed in Section 4.1 is shown asthe Proposed method.
Filter denotes the train-ing set filtering method proposed by Schnabel andSch?utze (2013) for the DA of POS taggers.From Table 2, we see that the Proposed methodachieves the best performance in all five domains,followed by the Tpredbaseline.
Recall that theTpredbaseline cannot find source domain wordsthat do not appear in the target domain as distri-Target NA SpredTpredFilter ProposedQA 67.34 68.18 68.75 57.08 69.28?Emails 65.62 66.62 67.07 65.61 67.09Newsgroups 75.71 75.09 75.57 70.37 75.85?Reviews 56.36 54.60 56.68 47.91 56.93?Blogs 76.64 54.78 76.90 74.56 76.97?Table 2: POS tagging accuracies on SANCL.butional features for the words in the target do-main test reviews.
Therefore, when the overlap be-tween the vocabularies used in the source and thetarget domains is small, Tpredcannot reduce themismatch between the feature spaces.
Poor perfor-mance of the Spredbaseline shows that the distri-butions of a word in the source and target domainsare different to the extent that the distributionalfeatures found using source domain distributionsare inadequate.
The two baselines Spredand Tpredcollectively motivate our proposal to learn a distri-bution prediction model from the source domainto the target.
The improvements of Proposed overthe previously proposed Filter are statistically sig-nificant in all domains except the Emails domain(denoted by ?
in Table 2 according to the Bino-mial exact test at 95% confidence).
However, thedifferences between the Tpredand Proposed meth-ods are not statistically significant.6.2 Sentiment Classification ResultsIn Figure 1, we compare the Proposed cross-domain sentiment classification method (Section4.2) against several baselines and the current state-of-the-art methods.
The baselines NA, Spred, andTpredare defined similarly as in Section 6.1.
SSTis the Sentiment Sensitive Thesaurus proposed byBollegala et al (2011).
SST creates a single distri-bution for a word using both source and target do-main reviews, instead of two separate distributionsas done by the Proposed method.
SCL denotesthe Structural Correspondence Learning methodproposed by Blitzer et al (2006).
SFA denotesthe Spectral Feature Alignment method proposedby Pan et al (2010).
SFA and SCL represent thecurrent state-of-the-art methods for cross-domainsentiment classification.
All methods are evalu-ated under the same settings, including train/testsplit, feature spaces, pivots, and classification al-gorithms so that any differences in performancecan be directly attributable to their domain adapt-ability.
For each domain, the accuracy obtainedby a classifier trained using labeled data from that619E?>B D?>B K?>B5055606570758085AccuracyB?>E D?>E K?>E5060708090AccuracyB?>D E?>D K?>D5055606570758085AccuracyNA SFA SST SCL Spred Tpred ProposedB?>K E?>K D?>K5060708090AccuracyFigure 1: Cross-Domain sentiment classification.domain is indicated by a solid horizontal line ineach sub-figure.
This upper baseline representsthe classification accuracy we could hope to obtainif we were to have labeled data for the target do-main.
Clopper-Pearson 95% binomial confidenceintervals are superimposed on each vertical bar.From Figure 1 we see that the Proposed methodreports the best results in 8 out of the 12 domainpairs, whereas SCL, SFA, and Spredreport thebest results in other cases.
Except for the D-E set-ting in which Proposed method significantly out-performs both SFA and SCL, the performance ofthe Proposed method is not statistically signifi-cantly different to that of SFA or SCL.The selection of pivots is vital to the perfor-mance of SFA.
However, unlike SFA, which re-quires us to carefully select a small subset of pivots(ca.
less than 500) using some heuristic approach,our Proposed method does not require any pivotselection.
Moreover, SFA projects source domainreviews to a lower-dimensional latent space, inwhich a binary sentiment classifier is subsequentlytrained.
At test time SFA projects a target reviewinto this lower-dimensional latent space and ap-plies the trained classifier.
In contrast, our Pro-posed method predicts the distribution of a wordin the target domain, given its distribution in thesource domain, thereby explicitly translating thesource domain reviews to the target.
This propertyenables us to apply the proposed distribution pre-diction method to tasks other than sentiment anal-ysis such as POS tagging where we must identifydistributional features for individual words.10 100 200 300 400 500 600 700 8000.640.660.680.70.720.740.760.78PLSR dimensionsAccuracyE??>BD?
?>BFigure 2: The effect of PLSR dimensions.Unlike our distribution prediction method,which is unsupervised, SST requires labeled datafor the source domain to learn a feature mappingbetween a source and a target domain in the formof a thesaurus.
However, from Figure 1 we seethat in 10 out of the 12 domain-pairs the Proposedmethod returns higher accuracies than SST.To evaluate the overall effect of the number ofsingular vectors k used in the SVD step, and thenumber of PLSR components L used in Algorithm1, we conduct two experiments.
To evaluate the ef-fect of the PLSR dimensions, we fixed k = 1000and measured the cross-domain sentiment classi-fication accuracy over a range of L values.
Asshown in Figure 2, accuracy remains stable acrossa wide range of PLSR dimensions.
Because thetime complexity of Algorithm 1 increases linearlywith L, it is desirable that we select smaller L val-6201000 1500 2000 2500 30000.580.60.620.640.660.680.70.720.740.76SVD dimensionsAccuracyE??>BD?
?>BFigure 3: The effect of SVD dimensions.Measure Distributional featuressim(uS, wS) thin (0.1733), digestible (0.1728),small+print (0.1722)sim(uT, wT) travel+companion (0.6018), snap-in(0.6010), touchpad (0.6016)sim(uS, wT) segregation (0.1538), participation(0.1512), depression+era (0.1508)sim(MuS, wT) small (0.2794), compact (0.2641),sturdy (0.2561)Table 3: Top 3 distributional features u ?
S forthe word lightweight (w).ues in practice.To evaluate the effect of the SVD dimensions,we fixed L = 100 and measured the cross-domainsentiment classification accuracy for different kvalues as shown in Figure 3.
We see an overalldecrease in classification accuracy when k is in-creased.
Because the dimensionality of the sourceand target domain feature spaces is equal to k, thecomplexity of the least square regression problemincreases with k. Therefore, larger k values resultin overfitting to the train data and classification ac-curacy is reduced on the target test data.As an example of the distribution predictionmethod, in Table 3 we show the top 3 similardistributional features u in the books (source) do-main, predicted for the electronics (target) domainword w = lightweight, by different similaritymeasures.
Bigrams are indicted by a + sign andthe similarity scores of the distributional featuresare shown within brackets.Using the source domain distributions for bothu and w (i.e.
sim(uS, wS)) produces distribu-tional features that are specific to the books do-main, or to the dominant adjectival sense of hav-ing no importance or influence.
On the otherhand, using target domain distributions for u andw (i.e.
sim(uT, wT)) returns distributional fea-tures of the dominant nominal sense of lower inweight frequently associated with electronic de-vices.
Simply using source domain distributionsuS(i.e.
sim(uS, wT)) returns totally unrelated dis-tributional features.
This shows that word distribu-tions in source and target domains are very differ-ent and some adaptation is required prior to com-puting distributional features.Interestingly, we see that by using the dis-tributions predicted by the proposed method(i.e.
sim(MuS, wT)) we overcome this problemand find relevant distributional features from thesource domain.
Although for illustrative purposeswe used the word lightweight, which occurs inboth the source and the target domains, our pro-posed method does not require the source domaindistribution wSfor a word w in a target domaindocument.
Therefore, it can find distributional fea-tures even for words occurring only in the targetdomain, thereby reducing the feature mismatchbetween the two domains.7 ConclusionWe proposed a method to predict the distributionof a word across domains.
We first create a distri-butional representation for a word using the datafrom a single domain, and then learn a PartialLeast Square Regression (PLSR) model to pre-dict the distribution of a word in a target domaingiven its distribution in a source domain.
We eval-uated the proposed method in two domain adapta-tion tasks: cross-domain POS tagging and cross-domain sentiment classification.
Our experimentsshow that without requiring any task-specific cus-tomisations to our distribution prediction method,it outperforms competitive baselines and achievescomparable results to the current state-of-the-artdomain adaptation methods.ReferencesAnthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: a casestudy.
Technical report, Microsoft Research.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673 ?
721.Romaric Besanc?on, Martin Rajman, and Jean-C?edricChappelier.
1999.
Textual similarities based on a621distributional approach.
In Proc.
of DEXA, pages180 ?
184.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proc.
of EMNLP, pages 120 ?128.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proc.
of ACL, pages 440 ?
447.Danushka Bollegala, David Weir, and John Carroll.2011.
Using multiple sources to construct a senti-ment sensitive thesaurus for cross-domain sentimentclassification.
In Proc.
of ACL/HLT, pages 132 ?141.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proc.
ofCOLING/ACL Interactive Presentation Sessions.John A. Bullinaria and Jospeh P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39(3):510 ?
526.Jinho D. Choi and Martha Palmer.
2012.
Fast androbust part-of-speech tagging using dynamic modelselection.
In Proc.
of ACL Short Papers, volume 2,pages 363 ?
367.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22 ?
29,March.James Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics, pages 26 ?
33.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proc.
of ACL, pages 256 ?
263.John R. Firth.
1957.
A synopsis of linguistic theory1930-55.
Studies in Linguistic Analysis, pages 1 ?32.Paul Geladi and Bruce R. Kowalski.
1986.
Partialleast-squares regression: a tutorial.
Analytica Chim-ica Acta, 185(0):1 ?
17.Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,Xian Wu, and Zhong Su.
2009.
Domain adapta-tion with latent semantic association for named en-tity recognition.
In Proc.
of NAACL, pages 281 ?289.Yulan He, Chenghua Lin, and Harith Alani.
2011.Automatically extracting polarity-bearing topics forcross-domain sentiment classification.
In Proc.
ofACL/HLT, pages 123 ?
131.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence-labeling.
In ACL-IJCNLP?09, pages 495?
503.Fei Huang and Alexander Yates.
2012.
Biased repre-sentation learning for domain adaptation.
In Proc.of EMNLP/CoNLL, pages 1313 ?
1323.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2012.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359 ?
389.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
of ACL, pages 768 ?
774.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503 ?
528.David McClosky, Eugene Charniak, and Mark John-son.
2010.
Automatic domain adaptation for pars-ing.
In Proc.
of NAACL/HLT, pages 28 ?
36.John E. Miller, Manabu Torii, and K. Vijay-Shanker.2011.
Building domain-specific taggers without an-notated (domain) data.
In Proc.
of EMNLP/CoNLL,pages 1103 ?
1111.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161 ?199.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In Proc.
of WWW, pages 751 ?
760.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProc.
of EMNLP, pages 938 ?
947.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Notes ofthe 1st SANCL Workshop.Natalia Ponomareva and Mike Thelwall.
2012.
Doneighbours help?
an exploration of graph-based al-gorithms for cross-domain sentiment classification.In Proc.
of EMNLP, pages 655 ?
665.Roman Rosipal and Nicole Kramer.
2006.
Overviewand recent advances in partial least squares.
InC. Saunders et al, editor, SLSFS?05, volume 3940 ofLNCS, pages 34 ?
51, Berlin Heidelberg.
Springer-Verlag.G.
Salton and C. Buckley.
1983.
Introduction toModern Information Retreival.
McGraw-Hill BookCompany.Tobias Schnabel and Hinrich Sch?utze.
2013.
Towardsrobust cross-domain domain adaptation for part-of-speech tagging.
In Proc.
of IJCNLP, pages 198 ?206.622Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Proc.
of EMNLP, pages 1631 ?
1642.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
Journal of Aritificial Intelligence Research,37:141 ?
188.Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,Yoshua Bengio, and Pierre-Antonie Manzagol.2010.
Stacked denoising autoencoders: Learninguseful representations in a deep network with a localdenoising criterion.
Journal of Machine LearningResearch, 11:3371 ?
3408.Herman Wold.
1975.
Path models with latent vari-ables: the NIPALS approach.
In H. M. Blalocket al, editor, Quantitative socialogy: internationalperspective on mathematical and statistical model-ing, pages 307 ?
357.
Academic.Herman Wold.
1985.
Partial least squares.
In SamelKotz and Norman L. Johnson, editors, Encyclopediaof the Statistical Sciences, pages 581 ?
591.
Wiley.Min Xiao, Feipeng Zhao, and Yuhong Guo.
2013.Learning latent word representations for domainadaptation using supervised word clustering.
InProc.
of EMNLP, pages 152 ?
162.623
