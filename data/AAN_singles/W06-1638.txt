Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317?326,Sydney, July 2006. c?2006 Association for Computational LinguisticsBetter Informed Training of Latent Syntactic FeaturesMarkus Dreyer and Jason EisnerDepartment of Computer Science / Center for Language and Speech ProcessingJohns Hopkins University3400 North Charles Street, Baltimore, MD 21218 USA{markus,jason}@clsp.jhu.eduAbstractWe study unsupervised methods for learn-ing refinements of the nonterminals ina treebank.
Following Matsuzaki et al(2005) and Prescher (2005), we may forexample split NP without supervision intoNP[0] and NP[1], which behave differently.We first propose to learn a PCFG that addssuch features to nonterminals in such away that they respect patterns of linguis-tic feature passing: each node?s nontermi-nal features are either identical to, or inde-pendent of, those of its parent.
This lin-guistic constraint reduces runtime and thenumber of parameters to be learned.
How-ever, it did not yield improvements whentraining on the Penn Treebank.
An orthog-onal strategy was more successful: to im-prove the performance of the EM learnerby treebank preprocessing and by anneal-ing methods that split nonterminals selec-tively.
Using these methods, we can main-tain high parsing accuracy while dramati-cally reducing the model size.1 IntroductionTreebanks never contain enough information; thusPCFGs estimated straightforwardly from the PennTreebank (Bies et al, 1995) work only moderatelywell (Charniak, 1996).
To address this problem,researchers have used heuristics to add more infor-mation.
Eisner (1996), Charniak (1997), Collins(1997), and many subsequent researchers1 anno-tated every node with lexical features passed upfrom its ?head child,?
in order to more precisely re-flect the node?s ?inside?
contents.
Charniak (1997)and Johnson (1998) annotated each node with itsparent and grandparent nonterminals, to more pre-cisely reflect its ?outside?
context.
Collins (1996)split the sentence label S into two versions, repre-senting sentences with and without subjects.
He1Not to mention earlier non-PCFG lexicalized statisticalparsers, notably Magerman (1995) for the Penn Treebank.also modified the treebank to contain different la-bels for standard and for base noun phrases.
Kleinand Manning (2003) identified nonterminals thatcould valuably be split into fine-grained ones us-ing hand-written linguistic rules.
Their unlexical-ized parser combined several such heuristics withrule markovization and reached a performancesimilar to early lexicalized parsers.In all these cases, choosing which nonterminalsto split, and how, was a matter of art.
Ideallysuch splits would be learned automatically fromthe given treebank itself.
This would be less costlyand more portable to treebanks for new domainsand languages.
One might also hope that the auto-matically learned splits would be more effective.Matsuzaki et al (2005) introduced a model forsuch learning: PCFG-LA.2 They used EM to in-duce fine-grained versions of a given treebank?snonterminals and rules.
We present models thatsimilarly learn to propagate fine-grained featuresthrough the tree, but only in certain linguisticallymotivated ways.
Our models therefore allocatea supply of free parameters differently, allow-ing more fine-grained nonterminals but less fine-grained control over the probabilities of rewritingthem.
We also present simple methods for decid-ing selectively (during training) which nontermi-nals to split and how.Section 2 describes previous work in findinghidden information in treebanks.
Section 3 de-scribes automatically induced feature grammars.We start by describing the PCFG-LA model, thenintroduce new models that use specific agreementpatterns to propagate features through the tree.Section 4 describes annealing-like procedures fortraining latent-annotation models.
Section 5 de-scribes the motivation and results of our experi-ments.
We finish by discussing future work andconclusions in sections 6?7.2Probabilistic context-free grammar with latent annota-tions.317Citation Observed data Hidden dataCollins (1997) Treebank tree with head child an-notated on each nonterminalNo hidden data.
Degenerate EMcase.Lari and Young (1990) Words Parse treePereira and Schabes (1992) Words and partial brackets Parse treeKlein and Manning (2001) Part-of-speech tags Parse treeChiang and Bikel (2002) Treebank tree Head child on each nonterminalMatsuzaki et al (2005) Treebank tree Integer feature on each nontermi-nalINHERIT model (this paper) Treebank tree and head childheuristicsInteger feature on each nontermi-nalTable 1: Observed and hidden data in PCFG grammar learning.2 Partially supervised EM learningThe parameters of a PCFG can be learned withor without supervision.
In the supervised case,the complete tree is observed, and the rewrite ruleprobabilities can be estimated directly from theobserved rule counts.
In the unsupervised case,only the words are observed, and the learningmethod must induce the whole structure abovethem.
(See Table 1.
)In the partially supervised case we will con-sider, some part of the tree is observed, andthe remaining information has to be induced.Pereira and Schabes (1992) estimate PCFG pa-rameters from partially bracketed sentences, usingthe inside-outside algorithm to induce the miss-ing brackets and the missing node labels.
Someauthors define a complete tree as one that speci-fies not only a label but also a ?head child?
foreach node.
Chiang and Bikel (2002) induces themissing head-child information; Prescher (2005)induces both the head-child information and thelatent annotations we will now discuss.3 Feature Grammars3.1 The PCFG-LA ModelStaying in the partially supervised paradigm, thePCFG-LA model described in Matsuzaki et al(2005) observe whole treebank trees, but learnan ?annotation?
on each nonterminal token?anunspecified and uninterpreted integer that distin-guishes otherwise identical nonterminals.
Just asCollins manually split the S nonterminal label intoS and SG for sentences with and without subjects,Matsuzaki et al (2005) split S into S[1], S[2], .
.
.
,S[L] where L is a predefined number?but they doit automatically and systematically, and not onlyfor S but for every nonterminal.
Their partiallysupervised learning procedure observes trees thatare fully bracketed and fully labeled, except forthe integer subscript used to annotate each node.After automatically inducing the annotations withEM, their resulting parser performs just as well asone learned from a treebank whose nonterminalswere manually refined through linguistic and erroranalysis (Klein and Manning, 2003).In Matsuzaki?s PCFG-LA model, rewrite rulestake the formX[?]
?
Y [?]
Z[?]
(1)in the binary case, andX[?]
?
w (2)in the lexical case.
The probability of a tree con-sisting of rules r1, r2, .
.
.
is given by the probabil-ity of its root symbol times the conditional prob-abilities of the rules.
The annotated tree T1 inFig.
1, for example, has the following probability:P (T1) = P (ROOT ?
S[2])?P (S[2] ?
NP[1] VP[3])?P (NP[1] ??
He)?P (VP[3] ??
loves cookies)where, to simplify the notation, we useP (X ?
Y Z) to denote the conditional probabil-ity P (Y Z | X) that a given node with label Xwill have children Y Z.Degrees of freedom.
We will want to comparemodels that have about the same size.
Models withmore free parameters have an inherent advantageon modeling copious data because of their greater318Figure 1: Treebank tree with annotations.expressiveness.
Models with fewer free parame-ters are easier to train accurately on sparse data,as well as being more efficient in space and oftenin time.
Our question is therefore what can be ac-complished with a given number of parameters.How many free parameters in a PCFG-LAmodel?
Such a model is created by annotatingthe nonterminals of a standard PCFG (extractedfrom the given treebank) with the various integersfrom 1 to L. If the original, ?backbone?
grammarhas R3 binary rules of the form X ?
Y Z, thenthe resulting PCFG-LA model has L3 ?
R3 suchrules: X[1] ?
Y [1] Z[1], X[1] ?
Y [1] Z[2],X[1] ?
Y [2] Z[1], .
.
.
, X[L] ?
Y [L] Z[L].
Sim-ilarly, if the backbone grammar has R2 rules ofthe form X ?
Y the PCFG-LA model has L2 ?R2 such rules.3 The number of R1 terminal rulesX ?
w is just multiplied by L.The PCFG-LA has as many parameters to learnas rules: one probability per rule.
However, notall these parameters are free, as there are L ?
Nsum-to-one constraints, where N is the number ofbackbone nonterminals.
Thus we haveL3R3 + L2R2 + LR1 ?
LN (3)degrees of freedom.We note that Goodman (1997) mentioned possi-ble ways to factor the probability 1, making inde-pendence assumptions in order to reduce the num-ber of parameters.Runtime.
Assuming there are no unary rule cy-cles in the backbone grammar, bottom-up chartparsing of a length-n sentence at test time takestime proportional to n3L3R3 + n2L2R2 + nLR1,by attempting to apply each rule everywhere in thesentence.
(The dominating term comes from equa-tion (4) of Table 2: we must loop over all n3 triplesi, j, k and all R3 backbone rules X ?
Y Z and all3We use unary rules of this form (e.g.
the Treebank?s S?NP) in our reimplementation of Matsuzaki?s algorithm.L3 triples ?, ?, ?.)
As a function of n and L only,this is O(n3L3).At training time, to induce the annotations ona given backbone tree with n nodes, one can runa constrained version of this algorithm that loopsover only the n triples i, j, k that are consistentwith the given tree (and considers only the singleconsistent backbone rule for each one).
This takestime O(nL3), as does the inside-outside versionwe actually use to collect expected PCFG-LA rulecounts for EM training.We now introduce a model that is smaller, andhas a lower runtime complexity, because it adheresto specified ways of propagating features throughthe tree.3.2 Feature Passing: The INHERIT ModelMany linguistic theories assume that features getpassed from the mother node to their children orsome of their children.
In many cases it is thehead child that gets passed its feature value fromits mother (e.g., Kaplan and Bresnan (1982), Pol-lard and Sag (1994)).
In some cases the feature ispassed to both the head and the non-head child, orperhaps even to the non-head alone.Figure 2: Features are passed to different childrenat different positions in the tree.In the example in Fig.
2, the tense feature (pres)is always passed to the head child (underlined).How the number feature (sg/pl) is passed dependson the rewrite rule: S ?
NP VP passes it to bothchildren, to enforce subject-verb agreement, whileVP ?
V NP only passes it to the head child, sincethe object NP is free not to agree with the verb.A feature grammar can incorporate such pat-terns of feature passing.
We introduce additionalparameters that define the probability of passing afeature to certain children.
The head child of eachnode is given deterministically by the head rulesof (Collins, 1996).Under the INHERIT model that we propose, the319Model Runtime and d.f.
Simplified equation for inside probabilities (ignores unary rules)Matsuzakiet al (2005)test: O(n3L3)train: O(nL3)d.f.
: L3R3 +L2R2 +LR1?LNBX[?
](i, k) =XY,?,Z,?,jP (X[?]
?
Y [?]
Z[?])
(4)?BY [?
](i, j)?BZ[?
](j, k)INHERITmodel(this paper)test: O(n3L)train: O(nL)d.f.
: L(R3 + R2 +R1) + 3R3 ?NBX[?
](i, k) =XY,Z,jP (X[?]
?
Y Z) (5)?0B@P (neither | X,Y, Z) ?
BY (i, j) ?
BZ(j, k))+ P (left | X,Y, Z) ?
BY [?
](i, j) ?
BZ(j, k))+ P (right | X,Y, Z) ?
BY (i, j) ?
BZ[?
](j, k))+ P (both | X,Y, Z) ?
BY [?
]Y (i, j) ?
BZ[?
](j, k))1CABX(i, j) =X?Pann(?
| X)?BX[?
](i, j) (6)P (left | X,Y, Z) =?P (head | X,Y, Z) if Y heads X ?
Y ZP (nonhead | X,Y, Z) otherwise (7)P (right | X,Y, Z) =?P (head | X,Y, Z) if Z heads X ?
Y ZP (nonhead | X,Y, Z) otherwise (8)Table 2: Comparison of the PCFG-LA model with the INHERIT model proposed in this paper.
?d.f.
?stands for ?degrees of freedom?
(i.e., free parameters).
The B terms are inside probabilities; to computeViterbi parse probabilities instead, replace summation by maximization.
Note the use of the intermediatequantity BX(i, j) to improve runtime complexity by moving some summations out of the inner loop;this is an instance of a ?folding transformation?
(Blatz and Eisner, 2006).Figure 3: Two passpatterns.
Left: T2.
The featureis passed to the head child (underlined).
Right: T3.The feature is passed to both children.probabilities of tree T2 in Fig.
3 are calculated asfollows, with Pann(1 | NP ) being the probabilityof annotating an NP with feature 1 if it does notinherit its parent?s feature.
The VP is boldfaced toindicate that it is the head child of this rule.P (T2) = P (ROOT ?
S[2])?P (S[2] ?
NP VP)?P (pass to head | S ?
NP VP)?Pann(1 | NP) ?
P (NP[1] ??
He)?P (VP[2] ??
loves cookies)Tree T3 in Fig.
3 has the following probability:P (T3) = P (ROOT ?
S[2])?P (S[2] ?
NP VP)?P (pass to both | S ?
NP VP)?P (NP[2] ??
He)?P (VP[2] ??
loves cookies)In T2, the subject NP chose feature 1 or 2 indepen-dent of its parent S, according to the distributionPann(?
| NP).
In T3, it was constrained to inheritits parent?s feature 2.Degrees of freedom.
The INHERIT model maybe regarded as containing all the same rules(see (1)) as the PCFG-LA model.
However, theserules?
probabilities are now collectively deter-mined by a smaller set of shared parameters.4 Thatis because the distribution of the child features ?and ?
no longer depends arbitrarily on the rest ofthe rule.
?
is either equal to ?, or chosen indepen-dently of everything but Y .The model needs probabilities for L ?
R3binary-rule parameters like P (S[2] ?
NP VP)above, as well as L ?
R2 unary-rule and L ?
R1lexical-rule parameters.
None of these considerthe annotations on the children.
They are subjectto L?N sum-to-one constraints.The model also needs 4?R3 passpattern prob-abilities like P (pass to head | X ?
Y Z) above,with R3 sum-to-one constraints, and L ?
N non-inherited annotation parameters Pann(?|X), withN sum-to-one constraints.Adding these up and canceling the two L ?
N4The reader may find it useful to write out the probabilityP (X[?]
?
Y [?]
Z[?])
in terms of the parameters describedbelow.
Like equation (5), it is P (X[?]
?
Y Z) times a sumof up to 4 products, corresponding to the 4 passpattern cases.320terms, the INHERIT model hasL(R3 +R2 +R1) + 3R3 ?N (9)degrees of freedom.
Thus for a typical grammarwhere R3 dominates, we have reduced the numberof free parameters from about L3R3 to only aboutLR3.Runtime.
We may likewise reduce an L3 factorto L in the runtime.
Table 2 shows dynamic pro-gramming equations for the INHERIT model.
Byexercising care, they are able to avoid summingover all possible values of ?
and ?
within the in-ner loop.
This is possible because when they arenot inherited, they do not depend on X,Y, Z, or ?.3.3 Multiple FeaturesThe INHERIT model described above is linguisti-cally naive in several ways.
One problem (see sec-tion 6 for others) is that each nonterminal has onlya single feature to pass.
Linguists, however, usu-ally annotate each phrase with multiple features.Our example tree in Fig.
2 was annotated with bothtense and number features, with different inheri-tance patterns.As a step up from INHERIT, we propose anINHERIT2 model where each nonterminal carriestwo features.
Thus, we will have L6R3 binaryrules instead of L3R3.
However, we assume thatthe two features choose their passpatterns inde-pendently, and that when a feature is not inher-ited, it is chosen independently of the other fea-ture.
This keeps the number of parameters down.In effect, we are definingP (X[?][?]
?
Y [?][?]
Z[?][?
])= P (X[?][?]
?
Y Z)?P1(?, ?
| X[?]
?
Y Z)?P2(?, ?
| X[?]
?
Y Z)where P1 and P2 choose child features as if theywere separate single-feature INHERIT models.We omit discussion of dynamic programmingspeedups for INHERIT2.
Empirically, the hope isthat the two features when learned with the EMalgorithm will pick out different linguistic proper-ties of the constituents in the treebank tree.4 Annealing-Like Training ApproachesTraining latent PCFG models, like training mostother unsupervised models, requires non-convexoptimization.
To find good parameter values, itis often helpful to train a simpler model first anduse its parameters to derive a starting guess for theharder optimization problem.
A well-known ex-ample is the training of the IBM models for statis-tical machine translation (Berger et al, 1994).In this vein, we did an experiment in which wegradually increased L during EM training of thePCFG-LA and INHERIT models.
Whenever thetraining likelihood began to converge, we man-ually and globally increased L, simply doublingor tripling it (see ?clone all?
in Table 3 andFig.
5).
The probability of X[?]
?
Y [?]Z[?
]under the new model was initialized to be pro-portional to the probability of X[?
mod L] ?Y [?
mod L]Z[?
mod L] (where L refers to theold L),5 times a random ?jitter?
to break symme-try.In a second annealing experiment (?clonesome?)
we addressed a weakness of the PCFG-LA and INHERIT models: They give every non-terminal the same number of latent annotations.It would seem that different coarse-grained non-terminals in the original Penn Treebank have dif-ferent degrees of impurity (Klein and Manning,2003).
There are linguistically many kinds ofNP, which are differentially selected for by vari-ous contexts and hence are worth distinguishing.By contrast, -LRB- is almost always realized asa left parenthesis and may not need further refine-ment.
Our ?clone some?
annealing starts by train-ing a model with L=2 to convergence.
Then, in-stead of cloning all nonterminals as in the previ-ous annealing experiments, we clone only thosethat have seemed to benefit most from their previ-ous refinement.
This benefit is measured by theJensen-Shannon divergence of the two distribu-tions P (X[0] ?
?
?
? )
and P (X[1] ?
?
?
?
).
The5Notice that as well as cloning X[?
], this procedure mul-tiplies by 4, 2, and 1 the number of binary, unary, and lex-ical rules that rewrite X[?].
To leave the backbone gram-mar unchanged, we should have scaled down the probabili-ties of such rules by 1/4, 1/2, and 1 respectively.
Instead, wesimply scaled them all down by the same proportion.
Whilethis temporarily changes the balance of probability among thethree kinds of rules, EM immediately corrects this balance onthe next training iteration to match the observed balance onthe treebank trees?hence the one-iteration downtick in Fig-ure 5).321Jensen-Shannon divergence is defined asD(q, r) =12(D(q ||q + r2)+D(r ||q + r2))These experiments are a kind of ?poor man?sversion?
of the deterministic annealing cluster-ing algorithm (Pereira et al, 1993; Rose, 1998),which gradually increases the number of clus-ters during the clustering process.
In determinis-tic annealing, one starts in principle with a verylarge number of clusters, but maximizes likeli-hood only under a constraint that the joint distri-bution p(point , cluster) must have very high en-tropy.
This drives all of the cluster centroids to co-incide exactly, redundantly representing just oneeffective cluster.
As the entropy is permitted to de-crease, some of the cluster centroids find it worth-while to drift apart.6 In future work, we wouldlike to apply this technique to split nonterminalsgradually, by initially requiring high-entropy parseforests on the training data and slowly relaxing thisconstraint.5 Experiments5.1 SetupWe ran several experiments to compare the IN-HERIT with the PCFG-LA model and look into theeffect of different Treebank preprocessing and theannealing-like procedures.We used sections 2?20 of the Penn Treebank 2Wall Street Journal corpus (Marcus et al, 1993)for training, section 22 as development set andsection 23 for testing.
Following Matsuzaki et al(2005), words occurring fewer than 4 times in thetraining corpus were replaced by unknown-wordsymbols that encoded certain suffix and capitaliza-tion information.All experiments used simple add-lambdasmoothing (?=0.1) during the reestimation step(M step) of training.Binarization and Markovization.
Before ex-tracting the backbone PCFG and running the con-strained inside-outside (EM) training algorithm,we preprocessed the Treebank using center-parentbinarization Matsuzaki et al (2005).
Besides mak-ing the rules at most binary, this preprocessing alsohelpfully enriched the backbone nonterminals.
For6In practice, each very large group of centroids (effectivecluster) is represented by just two, until such time as thosetwo drift apart to represent separate effective clusters?theneach is cloned.all but the first (?Basic?)
experiments, we alsoenriched the nonterminals with order-1 horizon-tal and order-2 vertical markovization (Klein andManning, 2003).7 Figure 4 shows what a multiple-child structure X ?
A B H C D looks likeafter binarization and markovization.
The bina-rization process starts at the head of the sentenceand moves to the right, inserting an auxiliary nodefor each picked up child, then moving to the left.Each auxiliary node consists of the parent label,the direction (L or R) and the label of the childjust picked up.Figure 4: Horizontal and vertical markovizationand center-parent binarization of the rule X ?A B H C D where H is the head child.Initialization.
The backbone PCFG grammarwas read off the altered Treebank, and the initialannotated grammar was created by creating sev-eral versions of every rewrite rule.
The proba-bilities of these newly created rules are uniformand proportional to the original rule, multiplied bya random epsilon factor uniformly sampled from[.9999,1.0001] to break symmetry.5.2 DecodingTo test the PCFG learned by a given method,we attempted to recover the unannotated parseof each sentence in the development set.
Wethen scored these parses by debinarizing or de-markovizing them, then measuring their precisionand recall of the labeled constituents from thegold-standard Treebank parses.7The vertical markovization was applied before binariza-tion.
?
Matsuzaki et al (2005) used a markovized grammarto get a better unannotated parse forest during decoding, butthey did not markovize the training data.322Figure 5: Loge-likelihood during training.
Thetwo ?anneal?
curves use the ?clone all?
method.We increased L after iteration 50 and, for the IN-HERIT model, iteration 110.
The downward spikesin the two annealed cases are due to perturbationof the model parameters (footnote 5).An unannotated parse?s probability is the totalprobability, under our learned PCFG, of all of itsannotated refinements.
This total can be efficientlycomputed by the constrained version of the insidealgorithm in Table 2.How do we obtain the unannotated parse whosetotal probability is greatest?
It does not suffice tofind the single best annotated parse and then stripoff the annotations.
Matsuzaki et al (2005) notethat the best annotated parse is in fact NP-hard tofind.
We use their reranking approximation.
A1000-best list for each sentence in the decodingset was created by parsing with our markovizedunannotated grammar and extracting the 1000 bestparses using the k-best algorithm 3 described inHuang and Chiang (2005).
Then we chose themost probable of these 1000 unannotated parsesunder our PCFG, first finding the total probabilityof each by using the the constrained inside algo-rithm as explained above.85.3 Results and DiscussionTable 3 summarizes the results on developmentand test data.
9 Figure 5 shows the training log-likelihoods.First, markovization of the Treebank leads to8For the first set of experiments, in which the models weretrained on a simple non-markovized grammar, the 1000-besttrees had to be ?demarkovized?
before our PCFG was able torescore them.9All results are reported on sentences of 40 words or less.striking improvements.
The ?Basic?
block of ex-periments in Table 3 used non-markovized gram-mars, as in Matsuzaki et al (2005).
The next blockof experiments, introducing markovized gram-mars, shows a considerable improvement.
Thisis not simply because markovization increases thenumber of parameters: markovization with L = 2already beats basic models that have much higherL and far more parameters.Evidently, markovization pre-splits the labelsin the trees in a reasonable way, so EM has lesswork to do.
This is not to say that markovizationeliminates the need for hidden annotations: withmarkovization, going from L=1 to L=2 increasesthe parsing accuracy even more than without it.Second, our ?clone all?
training technique(shown in the next block of Table 3) did nothelp performance and may even have hurt slightly.Here we initialized the L=2x2 model with thetrained L=2 model for PCFG-LA, and the L=3x3model with the L=3 and the L=3x3x3 model withthe L=3x3 model.Third, our ?clone some?
training technique ap-peared to work.
On PCFG-LA, the L<2x2 con-dition (i.e., train with L=2 and then clone some)matched the performance of L=4 with 30% fewerparameters.
On INHERIT, L<2x2 beat L=4 with8% fewer parameters.
In these experiments, weused the average divergence as a threshold: X[0]and X[1] are split again if the divergence of theirrewrite distributions is higher than average.Fourth, our INHERIT model was a disappoint-ment.
It generally performed slightly worse thanPCFG-LA when given about as many degreesof freedom.
This was also the case on somecursory experiments on smaller training corpora.It is tempting to conclude that INHERIT simplyadopted overly strong linguistic constraints, butrelaxing those constraints by moving to the IN-HERIT2 model did not seem to help.
In ourone experiment with INHERIT2 (not shown in Ta-ble 3), using 2 features that can each take L=2values (d.f.
: 212,707) obtains an F1 score of only83.67?worse than 1 feature taking L=4 values.5.4 Analysis: What was learned by INHERIT?INHERIT did seem to discover ?linguistic?
fea-tures, as intended, even though this did not im-prove parse accuracy.
We trained INHERIT andPCFG-LA models (both L=2, non-markovized)and noticed the following.323PCFG-LA INHERITL d.f.
LP LR F1 L d.f.
LP LR F1Basic 1 24,226 76.99 74.51 75.73 1 35,956 76.99 74.51 75.732 72,392 81.22 80.67 80.94 2 60,902 79.42 77.58 78.494 334,384 83.53 83.39 83.46 12 303,162 82.41 81.55 81.988 2,177,888 85.43 85.05 85.24 80 1,959,053 83.99 83.02 83.50Markov.
1 41,027 79.95 78.43 79.18 1 88,385 79.95 78.43 79.182 132,371 83.85 82.23 83.032 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.314 220,343 85.30 84.06 84.683 506,427 86.44 85.19 85.81 9 440,273 86.16 85.12 85.644 1,120,232 87.09 85.71 86.39 26 1,188,035 86.55 85.55 86.05Cloneall 2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.313x3 440,273 85.99 84.88 85.432x2 1,120,232 87.06 85.49 86.27 3x3x3 1,232,021 86.65 85.70 86.17Cl.some 2 178,264 85.70 84.37 85.03 2 132,371 83.85 82.23 83.03<2x2 789,279 87.17 85.71 86.43 <2x2 203,673 85.49 84.45 84.97<2x2x2 314,999 85.57 84.60 85.08Table 3: Results on the development set: labeled precision (LP), labeled recall (LR), and their harmonicmean (F1).
?Basic?
models are trained on a non-markovized treebank (as in Matsuzaki et al (2005)); allothers are trained on a markovized treebank.
The best model (PCFG-LA with ?clone some?
annealing,F1=86.43) has also been decoded on the final test set, reaching P/R=86.94/85.40 (F1=86.17).We used both models to assign the most-probable annotations to the gold parses of the de-velopment set.
Under the INHERIT model, NP[0]vs. NP[1] constituents were 21% plural vs. 41%plural.
Under PCFG-LA this effect was weaker(30% vs. 39%), although it was significant in both(Fisher?s exact test, p < 0.001).
Strikingly, un-der the INHERIT model, the NP?s were 10 timesmore likely to pass this feature to both children(Fisher?s, p < 0.001)?just as we would expectfor a number feature, since the determiner andhead noun of an NP must agree.The INHERIT model also learned to use featurevalue 1 for ?tensed auxiliary.?
The VP[1] nonter-minal was far more likely than VP[0] to expand asV VP, where V represents any of the tensed verbpreterminals VBZ, VBG, VBN, VBD, VBP.
Further-more, these expansion rules had a very strong pref-erence for ?pass to head,?
so that the left childwould also be annotated as a tensed auxiliary, typ-ically causing it to expand as a form of be, have,or do.
In short, the feature ensured that it was gen-uine auxiliary verbs that subcategorized for VP?s.
(The PCFG-LA model actually arranged thesame behavior, e.g.
similarly preferring VBZ[1] inthe auxiliary expansion rule VP ?
VBZ VP.
Thedifference is that the PCFG-LA model was ableto express this preference directly without prop-agating the [1] up to the VP parent.
Hence neitherVP[0] nor VP[1] became strongly associated withthe auxiliary rule.
)Many things are equally learned by both mod-els: They learn the difference between subordinat-ing conjunctions (while, if ) and prepositions (un-der, after), putting them in distinct groups of theoriginal IN tag, which typically combine with sen-tences and noun phrases, respectively.
Both mod-els also split the conjunction CC into two distinctgroups: a group of conjunctions starting with anupper-case letter at the beginning of the sentenceand a group containing all other conjunctions.6 Future Work: Log-Linear ModelingOur approach in the INHERIT model made certainstrict independence assumptions, with no backoff.The choice of a particular passpattern, for exam-ple, depends on all and only the three nontermi-nals X,Y, Z.
However, given sparse training data,sometimes it is advantageous to back off to smalleramounts of contextual information; the nontermi-nal X or Y might alone be sufficient to predict thepasspattern.324A very reasonable framework for handling thisissue is to model P (X[?]
?
Y [?]
Z[?])
witha log-linear model.10 Feature functions wouldconsider the values of variously sized, over-lapping subsets of X,Y, Z, ?, ?, ?.
For exam-ple, a certain feature might fire when X[?]
=NP[1] and Z[?]
= N[2].
This approach can be ex-tended to the multi-feature case, as in INHERIT2.Inheritance as in the INHERIT model can thenbe expressed by features like ?
= ?, or ?
=?
and X = VP.
During early iterations, we coulduse a prior to encourage a strong positive weighton these inheritance features, and gradually re-lax this bias?akin to the ?structural annealing?
of(Smith and Eisner, 2006).When modeling the lexical rule P (X[?]
?
w),we could use features that consider the spellingof the word w in conjunction with the value of?.
Thus, we might learn that V [1] is particularlylikely to rewrite as a word ending in -s. Spellingfeatures that are predictable from string contextare important clues to the existence and behaviorof the hidden annotations we wish to induce.A final remark is that ?inheritance?
does notnecessarily have to mean that ?
= ?.
It is enoughthat ?
and ?
should have high mutual informa-tion, so that one can be predicted from the other;they do not actually have to be represented by thesame integer.
More broadly, we might like ?
tohave high mutual information with the pair (?, ?
).One might try using this sort of intuition directlyin an unsupervised learning procedure (Elidan andFriedman, 2003).7 ConclusionsWe have discussed ?informed?
techniques for in-ducing latent syntactic features.
Our INHERITmodel tries to constrain the way in which featuresare passed through the tree.
The motivation forthis approach is twofold: First, we wanted to cap-ture the linguistic insight that features follow cer-tain patterns in propagating through the tree.
Sec-ond, we wanted to make it statistically feasible andcomputationally tractable to increase L to highervalues than in the PCFG-LA model.
The hope wasthat the learning process could then make finer dis-tinctions and learn more fine-grained information.However, it turned out that the higher values ofL did not compensate for the perhaps overly con-10This affects EM training only by requiring a convex op-timization at the M step (Riezler, 1998).strained model.
The results on English parsingrather suggest that it is the similarity in degrees offreedom (e.g., INHERIT with L=3x3x3 and PCFG-LA with L=2x2) that produces comparable results.Substantial gains were achieved by usingmarkovization and splitting only selected nonter-minals.
With these techniques we reach a pars-ing accuracy similar to Matsuzaki et al (2005),but with an order of magnitude less parameters,resulting in more efficient parsing.
We hope toget more wins in future by using more sophisti-cated annealing techniques and log-linear model-ing techniques.AcknowledgmentsThis paper is based upon work supported by theNational Science Foundation under Grant No.0313193.
We are grateful to Takuya Matsuzakifor providing details about his implementation ofPCFG-LA, and to Noah Smith and the anonymousreviewers for helpful comments.ReferencesA.
Berger, P. Brown, S. Pietra, V. Pietra, J. Lafferty,H.
Printz, and L. Ures.
1994.
The CANDIDE sys-tem for machine translation.Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-Intyre, Victoria Tredinnick, Grace Kim, Mary AnnMarcinkiewicz, and Britta Schasberger.
1995.Bracketing guidelines for Treebank II style: PennTreebank project.
Technical Report MS-CIS-95-06,University of Pennsylvania, January.John Blatz and Jason Eisner.
2006.
Transforming pars-ing algorithms and other weighted logic programs.In Proceedings of the 11th Conference on FormalGrammar.Eugene Charniak.
1996.
Tree-bank grammars.
In Pro-ceedings of the 13th National Conference on Artifi-cial Intelligence.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Pro-ceedings of the Fourteenth National Conference onArtificial Intelligence, pages 598?603.David Chiang and Daniel M. Bikel.
2002.
Recov-ering latent information in treebanks.
In COLING2002: The 17th International Conference on Com-putational Linguistics, Taipei.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In ACL-96,pages 184?191, Santa Cruz, CA.
ACL.325Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics and 8th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 16?23, Madrid.
Association forComputational Linguistics.Jason Eisner, Eric Goldlust, and Noah A. Smith.
2005.Compiling comp ling: Weighted dynamic program-ming and the Dyna language.
In Proceedings ofHuman Language Technology Conference and Con-ference on Empirical Methods in Natural LanguageProcessing, pages 281?290, Vancouver, October.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In COL-ING96: Proceedings of the 16th International Con-ference on Computational Linguistics, pages 340?345, Copenhagen.
Center for Sprogteknologi.Gal Elidan and Nir Friedman.
2003.
The informationbottleneck EM algorithm.
In Proceedings of UAI.Joshua Goodman.
1997.
Probabilistic feature gram-mars.
In Proceedings of the 5th International Work-shop on Parsing Technologies, pages 89?100, MIT,Cambridge, MA, September.L.
Huang and D. Chiang.
2005.
Parsing and k-bestalgorithms.
In Proc.
of IWPT.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Ronald M. Kaplan and Joan Bresnan.
1982.
Lexical-functional grammar: A formal system for grammat-ical representation.
In Joan Bresnan, editor, TheMental Representation of Grammatical Relations,pages 173?281.
MIT Press, Cambridge, MA.Dan Klein and Christopher D. Manning.
2001.
Dis-tributional phrase structure induction.
In The FifthConference on Natural Language Learning.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Erhard Hinrichs andDan Roth, editors, Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 423?430, Sapporo, Japan.K.
Lari and S. Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outsidealgorithm.
Computer Speech and Language, 4:35?56.David M. Magerman.
1995.
Statistical Decision-Treemodels for parsing.
In Proceedings of the 33rd An-nual Meeting of the Association for ComputationalLinguistics, pages 276?283, Cambridge, Mass.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn treebank.
Computa-tional Linguistics, 19(2):313?330, June.Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL?05),University of Michigan.Fernando Pereira and Yves Schabes.
1992.
Inside-Outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th Meeting of the As-sociation for Computational Linguistics, pages 128?135, Newark.
University of Delaware.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of English words.
InProceedings of ACL, Ohio State University.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress, Chicago.Detlef Prescher.
2005.
Head-driven PCFGs withlatent-head statistics.
In Proceedings of the 9thInternational Workshop on Parsing Technologies,pages 115?124, Vancouver, BC, Canada, October.Stefan Riezler.
1998.
Statistical inference and prob-abilistic modeling for constraint-based NLP.
InB.
Schro?der, W. Lenders, W. Hess, and T. Portele,editors, Computers, Linguistics, and Phonetics be-tween Language and Speech: Proceedings of the 4thConference on Natural Language Processing (KON-VENS?98), pages 111?124, Bonn.
Lang.Kenneth Rose.
1998.
Deterministic annealing for clus-tering, compression, classification, regression, andrelated optimization problems.
Proceedings of theIEEE, 80:2210?2239, November.Noah A. Smith and Jason Eisner.
2006.
Annealingstructural bias in multilingual weighted grammar in-duction.
In Proceedings of ACL.326
