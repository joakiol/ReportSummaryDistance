Automatic Optimization of Dialogue ManagementDiane J. Litman, Michael S. Kearns, Satinder Singh, Mari lyn A. WalkerAT&T Labs -  Research180 Park  AvenueF lo rham Park ,  N J  07932 USA{d iane ,mkearns ,bave j  a,walker} @research.ar t .cornAbstractDesigning the dialogue strategy of a spoken dialoguesystem involves many nontrivial choices.
This pa-per I)resents a reinforcement learning approach forautomatically optimizing a dialogue strategy thataddresses the technical challenges in applying re-inforcement learning to a working dialogue systemwith hulnan users.
\?e then show that our approachmeasurably improves performance in an experimen-tal system.1 I n t roduct ionRecent advances in spoken language understandinghave made it 1)ossible to develop dialogue systemstbr many applications.
The role of the dialogue man-ager in such systems is to interact in a naturM w~yto hel 1 ) the user complete the tasks that the systemis designed to support.
Tyl)ically, an expert designsa dialogue manager by hand, and has to make m~nynontrivial design choices that can seriously impactsystem performance.
This paper applies reintbrce-ment learning (RL) to automatically learn designchoices that optimize system pertbrnmnee for a cho-seLL pertbrmance measure (Levin et al, 2000; Walkeret al, 1998).Consider the spoken dialogue system named N J-Fun, wlfich we built to help users find fun placesto go in New Jersey.
A sample dialogue with N J-Fun is shown in Figure 1, with system utteranceslabeled Si and user utterances labeled Ui.
WhenNJFun greets the user, it can provide an open greet-ing such as utterance S1, expecting that the userwill take the initiative.
Alternatively, it can pro-vide a directive greeting in which the system takesthe initiative, as in "Welcome to NJFun.
Please sayan activity name or say 'list activities' for a list ofactivities I know about."
After tile user's responseU1, NJFun must decide whether it should explicitlyconfirm its understanding, as in utterances $2 and$3.
NJFun can also simply continue on with the di-alogue, as when it does not explicitly confirm thatthe user wants to find out M)out wineries.
In NJFun,as shown in more detail below, decisions about ini-tiative and confirmation strategies alone result in aSI: Welcome to N.llSm.
How may I help you?UI: I'd like to find mn winetasting in Lambertvillein the morning.
(ASR output: I'd like to findout wirterics the in the Lambertvillc in themornin.q.
)$2: Did you say you are interested in Lambertville?U2: Yes.$3: Did you say you want to go in the morning?U3: Yes.$4: I found a winery near l~ambertville that isopen in the morning, it is \[...\] Please givelne feedback by saying 'good', ~so-so', or 'bad'.U4: Good.Figure 1: An example dialogue with NJFun.search space of 242 potential globnl dialogue strate-gies.
Furthermore, the performance of a dialoguestrategy depends on many other factors, such asthe user population, the robustness of the automaticspeech recognizer (ASR), and task difficulty (Kammet al, 1998; DanMi and Gerbino, 1995).In the main, previous research has treated thespecification of the dialogue management s rategyas an iterative design problem: several versions of asystem are created, dialogue corpora are collectedwith human users interacting with different ver-sions of tile system, a number of evaluation met-rics are collected ibr each dialogue, and the differ-ent versions are statistically compared (Danieli andGerbino, 1995; Sanderman et al, 1998).
Due to thecosts of experimentation, only a few global strategiesare typically explored in any one experiment.However, recent work has suggested that dialoguestrategy can be designed using tile formalism ofMarkov decision processes (MDPs) and the algo-rithms of RL (Biermann and Long, 1996; Levin etal., 2000; Walker et nl., 1998; Singh et al, 1999).More specifically, the MDP formalism suggests amethod for optimizing dialogue strategies from sam-ple dialogue data.
The main advantage of this ap-proach is the 1)otential tbr computing an optilnal di-alogue strategy within a much larger search space,using a relatively small nmnber of training dialogues.This paper presents an application of RL to the502problem of oi)timizing dialogue strategy selection inthe NJFnn system, and exl)erimentally demonstratesthe utility of the ~l)proach.
Section 2 exl)lahls howwe apply RL to dialogue systems, then Se('tion 3describes t.he NJFun system in detail.
Section 4 deescribes how NJFun optimizes its dialogue strategyfrom experimentally obtained dialogue data.
Sec-tion 5 reports results from testing the learned strat-egy demonstrating that our al)l)roach improves taskcoml)letion rates (our chosen measure for 1)erfor-mance optimization).
A conll)alliOll paper providesonly an al)brevi~tted system and dialogue managerdescription, but includes additional results not pre-sented here (Singh et al, 2000), such as analysis es-tablishing the veracity of the MDP we learn, andcomparisons of our learned strategy to strategieshand-picked by dialogue xperts.2 Reinforcement Learning forDialogueDue to space limitations, we 1)resent only a 1)riefoverview of how di~dogue strategy optimization canbe viewed as an llL 1)roblem; for more details,see Singh ctal .
(\]999), Walker el; a.1.
(\]998), Levinet al (2000).
A dialogue strategy is a mapl)ing h'oma set ot!
states (which summarize the entire dialogueso far) to a set of actions (such as the system's utter-mines and database queries).
There are nmltil)l(~ rea-sonable action choices in each state; tyl)ically thesechoices are made by the system designer.
Our RL-I)ased at)l)roach is to build a system that exploresthese choices in a systematic way through experi-ments with rel)resentative human us(!rs.
A scalari)erf()rmanee l l leasllre, called a rewal'd, is t h(m (;al-eulated for each Cxl)erimental diMogue.
(We dis-cuss various choices for this reward measure later,but in our experiments only terminal dialogue statesha,re nonzero  rewi-l,rds, s l id  the reward lneasul'(}s arcquantities directly obtMnable from the experimentalset-up, such as user satisfaction or task coml)letion.
)This experimental data is used to construct an MDPwhich models the users' intera(:tion with the system.The l)roblem of learning the best dialogue strategyfrom data is thus reduced to computing the optimalpolicy tbr choosing actions in an MDP - that is, thesystem's goal is to take actions so as to maximizeexpected reward.
The comput~ttion of the ol)timalpolicy given the MDP can be done etficiently usingstan&trd RL algorithms.How do we build the desired MDP from sampledialogues?
Following Singh et al (1999), we canview a dialogue as a trajectory in the chosen statespace determined by the system actions and userresl) onses:S1 -~a l , r l  '5'2 --}a~,rs 83 "-~aa,ra " '"Here si -%,,,.~ si+l indicates that at the ith ex-change, the system was in state si, executed actionai, received reward ri, and then the state changedto si+~.
Dialogue sequences obtained froln trainingdata can be used to empirically estimate the transi-tion probabilities P(.s"la', a) (denoting the probabil-ity of a mmsition to state s', given that the systemwas in state .s and took ;ration a), and the rewardfunction R(.s, (t).
The estilnated transition 1)tel)abil-ities and rewi~rd flmction constitute an MDP modelof the nser population's interaction with the system.Given this MDP, the exl)ected cumnlative reward(or Q-value) Q(s, a) of taking action a from state scan be calculated in terms of the Q-wdues of succes-sor states via the following recursive quation:Q(.% = ,) + r(,,'l,% ,)n,a:,: Q(,,", , ' ) .t ;  !These Q-values can be estimated to within a desiredthreshold using the standard RL value iteration al-gorithm (Sutton, 1.991.
), which iteratively updatesthe estimate of Q(s, a) based on the current Q-vahmsof neighboring states.
Once value iteration is con>pleted, the optima\] diah)gue strategy (according toour estimated model) is obtained by selecting theaction with the maximum Q-value at each dia.loguestate.While this apl)roach is theoretically appealing, thecost of obtaining sample human dialogues makes itcrucial to limit the size of the state space, to mini-mize data sparsity problems, while retaining enoughinformation in the state to learn an accurate model.Our approad~ is to work directly in a minimal butcarefully designed stat;e space (Singh et al, 1999).The contribution of this paper is to eml)iricallyvMi(tate a practical methodology for using IlL tobuild a dialogue system that ol)timizes its behav-ior from dialogue data.
Our methodology involves1) representing a dialogue strategy as a mapl)il~gfronl each state in the chosen state space S to aset of dialogue actions, 2) deploying an initial trah>ing system that generates exploratory training datawith respect o S, 3) eonstrncting an MDP modelfrom the obtained training data, 4) using value iter-ation to learn the optimal dialogue strategy in thelearned MDP, and 4) redeploying the system usingthe learned state/~mtion real)ping.
The next sectiondetails the use of this methodology to design theNJFun system.3 The N JFun  SystemNJFnn is a real-time spoken dialogue system thatprovides users with intbrmation about things to doin New Jersey.
NJFun is built using a general pur-pose 1)latt'ornl tbr spoken dialogue systems (Levinet al, 1.999), with support tbr modules tbr attto-rustic speech recognition (ASI/.
), spoken language503Action PromptGreets Welcome to NJIqm.
Please say an activity name or say 'list activities' for a list of activities I knowabout.GreetU \Velcome to NdPun.
How may I help you?ReAsklS Iknow about amusement parks, aquariums, cruises, historic sites, museums, parks, theaters,wineries, and zoos.
Please say an activity name from this list.ReAsklM Please tell me the activity type.You can also tell me the location and time.Ask2S Please say the name of the town or city that you are interested in.Ask2U Please give me more information.ReAsk2S Please teli me the name of the town or city that you are interested in.ReAsk2M Please tell me the location that you are interested in.
You call also tell me the time.Figure 2: Sample initiative strategy choices.understanding, text-to-speech (TTS), database ac-cess, and dialogue management.
NJFnn uses aspeech recognizer with stochastic language and un-derstanding models trained from example user ut-terances, and a TTS system based on concatena-tive diphone synthesis.
Its database is populatedfrom the nj .
on l ine  webpage to contain informationabout activities.
NJFun indexes this database usingthree attributes: activity type, location, and time ofday (which can assume values morning, afternoon,or evening).hffornmlly, the NJFun dialogue manager sequen-tially queries the user regarding the activity, loca-tion and time attributes, respectively.
NJFun firstasks the user for the current attribute (and 1)ossiblythe other attributes, depending on the initiative).If the current attribute's value is not obtained, N.J-Fun asks for the attrilmte (and possibly the laterattributes) again.
If NJFun still does not obtaina value, NJFun moves on to the next attribute(s).Whenever NJFun successihlly obtains a value, itcan confirm the vMue, or move on to the next at-tribute(s).
When NJFun has finished acquiring at-tributes, it queries the database (using a wildcardfor each unobtained attribute value).
The length ofNJFun dialogues ranges from 1 to 12 user utterancesbefore the database query.
Although the NJFun di-alogues are fairly short (since NJFun only asks foran attribute twice), the information access part ofthe dialogue is similar to more complex tasks.As discussed above, our methodology for using RLto optimize dialogue strategy requires that all poten-tim actions tbr each state be specified.
Note that atsome states it is easy for a human to make the cor-rect action choice.
We made obvious dialogue strat-egy choices in advance, and used learIfing only tooptimize the difficult choices (Walker et al, 1998).Ill NJFun, we restricted the action choices to 1) thetype of initiative to use when asking or reasking foran attribute, and 2) whether to confirm an attributevalue once obtained.
The optimal actions may varywith dialogue state, and are subject o active debatein the literature.Tile examples in Figure 2 show that NJFun canask the user about the first 2 attributes I using threetypes of initiative, based on the combination of tilewording of the system prompt (open versus direc-tive), and the type of grammar NJFun uses duringASR (restrictivc versus non-restrictive).
If NJFunuses an open question with m~ unrestricted gram-mar, it is using v.scr initiative (e.g., Greet\[l).
If N J-Fun instead uses a directive prompt with a restrictedgrammar, the system is using systcm initiative (e.g.,GreetS).
If NJFun uses a directive question with anon-restrictive granmlar, it is using mizcd initiative,because it allows the user to take the initiative bysupplying extra intbrnlation (e.g., ReAsklM).NJFun can also vary the strategy used to confirmeach attribute.
If NJFun asks the user to explicitlyverify an attribute, it is using czplicit confirmation(e.g., ExpConf2 for the location, exemplified by $2in Figure 1).
If NJFun does not generate any COlt-firnmtion prompt, it is using no confirmation (theNoConf action).Solely tbr the purposes of controlling its operation(as opposed to the le~trning, which we discuss in amoment), NJNm internally maintains an opcratio'nsvector of 14 variables.
2 variables track whether thesystem has greeted the user, and which attributethe system is currently attempting to obtain.
Foreach of the 3 attributes, 4 variables track whetherthe system has obtained the attribute's value, thesystent's confidence in the value (if obtained), thenumber of times the system has asked the user aboutthe attribute, and the type of ASR grammar mostrecently used to ask for the attribute.The formal state space S maintained by NJFunfor tile purposes of learning is nmch silnt)ler thanthe operations vector, due to the data sparsity con-cerns already discussed.
The dialogue state space$ contains only 7 variables, as summarized in Fig-sire 3.
S is computed from the operations vector us-ing a hand-designed algorithm.
The "greet" variable1 "Greet" is equivalent toasking for the first attribute.
N J-Fun always uses system initiative for the third attribute, be-cause at that point the user can only provide the time of (lay.504greet attr conf val times gram hist \]0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 \]Figure 3: State features and vahles.tracks whether tile system has greeted tile user ornot (no=0, yes=l).
"Attr ~: specifies which attrihuteNJFun is ('urrently ~tttelnpting to obtain or ver-ify (activity=l, location=2, time=a, done with at-tributes=4).
"Conf" tel)resents the confidence thatNaFun has after obtaining a wdue for an attribute.The values 0, 1, and 2 represent the lowest, middleand highest ASR confidence vahms?
The wdues 3and 4 are set when ASR hears "yes" or "no" after aconfirmation question.
"Val" tracks whether NaFunhas obtained a value, for tile attribute (no=0, yes=l).
"Times" tracks the number of times that N,lFun hasaske(1 the user ~d)out he attribute.
"(4ram" tracksthe type of grammar most ree(mtly used to obtainthe attribute (0=non-restrictive, 1=restrictive).
Fi-nally, "hist" (history) represents whether Nalflm hadtroullle understanding the user ill the earlier p~trt ofthe conversation (bad=0, good=l).
We omit the fulldetinition, but as a,n ex~unl>le , when N.lFun is work-ing on the secon(1 attribute (location), the historyvariable is set to 0 if NJFun does not have an ac-tivity, has an activity but has no confidence in thevalue, or needed two queries to obtain the activity.As mentioned above, the goal is to design a smallstate space that makes enough critical distin('tions tosuPi)ort learning.
The use of 6" redu(:es the mmfl)erof states to only 62, and SUl)l)orts the constru('tion ofmt MI)P model that is not sparse with respect o ,g,even using limite(1 trMning (btta.
:~ Tit(.'
state sp~t(;ethat we utilize here, although minimal, allows usto make initiative decisions based on the success ofearlier ex(:ha.nges, and confirmation de(:isions basedon ASR.
confidence scores and gralnmars.
'.Phe state/~t('tiol: real)ping r(-`l)resenting NaFun'sinitial dialogue strategy EIC (Explor:ttory for Ini-tiative and Confirmation) is in Figure 4.
Only theexploratory portion of the strategy is shown, namelythose states for which NaFun has an action choice.~klr each such state, we list tile two choices of actionsavailable.
(The action choices in boldfime are theones eventually identified as el)ritual 1)y the learningprocess, an(1 are discussed in detail later.)
The EICstrategy chooses random, ly between these two ac-21"or each uttermme, the ASH.
outfmt includes 11o|, only therecognized string, but also aIl asso(:ia.ted acoustic (:onJld(mcescore, iBased on data obtaintM dm'ing system deveJolmmnt ,we defined a mapl)ing from raw confidence, values into 3 ap-proximately equally potmlated p~rtitions.362 refers to those states that can actually occur in a di-alogue.
\[<)r example, greet=0 is only possible in the initialdialogue state "0 1 0 0 0 0 0".
Thus, all other states beginningwith 0 (e.g.
"0 I 0 0 I 0 0") will never occur.g1 0 0 (} 0 01 1 0 (} 1 0 01 1 0 1 0 0 01 1 0 1 0 1 01 1 1 1 0 0 01 1 1 1 0 \] 01 1 2 1 0 0 01 1 2 1 0 1 01 1 4 0 0 0 0\] 1 4 (} 1 0 0St~tte Action Choicesa c v t g it1 2 0 0 0 0 01 2 0 0 0 0 11 2 (} 0 1 (} 01 2 0 (} 1 0 11 2 0 1 0 (} 01 2 0 \] 0 0 11 2 0 1 0 1 01 2 0 1 0 1 11 2 1 1 0 (1 01 2 1 \] 0 0 11 2 1 1 0 1 0\] 2 1 1 0 \] 11 2 2 1 0 0 01 2 2 \] 0 0 11 2 2 \] 0 1 0\] 2 2 1 0 1 11 2 4 0 0 (1 01 2 4 0 0 0 11 2 4 0 1 0 01 2 4 0 1 0 11 3 () 1 () () 0\] 3 0 1 0 0 11 3 (1 1 0 1 01 3 \[) 1 0 \] \]1.
3 1 1 0 0 01 3 1 1 0 0 11 3 1 1.
0 1 01 3 1 1 0 \] 11 3 2 1 0 0 01 3 2 1 0 0 11 3 2 1 0 \] 0\] 3 2 1 0 1 1GreetS,GreetUReAsklS,ReAsklMNoConf, ExpConf lNoConf, ExpColfflNoCont, Exp ConflNoConf, ExpConf lNoConf~ExpConflNoConf, ExpConflReAsklS,ReAsklMReAsklS,RcAsklMAsk2S,Ask2UAsk2S,Ask2UI{eAsk2S,ReAsk2MReAsk2S,ReAsk2MNoConf, ExpCont'2NoConf, ExpConP2NoConf~ExpCont)2NoConf,ExpConf2NoCoiff, Exp C oaF2NoConf, ExpConf2NoConf, ExpCont2NoConf,ExpCong2NoConf~ExpConf2NoConf, ExpConf2NoConf, ExpConf2NoConf, ExpCon\[2ReAsk2S,RcAsk2MI/.eAsk2S,ReAsk2MRcAsk2S,ReAsk2MReAsk2S,ReAsk2MNoConf, ExpCon\[3NoConf, Exl) Conf3NoConf, ExpConf3NoConf,Ext)ConF3NoConf, ExpCont~NoConf, ExpConf3NoConf, ExpConf3NoConf,ExpConF3NoConf, ExpConi~JNoConf, ExpConf3NoColff, ExpConf3NoConf, ExpConf3Figure 4: Exploratory portion of EIC strategy.tions in the indicated state, to maximize xplorationand minimize data sparseness when constructing ourmodel.
Since there are 42 states with 2 choices each,there is n search space of 242 potential global di-alogue strategies; the goal of RL is to identify anapparently optimal strategy fl'om this large searchspace.
Note that due to the randomization of theEIC strategy, the prompts are designed to ensurethe coherence of all possible action sequences.Figure 5 illustrates how the dialogue strategy inFigure 4 generates the diMogue in Figure 1.
Eachrow indicates the state that NJFun is in, the ac-505State Action %Irn Rewardgacvtgh0100000 GreetU $1 01121000 NoConf 01221001 ExpConf2 $2 01 3 2 1 0 0 1 ExpConf3 $3 01400000 Tell $4 1Figure 5: Generating the dialogue in Figure 1.tion executed in this state, the corresponding turnin Figure 1, and the reward received.
The initialstate represents that NaFun will first attempt o ob-tain attribute 1.
NJFun executes GreetU (althoughas shown in Figure 4, GreetS is also possible), gen-erating the first utterance in Figure 1.
Alter theuser's response, the next state represents that N J-Fun has now greeted the user and obtained the ac-tivity value with high confidence, by using a non-restrictive grmnmar.
NJFnn then chooses the No-Conf strategy, so it does not attempt to confirmthe activity, which causes the state to change butno prompt to be generated.
The third state repre-sents that NJFun is now working on the second at-tribute (location), that it already has this vahle withhigh confidence (location was obtained with activityafter the user's first utterance), and that the dia-logue history is good.
4 This time NaFun chooses theExpConf2 strategy, and confirms the attribute withthe second NJFun utterance, and the state changesagain.
The processing of time is similar to that of lo-cation, which leads NJFun to the final state, where itperforms the action "Tell" (corresponding to query-ing the database, presenting the results to the user,and asking the user to provide a reward).
Note thatin NJFun, the reward is always 0 except at the ter-minal state, as shown in the last column of Figure 5.4 Experimental ly Optimizing aStrategyWe collected experimental dialogues for both train-ing and testing our system.
To obtain training di-alogues, we implemented NJFun using the EIC dia-logue strategy described in Section 3.
We used thesedialogues to build an empirical MDP, and then com-puted the optimal dialogue strategy in this MDP (asdescribed in Section 2).
In this section we describeour experimental design and the learned dialoguestrategy.
In the next section we present results fromtesting our learned strategy and show that it im-proves task completion rates, the performance mea-sure we chose to optimize.Experimental subjects were employees not associ-a, ted with the NJFun project.
There were 54 sub-4Recall that only the current attribute's features are ill thestate, lIowever, the operations vector contains informationregarding previous attributes.jects for training and 21 for testing.
Subjects weredistributed so tile training and testing pools werebalanced for gender, English as a first language, andexpertise with spoken dialogue systems.During both training and testing, subjects carriedout free-form conversations with NJFun to completesix application tasks.
For examl)le , the task exe-cuted by the user in Figure 1 was: "You feel thirstyand want to do some winetasting in the morning.Are there any wineries (;lose by your house in Lam-bertville?"
Subjects read task descriptions on a webpage, then called NJFun from their office phone.At the end of the task, NJFun asked for feedbackon their experience (e.g., utterance $4 in Figure 1).Users then hung up the phone and filled out a usersurvey (Singh et al, 2000) on the web.The training phase of the experiment resulted in311 complete dialogues (not all subjects completedall tasks), for which NJFun logged the sequenceof states and the corresponding executed actions.The number of samples per st~tte for the initi~fl askchoices are:0 1 0 0 0 0 0 GreetS=IS5 GreetU=1561 2 0 0 0 0 0 Ask2S=93 Ask2U=721 2 0 0 0 0 1 Ask2S=36 Ask2U=48Such data illustrates that the random action choicestrategy led to a fairly balanced action distributionper state.
Similarly, the small state space, and thefact that we only allowed 2 action choices per state,prevented a data sparseness problem.
The first statein Figure 4, the initial state for every dialogue, wasthe most frequently visited state (with 311 visits).Only 8 states that occur near the end of a dialoguewere visited less tlmn 10 times.The logged data was then used to construct heempirical MDP.
As we have mentioned, the measurewe chose to optinfize is a binary reward flmctionbased on the strongest possible measure of task com-pletion, called S t rongComp,  that takes on value1 if NJFun queries the database using exactly theattributes pecified in the task description, and 0otherwise.
Then we eoml)uted the optimal dialoguestrategy in this MDP using RL (cf.
Section 2).
Theaction choices constituting the learned strategy arein boldface in Figure 4.
Note that no choice wasfixed for several states, inealfing that the Q-valueswere identical after value iteration.
Thus, even whenusing the learned strategy, NJFun still sometimeschooses randomly between certain action pairs.Intuitively, the learned strategy says that the op-timal use of initiative is to begin with user initia-tive, then back off to either mixed or system ini-tiative when reasking for an attribute.
Note, how-ever, that the specific baekoff method differs withattribute (e.g., system initiative for attribute 1, butgcnerMly mixed initiative for attribute 2).
Withrespect to confirmation, the optimal strategy is to506mainly contirm at lower contidenee -values.
Again,however, the point where contirlnation becomes un-necessary difl'ers across attributes (e.g., confidencelevel 2 for attribute 1, but sometimes lower levelsfor attributes 2 and 3), and  also dt!txmds on otherfeatures of the state besides confidence (e.g., gram-mar and history).
This use (if ASP, (:ontidence.
by thedialogue strategy is more Sol)hisli('ated than previ-ous al)proaches, e.g.
(Niimi and Kot)ayashi, 1996;Lit\]nan and Pan, 2000).
N.lI,'un ('an learn such line-grained distinctions l}ecause the el)ritual strategy isbased on a eonll)arisoi) of 24~ l}ossible exl}h)ratorystrategies.
Both the initiative and confirmation re-suits sugge.sl that the begimfing of the dialogue wasthe most problenmtie for N.lli'un.
Figure I ix an ex-ample dialogue using the Ol)tilnal strategy.5 Experimentally Evaluating theStrategyFor the testing i)\]tase, NJFun was reilnplemented touse the learned strategy.
2:t test sul)je(;Is then per-formed the same 6 tasks used during training, re-sulling in 124 complete test dialogues.
()he of ourmain resull;s is that task completion its measured byStrongCom 11 increased front 52cX} in training 1o 64%in testing (p < .06))There is also a signilicant in~twaction (!II'(~c.tbetween strategy nnd task (p<.01) for Strong-Colnl).
\]'revious work has suggested l;hat novic(~users l)erform (:Oml)arably to eXl)erts after only 2tasks (Kamm et ill., \] 9!18).
Sill('e Ollr \]oarllt}d sl.rat-egy was based on 6 tasks with each user, one (?xpla-nation of the interaction eft'cot is that the learnc.dstrategy is slightly optimized for expert users.
~lbexplore this hyi)othesis, we divided our corpus intodiah)gues with "novice" (tasks \] and 2) and "ex-pert" (tasks 3-6) users.
We fOltltd that the learnedstrategy did in fact lc'a(l to a large an(1 significantimprovement in StrongComp tbr (;Xl)erts (EIC=.d6,learned==.69, 11<.001), and a non-signilieant degra-dation for novices (1,31C=.66, learned=.55, 11<.3).An apparent limitation of these results is that EICmay not 1)e the best baseline strategy tbr coral)arisento our learned strategy.
A more standard alternativewould be comparison to the very best hand-designedfixed strategy.
However, there is no itgreement in theliterature, nor amongst he authors, its to what the1)est hand-designed strategy might have been.
Thereis agreement, however, that the best strategy ix sen-sitive to lnally unknown and unmodeled factors: theaThe ('.xlmrimental design (lescribed above Colmists of 2factors: the within-in groul) fa(:tor sl~ntefly aim the l)etween-groui)s facl;or task.
\,Ve 11812, ~1, l,WO-~,g~l,y D.llO.ly,qiS of variance(ANOVA) to comtmte wlmtlmr main and int(!raction (!flk!ctsof strategy are statistically signitica nt (t)<.05) or indicativeof a statistical trend (p < .101.
Main effe.cts of strategy aretask-in(lel)endent , while interaction eIt'(!cts involving strat(%yare task-dependent.~4(~aSIlIX~StrongComp\VcakCompASRFecdlmckUserSatEIC(n=:31110.521.752.500.181.3.38v _ _l~eatned p(n=124)0.642.19 .022.67 .040.11 .d213.29 .86Table 1: Main ett'ects of dialogue strategy.user 1)olmlation, the specitics of the, task, the 1)ar-ticular ASR used, etc.
Furthernlore, \]P, IC was (:are-fully designed so that the random choices it makesnever results in tm unnatural dialogue.
Finally, acompanion paper (Singh et al, 2000) shows that the1)erforntanee of the learned strategy is better tha l lseveral "stmtdard" fixed strategies (such as alwaysuse system-initiative and no-confirmation).Although many types of measures have been usedto evaluate dialogue systems (e.g., task success,dialogue quality, ettit:ieney, usability (l)anieli andGerbino, 1995; Kamm et al, 11998)), we optimizedonly tbr one task success measure, StrongConll).Ilowever, we also examined the 1)erl 'ornmnee of thelearned strategy using other ewduation measures(which t)ossibly could have llo011 used its our  rewardfunction).
WeakComp is a relaxed version of taskcomt)letion that gives partial credit: if all attributevalues are either correct or wihh:ards, the value is thesum of the correct munl)er of attrilmtes.
()tlmrwise,at least one attribute is wrong (e.g., the user says"Lanfl)ertvilhf' but the system hears "Morristown"),and the wdue is -1.
ASR is a dialogue quality lllea-sure that itl)l)roxinmtes Sl)eech recognition act:uracyfor tl,e datM)ase query, a.nd is computed 1:) 3, adding1 for each correct attribute value altd .5 for everywihtca.rd.
Thus, if the task ix to go winetastingnear Lambertville in the morning, and the systenlqueries the database for an activity in New Jerseyin the morning, StrongComp=0, \VeakComp=l, andASR=2.
In addition to the objective measures dis-cussed a,bove, we also COmlmted two subjective us-ability measures.
Feedback  is obtained front thedialogue (e.g.
$4 in Figure 5), by mapping good,so-so, bad to 1, 0, m~d -1, respectively.
User satis-faction (UserSat, ranging front 0-20) is obtained bysumming the answers of the web-based user survey.Table I summarizes the diflhrence in performanceof NJFun tbr our original reward flmction and theabove alternative valuation measures, from trail>ing (EIC) to test (learned strategy for StrongComp).For WeakComp, the average reward increased from1.75 to 2.19 (p < 0.02), while tbr ASll the averagereward increased from 2.5 to 2.67 (p < 0.04).
Again,these iml)rovements occur even though the learnedstrategy was not optilnized for these measures.The last two rows of the table show that for the507subjective measures, i)erformmme does not signifi-cantly differ for the EIC and learned strategies.
In-terestingly, the distributions of the subjective mea-sures move to the middle from training to testing,i.e., test users reply to the survey using less extremeanswers than training users.
Explaining the subjec-tire results is an area for future work.6 DiscussionThis paper presents a practical methodology for ap-plying RL to optimizing dialogue strategies in spo-ken dialogue systems, and shows empirically that themethod improves performance over the EIC strategyin NJFun.
A companion paper (Singh et al, 2000)shows that the learned strategy is not only betterthan EIC, but also better than other fixed choicesproposed in the literature.
Our results demonstratethat the application of RL allows one to empiricallyoptimize a system's dialogue strategy by searchingthrough a much larger search space than can be ex-plored with more traditional lnethods (i.e.
empiri-cally testing several versions of a systent).RL has been appled to dialogue systems in pre-vious work, but our approach ditlhrs from previouswork in several respects.
Biermann and Long (1996)did not test RL in an implemented system, and theexperiments of Levin et 31.
(2000) utilized a simu-lated user model.
Walker et al (1998)'s methodol-ogy is similar to that used here, in testing RL withan imt)lelnented system with human users.
Howeverthat work only explored strategy choices at 13 statesin the dialogue, which conceivably could have beenexplored with more traditional methods (~ts com-pared to the 42 choice states explored here).We also note that our learned strategy made di-alogue decisions based on ASR confidence in con-junction with other features, mid alto varied initia-tive and confirmation decisions at a finer grain thanprevious work; as such, our learned strategy is not;a standard strategy investigated in the dialogue sys-teln literature.
For example, we would not have pre-dicted the complex and interesting back-off strategywith respect o initiative when reasking for an at-tribute.To see how our method scales, we are al)plying RLto dialogue systems for eustolner care and tbr travelplanning, which are more complex task-oriented do-mains.
As fllture work, we wish to understandthe aforementioned results on the subjective rewardmeasures, explore the potential difference betweenoptimizing tbr expert users and novices, automatethe choice of state space for dialogue systems, ilwes-tigate the use of a learned reward function (Walkeret al, 1998), and explore the use of more informativenon-terminal rewards.AcknowledgementsThe authors thank Fan Jiang for his substantial effortin implenmnting NJFun, Wieland Eckert, Esther Levin,Roberto Pieraccini, and Mazin R.ahinl for their technicalhelp, Julia Hirsehberg for her comments on a draft of thispaper, and David McAllester, I~ichard Sutton, EstherLevin and Roberto Pieraccini for hell)tiff conversations.ReferencesA.
W. Biermann and P. M. Long.
1996.
The compositionof messages in sl)eeeh-graphies interactive systems.
InProe.
of the International Symposium on Spoken Dia-logue, pages 97 100.M.
Danieli and E. Gerbino.
1995.
Metrics for evaluatingdialogue strategies in a spoken language system.
InP~vc.
of the AAAI  Spring Symposium on EmpiricalMethods in Discourse Interpretation and Generation,pages 34 39.C.
Kamm, D. Litman, and M. A. Walker.
1998.
Fromnovice to expert: The effect of tutorials on user exl)er-tise with spoken dialogue systems.
In P~vc.
of the In-ternational Conference on Spolccn Language P~vccss-in.q, ICSLP98.E.
Levin, R. Pieraccini, W. Eekere, G. Di Fabbrizio, andS.
Narayanan.
1999.
Spoken language dialogue: lh'omtheory to practice.
In Pwc.
IEEE Workshop on Au-tomatic Speech R.ecognition and Understanding, AS-R U U99.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A stochas-tic model of human machine interaction for learningdialog strategies.
IEEE TTnnsactions on Speech andAudio Processing, 8(1):11-23.D.
J. Litman and S. Pan.
2000.
Predicting and adaptingto poor Sl)eech recognition in a spoken dialogue sys-tern.
In Proc.
of the Scv('ntccnth National Confl:rcnccon Artificial Intclligcncc, AAAI-2000.Y.
Niimi and Y. Kobayashi.
1996.
A dialog control strat-egy based on the reliability of speech recognition.
InProc.
of the International Symposium on Spoken Dia-loguc, pages 157--160.A.
Sanderman, J. Sturm, E. den Os, L. Boves, andA.
Cremers.
1998.
Evaluation of the dutchtraintimetable inibrmation system developed in the ariseproject.
In Interactive Voice Technology for Tclccom-munications Applications, IVT2'A, pages 91-96.S.
Singh, M. S. Kearns, D. J. Litman, and M. A.
\Valker.1999.
Reinforcement learning for spoken dialogue sys-tems.
In Proc.
NIPS99.S.
B. Singh, M. S. Kearns, D. J. Litman, andM.
A. Walker.
2000.
Empirical evaluation of a rein-forccment learning spoken dialogue system.
In Proc.of thc Scvcntccnth National Conference on ArtificialIntelligence, AAAI-2000.R.
S. Sutton.
1991.
Plamfing by incremental dynamicprogramming.
In Proc.
Ninth Confcwztcc on MachineLearning, pages 353-357.M.
A. Walker, J. C. Promer, and S. Narayanan.
1998.Learning optimal dialogue strategies: A ease study ofa Sl)oken dialogue agent br email.
In P~vc.
of the 36thAnnual Meeting of the Association of ComputationalLinguistics, COLING//ACL 98, pages 1345 1352.508
