Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642?646,Dublin, Ireland, August 23-24, 2014.The Meaning Factory: Formal Semantics for Recognizing TextualEntailment and Determining Semantic SimilarityJohannes BjervaUniv.
of Groningenj.bjerva@rug.nlJohan BosUniv.
of Groningenjohan.bos@rug.nlRob van der GootUniv.
of Groningenr.van.der.goot@rug.nlMalvina NissimUniv.
of Bolognamalvina.nissim@unibo.itAbstractShared Task 1 of SemEval-2014 com-prised two subtasks on the same datasetof sentence pairs: recognizing textual en-tailment and determining textual similar-ity.
We used an existing system based onformal semantics and logical inference toparticipate in the first subtask, reachingan accuracy of 82%, ranking in the top5 of more than twenty participating sys-tems.
For determining semantic similar-ity we took a supervised approach using avariety of features, the majority of whichwas produced by our system for recogniz-ing textual entailment.
In this subtask oursystem achieved a mean squared error of0.322, the best of all participating systems.1 IntroductionThe recent popularity of employing distributionalapproaches to semantic interpretation has also leadto interesting questions about the relationship be-tween classic formal semantics (including its com-putational adaptations) and statistical semantics.A promising way to provide insight into thesequestions was brought forward as Shared Task 1 inthe SemEval-2014 campaign for semantic evalua-tion (Marelli et al., 2014).
In this task, a system isgiven a set of sentence pairs, and has to predict foreach pair whether the sentences are somehow re-lated in meaning.
Interestingly, this is done usingtwo different metrics: the first stemming from theformal tradition (contradiction, entailed, neutral),and the second in a distributional fashion (a simi-larity score between 1 and 5).
We participated inthis shared task with a system rooted in formal se-mantics.
In particular, we were interested in find-ing out whether paraphrasing techniques could in-crease the accuracy of our system, whether mean-ing representations used for textual entailment areThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/useful for predicting semantic similarity, and con-versely, whether similarity features could be usedto boost accuracy of recognizing textual entail-ment.
In this paper we outline our method andpresent the results for both the textual entailmentand the semantic similarity task.12 Recognizing Textual Entailment2.1 OverviewThe core of our system for recognizing textual en-tailment works as follows: (i) produce a formal se-mantic representation for each sentence for a givensentence pair; (ii) translate these semantic repre-sentations into first-order logic; (iii) use off-the-shelf theorem provers and model builders to checkwhether the first sentence entails the second, orwhether the sentences are contradictory.
This isessentially an improved version of the frameworkintroduced by Bos & Markert (2006).To generate background knowledge that couldassist in finding a proof we used the lexicaldatabase WordNet (Fellbaum, 1998).
We alsoused a large database of paraphrases (Ganitkevitchet al., 2013) to alter the second sentence in case noproof was found at the first attempt, inspired byBosma & Callison-Burch (2006).
The core sys-tem reached high precision on entailment and con-tradiction.
To increase recall, we used a classifiertrained on the output from our similarity task sys-tem (see Section 3) to reclassify the ?neutrals?
intopossible entailments.2.2 TechnicalitiesThe semantic parser that we used is Boxer (Bos,2008).
It is the last component in the pipeline ofthe C&C tools (Curran et al., 2007), comprisinga tokenizer, POS-tagger, lemmatizer (Minnen et1To reproduce these results in a linux environment (withSWI Prolog) one needs to install the C&C tools (this in-cludes Boxer and the RTE system), the Vampire theoremprover, the two model builders Paradox and Mace-2, and thePPDB-1.0 XL database.
Detailed instructions can be found inthe src/scripts/boxer/sick/README folder of theC&C tools.642al., 2001), and a robust parser for CCG (Steed-man, 2001).
Boxer produces semantic represen-tations based on Discourse Representation Theory(Kamp and Reyle, 1993).
We used the standardtranslation from Discourse Representation Struc-tures to first-order logic, rather than the one basedon modal first-order logic (Bos, 2004), since theshared task data did not contain any sentences withpropositional argument verbs.After conversion to first-order logic, wechecked with the theorem prover Vampire (Ri-azanov and Voronkov, 2002) whether a proofcould be found for the first sentence entailing thesecond, and whether a contradiction could be de-tected for the conjunction of both sentences trans-lated into first-order logic.
If neither a proof nora contradiction could be found within 30 seconds,we used the model builder Paradox (Claessen andS?orensson, 2003) to produce a model of the twosentences separately, and one of the two sentencestogether.
However, even though Paradox is an ef-ficient piece of software, it does not always returnminimal models with respect to the extensions ofthe non-logical symbols.
Therefore, in a secondstep, we asked the model builder Mace-2 (Mc-Cune, 1998) to construct a minimal model for thedomain size established by Paradox.
These mod-els are used as features in the similarity task (Sec-tion 3).Background knowledge is important to increaserecall of the theorem prover, but hard to acquireautomatically (Bos, 2013).
Besides translating hy-pernym relations of WordNet to first-order logicaxioms, we also reasoned that it would be benefi-cial to have a way of dealing with multi-word ex-pressions.
But instead of translating paraphrasesinto axioms, we used them to rephrase the inputsentence in case no proof or contradiction wasfound for the original sentence pair.
Given a para-phrase SRC7?TGT, we rephrased the first sen-tence of a pair only if SRC matches with up tofour words, no words of TGT were already in thefirst sentence, and every word of TGT appeared inthe second sentence.
The paraphrases themselveswere taken from PPDB-1.0 (Ganitkevitch et al.,2013).
In the training phrase we found that the XLversion (comprising o2m, m2o, phrasal, lexical)gave the best results (using a larger version causeda strong decrease in precision, while smaller ver-sions lead to a decrease in recall).We trained a separate classifier in order to re-classify items judged by our RTE system as be-ing neutral.
This classifier uses a single feature,namely the relatedness score for each sentencepair.
As training material, we used the gold relat-edness scores from the training and trial sets.
Forclassification of the test set, we used the related-ness scores obtained from our Semantic Similaritysystem (see Section 3).
The classifier is a SupportVector Machine classifier, in the implementationprovided by Scikit-Learn (Pedregosa et al., 2011),based on the commonly used implementation LIB-SVM (Chang and Lin, 2011).
We used the imple-mentation?s standard parameters.2.3 ResultsWe submitted two runs.
The first (primary) runwas produced by a configuration that included re-classifying the ?neutrals?.
The second run is with-out the reclassification of the neutrals.
After sub-mission we ran a system that did not use the para-phrasing technique in order to measure what in-fluence the PPDB had on our performance.
Theresults are summarized in Table 1.
In the train-ing phase we got the best results for the configu-ration using the PPDB and reclassication, whichwas submitted as our primary run.Table 1: Results on the entailment task for varioussystem configurations.System Configuration Accuracymost frequent class baseline 56.7?PPDB, ?reclassification 77.6+PPDB, ?reclassification 79.6+PPDB, +reclassification 81.6In sum, our system for recognizing entailmentperformed well reaching 82% accuracy and byfar outperforming the most-frequent class baseline(Table 1).
We show some selected examples illus-trating the strengths of our system below.Example 1627 (ENTAILMENT)A man is mixing a few ingredients in a bowlSome ingredients are being mixed in a bowl by a personExample 2709 (CONTRADICTION)There is no person boiling noodlesA woman is boiling noodles in waterExample 9051 (ENTAILMENT)A pair of kids are sticking out blue and green colored tonguesTwo kids are sticking out blue and green colored tonguesA proof for entailment is found for Ex.
1627,because for passive sentences Boxer producesa meaning representation equivalent to their ac-tive variants.
A contradiction is detected forEx.
2709 because of the way negation is han-dled by Boxer.
Both examples trigger backgroundknowledge from WordNet hyperonyms (man ?person; woman ?
person) that is used in the643proofs.2Ex.
9051 shows how paraphrasing helps,here ?a pair of?
7?
?two?.3 Determining Semantic Similarity3.1 OverviewThe Semantic Similarity system follows a super-vised approach to solving the regression problemof determining the similarity between each givensentence pair.
The system uses a variety of fea-tures, ranging from simpler ones such as wordoverlap, to more complex ones in the form ofdeep semantic features and features derived from acompositional distributional semantic model.
Themajority of these features are derived from themodels from our RTE system (see Section 2).3.2 Technicalities3.2.1 RegressorThe regressor used is a Random Forest Regressorin the implementation provided by Scikit-Learn(Pedregosa et al., 2011).
Random forests are ro-bust with respect to noise and do not overfit easily(Breiman, 2001).
These two factors make them ahighly suitable choice for our approach, since weare dealing with a relatively large number of weakfeatures, i.e., features which may be seen as indi-vidually containing a rather small amount of infor-mation for the problem at hand.Our parameter settings for the regressor is fol-lows.
We used a total of 1000 trees, with a maxi-mum tree depth of 20.
At each node in a tree theregressor looked at maximum 3 features in orderto decide on the split.
The quality of each suchsplit is determined using mean squared error asmeasure.
These parameter values were optimisedwhen training on the training set, with regards toperformance on the trial set.3.2.2 Feature overviewWe used a total of 32 features for our regres-sor.
Due to space constraints, we have sub-dividedour features into groups by the model/method in-volved.
For all features we compared the outcomeof the original sentence pair with the outcome ofthe paraphrased sentence pairs (see Section 2.2)3.If the paraphrased sentence pair yielded a higherfeature overlap score than the original sentencepair, we utilized the former.
In other words, we2In the training data around 20% of the proofs for entail-ment were established with the help of WordNet, but only 4%for detecting contradictions.3In addition to the PPDB we added handling of negations,by removing some negations {not, n?t} and substituting oth-ers {no:a, none:some, nobody:somebody}.assume that the sentence pair generated with para-phrases is a good representation of the originalpair, and that similarities found here are an im-provement on the original score.Logical model We used the logical models cre-ated by Paradox and Mace for the two sentencesseparately, as well as a combined model (see Sec-tion 2.2).
The features extracted from this modelare the proportion of overlap between the in-stances in the domain, and the proportion of over-lap between the relations in the model.Noun/verb overlap We first extracted and lem-matised all nouns and verbs from the sentencepairs.
With these lemmas we calculated two newseparate features, the overlap of the noun lemmasand the overlap of the verb lemmas.Discourse Representation Structure (DRS)The two most interesting pieces of informationwhich easily can be extracted from the DRS mod-els are the agents and patients.
We first extractedthe agents for both sentences in a sentence pair,and then computed the overlap between the twolists of agents.
Secondly, since all sentences in thecorpus have exactly one patient, we extracted thepatient of each sentence and used this overlap as abinary feature.Wordnet novelty We build one tree containingall WordNet concepts included in the first sen-tence, and one containing all WordNet conceptsof both sentences together.
The difference in sizebetween these two trees is used as a feature.RTE The result from our RTE system (entail-ment, neutral or contradiction) is used as a feature.Compositional Distributional Semantic ModelOur CDSM feature is based on word vectors de-rived using a Skip-Gram model (Mikolov et al.,2013a; Mikolov et al., 2013b).
We used the pub-licly available word2vec4tool to calculate thesevectors.
We trained the tool on a data set con-sisting of the first billion characters of Wikipedia5and the English part of the French-English 109corpus used in the wmt11 translation task6.
TheWikipedia section of the data was pre-processedusing a script7which made the text lower case, re-moved tables etc.
The second section of the datawas also converted to lower case prior to training.We trained the vectors using the following pa-rameter settings.
Vector dimensionality was set4code.google.com/p/word2vec/5mattmahoney.net/dc/enwik9.zip6statmt.org/wmt11/translation-task.html#download7mattmahoney.net/dc/textdata.html644Table 2: Pearson correlation and MSE obtained on the test set for each feature group in isolation.Feature group p [?PPDB] p [+PPDB] MSE [?PPDB] MSE [+PPDB]Logical model 0.649 0.737 0.590 0.476Noun/verb overlap 0.647 0.676 0.592 0.553DRS 0.634 0.667 0.610 0.569Wordnet novelty 0.652 0.651 0.590 0.591RTE 0.621 0.620 0.626 0.627CDSM 0.608 0.609 0.681 0.679IDs 0.493 0.493 0.807 0.807Synset 0.414 0.417 0.891 0.889Word overlap 0.271 0.340 0.944 0.902Sentence length 0.227 0.228 0.971 0.971All with IDs 0.836 0.842 0.308 0.297All without IDs 0.819 0.827 0.336 0.322to 1600 with a context window of 10 words.
Theskip-gram model with hierarchical softmax, and anegative sampling of 1e-3 was used.To arrive at the feature used for our regressor,we first calculated the element-wise sum of thevectors of each word in the given sentences.
Wethen calculated the cosine distance between thesentences in the sentence pair.IDs One surprisingly helpful feature was eachsentence pair?s ID in the corpus.8Since thisfeature clearly is not representative of what onewould have access to in a real-world scenario, itwas not included in the primary run.Synset Overlap We built one set for each sen-tence pair consisting of each possible lemma formof all possible noun synsets for each word.
Theproportion of overlap between the two resultingsets was then used as a feature.
Given cases whererelatively synonymous words are used (e.g.
kidand child), these will often belong to the samesynset, thus resulting in a high overlap score.Synset Distance We first generated each possi-ble word pair consisting of one word from eachsentence.
Using these pairings, we calculatedthe maximum path similarity between the nounsynsets available for these words.
This calculationis restricted so that each word in the first sentencein each pair is only used once.Word overlap Our word overlap feature wascalculated by first creating one set per sentence,containing each word occurring in that sentence.8We discovered that the ordering of the entire data set wasinformative for the prediction of sentence relatedness.
Wehave illustrated this by using the ordering of the sentences(i.e.
the sentence IDs) as a feature in our model, and therebyobtaining better results.
Relying on such a non-natural order-ing of the sentences would be methodologically flawed, andtherefore this feature was not used in our primary run.The four most common words in the corpus wereused as a stop list, and removed from each set.
Theproportion of overlap between the two sets wasthen used as our word overlap feature.Sentence Lengths The difference in length be-tween the sentence pairs proved to be a somewhatuseful feature.
Although mildly useful for this par-ticular data set, we do not expect this to be a par-ticularly helpful feature in real world applications.3.3 ResultsWe trained our system on 5000 sentence pairs, andevaluated it on 4927 sentence pairs.
Table 2 con-tains our scores for the evaluation, broken up perfeature group.
Our relatedness system yielded thehighest scores compared to all other systems inthis shared task, as measured by MSE and Spear-man correlation scores.
Although our system per-formed slightly worse as measured by Pearsoncorrelation, there is no significant difference to thescores obtained by the two higher ranked systems.4 ConclusionOur work shows that paraphrasing techniques canbe used to improve the results of a textual entail-ment system.
Additionally, the scores from oursemantic similarity measure could be used to im-prove the scores of the textual entailment system.Our work also shows that deep semantic featurescan be used to predict semantic relatedness.AcknowledgementsWe thank Chris Callison-Burch, Juri Ganitkevitch and ElliePavlick for getting the most out of PPDB.
We also thank ourcolleagues Valerio Basile, Harm Brouwer, Kilian Evang andNoortje Venhuizen for valuable comments and feedback.645ReferencesJohan Bos and Katja Markert.
2006.
Recognisingtextual entailment with robust logical inference.
InJoaquin Quinonero-Candela, Ido Dagan, BernardoMagnini, and Florence d?Alch?e Buc, editors, Ma-chine Learning Challenges, MLCW 2005, volume3944 of LNAI, pages 404?426.Johan Bos.
2004.
Computational Semantics in Dis-course: Underspecification, Resolution, and Infer-ence.
Journal of Logic, Language and Information,13(2):139?157.Johan Bos.
2008.
Wide-Coverage Semantic Analy-sis with Boxer.
In J. Bos and R. Delmonte, editors,Semantics in Text Processing.
STEP 2008 Confer-ence Proceedings, volume 1 of Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Johan Bos.
2013.
Is there a place for logic in rec-ognizing textual entailment?
Linguistic Issues inLanguage Technology, 9(3):1?18.Wauter Bosma and Chris Callison-Burch.
2006.
Para-phrase substitution for recognizing textual entail-ment.
In Proceedings of CLEF.Leo Breiman.
2001.
Random forests.
Machine learn-ing, 45(1):5?32.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology(TIST), 2(3):27.K.
Claessen and N. S?orensson.
2003.
New techniquesthat improve mace-style model finding.
In P. Baum-gartner and C. Ferm?uller, editors, Model Computa-tion ?
Principles, Algorithms, Applications (Cade-19 Workshop), pages 11?27, Miami, Florida, USA.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically Motivated Large-Scale NLP withC&C and Boxer.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions, pages 33?36, Prague,Czech Republic.Christiane Fellbaum, editor.
1998.
WordNet.
An Elec-tronic Lexical Database.
The MIT Press.Juri Ganitkevitch, Benjamin VanDurme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL 2013), Atlanta,Georgia, June.
Association for Computational Lin-guistics.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic; An Introduction to Modeltheoretic Seman-tics of Natural Language, Formal Logic and DRT.Kluwer, Dordrecht.M.
Marelli, L. Bentivogli, M. Baroni, R. Bernardi,S.
Menini, and R. Zamparelli.
2014.
Semeval-2014task 1: Evaluation of compositional distributionalsemantic models on full sentences through seman-tic relatedness and textual entailment.
In Proceed-ings of SemEval 2014: International Workshop onSemantic Evaluation.W.
McCune.
1998.
Automatic Proofs and Counterex-amples for Some Ortholattice Identities.
Informa-tion Processing Letters, 65(6):285?291.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of english.
Jour-nal of Natural Language Engineering, 7(3):207?223.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.A.
Riazanov and A. Voronkov.
2002.
The Design andImplementation of Vampire.
AI Communications,15(2?3):91?110.Mark Steedman.
2001.
The Syntactic Process.
TheMIT Press.646
