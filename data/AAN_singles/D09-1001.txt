Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1?10,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPUnsupervised Semantic ParsingHoifung Poon Pedro DomingosDepartment of Computer Science and EngineeringUniversity of WashingtonSeattle, WA 98195-2350, U.S.A.{hoifung,pedrod}@cs.washington.eduAbstractWe present the first unsupervised approachto the problem of learning a semanticparser, using Markov logic.
Our USPsystem transforms dependency trees intoquasi-logical forms, recursively induceslambda forms from these, and clustersthem to abstract away syntactic variationsof the same meaning.
The MAP semanticparse of a sentence is obtained by recur-sively assigning its parts to lambda-formclusters and composing them.
We evalu-ate our approach by using it to extract aknowledge base from biomedical abstractsand answer questions.
USP substantiallyoutperforms TextRunner, DIRT and an in-formed baseline on both precision and re-call on this task.1 IntroductionSemantic parsing maps text to formal meaningrepresentations.
This contrasts with semantic rolelabeling (Carreras and Marquez, 2004) and otherforms of shallow semantic processing, which donot aim to produce complete formal meanings.Traditionally, semantic parsers were constructedmanually, but this is too costly and brittle.
Re-cently, a number of machine learning approacheshave been proposed (Zettlemoyer and Collins,2005; Mooney, 2007).
However, they are super-vised, and providing the target logical form foreach sentence is costly and difficult to do consis-tently and with high quality.
Unsupervised ap-proaches have been applied to shallow semantictasks (e.g., paraphrasing (Lin and Pantel, 2001),information extraction (Banko et al, 2007)), butnot to semantic parsing.In this paper we develop the first unsupervisedapproach to semantic parsing, using Markov logic(Richardson and Domingos, 2006).
Our USP sys-tem starts by clustering tokens of the same type,and then recursively clusters expressions whosesubexpressions belong to the same clusters.
Ex-periments on a biomedical corpus show that thisapproach is able to successfully translate syntac-tic variations into a logical representation of theircommon meaning (e.g., USP learns to map activeand passive voice to the same logical form, etc.
).This in turn allows it to correctly answer manymore questions than systems based on TextRun-ner (Banko et al, 2007) and DIRT (Lin and Pantel,2001).We begin by reviewing the necessary back-ground on semantic parsing and Markov logic.
Wethen describe our Markov logic network for un-supervised semantic parsing, and the learning andinference algorithms we used.
Finally, we presentour experiments and results.2 Background2.1 Semantic ParsingThe standard language for formal meaning repre-sentation is first-order logic.
A term is any ex-pression representing an object in the domain.
Anatomic formula or atom is a predicate symbol ap-plied to a tuple of terms.
Formulas are recursivelyconstructed from atomic formulas using logicalconnectives and quantifiers.
A lexical entry de-fines the logical form for a lexical item (e.g., aword).
The semantic parse of a sentence is de-rived by starting with logical forms in the lexi-cal entries and recursively composing the mean-ing of larger fragments from their parts.
In tradi-tional approaches, the lexical entries and meaning-1composition rules are both manually constructed.Below are sample rules in a definite clause gram-mar (DCG) for parsing the sentence: ?Utah bor-ders Idaho?.V erb[?y?x.borders(x, y)]?
bordersNP [Utah]?
UtahNP [Idaho]?
IdahoV P [rel(obj)]?
V erb[rel] NP [obj]S[rel(obj)]?
NP [obj] V P [rel]The first three lines are lexical entries.
They arefired upon seeing the individual words.
For exam-ple, the first rule applies to the word ?borders?
andgenerates syntactic category Verb with the mean-ing ?y?x.borders(x, y) that represents the next-to relation.
Here, we use the standard lambda-calculus notation, where ?y?x.borders(x, y)represents a function that is true for any (x, y)-pair such that borders(x, y) holds.
The last tworules compose the meanings of sub-parts into thatof the larger part.
For example, after the firstand third rules are fired, the fourth rule fires andgenerates V P [?y?x.borders(x, y)(Idaho)]; thismeaning simplifies to ?x.borders(x, Idaho) bythe ?-reduction rule, which substitutes the argu-ment for a variable in a functional application.A major challenge to semantic parsing is syn-tactic variations of the same meaning, whichabound in natural languages.
For example, theaforementioned sentence can be rephrased as?Utah is next to Idaho,?
?Utah shares a border withIdaho,?
etc.
Manually encoding all these varia-tions into the grammar is tedious and error-prone.Supervised semantic parsing addresses this issueby learning to construct the grammar automati-cally from sample meaning annotations (Mooney,2007).
Existing approaches differ in the meaningrepresentation languages they use and the amountof annotation required.
In the approach of Zettle-moyer and Collins (2005), the training data con-sists of sentences paired with their meanings inlambda form.
A probabilistic combinatory cate-gorial grammar (PCCG) is learned using a log-linear model, where the probability of the finallogical form L and meaning-derivation tree Tconditioned on the sentence S is P (L, T |S) =1Zexp (?iwifi(L, T, S)).
Here Z is the normal-ization constant and fiare the feature functionswith weights wi.
Candidate lexical entries are gen-erated by a domain-specific procedure based onthe target logical forms.The major limitation of supervised approachesis that they require meaning annotations for ex-ample sentences.
Even in a restricted domain,doing this consistently and with high quality re-quires nontrivial effort.
For unrestricted text, thecomplexity and subjectivity of annotation render itessentially infeasible; even pre-specifying the tar-get predicates and objects is very difficult.
There-fore, to apply semantic parsing beyond limited do-mains, it is crucial to develop unsupervised meth-ods that do not rely on labeled meanings.In the past, unsupervised approaches have beenapplied to some semantic tasks, but not to seman-tic parsing.
For example, DIRT (Lin and Pan-tel, 2001) learns paraphrases of binary relationsbased on distributional similarity of their argu-ments; TextRunner (Banko et al, 2007) automati-cally extracts relational triples in open domains us-ing a self-trained extractor; SNE applies relationalclustering to generate a semantic network fromTextRunner triples (Kok and Domingos, 2008).While these systems illustrate the promise of un-supervised methods, the semantic content they ex-tract is nonetheless shallow and does not constitutethe complete formal meaning that can be obtainedby a semantic parser.Another issue is that existing approaches to se-mantic parsing learn to parse syntax and semanticstogether.1 The drawback is that the complexityin syntactic processing is coupled with semanticparsing and makes the latter even harder.
For ex-ample, when applying their approach to a differentdomain with somewhat less rigid syntax, Zettle-moyer and Collins (2007) need to introduce newcombinators and new forms of candidate lexicalentries.
Ideally, we should leverage the enormousprogress made in syntactic parsing and generatesemantic parses directly from syntactic analysis.2.2 Markov LogicIn many NLP applications, there exist rich rela-tions among objects, and recent work in statisti-cal relational learning (Getoor and Taskar, 2007)and structured prediction (Bakir et al, 2007) hasshown that leveraging these can greatly improveaccuracy.
One of the most powerful representa-tions for this is Markov logic, which is a proba-bilistic extension of first-order logic (Richardsonand Domingos, 2006).
Markov logic makes it1The only exception that we are aware of is Ge andMooney (2009).2possible to compactly specify probability distri-butions over complex relational domains, and hasbeen successfully applied to unsupervised corefer-ence resolution (Poon and Domingos, 2008) andother tasks.
A Markov logic network (MLN) isa set of weighted first-order clauses.
Togetherwith a set of constants, it defines a Markov net-work with one node per ground atom and one fea-ture per ground clause.
The weight of a featureis the weight of the first-order clause that origi-nated it.
The probability of a state x in such anetwork is given by the log-linear model P (x) =1Zexp (?iwini(x)), where Z is a normalizationconstant, wiis the weight of the ith formula, andniis the number of satisfied groundings.3 Unsupervised Semantic Parsing withMarkov LogicUnsupervised semantic parsing (USP) rests onthree key ideas.
First, the target predicate and ob-ject constants, which are pre-specified in super-vised semantic parsing, can be viewed as clustersof syntactic variations of the same meaning, andcan be learned from data.
For example, bordersrepresents the next-to relation, and can be viewedas the cluster of different forms for expressing thisrelation, such as ?borders?, ?is next to?, ?share theborder with?
; Utah represents the state of Utah,and can be viewed as the cluster of ?Utah?, ?thebeehive state?, etc.Second, the identification and clustering of can-didate forms are integrated with the learning formeaning composition, where forms that are usedin composition with the same forms are encour-aged to cluster together, and so are forms that arecomposed of the same sub-forms.
This amounts toa novel form of relational clustering, where clus-tering is done not just on fixed elements in rela-tional tuples, but on arbitrary forms that are builtup recursively.Third, while most existing approaches (manualor supervised learning) learn to parse both syn-tax and semantics, unsupervised semantic pars-ing starts directly from syntactic analyses and fo-cuses solely on translating them to semantic con-tent.
This enables us to leverage advanced syn-tactic parsers and (indirectly) the available rich re-sources for them.
More importantly, it separatesthe complexity in syntactic analysis from the se-mantic one, and makes the latter much easier toperform.
In particular, meaning composition doesnot require domain-specific procedures for gener-ating candidate lexicons, as is often needed by su-pervised methods.The input to our USP system consists of de-pendency trees of training sentences.
Comparedto phrase-structure syntax, dependency trees arethe more appropriate starting point for semanticprocessing, as they already exhibit much of therelation-argument structure at the lexical level.USP first uses a deterministic procedure to con-vert dependency trees into quasi-logical forms(QLFs).
The QLFs and their sub-formulas havenatural lambda forms, as will be described later.Starting with clusters of lambda forms at the atomlevel, USP recursively builds up clusters of largerlambda forms.
The final output is a probabilitydistribution over lambda-form clusters and theircompositions, as well as the MAP semantic parsesof training sentences.In the remainder of the section, we describethe details of USP.
We first present the procedurefor generating QLFs from dependency trees.
Wethen introduce their lambda forms and clusters,and show how semantic parsing works in this set-ting.
Finally, we present the Markov logic net-work (MLN) used by USP.
In the next sections, wepresent efficient algorithms for learning and infer-ence with this MLN.3.1 Derivation of Quasi-Logical FormsA dependency tree is a tree where nodes are wordsand edges are dependency labels.
To derive theQLF, we convert each node to an unary atom withthe predicate being the lemma plus POS tag (be-low, we still use the word for simplicity), and eachedge to a binary atom with the predicate beingthe dependency label.
For example, the node forUtah becomes Utah(n1) and the subject depen-dency becomes nsubj(n1, n2).
Here, the niareSkolem constants indexed by the nodes.
The QLFfor a sentence is the conjunction of the atoms forthe nodes and edges, e.g., the sentence above willbecome borders(n1) ?
Utah(n2) ?
Idaho(n3) ?nsubj(n1, n2) ?
dobj(n1, n3).3.2 Lambda-Form Clusters and SemanticParsing in USPGiven a QLF, a relation or an object is repre-sented by the conjunction of a subset of the atoms.For example, the next-to relation is representedby borders(n1)?
nsubj(n1, n2)?
dobj(n1, n3),and the states of Utah and Idaho are represented3by Utah(n2) and Idaho(n3).
The meaning com-position of two sub-formulas is simply their con-junction.
This allows the maximum flexibility inlearning.
In particular, lexical entries are no longerlimited to be adjacent words as in Zettlemoyer andCollins (2005), but can be arbitrary fragments in adependency tree.For every sub-formula F , we define a corre-sponding lambda form that can be derived by re-placing every Skolem constant nithat does notappear in any unary atom in F with a uniquelambda variable xi.
Intuitively, such constantsrepresent objects introduced somewhere else (bythe unary atoms containing them), and corre-spond to the arguments of the relation repre-sented by F .
For example, the lambda formfor borders(n1) ?
nsubj(n1, n2) ?
dobj(n1, n3)is ?x2?x3.
borders(n1) ?
nsubj(n1, x2) ?dobj(n1, x3).Conceptually, a lambda-form cluster is a set ofsemantically interchangeable lambda forms.
Forexample, to express the meaning that Utah bor-ders Idaho, we can use any form in the clusterrepresenting the next-to relation (e.g., ?borders?,?shares a border with?
), any form in the clusterrepresenting the state of Utah (e.g., ?the beehivestate?
), and any form in the cluster representingthe state of Idaho (e.g., ?Idaho?).
Conditionedon the clusters, the choices of individual lambdaforms are independent of each other.To handle variable number of arguments, wefollow Davidsonian semantics and further de-compose a lambda form into the core form,which does not contain any lambda variable(e.g., borders(n1)), and the argument forms,which contain a single lambda variable (e.g.,?x2.nsubj(n1, x2) and ?x3.dobj(n1, x3)).
Eachlambda-form cluster may contain some number ofargument types, which cluster distinct forms of thesame argument in a relation.
For example, in Stan-ford dependencies, the object of a verb uses the de-pendency dobj in the active voice, but nsubjpassin passive.Lambda-form clusters abstract away syntacticvariations of the same meaning.
Given an in-stance of cluster T with arguments of argumenttypes A1, ?
?
?
, Ak, its abstract lambda form is givenby ?x1?
?
?
?xk.T(n) ?
?ki=1Ai(n, xi).Given a sentence and its QLF, semantic pars-ing amounts to partitioning the atoms in the QLF,dividing each part into core form and argumentforms, and then assigning each form to a clusteror an argument type.
The final logical form is de-rived by composing the abstract lambda forms ofthe parts using the ?-reduction rule.23.3 The USP MLNFormally, for a QLF Q, a semantic parse L par-titions Q into parts p1, p2, ?
?
?
, pn; each part p isassigned to some lambda-form cluster c, and isfurther partitioned into core form f and argumentforms f1, ?
?
?
, fk; each argument form is assignedto an argument type a in c. The USP MLN de-fines a joint probability distribution over Q and Lby modeling the distributions over forms and ar-guments given the cluster or argument type.Before presenting the predicates and formu-las in our MLN, we should emphasize that theyshould not be confused with the atoms and formu-las in the QLFs, which are represented by reifiedconstants and variables.To model distributions over lambda forms,we introduce the predicates Form(p, f!)
andArgForm(p, i, f!
), where p is a part, i is the in-dex of an argument, and f is a QLF subformula.Form(p, f) is true iff part p has core form f, andArgForm(p, i, f) is true iff the ith argument in phas form f.3 The ?f!?
notation signifies that eachpart or argument can have only one form.To model distributions over arguments, we in-troduce three more predicates: ArgType(p, i, a!
)signifies that the ith argument of p is assigned toargument type a; Arg(p, i, p?)
signifies that theith argument of p is p?
; Number(p, a, n) signifiesthat there are n arguments of p that are assignedto type a.
The truth value of Number(p, a, n) isdetermined by the ArgType atoms.Unsupervised semantic parsing can be capturedby four formulas:p ?
+c ?
Form(p,+f)ArgType(p, i,+a) ?
ArgForm(p, i,+f)Arg(p, i, p?)
?
ArgType(p, i,+a) ?
p??
+c?Number(p,+a,+n)All free variables are implicitly universally quan-tified.
The ?+?
notation signifies that the MLNcontains an instance of the formula, with a sep-arate weight, for each value combination of the2Currently, we do not handle quantifier scoping or se-mantics for specific closed-class words such as determiners.These will be pursued in future work.3There are hard constraints to guarantee that these assign-ments form a legal partition.
We omit them for simplicity.4variables with a plus sign.
The first formula mod-els the mixture of core forms given the cluster, andthe others model the mixtures of argument forms,argument types, and argument numbers, respec-tively, given the argument type.To encourage clustering and avoid overfitting,we impose an exponential prior with weight ?
onthe number of parameters.4The MLN above has one problem: it oftenclusters expressions that are semantically oppo-site.
For example, it clusters antonyms like ?el-derly/young?, ?mature/immature?.
This issue alsooccurs in other semantic-processing systems (e.g.,DIRT).
In general, this is a difficult open problemthat only recently has started to receive some at-tention (Mohammad et al, 2008).
Resolving thisis not the focus of this paper, but we describe ageneral heuristic for fixing this problem.
We ob-serve that the problem stems from the lack of nega-tive features for discovering meanings in contrast.In natural languages, parallel structures like con-junctions are one such feature.5 We thus introducean exponential prior with weight ?
on the numberof conjunctions where the two conjunctive partsare assigned to the same cluster.
To detect con-junction, we simply used the Stanford dependen-cies that begin with ?conj?.
This proves very ef-fective, fixing the majority of the errors in our ex-periments.4 InferenceGiven a sentence and the quasi-logical form Qderived from its dependency tree, the conditionalprobability for a semantic parse L is given byPr(L|Q) ?
exp (?iwini(L,Q)).
The MAP se-mantic parse is simply argmaxL?iwini(L,Q).Enumerating all L?s is intractable.
It is also un-necessary, since most partitions will result in partswhose lambda forms have no cluster they can beassigned to.
Instead, USP uses a greedy algorithmto search for the MAP parse.
First we introducesome definitions: a partition is called ?-reduciblefrom p if it can be obtained from the current parti-tion by recursively ?-reducing the part containingp with one of its arguments; such a partition is4Excluding weights of?
or?
?, which signify hard con-straints.5For example, in the sentence ?IL-2 inhibits X in A andinduces Y in B?, the conjunction between ?inhibits?
and ?in-duces?
suggests that they are different.
If ?inhibits?
and ?in-duces?
are indeed synonyms, such a sentence will sound awk-ward and would probably be rephrased as ?IL-2 inhibits X inA and Y in B?.Algorithm 1 USP-Parse(MLN, QLF)Form parts for individual atoms in QLF and as-sign each to its most probable clusterrepeatfor all parts p in the current partition dofor all partitions that are ?-reducible fromp and feasible doFind the most probable cluster and argu-ment type assignments for the new partand its argumentsend forend forChange to the new partition and assignmentswith the highest gain in probabilityuntil none of these improve the probabilityreturn current partition and assignmentscalled feasible if the core form of the new part iscontained in some cluster.
For example, considerthe QLF of ?Utah borders Idaho?
and assumethat the current partition is ?x2x3.borders(n1) ?nsubj(n1, x2) ?
dobj(n1, x3), Utah(n2),Idaho(n3).
Then the following partition is?-reducible from the first part in the abovepartition: ?x3.borders(n1) ?
nsubj(n1, n2) ?Utah(n2) ?
dobj(n1, x3), Idaho(n3).
Whetherthis new partition is feasible depends on whetherthe core form of the new part ?x3.borders(n1) ?nsubj(n1, n2) ?
Utah(n2) ?
dobj(n1, x3) (i.e.borders(n1) ?
nsubj(n1, n2) ?
Utah(n2)) iscontained in some lambda-form cluster.Algorithm 1 gives pseudo-code for our algo-rithm.
Given part p, finding partitions that are ?-reducible from p and feasible can be done in timeO(ST ), where S is the size of the clustering inthe number of core forms and T is the maximumnumber of atoms in a core form.
We omit the proofhere but point out that it is related to the unorderedsubtree matching problem which can be solved inlinear time (Kilpelainen, 1992).
Inverted indexes(e.g., from p to eligible core forms) are used to fur-ther improve the efficiency.
For a new part p anda cluster that contains p?s core form, there are kmways of assigning p?s m arguments to the k argu-ment types of the cluster.
For larger k and m, thisis very expensive.
We therefore approximate it byassigning each argument to the best type, indepen-dent of other arguments.This algorithm is very efficient, and is used re-peatedly in learning.55 LearningThe learning problem in USP is to maximize thelog-likelihood of observing the QLFs obtainedfrom the dependency trees, denoted by Q, sum-ming out the unobserved semantic parses:L?
(Q) = logP?
(Q)= log?LP?
(Q,L)Here, L are the semantic parses, ?
are the MLN pa-rameters, and P?
(Q,L) are the completion likeli-hoods.
A serious challenge in unsupervised learn-ing is the identifiability problem (i.e., the opti-mal parameters are not unique) (Liang and Klein,2008).
This problem is particularly severe forlog-linear models with hard constraints, which arecommon in MLNs.
For example, in our USPMLN, conditioned on the fact that p ?
c, there isexactly one value of f that can satisfy the formulap ?
c ?
Form(p, f), and if we add some constantnumber to the weights of p ?
c ?
Form(p, f) forall f, the probability distribution stays the same.6The learner can be easily confused by the infinitelymany optima, especially in the early stages.
Toaddress this problem, we impose local normaliza-tion constraints on specific groups of formulas thatare mutually exclusive and exhaustive, i.e., in eachgroup, we require that?ki=1ewi= 1, where wiare the weights of formulas in the group.
Group-ing is done in such a way as to encourage theintended mixture behaviors.
Specifically, for therule p ?
+c ?
Form(p,+f), all instances givena fixed c form a group; for each of the remain-ing three rules, all instances given a fixed a form agroup.
Notice that with these constraints the com-pletion likelihood P (Q,L) can be computed inclosed form for any L. In particular, each formulagroup contributes a term equal to the weight of thecurrently satisfied formula.
In addition, the opti-mal weights that maximize the completion likeli-hood P (Q,L) can be derived in closed form us-ing empirical relative frequencies.
E.g., the opti-mal weight of p ?
c?
Form(p, f) is log(nc,f/nc),where nc,fis the number of parts p that satisfyboth p ?
c and Form(p, f), and ncis the numberof parts p that satisfy p ?
c.7 We leverage this factfor efficient learning in USP.6Regularizations, e.g., Gaussian priors on weights, allevi-ate this problem by penalizing large weights, but it remainstrue that weights within a short range are roughly equivalent.7To see this, notice that for a given c, the total contribu-tion to the completion likelihood from all groundings in itsformula group is?fwc,fnc,f.
In addition,?fnc,f= ncAlgorithm 2 USP-Learn(MLN, QLFs)Create initial clusters and semantic parsesMerge clusters with the same core formAgenda?
?repeatfor all candidate operations O doScore O by log-likelihood improvementif score is above a threshold thenAdd O to agendaend ifend forExecute the highest scoring operation O?
inthe agendaRegenerate MAP parses for affected QLFsand update agenda and candidate operationsuntil agenda is emptyreturn the MLN with learned weights and thesemantic parsesAnother major challenge in USP learning is thesummation in the likelihood, which is over all pos-sible semantic parses for a given dependency tree.Even an efficient sampler like MC-SAT (Poon andDomingos, 2006), as used in Poon & Domingos(2008), would have a hard time generating accu-rate estimates within a reasonable amount of time.On the other hand, as already noted in the previoussection, the lambda-form distribution is generallysparse.
Large lambda-forms are rare, as they cor-respond to complex expressions that are often de-composable into smaller ones.
Moreover, whileambiguities are present at the lexical level, theyquickly diminish when more words are present.Therefore, a lambda form can usually only belongto a small number of clusters, if not a unique one.We thus simplify the problem by approximatingthe sum with the mode, and search instead for theL and ?
that maximize logP?(Q,L).
Since the op-timal weights and log-likelihood can be derived inclosed form given the semantic parses L, we sim-ply search over semantic parses, evaluating themusing log-likelihood.Algorithm 2 gives pseudo-code for our algo-rithm.
The input consists of an MLN withoutweights and the QLFs for the training sentences.Two operators are used for updating semanticparses.
The first is to merge two clusters, denotedby MERGE(C1, C2) for clusters C1, C2, which doesthe following:and there is the local normalization constraint?fewc,f= 1.The optimal weights wc,fare easily derived by solving thisconstrained optimization problem.61.
Create a new cluster C and add all core formsin C1, C2to C;2.
Create new argument types for C by merg-ing those in C1, C2so as to maximize the log-likelihood;3.
Remove C1, C2.Here, merging two argument types refers to pool-ing their argument forms to create a new argumenttype.
Enumerating all possible ways of creatingnew argument types is intractable.
USP approxi-mates it by considering one type at a time and ei-ther creating a new type for it or merging it to typesalready considered, whichever maximizes the log-likelihood.
The types are considered in decreasingorder of their numbers of occurrences so that moreinformation is available for each decision.
MERGEclusters syntactically different expressions whosemeanings appear to be the same according to themodel.The second operator is to create a new clus-ter by composing two existing ones, denoted byCOMPOSE(CR, CA), which does the following:1.
Create a new cluster C;2.
Find all parts r ?
CR, a ?
CAsuch that a isan argument of r, compose them to r(a) by?-reduction and add the new part to C;3.
Create new argument types for C from the ar-gument forms of r(a) so as to maximize thelog-likelihood.COMPOSE creates clusters of large lambda-formsif they tend to be composed of the same sub-forms (e.g., the lambda form for ?is next to?
).These lambda-forms may later be merged withother clusters (e.g., borders).At learning time, USP maintains an agenda thatcontains operations that have been evaluated andare pending execution.
During initialization, USPforms a part and creates a new cluster for eachunary atom u(n).
It also assigns binary atoms ofthe form b(n, n?)
to the part as argument formsand creates a new argument type for each.
Thisforms the initial clustering and semantic parses.USP then merges clusters with the same core form(i.e., the same unary predicate) using MERGE.8 Ateach step, USP evaluates the candidate operationsand adds them to the agenda if the improvement is8Word-sense disambiguation can be handled by includinga new kind of operator that splits a cluster into subclusters.We leave this to future work.above a threshold.9 The operation with the highestscore is executed, and the parameters are updatedwith the new optimal values.
The QLFs whichcontain an affected part are reparsed, and opera-tions in the agenda whose score might be affectedare re-evaluated.
These changes are done very ef-ficiently using inverted indexes.
We omit the de-tails here due to space limitations.
USP terminateswhen the agenda is empty, and outputs the currentMLN parameters and semantic parses.USP learning uses the same optimization objec-tive as hard EM, and is also guaranteed to find alocal optimum since at each step it improves thelog-likelihood.
It differs from EM in directly opti-mizing the likelihood instead of a lower bound.6 Experiments6.1 TaskEvaluating unsupervised semantic parsers is dif-ficult, because there is no predefined formal lan-guage or gold logical forms for the input sen-tences.
Thus the best way to test them is by usingthem for the ultimate goal: answering questionsbased on the input corpus.
In this paper, we ap-plied USP to extracting knowledge from biomedi-cal abstracts and evaluated its performance in an-swering a set of questions that simulate the in-formation needs of biomedical researchers.
Weused the GENIA dataset (Kim et al, 2003) asthe source for knowledge extraction.
It contains1999 PubMed abstracts and marks all mentionsof biomedical entities according to the GENIAontology, such as cell, protein, and DNA.
As afirst approximation to the questions a biomedi-cal researcher might ask, we generated a set oftwo thousand questions on relations between enti-ties.
Sample questions are: ?What regulates MIP-1alpha?
?, ?What does anti-STAT 1 inhibit??.
Tosimulate the real information need, we sample therelations from the 100 most frequently used verbs(excluding the auxiliary verbs be, have, and do),and sample the entities from those annotated inGENIA, both according to their numbers of occur-rences.
We evaluated USP by the number of an-swers it provided and the accuracy as determinedby manual labeling.109We currently set it to 10 to favor precision and guardagainst errors due to inexact estimates.10The labels and questions are available athttp://alchemy.cs.washington.edu/papers/poon09.76.2 SystemsSince USP is the first unsupervised semanticparser, conducting a meaningful comparison of itwith other systems is not straightforward.
Stan-dard question-answering (QA) benchmarks do notprovide the most appropriate comparison, be-cause they tend to simultaneously emphasize otheraspects not directly related to semantic pars-ing.
Moreover, most state-of-the-art QA sys-tems use supervised learning in their key compo-nents and/or require domain-specific engineeringefforts.
The closest available system to USP inaims and capabilities is TextRunner (Banko et al,2007), and we compare with it.
TextRunner is thestate-of-the-art system for open-domain informa-tion extraction; its goal is to extract knowledgefrom text without using supervised labels.
Giventhat a central challenge to semantic parsing is re-solving syntactic variations of the same meaning,we also compare with RESOLVER (Yates and Et-zioni, 2009), a state-of-the-art unsupervised sys-tem based on TextRunner for jointly resolving en-tities and relations, and DIRT (Lin and Pantel,2001), which resolves paraphrases of binary rela-tions.
Finally, we also compared to an informedbaseline based on keyword matching.Keyword: We consider a baseline system basedon keyword matching.
The question substringcontaining the verb and the available argument isdirectly matched with the input text, ignoring caseand morphology.
We consider two ways to derivethe answer given a match.
The first one (KW) sim-ply returns the rest of sentence on the other side ofthe verb.
The second one (KW-SYN) is informedby syntax: the answer is extracted from the subjector object of the verb, depending on the question.
Ifthe verb does not contain the expected argument,the sentence is ignored.TextRunner: TextRunner inputs text and outputsrelational triples in the form (R,A1, A2), whereR is the relation string, and A1, A2the argumentstrings.
Given a triple and a question, we firstmatch their relation strings, and then match thestrings for the argument that is present in the ques-tion.
If both match, we return the other argumentstring in the triple as an answer.
We report resultswhen exact match is used (TR-EXACT), or whenthe triple string can contain the question one as asubstring (TR-SUB).RESOLVER: RESOLVER (Yates and Etzioni,2009) inputs TextRunner triples and collectivelyresolves coreferent relation and argument strings.On the GENIA data, using the default parameters,RESOLVER produces only a few trivial relationclusters and no argument clusters.
This is not sur-prising, since RESOLVER assumes high redun-dancy in the data, and will discard any strings withfewer than 25 extractions.
For a fair compari-son, we also ran RESOLVER using all extractions,and manually tuned the parameters based on eye-balling of clustering quality.
The best result wasobtained with 25 rounds of execution and with theentity multiple set to 200 (the default is 30).
To an-swer questions, the only difference from TextRun-ner is that a question string can match any stringin its cluster.
As in TextRunner, we report resultsfor both exact match (RS-EXACT) and substring(RS-SUB).DIRT: The DIRT system inputs a path and returnsa set of similar paths.
To use DIRT in questionanswering, we queried it to obtain similar pathsfor the relation of the question, and used thesepaths while matching sentences.
We first usedMINIPAR (Lin, 1998) to parse input text usingthe same dependencies as DIRT.
To determine amatch, we first check if the sentence contains thequestion path or one of its DIRT paths.
If so, and ifthe available argument slot in the question is con-tained in the one in the sentence, it is a match, andwe return the other argument slot from the sen-tence if it is present.
Ideally, a fair comparison willrequire running DIRT on the GENIA text, but wewere not able to obtain the source code.
We thusresorted to using the latest DIRT database releasedby the author, which contains paths extracted froma large corpus with more than 1GB of text.
Thisputs DIRT in a very advantageous position com-pared with other systems.
In our experiments, weused the top three similar paths, as including moreresults in very low precision.USP: We built a system for knowledge extrac-tion and question answering on top of USP.
Itgenerated Stanford dependencies (de Marneffe etal., 2006) from the input text using the Stan-ford parser, and then fed these to USP-Learn11,which produced an MLN with learned weightsand the MAP semantic parses of the input sen-tences.
These MAP parses formed our knowledgebase (KB).
To answer questions, the system firstparses the questions12 using USP-Parse with the11?
and ?
are set to ?5 and ?10.12The question slot is replaced by a dummy word.8Table 1: Comparison of question answering re-sults on the GENIA dataset.# Total # Correct AccuracyKW 150 67 45%KW-SYN 87 67 77%TR-EXACT 29 23 79%TR-SUB 152 81 53%RS-EXACT 53 24 45%RS-SUB 196 81 41%DIRT 159 94 59%USP 334 295 88%learned MLN, and then matches the question parseto parses in the KB by testing subsumption (i.e., aquestion parse matches a KB one iff the former issubsumed by the latter).
When a match occurs, oursystem then looks for arguments of type in accor-dance with the question.
For example, if the ques-tion is ?What regulates MIP-1alpha?
?, it searchesfor the argument type of the relation that containsthe argument form ?nsubj?
for subject.
If such anargument exists for the relation part, it will be re-turned as the answer.6.3 ResultsTable 1 shows the results for all systems.
USPextracted the highest number of answers, almostdoubling that of the second highest (RS-SUB).It obtained the highest accuracy at 88%, andthe number of correct answers it extracted isthree times that of the second highest system.The informed baseline (KW-SYN) did surpris-ingly well compared to systems other than USP, interms of accuracy and number of correct answers.TextRunner achieved good accuracy when exactmatch is used (TR-EXACT), but only obtained afraction of the answers compared to USP.
Withsubstring match, its recall substantially improved,but precision dropped more than 20 points.
RE-SOLVER improved the number of extracted an-swers by sanctioning more matches based on theclusters it generated.
However, most of those ad-ditional answers are incorrect due to wrong clus-tering.
DIRT obtained the second highest numberof correct answers, but its precision is quite lowbecause the similar paths contain many errors.6.4 Qualitative AnalysisManual inspection shows that USP is able to re-solve many nontrivial syntactic variations with-out user supervision.
It consistently resolves thesyntactic difference between active and passivevoices.
It successfully identifies many distinct ar-gument forms that mean the same (e.g., ?X stimu-lates Y?
?
?Y is stimulated with X?, ?expressionof X?
?
?X expression?).
It also resolves manynouns correctly and forms meaningful groups ofrelations.
Here are some sample clusters in coreforms:{investigate, examine, evaluate, analyze, study,assay}{diminish, reduce, decrease, attenuate}{synthesis, production, secretion, release}{dramatically, substantially, significantly}An example question-answer pair, together withthe source sentence, is shown below:Q: What does IL-13 enhance?A: The 12-lipoxygenase activity of murinemacrophages.Sentence: The data presented here indicatethat (1) the 12-lipoxygenase activity of murinemacrophages is upregulated in vitro and in vivoby IL-4 and/or IL-13, .
.
.7 ConclusionThis paper introduces the first unsupervised ap-proach to learning semantic parsers.
Our USPsystem is based on Markov logic, and recursivelyclusters expressions to abstract away syntacticvariations of the same meaning.
We have suc-cessfully applied USP to extracting a knowledgebase from biomedical text and answering ques-tions based on it.Directions for future work include: better han-dling of antonyms, subsumption relations amongexpressions, quantifier scoping, more complexlambda forms, etc.
; use of context and discourseto aid expression clustering and semantic parsing;more efficient learning and inference; applicationto larger corpora; etc.8 AcknowledgementsWe thank the anonymous reviewers for their comments.
Thisresearch was partly funded by ARO grant W911NF-08-1-0242, DARPA contracts FA8750-05-2-0283, FA8750-07-D-0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-D030010, NSF grants IIS-0534881 and IIS-0803481, andONR grant N00014-08-1-0670.
The views and conclusionscontained in this document are those of the authors andshould not be interpreted as necessarily representing the offi-cial policies, either expressed or implied, of ARO, DARPA,NSF, ONR, or the United States Government.9ReferencesG.
Bakir, T. Hofmann, B.
B. Scho?lkopf, A. Smola,B.
Taskar, S. Vishwanathan, and (eds.).
2007.
Pre-dicting Structured Data.
MIT Press, Cambridge,MA.Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In Pro-ceedings of the Twentieth International Joint Con-ference on Artificial Intelligence, pages 2670?2676,Hyderabad, India.
AAAI Press.Xavier Carreras and Luis Marquez.
2004.
Introductionto the CoNLL-2004 shared task: Semantic role la-beling.
In Proceedings of the Eighth Conference onComputational Natural Language Learning, pages89?97, Boston, MA.
ACL.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation, pages 449?454, Genoa, Italy.
ELRA.Ruifang Ge and Raymond J. Mooney.
2009.
Learninga compositional semantic parser using an existingsyntactic parser.
In Proceedings of the Forty Sev-enth Annual Meeting of the Association for Compu-tational Linguistics, Singapore.
ACL.Lise Getoor and Ben Taskar, editors.
2007.
Introduc-tion to Statistical Relational Learning.
MIT Press,Cambridge, MA.Pekka Kilpelainen.
1992.
Tree Matching Prob-lems with Applications to Structured Text databases.Ph.D.
Thesis, Department of Computer Science,University of Helsinki.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, andJun?ichi Tsujii.
2003.
GENIA corpus - a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19:180?82.Stanley Kok and Pedro Domingos.
2008.
Extract-ing semantic networks from text via relational clus-tering.
In Proceedings of the Nineteenth EuropeanConference on Machine Learning, pages 624?639,Antwerp, Belgium.
Springer.Percy Liang and Dan Klein.
2008.
Analyzing the er-rors of unsupervised learning.
In Proceedings of theForty Sixth Annual Meeting of the Association forComputational Linguistics, pages 879?887, Colum-bus, OH.
ACL.Dekang Lin and Patrick Pantel.
2001.
DIRT - dis-covery of inference rules from text.
In Proceedingsof the Seventh ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 323?328, San Francisco, CA.
ACM Press.Dekang Lin.
1998.
Dependency-based evaluationof MINIPAR.
In Proceedings of the Workshop onthe Evaluation of Parsing Systems, Granada, Spain.ELRA.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008.
Computing word-pair antonymy.
In Proceed-ings of the 2008 Conference on Empirical Methodsin Natural Language Processing, pages 982?991,Honolulu, HI.
ACL.Raymond J. Mooney.
2007.
Learning for semanticparsing.
In Proceedings of the Eighth InternationalConference on Computational Linguistics and Intel-ligent Text Processing, pages 311?324, Mexico City,Mexico.
Springer.Hoifung Poon and Pedro Domingos.
2006.
Sound andefficient inference with probabilistic and determin-istic dependencies.
In Proceedings of the TwentyFirst National Conference on Artificial Intelligence,pages 458?463, Boston, MA.
AAAI Press.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov logic.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages649?658, Honolulu, HI.
ACL.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning, 62:107?136.Alexander Yates and Oren Etzioni.
2009.
Unsuper-vised methods for determining object and relationsynonyms on the web.
Journal of Artificial Intelli-gence Research, 34:255?296.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammers.
In Proceedings of the Twenty FirstConference on Uncertainty in Artificial Intelligence,pages 658?666, Edinburgh, Scotland.
AUAI Press.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 878?887, Prague, Czech.
ACL.10
