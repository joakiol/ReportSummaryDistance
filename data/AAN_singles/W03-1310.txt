Answering Clinical Questions with Role IdentificationYun Niu, Graeme Hirst, Gregory McArthur, and Patricia Rodriguez-GianolliDepartment of Computer ScienceUniversity of TorontoToronto, Ontario, Canada M5S 3G4yun,gh,gregm,prg@cs.toronto.eduAbstractWe describe our work in progress on natu-ral language analysis in medical question-answering in the context of a broader med-ical text-retrieval project.
We analyze thelimitations in the medical domain of thetechnologies that have been developed forgeneral question-answering systems, anddescribe an alternative approach whose or-ganizing principle is the identification ofsemantic roles in both question and answertexts that correspond to the fields of PICOformat.1 MotivationIn every aspect of patient treatment, questions arisefor which a search of the published medical evidenceis appropriate, as it is very likely that the answer hasalready been found from the work of other clinicians.For example:1Q: In a child with asthma, do increased doses of inhaled cor-ticosteroids lead to a decrease in growth?A: Growth was significantly slower in the group receivinghigher dose inhaled steroids (3.6 cm, 95% CI 3.0 to 4.2with double dose beclometasone v 5.1 cm, 95% CI 4.5to 5.7 with salmeterol v 4.5 cm, 95% CI 3.8 to 5.2 withplacebo).
(Barton, 2002)Studies have shown that searching in the literaturecan help clinicians in answering questions generatedin patient treatment (Gorman et al, 1994; Cimino,1All the examples in this paper are taken from a collection ofquestions that arose over a two-week period in August 2001 ina clinical teaching unit at the University of Toronto.1996; Mendonc?a et al, 2001).
It has also beenfound that if high-quality evidence is available inthis way at the point of care?for example, the pa-tients?
bedside?clinicians will use it in their deci-sion making, and it frequently results in additional orchanged decisions (Sackett and Straus, 1998; Strausand Sackett, 1999).
But speed is very important.Investigation of potential end-users has shown thatphysicians need access to the information within 30seconds, and that if the search takes longer, it is likelyto be abandoned (Takeshita et al, 2002).The practice of using the current best evidenceto help clinicians in making decisions on thetreatment of patients is called Evidence-BasedMedicine (EBM).
Finding relevant evidence is atypical question-answering (QA) problem in themedical area.
There are many QA systems that haveachieved some success in other domains, such asfinding the answer to ?factoid?
questions in a corpusof general news stories, as in the question-answeringtrack of recent Text Retrieval Conferences (TREC,2001).
However, we have found that there are largedifferences between general QA (GQA) and med-ical QA (MQA) (see section 3 below).
This paperanalyzes the challenges of applying QA technologyto answer clinical questions automatically, and thendescribes our on-going work on the problem.2 The EPoCare ProjectOur work is part of the EPoCare project (?Evidenceat Point of Care?)
at the University of Toronto.
Theproject aims to provide fast access at the point ofcare to the best available medical information.
Clin-icians will be able to query sources that summarizeand appraise the evidence about the diagnosis, treat-ment, prognosis, etiology, and prevalence of medi-cal conditions.
In order to make the system availableat the point of care, the question-answering systemwill be accessible using hand-held computers.
Theproject is an interdisciplinary collaboration that in-volves research in several disciplines.
Project mem-bers in Industrial Engineering and Cognitive Psy-chology are investigating the design of the systemthrough a user-centered design process, in which re-quirements are elicited from end users who are alsoinvolved in the evaluation of prototypes.
Projectmembers in Knowledge Management and NaturalLanguage Processing aim to ensure that the answersto queries are accurate and complete.
And projectmembers in Health Informatics will test the influenceof the system on clinical decision-making and clini-cal outcomes.The system is presently based on keyword queriesand retrieval, as we describe in section 2.2 below.The goal of the work that we will report in the latersections of the paper is to allow the system to acceptquestions in natural language2 and to better identifyanswers in its natural-language data sources.
Ourinitial emphasis is on the latter.2.1 System architectureThere are two main components in the system.The data sources are stored in an XML documentdatabase.
The EPoCare server uses this database toprovide answers to queries posed by clinicians.The architecture of the system is shown in Fig-ure 1.
A clinical query is passed to the front con-troller to form a database query of keywords.
Thequery is sent by the retriever to the XML documentdatabase to retrieve relevant documents in the datasources using keyword matching.
The results arethen passed to the query?answer matcher to find thebest answer candidates.
Finally, the best answer isdetermined and returned to the user.The current data sources include the reviews of ex-perimental results for clinical problems that are pub-lished in Clinical Evidence (CE) (version 7) (Barton,2002), and Evidence-based On Call (EBOC) (Ball2Here and throughout the paper, we make the conven-tional distinction between query and question; the former is akeyword-basedstring or structure, and the latter is in natural lan-guage.
A query may represent a question, and vice versa.UMLSFTIToX Enginekeywordscandidate answersEPoCare ServerexpandedretrieveddocumentsCatalogclinical answersanswerskeywordsClient ApplicationFront Controllerclinical queryQuery-AnswerMatcherQuery Processor(relevant)documentscandidateCERetrieverEBOCexpansion of keywordsToX query / answerAnswer ExtractorFigure 1: EPoCare system architecture.and Phillips, 2001).
The texts are stored with XMLmark-up in the database.
The XML database is ma-nipulated by ToX, a repository manager for XMLdata (Barbosa et al, 2001).
Repositories of dis-tributed XML documents may be stored in a file sys-tem, a relational database, or remotely on the Web.ToX supports document registration, collection man-agement, storage and indexing choice, and querieson document content and structure.2.2 PICO-format queriesAt present, the system accepts queries in a formatknown in Evidence-Based Medicine as PICO format(Sackett et al, 2000).
In this format, a clinical ques-tion is represented by a set of four fields that corre-spond to the basic elements of the question:P: a description of the patient (or the problem);I: an intervention;C: a comparison or control intervention (may be omitted);O: the clinical outcome.For example, the sample question in section 1 can berepresented in a simple PICO format as follows:P: asthmaI: inhaled corticosteroidsC: ?O: growthA more-complete PICO representation of the samequestion is this:P: child with asthmaI: increased doses of inhaled corticosteroidsC: ?O: decrease in growthThis representation contains more information; butneither of the two expresses the complete semanticsof the natural-language question.
Thus, the PICOformat is limited in its ability to represent the mean-ing of questions.
Especially in the case of yes?noquestions, the point of the question is likely to be un-clear.
However, the PICO format indicates the basicsemantics of the question, and it is commonly usedin question representation in EBM.
Thus it was usedas a starting point in the development of the system.The keyword-based retrieval procedure is com-posed of three steps:Retrieving.
For each query keyword, XML pathsin which the keyword appears are found.Filtering.
Paths that are not meaningful contextsfor the PICO category of the keyword are filtered out.For each PICO category in the question, some XMLcontext is meaningful for it while others not.
For ex-ample, a chapter title is meaningful (and valuable)context for an instance of patient population in thekeyword matching.
But titles of cited references arenot.Building answers.
In the filtered paths, the systemidentifies cases in which all the key concepts in thequestion have been found, in context, in such a waythat an answer pattern is satisfied.
Then it returnsthe related segment of text in XML format so thatthe user can view it with a browser.
A set of answerpatterns were constructed for this matching process;each answer pattern consists of a set of XML pathsfor each of the four PICO categories.
To identify apath as relevant, all four components should find amatch in it.Clinical question: In a patient with a suspected MI doesthrombolysis decrease the risk of death if it is administered 10hours after the onset of chest pain?PICO format:P: myocardial infarctionI: thrombolysisC: ?O: mortalityKeywords: myocardial infarction thrombolysis mortalityAnswer: Systematic reviews of RCTs have found that promptthrombolytic treatment (within 6 hours and perhaps up to 12hours and longer after the onset of symptoms) reduces mortalityin people with AMI and ST elevation or bundle branch blockon their presenting ECG.Fifty six people would need treatment in the acutephase to prevent one additional death.
Strokes, intracranialhaemorrhage, and major bleeds are more common in peoplegiven thrombolysis; with one additional stroke for every250 people treated and one additional major bleed for every143 people treated.
The reviews have found that intracranialhaemorrhage is more common in people of advanced age andlow body weight, those with hypertension on admission, andthose given tPA rather than another thrombolytic agent.Figure 2: Example of clinical question, with corre-sponding EPoCare query and answer from ClinicalEvidence.While this searching strategy is based on the PICOformat, it is not confined to it.
The patterns can beextended so that additional categories (components)are included.
Thus, it could be applied to questionsthat are not expressed in PICO.Figure 2 shows an example of a clinical questionwith the corresponding EPoCare query and the seg-ment of text that was retrieved from Clinical Evi-dence in response.
The segment that was retrieved isclearly relevant to the question, but it has too muchirrelevant data.3 QA in medicine: The problemWe will now discuss medical question-answering,with the goal of refining the current EPoCare systemby accepting natural-language questions and betteridentifying answers in the data sources.In this section, we examine the difference betweengeneral and medical QA from the perspective of thethree main research problems of QA: question pro-cessing, question?answer matching, and answer ex-traction.
For each problem, we describe features thatcurrent QA technology is not appropriate for, andfeatures that are not addressed by existing technol-ogy.3.1 Question processingFor a question to be answered correctly, a QA sys-tem first has to understand what the question is ask-ing about.
This is an important task of question pro-cessing.
Most current QA systems address it by iden-tifying the type of answer sought.
As GQA systemsfocus on wh- questions, many of which have namedentities (NEs) as their answer, they usually classifyanswers according to different types of NE, such asproduct, organization, person, and so on.
This clas-sification is not appropriate in the medical domain,in which questions often ask about the treatment fora disease, outcome of a treatment, possible disease,and so on.
As a result, the method of identifying ananswer type must be different in MQA from GQA.Even for the same answer type, there may be adifferent understanding.
For example, when ques-tions ask for the time that an event happens.
In GQAsystems, they are usually answered by an absolutedate, e.g., 15 May 1932.
However, in the medicalarea, when questions are usually answered by rel-ative time, e.g., two hours after the onset of chestpain.
Sometimes the answers are not even a time; in-stead, they are a clinical condition, e.g., in responseto When should antibiotics be applied?Some problems of MQA are not addressed at allby current QA technologies:Question focus.
Sometimes, the answer typeis not enough to determine what a question isabout.
Other information contained in the questionis needed to understand its goal.
This information isdefined as the focus of the question (Moldovan andHarabagiu, 2000).
Although different systems usedifferent names for the idea of question focus, it is re-garded to be very important in question processing.However, there is still no special technique to tacklethis problem.Yes?no questions.
As mentioned, most currentQA systems focus on wh- questions; yes?no ques-tions are still left untouched.
However, we havefound that they are very common in our collectionof clinical questions that arose in patient treatment.Efficient processing of yes?no questions is an impor-tant task in MQA.3.2 Question?answer matchingThe matching of question and answer is the pro-cess that most GQA systems put great effort into.Different methods are applied according to differentviews of the problem.
The approaches can be clas-sified into two categories: knowledge-intensive anddata-intensive.
Knowledge-intensive approaches tryto find the correct match between a question andthe answer by using effective natural language pro-cessing techniques that combine linguistic and real-world knowledge.
Typical systems include those ofPas?ca and Harabagiu (2001) and Hovy, Hermjakob,and Lin (2001).
Data-intensive approaches exploreinformation embedded in the data sources to extractthe evidence that supports a good answer.
They canbe further divided into information extraction?based(Soubbotin, 2001), redundancy-based (Clarke et al,2001; Dumais et al, 2002), and statistical QA (It-tycheriah et al, 2001).
Many systems contain ele-ments of both approaches.Although there have been many technologies de-veloped for matching the answer with the question,they are not applicable to the medical area directlyfor the following reasons.Knowledge taxonomy.
WordNet is the mainknowledge base that most current GQA systems usein analyzing relationships among words when cal-culating the similarity of a question and a candidateanswer.
However, as a general-purpose knowledgebase, it is not possible for WordNet to cover all theconcepts in any particular domain, such as medicine.A domain-specific knowledge base is needed.
Forexample, it may be important to know that meto-prolol is an instance of ?-blocker in order to locatethe correct answer.
A good complement to WordNetis the Unified Medical Language System (UMLS)(Lindberg et al, 1993), developed by the NationalLibrary of Medicine.
UMLS contains three knowl-edge sources: the Metathesaurus, the Semantic Net-work, and the Specialist Lexicon.
The Metathe-saurus represents biomedical knowledge by orga-nizing concepts according to their relationships andmeanings.
It will be very helpful in tasks such asquery expansion and answer-type identification inMQA.Named entity identification.
As the types ofNE in the medical area are different, the method ofidentifying them must be changed accordingly.
Forexample, an MQA system must be able to distin-guish medication from diseases.
Medical terminol-ogy plays an important role in NE identification, asbefore a concept can be classified, the correspond-ing terminology has to be recognized to make surethat the correct concept is found.
In the medical do-main, different phrases can be used to refer to thesame medical concept.
For example, a drug may bereferred to by its abbreviation, its common name, orits formal name (ASA, Aspirin, acetylsalicylic acid).Also, different medical concepts may have the sameabbreviation, which will lead to ambiguities in con-cept understanding.Data source.
A medical data source is often or-ganized in accordance with a hierarchy of medicalconcepts.
For example, Clinical Evidence (Barton,2002) groups clinical data according to disease cat-egories.
The positive aspect of such well-organizeddata is that once the candidate answers are found, itis very likely that they include the correct answer.However, it is unlikely that the answer for a ques-tion will appear redundantly in many different placesin the data source.
This is different from GQA sys-tems, which usually require a relatively large num-ber of redundant answer candidates to support goodperformance by the system.In current GQA systems, a correct answer to aquestion is often independent of its context.
This isnot the case in the medical data, in which the con-text containing a candidate answer may be importantto the question?answer matching.
The context usu-ally explains a conclusion, provides more evidence,or even presents contrary evidence.
A correct answermay be missed or the incorrect answer may be ex-tracted if the context is not considered in the match-ing process.Complicated constraints.
Clinical questions of-ten contain a very specific description of the patientconditions, as shown in the following examples:Q: Should ?-blocker (metoprolol) be used to continue treat-ment for a male with hypertension and coronary arterydisease even though he has Type 2 diabetes mellitus?Q: Do patients surviving an AMI and experiencing transientor ongoing congestive heart failure (CHF) have reducedmortality and morbidity when treated with an ACE in-hibitor (ex.
Ramipril)?The detailed description of the patient acts as aconstraint in matching with candidate answers.As the complexity of questions increases, more-sophisticated techniques are needed to find amatching answer.3.3 Answer extractionAn MQA system should be able to answer clin-ical questions in the course of patient treatment.Hence the format of the answer is important, andthis will affect the answer extraction process.
Forthe three types of questions?wh- questions, yes?noquestions, and no-answer3 questions?the EPoCarestudy of user requirements shows that both a shortanswer and a long answer should be prepared.
Theshort answer provides accurate and concise informa-tion to the physicians so that they can make the deci-sion quickly.
For yes?no questions, the answer canbe just yes or no.
If the system cannot find an an-swer for a question, it should indicate this explic-itly as its short answer.
But sometimes clinicianswant to read a long answer that may contain expla-nation of the evidence or other results of related ex-periments.
For the no-answer questions, physiciansmay expect to read at least some relevant informa-tion.
It is thus important to determine what relevantinformation should be included in the answer extrac-tion.3.4 Evaluation metricsEvaluation of QA systems in the medical area is dif-ferent from current evaluation methods for generalQA systems.
The Text Retrieval Conference usesthe Mean Reciprocal Rank (MRR) as an evaluationmetric.
In this method, a system may return an or-dered list of up to five different candidate answersto a question, and the score received is 1=n, wheren is the position in the list of the correct answer (if itappears at all); for example, if the correct answer isfourth in the list, the system receives a score of 0.25for that test item.
This metric cannot be applied here,since returning a list of alternative candidate answersto a question, each of which must then be further ver-ified, is not acceptable for a clinical question that isposed on site.Different answer formats should be evaluated sep-arately.
The short answer has to be concise.
So what3A no-answer question is one for which an answer cannot befound.
It is not a yes?no question for which the answer happensto be no.a concise answer is must be defined (at least for thewh- questions).
A long answer needs to provide de-tailed information that explains the short answer.
Forno-answer questions, relevant information (if there issome) should be returned.
For these two types of an-swers, it has to be clear (1) what information can beviewed as ?detail?
or ?relevant?
; (2) what the differ-ence between the two is; and (3) how much informa-tion should be included.Partial answers should be considered in the eval-uation.
If part of the correct answer is included inthe system output, it should be evaluated accordingto the importance of the correct information.
A par-tial answer that contains more crucial informationshould obtain a higher score.
Similarly, if an answerhelps make a wrong decision, it should be punishedin the evaluation.4 Locating answers by role identificationFrom the discussion in the previous section, we cansee that MQA poses new challenges for QA researchthat require new approaches.
We have found that theuse of roles and role identification is effective, andwe take them as an organizing principle for MQAthat goes beyond the use of named entities in GQA.This section will explain the principle.
In this ap-proach, the four roles represented by PICO will firstbe located in both the natural-language question andthe candidate answer texts obtained by the retrievalphase.
For example, PICO roles would be identifiedin these candidate answers as shown by the labelledbracketing.One RCT found [no evidence that (low molecular weightheparin)I is superior to (aspirin alone)C]O for the treatmentof (acute ischaemic stroke in people with atrial fibrillation)P.(Thrombolysis)I (reduces the risk of dependency, but in-creases the risk of death)O.We found (no evidence of benefit)O from (surgical evacua-tion of cerebral or cerebellar haematomas)I.In the matching process, the roles in the question willbe compared with the corresponding roles in the an-swer candidates to determine whether a candidate isa correct answer.4.1 Why roles?In GQA systems, as mentioned, in the question?answer matching process, usually the answer candi-dates are first checked to see if they contain the ex-pected answer type, in order to rule out irrelevantcandidates.
This is shown to be efficient, as indi-cated by Harabagiu et al (2001): systems that didnot include NE recognizers performed poorly in theTREC evaluations.
The effectiveness of this methoddepends on successfully recognizing NEs in the an-swer candidates.
However, for questions that cannotbe answered by named entities, the QA task is morecomplex, as it will be more difficult to recognize thecorresponding answer type in the answer candidates.The same problem occurs in MQA.
The importantinformation in medical text usually corresponds tothe basic PICO fields.
For example, therapy-relatedtext describes the relationshipsamong four elements:the status of the patient, the therapy, the compari-son therapy, and the clinical outcome.
Descriptionsof the diagnosis process often consist of the patientstatus, the test method, and the outcome.
These el-ements are the key concepts of understanding medi-cal text.
They act as different roles, which togetherconstruct the meaning of the text.
While some of theroles correspond to NEs, others do not.
For exam-ple, in answering a therapy-related question, the pa-tient status and the therapy can often be treated asNEs, but the clinical outcome often cannot be.
Ina description of diagnosis, the test process often isnot represented by an NE.
While medical NEs can beexpected to be recognized by applying terminologytechniques with the support of UMLS, the recogni-tion of non-NE roles in the answer candidates, on theother hand, becomes the main challenge.Thus, it is not sufficient to use information-extraction techniques, as in some GQA systems(Pas?ca and Harabagiu, 2001; Soubbotin, 2001), inwhich patterns are matched against the text to fill inthe roles in the template.
In such systems, the cover-age of the pattern set is quite limited; it is very time-consuming to manually construct a large set of suit-able patterns, especially for complicated phrasings;and the patterns are very specific: specific words orphrases are usually required to occur at a fixed lo-cation in each pattern, making it applicable only toexpressions phrased in exactly the same way.
Whilewe will need to look for some specific words, weneed much greater flexibility than is afforded by sim-ple pattern-matching to identify the PICO roles inthe text.
This can be done by analyzing the differentroles and their relationships.4.2 Understanding the dataTo apply a role-based method in MQA, we need todeal with the following problems:1.
Identifying the roles in text.2.
Determining the textual boundary of each role.3.
Analyzing the relationships among different roles.4.
Determining which combinations of roles are most likelyto contain correct answers.Our work currently focuses on therapy-relatedquestions.
We manually analyzed 170 sentencesfrom the Cardiovascular Disorders section of Clini-cal Evidence to obtain a better understandingof theseproblems.
Among the sentences, 141 contained atleast one role that we are interested in.
For therapy-related questions, we found that often if an outcomerole appeared in a sentence, then the sentence pro-vided some interesting information related to clinicalevidence.
But clinical outcome is the most difficultnon-NE role to locate.4.2.1 Identifying clinical outcomesIn our analysis, we found that the lexical iden-tifiers of clinical outcome belong to three part-of-speech categories: noun, verb, and adjective.
For ex-ample:Thrombolysis reduces the risk of dependency, but increasesthe risk of death.Lubeluzole has also been noted to have adverse outcome, es-pecially at higher doses.Some words that identify outcomes are listed below:Nouns: death, benefit, dependency, effect, evidence, out-come.Verbs: improve, reduce, prevent, produce, increase.Adjectives: responsible, negative, adverse, slower.Clinical outcomes must be carefully distinguishedin the text from the outcomes of clinical trials them-selves.
We refer to the latter as results in the follow-ing.
A result might or might not include a clinicaloutcome.
They often involve a comparison of theeffects of two (or more) interventions on a disease.Sometimes a result will state that an outcome did notoccur:One RCT found evidence that hormone treatment plus radio-therapy versus radiotherapy alone improved survival in lo-cally advanced breast cancer.In the systematic review of calcium channel antagonists, in-direct and limited comparisons of intravenous versus oraladministration found no significant difference in adverseevents.We found no evidence of benefit from surgical evacuation ofcerebral or cerebellar haematomas.The identifiers of results form another group:Result: evidence, difference, comparison, superior to, ver-sus.4.2.2 Determining the textual boundary ofclinical outcomesIn determining the textual boundary of an out-come, the four groups of words are treated sepa-rately.
Our finding is that for the noun identifiers, thenoun phrase that contains the nouns will be an out-come.
For the verb identifiers, the verb and its ob-ject together constitutean outcome.
For the adjectiveidentifiers, usually the adjective itself is an outcome.If several identifiers occur in one sentence, the out-come is all the text indicated by one or more of theidentifiers.Determining the textual boundary of the results ofclinical trials is more complicated.
If a result is acomparison of two or more interventions, it will con-tain the interventions, words that indicate a compar-ison relationship, and often the aspects that are com-pared.
In the first of the previous group of exam-ples, the elements of the results are evidence, hor-mone treatment plus radiotherapy versus radiother-apy, and improved survival.
However, if the inter-ventions can be identified as NEs, it will not be toodifficult to determine the boundary.We tested these simple rules manually on 50 sen-tences from Clinical Evidence on the topic of acuteotitis media.
Out of 54 outcomes (including bothclinical outcomes and clinical trial results), 45 wereidentified correctly, and 40 correct textual bound-aries were found.4.2.3 Relationships among rolesWe have also found that roles are helpful in un-derstanding the relationshipsbetween sentences.
Forexample, if a sentence contains only the interven-tion role and the following sentence contains only theproblem and outcome, then it is very likely that thecombination of the two sentences represents a com-plete idea and the roles themselves are related.
Webelieve that as the work continues, more interestingrelations will be found.5 ConclusionWe have described our work in progress on addingnatural language analysis to querry-answering theEPoCare project.
Although this work is at a rela-tively early stage, we have analyzed the limitationsof GQA technologies in MQA and are developingtechniques whose organizing principle is the identifi-cation of the semantic roles in both question and an-swer texts, with our initial emphasis being on the lat-ter.AcknowledgementsThe EPoCare project is supported by grants fromBell University Laboratories at the Universityof Toronto.
This research is also supported by agrant from the Natural Sciences and EngineeringResearch Council of Canada.
We are grateful toSharon Straus, MD, and other members of theproject for discussion and assistance.ReferencesChristopher M. Ball and Robert S. Phillips.
2001.Evidence-based On-Call: Acute Medicine.
ChurchillLivingstone, Edinburgh.Denilson Barbosa, Attila Barta, Alberto Mendel-zon, George Mihaila, Flavio Rizzolo, and PatriciaRodriguez-Gianolli.
2001.
ToX ?
The Toronto XMLEngine.
In Proceedings of the International Workshopon Information Integration on the Web, Rio de Janeiro.Stuart Barton.
2002.
Clinical Evidence.
BMJ PublishingGroup, London.James J. Cimino.
1996.
Linking patient informationsystems to bibliographic resources.
Methods of Infor-mation in Medicine, 35(2):122?126.Charles L. A. Clarke, Gordon V. Cormack, and Thomas R.Lynam.
2001.
Exploiting redundancy in questionanswering.
In Proceedings of the 24th InternationalConference on Research and Development in Infor-mation Retrieval (SIGIR-2001), pages 358?365.Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,and Andrew Ng.
2002.
Web question answering: ismore always better?
In Proceedings of the 25th Inter-national Conference on Research and Development inInformation Retrieval (SIGIR-2002), pages 291?298.Paul N. Gorman, Joan Ash, and L. Wykoff.
1994.
Canprimary care physicians?
questions be answered usingthe medical journal literature?
Bulletin of MedicalLibrary Association, 82(2): 140?146.Sanda M. Harabagiu, et al 2001.
The role of lexico-semantic feedback in open-domain textual question?answering.
In Proceedings of the 39th Meeting ofthe Association for Computational Linguistics (ACL-2001), pages 274?281.Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin.
2001.The use of external knowledge in factoid QA.
In(TREC, 2001), pages 644?652.Abraham Ittycheriah, Martin Franz, and Salim Roukos.2001.
IBM?s statistical question answering system ?TREC-10.
In (TREC, 2001), pages 258?264.Donald A.
B. Lindberg, Betsy L. Humphreys, andAlexa T. McCray.
1993.
The Unified Medical Lan-guage System.
Methods of Information in Medicine,32(4):281?291.Eneida A. Mendonc?a, James J. Cimino, Stephen B.Johnson, and Yoon-Ho Seol.
2001.
Accessing het-erogeneous sources of evidence to answer clinicalquestions.
Journal of Biomedical Informatics, 34:85?98.Dan Moldovan and Sanda M. Harabagiu.
2000.
Thestructure and performance of an open-domain questionanswering system.
In Proceedings of the 38th Meetingof the Association for Computational Linguistics,(ACL-2000), pages 563?570.Marius A. Pas?ca and Sanda M. Harabagiu.
2001.
Highperformance question/answering.
In Proceedings ofthe 24th International Conference on Research andDevelopment in Information Retrieval (SIGIR-2001),pages 366?374.David L. Sackett and Sharon E. Straus.
1998.
Findingand applying evidence during clinical rounds: the?evidence cart?.
Journal of the American MedicalAssociation, 280(15):1336?1338.David L. Sackett, Sharon E. Straus, W. Scott Richardson,William Rosenberg, and R. Brian Haynes.
2000.Evidence-Based Medicine: How to Practice andTeach EBM.
Churchill Livingstone, Edinburgh.Martin M. Soubbotin.
2001.
Patterns of potential answerexpressions as clues to the right answers.
In (TREC,2001), pages 293?302.Sharon E. Straus and David L. Sackett.
1999.
Bringingevidence to the point of care.
Journal of the AmericanMedical Association, 281:1171?1172.Harumi Takeshita, Dianne Davis, and Sharon E. Straus.2002.
Clinical evidence at the point of care in acutemedicine: a handheld usability case study.
In Proceed-ings of the Human Factors and Ergonomics Society46th Annual Meeting, pages 1409?1413, Baltimore.TREC.
2001.
Proceedings of the Tenth Text RetrievalConference, Gaithersburg, MD, November 13?16.
Na-tional Institute of Standards and Technology.
