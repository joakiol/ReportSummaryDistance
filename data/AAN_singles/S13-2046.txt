Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 275?279, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsETS: Domain Adaptation and Stacking for Short Answer Scoring?Michael Heilman and Nitin MadnaniEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USA{mheilman,nmadnani}@ets.orgAbstractAutomatic scoring of short text responses toeducational assessment items is a challeng-ing task, particularly because large amountsof labeled data (i.e., human-scored responses)may or may not be available due to the va-riety of possible questions and topics.
Assuch, it seems desirable to integrate variousapproaches, making use of model answersfrom experts (e.g., to give higher scores toresponses that are similar), prescored studentresponses (e.g., to learn direct associationsbetween particular phrases and scores), etc.Here, we describe a system that uses stack-ing (Wolpert, 1992) and domain adaptation(Daume III, 2007) to achieve this aim, allow-ing us to integrate item-specific n-gram fea-tures and more general text similarity mea-sures (Heilman and Madnani, 2012).
We re-port encouraging results from the Joint Stu-dent Response Analysis and 8th RecognizingTextual Entailment Challenge.1 IntroductionIn this paper, we address the problem of automati-cally scoring short text responses to educational as-sessment items for measuring content knowledge.Many approaches can be and have been taken tothis problem?e.g., Leacock and Chodorow (2003),Nielsen et al(2008), inter alia.
The effectivenessof any particular approach likely depends on the theavailability of data (among other factors).
For exam-ple, if thousands of prescored responses are avail-?System description papers for SemEval 2013 are requiredto have a team ID (e.g., ?ETS?)
as a prefix.able, then a simple classifier using n-gram featuresmay suffice.
However, if only model answers (i.e.,reference answers) or rubrics are available, moregeneral semantic similarity measures (or even rule-based approaches) would be more effective.It seems likely that, in many cases, there willbe model answers as well as a modest number ofprescored responses available, as was the case forthe Joint Student Response Analysis and 8th Rec-ognizing Textual Entailment Challenge (?2).
There-fore, we desire to incorporate both task-specific fea-tures, such as n-grams, as well as more general fea-tures such as the semantic similarity of the responseto model answers.We also observe that some features may them-selves require machine learning or tuning on datafrom the domain, in addition to any machine learn-ing required for the overall system.In this paper, we describe a machine learning ap-proach to short answer scoring that allows us to in-corporate both item-specific and general features byusing the domain adaptation technique of Daume III(2007).
In addition, the approach employs stacking(Wolpert, 1992) to support the integration of com-ponents that require tuning or machine learning.2 Task OverviewIn this section, we describe the task to which we ap-plied our system: the Joint Student Response Anal-ysis and 8th Recognizing Textual Entailment Chal-lenge (Dzikovska et al 2013), which was task 7 atSemEval 2013.The aim of the task is to classify student responsesto assessment items from two datasets represent-275ing different science domains: the Beetle dataset,which pertains to basic electricity and electronics(Dzikovska et al 2010), and the Science Entail-ments corpus (SciEntsBank) (Nielsen et al 2008),which covers a wider range of scientific topics.Responses were organized into five categories:correct, partially correct, contradictory, irrelevant,and non-domain.
The SciEntsBank responses wereconverted to this format as described by Dzikovskaet al(2012).The Beetle training data had about 4,000 studentanswers to 47 questions.
The SciEntsBank trainingdata had about 5,000 prescored student answers to135 questions from 12 domains (different learningmodules).
For each item, one or more model re-sponses were provided by the task organizers.There were three different evaluation scenarios:?unseen answers?, for scoring new answers to itemsrepresented in the training data; ?unseen questions?,for scoring answers to new items from domains rep-resented in the training data; and ?unseen domains?,for scoring answers to items from new domains(only for SciEntsBank since Beetle focused on a sin-gle domain).Performance was evaluated using accuracy,macro-average F1 scores, and weighted average F1scores.For additional details, see the task description pa-per (Dzikovska et al 2013).3 System DetailsIn this section, we describe the short answer scoringsystem we developed, and the variations of it thatcomprise our submissions to task 7.
We begin bydescribing our statistical modeling approach.
There-after, we describe the features used by the model(?3.1), including the PERP feature that relies onstacking (Wolpert, 1992), and then the domain adap-tation technique we used (?3.2).Our system is a logistic regression model with`2 regularization.
It uses the implementation of lo-gistic regression from the scikit-learn toolkit (Pe-dregosa et al 2011).1 To tune the C hyperparame-ter, it uses a 5-fold cross-validation grid search (with1The scikit-learn toolkit uses a one-versus-all scheme, us-ing multiple binary logistic regression classifiers, rather than asingle multiclass logistic regression classifier.C ?
10{?3,?2,...,3}).During development, we evaluated performanceusing 10-fold cross-validation, with the 5-fold cross-validation grid search still used for tuning withineach training partition (i.e., each set of 9 folds usedfor training during cross-validation).3.1 FeaturesOur full system includes the following features.3.1.1 Baseline FeaturesIt includes all of the baseline features generatedwith the code provided by the task organizers.2There are four types of lexically-driven text similar-ity measures, and each is computed by comparingthe learner response to both the expected answer(s)and the question, resulting in eight features in total.They are described more fully by Dzikovska et al(2012).3.1.2 Intercept FeatureThe system includes an intercept feature that is al-ways equal to one, which, in combination with thedomain adaptation technique described in ?3.2, al-lows the system to model the a priori distributionover classes for each domain and item.
Having theseexplicit intercept features effectively saves the learn-ing algorithm from having to use other features toencode the distribution over classes.3.1.3 Word and Character n-gram FeaturesThe system includes binary indicator features forthe following types of n-grams:?
lowercased word n-grams in the response textfor n ?
{1, 2, 3}.?
lowercased word n-grams in the response textfor n ?
{4, 5, .
.
.
, 11}, grouped into 10,000bins by hashing and using a modulo operation(i.e., the ?hashing trick?)
(Weinberger et al2009).?
lowercased character n-grams in the responsetext for n ?
{5, 6, 7, 8}2At the time of writing, the baseline code couldbe downloaded at http://www.cs.york.ac.uk/semeval-2013/task7/.2763.1.4 Text Similarity FeaturesThe system includes the following text similarityfeatures that compare the student response either toa) the reference answers for the appropriate item, orb) the student answers in the training set that are la-beled ?correct?.?
the maximum of the smoothed, uncased BLEU(Papineni et al 2002) scores obtained by com-paring the student response to each correctreference answer.
We also include the wordn-gram precision and recall values for n ?
{1, 2, 3, 4} for the maximally similar referenceanswer.?
the maximum of the smoothed, uncased BLEUscores obtained by comparing the student re-sponse to each correct training set student an-swer.
We also include the word n-gram preci-sion and recall values for n ?
{1, 2, 3, 4} forthe maximally similar student answer.?
the maximum PERP (Heilman and Madnani,2012) score obtained by comparing the studentresponse to the correct reference answers.?
the maximum PERP score obtained by compar-ing the student response to the correct studentanswers.PERP is an edit-based approach to text similar-ity.
It computes the similarity of sentence pairs byfinding sequences of edit operations (e.g., insertions,deletions, substitutions, and shifts) that convert onesentence in a pair to the other.
Then, using variousfeatures of the edits and weights for those featureslearned from labeled sentence pairs, it assigns a sim-ilarity score.
Heilman and Madnani (2012) providea detailed description of the original PERP system.In addition, Heilman and Madnani (To Appear) de-scribe some minor modifications to PERP used inthis work.To estimate weights for PERP?s edit features, weneed labeled sentence pairs.
First, we describe howthese labeled sentence pairs are generated from thetask data, and then we describe the stacking ap-proach used to avoid training PERP on the same datait will compute features for.For the reference answer PERP feature, we usethe Cartesian product of the set of correct referenceanswers (?good?
or ?best?
for Beetle) and the setof student answers, using 1 as the similarity score(i.e., the label for training PERP) for pairs where thestudent answer is labeled ?correct?
and 0 for all oth-ers.
For the student answer PERP feature, we usethe Cartesian product of the set of correct studentanswers and the set of all student answers, using 1as the similarity score for pairs where both studentanswers are labeled ?correct?
and 0 for all others.3We use 10 iterations for training PERP.In order to avoid training PERP on the same re-sponses it will compute features for, we use 10-foldstacking (Wolpert, 1992).
In this process, the train-ing data are split up into ten folds.
To compute thePERP features for the instances in each fold, PERPis trained on the other nine folds.
After all 10 itera-tions, there are PERP features for every example inthe training set.
This process is similar to 10-foldcross-validation.3.2 Domain AdaptationThe system uses the domain adaptation techniquefrom Daume III (2007) to support generalizationacross items and domains.Instead of having a single weight for each feature,following Daume III (2007), the system has multiplecopies with potentially different weights: a genericcopy, a domain-specific copy, and an item-specificcopy.
For an answer to an unseen item (i.e., ques-tion) from a new domain in the test set, only thegeneric feature will be active.
In contrast, for an an-swer to an item represented in the training data, thegeneric, domain-specific, and item-specific copiesof the feature would be active and contribute to thescore.For our submissions, this feature copying ap-proach was not used for the baseline features(?3.1.1) or the BLEU and PERP text similarity fea-tures (?3.1.4), which are less item-specific.
Thosefeatures had only general copies.
We did not testwhether doing so would affect performance.3The Cartesian product of the sets of correct student answersand of all student answers will contain some pairs of identi-cal correct answers.
We decided to simply include these whentraining PERP, since we felt it would be desirable for PERP tolearn that identical sentences should be considered similar.277Beetle SciEntsBankSubmission A Q A Q DRun 1 .5520 .5470 .5350 .4870 .4470Run 2 .7050 .6140 .6250 .3560 .4340Run 3 .7000 .5860 .6400 .4110 .4140maximum .7050 .6140 .6400 .4920 .4710mean .5143 .3978 .4568 .3769 .3736Table 1: Weighted average F1 scores for 5-way classification for our SemEval 2013 task 7 submissions, along withthe maximum and mean performance, for comparison.
?A?
= unseen answers, ?Q?
= unseen questions, ?D?
= unseendomains (see ?2 for details).
Results that were the maximum score among submissions for part of the task are in bold.3.3 SubmissionsWe submitted three variations of the system.
Foreach variation, a separate model was trained for Bee-tle and for SciEntsBank.?
Run 1: This run included the baseline (?3.1.1),intercept (?3.1.2), and the text-similarity fea-tures (?3.1.4) that compare student responses toreference answers (but not those that compareto scored student responses in the training set).?
Run 2: This run included the baseline (?3.1.1),intercept (?3.1.2), and n-gram features (?3.1.3).?
Run 3: This run included all features.4 ResultsTable 1 presents the weighted averages of F1 scoresacross the five categories for the 5-way subtask, foreach dataset and scenario.
The maximum and meanscores of all the submissions are included for com-parison.
These results were provided to us by thetask organizers.For conciseness, we do not include accuracy ormacro-average F1 scores here.
We observed that, ingeneral, the results from different evaluation metricswere very similar to each other.
We refer the readerto the task description paper (Dzikovska et al 2013)for a full report of the task results.Interestingly, the differences in performance be-tween the unseen answers task and the other taskswas somewhat larger for the SciEntsBank datasetthan for the Beetle dataset.
We speculate that this re-sult is because the SciEntsBank data covered a morediverse set of topics.Note that Runs 1 and 2 use subsets of the featuresfrom the full system (Run 3).
While Runs 1 and 2are not directly comparable to each other, Runs 1and 3 can be compared to measure the effect of thefeatures based on other previously scored student re-sponses (i.e., n-grams, and the PERP and BLEU fea-tures based on student responses).
Similarly, Runs 2and 3 can be compared to measure the combined ef-fect of all BLEU and PERP features.It appears that features of the other student re-sponses improve performance for the unseen an-swers task.
For example, the full system (Run 3)performed better than Run 1, which did not includefeatures of other student responses, on the unseenanswers task for both Beetle and SciEntsBank.However, it is not clear whether the PERP andBLEU features improve performance.
The full sys-tem (Run 3) did not always outperform Run 2, whichdid not include these features.We leave to future work various additional ques-tions, such as whether student response features orreference answer similarity features are more use-ful in general, and whether there are any systematicdifferences between human-machine and human-human disagreements.5 ConclusionWe have presented an approach for short answerscoring that uses stacking (Wolpert, 1992) and do-main adaptation (Daume III, 2007) to support theintegration of various types of task-specific and gen-eral features.
Evaluation results from task 7 at Se-mEval 2013 indicate that the system achieves rela-tively high levels of agreement with human scores,as compared to other systems submitted to theshared task.278AcknowledgmentsWe would like to thank the task organizers for facil-itating this research and Dan Blanchard for helpingwith scikit-learn.ReferencesHal Daume III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Myroslava O. Dzikovska, Diana Bental, Johanna D.Moore, Natalie Steinhauser, Gwendolyn Campbell,Elaine Farrow, and Charles B. Callaway.
2010.
In-telligent tutoring with natural language support in theBEETLE II system.
In Proceedings of Fifth EuropeanConference on Technology Enhanced Learning (EC-TEL 2010).Myroslava O. Dzikovska, Rodney D. Nielsen, and ChrisBrew.
2012.
Towards effective tutorial feedbackfor explanation questions: A dataset and baselines.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 200?210, Montre?al, Canada, June.
Associationfor Computational Linguistics.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
Semeval-2013 task 7: The joint student re-sponse analysis and 8th recognizing textual entailmentchallenge.
In *SEM 2013: The First Joint Conferenceon Lexical and Computational Semantics, Atlanta,Georgia, USA, 13-14 June.
Association for Compu-tational Linguistics.Michael Heilman and Nitin Madnani.
2012.
ETS: Dis-criminative edit models for paraphrase scoring.
In*SEM 2012: The First Joint Conference on Lexi-cal and Computational Semantics ?
Volume 1: Pro-ceedings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation (SemEval 2012),pages 529?535, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.Michael Heilman and Nitin Madnani.
To Appear.
Henry:Domain adapation and stacking for text similarity.
In*SEM 2013: The Second Joint Conference on Lexicaland Computational Semantics.
Association for Com-putational Linguistics.C.
Leacock and M. Chodorow.
2003. c-rater: Scoring ofshort-answer questions.
Computers and the Humani-ties, 37.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2008.
Classification errors in a domain-independentassessment system.
In Proceedings of the Third Work-shop on Innovative Use of NLP for Building Educa-tional Applications, pages 10?18, Columbus, Ohio,June.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Kilian Weinberger, Anirban Dasgupta, John Langford,Alex Smola, and Josh Attenberg.
2009.
Feature hash-ing for large scale multitask learning.
In Proceedingsof the 26th Annual International Conference on Ma-chine Learning, ICML ?09, pages 1113?1120, NewYork, NY, USA.
ACM.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5:241?259.279
