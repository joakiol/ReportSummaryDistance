Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 70?74,Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational LinguisticsConversion of Procedural Morphologies to Finite-State Morphologies: aCase Study of ArabicMans HuldenUniversity of the Basque CountryIXA GroupIKERBASQUE, Basque Foundation for Sciencemhulden@email.arizona.eduYounes SamihHeinrich-Heine-Universita?t Du?sseldorfsamih@phil.uni-duesseldorf.deAbstractIn this paper we describe a conversion ofthe Buckwalter Morphological Analyzer forArabic, originally written as a Perl-script,into a pure finite-state morphological ana-lyzer.
Representing a morphological ana-lyzer as a finite-state transducer (FST) con-fers many advantages over running a procedu-ral affix-matching algorithm.
Apart from ap-plication speed, an FST representation imme-diately offers various possibilities to flexiblymodify a grammar.
In the case of Arabic, thisis illustrated through the addition of the abil-ity to correctly parse partially vocalized formswithout overgeneration, something not possi-ble in the original analyzer, as well as to serveboth as an analyzer and a generator.1 IntroductionMany lexicon-driven morphological analysis sys-tems rely on a general strategy of breaking downinput words into constituent parts by consulting cus-tomized lexicons and rules designed for a particu-lar language.
The constraints imposed by the lex-ica designed are then implemented as program codethat handles co-occurrence restrictions and analysisof possible orthographic variants, finally producinga parse of the input word.
Some systems designedalong these lines are meant for general use, such asthe hunspell tool (Hala?csy et al, 2004) which allowsusers to specify lexicons and constraints, while oth-ers are language-dependent, such as the BuckwalterArabic Morphological Analyzer (BAMA) (Buckwal-ter, 2004).In this paper we examine the possibility of con-verting such morphological analysis tools to FSTsthat perform the same task.
As a case study, we havechosen to implement a one-to-one faithful conver-sion of the Buckwalter Arabic analyzer into a finite-state representation using the foma finite state com-piler (Hulden, 2009b), while also adding some ex-tensions to the original analyzer.
These are usefulextensions which are difficult to add to the originalPerl-based analyzer because of its procedural nature,but very straightforward to perform in a finite-stateenvironment using standard design techniques.There are several advantages to representing mor-phological analyzers as FSTs, as is well noted in theliterature.
Here, in addition to documenting the con-version, we shall also discuss and give examples ofthe flexibility, extensibility, and speed of applicationwhich results from using a finite-state representationof a morphology.12 The Buckwalter AnalyzerWithout going into an extensive linguistic discus-sion, we shall briefly describe the widely used Buck-walter morphological analyzer for Arabic.
TheBAMA accepts as input Arabic words, with or with-out vocalization, and produces as output a break-down of the affixes participating in the word, thestem, together with information about conjugationclasses.
For example, for the input word ktb/I.
J?,BAMA returns, among others:LOOK-UP WORD: ktbSOLUTION 1: (kataba) [katab-u_1]katab/VERB_PERFECT+a/PVSUFF_SUBJ:3MS(GLOSS): + write + he/it <verb>1The complete code and analyzer are available athttp://buckwalter-fst.googlecode.com/70Figure 1: The Buckwalter Arabic Morphological Analyzer?s lookup process exemplified for the word lilkitAbi.2.1 BAMA lookupIn the BAMA system, every Arabic word is assumedto consist of a sometimes optional prefix, an oblig-atory stem, and a sometimes optional suffix.2 Thesystem for analysis is performed by a Perl-script thatcarries out the following tasks:1.
Strips all diacritics (vowels) from the inputword (since Arabic words may contain vocal-ization marks which are not included in the lex-icon lookup).
Example: kataba?
ktb2.
Factors the input word into all possiblecombinations of prefix-stem-suffix.
Stemsmay not be empty, while affixes are optional.Example: ktb ?
{ <k,t,b>,< kt,b,?>,<k,tb,?>, <?,k,tb>, <?,kt,b>,<?,ktb,?> }.3.
Consults three lexicons (dictPrefixes, dict-Stems, dictSuffixes) for ruling out impossi-ble divisions.
For example, <kt,b,?>, isrejected since kt does not appear as a prefixin dictPrefixes, while <k,tb,?> is acceptedsince k appears in dictPrefixes, tb in dict-Stems, and ?
in dictSuffixes.4.
Consults three co-occurrence constraint listsfor further ruling out incompatible prefix-stem combinations, stem-suffix combinations,and prefix-suffix combinations.
For example,2In reality, these are often conjoined prefixes treated as asingle entry within the system.<k,tb,?>, while accepted in the previousstep, is now rejected because the file dict-Prefixes lists k as a prefix belonging to classNPref-Bi, and the stem tb belonging to one ofPV V, IV V, NF, PV C, or IV C. However,the compatibility file tableAB does not permita combination of prefix class NPref-Bi and anyof the above-mentioned stem classes.5.
In the event that the lookup fails, the analyzerconsiders various alternative spellings of the in-put word, and runs through the same steps us-ing the alternate spellings.The BAMA lookup process is illustrated using adifferent example in figure 1.3 ConversionOur goal in the conversion of the Perl-code and thelookup tables is to produce a single transducer thatmaps input words directly to their morphologicalanalysis, including class and gloss information.
Inorder to do this, we break the process down intothree major steps:(1) We construct a transducer Lexicon that ac-cepts on its output side strings consisting ofany combinations of fully vocalized prefixes,stems, and suffixes listed in dictPrefixes, dict-Stems, and dictSuffixes.
On the input side,we find a string that represents the class eachmorpheme on the output side corresponds to,as well as the line number in the correspond-71LEXICON RootPrefixes ;LEXICON Prefixes[Pref-%0]{P%:34}:0 Stems;[Pref-Wa]{P%:37}:wa Stems;...LEXICON Stems[Nprop]{S%:23}:|b Suffixes;[Nprop]{S%:27}:%<ib?
Suffixes;...LEXICON Suffixes[Suff-%0]{X%:34}:0 #;[CVSuff-o]{X%:37}:o #;...Figure 2: Skeleton of basic lexicon transducer in LEXCgenerated from BAMA lexicons.ing file where the morpheme appears.
For ex-ample, the Lexicon transducer would containthe mapping:[Pref-0]{P:34}[PV]{S:102658}[NSuff-a]{X:72}katabaindicating that for the surface formkataba/ I.J?, the prefix class is Pref-0appearing on line 34 in the file dictPrefixes,the stem class is PV, appearing on line102,658 in dictStems, and that the suffixclass is NSuff-a, appearing on line 72 indictSuffixes.To construct the Lexicon, we produced aPerl-script that reads the contents of the BAMAfiles and automatically constructs a LEXC-format file (Beesley and Karttunen, 2003),which is compiled with foma into a finite trans-ducer (see figure 2).
(2) We construct rule transducers that filter out im-possible combinations of prefix classes basedon the data in the constraint tables tableAB,tableBC, and tableAC.
We then iterativelycompose the Lexicon transducer with eachrule transducer.
This is achieved by convertingeach suffix class mentioned in each of the classfiles to a constraint rule, which is compiledinto a finite automaton.
For example, the filetableBC, which lists co-occurrence constraintsbetween stems and suffixes contains only thefollowing lines beginning with Nhy:Nhy NSuff-hNhy NSuff-iyindicating that the Nhy-class only combineswith Nsuff-h or Nsuff-iy.
These lines areconverted by our script into the constraint re-striction regular expression:def Rule193 "[Nhy]" => _ ?
*"[NSuff-h]"|"[NSuff-iy]"];This in effect defines the language where eachinstance [Nhy] is always followed some-time later in the string by either [NSuff-h],or [NSuff-iy].
By composing this, andthe other constraints, with the Lexicon-transducer, we can filter out all illegitimatecombinations of morphemes as dictated by theoriginal Buckwalter files, by calculating:def Grammar Lexicon.i .o.Rule1 .o....RuleNNN ;In this step, it is crucial to note that one cannotin practice build a separate, single transducer(or automaton) that models the intersection ofall the lexicon constraints, i.e.
Rule1 .o.Rule2 .o.
... RuleNNN, and thencompose that transducer with the Lexicontransducer.
The reason for this is that thesize of the intersection of all co-occurrencerules grows exponentially with each rule.
Toavoid this intermediate exponential size, theLexicon transducer must be composed withthe first rule, whose composition is then com-posed with the second rule, etc., as above.
(3) As the previous two steps leave us with a trans-ducer that accepts only legitimate combina-tions of fully vocalized prefixes, stems, andsuffixes, we proceed to optionally remove shortvowel diacritics as well as perform optionalnormalization of the letter Alif ( @) from the72output side of the transducer.
This means,for instance, that an intermediate kataba/ I.J?,would be mapped to the surface forms kataba,katab, katba, katb, ktaba, ktab, ktba, andktb.
This last step assures that we canparse partially vocalized forms, fully vocal-ized forms, completely unvocalized forms, andcommon variants of Alif.def RemoveShortVowels[a|u|i|o|%?|%?]
(->) 0;def NormalizeAlif["|"|"<"|">"] (->) A .o.
"{" (->) [A|"<"] ;def RemovefatHatAn [F|K|N] -> 0;def BAMA 0 <- %{|%} .o.Grammar .o.RemoveShortVowels .o.NormalizeAlif .o.RemovefatHatAn;4 ResultsConverting the entire BAMA grammar as describedabove produces a final FST of 855,267 states and1,907,978 arcs, which accepts 14,563,985,397 Ara-bic surface forms.
The transducer occupies 8.5Mb.An optional auxiliary transducer for mapping linenumbers to complete long glosses and class namesoccupies an additional 10.5 Mb.
This is slightlymore than the original BAMA files which occupy4.0Mb.
However, having a FST representation ofthe grammar provides us with a number of advan-tages not available in the original BAMA, some ofwhich we will briefly discuss.4.1 Orthographical variantsThe original BAMA deals with spelling variants andsubstandard spelling by performing Perl-regex re-placements to the input string if lookup fails.
In theBAMA documentation, we find replacements suchas:- word final Y?
should be y?- word final Y?
should be }- word final y?
should be }In a finite-state system, once the grammar is con-verted, we can easily build such search heuristicsinto the FST itself using phonological replacementrules and various composition strategies such as pri-ority union (Kaplan, 1987).
We can thus mimic thebehavior of the BAMA, albeit without incurring anyextra lookup time.4.2 VocalizationAs noted above, by constructing the analyzer fromthe fully vocalized forms and then optionally remov-ing vowels in surface variants allows us to more ac-curately parse partially vocalized Arabic forms.
Wethus rectify one of the drawbacks of the originalBAMA, which makes no use of vocalization informa-tion even when it is provided.
For example, given aninput word qabol, BAMA would as a first step stripoff all the vocalization marks, producing qbl.
Dur-ing the parsing process, BAMA could then match qblwith, for instance, qibal, an entirely different word,even though vowels were indicated.
The FST de-sign addresses this problem elegantly: if the inputword is qabol, it will never match qibal because thevocalized morphemes are used throughout the con-struction of the FST and only optionally removedfrom the surface forms, whereas BAMA used the un-vocalized forms to match input.
This behavior is inline with other finite-state implementations of Ara-bic, such as Beesley (1996), where diacritics, if theyhappen to be present, are taken advantage of in orderto disambiguate and rule out illegitimate parses.This is of practical importance when parsing Ara-bic as writers often partially disambiguate wordsdepending on context.
For example, the wordHsbt/ I.
?kis ambiguous (Hasabat = compute,charge; Hasibat = regard, consider).
One wouldpartially vocalize Hsbt as Hsibt to denote ?sheregards?, or as Hsabt to imply ?she computes.
?The FST-based system correctly narrows down theparses accordingly, while BAMA would produce allambiguities regardless of the vocalization in the in-put.4.3 Surface lexicon extractionHaving the BAMA represented as a FST also al-lows us to extract the output projection of the gram-mar, producing an automaton that only accepts le-gitimate words in Arabic.
This can be then beused in spell checking applications, for example,by integrating the lexicon with weighted transduc-73ers reflecting frequency information and error mod-els (Hulden, 2009a; Pirinen et al, 2010).4.4 Constraint analysisInterestingly, the BAMA itself contains a vastamount of redundant information in the co-occurrence constraints.
That is, some suffix-stem-lexicon constraints are entirely subsumed by otherconstraints and could be removed without affectingthe overall system.
This can be observed during thechain of composition of the various transducers rep-resenting lexicon constraints.
If a constraint X failsto remove any words from the lexicon?somethingthat can be ascertained by noting that the numberof paths through the new transducer is the same asin the transducer before composition?it is an indi-cation that a previous constraint Y has already sub-sumed X .
In short, the constraint X is redundant.The original grammar cannot be consistently ana-lyzed for redundancies as it stands.
However, redun-dant constraints can be detected when compiling theLexicon FST together with the set of rules, offer-ing a way to streamline the original grammar.5 ConclusionWe have shown a method for converting the table-based and producedural constraint-driven Buckwal-ter Arabic Morphological Analyzer into an equiva-lent finite-state transducer.
By doing so, we can takeadvantage of established finite-state methods to pro-vide faster and more flexible parsing and also use thefinite-state calculus to produce derivative applica-tions that were not possible using the original table-driven Perl parser, such as spell checkers, normaliz-ers, etc.
The finite-state transducer implementationalso allows us to parse words with any vocalizationwithout sacrificing accuracy.While the conversion method in this case is spe-cific to the BAMA, the general principle illustratedin this paper can be applied to many other procedu-ral morphologies that rule out morphological parsesby first consulting a base lexicon and subsequentlyapplying a batch of serial or parallel constraints overaffix occurrence.ReferencesAttia, M., Pecina, P., Toral, A., Tounsi, L., andvan Genabith, J.
(2011).
An open-source finitestate morphological transducer for modern stan-dard Arabic.
In Proceedings of the 9th Interna-tional Workshop on Finite State Methods and Nat-ural Language Processing, pages 125?133.
Asso-ciation for Computational Linguistics.Beesley, K. R. (1996).
Arabic finite-state morpho-logical analysis and generation.
In Proceedingsof COLING?96?Volume 1, pages 89?94.
Associ-ation for Computational Linguistics.Beesley, K. R. and Karttunen, L. (2003).
Finite StateMorphology.
CSLI Publications, Stanford, CA.Buckwalter, T. (2004).
Arabic Morphological Ana-lyzer 2.0.
Linguistics Data Consortium (LDC).Habash, N. (2010).
Introduction to Arabic naturallanguage processing.
Synthesis Lectures on Hu-man Language Technologies.Hala?csy, P., Kornai, A., Ne?meth, L., Rung, A., Sza-kada?t, I., and Tro?n, V. (2004).
Creating open lan-guage resources for Hungarian.
In Proceedings ofLanguage Resources and Evaluation Conference(LREC04).
European Language Resources Asso-ciation.Hulden, M. (2009a).
Fast approximate string match-ing with finite automata.
Procesamiento dellenguaje natural, 43:57?64.Hulden, M. (2009b).
Foma: a finite-state compilerand library.
In Proceedings of the 12th confer-ence of the European Chapter of the Associationfor Computational Linguistics,, pages 29?32.
As-sociation for Computational Linguistics.Kaplan, R. M. (1987).
Three seductions of computa-tional psycholinguistics.
In Whitelock, P., Wood,M.
M., Somers, H. L., Johnson, R., and Bennett,P., editors, Linguistic Theory and Computer Ap-plications, London.
Academic Press.Pirinen, T., Linde?n, K., et al (2010).
Finite-statespell-checking with weighted language and errormodels.
In Proceedings of LREC 2010 Workshopon creation and use of basic lexical resources forless-resourced languages.74
