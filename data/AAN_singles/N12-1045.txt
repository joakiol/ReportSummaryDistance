2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 407?416,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsA Hierarchical Dirichlet Process Model for Joint Part-of-Speech andMorphology InductionKairit SirtsInstitute of Cybernetics atTallinn University of Technologykairit.sirts@phon.ioc.eeTanel Aluma?eInstitute of Cybernetics atTallinn University of Technologytanel.alumae@phon.ioc.eeAbstractIn this paper we present a fully unsupervisednonparametric Bayesian model that jointly in-duces POS tags and morphological segmen-tations.
The model is essentially an infi-nite HMM that infers the number of statesfrom data.
Incorporating segmentation intothe same model provides the morphologicalfeatures to the system and eliminates the needto find them during preprocessing step.
Weshow that learning both tasks jointly actuallyleads to better results than learning either taskwith gold standard data from the other taskprovided.
The evaluation on multilingual datashows that the model produces state-of-the-artresults on POS induction.1 IntroductionNonparametric Bayesian modeling has recently be-come very popular in natural language processing(NLP), mostly because of its ability to provide pri-ors that are especially suitable for tasks in NLP (Teh,2006).
Using nonparametric priors enables to treatthe size of the model as a random variable with itsvalue to be induced during inference which makesits use very appealing in models that need to decideupon the number of states.The task of unsupervised parts-of-speech (POS)tagging has been under research in numerous pa-pers, for overview see (Christodoulopoulos et al,2010).
Most of the POS induction models use thestructure of hidden Markov model (HMM) (Rabiner,1989) that requires the knowledge about the num-ber of hidden states (corresponding to the numberof tags) in advance.
According to our consider-ations, supplying this information is not desirablefor two opposing reasons: 1) it injects into the sys-tem a piece of knowledge which in a truly unsu-pervised setting would be unavailable; and 2) thenumber of POS tags used is somewhat arbitrary any-way because there is no common consensus of whatshould be the true number of tags in each languageand therefore it seems unreasonable to constrain themodel with such a number instead of learning it fromthe data.Unsupervised morphology learning is anotherpopular task that has been extensively studied bymany authors.
Here we are interested in learningconcatenative morphology of words, meaning thesubstrings of the word corresponding to morphemesthat, when concatenated, will give the lexical repre-sentation of the word type.
For the rest of the paperwe will refer to this task as (morphological) segmen-tation.Several unsupervised POS induction systemsmake use of morphological features (Blunsom andCohn, 2011; Lee et al, 2010; Berg-Kirkpatrick etal., 2010; Clark, 2003; Christodoulopoulos et al,2011) and this approach has been empirically provedto be helpful (Christodoulopoulos et al, 2010).
In asimilar fashion one could think that knowing POStags could be useful for learning morphological seg-mentations and in this paper we will study this hy-pothesis.In this paper we will build a model that combinesPOS induction and morphological segmentation intoone learning problem.
We will show that the unsu-pervised learning of both of these tasks in the same407model will lead to better results than learning bothtasks separately with the gold standard data of theother task provided.
We will also demonstrate thatour model produces state-of-the-art results on POStagging.
As opposed to the compared methods, ourmodel also induces the number of tags from data.In the following, section 2 gives the overviewof the Dirichlet Processes, section 3 describes themodel setup followed by the description of infer-ence procedures in section 4, experimental resultsare presented in section 5, section 6 summarizes theprevious work and last section concludes the paper.2 Background2.1 Dirichlet ProcessLet H be a distribution called base measure.
Dirich-let process (DP) (Ferguson, 1973) is a probabilitydistribution over distributions whose support is thesubset of the support of H:G ?
DP (?,H), (1)where ?
is the concentration parameter that controlsthe number of values instantiated by G.DP has no analytic form and therefore other rep-resentations must be developed for sampling.
In thenext section we describe Chinese Restaurant Processthat enables to obtain samples from DP.2.2 Chinese Restaurant ProcessChinese Restaurant Process (CRP) (Aldous, 1985)enables to calculate the marginal probabilities of theelements conditioned on the values given to all pre-viously seen items and integrating over possible DPprior values.Imagine an infinitely big Chinese restaurant withinfinitely many tables with each table having ca-pacity for infinitely many customers.
In the begin-ning the restaurant is empty.
Then customers, corre-sponding to data points, start entering one after an-other.
The first customer chooses an empty table tosit at.
Next customers choose a new table with prob-ability proportional to the concentration parameter?
or sit into one of the already occupied tables withprobability proportional to the number of customersalready sitting there.
Whenever a customer choosesan empty table, he will also pick a dish from H tobe served on that table.
The predictive probabilitydistribution over dishes for the i-th customer is:P (xi = ?k|x?i, ?,H) =n?k + ?i?
1 + ?pH(?k), (2)where x?i is the seating arrangement of customersexcluding the i-th customer and n?k is the number ofcustomers eating dish ?k and pH(?)
is the probabilityaccording to H .2.3 Hierarchical Dirichlet ProcessThe notion of hierarchical Dirichlet Process (HDP)(Teh et al, 2006) can be derived by letting the basemeasure itself to be a draw from a DP:G0|?0, H ?
DP (?0, H) (3)Gj |?,G0 ?
DP (?,G0) j = 1 ?
?
?
J (4)Under HDP, CRP becomes Chinese RestaurantFranchise (Teh et al, 2006) with several restaurantssharing the same franchise-wide menu G0.
When acustomer sits at an empty table in one of the Gj-threstaurants, the event of a new customer entering therestaurant G0 will be triggered.
Analogously, whena table becomes empty in one of the Gj-th restau-rants, it causes one of the customers leaving fromrestaurant G0.3 ModelWe consider the problem of unsupervised learningof POS tags and morphological segmentations in ajoint model.
Similarly to some recent successful at-tempts (Lee et al, 2010; Christodoulopoulos et al,2011; Blunsom and Cohn, 2011), our model is type-based, arranging word types into hard clusters.
Un-like many recent POS tagging models, our modeldoes not assume any prior information about thenumber of POS tags.
We will define the model asa generative sequence model using the HMM struc-ture.
Graphical depiction of the model is given inFigure 1.3.1 Generative storyWe assume the presence of a fixed length vocabu-lary W .
The process starts with generating the lex-icon that stores for each word type its POS tag andmorphological segmentation.408?
Draw a unigram tag distribution from the re-spective DP;?
Draw a segment distribution from the respec-tive DP;?
For each tag, draw a tag-specific segment distri-bution from HDP with the segment distributionas base measure;?
For each word type, draw a tag from the uni-gram tag distribution;?
For each word type, draw a segmentation fromthe respective tag-specific segment distribution.Next we proceed to generate the HMM parame-ters:?
For each tag, draw a bigram distribution fromHDP with the unigram tag distribution as basemeasure;?
For each tag bigram, draw a trigram distribu-tion from HDP with the respective bigram dis-tribution as base measure;?
For each tag, draw a Dirichlet concentration pa-rameter from Gamma distribution and an emis-sion distribution from the symmetric Dirichlet.Finally the standard HMM procedure for generat-ing the data sequence follows.
At each time step:?
Generate the next tag conditioned on the lasttwo tags from the respective trigram HDP;?
Generate the word from the respective emissiondistribution conditioned on the tag just drawn;?
Generate the segmentation of the word deter-ministically by looking it up from the lexicon.3.2 Model setupThe trigram transition hierarchy is a HDP:GU ?
DP (?U , H) (5)GBj ?
DP (?B, GU ) j = 1 ?
?
??
(6)GTjk ?
DP (?T , GBj ) j, k = 1 ?
?
?
?, (7)where GU , GB and GT denote the unigram, bigramand trigram context DP-s respectively, ?-s are thew1 w2 w3s1 s2 s3t1 t2 t3 Gjk Gj GUEj jGj GS S.........Bj=1...THTSk=1...j=1...Figure 1: Plate diagram representation of the model.
ti-s, wi-s and si-s denote the tags, words and segmentationsrespectively.
G-s are various DP-s in the model, Ej-s and?j-s are the tag-specific emission distributions and theirrespective Dirichlet prior parameters.
H is Gamma basedistribution.
S is the base distribution over segments.Coupled DP concetrations parameters have been omittedfor clarity.respective concentration parameters coupled for DP-s of the same hierarchy level.
Emission parame-ters are drawn from multinomials with symmetricDirichlet priors:Ej |?j , H ??Mult(?)Dir(?j)d?
j = 1 ?
?
?
?,(8)where each emission distribution has its own Dirich-let concentration parameter ?j drawn from H .Morphological segments are modelled with an-other HDP where the groups are formed on the basisof tags:GS ?
DP (?S , S) (9)GTSj ?
DP (?TS , GS) j = 1 ?
?
?
?, (10)where GTSj are the tag-specific segment DP-s andGS is their common base distribution with S as basemeasure over all possible strings.
S consists of twocomponents: a geometric distribution over the seg-ment lengths and collapsed Dirichlet-multinomialover character unigrams.4 InferenceWe implemented Gibbs sampler to draw new val-ues for tags and Metropolis-Hastings sampler for re-sampling segmentations.
We use a type-based col-409lapsed sampler that draws the tagging and segmen-tation values for all tokens of a word type in one stepand integrates out the random DP measures by usingthe CRP representation.
The whole procedure alter-nates between three sampling steps:?
Sampling new tag value for each word type;?
Resampling the segmentation for each type;?
Sampling new values for all parameters.4.1 Tag samplingThe tags will be sampled from the posterior:P (T|W,S,w,?
), (11)where W is the set of words in the vocabulary, Tand S are tags and segmentations assigned to eachword type, w is the actual word sequence, and ?
de-notes the set of all parameters relevant for tag sam-pling.
For brevity, we will omit ?
notation in theformulas below.
For a single word type, this poste-rior can be factored as follows:P (Ti = t|T?i,S,W,w) ?P (Si|Ti = t,T?i,S?i)?P (Wi|Ti = t,T?i,W?i)?P (w|Ti = t,T?i,W),(12)where ?i in the subscript denotes the observationswith the i-th word type excluded.The first term is the segmentation likelihood andcan be computed according to the CRP formula:P (Si|Ti = t,T?i,S?i) =|Wi|?j=1?s?Si(n?Sitsn?Sit?
+ ?+ ?
(m?Sis + ?P0(s))(n?Sit?
+ ?)(m?Si?
+ ?
)),(13)where the outer product is over the word type count,nts and ms denote the number of customers ?eat-ing?
the segment s under tag t and the number oftables ?serving?
the segment s across all restaurantsrespectively, dot represents the marginal counts and?
and ?
are the concentration parameters of the re-spective DP-s. ?Si in upper index means that thesegments belonging to the segmentation of the i-thword type and not calculated into likelihood term yethave been excluded.The word type likelihood is calculated accord-ing to the collapsed Dirichlet-multinomial likeli-hood formula:P (Wi|Ti = t,T?i,W?i,w) =|Wi|?1?j=0ntWi + j + ?nt?
+ j + ?N(14)where ntWi is the number of times the word Wi hasbeen tagged with tag t so far, nt?
is the number oftotal word tokens tagged with the tag t and N is thetotal number of words in the vocabulary.The last factor is the word sequence likelihoodand covers the transition probabilities.
Relevant tri-grams are those three containing the current word,and in all contexts where the word token appears in:P (w|Ti = t,T?i,W) ?
?c?CWiP (t|t(c?2), t(c?1))?P (t(c+1)|t(c?1), t)?P (t(c+2)|t, t(c+1))(15)where CWi denotes all the contexts where the wordtype Wi appears in, t(c) are the tags assigned to thecontext words.
All these terms can be calculatedwith CRP formulas.4.2 Segmentation samplingWe sample the whole segmentation of a word typeas a block with forward-filtering backward-samplingscheme as described in (Mochihashi et al, 2009).As we cannot sample from the exact marginalconditional distribution due to the dependen-cies between segments induced by the CRP, weuse the Metropolis-Hastings sampler that drawsa new proposal with forward-filtering backward-sampling scheme and accepts it with probabilitymin(1, P (Sprop)P (Sold) ), where Sprop is the proposed seg-mentation and Sold is the current segmentation of aword type.
The acceptance rate during experimentsvaried between 94-98%.For each word type, we build a forward filter-ing table where we maintain the forward variables?
[t][k] that present the probabilities of the last kcharacters of a t-character string constituting a seg-ment.
Define:?
[0][0] = 1 (16)410?
[t][0] = 0, t > 0 (17)Then the forward variables can be computed recur-sively by using dynamic programming algorithm:?
[t][k] = p(ctt?k)t?k?j=0?[t?
k][j], t = 1 ?
?
?L,(18)where cnm denotes the characters cm ?
?
?
cn of a stringc and L is the length of the word.Sampling starts from the end of the word becauseit is known for certain that the word end coincideswith the end of a segment.
We sample the begin-ning position k of the last segment from the forwardvariables ?
[t][k], where t is the length of the word.Then we set t = t ?
k and continue to sample thestart of the previous to the last segment.
This pro-cess continues until t = 0.
The segment probabili-ties, conditioned on the tag currently assigned to theword type, will be calculated according to the seg-mentation likelihood formula (13).4.3 Hyperparameter samplingAll DP and Dirichlet concentration parameters aregiven vague Gamma(10, 0.1) priors and new valuesare sampled by using the auxiliary variable samplingscheme described in (Escobar and West, 1995) andthe extended version for HDP-s described in (Tehet al, 2006).
The segment length control parame-ter is given uniform Beta prior and its new valuesare sampled from the posterior which is also a Betadistribution.5 Results5.1 EvaluationWe test the POS induction part of the model onall languages in the Multext-East corpora (Erjavec,2010) as well as on the free corpora from CONLL-X Shared Task1 for Dutch, Danish, Swedish andPortuguese.
The evaluation of morphological seg-mentations is based on the Morpho Challenge goldsegmented wordlists for English, Finnish and Turk-ish2.
We gathered the sentences from Europarl cor-pus3 for English and Finnish, and use the Turkish1http://ilk.uvt.nl/conll/free_data.html2http://research.ics.tkk.fi/events/morphochallenge2010/datasets.shtml3http://www.statmt.org/europarl/text data from the Morpho Challenge 20094.
Es-tonian gold standard segmentations have been ob-tained from the Estonian morphologically annotatedcorpus5.We report three accuracy measures for tagging:greedy one-to-one mapping (1-1) (Haghighi andKlein, 2006), many-to-one mapping (m-1) and V-measure (V-m) (Rosenberg and Hirschberg, 2007).Segmentation is evaluated on the basis of standardF-score which is the harmonic mean of precision andrecall.5.2 Experimental resultsFor each experiment, we made five runs with ran-dom initializations and report the results of the me-dian.
The sampler was run 200 iterations for burnin,after which we collected 5 samples, letting the sam-pler to run for another 200 iterations between eachtwo sample.
We start with 15 segmenting iterationsduring each Gibbs iteration to enable the segmenta-tion sampler to burnin to the current tagging state,and gradually reduce this number to one.
Segmenta-tion likelihood term for tagging is calculated on thebasis of the last segment only because this settinggave the best results in preliminary experiments andit also makes the whole computation less expensive.The first set of experiments was conducted to testthe model tagging accuracy on different languagesmentioned above.
The results obtained were in gen-eral slightly lower than the current state-of-the-artand the number of tags learned was generally biggerthan the number of gold standard tags.
We observedthat different components making up the corpus log-arithmic probability have different magnitudes.
Inparticular, we found that the emission probabilitycomponent in log-scale is roughly four times smallerthan the transition probability.
This observation mo-tivated introducing the likelihood scaling heuristicinto the model to scale the emission probability up.We tried a couple of different scaling factors onMultext-East English corpus and then set its valueto 4 for all languages for the rest of the experi-ments.
This improved the tagging results consis-tently across all languages.4http://research.ics.tkk.fi/events/morphochallenge2009/datasets.shtml5http://www.cl.ut.ee/korpused/morfkorpus/index.php?lang=eng411POS induction results are given in Table 1.
Whencomparing these results with the recently publishedresults on the same corpora (Christodoulopoulos etal., 2011; Blunsom and Cohn, 2011; Lee et al,2010) we can see that our results compare favorablywith the state-of-the-art, resulting with the best pub-lished results in many occasions.
The number of tagclusters learned by the model corresponds surpris-ingly well to the number of true coarse-grained goldstandard tags across all languages.
There are twothings to note here: 1) the tag distributions learnedare influenced by the likelihood scaling heuristic andmore experiments are needed in order to fully under-stand the characteristics and influence of this heuris-tic; 2) as the model is learning the coarse-grainedtagset consistently in all languages, it might as wellbe that the POS tags are not as dependent on the mor-phology as we assumed, especially in inflectionallanguages with many derivational and inflectionalsuffixes, because otherwise the model should havelearned a more fine-grained tagset.Segmentation results are presented in Table 2.For each language, we report the lexicon-based pre-cision, recall and F-measure, the number of wordtypes in the corpus and and number of word typeswith gold segmentation available.
The reported stan-dard deviations show that the segmentations ob-tained are stable across different runs which is prob-ably due to the blocked sampler.
We give the seg-mentation results both with and without likelihoodscaling heuristic and denote that while the emissionlikelihood scaling improves the tagging accuracy, itactually degrades the segmentation results.It can also be seen that in general precision scoreis better but for Estonian recall is higher.
This canbe explained by the characteristics of the evalua-tion data sets.
For English, Finnish and Turkish weuse the Morpho Challenge wordlists where the goldstandard segmentations are fine-grained, separatingboth inflectional and derivational morphemes.
Espe-cially derivational morphemes are hard to learn withpure data-driven methods with no knowledge aboutsemantics and thus it can result in undersegmenta-tion.
On the other hand, Estonian corpus separatesonly inflectional morphemes which thus leads tohigher recall.
Some difference can also come fromthe fact that the sets of gold-segmented word typesfor other languages are much smaller than in Esto-6810121416182022240 100 200 300 400 500 600 700 800 900 1000?105 ?log(p)IterationJointPOS taggingSegmentationFigure 2: Log-likelihood of samples plotted against iter-ations.
Dark lines show the average over five runs, greylines in the back show the real samples.nian and thus it would be interesting to see whetherand how the results would change if the evaluationcould be done on all word types in the corpus forother languages as well.
In general, undersegmen-tation is more acceptable than oversegmentation, es-pecially when the aim is to use the resulting segmen-tations in some NLP application.Next, we studied the convergence characteristicsof our model.
For these experiments we made fiveruns with random initializations on Estonian cor-pus and let the sampler run up to 1100 iterations.Samples were taken after each ten iterations.
Fig-ure 2 shows the log-likelihood of the samples plot-ted against iteration number.
Dark lines show theaverages over five runs and gray lines in the back-ground are the likelihoods of real samples showingalso the variance.
We first calculated the full like-lihood of the samples (the solid line) that showeda quick improvement during the first few iterationsand then stabilized by continuing with only slow im-provements over time.
We then divided the full like-lihood into two factors in order to see the contribu-tion of both tagging and segmentation parts sepa-rately.
The results are quite surprising.
It turnedout that the random tagging initializations are verygood in terms of probability and as a matter of factmuch better than the data can support and thus thetagging likelihood drops quite significantly after thefirst iteration and then continues with very slow im-provements.
The matters are totally different withsegmentations where the initial random segmenta-tions result in a low likelihood that improves heavily412Types 1-1 m-1 V-m Induced True Best Pub.Bulgarian 15103 50.3 (0.9) 71.9 (3.8) 54.9 (2.2) 13 (1.6) 12 - 66.5?
55.6?Czech 17607 46.0 (1.0) 60.7 (1.6) 46.2 (0.7) 12 (0.8) 12 - 64.2?
53.9?Danish 17157 53.2 (0.2) 69.5 (0.1) 52.7 (0.4) 14 (0.0) 25 43.2?
76.2?
59.0?Dutch 27313 60.5 (1.9) 74.0 (1.6) 59.1 (1.1) 22 (0.0) 13 55.1?
71.1?
54.7?English 9196 67.4 (0.1) 79.8 (0.1) 66.7 (0.1 13 (0.0) 12 - 73.3?
63.3?Estonian 16820 47.6 (0.9) 64.5 (1.9) 45.6 (1.4) 14 (0.5) 11 - 64.4?
53.3?Farsi 11319 54.9 (0.1) 65.3 (0.1) 52.1 (0.1) 13 (0.5) 12 - - -Hungarian 19191 62.1 (0.7) 71.4 (0.3) 56.0 (0.6) 11 (0.9) 12 - 68.2?
54.8?Polish 19542 48.5 (1.8) 59.6 (1.9) 45.4 (1.0) 13 (0.8) 12 - - -Portuguese 27250 45.4 (1.1) 71.3 (0.3) 55.4 (0.3) 21 (1.1) 16 56.5?
78.5?
63.9?Romanian 13822 44.3 (0.5) 60.5 (1.7) 46.7 (0.5) 14 (0.8) 14 - 61.1?
52.3?Serbian 16813 40.1 (0.2) 60.1 (0.2) 43.5 (0.2) 13 (0.0) 12 - 64.1?
51.1?Slovak 18793 44.1 (1.5) 56.2 (0.8) 41.2 (0.6) 14 (1.1) 12 - - -Slovene 16420 51.6 (1.5) 66.8 (0.6) 51.6 (1.0) 12 (0.7) 12 - 67.9?
56.7?Swedish 18473 50.6 (0.1) 60.3 (0.1) 55.8 (0.1) 17 (0.0) 41 38.5?
68.7?
58.9?Table 1: Tagging results for different languages.
For each language we report median one-to-one (1-1), many-to-one(m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure.Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the mediannumber of tags induced by the model together with standard deviation.
Best Pub.
lists the best published results so far(also 1-1, m-1 and V-m) in (Christodoulopoulos et al, 2011)?, (Blunsom and Cohn, 2011)?
and (Lee et al, 2010)?.Precision Recall F1 Types SegmentedEstonian without LLS 43.5 (0.8) 59.4 (0.6) 50.3 (0.7) 16820 16820with LLS 42.8 (1.1) 54.6 (0.7) 48.0 (0.9)English without LLS 69.0 (1.3) 37.3 (1.5) 48.5 (1.1) 20628 399with LLS 59.8 (1.8) 29.0 (1.0) 39.1 (1.3)Finnish without LLS 56.2 (2.5) 29.5 (1.7) 38.7 (2.0) 25364 292with LLS 56.0 (1.1) 28.0 (0.6) 37.4 (0.7)Turkish without LLS 65.4 (1.8) 44.8 (1.8) 53.2 (1.7) 18459 293with LLS 68.9 (0.8) 39.2 (1.0) 50.0 (0.6)Table 2: Segmentation results on different languages.
Results are calculated based on word types.
For each languagewe report precision, recall and F1 measure, number of word types in the corpus and number of word types with goldstandard segmentation available.
For each language we report the segmentation result without and with emissionlikelihood scaling (without LLS and with LLS respectively).with the first few iterations and then stabilizes butstill continues to improve over time.
The explana-tion for this kind of model behaviour needs furtherstudies and we leave it for future work.Figure 3 plots the V-measure against the taggingfactor of the log-likelihood for all samples.
It canbe seen that the lower V-measure values are morespread out in terms of likelihood.
These points cor-respond to the early samples of the runs.
The sam-ples taken later during the runs are on the right inthe figure and the positive correlation between theV-measure and likelihood values can be seen.Next we studied whether the morphological seg-10.3510.410.4510.510.5510.610.6510.710.7510.810.8510.90.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5?105 ?log(p)V-measureFigure 3: Tagging part of log-likelihood plotted againstV-measure4131-to-1 m-to-1 V-mFixed seg 40.5 (1.5) 53.4 (1.0) 37.5 (1.3)Learned seg 47.6 (0.4) 64.5 (1.9) 45.6 (1.4)Precision Recall F1Fixed tag 36.7 (0.3) 56.4 (0.2) 44.5 (0.3)Learned tag 42.8 (1.1) 54.6 (0.7) 48.0 (0.9)Morfessor 51.29 52.59 51.94Table 3: Tagging and segmentation results on EstonianMultext-East corpus (Learned seg and Learned tag) com-pared to the semisupervised setting where segmentationsare fixed to gold standard (Fixed seg) and tags are fixedto gold standard (Fixed tag).
Finally the segmentatationresults from Morfessor system for comparison are pre-sented.mentations and POS tags help each other in thelearning process.
For that we conducted two semisu-pervised experiments on Estonian corpus.
First weprovided gold standard segmentations to the modeland let it only learn the tags.
Then, we gave themodel gold standard POS tags and only learned thesegmentations.
The results are given in Table 3.We also added the results from joint unusupervisedlearning for easier comparison.
Unfortunately wecannot repeat this experiment on other languagesto see whether the results are stable across differ-ent languages because to our knowledge there is noother free corpus with both gold standard POS tagsand morphological segmentations available.From the results it can be seen that the unsu-pervised learning results for both tagging and seg-mentation are better than the results obtained fromsemisupervised learning.
This is surprising becauseone would assume that providing gold standard datawould lead to better results.
On the other hand, theseresults are encouraging, showing that learning twodependent tasks in a joint model by unsupervisedmanner can be as good or even better than learn-ing the same tasks separately and providing the goldstandard data as features.Finally, we learned the morphological segmen-tations with the state-of-the-art morphology induc-tion system Morfessor baseline6 (Creutz and Lagus,2005) and report the best results in the last row ofTable 3.
Apparently, our joint model cannot beatMorfessor in morphological segmentation and when6http://www.cis.hut.fi/projects/morpho/using the emission likelihood scaling that influencesthe tagging results favorably, the segmentation re-sults get even worse.
Altough the semisupervisedexperiments showed that there are dependencies be-tween tags and segmentations, the conducted exper-iments do not reveal of how to use these dependen-cies for helping the POS tags to learn better morpho-logical segmentations.6 Related WorkWe will review some of the recent works relatedto Bayesian POS induction and morphological seg-mentation.One of the first Bayesian POS taggers is describedin (Goldwater and Griffiths, 2007).
The model pre-sented is a classical HMM with multinomial transi-tion and emission distributions with Dirichlet priors.Inference is done using a collapsed Gibbs samplerand concentration parameter values are learned dur-ing inference.
The model is token-based, allowingdifferent words of the same type in different loca-tions to have a different tag.
This model can actu-ally be classified as semi-supervised as it assumesthe presence of a tagging dictionary that containsthe list of possible POS tags for each word type -an assumption that is clearly not realistic in an unsu-pervised setting.Models presented in (Christodoulopoulos et al,2011) and (Lee et al, 2010) are also built onDirichlet-multinomials and, rather than defining asequence model, present a clustering model basedon features.
Both report good results on type basisand use (among others) also morphological features,with (Lee et al, 2010) making use of fixed lengthsuffixes and (Christodoulopoulos et al, 2011) usingthe suffixes obtained from an unsupervised morphol-ogy induction system.Nonparametric Bayesian POS induction has beenstudied in (Blunsom and Cohn, 2011) and (Gael etal., 2009).
The model in (Blunsom and Cohn, 2011)uses Pitman-Yor Process (PYP) prior but the modelitself is finite in the sense that the size of the tagset isfixed.
Their model also captures morphological reg-ularities by modeling the generation of words withcharacter n-grams.
The model in (Gael et al, 2009)uses infinite state space with Dirichlet Process prior.The model structure is classical HMM consisting414only of transitions and emissions and containing nomorphological features.
Inference is done by us-ing beam sampler introduced in (Gael et al, 2008)which enables parallelized implementation.One close model for morphology stems fromBayesian word segmentation (Goldwater et al,2009) where the task is to induce word borders fromtranscribed sentences.
Our segmentation model is inprinciple the same as the unigram word segmenta-tion model and the main difference is that we are us-ing blocked sampler while (Goldwater et al, 2009)uses point-wise Gibbs sampler by drawing the pres-ence or absence of the word border between everytwo characters.In (Goldwater et al, 2006) the morphology islearned in the adaptor grammar framework (John-son et al, 2006) by using a PYP adaptor.
PYP adap-tor caches the numbers of observed derivation treesand forces the distribution over all possible trees totake the shape of power law.
In the PYP (and alsoDP) case the adaptor grammar can be interpreted asPYP (or DP) model with regular PCFG distributionas base measure.The model proposed in (Goldwater et al, 2006)makes several assumptions that we do not: 1) seg-mentations have a fixed structure of stem and suffix;and 2) there is a fixed number of inflectional classes.Inference is performed with Gibbs sampler by sam-pling for each word its stem, suffix and inflectionalclass.7 ConclusionIn this paper we presented a joint unsupervisedmodel for learning POS tags and morphologicalsegmentations with hierarchical Dirichlet Processmodel.
Our model induces the number of POS clus-ters from data and does not contain any hand-tunedparameters.
We tested the model on many languagesand showed that by introcing a likelihood scalingheuristic it produces state-of-the-art POS inductionresults.
We believe that the tagging results couldfurther be improved by adding additional featuresconcerning punctuation, capitalization etc.
whichare heavily used in the other state-of-the-art POS in-duction systems but these features were intentionallyleft out in the current model for enabling to test theconcept of joint modelling of two dependent tasks.We found some evidence that the tasks of POSinduction and morphological segmentation are de-pendent by conducting semisupervised experimentswhere we gave the model gold standard tags and seg-mentations in turn and let it learn only segmentationsor tags respectively and found that the results in fullyunsupervised setting are better.
Despite of that, themodel failed to learn as good segmentations as thestate-of-the-art morphological segmentation modelMorfessor.
One way to improve the segmentationresults could be to use segment bigrams instead ofunigrams.The model can serve as a basis for several furtherextensions.
For example, one possibility would beto expand it into multilingual setting in a fashion of(Naseem et al, 2009), or it could be extended to addthe joint learning of morphological paradigms of thewords given their tags and segmentations in a man-ner described by (Dreyer and Eisner, 2011).AcknowledgmentsWe would like to thank the anonymous reviewerswho helped to improve the quality of this paper.This research was supported by the Estonian Min-istry of Education and Research target-financed re-search theme no.
0140007s12, and by European So-cial Funds Doctoral Studies and InternationalisationProgramme DoRa.ReferencesD.
Aldous.
1985.
Exchangeability and related topics.In E?cole d?e?te?
de Probabilite?s de Saint-Flour, XIII?1983, pages 1?198.
Springer.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590.Phil Blunsom and Trevor Cohn.
2011.
A hierarchicalPitman-Yor process HMM for unsupervised Part ofSpeech induction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, pages865?874.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: How far have we come?
In Proceed-415ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 575?584.Christos Christodoulopoulos, Sharo Goldwater, andMark Steedman.
2011.
A Bayesian mixture modelfor PoS induction using multiple features.
In Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing, pages 638?647, Edin-burgh, Scotland, UK.Alexander Clark.
2003.
Combining distributional andmorphological information for Part of Speech induc-tion.
In Proceedings of the Tenth Conference on Eu-ropean Chapter of the Association for ComputationalLinguistics - Volume 1, pages 59?66.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural language fromunannotated text.
In In Proceedings of the Inter-national and Interdisciplinary Conference on Adap-tive Knowledge Representation and Reasoning, pages106?113.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text using aDirichlet Process mixture model.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 616?627.Toma Erjavec.
2010.
MULTEXT-East version 4: Mul-tilingual morphosyntactic specifications, lexicons andcorpora.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation.Michael D. Escobar and Mike West.
1995.
Bayesiandensity estimation and inference using mixtures.
Jour-nal of the American Statistical Association, 90(430).Thomas S. Ferguson.
1973.
A Bayesian analysis ofsome nonparametric problems.
The Annals of Statis-tics, 1(2):209?230.Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam sampling for theinfinite Hidden Markov Model.
In Proceedings of the25th International Conference on Machine Learning,pages 1088?1095.Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-mani.
2009.
The infinite HMM for unsupervised PoStagging.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing:Volume 2 - Volume 2, pages 678?687.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised Part-of-Speechtagging.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages744?751, Prague, Czech Republic.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Interpolating between types and tokens byestimating power-law generators.
In Advances in Neu-ral Information Processing Systems 18, Cambridge,MA.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112:21?54.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 320?327, New YorkCity, USA.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2006.
Adaptor grammars: A framework for speci-fying compositional nonparametric Bayesian models.In Advances in Neural Information Processing Sys-tems 19, pages 641?648.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised POS tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 853?861.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested Pitman-Yor language modeling.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP:Volume 1 - Volume 1, pages 100?108.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual part-of-speechtagging: Two unsupervised approaches.
Journal of Ar-tificial Intelligence Research, 36:1?45.Lawrence R. Rabiner.
1989.
A tutorial on HiddenMarkov Models and selected applications in speechrecognition.
In Proceedings of the IEEE, pages 257?286.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 410?420,Prague, Czech Republic.Yee Whye Teh, Michel I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Yee Whye Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 985?992.416
