AbstractWe introduce a method for using images forword sense disambiguation, either alone, or inconjunction with traditional text basedmethods.
The approach is based in recent workon a method for predicting words for imageswhich can be learned from image datasets withassociated text.
When word prediction isconstrained to a narrow set of choices such aspossible senses, it can be quite reliable, and weuse these predictions either by themselves orto reinforce standard methods.
We providepreliminary results on a subset of the Corelimage database which has three to fivekeywords per image.
The subset wasautomatically selected to have a greaterportion of keywords with sense ambiguity andthe word senses were hand labeled to provideground truth for testing.
Results on this datastrongly suggest that images can help withword sense disambiguation.1 IntroductionIn this paper we investigate using words and pictures todisambiguate each other.
Word sense disambiguationhas long been studied as an important problem innatural language processing (Agirre and Rigau, 1995;Gale et al, 1992; Manning and Sch?tze, 1999; Mihalceaand Moldovan., 1998; Traupman and Wilensky, 2003;Yarowsky, 1995).
It is illustrated in Figure 1 with thearguably overused ?bank?
example.
A priori, the word?bank?
has a number of meanings including financialinstitution and a step or edge as in ?snow bank?
or?river bank?.
Words which are spelt the same but havedifferent meanings are very common, and clearly canconfuse attempts to automatically deduce meaning fromlanguage.
Furthermore, they cannot simply be identifiedas ambiguous and then ignored, as there are too manysuch words and they do constrain the possible meaningsof a body of text.Since the words are spelt the same, resolving whatthey mean requires considering context.
A purelynatural language based approach considers words nearthe one in question.
Thus in the bank example, wordslike ?financial?
or ?money?
are strong hints that thefinancial institution sense is meant.
Interestingly,despite much work, and a number of innovative ideas,doing significantly better than choosing the mostcommon sense is difficult (Traupman and Wilensky,2003).In this work we present preliminary work on whetheran associated images can help in word sensedisambiguation.
In the simplest application, text andimages might be analyzed in conjunction; for example,a news photograph with a caption, or a larger documentwith illustrations.Word Sense Disambiguation with PicturesKobus Barnard, Matthew JohnsonDepartment of Computing Science,721 Gould-Simpson, University of Arizona,Tucson, Arizona, 85721-0077{kobus, mjohnson}@cs.arizona.eduDavid ForsythComputer Science Division,University of California at Berkeley,387 Soda Hall #1776Berkeley California, 94720-1776daf@cs.berkeley.edupiggy bank coins currencymoneywater grass trees banksbank buildings trees citybank machine moneycurrency billssnow banks hills winterFigure 1.
Word sense ambiguity in the Corel dataset.2 Predicting Words from ImagesTo integrate image information with text data we exploitour previous work on linking images and words(Barnard et al, 2001; Barnard et al, 2003; Barnard andForsyth, 2001; Duygulu et al, 2002).
We havedeveloped a variety of methods which can be used topredict words for image regions (region-labeling), andentire images (auto-annotation).
This is achieved inpractice by exploiting large image data sets withassociated text.
Critically, we do not require that the textbe associated with the image regions, as such data israre.
Region labeling is illustrated in Figure 2.
It isimportant to understand that we compute a posteriorover the complete vocabulary for each region (and/orimage), but for illustration we show the word for eachregion which has maximal probability.For the results reported in this paper we use a specialcase of one of the models in (Barnard et al, 2003).Specifically, we model the joint probability of wordsand images regions as being generated by a collection ofnodes, each of which has a probability distribution overboth words and regions.
The word probabilities areprovided by simple frequency tables, and the regionprobability distribution are Gaussians over featurevectors.
We restrict the Gaussians to have diagonalcovariance (features are modeled as being independent).Given an image region, its features imply aprobability of being generated from each node.
Theseprobabilities are then used to weight the nodes for wordemission.
Thus words are emitted conditioned on imageregions.
In order to emit words for an entire image(auto-annotation), as needed for our word sensedisambiguation method, we simply sum thedistributions for the N largest regions.
Thus each regionis given equal weight, and the image words are forced tobe generated through region labeling.To be consistent with the more general modelsreferenced above, we index the nodes by ?levels?, l.Given a region (?blob?
), b, and a word w, we have?P(w | b) = P(w | l)P(b | l)P(l ) P(b)l?
(1)where P(l) is the level prior, P(w|l) is a frequency table,and P(b|l) is a Gaussian over features.
To estimate theconditional density of words given blobs for the entireimage these probabilities are summed over the N largestblobs.
Parameters for the conditional probabilitieslinking words and blobs are estimated from the word-blob co-occurrence data using ExpectationMaximization (Dempster et al, 1977).
For allexperiments reported in this paper we use 100 nodes.3 Constrained Word PredictionIn most of our applications we have studied wordprediction from images in the case where prediction isapplied to a completely new image with no associatedwords.
However, if the new image has associatedwords, our infrastructure for image/languageunderstanding can be exploited further.
Here the maintask is not word prediction, but understanding therelationship between the supplied words and the imagecomponents, and hence the meaning of both.
Since theset of words of interest is known, the rest of thevocabulary can be ignored in computation.
Constrainingthe vocabulary in this way makes a number of taskssimpler.
As much noise has been removed, therelationship between the words and the imagecomponents can be established more accurately.
Thesystem is now chiefly determining the correspondencerelationships between known text and image regions.One application is automatic labeling of images forsearching and browsing based on the semantics of theimage parts.
As shown in Figure 3, the labelingperformance is much improved when we can constrainthe vocabulary to largely relevant words.
The presenceof the words has removed ambiguity from theinterpretation of the image.The second application is, of course, thereverse?using the image to help reduce the ambiguityof the words.
We assume that the system has beentrained on a set of senses, S, for the vocabulary W. Toclarify the notation, we may have?bank ?
W and?bank _1,  bank _ 2 ?
S. Each element of S is the senseof exactly one word in W. If we have posteriorprobabilities over S based on the image, then for eachobserved word, w, we can look at the correspondingsenses for w in S, and provide the sense which has thehighest posterior among the senses.
More formally,Figure 2.
Illustration of labeling.
Each region islabeled with the maximally probable word, but aprobability distribution over all words is available foreach region.
(a)(b)Figure 3.
Illustration of the observation that our ability to predict word-region correspondences increasessignificantly when the words are constrained to a small set known to be relevant.
We show two groups ofimages which have a high probability of having a region associated with the word tiger, as computed by twodifferent processes.
The region in the images with the highest posterior probability of ?tiger?
is labeled as such.In both cases the images shown were not used for training.
In the top group (a) only image region features wereused to predict words.
In the bottom group (b), words associated with the images were also available to theprogram, and thus their the main task is to supply the correspondence between words and regions.
?P(s | w,I ) ?
P(s | I)P(s | w)(2)where?P(s | w) ?1 if  s  is  a  sense  of  w0  otherwise??????
(3)4 Word Sense from Images and TextTo integrate the above with traditional word sensedisambiguation, we assume that we have a text onlymethod which provides a better value for?P(s | w)in (2)than the trivial one defined in (3).
This strategy assumesthat the image provides information which isindependent of the text only method, allowing thesimplification from the factorization used in (2).For this work, we do not compute a true probabilityfor?P(s | w), but rather a score which we use as asurrogate.
As it is the goal of this work to show thatimprovement in word sense disambiguation is possible,we use a simple word sense disambiguation methodbased on the work in (Barnard et al, 2001) which itselftakes ideas from (Agirre and Rigau, 1995; Mihalcea andMoldovan., 1998).
Specifically we use the WordNetsemantic hierarchy (Miller et al, 1990) to define senses,and give higher weight to senses more closely aligned(lower in the tree) with the neighboring words.
In theexperiments described below, the words are part ofkeyword sets, and all keywords for an image areconsidered neighbours.For example, if we had the keywords [lion, pride,rock] for a picture of a group of lions on a rock, theword pride presents a problem to the disambiguator.
Byfar, pride's most common meaning is that regarding it asa deadly sin, however in this case we wish it to bedisambiguated as a group of lions.
Even so, one canlook at the different WordNet sense hierarchies for prideand find that one, namely:pride=> animal group=> biological group=> group, groupingcontains the words animal and biological, making it abetter fit for the hierarchy of lion.With this structure in mind, our algorithm takes theset of keywords and, for each keyword in the set,performs a query such as the one shown above.
Then,for each sense of the keyword, we perform the queriesfor the other keywords, and for each of their senses weexamine the similarities between their hypernym trees.We total up these similarities (shared nodes in the tree)and for each sense of the keyword produce a subtotal forthat sense.
After we have performed this operation forall senses we divide the subtotal by the complete totalfor all senses to receive a score for that sense as the truedefinition of the keyword.5 ExperimentsFor our initial test, we studied word sensedisambiguation on the Corel image dataset which wehave used extensively for studying word predictionfrom images.
Each Corel image has 3-5 keywordsassociated with it.
Unfortunately, these keywords areunusual in that they do not have much sense conflictover the data set.
Put differently, although a keywordlike ?head?
has many senses, one sense predominates inthis data set.To use the data despite this problem, we computedall the senses from WordNet  (Miller et al, 1990) of allthe words for an initial set of 16,000 images, togetherwith the score for each sense using the methoddescribed above.
We then applied some heuristics tocreate a subset of 1,800 images which had candidatesense problems.
Each word was then hand labeled withthe correct sense.
The resulting dataset had only ahandful of words with ambiguous senses present insufficient quantity, but fortunately these were commonenough such that about 1/6 of the documents had trueword sense problems.
The results reported below wererestricted to documents that had at least one word senseambiguity.The data set prepared as above thus consists of avocabulary of word-sense combinations, together with ahuman labeling of whether the sense was valid or not.We restrict the vocabulary to word-sense pairs whichoccur in at least 5 images.
We represent the observedsenses for a word occurring in a document as a vectorover the senses for that word from the vocabulary.
Wegive all relevant senses of the word a score of one, andincorrect senses a score of zero.
We normalize thisvector so that its sum is one.
Although potentially aword could be ambiguous to a human examiner,typically the word sense vector would simply contain asingle value of one, with the other values being zero.We evaluate word-sense disambiguation strategies bycomparing the vector of observed word-sensesdescribed above (the truth) to the vector containingestimates of the relevance of each word-sense paircorresponding to each occurring word.
For example,suppose an image has the word ?bank?, which maps tobank_1 with hand labeling, and suppose that bank_1and bank_3 are in the vocabulary, but no other senses ofbank.
Then the vector{?,bank_1(0.7),bank_3(0.3),?
}should be ranked better than{?,bank_1(0.3),bank_3(0.7),?
}when compared to the observed vector{?,bank_1(1.0),bank_3(0.0),?}
(hand-labeled)To compare the vectors we simply normalize them andtake the dot-product.For the experiments we divided the data into  trainingdata (75%), and test data (25%).
We averaged resultsfor 10 separate runs using different samples for the testand training sets.
We restricted the computation ofresults to those documents where there was clear senseambiguity.
Because each such document typically hadonly one sense problem amid 3 or 4 words withoutsense problems, the baseline score using the measureabove is greater than 0.80 because any strategy will getabout 3 out or 4 correct for free.
To clarify this further,we include the results of randomly chosen among senseswhen there is more than one available.The results are shown in Table 1 strongly suggestthat images can help disambiguate senses.
The na?vemethod of text based disambiguation is comparable tochance, whereas adding image information substantiallyincreased the performance.6 ConclusionThese preliminary studies strongly suggest that it isworthwhile to explore combining image informationwith more sophisticated text based words sensedisambiguation approaches.
However, while thepreliminary results are encouraging, it is critical that wetake the next step and apply the method to a data setwhere there is more sense ambiguity.
Possiblecandidates which we are actively investigating includethe museum data used in (Barnard et al, 2001) andnews photos with captions available on the web.In general we have found that it is fruitful to studyhow image and text information can both complimenteach other and disambiguate one another.
Differentrepresentations of the same thing can help learn co-constructed meaning.
Properties which may be implicitin one representation may be more explicit and thusmore amenable for automatic extraction in another.Furthermore, relationships between the representations,which can be learnt from large corpora, can be broughtto bear on the problem.
In particular, in the case ofdisambiguating words, we have shown that images canprovide a non-negligible amount of information whichcan be exploited by more traditional approaches.ReferencesAgirre, E. and Rigau, G., 1995.
A proposal for wordsense disambiguation using conceptual distance, 1stInternational Conference on Recent Advances inNatural Language Processing, Velingrad.Barnard, K., Duygulu, P. and Forsyth, D., 2001.Clustering Art, IEEE Conference on ComputerVision and Pattern Recognition, Hawaii, pp.
II:434-441.Barnard, K. et al, 2003.
Matching Words and Pictures.Journal of Machine Learning Research, 3: 1107-1135.Barnard, K. and Forsyth, D., 2001.
Learning theSemantics of Words and Pictures, InternationalConference on Computer Vision, pp.
II:408-415.Dempster, A.P., Laird, N.M. and Rubin, D.B., 1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 39(1): 1-38.Duygulu, P., Barnard, K., de Freitas, J.F.G.
and Forsyth,D.A., 2002.
Object recognition as machinetranslation: Learning a lexicon for a fixed imagevocabulary, The Seventh European Conference onComputer Vision, Copenhagen, Denmark, pp.
IV:97-112.Gale, W., Church, K. and Yarowsky, D., 1992.
OneSense Per Discourse, DARPA Workshop on Speechand Natural Language, New York, pp.
233-237.Manning, C. and Sch?tze, H., 1999.
Foundations ofStatistical Natural Language Processing.
MIT Press.Cambridge, MA.Mihalcea, R. and Moldovan., D., 1998.
Word sensedisambiguation based on semantic density,COLING/ACL Workshop on Usage of WordNet inNatural Language Processing Systems, Montreal.Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D. andMiller, K.J., 1990.
Introduction to WordNet: an on-line lexical database.
International Journal ofLexicography, 3(4): 235 - 244.Traupman, J. and Wilensky, R., 2003.
Experiments inImproving Unsupervised  Word SenseDisambiguation.
CSD-03-1227, Computer ScienceDivision, University of California Berkeley.Yarowsky, D., 1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods, 33rdConference on Applied Natural LanguageProcessing.
ACL, Cambridge.Word sense disambiguation strategy ScoreNa?ve text based method 0.858 (0.008)Random sense choice 0.875 (0.012)Image and text method 0.948 (0.015)Table 1.
Word sense disambiguation results on dataheld out from training.
The results are the average over10 runs where a different 75% of the data was used fortraining and the other 25% was held out for testing.Results were computed only on images with at leastone ambiguous term.
Because typically only one out offour or five words was ambiguous, the baseline score isquite high as reinforced by the random result.
Thescoring is explained in ?5.
Error estimates are inparentheses.
