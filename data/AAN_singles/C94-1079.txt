PRINCIPAR- -An  Efficient, Broad-coverage,  Pr inc ip le -basedParserDekang L inDepartment of Computer Science, University of ManitobaWinnipeg, Manitoba, Canada RaT 2N2, lindek@cs.umanitoba.caAbstractWe present an efI\]cient, broad-coverage,principle-based parser for English.
The parserhas been implemented in C++ and runs onSUN Sparcstations with X-windows.
It con-rains a lexicon with over 90,000 entries, con-structed automatically b  applying a set of ex-traction and conversion rules to entries frommachine readable dictionaries.1.
IntroductionPrinciple-based grammars, such as Govern-ment-Binding (GB) theory (Chomsky, 1981;Haegeman, 1991), offer many advantages overrule-based and unification-based grammars,such as the universality ofprinciples and mod-ularity of components in the grammar.
Prin-ciples are constraints over X-bar structures.Most previous principle-based parsers, e.g.,(Dorr, 1991; Font, 1991; Johnson, 1991), es-sentially generate all possible X-bar structuresof a sentence and then use the principles to fil-ter out the illicit ones.
The drawback of thisapproach is the inefficiency due 1;o the largenumber of candidate structures to be.
filteredout.
The problem persists even when w~rionstechniques such as optimal ordering of princi-ples (Fong, 1991), and corontining (Dorr, 1991;Johnson, 1991) are used.
This problem mayalso account for the fact that these parsers areexperimental nd have limited coverage.This paper describes an efficient, broad-coverage, principle-based parser, called PRIN-CIPAR.
The main innovation in PRINCIPARis that it applies principles to descriptions o17 X-bar structures rather than the structures them-selves.
X-bar structures of a sentence are onlybuilt when their descriptions have satisfied allthe pri ncil)les.O dynamicdata\ [~  staticdalal)rocegsingmoduledata flowFigure 1: '.Pile architecture of PRINCIPARFigure I shows the architecture of PRIN-CIPAR.
Sentence analysis is divided into threesteps.
The lexical analyser first converts the in-put sentence into a set of texical items.
Then,a message passing algorithm for OB-parsing isused to construct a shared parse forest.
Fi-nally, a parse tree retriever is used to enumer-ate the parse trees.The key idea of the parsing algorithm waspresented in (tin, 199:1).
This paper presentssome implementation details and experimentalresults.2.
Parsing by Message PassingThe parser in PIHNCIPAR is based on amessage-passing framework proposed by \],in(1993) and l,in and Ooebel (1993), which usesa network to encode the grammar.
The nodesin tile grammar network represent grammati-cal categories (e.g., NP, Nbar, N) or subcate-gories, such as V:NP (transitive verbs that takeNPs as complements).
The links in the net-work re.present relationships bel;ween the cat-egories.
GB-principles are implemented as lo-cal constraints attached to the nodes and482perco la t ion  cormtra in ts  attached to links inthe network.
F igure'2 depicts ~ port:ion C" tilegr ;unmar network for |Dnglish.'
2 \  I t " "IPcpspe~.. , - /~ / \~ iAAI ~ I'P " NI i VI :1 t .
.
".,....
"...".. ...." ""....A ul , P ,,,. '
.
.
.
.
.
.
.
.
.
.
':'....".v.v.v.v.v.v.v; .
.............. '"' "" ,~V:N~ V:(,~x 'adjullct dominance conlplement domln:lnce specializationspecifier doininailce head donlinanee barrierFigure 2: A Grtunma.r NetworkTh(;re ~u'e two types of l inks in 1,he network:subsumpt ion  l{nks and dominance  l inks.?
\[l.'here is a SlXi)sttln\[)tiOlX link \['rotn (v l;ofl if a subsume.s ft. For exa,ini)le , sinceV subsumes V:NP and V:CP, l;here is a,sul)smnption l ink from V to ca.oh ()11o, ofthem.?
There.
is a donxhia.nce l ink frolil node (v i.o/7 i f /7 cfl, ll })e imme.dia.tely doininal~ed byO& l.'~Ol ' CXi/dl lp lc,  SillCC a.IX Nl)a.r l i i&y i lt l-media?cly dominate  a. PP  adjimct,, t;hereis a dominance link from Nbar to pp.A dominance link fi:om a to fl is a.ssoci~tedwith an integer id that determii les tile linearorder between fl and other cat;egories dolni-m~t(xl t)y a, and a, binary att;ril)ute to specifywhether  fl is optional or oblig~l;ory.
It ln order to simplify the diagrain, we did nol.
labeltile links with their ids in l"igure 2.
\[nstead, the prece-dence between dominance links is ilMie~t>ed l)y theirInput sentences a.rc p;u'sed by passing me.s-sa.ges iu t,he gramm;u'  network.
'l.
'he nodes illthe nel, wor\]( are compul, ing agents t;lxi~t com-nulnica.t.e wil;h e;~ch oi l ier 1)y sending messa,gesin tile rcv(HJso direcl, ion of the links ilx the.
net-work.
I'\]acll node ha.s a. local n lemory tlxa.t,sDol'es a. set of it;ellx.~.
Ail il;em is a tr iplet thai;represe.nts a.
(possibly intern plei, e) X-ba, r strltc-i>ll I'(?
\ [ t :<str ,  ar t ,  s rc>,where~tr is an intx_'ger interva.l \[i,j\] denoi, ing t:ixe i'i~hLo j'l, tl word ill I, he ill\[)llt; still;el\]eel art  is theal;trilml,c vMues of the.
reel; node o\[ the X-barst;rtlCtAll:(':; ~Uid src is i'~ set o\[ St)Ill'CO mess~.~gesProm which this item is combined.
The  sourcei~lessa,ges represent inlinedi~te constituctlLs o\[the reel; node.
li',a.ch node in l, he grannil lu: net-work has a. conll) letion I)redicate tllal, deter-tllillCS whether a.n ilieln a.t l;lie node.
is "coin-plete," ilx w i lM i  ca.se the it;elXl is sent a.s a, ines-sltge 1;o el;tier l l()dOS i l l  1~110 \]X}VOI'SC direct ion ofthe links.~Vilen a, node receives mi itcnl> il; adiLel31pts{o (:onll)ine the itenl w i th  il;ems \['rein othernodes 1,o forln Hew il;enis.
'l~wo it;ores<\[ i , , j l \ ] ,  A , ,  S ,> a.nd <\ [ i2 , j2 \ ] ,  A,2, S~,>can I)e combilxed if? '
" a,(Ijacent o each \] l, heir Slll'\[a.ce sl, riilgs Arcel, her: i7 - :  jl-I-1.2.
t i ieir a.tl, r ibute vMues At mid A~ a.ret lHifli~ble..{{.
t i l e  SOtlrc(~ lTxessa,~es COTHe Vii/~ d i f fe .
rentIi,,ks: li,,ks(,g,) r~ li,,ks(S,~) =-- (k, wherelinks(,q) is a. I'illlC~iOlX {hal,> given i~ seto\[ nlessa.ges, returl is the sel; of l inks viawhich the iiicssa.ges a, rrived.
{l'he result o\[ I~ixe colnbinM;ion is a.
\[leW il;Oll;l:<\[il,.i~\], ,mil 'y(A,, A2), S, U S.~>.The new il;em represelxt:s a, la,rger N-ba, r sl;ruc-t,u re result;i ng from t, hc combinat ion of the twosnla.ller cues.
111 1;lie new it;era s<%isfles the lo-ca.l constraint, o\[ I;he node it is considered valida.nd sa.ved inl;o the local lnOIxlory.
()l:herwise, igis disca.rded.
A valid ito.nl si~t;isfying i;he com-sLarting poinl, s, e.g, (J precedes IP under Char sincethe link leading to (J is to I;he left, of t.he link leading1,o 1 P.48.7pletion predicate of the node is sent further asmessages to other nodes.The input sentence is parsed in the follow-ing steps.Step 1: Lex iea l  Look-up :  Retrieve the lex-ical entries for all the words in the sentenceand create a lexical item for each word sense.A lexical item is a triple: <\[i,j\], av~lf, av ..... p>,where \[i,j\] is an interval denoting the positionof the word in the sentence; av~lf is the at-tribute values of the word sense; and av,:o,,,, isthe attr ibute values of the complements of theword sense.Step 2: Message Passing: For each lexi-eel item <\[i,j\], av~lf, av ..... p>, create an initiMmessage <\[i,j\], av~r, 0> and send this messageto the grammar network node that representsthe category or subcategory of the word sense.When the node receives the initial message, itmay forward the message to other nodes or itma,y combine the message with other messagesand send the resulting combination to othernodes.
This initiates a message passing pro-cess which stops when there are no more mes-sages to be passed around.
At that point, theinitial message for the next lexical item is fedinto the network.Step 3: Build a Shared Parse ForestWhen all lexieal items have been processed, ashared parse forest for the input sentence canbe built by tracing the origins of the messagesat the highest node (CP or IP), whose str  com-ponent is the whole sentence.
The parse forestconsists of the links of the grammar networkthat are traversed uring the tracing process.The structure of the parse forest is similar to(Billot and Long, 1989) and (Tomita, 1986),but extended to include attribute values.The parse trees of the input sentence canbe retrieved h'om the parse forest one by one.The next section explains how tile constraintsattached to the nodes and links in the networkensure that the parse trees satisfy all the prin-ciples.3.
Implementat ion of P r inc ip lesGB principles are implemented as local andpercolation constraints on the items.
Lo-cal constraints are attached to nodes in thenetwork.
All items at a node must satisfythe node's local constraint, l?ercolation con-straints are attached to the links in the net-work.
A message can be sent across a link onlyif the item satisfies the percolation constraintof the link.We will only use two examples to give thereader a general idea about how GB principlesare interpreted as loc, al and percolation con-straints.
Interested reader is referred to Lin(1993) for more details.3.1.
Bound ing  rpheoryThe Bounding Theory (Subjaneency) statesthat a movement can cross at most one bar-rier without leaving an intermedia~te trace.
Anattribute named ~hbarr?0r is used to imple-ment this l)rinciple.
A message containingthe attribute value -whbarrier iS used to rep-resent an X-bar structure contMnlng a posi-tion out ol7 which a wh-constituent has moved,but without yet crossing a barrier.
The wdue+whbarrier means that the movement has M-ready crossed one barrier.
Certain dominancelinks in the network are designated as bar-rier links.
Bounding condition is implementedby tile percolation constraints attached to thebarrier links, which block any message with+whbarrier and change -whbarrior to +whbarrierbefore the message is allowed to pass through.3.2.
Case TheoryCase.
Theory reqlfires tha.t every lexicM NP beassigned an al)stl'act case.
'\]'he implementationof case theory in PI{,INCII~AII, is based on thefollowing attribute vaJues: ca, govern, cm.+ca the head is ,~ c~se assigner-ca the head is not a case assigner+govern the head is a governor-govern the head is not a governor-cr~ an NP m-commanded by thehead needs case markingThe case filter is implemented as follows:1.
LocM constraints attached to the nodesassign +ca to items that represent X-barstructures whose heads are case assigners(P, actiw.'
V, and tensed I).484...-No&~.
Local C<mstraint- -  l ) \] assign +ca to every item\[ assign +ca to items with-passzveassign +ca to items with tenseattril)nte\]';very item at NI' node is assigned ana.ttribute value -cm, which means thatl;he NI' represented by l, he item needs 1,obe case-marked.
The -cm al;tril)ute thenpropagates with tile item as it is sent toel;her nodes.
'\]'his item is said t<) be theorigin of the -cm attribute.Barrier links do not Mlow any item with-cm l;o pass through, \])ceause, once theitem goes beyond the 1)arri<:r, the originOf-era will not be governed, let alne case-marked.Since each node in X-1)ar strncture hasat most one governor, if the governor isnot a case assigner, the node will not l)ecase-marked.
Therei'ore, a case-filter vi-olation is detected if +govern -cm - ca  co-occur  in an item.
On the other han<l,if +govern +ca -cm co-ocetlr itl all item,+,;lien the  head  daughter of th<; it<,m gov-e rns  and  case:marks the origin of-cm.
'l'he case-filter condition on the origin of-cm is met.
'\]'he -cm attril)ute is cleared.The local constraints attached to all thenodes check for the ('.o-occurrences l ca,cm, and govern to ensure <:ase-filter is notviolated by any item.4.
Lex iconThe lexicon in PRINCIPAl{ consists of twohash tables: a primary one in memory and asecondary one on disk.
Tile secondary hash ta.=ble contains over 90,000 entries, most of whichare constructed automatically by applying aset of extraction and conw:rsion rules to etPtries in Oxford Adwmced \],eaner's l)ictionaryand Collins English I)ictionary.When a word is looked up, t;he F, rimaryhashtable is searched first.
If a,n entry for theword is found, the lexical search is done.
Oth-erwise, the secondary hash table is searched.The entry retrieved from the secondary LaI)Ieis inserted into the primary one, so, tha,t whenthe word is encouutered again only in-memorysearch will be necessary.The primary hash table is lc, aded from a filea.L l;he system start-up.
The file also serves as abuffer for changes to the secondary hash tM)le.When a lexical entry is ad(led or \]nc, dified, itis saved in the file for the prhnary hash table.The entry in the se<:(mdary hash tal)le remainsunchanged.
Since the i)rimary hash tM)le isa lw~ws consulted first, its entrios override the(;orresponditlg entries in the seco\[ldary La})\]C.The reason why the buffer in needed is thatthe secondary hash table is designed ill such away that update speed is sacrificed for the sakeof ef\[icie.t retriewd.
Therefore, updates to thesecondary hash tal)le should I>e done in batchand relatively infrequently.The tw(>tier organization of the lexicon istransparent to the l)arser.
That is, as far asthe.
parser is concerned, the lexic<m is an o1>jec{, that, given a word or a phrase, returns itslexical entry or ni l  if the entry (lees not exist inthe lexicon.
I,cxical rctrievM is very el\[icient,with over 90,000 entries, the average l;ime toretrieve an entry is 0.002 secon<l.4.1 .
Lexical  Ent r iesAll, hot@l the lexicon currently ttsed in I)I{IN -C'II>AI{, contains only syl~.tactic information, it;may also be used to hoM other types of ilffofmation.
Each lexical entry consists of ai1 eIltryword or phrase and a, list of functions with a,r-~tllllClltS:(< en~;ry-~ord-or-phras e>(<tune-name> <arg> .
.
.
<arg>)(<gunc-name> <arg> .
.
.
<art>)(<-June-name> <arg>.
.
.
<ar t>) )For exanq)le,(acknowledge(subcat ((cat v)) (((cat i) -bare inf)))(subcat  ( (ca t  v ) )  ( ( ( ca t  n) ( case  acc ) ) ) )(subcat  ( (ca t  v ) )  ( ( ( ca t  c ) ) ) )q'\]le f'/ltlctioII subcat  t'eturt/s a stll)c&|,egoriz&-Lion frame of the word.
The first argt l tne I l ( ;  oft}te function is the attrHmte va,lues of the word485itself.
The second argument of the function isa list of attribute value vector for the comple-ments of the word.
For example, the above en-try means that acknowl edge is a verb that takesan IP, NP or CP as the complement.
The lex-icon is extensible in that users can define newfunctions to suit their own needs.
Current im-plementation of the lexicon also includes func-tions ref and phrase, which are explained inthe next two subsections.4.2.
Reference Entr iesThe lexicon does not contain separate ntriesfor regular variations of words.
When a wordis not found in the lexicon, the lexleal retrieverstrips the endings of the word to recow~'r pos-sible base forms of the word and look them upin the lexicon.
For example, when the lc'xiealretriever fails to find an entry for "studies," itsearches the lexicon for "studie," "studi" and"study."
Only the last one of these has an en-try in the lexicon and its entry is returned.Irregular variations of words are explicitlylisted in the lexicon.
For example, there is anentry for the word "began."
IIowever, the snb-catgorization frames of "begin" are not listedagain under "began."
Instead, the entry con-tains a ref fimction which returns a referenceto the entry for "begin.
"(began(ref ((cat v) (vform ed) -prog-perf-passive(tense past))) (begin (cat))))The first argument of ref is the attribute val-ues of "began."
The second argument containsthe base form of the word and a set of at-tribute names.
The lexical items for the word"began" is obtained by unifying its attributevalues with the attribute wdues in the lexiea\]entry for "begin."
The advantage of makingreferences to the base form is that when thebase form is modified, one does not have tomake changes to the entries for its variations.4.a.
Phrasal Entr ies\]'he lexicon also allows for phrases that consistof multiple words.
One of the words in a phraseis designated as the head word.
The head wordshould be a word in the phrase that can un-dergo morphological changes and is the mostin frequent.
For example, in the phrase, "downpayment," the head word is "payment."
Ind~e lexicon, a phrase "wl .
.
.
wj .
.
.
.
w,,/' isstored as a s t r ing  "'Wh .
.
.
'tOn, 101 .
.
.
'U,~h_l.
"That is, the first word in the string is alwayshead word and the words Mter "," should ap-pear before the head word in texts.
The rune-don phrases converts il, s arguments into a listof phrases where tile entry word is the head.l,'or example, the lexical entry for "paymenC'is as follows:(payment(subcat ((cat n) (nform norm)))(phrases(payment, down)(payment, stop)(payment, token)(payment, transfer)))After retrieving the entry for a word, eachphrase in the phrase list is compared withthe surrounding words in the sentence.
If thephrase is found in the sentence, the entry forthe phrase is retrieved froin the lexicon.5.
Reducing AmbiguitiesOne of the problems with many parsers is thatthey typically generate far more parses thanhumans normally do.
I"or example, the averagenumber of parses pet' word is 1.35 in (l\]lacket al, 1992).
That means that their parserproduces, on average, 8 parses for a 7-wordsentence, 3d parses for a, l%word sentence, andld4 l)a.rses for a 17-word seiRe.nce, rphe la.rgenumber of parse trees make tim l~roe(,ssing atlater stages more dillicult and error l)ruTte.PI{INCII)AI{ defines a weight for everyparse tree.
A weight is associated with everyword sense and every link in the parse tree.\[Pile weight of the parse tree is the total weightof the links and the word senses ~tt the leafnodes of the tree.The packed shared parse forest in PtUN-CIPAI{.
is organized in such a way that theparse tree with minimum weight is retrievedfirst.
I~IUNCIPAII, then uses the minimumweight and a predetermined number calledBIGWEIGHT, which is currently arbitraryly de-fined to be 20, to prune the parse forest.
Only486the parse trees whose weights are less than(minimum weiglit -F BIGWEIGHT/2) are sparedand output.The weights of the links and word sensesare determined as follows:e 'I'he links fi'om Xbar to an ad,imlct YPhave weight=nlGWEIglIW and all the~other links have weight=l.0.?
The words in the lexicon ma,y havean attribute rar% which takes wduesfrom {very, very-very}.
If a word sensehas the attribute value (rare very), itsweight is BIGWEIGIIT.
I fa  word sensehas the attribute value (rare very-very),its weight is 2?BIGWEIGIIT.
Otherwise,the weight is 0,Note that the att;ribute rare is used to indicatethe relative frequency among different stmses ofthe same word./II~ /I L bigwe!ghtL ',John John V/~; NP'~/N p /~N~, about Kimread a/ ~b~r read /NP.a /)N barN I~Ps tory /X  N story about Kim(a) (b)Figure 3: Adjunct links ha,re higher weightsExample  5.1.
Comparing the two parses ofthe sentence "John read the story a,bout Kim"in Figure 3: in (a), lee about Kim\] is the co,n-plement of "story"; in (b), it is the a.djunct of"read".
Since the adjunct dominance link fromVbar to PP has much higher weight than thecomplement dominance link from Nba.r to PP,the total weight of (a) is much smaller them theweight of (b).
Therefore, only (a) is output asthe parse tree of the sentence.Example  5.2.
The lexical entry for tlm word"do" is as follows:7% 7%" .p v/,.
Who Z_~ /bar  Who (traCe)VKim \~, bigweight \/v%did NP NPlove (trace) A A(a) (b) Kim loveFigure 4: l,exical items have diffc,'ent weights(do(subcat ((cat i) -passive -per~ (auxform do)-prog (cgorm fin) (tense present)))(subcat ((cat v) (rare very))(((cat n) (case acc) (nform norm))))(subcat ((cat v) (rare very-very))(((cat n) (case ace) (nform norm))((cat n) (case acc) (nform norm))))'\]'ha.t is "do" (:a.n bc an auxiliary verb, a tran-sitive verb or a (li-trmlsitive verb.
\[,'igure lshows two parse trees for the sentence "Whodid Kim love?"
The parse l;ree (a) corrcsI)ondsto the correct; understanding of the sentence.hi (b), "did" is analyzed as a bi-tra,nsitivew,'b as in "Who did Kim a fawn'?"
llow-eww, since the latter sense of the word has anattribute value (rare very-very), tree (17) hasmuch higher weight tha,n tt'ee (a) and only (a,)is otd.lmt, by the i)ai's(~l ..6.
I rnp lementat ion  and Exper imenta lFtesult;sPRINCII~AR lms been implemented in C-I--I ~.The graphica,1 user interface is developed witha toolkit called interViews.
The program runson SUN Spa.rcstatlons with X-windows.
A ver-sion without; gral)hica, l user interface can alsobe run on most Unix machines with GNU g-f-t-compiler.l,iu m~d Coebel (1993) showed that theCOml)lexlty of the message passing algorithmis O(ICl',,.
:' ) ro,.
co.l;(.xt-f,:ee gra,,~,nars, wl.',','~.
is the length of input sc'utenco, \[C\[ is size487Table 1: Experimental ResultsExample sentencesWho do you think Bill saM Mary expected to seeI asked which books he told me that I should readThe petition listed the mayor's occup~ttion as attorney and his age a,s 71lie said evidence was obtained in violation o\[' the legal rights of citizensMr.
Nixon, for his part,  wouhl oppose intervention ill Cllba without specificprovocationThe ~Lssembly la.ngu~tge provides a means for w,'iting a progra.m and you are,not concerned with actual memory addresses "Labels can be assigned to a particular instruction step in a source programthat identify that step as an entry point for use in subsequent instructions* time (in seconds) taken on a Sparcstation \];~LC.- -  .
, I words \[ tmte* p~trses:10  -11 0.76i3 0.60 t413 0.55 4\]3 0.51 619 O.80 226 4.13 32of the grammar (measure by the number ofthe total length of the phrase structure rules).When attribute values are used in messages,the complexity of the Mgorithm is not yetknown.
Our experiments have shown that theparser is very fast.
Table 1 lists the parsingtime and the number of parses for several ex-ample sentences.
The correct parses for all thesentences in TM)le 1 are returned by the parser.Even though the lexicon is derived from ma-chine readable dictionaries and contains a \]a.rgenumber of senses for many words, the ratio be-tween the number of parse trees and the sen-tence length here is well bellow the ratio re-ported in (Black et al, 1992).AcknowledgementsThe author wishes to thanl?
Bonnie Dorr forcomments about Sections 1, 2, and 3.
This re'-search was supported by NaturM Sciences andEngineering Research Council of Canada grantOGP121338.ReferencesBerwick, I1.. C., Abney, S. P., and Tenny, C., edi-tors (1991).
Principle-Based Parsing: Com-putation and Psyeholinguislics.
Kluwer Aca-demic Publishers.Billot, S. and Lang, B.
(1989).
The structure ofshared forests in ambiguous parsing.
In Pro-ceedings of ACL-80, pages 143-151, Vancou-ver.Black, E., L~dDrty, J., and Roukos, S. (:1992).l)evelopment and ewduation of a broad-coverage probM)ilistic grammar of english-language computer manua.ls.
In Proceed-in:is of ACL-92, pages 1185-1192, Newark,l) alaware.Chomsky, N. (:1981).
Lectures on Government antiBinding.
Foris Publications, Cinnaminson,USA.Dorr, B. J.
(1991 ).
Principle-based pa.rsing for nla,-chhm translation.
In (Herwick et al, 1991),p~ges 15a <18,1.Fong, S. (i 991).
The computationM implementa-tion of principle-based parsers.
In (Berwiekel, al., 1991), pages 05-82.lla.egeman, L. (11997l).
lnl,roductio'n to Governmentm,f Binding Theory.
Basil \]~Ia.ckwell Ltd.,\]ohnson, M. (199l).
1)eductiw~ l>a.rsing: The useof knowledge o\[' la.ngua.ge.
In (l~erwiek el, al.,Ig.Ol), pages 39 64.l,in, 1).
(199"{).
Prhlciple-based parsing withoutow'.rgeneration.
In Proceedings of A 6'13-93,pages :112--120, Columbus, Ohio.Lin, I).
and Coebel, R. (1993).
Context-free gram-m~r parsing by message passing.
In Proceed-ings of l,he Fi'lwZ Cm@renee of the PacificAssociation for Comw.tational Linguistics,pages 203-211, V~mcouver, British Columbia.Tomit',~, M. (1986).
l'Jffieient Parsing for Nal,u-ral Language.
Kluwer Ac~u\[emic Publishers,Norwell, Massachusetts.488
